{
  "article_text": [
    "one of the challenging aspects of astrophysical simulations is to accurately and efficiently compute the gravitational potential @xmath0 for a given density field @xmath1 . for non - trivial cases ,",
    "it involves solving the well - known poisson equation :    @xmath2    which for simplicity is often solved in fourier space :    @xmath3    where @xmath4 is the wavenumber , and @xmath5 , @xmath6 , and @xmath7 are the fourier transform of the green s function , the potential , and the density , respectively . within the context of grid - based methods , adaptive mesh refinement ( amr ) can be crucial because of the large spatial ranges covered by self - gravitating systems .",
    "whilst solving equations  ( [ eq : poissonequation ] )  and  ( [ eq : poissonfourier ] ) is relatively trivial in uniform grid situations , it becomes much more difficult in multi - level nested simulations .",
    "many techniques have been proposed to compute the gravitational potential in such configurations ( see @xcite for a review ) .",
    "this includes tree - based method ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* ) , basis function expansions for specific geometries ( e.g. , * ? ?",
    "? * ; * ? ? ?",
    "here we focus on hierarchical mesh methods ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "the _ enzo _  code is an open - source multi - dimensional hydrodynamics and @xmath8-body grid - based code with amr @xcite . in the _ enzo _  code , gravity is solved using a particular method , which uses multigrid @xcite on refined patches .",
    "this method is fast , but has a number of shortcomings in certain situations , as we will demonstrate in this paper . an an alternative to that method",
    ", we present another method based on the adaptive particle - mesh ( apm ) algorithm derived from @xcite ( hereafter he87 ) , and @xcite , which allows a more accurate calculation of the potential when point potentials are employed .",
    "we implement this apm technique within the _ enzo _  code and compare its accuracy with the multigrid solver available by default . in section",
    "[ sec : method ] we describe the different algorithms for the multigrid and the apm gravity solvers .",
    "several test cases are performed and analyzed in section  [ sec : tests ] in order to compare both techniques .",
    "we summarize our results and conclude in section  [ sec : conclusion ] .",
    "to calculate the acceleration of the gas and the particles due to self - gravity , one must first compute the total gravitating field of the system .",
    "the particles are deposited into the @xmath9 nearest cells using either a cloud - in - cell ( cic , @xmath10 ) for the multigrid solver , or a triangular - shape cloud ( tsc , @xmath11 ) interpolation technique for the apm solver . for the cic deposition we use a cloud size equal to the target grid size , therefore switch the value of the parameter _ particlesubgriddepositmode _ to 0 .",
    "note that both cic and tsc algorithms are equivalent for a particle located at a corner of a cell .",
    "we have slightly modified the deposition algorithm for the particles .",
    "normally , particles that belong to a given grid or to any of its children are deposited onto that grid directly . more specifically , the particles density is copied into its _",
    "gravitatingmassfield_which contains the density contribution from the particles and the baryons .",
    "the _ gravitatingmassfield _  is larger that the grid , therefore can have boundary points set by the parent grid .",
    "thus we eventually need to copy the _ gravitatingmassfield _  of the parent grid onto the grid boundary ( for more details , see @xcite ) .",
    "we have found that this scheme could lead to the contribution from a particle located near the grid edges , being accounted twice  once from the grid itself and once from the parent grid .",
    "therefore we instead deposit the particles from the parent grid and the parent siblings also , and then copy the baryonic contribution from the parent grid only onto the grid boundary .    before mass deposition , the particles are advanced by half a timestep in order to obtain a time - centered density field . the density field generated by the particles",
    "is then added to the gas density of the cells which are also time - centered . at this point",
    ", the total gravitating mass field has been calculated and will be used to compute the gravitational potential .",
    "we describe this step for both solvers in the next two sections .      the default gravity solver implemented in _ enzo",
    "is based on a combination of fourier and multigrid algorithms .",
    "we recall here the basics of this method , but for more details , see @xcite .",
    "first the gravitating potential is computed on the root grid in fourier space using a fast fourier transform ( fft , he87 ) . for periodic boundary conditions ,",
    "the green s function is generated directly in fourier - space .",
    "for isolated boundary conditions , it is calculated first in real space to obtain the correct zero - padding , and transformed in the fourier domain @xcite . in both cases , the resulting potential",
    "is then transformed to the real domain and differentiated in order to obtain the face - centered ( cell - centered ) acceleration field depending on whether the hydro solver requires face - center or cell - centered accelerations . on the subgrids ,",
    "boundary conditions for the potential are obtained by interpolation from the parent grid and then equation  ( [ eq : poissonequation ] ) is solved using a gauss - seidel multigrid relaxation method with dirichlet boundary conditions ( he87 ) . as we will highlight in section  [ sec : tests ] , such algorithm may introduce errors on the refined grids due to inaccuracies in the coarse - grid solution .",
    "we should also emphasize that the particular multigrid solver described here is not strictly speaking a pure multigrid solver in the traditional sense . indeed , classic multigrid solvers calculate the solution on all levels at the same time .",
    "the apm solver is based on the particle - particle adaptive particle - mesh method ( he87 , * ? ? ?",
    "the general idea of the algorithm is to split the gravitational force between a long - range component , and one or more short - range components which are nonzero only for a narrow range of wavenumbers .",
    "the algorithm as described in this section is suitable for three - dimensional problems only , but could be extended in the future to treat one- and two - dimensional systems .",
    "the calculation on the root grid is nearly identical to the one performed with the multigrid solver ( although the greens function is slightly modified ) . on a refined grid",
    "one first calculates the short - range component of the force .",
    "therefore , the green s function needs to be modified in order to account for the contribution from the smallest scales only .",
    "we thus need a smoothing function , which here is the sphere with uniformly decreasing density @xmath12 : @xmath13 where @xmath14 is the radius and @xmath15 is a positive parameter .",
    "the corresponding fourier transform is @xmath16 where @xmath17 and @xmath18 . for each refinement level",
    "we take @xmath19 , where @xmath20 is the linear size of a grid cell on the current level @xmath21 .",
    "the effects of the smoothing function and the smoothing parameter have been studied extensively in he87 , who concluded that this profile gives a better accuracy in three - dimensional schemes and this chosen value for @xmath15 minimizes the numerical noise .    minimizing the error in the mesh force leads to the optimal green s function ( he87 , equation  8 - 22 ) : @xmath22 ^ 2 } ,      \\label{eq : influence}\\ ] ] where @xmath23 ^ 3      \\label{eq : brillouin}\\ ] ] are the brillouin zones , @xmath24 is the gradient operator , @xmath25 is the assignment function , and @xmath26 is the reference force . for the gradient operator we use either a direct fourier - space solver ( @xmath27 ) or a finite - difference representation ( see he87  for an explcit expression ) . for tsc deposition ,",
    "the assignment function @xmath25 is ( he87 , equations 8 - 42 ,  8 - 45 ) : @xmath28 and verifies @xmath29 finally , the reference force is linked to the smoothing function by : @xmath30 @xmath31 signifies the smoothing function with cell size on level @xmath32 . for the coarsest level ( @xmath33 ) , this becomes @xmath34 .",
    "note that , since @xmath35 and @xmath36 , we only consider the brillouin zones with @xmath37 ^ 3 $ ] in equation  ( [ eq : influence ] ) .",
    "the resulting potential is computed using equations  ( [ eq : poissonfourier ] )  and  ( [ eq : influence ] ) , and then differenced to obtain the acceleration in fourier space .",
    "an inverse fft is used to obtain the mesh force in real space .",
    "finally , the total acceleration field is computed by interpolating the acceleration field from the parent grid onto the refined grid , and adding it to the fine mesh force .",
    "the ffts used for the refined meshes require zero - padding , but because the short - range force is compact , we have found that we only require an extra @xmath38 cells , where @xmath39 is the usual refinement factor between levels . in this paper",
    "we always use @xmath40 .    for illustrative purposes a decomposition of the gravitational force",
    "is shown in figure  [ fig : apm ] for a particle located at the center of the computational domain .",
    "the resolution of the root grid is @xmath41 zones , and we add three levels of refinement with respective coordinates [ 0.1875;0.8125]@xmath42 , [ 0.3125;0.6875]@xmath42 , and [ 0.40625;0.59375]@xmath42 .",
    "the components of the force on each level are shown in figure  [ fig : apm ] , and compared to the analytical force @xmath43 , where @xmath14 is the distance from the central particle .",
    "we can clearly see how each subgrid contributes on the smallest scales only , while the larger scale components are computed on the levels above .",
    "note that the computed acceleration field is always cell - centered .",
    "it is therefore interpolated linearly when the euler equations are solved in case a staggered mesh hydro scheme is used .",
    "our implementation within the _ enzo _  amr code includes parallelization through the mpi library .",
    "this is relatively straightforward , as most of the density construction was already parallelized , with the largest change being communication of the coarse accelerations from the parent grid to the refine grid .",
    "in addition to parallelization , we have implemented a method to cache frequently used green s functions , so that they do not need to be recomputed for each new grid .",
    "in this section we perform a series of tests in order to compare both gravity solvers in terms of accuracy .",
    "all the tests are performed in parallel on four cores using our modified version of _ enzo _",
    "v2.3 on a macbook pro os x 10.8.5 with gnu compilers ( gcc 4.2.1 and gfortran 4.9.0 ) and openmpi 1.6.5 .",
    "we use the ppm scheme @xcite to solve the hydrodynamics equations .",
    "the coarse grid covers the entire three - dimensional computational domain which has coordinates [ 0;1]@xmath42 in code units .",
    "part of the domain is refined using nested grids .",
    "each subgrid is refined by a factor 2 in comparison with the resolution of its parent grid .",
    "grids can be therefore connected and organized using a tree where level 0 corresponds to the root grid , and all grids with the same resolution are located on the same level . for the tests that only involve particles , it might very well be that a grid does not contain any particle , thus preventing us from using the usual courant conditions to determine timestepping ( for more details , see * ? ? ?",
    "therefore we include an additional condition that constrains the timestep on level @xmath44 to be at most smaller than half of the timestep on level @xmath21 : @xmath45 note that this condition is automatically fulfilled if gas or particles are present in the grid .",
    "for all tests except for the sine wave problem ( section  [ subsec : sinewave ] ) we use isolated boundary conditions on the root grid .    in what follows",
    "we note @xmath46 is the mass of a given particle and @xmath47 is its density , where @xmath48 is the volume of a zone of the grid in which the particle is located .",
    "we first verify that there is no self force acting on a particle .",
    "this is done by modeling the evolution of an isolated particle that is given an initial velocity and goes through the different levels of refinement .",
    "if the particle does not experience any self force , it s velocity should not change .",
    "the root grid resolution is @xmath49 zones and there are three static levels of refinement with coordinates [ 0.1875;0.8125]@xmath42 , [ 0.3125;0.6875]@xmath42 , and [ 0.40625;0.59375]@xmath42 . the particle of mass @xmath50 is initially located on level 0 at coordinates @xmath51 , and is given a velocity @xmath52 .",
    "the system is evolved until @xmath53 , and we investigate two cases .    in the first case , we do nt allow any subcycling and all levels are advanced at a constant timestep @xmath54 . with the apm solver",
    ", the particle acceleration is found to be of the order @xmath55 on the deepest level , therefore completely negligible .    in the second case ,",
    "the system is evolved at a fixed timestep @xmath56 on level 0 , and with subcycles on the refined levels . using the added criterion for timestepping ( equation  [ eq : timestep ] )",
    ", the value of @xmath57 leads to the deepest level ( level 3 ) being evolved with a timestep similar to the constant timestep chosen in the first case .",
    "we show in figure  [ fig : selfforce ] the error on the particle velocity @xmath58 for both solvers .",
    "the error for both methods is still small , of the order @xmath59 , and the apm solver is slightly more accurate than the multigrid solver .",
    "however , allowing subcycles introduced an error due to the time - integration , error that is symmetric in all directions .",
    "we will witness this effect in section  [ subsec : testorbit ] as well .",
    "we thus conclude that the apm solver does not directly introduce any self - force but that evolving the different levels with different timestepping decreases the time - integration accuracy .",
    "this is essentially because with subcycling , the force contribution from the different levels are computed at different times .",
    "figure  [ fig : selfforce ] also demonstrates that the multigrid solver does introduce self - forces , in particular for particles located near the boundary of a refined region .      in this test",
    "we compute the acceleration of nearly massless particles in a gravitational field created by a heavy central particle .",
    "the dimensions of the root grid are @xmath41 , and we add one static refined grid .",
    "we set up at the center a particle of density @xmath60 , and @xmath61 test particles of mass @xmath62 distributed randomly in @xmath63 , where @xmath14 is the distance from the central particle .",
    "we study two cases : one in which the refined grid covers the region [ 0.4375;0.5625]@xmath42 such that the particle is at the center of the subgrid , and one in which the refined region is [ 0.5;0.5625]@xmath42 such that the particle is deposited on the corner of the subgrid . in both cases",
    "we evolve the system for @xmath64 .",
    "the different force components are plotted in figure  [ fig : gravitytest ] .",
    "the tangential component of the force @xmath65 should be zero while the radial component @xmath66 should follow the analytical result @xmath43 , but is softened for radii about two cell lengths .",
    "the largest inaccuracy in the force calculation is reached at the boundary of the nested grid ( @xmath67 ) .",
    "both solvers give an overall good result , as the relative mean errors of the radial and tangential accelerations are only of the order of a few percent .",
    "however , the transition is much smoother with the apm solver than with the multigrid solver . with",
    "the apm solver the overall noise in the radial force is reduced in comparison with the multigrid solver ( figure  [ fig : gravitytest ] ) .",
    "note that in the case where the massive particles is located at the edge of the refined grid , one can see how the smoothing of the potential starts at larger distances on the root grid than on the refined grid . moreover , the acceleration of the central particle in the first case is @xmath68 and @xmath69 for the multigrid and the apm solvers , respectively . in the second case , the respective accelerations are @xmath70 and @xmath71 .",
    "we therefore confirm that there is no self - force introduced by the apm solver , as demonstrated in section  [ subsec : selforce ] .",
    "we investigate here the error in a spherical distributions for which we can analytically calculate the potential .",
    "these are the sphere of constant density ( @xmath72 ) , the isothermal sphere ( @xmath73 ) and the plummer sphere ( @xmath74 ) .",
    "we note @xmath75 and @xmath76 are the total mass and radius of the sphere , respectively .",
    "the density distributions for the three cases are given by : @xmath77^{5/2 } } \\hspace{1 cm } r \\leq r , \\",
    "i = 3\\\\ \\\\ \\\\",
    "\\rho_{{\\rm out } } \\hspace{2.2 cm } r > r       \\end{array }      \\right.\\ ] ] where @xmath78 and @xmath79 .",
    "the corresponding potentials are obtained using gauss s law : @xmath80 \\hspace{0.70 cm } r \\leq r , \\",
    "i = 2\\\\ \\\\",
    "-\\frac{m}{r } \\left(\\frac{2\\sqrt{2}}{\\sqrt{1+r^2/r^2}}-1\\right ) \\hspace{0.25 cm } r \\leq r , \\",
    "i = 3\\\\ \\\\ \\\\",
    "-\\frac{m}{r } \\hspace{3 cm } r \\geq r       \\end{array }      \\right.\\ ] ]    in each case the dimensions of the root grid are @xmath41 and @xmath81 .",
    "we allow up to two levels of refinement and the domain is refined using the parameter _ minimummassforrefinement _ which we set to @xmath82 for @xmath83 .",
    "refinement occurs if the mass in a cell exceeds this parameter .",
    "the resulting hierarchies are plotted in figure  [ fig : grids_sphere ] .",
    "the two solvers produce comparable results , therefore we show in figure  [ fig : spheres - apm ] the results for the apm solver only . in all three cases",
    "the accuracy in both the radial and the tangential directions is at least at the percent level , except for the isothermal sphere at small radii .",
    "this is due to the fact that the gravitational acceleration is smoothed near the center because it diverges .",
    "in order to compare the gravity solvers with periodic boundary conditions , we study the case where the gas density is a sinusoidal function : @xmath84 which according to equation  ( [ eq : poissonequation ] ) leads to the periodic potential : @xmath85 where @xmath86 is the period .",
    "we use periodic boundary conditions and two static levels of refinement with coordinates @xmath87 ^ 3 $ ] ( level 1 ) , and @xmath88 ^ 3 $ ] ( level 2 ) .",
    "we perform simulations for @xmath89 and @xmath90 , and with @xmath91 zones on the root grid . solving the poisson equation with periodic boundary conditions",
    "requires that there is no net - mass in the computational domain .",
    "we thus subtract the average density of the system ( @xmath92 ) from the _ gravitatingmassfield _  before solving equation  [ eq : poissonfourier ] .",
    "the multigrid and the apm solvers give comparable results on this test .",
    "in figure  [ fig : poissonwave ] we therefore show the computed force and the relative error on the force along on the @xmath93-axis ( equation  [ eq : sinewave])for the apm solver only . for the case",
    "@xmath90 , the apm solver is quite accurate because the density distribution varies on larger scales . for the case",
    "@xmath89 , the accuracy slightly decreases .",
    "this behavior is due to the combined facts that the density varies on smaller scales and that the acceleration is smoothed by the tsc deposition and the @xmath12 function , leading to an effective loss of resolution .",
    "finally , we perform a two - body problem with the particles initially in a circular orbit in the @xmath93-@xmath94 plane . the central particle has a mass @xmath95 and is located initially at the center of the domain .",
    "the test particle has a mass @xmath96 and is placed at a distance @xmath97 from the central particle , at coordinates @xmath98 .",
    "the resolution of the root grid is @xmath49 zones and we study four different cases .",
    "we investigate four different cases .    * case 1 : particles in different levels with subcycling .",
    "+ in the first case , we allow one level of refinement .",
    "refinement is forced around the central particle such that the latter lives in level 1 .",
    "the timestep on the root grid is @xmath99 , and subcycles are allowed such that level 1 is evolved with a timestep @xmath100 .",
    "the system is evolved for 5 orbits with the multigrid solver , and for 10 orbits with the apm solver .",
    "* case 2 : particles in different levels without subcycling .",
    "+ this case is identical to the previous one except that all levels are evolved with a constant timestep @xmath101 .",
    "* case 3 : particles in the same level with subcycling .",
    "+ here we allow two levels of refinement and modify the refinement criterion such that both particles are located in level 2 . the binary system is evolved with a constant time step @xmath102 for one orbit with the multigrid solver , and 10 orbits with the apm solver .",
    "* case 4 : particles in the same level without subcycling .",
    "+ this final case is similar to case 3 but all the levels are updated at a constant timestep @xmath101 .",
    "the trajectories of the particles are showed for the different cases in figure  [ fig : testorbit ] .    in all cases",
    "the apm solver gives much more accurate results than the multigrid solver . with the multigrid solver",
    ", particles have already left the computational domain after less then five orbits ( figure  [ fig : testorbit ] , left column ) .",
    "note that the particles follow periodic boundary conditions .",
    "one should also highlight that subcycling has very little effect on the multigrid solver .    on the contrary",
    "the apm solver yields a much more accurate evolution of the system ( figure  [ fig : testorbit ] , right column ) .",
    "simulations in which subcycling is allowed ( cases 1 and 3 ) show a small resonance of the system : the center of mass of the system is shifted during the evolution .",
    "this behavior is most prominent in the cases where both particles are located in the same refined level ( figure  [ fig : testorbit ] , right column , row 3 ) .",
    "note that while the particles are located in the same level of refinement , they are usually not covered by the same grid .",
    "this resonance is due to the fact that the level where the particles live ( level 2 ) is evolved through 4 subcycles while the root grid level only goes through one , leading to a time - integration inaccuracy .",
    "more quantitatively , the orbital separation of the system has changed by about 3% at the end of the simulation ( case 3 ) .",
    "the center of mass has also moved by about 1% in the @xmath93 and @xmath94-directions .",
    "if subcycling is turned off and all levels are evolved with the same timestep , the results become much more accurate with the apm solver ( figure  [ fig : testorbit ] , right column , rows 2 and 4 ) . regarding case 4 ,",
    "the orbital separation of the system has changed by 0.7% only .",
    "moreover the center of mass is stable within a relative error @xmath103 .",
    "we thus conclude that the force calculation with the apm solver is quite accurate , and that resonance in the orbit can be avoided by forcing the different levels to be evolved with the same timestepping .",
    "+    +    +    +",
    "modeling accurately self - gravity represents a difficult part of astrophysical simulations . in this paper",
    "we have presented and tested an implementation of the adaptive particle - mesh solver based on the algorithm presented in @xcite and @xcite .",
    "the primary aim of such a technique is to provide a more accurate computation of the gravitational field at small scales .",
    "the solver has been implemented within the astrophysical code _ enzo _   but could be used in any other grid - based code with a structured mesh .    we have performed a series of tests in order to examine the accuracy of the apm solver , and compared the results with those obtained with the default multigrid solver implemented in _",
    "enzo_. for tests in which the gravitating material is distributed over a number of cells ( as in , for example , cosmological simulations ) the apm solver and the multigrid solver show similar accuracies .",
    "however , when a small number of particles are used ( such that the potentials are very steep ) , the apm solver provides much improved results . in particular , we show that in a two - body problem , the code can produce accurate orbits if all the levels are evolved with the same timesteps .",
    "an important aspect of the apm algorithm is timestepping .",
    "if the system is evolved with subcycling , the force components on different scales are evolved at different time , which leads to some inaccuracy in the time - integration , even though the force calculation on a given level is computed to high accuracy .",
    "this behavior can be seen in particular for the test orbit problem ( section  [ subsec : testorbit ] ) .",
    "one solution to get rid of these spurious effects is to disable subcycling and evolve all levels with the same timestepping",
    ". this will increase the computational cost of the calculation , but it may only be a factor of a few depending on the grid hierarchy .",
    "such extra accuracy may also only be needed for simulations with only a small number of particles , such as the testorbit problem .",
    "indeed , the multigrid solver is usually sufficiently accurate with a large number of particles .",
    "finally , one should recall that the multigrid algorithm relies on interpolating the potential values of the root grid onto the refined grid , and using a relaxation method until convergence is reached . in the apm algorithm ,",
    "ffts are used on the refined levels .",
    "consequently , the work load on each processor is larger and one expect the apm solver to be slower than the multigrid solver .",
    "however , the potential values do not need to be communicated between overlapping grids so communication between processors are decreased .",
    "therefore , we also expect the apm solver to scale efficiently to a higher number of processors .",
    "the study of the performance of the apm solver is beyond the scope of this paper , but could be investigated in greater detail in the future .",
    "unlike the multigrid solver , the apm implementation has not been optimized for parallel performance yet as it does not use the mpi non - blocking communications ( for more details , see @xcite ) . as an indication ,",
    "the runtime for the testorbit problem with the apm solver is about three to four times as long as with the multigrid solver .",
    "j - cp acknowledges funding from nsf grant ast-0607111 and from the alexander von humboldt foundation .",
    "glb acknowledges support from nsf grant 1008134 and nasa grant nnx12ah41 g .",
    "j - cp thanks norbert langer , mordecai - mark mac low , falk herwig , and orsola de marco for their support . we are thankful to the referee for useful comments that improved the clarity of the paper , and to colin p. mcnally for suggesting the sine wave test presented in this paper ."
  ],
  "abstract_text": [
    "<S> we describe and implement an adaptive particle - mesh algorithm to solve the poisson equation for grid - based hydrodynamics codes with nested grids . the algorithm is implemented and extensively tested within the astrophysical code _ enzo_against the multigrid solver available by default . </S>",
    "<S> we find that while both algorithms show similar accuracy for smooth mass distributions , the adaptive particle - mesh algorithm is more accurate for the case of point masses , and is generally less noisy . we also demonstrate that the two - body problem can be solved accurately in a configuration with nested grids . </S>",
    "<S> in addition , we discuss the effect of subcycling , and demonstrate that evolving all the levels with the same timestep yields even greater precision . </S>"
  ]
}