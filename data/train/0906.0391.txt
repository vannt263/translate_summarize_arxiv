{
  "article_text": [
    "the problem of similarity search in databases is addressed by building indexing schemes of various types @xcite .",
    "the goal of such structures is that a search algorithm can exploit them to perform similarity search in time sublinear in the database size . that indexing schemes do not scale well with increasing dimension",
    "has been referred to as `` the curse of dimensionality '' @xcite .",
    "we feel that in order to gain a better insight into the nature of the curse of dimensionality , it is necessary to have a precise mathematical understanding of the geometric and algorithmic aspects of what happens in genuinely high - dimensional datasets . with this purpose ,",
    "we have chosen to analyse one of the most popular indexing schemes for similarity search , the one based on pivots @xcite .",
    "the mathematical setting for our analysis is a rigorous model of statistical learning theory @xcite , where datasets are drawn randomly from domains of increasing dimension .",
    "this probabilistic setting is similar to that used in a previous asymptotic analysis of similarity search @xcite .",
    "we also adopt a cost model where we count distance computations only , in line with @xcite . unlike this previous work ,",
    "we let both the dimension @xmath0 and the size of the dataset @xmath1 grow as described in @xcite .",
    "we also make the distinction between the dataset and the data space mathematically explicit .",
    "in particular we emphasize that statements of the type `` all indexing scheme will degenerate to linear scan with increasing dimension '' ( to paraphrase @xcite ) will always need to be qualified with estimates of the probability . for it is not impossible to sample a hypercube uniformly and",
    "come up with a `` distribution with a million clusters''@xcite .",
    "our analysis is done on a sequence of ( data ) spaces that exhibit the _ concentration of measure _ phenomenon @xcite ( sect .",
    "[ s : conc ] ) , a concept linked to what is called in @xcite _ workloads with vanishing variance_. it is also in terms of this concentration of measure that we define the dimension @xmath0 . to show that the above situation with a million clusters can not happen ( too often ) we study the convergence of empirical probabilities to their true values using a result from statistical learning theory @xcite .",
    "we introduce a property of a sequence of spaces which is sufficient to invoke this result .    the conclusion of our analysis ( sect .",
    "[ s : main ] ) is that for high dimensional datasets the class of pivot - based indexing schemes can not significantly outperform the baseline linear scan of checking every element of the database .",
    "we model the dataset as a sample of a metric space with measure .",
    "metric _ ( or : _ distance _ ) on a set @xmath2 will be denoted by @xmath3 , and we will not remind the definition .",
    "the ( open ) ball of radius @xmath4 and centre @xmath5 in a metric space @xmath6 is denoted @xmath7 the family @xmath8 of _ borel subsets _ of a metric space @xmath6 is the smallest family containing all the open balls and the entire set @xmath9 and closed under complements and countable unions .",
    "a _ ( borel ) probability measure _ on the space @xmath6 is a function @xmath10 $ ] s.t .",
    "@xmath11 , and which is countably additive : for a sequence @xmath12 of pairwise disjoint sets from @xmath13 , @xmath14 a dataset however large is always a finite subset @xmath15 .",
    "it naturally inherits the metric @xmath16 and in place of @xmath17 supports the _ normalized counting _ ( also : _ empirical _ ) probability measure : @xmath18 we will treat @xmath2 as a sample of @xmath9 with regard to the measure @xmath17 , that is , a sequence of i.i.d . random variables @xmath19 .",
    "given a domain together with a dataset @xmath20 , we can perform several kinds of similarity queries , with our focus on two main ones .",
    "@xmath21 nearest neighbour query _ consists of , given query centre ( key ) @xmath22 , finding the k closest elements in @xmath2 to @xmath5 . to answer",
    "a _ range query _ is to find all the elements in @xmath2 within distance @xmath4 from @xmath5 .",
    "to distinguish between @xmath2 and @xmath9 formally is necessary precisely because a typical search will begin with a centre @xmath22 , with @xmath23 as well being rare .    to answer a similarity query we can revert to the strategy of looking up every element in @xmath24 and calculating @xmath25 .",
    "following @xcite , we adopt the number of distance calculations @xmath25 as the unit of time complexity . in that framework",
    "we will call the above strategy a _ linear scan . _",
    "an _ indexing scheme _ is a structure whose aim is to speed up the execution of similarity queries on a particular dataset , typically consisting of some pre - calculated values and an algorithm .",
    "an often repeated observation is the inability of many existing algorithms to deal with high dimensional datasets ( e.g. @xcite) a phenomenon described as the _ curse of dimensionality _ , when performance drops exponentially as a function of dimension .",
    "the concept of dimension in a general metric space with measure is less precise .",
    "clearly it has to obey our intuition in euclidian space so for example a plane in the 10-dimensional space @xmath26 is still 2-dimensional , and it would be desirable for a uniformly distributed ball in @xmath27 to be @xmath0-dimensional .",
    "a version of intrinsic dimension was proposed by the authors @xcite as @xmath28 where @xmath29 , the distribution of points in @xmath9 .",
    "it is based on the observation that if the histogram of distances from @xmath5 to points in @xmath2 shows a lot of `` concentration '' , this will be a hard query to process as it is harder to rule out points using a triangle inequality type approach . that the above dimension is asymptotically equal to the usual notion in euclidian spaces is mentioned in @xcite , where a result on time complexity of search in term of @xmath30 is also stated .",
    "it is a lower bound on the order of @xmath31 .    in this article",
    ", we will use another approach to the intrinsic dimension , elaborated in @xcite and also based on the phenomenon of concentration of measure , cf .",
    "[ s : conc ] .    in general the time complexity we are looking for in search",
    "depends both on dimension ( henceforth we will simply call it @xmath0 ) and size of dataset @xmath1 .",
    "an asymptotic analysis of the performance of indexing schemes will therefore involve both @xmath32 and @xmath33 .",
    "search in sublinear time in @xmath1 is an obvious goal : @xmath34 where by querytime we mean the average time it takes for a similarity query to execute , time measured in distance computations .",
    "storage is also important , with at most polynomial storage allowed in theoretical analysis ( though in practice even @xmath35 may be too much ) : @xmath36 for the pivot - based indexing scheme the storage will be measured by the number of distances _",
    "we will follow an approach in the authoritative survey by @xcite and focus on a particular range for rate of growth for dimension @xmath0 , superlogarithmic but subpolynomial in @xmath1 : @xmath37 @xmath38 this choice of bounds is due to a case study of the hamming cubes . recall that the _ hamming cube _",
    "@xmath39 of dimension @xmath0 is the set of all binary sequences of length @xmath0 , and the distance between two strings is just the number of elements they do nt have in common divided by @xmath0 : @xmath40 ( the _ normalized hamming distance _ ) . in the case where @xmath0 grows slowly , @xmath41 , all possible queries can be pre - computed and stored without breaking the polynomial storage requirement . hence the lower bound .",
    "the upper bound results from the observation that if @xmath0 grew so fast that @xmath42 , a sequential scan would be polynomial in @xmath0 and so acceptable . summarizing : _",
    "the goal of finding a scalable index is to find polynomial ( preferably degree less than 2 ) @xmath1 storage algorithm that allows search in polynomial @xmath0 time .",
    "_    this stands in contrast to the _ curse of dimensionality conjecture _ , as stated in @xcite :    _ if @xmath43 and @xmath44 , any sequence of indexes built on a sequence of datasets @xmath45 allowing exact nearest neighbour search in time polynomial in @xmath0 must use @xmath46 space . _",
    "the conjecture remains unproven in the case of general indexing schemes .",
    "the goal of this article is to show that at least for pivot - based indexes the above conjecture holds even in a strengthened form .",
    "we will focus on one class of indexing schemes , the pivot - based index ( e.g. aesa , mvpt , bkt, ... see @xcite and @xcite ) .",
    "the index is built using a set of _ pivots _ @xmath47 from @xmath9 , and consists of the array of @xmath48 distances @xmath49 given a range query with radius @xmath4 and centre @xmath5 , the @xmath21 distances @xmath50 are computed so that @xmath25 can be lower - bounded by the triangle inequality : @xmath51 it is useful to think of a new distance function , @xmath52    the fact that @xmath53 can be used to discard all @xmath54 satisfying @xmath55 . for the remaining points ,",
    "the algorithm will verify if @xmath56 .",
    "if it is true , the point is returned .",
    "we will only analyze _ range _ queries ; @xmath21-nearest neighbour queries can always be simulated by a range query of suitable radius @xcite .    for a query centre @xmath5 denote by @xmath57 all the points of @xmath2 satifying @xmath55 , i.e. all the elements to be discarded .",
    "making @xmath57 large is the primary way of cutting the cost of search in our cost model . of course we can achieve this trivially with a very large number of pivots",
    "this will defeat the purpose however as @xmath58    the most often used solution is to keep adding pivots as long as it is found experimentally to decrease the cost of search .",
    "if @xmath21 is small , on the order of @xmath59 ( as often space limitations require ) , the most important component of cost becomes the size of @xmath60 and this is where the choice of pivots would seem to matter .",
    "various approaches to pivot selection have been investigated in @xcite .",
    "the empirical results seem to suggest that a moderate reduction in the number of distance computations can be achieved , although the relative improvement drops with increasing dimension .",
    "there are indexing schemes , like aesa @xcite where @xmath61 .",
    "however , in many situations @xmath35 storage is not practical , and it has even been argued that under certain assumptions the optimal number of pivots is on the order of @xmath62 @xcite .",
    "it is also true that the query algorithm we analyze has complexity at least @xmath21 so only schemes with @xmath63 can claim to beat the curse of dimensionality .",
    "perhaps the most compelling way to describe the concentration of measure phenomenon is to draw a picture . we will attempt to draw the ( surface of the ) unit sphere @xmath64 for various @xmath0 , by sampling points and projecting them onto a flat surface . any orthogonal projection ,",
    "say taking the first 2 coordinates , will give the picture similar to that in figure [ figure : spheres ] . under the sampling approach",
    ", it appears that high dimensional spheres are `` small '' even if we know their diameter to be a constant irrespective of @xmath0 .",
    "this phenomenon is observed in a much greater variety of situations and formalized as follows .",
    "given a metric space @xmath65 , define the @xmath66-neighborhood @xmath67 of @xmath68 as @xmath69    the _ concentration function _",
    "@xmath70 of a metric space with measure @xmath71 is defined as @xmath72    to put it less formally , we are trying to measure how much of the space remains after `` fat '' is added to a somewhat large set in the form of an @xmath66 neighborhood . when very little remains , we say that the concentration of measure takes place .    the spheres @xmath64 in @xmath73 , taken with the geodesic or euclidian distance and the normalized invariant measure , produce a concentration function bounded as follows @xcite : @xmath74        in this case an exact expression for the concentration function is known @xcite , based on the fact that the half - sphere , among all subsets of measure at least @xmath75 , will always produce the smallest @xmath66-neighborhood , no matter the @xmath66",
    ". a plot of the resulting concentration functions , for several values of @xmath0 , appears in figure [ figure : spheresconc ] .",
    "a sequence of spaces @xmath76 is a _ normal lvy family _ @xcite if @xmath77 , @xmath78 exist such that @xmath79    the balls @xmath80 , taken with the euclidian distance and the uniform probability measure ( @xmath0-dimensional lebesgue ) , form a normal lvy family .",
    "the hamming cubes @xmath39 form a normal lvy family under the normalized hamming metric and the uniform measure .",
    "the concentration of measure can be equivalently described in terms of lipschitz functions . recalling that a function @xmath81 is _",
    "1-lipschitz _ if @xmath82 recalling further that a _ median _ of function @xmath83 is any number @xmath84 satisfying : @xmath85 it is then relatively straightforward to prove :    [ thm : lip ] for a 1-lipschitz function f defined on space @xmath86 : @xmath87    the relevance of concentration of measure in indexing is noted in @xcite .",
    "observe that @xmath88 is 1-lipschitz for any @xmath89 and in particular a pivot .",
    "hence theorem [ thm : lip ] can be applied to obtain a bound on the deviation from the median @xmath90 of function @xmath91 : @xmath92 we combine these statements for all pivots @xmath93 : @xmath94 as the probability of the union can always be upperbounded by the sum of the probabilities .",
    "we note that no assumptions about independence are used : the sequence @xmath95 can be chosen in any way .",
    "next , for all query centres @xmath5 except a set of measure @xmath96 : @xmath97 we could introduce a set @xmath98 and think of @xmath57 as the observation of @xmath99 under @xmath100 . to recap : for a randomly chosen query centre and each query radius @xmath101 , with probability @xmath102 , @xmath103    we point out that theorem [ thm : lip ] applied to the distance function @xmath3 gives a bound on the variance of @xmath104 .",
    "this , together with a `` uniformity of view '' type assumption as in @xcite leads us to conclude that the variance of @xmath105 converges to zero in lvy families .",
    "this argument can be formalized to demonstrate the connection to the assumption of vanishing variance on the sequence of data spaces made in @xcite . in our view that assumption is just a variation on concentration of measure .",
    "the differences lie in certain technical details , like the division by expectation of @xmath105 in @xcite . here",
    "we simply avoid the issue by normalizing spaces so that the expectation of @xmath105 tends to a constant .",
    "this normalization also fixes the problem of distance to nearest neigbour ( e.g. @xcite ) as we demonstrate in the next section .",
    "in our asymptotic analysis , we would like to normalize spaces so that the median distance between two points stays about the same . here",
    "we will extract consequences for the typical radius of a query  which we will assume to be the distance to the nearest neighbour of query centre .",
    "@xcite[l : m ] let @xmath106 denote a metric space with measure and @xmath107 its concentration function",
    ". then if @xmath68 is such that @xmath108 for some @xmath109 , it implies that @xmath110 .",
    "let @xmath111 be a sequence of metric spaces with measure , forming a lvy family , together with i.i.d .",
    "samples @xmath112 .",
    "assume that @xmath113 .",
    "furthermore , if @xmath114 denotes the median value of @xmath115 , we assume that @xmath116 , that is , for some fixed @xmath117 , @xmath118 .",
    "let @xmath119 denote the distance to the nearest neighbour of @xmath120 in @xmath112 .",
    "define @xmath121 to be the median of @xmath119 .",
    "then there exists some @xmath122 and some @xmath123 such that @xmath124 , @xmath125 .",
    "assume the conclusion fails , then without loss of generality and proceeding to subsequence if necessary , @xmath126 .",
    "by definition of @xmath121 , we know that for any @xmath0 , @xmath127 it follows that @xmath128 and so we can find for any @xmath0 a point @xmath129 such that @xmath130 if we denote by @xmath131 the concentration functions of our spaces @xmath132 we know by assumption the existence of @xmath133 s.t . @xmath134 hence we can find @xmath135 s.t .",
    "@xmath136 and @xmath137 , where @xmath138 as well",
    ". by lemma [ l : m ] @xmath139 it then follows that @xmath140 that is , since @xmath141 , @xmath142 but @xmath143 , so in @xmath144 the measure of the set of points @xmath145 for which @xmath146 is at least @xmath147 obviously contradicting @xmath148 .",
    "this result frees us from having to consider a radius that vanishes as @xmath1 , @xmath0 go to infinity . with this achieved ,",
    "let us recap our goal : to show that a large proportion of queries are slow , something along the lines of : @xmath149 where the median is taken over all the queries under consideration : any @xmath150 and any @xmath4 at least as large as the distance to the nearest neighbour of @xmath5 in @xmath2 .",
    "as well , for each @xmath0 and @xmath151 we would like to also consider all possible pivot - based index schemes ( as long as @xmath21 is within certain ranges we will specify later ) .",
    "so far we have shown , although the proof was just sketched ( and with the detail about @xmath21 left out ) that @xmath152 what we need is to find out when ( [ eq : z1 ] ) implies ( [ eq : z2 ] ) .",
    "to do so we will summon the powerful machinery of statistical learning theory .",
    "statistical learning theory has already been used in the analysis and design of indexing algorithms @xcite and is a vast subject .",
    "we will just focus on the generalization of the glivenko - cantelli theorem due to vapnik and chervonenkis .",
    "given sample @xmath153 distributed i.i.d .",
    "according to _ any _ measure @xmath17 on @xmath26 , we have : @xmath154 - \\mu ( -\\infty , r]\\right\\vert \\stackrel{p}{\\longrightarrow } 0.\\ ] ]    we can see this statement in terms of the empirical measures of particular subsets converging to their true measure .",
    "this is made clear when we restate the theorem as follows : @xmath155 where @xmath156 | r\\in\\mathbb{r}\\},\\ ] ] which makes more apparent a path for extension : to generalize to other collections of subsets @xmath157 .",
    "a collection @xmath157 `` colours '' the sample @xmath2 as follows .",
    "each @xmath158 will assign 1 to @xmath159 if @xmath160 , and @xmath161 otherwise .",
    "we denote by @xmath162 the _ number _ of such different colourings of @xmath2 generated by all @xmath158 .",
    "clearly @xmath163 .",
    "what is surprising is that in many situations , despite a seemingly rich @xmath157 , we have @xmath164 .    the _ growth function _",
    "@xmath165 of a family @xmath157 is defined by @xmath166 it is independent of @xmath17 and the choice of sample @xmath2 .",
    "there are two cases to consider for an upper bound for the growth function @xcite :    * for all @xmath1 , @xmath167 * or , for the largest @xmath168 such that @xmath169 , @xmath170    this @xmath168 is the so - called _ vc dimension _ and it turns out that its finiteness is a necessary and sufficient condition for .",
    "the rate of convergence is as follows ( @xcite p.148 ) :    [ thm : conv][vapnik  chervonenkis][thm : gkg ] for a collection @xmath157 of subsets of @xmath9 , of finite vc dimension @xmath168 , and any measure @xmath17 on @xmath9 , we have that for any @xmath171 , @xmath172 < \\\\    & 4\\exp \\left[\\left(\\frac{\\delta ( 1+\\ln ( 2n/\\delta))}{n } -\\left(\\varepsilon - \\frac{1}{n}\\right)^2\\right ) n \\right].\\\\\\end{aligned}\\ ] ]    the convergence is eventually like @xmath173 , which is again a fast rate of convergence . since no information about the measure @xmath17 is incorporated , the left side can be replaced by its supremum taken over all possible probability measures on the domain @xmath9 .    a natural restatement of these results is to ask how big does the sample size @xmath1 have to be for the expression on the left to be less than some @xmath174 . solving for @xmath175 and the use of some technical inequalities ( cf e.g. @xcite ) yields : @xmath176    calculations of vc dimension have been done for various objects ( e.g. @xcite , @xcite , @xcite ) : the vc dimension of half - spaces @xmath177 in @xmath27 is @xmath178 .",
    "the vc dimension of all open ( or closed ) balls in @xmath27 is also @xmath178 .",
    "axis - aligned rectangular parallelepipeds in @xmath27 , i.e. sets of form @xmath179\\times [ a_2,b_2]\\times\\ldots\\times [ a_d , b_d]\\ ] ] have a vc dimension of @xmath180 .",
    "our interest is in calculating the vc dimension of all possible set of form @xmath99 , the collection of which for a fixed @xmath21 we denote : @xmath181    as @xmath182 we can proceed through several steps .",
    "a set of the form @xmath183 is a `` spherical shell , '' and an intersection of shells is an interesection of sets from @xmath184 , where @xmath157 is the collection of all balls .",
    "it is easy to show that given a collection @xmath157 the complement collection @xmath185 has the same vc dimension .",
    "the vc dimension of balls was quoted above as @xmath178 , hence the vc dimension of complements of balls is @xmath178 as well .",
    "the vc dimension of the _ union _ of the two collections is @xmath186 as a consequence of a general result @xcite : if a collection @xmath157 has vc dimension @xmath187 and a collection @xmath13 has vc dimension @xmath188 , the union @xmath189 has vc dimension at most @xmath190 . a result for intersection of sets",
    "is mentioned in @xcite :    for @xmath191 , an upper bound on the vc dimension of @xmath192 , composed of @xmath21-fold interesections of elements of a family @xmath157 of vc dimension @xmath168 is @xmath193 .",
    "hence we can conclude that the vc dimension of @xmath194 for the case @xmath195 is bounded by @xmath196 where @xmath21 is the number of pivots .",
    "another example comes from considering the hamming cube . as there are @xmath197 points in a @xmath0-dimensional hamming cube , and at most @xmath0 different radii , so at most @xmath198 different balls exist .",
    "we know from e.g. @xcite that if the class @xmath157 is finite , its vc dimension is bounded by @xmath199 .",
    "disregarding the small leftover term , the vc dimension for balls in the hamming cube is about @xmath0 .",
    "summarizing :    [ thm : deltabounds ] let us denote by @xmath168 the vc dimension of collection @xmath194 as defined in equation .",
    "then upper bounds on @xmath168 , depending on the metric space , are as follows :    * for @xmath200 , @xmath201 . * for @xmath202 , @xmath203 . * for @xmath204 , @xmath205 .",
    "consider a sequence of metric spaces @xmath206 , where @xmath207 and the vc dimension of closed balls in @xmath206 is @xmath208 .",
    "assume every @xmath132 supports a borel probability measure @xmath209 so that for some @xmath133 the concentration functions @xmath131 of @xmath210 satisfy @xmath211 select for each @xmath0 an i.i.d .",
    "sample @xmath112 of size @xmath212 from @xmath132 , according to @xmath209 , where the sample size @xmath212 satisfies @xmath213 and @xmath214 .",
    "suppose further for every @xmath0 a pivot index for similarity search is built using @xmath21 pivots , where @xmath215 fix arbitrarily small @xmath216 .",
    "suppose we only ask queries whose radius is equal or greater to the distance to nearest neighbour of query centre @xmath150 in @xmath112 .",
    "* then there exists a @xmath123 such that for all @xmath217 , the probability that _ at least half _ the queries on dataset @xmath112 take less than @xmath218 time is less than @xmath175 . *    furthermore , if we allow the likelihood @xmath175 to depend on @xmath0 , we can pick @xmath219 so that the above holds true and @xmath220 we emphasize that this result is independent of the selection of pivots .",
    "we will sacrifice a certain number of sets of form @xmath99 so that @xmath4 can be considered a constant ( see section [ sec : radius ] ) : we will proceed with at least half the queries having radius @xmath4 above a constant independent of @xmath0 . hence the quantities that vary in @xmath0 are @xmath1 and @xmath21 . since @xmath0 is superlogarithmic in @xmath1 , @xmath223    so @xmath224 , and hence @xmath225 .",
    "in fact this holds for at least half the queries @xmath5 simultaneously , so : @xmath226 from the previous section , we know that only for large values of @xmath1 will empirical measures be close ( up to @xmath227 ) to actual measures with likelihood ( 1-@xmath175 ) .",
    "the lower bound on @xmath1 then naturally depends on @xmath227 , @xmath175 but also on the vc dimension @xmath168 of the collection @xmath194 .",
    "let us fix @xmath228 and assume @xmath175 is bounded by some value less than 1 .",
    "then by pooling all constants , including @xmath227 and @xmath175 but not @xmath168 we can rewrite expression as : @xmath229 where @xmath230 .",
    "what we would like to avoid is to have the right part of this expression grow linearly in @xmath1 .",
    "we know an upper bound on @xmath168 depends on @xmath21 and @xmath0 as established in theorem [ thm : deltabounds ] .",
    "as our concern is for asymptotic behaviour we will simplify this bound to @xmath231 .    combining @xmath232 with the asymptotic condition on @xmath21 ,",
    "we conclude that : @xmath233 and hence asymptotically we know that the right side of expression falls ( much ) under @xmath1 .",
    "therefore we are able to conclude : @xmath234 which , combined with @xmath228 and @xmath235 , gives the first part of the result .      assuming independent choices of the datasets @xmath112 , and assuming that for each @xmath0 the probability of an event is at least @xmath237 , we aim to prove that @xmath220 as @xmath219 goes to 0 at least as fast as @xmath238 , it is enough to show that @xmath239 observing @xcite that for any sequence @xmath240 , @xmath241 we can extend this , for any @xmath123 to : @xmath242 summing the geometric series , we obtain eq .",
    "( [ eq : enough ] ) .",
    "we have established a rigorous asymptotically linear lower bound on the expected average performance of the optimal pivot - based indexing schemes for similarity search in datasets randomly sampled from domains whose dimension goes to infinity .",
    "the examples given above of the various spaces exhibiting normal concentration of measure should convince the reader that many of the most naturally occuring domains and measure distributions are such .",
    "this is not the first lower bound result for pivoting algorithms for exact similarity search .",
    "a specific lower bound for pivot - based indexing already mentioned above is that of @xcite : @xmath243 this result assumes that @xmath244 .",
    "furthermore and more importantly , the pivot selection is assumed to be random , as opposed to our ( much stronger ) bound that is applicable to _ any _ pivot selection technique .",
    "other , more general asymptotic analyses considering more classes of indexing schemes @xcite fix @xmath1 or in the case of @xcite also fail to distinguish between the dataset and the dataspace making results appear stronger than they actually are .",
    "the aim in @xcite was to demonstrate that @xmath245 which came at the expense of any results on the rate of convergence .",
    "we chose instead to prove a weaker result , with convergence to some number close to 1/2 but with estimates on the rate of convergence .",
    "it should be assumed that the hypotheses of our paper are universal .",
    "rather , our theoretical analysis confirms that at least in some settings , the curse of dimensionality for pivot - based schemes is indeed in the nature of data .",
    "probably a more realistic situation from the viewpoint of applications would be that of an intrinsically low dimensional dataset contained in a high - dimensional domain , and performing an asymptotic analysis of various indexing schemes in this setting is an interesting open problem .",
    "pestov , v. ( 2008 ) . an axiomatic approach to intrinsic dimension of a dataset .",
    "neural networks 21 , 204213 .",
    "shaft , u. , ramakrishnan , r. ( 2006 ) theory of nearest neighbors indexability .",
    "acm transactions on database systems , volume 31 , issue 3 , pp 814  838 .",
    "weber , r. , schek , hans - j .",
    ", blott , s. ( 1998 ) a quantitative analysis and performance study for similarity - search methods in high - dimensional spaces proceedings of the 24rd international conference on very large data bases , pp194 - 205 ."
  ],
  "abstract_text": [
    "<S> we offer a theoretical validation of the curse of dimensionality in the pivot - based indexing of datasets for similarity search , by proving , in the framework of statistical learning , that in high dimensions no pivot - based indexing scheme can essentially outperform the linear scan .    </S>",
    "<S> a study of the asymptotic performance of pivot - based indexing schemes is performed on a sequence of datasets modeled as samples picked in i.i.d . </S>",
    "<S> fashion from a sequence of metric spaces . </S>",
    "<S> we allow the size of the dataset to grow in relation to dimension , such that the dimension is superlogarithmic but subpolynomial in the size of the dataset . </S>",
    "<S> the number of pivots is sublinear in the size of the dataset . </S>",
    "<S> we pick the least restrictive cost model of similarity search where we count each distance calculation as a single computation and disregard the rest .    </S>",
    "<S> we demonstrate that if the intrinsic dimension of the spaces in the sense of concentration of measure phenomenon is linear in dimension , then the performance of similarity search pivot - based indexes is asymptotically linear in the size of the dataset .    </S>",
    "<S> data structures ; similarity search ; curse of dimensionality ; concentration of measure ; </S>"
  ]
}