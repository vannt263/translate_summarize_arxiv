{
  "article_text": [
    "video cutout aims at pixel - level labeling of video frames given the user annotations by taking advantage of the continuity of the video content .",
    "it contributes to a variety of computer vision applications including human action recognition , object detection and recognition , and video editing , to name a few .    in this context , many video cutout techniques @xcite have been proposed to propagate user annotations , adopting optical flow and spatiotemporally connected markov chains as basic principles .",
    "nevertheless , there are many challenging factors in inter - frame propagation including unreliable optical flow estimation , changing motion patterns , motion blur , and cluttered backgrounds that adversely affect the performance .",
    "to tackle these challenges , appearance models such as gaussian mixture models ( gmms ) and color histograms are often incorporated .",
    "however , the discriminative capability of appearance models diminishes when object - background color models become ambiguous .",
    "besides , most appearance models only account for higher level image features that can not be explicitly tailored for a particular configuration of local receptive fields .",
    "another concern with video cutout is it labor intensive nature .",
    "although the state - of - the - art methods  @xcite partly reduced the time - consuming burden of manual labeling , still a considerable human interaction is required . for long , interactive video cutout approaches focused only on segmentation engines , ignoring how the interaction between the user and the algorithm should be formed for an optimal utilization of the human effort . for instance , existing methods propagate annotations from an arbitrarily selected frame ( _ e.g. _ , the first frame ) yet there is no guarantee arbitrarily selected frames ( or even human - selected frames ) provide sufficient information for an optimal cutout .    in this paper",
    ", we propose a novel pyramid model empowered confidence maps and an adaptive segmentation framework that minimizes inter - frame propagation errors for boundary - accurate foreground segmentation and automatic selection of the key frames for manual labeling .",
    "the confidence map is a probability density function over the image , assigning each pixel a probability of being foreground .",
    "we treat our confidence maps as _ classifiers_. inspired by the multi - level histograms in image classification @xcite , we incorporate a spatial pyramid color histogram model into a novel geodesic distance based dynamic model that is enhanced with locality information .",
    "this model provides efficient and reliable representations at several spatial resolution levels at the same time .",
    "thus , it improves the discriminative power of the confidence maps compared to conventional classifiers that apply only at a single level . our use of global and local models results in significantly improved performance over @xcite that use either local or global cues .",
    "in addition , we explicitly formulate a _ propagation uncertainty _",
    "term to identify the pixels where the label is ambiguous . within such regions ,",
    "we leverage on the local model to fuse appearance features such as color and shape .",
    "the segmentation is achieved by fusing confidence maps in a principled manner .    to make the best use of human feedback",
    ", our method selects the key frames for human annotation . to this end , a substantial effort is spent for pixel - level predictions in @xcite , which precludes it from practical use .",
    "we revise it on superpixel granularity .",
    "our prediction model makes the propagation algorithm computationally more efficient by eliminating motion estimation .",
    "to summarize , the main contributions of this work are threefold :    * it proposes a new pyramid model to capture appearance and location information .",
    "this model is supplemented with a set of geodesic distance based dynamic foreground models . contriving them as confidence maps , our method provides",
    "significantly improved foreground detection performance ( [ sec : globalclassifier ] ) .",
    "* it introduces a local classifier based estimation of propagation - uncertainty for effective handling of regions where ambiguous labeling may occur ( [ sec : localclassifier ] ) . *",
    "it incorporates an annotation frame selection technique to automatically determine the frames for human annotation , significantly reducing the labeling effort while further improving the segmentation results ( [ sec : intelligentannotationframeselection ] ) .",
    "compared with unsupervised video segmentation methods @xcite , interactive video segmentation extracts foreground object from video sequences with human interaction .",
    "numerous techniques have been proposed to help the human operator to accomplish video editing task .",
    "these methods , in general , propagate the given annotations to the entire video sequence , by typically tracking them using spatiotemporal markov random fields based probabilistic models @xcite , or frame - matching based propagation @xcite . to obtain foreground estimates",
    ", the video snapcut system  @xcite incorporates a set of local classifiers using multiple local image features and integrates a propagation based shape model into color models .",
    "@xcite presents a geodesic symmetric filter ( gsf ) that is applied to real - valued pixel likelihoods by imposing contrast - sensitive spatial smoothness and considering object topology .",
    "@xcite introduces directional classifiers to handle temporal discontinuities while remaining robust to inseparable color statistics .",
    "more recently , @xcite obtains the segmentation at each frame by transferring the foreground mask using nearest - neighbor fields .",
    "tracking of segmentation results in video sequences has also been investigated by many works  @xcite .",
    "the segmentation is obtained by either solving an optimization problem with patch seams across frames @xcite , using fully connected graphs to model long - range spatiotemporal connections @xcite , or operating on bilateral space @xcite .",
    "such methods often require human annotations in the first frame , and then track them in the rest of the video .",
    "only recently in the computer vision community , the work in @xcite proposed an active frame selection method to select a subset of frames that together minimize the total mislabeling risk over the entire sequence .",
    "however , this strategy is computationally expensive .",
    "the propagation error prediction is done using a forward and backward pixel flow model , which is impractical for user - in - the - loop applications .",
    "in contrast , our model for predicting propagation error is based on superpixel - level matching , which is efficient .",
    "in  [ sec : globalclassifier ] , we first introduce our static and dynamic global models for confidence map estimation within and across video frames .",
    "then , in [ sec : localclassifier ] , we describe in detail our localized models and propagation uncertainty estimation .",
    "finally , we present our key frame selection technique where the selection criterion is coupled with our segmentation method by design ( [ sec : intelligentannotationframeselection ] ) .      given an input video sequence @xmath0 containing @xmath1 frames and a subset of @xmath2 frames @xmath3 with corresponding binary annotation masks @xmath4 such that @xmath5 , our goal is to assign each pixel a label @xmath6 ( _ background _",
    "= 0 , _ foreground _ = 1 ) .",
    "we propagate initial annotations @xmath7 to the entire video .",
    "each @xmath8 is a label matrix having the same dimensions as the image frame indexed by the 2d pixel coordinates @xmath9 , and each @xmath10 .",
    "we begin by oversegmenting each frame by slic  @xcite for computational efficiency and boundary accurate final segmentation cutouts .",
    "we then build global appearance models to assign each superpixel a probability of being foreground object .",
    "the number of superpixels is set to 2000 for a @xmath11 resolution image .",
    "we denote by @xmath12 the superpixel set of the frame @xmath13 . + * static confidence map by global pyramid  * traditional global appearance models do not consider the spatial arrangements of features and fail to take the full advantage of the prior distributions available in user annotations .",
    "this motivated us to enhance histogram based models with the structural information by building a pyramid in the spatial domain then binning the feature space .",
    "our method repeatedly subdivides the frame into cells and computes histograms of color features in these cells .",
    "more specifically , it constructs a sequence of grids at resolutions @xmath14 for each annotated frame @xmath15 , such that the grid at level @xmath16 has @xmath17 cells along the coordinate directions .",
    "then , it computes two color histograms , @xmath18 and @xmath19 , from rgb color features at each level for all annotated frames @xmath20 .",
    "each pixel contributes into @xmath18 and @xmath19 according to its color value , label , and coordinate .",
    "therefore , a global pyramid histogram model can be setup , consisting of a set of atomic histograms @xmath21 and @xmath22 at different grid levels .",
    "the foreground probability @xmath23 of superpixel @xmath24 from the global model is formulated as : @xmath25 where @xmath26 represents the mean feature value ( color , _",
    "etc_. ) of the superpixel .",
    "an example of constructing a three - level pyramid of an annotated frame in shown in fig .",
    "[ fig : pyramidmodel ] . in our implementation ,",
    "each color channel is uniformly quantized into 32 bins , thus there is a total of @xmath27 bins in the histogram of each cell .",
    "the finest resolution @xmath28 is set as 3 .",
    "we provide detailed discussion for @xmath28 in  [ sec : ppc ] .",
    "[ fig : pyramidvisual ] shows two challenging sample frames where the foreground object and the background scene have overlapping color histograms , which confuses traditional classifiers built upon color statistics ( fig .",
    "[ fig : pyramidvisual ] ( b ) ) .",
    "in contrast , our pyramid appearance model fuses color and location information , resulting in a stronger discriminative power and more accurate probability maps ( fig .",
    "[ fig : pyramidvisual ] ( c ) ) .",
    "+     + ( a ) ( b ) ( c )    * dynamic confidence map by geodesic distance  * to build a confidence map for capturing inter - frame correspondences and accommodating dynamic variations between successive frames , we use a set of dynamic global confidence maps that are estimated between consecutive frames using geodesic distance .     + ( a ) ( b ) ( c ) ( d ) ( e )    for two adjacent frames @xmath29 and @xmath13 , where frame @xmath29 with known segmentation @xmath30",
    ", we construct an undirected weighted graph @xmath31 , where @xmath32 .",
    "there are two types of edges @xmath33 : intra - frame edges that link spatially adjacent superpixels , and inter - frame edges that connect temporally adjacent superpixels .",
    "the superpixels are spatially connected if they are adjacent in same frame .",
    "temporally adjacent superpixels refer to the superpixels which belong to different frames but have overlap . based on the graph structure ,",
    "we derive a weight matrix @xmath34 .",
    "the @xmath35-th element of @xmath34 indicates the weight of edge @xmath36 between two connected nodes @xmath37 : @xmath38 where @xmath39 .",
    "the edge weight is assigned as the euclidean distance between the mean colors of two connected nodes ( superpixels ) @xmath40 .    the geodesic distance between any two nodes @xmath37",
    "is computed as the accumulated edge weights along their shortest path on the graph @xmath41 : @xmath42 where @xmath43 is a path connecting the nodes @xmath37 .    for frame",
    "@xmath29 , the superpixels @xmath44 can be decomposed into foreground regions @xmath45 and background regions @xmath46 according to the segmentation @xmath30 , where @xmath47 .",
    "is pixel - level , we consider the superpixel contains more ( less ) foreground pixels than background ones as foreground ( background ) . ]",
    "based on the graph @xmath41 , we use geodesic distance to define the similarity for superpixel @xmath48 of frame @xmath49 to foreground regions @xmath45 and background regions @xmath46 : @xmath50 if a superpixel is close to the foreground ( background ) , there exists a relatively short path to the foreground ( background ) nodes , and the value of @xmath51 ( @xmath52 ) is small . for superpixel @xmath48 of frame @xmath49 ,",
    "its dynamic foreground probability using geodesic distance is computed as : @xmath53    our geodesic distance based confidence map is facilitated by the segmentation of prior frame and evaluated in a frame - by - frame fashion .",
    "we find the dynamic global model captures variations across the frames accurately , separates object from the background clearly , and complements pyramid model nicely .",
    "therefore , we combine these two models .",
    "the final foreground probability of a superpixel @xmath48 is : @xmath54 an example for the integration of dynamic and static confidence maps is presented in fig .",
    "[ fig : globalvisual ] .      after computing global confidence maps ,",
    "we obtain a coarse segmentation mask @xmath55 ( see fig .  [",
    "fig : globalvisual ] ( e ) ) :    @xmath56    for each pixel @xmath57 . obviously , this thresholding strategy is not always reliable . to resolve segmentation ambiguities ,",
    "we adopt local classifiers on local image features .",
    "previous methods , e.g. @xcite , track classifier windows along the foreground boundaries , which is computationally expensive due to motion estimation and sensitive to topological changes . instead",
    ", we determine ambiguous regions and apply local classifiers . to this end",
    ", we use propagation uncertainty estimation : @xmath58 as the difference between two successive frames .",
    "this measurement is simple yet effective .",
    "the intuition is straightforward , the variations of appearance usually accompany the changes of label and potentially lead ambiguity .",
    "we separate the pixels in frame @xmath13 into two parts : _ propagation - uncertainty set _ @xmath59 and _ propagation - certainty set _ @xmath60 : @xmath61 the propagation - uncertainty set @xmath59 consists of all the pixels with changed labels and the pixels with high propagation uncertainty .",
    "the labeling in the propagation - certainty @xmath60 are relatively reliable , as their labels are consistent and appearance differences are small for two successive frames .",
    "then we sample a set of overlapping classifier windows @xmath62 cover all the image frame domain , where the neighboring windows overlap for half - window size .",
    "for frame @xmath13 , we only enable the local classifiers whose classification window covers the propagation - uncertainty area @xmath59 . for each window @xmath63 with its segments",
    "@xmath64 , we find its _ best matched _ window @xmath65 in frame @xmath29 via a match score using shape and color information : @xmath66 where @xmath67 indicates the mean colors of the window @xmath68 .",
    "we restrict the matching process inside a @xmath69 search area , where @xmath70 ( @xmath71 ) is set to the one fourth of the image frame s height ( width ) . using this function ,",
    "the classification window @xmath63 is aligned with a best - matched window of the previous frame .",
    "we establish a discriminative classifier @xmath72 within @xmath63 : @xmath73 where @xmath74 corresponds to the nearest pixel of @xmath9 in @xmath65 , measured via color similarity .",
    "a pixel within propagation - uncertainty set @xmath59 is assigned to the label of the nearest pixel of its best matched window .",
    "overall , our local classifier integrates patch - level features such as shape and appearance information , and pixel - level matching .",
    "color distributions .    finally , we derive our pixel - wise foreground probability estimation from global confidence map and local classifiers : @xmath75 using the foreground probability @xmath76",
    ", we obtain segmentation mask @xmath77 for frame @xmath13 and then propagate this mask frame - by - frame in forward direction .",
    "so far , we described a forward propagation workflow .",
    "recall our approach accepts @xmath2 annotation frames @xmath3 as input , the segmentation is proceeded in a bi - directional workflow .",
    "let @xmath78 be the indices of the closest labeled frames before and after frame @xmath13 , respectively ( ` left ' and ` right ' of @xmath13 ) . might not exist if @xmath79 or @xmath80 . for clarity",
    "we omit such cases , as they do not affect the method description . ]",
    "our method processes cutout for frame @xmath13 starting from its closest labeled frames on either side in both the forward and backward directions . for a frame , there are two foreground probability maps @xmath81 and @xmath82 .",
    "we merge these estimation : @xmath83 the final label of pixel @xmath9 in frame @xmath13 is chosen according to its combined foreground probability @xmath84 .",
    "finally , morphological operations are adopted for filling the small holes .",
    "previous cutout systems arbitrarily select frames with initial annotations ( the first frame or key frames ) , which can not ensure optimal propagation of user annotations .",
    "besides , from the interaction point of view , it is preferable if the user is asked to annotate the frames that would best benefit the segmentation performance ( instead of being forced to label random or less informative frames ) . to this end",
    ", we introduce an intelligent strategy to select the frames for user annotation where propagation uncertainty guides the selection of the frames for manual labeling .",
    "we simplify the local uncertainty propagation as a region matching problem by finding the closest region in the neighboring frame .",
    "the match score is computed as the color difference between two superpixels .",
    "we impose a center distance constraint .",
    "we then model the probability that a region is mislabeled .",
    "we define a propagation error for a prior frame @xmath85 forward to frame @xmath49 via a probability model , similar to @xcite .",
    "all terms are analogously defined for propagating from a later frame @xmath86 .",
    "the probability of superpixel @xmath24 will be mislabeled when we obtain its label from frame @xmath29 is : @xmath87 the component distances reflect the expected propagating error .",
    "the term @xmath88 computes the color difference between the region @xmath24 and the best matched region @xmath89 in frame @xmath29 , i.e. @xmath90 . the term @xmath91 measures occlusions using the consistency of the forward and backward matching : @xmath92 where @xmath93 is the vector from the center of a superpixel to the center of its best matched superpixel in next frame , similarly , @xmath94 in the previous frame . ideally ,",
    "if two superpixels @xmath24 and @xmath89 are matched , these two flows should be opposite in direction .    when there is more than one frame between labeled frame @xmath95 and current frame @xmath13 , we predict errors accumulated over successive frames . defining the error recursively , we have : @xmath96 where @xmath97 indicates the matched superpixel of @xmath24 in frame @xmath98",
    ". then we derive a @xmath99 propagation error matrix , where @xmath100-th element indicates the total propagation error from @xmath101-th frame to @xmath102-th frame : @xmath103 where @xmath104 is the number of the elements in the collection . for modeling our global pyramid appearance estimation",
    ", we introduce an adjustment coefficient @xmath105 , which represents the difficulty of propagating labels from frame @xmath106 to frame @xmath49 according to their difference on frame level , i.e. , @xmath107 where @xmath108 is our pyramid model .",
    "we find that , the more similarities of two frames , the higher propagation accuracy we can achieve .",
    "this observation is intuitive ; when two frames are similar in their pyramid representation , our pyramid model would performance well .",
    "we define the optimization problem for selecting the best set of frames from which to propagate , aiming to choose @xmath109 by minimizing the total expected number of erroneous pixels in entire video : @xmath110 this propagation error model is derived from our propagation strategy in .",
    "[ sec : localclassifier ] . by above minimization",
    ", we select @xmath2 number of useful frames according to the expected label error . here , we set @xmath2 as a manually selected parameter .",
    "[ eq:20 ] can be efficiently solved by dynamic programming algorithm , similar to @xcite . in this way",
    ", our method reduces total manual effort by keeping the number of selected frames low .",
    "in this section , we first evaluate the overall performance by comparing to several sota methods ( .  [",
    "sec : performancecomparison ] ) . to gain a deeper insight of our method",
    ", we validate the effectiveness of the pyramid model and annotation frame selection strategy by comparisons to several baselines ( .",
    "[ sec : ppc ] ) and conducting a user study ( .",
    "[ sec : us ] ) .",
    "finally , runtime analysis is presented in .",
    "[ sec : ra ] .",
    "our evaluations are on two benchmarks : the jumpcut dataset @xcite and recently released davis dataset @xcite .",
    "jumpcut contains five sets of video clips ( _ snapcut _ , _ animal _ , _ human _ , _ static _ , and _ fast _ sets ) provided by @xcite .",
    "there are 22 video clips in total and full pixel - level segmentation ground - truth for each frame is available .",
    "the davis dataset consists of 50 videos , accompanied by per - frame and pixel - level ground - truth masks .",
    "those two datasets cover a wide degree of difficulty , such as occlusions , dramatic topology changes , and large appearance variations .",
    "+ ( a ) ( b ) ( c ) ( a ) ( b ) ( c )      we first evaluate our approach with respect to existing methods on the task of foreground mask propagation , which is the core of our video cutout technique .",
    "these experiments are conducted by comparing with the state - of - the - art alternatives , including the rotobrush of adobe aftereffects ( rb09 ) , which is based on the video snapcut  @xcite , discontinuity - aware video cutout ( da12 )  @xcite , and jumpcut ( jc15 )  @xcite using non - successive mask transfer .",
    "these three methods are designed for video cutout purposes .",
    "additionally , we compare to five very recent segmentation tracking methods : bv16  @xcite , fp15  @xcite , ss14  @xcite , ts13  @xcite , and hv10  @xcite . for each test video sequence , we compare the segmentation performance with manual annotation of the first frame as initialization .",
    "to keep our analysis fair , all the methods predict subsequent frames without any additional user input and our method only adopts forward propagation workflow .",
    "we analyze the effectiveness of our approach on the davis dataset  @xcite using the intersection - over - union metric ( iou score ) .",
    "the iou scores for representative sequences and the average performance over the * entire * dataset ( 50 videos ) are reported in table  [ table1 ] . as can be seen",
    ", our approach performs better than all other methods , achieving the best iou score on most of the videos with an average score up to @xmath111 .",
    "qualitative comparison results are shown in fig .",
    "[ fig : exp_comparisondavis ] , which demonstrate the superiority of the proposed method on challenging scenarios .",
    "following the experimental setup in @xcite , we report the error rates of the different methods for automatically propagating segmentation mask across different frame distances in the five video sets from the jumpcut benchmark .",
    "the propagation errors are computed as the ratio between the wrongly classified areas of the transferred mask and the foreground area of the ground - truth mask . for each video sequence ,",
    "the first 128 frames are tested for automatically propagating a ground - truth mask from frame @xmath102 to @xmath112 , for @xmath113 , with different transfer distances @xmath114 . from the results in table  [ table2 ]",
    ", it can be observed that our method outperforms all other methods , yielding the smallest error in the majority of the cases .",
    "we additionally observe that many methods degrade in quality over long sequences , as errors accumulate over time .",
    "in contrast , our method achieves better scores on long videos , experiencing less drift of the object region than the others .      to demonstrate the effectiveness of our global pyramid appearance model , we evaluate the performance with different pyramid resolution levels . with a foreground probability map estimated via pyramid classifier",
    ", we obtain a segmentation via a simple threshold ( 0.5 ) .",
    "we choose the first 100 frames of each test video of jumpcut dataset  @xcite and use a pair of annotated masks of the beginning and the end frames to build the global pyramid classifiers .",
    "we set the max resolution level @xmath115 .",
    "additionally , as our annotation frame selection prompts pyramid appearance model , we investigate the performance of our approach with two annotation frames recommended by our annotation frame selection system .",
    "the overall iou scores on jumpcut dataset  @xcite with different pyramid layers are presented in fig .",
    "[ fig : exp_pyramidlayer ] .",
    "note that , when the max pyramid layer @xmath116 , our pyramid classifier is equal to traditional single - level color histogram model .",
    "overall , the performance of the pyramid classifier increases with cumulatively larger pyramid layers ( @xmath117 ) , since , obviously , finer grid size is used and more localized structure information is considered .",
    "however , the classification performance drops as pyramid layer @xmath28 continues to increase .",
    "this is mainly due to the overemphasis of location information .",
    "the maximum performance is obtained when @xmath118 .    additionally , it is clear that the classification performance with the recommended annotation frames consistently better than that with fixed annotation frames for all the pyramid layers , which confirms the annotation frame selection system plays a positive role in promoting the pyramid appearance classifier .",
    "we conducted a user study on jumpcut dataset  @xcite to assess the degree to which our annotation frame selection system can reduce the user effort and improve the segmentation accuracy .",
    "a corpus of 10 participants ( 7 female ) with diverse backgrounds and ages were recruited to participate in our user study .",
    "none of the participants had received any special technical instructions or had any prior knowledge about the experimental hypotheses .",
    "12 video clips , each with 100 frames , are used for training the participants with our cutout system and rotobrush tool @xcite . in the training process",
    ", the participants were asked to select @xmath2 annotation frames ( @xmath119 ) for obtaining the best possible segmentation results .",
    "after the participants had confirmed they were familiar with the cutout systems , they were asked to segment 5 test video clips via two cutout methods .",
    "those test video sequences are different from the training ones , and each of them also has 100 frames .",
    "the participants were presented with the test video and tried to select @xmath2 annotation frames for segmenting as accurately as possible .",
    "thus a total of 50 video cutout tasks were assigned to each user . to exclude the influences of difference between annotations initialized by different users",
    ", we use the same annotations for all participants in the testing process .",
    "the order of the test cases is kept the same for all participants . our method and",
    "rotobrush tool worked in the bi - directional workflow .",
    "we recorded the time that each participant took to select the annotations and computed the average iou score using the ground truth masks . to gain a deeper insight ,",
    "we further offer a baseline ( _ random _ ) for both two methods : generates segments via randomly selecting @xmath2 annotation frames .",
    "the results of the study are summarized in table  [ table3 ] , averaged over all users for each task .",
    "it shows the human decision is positive for cutout task as the results with human participation are generally better that with randomly selected annotations .",
    "however , according to user feedbacks , scanning all the frames has already been consumed much time and effort , let alone determining which frames should be best selected .",
    "these results confirm that our annotation frame selection system successfully reduces the user effort and improves the segmentation performance .",
    "it also demonstrates that our cutout system consistently outperforms rotobrush .",
    "we measure the running time of the proposed method and the current fastest video cutout and segmentation tracing methods : rb09  @xcite , hv10  @xcite , ss14  @xcite , jc15  @xcite , bv16  @xcite .",
    "we directly run their publicly available codes .",
    "all the tests were performed on the @xmath120 video clips of davis dataset @xcite . as shown in table  [ table4 ] ,",
    "our method is faster than the others except rb09 and bv16 .    [ ! htbp ]",
    "we presented a novel video cutout technique that efficiently propagates user annotations to whole video sequence .",
    "state - of - the - art results are achieved by the integration of global models and local classifiers .",
    "our global pyramid appearance model has shown superior results over traditional models .",
    "our local classifiers integrates multiple local image features , generating optimal results on propagation - uncertainty areas .",
    "comprehensive evaluations on two big benchmarks demonstrated the effectiveness of our method at achieving the favorable results .",
    "additionally , our large scale user study showed the proposed annotation frame selection technique reduces manual effort and provides further performance boost for segmentation performance ."
  ],
  "abstract_text": [
    "<S> conventional video segmentation approaches rely heavily on appearance models . </S>",
    "<S> such methods often use appearance descriptors that have limited discriminative power under complex scenarios . to improve the segmentation performance , </S>",
    "<S> this paper presents a pyramid histogram based confidence map that incorporates structure information into appearance statistics . </S>",
    "<S> it also combines geodesic distance based dynamic models . </S>",
    "<S> then , it employs an efficient measure of uncertainty propagation using local classifiers to determine the image regions where the object labels might be ambiguous . </S>",
    "<S> the final foreground cutout is obtained by refining on the uncertain regions . additionally , to reduce manual labeling </S>",
    "<S> , our method determines the frames to be labeled by the human operator in a principled manner , which further boosts the segmentation performance and minimizes the labeling effort . </S>",
    "<S> our extensive experimental analyses on two big benchmarks demonstrate that our solution achieves superior performance , favorable computational efficiency , and reduced manual labeling in comparison to the state - of - the - art . </S>"
  ]
}