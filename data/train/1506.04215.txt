{
  "article_text": [
    "given a way to evaluate or _ sample _ an unknown function or procedure , interpolation is the fundamental and important problem of recovering a formula which accurately and completely describes that unknown function . as discovering an arbitrary unknown function from a finite set of evaluations with any reliability would be impossible , some constraints on the size and form of the output are inevitably required .",
    "here we consider the problem of _ sparse polynomial interpolation _ , in which we are guaranteed that the unknown function is a multivariate polynomial with bounded degree .",
    "sparse interpolation algorithms date to the 18th century , but have been the focus of considerable recent work in numeric and symbolic computation , with applications ranging from power consumption in medical devices , to reducing intermediate expression swell in mathematical computations @xcite .    specifically , this work focuses on algorithms to interpolate an unknown _ supersparse _ polynomial with integer coefficients , which make efficient use of modern _ parallel _ computing hardware .",
    "focusing on the `` supersparse '' ( a.k.a .",
    "`` lacunary '' ) case means that our running time will be in terms of the number of variables , number of nonzero terms , and the logarithm of the output degree .",
    "the first algorithms to solve this problem in polynomial - time were based on the exponential sums technique of prony , and can efficiently solve the integer problem by working in a large finite field modulo a single `` big prime . ''",
    "a number of theoretical improvements and practical implementations have been made in this vein , including work on fast parallel implementations @xcite .",
    "we consider another type of approach , whereby the unknown sparse integer polynomial is reduced in degree modulo many small primes .",
    "this technique , first used by garg and schost to avoid the need for discrete logarithm computations in arbitrary finite fields , can also be applied to the integer polynomial case . while it can not yet match the theoretical efficiency of the big primes algorithms",
    ", we will show that the `` small primes '' method is very effectively parallelized .",
    "furthermore , we develop a practical heuristic version of this method which reduces further the size and number of primes required based on experimental results rather than on the theoretical worst - case bounds .",
    "while it would be impossible to list all of the related work on sparse interpolation , we will mention some of the most recent results which are most closely connected to the current study , and which may provide the reader with useful background .",
    "the now - classical approach to sparse interpolation is variously credited to prony , blahut @xcite , or ben - or and tiwari @xcite .",
    "only the last of these considered explicitly the case of integer coefficients , but all share the key property of requiring the minimal number @xmath0 of evaluations in order to recover a polynomial with @xmath1 nonzero terms .",
    "we refer to these approaches as `` big prime '' techniques , as the more modern variants @xcite adapt to the case of integer coefficients by choosing carefully a single large modulus , then perform the interpolation over a finite field in order to avoid exponential growth in the bit - length of evaluations .    the approach which we take in this work",
    "is based on that of garg and schost @xcite , who developed the first polynomial - time supersparse interpolation algorithm over an arbitrary finite field . by reducing the unknown polynomial modulo @xmath2 ,",
    "the full coefficients are discovered immediately , but the exponents are only discovered after repeating for multiple values of @xmath3 .",
    "this `` small primes '' approach , described in more details below , has the considerable advantage of relying only on low - degree , dense polynomial arithmetic .",
    "there are , of course , other sparse interpolation methods which do not fit nicely into this big / small prime characterization .",
    "most notable are zippel s algorithm and hybrid variants of it @xcite , the symbolic - numeric method of @xcite , and the newton - hensel lifting approach of @xcite .",
    "we point out some recent work on efficient implementations which are of particular interest to the current study . in @xcite , a new variant of the big prime approach",
    "is developed which can be performed variable by variable , in parallel .",
    "more recently , @xcite investigated a number of tricks and techniques towards practical , efficient sparse interpolation , and posed some new benchmark problems .",
    "their methods are also based on the big prime approach ; to our knowledge there has been no reported implementation work on the small primes technique other than the numerical interpolation code reported in @xcite .",
    "suppose @xmath4 $ ] is an unknown polynomial in @xmath5 variables , with partial degrees less than @xmath6 in each variable , at most @xmath1 nonzero terms , and coefficients less than @xmath7 in absolute value .",
    "then @xmath8 can be written as @xmath9 where each exponent @xmath10 and each @xmath11 .",
    "given a way to evaluate @xmath12 , for any modulus @xmath13 and @xmath5-tuple of evaluation points @xmath14 , the _ sparse integer polynomial interpolation problem _ is to determine the coefficients @xmath15 and exponent tuples @xmath16 , for each @xmath17 .",
    "we will actually consider a slight relaxation of this problem , wherein evaluations are of the form @xmath18.\\ ] ] that is , the @xmath5 variables are replaced by a single indeterminate @xmath19 , all coefficients are reduced modulo @xmath20 , and the exponents are reduced modulo @xmath3 .",
    "the reduction modulo @xmath2 is possible without affecting the overall complexity whenever the unknown polynomial @xmath8 is given as a straight - line program or algebraic circuit , or if the prime @xmath20 is chosen so that @xmath21 has a @xmath3th root of unity .",
    "an algorithm for this problem is said to handle the _",
    "case if it requires a number of evaluations and running time which are polynomial in @xmath5 , @xmath1 , @xmath22 , and @xmath23 .",
    "this corresponds to the size of the sparse representation of @xmath8 as a list of coefficient - exponent tuples , which requires @xmath24 bits in memory .",
    "we present a randomized algorithm for the sparse integer polynomial interpolation problem , derived from the existing literature , whose running time for some function @xmath25 if and only if it is @xmath26 . ] for provable correctness is @xmath27 where @xmath28 is the number of parallel processors available for the task .",
    "furthermore , we demonstrate a heuristic variant on this algorithm , which works well in our experimental testing , and reduces the running time further to @xmath29 in the typical case that @xmath5 and @xmath22 are both @xmath30 .",
    "we have implemented this heuristic approach using the c library flint for dense polynomial arithmetic and mpi for parallelization .",
    "our experiments demonstrate the smallest effective settings for the parameters in our heuristic approach . with those parameters , we show that the heuristic method is competitive with the state of the art in the single - processor setting , and that its running time scales well with increasing numbers of parallel processors .",
    "specifically , our contributions are :    1 .   a sparse interpolation algorithm whose potential parallel speedup is @xmath31 , compared with the @xmath32 parallel speedup that has been shown in previous work for other sparse interpolation algorithms .",
    "2 .   a heuristic variant of our algorithm , which is demonstrated to be effective on our ( limited ) random experiments , that brings the running time complexity of the small primes approach to be competitive with that of the big prime approach .",
    "3 .   an efficient c implementation of our interpolation algorithm which demonstrates its competitiveness on a standard benchmark problem .",
    "we first outline the two main classes of existing approaches to supersparse integer polynomial interpolation , which we call the `` big prime '' and the `` small primes '' methods , in section  [ sec : exist ] .",
    "we then present our own heuristic method in section  [ sec : ouralg ] , which is based on previously - known `` small primes '' algorithms , but goes beyond theoretical worst - case bounds on the sizes needed in order to further improve the efficiency .",
    "section  [ sec : impl ] reports the details of our parallel implementation of this heuristic method , and section  [ sec : exper ] presents the preliminary experimental results which demonstrate the efficacy of this approach .",
    "finally , we state some conclusions and directions for further investigation in section  [ sec : conc ] .",
    "the original sparse interpolation algorithm of @xcite used evaluations at powers of the first @xmath5 prime numbers along with prony s method to deterministically recover the nonzero integer coefficients and exponents of an unknown polynomial .",
    "this can not be considered a `` supersparse '' algorithm , as it performs the evaluations over the integers directly and requires working with integers with more than @xmath6 bits .",
    "however , it was soon recognized that , by choosing a single large prime @xmath33 , with @xmath34 smooth so as to facilitate discrete logarithms , a supersparse integer polynomial could be interpolated efficiently modulo @xmath3 @xcite .",
    "the basic steps of this approach are as follows :    1 .   choose prime @xmath3 so that @xmath35 has a large `` smooth '' factor greater than @xmath36 and let @xmath37 be a primitive element modulo @xmath3 .",
    "2 .   for @xmath38 ,",
    "evaluate @xmath39 3 .",
    "compute the minimum polynomial @xmath40 $ ] of the sequence @xmath41 with the berlekamp - massey algorithm .",
    "4 .   factor @xmath42 over @xmath43 $ ] ; each root of @xmath42 can be written as @xmath44 , corresponding to a single term @xmath45 of @xmath8 .",
    "5 .   compute @xmath1 discrete logarithms of the roots , and then the @xmath6-adic expansion of each one , to discover the actual exponents @xmath16 , for @xmath17 .",
    "once the exponents are known , the coefficients can be computed from the evaluations @xmath46 by solving a transposed vandermonde system of dimension @xmath1 .",
    "the two steps which can be trivially parallelized are the evaluations and discrete log computations on steps 2 and 5 .",
    "however , the dominating cost in the complexity is factoring a polynomial over @xmath47 $ ] on step 4 .",
    "this is also confirmed to be the dominating cost in practice by @xcite , and it is not clear how to efficiently parallelize the factorization . using the",
    "fastest known algorithms , the running time of this step is @xmath48 @xcite , which is at least @xmath49 bit operations from the condition @xmath50 .    in the description above ,",
    "the evaluations at powers of @xmath37 on step 2 amount to a kronecker substitution from multivariate to univariate . of note",
    "is the algorithm of @xcite , which uses a different approach than kronecker substitution in order to work one variable at a time , gaining a potential @xmath5-fold parallel speedup .",
    "the algorithm described above works the same over an arbitrary finite field , except that the discrete logarithms required in step 5 can not be performed in polynomial - time in general .",
    "this difficulty was first overcome in @xcite , where the idea is as follows :    1 .",
    "choose a series of small primes @xmath51 2 .",
    "apply the kronecker substitution and for each @xmath52 compute @xmath53 .",
    "each @xmath54 of maximal sparsity contains all the coefficients of @xmath8 , and all the exponents modulo @xmath55 .",
    "collect sufficiently many modular images of the exponents in order to recover the full exponents over @xmath56 .",
    "4 .   recover the multivariate exponents by @xmath6-adic expansion of each univariate exponent , and use any @xmath54 of maximal sparsity to discover the coefficient of each term .",
    "there are two significant challenges of this approach .",
    "the first is that , in reducing the polynomial modulo @xmath57 , it is possible that some exponents are equivalent modulo @xmath55 , and then multiple terms in the original polynomial will _ collide _ to form a single term in @xmath54 . by choosing random primes",
    "whose values are roughly @xmath58 , the probability of encountering any collisions can be made arbitrarily small .",
    "the second challenge is how to correlate the exponents from different @xmath54 s in step 3 , in order to recover the full exponents via chinese remaindering .",
    "the approach of @xcite was to compute an auxiliary polynomial whose roots are the unknown exponents ; however this increases the overall running time to @xmath59 .",
    "the technique of _ diversification _ in @xcite is another randomization which chooses a random element @xmath60 and interpolates @xmath61 instead of @xmath62 itself . with high probability ,",
    "the `` diversified ''",
    "polynomial @xmath61 has distinct coefficients , which can then be used to correlate the exponents in different @xmath54 s .",
    "this avoids the factoring step and reduces the complexity by a factor of @xmath1 .",
    "subsequently , and separately , @xcite showed how to allow the magnitude of each @xmath55 to decrease by a factor of @xmath1 , by allowing some constant fraction of the terms in each @xmath54 to collide .",
    "these separate approaches are combined and further improved in @xcite , which in the typical case that @xmath63 , brings the theoretical complexity down to @xmath64 .",
    "observe that this is competitive with the `` big primes '' algorithm , but could be slower by up to a factor of @xmath23 .",
    "the algorithm we describe next first shows how to effectively parallelize this small primes approach , and then further reduces the complexity through a heuristic argument .",
    "after the gains from parallelization or from the heuristic improvement , the complexity of our algorithm will be less than the `` big primes '' approach as well .",
    "@xmath65 random prime in the range @xmath66 $ ] @xmath67 random element of @xmath68 @xmath69 @xmath70 @xmath71 @xmath72 thread - safe list of integer triples sort @xmath73 in parallel by the coefficients @xmath74 in each triple @xmath75 thread - safe list of coefficient / exponent tuples sort @xmath76 in parallel by the exponents and * return * the resulting sparse polynomial    our algorithm , which is detailed in procedure [ proc : interp ] , is based on the reduction idea in @xcite , with the diversification method introduced by @xcite and the partial collisions handling of @xcite .",
    "it depends crucially on the parameters @xmath77 and @xmath78 , which in theory grow as @xmath31 and @xmath79 , respectively , but according to our heuristic method can both be treated as constants .      the main idea is first to choose a prime @xmath20 large enough to recover the coefficients , and then to apply the kronecker substitution so that we are really interpolating a univariate polynomial @xmath80 with coefficients in @xmath21 : @xmath81.\\ ] ] each term @xmath82 of the original polynomial @xmath8 maps uniquely to a term with exponent @xmath83 and coefficient @xmath84 in @xmath85 .",
    "the parameter @xmath86 controls the size of the prime @xmath20 , and so should always be set larger than @xmath87 in order to recover the full precision of the coefficients .",
    "however , it may be necessary to set @xmath86 even larger than this when the height bound @xmath7 is very small .",
    "we comment that choosing a random prime @xmath20 ( rather than one with special properties , as in the `` big primes '' algorithm ) is important for the probabilistic analysis below .",
    "the evaluation phase of the algorithm computes the polynomial @xmath85 modulo @xmath2 using dense arithmetic , for many small primes @xmath3 .",
    "the details of how this evaluation is performed will depend on the particular application . in our multivariate multiplication application below , the exponents of the original multiplicands are reduced modulo @xmath3 , followed by dense polynomial arithmetic in the ring of polynomials modulo @xmath88 . more generally ,",
    "if the unknown polynomial @xmath8 is given as a straight - line program or arithmetic circuit , each operation in the circuit can be computed over that ring @xmath89/\\langle z^p-1\\rangle$ ] , as in @xcite . in our complexity analysis",
    "below , we assume a cost of @xmath90 for each evaluation on this step , according to the cost of dense degree-@xmath3 polynomial arithmetic over @xmath21 .    the next phase of the algorithm is to gather the images of each term , discard those which appear infrequently ( and thus were resulting from collisions in the reduction modulo @xmath88 ) , and use chinese remaindering to recover the exponents of each nonzero term in @xmath8 .",
    "observe that the list @xmath73 can be implemented in any convenient way according to the details of the parallel implementation ; it serves only as an unordered accumulator of coefficient - exponent - prime tuples .",
    "the output polynomial @xmath8 could also be considered as an unordered accumulator , followed by another parallel sort before the final return statement .",
    "we state the parameterized running time of the algorithm as follows :    [ thm : rtime ] given bounds @xmath91 on the sparsity and degree of an unknown polynomial @xmath92 $ ] , and parameters @xmath93 , algorithm [ proc : interp ] has worst - case running time @xmath94    the computation of @xmath95 at the beginning incurs the @xmath96 cost in the complexity , which for our ultimate choices of @xmath78 will never actually dominate the complexity .",
    "the first loop executes @xmath97 times in each thread . computing the powers of @xmath6 modulo @xmath55 on step  [ step : dpow ] requires @xmath98 bit operations .",
    "the subsequent evaluations using dense arithmetic on step  [ step : eval ] will usually dominate the complexity of the entire algorithm , as each costs @xmath99 , which is @xmath100 .",
    "( the addition of this term makes the @xmath101 factor in the cost of step  [ step : dpow ] become @xmath102 . )",
    "both parallel sorts are on lists of size at most @xmath103 , which means their cost of @xmath104 does not dominate the complexity .    the final for loop executes @xmath105 times in each thread , but the nested if statement can only be triggered @xmath0 times overall .",
    "within it , the most expensive step is computing each @xmath106 , requiring @xmath107 bit operations .",
    "this contributes only @xmath107 to the overall complexity , as the term @xmath108 is already dominated by the parallel cost of step  [ step : eval ] .",
    "the key feature of procedure [ proc : interp ] is that its potential parallel speedup , from the previous theorem , is a factor of @xmath109 , depending on the number of parallel processors @xmath110 that are available .",
    "this exceeds the @xmath32 parallel speedup of previous approaches , and means that our algorithm should scale better to a large number of processors when the number of variables and/or degree are sufficiently large .",
    "we first use prior work to prove the bounds necessary to ensure correctness with provably high probability .",
    "[ thm : bounds ] if the parameters @xmath93 and bounds @xmath111 satisfy @xmath112 then with probability at least @xmath113 , procedure [ proc : interp ] correctly computes the coefficients and exponents of @xmath92 $ ] .",
    "the condition @xmath114 guarantees that positive or negative coefficients whose absolute value is at most @xmath7 will all be distinct modulo @xmath20 , for any prime @xmath115 as chosen in the algorithm .    because @xmath116 , lemma 8 from @xcite tells us that , for each @xmath55 , the probability that more than @xmath117 terms collide modulo @xmath118 is at most @xmath119 .",
    "since the most number of terms that could collide is @xmath1 , this means the _ expected _ number of collisions , for each @xmath55 , is at most @xmath120 .",
    "the total number of nonzero coefficients in all polynomials @xmath54 examined on step  [ step : growl ] is at most @xmath121 .",
    "many of these correspond to single terms in @xmath8 itself , but some will be collisions of terms in @xmath8 . using the reasoning of lemma 4.1 in @xcite , and",
    "the proof of theorem 3.1 from @xcite , every coefficient of a single term in @xmath8 , or a collision that appears in any of the @xmath54 s , is distinct modulo @xmath20 , due to the bound on @xmath86 and because @xmath122 .",
    "we conclude that , with probability at least @xmath113 , each term in @xmath8 appears un - collided in at least @xmath123 of the polynomials @xmath54 , and furthermore that these coefficients are all distinct and are the only ones that repeat in the list @xmath73 .",
    "this guarantees that the correct coefficients and exponents are recovered in the final loop , and the algorithm outputs the correct interpolated polynomial .",
    "applying the bounds in theorem  [ thm : bounds ] to the analysis in theorem  [ thm : rtime ] gives the provable complexity bound for the algorithm as stated in .",
    "as usual , the probability of success in either the provable or the heuristic version of the algorithm ( described below ) can be increased arbitrarily high by running the same algorithm repeatedly and choosing the most common polynomial returned among all runs to be the most likely candidate for @xmath8 .",
    "the heuristic version of this procedure is simply to choose appropriate constants for the parameters @xmath77 and @xmath78 , with the intuition that there can be some trade - off between the size of each prime ( governed by @xmath77 ) and the number of chosen primes ( governed by @xmath78 ) .",
    "furthermore , the bound on @xmath86 required in theory to obtain diversification between the coefficients in @xmath8 and in any collisions is unnecessarily high for most `` typical '' polynomials .",
    "there do exist pathological counterexamples , but they require the degrees of many terms to be equivalent modulo @xmath20 . as the prime @xmath20 is also chosen randomly in our approach , we have a good indication that this heuristic approach will work with high probability for a randomly - chosen sparse polynomial .",
    "we state this as a conjecture , which is also backed by the experimental evidence reported in section  [ sec : exper ] below .    for any sufficiently large height bound @xmath7 , and using @xmath124",
    ", there exist constants @xmath125 such that , for a polynomial @xmath92 $ ] chosen at random with at most @xmath1 nonzero terms , the probability that algorithm [ proc : interp ] successfully interpolates @xmath8 is at least 1/2 .    the heuristic complexity under this conjecture is stated in .      consider an unknown bivariate polynomial @xmath126 with 3 nonzero terms and degree less than 10 .",
    "the primes that we are going to use are going to be small for the sake of this example .",
    "the primes are 7 , 13 , and 17 .    1 .",
    "now we compute @xmath8 modulo @xmath127 , we get : + @xmath128 + we notice here that a collision happened with the prime 13 .",
    "this wo nt affect our calculation later because resulting coefficient @xmath129 does nt appear anywhere else ( for this example , we are going to assume that we have a good coefficient if it appears twice or more ) .",
    "+ now we use these values to fill the list @xmath73 . every triple in @xmath73",
    "consists of the coefficient , the prime , and the exponent .",
    "+ @xmath130 + we then sort @xmath73 based on coefficient values .",
    "+ @xmath131 + we use the chinese remainder theorem to get back the exponent that corresponds to each coefficient .",
    "+ @xmath132 + @xmath133 + @xmath134 + this results in the univariate polynomial @xmath135 + finally , inverting the kronecker map @xmath136 , we obtain @xmath137",
    "we completed a low - level implementation of procedure [ proc : interp ] written in the c programming language .",
    "our complete source code , as well as the exact source we tested for the comparisons and benchmark problems listed later , is available upon request by email .",
    "we give a few details here on the choices of our implementation , in particular the libraries that were utilized .      the key advantage to the `` small primes '' approach which we employed is the reliance on fast subroutines for dense polynomial arithmetic .",
    "the experiments we ran always used a word - sized modulus @xmath20 , and so the most expensive computations involved computing with dense , low - degree polynomials with word - sized modular coefficients .",
    "flint ( http://flintlib.org/ ) is a free , open - source c library for fast number theoretic computations @xcite .",
    "our dense polynomial arithmetic , which is the dominating cost both in theory and in practice in our experiments , was performed using the ` nmod_poly ` data type .    in order to store the result of sparse interpolation and",
    "complete the correctness testing in our experiments , we also added rudimentary support for sparse integer polynomials on top of flint .",
    "we created a new data type , ` fmpz_sparse ` , to represent sparse univariate polynomials in @xmath138 $ ] for testing purposes , using flint s multiple precision type ` fmpz ` as both the coefficient and exponent storage for sparse polynomials .",
    "note that using multiple - precision integers for exponents is especially important , as we have _ not _ yet completed full multivariate polynomial support within flint .",
    "instead , for the purposes of our experiments , we always used a standard kronecker substitution to store @xmath5-variate , degree-@xmath6 multivariate polynomials as univariate sparse polynomials with degree bounded by @xmath36 .",
    "employing a multiple - precision data type allows for the largest possible degree and number of variables , which is crucial as it is precisely this _ supersparse _ case in which our approach has the greatest potential parallel speedup .",
    "message passing interface ( mpi ) allows us to parallelize our algorithm .",
    "as stated earlier , since the slowest part of our algorithm involves calculating the unknown polynomial @xmath8 modulo many polynomials @xmath139 , we use mpi to perform each of these evaluations in parallel .    the function ` mpi_init ` is called at the beginning of the program to spawn an arbitrary number of processes , as specified on the command line .",
    "each of the allocated processes will be executing separately with separate copies of all variables in the original process .",
    "all processes will have unique i d numbers .",
    "the root process will have i d number 0 . by knowing processes i d s we can separate what each process executes .",
    "we used a master - slave model for our algorithm .",
    "the master process evenly distributes how many primes each slave calculates . after getting the primes",
    ", the slave process will compute the following for every prime : @xmath140 , then traverse the resulting univariate polynomial and save all nonzero terms , along with the prime @xmath55 , to an array .",
    "this array of triples is sent back to the master process , which later sorts all the concatenated evaluation arrays and uses chinese remaindering to recover the full polynomial @xmath8 , as described above .",
    "while our experiments were performed on a multi - core machine , and hence using a simpler threading library would have also worked , our goal in using mpi was to demonstrate the full parallel potential of this approach by explicitly detailing the inter - process communication .",
    "furthermore , the mpi implementation could also be used without modification on heterogeneous clusters or other architectures besides multi - core .",
    "we ran our tests on a machine using an intel(r ) core(tm ) i7 - 3930k cpu @ 3.20ghz simulating 12 hyper - threaded cores on 6 physical cores , with 32 gb of ram .",
    "we used a debian gnu system , running the `` unstable '' branch , with linux kernel version 3.16.0 - 4-amd64 .",
    "this is a bleeding - edge system with the most current versions of all software available within the debian repositories .",
    "the first task was to determine experimentally what kind of settings for the parameters @xmath77 and @xmath78 would be appropriate for our heuristic interpolation method .",
    "we found that @xmath141 and @xmath142 worked for a wide range of problem sizes with almost no failures in the randomized algorithm .",
    "the only exceptions we found here were that when the bound on the number of terms @xmath1 in the output was very small , even setting @xmath141 the number of primes @xmath3 in the range @xmath143 $ ] was simply not sufficient to ensure a high success probability .",
    "in these small - size extremes , the value of @xmath77 was increased to accommodate ; in particular , we settled on @xmath144 and @xmath142 when @xmath145 , and when @xmath146 we had to select @xmath147 . under these parameter settings ,",
    "no failures were observed in any of our experiments .",
    "we comment that a smaller @xmath77 value and a larger @xmath78 value would be preferable , because that would increase the number of primes @xmath95 and hence the potential parallel speedup for the algorithm , while decreasing the size @xmath148 of each prime @xmath55 .",
    "however , we found that modestly larger values for @xmath78 did not allow for @xmath77 to be reduced and consistently result in correct output .",
    "we consider the exploration of some better balance between the parameters @xmath77 and @xmath78 as future work .      *",
    "6r*3|r & & & & & & & & + 1 & 20 & 3 & 40 & 14 & 50000 & 0.078 & 0.035 & 0.029 + 2 & 20 & 9 & 80 & 18 & 5000 & 0.155 & 0.151 & 0.048 + 3 & 20 & 27 & 120 & 18 & 6900 & 0.305 & 0.329 & 0.116 + 4 & 20 & 81 & 160 & 18 & 4900 & 0.598 & 0.323 & 0.085 + 5 & 20 & 243 & 200 & 17 & 9900 & 2.156 & 0.785 & 0.175 + 6 & 20 & 729 & 240 & 15 & 39900 & 5.053 & 3.084 & 0.814 + 7 & 20 & 2187 & 280 & 14 & 80050 & 13.333 & 8.714 & 2.225 + 8 & 20 & 6561 & 320 & 13 & 321300 & 41.070 & 43.911 & 10.605 +    ]    the first experiment was to test our algorithm without parallelization against the efficient `` big primes '' implementation in mathemagix ( http://www.mathemagix.org/ ) , as reported in @xcite .",
    "we downloaded the source code for the mathemagix sparse interpolation program , then ran it with @xmath149 polynomials being multiplied .",
    "we kept each polynomial at 20 variables , with degree 40 , 3 terms and coefficients up to @xmath150 .",
    "then we ran our algorithm with the same parameters .",
    "the results are shown in taible  [ tab : comp ] and summarized in figure  [ fig : comparison ] .",
    "( we also attempted a comparison against the ` sinterp ` function in maple 2015 , but it was more than an order of magnitude slower in all experiments . )    our non - parallelized algorithm seems to be on par with the mathemagix implementation for this range of problem sizes . as a comparison , and to emphasize the main point of our paper , we also show the full parallel speedup for the same problems in figure  [ fig : comparison ] .      ]",
    "the second experiment was to test the parallel speedup for our implementation by varying the degree size @xmath6 and leaving @xmath141 , @xmath151 , partial @xmath54 , and @xmath152 constant .",
    "@xmath6 was varied from @xmath153 , @xmath154 , @xmath155 .",
    "this test was performed 3 separate times and the resultant data was calculated from median of the three trials .",
    "the results are seen in figure  [ fig : speedup ] .",
    "figure  [ fig : speedup ] shows a linear speedup increase as the number of parallel processes used gets closer to the physical number of cores on the machine .",
    "additionally , we can see the most significant parallel speedup occurs for the highest degree tested .",
    "recall that on our machine , there are only 6 physical cores that are hyper - threaded to 12 virtual cores .",
    "furthermore , when running only six threads , the `` turbo mode '' clock rate is increased to 3.8ghz .",
    "this may help to explain the dip in performance seen around @xmath152 parallel processes .",
    "observe also that the parallel speedup is best , and continues the furthest , in the most extremely sparse case with very high degree .",
    "our parallel speedup is demonstrated in figure  [ fig : speedup ] .",
    "we have shown that the `` small primes '' sparse interpolation algorithm is competitive with the state of the art , even without parallelization , especially for very sparse problem instances .",
    "furthermore , there is greater potential for parallelism in the small primes technique .",
    "these theoretical results are borne out in practice in our experimental results compared to other available software implementations .",
    "there is significantly more work to be done , however , before we might suggest widespread adoption of our heuristic sparse interpolation method .",
    "we would like to understand the theory behind the heuristic approach in order to have a less _ ad hoc _ way of determining the parameters @xmath77 and @xmath78 . on the other hand ,",
    "our implementation could be greatly enhanced with further experimentation on a wider range of benchmark problems and incorporating true multivariate sparse polynomial representations .",
    "this work was supported by the national science foundation under award number # 1319994 .",
    "we thank andrew arnold and the anonymous pasco 2015 reviewers for their comments on an earlier draft of this paper .",
    "andrew arnold , mark giesbrecht , and daniel  s. roche .",
    "faster sparse interpolation of straight - line programs . in vladimir",
    "p. gerdt , wolfram koepf , ernst  w. mayr , and evgenii  v. vorozhtsov , editors , _ proc .",
    "computer algebra in scientific computing ( casc 2013 ) _ , volume 8136 of _ lecture notes in computer science _",
    ", pages 6174 .",
    "springer , september 2013 .",
    "doi : 10.1007/978 - 3 - 319 - 02297 - 0_5 .",
    "andrew arnold , mark giesbrecht , and daniel  s. roche .",
    "sparse interpolation over finite fields via low - order roots of unity . in _ proceedings of the 39th international symposium on symbolic and algebraic computation _ , issac 14 , pages 2734 , new york , ny , usa , 2014 .",
    "doi : 10.1145/2608628.2608671 .",
    "michael ben - or and prasoon tiwari .",
    "a deterministic algorithm for sparse multivariate polynomial interpolation . in _ proceedings of the twentieth annual acm symposium on theory of computing _ , stoc 88 , pages 301309 , new york , ny , usa , 1988 .",
    "isbn 0 - 89791 - 264 - 0 .",
    "doi : 10.1145/62212.62241 .",
    "brice boyer , matthew  t. comer , and erich  l. kaltofen .",
    "sparse polynomial interpolation by variable shift in the presence of noise and outliers in the evaluations . in _ electr .",
    "tenth asian symposium on computer mathematics ( ascm 2012 ) _ , 2012 .",
    "annie cuyt and wen - shin lee . a new algorithm for sparse interpolation of multivariate polynomials .",
    "_ theoretical computer science _ ,",
    "4090 ( 2):0 180185 , 2008 .",
    "issn 0304 - 3975 .",
    "doi : 10.1016/j.tcs.2008.09.002 .",
    "symbolic - numerical computations .",
    "sanchit garg and ric schost .",
    "interpolation of polynomials given by straight - line programs .",
    "_ theoretical computer science _ ,",
    "4100 ( 27 - 29):0 26592662 , 2009 .",
    "issn 0304 - 3975 .",
    "doi : 10.1016/j.tcs.2009.03.030 .",
    "mark giesbrecht and daniel  s. roche .",
    "diversification improves interpolation . in _ proceedings of the 36th international symposium on symbolic and algebraic computation _",
    ", issac 11 , pages 123130 , new york , ny , usa , 2011 .",
    "isbn 978 - 1 - 4503 - 0675 - 1 .",
    "doi : 10.1145/1993886.1993909 .",
    "bruno grenet , joris van der hoeven , and grgoire lecerf .",
    "randomized root finding over finite fft - fields using tangent graeffe transforms . in _ proc .",
    "40th international symposium on symbolic and algebraic computation _ , issac 15 , page to appear , 2015 .",
    "seyed mohammad  mahdi javadi and michael monagan .",
    "parallel sparse polynomial interpolation over finite fields . in _ proceedings of the 4th international workshop on parallel and symbolic computation _ ,",
    "pasco 10 , pages 160168 , new york , ny , usa , 2010 .",
    "isbn 978 - 1 - 4503 - 0067 - 4 .",
    "doi : 10.1145/1837210.1837233 .",
    "erich kaltofen and wen - shin lee .",
    "early termination in sparse interpolation algorithms .",
    "_ journal of symbolic computation _ , 360 ( 3 - 4):0 365400 , 2003 .",
    "issn 0747 - 7171 .",
    "doi : 10.1016/s0747 - 7171(03)00088 - 9 .",
    "issac 2002 .",
    "erich kaltofen and lakshman yagati .",
    "improved sparse multivariate polynomial interpolation algorithms . in p.",
    "gianni , editor , _ symbolic and algebraic computation _ , volume 358 of _ lecture notes in computer science _ , pages 467474 .",
    "springer berlin / heidelberg , 1989 .",
    "doi : 10.1007/3 - 540 - 51084 - 2_44 .",
    "erich kaltofen , y.  n. lakshman , and john - michael wiley .",
    "modular rational sparse multivariate polynomial interpolation . in _ proceedings of the international symposium on symbolic and algebraic computation _",
    ", issac 90 , pages 135139 , new york , ny , usa , 1990 .",
    "isbn 0 - 201 - 54892 - 5 .",
    "doi : 10.1145/96877.96912 .",
    "erich  l. kaltofen .",
    "fifteen years after dsc and wlss2 : what parallel computations i do today [ invited lecture at pasco 2010 ] . in _ proceedings of the 4th international workshop on parallel and symbolic computation _ ,",
    "pasco 10 , pages 1017 , new york , ny , usa , 2010 .",
    "isbn 978 - 1 - 4503 - 0067 - 4 .",
    "doi : 10.1145/1837210.1837213 .",
    "erich  l. kaltofen , wen - shin lee , and zhengfeng yang .",
    "fast estimates of hankel matrix condition numbers and numeric sparse interpolation . in _ proceedings of the 2011 international workshop on symbolic - numeric computation _ , snc 11 , pages 130136 , new york , ny , usa , 2011 .",
    "isbn 978 - 1 - 4503 - 0515 - 0 .",
    "doi : 10.1145/2331684.2331704 .",
    "richard zippel .",
    "interpolating polynomials from their values .",
    "_ journal of symbolic computation _ , 90 ( 3):0 375403 , 1990 .",
    "issn 0747 - 7171 .",
    "doi : 10.1016/s0747 - 7171(08)80018 - 1 . computational algebraic complexity editorial ."
  ],
  "abstract_text": [
    "<S> to interpolate a supersparse polynomial with integer coefficients , two alternative approaches are the prony - based `` big prime '' technique , which acts over a single large finite field , or the more recently - proposed `` small primes '' technique , which reduces the unknown sparse polynomial to many low - degree dense polynomials . while the latter technique has not yet reached the same theoretical efficiency as prony - based methods , it has an obvious potential for parallelization . </S>",
    "<S> we present a heuristic `` small primes '' interpolation algorithm and report on a low - level c implementation using flint and mpi . </S>"
  ]
}