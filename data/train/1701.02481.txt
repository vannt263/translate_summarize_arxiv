{
  "article_text": [
    "distributed word representation , which is also called word embedding , has been a hot topic in the area of natural language processing ( nlp ) . for its effectiveness ,",
    "the learned word representations have been used in plenty of tasks such as text classification @xcite , information retrieval @xcite and sentiment analysis @xcite .",
    "there are many classic word embedding methods including the semantic extraction using a neural network architecture ( senna ) @xcite , continuous bag - of - word ( cbow ) , skip - gram @xcite , global vector ( glove ) @xcite , etc .",
    "these models can only capture word - level semantic information and ignore the meaningful inner structures of words such as english morphemes and chinese characters .",
    "the effectiveness of exploiting the internal structures has been validated by several models @xcite .",
    "these models split a word into several parts , which are directly added into the input layer of their models to improve the representation of target word . in this paper , we call these methods which directly utilize the internal structures as _ explicit _ models .",
    "actually , incorporating the inner information of words into word embedding is meaningful and promising . for english morphemes , words starting with prefix  a \" or  an \" will have the meanings of  not `` and ' ' without \" like  asexual \" and  anarchy \" . in addition , words ending with suffix  able \" or  ible \" will have the meaning of  capable \" like  edible \" and  visible \"",
    ". obviously , both the morpheme and their meanings are beneficial .",
    "nevertheless , the explicit models only care about morphemes themselves and ignore their meanings . in vector space of explicit model ,",
    "morpheme - similar words will have a trendency to stay together while the long distances between these words and their morphemes meanings are still remained .",
    "for clarity , we illustrate the basic idea of these models in the left part of figure 1 .        in this paper , we propose three more refined word embedding models by implicitly exploiting the meanings of morphemes rather than morphemes themselves . based on our knowledge ,",
    "morphemes used in this paper are divided into 3 parts including prefix , root and suffix . in the vector space of our models , morpheme - similar words in vector space",
    "will not only have a trendency to gather together but also tend to locate close to their morphemes meanings . the basic ideas of our methods are shown in the right part of figure 1 .",
    "for comparison , our models together with the state - of - the - art baselines are tested on word similarity and syntactic analogy .",
    "the result shows that our models outperform the baselines and get significantly improvement on both tasks .",
    "the contributions of this paper are summarized as follows .",
    "* to the best of our knowledge , we are the first to implicitly utilize the morphological information . based on this strategy , word embeddings in vector space have a more precise structure , which make our models significantly outperform the state - of - the - art baselines on experiments .",
    "* according to parameter analysis , our models seem to have the ability to increase the semantic information in the corpus , which lead us to believe our models are of great advantages to deal with some morpheme - rich but low - resource languages . *",
    "the strategy of implicitly incorporating morphological information can benefit the areas where the morphemes are directly utilized in the past . for convenience to researchers interested in this area ,",
    "we create a word map where the key is a word and the value is its morphemes meanings set .",
    "our implicit models are built upon the cbow @xcite .",
    "we firstly review the backgrounds of cbow , and then present related work on recent word - level and inner structure - based word embedding methods .    * * cbow with negative sampling**with a slide window , cbow @xcite utilizes the context words in the window to predict the target word .",
    "given a sequence of tokens @xmath0 , the objective of cbow is to maximize the following average log probability equation : @xmath1 where @xmath2 means the context words of @xmath3 in the slide window . based on softmax ,",
    "@xmath4 is defined as follows : @xmath5 where @xmath6 and @xmath7 are the  input \" and  output \" vector representation of token @xmath8 , @xmath9 means the slide window size and @xmath10 means the vocabulary size . due to huge size of english vocabulary , however , eq.([cbow1 ] ) can not be calculated in a tolerable time .",
    "therefore , negative sampling and hierarchical softmax are proposed to solve this problem . due to the efficiency of negative sampling ,",
    "all related models are trained based on it . in terms of negative sampling ,",
    "every item whose form is like @xmath11 is defined as follows : @xmath12 , \\end{split}\\ ] ] where @xmath13 indicates the number of negative samples , and @xmath14 is the sigmoid function .",
    "the first item of eq.([negative ] ) is the probability of target word when its context is given .",
    "the second item eq.([negative ] ) denotes the probability that negative samples do nt share the same context as the target word .    * * word - level word embedding**in general , word embedding models are divided into two main branches .",
    "one is based on neural network while the other is based on matrix factorization . except for the cbow ,",
    "skip - gram is another widely used model , which predicts the context by using the target word @xcite . based on matrix factorization , dhillon et al .",
    "proposed a spectral word embedding method to measure the correlation between word information matrix and context information matrix @xcite . in order to combine the advantage of neural network - based model with the advantage of matrix factorization ,",
    "pennington proposed another famous word embedding model named glove , which is reported to outperform the cbow and skip - gram model @xcite on some tasks @xcite .",
    "these models are all effective to capture word - level semantic information while neglect inner information of words . on the contrast",
    ", the unheeded inner inner information is utilized in both our implicit models and other explicit models .    *",
    "inner structure - based word embedding * recently , some more fine - grained word embedding models are proposed by exploiting the internal structures .",
    "chen et al . proposed a character - enhanced chinese word embedding model , which split a chinese word into several characters and add the characters into the input layer of their models @xcite .",
    "based on recursive neural network , luong et al . utilized the weighted morpheme part and basic part of an english word to improve the representation of it @xcite .",
    "kim et al utilized the convolutional character information to embed the english words@xcite .",
    "their model can learn semantic information from character - level , which is proved to be effective to deal with some morpheme - rich languages . with a huge size architecture , however , it is very time - consuming .",
    "cotterell et al .",
    "augmented the log linear model to make the words , which share morphological information , gather together in vector space @xcite .",
    "these models can improve the linguistic similarity with respect to semantic , syntactic or morphology to some degree .",
    "nevertheless , these explicit models can only capture the facial linguistic information of internal structures and ignore their meanings .",
    "in contrast , our models not only exploit the inner information but also consider about the valuable meanings of inner structures .",
    "in this section , we present 3 novel models named mwe - a , mwe - s and mwe - m by implicitly incorporating the morphological information .",
    "mwe - a assumes that all morphemes meanings of a token have equal contributions to the representation of the target word .",
    "mwe - a is applicable to the refined morphemes meanings set which contains a few outliers .",
    "however , refining the morphemes meanings set is time - consuming and needs a lot of human annotation , which costs a lot .",
    "mwe - s is proposed to solve this problem .",
    "motivated by the attention scheme , mwe - s holds the assumption that all morphemes meanings have different contributions . in this model ,",
    "the outliers are weighed with a small value , which means they will have a few effects on the representation of the target word .",
    "different from mwe - a and mwe - s , mwe - m actually is an approximate method . in this model , we only care about the morphemes meanings which have the greatest contribution to the target word .",
    "the remaining words are viewed as outliers and removed out of the morphemes meanings set .",
    "we also discussed the strategy to find the words morphemes in this section .",
    "this model is built on the assumption that all morphemes meanings of token @xmath3 have equal contributions to @xmath3 .",
    "given a sequence of tokens @xmath0 , we assume that the morphemes meanings set of @xmath15)$ ] is @xmath16 .",
    "@xmath16 can be divided into three parts @xmath17 , @xmath18 and @xmath19 , which indicate the prefix meaning set , root meaning set and suffix meaning set of @xmath3 , respectively .",
    "hence , when @xmath3 is the context word of @xmath20 , the modified embedding of @xmath3 can be defined as follows : @xmath21 where @xmath22 is the original word embedding of @xmath3 , @xmath23 denotes the length of @xmath16 and @xmath24 indicates the vector of @xmath25 . in this paper , we assume the original word embedding and the sum of its morphemes meanings embedding have equal weight .",
    "namely , they are both 0.5 .",
    "more details can be found in figure 2 .",
    "this model is built based on the attention scheme .",
    "we observe that a lot of words have more than one morpheme s meanings .",
    "for instance , word  anthropologist \" , its morpheme s meaning set is \\{man , human , who , which}. obviously ,  man \" and  human \" are closer to  anthropologist \" . motivated by this observation",
    ", we assume that different morphemes have different contributions . in this paper",
    ", we firstly train a part of word embeddings based on cbow .",
    "then , based on the pre - trained word embedding , the different contributions can be measured by calculating the cosine distances between target word and its morphemes meanings .",
    "we illustrate the main idea of this model in figure 3 .",
    "the notation in mwe - a is reused to describe mwe - s .",
    "mathematical formation is given by @xmath26,\\ ] ] where @xmath22 is the original vector of @xmath3 , @xmath27 is the function used to denote the similarities between @xmath3 and its morphemes meanings .",
    "we define the function @xmath28 to denote the cosine distance between @xmath29 and @xmath30 .",
    "then , @xmath27 is normalized as follows : @xmath31          from the collections of morphemes and their meanings , we find that except for multiple meanings of a morpheme , the corresponding meanings of a token s prefix , root and suffix are usually different .",
    "however , some meanings are not close to the target word , which may affect the quality of word representations . to achieve better performance",
    ", we only exploit the morpheme s meaning whose distance to target word is the max . more details are shown in figure 4 . in that figure ,",
    "the suffix s meanings of  anthropologist \" are  who",
    "\" and  which \" . according to our description , the word  which \" in red will not be applied for its shorter distance to  anthropologist \" . for a single token @xmath3",
    ", the new morphemes meanings set is defined as @xmath32 where @xmath33 are defined as follows : @xmath34 where @xmath35 stands for cosine distance .",
    "hence , mwe - m is defined mathematically as follows : @xmath36,\\ ] ] where @xmath37 indicates the similarity between two words and needs to be normalized like eq.([normalize ] ) shows .",
    "till now , we need to discuss how to find the morphemes in a single token .",
    "firstly , it is obvious that a number of words contain more than one prefix , root and suffix .",
    "for instance , `` anthropologist '' has the prefix `` a , an , anthrop '' .",
    "the question raised here is which one should be used .",
    "secondly , some morphemes meanings of a word have nothing to do with that word .",
    "for instance , although  apple \" has the prefix  a \" , it does nt have the meanings of  without , not \" . to solve these problems ,",
    "some rules are defined as follows :    * when matching the morphemes of word @xmath3 , the longest character sequence will be viewed as the morpheme . *",
    "we define a threshold @xmath38 . if the cosine distance between word @xmath3 and its morpheme @xmath39 , ( @xmath40 ) is larger than @xmath38 , @xmath39 will be remained or abandoned otherwise .",
    "based on cross validation , @xmath38 is set as 0.4 in this paper .",
    "our models together with the baselines are tested on word similarity and syntactic analogy .      in this paper",
    ", we utilize a medium - sized english corpus to train all word embedding models .",
    "the corpus stems from the website of the 2013 acl workshop on machine translation and is used in @xcite .",
    "this website lists a lot of corpora ordered by year when they are released .",
    "we choose the news corpus of 2009 whose size is about 1.7 gb . it contains approximately 500 millions of tokens and 600 thousands of vocabularies . for better quality of all word embeddings ,",
    "we filter all numbers and some punctuations out of the corpora .",
    "all morphemes used in this paper and their meanings are both collected from the website .",
    "the morphemes set includes 90 prefixes , 241 roots and 64 suffixes . based on our knowledge",
    ", the morphemes meanings are refined . by using the lexicon ppdb @xcite",
    ", we create a map where the key is a word and the value is its morphemes meanings set for convenience to scholars interested in this area .      for comparison , we choose three state - of - the - art baselines including cbow , skip - gram @xcite and glove @xcite .",
    "all baselines are explicit methods , and hold some state - of - the - art performance on the nlp tasks . for testing the differences between our models and traditional explicit ways",
    ", we directly utilize the morphemes rather than their meanings on the input layer of mwe - a . for clarity , this model is named explicitly morpheme - enhanced word embedding ( emwe ) whose architecture is similar to the context - sensitive morphological recursive neural network ( csm - rnn ) in @xcite . in this paper",
    ", we utilize the source code of word2vec to train cbow and skip - gram .",
    "glove is trained based on the code .",
    "we modify the source of word2vec and train our models and emwe .",
    "parameter settings have a great effect on the performance of word embeddings @xcite . for fairness ,",
    "all models are trained based on equal parameter settings . in order to accelerate the training process , cbow , skip - gram , emwe together with our models",
    "are trained by using negative sampling .",
    "it is reported that the number of negative samples in the range 5 - 20 is useful for small corpus @xcite .",
    "if huge - sized corpus is used , the number of negative samples should be chosen from 2 to 5 . according to the corpus we used ,",
    "the number of negative samples is set as 20 in this paper .",
    "the dimension of word embedding is set as 200 like that in @xcite .",
    "we set the context window size as 5 which is equal to the setting in @xcite .",
    "we test all word embeddings from two aspects including word similarity and syntactic analogy .",
    "this experiment is used to evaluate word embedding s ability to capture semantic information from corpus .",
    "each dataset is divided into three columns .",
    "the first two columns stand for word pairs and the last column is human score . on the task ,",
    "there are two problems need to be solved .",
    "one is how to calculate the distance between two words .",
    "the other is how to evaluate the similarity between our results and human scores .",
    "for the first problem , we utilize the cosine distance to measure the distance between two words .",
    "this strategy is used in many previous works @xcite .",
    "the second problem is solved via spearman s rank correlation coefficient @xmath41 .",
    "higher @xmath42 means better performance . for english word similarity , we employ two golden standard datasets including wordsim-353 @xcite and rg-65 @xcite .",
    "to avoid occasionality , however , we also utilize some other widely - used datasets including rare - word @xcite , scws @xcite , men-3k @xcite and ws-353-related @xcite .",
    "more details of these datasets are shown in table 1 .",
    "[ wordsim ]    [ size ]    .details of datasets .",
    "the columns of pairs indicate the number of word pairs in each dataset . [",
    "cols=\"^,^,^,^\",options=\"header \" , ]      +    to further demonstrate the validity and effectiveness of our models , the dimension of word embedding is reduced from 200 to 2 with principal component analysis ( pca ) .",
    "we randomly select several words from the embedding of mwe - a and illustrate them in figure 7 .",
    "different colors stand for words with different morphemes .",
    "it is apparent that words with similar morpheme have a trend to group together .",
    "in this paper , we proposed a novel method to implicitly incorporate morphological information into word embedding . in our models , the morphemes meanings are added on the input layer of cbow rather than morphemes themselves .",
    "based on this strategy , morpheme - similar words will not only gather together but also have a trend to group near their morphemes meanings .",
    "based on the different strategies to add the morphemes meanings , three models named mwe - a , mwe - s and mwe - m were built . to test the performance of our embeddings , we utilized three comparative baselines and tested them on the word similarity and syntactic analogy tasks .",
    "the results show that our models outperform the baselines on five word similarity datasets . on the golden standard wordsim-353 and rg-65",
    ", our models even approximately outperform cbow for 5 percent and 7 percent , respectively . on syntactic analogy task ,",
    "our models surpass the baselines to a great extent . compared with cbow , mwe - a approximately",
    "outperforms it for 7 percent .",
    "compared to the explicit method , our models also show a great advantage .",
    "significant improvements on both tasks lead us to believe the validity of our models .",
    "last but not the least , our models seem to have great advantages to train some low - resource but morpheme - rich languages according to the observation of token size analysis . in future work , we will test our models ability to deal with some morpheme - rich languages like germany and french .",
    "10 e.  agirre , e.  alfonseca , k.  hall , j.  kravalova , m.  paca , and a.  soroa , `` a study on similarity and relatedness using distributional and wordnet - based approaches , '' in _ proceedings of human language technologies : the 2009 annual conference of the north american chapter of the association for computational linguistics_.1em plus 0.5em minus 0.4emassociation for computational linguistics , 2009 , pp .",
    "1927 .",
    "r.  collobert and j.  weston , `` fast semantic extraction using a novel neural network architecture , '' in _ acl 2007 , proceedings of the meeting of the association for computational linguistics , june 23 - 30 , 2007 , prague , czech republic _ , 2007 .",
    "r.  cotterell and h.  schtze , `` morphological word - embeddings , '' in _ conference of the north american chapter of the association for computational linguistics : human language technologies _ , 2015 , pp .",
    "12871292 .",
    "l.  finkelstein , e.  gabrilovich , y.  matias , e.  rivlin , z.  solan , g.  wolfman , and e.  ruppin , `` placing search in context : the concept revisited , '' in _ proceedings of the 10th international conference on world wide web_.1em plus 0.5em minus 0.4emacm , 2001 , pp .",
    "406414 .",
    "e.  h. huang , r.  socher , c.  d. manning , and a.  y. ng , `` improving word representations via global context and multiple word prototypes , '' in _ meeting of the association for computational linguistics : long papers _ , 2012 , pp .",
    "873882 .",
    "j.  xu , j.  liu , l.  zhang , z.  li , and h.  chen , `` improve chinese word embeddings by exploiting internal structure , '' in _ conference of the north american chapter of the association for computational linguistics : human language technologies _ , 2016 , pp ."
  ],
  "abstract_text": [
    "<S> in this paper , we propose three novel models to enhance word embedding by implicitly using morphological information . </S>",
    "<S> experiments on word similarity and syntactic analogy show that the implicit models are superior to traditional explicit ones . </S>",
    "<S> our models outperform all state - of - the - art baselines and significantly improve the performance on both tasks . </S>",
    "<S> moreover , our performance on the smallest corpus is similar to the performance of cbow on the corpus which is five times the size of ours . </S>",
    "<S> parameter analysis indicates that the implicit models can supplement semantic information during the word embedding training process . </S>"
  ]
}