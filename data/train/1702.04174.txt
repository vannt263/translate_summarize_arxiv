{
  "article_text": [
    "facial expression analysis is a rapidly growing field of research , due to the constantly increasing interest in , and feasibility of applying automatic human behaviour analysis to all kinds of multimedia recordings involving people .",
    "applications include classical psychology studies , market research , interactions with virtual humans , multimedia retrieval , and the study of medical conditions that alter expressive behaviour @xcite . given the increasing prominence and utility of expression recognition systems , it is is increasingly important that such systems can be evaluated fairly and compared systematically . the fg 2017 facial expression recognition and analysis challenge ( fera 2017 ) shall support this effort by addressing three aspects frequently ignored in existing benchmarks : head - pose , expression intensity , and video duration .    most facial expression recognition and analysis systems proposed in the literature focus on analysis of expressions from frontal faces .",
    "while it can be argued that in many scenarios people s faces will indeed be largely frontal most of the time , there are also many conditions in which either the camera angle is such that obtaining frontal views is unrealistic , or where the head pose with respect to the camera varies widely over time .",
    "there are a few databases that include a number of non - frontal head - poses of a limited set of posed expressions .",
    "multi - pie recorded a very small number of expressions simultaneously with 15 cameras placed around the subject in a well - lit office setting @xcite .",
    "the mmi - face database obtained frontal and profile views of all facial action coding system action units ( facs aus , @xcite ) and six basic emotions using a mirror @xcite .",
    "but only databases recorded with depth - sensing cameras can generate an almost arbitrary set of face views of the same facial expression .",
    "one example database is the bosphorus corpus @xcite .",
    "a second limitation of many existing benchmark databases is that they assume expression intensity is fixed , and as such they do not support the evaluation of intensity estimation .",
    "most databases focus on detecting the occurrence of expressions , regardless of the significant differences in appearance , shape , and temporal dynamics caused by different expression intensities . in reality",
    ", expressions can vary greatly in intensity , and intensity is often a crucial cue for the interpretation of the meaning of expressions .",
    "indeed mckeown et al .",
    "@xcite have argued that the level of intensity is the key dimension in facial expressions that distinguishes whether they are delivered for socio - communicative functions at low levels of intensity or that they become hard - to - fake signals indicating that the expression is associated with a genuine felt emotion at high levels of intensity .",
    "if this is the case then intensity may be one of the most important features in assessing a user s psychological state from facial expressions .",
    "however , very little annotated data is available for the evaluation of au intensity estimation approaches .",
    "fera 2015 made a significant step towards benchmarking au intensity estimation , however , the data used in that challenge was predominantly of ( near ) frontal views @xcite .    finally , despite efforts towards evaluation standards of face video lasting longer than a few seconds ( e.g. fera 2011 @xcite ) , video duration remains an issue that must be addressed by benchmarking challenges .",
    "in particular , the community needs to move away from evaluation procedures where each video recording consists of only a single expression , often with the onset and offset of an expression expressly defined .",
    "instead , we need unsegmented videos that show multiple expression , with ideally expressions naturally transitioning one into another , without explicit neutral divisions in between .    in these respects ,",
    "fera 2017 shall help raise the bar for expression recognition by challenging participants to estimate au intensity in face video of variable duration with unknown head - pose , thereby continuing to bridge the gap between excellent research on facial expression recognition and comparability and replication of results . in fera 2017",
    ", we will use the bp4d+ dataset @xcite to generate from every video 9 different 2d views , based the underlying 3d source data .",
    "the challenge is to detect the occurrence and intensity of aus , without knowing a priori what the facial view will be .",
    "we do this by means of two selected tasks : the detection of facs action unit occurrence ( occurrence detection sub - challenge ) , and fully automatic au intensity estimation where the occurrence of aus is not known beforehand ( intensity estimation sub - challenge ) .",
    "facial expression recognition in general and action unit detection in particular have been studied extensively over the past decade . as a result",
    ", it is impossible to provide a comprehensive review of the field here .",
    "instead we provide an overview of the relevant works only , focussing on methods that target au occurrence detection and intensity estimation . for a general overview of the field of expression recognition",
    "we refer the reader to excellent recent surveys @xcite .",
    "common binary classifiers applied to this problem include artificial neural networks ( ann ) , boosting techniques , and support vector machines ( svm ) .",
    "anns were the most popular method in earlier works ( e.g. @xcite , @xcite ) .",
    "boosting algorithms , such as adaboost and gentleboost , have been a common choice for au recognition ( e.g. @xcite , @xcite ) .",
    "boosting algorithms are simple and quick to train .",
    "they have fewer parameters than svm or ann , and can be less prone to overfitting .",
    "they implicitly perform feature selection , which is desirable for handling high - dimensional data and speeding up inference , and can handle multiclass classification .",
    "svms are currently the most popular choice ( e.g. @xcite , @xcite , @xcite ) .",
    "svms provide good performance , can be non - linear , parameter optimisation is relatively easy , as efficient implementations are readily available , and a choice of kernel functions provides extreme flexibility of design .      the goal in au intensity estimation is to assign a per - frame label with possible integer value from 0 to 5 for each au .",
    "this problem can be approached using either a classification or a regression learning method .    _",
    "classification - based methods : _ some approaches use the confidence of a ( binary ) frame - based au activation classifier to estimate au intensity .",
    "the rationale is that the lower the intensity is , the harder the classification will be . for example , bartlett et al . used the distance of the test sample to the svm separating hyperplane @xcite , while hamm et al .",
    "used the confidence of the decision given by adaboost @xcite .",
    "it is however more natural to treat the problem as 6-class classification .",
    "for example , mahoor et al . employed six one - vs .- all binary svm classifiers @xcite",
    "alternatively , a single multi - class classifier ( e.g. ann or a boosting variant ) could be used .",
    "the extremely large class overlap means however that such approaches are unlikely to be optimal .",
    "girard @xcite found that multi - class and regression - based approaches more accurately detected intensity in comparison with distance - from - hyperplane based measures .",
    "_ regression - based methods : _ au intensity estimation is nowadays often posed as a regression problem .",
    "regression methods penalise incorrect labelling proportionally to the difference between ground truth and prediction .",
    "such ordinal consideration of the labels is absent in classification methods .",
    "the large overlap between classes also implies an underlying continuous nature of intensity that regression techniques are better equipped to model .",
    "examples include support vector regression ( @xcite , @xcite and @xcite ) .",
    "kaltwang et al .",
    "instead used relevance vector regression to obtain a probabilistic prediction @xcite .",
    "the training data for the fera 2017 challenge is derived from the bp4d - spontaneous database @xcite , and the validation and test data of fera 2017 is derived from a subset of bp4d+ database @xcite .",
    "data is split into train , validation , and test partitions .",
    "the train and development partitions are publicly available for researchers to train and develop their au analysis systems , and to allow participants to uniformly report performance ( i.e. using cross - validation ) .",
    "the test partition is held back by the organisers .",
    "participants submit their trained systems and the fera 2017 organisers apply their systems on this held - back data to create a fair comparison .",
    "the challenge will focus on 10 aus that occurred frequently in the bp4d dataset .",
    "the occurrence detection sub - challenge requires participants to detect 10 aus from the bp4d database ( see table [ t : challengeaus ] ) .",
    "aus were selected based on their frequency of occurrence and sufficiently high inter - rater reliability scores .",
    "au intensity estimation will be done on a subset of 7 aus only ( see table [ t : challengeaus ] ) .",
    "contrary to the fera 2015 challenge , for fera 2017 , nine videos were created for each corresponding recording , ranging different face orientations , using the 3d models captured for each of the subjects . to create nine different face orientations , 3d sequences in bp4d and bp4d+",
    "were rotated by -40 , -20 , and 0 degrees pitch and -40 , 0 , and 40 degrees yaw from frontal pose using zface @xcite and the known correspondence between 2d and 3d vertices .",
    "zface is real - time face alignment software that accomplishes dense 3d registration from 2d videos and images without requiring person - specific training .",
    "we calculated the true 3d locations of the facial landmarks by mapping them to the ground truth 3d meshes .",
    "faces were centred and scale was normalised to the average interocular distance of all subjects .",
    "an example of the resulting pose orientations addressed in this challenge is shown in figure  [ f : views ] .",
    ".overview of aus included in the two sub - challenges [ cols=\"<,^,^ \" , ]      the baseline system is kept simple on purpose since it should be easy to interpret and simple to replicate .",
    "contrary to previous challenge editions , we decided to model the temporal dynamics , with the aim of covering misaligned frames .",
    "the learning method for the temporal dynamics modelling is conditional random field ( crf , @xcite ) , for the au occurrence sub - challenge and conditional ordinal random field ( corf , @xcite ) , for the intensity sub - challenge using the geometric features extracted using the method described above .",
    "we have divided the training videos into segments of 90 frames with a stride window of 30 frames .",
    "this way , given that expressions are spontaneous , we can encode short sequences .",
    "moreover , short segments of missing frames can be compensated by the dynamics predicted by a graphical model , something that can not be done using a simple frame - by - frame estimator , which would be highly affected by the tracking system . also , to avoid the influence of inaccurate tracked points for model training , and in order to prove the generalisation of our method to different views , we used for training only the geometric features extracted in the videos corresponding to views 5 and 6 ( i.e. , containing frontal faces ) . for each of the aus , the training set",
    "is balanced so that the amount of samples per class is approximately the same ( when possible ) .",
    "the dimensionality of the feature vector is 158 , although it is reduced using a correlation feature selection ( cfs ) method .    at test time",
    ", videos are evaluated following the same process : 90-frames windows are evaluated by the crf / corf model , with a stride of 30 frames . for each of the windows ,",
    "inference returns the likelihood of the chosen class .",
    "then , we have three predictions per - frame , with their corresponding likelihoods . the final per - frame assignment is given by the prediction attached to the maximum likelihood .",
    "baseline results for occurrence / activation detection , and intensity estimation , are shown in table  [ t : baseline_occurrence_results ] and table  [ t : baseline_intensity_results ] respectively , for the development and test partitions .",
    "detection performance is measured by f1 as well as accuracy and 2afc scores , for the occurrence challenge , and by icc , rmse and pcc scores for the intensity estimation challenge . a number of different performance measures are shown since each has their own merits , and combined they provide a deeper analysis of the results . however the challenge participants will only be judged based on f1/icc scores .    in order to demonstrate and evaluate the generalisation capabilities of the baseline system to different views , we have also included f1 scores and accuracy per view , which are shown in table  [ t : baseline_occurrence_per_view ] and table  [ t : baseline_occurrence_per_view_test ] .",
    "the first row shows the percentage of frames per view that were properly detected / tracked .",
    "however , a high percentage does not necessarily mean that the accuracy of detected points is good enough to encode facial expressions .",
    "from the results shown in table  [ t : baseline_occurrence_per_view ] , and after visual inspection , it can be seen that videos corresponding to view 4 have results close to those of views 5 and 6 , despite view 4 being non - frontal .",
    "given that the face tracker performs well in these sequences , the frontalisation approach serves to achieve a good performance , especially given that none of the videos corresponding to view 4 were used to train the models .",
    "however , other extreme views were harder to track , and inaccurate point localisations were given , thus affecting the system s performance .",
    "in addition , it can be seen that the frontalisation also yields good results for the intensity subchallenge .",
    "results per view ( rmse and icc ) are shown in table  [ t : baseline_intensity_per_view ] and table  [ t : baseline_intensity_per_view_test ] , in which also the chance level is shown , understood as the error that is given by a naive classifier returning always the class that has the highest frequency in the training set ( 0 in all of them ) .",
    "results show to outperform chance level , giving reasonable results .",
    "however , for au1 , au4 , and au17 , the amount of frames labelled with intensity zero is around the @xmath0 for the development set .",
    "this makes it hard for a graphical model to be accurate .",
    "this explains the low mse measured , as well as the low icc level .",
    "despite the good results given for some of the views , there is still a huge gap to improve , especially for challenging views , such as view 1 and 9 .",
    "in this paper we have presented the third facial expression recognition and analysis challenge ( fera 2017 ) dedicated to facs action units detection and intensity estimation on the highly challenging set of data .",
    "the dataset for this challenge has been derived from the bp4d , and extended to generate an extensive set of videos comprising 9 different views .",
    "this is the first time that a facs au annotated dataset is focused on expression analysis under different camera views , ranging extreme poses .",
    "the challenge addresses such significant problems of the field as expression intensity estimation as well as robust detection under non - frontal head poses , or partial self - occlusions .",
    "baseline results obtained using geometric features demonstrate a huge room for potential improvements to be brought by the challenge participants , especially corresponding to challenging views .            c.  corneanu , m.  oliu , j.  f. cohn , and s.  escalera .",
    "survey on rgb , thermal , and multimodal approaches for facial expression analysis : history , trends , and affect - related applications . , 38(8):15481568 , 2016 .",
    "l.  a. jeni , j.  m. girard , j.  cohn , and f.  de la torre .",
    "continuous au intensity estimation using localized , sparse facial feature space . in",
    "_ ieee intl conf . on automatic face and gesture recognition workshop _ , 2013 .",
    "m.  h. mahoor , s.  cadavid , d.  s. messinger , and j.  f. cohn .",
    "a framework for automated measurement of the intensity of non - posed facial action units . in _ ieee conference on computer vision and pattern recognition workshop _ , pages 7480 , 2009 .",
    "e.  snchez - lozano , b.  martinez , g.  tzimiropoulos , and m.  valstar .",
    "cascaded continuous regression for real - time incremental face tracking . in",
    "european conference on computer vision ",
    "eccv 2016 , part viii _ ,",
    "pages 645661 , 2016 .",
    "a.  savran , n.  alyz , h.  dibekliolu , o.  eliktutan , b.  gkberk , b.  sankur , and l.  akarun .",
    "bosphorus database for 3d face analysis . in _",
    "european workshop on biometrics and identity management _ ,",
    "pages 4756 .",
    "springer , 2008 .",
    "j.  shen , s.  zafeiriou , g.  chrysos , j.  .kossaifi , g.  tzimiropoulos , and m.  pantic .",
    "the first facial landmark tracking in - the - wild challenge : benchmark and results . in _ proc .",
    "computer vision workshop _ , 2015 .",
    "y.  tian , t.  kanade , and j.  f. cohn .",
    "evaluation of gabor - wavelet - based facial action unit recognition in image sequences of increasing complexity . in _",
    "ieee intl conf . on automatic face and gesture recognition _",
    ", pages 229234 , 2002 .",
    "m.  valstar .",
    "automatic behaviour understanding in medicine . in _ proceedings of the 2014 workshop on roadmapping the future of multimodal interaction research including business opportunities and challenges _ , pages 5760 .",
    "acm , 2014 .",
    "m.  f. valstar , t.  almaev , j.  m. girard , g.  mckeown , m.  mehu , l.  yin , m.  pantic , and j.  f. cohn .",
    "fera 2015-second facial expression recognition and analysis challenge . in _ automatic face and gesture recognition ( fg ) , 2015 11th ieee international conference and workshops on",
    "_ , volume  6 , pages 18 .",
    "ieee , 2015 .",
    "m.  f. valstar , b.  jiang , m.  mehu , m.  pantic , and k.  scherer . the first facial expression recognition and analysis challenge . in _",
    "ieee intl conf . on automatic face and gesture recognition workshop _",
    ", 2011 .",
    "m.  f. valstar and m.  pantic . induced disgust , happiness and surprise : an addition to the mmi facial expression database . in _ proc .",
    "intl conf .",
    "language resources and evaluation , wshop on emotion _ , pages 6570 , 2010 .",
    "z.  zhang , j.  m. girard , y.  wu , x.  zhang , p.  liu , u.  ciftci , s.  canavan , m.  reale , a.  horowitz , h.  yang , j.  cohn , q.  ji , and l.  yin .",
    "multimodal spontaneous emotion corpus for human behavior analysis . in _ proceedings of the ieee conference on computer vision and pattern recognition _ , pages 34383446 , 2016 ."
  ],
  "abstract_text": [
    "<S> the field of automatic facial expression analysis has grown rapidly in recent years . however , despite progress in new approaches as well as benchmarking efforts , most evaluations still focus on either posed expressions , near - frontal recordings , or both . </S>",
    "<S> this makes it hard to tell how existing expression recognition approaches perform under conditions where faces appear in a wide range of poses ( or camera views ) , displaying ecologically valid expressions . the main obstacle for assessing </S>",
    "<S> this is the availability of suitable data , and the challenge proposed here addresses this limitation . </S>",
    "<S> the fg 2017 facial expression recognition and analysis challenge ( fera 2017 ) extends fera 2015 to the estimation of action units occurrence and intensity under different camera views . in this paper </S>",
    "<S> we present the third challenge in automatic recognition of facial expressions , to be held in conjunction with the 12th ieee conference on face and gesture recognition , may 2017 , in washington , united states . </S>",
    "<S> two sub - challenges are defined : the detection of au occurrence , and the estimation of au intensity . in this work </S>",
    "<S> we outline the evaluation protocol , the data used , and the results of a baseline method for both sub - challenges . </S>"
  ]
}