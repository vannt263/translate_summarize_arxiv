{
  "article_text": [
    "recovery for jointly sparse signals concerns accurately estimating the non - zero component locations shared by a set of sparse signals based on a limited number of noisy linear observations . more specifically ,",
    "suppose that @xmath0 is a sequence of jointly sparse signals ( possibly under a sparsity - inducing basis @xmath1 instead of the canonical domain ) with a common support @xmath2 , which is the index set indicating the non - vanishing signal coordinates .",
    "this model is the same as the joint sparsity ( jsm-2 ) in @xcite .",
    "the observation model is linear : @xmath3 in , @xmath4 is the measurement matrix , @xmath5 the noisy data vector , and @xmath6 an additive noise . in most cases , the sparsity level @xmath7 and the number of observations @xmath8 is far less than @xmath9 , the dimension of the ambient space .",
    "this problem arises naturally in several signal processing areas such as compressive sensing candes2006uncertainty , donoho2006compressed , candes2005decoding , candes2008introcs , baraniuk2007compressivesensing , source localization willsky2005source , model2006signal , cevher2008distributed , cevher2009bayesian , sparse approximation and signal denoising @xcite .",
    "compressive sensing candes2006uncertainty , donoho2006compressed , candes2005decoding , a recently developed field exploiting the sparsity property of most natural signals , shows great promise to reduce signal sampling rate . in the classical setting of compressive sensing , only one snapshot is considered ; _",
    "i.e. _ , @xmath10 in . the goal is to recover a long vector @xmath11 with a small fraction of non - zero coordinates from the much shorter observation vector @xmath12 .",
    "since most natural signals are compressible under some basis and are well approximated by their @xmath13sparse representations mallat1999wavelet , this scheme , if properly justified , will reduce the necessary sampling rate beyond the limit set by nyquist and shannon baraniuk2007compressivesensing , candes2008introcs .",
    "surprisingly , for exact @xmath13sparse signals , if @xmath14 and the measurement matrix is generated randomly from , for example , a gaussian distribution , we can recover @xmath15 exactly in the noise - free setting by solving a linear programming task .",
    "besides , various methods have been designed for the noisy case@xcite . along with these algorithms ,",
    "rigorous theoretic analysis is provided to guarantee their effectiveness in terms of , for example , various @xmath16-norms of the estimation error for @xmath15 @xcite .",
    "however , these results offer no guarantee that we can recover the support of a sparse signal correctly .",
    "the accurate recovery of signal support is crucial to compressive sensing both in theory and in practice . since for signal recovery",
    "it is necessary to have @xmath17 , signal component values can be computed by solving a least squares problem once its support is obtained . therefore , support recovery is a stronger theoretic criterion than various @xmath16-norms . in practice ,",
    "the success of compressive sensing in a variety of applications relies on its ability for correct support recovery because the non - zero component indices usually have significant physical meanings .",
    "the support of temporally or spatially sparse signals reveals the timing or location for important events such as anomalies .",
    "the indices for non - zero coordinates in the fourier domain indicate the harmonics existing in a signalborgnat2008timefrequency , which is critical for tasks such as spectrum sensing for cognitive radios @xcite . in compressed dna microarrays for bio - sensing ,",
    "the existence of certain target agents in the tested solution is reflected by the locations of non - vanishing coordinates , while the magnitudes are determined by their concentrationsbaraniuk2007dna , vikalo2007dna , parvaresh2008dna , vikalo2008dna . for compressive radar imaging",
    ", the sparsity constraints are usually imposed on the discretized time  frequency domain .",
    "the distance and velocity of an object have a direct correspondence to its coordinate in the time - frequency domain .",
    "the magnitude determined by coefficients of reflection is of less physical significancebaraniuk2007radar , herman2008radar , herman2008highradar . in sparse linear regression @xcite",
    ", the recovered parameter support corresponds to the few factors that explain the data . in all these applications ,",
    "the support is physically more significant than the component values .",
    "our study of sparse support recovery is also motivated by the recent reformulation of the source localization problem as one of sparse spectrum estimation . in @xcite ,",
    "the authors transform the process of source localization using sensory arrays into the task of estimating the spectrum of a sparse signal by discretizing the parameter manifold .",
    "this method exhibits super - resolution in the estimation of direction of arrival ( doa ) compared with traditional techniques such as beamforming johnson1993array , capon@xcite , and music schmidt1986music , bienvenu1980music .",
    "since the basic model employed in willsky2005source applies to several other important problems in signal processing ( see @xcite and references therein ) , the principle is readily applicable to those cases .",
    "this idea is later generalized and extended to other source localization settings in model2006signal , cevher2008distributed , cevher2009bayesian .",
    "for source localization , the support of the sparse signal reveals the doa of sources .",
    "therefore , the recovery algorithm s ability of exact support recovery is key to the effectiveness of the method .",
    "we also note that usually multiple temporal snapshots are collected , which results in a jointly sparse signal sets as in .",
    "in addition , since @xmath8 is the number of sensors while @xmath18 is the number of temporal samples , it is far more expensive to increase @xmath8 than @xmath18 .",
    "the same comments apply to several other examples in the compressive sensing applications discussed in the previous paragraph , especially the compressed dna microarrays , spectrum sensing for cognitive radios , and compressive sensing radar imaging .",
    "the signal recovery problem with joint sparsity constraint duarte2005distributed , duarte2006dcs , fornasier2006,eldar2007continuous , also termed the multiple measurement vector ( mmv ) problemcotter2005inverse , chen2005mmv , chen2006mmv , tropp2006greedy , tropp2006convex , has been considered in a line of previous works .",
    "several algorithms , among them simultaneous orthogonal matching pursuit ( somp ) cotter2005inverse , tropp2006greedy , duarte2006dcs ; convex relaxation tropp2006convex ; @xmath19minimization @xcite ; and m - focuss @xcite , are proposed and analyzed , either numerically or theoretically .",
    "these algorithms are multiple - dimension extensions of their one - dimension counterparts .",
    "most performance measures of the algorithms are concerned with bounds on various norms of the difference between the true signals and their estimates or their closely related variants .",
    "the performance bounds usually involve the mutual coherence between the measurement matrix @xmath20 and the basis matrix @xmath1 under which the measured signals @xmath21 have a jointly sparse representation .",
    "however , with joint sparsity constraints , a natural measure of performance would be the model s potential for correctly identifying the true common support , and hence the algorithm s ability to achieve this potential .",
    "as part of their research , j. chen and x. huo derived , in a noiseless setting , sufficient conditions on the uniqueness of solutions to under @xmath22 and @xmath23 minimization . in cotter2005inverse , s. cotter _ et .",
    "al . _ numerically compared the probabilities of correctly identifying the common support by basic matching pursuit , orthogonal matching pursuit , focuss , and regularized focuss in the multiple - measurement setting with a range of snrs and different numbers of snapshots .",
    "the availability of multiple temporal samples offers serval advantages to the single - sample case . as suggested by the upper bound on the probability of error ,",
    "increasing the number of temporal samples drives the probability of error to zero exponentially fast as long as certain condition on the inconsistency property of the measurement matrix is satisfied .",
    "the probability of error is driven to zero by scaling the snr according to the signal dimension in @xcite , which is not very natural compared with increasing the samples , however .",
    "our results also show that under some conditions increasing temporal samples is usually equivalent to increasing the number of observations for a single snapshot .",
    "the later is generally much more expensive in practice .",
    "in addition , when there is considerable noise and the columns of the measurement matrix are normalized to one , it is necessary to have multiple temporal samples for accurate support recovery as discussed in section [ sec : lower ] and section [ sec : gaussian_nece ] .",
    "our work has several major differences compared to related work @xcite and @xcite , which also analyze the performance bounds on the probability of error for support recovery using information theoretic tools .",
    "the first difference is in the way the problem is modeled : in @xcite , the sparse signal is deterministic with known smallest absolute value of the non - zero components while we consider a random signal model .",
    "this leads to the second difference : we define the probability of error over the signal and noise distributions with the measurement matrix fixed ; in @xcite , the probability of error is taken over the noise , the gaussian measurement matrix and the signal support .",
    "most of the conclusions in this paper apply to general measurement matrices and we only restrict ourselves to the gaussian measurement matrix in section [ sec : gaussian ] .",
    "therefore , although we use a similar set of theoretical tools , the exact details of applying them are quiet different .",
    "in addition , we consider a multiple measurement model while only one temporal sample is available in @xcite . in particular , to get a vanishing probability of error , aeron _ et.al .",
    "_ @xcite require to scale the snr according to the signal dimension , which has a similar effect to having multiple temporal measurements in our paper .",
    "although the first two differences make it difficult to compare corresponding results in these two papers , we will make some heuristic comments in section [ sec : gaussian ] .",
    "the contribution of our work is threefold .",
    "first , we introduce a hypothesis - testing framework to study the performance for multiple support recovery .",
    "we employ well - known tools in statistics and information theory such as the chernoff bound and fano s inequality to derive both upper and lower bounds on the probability of error .",
    "the upper bound we derive is for the _ optimal _ decision rule , in contrast to performance analysis for specific sub - optimal reconstruction algorithmsgorodnitsky1997focuss , candes2007dantzig , tropp2007omp , dai2008subspace , needell2008cosamp .",
    "hence , the bound can be viewed as a measure of the measurement system s ability to correctly identify the true support .",
    "our bounds isolate important quantities that are crucial for system performance .",
    "since our analysis is based on measurement matrices with as few assumptions as possible , the results can be used as a guidance in system design .",
    "second , we apply these performance bounds to other more specific situations and derive necessary and sufficient conditions in terms of the system parameters to guarantee a vanishing probability of error .",
    "in particular , we study necessary conditions for accurate source localization by the mechanism proposed in willsky2005source . by restricting our attention to gaussian measurement matrices ,",
    "we derive a result parallel to those for classical compressive sensing @xcite , namely , the number of measurements that are sufficient for signal reconstruction .",
    "even if we adopt the probability of error as the performance criterion , we get the same bound on @xmath8 as in @xcite",
    ". however , our result suggests that generally it is impossible to obtain the true support accurately with only one snapshot when there is considerable noise .",
    "we also obtain a necessary condition showing that the @xmath24 term can not be dropped in compressive sensing .",
    "last but not least , in the course of studying the performance bounds we explore the eigenvalue structure of a fundamental matrix in support recovery hypothesis testing for both general measurement matrices and the gaussian measurement ensemble .",
    "these results are of independent interest .",
    "the paper is organized as follows . in section [ sec : modelandpre ] , we introduce the mathematical model and briefly review the fundamental ideas in hypothesis testing . section [ sec : upper ] is devoted to the derivation of upper bounds on the probability of error for general measurement matrices .",
    "we first derive an upper bound on the probability of error for the binary support recovery problem by employing the well - known chernoff bound in detection theory @xcite and extend it to multiple support recovery .",
    "we also study the effect of noise on system performance . in section [ sec : lower ] , an information theoretic lower bound is given by using the fano s inequality , and a necessary condition is shown for the doa problem considered in willsky2005source .",
    "we focus on the gaussian ensemble in section [ sec : gaussian ] .",
    "necessary and sufficient conditions on system parameters for accurate support recovery are given and their implications discussed .",
    "the paper is concluded in section [ sec : conclusion ] .",
    "we first introduce some notations used throughout this paper .",
    "suppose @xmath25 is a column vector .",
    "we denote by @xmath26 the support of @xmath15 , which is defined as the set of indices corresponding to the non - zero components of @xmath15 . for a matrix @xmath27",
    ", @xmath28 denotes the index set of non - zero rows of @xmath27 . here",
    "the underlying field @xmath29 can be assumed as @xmath30 or @xmath31 .",
    "we consider both real and complex cases simultaneously . for this purpose",
    ", we denote a constant @xmath32 or @xmath33 for the real or complex case , respectively .",
    "suppose @xmath2 is an index set .",
    "we denote by @xmath34 the number of elements in @xmath2 . for any column vector @xmath35",
    ", @xmath36 is the vector in @xmath37 formed by the components of @xmath15 indicated by the index set @xmath2 ; for any matrix @xmath38 , @xmath39 denotes the submatrix formed by picking the rows of @xmath38 corresponding to indices in @xmath2 , while @xmath40 is the submatrix with columns from @xmath38 indicated by @xmath2 . if @xmath41 and @xmath42 are two index sets , then @xmath43 , the submatrix of @xmath38 with rows indicated by @xmath41 and columns indicated by @xmath44 .",
    "transpose of a vector or matrix is denoted by @xmath45 while conjugate transpose by @xmath46 .",
    "@xmath47 represents the kronecker product of two matrices . for a vector @xmath48",
    ", @xmath49 is the diagonal matrix with the elements of @xmath48 in the diagonal .",
    "the identity matrix of dimension @xmath8 is @xmath50 .",
    "the trace of matrix @xmath20 is given by @xmath51 , the determinant by @xmath52 to denote the probability of an event and @xmath53 the expectation .",
    "the underlying probability space can be inferred from the context .",
    "gaussian distribution for a random vector in field @xmath29 with mean @xmath54 and covariance matrix @xmath55 is represented by @xmath56 .",
    "matrix variate gaussian distribution @xcite for @xmath57 with mean @xmath58 and covariance matrix @xmath59 , where @xmath60 and @xmath61 , is denoted by @xmath62    suppose @xmath63 are two positive sequences , @xmath64 means that @xmath65 .",
    "an alternative notation in this case is @xmath66 .",
    "we use @xmath67 to denote that there exists an @xmath68 and @xmath69 independent of @xmath9 such that @xmath70 for @xmath71 .",
    "similarly , @xmath72 means @xmath73 for @xmath71 .",
    "these simple but expedient notations introduced by g. h. hardy greatly simplify derivations @xcite .",
    "next , we introduce our mathematical model .",
    "suppose @xmath74 are jointly sparse signals with common support ; that is , only a few components of @xmath75 are non - zero and the indices corresponding to these non - zero components are the same for all @xmath76 .",
    "the common support @xmath77 has known size @xmath78 .",
    "we assume that the vectors @xmath79 formed by the non - zero components of @xmath80 follow _ i.i.d .",
    "_ @xmath81 .",
    "the measurement model is as follows:@xmath82where @xmath20 is the measurement matrix and @xmath83 the measurements .",
    "the additive noise @xmath84 is assumed to follow _",
    "_ @xmath85 .",
    "note that assuming unit variance for signals loses no generality since only the ratio of signal variance to noise variance appears in all subsequence analyses . in this sense",
    ", we view @xmath86 as the signal - to - noise ratio ( snr ) .",
    "let @xmath87 and @xmath88 , @xmath89 be defined in a similar manner .",
    "then we write the model in the more compact matrix form : @xmath90we start our analysis for general measurement matrix @xmath20 .",
    "for an arbitrary measurement matrix @xmath4 , if every @xmath91 submatrix of @xmath20 is non - singular , we then call @xmath20 a measurement matrix . in this case , the corresponding linear system @xmath92 is said to have the _ unique representation property ( urp ) _ , the implication of which is discussed in @xcite .",
    "while most of our results apply to general non - degenerate measurement matrices , we need to impose more structure on the measurement matrices in order to obtain more profound results . in particular , we will consider gaussian measurement matrix @xmath93 whose elements @xmath94 are generated from _",
    "_ @xmath95 .",
    "however , since our performance analysis is carried out by conditioning on a particular realization of @xmath93 , we still use non - bold @xmath20 except in section [ sec : gaussian ] . the role played by the variance of @xmath94",
    "is indistinguishable from that of a signal variance and hence can be combined to @xmath86 , the snr , by the note in the previous paragraph .",
    "we now consider two hypothesis - testing problems .",
    "the first one is a binary support recovery problem : @xmath96the results we obtain for binary binary support recovery offer insight into our second problem : the multiple support recovery .",
    "in the multiple support recovery problem we choose one among @xmath97 distinct candidate supports of @xmath98 , which is a multiple - hypothesis testing problem : @xmath99      we now briefly introduce the fundamentals of hypothesis testing . the following discussion is based mainly on @xcite . in a simple binary hypothesis test , the goal is to determine which of two candidate distributions is the true one that generates the data matrix ( or vector ) @xmath100 : @xmath101    there are two types of errors when one makes a choice based on the observed data @xmath100 .",
    "a _ false alarm _ corresponds to choosing @xmath102 when @xmath103 is true , while a _ miss _ happens by choosing @xmath104 when @xmath102 is true .",
    "the probabilities of these two types of errors are called the probability of a false alarm and the probability of a miss , which are denoted by @xmath105respectively . depending on",
    "whether one knows the prior probabilities @xmath106 and @xmath107 and assigns losses to errors , different criteria can be employed to derive the optimal decision rule . in this paper",
    "we adopt the probability of error with equal prior probabilities of @xmath103 and @xmath102 as the decision criterion ; that is , we try to find the optimal decision rule by minimizing @xmath108the optimal decision rule is then given by the _ likelihood ratio test _ : @xmath109 where @xmath110 is the natural logarithm function .    the probability of error associated with the optimal decision rule , namely , the likelihood ratio test , is a measure of the best performance a system can achieve . in many cases of interest ,",
    "the simple binary hypothesis testing problem is derived from a signal - generation system .",
    "for example , in a digital communication system , hypotheses @xmath111 and @xmath112 correspond to the transmitter sending digit @xmath113 and @xmath33 , respectively , and the distributions of the observed data under the hypotheses are determined by the modulation method of the system .",
    "therefore , the minimal probability of error achieved by the likelihood ratio test is a measure of the performance of the modulation method . for the problem addressed in this paper , the minimal probability of error reflects the measurement matrix s ability to distinguish different signal supports .",
    "the chernoff bound@xcite is a well - known tight upper bound on the probability of error . in many cases , the optimum",
    "test can be derived and implemented efficiently but an exact performance calculation is impossible . even if such an expression can be derived , it is too complicated to be of practical use .",
    "for this reason , sometimes a simple bound turns out to be more useful in many problems of practical importance .",
    "the chernoff bound , based on the moment generating function of the test statistic @xmath114 , provides an easy way to compute such a bound .",
    "define @xmath115 as the logarithm of the moment generating function of @xmath116 : @xmath117^{s}[p(\\bs y|\\hn)]^{1-s}d\\bs y.\\end{aligned}\\]]then the chernoff bound states that @xmath118\\leq \\exp [ \\mu ( s ) ] ,   \\label{chernoff_f } \\\\ p_{\\mathrm{m } } & \\leq & \\exp [ \\mu ( s_{m})]\\leq \\exp [ \\mu ( s ) ] ,   \\label{chernoff_m}\\end{aligned}\\]]and @xmath119\\leq \\frac{1}{2}\\exp [ \\mu ( s)],\\ ] ] where @xmath120 and @xmath121 .",
    "note that a refined argument gives the constant @xmath122 in instead of @xmath33 as obtained by direct application of and @xcite .",
    "we use these bounds to study the performance of the support recovery problem .",
    "we next extend to multiple - hypothesis testing the key elements of the binary hypothesis testing .",
    "the goal in a simple multiple - hypothesis testing problem is to make a choice among @xmath123 distributions based on the observations : @xmath124using the total probability of error as a decision criterion and assuming equal prior probabilities for all hypotheses , we obtain the optimal decision rule given by @xmath125application of the union bound and the chernoff bound shows that the total probability of error is bounded as follows : @xmath126 , 0\\leq s\\leq 1 , \\label{bound_m}\\end{aligned}\\]]where @xmath127 $ ] is the moment - generating function in the binary hypothesis testing problem for @xmath128 and @xmath129 .",
    "hence , we obtain an upper bound for multiple - hypothesis testing from that for binary hypothesis testing .",
    "in this section , we apply the general theory for hypothesis testing , the chernoff bound on the probability of error in particular , to the support recovery problems and .",
    "we first study binary support recovery , which lays the foundation for the general support recovery problem .      under model and the assumptions pertaining to it , observations @xmath100 follow a matrix variate gaussian distribution gupta1999matrix when the true support is @xmath2 : @xmath130with the probability density function ( pdf ) given by @xmath131 , \\]]where @xmath132 is the common covariance matrix for each column of @xmath100 .",
    "the binary support recovery problem is equivalent to a linear gaussian binary hypothesis testing problem : @xmath133from now on , for notation simplicity we will denote @xmath134 by @xmath135 .",
    "the optimal decision rule with minimal probability of error given by the likelihood ratio test @xmath114 reduces to @xmath136 -\\kappa t\\log \\frac{\\left\\vert \\sigma _ {",
    "1}\\right\\vert } { \\left\\vert \\sigma _ { 0}\\right\\vert } \\overset{\\mathrm{h}_{1}}% { \\underset{\\mathrm{h}_{0}}{\\gtreqless } } 0.\\ ] ]    to analyze the performance of the likelihood ratio test , we first compute the log - moment - generating function of @xmath114 according to : @xmath137 ^{s}% \\left [ p\\left ( \\boldsymbol{y}|\\mathrm{h}_{0}\\right ) \\right ] ^{1-s}d% \\boldsymbol{y }   \\notag \\\\ & = & \\log \\bigg[\\frac{1}{(\\pi /\\kappa ) ^{\\kappa mt}|\\sigma _ { 1}|^{\\kappa st}|\\sigma _ { 0}|^{\\kappa \\left ( 1-s\\right ) t } }   \\notag \\\\ & & \\times \\int \\exp \\left\\ { -\\kappa \\tr\\left [ \\boldsymbol{y}^{\\dagger } \\left ( s\\sigma _ { 1}^{-1}+\\left ( 1-s\\right ) \\sigma _ { 0}^{-1}\\right ) \\boldsymbol{y}% \\right ] \\right\\ } d\\boldsymbol{y}\\bigg ]   \\notag \\\\ & = & \\log \\bigg[\\frac{\\left\\vert s\\sigma _ { 1}^{-1}+\\left ( 1-s\\right ) \\sigma _ { 0}^{-1}\\right\\vert ^{-\\kappa t}}{|\\sigma _ { 1}|^{\\kappa st}|\\sigma _ { 0}|^{\\kappa \\left ( 1-s\\right ) t}}\\bigg ]   \\notag \\\\ & = & { -\\kappa t}\\log \\left\\vert sh^{1-s}+\\left ( 1-s\\right ) h^{-s}\\right\\vert , \\ \\ \\ \\ 0\\leq s\\leq 1,\\end{aligned}\\]]where @xmath138 .",
    "the computation of the exact minimizer @xmath139 is non - trivial and will lead to an expression of @xmath140 too complicated to handle . when latexmath:[$%    correlated , for example in the case of @xmath20 with _ i.i.d .",
    "_ elements , @xmath142 .",
    "we then take @xmath143 in the chernoff bounds , , and . whereas the bounds obtained in this way may not be the absolute best ones , they are still valid .    as positive definite hermitian matrices , @xmath144 and @xmath145 can be simultaneously diagonalized by a unitary transformation .",
    "suppose that the eigenvalues of @xmath144 are @xmath146 and @xmath147 $ ] .",
    "then it is easy to show that @xmath148.\\end{aligned}\\ ] ]    therefore , it is necessary to count the numbers of eigenvalues of @xmath144 that are greater than 1 , equal to 1 and less than 1 , _",
    "i.e. _ , the values of @xmath149 and @xmath150 for general non - degenerate measurement matrix @xmath20 .",
    "we have the following theorem on the eigenvalue structure of @xmath144 :    [ eig_count ] for any non - degenerate measurement matrix @xmath151 let @xmath152 , @xmath153 and assume @xmath154 ; then @xmath155 eigenvalues of matrix @xmath144 are greater than @xmath33 , @xmath150 less than @xmath33 , and @xmath156 equal to @xmath33 .",
    "* proof : * see appendix a.    for binary support recovery with @xmath157 .",
    "the subscripts @xmath158 and @xmath159 in @xmath160 and @xmath161 are short for  intersection \" and  difference \" , respectively . employing the chernoff bounds and proposition [ eig_count ] , we have    [ thm_binarybound ] if @xmath162 , the probability of error for the binary support recovery problem is bounded by @xmath163 ^{-\\kappa k_{\\mathrm{d}}t/2 } , \\label{bd_binary}\\end{aligned}\\]]where @xmath164 is the geometric mean of the eigenvalues of @xmath165 that are greater than one .",
    "* proof : * according to and , we have @xmath166 \\\\ & \\leq & \\frac{1}{2}\\left [ \\prod_{j=1}^{k_{\\mathrm{d}}}\\left ( \\frac{\\sqrt{\\lambda _ { j}}% } { 2}\\right ) \\prod_{j=1}^{k_{\\mathrm{d}}}\\left ( \\frac{1/\\sqrt{\\sigma _ { j}}}{2}\\right ) % \\right ] ^{-\\kappa t } \\\\ & = & \\frac{1}{2}\\left [ \\frac{\\left ( \\prod_{j=1}^{k_{\\mathrm{d}}}\\lambda _ { j}\\right ) ^{1/k_{\\mathrm{d}}}\\left ( \\prod_{j=1}^{k_{\\mathrm{d}}}\\frac{1}{\\sigma _ { j}}\\right ) ^{1/k_{\\mathrm{d}}}}{% 16}\\right ] ^{-\\kappa k_{\\mathrm{d}}t/2}.\\end{aligned}\\]]define @xmath164 as the geometric mean of the eigenvalues of @xmath167 that are greater than one .",
    "then obviously we have @xmath168 .",
    "since @xmath145 and @xmath169 have the same set of eigenvalues , @xmath170 are the eigenvalues of @xmath169 that are greater than 1 .",
    "we conclude that @xmath171 .",
    "@xmath172    note that @xmath173 and @xmath174 completely determine the measurement system s performance in differentiating two different signal supports .",
    "it must be larger than the constant 16 for a vanishing bound when more temporal samples are taken .",
    "once the threshold 16 is exceeded , taking more samples will drive the probability of error to 0 exponentially fast . from numerical simulations and our results on the gaussian measurement matrix",
    ", @xmath175 does not vary much when @xmath176 and @xmath161 change , as long as the elements in the measurement matrix @xmath20 are highly uncorrelated .",
    "are samples from uniform linear sensor array manifold . ] therefore , quite appealing to intuition , the larger the size @xmath161 of the difference set between the two candidate supports , the smaller the probability of error .",
    "now we are ready to use the union bound to study the probability of error for the multiple support recovery problem .",
    "we assume each candidate support @xmath177 has known cardinality @xmath178 , and we have @xmath179 such supports . our general approach is also applicable to cases for which we have some prior information on the structure of the signal s sparsity pattern , for example the setup in model - based compressive sensing@xcite . in these cases , we usually have @xmath180 supports , and a careful examination on the intersection pattern of these supports will give a better bound .",
    "however , in this paper we will not address this problem and will instead focus on the full support recovery problem with @xmath179 . defining @xmath181",
    ", we have the following theorem :    [ thm_mul_bd ] if @xmath182 and @xmath183 ^{\\frac{1}{\\kappa t}}$ ] , then the probability of error for the full support recovery problem with @xmath184 and @xmath179 is bounded by @xmath185    * proof : * combining the bound in proposition [ thm_binarybound ] and equation , we have @xmath186 ^{-\\kappa k_{\\mathrm{d}}t/2 } \\\\ & \\leq & \\frac{1}{2l}\\sum_{i=0}^{l-1}\\sum_{\\substack { j=1   \\\\",
    "j\\neq i}}% ^{l-1}\\left ( \\frac{\\bar{\\lambda}}{4}\\right ) ^{-\\kappa k_{\\mathrm{d}}t}.\\end{aligned}\\]]here @xmath161 depends on the supports @xmath177 and @xmath187 . for fixed @xmath177 , the number of supports that have a difference set with @xmath177 with cardinality @xmath161 is @xmath188",
    "therefore , using @xmath189 and @xmath190 and the summation formula for geometric series , we obtain @xmath191 ^{k_{\\mathrm{d } } } \\\\ & \\leq & \\frac{1}{2}\\frac{\\frac{k\\left ( n - k\\right ) } { \\left ( \\bar{\\lambda}% /4\\right ) ^{\\kappa t}}}{1-\\frac{k\\left ( n - k\\right ) } { \\left ( \\bar{\\lambda}% /4\\right ) ^{\\kappa t}}}.\\ \\ \\ \\ \\blacksquare\\end{aligned}\\ ] ]    we make several comments here .",
    "first , @xmath192 depends solely on the measurement matrix @xmath20 . compared with the results in wainwright2007bound ,",
    "where the bounds involve the signal , we get more insight into what quantity of the measurement matrix is important in support recovery .",
    "this information is obtained by modelling the signals @xmath80 as gaussian random vectors .",
    "the quantity @xmath192 effectively characterizes system s ability to distinguish different supports .",
    "clearly , @xmath192 is related to the restricted isometry property ( rip ) , which guarantees stable sparse signal recovery in compressive sensing @xcite .",
    "we discuss the relationship between rip and @xmath192 for the special case with @xmath193 at the end of section [ sec : noiseeffectupper ] .",
    "however , a precise relationship for the general case is yet to be discovered .",
    "second , we observe that increasing the number of temporal samples plays two roles simultaneously in the measurement system .",
    "for one thing , it decreases the the threshold @xmath194^{\\frac{1}{\\kappa t}}$ ] that @xmath192 must exceed for the bound to hold .",
    "however , since @xmath195^{\\frac{1}{\\kappa t}}=4 $ ] for fixed @xmath178 and @xmath9 , increasing temporal samples can reduce the threshold only to a certain limit . for another",
    ", since the bound is proportional to @xmath196 , the probability of error turns to 0 exponentially fast as @xmath18 increases , as long as @xmath183 ^{\\frac{1}{\\kappa t}}$ ] is satisfied .",
    "in addition , the final bound is of the same order as the probability of error when @xmath197 .",
    "the probability of error @xmath198 is dominated by the probability of error in cases for which the estimated support differs by only one index from the true support , which are the most difficult cases for the decision rule to make a choice . however , in practice we can imagine that these cases induce the least loss .",
    "therefore , if we assign weights / costs to the errors based on @xmath161 , then the weighted probability of error or average cost would be much lower .",
    "for example , we can choose the costs to exponentially decrease when @xmath161 increases .",
    "another possible choice of cost function is to assume zero cost when @xmath161 is below a certain critical number .",
    "our results can be easily extended to these scenarios .",
    "finally , note that our bound applies to any non - degenerate matrix . in section [ sec : gaussian ]",
    ", we apply the bound to gaussian measurement matrices .",
    "the additional structure allows us to derive more profound results on the behavior of the bound .      in this subsection",
    ", we explore how the noise variance affects the probability of error , which is equivalent to analyzing the behavior of @xmath199 and @xmath192 as indicated in and .",
    "we now derive bounds on the eigenvalues of @xmath144 .",
    "the lower bound is expressed in terms of the qr decomposition of a submatrix of the measurement matrix with the noise variance @xmath200 isolated .",
    "[ eig_bound ] for any non - degenerate measurement matrix @xmath20 , let @xmath152 with @xmath201 , @xmath153 .",
    "we have the following :    1 .   if @xmath154 , then the sorted eigenvalues of @xmath144 that are greater than 1 are lower bounded by the corresponding eigenvalues of  @xmath202 , where @xmath203 is the @xmath204 submatrix at the lower - right corner of the upper triangle matrix in the qr decomposition of @xmath205 2 .",
    "the eigenvalues of @xmath144 are upper bounded by the corresponding eigenvalues of @xmath206 ; in particular , the sorted eigenvalues of @xmath144 that are greater than @xmath33 are upper bounded by the corresponding ones of @xmath207 .",
    "* proof : * see appendix b.    the importance of this proposition is twofold .",
    "first , by isolating the noise variance from the expression of matrix @xmath144 , this theorem clearly shows that when noise variance decreases to zero , the relatively large eigenvalues of @xmath144 will blow up , which results in increased performance in support recovery .",
    "second , the bounds provide ways to analyze special measurement matrices , especially the gaussian measurement ensemble discussed in section [ sec : gaussian ] .",
    "we have the following corollary :    [ noise_effect ] for support recovery problems and with support size @xmath178 , suppose @xmath208 ; then there exist constants @xmath209 that depend only on the measurement matrix @xmath210 such that @xmath211from and , we then conclude that for any temporal sample size @xmath18 @xmath212and the speed of convergence is approximately @xmath213 and @xmath214 for the binary and multiple cases , respectively .",
    "* proof : * according to proposition [ eig_bound ] , for any fixed @xmath215 , the eigenvalues of @xmath216 that are greater than @xmath33 are lower bounded by those of @xmath217 ; hence we have @xmath218 ^{1/k_{\\mathrm{d } } }   \\notag \\\\ & \\geq & 1+\\frac{1}{\\sigma ^{2}}\\left ( \\prod_{l=1}^{k_{\\mathrm{d}}}r_{ll}^{2}\\right ) ^{1/k_{\\mathrm{d } } } ,   \\label{bd_l_barlambda}\\end{aligned}\\]]where @xmath219 is the @xmath220th diagonal element of @xmath203 . for the second inequality",
    "we have used fact 8 .",
    "11 . 20 in @xcite . since @xmath221 is non - degenerate and @xmath182",
    ", @xmath222 is of full rank and @xmath223 for all @xmath224 . defining @xmath225 as the minimal value of @xmath226 s over all possible support pairs @xmath227 , we then have @xmath228 and @xmath229on the other hand , the upper bound on the eigenvalues of @xmath144 yields @xmath230therefore , we have @xmath231with latexmath:[$c_{2}=\\max_{s:|s|\\leq k}\\frac{1}{|s|}\\sum_{\\substack { 1\\leq m\\leq m   \\\\ n\\in s}}%    from and .",
    "@xmath172    corollary [ noise_effect ] suggests that in the limiting case where there is no noise , @xmath208 is sufficient to recover a @xmath13sparse signal .",
    "this fact has been observed in @xcite .",
    "our result also shows that the optimal decision rule , which is unfortunately inefficient , is robust to noise .",
    "another extreme case is when the noise variance @xmath200 is very large .",
    "then from @xmath233 , the bounds in and are approximated by @xmath234 and @xmath235 .",
    "therefore , the convergence exponents for the bounds are proportional to the snr in this limiting case .",
    "the diagonal elements of @xmath203 , @xmath219 s , have clear meanings .",
    "since qr factorization is equivalent to the gram - schmidt orthogonalization procedure , @xmath236 is the distance of the first column of @xmath237 to the subspace spanned by the columns of @xmath238 ; @xmath239 is the distance of the second column of @xmath237 to the subspace spanned by the columns of @xmath238 plus the first column of @xmath237 , and so on .",
    "therefore , @xmath164 is a measure of how well the columns of @xmath237 can be expressed by the columns of @xmath238 , or , put another way , a measure of the incoherence between the columns of @xmath240 and @xmath238 .",
    "similarly , @xmath192 is an indicator of the incoherence of the entire matrix @xmath20 of order @xmath178 .    to relate @xmath192 with the incoherence",
    ", we consider the case with @xmath241 and @xmath242 . by restricting our attention to matrices with _ unit _ columns",
    ", the above discussion implies that a better bound is achieved if the minimal distance of all pairs of column vectors of matrix @xmath20 is maximized .",
    "finding such a matrix @xmath20 is equivalent to finding a matrix with the inner product between columns as large as possible , since the distance between two unit vectors @xmath243 and @xmath48 is @xmath244 where @xmath245 is the inner product between @xmath243 and @xmath48 .",
    "for each integer @xmath246 , the rip constant @xmath247 is defined as the smallest number such that @xcite : @xmath248a direct computation shows that @xmath249 is equal to the minimum of the absolute values of the inner products between all pairs of columns of @xmath20 .",
    "hence , the requirements of finding the smallest @xmath249 that satisfies and maximizing @xmath192 coincide when @xmath193 . for general @xmath178 , milenkovic _ et.al .",
    "_ established a relationship between @xmath250 and @xmath251 via gergorin s disc theorem @xcite and discussed them as well as some coding theoretic issues in compressive sensing context @xcite .",
    "in this section , we derive an information theoretic lower bound on the probability of error for _ any _ decision rule in the multiple support recovery problem .",
    "the main tool is a variant of the well - known fano s inequality@xcite . in the variant ,",
    "the average probability of error in a multiple - hypothesis testing problem is bounded in terms of the kullback - leibler divergence@xcite .",
    "suppose that we have a random vector or matrix @xmath100 with @xmath123 possible densities @xmath252 .",
    "denote the average of the kullback - leibler divergence between any pair of densities by @xmath253then by fano s inequality @xcite,@xcite , the probability of error for _ any _ decision rule to identify the true density is lower bounded by @xmath254    since in the multiple support recovery problem , all the distributions involved are matrix variate gaussian distributions with zero mean and different variances , we now compute the kullback - leibler divergence between two matrix variate gaussian distributions .",
    "suppose @xmath255 , the kullback - leibler divergence has closed form expression : @xmath256 -\\kappa t\\log \\frac{\\left\\vert \\sigma _ { i}\\right\\vert } { \\left\\vert \\sigma _ { j}\\right\\vert } \\right ] \\\\ & = & \\frac{1}{2}\\kappa t\\left[\\func{tr}\\left ( h_{i , j}-\\eye_{m}\\right ) + \\log \\frac{\\left\\vert \\sigma _ { j}\\right\\vert } { \\left\\vert \\sigma _ { i}\\right\\vert } \\right],\\end{aligned}\\]]where @xmath257 .",
    "therefore , we obtain the average kullback - leibler divergence for the multiple support recovery problem as @xmath258 \\\\ & = & \\frac{\\kappa t}{2l^{2}}\\sum_{s_i , s_j}\\left[\\mathrm{\\func{tr}}(h_{i , j})-m% \\right],\\end{aligned}\\]]where the @xmath259 terms all cancel out and @xmath260 . invoking the second part of proposition [ eig_bound ] , we get @xmath261therefore , the average kullback - leibler divergence is bounded by @xmath262    due to the symmetry of the right - hand side , it must be of the form @xmath263 , where @xmath264 is the frobenius norm .",
    "setting all @xmath265 gives @xmath266 therefore , we get @xmath267 using the mean expression for hypergeometric distribution : @xmath268hence , we have @xmath269therefore , the probability of error is lower bounded by @xmath270we conclude with the following theorem :    [ thm_bd_lower ] for multiple support recovery problem , the probability of error for any decision rule is lower bounded by@xmath271    each term in bound @xmath272 has clear meanings .",
    "the frobenius norm of measurement matrix @xmath273 is total gain of system .",
    "since the measured signal is @xmath13sparse , only a fraction of the gain plays a role in the measurement , and its average over all possible @xmath13sparse signals is @xmath274 .",
    "while an increase in signal energy enlarges the distances between signals , a penalty term @xmath275 is introduced because we now have more signals .",
    "the term @xmath276 is the total uncertainty or entropy of the support variable @xmath277 since we impose a uniform prior on it .",
    "as long as @xmath278 , increasing @xmath178 increases both the average gain exploited by the measurement system , and the entropy of the support variable @xmath279 .",
    "the overall effect , quite counterintuitively , is a decrease of the lower bound in .",
    "actually , the term involving @xmath178 , @xmath280 , is approximated by an increasing function @xmath281 with @xmath282 and the binary entropy function @xmath283 .",
    "the reason for the decrease of the bound is that the bound only involves the _ effective _ snr without regard to any inner structure of @xmath20 ( e.g. the incoherence ) and the effective snr increases with @xmath178 . to see this",
    ", we compute the effective snr as @xmath284=\\frac{\\frac{k}{n}\\|a\\|_{\\mathrm{f}}^2}{m\\sigma^2}$ ] .",
    "if we scale down the effective snr through increasing the noise energy @xmath285 by a factor of @xmath178 , then the bound is strictly increasing with @xmath178 .",
    "the above analysis suggests that the lower bound is weak as it disregards any incoherence property of the measurement matrix @xmath20 .",
    "for some cases , the bound reduces to @xmath286 ( refer to corollary [ snet ] , theorem [ doa ] and [ gaussian_nece ] ) and is less than @xmath178 when the noise level or @xmath178 is relatively large . certainly recovering the support is not possible with fewer than @xmath178 measurements .",
    "the bound is loose also in the sense that when @xmath18 , @xmath287 , or the snr @xmath288 is large enough the bound becomes negative , but when there is noise , perfect support recovery isgenerally impossible . while the original fano s inequality @xmath289is tight in some cases@xcite , the adoption of the average divergence as an upper bound on the mutual information @xmath290 between the random support @xmath291 and the observation @xmath88 reduces the tightness ( see the proof of in birg1983approximation ) . due to the difficulty of computing @xmath292 and @xmath293 analytically ,",
    "it is not clear whether a direction application of results in a significantly better bound .    despite of its drawbacks we discussed ,",
    "the bound identifies the importance of the gain @xmath294 of the measurement matrix , a quantity usually ignored in , for example , compressive sensing .",
    "we can also draw some interesting conclusions from for measurement matrices with special properties .",
    "in particular , in the following corollary , we consider measurement matrices with rows or columns normalized to one .",
    "the rows of a measurement matrix are normalized to one in sensor network scenario ( snet ) where each sensor is power limited while the columns are sometimes normalized to one in compressive sensing ( refer to @xcite and references therein ) .",
    "[ snet ] in order to have a probability of error @xmath295 with @xmath296 , the number of measurements must satisfy : @xmath297 if the rows of @xmath20 have unit norm ; and @xmath298 if the columns of @xmath20 have unit norm .",
    "note that the necessary condition has the same critical quantity as the sufficient condition in compressive sensing .",
    "the inequality in is independent of @xmath8 .",
    "therefore , if the columns are normalized to have unit norm , it is necessary to have multiple temporal measurements for a vanishing probability of error .",
    "refer to theorem [ doa ] and [ gaussian_nece ] and discussions following them .    in the work of @xcite , each column of @xmath20 is the array manifold vector function evaluated at a sample of the doa parameter .",
    "the implication of the bound for optimal design is that we should construct an array whose geometry leads to maximal @xmath273 .",
    "however , under the narrowband signal assumption and narrowband array assumption @xcite , the array manifold vector for isotropic sensor arrays always has norm @xmath299vantrees2002optimum , which means that @xmath300 .",
    "hence in this case , the probability of error is always bounded by @xmath301therefore , we have the following theorem ,    [ doa ] under the narrowband signal assumption and narrowband array assumption , for an isotropic sensor array in the doa estimation scheme proposed in @xcite , in order to let the probability of error @xmath302 with @xmath303 for any decision rule , the number of measurements must satisfy the following : @xmath304    we comment that the same lower bound applies to fourier measurement matrix ( not normalized by @xmath305 ) due to the same line of argument",
    ". we will not explicitly present this result in the current paper .    since in radar and sonar",
    "applications the number of targets @xmath178 is usually small , our result shows that the number of samples is lower bounded by @xmath306 . note that @xmath9 is the number of intervals we use to divide the whole range of doa ; hence , it is a measure of resolution .",
    "therefore , the number of samples only needs to increase in the logarithm of @xmath9 , which is very desirable .",
    "the symmetric roles played by @xmath8 and @xmath18 are also desirable since @xmath8 is the number of sensors and is expensive to increase . as a consequence , we simply increase the number of samples to achieve a desired probability of error .",
    "in addition , unlike the upper bound of theorem [ thm_mul_bd ] , we do not need to assume that @xmath182 in theorem [ thm_bd_lower ] and [ doa ] .",
    "actually , malioutov _ et.al .",
    "_ made the empirical observation that @xmath307svd technique can resolve @xmath308 sources if they are well separated @xcite .",
    "theorem [ doa ] still applies to this extreme case .",
    "analysis of support recovery problem with measurement matrix obtained from sampling a manifold has considerable complexity compared with the gaussian case .",
    "for example , it presents significant challenge to estimate @xmath309 in the doa problem except for a few special cases that we discuss in @xcite . as we mentioned before , unlike the gaussian case",
    ", @xmath309 for uniform linear arrays varies greatly with @xmath310 and @xmath311 .",
    "therefore , even if we can compute @xmath309 , replacing it with @xmath192 in the upper bound of theorem [ thm_mul_bd ] would lead to a very loose bound . on the other hand ,",
    "the lower bound of theorem [ thm_bd_lower ] only involves the frobenius norm of the measurement matrix , so we apply it to the doa problem effortlessly .",
    "however , the lower bound is weak as it does not exploit any inner structure of the measurement matrix .",
    "donoho _ et.al .",
    "_ considered the recovery of a  sparse \" wide - band signal from narrow - band measurements @xcite , a problem with essentially the same mathematical structure when we sample the array manifold uniformly in the wave number domain instead of the doa domain .",
    "it was found that the spectral norm of the product of the band - limiting and time - limiting operators is crucial to stable signal recovery measured by the @xmath312 norm . in @xcite ,",
    "donoho and stark bounded the spectral norm using the frobenius norm , which leads to the well - known uncertainty principle .",
    "the authors commented that the uncertainty principle condition demands an extreme degree of sparsity for the signal .",
    "however , this condition can be relaxed if the signal support are widely scattered . in @xcite ,",
    "malioutov _ et.al .",
    "_ also observed from numerical simulations that the @xmath307svd algorithm performs much better when the sources are well separated than when they are located close together . in particular",
    ", they observed that presence of bias is mitigated greatly when sources are far apart .",
    "donoho and logan @xcite explored the effect of the scattering of the signal support by using the ",
    "analytic principle of the large sieve \" .",
    "they bounded the spectral norm for the limiting operator by the maximum nyquist density , a quantity that measures the degree of scattering of the signal support .",
    "we expect that our results can be improved in a similar manner .",
    "the challenges include using support recovery as a performance measure , incorporating multiple measurements , as well as developing the whole theory within a probabilistic framework .",
    "in this section , we refine our results in previous sections from general non - degenerate measurement matrices to the gaussian ensemble . unless otherwise specified , we always assume that the elements in a measurement matrix @xmath313 are _ i.i.d . _ samples from unit variance real or complex gaussian distributions . the gaussian measurement ensemble is widely used and studied in compressive sensing candes2006uncertainty , donoho2006compressed , candes2005decoding , candes2008introcs , baraniuk2007compressivesensing .",
    "the additional structure and the theoretical tools available enable us to derive deeper results in this case . in this section , we assume general scaling of @xmath314 .",
    "we do not find in our results a clear distinction between the regime of sublinear sparsity and the regime of linear sparsity as the one discussed in @xcite .",
    "we first show two corollaries on the eigenvalue structure for the gaussian measurement ensemble .",
    "then we derive sufficient and necessary conditions in terms of @xmath315 and @xmath18 for the system to have a vanishing probability of error .",
    "first , we observe that a gaussian measurement matrix is non - degenerate with probability one , since any @xmath316 random vectors @xmath317 from @xmath318 with @xmath319 positive definite are linearly independent with probability one ( refer to theorem 3.2.1 in gupta1999matrix ) . as a consequence ,",
    "we have    [ eig_count_gaussian ] for gaussian measurement matrix @xmath313 , let @xmath320 , @xmath321 .",
    "if @xmath154 , then with probability one , @xmath155 eigenvalues of matrix @xmath322 are greater than @xmath33 , @xmath150 less than @xmath33 , and @xmath156 equal to @xmath33 .",
    "we refine proposition [ eig_bound ] based on the well - known qr factorization for gaussian matrices @xcite,@xcite .    with the same notations as in corollary [ eig_count_gaussian ] , then with probability one , we have :    1 .   if @xmath154 , then the sorted eigenvalues of @xmath323 that are greater than 1 are lower bounded by the corresponding ones of  @xmath324 , where the elements of @xmath325 satisfy : @xmath326 2 .",
    "the eigenvalues of @xmath323 are upper bounded by the corresponding eigenvalues of @xmath327 ; in particular , the sorted eigenvalues of @xmath323 that are greater than @xmath33 are upper bounded by the corresponding ones of @xmath328 .    now with the distributions on the elements of the bounding matrices , we can give sharp estimate on @xmath164 .",
    "in particular , we have the following proposition :    for gaussian measurement matrix @xmath313 , suppose @xmath177 and @xmath187 are a pair of distinct supports with the same size @xmath178 .",
    "then we have@xmath329    * proof : * we copy the inequalities , on @xmath164 here :    @xmath330    the proof then reduces to the computation of two expectations , one of which is trivial:@xmath331    next , the independence of the @xmath332 s and the convexity of exponential functions together with jensen s inequality yield @xmath333 \\\\ & \\geq & \\frac{1}{2\\kappa \\sigma ^{2}}\\exp \\left [ \\frac{1}{k_{\\mathrm{d}}}% \\sum_{n=1}^{k_{\\mathrm{d}}}\\mathbb{e}\\log \\left ( 2\\kappa r_{nn}^{2}\\right ) \\right ] .\\end{aligned}\\ ] ]    since @xmath334 , the expectation of logarithm is @xmath335 , where @xmath336 is the digamma function .",
    "note that @xmath337 is increasing and satisfies @xmath338 .",
    "therefore , we have@xmath339 \\\\",
    "& \\geq & \\frac{1}{\\kappa \\sigma ^{2}}\\exp \\left [ \\psi \\left ( \\kappa ( m - k - k_{\\mathrm{d}}+1)\\right ) \\right ] \\\\ & \\geq & \\frac{1}{\\kappa \\sigma ^{2}}\\exp \\left [ \\log \\left ( \\kappa ( m - k - k_{\\mathrm{d}})\\right ) \\right ] \\\\ & \\geq & \\frac{m - k - k_{\\mathrm{d}}}{\\sigma ^{2}}.\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\blacksquare\\end{aligned}\\ ] ]    the expected value of the critical quantity @xmath164 lies between @xmath340 and @xmath341 , linearly proportional to @xmath8 .",
    "note that in conventional compressive sensing , the variance of the elements of @xmath342 is usually taken to be @xmath343 , which is equivalent to scaling the noise variance @xmath200 to @xmath344 in our model .",
    "the resultant @xmath164 is then centered between @xmath345 and @xmath346 .",
    "[ sec : gaussian_nece ] one fundamental problem in compressive sensing is how many samples should the system take to guarantee a stable reconstruction .",
    "although many sufficient conditions are available , non - trivial necessary conditions are rare . besides , in previous works , stable reconstruction has been measured in the sense of @xmath16 norms between the reconstructed signal and the true signal . in this section , we derive two necessary conditions on @xmath8 and @xmath18 in terms of @xmath9 and @xmath178 in order to guarantee respectively that , first , @xmath347 turns to zeros and , second , for majority realizations of @xmath93 , the probability of error vanishes .",
    "more precisely , we have the following theorem :    [ gaussian_nece ] in the support recovery problem , for any @xmath348 , a necessary condition of @xmath349 is @xmath350and a necessary condition of @xmath351 is @xmath352    * proof : * equation and @xmath353 give @xmath354hence , @xmath355 entails and .",
    "denote by @xmath356 the event @xmath357 ; then @xmath358 and we have@xmath359therefore , from the first part of the theorem , we obtain and .",
    "@xmath360    we compare our results with those of @xcite and @xcite . as we mentioned in the introduction ,",
    "the differences in problem modeling and the definition of the probability of error make a direct comparison difficult .",
    "we first note that theorem 2 in @xcite is established for the restricted problem where it is known _ a priori _ that all non - zero components in the sparse signal are equal .",
    "because the set of signal realizations with equal non - zero components is a rare event in our signal model , it is not fitting to compare our result with the corresponding one in @xcite by computing the distribution of the smallest on - support element , e.g. , the expectation",
    ". actually , the square of the smallest on - support element for the restricted problem , @xmath361 ( or @xmath362 in @xcite ) , is equivalent to the signal variance in our model : both are measures of the signal energy .",
    "if we take into account the noise variance and replace @xmath361 ( or @xmath363 in @xcite ) with @xmath364 , the necessary conditions in these papers coincide with ours when only one temporal sample is available .",
    "our result shows that as far as support recovery is concerned , one can not avoid the @xmath24 term when only given one temporal sample .",
    "worse , for conventional compressive sensing with a measurement matrix generated from a gaussian random variable with variance @xmath365 , the necessary condition becomes    @xmath366    which is independent of @xmath8 .",
    "therefore , when there is _ considerable _ noise , it is impossible to have a vanishing @xmath367 no matter how large an @xmath8 one takes .",
    "basically this situation arises because while taking more samples , one scales down the measurement gains @xmath368 , which effectively reduces the snr and thus is not helpful in support recovery . as discussed below theorem [ doa ]",
    ", @xmath369 is the uncertainty of the support variable @xmath2 , and @xmath370 actually comes from it .",
    "therefore , it is no surprise that the number of samples is determined by this quantity and can not be made independent of it .",
    "[ sec : gaussian_suff ] we derive a sufficient condition in parallel with sufficient conditions in compressive sensing . in compressive",
    "sensing , when only one temporal sample is available , @xmath371 is enough for stable signal reconstruction for the majority of the realizations of measurement matrix @xmath372 from a gaussian ensemble with variance @xmath343 .",
    "as shown in the previous subsection , if we take the probability of error for support recovery as a performance measure , it is impossible in this case to recover the support with a vanishing probability of error unless the noise is small .",
    "therefore , we consider a gaussian ensemble with unit variance .",
    "we first establish a lemma to estimate the lower tail of the distribution for @xmath175 .",
    "we have shown that the @xmath373 lie between @xmath340 and @xmath374 .",
    "when @xmath375 is much less than @xmath376 , we expect that @xmath377 decays quickly .",
    "more specifically , we have the following large deviation lemma :    [ tailprob ] suppose that @xmath378 .",
    "then there exists constant @xmath379 such that for @xmath380 sufficiently large , we have @xmath381 .\\ ] ]    this large deviation lemma together with the union bound yield the following sufficient condition for support recovery :    [ thm_suff_gaussian ] suppose that @xmath382and @xmath383 .",
    "\\label{sufficient_tlogm}\\]]then given any realization of measurement matrix @xmath93 from a gaussian ensemble , the optimal decision rule for multiple support recovery problem has a vanishing @xmath384 with probability turning to one .",
    "in particular , if @xmath385 and @xmath386then the probability of error turns to zero as @xmath9 turns to infinity .",
    "* proof : * denote @xmath378 . then according to",
    "the union bound , we have @xmath387 \\right\\ }   \\\\ & \\leq & \\sum_{s_{i}\\neq s_{j}}\\pr \\left\\ { \\bar{\\lambda}_{s_{i},s_{j}}\\leq \\gamma \\right\\ } .\\end{aligned}\\]]therefore , application of lemma [ tailprob ] gives @xmath388 .\\end{aligned}\\]]hence , as long as @xmath371 , we know that the exponent turns to @xmath389 as @xmath390 .",
    "we now define @xmath391 , where @xmath392 approaches one as @xmath9 turns to infinity . now",
    "the upper bound becomes @xmath393hence , if @xmath394 $ ] , we get a vanishing probability of error . in particular , under the assumption that @xmath395 , if @xmath396 , then @xmath397 } { \\log \\left [ k\\log \\frac{n}{k}\\right ] } \\leq \\frac{\\log n}{% \\log \\log n}$ ] implies that @xmath398 for suitably selected constants .",
    "@xmath172    we now consider several special cases and explore the implications of the sufficient conditions .",
    "the discussions are heuristic in nature and their validity requires further checking .",
    "if we set @xmath10 , then we need @xmath8 to be much greater than @xmath9 to guarantee a vanishing probability @xmath399 .",
    "this restriction suggests that even if we have more observations than the original signal length @xmath9 , in which case we can obtain the original sparse signal by solving a least squares problem , we still might not be able to get the correct support because of the noise , as long as @xmath8 is not sufficiently large compared to @xmath9 .",
    "we discussed in the introduction that for many applications , the support of a signal has significant physical implications and its correct recovery is of crucial importance .",
    "therefore , without multiple temporal samples and with moderate noise , the scheme proposed by compressive sensing is questionable as far as support recovery is concerned .",
    "worse , if we set the variance for the elements in @xmath400 to be @xmath365 as in compressive sensing , which is equivalent to replacing @xmath200 with @xmath344 , even increasing the number of temporal samples will not improve the probability of error significantly unless the noise variance is very small .",
    "hence , using support recovery as a criterion , one can not expect the compressive sensing scheme to work very well in the low snr case .",
    "this conclusion is not a surprise , since we reduce the number of samples to achieve compression .",
    "another special case is when @xmath401 . in this case , the sufficient condition becomes @xmath402 and @xmath403 now the number of total samples should satisfy @xmath404 while the necessary condition states that @xmath405 the smallest gap between the necessary condition and sufficient condition is achieved when @xmath401 .    from a denoising perspective ,",
    "fletcher _ et.al .",
    "_ @xcite upper bounded and approximated the probability of error for support recovery averaged over the gaussian ensemble .",
    "the bound and its approximation are applicable only to the special case with @xmath401 and involve complex integrals .",
    "the authors obtained interesting snr threshold as a function of @xmath406 and @xmath178 through the analytical bound .",
    "note that our bounds are valid for general @xmath178 and have a simple form .",
    "besides , most of our derivation is conditioned on a realization of the gaussian measurement ensemble .",
    "the conditioning makes more sense than averaging since in practice we usually make observations with fixed sensing matrix and varying signals and noise .",
    "the result of theorem [ thm_suff_gaussian ] also exhibits several interesting properties in the general case . compared with the necessary condition and ,",
    "the asymmetry in the sufficient condition is even more desirable in most cases because of the asymmetric cost associated with sensors and temporal samples .",
    "once the threshold @xmath407 of @xmath8 is exceeded , we can achieve a desired probability of error by taking more temporal samples .",
    "if we were concerned only with total the number of samples , we would minimize @xmath408 subject to the constraints and to achieve a given level of probability of error .",
    "however , in applications for which timing is important , one has to increase sensors to reduce @xmath399 to a certain limit .    the sufficient condition , , and is separable in the following sense .",
    "we observe from the proof that the requirement @xmath371 is used only to guarantee that the randomly generated measurement matrix is a good one in the sense that its incoherence @xmath192 is sufficiently large , as in the case of compressive sensing .",
    "it is in lemma [ tailprob ] that we use the gaussian ensemble assumption .",
    "if another deterministic construction procedure ( for attempts in this direction , see devore2007deterministic ) or random distribution give measurement matrix with better incoherence @xmath192 , it would be possible to reduce the orders for both @xmath8 and @xmath18 .",
    "in this paper , we formulated the support recovery problems for jointly sparse signals as binary and multiple - hypothesis testings . adopting the probability of error as the performance criterion ,",
    "the optimal decision rules are given by the likelihood ratio test and the maximum _ a posteriori _ probability estimator .",
    "the latter reduces to the maximum likelihood estimator when equal prior probabilities are assigned to the supports .",
    "we then employed the chernoff bound and fano s inequality to derive bounds on the probability of error .",
    "we discussed the implications of these bounds at the end of section [ sec : multipleupper ] , section [ sec : noiseeffectupper ] , section [ sec : lower ] , section [ sec : gaussian_nece ] , and section [ sec : gaussian_suff ] , in particular when they are applied to the doa estimation problem considered in @xcite and compressive sensing with a gaussian measurement ensemble .",
    "we derived sufficient and necessary conditions for compressive sensing using gaussian measurement matrices to achieve a vanishing probability of error in both the mean and large probability senses .",
    "these conditions show the necessity of considering multiple temporal samples .",
    "the symmetric and asymmetric roles played by the spatial and temporal samples and their implications in system design were discussed . for compressive",
    "sensing , we demonstrated that it is impossible to obtain accurate signal support with only one temporal sample if the variance for the gaussian measurement matrix scales with @xmath365 and there is considerable noise .",
    "this research on support recovery for jointly sparse signals is far from complete .",
    "several questions remain to be answered .",
    "first , we notice an obvious gap between the necessary and sufficient conditions even in the simplest case with @xmath401 .",
    "better techniques need to be introduced to refine the results .",
    "second , as in the case for rip , computation of the quantity @xmath409 for an arbitrary measurement matrix is extremely difficult .",
    "although we derive large derivation bounds on @xmath192 and compute the expected value for @xmath164 for the gaussian ensemble , its behaviors in both the general and gaussian cases require further study . its relationship with rip also needs to be clarified .",
    "finally , our lower bound derived from fano s inequality identifies only the effect of the total gain .",
    "the effect of the measurement matrix s incoherence is elusive .",
    "the answers to these questions will enhance our understanding of the measurement mechanism .",
    "in this proof , we focus on the case for which both @xmath410 and @xmath411 .",
    "other cases have similar and simpler proofs .",
    "the eigenvalues of @xmath144 satisfy @xmath412 , which is equivalent to @xmath413 .",
    "the substitution @xmath414 defines @xmath415the following algebraic manipulation @xmath416   \\\\ & & \\ \\ \\ -\\left [ a_{s_{0}\\cap s_{1}}a_{s_{0}\\cap s_{1}}^{\\dagger } + a_{s_{1}\\backslash s_{0}}a_{s_{1}\\backslash s_{0}}^{\\dagger } \\right ]   \\\\ & = & a_{s_{0}\\backslash s_{1}}a_{s_{0}\\backslash s_{1}}^{\\dagger } -a_{s_{1}\\backslash s_{0}}a_{s_{1}\\backslash s_{0}}^{\\dagger } \\end{aligned}\\]]leads to @xmath417    therefore , to prove the theorem , it suffices to show that @xmath418 has @xmath155 positive eigenvalues , @xmath150 negative eigenvalues and @xmath156 zero eigenvalues or , put another way , @xmath419 has inertia @xmath420 .",
    "the sylvester s law of inertia ( @xcite , theorem 4.5.8 , p. 223 ) states that the inertia of a symmetric matrix is invariant under congruence transformations .",
    "hence , we need only to show that @xmath421 has inertia @xmath420 .",
    "clearly @xmath422 with @xmath423 $ ] and @xmath424 .$ ] to find the number of zero eigenvalues of @xmath421 , we calculate the rank of @xmath421 .",
    "the non - degenerateness of measurement matrix @xmath20 implies that @xmath425 therefore , @xmath426from rank inequality ( @xcite , theorem 0.4.5 , p. 13 ) , @xmath427we conclude that @xmath428    to count the number of negative eigenvalues of @xmath421 , we use the jocobi - sturm rule ( @xcite , theorem a.1.4 , p. 320 ) , which states that for an @xmath91 symmetric matrix whose @xmath429 leading principal minor has determinant @xmath430 , the number of nonnegative eigenvalues is equal to the number of sign changes of sequence @xmath431 .",
    "we consider only the first @xmath432 leading principal minors , since higher order minors have determinant @xmath113 .",
    "suppose @xmath433 is an index set . without loss of generality , we assume that @xmath434 is nonsingular . applying @xmath435 factorization ( one variation of @xmath436 factorization , see golub1996matrix ) to matrix @xmath434 , we obtain @xmath437 , where @xmath438 is an orthogonal matrix , @xmath439 , and @xmath440 is an lower triangular matrix .",
    "the diagonal entries of @xmath123 are nonzero since @xmath434 is nonsingular . the partition of @xmath123 into @xmath441\\]]with @xmath442 , and @xmath443 $ ] with @xmath444 implies @xmath445 \\left [ \\begin{array}{c } l_{1}^{\\dagger } \\\\",
    "-l_{2}^{\\dagger } % \\end{array}% \\right ] o^{\\dagger } .\\]]again using the invariance property of inertia under congruence transformation , we focus on the leading principal minors of @xmath446 \\left [ \\begin{array}{c } l_{1}^{\\dagger } \\\\",
    "-l_{2}^{\\dagger } % \\end{array}% \\right ] .$ ] suppose @xmath447 . for @xmath448 , from the lower triangularity of @xmath123 ,",
    "it is clear that @xmath449for @xmath450 , suppose @xmath451 and @xmath452 .",
    "we then have @xmath453 ^{\\dagger } \\right\\vert   \\\\ & = & \\left ( -1\\right ) ^{j - k_{0}}\\left\\vert \\left ( l_{1}\\right ) _ { j_{0}}^{j_{0}}\\right\\vert ^{2}\\left\\vert \\left ( l_{3}\\right ) _ { j_{1}}^{j_{1}}\\right\\vert ^{2 } \\\\ & = & \\left ( -1\\right ) ^{j - k_{0}}\\prod_{i=1}^{j}\\left\\vert l_{ii}\\right\\vert ^{2}.\\end{aligned}\\]]therefore ,",
    "the sequence @xmath454 has @xmath150 sign changes , which implies that @xmath455hence @xmath421has @xmath150 negative eigenvalues .",
    "finally , we conclude that the theorem holds for @xmath144 .",
    "we first prove the first claim . from the proof of proposition [ eig_count ]",
    ", it suffices to show that the sorted positive eigenvalues of @xmath418 are greater than those of @xmath456 , where @xmath457 .",
    "since cyclic permutation of a matrix product does not change its eigenvalues , we restrict ourselves to @xmath458 .",
    "consider the @xmath436 decomposition @xmath459where @xmath460 is an orthogonal matrix with partitions @xmath461",
    ", @xmath462 is an upper triangular matrix with partitions @xmath463 , and other submatrices have corresponding dimensions .",
    "first , we note that@xmath464therefore , the last @xmath465 rows and columns of @xmath466and hence of @xmath467are zeros , which lead to the @xmath468 zero eigenvalues of @xmath469 .",
    "we then drop these rows and columns in all matrices involved in subsequent analysis . in particular , the submatrix of @xmath470 without the last @xmath471 rows and columns is@xmath472define    @xmath473    due to the invariance of eigenvalues with respect to orthogonal transformations and switching to the symmetrized version , we focus on @xmath474    next we argue that the sorted positive eigenvalues of @xmath475 are greater than the corresponding sorted eigenvalues of @xmath476 .    for any @xmath477",
    ", we define a matrix @xmath478 $ ]",
    ". then we have    @xmath479note that @xmath480 is congruent to    @xmath481    clearly @xmath482 is positive definite when @xmath9 is sufficiently large . hence ,",
    "when @xmath9 is large enough , we obtain @xmath483using corollary 4.3.3 of @xcite , we conclude that the eigenvalues of @xmath475 are greater than those of @xmath484 if sorted . from proposition eig_count , we know that @xmath480 has exactly @xmath155 positive eigenvalues , which are the only eigenvalues that could be greater than @xmath485 . since @xmath486 is arbitrary , we finally conclude that the positive eigenvalues of @xmath487 are greater than those of @xmath488 if sorted in the same way .    for the second claim ,",
    "we need some notations and properties of symmetric and hermitian matrices . for any pair of symmetric ( or hermitian ) matrices @xmath489 and @xmath490 , @xmath491 means that @xmath492 is positive definite and @xmath493 means @xmath492 is nonnegative definite .",
    "note that if @xmath489 and @xmath490 are positive definite , then from corollary 7.7.4 of @xcite @xmath493 if and only if @xmath494 ; if @xmath495 then the eigenvalues of @xmath489 and @xmath490 satisfy @xmath496 , where @xmath497 denotes the @xmath498th largest eigenvalue of @xmath489 ; furthermore , @xmath499 implies that @xmath500 for any @xmath489 , square or rectangular .",
    "therefore , @xmath501yields @xmath502    recall that from the definition of eigenvalues , the non - zero eigenvalues of @xmath503 and @xmath504 are the same for any matrices @xmath20 and @xmath38 .",
    "since we are interested only in the eigenvalues , a cyclic permutation in the matrix product on the previous inequality s right - hand side gives us @xmath505    until now we have shown that the sorted eigenvalues of @xmath144 are less than the corresponding ones of @xmath506 .",
    "the non - zero eigenvalues of @xmath507 is the same as the non - zero eigenvalues of @xmath508 . using the same fact again",
    ", we conclude that the non - zero eigenvalues of @xmath509 is the same as the non - zero eigenvalues of @xmath510 .",
    "therefore , we obtain that @xmath511    in particular , the eigenvalues of @xmath144 that are greater than @xmath33 are upper bounded by the corresponding ones of  @xmath512 if they are both sorted ascendantly .",
    "hence , we get that the eigenvalues of @xmath144 that are greater than @xmath33 are less than those of @xmath513 .",
    "therefore , the conclusion of the second part of the theorem holds .",
    "we comment here that usually it is not true that @xmath514 only the inequality on eigenvalues holds .",
    "for arbitrary fixed supports @xmath516 , we have    @xmath517    where @xmath518 can be written as a sum of @xmath519 independent squared standard gaussian random variables and @xmath520 is obtained by dropping @xmath521 of them .",
    "therefore , using the union bound we obtain @xmath522 \\right\\ }   \\\\ & \\leq & k_{\\mathrm{d}}\\pr \\left\\ { q_{l}\\leq 2\\kappa \\sigma ^{2}\\gamma \\right\\ } .\\end{aligned}\\]]since @xmath378 implies that @xmath523 , the mode of @xmath524 , when @xmath380 is sufficiently large , we have @xmath525 ^{\\kappa ( m-2k)}}{% \\gamma \\left ( \\kappa ( m-2k)\\right ) } e^{-\\kappa \\sigma ^{2}\\gamma } .\\end{aligned}\\]]the inequality @xmath526 says that when @xmath380 is large enough , @xmath527 \\log \\left [ \\kappa ( m-2k)\\right ]   \\\\ & & \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ + \\kappa ( m-2k){\\huge \\ } } \\\\ & \\leq & \\exp \\left\\ { -c(m-2k)\\right\\ } , \\end{aligned}\\]]where @xmath528 .",
    "therefore , we have @xmath529",
    "the authors thank the anonymous referees for their careful and helpful comments .",
    "100 m.  b. wakin , s.  sarvotham , m.  f. duarte , d.  baron , and r.  g. baraniuk , `` recovery of jointly sparse signals from few random projections , '' in _ proc .",
    "neural inform . processing systems _ ,",
    "vancouver , canada , dec .",
    "2005 , pp . 14351442 .",
    "e.  cands , j.  romberg , and t.  tao , `` robust uncertainty principles : exact signal reconstruction from highly incomplete frequency information , '' _ ieee trans .",
    "inf . theory _ ,",
    "52 , no .  2 , pp . 489509 , feb .",
    "2006 .",
    "d.  malioutov , m.  cetin , and a.  willsky , `` a sparse signal reconstruction perspective for source localization with sensor arrays , '' _ ieee trans .",
    "signal process .",
    "_ , vol .",
    "53 , no .  8 , pp . 30103022 , aug .",
    "2005 .",
    "v.  cevher , p.  indyk , c.  hegde , and r.  g. baraniuk , `` recovery of clustered sparse signals from compressive measurements , '' in _ int .",
    "sampling theory and applications ( sampta 2009 ) _ , marseille , france , may 2009 , pp .",
    "1822 .",
    "p.  borgnat and p.  flandrin , `` time - frequency localization from sparsity constraints , '' in _ proc .",
    "acoustics , speech and signal processing ( icassp 2008 ) _ , las vegas , nv , apr . 2008 , pp . 37853788 .",
    "z.  tian and g.  giannakis , `` compressed sensing for wideband cognitive radios , '' in _ proc .",
    "acoustics , speech and signal processing ( icassp 2007 ) _ , honolulu , hi , apr . 2007 , pp .",
    "iv1357iv1360 .",
    "m.  a. sheikh , s.  sarvotham , o.  milenkovic , and r.  g. baraniuk , `` dna array decoding from nonlinear measurements by belief propagation , '' in _ proc .",
    "ieee workshop statistical signal processing ( ssp 2007 ) _ , madison , wi , aug . 2007 , pp .",
    "215219 .",
    "h.  vikalo , f.  parvaresh , and b.  hassibi , `` on recovery of sparse signals in compressed dna microarrays , '' in _ proc .",
    "asilomar conf .",
    "signals , systems and computers ( acssc 2007 ) _ , pacific grove , ca , nov .",
    "2007 , pp . 693697 .",
    "f.  parvaresh , h.  vikalo , s.  misra , and b.  hassibi , `` recovering sparse signals using sparse measurement matrices in compressed dna microarrays , '' _ ieee j. sel .",
    "topics signal processing _ , vol .  2 , no .  3 , pp .",
    "275285 , jun . 2008 .",
    "h.  vikalo , f.  parvaresh , s.  misra , and b.  hassibi , `` sparse measurements , compressed sampling , and dna microarrays , '' in _ proc .",
    "acoustics , speech and signal processing ( icassp 2008 ) _ , las vegas , nv , apr .",
    "2008 , pp . 581584 .",
    "g.  bienvenu and l.  kopp , `` adaptivity to background noise spatial coherence for high resolution passive methods , '' in _ proc .",
    "acoustics , speech and signal processing ( icassp 1980 ) _ , vol .  5 , denver , co , apr .",
    "1980 , pp .",
    "307310 .",
    "p.  stoica and a.  nehorai , `` music , maximum likelihood , and cramr - rao bound : further results and comparisons , '' _ ieee trans .",
    "speech , signal process .",
    "_ , vol .",
    "38 , no .",
    "21402150 , dec . 1990 .",
    "m.  duarte , s.  sarvotham , d.  baron , m.  wakin , and r.  baraniuk , `` distributed compressed sensing of jointly sparse signals , '' in _ proc .",
    ". signals , systems and computers ( acssc 2005 ) _ , pacific grove , ca , nov . 2005 , pp . 15371541 .",
    "m.  duarte , m.  wakin , d.  baron , and r.  baraniuk , `` universal distributed sensing via random projections , '' in _ int .",
    "information processing in sensor networks ( ipsn 2006 ) _ , nashville , tn , apr .",
    "2006 , pp . 177185 .",
    "m.  mishali and y.  eldar , `` the continuous joint sparsity prior for sparse representations : theory and applications , '' in _ ieee int . workshop computational advances in multi - sensor adaptive processing ( campsap 2007 ) _ , st . thomas , u.s .",
    "virgin islands , dec .",
    "2007 , pp . 125128 .",
    "s.  cotter , b.  rao , k.  engan , and k.  kreutz - delgado , `` sparse solutions to linear inverse problems with multiple measurement vectors , '' _ ieee trans .",
    "signal process .",
    "_ , vol .",
    "53 , no .  7 , pp . 24772488 , jul . 2005 .",
    "j.  chen and x.  huo , `` sparse representations for multiple measurement vectors ( mmv ) in an over - complete dictionary , '' in _ proc .",
    "acoustics , speech and signal processing ( icassp 2005 ) _ , philadelphia , pa , mar . 2005 , pp . 257260 .",
    "m.  wainwright , `` information - theoretic bounds on sparsity recovery in the high - dimensional and noisy setting , '' in _ ieee int .",
    "information theory ( isit 2007 ) _ , nice , france , jun .",
    "2007 , pp . 961965 .",
    "o.  milenkovic , h.  pham , and w.  dai , `` sublinear compressive sensing reconstruction via belief propagation decoding , '' in _ int .",
    "information theory ( isit 2009 ) _ , seoul , south korea , jul .",
    "2009 , pp .",
    "674678 .",
    "g.  tang and a.  nehorai , `` support recovery for source localization based on overcomplete signal representation , '' submitted to _ proc .",
    "acoustics , speech and signal processing ( icassp 2010)_.          a.  k. fletcher , s.  rangan , v.  k. goyal , and k.  ramchandran , `` denoising by sparse approximation : error bounds based on rate - distortion theory , '' _ eurasip journal on applied signal processing _ , vol .",
    "2006 , pp .",
    "119 , 2006 .            currently , he is a ph.d .",
    "candidate with the department of electrical and systems engineering , washington university , under the guidance of dr .",
    "arye nehorai .",
    "his research interests are in the area of compressive sensing , statistical signal processing , detection and estimation , and their applications .",
    "arye nehorai ( s80-m83-sm90-f94 ) earned his b.sc . and m.sc .",
    "degrees in electrical engineering from the technion ",
    "israel institute of technology , haifa , israel , and the ph.d .",
    "degree in electrical engineering from stanford university , stanford , ca .    from 1985",
    "to 1995 , he was a faculty member with the department of electrical engineering at yale university . in 1995",
    ", he became a full professor in the department of electrical engineering and computer science at the university of illinois at chicago ( uic ) . from 2000 to 2001 , he was chair of the electrical and computer engineering ( ece ) division , which then became a new department . in 2001",
    ", he was named university scholar of the university of illinois . in 2006 , he became chairman of the department of electrical and systems engineering at washington university in st .",
    "he is the inaugural holder of the eugene and martha lohman professorship and the director of the center for sensor signal and information processing ( cssip ) at wustl since 2006 .",
    "nehorai was editor - in - chief of the ieee transactions on signal processing from 2000 to 2002 . from 2003 to 2005 , he was vice president ( publications ) of the ieee signal processing society ( sps ) , chair of the publications board , member of the board of governors , and member of the executive committee of this society . from 2003 to 2006",
    ", he was the founding editor of the special columns on leadership reflections in the ieee signal processing magazine .",
    "he was co - recipient of the ieee sps 1989 senior award for best paper with p. stoica , coauthor of the 2003 young author best paper award , and co - recipient of the 2004 magazine paper award with a. dogandzic .",
    "he was elected distinguished lecturer of the ieee sps for the term 2004 to 2005 and received the 2006 ieee sps technical achievement award .",
    "he is the principal investigator of the new multidisciplinary university research initiative ( muri ) project entitled adaptive waveform diversity for full spectral dominance .",
    "he has been a fellow of the royal statistical society since 1996 ."
  ],
  "abstract_text": [
    "<S> the performance of the common support for jointly sparse signals based on their projections onto lower - dimensional space is analyzed . </S>",
    "<S> support recovery is formulated as a testing problem . </S>",
    "<S> both upper and lower bounds on the probability of error are derived for general measurement matrices , by using the chernoff bound and fano s inequality , respectively . </S>",
    "<S> the upper bound shows that the performance is determined by a quantity measuring the measurement matrix incoherence , while the lower bound reveals the importance of the total measurement gain . </S>",
    "<S> the lower bound is applied to derive the minimal number of samples needed for accurate direction - of - arrival ( doa ) estimation for a sparse representation based algorithm . when applied to gaussian measurement ensembles , </S>",
    "<S> these bounds give necessary and sufficient conditions for a vanishing probability of error for majority realizations of the measurement matrix . </S>",
    "<S> our results offer surprising insights into sparse signal recovery . </S>",
    "<S> for example , as far as support recovery is concerned , the well - known bound in compressive sensing with the gaussian measurement matrix is generally not sufficient unless the noise level is low . </S>",
    "<S> our study provides an alternative performance measure , one that is natural and important in practice , for signal recovery in compressive sensing and other application areas exploiting signal sparsity .    </S>",
    "<S> chernoff bound , compressive sensing , fano s inequality , jointly sparse signals , multiple hypothesis testing , probability of error , support recovery </S>"
  ]
}