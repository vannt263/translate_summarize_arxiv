{
  "article_text": [
    "suppose @xmath0 @xmath1 is a stochastic process such that @xmath8 as @xmath3  typically @xmath9 is a deterministic seed or arbitrary value initiating the iteration and we are interested in the limiting value @xmath10 the sequence @xmath4 may be deterministic , obtained by using a numerical integration scheme to approximate an integral , or a newton - raphson scheme to approximate the root of an equation .",
    "it may be a ratio estimator estimating a population ratio or the result of a stochastic or deterministic approximation to a root or maximum .",
    "in general we will only assume that it is possible to sample from the distribution of the stochastic process for a finite period , i.e. sample @xmath11  for fixed @xmath6 .",
    "a common argument advanced in favour of the use monte carlo ( mc ) methods as an alternative to numerical ones is that the mc estimator is usually unbiased with estimable variance . by increasing the sample size we are assured by unbiasedness that the estimator is consistent and we can produce , for any sample size",
    ", a standard error of the estimator .  the statistical argument is advanced against the use of numerical methods that they do not offer easily obtained estimates of error .",
    "the purpose of this brief note is to show that this argument is flawed ; generally any consistent sequence of estimators can be easily rendered unbiased and an error estimate is easily achieved .",
    "we do not attempt to merely reduce the bias , but by introducing randomization into the sequence , to completely eliminate it .",
    "the price we pay is an additional randomization inserted into the sequence and a possible increase in the mean squared error ( mse ) .",
    "suppose @xmath12 is a random variable , independent of the sequence @xmath13 @xmath14 taking finite _ non - negative _ integer values .",
    "suppose @xmath15 @xmath16for all @xmath17 for a given sequence @xmath0 @xmath1 we define the first backward difference as @xmath18 define the random variable @xmath19 this can be written in the more general form @xmath20  where @xmath21are random variables with @xmath22=1 $ ] and  for some value of @xmath23 we have @xmath24 for @xmath25  we will show that @xmath26 is an unbiased estimator of the limit @xmath7 with easily estimated standard error .",
    "it is obviously unbiased provided that it is integrable and one can interchange the sum and the expected value since with @xmath27 @xmath28   = x_{0}+\\sum_{n=1}^{\\infty}\\nabla x_{n}=x_{\\infty}.$ ]    assume for the calculation of the variance that @xmath29 as @xmath3 then @xmath30+var(e(y|\\mathcal{f}% ) ) \\nonumber\\\\ &   = e[var(y|\\mathcal{f})]+var(x_{\\infty})\\nonumber\\\\ &   = e\\left (   \\sum_{n=1}^{\\infty}\\frac{(\\nabla x_{n})^{2}}{q_{n}^{2}}% q_{n}(1-q_{n})+2\\sum_{n=2}^{\\infty}\\sum_{j=1}^{n-1}\\frac{\\nabla x_{n}\\nabla x_{j}}{q_{n}q_{j}}q_{n}(1-q_{j})\\right )   \\nonumber\\\\ &   = e\\left (   \\sum_{n=1}^{\\infty}\\frac{(\\nabla x_{n})^{2}}{q_{n}}% ( 1-q_{n})+2\\sum_{j=1}^{\\infty}\\sum_{n = j+1}^{\\infty}\\frac{\\nabla x_{n}\\nabla x_{j}}{q_{j}}(1-q_{j})\\right )   \\label{vary}\\\\ &   = e\\left (   \\sum_{n=1}^{\\infty}\\frac{(\\nabla x_{n})^{2}}{q_{n}}% ( 1-q_{n})+2\\sum_{j=1}^{\\infty}\\frac{(x_{\\infty}-x_{j})\\nabla x_{j}}{q_{j}% } ( 1-q_{j})\\right )   \\nonumber\\\\ &   = e\\left (   \\sum_{n=1}^{\\infty}\\left [   ( \\nabla x_{n})^{2}+2(x_{\\infty}% -x_{n})\\nabla x_{n}\\right ]   ( \\frac{1-q_{n}}{q_{n}})\\right )   \\nonumber\\\\ &   = \\sum_{n=1}^{\\infty}e\\left (   \\frac{\\nabla x_{n}\\left [   2x_{\\infty}% -x_{n}-x_{n-1}\\right ]   } { q_{n}}-\\sum_{n=1}^{\\infty}\\nabla x_{n}\\left [ 2x_{\\infty}-x_{n}-x_{n-1}\\right ]   \\right )   \\nonumber\\\\ &   = \\sum_{n=1}^{\\infty}e\\left (   \\frac{2x_{\\infty}\\nabla x_{n}-\\nabla x_{n}% ^{2}}{q_{n}}-\\sum_{n=1}^{\\infty}\\left (   2x_{\\infty}\\nabla x_{n}-\\nabla x_{n}^{2}\\right )   \\right )   \\nonumber\\\\ &   = \\sum_{n=1}^{\\infty}e\\left (   \\frac{2x_{\\infty}\\nabla x_{n}-\\nabla x_{n}% ^{2}}{q_{n}}\\right )   -\\left (   2x_{\\infty}(x_{\\infty}-x_{0})-\\left (   x_{\\infty } ^{2}-x_{0}^{2}\\right )   \\right )   \\nonumber\\\\ &   = \\sum_{n=1}^{\\infty}e\\left (   \\frac{2x_{\\infty}\\nabla x_{n}-\\nabla x_{n}% ^{2}}{q_{n}}\\right )   -(x_{\\infty}-x_{0})^{2}\\nonumber\\\\ &   = \\sum_{n=1}^{\\infty}\\frac{2x_{\\infty}\\nabla\\mu_{n}-\\nabla(\\sigma_{n}% ^{2}+\\mu_{n}^{2})}{q_{n}}-(x_{\\infty}-x_{0})^{2}\\nonumber\\\\ &   = \\sum_{n=1}^{\\infty}\\frac{2\\left (   x_{\\infty}-\\xi_{j}\\right )   \\nabla\\mu _ { j}-\\nabla\\sigma_{j}^{2}}{q_{n}}-(x_{\\infty}-x_{0})^{2}%\\end{aligned}\\ ] ] where @xmath31=ex_{n}^{2}-ex_{n-1}^{2}=\\sigma_{n}^{2}+\\mu_{n}% ^{2}-\\sigma_{n-1}^{2}-\\mu_{n-1}^{2}=\\nabla(\\sigma_{n}^{2}+\\mu_{n}^{2})$ ] and @xmath32 then @xmath33 as given in ( [ vary ] ) can be unbiasedly estimated using @xmath34    suppose @xmath35 with probability one .",
    "then the average over a large number of values of @xmath26 ,  i.e. a large number of values of @xmath36 takes the form @xmath37 where @xmath38 is the largest observed value of @xmath12 and @xmath39  denotes the average of the observed values of @xmath40 for which the corresponding @xmath41 this takes  the form of ( [ gf ] ) with term @xmath42 obtained from simulating values of @xmath12 alone .",
    "suppose we wish to minimize the variance subject to a constraint on the expected value of @xmath43 i.e. @xmath44@xmath45 of course we also require that @xmath46 is non - increasing and positive but for the moment we will ignore these additional constraints .",
    "then we obtain , on differentiating the lagrangian with respect to @xmath47with @xmath48 @xmath49 where @xmath50  is determined by the constraint @xmath51 and the minimum variance is finite provided that @xmath52  while this is not entirely practical because it requires @xmath53 it is common to have some information on the rate of convergence of the sequence that can be used to design an asymptotically appropriate sequence @xmath54 for example if we believe @xmath55  for some @xmath56 and @xmath57 then we might choose @xmath58 or a random variable @xmath12 which has a geometric distribution , at least in the tails .",
    "suppose the sequence @xmath59 is deterministic and we use @xmath60",
    ".  then the variance is finite provided the series @xmath61is convergent .",
    "let us consider a simple example before we look at more complex ones .",
    "suppose @xmath62  for @xmath63 and @xmath64  then @xmath65 and @xmath66and , as we already verified more generally , @xmath67   = b$ ]  whatever the distribution of @xmath68  suppose we use a shifted geometric distribution for @xmath12 so that @xmath69 for @xmath70 .  evidently to minimize the variance we should choose @xmath71 so that @xmath72 the variance for general @xmath73 is @xmath74   \\text { \\ where } 1>q > r^{2}.\\end{aligned}\\ ] ] suppose we wish to minimize this over the values of @xmath75 and @xmath73 subject to the constraint that @xmath76 is constant ( ignoring the integer constraint on @xmath75 ) .",
    "then with @xmath77 @xmath78   \\text { subject to } % \\frac{1}{z-1}+s=\\mu_{n}\\text { or } % \\]]@xmath79   , \\text { for } \\frac { 1}{r^{2}}>z>1\\ ] ] which minimum occurs when @xmath80 or @xmath81 and @xmath82 and then the minimum variance is @xmath83 notice that the mean squared error , if we were to stop after @xmath84 iterations ,  is @xmath85 so we have purchased unbiasedness of the estimator at a cost of increasing the mse by a factor of approximately @xmath86  this factor is plotted in figure [ plotr ] .",
    "it can be interpreted as follows :  in the worst case scenario when @xmath87 is around .4 ,  we will need about 3 times the sample size for the debiased estimator to achieve the same mean squared error as a conventional iteration using determinisitic @xmath68 however when @xmath88 is close to @xmath89 indicating a slower rate of convergence , there is very little relative increase in the mse .",
    "[ ptb ]    plotr.eps    note :  the optimisation problem above tacitly assumed that the computation time required to generate the sequence is @xmath90  this is not the case with some applications ; for example in the numerical integral below the computation time is @xmath91 since there are @xmath92 intervals and @xmath93 function evaluations and in this case  a more appropriate minimization problem , having budget constraint @xmath94 is , with @xmath95 @xmath96   \\text { \\ subject to } % \\frac{1}{2}>q > r^{2}\\text { and \\ } 2^{s}\\frac{1-q}{1 - 2q}=c>2^{s},s=0,1,2, ... \\ ] ] or , putting @xmath97 @xmath98\\ ] ] which minimum appears to occur for @xmath99   $ ]  and @xmath100 when @xmath101 and otherwise @xmath75 may be somewhat smaller .",
    "intuitively , when the rate of convergence is reasonably fast ( so @xmath102 is small )  then the minimum variance is achieved by a large guarantee on the value of @xmath12 ( @xmath75 large ) and then the residual budget @xmath103 used to produce unbiasedness by appropriate choice of @xmath104",
    "unbiased estimation of a root    suppose we wish to find the root of a nonlinear function @xmath105 . for a toy example , suppose we wish to solve for @xmath106 the equation @xmath107 we might wish to use ( modified ) newton s method with a random starting value to solve this problem ,  requiring randomly generating the initial value @xmath108 and then iterating @xmath109 but of course after a finite number of steps , the current estimate @xmath59 is likely a  biased estimator of the true value of the root .",
    "we implemented the debiasing procedure above with @xmath110 and @xmath111 .",
    "we generated @xmath108 from a @xmath112 distribution , chose @xmath113and for simplicity used @xmath114 and repeated for @xmath115 simulations .",
    "the sample mean of the estimates was @xmath116  and the sample variance @xmath117 .",
    "although the procedure works well in this case when we start sufficiently close to the root , it should be noted that this example argues for an adaptive choice of @xmath47 one which permits a larger number of iterations ( larger values of @xmath118 when the sequence seems to indicate that we have not yet converged .",
    "this is discussed below .    *",
    "stopping times for * @xmath68    in view of the last example , particularly if @xmath108 is far from @xmath53 it would appear desirable to allow @xmath12 to be a stopping time . in order to retain unbiasedness ,",
    "it is sufficient that @xmath119   = e\\left [   x_{0}+\\sum_{n=1}^{\\infty}\\nabla x_{n}\\right ]   \\nonumber\\ ] ] or @xmath120   = 1.\\ ] ] therefore it is sufficient that @xmath121 and one simple rule for an adaptive construction of @xmath12 is : @xmath122{ccc}% p(n\\geq n-1|x_{1},x_{2}, ... x_{n-1 } ) & \\text{if } & \\nabla x_{n}>\\varepsilon\\\\ pp(n\\geq n-1|x_{1},x_{2}, ... x_{n-1 } ) & \\text{if } & \\nabla x_{n}\\leq\\varepsilon \\end{array } \\right.\\ ] ] there are , of course , many potential more powerful rules for determining the shift in the distribution of @xmath12  but we we concentrate here on establishing the properties of the simplest version of this procedure .    simpson s rule .     consider using a trapezoidal rule for estimating the integral @xmath123 using @xmath93 function evaluations which evaluate the function on the grid @xmath124 .",
    "denote the estimate of the integral @xmath125 here @xmath126  and the error in simpson s rule assuming that the function has bounded fourth derivative is @xmath127 .",
    "this suggests a random @xmath12 such that @xmath128 or a ( possibly shifted ) geometric distribution with @xmath129 suppose @xmath130 this means @xmath131  which is quite small . in general ,",
    "the estimator has finite variance since @xmath132 more generally , if @xmath12  has a shifted geometric distribution with probability function @xmath133 parameter @xmath134 the expected number of function evaluations in the quadrature rule is @xmath135 and this is @xmath136 for example , when @xmath137 how well does this perform ?",
    "this provides an unbiased estimator of the integral with variance @xmath138\\ ] ] which can be evaluated or estimated in particular examples and compared with the variance of the corresponding crude monte carlo estimator . for a reasonable comparison",
    ", the latter should have the same ( expected ) number of function evaluations , i.e. 7 and therefore has variance @xmath139 take , for example , the function @xmath140 so that  @xmath141 and @xmath142  in this case the variance of the mc estimator with seven function evaluations is @xmath143 we compare this with the estimator obtained by randomizing the number of points in a simpson s rule .",
    "here it is easy to check that    [ c]||l|l|l|l|l|l|l||@xmath144 & 1 & 2 & 3 & 4 & 5 & 6 + @xmath145 & 0.3047 & 0.2149 & 0.2124 & 0.2122 & 0.2122 & 0.2122 + @xmath146 & -0.089821 & -0.002562 & -0.000139 & -0.000008 & -0.000001 & -0.00000003 +    table 1 : values of the numerical integral @xmath145  and @xmath146 with @xmath92 intervals    and in this case the variance of the debiased simpson s rule estimate is @xmath147 indicating more than a two thousand - fold gain in efficiency over crude monte carlo .",
    "* note : * we have chosen the grid size @xmath148 in view of the fact that when @xmath149 we need the integrals @xmath150 for all @xmath151 in this case , we can simply augment the function evaluations we used for @xmath150 in order to obtain @xmath152    the major advantage of this debiasing procedure however is not as a replacement for crude monte carlo in cases where unbiased estimators exist , but as a device for creating unbiased estimators when their construction is not at all obvious .",
    "this is the case whenever the function of interest is a nonlinear function of variables that can be easily and unbiasedly estimated as in the following example .",
    "heston stochastic volatility model    in the heston stochastic volatility model , under the risk neutral measure @xmath153 the price of an asset @xmath154 and the volatility process @xmath155 are governed by the pair of stochastic differential equations @xmath156 where @xmath157 and @xmath158 are independent  brownian motion processes ,  @xmath102 is the interest rate , @xmath159 is the correlation between the brownian motions driving the asset price  and the volatility process , @xmath160 is the long - run level of volatility and @xmath161 is a parameter governing the  denote by @xmath162  the black - scholes price of a call option having initial stock value @xmath163 volatility @xmath164 , interest rate @xmath165 expiration time @xmath166 option stike price @xmath167 and @xmath168  dividend yield .",
    "the price of a call option in the heston model can be written as an expected value under the risk - neutral measure @xmath169 of a function of two variables @xmath170 ( see for example willard ( 1997 ) and broadie and kaya ( 2006))@xmath171\\text { \\ where } \\\\ \\xi &   = \\xi(v_{t},i(t))=\\exp(-\\frac{\\rho^{2}}{2}i(t)+\\rho\\int_{0}^{t}% \\sqrt{v_{s}}dw_{1}(s))\\\\ &   = \\exp(-\\frac{\\rho^{2}}{2}i(t)+\\frac{\\rho}{\\sigma}(v_{t}-v_{0}+\\kappa i(t)-\\kappa\\theta t))\\text { and}\\\\ \\widetilde{\\sigma }   &   = \\widetilde{\\sigma}(i(t))=\\sqrt{\\frac{i(t)}{t}}\\text { \\ where } i(t)=\\int_{0}^{t}v_{s}ds.\\end{aligned}\\ ] ] this can be valued conditionally on @xmath172 with the usual black - scholes formula . in particular with @xmath173  the option price is @xmath174    note that @xmath175 is clearly a highly nonlinear function of @xmath176 and @xmath177  and so , even if exact simulations of the latter were available , it is not clear how to obtain an unbiased simulation of @xmath178 in the heston model , and indeed various other stochastic volatility models , it is possible to obtain an exact simulation of the value of the process @xmath155 at finitely many values of @xmath179  so it is possible to approximate the integral @xmath177 using @xmath180  obtained from a trapezoidal rule with @xmath181 points .",
    "this raises the question of what we should choose as a distribution for @xmath68 under conditions on the continuity of the functional of the process whose expected value is sought ,  kloeden and platen ( 1995 , theorem 14.1.5 , page 460 ) show that the euler approximation to the process with interval size @xmath148 results in an error in the expected value of order @xmath182  where @xmath183  for sufficiently smooth ( four times continuously differentiable ) drift and diffusion coefficients",
    "so for simplicity consider this case .",
    "this implies that @xmath184 which suggests we choose @xmath185 .    as before we randomly generate @xmath12 from a ( possibly shifted ) geometric@xmath186 distribution with @xmath187 .",
    "the function to be integrated @xmath188 is not twice differentiable so we need to determine empirically the amount of the shift ( and we experimented with reasonable values of @xmath87 ) .",
    "we chose parameters @xmath189 and shifted the geometric random variable by @xmath190 so that @xmath191 for @xmath192 the parameters used in our simulation were taken from broadie and kaya(2004 ) : @xmath193  for which , according to broadie and kaya , the true option price  is around 34.9998 .",
    "1,000,000  simulations with @xmath194 and @xmath195 provided an estimate of this option price of 34.9846  with a standard error  of 0.0107 so there is no evidence of bias in these simulations .",
    "with parameter values @xmath196",
    "@xmath197 @xmath198 and @xmath194 and @xmath199 we conducted @xmath200 simulations leading to an estimate of 6.8115 with a standard error of 0.0048998 .",
    "this is in agreement with the broadie and kaya `` true option price '' of 6.801 .",
    "note that the feller condition for positivity requires @xmath201  which fails in the above cases .",
    "this means that the volatility process hits zero with probability one , and for some parameter values , it does so frequently which may call into question the value of this model with these parameter values .",
    "100,000 simulations from these models used about 10 - 13 minutes running matlab 5.0 on an intel core 2 quad cpu @2.5 ghz .",
    "when numerical methods such as quadrature or numerical solutions to equations may result in a biased estimator , a procedure is suggested which eliminates this bias and provides statistical estimates of error .",
    "this procedure is successfully implemented both in simple root finding problems and in more complicated problems in finance and has enormous potential for providing monte carlo extensions of numerical procedures which allow unbiased estimates and error estimates ."
  ],
  "abstract_text": [
    "<S> consider a stochastic process @xmath0 @xmath1such that @xmath2 as @xmath3  the sequence @xmath4 may be a deterministic one , obtained by using a numerical integration scheme ,  or obtained from monte - carlo methods involving an approximation to an integral , or a newton - raphson iteration to approximate the root of an equation but we will assume that we can sample from the distribution of @xmath5  for finite @xmath6 .  </S>",
    "<S> we propose a scheme for unbiased estimation of the limiting value @xmath7 ,  together with estimates of standard error and apply this to examples including numerical integrals , root - finding and option pricing in a heston stochastic volatility model.*keywords and phrases : * monte carlo simulation , unbiased estimates , numerical integration , finance , stochastic volatility model </S>"
  ]
}