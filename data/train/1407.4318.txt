{
  "article_text": [
    "in 2008 , i submitted a short semi - technical paper @xcite to the it transactions on the occasion of james l.  massey s @xmath0 birthday .",
    "one aim of the paper was to please jim who often expressed his liking for conceptual papers with simple technical content .",
    "although the paper was dropped for reasons that will become apparent , it achieved its aim of pleasing jim who repeatedly commented positively on the paper in the years before he passed away .",
    "i would speculate that jim also liked the pun on the `` role model '' metaphor in the paper mirrorring our relationship as past student to phd advisor .",
    "the present paper re - visits the ideas presented in @xcite and brings a fresh perspective on the subject . in the following section , we will introduce and discuss the role model strategy .",
    "section  [ sec : mcint ] shows how the solution of the role model convex program reduces to monte carlo integration in the non - parametric case , a much simpler technique well known in the bayesian community .",
    "this realization is the reason why the original paper project @xcite was dropped .",
    "section  [ sec : parcase ] discusses the parametric case , where the role model strategy may be of use after all , and why its relevance was not immediately obvious because we operate in the domain of discrete probability mass functions where parametric estimation is not normally considered .",
    "an example involving the constraint node operation in a factor graph based sudoku solver is presented where parametric estimation is useful , and other potential applications are discussed .",
    "the role model framework introduced in @xcite is illustrated in figure  [ fig : rolemodel ] .",
    "( 0,6 ) rectangle ( 2,7.5 ) ; at ( 1,6.75 ) discrete + memoryless + source ; ( 1,6 )  ( 1,5 ) ; at ( 1.1,5.5 ) @xmath1 ; ( 0,3.5 ) rectangle ( 2,5 ) ; at ( 1,4.25 ) discrete + memoryless + channel 1 ; ( 1,3.5 )  ( 1,2.5 ) ; at ( .9,3 ) @xmath2 ; ( 0,1 ) rectangle ( 2,2.5 ) ; at ( 1,1.75 ) discrete + memoryless + channel 2 ; ( 2,1.75 )  ( 3,1.75 ) ; at ( 2.5,1.85 ) @xmath3 ; ( 3,1 ) rectangle ( 5,2.5 ) ; at ( 4,1.75 ) estimator in + training ; ( 1,3 )  ( 2.5,3 )  ( 2.5,4.75 )  ( 3,4.75 ) ; ( 3,4 ) rectangle ( 5,5.5 ) ; at ( 4,4.75 ) role model + estimator ; ( 5,4.75 )  ( 6,4.75 ) ; ( 5,1.75 )  ( 6,1.75 ) ; at ( 6,4.75 ) @xmath4 ; at ( 6,1.75 ) @xmath5 ;    the discrete random variables @xmath1 , @xmath2 and @xmath3 form a markov chain for every @xmath6 . in the following , we drop the time index @xmath6 when not essential as our source and channels are assumed to be memoryless .",
    "the role model estimator is the optimal estimator for @xmath7 given the observation @xmath8 , which provides for every observation @xmath9 the full a - posteriori probability mass function @xmath10 over the domain of @xmath7 .",
    "our aim is to train an estimator for @xmath7 using the random variable @xmath11 , which is labeled `` estimator in training '' in the figure .",
    "the output of this estimator is labeled @xmath12 to reflect the fact that it is not necessarily the true a - posteriori probability mass function of @xmath7 given the observation @xmath11 , but an approximation thereof .",
    "the estimator in training is bayesian optimal if @xmath13 for every @xmath14 .",
    "the reason why @xmath15 is not available directly may be that the channel @xmath16 is unknown , or that the channel @xmath16 is known but that the resulting exact computation of @xmath15 is too complex for practical use .",
    "the particularity of the role model framework , in contrast to more complicated estimation frameworks such as those where the em and similar algorithms operate , is that we _ have access _ to the role model estimator and to its output to help design the estimator in training .",
    "this brings up the justified question of why we do nt just use the role model estimator directly instead of training an estimator based on @xmath17 .",
    "this may have several reasons :    * the observations @xmath2 and the resulting a - posteriori distributions @xmath10 may only be available during a training phase but not when our estimator goes live ; * the observations @xmath2 may only be available intermittently and our estimator in training is required to fill the gaps at times @xmath6 when @xmath2 is not available ; * the computation of @xmath10 may be too costly and only feasible offline during a simulation , or online intermittently for the purpose of training the estimator @xmath12 .",
    "we will later discuss a few technical examples in the context of iterative decoding and communication receivers where these conditions are fulfilled .",
    "@xcite gives hypothetical general examples outside the domain of communications where this scenario could also be of interest .",
    "what we call the _ role model strategy _ consists in aiming to minimize the expected divergence between the a - posteriori distribution @xmath10 computed by the role model estimator , and the distribution - valued heuristic output @xmath12 of the estimator in training , i.e. , to seek the @xmath12 for every @xmath18 that minimizes @xmath19 where we use the notation @xmath20 as in @xcite to signify the expected information divergence , where expectation is always taken on the joint distribution of the conditioning variables .",
    "the averaging required to compute this expression may be impractical , and hence we use the law of large numbers and the fact that all our processes are ergodic to state @xmath21 and approximate the quantity to be minimized by a time average of the divergence between the two distribution - valued outputs of our estimators . note that this may look like a frequentist / empirical approach , but we are at no point counting frequencies here , so the divergences being averaged are true divergences .",
    "it is only the average divergence that becomes an approximation if we perform the time averaging over a finite time interval of length @xmath22 rather than taking the limit as @xmath22 goes to infinity .",
    "we note that @xmath23 is convex in @xmath24 , and hence the set of distributions @xmath12 for every @xmath18 that we need can be sought using numerical convex optimization techniques .",
    "we devised the role model strategy as a heuristic approach to address this type of scenario .",
    "we had no expectation that this strategy could be optimal .",
    "the divergence that is minimized can not in general be reduced to zero , unless @xmath17 is a sufficient statistic for @xmath8 with respect to @xmath7 , which is never the case in the applications of interest .",
    "hence , this is not a system identification problem , where the estimator in training eventually models the role model .",
    "it therefore came as a surprise when we realized that the following holds :    if @xmath7 , @xmath8 and @xmath17 form a markov chain @xmath25 , then @xmath26 in particular , @xmath27 with equality if and only if @xmath28 for all @xmath18 such that @xmath29 .",
    "[ th : rolemodel ]    the theorem shows that the minimization we suggested converges to the optimal solution @xmath30 .",
    "hence , by imitating the role model , we converge to the best solution given our degraded observations , despite the fact that the role model we seek to imitate has better observations .",
    "the proof of the theorem is trivial and given in  @xcite .",
    "note that the theorem requires the markov property .",
    "a similar - looking result can be shown when the markov property does not hold by stating the identity @xmath31 effectively showing that @xmath32 minimizes the expression on the left , but this expression is only equal to @xmath23 when the markov condition is verified .",
    "it should be stressed that the appellation `` theorem '' was chosen for this result not on the basis of its mathematical intricacy , which it clearly lacks , but on the basis of its conceptual counter - intuitiveness ( from the author s perspective ) and central role it was thought to have in the applications under consideration . in the following section",
    ", we will show that the role model strategy reduces to a much simpler form that is well known in the bayesian estimation community , after discussing a class of applications and their constraints .",
    "the role model strategy will regain some meaning in the last section of the paper , where we show a class of applications where the simpler method does not apply but where the role model strategy remains a valid approach .",
    "initial interest for the scenario described was born out of efforts to design optimal post - processing procedures for sub - optimal components in iterative decoders . in the min - sum approximation of the sum product algorithm for decoding low - density parity - check ( ldpc ) codes ,",
    "the optimal bayesian operation under independence assumption in the constraint nodes of the decoder is replaced by a sub - optimal operation as illustrated in figure  [ fig : min - sum ] .",
    "( 1,1 ) rectangle ( 1.5,1.5 ) ; ( 0.2,0.2 )  ( 1,1 ) ; ( 0,1.25 )  ( 1,1.25 ) ; ( 0.2,2.3 )  ( 1,1.5 ) ; ( 1.5,1.25 )  ( 2.5,1.25 ) ; at ( .9,.7 ) @xmath33 ; at ( .2,1.25 ) @xmath34 ; at ( .3,2 ) @xmath35 ; at ( 2.5,1.25 ) @xmath36 ; at ( 2.5,1.2 ) @xmath24 ;    in the figure , the incoming observations @xmath37 are aggregated from channel observations during previous decoder iterations and are assumed independent , as is common practice in the design of belief propagation algorithms . in this case , the role model estimator , expressed as a mapping of log - likelihood ratios , is given by @xmath38 which is a fairly complex scalar function of multiple variables often considered too costly for implementation , while the estimator in training is a function @xmath12 of @xmath39 for this simple binary case , the optimal post - processing @xmath15 can be computed analytically @xcite under gaussian assumption and is fairly simple to compute when the variances of all incoming observations are identical . however , as soon as we deviate from this case , i.e. , when the incoming observations have different variances as is the case for irregular ldpc codes , or if we wish to go beyond the gaussian simplifying assumption , @xmath15 becomes very difficult to compute .",
    "hence the role model approach allows us to use numerical optimization algorithms to train a post - processing function to converge to the optimal estimator by running the sum - product rule and the estimator - in - training in parallel offline during a simulation , and then using the resulting low - complexity estimator online in the device ( e.g. , a mobile handset ) .",
    "another potential practical scenario consists in running both estimators in parallel in the device for a limited time while training the low complexity estimator , then shutting off the more complex estimator to save energy and conserve battery time .",
    "note that the random variable @xmath17 in this example is continuous but scalar .",
    "a fairly accurate estimator can be trained by quantizing @xmath17 finely and computing a lookup table of the a - posteriori distributions of @xmath7 for each quantized value of @xmath17 .",
    "we will now show that , for this non - parametric approach that aims to estimate the a - posteriori distributions of @xmath7 for all values of @xmath17 , the role model strategy reduces to a much simpler method well known in the bayesian community as a case of monte carlo integration .    for now , let us approach the optimization problem via the time - averaging formulation ( [ eq : time - average ] ) where we operate on a finite block length and drop the limit for simplicity .",
    "it is easy to see that the minimization with respect to the matrix @xmath40 for all @xmath41 and @xmath18 that we require simplifies to separate maximizations for each individual @xmath18 of the type @xmath42 we now take the liberty of ignoring the inequality constraints and setting up the lagrange conditions rather than the kkt conditions , because the solution will show that there is no danger of any variables becoming negative . for any @xmath18",
    ", we obtain by differentiating with respect to @xmath43 @xmath44 and hence @xmath45 the normalization condition requires that @xmath46 and the solutions clearly satisfy @xmath47 since they are obtained as an average of probabilities .",
    "we conclude that the solution of the role model strategy for any @xmath18 in the time averaging case is simply the time average of the a - posteriori distributions computed by the role model for all @xmath2 such that @xmath48 .",
    "again , we insist that this is not simply a frequentist / empirical approach as may appear .",
    "the quantities being added here are not numbers of occurrences but true bayesian a - posteriori distributions computed by the role model . the correct training for our estimator of @xmath7 for the symbol @xmath18 is to average the distribution - valued estimations of the role model componentwise over the time instances when @xmath18 is observed .    although we showed this for finite @xmath22",
    ", it is easy to see that the same holds in the limit as @xmath22 goes to infinity , and hence for the expectation @xmath23 .",
    "an alternative view is that the optimal strategy is to evaluate the sum @xmath49\\ ] ] as a time average , as briefly stated in @xcite .    in the min - sum algorithm",
    "discussed above , and in any similar applications where it is possible to adapt the full @xmath50 parameter set for @xmath24 , the role model strategy is an overkill and monte carlo integration gives the same solution by elementary averaging without resorting to complicated numerical convex optimization methods . in the next section , we will see that there is still a niche for the role model strategy when the full parameter set is too large for practice .",
    "when the full parameter set is not available , monte carlo integration is not an option and the role model strategy becomes a possibly interesting approach .",
    "while this is easy to state , it is not an obvious proposition because we do nt tend to think of parametric estimation for discrete random variables .",
    "indeed , we are not proposing to constrain the conditional probability mass functions @xmath24 to be parametric distributions in the sense that a gaussian density is a parametric probability density function .",
    "rather , as we will see in our examples , there are scenarios where the domain @xmath51 of @xmath17 makes it impractical to estimate an a - posteriori model @xmath12 for every possible @xmath18 . in such scenarios ,",
    "we may be constrained to using a parametric function of @xmath17 , i.e. , @xmath52 .",
    "in such a case , the role model strategy loses its optimality as the space of possible functions @xmath53 will not in general include the mapping that makes @xmath24 converge to @xmath54 . hence , the role model strategy in this context is a purely heuristic approach that may or may not exhibit advantages or weaknesses with respect to other heuristic optimization criteria and can be judged solely on the basis of its numerical performance .    an early example applying the role model strategy in a semi - parametric manner was described in @xcite for a hypothetical rank - based message - passing decoder for non - binary ldpc codes .",
    "in fact , a more pertinent question than that studied in @xcite would be to design post - processing operations for the suboptimal operations in the extended min - sum ( ems ) algorithm @xcite , a reduced complexity version of the sum - product algorithm for non - binary ldpc codes . however , the ems algorithm is quite a difficult construct to understand , so that a full study of parametric post - processing , while practically relevant , would obscure rather than clarify matters in the context of this paper .",
    "hence , we have chosen to treat an alternative example of lesser practical relevance but that is easier to understand .",
    "the example is the use of graph - based decoding for solving soft sudoku puzzles .",
    "we omit an introduction to universally known sudoku puzzles and refer the reader to @xcite for futher details and definitions . by `` soft '' sudoku , we mean puzzles that receive general noisy observations of the correct entries in the grid rather than observations that are either correct or erased .",
    "observations for every entry in the grid are available as a - posteriori probability mass functions over the 9-ary alphabet using a known and accurate channel model .",
    "sudoku puzzles can be represented as a factor graph where every one of the 81 variables is connected to 3 constraints and every constraint ( 9 rows , 9 columns and 9 subgrids ) involves 3 variables .",
    "the factor graph of a @xmath55 sudoku defined over the alphabet @xmath56 is represented in figure  [ fig : sudoku ] .",
    "sudoku solver ]    our interest in the context of this paper is for the operation in the constraint nodes .",
    "constraint nodes receive 9 a - posteriori observations of their participating variables , which we assume to be independent in line with common practice in belief propagation algorithms .",
    "the constraint node s task is to return to each variable its a - posteriori probability given the observations of the remaining 8 variables in the constraint .",
    "let us denote by @xmath57 $ ] the @xmath58 matrix of incoming messages into a constraint node , where @xmath59 where @xmath60 generically denotes the set of channel observations that led to the incoming message on the @xmath61-th branch into the constraint node .",
    "it is clear that the probability that the first variable in the constraint has value @xmath62 , given observations of the other 8 variables , is the sum of probabilities of all configurations of the other 8 variables that do nt include the value @xmath62 .",
    "if we denote by @xmath63 the matrix @xmath64 with its @xmath61-th row and @xmath62-th column removed , we can state that an outgoing message component from the constraint node can be expressed as @xmath65 where @xmath66 denotes the cauchy permanent @xcite of the matrix @xmath67 .",
    "the permanent is a patently difficult function to compute and the best algorithms known compute an approximation of the permanent in probabilistically polynomial time , which polynomial is considerably larger than @xmath68 for sizes @xmath69 of interest to us .",
    "we can hence assume that @xmath70 operations are needed to compute the permanent above .",
    "this is a large number of operations for every node at every iteration of a belief propagation solver , but well within the range of offline simulation , so a perfect testing ground for our role model strategy .",
    "we can now try to replace the permanent computation by any approximation and use the role model strategy to design post - processing functions for the approximation .",
    "for example , we can opt to do the following :    * take the 3 largest elements in each row of @xmath64 and replace the remaining entries by a uniform distribution adding to the same sum to obtain the matrix @xmath71 , * re - write the matrix @xmath71 as @xmath72 where @xmath73 contains uniform rows whose values are consistent with the uniform tails produced in the previous step , and @xmath74 contains the non - uniform values minus the uniform tail value for those head entries not in the tails , and zero where the tail entries of @xmath71 are ; * we now approximate the required permanent as @xmath75 * we compute the elements of the outgoing matrix using this approximation and normalize the rows so they sum to 1 and look like true probability mass functions .",
    "this is much easier to compute because @xmath74 is sparse and @xmath73 has uniform rows , but it is a very poor approximation because the permanent of a sum of matrices is not at all well approximated by the sum of their permanents .",
    "figure  [ fig : exit ] shows the exit chart of a factor - graph based sudoku solver , where the two red curves correspond to the optimal and sub - optimal constraint node operations , and the blue curves correspond to the constraint node operations for various channel signal to noise ratios ( snr ) .",
    "surprisingly , the red curves are not too far apart in particular in the top half of the exit chart , indicating that despite the very rough approximation we are using , the result is sufficiently informative to achieve acceptable performance , and the full complexity permanent computation should only be used in the early iterations .",
    "now for the application of the role model strategy . the observations for our role model postprocessor in this case consist of the rows of the outgoing matrix computed using the permanent approximations .",
    "the observation space is the set of 9-ary probability distributions .",
    "this is a continuous space and is no longer scalar like in the binary min - sum case .",
    "hence , we can not simply quantize it finely in order to apply monte carlo integration and converge to the optimal a - posteriori estimation . what we can do , however , is to apply arbitrary transformations to the probability vectors .",
    "for example , we can take replace the sum @xmath76 by a weighted sum @xmath77 and optimize the weights @xmath78 .",
    "hence the problem becomes one of finding the best parameters @xmath78 to optimize the solver performance .",
    "the problem is that solver performance itself is difficult to measure and can only be optimized by exhaustive search algorithm .",
    "the role model strategy in this case yields a tracktable convex optimization procedure where @xmath23 is the optimization metric .",
    "@xmath79 here is the correct a - posteriori distribution obtained with the true permanent , and @xmath24 is the @xmath78-corrected result of the sum of permanents approximation .",
    "note that with this approach , we have lost any claim of optimality , and anyone who prefers another metric over ours is entitled to do so .",
    "the only valid criterion for comparing metrics is simulated performance of the resulting optimized solvers .",
    "we have described the role model strategy as a convex program whose solution is the bayesian optimal estimator in training .",
    "we showed that the strategy reduces to monte carlo integration in the non - parametric case , and discussed the parametric case with an example where the strategy can be used but monte carlo integration would not work .",
    "in fact , applications of post - processing optimization for sub - optimal estimators are burgeoning in the literature and many metrics have been proposed for optimizing the post - processing stage of , say , the ems algorithm for non - binary ldpc codes , demodulators for bit - interleaved coded modulation ( bicm ) and many others . some , such as @xcite claim theoretical motives for their approaches , while others , such as @xcite , are self - declaredly heuristic in their approach . given our analysis so far and the fact that these are all parametric models , we tend to agree with the latter .",
    "i learned that `` my '' role model strategy reduces to elementary monte carlo integration upon joining the university of cambridge in a conversation with my then officemate and now friend simon hill , a fact that earns him my warmest gratitude as well as my sincere apologies for the inappropriate language he may have heard when i found out .",
    "i also wish to thank to the then associate editor of the it transactions michael gastpar who handled my submission and the three gracious anonymous reviewers who i realize put considerable effort into providing constructive feedback , with apologies for never writing back to explain that i was not revising the paper as a result of the conversation mentioned above .",
    "d.  declercq and m.  p. fossorier , `` decoding algorithms for nonbinary ldpc codes over gf(q ) , '' _ ieee trans .",
    "_ , vol .",
    "55 , no .  4 , pp . 633643 , apr . 2007 .",
    "[ online ] .",
    "available : http://publi-etis.ensea.fr/2007/df07\"[http://publi-etis.ensea.fr/2007/df07 \" ]"
  ],
  "abstract_text": [
    "<S> we re - visit the role model strategy introduced in an earlier paper , which allows one to train an estimator for degraded observations by imitating a reference estimator that has access to superior observations . </S>",
    "<S> we show that , while it is true and surprising that this strategy yields the optimal bayesian estimator for the degraded observations , it in fact reduces to a much simpler form in the non - parametric case , which corresponds to a type of monte carlo integration . </S>",
    "<S> we then show an example for which only parametric estimation can be implemented and discuss further applications for discrete parametric estimation where the role model strategy does have its uses , although it loses claim to optimality in this context . </S>"
  ]
}