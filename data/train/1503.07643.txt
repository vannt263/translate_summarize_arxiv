{
  "article_text": [
    "suppose that we have independent observations @xmath2 from a probability density @xmath3 that belongs to a parametric model @xmath4 , where @xmath5 is an unknown @xmath6-dimensional parameter and @xmath7 is the parameter space .",
    "the random variable @xmath1 to be predicted is independently distributed according to a density @xmath8 in a parametric model @xmath9 , possibly different from @xmath4 , with the same parameter @xmath10 .",
    "the objective is to construct a predictive density @xmath11 for @xmath1 by using @xmath12 .",
    "the performance of @xmath13 is evaluated by the kullback ",
    "leibler divergence @xmath14 from the true density @xmath8 to the predictive density @xmath11 .",
    "the risk function is given by @xmath15 = \\iint p(x^n \\mid \\theta ) { \\displaystyle \\tilde{p}}(y \\mid \\theta ) \\log \\frac{{\\displaystyle \\tilde{p}}(y \\mid \\theta)}{\\hat{p}(y;x^n ) } { \\mbox{d}}y { \\mbox{d}}x^n.\\ ] ] it is widely recognized that plug - in densities @xmath16 constructed by replacing the unknown parameter @xmath10 by an estimate @xmath17 may not perform very well and that bayesian predictive densities @xmath18 constructed by using a prior @xmath19 perform better than plug - in densities . if the value of @xmath10 is given , there is no specific meaning of considering the conditional density of @xmath1 given @xmath20 since the obvious relation @xmath21 holds .",
    "however , if @xmath10 is unknown , bayesian predictive densities @xmath22 constructed by introducing a prior density @xmath23 on the parameter space are useful to approximate the true density @xmath24 as discussed in @xcite and @xcite .",
    "in fact , there exists a predictive density whose asymptotic risk is smaller than that of a plug - in density unless the mean mixture curvature of the model manifold vanishes , see @xcite and @xcite for details .",
    "the choice of @xmath19 becomes important especially when the sample size @xmath25 is not very large .",
    "although the jeffreys prior is a widely known default prior , it does not perform satisfactorily especially when the unknown parameter is multidimensional as jeffreys himself pointed out .",
    "@xcite constructed a bayesian predictive density incorporating the advantage of shrinkage methods for the multivariate normal model .",
    "see also @xcite for useful results for the normal model .    in the conventional setting in which the distributions of @xmath26 , @xmath27 , and @xmath1 are",
    "the same , asymptotic theory of prediction based on general parametric models has been studied by using the framework of information geometry , see @xcite . in information geometry ,",
    "a parametric statistical model is regarded as a differentiable manifold , which we call the model manifold , and the parameter space is regarded as a coordinate system of the manifold , see @xcite .",
    "the fisher  rao metric is a riemannian metric based on the fisher information matrix on the model manifold .",
    "the jeffreys prior @xmath28 corresponds to the volume element of the model manifold associated with the fisher  rao metric .",
    "when the distributions of @xmath26 , @xmath27 , and @xmath1 are the same , the asymptotic difference between the risks of @xmath29 and @xmath30 is given by @xmath31 \\notag\\\\ = & \\frac{\\displaystyle \\delta \\left(\\frac{\\pi}{{\\pi_\\mathrm{j}}}\\right)}{\\displaystyle \\left ( \\frac{\\pi}{{\\pi_\\mathrm{j } } } \\right ) } - \\frac{1}{2 } \\sum_{i=1}^d \\sum_{j=1}^d g^{ij } \\frac{\\displaystyle \\partial_i \\left(\\frac{\\pi}{{\\pi_\\mathrm{j}}}\\right ) \\partial_j \\left(\\frac{\\pi}{{\\pi_\\mathrm{j}}}\\right ) } { \\displaystyle \\left(\\frac{\\pi}{{\\pi_\\mathrm{j}}}\\right)^2 } + { \\mathrm{o}}(1 ) = 2 \\frac{\\displaystyle \\delta \\left(\\frac{\\pi}{{\\pi_\\mathrm{j}}}\\right)^{\\frac{1}{2 } } } { \\displaystyle \\left ( \\frac{\\pi}{{\\pi_\\mathrm{j } } } \\right)^{\\frac{1}{2 } } } + { \\mathrm{o}}(1 ) , \\label{main0}\\end{aligned}\\ ] ] where @xmath32 denotes @xmath33 , @xmath34 , @xmath35 denotes the @xmath36-element of the inverse of the @xmath37 matrix @xmath38 , and @xmath39 is the laplacian , see @xcite .",
    "the laplacian @xmath39 on a riemannian manifold endowed with a metric @xmath40 is defined by @xmath41 where @xmath42 is the determinant of the @xmath37 matrix @xmath38 , @xmath43 is a smooth real function on @xmath7 , and @xmath44 denotes the covariant derivative , defined in the next section .",
    "the indices @xmath45 run from @xmath46 to @xmath6 .",
    "note that both the definition of the laplacian and the definition @xmath47 that differs in sign are widely adopted in the mathematics literature , although it is confusing .",
    "because of , if there exists a non - constant positive superharmonic function @xmath43 , i.e.  a non - constant positive function satisfying @xmath48 for every @xmath10 , on the model manifold , then the bayesian predictive density based on the prior density defined by @xmath49 asymptotically dominates that based on the jeffreys prior . here , the riemannian geometric structure of the model manifold based on the fisher  rao metric plays a fundamental role .    in practical applications , it often occurs that observed data @xmath26 , @xmath50 , and the target variable @xmath1 to be predicted have different distributions .",
    "regression models are a typical example .",
    "suppose that we observe @xmath51 , where @xmath52 is a given @xmath53 matrix @xmath54 , and predict @xmath55 , where @xmath56 is a given @xmath57 matrix and @xmath58 is an unknown parameter .",
    "then , the fisher information matrices for the same parameter @xmath10 based on @xmath59 and @xmath60 are different .",
    "similar situations also occur in nonlinear regression problems . @xcite and @xcite showed that shrinkage priors are useful for constructing bayesian predictive densities for linear regression models when the observations are normally distributed with known variance .",
    "however , it has been difficult to construct useful priors for general models other than the normal models when @xmath0 and @xmath1 have different distributions .    in the present paper ,",
    "we study asymptotic theory for the setting in which @xmath26 , @xmath27 , and @xmath1 have different distributions .",
    "although several asymptotic properties of predictive distributions for such a setting are studied by @xcite , the result corresponding to has not been explored .",
    "the generalization is not straightforward because two different differential geometric structures , one for @xmath3 and the other for @xmath8 , such as the fisher  rao metrics exist in the present setting .",
    "we introduce a new metric @xmath61 , which we call the predictive metric , depending on both @xmath3 and @xmath8 .",
    "the predictive metric @xmath61 and the volume element @xmath62 of it correspond to the fisher  rao metric and the jeffreys prior in the conventional setting .    in section 2",
    ", we obtain an expansion of the difference of the risk functions of bayesian predictive densities . each term in the expansion is represented by using geometrical quantities and is invariant with respect to parameter transformations . in section 3 , we introduce the predictive metric @xmath61 and evaluate the asymptotic risk difference between a bayesian predictive density based on a prior @xmath19 and that based on the volume element prior @xmath62 of the predictive metric @xmath61 .",
    "the asymptotic risk difference is represented by using the laplacian associated with the predictive metric @xmath61 . in section 4 ,",
    "we consider three examples and construct superior priors by using the formula obtained in section 3 .",
    "first , we prepare several information geometrical notations to be used . in the following ,",
    "the quantities associated with the model @xmath63 are denoted without tilde , and those associated with the model @xmath64 are denoted with tilde .",
    "we put @xmath65 and @xmath66 .",
    "the fisher  rao metrics on the model manifolds @xmath63 and @xmath67 are given by @xmath68 respectively .",
    "the @xmath36-elements of the inverses of the @xmath37 matrices @xmath69 and @xmath70 are denoted by @xmath71 and @xmath72 , respectively .",
    "we define @xmath73 and @xmath74 here , @xmath75 are the e - connection coefficients , @xmath76 are the m - connection coefficients , and @xmath77 are the riemannian connection coefficients .",
    "the relations @xmath78 represent the duality between @xmath75 and @xmath76 with respect to the metric @xmath79 , and the duality between @xmath80 and @xmath81 with respect to the metric @xmath82 , respectively .",
    "covariant derivatives @xmath83 , @xmath84 , and @xmath85 of a vector field @xmath86 with respect to the connection coefficients @xmath87 , @xmath88 , and @xmath89 are defined by @xmath90 , @xmath91 , and @xmath90 , respectively , where @xmath92 , @xmath93 , and @xmath94 . in the same way , the covariant derivatives @xmath95 , @xmath96 , and @xmath97 , with respect to the connection coefficients @xmath98 , @xmath99 , and @xmath100 , are defined .",
    "theorem [ riskdiff ] below is used in the following sections .",
    "[ riskdiff ] the difference between the risk functions of bayesian predictive densities @xmath101 and @xmath102 based on priors @xmath103 and @xmath104 , respectively , is given by @xmath105 \\nonumber \\\\ = & \\left ( \\frac{1}{2 } \\sum_{i , j }   { \\tilde{g}_{ij}^ { } } { u_\\pi}^i { u_\\pi}^j + \\sum_{i , j , k }   { \\tilde{g}_{ij}^ { } } { g_{}^{jk } } { \\widetilde{\\nabla}_{k}^{(\\mathrm{e } ) } } { u_\\pi}^i \\right ) \\notag \\\\ & - \\left ( \\frac{1}{2 } \\sum_{i , j } { \\tilde{g}_{ij}^ { } } { u_{\\pi'}}^i { u_{\\pi'}}^j + \\sum_{i , j , k }   { \\tilde{g}_{ij}^ { } } { g_{}^{jk } } { \\widetilde{\\nabla}_{k}^{(\\mathrm{e } ) } } { u_{\\pi'}}^i \\right ) + \\mathrm{o}(1 ) , \\label{6 - 2}\\end{aligned}\\ ] ] where @xmath106    the proof of theorem [ riskdiff ] is given in the appendix .",
    "in this section , we introduce a new metric defined by @xmath107 which we call the predictive metric . since @xmath61 is positive definite , it can be adopted as a riemannian metric on @xmath7 .",
    "it will be shown that the predictive metric @xmath61 , the corresponding volume element @xmath108 and the laplacian @xmath109 based on @xmath61 play essential roles corresponding to those played by the fisher  rao metric @xmath40 , the jeffreys prior @xmath110 , and the laplacian @xmath39 based on @xmath40 in the conventional setting where @xmath111 . here ,",
    "@xmath112 , @xmath113 , and @xmath114 denote determinants of @xmath37 matrices @xmath115 , @xmath38 , and @xmath116 , respectively .",
    "the @xmath36-element of the inverse of the @xmath37 matrix @xmath115 is given by @xmath117 .    here",
    ", we give an intuitive meaning of the predictive metric @xmath61 by a nonrigorous argument . in the standard estimation theory , the fisher - rao metric @xmath40 , which is the fisher information matrix , corresponds to the inverse of the asymptotic variance of the maximum likelihood estimator . in the setting we consider ,",
    "the asymptotic variance of the maximum likelihood estimator based on @xmath20 is @xmath118 , where @xmath119 is the @xmath37 matrix @xmath38 , and the asymptotic variance of the maximum likelihood estimator based on both of @xmath20 and @xmath1 is @xmath120 , where @xmath121 is the @xmath37 matrix @xmath116 .",
    "the inverse of the reduction of the asymptotic variance by observing @xmath1 in addition to @xmath26 @xmath122 are given by @xmath123 , as we see in example 1 in section 4 , corresponding to the predictive metric @xmath124 .",
    "the riemannian connection coefficients with respect to the predictive metric @xmath61 are given by @xmath125 and we put @xmath126 .",
    "then , @xmath127 in the same way , we have @xmath128 thus , @xmath129 the laplacian @xmath109 with respect to the predictive metric @xmath61 is defined by @xmath130 where @xmath131 , and @xmath43 is a real smooth function on @xmath7 .    by using these quantities ,",
    "we obtain the following theorem corresponding to in the conventional setting .",
    "[ maintheorem ] the difference between the risk functions of bayesian predictive densities @xmath101 based on a @xmath103 and @xmath132 based on @xmath133 is given by @xmath134 \\notag\\\\ = & \\frac{\\displaystyle { \\mathring{\\delta}}\\left(\\frac{\\pi}{{\\pi_\\mathrm{p}}}\\right)}{\\displaystyle \\left ( \\frac{\\pi}{{\\pi_\\mathrm{p } } } \\right ) } - \\frac{1}{2 } \\sum_{i , j } { \\mathring{g}_{}^{ij } } \\frac{\\displaystyle \\partial_i \\left(\\frac{\\pi}{{\\pi_\\mathrm{p}}}\\right ) \\partial_j \\left(\\frac{\\pi}{{\\pi_\\mathrm{p}}}\\right ) } { \\displaystyle \\left(\\frac{\\pi}{{\\pi_\\mathrm{p}}}\\right)^2 } + { \\mathrm{o}}(1 ) = 2 \\ ; \\frac{\\displaystyle { \\mathring{\\delta}}\\left(\\frac{\\pi}{{\\pi_\\mathrm{p}}}\\right)^{\\frac{1}{2 } } } { \\displaystyle \\left ( \\frac{\\pi}{{\\pi_\\mathrm{p } } } \\right)^{\\frac{1}{2 } } } + { \\mathrm{o}}(1).\\label{main}\\end{aligned}\\ ] ]    the proof of theorem [ maintheorem ] is given in the appendix .",
    "if there exists a positive constant @xmath135 such that @xmath136 , we identify the prior @xmath137 with @xmath19 because the posterior densities based on them are identical .",
    "in fact , the risk difference between @xmath19 and @xmath138 coincides with that between @xmath137 and @xmath138 .",
    "[ usefulcor ] if a positive function @xmath139 is superharmonic with respect to the predictive metric @xmath124 , i.e.  @xmath140 for every @xmath141 , and the strict inequality holds at a point in @xmath7 , then the bayesian predictive density based on the prior density @xmath142 asymptotically dominates the bayesian predictive density @xmath132 based on the prior density @xmath143 .",
    "if there exists a non - constant positive superharmonic function @xmath139 with respect to the predictive metric @xmath124 , then the bayesian predictive density based on the prior density @xmath144 @xmath145 asymptotically dominates @xmath132 .    the first statement is a straightforward conclusion from theorem [ maintheorem ] .",
    "we show the second statement .",
    "the function @xmath146 is superharmonic because @xmath147 if @xmath139 is a positive superharmonic function .",
    "the strict inequality holds at @xmath10 satisfying @xmath148 for any @xmath149 .",
    "such @xmath10 exists since @xmath139 is a non - constant function .",
    "thus , the second statement follows from the first statement .    by setting @xmath150",
    ", it follows from corollary [ usefulcor ] that the bayesian predictive density based on the prior @xmath151 asymptotically dominates the bayesian predictive density based on @xmath138 if @xmath139 is a non - constant positive superharmonic function .",
    "note that corollary [ usefulcor ] also holds if we replace the predictive metric @xmath152 with another metric @xmath153 satisfying @xmath154 with a positive constant @xmath135 .",
    "this is because the volume element with respect to @xmath153 is proportional to that with respect to @xmath152 and the relation @xmath155 holds , where @xmath156 is the laplacian with respect to @xmath153 .",
    "in this section , we see three examples . we verify that the results in the previous sections are consistent with several known results in examples 1 and 2 and",
    "obtain some new results in examples 2 and 3 .",
    "example 1 .",
    "normal models    suppose that @xmath0 is distributed according to the @xmath6-dimensional normal distribution @xmath157 with mean vector @xmath158 and covariance matrix @xmath159 and that @xmath1 is distributed  according to the @xmath6-dimensional normal distribution @xmath160 with the same mean vector @xmath158 and possibly different covariance matrix @xmath161 . here , @xmath158 is the unknown parameter and @xmath162 and @xmath163 are known .",
    "the fisher information matrix for @xmath164 is @xmath165 and that for @xmath164 is @xmath166 , where @xmath167 and @xmath168 are inverse matrices of @xmath167 and @xmath168 , respectively .",
    "since the coefficients of the predictive metric @xmath169 do not depend on @xmath158 , the volume element with respect to the predictive metric is @xmath170 which is the uniform distribution @xmath171 .",
    "@xcite and @xcite considered shrinkage pri - ors for this model .",
    "the bayesian predictive density @xmath29 dominates @xmath172 based on the uniform measure @xmath173 if @xmath174 is a superharmonic function on the euclidean space @xmath175 endowed with the metric @xmath176 , see theorem 3.2 in @xcite .",
    "this result holds for every positive integer @xmath25 .    since @xmath177^{-1 }",
    "( n{g_{}^{}})^{\\frac{1}{2 } } \\\\[2pt ] = & ( n{g_{}^{}})^{\\frac{1}{2 } } \\left [ i - i + ( n{g_{}^{}})^ { -\\frac{1}{2 } } { \\tilde{g}_{}^ { } } ( n{g_{}^{}})^ { -\\frac{1}{2 } } + \\mathrm{o}(n^{-2 } ) \\right]^{-1 } ( n{g_{}^{}})^{\\frac{1}{2 } }    \\\\[2pt ] = & n^2 \\ , { g_{}^ { } } \\ , { \\tilde{g}_{}^{-1 } } \\ , { g_{}^ { } } + \\mathrm{o}(n)\\end{aligned}\\ ] ] corresponds to the predictive metric @xmath124 , theorem [ maintheorem ] is consistent with theoretical and numerical results in @xcite and @xcite .",
    "example 2 .",
    "location - scale models    suppose that @xmath178 and @xmath179 are probability densities on @xmath180 that are symmetric about the origin .",
    "let @xmath181 where @xmath182 and @xmath183 are unknown parameters .",
    "suppose that we have a set of @xmath25 independent observations @xmath184 distributed according to @xmath185 .",
    "the variable @xmath1 to be predicted is independently distributed according to @xmath186 .",
    "the objective is to construct a prior @xmath19 for a bayesian predictive density @xmath187 .",
    "the fisher  rao metrics on the model manifolds @xmath188 and @xmath189 are @xmath190 respectively , where @xmath191 and @xmath192 are positive constants depending on @xmath178 , and @xmath193 and @xmath194 are positive constants depending on @xmath179 .",
    "the predictive metric is given by @xmath195 define @xmath196 by rescaling the location parameter @xmath158 .",
    "we call this coordinate system @xmath197 the upper - half plane coordinates .",
    "then , the predictive metric is represented by @xmath198 coinciding with the metric on the hyperbolic plane @xmath199 , which is a 2-dimensional complete manifold with constant sectional curvature @xmath200 .",
    "thus , the model manifold endowed with the predictive metric @xmath124 is isometric to @xmath199 .    the volume element with respect to the predictive metric @xmath124",
    "is given by @xmath201 and coincides with the jeffreys priors @xmath202 for @xmath185 and @xmath203 for @xmath186 .",
    "the laplacian on the model manifold endowed with the predictive metric @xmath124 is given by @xmath204 by corollary [ usefulcor ] , the bayesian predictive density @xmath205 based on the prior @xmath206 asymptotically dominates @xmath207 based on @xmath208 because @xmath209 by theorem [ maintheorem ] , the asymptotic risk difference is @xmath210 \\notag\\\\ = & 2 \\ ; \\frac{\\displaystyle { \\mathring{\\delta}}\\left(\\frac{\\pi_\\mathrm{r}}{{\\pi_\\mathrm{p}}}\\right)^{\\frac{1}{2 } } } { \\displaystyle \\left ( \\frac{\\pi_\\mathrm{r}}{{\\pi_\\mathrm{p } } } \\right)^{\\frac{1}{2 } } } + { \\mathrm{o}}(1 ) = 2 \\ ; \\frac{{\\mathring{\\delta}}\\sigma^{\\frac{1}{2 } } } { \\sigma^{\\frac{1}{2 } } } + { \\mathrm{o}}(1 ) = - \\frac{\\tilde{b}}{2 b^2 } + { \\mathrm{o}}(1 )",
    ". \\label{riskr}\\end{aligned}\\ ] ] in fact , it can be shown that the bayesian predictive density @xmath205 exactly dominates @xmath207 for finite @xmath25 because @xmath208 is the left invariant prior and @xmath211 is the right invariant prior with respect to the location - scale group .",
    "the bayesian procedures based on the right invariant prior dominate those based on the left invariant prior in many problems associated with group models as shown in @xcite .",
    "the prior @xmath211 is also derived as a reference prior , see @xcite .",
    "furthermore , as we see below , the bayesian predictive density @xmath212 based on the prior @xmath213 defined by @xmath214 asymptotically dominates @xmath205 and thus also dominates @xmath207 .    to clarify the meaning of the prior @xmath213",
    ", we introduce another coordinate system on the model manifold .",
    "let @xmath215 be the riemannian distance based on the predictive metric @xmath124 between a point @xmath216 and an arbitrary fixed point @xmath217 on @xmath218 .",
    "the direction of @xmath216 from @xmath217 is represented by a point @xmath219 on the unit circle in the tangent space at @xmath217 .",
    "then , the point @xmath216 is represented by @xmath220 and @xmath219 , see e.g.  @xcite p.  152 .",
    "this coordinate system @xmath221 is called the geodesic polar coordinates .",
    "then , the predictive metric is given by @xmath222 the laplacian is represented by @xmath223 where @xmath224 is the laplacian on the unit circle in the tangent space at @xmath217 , see e.g. @xcite p.  158 .",
    "when the upper - half plane coordinate system is adopted , the riemannian distance @xmath215 between @xmath197 and @xmath225 is represented by @xmath226 see e.g.  @xcite p. 176 .",
    "thus , in the original coordinate system @xmath227 , theriemannian distance @xmath215 between and @xmath228 and @xmath229 is @xmath230 thus , the ratio of prior densities is given by @xmath231 note that @xmath232 depends on @xmath227 only through @xmath233 defined by .",
    "thus , from , , and theorem [ maintheorem ] , we have @xmath234 \\notag\\\\ = & 2 \\ ; \\frac{\\displaystyle { \\mathring{\\delta}}\\left(\\frac{\\pi_{c,\\kappa}}{{\\pi_\\mathrm{p}}}\\right)^{\\frac{1}{2 } } } { \\displaystyle \\left ( \\frac{\\pi_{c,\\kappa}}{{\\pi_\\mathrm{p } } } \\right)^{\\frac{1}{2 } } } + { \\mathrm{o}}(1 ) = - \\frac{\\tilde{b}}{b^2 } \\left\\ { \\frac{1}{2 } + c \\frac{\\pi_{c,\\kappa}}{{\\pi_\\mathrm{p } } } + \\frac{3}{2 } ( 1-c^2 ) \\left(\\frac{\\pi_{c,\\kappa}}{{\\pi_\\mathrm{p } } } \\right)^2 \\right\\ } + { \\mathrm{o}}(1 ) \\notag \\\\ = & - \\frac{\\tilde{b}}{b^2 } \\left\\ { \\frac{1}{2 } + c \\frac{1}{\\cosh \\rho + c } + \\frac{3}{2 } ( 1 - c^2 ) \\frac{1}{(\\cosh",
    "\\rho + c)^2 } \\right\\ } + { \\mathrm{o}}(1 ) , \\label{riskhs}\\end{aligned}\\ ] ] and is smaller than when @xmath235 and @xmath236 .",
    "the asymptotic risk difference can also be derived from and the laplacian in the original coordinate system .    by corollary [ usefulcor",
    "] , the bayesian predictive density @xmath237 @xmath238 asymptotically dominates @xmath239 since the function is superharmonic for @xmath240 .",
    "however , @xmath237 asymptotically dominates @xmath241 only when @xmath242 .",
    "+ { \\mathrm{o}}(1 ) = -(\\tilde{b}/b^2 ) \\ { 1/2 + c ( \\pi/{\\pi_\\mathrm{p } } ) + ( 3/2 ) ( 1-c^2 ) ( \\pi/{\\pi_\\mathrm{p } } ) \\}^2 $ ] for bayesian predictive densities based on @xmath138 , @xmath211 , @xmath243 , @xmath244 , and @xmath245 .",
    "we put @xmath246 just for simplicity . ]",
    "several properties of the function are discussed in @xcite .",
    "as @xmath247 , the prior @xmath248 converges to the right invariant prior @xmath211 , because @xmath249 when @xmath247 . here , priors are identified up to a positive multiplicative constant .",
    "as @xmath250 , the prior @xmath248 converges to @xmath251 because @xmath252    when @xmath250 .",
    "the prior density with respect to the rescaled parameter @xmath197 defined by is given by @xmath253 note that the cauchy prior for @xmath254 , discussed by jeffreys and many researchers , appears in .",
    "thus , the class @xmath248 of priors bridges the right invariant prior @xmath211 , coinciding with the reference prior , and the cauchy prior @xmath243 .",
    "figure [ fig : location - scale - risk ] illustrates the difference between the risk functions of bayesian predictive densities based on @xmath211 , @xmath243 , @xmath255 , and @xmath256 and the risk function of @xmath239 .",
    "the risk functions of the right invariant prior @xmath211 and the cauchy prior @xmath243 are uniformly smaller than that of @xmath138 .",
    "the asymptotic risk of the cauchy prior @xmath243 coincides with that of @xmath211 .",
    "furthermore , the asymptotic risks of @xmath257 and @xmath258 are smaller than that of @xmath211 for every @xmath228 . therefore , the use of @xmath259 @xmath260 is recommended .",
    "the risk of @xmath255 is smaller than that of @xmath261 when @xmath220 is small , and vice versa .",
    "thus , there does not exist a unique best value of @xmath135 .",
    "the choice of the value of @xmath236 is arbitrary because it corresponds to the center of shrinkage .",
    "finite - sample decision theoretic properties such as admissibility of bayesian predictive densities @xmath262 based on proposed priors @xmath263 @xmath264 require further research .",
    "example 3 .",
    "poisson models    suppose that @xmath265 @xmath266 are independently distributed according to the poisson distribution @xmath267 with mean @xmath268 and that @xmath269 @xmath266 are independently distributed according to the poisson distribution @xmath270 with mean @xmath271 . here , @xmath272 are known positive constants .",
    "the unknown parameter is @xmath273 .",
    "the objective is to construct a predictive density for @xmath1 by using @xmath0 .",
    "this problem in the conventional setting , in which @xmath274 , is studied in @xcite .",
    "the laplacian @xmath281 based on the predictive metric @xmath124 is given by @xmath282 where @xmath43 is a smooth real function of @xmath283 .",
    "define @xmath284 then , from @xmath285 and @xmath286 we have @xmath287 since @xmath288 is a non - constant positive superharmonic function of @xmath283 , the bayesian predictive density @xmath289 based on @xmath290 asymptotically dominates @xmath207 by corollary  [ usefulcor ] .",
    "the model manifold endowed with the predictive metric @xmath61 is isometric to the first orthant @xmath291 of the euclidean space @xmath292 , as we see below .",
    "define @xmath293 then , @xmath294 thus , from , the coefficients of the metric with respect to @xmath295 are given by @xmath296 this coincides with the usual metric on @xmath297 .    here , the function @xmath298 of @xmath299 is the green function of the heat equation on @xmath292 and plays an essential role in bayesian methods for model manifolds isometric to the euclidean space .",
    "for example ,",
    "the prior density @xmath300 for the @xmath6-dimensional normal model @xmath301 , where @xmath158 is the @xmath6-dimensional unknown mean vector and @xmath302 is the @xmath303 identity matrix , is known as the stein prior .",
    "the bayesian predictive density based on @xmath208 is @xmath304 where @xmath305 .",
    "the bayesian predictive density based on @xmath290 is @xmath306 we have the asymptotic risk difference @xmath307 \\notag\\\\ = & \\frac{{\\mathring{\\delta}}(\\pi_\\mathrm{s}/\\pi_\\mathrm{p})}{\\pi_\\mathrm{s}/\\pi_\\mathrm{p } } - \\frac{1}{2 } \\sum_{i , j } { \\mathring{g}_{}^{ij } } \\frac { \\partial_i ( \\pi_\\mathrm{s}/\\pi_\\mathrm{p } ) \\partial_j ( \\pi_\\mathrm{s}/\\pi_\\mathrm{p})}{(\\pi_\\mathrm{s}/\\pi_\\mathrm{p})^2 } + \\mathrm{o}(1 ) \\notag\\\\ = & - \\frac{1}{2 }   \\left ( \\frac{d}{2 } -1\\right)^2 \\left ( \\frac{\\lambda_1}{s_1 } + \\dotsb + \\frac { \\lambda_d}{s_d } \\right)^{-1 } + \\mathrm{o}(1 ) \\label{poissonard}\\end{aligned}\\ ] ] by theorem [ maintheorem ] , , , and @xmath308 the asymptotic risk difference depends on @xmath283 only through @xmath309 .",
    "when @xmath309 is small the improvement is large , and it converges to zero as @xmath309 goes to infinity .    it can be shown that @xmath290 dominates @xmath208 in the sense of infinitesimal prediction , and we can construct a bayesian predictive density dominating @xmath310 for arbitrary @xmath311 @xmath266 by modifying the prior @xmath290 .",
    "finite sample properties of this prior will be discussed in a another paper by using an approach different from the asymptotic methods in the present paper .    in examples 1 , 2 , and 3 ,",
    "the volume element based on the predictive metric @xmath61 coincides with the jeffreys priors based on @xmath40 and @xmath312 , i.e. @xmath313 , although the three metrics are different . in general , if two metrics @xmath40 and @xmath312 satisfy the relation @xmath314 where @xmath315 is a @xmath37 regular matrix not depending on @xmath10 , then latexmath:[\\[|\\tilde{g}_{ij}|^{\\frac{1}{2 } } = |a^l_k| |g_{ij } |^{\\frac{1}{2 } } , ~~ \\mbox{and } ~~    and the volume elements based on @xmath40 , @xmath312 , and @xmath61 are proportional to each other .",
    "the relation appears in many examples as in examples 1 , 2 , and 3 .",
    "first , we prepare a preliminary result , theorem [ risk - invariant ] , to prove theorem [ riskdiff ] .      @xcite generalized these results for the setting in which @xmath26 , @xmath317 , and @xmath1 have different distributions .",
    "the bayesian predictive density is expanded as @xmath318 \\partial_k { \\displaystyle \\tilde{p}}(y \\mid { \\hat{\\theta}}_{\\mathrm{mle } } ) + \\mathrm{o}_\\mathrm{p}(n^{-1 } ) , \\label{fka-1}\\end{aligned}\\ ] ] where @xmath319 is the maximum likelihood estimator , and @xmath320 . the estimatorminimizing the bayes risk @xmath321 \\pi(\\theta ) { \\mbox{d}}\\theta$ ]",
    "is given by @xmath322 where @xmath323 which is a covariant vector .",
    "the expansion of the risk function of a bayesian predictive density @xmath29 up to the order @xmath324 is given in theorem [ risk - invariant ] below .",
    "the expansion is invariant in the sense that each term is a scalar not depending on parametrization . in theorem [ risk - invariant ] ,",
    "we put @xmath325 here , @xmath326 and @xmath327 are vectors orthogonal to the model manifolds @xmath328 and @xmath64 , respectively .",
    "these vectors are closely related to the curvature of the manifolds",
    ".      expansions of the risk functions corresponding to when the distributions of @xmath26 , @xmath27 , and @xmath1 are the same are obtained by @xcite for curved exponential families by using differential geometrical notions and by @xcite for general models under rigorous regularity conditions .",
    "@xcite obtained several related results when when the distributions of @xmath26 , @xmath27 , and @xmath1 are different .",
    "the expansion is shown by lengthy calculations parallel to those in @xcite and @xcite by using the results such as , , and obtained by @xcite .",
    "the quantity @xmath330 is the efron curvature @xcite of the model manifold @xmath331 at @xmath10 , and @xmath332 is the mixture mean curvature discussed in @xcite of the model manifold @xmath333 at  @xmath10 .          from and",
    ", we have @xmath338 let @xmath339 and @xmath340 .",
    "then , from , @xmath341 thus , when @xmath342 , @xmath343 . from , we have @xmath344 - { { \\rm e}}\\big [ d ( { \\displaystyle \\tilde{p}}(y \\mid \\theta ) , { \\displaystyle \\tilde{p}}_{\\mathrm{p}}(y \\mid x^n ) \\big ] \\biggr ) \\notag \\\\ = & \\frac{1}{2 } \\sum_{i , j } { \\tilde{g}_{ij}^ { } } { u_\\pi}^i { u_\\pi}^j + \\sum_{i , j , k } { \\tilde{g}_{ij}^ { } } { g_{}^{jk } } \\left(\\partial_k u_{\\pi}^i + \\sum_l { \\tilde{\\gamma}_{kl}^{\\,(\\mathrm{e})i } } u_{\\pi}^l \\right )   \\notag \\\\ & - \\frac{1}{2 } \\sum_{i , j } { \\tilde{g}_{ij}^ { } } u_{\\mathrm{p}}^i u_{\\mathrm{p}}^j - \\sum_{i , j , k } { \\tilde{g}_{ij}^ { } } { g_{}^{jk } } \\left ( \\partial_k u_{\\mathrm{p}}^i + \\sum_l { \\tilde{\\gamma}_{kl}^{\\,(\\mathrm{e})i } } u_{\\mathrm{p}}^l \\right ) + { \\mathrm{o}}(1 ) \\notag \\\\ \\label{riskdiff-2 } = & \\frac{1}{2 } \\sum_{i , j } { \\tilde{g}_{ij}^ { } } ( \\sum_k { g_{}^{ik } } \\partial_k \\log f + s^i + r^i ) ( \\sum_l { g_{}^{jl } } \\partial_l \\log f + s^j + r^j ) \\notag \\\\ & - \\frac{1}{2 } \\sum_{i , j } { \\tilde{g}_{ij}^ { } } ( s^i + r^i ) ( s^j + r^j ) \\notag \\\\ & + \\sum_{i , j , k } { \\tilde{g}_{ij}^ { } } { g_{}^{jk } } \\left\\ { \\sum_l \\partial_k ( { g_{}^{il } } \\partial_l \\log f )       + \\sum_{l , m }   { \\tilde{\\gamma}_{kl}^{\\,(\\mathrm{e})i } } { g_{}^{lm } } \\partial_m \\log f \\right\\ } + { \\mathrm{o}}(1 ) \\notag \\\\ = & \\frac{1}{2 } \\sum_{i , j } { \\mathring{g}_{}^{ij } } \\partial_i \\log f \\partial_j \\log f + \\sum_{i , j , k } { \\tilde{g}_{ij}^ { } } { g_{}^{ik } } ( \\partial_k \\log f ) ( s^j + r^j ) \\notag \\\\ & + \\sum_{i , j , k , l } { \\tilde{g}_{ij}^ { } } { g_{}^{ik } } \\partial_k ( { g_{}^{jl } } \\partial_l \\log f ) + \\sum_{i , j , k , l , m } { \\tilde{g}_{ij}^ { } } { g_{}^{jk } } { \\tilde{\\gamma}_{jk}^{\\,(\\mathrm{e})i } } { g_{}^{lm } } \\partial_m \\log f + { \\mathrm{o}}(1).\\end{aligned}\\ ] ] let @xmath345 . from , it is sufficient to show that @xmath346 is equal to @xmath347 .",
    "since @xmath348 we have @xmath349 thus , from @xmath350 we have @xmath351 hence , because of the duality of the e - connection and the m - connection , is equal to @xmath352        berger , j.  o. and bernardo , j.  m. ( 1992 ) .",
    "`` on the development of reference priors ( with discussion ) . '' in bernardo , j.  m. , berger , j.  o. , dawid , a.  p. , and smith , a.  f.  m. ( eds . ) , _ bayesian statistics 4 _ , 3560 .",
    "new york : oxford university press .",
    "davies , e.  b. ( 1989 ) . _ heat kernels and spectral theory_. cambridge : cambridgeuniversity press ."
  ],
  "abstract_text": [
    "<S> bayesian predictive densities when the observed data @xmath0 and the target variable @xmath1 to be predicted have different distributions are investigated by using the framework of information geometry . </S>",
    "<S> the performance of predictive densities is evaluated by the kullback  leibler divergence . </S>",
    "<S> the parametric models are formulated as riemannian manifolds . in the conventional setting </S>",
    "<S> in which @xmath0 and @xmath1  have the same distribution , the fisher  rao metric and the jeffreys prior play essential roles . in the present </S>",
    "<S> setting in which @xmath0 and @xmath1 have different distributions , a new metric , which we call the predictive metric , constructed by using the fisher information matrices of @xmath0 and @xmath1 , and the volume element based on the predictive metric play the corresponding roles . </S>",
    "<S> it is shown that bayesian predictive densities based on priors constructed by using non - constant positive superharmonic functions with respect to the predictive metric asymptotically dominate those based on the volume element prior of the predictive metric . </S>"
  ]
}