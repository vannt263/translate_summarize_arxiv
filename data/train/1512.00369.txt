{
  "article_text": [
    "privacy preservation is an increasingly critical issue that plays a key role in preventing catastrophic failures in physical infrastructure as well as easing the social adoption of new technology .",
    "power networks , manufacturing systems , and smart transportation are just but a few examples of cyberphysical applications in need of privacy - aware design of control and coordination strategies . in these scenarios ,",
    "the problem of optimizing the operation of a group of networked resources is a common and important task , where the individual objective functions associated to the entities , the estimates of the optimizer , or even the constraints on the optimization might reveal sensitive information .",
    "our work here is motivated by the goal of synthesizing distributed coordination algorithms that solve networked optimization problems with privacy guarantees _ and _ high accuracy .",
    "our work builds upon the existing literature of distributed convex optimization and differential privacy . in the area of networked systems , an increasing body of research , e.g. ,  @xcite and references therein , designs and analyzes algorithms for distributed convex optimization both in discrete and continuous time as well as in deterministic and stochastic scenarios .",
    "while these works consider an ambitious suite of topics related to convergence and performance under various constraints imposed by real - world applications , privacy is an aspect generally absent in their treatment .",
    "the concept of differential privacy  @xcite was originally proposed for databases of individual records subject to public queries and has been extended to several areas thereafter .",
    "the recent work  @xcite provides a comprehensive recent account of this area . in machine learning ,",
    "the problem of differentially private optimization has received attention , see e.g.  @xcite , as an intermediate , usually centralized , step for solving other learning or statistical tasks .",
    "the common paradigm is having the sensitive information correspond to the entries of a finite database of records or training data that usually constitute the parameters of an additive objective function .",
    "threat models are varied , including releasing to the adversary the whole sequence of internal states of the optimization process or only the algorithm s final output . in  @xcite ,",
    "the authors design a differentially private classifier by perturbing the objective function with a linear finite - dimensional function ( hyper - plane ) .",
    "it is shown in  @xcite that this method also works in the presence of constraints and non - differentiable regularizers .",
    "although this is sufficient to preserve the privacy of the underlying finite - dimensional parameter set ( learning samples ) , it can not keep the whole objective functions private .",
    "the authors of  @xcite design a sensitivity - based differentially private algorithm for regression analysis which , instead of perturbing the optimal weight vector , perturbs the regression cost function by injecting noise into the coefficients of the quadratic truncation of its taylor expansion .",
    "this truncation limits the functional space to the ( finite - dimensional ) space of quadratic functions . in  @xcite ,",
    "the authors propose the addition of a sample path of a gaussian random process to the objective function , but do not explore the generalization to arbitrary dimensions or ensure the smoothness and convexity of the resulting function . in general , the proposed algorithms are not distributed and neither designed for nor capable of preserving the privacy of infinite - dimensional objective functions .",
    "furthermore , the work in this area does not rigorously study the effect of added noise on the global optimizer or on the smoothness and convexity properties of the objective functions . in addition to addressing these issues , the present treatment is applicable to scenarios where the sensitive information consists of elements of any separable hilbert space , including ( objective ) functions coming from the ( infinite - dimensional ) @xmath0 space .    of more relevance to our paper",
    "are recent works  @xcite that are focused on differentially private distributed optimization problems for multi - agent systems .",
    "these papers consider as private information , respectively , the objective functions , the optimization constraints , and the agents states .",
    "the underlying commonality is the algorithm design approach based on the idea of message perturbation .",
    "this idea consists of adopting a standard distributed optimization algorithm and modifying it by having agents perturb the messages to their neighbors or a central coordinator with laplace or gaussian noise .",
    "this approach has the advantage of working with the original objective functions and thus is easy to implement .",
    "however , for fixed design parameters , the algorithm s output does not correspond to the true optimizer in the absence of noise , suggesting the presence of a steady - state accuracy error .",
    "this problem is addressed in  @xcite by terminating the algorithm after a finite number of steps and optimizing this number offline as a function of the desired level of privacy .",
    "nevertheless , for any fixed level of privacy , there exists an amount of bias in the algorithm s output which is not due to the added noise but to the lack of asymptotic stability of the underlying noiseless dynamics . to address this issue ,",
    "our approach explores the use of functional perturbation to achieve differential privacy .",
    "the concept of functional differential privacy combines the benefits of metrics and adjacency relations .",
    "the authors of  @xcite also employ metrics instead of binary adjacency relations in the context of differential privacy .",
    "this approach has the advantage that the difference between the probabilities of events corresponding to any pair of data sets is bounded by a function of the distance between the data sets , eliminating the need for the computation of conservative sensitivity bounds .",
    "we consider a group of agents that seek to minimize the sum of their individual objective functions over a communication network in a differentially private manner .",
    "our first contribution is to show that coordination algorithms which rely on perturbing the agents messages with noise can not satisfy the requirements of differential privacy if the underlying noiseless dynamics are locally asymptotically stable .",
    "the presence of noise necessary to ensure differential privacy is known to affect the algorithm accuracy in solving the distributed convex optimization problem . however",
    ", this result explains why message - perturbing strategies incur additional inaccuracies that are present even if no noise is added .",
    "our second contribution is motivated by the goal of guaranteeing that the algorithm accuracy is only affected by the presence of noise .",
    "we propose a general framework for functional differential privacy over hilbert spaces and introduce a novel definition of adjacency using adjacency spaces .",
    "the latter notion is quite flexible and includes , as a special case , the conventional bounded - difference notion of adjacency .",
    "we carefully specify these adjacency spaces within the @xmath0 space such that the requirement of differential privacy can be satisfied with bounded perturbations .",
    "our third contribution builds on these results on functional perturbation to design a class of distributed , differentially private coordination algorithms .",
    "we let each agent perturb its own objective function based on its desired level of privacy , and then the group uses any provably correct distributed coordination algorithm to optimize the sum of the individual perturbed functions .",
    "two challenges arise to successfully apply this strategy : the fact that the perturbed functions might lose the smoothness and convexity properties of the original functions and the need to characterize the effect of the added noise on the minimizer of the resulting problem .",
    "we address the first challenge using a cascade of smoothening and projection steps that maintain the differential privacy of the functional perturbation step .",
    "we address the second challenge by explicitly bounding the absolute expected deviation from the original optimizer using a novel lipschitz characterization of the @xmath1 map . by construction ,",
    "the resulting coordination algorithms satisfy the requirement of recovering perfect accuracy in the absence of noise .",
    "various simulations illustrate our results .",
    "we introduce our notation and basic preliminaries in section  [ sec : prelims ] and formulate the private distributed optimization problem in section  [ sec : prob - state ] .",
    "section  [ sec : rationale ] presents the rationale for our design strategy and section  [ sec : func - diff - privacy ] describes a general framework for functional differential privacy .",
    "we formulate our solution to the private distributed optimization problem in section  [ sec : diff - private - dist - opt ] .",
    "we present simulations in section  [ sec : sims ] and collect our conclusions and ideas for future work in section  [ sec : conclusion ] .",
    "appendix  [ sec : argmin - lip ] gathers our results on the lipschitzness of the @xmath1 map under suitable assumptions .",
    "in this section , we introduce our notational conventions and some fundamental facts about hilbert spaces and robust stability of discrete - time systems .",
    "we use @xmath2 , @xmath3 , @xmath4 , and @xmath5 to denote the set of reals , positive reals , nonnegative integers , and positive integers , respectively .",
    "the space of scalar- and @xmath6-vector - valued infinite sequences are denoted by @xmath7 and @xmath8 , respectively . given @xmath9 and an element @xmath10 of @xmath7 or @xmath8 , we use the shorthand notation @xmath11 .",
    "if the index of @xmath12 starts at @xmath13 , with a slight abuse of notation we also denote @xmath14 by @xmath15 .",
    "we denote by @xmath16 the space of square - summable infinite sequences .",
    "we use @xmath17 and @xmath18 for the @xmath19-norm in finite and infinite - dimensional normed vector spaces , respectively ( we drop the index @xmath19 for @xmath20 ) .",
    "we let @xmath21 denote the closed ball with center @xmath22 and radius @xmath23 in euclidean space . for @xmath24",
    ", @xmath25 denotes its interior and @xmath26 and @xmath27 denote the set of square - integrable measurable functions and the set of twice continuously differentiable functions over @xmath28 , respectively . throughout the paper",
    ", @xmath29 denotes the lebesgue measure .",
    "if @xmath30 is a sequence of subsets of @xmath31 such that @xmath32 and @xmath33 , then we write @xmath34 as @xmath35 .",
    "we say @xmath36 as @xmath35 if @xmath37 as @xmath38 , where @xmath39 is the complement of @xmath40 .",
    "given any closed and convex subset @xmath41 of a hilbert space , we denote by @xmath42 the orthogonal projection onto  @xmath43 .",
    "we denote by @xmath44 the set of strictly increasing continuous functions @xmath45 such that @xmath46 .",
    "a function @xmath47 belongs to @xmath48 if @xmath49 and @xmath50 .",
    "we denote by @xmath51 the set of functions @xmath52 such that , for each @xmath53 , @xmath54 is nondecreasing and continuous and @xmath55 and , for each @xmath56 , @xmath57 is monotonically decreasing with @xmath58 as @xmath59 . a map @xmath60 between two normed vector spaces",
    "is @xmath44-lipschitz if there exists @xmath61 such that @xmath62 for all @xmath63 .",
    "the ( zero - mean ) laplace distribution with scale @xmath64 is a continuous distribution with probability density function @xmath65 it is clear that @xmath66 .",
    "we use @xmath67 to denote a random variable @xmath68 with laplace distribution .",
    "it is easy to see that if @xmath69 , @xmath70 has an exponential distribution with rate @xmath71 .",
    "similarly , we use the notation @xmath72 when @xmath68 is normally distributed with mean @xmath73 and variance @xmath74 .",
    "the error function @xmath75 is defined as @xmath76 therefore , @xmath77 if @xmath78 .",
    "given any random variable @xmath68 and any convex function @xmath79 , jensen s inequality states that @xmath80 \\ge \\phi({\\mathbb{e}}[\\eta])$ ] .",
    "the opposite inequality holds if @xmath79 is concave .",
    "we review some basic facts on hilbert spaces and refer the reader to  @xcite for a comprehensive treatment .",
    "a hilbert space  @xmath81 is a complete inner - product space .",
    "a set @xmath82 is an orthonormal system if @xmath83 for @xmath84 and @xmath85 for all @xmath86 .",
    "if , in addition , the set of linear combinations of @xmath87 is dense in  @xmath81 , then @xmath87 is an orthonormal basis . here",
    ", @xmath88 might be uncountable : however , if @xmath81 is separable ( i.e. , it has a countable dense subset ) , then any orthonormal basis is countable . in this case",
    ", we have @xmath89 for any @xmath90 .",
    "we define the coefficient sequence @xmath91 by @xmath92 for @xmath93 . then , @xmath94 and , by parseval s identity , @xmath95 . for ease of notation",
    ", we define @xmath96 to be the linear bijection that maps the coefficient sequence @xmath97 to @xmath98 . for an arbitrary @xmath24",
    ", @xmath99 is a hilbert space if and only if @xmath20 , and the inner product is the integral of the product of functions .",
    "moreover , @xmath26 is separable . in the remainder of the paper , we assume @xmath100 is an orthonormal basis for @xmath26 and @xmath101 is the corresponding linear bijection between coefficient sequences and functions .",
    "we briefly present some definitions and results on robust stability of discrete - time systems following  @xcite . given the vector field @xmath102 , consider the system @xmath103 with state @xmath104 and input @xmath105 .",
    "given an equilibrium point @xmath106 of the unforced system , we say that   is    1 .   _",
    "0-input locally asymptotically stable ( 0-las ) relative to @xmath107 _ if , by setting @xmath108 , there exists @xmath109 and @xmath110 such that , for every initial condition @xmath111 , we have for all @xmath112 , @xmath113 2 .   _ locally input - to - state stable ( liss ) relative to @xmath107 _ if there exist @xmath109 , @xmath110 , and @xmath114 such that , for every initial condition @xmath115 and every input satisfying @xmath116 , we have @xmath117 for all @xmath118 . in this case",
    ", we refer to @xmath119 as the _ robust stability radius _ of   relative to @xmath107 .    by definition , if the system   is liss , then it is also 0-las .",
    "the converse is also true , cf .",
    "* theorem 1 ) .",
    "the following result is a local version of  ( * ? ? ?",
    "* lemma 3.8 ) and states an important asymptotic behavior of liss systems .    _",
    "( asymptotic gain of liss systems):_[prop : asym - gain ] assume system   is liss relative to  @xmath107 with associated robust stability radius  @xmath119 .",
    "if @xmath115 and @xmath120 ( where @xmath121 if @xmath119 is not in the range of @xmath122 ) , then @xmath123 in particular , @xmath124 if @xmath125 as @xmath35 .    from  , we have @xmath126 where we have used @xmath115 .",
    "now , for each @xmath127 , let @xmath128 } \\in ( { \\ensuremath{\\mathbb{r}}}^n)^{{\\mathbb{n}}}$ ] be defined by @xmath129}(\\ell ) = \\eta(k + \\ell)$ ] for all @xmath130 . if there exists @xmath131 such that @xmath132}\\|_\\infty = 0 $ ]",
    ", then we need to show that @xmath133 .",
    "since @xmath110 , there exists @xmath134 such that @xmath135 for all @xmath136 , and since @xmath137 as well , it follows from   that @xmath138 for all @xmath139 .",
    "let @xmath140 .",
    "using  , we get @xmath141 and the result follows . assume then that no @xmath131 exists such that @xmath132}\\|_\\infty = 0 $ ] .",
    "let @xmath142 and , for each @xmath143 , let @xmath144 be such that @xmath145}\\|_\\infty)$ ] for all @xmath146 ( this sequence is well - defined because @xmath147 ) .",
    "since @xmath148}\\|_\\infty )    \\le \\kappa(\\|\\boldsymbol{\\eta}\\|_\\infty ) \\le \\rho$ ] , holds if we set the `` initial '' state to @xmath149 which implies that @xmath150}\\|_\\infty)$ ] for all @xmath151 .",
    "therefore , @xmath152}\\|_\\infty ) ,",
    "\\quad \\forall j \\in      { { \\mathbb{z}}_{\\ge 0}}.    \\end{aligned}\\ ] ] the result follows by taking limit of both sides as @xmath153 .",
    "consider a group of @xmath6 agents whose communication topology is described by a digraph @xmath154 .",
    "each agent @xmath155 has a local objective function @xmath156 , where @xmath157 is convex and compact and has nonempty interior .",
    "we assume that each @xmath158 is convex and twice continuously differentiable , and use the shorthand notation @xmath159 .",
    "consider the following convex optimization problem @xmath160 where the component functions of @xmath161 are convex , @xmath162 , and @xmath163 .",
    "denote by @xmath164 the feasibility set .",
    "the optimization problem can be equivalently written as , @xmath165 we assume that @xmath166 is a global piece of information known to all agents .",
    "the group objective is to solve the convex optimization problem   in a distributed and private way .",
    "by distributed , we mean that each agent can only interact with its neighbors in  @xmath154 . for privacy , we consider the case where the function @xmath167 ( or some of its attributes ) constitute the local and sensitive information known to agent @xmath155 that has to be kept confidential .",
    "each agent assumes ( the worst - case where ) the adversary has access to all the `` external '' information ( including all the network communications and all other objective functions ) .",
    "this setting is sometimes called local ( differential ) privacy in the literature , see e.g. ,  @xcite . in order to define privacy ,",
    "we first introduce the notion of adjacency . given any normed vector space @xmath168 with @xmath169 , two sets of functions @xmath170 are @xmath171-adjacent if there exists @xmath172 such that @xmath173 the set @xmath171 is a design choice that we specify later in section  [ subsec : func - perturb ] .",
    "moreover , this definition can be readily extended to the case where @xmath171 is any subset of another normed vector space @xmath174 . with this generalization ,",
    "the conventional bounded - difference notion of adjacency becomes a special case of the definition above , where @xmath171 is a closed ball around the origin .",
    "we provide next a general definition of differential privacy for a map .    _",
    "( differential privacy):_[def : func - diff - privacy ] let @xmath175 be a probability space and consider a random map @xmath176 from the function space @xmath177 to an arbitrary set @xmath178 . given @xmath179 ,",
    "the map @xmath180 is @xmath181-differentially private if , for any two @xmath171-adjacent sets of functions @xmath182 and @xmath183 that ( at most ) differ in their @xmath184th element and any set @xmath185 , one has @xmath186    essentially , this notion requires the statistics of the output of  @xmath180 to change only ( relatively ) slightly if the objective function of one agent changes ( and the change is in @xmath171 ) , making it hard for an adversary who observes the output of  @xmath180 to determine the change . in the case of an iterative asymptotic distributed optimization algorithm",
    ", @xmath180 represents the action ( observed by the adversary ) of the algorithm on the set of local functions @xmath182 . in other words",
    ", @xmath180 is the map ( parameterized by the initial network condition ) that assigns to @xmath182 the whole sequence of messages transmitted over the network . in this case ,   has to hold for all allowable initial conditions .",
    "we are ready to formally state the network objective .    _",
    "( differentially private distributed optimization):_[problem ] design a distributed and differentially private optimization algorithm whose guarantee on accuracy improves as the level of privacy decreases , leading to the exact optimizer of the aggregate objective function in the absence of privacy .",
    "the reason for the requirement of recovering the exact optimizer in the absence of privacy in problem  [ problem ] is the following .",
    "it is well - known in the literature of differential privacy that there always exists a cost for an algorithm to be differentially private , i.e. , the algorithm inevitably suffers a performance loss that increases as the level of privacy increases .",
    "this phenomenon is a result of the noise added in the map  @xmath180 , whose variance increases as @xmath181 decreases . with the above requirement on the noise - free behavior of the algorithm",
    ", we aim to make sure that the cause of this performance loss is _ only _ due to the added noise and not to any other factor .    _",
    "( linear classification with logistic loss function):_[example ]    we introduce here a supervised classification problem that will serve to illustrate the discussion along the paper .",
    "consider a database of training records composed by the labeled samples @xmath187 , where each @xmath188 ( containing the features of a corresponding object ) may belong to one of two possible classes and @xmath189 determines to which class it belongs .",
    "the goal is to train a classifier with the samples so that it can automatically classify future unlabeled samples . for simplicity ,",
    "we let @xmath190 and assume @xmath191 ^ 2 $ ] and @xmath189 are independently and uniformly randomly selected . the aim is to find the best hyperplane @xmath192 that can separate the two classes .",
    "the parameters @xmath193 defining the hyperplane can be found by solving the convex problem , @xmath194 where @xmath195 is the loss function and @xmath196 is the regularizing term . since the objective function is strongly convex , we choose @xmath166 large enough so that @xmath107 is the same as the unique unconstrained minimizer .",
    "popular choices of @xmath197 are the logistic loss @xmath198 and the hinge loss @xmath199 .",
    "we focus on the logistic loss here due to its smoothness .",
    "consider a group of @xmath6 agents , each one owning a portion @xmath200 of the training samples , who seek to collectively solve   in a distributed fashion , i.e. , only by communicating with their neighbors ( without a central aggregator ) .",
    "various iterative algorithms have been proposed in the literature , cf .",
    "@xcite , to address this problem . as an example",
    ",  @xcite proposes that each agent @xmath201 starts with an initial estimate @xmath202 of @xmath107 and , at each iteration @xmath203 , update its estimate as    @xmath204    where @xmath205 are the edge weights of the communication graph at node @xmath206 and @xmath207 is the stepsize . from",
    ", one can see that agents only need to share their estimates with their neighbors to run the algorithm . under reasonable connectivity assumptions",
    ", one can show  @xcite that @xmath208 converges to @xmath107 asymptotically if the sequence of stepsizes is square - summable ( @xmath209 ) but not summable ( @xmath210 ) .",
    "in this paper , we are interested in endowing distributed coordination algorithms such as this with privacy guarantees so that their execution does not reveal information about the local objective functions to the adversary .",
    "in this section , we discuss two algorithm design strategies to solve problem  [ problem ] based on the perturbation of either inter - agent messages or the local objective functions .",
    "we point out an important limitation of the former , and this provides justification for the ensuing design of our objective - perturbing algorithm based on functional differential privacy .",
    "we use the term _ message - perturbing strategy _ to refer to the result of modifying any of the distributed optimization algorithms available in the literature by adding ( gaussian or laplace ) noise to the messages agents send to either neighbors or a central aggregator in order to preserve privacy .",
    "a generic message - perturbing distributed algorithm takes the form @xmath211 where @xmath212 , @xmath213 are the sequences of messages and perturbations , respectively , and @xmath214 depends on the agents sensitive information set  @xmath215 with associated optimizer @xmath216 .",
    "this formulation is quite general and can also encode algorithmic solutions for optimization problems other than the one in section  [ sec : prob - state ] , such as the ones studied in  @xcite . in the problem of interest here , @xmath217 .",
    "the following result provides conditions on the noise variance that ensure that the noise vanishes asymptotically almost surely and remains bounded with nonzero probability .    _",
    "( convergence and boundedness of laplace and normal random sequences with decaying variance):_[lem : conv - bound ] let @xmath12 be a sequence of independent random variables defined over the sample space @xmath218 , with @xmath219 or @xmath220 for all @xmath118 . given @xmath221 , consider the events @xmath222 if @xmath223 is @xmath224 for some @xmath225 , then @xmath226 and @xmath227 for all @xmath221 .    first , consider the case where @xmath219 . by the independence of the random variables and",
    "the fact that @xmath228 is exponentially distributed with rate @xmath229 , @xmath230 by assumption , @xmath231 for all @xmath118 and some @xmath232 .",
    "thus , given that the series @xmath233 converges  @xcite , @xmath234 next , let @xmath235 where @xmath236 is a monotonically decreasing sequence that converges to zero as @xmath237 ( e.g. , @xmath238 ) .",
    "note that @xmath239 since @xmath240 for all @xmath241 as @xmath242 , and @xmath243 as @xmath237 , we have @xmath244 then , @xmath245 . for the case of normal distribution of random variables , @xmath246 and",
    "the results follows from the arguments above .",
    "note that lemma  [ lem : conv - bound ] also ensures that the probability that the noise simultaneously converges to zero and remains bounded is nonzero .",
    "one might expect that lemma  [ lem : conv - bound ] would hold if @xmath247 at any rate . however , this is not true .",
    "for instance , if @xmath248 , one can show that the probability that @xmath249 eventually remains bounded is zero for any bound @xmath250 , so the probability that @xmath125 is zero as well .",
    "the following result shows that a message - perturbing algorithm of the form   can not achieve differential privacy if the underlying ( noise - free ) dynamics are asymptotically stable . for convenience ,",
    "we employ the short - hand notation @xmath251 to refer to  .    _",
    "( impossibility result for 0-las message - perturbing algorithms):_[prop : impos - result ] consider _ any _ algorithm of the form   with either @xmath252 or @xmath253 .",
    "if @xmath254 is 0-las relative to @xmath255 for two information sets @xmath215 and @xmath256 with different optimizers @xmath257 and associated robust stability radii @xmath119 and @xmath258 , respectively , @xmath259 is @xmath224 for all @xmath155 and some @xmath225 , and at least one of the following holds ,    1 .",
    "@xmath260 is not an equilibrium point of @xmath261 and @xmath262 is continuous , 2 .",
    "@xmath260 belongs to the interior of @xmath263 ,    then , the algorithm can not preserve the @xmath181-differentially privacy of the information set @xmath215 for any @xmath264 .",
    "our proof strategy consists of establishing that , if the initial state is close to the equilibrium of the system for one information set , the state trajectory converges to that equilibrium with positive probability but to the equilibrium of the system with the other information set with probability zero .",
    "we then use this fact to rule out differential privacy . for any fixed initial state @xmath265 ,",
    "if either of @xmath212 or @xmath12 is known , the other one can be uniquely determined from  .",
    "therefore , the mapping @xmath266 such that @xmath267 is well - defined and bijective .",
    "let @xmath268 be as in   corresponding to @xmath269 and @xmath270 , respectively .",
    "consider as initial condition @xmath271 and define @xmath272 by lemma  [ lem : conv - bound ] , we have @xmath273 . by proposition  [ prop : asym - gain ] , since @xmath274 and @xmath275 for all @xmath276 , the sequence @xmath277 converges to @xmath260 .",
    "let @xmath278 and @xmath279 ( where we are using the forward and inverse images of sets , respectively ) .",
    "next , we show that no @xmath280 converges to @xmath281 under either hypothesis ( i ) or ( ii ) of the statement . under ( i ) , there exists a neighborhood of @xmath282 in which the infimum of the absolute value of at least one of the components of @xmath283 is positive , so whenever @xmath284 enters this neighborhood , it exits it in finite time .",
    "therefore , given that any @xmath285 converges to @xmath260 , no @xmath280 can converge to zero . under ( ii )",
    ", there exists a neighborhood of @xmath260 included in @xmath263 .",
    "since @xmath286 , there exists @xmath9 such that @xmath287 belongs to @xmath288 for all @xmath136 .",
    "therefore , if @xmath289 indefinitely after any point of time , @xmath290 by proposition  [ prop : asym - gain ] which is a contradiction , so @xmath291 can not converge to zero . in both cases , by lemma  [ lem : conv - bound ] , @xmath292 , which , together with @xmath273 and the definition of @xmath181-differential privacy , cf .",
    ", implies the result .    note that the hypotheses of proposition  [ prop : impos - result ] are mild and easily satisfied in most cases .",
    "in particular , the result holds if the dynamics are continuous and globally asymptotically stable relative to @xmath255 for two information sets .",
    "the main take - away message of this result is that a globally asymptotically stable distributed optimization algorithm can not be made differentially private by perturbing the inter - agent messages with asymptotically vanishing noise .",
    "this observation is at the core of the design choices made in the literature regarding the use of stepsizes with finite sum to make the zero - input dynamics not asymptotically stable , thereby causing a steady - state error in accuracy which is present independently of the amount of noise injected for privacy .",
    "for instance , the algorithmic solution proposed in  @xcite replaces   by @xmath293 , where @xmath294 is the perturbed message received from agent @xmath295 , and chooses a finite - sum sequence of stepsizes @xmath296 in the computation  , leading to a dynamical system which is not 0-gas , see figure  [ fig : huang ] .",
    "similar observations can be made in the scenario considered in  @xcite , where the agents local constraints are the sensitive information ( instead of the objective function ) .",
    "this algorithmic solution uses a constant - variance noise , which would make the dynamics unstable if executed over an infinite time horizon .",
    "this problem is circumvented by having the algorithm terminate after a finite number of steps , and optimizing this number offline as a function of the desired level of privacy  @xmath181 .      to overcome the limitations of message - perturbing strategies , here we outline an alternative design strategy to solve problem  [ problem ] based on the perturbation of the agents objective functions .",
    "the basic idea is to have agents independently perturb their objective functions in a differentially private way and then have them participate in a distributed optimization algorithm with the perturbed objective functions instead of their original ones . in the context of example",
    "[ example ] , this would correspond to leave   and the sequence of stepsizes unchanged , and instead use perturbed functions in the computation  .",
    "the latter in turn automatically adds noise to the estimates shared with neighbors . the following result , which is a special case of ( * ? ? ?",
    "* theorem 1 ) , ensures that the combination with the distributed optimization algorithm does not affect the differential privacy at the functional level .    _",
    "( resilience to post - processing):_[prop : post - proc ] let @xmath297 be @xmath181-differentially private ( cf . definition  [ def : func - diff - privacy ] ) and @xmath298 , where @xmath299 is an arbitrary measurable space .",
    "then , @xmath300 is @xmath181-differentially private .",
    "first , note that although a slightly different definition of differential privacy is used in  @xcite , the exact same proof of  ( * ? ? ?",
    "* theorem 1 ) works with definition  [ def : func - diff - privacy ] .",
    "consider the @xmath301-algebra @xmath302 on @xmath177 where @xmath303 denotes the power set . with the notation of  ( * ?",
    "* theorem 1 ) , @xmath304 is a deterministic function of the output of @xmath305 .",
    "then , it is easy to verify that , for any @xmath306 , @xmath307 ( with @xmath308 being the indicator function ) is measurable as a function of @xmath309 ( because @xmath310 and @xmath311 are trivially measurable ) and defines a probability measure on @xmath299 ( associated to a singleton ) , so it is a probability kernel .",
    "hence , the conditions of  ( * ? ? ?",
    "* theorem 1 ) are satisfied and @xmath312 is @xmath181-differentially private .",
    "our design strategy based on the perturbation of individual objective functions requires solving the following challenges :    1 .   establishing a differentially private procedure to perturb the individual objective functions ; 2 .",
    "ensuring that the resulting perturbed functions enjoy the smoothness and regularity properties required by distributed optimization algorithms to converge ; 3 .   with ( i ) and",
    "( ii ) in place , characterizing the accuracy of the resulting differentially private , distributed coordination algorithm .",
    "section  [ sec : func - diff - privacy ] addresses  ( i ) and section  [ sec : diff - private - dist - opt ] deals with  ( ii ) and  ( iii ) .",
    "we explore here the concept of functional differential privacy to address the challenge  ( i ) laid out in section  [ sec : algorithm - design - plan ] .",
    "the generality of this notion makes it amenable for problems where the sensitive information is a function or some of its attributes ( e.g. , sample points , optimizers , derivatives and integrals ) . for simplicity of exposition and without loss of generality , we limit our discussion in this section to the privacy of a single function .",
    "let @xmath313 be a function whose differential privacy has to be preserved . with the notation of section  [ sec : prelims ]",
    ", we decompose @xmath314 into its coefficients @xmath315 and perturb this sequence by adding noise to all of its elements . specifically , we set @xmath316 where @xmath317 for all @xmath118 . clearly , for @xmath12 to belong to @xmath318 and for the series in @xmath319 to converge , the scales @xmath320 can not be arbitrary . the next result addresses this issue .    _",
    "( sufficient condition for boundedness of perturbed functions):_[lem : l_2-condition ] if there exists @xmath9 such that , for some @xmath321 and @xmath322 , @xmath323 then @xmath12 defined by   belongs to @xmath324 with probability one .",
    "in particular , if for some @xmath325 and @xmath326 , @xmath327 then @xmath12 defined by   belongs to @xmath318 with probability one .    equation   can be equivalently written as @xmath328 , for @xmath136 .",
    "in particular , this implies that @xmath329 is convergent .",
    "therefore  @xcite , @xmath330 converges ( i.e. , the limit exists and is nonzero ) , so @xmath331 where @xmath332 and we have used the fact that @xmath333 is exponentially distributed with rate @xmath334 . since @xmath335 as @xmath242 , we have @xmath336 as stated .",
    "if equation   holds , we define @xmath337 and equivalently write   as @xmath338 since @xmath339 , for any @xmath322 there exists @xmath9 such that @xmath340 for all @xmath136 , and the result follows .",
    "having established conditions on the noise variance under which the map   is well defined , we next turn our attention to establish its differential privacy .      here",
    ", we establish the differential privacy of the map  . in order to do so , we first specify our choice of adjacency space @xmath171 . given @xmath341 , consider the weight sequence @xmath342 and define the adjacency vector space to be the image of the resulting weighted @xmath318 space under @xmath343 , i.e. , @xmath344 it is not difficult to see that @xmath345 is a vector space .",
    "moreover , @xmath346 is a norm on  @xmath345 .",
    "the next result establishes the differential privacy of   for a properly chosen noise scale sequence @xmath347 .    _",
    "( differential privacy of functional perturbation):_[thm : diff - privacy ] given @xmath348 , @xmath349 and @xmath350 , let @xmath351 then , the map   is @xmath181-differentially private with @xmath352 where @xmath353 is the riemann zeta function .",
    "note that the map @xmath180 defined by   is well defined because   ensures , by lemma  [ lem : l_2-condition ] , that @xmath12 belongs to @xmath354 almost surely .",
    "our proof consists of showing that @xmath180 satisfies the definition of differential privacy , cf . definition  [ def : func - diff - privacy ] . to this effect ,",
    "consider two functions @xmath314 and @xmath355 , with @xmath356 , and an arbitrary set @xmath357 .",
    "let @xmath358 be the map that returns the first @xmath359 coefficients of @xmath360 and @xmath361 we have @xmath362 where @xmath363 denotes the inverse image of the set @xmath364 and the second equality follows from the continuity of probability  ( * ? ? ?",
    "* theorem  1.1.1.iv ) ( since @xmath365 as @xmath366 ) .",
    "similarly , @xmath367 by linearity of @xmath368 , we have @xmath369 where @xmath370 .",
    "therefore , @xmath371 note that @xmath372 after multiplying both sides by @xmath373 , integrating over @xmath363 , and letting @xmath242 , we have @xmath374 finally , the coefficient of the exponential can be upper bounded using holder s inequality with @xmath375 as @xmath376 which completes the proof .    _",
    "( choice of @xmath377 ) : _ the choice of parameter  @xmath377 affects the trade - off between the size of the adjacency space  @xmath345 and the noise required for privacy . from  , we see that decreasing @xmath377 makes @xmath345 larger , allowing for the privacy preservation of a larger collection of functions .",
    "however , as expected , preserving privacy in a larger space requires more noise . from",
    "( 12 ) , @xmath378 will be larger for a fixed @xmath181 , ( since @xmath19 can not be decreased by the same amount as @xmath377 and @xmath353 is monotonically decreasing ) , resulting in larger @xmath379 and larger noise .",
    "we show later in theorem vi.2 that the guaranteed upper bound on the expected minimizer deviation also increases as @xmath380 decrease .",
    "in this section , we employ functional differential privacy to solve the differentially private distributed optimization problem formulated in section  [ sec : prob - state ] for a group of @xmath381 agents . for convenience ,",
    "we introduce the shorthand notation @xmath382 and , for given @xmath383 , @xmath384 , @xmath385 for twice continuously differentiable functions with bounded gradients and hessians . in the rest of the paper , we assume that the agents local objective functions @xmath386 belong to  @xmath43 .      we address here",
    "the challenge  ( ii ) laid out in section  [ sec : algorithm - design - plan ] . to exploit the framework of functional differential privacy for optimization",
    ", we need to ensure that the perturbed functions have the smoothness and regularity properties required by the distributed coordination algorithm . in general , the output of   might neither be smooth nor convex .",
    "we detail next how to address these problems by defining appropriate maps that , when composed with @xmath180 in  , yield functions with the desired properties .",
    "proposition  [ prop : post - proc ] ensures that differential privacy is retained throughout this procedure .      to ensure smoothness , we rely on the fact that @xmath387 is dense in @xmath26 and , therefore , given any function @xmath388 in @xmath26 , there exists a smooth function arbitrarily close to it , i.e. , @xmath389 here",
    ", @xmath390 is a design parameter and can be chosen sufficiently small ( later , we show how to do this so that the accuracy of the coordination algorithm is not affected ) .    _ ( smoothening and truncation ) : _ a natural choice for the smoothening step , if the basis functions are smooth ( i.e. , @xmath391 ) , is truncating the infinite expansion of  @xmath388 .",
    "such truncation is also inevitable in practical implementations due to the impossibility of handling infinite series .",
    "the appropriate truncation order depends on the specific function , the basis set , and the noise decay rate ( @xmath19 in  ) .",
    "the next result ensures that the orthogonal projection from @xmath387 onto @xmath43 is well defined , and can therefore be used to ensure strong convexity and bounded hessian of the perturbed functions .    _",
    "( convexity of @xmath43 and its closedness relative to @xmath387):_[prop : closure ] the set @xmath43 is convex and closed as a subset of @xmath387 under the @xmath392-norm .",
    "the set @xmath43 is clearly convex because , if @xmath393 and @xmath394 $ ] , then for all @xmath395 , @xmath396 similarly , @xmath397 .",
    "also , @xmath398 for all @xmath399 . to establish closedness ,",
    "let @xmath400 since @xmath401 , it is enough to show that @xmath402 and @xmath403 are both closed subsets of @xmath387 .    to show that @xmath402 is closed , let @xmath404 be a sequence of functions in @xmath402 such that @xmath405 .",
    "we show that @xmath406 .",
    "since @xmath407 and @xmath0 convergence implies pointwise convergence of a subsequence almost everywhere , there exists @xmath408 and @xmath409 such that @xmath410 and @xmath411 for all @xmath412 .",
    "it is straightforward to verify that @xmath413 is dense in @xmath28 and therefore @xmath414 is dense in @xmath25 .",
    "then , by  ( * ? ? ?",
    "* theorem 10.8 ) , @xmath415 is convex on @xmath25 , so @xmath416 for all @xmath395 .",
    "similarly , one can show that @xmath417 for all @xmath418 .",
    "therefore , @xmath419 .",
    "the proof of closedness of @xmath403 is more involved . by contradiction , assume that @xmath404 is a sequence of functions in @xmath403 such that @xmath420 but @xmath421 .",
    "therefore , there exist @xmath422 such that @xmath423 and , by continuity of @xmath424 , @xmath425 and @xmath426 such that @xmath427 let @xmath428 . by continuity of @xmath424 , for all @xmath429 there exists @xmath430 $ ] such that @xmath431 as mentioned above , @xmath0 convergence implies pointwise convergence of a subsequence @xmath408 almost everywhere . in turn",
    ", this subsequence converges to @xmath98 almost uniformly , i.e. , for all @xmath432 and all @xmath433 , there exist @xmath434 and @xmath435 such that @xmath436 and @xmath437 for ease of notation , let @xmath438 . using the fundamental theorem of line integrals  @xcite , for all @xmath439 , @xmath440 similarly , for all @xmath439 and all @xmath441 , @xmath442 putting  ,  , and   together and choosing @xmath443",
    ", we have for all @xmath444 and all @xmath445 , @xmath446 the quantity @xmath447 can be made strictly positive choosing @xmath448 .",
    "let @xmath449 and @xmath450 .",
    "then , can be rewritten as @xmath451 which , by choosing @xmath452 , implies @xmath453 this contradicts @xmath454 , so @xmath403 must be closed .",
    "given the result in proposition  [ prop : closure ] , the best approximation in @xmath43 of a function @xmath455 is its unique projection onto @xmath43 , i.e. , @xmath456 by definition , @xmath457 has bounded gradient and hessian .      we address here the challenge  ( iii ) laid out in section  [ sec : algorithm - design - plan ] and put together the discussion above to propose a class of differentially private , distributed optimization algorithms that solve problem  [ problem ] .",
    "unlike the message - perturbing algorithms where agents use the original objective functions in the computations and rely on perturbing the inter - agent messages , here we propose that agents locally perturb their objective functions and use them in their computations , without adding any additional noise to the inter - agent messages .",
    "therefore , we require each agent @xmath458 to first compute    [ eq : perturb ] @xmath459 where @xmath460 is a sequence of laplace noise generated by  @xmath206 according to   with the choice  , then select @xmath461 such that @xmath462 and finally compute @xmath463    after this process , agents participate in _ any _ distributed optimization algorithm with the modified objective functions @xmath464 . let @xmath465 denote , respectively , the output of the distributed algorithm and the optimizer for the original optimization problem ( with objective functions @xmath466 ) .",
    "the following result establishes the connection between the algorithm s accuracy and the design parameters .    _",
    "( accuracy of the proposed class of distributed , differentially private coordination algorithms):_[thm : accuracy ] consider a group of @xmath6 agents which perturb their local objective functions according to   with laplace noise   of variance  , where @xmath467 , @xmath468 , and @xmath469 for all @xmath155 .",
    "let the agents participate in any distributed coordination algorithm that asymptotically converges to the optimizer  @xmath470 of the perturbed aggregate objective function .",
    "then , @xmath471-differential privacy of each agent @xmath206 s original objective function is preserved with @xmath472 and @xmath473 where the function @xmath474 is defined in proposition  [ prop : argmin - lip ] .    since the distributed algorithm is a post - processing step on the perturbed functions , privacy preservation of the objective functions follows from theorem  [ thm : diff - privacy ] and proposition  [ prop : post - proc ] . for convenience ,",
    "let @xmath475 since @xmath476 is convex and belongs to class  @xmath48 ( so is monotonically increasing ) , @xmath477 is concave and belongs to class @xmath48 and so is subadditive .",
    "therefore , using proposition  [ prop : argmin - lip ] , @xmath478      \\\\      & \\le { \\mathbb{e}}\\big[\\kappa_n \\big ( \\sum_{i = 1}^n \\| \\tilde f_i - f_i \\|      \\big ) \\big ] \\le \\sum_{i = 1}^n { \\mathbb{e}}\\big[\\kappa_n \\big ( \\|\\tilde f_i      - f_i \\|\\big ) \\big ] .",
    "\\end{aligned}\\ ] ] then , by the non - expansiveness of projection , we have @xmath479      \\\\      \\notag & \\le \\sum_{i = 1}^n { \\mathbb{e}}\\big [ \\kappa_n \\big(\\| \\hat f_i^s -      \\hat f_i \\|\\big ) + \\kappa_n \\big(\\| \\hat f_i - f_i \\|\\big ) \\big ]      \\\\      & \\le \\sum_{i = 1}^n \\big ( \\kappa_n ( \\varepsilon_i ) + { \\mathbb{e}}\\big [      \\kappa_n \\big(\\|",
    "\\boldsymbol{\\eta}_i \\| \\big ) \\big]\\big ) .    \\end{aligned}\\ ] ] by invoking jensen s inequality twice , for all @xmath155 , @xmath480 \\le \\kappa_n      \\big({\\mathbb{e}}\\big [ \\| \\boldsymbol{\\eta}_i \\| \\big ] \\big ) =      \\kappa_n \\big({\\mathbb{e}}\\big[\\sqrt{\\| \\boldsymbol{\\eta}_i        \\|^2 } \\big ] \\big ) \\\\",
    "\\notag & \\!\\!\\le \\kappa_n\\big(\\sqrt{{\\mathbb{e}}\\big[\\|\\boldsymbol{\\eta}_i\\|^2        \\big ] } \\big ) \\!=\\ !",
    "\\kappa_n\\big(\\sqrt{\\sum\\nolimits_{k = 1}^\\infty        b_{i , k}^2 } \\big ) \\!=\\ !",
    "\\kappa_n \\big(\\gamma_i \\sqrt{\\zeta(2 p_i ) } \\big ) .",
    "\\end{aligned}\\ ] ] the result follows from   and  .",
    "the following result describes the trade - off between accuracy and privacy .",
    "the proof follows by direct substitution .    _",
    "( privacy - accuracy trade - off):_[cor : pri - acc - rel ] under the hypotheses of theorem  [ thm : accuracy ] , if @xmath481 in   for all @xmath206 , then @xmath482    in corollary  [ cor : pri - acc - rel ] , @xmath483 and @xmath471 are chosen independently , which in turn determines the value of @xmath484 according to  .",
    "also , it is clear from   that in order for the accuracy of the coordination algorithm not to be affected by the smoothening step , each agent @xmath155 has to take the value of @xmath485 sufficiently small so that it is negligible relative to @xmath486 . in particular , this procedure can be executed for any arbitrarily large value of @xmath471 , so that in the case of no privacy requirements at all , perfect accuracy is recovered , as specified in problem  [ problem ] .",
    "_ ( accuracy bound for sufficiently large domains ) : _ one can obtain a less conservative bound than   on the accuracy of the proposed class of algorithms if the minimizers of all the agents objective functions are sufficiently far from the boundary of @xmath166 .",
    "this can be made precise via corollary  [ cor : large - domain ] .",
    "if the aggregate objective function satisfies   and the amount of noise is also sufficiently small so that the minimizer of the sum of the perturbed objective functions satisfies this condition , then invoking corollary  [ cor : large - domain ] , we have @xmath487 ,    \\end{aligned}\\ ] ] where the equality holds under the assumption that @xmath481 in   for all @xmath155 .",
    "in this section , we report simulation results of our algorithm for example  [ example ] with @xmath488 ^ 2 $ ] , @xmath489 , @xmath490 , and @xmath491 .",
    "the orthonormal basis of @xmath26 is constructed from the gram - schmidt orthogonalization of the taylor functions and the series is truncated to the second , sixth , and fourteenth orders , resulting in @xmath492 , @xmath493 , and @xmath494-dimensional coefficient spaces , respectively .",
    "this truncation also acts as the smoothening step described in section  [ subsec : en - smooth ] , where higher truncation orders result in smaller  @xmath390 .",
    "we evaluate the projection operator in   by numerically solving the convex optimization problem @xmath495 , where @xmath496 is the result of the truncation .",
    "the parameters of @xmath43 are given by @xmath497 , @xmath498 , and @xmath499 where @xmath500 .",
    "rather than implementing any specific distributed coordination algorithm , we use an iterative interior - point algorithm on @xmath501 and @xmath314 to find the perturbed @xmath470 and original @xmath107 optimizers , respectively ( these points correspond to the asymptotic behavior of any provably correct distributed optimization algorithm with the perturbed and original functions , respectively ) .",
    "the privacy levels are chosen the same for all agents , i.e. , @xmath502 , and @xmath503 is swept logarithmically over @xmath504 $ ] . for each @xmath155 , we set @xmath505 and @xmath506 . for each value of @xmath503 and truncation order",
    ", the simulations are repeated 20 times to capture the stochasticity of the solutions .",
    "figure  [ fig : sweep ] illustrates the error @xmath507 as a function of @xmath503 for different truncation orders , together with the best linear fit of @xmath508 against @xmath509 , and the upper bound obtained in corollary  [ cor : pri - acc - rel ] .",
    "the conservative nature of this upper bound can be explained by noting the approximations leading to the computation of @xmath477 in proposition  [ prop : argmin - lip ] , suggesting that there is room for refining this bound .",
    "figure  [ fig : sweep ] shows that accuracy keeps improving as the privacy requirement is relaxed until the @xmath390-term ( resulting from the smoothening / truncation ) dominates the error .",
    "this `` saturation '' value can be decreased by increasing the truncation order ( which comes at the expense of more computational complexity ) , in contrast with the behavior of message - perturbing algorithms , cf .",
    "figure  [ fig : huang ] .",
    "it is important to mention that the respective error values for a fixed @xmath181 can not be compared between figures  [ fig : huang ] and  [ fig : sweep ] because , in  @xcite , @xmath181 is defined as the total exponent in  , i.e. , @xmath510 .",
    "however , it can be seen that the accuracy in figure  [ fig : huang ] is almost indifferent to the value of @xmath181 and is in the same order as @xmath511 .",
    "this is explained by the impossibility result of proposition iv.2 : since the noise - free algorithm of  @xcite is not asymptotically stable , depending on the specific application , its accuracy may not be desirable regardless of the value of  @xmath181 .",
    "in contrast , the accuracy in figure  [ fig : sweep ] keeps improving as @xmath181 is increased ( with an appropriate choice of truncation order ) .",
    "we have studied the distributed optimization of the sum of local strongly convex objective functions by a group of agents subject to the requirement of differential privacy .",
    "we have established the incompatibility between differential privacy and the asymptotic stability of the underlying ( noise - free ) dynamics for coordination strategies based on the perturbation of the messages among agents .",
    "this has motivated us to develop a new framework for differentially private release of functions from the @xmath0 space that can be applied to arbitrary separable hilbert spaces .",
    "we have also carefully described how perturbed functions with the desired regularity requirements can be provided to any distributed coordination algorithm while preserving privacy .",
    "finally , we upper bounded the accuracy of the resulting class of distributed , differentially private coordination algorithms .",
    "future work will analyze how to relax the smoothness and convexity assumptions on the local objective functions and the compactness hypothesis on their domains , characterize the _ optimal _ privacy - accuracy trade - off curve of distributed coordination algorithms based on objective perturbation , characterize the expected suboptimality gap of distributed coordination algorithms for a given desired level of privacy , provide procedures for choosing the truncation order of the functional expansion based on the objective function properties ( which must itself be differentially private ) , compare the numerical efficiency of different orthonormal bases for the @xmath0 space , and further understand the appropriate scale of the privacy parameter for specific application domains .",
    "the authors would like to thank the anonymous reviewers for helpful comments and suggestions that helped improve the presentation .",
    "this research was partially supported by nsf award cns-1329619 and award fa9550 - 15 - 1 - 0108 .",
    "10 [ 1]#1 url@rmstyle [ 2]#2    d.  p. bertsekas and j.  n. tsitsiklis , _ parallel and distributed computation : numerical methods_.1em plus 0.5em minus 0.4em athena scientific , 1997 .",
    "a.  nedic , a.  ozdaglar , and p.  a. parrilo , `` constrained consensus and optimization in multi - agent networks , '' _ ieee transactions on automatic control _ ,",
    "55 , no .  4 , pp . 922938 , 2010 .",
    "b.  johansson , m.  rabi , and m.  johansson , `` a randomized incremental subgradient method for distributed optimization in networked systems , '' _ siam journal on control and optimization _",
    "20 , no .  3 , pp .",
    "11571170 , 2009 .",
    "m.  zhu and s.  martnez , `` on distributed convex optimization under inequality and equality constraints , '' _ ieee transactions on automatic control _ , vol .",
    "57 , no .  1 ,",
    "pp . 151164 , 2012 .",
    "b.  gharesifard and j.  corts , `` distributed continuous - time convex optimization on weight - balanced digraphs , '' _ ieee transactions on automatic control _ ,",
    "59 , no .  3 , pp .",
    "781786 , 2014 .",
    "j.  c. duchi , a.  agarwal , and m.  j. wainwright , `` dual averaging for distributed optimization : convergence analysis and network scaling , '' _ ieee transactions on automatic control _ ,",
    "57 , no .  3 , pp .",
    "592606 , 2012 .    c.  dwork , f.  mcsherry , k.  nissim , and a.  smith , `` calibrating noise to sensitivity in private data analysis , '' in _ proceedings of the 3rd theory of cryptography conference",
    "_ , new york , ny , mar .",
    "2006 , pp . 265284 .    c.  dwork ,",
    "`` differential privacy , '' in _ proceedings of the 33rd international colloquium on automata , languages and programming ( icalp ) _ , venice ,",
    "italy , july 2006 , pp .",
    "112 .    c.  dwork and a.  roth ,",
    "`` the algorithmic foundations of differential privacy , '' _ found .",
    "trends theor .",
    "_ , vol .  9 , no . 3 - 4 , pp . 211407 , aug . 2014 .",
    "k.  chaudhuri , c.  monteleoni , and a.  d. sarwate , `` differentially private empirical risk minimization , '' _ the journal of machine learning research _ , vol .",
    "12 , pp . 10691109 , 2011 .",
    "d.  kifer , a.  smith , and a.  thakurta , `` private convex empirical risk minimization and high - dimensional regression , '' in _",
    "25th annual conference on learning theory _ , vol .  23 , 2012 ,",
    ". 25.125.40 .    j.  zhang , z.  zhang , x.  xiao , y.  yang , and m.  winslett , `` functional mechanism : regression analysis under differential privacy , '' _ proceedings of the vldb endowment _ , vol .  5 , no .  11 , pp .",
    "13641375 , july 2012 .",
    "r.  hall , a.  rinaldo , and l.  wasserman , `` differential privacy for functions and functional data , '' _ the journal of machine learning research _ , vol .  14 , no .  1 ,",
    "703727 , jan . 2013 .    a.  rajkumar and s.  agarwal , `` a differentially private stochastic gradient descent algorithm for multiparty classification , '' in _ proceedings of the 15th international conference on artificial intelligence and statistics _ , vol .  22 , 2012 , pp . 933941",
    ".    s.  song , k.  chaudhuri , and a.  d. sarwate , `` stochastic gradient descent with differentially private updates , '' in _ proceedings of the global conference on signal and information processing_.1em plus 0.5em minus 0.4emieee , dec .",
    "2013 , pp . 245248 .",
    "k.  chaudhuri , d.  j. hsu , and s.  song , `` the large margin mechanism for differentially private maximization , '' in _ advances in neural information processing systems 27_.1em plus 0.5em minus 0.4em curran associates , inc . , 2014 , pp .",
    "12871295 .",
    "z.  huang , s.  mitra , and n.  vaidya , `` differentially private distributed optimization , '' in _ proceedings of the 2015 international conference on distributed computing and networking _ , pilani , india , jan .",
    "s.  han , u.  topcu , and g.  j. pappas , `` differentially private distributed constrained optimization , '' _ ieee transactions on automatic control _ , 2016 , to appear .",
    "m.  t. hale and m.  egerstedt , `` differentially private cloud - based multi - agent optimization with constraints , '' in _ american control conference_.1em plus 0.5em minus 0.4emchicago , il : ieee , july 2015 , pp .",
    "12351240 .",
    "k.  chatzikokolakis , m.  e. andrs , n.  e. bordenabe , and c.  palamidessi , `` broadening the scope of differential privacy using metrics , '' in _ privacy enhancing technologies _ , ser .",
    "lecture notes in computer science.1em plus 0.5em minus 0.4emspringer berlin heidelberg , 2013 , vol . 7981 , pp .",
    "82102 .",
    "e.  kreyszig , _ introductory functional analysis with applications_. 1em plus 0.5em minus 0.4emjohn wiley & sons , 1989 .",
    "c.  cai and a.  r. teel , `` results on input - to - state stability for hybrid systems , '' in _ 44th ieee conference on decision and control and european control conference_.1em plus 0.5em minus 0.4emseville , spain : ieee , dec . 2005 , pp .",
    "54035408 .",
    "jiang and y.  wang , `` input - to - state stability for discrete - time nonlinear systems , '' _ automatica _ , vol .",
    "37 , no .",
    "857869 , 2001 .",
    "j.  c. duchi , m.  i. jordan , and m.  j. wainwright , `` local privacy and statistical minimax rates , '' in _ foundations of computer science ( focs ) , 2013 ieee 54th annual symposium on _ , oct 2013 , pp . 429438 .    h.  jeffreys and b.  s. jeffreys , _ methods of mathematical physics _ , 3rd  ed.1em plus 0.5em minus 0.4emcambridge university press , 1999 .",
    "j.  l. ny and g.  j. pappas , `` differentially private filtering , '' _ ieee transactions on automatic control _",
    "59 , no .  2 ,",
    "pp . 341354 , 2014 .",
    "r.  durrett , _ probability : theory and examples _ , 4th  ed .",
    "series in statistical and probabilistic mathematics.1em plus 0.5em minus 0.4emcambridge university press , 2010 .",
    "r.  t. rockafellar , _",
    "convex analysis_.1em plus 0.5em minus 0.4emprinceton university press , 1970 .",
    "r.  e. williamson and h.  f. trotter , _ multivariable mathematics _ , 4th  ed.1em plus 0.5em minus 0.4empearson education , inc .",
    ", june 2003 .",
    "here , we establish the lipschitzness of the @xmath1 map under suitable assumptions . this is a strong result of independent interest given that @xmath1 is not even continuous for arbitrary @xmath512 functions .",
    "our accuracy analysis for the proposed class of distributed , differentially private algorithms in section  [ subsec : alg - des - anal ] relies on this result .",
    "we begin with a lemma stating a geometric property of balls contained in convex , compact domains .    _",
    "( minimum portion of balls contained in convex compact domains):_[lem : portion ] assume @xmath157 is convex , compact , and has nonempty interior and let @xmath513 denote its inradius",
    ". then , there exists @xmath514 such that , @xmath515 for any @xmath399 and @xmath516 .",
    "let @xmath517 be the inball of @xmath28 , i.e. , the largest ball contained in @xmath28 .",
    "if this ball is not unique , we pick one arbitrarily . since @xmath518 , @xmath513 .",
    "let @xmath519 be the radius of the largest ball centered at @xmath520 that contains @xmath28 . since @xmath28 is compact , @xmath521 .",
    "for any @xmath522 that is on or outside of @xmath517 , let @xmath523 be the intersection of @xmath517 and the hyperplane passing through @xmath520 and perpendicular to @xmath524 .",
    "consider the cone @xmath525 where @xmath526 denotes convex hull . since @xmath28 is convex , @xmath527 .",
    "note that @xmath528 has half angle @xmath529 so the solid angle at its apex is @xmath530 therefore , for any @xmath516 , the proportion @xmath531 of @xmath532 is contained in @xmath28 where @xmath533 is the total @xmath534-dimensional solid angle . for any @xmath193 inside @xmath517",
    ", the same argument holds with @xmath535 therefore , for arbitrary @xmath399 , the statement holds with @xmath536      _",
    "( @xmath44-lipschitzness of @xmath1):_[prop : argmin - lip ] for any two functions @xmath537 , @xmath538 where @xmath539 is given by @xmath540 @xmath541 and @xmath542 are as in lemma  [ lem : portion ] , @xmath543 is the diameter of @xmath28 , and @xmath544 is defined for all @xmath56 by @xmath545    we consider the case where @xmath546 since the statement is trivial otherwise .",
    "let @xmath547 , @xmath548 , @xmath549 , @xmath550 , and @xmath551 .",
    "without loss of generality , assume @xmath552 .",
    "define , @xmath553 for all @xmath399 .",
    "since @xmath537 , we can integrate @xmath554 and @xmath555 twice to get , @xmath556 it follows that , for all @xmath399 , @xmath557^+ \\ge [ f_l(x ) - g_u(x ) - m]^+ ,     \\end{aligned}\\ ] ] where @xmath558^+ = \\max\\{z , 0\\}$ ] for any @xmath559 . after some computations , we get @xmath560 where @xmath561 therefore , the region where @xmath562 is @xmath21 .",
    "next , we seek to identify a subset inside this ball where we can determine a strictly positive lower bound of @xmath563 that depends on the difference @xmath564 . to this effect ,",
    "note that @xmath565 , since @xmath566 and , by the convexity of the problem , @xmath567 .",
    "let @xmath568 be the radius of the largest ball centered at @xmath569 and contained in @xmath21 .",
    "we have , @xmath570 where in the last inequality , we have used @xmath571 .",
    "next , note that for all @xmath572 , @xmath573 using the bound @xmath574 , we get after some simplifications , @xmath575 for all @xmath576 .",
    "therefore , @xmath577^+)^2 d x      \\\\      & \\ge \\int_{b(b , \\frac{\\underline r}{2 } ) \\cap d } ( f_l(x ) - g_u(x )      - m)^2 d x      \\\\      & \\ge \\frac{\\alpha^2}{16 } |a - b|^4 m\\left(b(b , \\tfrac{\\underline          r}{2 } ) \\cap d\\right )      \\\\      & \\ge \\frac{\\alpha^2}{16 } |a - b|^4 m\\left(b(b ,        \\tfrac{\\mu_{\\alpha,\\beta}(|a - b|)}{2 } ) \\cap d\\right ) .",
    "\\end{aligned}\\ ] ] now , we invoke lemma  [ lem : portion ] to lower bound the last term .",
    "note that @xmath578 for all @xmath579 .",
    "therefore , @xmath580 , so by lemma  [ lem : portion ] , @xmath581 which yields  .",
    "the next result shows that if the minimizers of @xmath314 and @xmath388 are sufficiently far from the boundary of @xmath28 , then their gradients need not be uniformly bounded and yet one can obtain a less conservative characterization of the @xmath44-lipschitz property of the @xmath1 map .    _",
    "( @xmath44-lipschitzness of @xmath1 for sufficiently large domains):_[cor : large - domain ] if @xmath314 and @xmath388 belong to @xmath582 and @xmath583 where @xmath584 and @xmath585 , then @xmath586 where @xmath587    the proof follows in the same lines as the proof of proposition  [ prop : argmin - lip ] ( and we use here the same notation ) . since the minimizers of @xmath314 and @xmath388 lie in the interior of @xmath166 , @xmath588 .",
    "the main difference here is that due to  , we have for all @xmath589 that @xmath590 so @xmath591 .",
    "therefore , one can integrate @xmath592 on the whole @xmath21 instead of its lower bound   on the smaller ball @xmath593 . to explicitly calculate the value of the resulting integral , one can use the change of variables @xmath594 and then use the formula @xmath595 where @xmath596 .",
    "the result then follows from straightforward simplifications of the integral .",
    "erfan nozari received his b.sc .",
    "degree in electrical engineering - controls from isfahan university of technology , isfahan , iran in 2013 and m.sc . in mechanical engineering from university of california , san diego , ca , usa in 2015 .",
    "he is currently pursuing his ph.d .",
    "degree in mechanical engineering from university of california , san diego , ca , usa . he has been the recipient of the campus - wide best undergraduate student award in 2013 from isfahan university of technology and the mechanical and aerospace engineering recruitment fellowship in 2014 from the university of california , san diego .",
    "his research interests include complex networks , brain networks , networked and distributed control systems , and differential privacy .",
    "pavankumar tallapragada received the b.e .",
    "degree in instrumentation engineering from sggs institute of engineering @xmath597 technology , nanded , india in 2005 , m.sc .",
    "degree in instrumentation from the indian institute of science , bangalore , india in 2007 and the ph.d .",
    "degree in mechanical engineering from the university of maryland , college park in 2013 .",
    "he is currently a postdoctoral scholar in the department of mechanical and aerospace engineering at the university of california , san diego .",
    "his research interests include event - triggered control , networked control systems , distributed control and networked transportation systems .",
    "[ ] jorge corts received the licenciatura degree in mathematics from universidad de zaragoza , zaragoza , spain , in 1997 , and the ph.d .",
    "degree in engineering mathematics from universidad carlos iii de madrid , madrid , spain , in 2001 .",
    "he held postdoctoral positions with the university of twente , twente , the netherlands , and the university of illinois at urbana - champaign , urbana , il , usa . he was an assistant professor with the department of applied mathematics and statistics , university of california , santa cruz , ca , usa , from 2004 to 2007 .",
    "he is currently a professor in the department of mechanical and aerospace engineering , university of california , san diego , ca , usa .",
    "he is the author of geometric , control and numerical aspects of nonholonomic systems ( springer - verlag , 2002 ) and co - author ( together with f. bullo and s. martnez ) of distributed control of robotic networks ( princeton university press , 2009 ) .",
    "he is an ieee fellow and an ieee control systems society distinguished lecturer .",
    "his current research interests include distributed control , networked games , opportunistic state - triggered control and coordination , power networks , distributed optimization , spatial estimation , and geometric mechanics ."
  ],
  "abstract_text": [
    "<S> we study a class of distributed convex constrained optimization problems where a group of agents aim to minimize the sum of individual objective functions while each desires that any information about its objective function is kept private . </S>",
    "<S> we prove the impossibility of achieving differential privacy using strategies based on perturbing the inter - agent messages with noise when the underlying noise - free dynamics are asymptotically stable . </S>",
    "<S> this justifies our algorithmic solution based on the perturbation of individual functions with laplace noise . to this end </S>",
    "<S> , we establish a general framework for differentially private handling of functional data . </S>",
    "<S> we further design post - processing steps that ensure the perturbed functions regain the smoothness and convexity properties of the original functions while preserving the differentially private guarantees of the functional perturbation step . </S>",
    "<S> this methodology allows us to use any distributed coordination algorithm to solve the optimization problem on the noisy functions . finally , we explicitly bound the magnitude of the expected distance between the perturbed and true optimizers which leads to an upper bound on the privacy - accuracy trade - off curve . </S>",
    "<S> simulations illustrate our results .    networks of autonomous agents , optimization , distributed algorithms / control , differential privacy , cyber - physical systems </S>"
  ]
}