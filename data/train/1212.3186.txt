{
  "article_text": [
    "acquiring and processing information about the instantaneous state of the environment is a prerequisite for survival for any living system .",
    "sensory and signal transducting networks have evolved to achieve this task under a variety of external conditions as , e.g. , the work on bacteria like _ escherichia coli _ has demonstrated so beautifully @xcite . maintaining any biochemical network",
    ", however , has a metabolic cost associated with its inherent non - equilibrium nature .",
    "this fact prompts the question whether there is a relation between the performance of the network and its free energy consumption . for sensory adaptation in _ e. coli _ , such a trade - off between adaptation speed and accuracy , and the energy dissipation rate has recently been found in a theoretical model and confirmed with experimental data @xcite .    from a more information - theoretic perspective , the question alluded to above can be formulated more generally as to whether there is any quantitative relation between the rate with which such a network acquires information about the ever changing environmental conditions and the rate of entropy production associated with the functioning of the network .",
    "the universal concept for quantifying one side of this balance is mutual information , or , more precisely , the rate of mutual information . in the context of sensing , this rate has been introduced in @xcite , where it was explored for the special case of gaussian input and output signals and various network motifs . importantly , as pointed out in @xcite , because it takes temporal correlations into account the rate of mutual information is different from the static mutual information , which has been the subject of several recent investigations in genetic regulatory networks @xcite . on the other hand ,",
    "calculating the rate of mutual information without the assumption of gaussian statistics represents a main challenge .    in this paper",
    ", we obtain an upper bound on the rate of mutual information for a general bipartite model consisting of an environment that switches stochastically between an arbitrary number of states and a network for which the internal transition rates depend on the instantaneous state of the environment .",
    "this set - up has the advantage that the total system is in a non - equilibrium steady state generated by a markovian dynamics which facilitates the analysis .",
    "still , since the internal process is non - markovian , deriving the expression for this bound as done below becomes non - trivial .",
    "moreover , in order to calculate the precise value of the rate of mutual information we apply a numerical method that estimates the entropy rate of a single time - series produced from a numerical simulation @xcite . for evaluating the second side of the balance introduced above",
    ", we need the rate of ( thermodynamic ) entropy production , as it has been derived for any network with given transition rates some time ago @xcite and revitalized in the context of stochastic thermodynamics as reviewed in @xcite .",
    "significant progress in relating information - theoretic concepts to thermodynamic ones has recently been achieved in the context of feedback - driven systems @xcite . for these systems ,",
    "the amount of mutual information between system and controller enters the corresponding thermodynamic expressions on the level of generalized fluctuation theorems and second - law like inequalities .",
    "in particular , the net - power output of such systems can not exceed the rate of mutual information acquired .",
    "the single cell , however , is an autonomous system for which a separation into an act of measurement and subsequent feedback control does not come naturally . whether despite this fundamental difference between a feedback - driven system and an autonomous one , an analogous constraint on a putative efficiency relating the rate of acquiring information with the thermodynamic cost of maintaining the sensory network",
    "exists will be explored here .",
    "the paper is organized as follows . in section [ sec2 ]",
    "we analyze the rate of mutual information in a simple toy model .",
    "the general framework , for which our results are valid , is introduced in section [ sec3 ] .",
    "section [ sec4 ] contains the comparison between the dissipation rate and the mutual information rate for a model of a cell computing an external ligand concentration .",
    "the rate of mutual information in equilibrium is discussed in section [ sec5 ] .",
    "we conclude in section [ sec6 ] .",
    "coupled to an external one @xmath0.,width=272 ]",
    "in order to introduce this rate of mutual information between external and internal processes and compare it to the entropy production , we first consider the minimalistic model shown in fig . [ fig1 ] . the environment is supposed to switch stochastically with rate @xmath1 between two possible states @xmath2 and @xmath3 . within the system",
    ", there is an internal degree of freedom , which exists also in two different states that are correlated with the environment state .",
    "specifically , if the external state is @xmath2 ( @xmath3 ) and the internal state is in the `` false '' state @xmath4 ( @xmath5 ) , the internal state changes to @xmath5 ( @xmath4 ) with a rate @xmath6 .",
    "we also allow for a ( small ) rate @xmath7 inducing `` false '' transitions .",
    "later we will consider a more realistic model for bacterial chemotaxis , where the external states correspond to the state of a receptor sitting on the membrane , while the internal states correspond to some internal protein that can be transformed to an active form with transition rates that depend on the state of the receptor .",
    "as we will show below in equation ( [ mutualcont ] ) using a general framework , the rate @xmath8 at which the internal system acquires information about the time - dependent state of the environment is bounded from above by @xmath9 where @xmath10 for @xmath11 and in the limit @xmath12 , the rate of mutual information can be understood as follows .",
    "the uncertainty about the external time - series @xmath13 still left after recording the internal time - series @xmath14 is basically the specific time lapse between a change in the external conditions and the change of the internal state , since for @xmath11 an internal jump takes place only after an external jump .",
    "thus the system can localize the change in the external conditions within a time window of width @xmath15 .",
    "encoding the length of intervals between switching events on a scale @xmath15 requires a number of order @xmath16 , which carries @xmath17 units of information ( measured throughout this paper using natural logarithms ) .",
    "since these switching events take place with a rate @xmath1 , the rate of mutual information , for @xmath11 and @xmath12 , is @xmath18 .",
    "therefore , in this limit , the upper bound ( [ mutualtoy1 ] ) gives the correct value of the rate of mutual information .",
    ", estimated with the numerical method explained in the text , the upper bound @xmath19 , and the thermodynamic entropy production @xmath20 as a function of @xmath7 for @xmath21 and @xmath22 . here and in the following figures the common unit of time is arbitrary and the error in the numerics is less than the size of the symbols . , width=272 ]    the dissipation rate @xmath20 associated with this network follows from the standard expression for markovian processes , recalled in equation ( [ sdef ] ) below , which gives @xmath23 where @xmath24 the thermodynamic entropy production fulfills @xmath25 and is zero only if detailed balance is fulfilled ( @xmath26 ) .",
    "whereas the rate of mutual information is finite at @xmath11 , the entropy production diverges as @xmath27 .",
    "therefore , when the system is far from equilibrium ( @xmath28 ) the dissipation rate is larger than the mutual information rate . on the other hand , at equilibrium ( @xmath26 ) both quantities are zero : there is no dissipation and the internal and external processes are uncorrelated .",
    "note that in this case also the internal process becomes markovian . in fig .",
    "[ fig2 ] we compare the rate of mutual information obtained from numerical simulations , as explained below , with the thermodynamic entropy production .",
    "there is a value of @xmath7 , which depends on @xmath6 and @xmath1 , beyond which the mutual information rate becomes larger than the thermodynamic entropy production .",
    "thus , as our first main result , we have found that in the most simple conceivable model the rate of mutual information is not bounded by the dissipation rate .",
    "we now address the problem of calculating the upper bound on the rate of mutual information within a general bipartite model . it will be convenient to first treat time as discrete with a spacing @xmath29 .",
    "we denote the external states by @xmath30 and their transition probabilities by @xmath31 , where the transition is from @xmath32 to @xmath33 . the transition probability from the internal state @xmath34 to the internal state @xmath35 , given that the external state is @xmath32 , is @xmath36 .",
    "hence , the transition probabilities of the internal transitions depend on the instantaneous external state . states of the total system are thus determined by the pair @xmath37 , with the transition probabilities of this full markov process given by @xmath38 in the context of cellular sensing , the external transition rates @xmath39 are related to processes that happen outside the cell and that are not influenced by the biochemical reactions inside the cell .",
    "the internal biochemical reactions are related to the transition rates @xmath40 , depending on the external state @xmath32 .",
    "a central consequence of the independence of @xmath39 on the internal state @xmath34 is that the external process is also markovian , while the internal process is in general non - markovian .",
    "a stochastic trajectory with @xmath41 jumps is denoted by @xmath42 , where @xmath43 ( @xmath44 ) represents the external ( internal ) process .",
    "the information - theoretic entropy rate of the full process measures how much the entropy of the trajectory grows with @xmath41 . in the limit @xmath45 , it is given by @xcite @xmath46 where @xmath47 denotes the stationary state probability distribution and , from equation ( [ defrates ] ) ,",
    "the diagonal terms take the form @xmath48 moreover , the external process @xmath43 is also markovian , which implies for its entropy rate @xmath49 where @xmath50 and @xmath51 . here , we are interested in calculating the rate of mutual information defined as @xcite @xmath52    the still missing piece for evaluating this expression is the entropy rate of the internal process @xmath53 , which is not known because @xmath54 is in general non - markovian .",
    "however , this quantity is bounded from above and from below by the relation @xcite @xmath55 which involves conditional entropies . here ,",
    "we are considering a finite sequence of @xmath56 jumps of the internal process starting with the stationary state probability distribution .",
    "these bounds become better as @xmath56 increases and for @xmath57 both converge to the same value which is the entropy rate @xmath53 @xcite .",
    "the first upper bound is given by @xmath58 using the transition rates ( [ defrates ] ) , for @xmath59 , we have @xmath60 where we substituted @xmath61 , @xmath62 , and @xmath63 . with this relation",
    "it is easy to obtain @xmath64 moreover , from ( [ entropyxy ] ) and ( [ entropyx ] ) , we get @xmath65 therefore , the first upper bound for the rate of mutual information , as obtained from ( [ mutualdef ] ) , ( [ hy1 ] ) , and ( [ hxyhx ] ) , reads @xmath66 where @xmath67 is the mean internal transition rate and @xmath68 is the probability of being in the external state @xmath33 given that the internal state is @xmath34 .",
    "more generally , we find that up to @xmath69 all upper bounds with finite @xmath56 are given by this expression , meaning that @xmath70 as for the lower bounds , we find that @xmath71 for all @xmath56 .",
    "therefore , we conclude that the rate of mutual information in the continuous time limit is bounded from above by @xmath72 we note that all the bare entropy rates diverge as @xmath73 for @xmath74 and , therefore , can not be defined in this limit @xcite",
    ". however , the rate of mutual information is a well behaved quantity in this limit with no logarithmic divergences .     for @xmath75 , @xmath76 , and @xmath77 .",
    "the simulations were done for a time series with @xmath78.,width=272 ]    let us now explain the numerical method we use to estimate the entropy rate @xmath53 of the non - markovian time series @xmath44 .",
    "the random matrix @xmath79 , is defined as @xmath80 $ ] , where @xmath81 $ ] is a conditional probability .",
    "if the external states take the values @xmath82 , then @xmath83 is a @xmath84 matrix , where the internal variables @xmath85 make the matrix random .",
    "it is simple to show that @xcite @xmath86=   \\mathcal{v } \\mathcal{t}(y_n , y_{n-1})\\ldots \\mathcal{t}(y_1,y_{0 } ) \\mathcal{p}_0\\ ] ] where @xmath87 $ ] is the probability of the time series , @xmath88 is a vector with all components equal to one and @xmath89 is a vector with components @xmath90 , for @xmath91 . from this relation , it is then possible to show that given a typical long time series @xmath92 , one can estimate the entropy rate @xmath53 by the formula @xcite @xmath93 where @xmath94 is any matrix norm .",
    "therefore , following ref .",
    "@xcite , we can calculate @xmath53 numerically by generating an internal time - series with a simulation and calculating the above product by normalizing it after a certain number of steps ( keeping track of the normalization factors ) , as explained in @xcite . in fig .",
    "[ fig3 ] , we plot upper and lower bounds on the mutual information rate , obtained from ( [ bounds ] ) , and the numerical simulation result .",
    "for small @xmath29 , the numerically obtained rate of mutual information shows linear behavior , which we extrapolate to @xmath74 to estimate the rate of mutual information in the continuous time limit .",
    "the other central physical observable we consider here is the thermodynamic entropy production @xcite , which for the present class of markov processes reads @xmath95 where the first term on the right hand side is due to the external transitions and the second is the contribution from internal transitions .",
    "if the same pair of internal states can be connected by different types of transitions , as in the model discussed next , the second term requires an additional summation over the different `` channels '' @xmath96 with corresponding rates @xmath97 @xcite .",
    "we finally use this framework to analyze a thermodynamically consistent minimal four - states model for a cell estimating a ligand concentration @xmath98 @xcite ) .",
    "the external states are related to a receptor that can be either bound by a ligand or empty .",
    "moreover , the receptor can be in an `` on '' state or `` off '' state .",
    "we assume a high binding affinity of the on state , i. e. , whenever the receptor is occupied by an external ligand it is in the on state . likewise , any unbound receptor is in the off state . under these simplifying conditions",
    "the state of the receptor ( on / off ) corresponds to what was called above the external process @xmath99 .",
    "the transition from on to off happens at a rate @xmath100 , whereas the binding rate @xmath101 is proportional to the ligand concentration @xmath98 .",
    "the internal state is associated with a downstream protein that can be in an inactive ( @xmath102 ) or , due to phosphorylation , in an active state @xmath103 . for simplicity",
    ", we assume a two states internal system corresponding to one downstream protein . in reality , the number of downstream proteins is much larger than one @xcite , however , this simplification is not harmful for the qualitative comparison between the mutual information rate and thermodynamic entropy production .",
    "the crucial coupling of the internal process to the instantaneous state of the environment , here encoded by the state of the receptor , arises from the fact that the receptor in the on state speeds up phosphorylation , which happens at a rate @xmath104 , by a factor @xmath105 compared to the action of an empty receptor .",
    "dephosphorylation occurs at rate @xmath106 , which leads to the following internal atp consumption cycle @xmath107{(a)\\kappa_+ } y^*+adp \\xrightleftharpoons[\\omega_-]{\\omega_+ } y+adp+p_i , \\label{eqreaction}\\ ] ] where the factor @xmath108 arises only if the receptor is in the on state .",
    "the full network of transitions is shown in fig .",
    "thermodynamic consistency requires , first , that we also allow for the reverse transitions of phosphorylation and dephosphorylation with non - zero rates @xmath109 and @xmath110 , respectively .",
    "second , it imposes a relation between the free energy associated with the atp hydrolysis @xmath111 and the kinetic rates , which reads @xmath112 where @xmath113 is boltzmann s constant and @xmath114 the temperature @xcite .    , the upper bound @xmath19 and the numerically determined rate of mutual information @xmath8 as a function of @xmath108 for the thermodynamically consistent model shown in the inset .",
    "the other parameters are set to @xmath115 , @xmath116 , @xmath117 , @xmath118 , and @xmath119.,width=272 ]    in fig .",
    "[ fig4 ] , we compare the rate of mutual information , obtained by the numerical simulation with the rate of free energy consumption @xmath20 as a function of the enzymatic enhancement @xmath108 . for @xmath120 , the internal state",
    "is decorrelated from the external one , leading to @xmath121 . however , if @xmath122 , the internal network still consumes free energy . with increasing @xmath108 the rate of mutual information increases , eventually becoming larger than the rate of free energy consumption .",
    "thus , this thermodynamically fully consistent model confirms what we have found previously in the simple model : in such autonomous networks the rate of mutual information is not bounded by the free energy consumption required to maintain the network .",
    "while a cell undoubtedly is a nonequilibrium system , it is instructive to explore whether mutual information can be non - zero even under equilibrium conditions , i.e. , for transition rates such that the steady state fulfills detailed balance .",
    "two cases must be distinguished .",
    "first , we stay within our framework of markov processes of the form ( [ defrates ] ) , where the external transition rates @xmath39 are independent of the internal state @xmath34 . a non - zero rate of mutual information in equilibrium can then occur if the external process only affects the time - scale of the internal transition rates , i.e. , if for all @xmath123 the ratio @xmath124 is independent of @xmath32 and if @xmath125 does not hold for all @xmath30 . as an example , consider the model of fig .",
    "[ fig1 ] with @xmath126 and the left vertical transition rates for @xmath127 multiplied by a factor @xmath128 .",
    "this is clearly an equilibrium model obeying detailed balance and for @xmath129 the rate of mutual information is not zero .    the above example is a rather contrived case where the external states distinguish between internal processes that differ only by the time - scale of the transitions .",
    "the question we raise in this paper , whether the dissipation rate bounds the rate of mutual information , is non - trivial for markov processes of the form ( [ defrates ] ) , for which the external process truly affects the internal process and for which if detailed balance is fulfilled in a point of the phase diagram then all internal processes are exactly the same , i.e. , @xmath130 . in this case , at equilibrium the internal process becomes also markovian and decoupled from the external process , implying a zero rate of mutual information .",
    "hence , it is not clear a priori whether the rate of mutual information is bounded by the dissipation rate under nonequilibrium conditions .",
    "second , a different situation arises if one gives up the condition that the external process @xmath99 is unaffected by the internal process @xmath54 . by dropping this distinction",
    ", a general pair of stochastic variables that together fully specify an underlying markov process fulfilling detailed balance might have a non - zero rate of mutual information .",
    "an important example in the context of chemotaxis is the monod - wyman - changeux ( mwc ) model @xcite .",
    "this model describes the allosteric interaction between the kinase activity of the receptor ( on / off ) and its affinity for ligand binding ( bound / unbound ) .",
    "this aspect can be made explicit also with the toy model of fig .",
    "[ fig1 ] . if we modify the transition rates from @xmath3 to @xmath2 when @xmath131 by a factor @xmath132 and when @xmath133 by an factor @xmath134 ( see @xcite for a similar model ) , we get an equilibrium mwc like model which should have a non zero rate of mutual information between @xmath99 and @xmath54 . for this variant ,",
    "it is not possible to distinguish an external process influencing the internal process but not being influenced by it .",
    "more precisely , the transition rates are no longer of the form ( [ defrates ] ) with @xmath39 independent of the internal state @xmath34 . while it would be interesting to calculate the rate of mutual information for the mwc model , our framework based on the rates of the form ( [ defrates ] ) is not yet appropriate to do so . in general",
    ", our framework is suited to study the rate of mutual information involving an external process that influences the chemical reactions inside the cell but is not affected by them .",
    "in this final section we first come back to a topic raised in the introduction , namely how our main result , i.e. , no bound between the rate of mutual information and dissipation , relates to the apparently quite different results for feedback - driven systems . in the latter ,",
    "information acquired through a measurement is used to extract net work from a thermal bath .",
    "the amount of net work is limited by the information . here , for the sensory network",
    ", we have investigated a complementary issue , namely whether the amount of information is limited by the chemical work , or free energy consumption , required to maintain the network .",
    "how this information is now used in a second step for an action that possibly performs work on some other element is an interesting but quite different question . as an important aside",
    ", we note that for an autonomous network the whole issue of writing the information into a memory whose erasure will require free energy @xcite is irrelevant as the erasure process is trivially included in the reaction scheme .    clearly , these concluding remarks touch on deep issues concerning a future theory comparing comprehensively autonomous with feedback - driven systems which is a distinction that may become blurred in the micro world . on a more specific level , our study",
    "should now be refined by incorporating further elements of a more elaborate model of a sensory network , i.e. , including several receptors , several proteins , allowing for adaptation and alike . the quantitative balance between information rate and entropy production",
    "will depend on such details , and thus , e.g. , optimization of the `` efficiency '' will become an interesting issue . searching for a hard universal thermodynamic bound for it , however , should be futile , as our simple model shows .",
    "furthermore , it would be worthwhile to investigate the relation between the rate of mutual information , the dissipation rate and the adaptation error in the context of the energy - speed - accuracy relation found in @xcite .",
    "finally , an exact calculation of the rate of mutual information which would replace the upper bound we have derived here remains an open mathematical challenge ."
  ],
  "abstract_text": [
    "<S> for sensory networks , we determine the rate with which they acquire information about the changing external conditions . comparing this rate with the thermodynamic entropy production that quantifies the cost of maintaining the network </S>",
    "<S> , we find that there is no universal bound restricting the rate of obtaining information to be less than this thermodynamic cost . </S>",
    "<S> these results are obtained within a general bipartite model consisting of a stochastically changing environment that affects the instantaneous transition rates within the system . </S>",
    "<S> moreover , they are illustrated with a simple four - states model motivated by cellular sensing . </S>",
    "<S> on the technical level , we obtain an upper bound on the rate of mutual information analytically and calculate this rate with a numerical method that estimates the entropy of a time - series generated with a simulation .    </S>"
  ]
}