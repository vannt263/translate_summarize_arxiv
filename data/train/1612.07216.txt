{
  "article_text": [
    "the histogram , introduced by karl pearson in 1895 , is one of the most basic but still one of the most widely used tools to visualize data .",
    "however , the construction of the histogram is not uniquely defined , leaving the user considerable freedom to choose the number of bins and their widths , see @xcite .",
    "this arbitrariness allows for radically different visual representations of the data , and it appears that no satisfactory rule for the construction is known , as evidenced by the large number of rules proposed in the literature . in the case of equal bin widths , popular examples of rules for the number of bins are those given by @xcite , which is still the default rule in r , @xcite , @xcite , @xcite , and @xcite .",
    "most of these rules are derived by viewing the histogram as an estimator of a density and choosing the number of bins to minimize an asymptotic estimate of risk .",
    "this leads to questions about the performance for small samples as well as about smoothness assumptions that are not verifiable .",
    "instead of making all bins equally wide , it is also common to give equal area to all blocks .",
    "@xcite point out that the first approach typically leads to oversmoothing in regions of high density and is poor at identifying sharp peaks , whereas the second oversmooths in regions of low density and does not identify small outlying groups of data .",
    "they advocate for a compromise of these two approaches that is motivated by regarding the histogram as an exploratory tool to identify structure in the data such as gaps and spikes , rather than as an estimator of a density , and they argue that relying on asymptotic risk minimization may lead to inappropriate recommendations for choosing the number of bins .",
    "this is in line with recent findings for the regressogram  @xcite , the regression ` counterpart ' for the histogram  @xcite . here",
    "the bin choice corresponds to finding _ locations _ of constant segments , which is a different target than conventional risk minimization , e.g. of @xmath1 norm , @xmath2 .",
    "this paper proposes a rule for constructing a histogram that is motivated by the two main goals of the histogram , see @xcite :    1 .",
    "the histogram provides estimates of probabilities via relative areas .",
    "2 .   the histogram provides a display of the  density  of the data that is simple but informative , i.e. it aims to have few bins , but still shows the important features of the data , such as modes .    the idea of the paper is to construct a confidence set of cumulative distribution functions ( cdfs ) such that each cdf in the confidence set satisfies 1 .  in an ( asymptotically ) optimal way . to meet 2 . , we select the simplest cdf in the confidence set , i.e. the one with the fewest bins , as our histogram cdf .",
    "the resulting histogram is the simplest histogram that shows important features of the data , such as increases , modes , or troughs .",
    "we call this histogram the _ essential histogram_. our approach is motivated by the fact that simplicity is a key aspect of the histogram : not only is it implicit in its goal to serve as an exploratory tool , but also in its definition as a piecewise constant function .",
    "we show that in a large sample setting , each cdf in the confidence set estimates probabilities of intervals with a standardized simultaneous estimation error that is at most twice of what is achievable and which is typically much smaller than those obtained from histograms that are constructed via traditional rules .",
    "likewise , we show that the cdfs are asymptotically optimal for detecting important features , such as increases or modes of the distribution .",
    "therefore , we attain the above two goals of the histogram asymptotically , but we stress that one of the main benefits of our construction is that is provides finite sample guaranteed confidence statements about features of the data : large increases ( or decreases ) of any histogram in the confidence set ( and hence of the essential histogram ) indicate significant increases ( or decreases ) in the true density ( cf .  theorem  [ thmfeatureinfer ] ) . we illustrate this by an example in figure  [ fig : empintro ] .",
    "it shows a computationally faster modification of the essential histogram ( see section  [ computation ] ) , still within our 90% confidence set .",
    "the finite sample guarantee then says that the true density has an increase on the two pink intervals , and has a decrease on the two blue ones , respectively , with simultaneous confidence at least 90% .",
    "it readily indicates that the truth has 2 modes and 1 trough , as the plotted intervals are disjoint  ( cf .",
    "these intervals are a selection of a much larger set of intervals of increase and decrease at all scales , the method offers ( see sections  [ optimalfeatures ] and  [ computation ] ) .",
    "thus , we can state with 90% guaranteed finite sample confidence that these modes or troughs are really there in the underlying population .",
    "we think that these confidence statements are quite valuable enhancements to the essential histogram as an exploratory tool .",
    "we also mention that any other histogram can be accompanied with our method to obtain such statements for it in order to justify ( or question ) modes it suggests ( see section  [ ss : evatool ] ) .",
    "is given in the left panel .",
    "in the upper part of the right panel , the reconstruction by the essential histogram ( eh ) with significance level @xmath3 and the true density are shown ; in the lower part , intervals indicating regions which are inferred to contain a point of increase ( decrease ) are plotted in pink ( blue ) .",
    "[ fig : empintro],scaledwidth=100.0% ]    the construction of the confidence set is based on the multiscale likelihood ratio test introduced by @xcite , which guarantees optimal detection of certain features in the data .",
    "@xcite use such a multiscale likelihood ratio test for inference on change - points in a regression setting and they employ the idea of selecting the function in the confidence set that has the fewest jumps . in the context of the histogram , it turns out that this approach includes jumps only at locations where the evidence in data requires the placement of jumps in order to show significant features and to provide good probability estimates .",
    "hence the methodology will not put any bins in regions where the density is close to flat .",
    "this built - in parsimony is what one would expect from an automatic method for constructing a histogram , see also the comments about open research problems in @xcite .",
    "the taut string method of @xcite can be interpreted as producing a histogram ( although not satisfying requirement 1 .  from above ) that has the smallest number of modes subject to the constraint that it lies in a confidence ball given by the kolmogorov metric .",
    "it is known that the kolmogorov metric will not result in good probability estimates for intervals unless they have large probability content .",
    "this procedure does not aim at parsimony of bins and will typically produce many more bins than the essential histogram ( although often providing visually appealing solutions , and estimating the number of modes very well , see section  [ examples ] ) , while the essential histogram automatically results in parsimony of bins and as a consequence also of modes as explained above .",
    "the rest of the paper is organized as follows . in section  [ confset ] ,",
    "we propose a multiscale confidence set of distribution functions , and an estimator , the essential histogram , within the confidence set .",
    "the optimality of the confidence set is examined from the probability estimation perspective in section  [ optimalprobs ] , and from the feature detection perspective in section  [ optimalfeatures ] . in section",
    "[ computation ] , we present an accelerated dynamic programming algorithm for a slightly relaxed version of the essential histogram , and show that the theoretical properties of this relaxed estimator essentially remain valid . the performance of the ( relaxed ) essential histogram is demonstrated by simulation in section  [ examples ] , where we also illustrate how the proposed confidence set works as an evaluation tool for any histogram estimator .",
    "a brief conclusion is given in section  [ conclusion ] .",
    "some further optimality results and all the proofs are in sections  [ optconfint ] and [ proofs ] in the appendix .    an implementation of the proposed method is provided in r - package `` esshist '' , available from http://www.stochastik.math.uni-goettingen.de/esshist .",
    "for any cdf @xmath4 and any interval @xmath5 we define @xmath6 @xmath7 provides a measure of the ` average density ' over @xmath5 without requiring any smoothness assumptions on @xmath4 .",
    "if @xmath4 does have a density , then @xmath8 equals the average of the density over @xmath5 . for a partition @xmath9 of the real line into intervals",
    "we define the corresponding _ histogram of @xmath4 _ as the density @xmath10 given by @xmath11 , where @xmath12 is the interval containing @xmath13 .",
    "we say that @xmath14 is a _ histogram cdf _ iff it is the cdf of a histogram , or equivalently , iff @xmath14 is a piecewise linear and continuous cdf .",
    "the histogram can be recovered from its cdf @xmath14 as the left - hand derivative of @xmath14 .    given univariate @xmath15 i.i.d .  with cdf @xmath4 , which from now on we assume to be _ continuous _",
    ", we consider the following @xmath16-confidence region for @xmath4 based on the empirical cdf @xmath17 : @xmath18 here @xmath19 is the log - likelihood ratio statistic for testing @xmath20 , and @xmath21 is the @xmath16-quantile of the distribution of @xmath22 with @xmath23 being a collection of intervals : @xmath24 : j , k \\in \\{1+i d_{\\ell } , i=0,1,\\ldots \\}\\ \\mbox { and } m_{\\ell}<k - j\\leq 2m_{\\ell}\\br\\},\\\\ & \\;\\;\\;\\;\\mbox { where } m_{\\ell}=n2^{-\\ell},\\ ; d_{\\ell}= \\bl\\lceil \\frac{m_{\\ell}}{6 \\sqrt{\\ell}}\\br\\rceil . \\end{aligned}\\ ] ] this collection of intervals was introduced in @xcite to approximate the collection of all intervals on the line in a computationally efficient manner : @xcite show that the above multiscale likelihood ratio statistic can be computed in in @xmath25 steps while at the same time the collection is still rich enough to guarantee optimal detection of certain effects .",
    "note that @xmath26 in   is distribution free .",
    "it is further known that @xmath27 is uniformly tight , and equivalently @xmath28 for every @xmath29 , see  ( * ? ? ?",
    "* proposition 1 ) , and figure  [ fig : symd ] for a visual illustration .     for @xmath30 ( dotted line ) , @xmath31 ( dashed line ) , and @xmath32 ( solid line ) .",
    "[ fig : symd],scaledwidth=95.0% ]    the _ essential histogram _ is defined as a histogram whose cdf lies in @xmath33 and whose number of bins minimizes the number of bins among such histograms .",
    "the essential histogram exists since @xmath33 clearly contains @xmath17 and hence also the histogram cdf obtained by linearly interpolating @xmath17 .",
    "we now show that all cdfs in @xmath33 possess certain optimality properties .",
    "in particular , those properties apply to the essential histogram and to the relaxed version ( see section  [ computation ] ) which admits fast computation .",
    "first we investigate how well @xmath34 perform with regard to the first goal of the histogram , namely estimating probabilities @xmath35 for intervals @xmath5 .",
    "to this end , for probabilities of size @xmath36 , we introduce the simultaneous standardized estimation error of @xmath14 as @xmath37 note that @xmath38 .",
    "thus , it suffices to consider @xmath39 $ ] for @xmath40 .",
    "our first result establishes a benchmark for this task by deriving the performance of the empirical cdf @xmath17 :    [ thmc1 ] for @xmath41 arbitrarily slowly as @xmath42 @xmath43 \\br ) \\ra 1\\ ] ] uniformly in @xmath4 .",
    "if @xmath44 , then @xmath45 uniformly in @xmath4 .",
    "the proposition shows that for the empirical cdf @xmath17 the simultaneous estimation error is very close to @xmath46 .",
    "in fact , it is not possible to improve on this bound , as explained in the proof .",
    "thus @xmath17 provides an optimal estimator for the collection @xmath47 , but its @xmath48 bins make it very ill suited for the use as a histogram .    as a second reference , proposition  [ thmc2 ] gives the performance of a histogram that uses @xmath49 equally sized bins .",
    "if one chooses @xmath50 bins as recommended by the common rules in the literature , then @xmath51 blows up at the rate @xmath52 for some rather typical continuous @xmath4 and @xmath53 , while the benchmark given by @xmath17 grows very slowly at a rate of @xmath54 .",
    "a similar result obtains if one uses bins with equal probabilty content rather than equal length .",
    "[ thmc2 ] let @xmath55 denote the cdf of a histogram that partitions @xmath56 $ ] into @xmath49 equally sized bins .",
    "then there exists a continuous @xmath4 such that for @xmath53 and odd @xmath49 @xmath57    if one is willing to make higher order smoothness assumptions on @xmath4 , then it can be shown that the performance of these common histogram rules gets much closer to the benchmark .",
    "one key advantage of our proposed histogram is that it essentially attains the benchmark in every case by automatically adapting to the local smoothness : theorem  [ thma](ii ) shows that with high probability all @xmath34 have a simultaneous estimation error equal to that of the benchmark @xmath17 up to a factor of at most 2 .",
    "this optimality property is due to the use of the likelihood ratio test in a multiscale fashion . at the same time",
    ", some @xmath14 in @xmath33 will have many fewer than the @xmath48 bins produced by @xmath17 : if the underlying density is locally close to flat , then the multiscale likelihood ratio test will not exclude a candidate @xmath14 that has no jumps in that local region .",
    "thus the @xmath34 with the fewest bins provide a histogram that gives a simple visualization of the data while still guaranteeing essentially optimal estimation of @xmath47 .    [ thma ]",
    "let @xmath41 and @xmath58 .",
    "then    * for @xmath59 @xmath60 * @xmath61 uniformly in @xmath4 .",
    "@xmath62 can also be interpreted as a distance between @xmath4 and @xmath14 that focuses on intervals having probability content @xmath63 .",
    "we show in section  [ optconfint ] that @xmath33 is an optimal confidence region for @xmath4 with respect to the distance @xmath64 for arbitrary @xmath65 .",
    "the optimality results for estimating @xmath35 provided by theorems  [ thma ] and [ thmb ] carry over to estimating the average density @xmath66 by simply dividing the inequalities by @xmath67 .",
    "we note that the construction of @xmath33 via @xmath68 rather than , say , the standardized binomial statistic @xmath69 is crucial for these optimality results , see the discussion in section  [ optconfint ] .",
    "arguably the most important features that one would like to detect with a histogram are the presence of increases / decreases in a density as well as ( anti)modes . since we are considering a general cdf @xmath4 and do not want to make any smoothness assumptions , we can formulate the problem as detecting increases / decreases in @xmath70 defined in  .",
    "we denote by @xmath71 the set of cdfs that have a detectable increase in @xmath70 : @xmath72 where @xmath41 .",
    "[ thmd ] @xmath73    similarly as in theorem  [ thmb ] it can be shown that in a large sample setting it is not possible to detect smaller values of @xmath74 as both @xmath75 and @xmath76 go to zero . by @xmath77",
    "we denote a slightly larger set than @xmath71 that contains some cdf having an undetectable increase : @xmath78 where @xmath79    [ thmlbmon ] let @xmath80 with density @xmath81 .",
    "let @xmath82 be any test with level @xmath83 under @xmath84 is non - increasing , in the sense that @xmath85 if @xmath79 , then @xmath86    this optimality result clearly also applies to the simultaneous detection of a finite number of increases / decreases and hence to the detection of ( anti)modes .",
    "moreover , it is possible to infer increases / decreases and modes / troughs of @xmath70 from @xmath87 .",
    "for instance , large increases ( or decreases ) of a histogram in @xmath87 imply increases ( or decreases ) of @xmath70 with confidence at least @xmath88 .",
    "[ thmfeatureinfer ] let @xmath89 , and @xmath90 .",
    "denote by @xmath10 the density of some @xmath91 .    *",
    "if there are intervals @xmath92 such that @xmath93 then it holds that @xmath94 this assertion also holds if we replace `` @xmath95 '' by `` @xmath96 '' .",
    "* if there are intervals @xmath97 , for some @xmath98 , such that @xmath99 then it holds with confidence at least @xmath100 that @xmath101",
    "we recall from section  [ confset ] that the essential histogram is defined as the histogram with the least number of bins within the confidence set @xmath87 .",
    "its computation requires the solution of a nonconvex combinatorial optimization problem .",
    "this makes it practically not feasible for most real world applications .",
    "however , it is possible to compute the exact solution of a slight relaxation ( still nonconvex ) of the original optimization problem in almost linear run time ( see section  [ subalg ] ) .",
    "this is given by @xmath102 where @xmath103 is a subset of distributions whose density is a histogram , given by @xmath104 and @xmath105 the number of bins of the density of @xmath14 .",
    "that is , in the original confidence set @xmath87 , we consider only the intervals on which a candidate is constant . in general , solutions to",
    "are not unique . in this case",
    ", we will pick @xmath106 with density @xmath107 , that maximizes the following entropy ( up to a factor of @xmath48 ) @xmath108 note that is the log - likelihood if we assume the data are distributed according to @xmath106 , since @xmath109 in other words , we select the one that explains data best in terms of likelihood among all solutions of  . from now on ,",
    "this particular solution will be referred to as the _ essential histogram _ for brevity .    due to this relaxation",
    "the solution will potentially contain less bins than the exact solution ( but never will have more ) and the significance level of the confidence set carries over to the features in the computed histogram .",
    "[ thmrelaxesshist ]    * the assertions in theorem  [ thma ] still hold if we replace @xmath33 by @xmath103 , and @xmath110 by @xmath111 , which is defined as @xmath112 * similar to theorem  [ thmd ] , it holds that @xmath113 * let @xmath89 , and @xmath114 .",
    "assume that @xmath115 , and that the density @xmath10 of @xmath14 is constant on some intervals @xmath92 .",
    "if further @xmath93 then it holds that @xmath94 this assertion also holds if we replace `` @xmath95 '' by `` @xmath96 '' .",
    "* under the same notation as in ( iii ) , assume that on intervals @xmath97 the density @xmath10 is constant , for some @xmath98 , and that @xmath116 then it holds with confidence at least @xmath100 that @xmath101    furthermore , in case that the true density @xmath81 itself is a histogram ( i.e.  piecewise constant ) , we have an explicit control on the number of modes .",
    "[ thmhistdensity ] let @xmath4 have a piecewise constant density @xmath117",
    ". denote @xmath118 , @xmath119 , and @xmath120 .",
    "then for the essential histogram @xmath10 ( with cdf @xmath14 ) in   it holds that    * it controls overestimating the number of bins uniformly over all @xmath4 s @xmath121 * it controls underestimating the number of bins , for @xmath122 , @xmath123 with @xmath124 * it controls the number of modes and troughs , for @xmath125 , @xmath126 with @xmath127 .",
    "* remarks : * note first that for a fixed density @xmath81 and a fixed significance level @xmath128 , @xmath129 ( or @xmath130 ) is of order @xmath131 as @xmath132 .",
    "thus , the terms within the exponents in theorem  [ thmhistdensity ] ( ii ) and ( iii ) are essentially @xmath133 and @xmath134 .",
    "moreover we point out that the assertions in theorem  [ thmhistdensity ] even hold for sequences of @xmath135 with @xmath136 , @xmath137 , @xmath138 , @xmath139 , and @xmath140 .",
    "it is known that @xmath141 for some constant @xmath142  ( see * ? ? ?",
    "* ) , so @xmath143 is of order @xmath54 if @xmath128 tends to zero at a polynomial rate .",
    "thus , a sufficient condition for the consistent estimation of number of modes and troughs , in case of @xmath135 itself being a histogram , is @xmath144 where @xmath145 , @xmath146 no faster than a polynomial rate , and @xmath142 is some proper constant .",
    "the condition   quantifies how the underlying difficulty in estimating the numbers of modes and troughs is determined by the minimal size of bins and the minimal difference between heights over neighboring bins of the unknown truth .      by @xmath147",
    "we denote the order statistics of observations @xmath148 .",
    "we treat each @xmath149 as a node in a graph , and set the edge length between nodes @xmath149 and @xmath150 as the minimal number of blocks of a step function on @xmath151 $ ] , which satisfies the multiscale constraint in  .",
    "then the computation of the essential histogram amounts to finding the shortest path between @xmath152 and @xmath153 , which can be exactly computed by dynamic programming algorithms , see e.g.  @xcite .",
    "similar to  @xcite , we exploit an accelerated dynamic program for the computation of the essential histogram , by incorporating pruning ideas , see also  @xcite",
    ". more precisely , our pruning operation exploits the structure of the problem : whenever there are no piecewise constant candidates with @xmath154 changes on @xmath155 $ ] , then there are no piecewise constant candidates with @xmath154 changes on @xmath156 $ ] for @xmath157 and @xmath158 .",
    "this accelerated dynamic program turns out to be significantly faster than standard dynamic programs , and most of the time has nearly linear computation complexity in terms of number of samples , with the worst case computation complexity being quadratic .",
    "moreover , the memory complexity is always linear , i.e.  @xmath159 . for brevity ,",
    "we omit the technical details and refer interested readers to e.g.  @xcite .",
    "an implementation of the essential histogram is provided in the r - package `` esshist '' , available from http://www.stochastik.math.uni-goettingen.de/esshist .",
    "now we examine finite sample performance of the essential histogram ( eh ) in   on some simulation examples , which are designed to reflect a range of difficulties in density estimation and data exploration . for comparison ,",
    "we include two classical histograms , one with equal widths of bins ( hw ) @xcite and the other one with equal areas of blocks ( ha ) @xcite , and also a multiscale density estimator ( dk ) by @xcite , which aims to control the numbers of modes and to produce candidate approximating densities .",
    "the number of bins for the classical histograms is determined according to the default rule in r ( i.e.  @xcite s rule , denoted by hw ( default ) in simulation ) , the asymptotically optimal rule with the number of bins @xmath160 , or the rule with relatively more bins @xmath161 .",
    "the dk has a similar flavor as eh , being defined as a solution to a variational problem under a certain multiscale constraint , but it computes only an approximate solution using taut strings together with some heuristically mentioned adjustments ( e.g.  local squeezing ) , and hence statistical error guarantees or confidence statements as for eh appear to be difficult .",
    "the dk is computed by function ` pmden ` with default parameters in the r - package `` ftnonpar '' , available from cran , ha and hw by r built - in function ` hist ` , and the proposed eh by function ` esshistogram ` in our r - package `` esshist '' , see section  [ computation ] . throughout all experiments ,",
    "the threshold @xmath143 in is estimated by 5000 monte - carlo simulations , which needs to be done only once for a fixed sample size @xmath48 . for a histogram density @xmath162 with @xmath163",
    ", we define the number of maxima ( i.e.  modes ) as @xmath164 and the number of minima ( i.e.  troughs ) as @xmath165 .",
    "the number of extrema is defined as the total number of maixma and minima .",
    "observations are sampled from the uniform distribution @xmath166 .",
    "the comparison between eh with various choices of significance levels , dk , and hw and ha with different selection rules for the number of bins is given in figure  [ fig : uniform ] and table  [ tab : monotone ] .",
    "the eh with small significance levels ( @xmath167 ) performs best as it recovers the true density almost perfectly , while eh with large @xmath168 ( e.g.  @xmath169 ) , as well as dk , tends to include false bins . by sharp contrast , the classical histograms hw / a overall perform worst , and report many false bins and also multiple false modes , which become even worse as the sample size increases .    ,",
    "scaledwidth=93.0% ]    lc|*7c & + & & @xmath170 & @xmath171 & @xmath31 & @xmath172 & @xmath173 + & @xmath3 & 0.00 / 0.00 & 0.00 / 0.00 & 0.00 / 0.00 & 0.00 / 0.00 & 0.00 / 0.00 + & @xmath174 & 0.00 / 0.00 & 0.00 / 0.00 & 0.00 / 0.00 & 0.01 / 0.00 & 0.00 / 0.00 + & @xmath175 & 0.00 / 0.00 & 0.01 / 0.00 & 0.00 / 0.00 & 0.01 / 0.00 & 0.01 / 0.00 + & @xmath176 & 0.01 / 0.01 & 0.02 / 0.01 & 0.03 / 0.01 & 0.05 / 0.01 & 0.03 / 0.01 + & @xmath177 & 0.06 / 0.03 & 0.06 / 0.02 & 0.09 / 0.03 & 0.13 / 0.02 & 0.11 / 0.03 + & @xmath178 & 0.18 / 0.08 & 0.21 / 0.08 & 0.29 / 0.10 & 0.31 / 0.06 & 0.33 / 0.07 + & 0.06 / 0.07 & 0.05 / 0.02 & 0.13 / 0.01 & 0.04 / 0.00 & 0.02 / 0.03 + & r - default & 2.56 / 0.59 & 2.69 / 0.70 & 2.66 / 0.56 & 2.61 / 0.70 & 2.65 / 0.61 + & @xmath179 & 0.89 / 0.07 & 1.58 / 0.16 & 1.90 / 0.18 & 2.28 / 0.24 & 2.59 / 0.27 + & @xmath180 & 0.00 / 0.00 & 1.28 / 0.06 & 2.61 / 0.42 & 3.86 / 0.99 & 5.20 / 1.63 + & @xmath179 & 1.00 / 0.43 & 1.67 / 0.55 & 1.94 / 0.63 & 2.29 / 0.73 & 2.64 / 0.84 + & @xmath180 & 0.00 / 0.00 & 1.35 / 0.32 & 2.70 / 1.23 & 3.92 / 2.37 & 5.28 / 3.54 +      , scaledwidth=93.0% ]    to further examine false positives in mode detection , we compare different methods on monotone densities .",
    "for example , figure  [ fig : exponential ] and table  [ tab : monotone ] give the comparison results on the exponential distribution @xmath181 ( i.e.  mean equals to 1 ) . the eh is better than all the other methods from both density estimation and feature detection perspectives , while requiring the least number of bins , which would ease the interpretation of the data .",
    "the dk performs comparably well , but sometimes distorts the shape of the true density , e.g.  the sharp spike reported by dk ( in the bottom left panel of figure  [ fig : exponential ] ) appears to be artificial .",
    "similar to the previous example , the classical histograms hw / a are less competitive , and tend to include more false modes as the sample size increases .",
    "in addition , we point out that the comparison results on other monotone densities ( not shown ) are similar to the example reported here .    , scaledwidth=95.0% ]      lc|*6c & + & & @xmath182 & @xmath172 & @xmath183 & @xmath173 & @xmath184 + & @xmath3 & 61% / 74% & 72.2% / 92.2% & 76.2% / 95.6% & 82.8% / 95.8% & 89.4% / 96.8% + & @xmath174 & 63.4% / 83.4% & 74.8% / 92.6% & 78% / 93.4% & 84.2% / 91.6% & 89.4% / 91.2% + & @xmath175 & 66% / 84.4% & 76.8% / 89.6% & 79% / 88.8% & 85% / 85.2% & 90% / 88% + & @xmath176 & 70.2% / 76% & 78.6% / 78.8% & 81% / 78.2% & 88.2% / 71% & 89.6% / 74.8% + & @xmath177 & 73.6% / 62.2% & 79.6% / 61.4% & 82% / 61.8% & 86.8% / 51.4% & 89% / 54.6% + & @xmath178 & 73.2% / 36.8% & 79.4% / 35% & 80.8% / 35.2% & 82.2% / 25.2% & 85% / 27.8% + & 64% / 0.6% & 68.6% / 0.4% & 71% / 0.2% & 74% / 0% & 72% / 0.2% +    & r - default & 49% / 0% & 49.8% / 0% & 50.8% / 0% & 54.4% / 0% & 53.2% / 0% + & @xmath179 & 32.4% / 0% & 83.2% / 0% & 84.8% / 0% & 71.2% / 0% & 63.6% / 0% + & @xmath180 & 57.2% / 0% & 37.6% / 0% & 21.4% / 0% & 8% / 0% & 2.2% / 0% + & @xmath179 & 25.4% / 0% & 37.8% / 0% & 30.4% / 0% & 57.2% / 0% & 63%",
    "/ 0% + & @xmath180 & 47.2% / 0% & 22.4% / 0% & 5.8% / 0% & 0.2% / 0% & 0% / 0% +    in the third example , we make the same comparison but with a underlying distribution given by @xmath185 it consists of three regions of different nature : an ordinary one mode region , a sharp spike region , and a flat region , cf .  figure  [ fig : mix_unif ] , to imitate various types of features in real data .",
    "the piecewise constancy of the density makes the comparison of different estimated histograms even clearer .",
    "the simulation results are summarized in figure  [ fig : mix_unif ] and table  [ tab : mix_unif ] .",
    "the eh method with a wide range of significance levels performs comparably well as dk when the sample size is small , and is substantially better than all the others when the sample size is large ( i.e. @xmath186 ) .",
    "it recovers all three regions of the true density fairly well , and greatly outperforms other methods with respect to the detection of correct number of bins , which reflects the theoretical finding in theorem  [ thmhistdensity ] . for a fixed sample size ,",
    "the eh tends to introduce more false bins for larger significance levels @xmath128 ( according to its theoretical performance , theorem  [ thmhistdensity ] ( i ) ) , but it hardly ever includes more false modes ( theorem  [ thmhistdensity ] ( iii ) ) .",
    "by contrast , the dk often noticeably over - estimates the height of the spike , and introduces many distinct modes in the one mode and the flat regions .",
    "its ability in identifying the true number of modes first improves , but later deteriorates as the sample size increases . among classical histograms",
    ", the hw with @xmath187 bins performs best in detecting the correct number of modes and troughs ( which , as we will see in the next example , cf .",
    "table  [ tab : claw ] , is not always the case ) , but seriously flattens the spike .      ,",
    "scaledwidth=95.0% ]    lc|*8c & + & & @xmath184 & @xmath188 & @xmath189 & @xmath190 & @xmath191 & @xmath192 + & @xmath3 & 1.6 / 0.6 & 2.0 / 1.0 & 2.5 / 1.5 & 3.3 / 2.3 & 4.3 / 3.3 & 4.8 / 3.8 + & @xmath174 & 1.9 / 0.9 & 2.4 / 1.4 & 3.1 / 2.1 & 3.8 / 2.8 & 4.6 / 3.6 & 5.0 / 4.0 + & @xmath175 & 2.2 / 1.2 & 2.7 / 1.7 & 3.4 / 2.4 & 4.2 / 3.2 & 4.8 / 3.8 & 5.0 / 4.0 + & @xmath176 & 2.7 / 1.7 & 3.3 / 2.3 & 4.0 / 3.0 & 4.5 / 3.5 & 4.9 / 3.9 & 5.0 / 4.0 + & @xmath177 & 3.2 / 2.2 & 3.8 / 2.8 & 4.4 / 3.4 & 4.8 / 3.8 & 5.0 / 4.0 & 5.0 / 4.0 + & @xmath178 & 3.9 / 2.9 & 4.4 / 3.4 & 4.8 / 3.8 & 5.0 / 4.0 & 5.0 / 4.0 & 5.0 / 4.0 + & 4.98 / 3.98 & 5.0 / 4.0 & 5.0 / 4.0 & 5.0 / 4.0 & 5.0 / 4.0 & 5.01 / 4.01 + & r - default & 1.3 / 0.4 & 1.2 / 0.3 & 1.2 / 0.2 & 1.1 / 0.2 & 1.1 / 0.1 & 1.1 / 0.1 + & @xmath179 & 1.1 / 0.2 & 1.1 / 0.2 & 1.1 / 0.1 & 1.1 / 0.3 & 1.2 / 0.2 & 1.1 / 0.2 + & @xmath180 & 4.2 / 3.8 & 5.7 / 5.3 & 7.1 / 6.8 & 8.8 / 8.6 & 10.6 / 10.4 & 12 .",
    "3 / 12.1 + & @xmath179 & 3.7 / 2.7 & 5.0 / 4.0 & 5.0 / 4.0 & 5.0 / 4.0 & 5.0 / 4.0 & 5.0 / 4.0 + & @xmath180 & 5.0 / 4.0 & 5.1 / 4.1 & 5.5 / 4.5 & 7.1 / 6.1 & 8.8 / 7.8 & 11.1 / 10.1 +    lc|*8c & + & & @xmath184 & @xmath188 & @xmath189 & @xmath190 & @xmath191 & @xmath192 + & @xmath3 & 7.2 ( 1.1 ) & 8.4 ( 1.2 ) & 9.8 ( 1.1 ) & 11.2 ( 0.9 ) & 12.5 ( 0.7 ) & 13.4 ( 0.7 ) + & @xmath174 & 8.1 ( 1.1 ) & 9.3 ( 1.1 ) & 10.7 ( 1.0 ) & 11.9 ( 0.8 ) & 13.1 ( 0.8 ) & 14.0 ( 0.7 ) + & @xmath175 & 8.8 ( 1.1 ) & 9.9 ( 1.1 ) & 11.3 ( 1.0 ) & 12.4 ( 0.8 ) & 13.5 ( 0.8 ) & 14.3 ( 0.7 ) + & @xmath176 & 9.8 ( 1.1 ) & 11.0 ( 1.0 ) & 12.1 ( 0.9 ) & 13.1 ( 0.8 ) & 14.1 ( 0.8 ) & 14.7 ( 0.6 ) + & @xmath177 & 10.8 ( 1.0 ) & 11.8 ( 1.0 ) & 12.8 ( 0.9 ) & 13.7 ( 0.9 ) & 14.6 ( 0.8 ) & 15.1 ( 0.6 ) + & @xmath178 & 12.0 ( 0.9 ) & 12.8 ( 1.0 ) & 13.7 ( 0.9 ) & 14.5 ( 0.9 ) & 15.3 ( 0.9 ) & 15.9 ( 0.9 ) + & 48.6 ( 5.1 ) & 52.8 ( 5.5 ) & 57.8 ( 5.3 ) & 67.0 ( 6.0 ) & 73.5 ( 6.0 ) & 79.4 ( 6.3 ) +     and r - default hw are shown as representatives .",
    "the computation time is recorded on a laptop with two 2.5 ghz processors and 4 gb memory .",
    "[ fig : claw_tm],scaledwidth=66.0% ]    the next example is the five - modal claw density from  @xcite .",
    "the comparison between different methods is given in figure  [ fig : claw ] and table  [ tab : claw ] .",
    "the eh performs well in both mode detection and density estimation for large sample sizes or high significance levels . for a fixed sample size",
    ", it recovers more details of the density from the data as the significance level increases , at the expense of statistical confidence ( again in accordance with theorem  [ thmhistdensity ] ) .",
    "this reveals the ability of eh as a potential exploratory tool for the analysis of data at hand , and we suggest to view the nominal level @xmath128 as a screening parameter .",
    "small @xmath128 provides reliable confidence statements as in theorem  [ thmhistdensity ] and corollary  [ thmrelaxesshist ] ; a large @xmath128 typically leads to a better recovery in terms of @xmath1 risk , @xmath193 . for a fixed significance level ,",
    "the performance of eh improves as the sample size increases , which supports the theoretical finding in corollary  [ thmrelaxesshist ] .",
    "note , additionally , that eh needs the least number of bins to detect the correct number of modes , see table  [ tab : claw_nbin ] .",
    "we found empirically that solutions in a range of @xmath128 between 0.5 and 0.9 always look very similar ( cf .",
    "figure  [ fig : claw ] ) revealing a certain stability if estimation is the primary goal .",
    "the dk is among the best with respect to mode detection , while it slowly starts to include false modes as sample size increases .",
    "however , it performs not so well in estimating the height of each mode , and the number of bins within each peak varies to a large extent , with the leftmost one containing around 7 bins , while the rightmost one nearly 15 bins , see the top - right panel in figure  [ fig : claw ] .",
    "the latter phenomenon complicates the interpretation of the data , and potentially leads to misunderstandings of the underlying truth ( e.g.  one might wrongly infer that the peaks are of completely different shape )",
    ".    for classical histograms , the hw and ha tend to report more ( both true and false ) modes as the number of bins gets larger for a given sample size . for this example , ha with @xmath187 bins performs better than other rules in terms of mode detection . together with the previous example",
    ", it can be seen that the relative performance of hw and ha with different rules is rather uncertain , which illustrates the difficulty one often faces when applying the classical histograms .",
    "moreover , hw gives better estimation at tail region ( low density ) , while ha is more preferable in the central region ( high density ) .",
    "regarding computation time , the eh is slower than dk , hw and ha , while being still affordable : e.g.  it just takes around 3 seconds for 3000 samples on a standard laptop , see figure  [ fig : claw_tm ] .",
    "seemingly , the computation time is of the same order for all three methods , and linearly increasing in @xmath48 .",
    "this example resembles the difficulty to have modes at several scales , increasingly more difficult to detect from left to right . the underlying distribution is chosen as the following mixture of normals @xmath194 whose density we refer to as the harp density due to the resemblance in shape ( see figure  [ fig : traceplot ] ) . instead of presenting a single realization , we plot estimated densities for 100 random draws .",
    "as shown in figure  [ fig : traceplot ] , the eh with different significance levels is overall the best in recovering the correct shape of the true density . the eh with a low significance level ( @xmath3 ) performs slightly worse in detecting the number of modes , but it never introduces any artificial modes or features .",
    "it presents similarly good performance in density estimation , and at the same time its detection power in terms of local extrema is largely improved , for a medium significance level ( @xmath195 ) . with a high significance level ( @xmath178 ) ,",
    "the eh successfully identifies the correct number of modes at the expense of rather low confidence about the inference , while being slightly more sensitive to data with large deviations , as noticed by sometimes overestimated heights of local maxima .",
    "albeit it also detects the correct number of modes , the dk has a tendency to bias the exact shapes and locations of modes , see for instance the two local minima on the left .",
    "the classical histograms hw / a fail to recover the main features of the true density , which indicates the difficulty of the problem .",
    "to save space , we only report the default histogram in r as an illustration .",
    "the performance of classical histograms improves for large sample sizes ( cf .",
    "figure  [ fig : eval_msconst ] ) .    ,",
    "@xmath196 and @xmath197 , dk , and hw from 800 observations for 100 repetitions .",
    "the corresponding numbers of maxima and minima of the estimated densities are reported on the right .",
    "[ fig : traceplot],scaledwidth=100.0% ]      in the last example , we compare the performance of different methods on the standard cauchy density @xmath198 , a typical density with heavy tails .",
    "the comparison results are summarized in figure  [ fig : cauchy ] and tables  [ tab : cauchy ] and  [ tab : cauchy_nbin ] .",
    "overall , the multiscale type methods , eh and dk , outperform the classical histograms . the eh and dk present a nearly perfect performance in mode detection , and only in rare cases include artificial modes . in the perspective of density estimation ,",
    "the eh recovers the underlying truth rather well with a very small number of bins , while dk tends to include many `` unnecessary '' slim bins , and sometimes largely overestimates the peak of the truth , see figure  [ fig : cauchy ] and table  [ tab : cauchy_nbin ] . among classical histograms , the ones with equal area of bins ( i.e.  ha ) are superior than those with equal width of bins ( i.e.  hw ) . the ha detects the major features , but flattens the close - to - tail part ( the region @xmath199 \\cup[3,9]$ ] in figure  [ fig : cauchy ] ) of the truth .",
    "by contrast , the hw completely distorts the shape of the truth , although still identifies the correct number of modes with moderate frequency .    , scaledwidth=100.0% ]    lc|*7c & + & & @xmath170 & @xmath200 & @xmath171 & @xmath201 & @xmath31 + & @xmath3 & 100% / 1.0 & 100% / 1.0 & 100% / 1.0 & 100% / 1.0 & 100% / 1.0 + & @xmath174 & 100% / 1.0 & 100% / 1.0 & 100% / 1.0 & 100% / 1.0 & 100% / 1.0 + & @xmath175 & 100% / 1.0 & 100% / 1.0 & 100% / 1.0 & 100% / 1.0 & 100% / 1.0 + & @xmath176 & 100% / 1.0 & 100% / 1.0 & 100% / 1.0 & 99.8% / 1.01 & 100% / 1.0 + & @xmath177 & 99.8% / 1.01 & 100% / 1.0 & 100% / 1.0 & 99.8% / 1.01 & 100% / 1.0 + & @xmath178 & 99.6% / 1.01 & 99.6% / 1.01 & 99.4% / 1.01 & 99.8% / 1.01 & 100% / 1.0 + & 99.8% / 1.04 & 100% / 1.0 & 100% / 1.0 & 100% / 1.0 & 99.8% / 1.01 + & r - default & 76.8% / 1.04 & 77.2% / 1.07 & 73.2% / 1.15 & 78.4% / 1.13 & 74.6% / 1.01 + & @xmath179 & 46% / 0.81 & 46.4% / 1.02 & 48.2% / 1.08 & 45% / 1.14 & 47.4% / 1.14 + & @xmath180 & 0% / 0 & 43.6% / 0.6 & 47.4% / 0.93 & 47.4% / 1.24 & 43.8% / 1.42 + & @xmath179 & 97.8% / 1.05 & 98.8% / 1.03 & 91.2% / 1.18 & 95.6% / 1.09 & 94.4% / 1.11 + & @xmath180 & 0% / 0 & 100% / 1.0 & 98.8% / 1.01 & 93% / 1.14 & 78.2% / 1.44 +    lc|*7c & + & & @xmath170 & @xmath200 & @xmath171 & @xmath201 & @xmath31 + & @xmath3 & 4.4 ( 0.6 ) & 5.4 ( 0.5 ) & 6.2 ( 0.6 ) & 6.9 ( 0.5 ) & 7.3 ( 0.5 ) + & @xmath174 & 4.6 ( 0.6 ) & 5.7 ( 0.6 ) & 6.5 ( 0.6 ) & 7.2 ( 0.5 ) & 7.7 ( 0.6 ) + & @xmath175 & 4.7 ( 0.5 ) & 5.9 ( 0.6 ) & 6.7 ( 0.6 ) & 7.4 ( 0.6 ) & 7.9 ( 0.6 ) + & @xmath176 & 4.9 ( 0.5 ) & 6.3 ( 0.7 ) & 7.0 ( 0.6 ) & 7.8 ( 0.7 ) & 8.4 ( 0.7 ) + & @xmath177 & 5.2 ( 0.6 ) & 6.6 ( 0.7 ) & 7.3 ( 0.6 ) & 8.3 ( 0.7 ) & 8.8 ( 0.7 ) + & @xmath178 & 5.6 ( 0.7 ) & 7.2 ( 0.7 ) & 7.8 ( 0.7 ) & 9.0 ( 0.8 ) & 9.6 ( 0.8 ) + & 14.6 ( 3.0 ) & 22.8 ( 3.4 ) & 28.3 ( 3.6 ) & 32.9 ( 3.8 ) & 38.0 ( 4.1 ) +      ) , ha ( @xmath52 ) and dk , respectively .",
    "each short vertical line on the horizontal line marks a removable change - point , with its intensity proportional to the number of merged segments whose interior contains this change - point .",
    "the sample size @xmath48 is @xmath202.[fig : eval_msconst],scaledwidth=90.0% ]    the multiscale constraint in  , an adjusted version of @xmath87 , can actually be beneficial to any histogram estimator . given a histogram estimator @xmath203 , we can always check , for every interval @xmath5 in @xmath204 , where @xmath203 is constant , whether the corresponding local constraint @xmath205 is fulfilled . the set of violation intervals defined here is indeed a subset of all violation intervals from @xmath87 , so the statistical justification of the original confidence set @xmath87 carries over to this subset as well , see corollary  [ thmrelaxesshist ] .",
    "for instance , the given estimator departs from the truth uniformly over every violation interval with probability at least @xmath88 .",
    "there are mainly two reasons to consider the multiscale constraint in   instead of @xmath87 for the violation intervals .",
    "one is to be compatible with eh , i.e. , the set of its violation intervals should be empty .",
    "the other is to improve the clarity in visualization and the interpretability of violation locations by avoiding long intervals across change - points .",
    "as argued above , the set of all intervals where the local constraints are violated provides crucial information for the performance of @xmath203 .",
    "we illustrate this by a numerical example in figure  [ fig : eval_msconst ] .",
    "the set of all violation intervals , plotted in the lower part of each panel , nicely depicts the deviation from the true density , which clearly shows where an estimator gives faithful estimation , and where it fails .",
    "furthermore , a horizontal gray scale bar is shown with the darkness proportional to the number of violation intervals covering a given location .",
    "this , to some extent , indicates how seriously an estimator deviates from the truth . in this example , the set of violation intervals for dk is empty , since it is defined under a similar multiscale constraint , and also has lots of changes , which greatly reduces the number of local constraints .",
    "the multiscale constraint , as we have seen , can be used as an evaluation tool to examine whether and where a given histogram estimator misses significant features ( i.e.  to detect false negatives ) . on the other hand",
    ", it can also be applied to find superfluous jumps of any histogram estimator ( i.e.  to detect false positives ) .",
    "to this end , we consider , for each change - point of a histogram estimator , whether merging its two nearby segments still satisfies the multiscale constraint in  .",
    "if it is the case , the change - point is said to be removable . in figure",
    "[ fig : eval_msconst ] , each vertical short line , plotted on the horizontal line , corresponds to a removable change - point .",
    "note that it by no means indicates that all the removable change - points are removable at the same time .",
    "one can , however , claim that any sub - collection of removable change - points , such that every two are not end points of a common segment , are simultaneously removable with probability at least @xmath88 . for instance",
    ", it suggests many jumps by dk are unnecessary . sometimes , for a removable change - point",
    ", it is possible to merge more than two nearby segments , which potentially strengthens the confidence on its removability .",
    "thus , we also encode this information as the intensity of vertical short lines , which scales with the number of possible ways of merging , see figure  [ fig : eval_msconst ] .",
    "the evaluation in terms of violation intervals and removable change - points is implemented via function  ` checkhistogram ` in our r - package , together with the visualization functionality ( as shown in figure  [ fig : eval_msconst ] ) .",
    "the eh shows a great potential in meeting the two main goals of the histogram , namely , probability estimation , and feature detection .",
    "for instance , it is as competitive as the state - of - art methods that are tailored to mode detection , such as dk , in terms of identifying the number of modes for large sample sizes .",
    "attractively , the eh gives a histogram as simple as possible , as it minimizes the number of bins , which greatly eases its interpretation .",
    "besides , the eh methods with various choice of significance levels serve as a useful data exploration tool , providing a cascade of finite sample inferences with user - specified confidence levels for a given dataset .",
    "we are not aware of any histogram which meets this goal , or can provide similar guarantee on modes , troughs or number of bins . based on extensive simulation study , we recommend @xmath176 as the _ default _ choice of significance level if estimation if of primary intent , and @xmath206 ( or even smaller ) if significant conclusions have to be made on modes and troughs .    *",
    "theorems  [ thma](i ) and [ thmb ] show that @xmath33 is an optimal confidence region for @xmath4 with respect to the distance @xmath64 for arbitrary @xmath65 : theorem  [ thma](i ) shows that with probability converging to one , @xmath33 will exclude @xmath14 with @xmath207 , where @xmath208 sufficiently slowly . in the case of small @xmath65 ,",
    "theorem  [ thmb ] shows that if @xmath209 is replaced by @xmath210 , then no test can distinguish @xmath4 and @xmath14 with nontrivial power . in the case of larger @xmath65 ,",
    "i.e. when @xmath65 stays bounded away from zero , the condition of theorem  [ thma](i ) becomes @xmath211 with @xmath41 . on the other hand ,",
    "a contiguity argument as in the proof of theorem  4.1(c ) in dmbgen and walther  ( 2008 ) shows that for any test to have asymptotic power 1 against a sequence @xmath212 requires @xmath213 with @xmath41 .",
    "[ thmb ] let @xmath82 be any test with level @xmath83 under @xmath214 i.i.d .",
    "@xmath4 . if @xmath44 and @xmath215 such that @xmath216 , then @xmath217    * remarks : * 1 .",
    "the price for simultaneously considering all @xmath34 in part ( ii ) of theorem  [ thma ] , as opposed to a fixed sequence @xmath218 in ( i ) , is a doubling of the distance @xmath219 : for a fixed sequence of intervals @xmath220 , the standardized distance between @xmath35 and @xmath221 becomes negligible compared to the radius @xmath222 of the confidence ball around @xmath221 .",
    "but if one needs to consider all intervals simultaneously , then for the worst - case interval @xmath5 the standardized distance between @xmath221 and @xmath35 is also about @xmath223",
    "the proof of theorem  [ thma ] shows that ( i ) holds even for smaller intervals , namely for @xmath224 , provided that also @xmath225 . if @xmath226 , then ( [ res ] ) requires a different bound .",
    "for example , if @xmath227 , @xmath228 , then ( [ res ] ) requires [ smalli ] d_p_n ( f , h )  >  ( 1+_n ) ( + ) and it is not clear whether this result can be improved .",
    "note that theorem  [ thmb ] does not provide a lower bound for scales of order @xmath229 .",
    "the construction of @xmath33 via @xmath68 rather than , say , the standardized binomial statistic @xmath69 is crucial for these optimality results : while the tail of @xmath230 is close to subgaussian , it does vary with @xmath35 and becomes increasingly heavy as @xmath35 decreases to 0 , see ch . 11.1 in @xcite .",
    "it is thus not clear how to construct a penalty that is effective in combining the evidence on the various scales @xmath231 .",
    "for example , if @xmath232 for some fixed @xmath233 , then the penalty @xmath234 in the definition of @xmath33 would not be sufficiently large for the standardized binomial statistic and therefore the optimality result ( [ resi ] ) would not hold , at least in the case @xmath235 .",
    "we will make use of the following              * proof of proposition  [ thmc1 ] * : recall that @xmath4 is continuous . since the law of @xmath248 does not depend on @xmath4",
    ", we may assume @xmath15 i.i.d .",
    "@xmath249 $ ] .",
    "the statement of the proposition is closely related to the modulus of continuity of the uniform empirical process , see ch .",
    "14.2 in @xcite .",
    "unfortunately , the available results appear not strong enough to cover the case where @xmath41 slowly . therefore we employ the hungarian construction together with elementary calculations and recent results about brownian motion .    by @xcite ch .",
    "12.3 , there exists a sequence @xmath250 of brownian motions on the same probability space such that @xmath251 } \\bl| \\sqrt{n } \\bl(f_n(i)-|i|\\br ) -b_n(i ) \\br| \\leq m < \\infty",
    "\\ a.s.,\\ ] ] where @xmath252 , a standard brownian bridge . writing    @xmath253:\\ |i| \\in [ \\frac{\\log^2n}{n},\\frac{1}{2 } ) }    \\bl| \\frac{\\sqrt{n } ( f_n(i)-|i|)- w_n(i)}{\\sqrt{|i|(1-|i| ) } } \\br| \\\\ & \\leq   \\sup_{i \\subset [ 0,1 ] } \\sqrt{\\frac{2n}{\\log^2 n } }    \\ \\bl| \\sqrt{n } ( f_n(i)-|i| ) -b_n(i ) \\br| + |w_n(1)|\\\\ \\end{split}\\ ] ]        for the second claim we note that @xmath258 where the @xmath259 are i.i.d .",
    "n(0,1 ) and @xmath260 .",
    "wlog we may assume that @xmath261 is such that @xmath262 .",
    "mill s ratio gives @xmath263}{4 \\lam_n } \\br\\ } \\\\ & \\leq \\exp \\bl\\{- \\frac{\\exp \\bl ( b_n \\sqrt{2 \\log e / p_n } /4\\br)}{5e",
    "\\sqrt{2 \\log e / p_n } } \\br\\ } = o(1)\\\\ \\end{split}\\ ] ]    theorem  7.1(b ) in @xcite suggests that the second statement of the proposition holds for any collection of estimators @xmath264 , so it is not possible to improve on the performance of @xmath17 .",
    "we will not engage in the lengthy technical work required to establish that claim .",
    "@xmath247    * proof of proposition  [ thmc2 ] : * let @xmath4 have density @xmath265\\br)$ ] .",
    "if @xmath49 is odd , then the @xmath266th bin is @xmath267 .",
    "denote the height of the histogram ( i.e. the slope of @xmath55 ) on that bin by @xmath10 .",
    "set @xmath268 then @xmath269 and @xmath270 , hence @xmath271    * proof of theorem  [ thma ] : * to avoid lengthy technical work we will prove the theorem using @xmath272 all intervals in @xmath273 in the definition of @xmath33 .",
    "the technical work in @xcite shows that the approximating set of intervals used in section  [ confset ] is fine enough so that the optimality results continue to hold with that approximating set .          to prove ( [ resi ] ) set @xmath281",
    ". then the inequality ( [ res ] ) reads [ a0 ] >",
    "c_n +  1(f(i)<h(i ) ) we have @xmath282 by the assumption of the theorem , and we define the event @xmath283 .",
    "we will show that on the event @xmath284 , ( [ a0 ] ) implies [ a1 ] _",
    "n +  1 ( f_n(i)<h(i ) )  ev .",
    ", uniformly in @xmath14 and @xmath5 , where @xmath285 . hence lemma  [ quadapprox ] ( b , c ) gives @xmath286 by chebychev s inequality , and the above conclusions are uniform in @xmath287 and @xmath5 .",
    "( [ resi ] ) follows since @xmath288 by proposition  1 in @xcite .      since @xmath294 .",
    "as in the proof of ( i ) one finds @xmath295 eventually .",
    "moreover , on @xmath296 we have @xmath297 , hence @xmath298 and so ( [ astar ] ) is not smaller than @xmath299 completing the proof of ( ii ) .",
    "@xmath300      it is enough to prove that the second probability goes to 0 uniformly in @xmath4 ; the third one is analogous .",
    "the proof follows essentially the argument of theorem  [ thma](i ) .",
    "the reason why we are not in the case ( ii ) , which would require doubling @xmath234 , is that we do not need to control the probability simultaneously for all intervals @xmath306 and hence we can guarantee that @xmath307 is sufficiently close to @xmath75 .",
    "hence the inequality in the second probability in ( [ 2mode ] ) implies ( [ a0 ] ) ( setting @xmath312 ) , and it was shown in the proof of theorem  [ thma](i ) that on the event @xmath313 , ( [ a0 ] ) implies [ 1new ] _",
    "n +  1 ( f_n(i_1)<h(i_1 ) )  ev .",
    "where @xmath314 .",
    "but if @xmath48 is large enough so that @xmath315 , then ( [ 1new ] ) implies @xmath316 by lemma  [ quadapprox](b , c ) .",
    "( as in the proof of theorem  [ thma ] we assumed @xmath317 all real intervals @xmath318 and refer to @xcite for the technical work showing that the conclusion also obtains with the approximating set @xmath319 used in section  2 . )",
    "hence the second probability in ( [ 2mode ] ) is not larger than @xmath320 by chebychev s inequality . @xmath247",
    "* proof of theorem  [ thmlbmon ] : * w.l.o.g .",
    "we assume @xmath321 $ ] .",
    "let @xmath322 , and @xmath323 , for @xmath324 .",
    "define the densities @xmath325 } ( x ) - c_{1n } 1_{i^1_{nj } } ( x ) + c_{2n } 1_{i^2_{nj } } ( x)\\qquad\\text { for } j = 1 , \\ldots , m_n : = \\lfloor 1/(p_{1n } + p_{2n } ) \\rfloor,\\ ] ] with @xmath326 and @xmath327 .",
    "the rest of the proof is similar to that of theorem  [ thmb ] . @xmath328    * proof of theorem  [ thmfeatureinfer ] : * for ( i ) , it suffices to prove the `` @xmath95 '' case due to symmetry . by lemma  [ quadapprox ] ( c )",
    ", we have @xmath329 for @xmath330 or @xmath331 .",
    "then , it holds with probability at least @xmath88 that @xmath332 part ( ii ) follows in the same way by noting that   holds simultaneously over @xmath333 with probability no less than @xmath88 .",
    "@xmath328    * proof of corollary  [ thmrelaxesshist ] : * parts ( i ) and ( ii ) follow readily from the proofs of theorems  [ thma ] and  [ thmd ] , and parts ( iii ) and ( iv ) can be proven in exactly the same way as theorem  [ thmfeatureinfer ] .",
    "@xmath328    * proof of theorem  [ thmhistdensity ] : * for part ( i ) , by the definition of the essential histogram in  , we have @xmath334 for parts",
    "( ii ) and ( iii ) , we use arguments similar to  ( * ? ?",
    "* theorem 7.10 ) , but with notable differences due to the use of the reduced system @xmath23 .",
    "we will frequently use the following inequality , which comes as an application of the hoeffding s inequality , @xmath335 the detail is as follows : for part ( ii ) , let @xmath336 be the mid - point of @xmath337 , @xmath338 $ ] , and @xmath339 . for a fixed @xmath154 , we have @xmath340\\br ) & = \\pr_f\\bl(h \\equiv c \\ge \\frac{c_{k-1}+c_k}{2 } \\text { on } ( m_{k-1 } , m_k]\\text { for some constant } c \\br ) \\\\ & { } \\qquad + \\pr_f\\bl(h \\equiv c < \\frac{c_{k-1}+c_k}{2 } \\text { on } ( m_{k-1 } , m_k]\\text { for some constant } c \\br )   \\\\ & \\le \\pr_f\\bl({\\left| \\bar{h}_i - \\bar{f}_i \\right| } \\ge \\frac{\\delta}{2},\\ , h \\text { is constant on } i \\equiv i_{k-1}^+\\br )   \\\\ & { } \\qquad + \\pr_f\\bl({\\left| \\bar{h}_i - \\bar{f}_i \\right| } \\ge \\frac{\\delta}{2},\\ , h \\text { is constant on } i \\equiv i_{k}^-\\br ) .",
    "\\end{aligned}\\ ] ] by symmetry we only need to consider the first term in the r.h.s of the above equation , where @xmath341 . by the construction of @xmath23 in",
    ", it holds that for any @xmath5 with @xmath342 there is an interval @xmath343 and @xmath344 such that @xmath345 .",
    "conditioned on @xmath346 and @xmath347 , we have for @xmath348 @xmath349 which implies @xmath350 .",
    "thus , for @xmath351 @xmath352 \\\\ \\le & \\ , 4\\exp\\left(-\\frac{1}{128}n\\lambda^2\\underline\\theta^2\\right )   + \\pr_f\\bl({\\left| \\bar{h}_j - \\bar{f}_j \\right| } \\ge \\frac{\\delta}{2 } - \\frac{12\\delta_n}{\\lambda\\sqrt{n}}\\br)\\qquad\\text{[by lemma~\\ref{quadapprox } ( i ) ] } \\\\ \\le & \\ , 4\\exp\\left(-\\frac{1}{128}n\\lambda^2\\underline\\theta^2\\right ) + 2 \\exp\\left(-\\frac{1}{72}n\\lambda^2\\left(\\frac{\\delta}{2}-\\frac{12\\delta_n}{\\lambda\\sqrt{n}}\\right)_+^2\\right ) \\qquad\\text{[by~\\eqref{eqempf}]}.\\end{aligned}\\ ] ] the same bound holds for @xmath353 due to symmetry .",
    "therefore , we have for @xmath351 @xmath354 \\text { for some } k\\br ) \\\\ & \\le 4k\\left(2\\exp\\left(-\\frac{1}{128}n\\lambda^2\\underline\\theta^2\\right ) + \\exp\\left(-\\frac{1}{72}n\\lambda^2\\left(\\frac{\\delta}{2}-\\frac{12\\delta_n}{\\lambda\\sqrt{n}}\\right)_+^2\\right)\\right).\\end{aligned}\\ ] ] for ( iii ) , we further divide @xmath355 ( or @xmath356 ) into two subintervals @xmath357 , @xmath358 ( or @xmath359 , @xmath360 ) of equal lengths . for any fixed @xmath154",
    ", it holds that @xmath361\\br)\\\\   \\le \\pr_f\\bl({\\left| \\bar{h}_i - \\bar{f}_i \\right| } \\ge \\frac{\\delta}{2 } , \\text { and } h \\text { is constant on } i \\equiv i^+_{k,1 }   \\br )   + \\pr_f\\bl({\\left| \\bar{h}_i - \\bar{f}_i \\right| } \\ge \\frac{\\delta}{2 } , \\text { and } h \\text { is constant on } i \\equiv i^+_{k,2 }   \\br)\\\\   + \\pr_f\\bl({\\left| \\bar{h}_i - \\bar{f}_i \\right| } \\ge \\frac{\\delta}{2 } , \\text { and } h \\text { is constant on } i \\equiv i^-_{k,1 }   \\br )   + \\pr_f\\bl({\\left| \\bar{h}_i - \\bar{f}_i \\right| } \\ge \\frac{\\delta}{2 } , \\text { and } h \\text { is constant on } i \\equiv i^-_{k,2 }   \\br ) .",
    "\\end{gathered}\\ ] ] each term above can be bounded in a similar way as in ( ii ) , which leads to @xmath361\\br)\\\\ \\le 16\\exp\\left(-\\frac{1}{512}n\\lambda^2\\underline\\theta^2\\right ) + 8 \\exp\\left(-\\frac{1}{288}n\\lambda^2\\left(\\frac{\\delta}{2}-\\frac{24\\tilde{\\delta}_n}{\\lambda\\sqrt{n}}\\right)_+^2\\right)\\qquad\\text { for } n \\ge \\frac{32\\log n}{\\lambda\\underline\\theta}.\\end{gathered}\\ ] ] it follows from ( i ) and ( ii ) that for for @xmath348 @xmath362\\br ) \\ge 1-\\alpha \\\\ - 4k\\left(2\\exp\\left(-\\frac{1}{128}n\\lambda^2\\underline\\theta^2\\right ) + \\exp\\left(-\\frac{1}{72}n\\lambda^2\\left(\\frac{\\delta}{2}-\\frac{12\\delta_n}{\\lambda\\sqrt{n}}\\right)_+^2\\right)\\right).\\end{gathered}\\ ] ] thus , for @xmath363 @xmath364\\br ) \\\\",
    "\\ge & \\ ,   1 - \\alpha -   12k \\left(2\\exp\\left(-\\frac{1}{512}n\\lambda^2\\underline\\theta^2\\right ) + \\exp\\left(-\\frac{1}{288}n\\lambda^2\\left(\\frac{\\delta}{2}-\\frac{24\\tilde{\\delta}_n}{\\lambda\\sqrt{n}}\\right)_+^2\\right)\\right).\\ \\ \\ \\box\\end{aligned}\\ ] ]    * proof of theorem  [ thmb ] : * using the probability integral transformation we may assume @xmath249 $ ] .",
    "define the densities @xmath365\\setminus i_{nj}}(x)$ ] for @xmath366 and @xmath367 , where @xmath368 and @xmath369 . then @xmath370 .",
    "further , @xmath371 implies @xmath372 , hence @xmath373 } |f_{nj}(x)-1| = c_n \\leq \\sqrt{2 \\log ( e / p_n)/(np_n ) } \\leq \\sqrt{2 } ( \\log e / p_n)^{-1/2 } \\leq \\sqrt{2 } ( \\log m_n)^{-1/2}$ ] .",
    "further @xmath374 , hence @xmath375 by the assumption on @xmath376 .",
    "the claim now follows as in the proof of theorem  4.1(b ) in @xcite using their lemma  7.4 . that lemma uses the assumption that the sets @xmath377 are pairwise disjoint to establish that the likelihood ratio statistics @xmath378 are conditionally independent given the @xmath379 and to establish @xmath380 .",
    "this assumption is not met here , but the proof goes through by defining @xmath381 . since these sets are pairwise disjoint , conditional independence follows for the corresponding @xmath382 . finally , @xmath380 is not needed to establish @xmath383 , since this follows from jensen s inequality and @xmath384 .",
    "@xmath328                                        scott , d.  w. ( 1992 ) . .",
    "wiley series in probability and mathematical statistics : applied probability and statistics .",
    "john wiley & sons , inc .",
    ", new york . theory , practice , and visualization , a wiley - interscience publication ."
  ],
  "abstract_text": [
    "<S> the histogram is widely used as a simple , exploratory display of data , but it is usually not clear how to choose the number and size of bins for this purpose . </S>",
    "<S> we construct a confidence set of distribution functions that optimally address the two main tasks of the histogram : estimating probabilities and detecting features such as increases and ( anti)modes in the distribution . </S>",
    "<S> we define the _ essential histogram _ as the histogram in the confidence set with the fewest bins . </S>",
    "<S> thus the essential histogram is the simplest visualization of the data that optimally achieves the main tasks of the histogram . </S>",
    "<S> we provide a fast algorithm for computing a slightly relaxed version of the essential histogram , which still possesses most of its beneficial theoretical properties , and we illustrate our methodology with examples . an r - package is available online .    * keywords and phrases . </S>",
    "<S> * histogram , significant features , optimal estimation , multiscale testing , mode detection .    </S>",
    "<S> * ams 2000 subject classification . </S>",
    "<S> * 62g10 , 62h30    @xmath0 am and hs acknowledge support of dfg for 916 , hl support of dfg rtg 2088 subproject b2 , gw acknowledges support of nsf grants dms-1220311 and dms-1501767 . </S>"
  ]
}