{
  "article_text": [
    "models of learning in games fall roughly into two categories . in the first , the learning player forms beliefs about the future behavior of other players and nature , and",
    "directs her behavior according to these beliefs .",
    "we refer to these as fictitious - player - like models . in the second ,",
    "the player is attuned only to her own performance in the game , and uses it to improve future performance .",
    "these are called models of reinforcement learning .",
    "reinforcement learning has been used extensively in artificial intelligence ( ai ) .",
    "samuel wrote a checkers - playing learning program as far back as 1955 , which marks the beginning of reinforcement learning ( see @xcite ) . since then many other sophisticated algorithms , heuristics , and computer programs , have been developed , which are based on reinforcement learning .",
    "( @xcite ) .",
    "such programs try neither to learn the behavior of a specific opponent , nor to find the distribution of opponents behavior in the population . instead , they learn how to improve their play from the achievements of past behavior .",
    "until recently , game theorists studied mostly fictitious - player - like models .",
    "reinforcement learning has only attracted the attention of game theorists in the last decade in theoretical works like @xcite , @xcite , @xcite , and in experimental works like @xcite .",
    "in all these studies the basic model is given in a strategic form , and the learning player identifies those of her strategies that perform better .",
    "this approach seems inadequate where learning of games in extensive form is concerned .",
    "except for the simplest games in extensive form , the size of the strategy space is so large that learning , by human beings or even machines , can not involve the set of all strategies .",
    "this is certainly true for the game of chess , where the number of strategies exceeds the number of particles in the universe .",
    "but even a simple game like tic - tac - toe is not perceived by human players in the full extent of its strategic form .",
    "the process of learning games in extensive form can involve only a relatively small number of simple strategies .",
    "but when the strategic form is the basic model , no subset of strategies can be singled out . thus , for games in extensive form the structure of the game tree should be taken into consideration . instead of _ strategies",
    "_ being reinforced , as for games in strategic form , it is the _ moves _ of the game that should be reinforced for games in extensive form .",
    "this , indeed , is the approach of heuristics for playing games which were developed by ai theorists .",
    "one of the most common building block of such heuristics is the _ valuation _ , which is a real valued function on the possible moves of the learning player .",
    "the valuation of a move reflects , very roughly , the desirability of the move . given a valuation , a learning process can be defined by specifying two rules :    * a _ strategy rule _ , which specifies how the game is played for any given valuation of the player ; * a _ revision rule _ , which specifies how the valuation is revised after playing the game .",
    "our purpose here is to study learning - by - valuation processes , based on simple strategy and revision rules . in particular , we want to demonstrate the convergence properties of these processes in repeated games , where the stage game is given in an extensive form with perfect information and any number of players . converging results of the type we prove here are very common in the literature of game theory . but as noted before , convergence of reinforcement is limited in this literature to strategies rather than moves . to the best of our knowledge ,",
    "the ai literature while describing dynamic processes closely related to the ones we study here do not prove convergence results of this type .",
    "first , we study stage games in which the learning player has only two payoffs , 1 ( win ) and 0 ( lose ) .",
    "two - person win - lose games are a special case . but here , there is no restriction on the number of the other players or their payoffs .    for these games",
    "we adopt the simple _ myopic strategy rule_. by this rule , the player chooses in each of her decision node a move which has the highest valuation among the moves available to her at this node . in case",
    "there are several moves with the highest valuation , she chooses one of them at random .    as a revision rule",
    "we adopt the simple _ memoryless revision _ : after each round the player revises only the valuation of the moves made in the round .",
    "the valuation of such a move becomes the payoff ( 0 or 1 ) in that round .",
    "equipped with these rules , and an initial valuation , the player can play a repeated game . in each",
    "round she plays according to the myopic strategy , using the current valuation , and at the end of the round she revises her valuation according to the memoryless revision .",
    "this learning process , together with the strategies of the other players in the repeated game , induce a probability distribution over the infinite histories of the repeated game .",
    "we show the following , with respect to this probability .    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ suppose that the learning player can guarantee a win in the stage game .",
    "if she plays according to the myopic strategy and the memoryless revision rules , then starting with any nonnegative valuation , there exists , with probability 1 , a time after which the player always wins .",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    when the learning player has more than two payoffs , the previous learning process is of no help . in this case",
    "we study the _ exploratory myopic strategy rule _",
    ", by which the player opts for the maximally valued move , but chooses also , with small probability , moves that do not maximize the valuation .",
    "the introduction of such perturbations makes it necessary to strengthen the revision rule .",
    "we consider the _ averaging revision_. like the memoryless revision , the player revises only the valuation of moves made in the last round .",
    "the valuation of such a move is the average of the payoffs in all previous rounds in which this move was made .    _",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ if the learning player obeys the exploratory myopic strategy and the averaging revision rules , then starting with any valuation , there exists , with probability 1 , a time after which the player s payoff is close to her individually rational payoff ( the maxmin payoff ) in the stage game . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    the two previous results indicate that reinforcement learning achieves learning of playing the stage game itself , rather than playing against certain opponents .",
    "the learning processes described guarantee the player her individually rational payoff ( which is the win in the first result ) .",
    "this is exactly the payoff that she can guarantee even when the other players are disregarded .",
    "our next result concerns the case where all the players learn the stage game . by the previous result",
    "we know that each can guarantee his individually rational payoff .",
    "but , it turns out that the synergy of the learning processes yields the players more than just learning the stage game . indeed",
    ", they learn in this case each other s behavior and act rationally on this information .    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ suppose the stage game has a unique perfect equilibrium .",
    "if all the players employ the exploratory myopic strategy and the averaging revision rules , then starting with any valuation , with probability 1 , there is a time after which their strategy in the stage game is close to the perfect equilibrium . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    although valuation is defined for all moves , the learning player needs no information concerning the game when she start playing it .",
    "indeed , the initial valuation can be constant . to play the stage game with this valuation ,",
    "the player needs to know which moves are possible to her , only when it is her turn to play , and then choose one of them at random . during the repeated game , the player should be able to record the moves she made and their valuations .",
    "still , the learning procedure does not require that the player knows how many players there are , let alone the moves they can make and their payoffs .",
    "the learning processes discussed here treat separately the valuation for every node . for games with large number of nodes ( or states of the board ) , that may be unrealistic because the chance of meeting a given node several times is too small . in chess , for example , almost any state of the board , except for the few first ones , has been seen in recorded history only once . in order to make these processes more practical , similar moves ( or states of the board )",
    "should be grouped together , such that the number of similarity classes is manageable .",
    "when the valuation of a move is revised , so are all the moves similar to it .",
    "we will deal with such learning processes , as well as with games with incomplete information , in a later paper .",
    "consider a finite game @xmath0 with complete information and a finite set of players @xmath1 .",
    "the game is described by a tree @xmath2 , where @xmath3 and @xmath4 are the sets of terminal and non - terminal nodes , correspondingly , the root of the tree is @xmath5 , and the set of arcs is @xmath6 .",
    "elements of @xmath6 are ordered pairs @xmath7 , where @xmath8 is the immediate successor of @xmath9 .",
    "the set @xmath10 , for @xmath11 , is the set of nodes in which it is @xmath12 s turn to play .",
    "the sets @xmath10 form a partition of @xmath4 .",
    "the _ moves _ of player @xmath12 at node @xmath13 are the nodes in @xmath14 .",
    "denote @xmath15 .",
    "for each @xmath12 the function @xmath16 is @xmath12 s payoff function .",
    "the depth of the game is the length of the longest path in the tree .",
    "a game with depth 0 is one in which @xmath17 and @xmath18 .    a behavioral strategy , ( strategy for short ) for player @xmath12 is a function @xmath19 defined on @xmath10 , such that for each @xmath13 , @xmath20 is a probability distribution on @xmath21 .    the super game @xmath22 is the infinitely repeated game , with stage game @xmath0 .",
    "an infinite history in @xmath22 is an element of @xmath23 .",
    "a finite history of @xmath24 rounds , for @xmath25 , is an element of @xmath26 .",
    "a _ super strategy _ for player @xmath12 in @xmath22 is a function @xmath27 on finite histories , such that for @xmath28 , @xmath29 is a strategy of @xmath12 in @xmath0 , played in round @xmath30 .",
    "the super strategy @xmath31 induces a probability distribution on histories in the usual way .",
    "we fix one player @xmath12 ( the learning player ) and omit subscripts of this player when the context allows it .",
    "we first introduce the basic notions of playing by valuation .",
    "valuation _ for player @xmath12 is a function @xmath32 .",
    "playing the repeated game @xmath22 by valuation requires two rules that describe how the stage game @xmath0 is played for a given valuation , and how a valuation is revised after playing @xmath0 .    * a _ strategy rule _ is a function @xmath33 .",
    "when player @xmath12 s valuation is @xmath34 , @xmath12 s strategy in @xmath0 is @xmath35 . * a _ revision rule _ is a function @xmath36 , such that for the empty history @xmath37 , @xmath38 . when player @xmath12 s initial valuation is @xmath34 , then after a history of plays @xmath39 , @xmath12 s valuation is @xmath40 .",
    "the _ valuation super strategy _ for player @xmath12 , induced by a strategy rule @xmath33 , a revision rule @xmath41 , and an initial valuation @xmath34 , is the super strategy @xmath42 , which is defined by @xmath43 for each finite history @xmath39 .",
    "we consider first the case where player @xmath12 has two possible payoffs in @xmath0 , which are , without loss of generality , 1 ( win ) and 0 ( lose ) .",
    "a two - person win - lose game is a special case , but here we place no restrictions on the number of players or their payoffs .",
    "we assume that learning by valuation is induced by a strategy rule and a revision rule of a simple form .",
    "this rule associates with each valuation @xmath34 the strategy @xmath44 , where for each node @xmath13 , @xmath45 is the the uniform distribution over the maximizers of @xmath34 on @xmath21 . that is , in each node of player @xmath12 , the player selects at random one of the moves with the highest valuation .    for a history @xmath46 of length 1 , the valuation @xmath34 is revised to @xmath47 which is defined for each node @xmath48 by @xmath49 for a history @xmath50 , the current valuation is revised in each round according to the terminal node observed in this round .",
    "thus , @xmath51 .    the temporal horizons , future and past , required for these two rules are very narrow .",
    "playing the game @xmath0 , the player takes into consideration just her next move .",
    "the revision of the valuation after playing @xmath0 depends only on the current valuation , and the result of this play , and not on the history of past valuations and plays .",
    "in addition , the revision is confined only to those moves that were made in the last round .    [",
    "main ] let @xmath0 be a game in which player @xmath12 either wins or loses .",
    "assume that player @xmath12 has a strategy in @xmath0 that guarantees him a win .",
    "then for any initial nonnegative valuation @xmath34 of @xmath12 , and super strategies @xmath52 in @xmath22 , if @xmath27 is the valuation super strategy induced by the myopic strategy and the memoryless revision rules , then with probability 1 , there is a time after which @xmath12 is winning forever .",
    "the following example demonstrates learning by valuation .",
    "consider the game in figure [ f : goodcase ] , where the payoffs are player 1 s .",
    "( 600,380 ) ( 300,340)(1,1)200 [ ] [ @xmath53 ( 100,140)(1,1)100 [ l]@xmath54@xmath55[@xmath53[@xmath56",
    "suppose that 1 s initial valuation of each of the moves @xmath57 and @xmath58 is 0 .",
    "the valuations that will follow can be one of @xmath59 , @xmath60 , and @xmath61 , where the first number in each pair is the valuation of @xmath57 and the second of @xmath58 .",
    "( the valuation @xmath62 can not be reached from any of these valuations ) .",
    "we can think of these possible valuations as states in a stochastic process .",
    "the state @xmath61 is absorbing .",
    "once it is reached , player 1 is choosing @xmath58 and being paid 1 forever .",
    "when the valuation is @xmath60 , player 1 goes @xmath57 . she will keep going @xmath57 , and winning 1 , as long as player 2 is choosing @xmath54 .",
    "once player 2 chooses @xmath55 , the valuation goes back to @xmath59 .",
    "thus , the only way player 1 can fail to be paid 1 from a certain time on is when @xmath59 recurs infinitely many times .",
    "but the probability of this is 0 , as the probability of reaching the absorbing state @xmath61 from state @xmath59 is 1/2 .",
    "note that the theorem does not state that with probability 1 there is a time after which player 1 s strategy is the one that guarantees him payoff 1 . indeed , in this example ,",
    "if player 2 s strategy is always @xmath54 , then there is a probability 1/2 that player 1 will play @xmath57 for ever , which is not the strategy that guarantees player 1 the payoff 1 .",
    "we now turn to the case in which payoff functions take more than two values .",
    "the next example shows that in this case the myopic strategy and the memoryless revision rules may lead the player astray .",
    "[ badcase ] player 1 is the only player in the game in figure [ f : badcase ] .",
    "( 600,380 ) ( 300,340)(1,1)200 [ ] [ @xmath63 ( 100,140)(1,1)100 [ l]@xmath54@xmath55[@xmath64[@xmath65    in this game player 1 can guarantee a payoff of 10 , and therefore we expect a learning process to lead player 1 to this payoff .",
    "but , no reasonable restriction on the initial valuation can guarantee that the learning process induced by the myopic strategy and the memoryless revision results in the payoff 10 in the long run . for example , for any constant initial valuation",
    ", there is a positive probability that the valuation @xmath66 for @xmath67 is obtained , which is absorbing .",
    "we can not state for general payoff functions any theorem analogous to theorem [ main ] or even a weaker version of this theorem . but something meaningful can be stated when _ all _ players play the repeated game according to the myopic strategy and the memoryless revision rules .",
    "we say that game @xmath0 is _ generic _ if for every player @xmath12 and for every pair of distinct terminal nodes @xmath68 and @xmath69 , we have @xmath70 .    [ generic ]",
    "let @xmath0 be a generic game .",
    "assume that each player @xmath12 plays @xmath71 according to the myopic strategy rule and uses the memoryless revision rule .",
    "then for any initial valuation profile , with probability 1 , there is a time after which the same terminal node is reached in each round .",
    "the limit plays guaranteed by this theorem depend on the initial valuations and have no special structure in general .",
    "moreover , it is obvious that for any terminal node there are initial valuations that guarantee that this terminal node is reached in all rounds .",
    "we return , now , to the case where only one player learns by reinforcement . in order to prevent a player from being paid an inferior payoff forever , like in example [ badcase ] , we change the strategy rule .",
    "we allow for exploratory moves that remind her of all possible payoffs in the game , so that she is not stuck in a bad valuation .",
    "assume , then , that having a certain valuation , the player opts for the highest valued nodes , but still allows for other nodes with a small probability @xmath72 .",
    "such a rule guarantees that player in example [ badcase ] will never be stuck in the valuation @xmath66 .",
    "we introduce formally this new rule .",
    "this rule associates with each valuation @xmath34 the strategy @xmath73 , where for each node @xmath13 , @xmath74 . here , @xmath75 is the strategy associated with @xmath34 by the myopic strategy rule , and @xmath76 is the strategy that uniformly selects one of the moves at @xmath9 .",
    "unfortunately , adding exploratory moves does not help the player to achieve 10 in the long run , as we show now .",
    "assume that the initial valuation of @xmath54 and @xmath55 is 10 and @xmath77 correspondingly , and the valuation of the fist two moves is also favorable : @xmath78 .",
    "we assume now that in each of the two nodes player 1 chooses the higher valued node with probability @xmath79 and the other with probability @xmath72 . the valuation of @xmath54 and @xmath55 can not change over time . the valuation of @xmath67 form an ergodic markov chain with the two states @xmath80 .",
    "thus , for example , the probability of transition from @xmath78 to itself occurs when the player chooses either @xmath57 and @xmath54 , with probability @xmath81 , or @xmath58 with probability @xmath72 , which sum to @xmath82 .",
    "the following is the transition matrix of this markov chain .",
    "@xmath83 the two states @xmath78 and @xmath66 are symmetric and therefore the stationary probability of each is 1/2 .",
    "thus , the player is paid 10 and 2 , half of the time each .",
    "note that the exploratory moves are required because the payoff function has more than two values .",
    "however , the failure to achieve the payoff 10 after introducing the the @xmath84-exploratory myopic strategy rule is the result of this rule , and has nothing to do with the number of values of the payoff function .",
    "that is , even in a win - lose game , a player who has a winning strategy may fail to guarantee a win in the long run by playing according to the rules of @xmath84-exploratory myopic strategy and memoryless revision .",
    "thus , the introduction of the @xmath84-exploratory myopic strategy rule forces us also to strengthen the revision rule as follows .    for a node @xmath85 , and a history @xmath50 , if the node @xmath8 was never reached in @xmath39 , then @xmath86 .",
    "else , let @xmath87 be the times at which @xmath8 was reached in @xmath39 , then @xmath88    we state , now , that by using little exploration , and averaging revision , player @xmath12 can guarantee to be close to his individually rational ( maxmin ) payoff in @xmath0 .",
    "[ maxmin ] let @xmath89 be a super strategy such that @xmath90 is the valuation super strategy induced by the @xmath84-exploratory myopic strategy and the averaging revision rules .",
    "denote by @xmath91 the distribution over histories in @xmath22 induced by @xmath52 .",
    "let @xmath92 be @xmath12 s individually rational payoff in @xmath0 .",
    "then for every @xmath93 there exists @xmath94 such that for every @xmath95 , for @xmath91-almost all infinite histories @xmath96 , @xmath97    we consider now the case where all players learn to play @xmath0 , using the @xmath72-exploratory myopic strategy and the averaging revision rules .",
    "we show that in such a case , in the long run , the players strategy in the stage game is close to a perfect equilibrium .",
    "we assume for simplicity that the game @xmath0 has a unique perfect equilibrium ( which is true generically ) .",
    "[ perfect ] assume that @xmath0 has a unique perfect equilibrium @xmath98 .",
    "let @xmath99 be the super strategy such that for each @xmath12 , @xmath100 is the valuation super strategy induced by the @xmath72-exploratory myopic strategy , and the averaging revision rules .    let @xmath101 be the distribution over histories induced by @xmath99 .",
    "then there exists @xmath102 , such that for all @xmath103 , for @xmath101-almost all infinite histories @xmath104 , there exists @xmath105 , such that for all @xmath106 , @xmath107 , for each player @xmath12 and node @xmath85 .",
    "we prove all the theorems by induction on the depth of the game tree . for this",
    "we need to be able to deduce properties of @xmath22 from properties of repeated games of stage games @xmath108 which are subgames of @xmath0 .",
    "this can be more naturally done when we consider a wider class of repeated games which we call _ stochastic repeated games_. within this class the repeated game of @xmath108 can be imbedded in the repeated game of @xmath0 , thus enabling us to make the required deductions .",
    "let @xmath109 be a countable set of states which also includes an _ end state _ @xmath110 .",
    "we consider a game @xmath111 in which the game @xmath0 is played repeatedly . before",
    "each round a state from @xmath109 is selected according to a probability distribution which depends on the history of the previous terminal nodes and states . when the state @xmath110 is realized the game ends .",
    "the selected state is known to the players .",
    "the strategy played in each round depends on the history of the terminal nodes and states .",
    "we now describe @xmath111 formally",
    ".    * histories . *",
    "the set of infinite histories in @xmath111 , is @xmath112 . for @xmath25 the set of finite history of @xmath24 rounds ,",
    "is @xmath113 , and the set of preplay histories of @xmath24 rounds is @xmath114 .",
    "denote @xmath115 and @xmath116 .",
    "the subset of @xmath117 of histories that terminate with @xmath110 is denoted by @xmath118 . for @xmath119 and @xmath25",
    "we denote by @xmath120 the history in @xmath121 which consists of the first @xmath24 rounds in @xmath39 . for finite and infinite histories",
    "@xmath39 we denote by @xmath122 the sequence of terminal nodes in @xmath39 .    * transition probabilities . * for each @xmath123 , @xmath124 is a probability distribution on @xmath109 . for @xmath125 , @xmath126 is the probability of transition to state @xmath127 after history @xmath39 .",
    "the probability that the game ends after @xmath39 is @xmath128 .",
    "* super strategies . * after @xmath24 rounds the player observes the history of @xmath24 pairs of a state and a terminal node , and the state that follows them , and then plays @xmath0 .",
    "thus , a super strategy for player @xmath12 is a function @xmath27 from @xmath129 to @xmath12 s strategies in @xmath0 .",
    "we denote by @xmath130 the probability of reaching terminal node @xmath68 when @xmath131 is played .    * the super play distribution . * the super strategy @xmath52 induces the _ super play distribution _ which is a probability distribution @xmath132 over @xmath133 .",
    "it is the unique extension of the distribution over finite histories which satisfies @xmath134 for @xmath123 , and @xmath135 for @xmath136 .          for a node @xmath9 in @xmath0 , denote by @xmath138 the subgame starting at @xmath9 .",
    "fix a super strategy profile @xmath52 in @xmath111 and the induced super play distribution @xmath132 on @xmath139 . in what follows",
    "we describe a stochastic super game @xmath140 , in which the stage game is @xmath138 . for this",
    "we need to define the state space @xmath141 .",
    "we tag histories and states in the game @xmath140 , as well as terminal nodes in @xmath138 .",
    "our purpose in this construction is to imbed @xmath142 in @xmath139 .",
    "the idea is to regard these rounds in a history @xmath39 in @xmath139 in which node @xmath9 is not reached as states in @xmath141 .",
    "let @xmath141 be defined as the set of all @xmath136 , such that node @xmath9 is never reached in @xmath39 .",
    "obviously , @xmath141 subsumes @xmath109 , and in particular includes the end state @xmath110 .",
    "note that the set @xmath142 of infinite history in @xmath140 can be naturally viewed as a subset of @xmath139 , @xmath143 as a subset of @xmath144 , and @xmath145 as a subset of @xmath117 .",
    "we use this fact to define the transition probability distribution @xmath146 in @xmath140 as follows .    for any @xmath147 in @xmath141 and @xmath148 with @xmath149 , @xmath150 where @xmath151 is the probability that node @xmath9",
    "is reached under the strategy profile @xmath152 . for @xmath110 , @xmath153 , where @xmath154 consists of all histories @xmath155 with initial segment @xmath156 such that @xmath9 is never reached after this initial segment .",
    "note that @xmath157 is the probability of all histories in @xmath133 that start with @xmath158 and followed by a terminal node of the game @xmath138 .",
    "these events and the event @xmath154 described above , form a partition of @xmath133 , and therefore @xmath146 is a probability distribution .",
    "[ thesamedistribution ] define a super strategy profile @xmath159 in @xmath140 , by @xmath160 for each @xmath161 , where the right - hand side is the restriction of @xmath162 to @xmath138 .",
    "then , the restriction of @xmath163 to @xmath142 coincides with the super play probability distribution @xmath164 , induced by @xmath159 .",
    "it is enough to show that @xmath165 and @xmath164 coincide on @xmath143 .",
    "the proof is by induction on the length of @xmath166 .",
    "suppose @xmath167 and consider the history @xmath168 .",
    "then , by the definition of the super play distribution ( [ playdistribution ] ) and ( [ preplaydistribution ] ) , @xmath169 by the induction hypothesis and the definitions of @xmath146 in ( [ transitionsub ] ) , the righthand side is @xmath170 . by the definition of @xmath159 in ( [ strategysub ] ) , this is just @xmath171 the right - hand side , in turn , is just @xmath172 .",
    "[ thesamestrategy ] suppose that @xmath12 s strategy in @xmath111 , @xmath27 , is the valuation super strategy starting with @xmath34 , and using either the myopic strategy and the memoryless revision rules , or the @xmath84-exploratory myopic strategy and the averaging revision rules .",
    "then the induced strategy in @xmath140 , @xmath173 , is the valuation super strategy starting with @xmath174 the restriction of @xmath34 to the subgame @xmath138and following the corresponding rules .    the valuation super strategy in @xmath140 , starting with @xmath174 , requires that",
    "after history @xmath166 , strategy @xmath175 is played . here",
    ", @xmath176 is the sequence of all terminal nodes in @xmath156 , which consists of terminal nodes in @xmath138 .",
    "these are also all the terminal nodes of @xmath138 , in @xmath156 , when the latter is viewed as a history in @xmath144 .",
    "when @xmath156 is considered as a history in @xmath144 , then the strategy @xmath177 is @xmath178 , where @xmath179 is the sequence of all terminal nodes in @xmath156 .",
    "@xmath180 is the restriction of @xmath178 to @xmath138 .",
    "but along the history @xmath156 , the valuation of nodes in the game @xmath138 does not change in rounds in which terminal nodes which are _ not _ in @xmath138 are reached",
    ". therefore , @xmath180 and @xmath181 are the same .",
    "the game @xmath22 is in particular a stochastic repeated game , where there is only one state , besides @xmath110 , and transition to @xmath110 ( that is , termination of the game ) has null probability .",
    "we prove all three theorems for the wider class of stochastic repeated games .",
    "the theorems can be stated verbatim for this wider class of games , with one obvious change : any claim about almost all histories should be replaced by a corresponding claim for almost all _ infinite _ histories .",
    "all the theorems are proved by induction on the depth of the game @xmath0 .",
    "the proofs for games of depth 0 ( that is , games in which payoffs are determined in the root , with no moves ) are straightforward and are omitted .",
    "in all the proofs , @xmath182 is the set of all the immediate successors of the root @xmath5 .",
    "assume that the claim of the theorem holds for all the subgames of @xmath0 .",
    "we examine first the case that the first player is not @xmath12 . by the stipulation of the theorem , player",
    "@xmath12 can guarantee payoff 1 in each of the games @xmath183 for @xmath184 .",
    "consider now the game @xmath185 , the super strategy profile @xmath159 , and the induced super play distribution @xmath164 . by the induction hypothesis , and claim 2 , for each @xmath186 , for @xmath187-almost all infinite histories there is a time after which player @xmath12 is paid 1 . in view of claim 1 , for @xmath165-almost",
    "all histories in @xmath111 in which @xmath188 is reached infinitely many times , there exist a time after which player @xmath12 is paid 1 , whenever @xmath188 is reached .",
    "consider now a nonempty subset @xmath189 of @xmath58 .",
    "let @xmath190 be the set of infinite histories in @xmath111 in which node @xmath188 is reached infinitely many times iff @xmath191 . then , for @xmath165-almost all histories in @xmath190 there is a time after which player @xmath12 is paid 1 .",
    "the events @xmath190 when @xmath189 ranges over all nonempty subsets of @xmath58 , form a partition of the set of all infinite histories , which completes the proof in this case .",
    "consider now the case that @xmath12 is the first player in the game . in this case",
    "there is at least one subgame @xmath183 in which @xmath12 can guarantee the payoff 1 .",
    "assume without loss of generality that this holds for @xmath192 .",
    "for a history @xmath39 denote by @xmath193 the random variable that takes as values the subset of the nodes in @xmath58 that have a positive valuation after @xmath24 rounds .",
    "when @xmath193 is not empty , then @xmath12 chooses at @xmath5 , with probability 1 , one the nodes in @xmath193 . as a result",
    "the valuation of this node after the next round is 0 or 1 , while the valuation of all other nodes does not change .",
    "therefore we conclude that @xmath193 is weakly decreasing when @xmath194 .",
    "that is , @xmath195 .",
    "let @xmath196 be the event that @xmath197 for only finitely many @xmath24 s .",
    "then , for @xmath165-almost all histories in @xmath196 there exists time @xmath105 such that @xmath193 is decreasing for @xmath198 .",
    "hence , for @xmath165-almost all histories in @xmath196 there is a nonempty subset @xmath199 of @xmath58 , and time @xmath105 , such that @xmath200 for @xmath198 .",
    "but in order for the set of nodes in @xmath58 with positive valuation not to change after @xmath105 , player @xmath12 must be paid 1 in each round after @xmath105 .",
    "thus we only need to show that @xmath201 .",
    "consider the event @xmath202 that @xmath203 is reached in infinitely many rounds .",
    "as proved before by the induction hypothesis , for @xmath163-almost all histories in @xmath202 , there exists @xmath105 , such that the valuation of @xmath203 is 1 , for each round @xmath198 in which @xmath203 is reached .",
    "the valuation of this node does not change in rounds in which it is not reached .",
    "thus , @xmath204 @xmath165-almost surely .",
    "we conclude that for @xmath165-almost all histories in @xmath205 there is a time @xmath105 , such that @xmath203 is not reached after time @xmath105 .",
    "but @xmath165-almost surely for such histories there are infinitely many @xmath24 s in which the valuation of all nodes in @xmath58 is 0 . in each such history ,",
    "the probability that @xmath203 is not reached is @xmath206 , which establishes @xmath207 .",
    "let @xmath12 be the player at the root of @xmath0 . by the induction hypothesis and claim 1 , for each of the supergames @xmath185 , @xmath184 , for @xmath208-almost infinite histories in this super game",
    ", there is a time after which the same terminal node is reached . by claim 2 , for @xmath132-almost all histories of @xmath22",
    "in which @xmath188 recurs infinitely many times there is a time after which @xmath12 s valuation of this node is constantly the payoff of the same terminal node of @xmath183 .",
    "it is enough that we show that for @xmath132-almost all infinite histories in @xmath111 , there is a time after which the same node from @xmath58 is selected with probability 1 at the root .",
    "suppose that this is not the case .",
    "then there must be a set of histories @xmath154 with @xmath209 , two nodes @xmath188 and @xmath210 , and two terminal nodes @xmath211 and @xmath212 in @xmath183 and @xmath213 correspondingly , that recur infinitely many times in this set .",
    "therefore , for @xmath132-almost all histories in @xmath154 , @xmath12 s valuation of @xmath188 and @xmath210 is @xmath214 and @xmath215 .",
    "since @xmath0 is generic , we may assume that @xmath216 .",
    "thus , for @xmath132-almost all histories in @xmath154 , there is a time after which the conditional probability of @xmath210 given the history is 0 . which is a contradiction .",
    "we denote by @xmath218 , @xmath12 s average payoff at time @xmath24 in history @xmath39 .",
    "fix a subgame @xmath183 .",
    "histories in the game @xmath219 are tagged .",
    "thus , @xmath220 is @xmath12 s average payoff at time @xmath24 in history @xmath156 in @xmath219 .",
    "let @xmath39 be a history in @xmath22 in which @xmath188 recurs infinitely many times at @xmath221 .",
    "let @xmath222 . denote by @xmath223 @xmath12 s average payoff until @xmath24 _ at the times @xmath188 was reached _",
    ", that is , @xmath224    the history @xmath39 can be viewed as an infinite history @xmath156 in @xmath219 . moreover , for each @xmath225 , @xmath226 . by the definition of @xmath223",
    ", it follows that if there exists @xmath57 such that for each @xmath227 , @xmath228 , then there exits @xmath105 such that for each @xmath106 , @xmath229",
    ". by the induction hypothesis there is @xmath102 , such that for all @xmath103 , for @xmath230-almost all histories @xmath156 there exists such an @xmath57 .",
    "thus , by claims [ thesamedistribution ] and [ thesamestrategy ] , there exists @xmath102 , such that for all @xmath186 and @xmath103 , for @xmath101-almost all histories @xmath39 in @xmath111 in which @xmath188 recurs infinitely many times , there exists a time @xmath105 such that for each @xmath106 , @xmath231 .",
    "let @xmath189 be a nonempty subset of @xmath58 , and let @xmath190 be the set of all infinite histories in which the set of nodes that recurs infinitely many times is @xmath189 .",
    "consider a history @xmath39 in @xmath190 , with @xmath222 .",
    "let @xmath233 be the number of times @xmath188 is reached in @xmath39 until time @xmath24 .",
    "then , @xmath234 where the inequality holds , because @xmath235 , and for @xmath236 , @xmath237 .",
    "thus for @xmath91-almost all histories @xmath39 in @xmath190 , @xmath238 since this is true for all @xmath189 , the conclusion of the theorem follows for all infinite histories .",
    "next , we examine the case that @xmath12 is the first player .",
    "note that in this case , for each node @xmath188 , @xmath239 .",
    "observe , also , that for @xmath101-almost all infinite histories @xmath39 in @xmath111 , each of the subgames @xmath183 recurs infinitely many times in @xmath39 . indeed , after each finite history , each of the games @xmath183 is selected by @xmath12 with probability @xmath84 at least .",
    "thus , the event that one of these games is played only finitely many times has probability 0 .",
    "let @xmath240 be a binary random variable over histories such that @xmath241 for histories @xmath39 in which the node @xmath242 selected by player @xmath12 at time @xmath24 satisfies , @xmath243 and @xmath244 otherwise .",
    "[ epsilon1 ] there exists @xmath102 such that for all @xmath245 and any @xmath103 , for @xmath101-almost all infinite histories @xmath39 in @xmath111 there is time @xmath105 such that for all @xmath106 , @xmath246 latexmath:[\\[\\label{nextround }      the inequality ( [ convergence ] ) follows from the induction hypothesis . for ( [ nextround ] ) , note that if @xmath188 is not reached in round @xmath30 then the difference in ( [ nextround ] ) is 0 . if @xmath188 is reached then @xmath251 , where @xmath252 is the number of times @xmath188 was reached in @xmath120 and @xmath253 is the payoff in round @xmath30 .",
    "but , @xmath252 goes to infinity with @xmath24 , and thus ( [ nextround ] ) holds for large enough @xmath24 .    for ( [ expectation ] ) , observe that ( [ convergence ] ) implies @xmath254 , as @xmath255 .",
    "then , by ( [ nextround ] ) , @xmath256 for each history @xmath156 such that @xmath248 . therefore , after @xmath120 , player @xmath12 chooses , with probability at least @xmath72 , a node @xmath242 that satisfies ( [ jzero ] ) , which shows ( [ expectation ] ) .    the information about the conditional expectations in ( [ expectation ] ) has a simple implication for the averages of @xmath240 . to see it we use the following convergence theorem from love ( 1963 ) p. 387 .",
    "consider now the restriction of the random variables @xmath240 to the set of infinite histories with @xmath101 conditioned on this space . from ( [ expectation ] ) it follows that on this space , almost surely @xmath261",
    ". therefore , almost surely @xmath262 .",
    "this is so , because the field generated by the the random variables @xmath263 is coarser than the field generated by histories @xmath120 .",
    "since condition ( [ variances ] ) holds for @xmath240 , it follows by the stability theorem that for @xmath101-almost all infinite histories @xmath39 , @xmath264    by the definition of @xmath240 , @xmath265 where @xmath266 is the minimal payoff in @xmath0 . if we choose @xmath102 such that @xmath267 , then by ( [ limit ] ) , for each @xmath268 , @xmath269 for @xmath101-almost all infinite histories .",
    "assume that the claim of the theorem holds for all the subgames of @xmath0 .",
    "we denote by @xmath270 the restriction of the valuation @xmath34 to @xmath183 , and by @xmath271 , @xmath12 s perfect equilibrium strategy there , which is also the restriction of @xmath272 to this game",
    ".      then there exists @xmath275 such that for all @xmath103 , node @xmath188 , and player @xmath12 , for @xmath230 almost all infinite histories @xmath156 of @xmath219 there exists @xmath105 such that for all @xmath106 , @xmath276 for each node @xmath85 in @xmath183 , and latexmath:[\\[\\label{perfect - expectation }      the equality ( [ perfect - induction ] ) is the induction hypothesis .",
    "consider a history @xmath279 for which ( [ perfect - induction ] ) holds . in the round that follows @xmath279",
    ", the perfect equilibrium path in @xmath183 is played with probability @xmath280 at least , where @xmath281 is the depth of @xmath0 .",
    "player @xmath273 s payoff in this path is @xmath274 .",
    "thus for small enough @xmath102 , ( [ perfect - expectation ] ) holds .    by claims [ thesamedistribution ] and [ thesamestrategy ] it follows from ( [ perfect - induction ] ) that for @xmath103 , for @xmath101 all histories @xmath39 in @xmath22",
    ", there exists @xmath105 such that for all @xmath106 the strategies played in each of the games @xmath219 is the perfect equilibrium of @xmath183 .",
    "thus , to complete the proof it is enough to show that in addition , at the root , @xmath273 chooses in these rounds , with probability @xmath79 , the node @xmath242 for which @xmath282 . for this we need to show that @xmath273 s valuation of @xmath242 is higher than the valuation of all other nodes @xmath188 .    to show it ,",
    "let @xmath283 be the difference between @xmath284 and the second highest payoffs @xmath274 . by the assumption of the uniqueness of the perfect equilibrium , @xmath93 .",
    "note that as all players strategies are fixed for @xmath106 , @xmath285 exists . using the stability theorem , as in theorem [ maxmin ]",
    ", we conclude that @xmath286 exists , and by ( [ perfect - expectation ] ) the inequality @xmath287 holds , where @xmath288 is @xmath273 s average payoff until round @xmath24 of history @xmath156 , in the game @xmath219 .    as in the proof of theorem [ maxmin ]",
    ", it follows that for @xmath101-almost all infinite histories @xmath39 in @xmath22 , @xmath289 .",
    "but then , for @xmath101-almost all infinite histories @xmath39 there exists @xmath105 such that for all @xmath106 , @xmath290 is the highest valuation of all the nodes @xmath188 ."
  ],
  "abstract_text": [
    "<S> a valuation for a player in a game in extensive form is an assignment of numeric values to the players moves . </S>",
    "<S> the valuation reflects the desirability moves . </S>",
    "<S> we assume a myopic player , who chooses a move with the highest valuation . </S>",
    "<S> valuations can also be revised , and hopefully improved , after each play of the game . here , </S>",
    "<S> a very simple valuation revision is considered , in which the moves made in a play are assigned the payoff obtained in the play . </S>",
    "<S> we show that by adopting such a learning process a player who has a winning strategy in a win - lose game can almost surely guarantee a win in a repeated game . when a player has more than two payoffs , a more elaborate learning procedure is required . </S>",
    "<S> we consider one that associates with each move the average payoff in the rounds in which this move was made . </S>",
    "<S> when all players adopt this learning procedure , with some perturbations , then , with probability 1 , strategies that are close to subgame perfect equilibrium are played after some time . a single player who adopts this procedure can guarantee only her individually rational payoff .    </S>",
    "<S> # 1*proof#1 .   </S>",
    "<S> *       </S>",
    "<S>   </S>"
  ]
}