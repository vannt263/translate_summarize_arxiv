{
  "article_text": [
    "measurements of the hubble constant are a unique data set for statistical analysis for two reasons .",
    "first , huchra s compilationhuchra/. ] with over 400 measurements is one of the largest collection of measurements of a single quantity .",
    "second , the hubble constant is now one of the more precisely determined cosmological parameters ( see , e.g. , freedman et al .  2001 ; bennett et al .",
    "2003 ) .",
    "it is also of great interest to understand how well the hubble constant has been measured , both because it is an important cosmological parameter and because of the role it plays in various cosmological tests , most importantly the expansion time test ( see , e.g. , peebles & ratra 2003 for a review )",
    ".    assuming a value for the hubble constant  in the body of this paper we work with @xmath0 = 71 km s@xmath1 mpc@xmath1 , the central value from the combined wmap and other data analysis of spergel et al .",
    "( 2003 ) = 67 km s@xmath1 mpc@xmath1 from the gott et al .",
    "( 2001 ) median statistics analysis of a major , earlier subset of the @xmath0 measurements considered here , showing that our conclusions are not sensitive to the precise central value of @xmath0 assumed in the estimated 10 % ( two standard deviation ) range now under discussion ( see , e.g. , gott et al .",
    " one may use huchra s compilation of @xmath5 ( where @xmath6 and @xmath7 are the upper and lower one standard deviation error bars ) to construct the distribution of errors of the hubble constant measurements .",
    "this is a plot of the number of measurements as a function of the number of standard deviations ( @xmath8 ) the measurement deviates from the actual value @xmath0 .",
    "here @xmath9 when @xmath10 and @xmath11 when @xmath12 .    in our analysis here we use measurements from huchra s compilation up to and including measurement 2003.239 . deleting the four entries from 1924 and 1925 that lack actual estimates of @xmath0 , we use",
    "461 published estimates of @xmath0 in our analysis here , 40 % more than the 331 used in the analysis of gott et al .",
    "observers often note that there could be unknown systematic errors , however authors quoted errors have been used to evaluate the accuracy of @xmath0 estimates and so it is important to understand the quoted error distribution .    in @xmath13 2",
    "we describe our analysis of this collection of 461 measurements , assuming that @xmath0 = 71 km s@xmath1 mpc@xmath1 , the central value from the combined wmap and other data analysis of bennett et al .",
    "( 2003 ) and spergel et al .",
    "( 2003 ) . for comparison , to show that the results are robust to small changes in the true value of @xmath0 , summary results from an analysis based on the central value of @xmath0 = 67 km s@xmath1 mpc@xmath1 from the gott et al .",
    "( 2001 ) median statistics study are presented in the appendix .",
    "we conclude in @xmath13 3 .",
    "figure 1 shows the distribution of deviations of the 461 measurements from the central wmap value of @xmath0 = 71 km s@xmath1 mpc@xmath1 , in units of the quoted standard deviation of the measurement .",
    "this is the error distribution of the @xmath0 measurements ; the left panel shows the signed error distribution and the right panel shows the absolute magnitude of the errors ( the distribution in the right panel is symmetric about @xmath14 ) .",
    "these error distributions have significant tails : there are numerous measurements 5 and even 10 standard deviations away .",
    "more precisely , in the signed error distribution of fig .",
    "1@xmath15 68.3 % and 95.4 % of the probability lies in the range @xmath16 and @xmath17 , respectively , and for the absolute magnitude error distribution of fig .",
    "1@xmath18 the corresponding limits are @xmath19 and @xmath20 , respectively .",
    "an alternative characterization of the tails of this distribution is provided by the fraction of data within the @xmath21 = 1 and 2 ranges , which for the distribution shown in fig .",
    "1@xmath18 is 48 % and 69 % , respectively .",
    "these are impressively high ( nearly half the observed values are within one standard deviation of 71 km s@xmath1 mpc@xmath1 ) but still clearly at odds with what is expected for a gaussian distribution .",
    "it is of interest to quantify how well the data of fig .",
    "1 are fit by various simple distribution functions , and to determine the parameters of these functions that result in the best fit to the data . to do this",
    "we proceed as follows . for our purposes",
    "it is useful to maximize the number of data points in each bin as well as the number of bins .",
    "this is perhaps best accomplished by using 21 bins ( close to the square root of 461 ) , labelled by integer @xmath22 that runs from 1 to 21 , and adjusting the widths of the bins , @xmath23 , to ensure equal expected probability ( for the assumed distribution function ) in each bin .",
    "thus , for an assumed distribution ( such as a gaussian ) we construct 21 bins such that the expected number of data points in each bin would be 21.95 .",
    "then we compare with the number of data points observed in each of the 21 bins and do a @xmath24 analysis , as discussed in the next paragraph .",
    "( since the number expected in each bin is large compared to unity , a @xmath24 analysis is justified . ) with this prescription , the data binning depends on the assumed probability distribution function , @xmath25 ( in this paper we present results only from the fit to the symmetric absolute error distribution , e.g. , that in fig .",
    "1@xmath18 ) .    to estimate goodness of fit we use the assumed probability distribution function to compute the expected number of measurements in each bin @xmath22 , @xmath26 , where @xmath27 is the total number of measurements .",
    "since there are a finite number of measurements in each bin , they should be poisson distributed with mean value @xmath26 for the @xmath28 bin . for the poisson distribution",
    "the variance @xmath29 is equal to the mean hence the total @xmath24 is @xmath30 ^ 2 } \\over n p(|n_\\sigma|_j ) } , \\ ] ] where @xmath31 is the observed number of measurements in each bin .",
    "we shall tabulate the reduced @xmath24 , @xmath32 , where @xmath33 is the number of degrees of freedom , i.e. , the number of bins ( 21 ) less the number of constraints and fitting parameters .",
    "given @xmath34 and @xmath33 one may compute the probability that the assumed distribution well describes the spread of the measurements . in the computation of this probability",
    "we assume that the bins are uncorrelated , which is not necessarily true ( since lower rungs of the distance ladder introduce correlations in subsets of the measurements ) .",
    "it is therefore wise to place quantitative emphasis on just the @xmath34 values and use the corresponding probabilities as simply a qualitative indicator of goodness of fit .",
    "we consider four probability distribution functions and as mentioned above focus on the absolute magnitude error distribution , as in fig .",
    "1@xmath18 , so all distributions we consider will be centered at @xmath14 .",
    "one constraint that must be satisfied is that the total number of measurements must sum to 461 .",
    "since we consider 21 bins and normalize to fit the total number of measurements , a probability distribution function with no free parameters will have @xmath35 degrees of freedom .    even though we have noted the existence of extended tails in the error distributions of fig .",
    "1 , it is natural  perhaps pavlovian  to first consider the gaussian distribution , initially with width chosen so that @xmath36 corresponds to one standard deviation , and then with a scale factor to vary the width of the distribution .",
    "that is , we take as probability distribution function the gaussian expression @xmath37 , \\ ] ] for the case where @xmath36 is equivalent to one standard deviation , and then consider the function @xmath38 where @xmath39 is a scale factor that is adjusted to minimize @xmath24 .",
    "( we allow @xmath39 to vary over the range 0.5 to 3 in steps of 0.1 when computing @xmath24 . ) in the first case there are no additional free parameters so @xmath35 ; the scale factor @xmath39 is an additional free parameter in the second case so here we have @xmath40 degrees of freedom .",
    "figure 2 shows the measurement error histograms and the best - fit gaussians , both normalized to unit area .",
    "numerical values are listed in table 1 .",
    "these show that if @xmath0 = 71 km s@xmath1 mpc@xmath1 then the measurement error distributions are extremely poorly fit by a gaussian , even if the gaussian width is allowed to be a free parameter . interestingly , if the width is allowed to float while minimizing @xmath24 it favors 1.8 , i.e. , the assumed distribution favors identifying @xmath41 with one standard deviation , almost double the value one would naively infer from the measurement errors , thus perhaps indicating that in this gaussian case it might not be unreasonable to roughly double the quoted error bars , consistent with our earlier discussion of extended tails .",
    "this situation might profitably be contrasted with what happened in the early days of cosmic microwave background spatial anisotropy measurements , where a number of models fit the measurements extremely well , perhaps indicating that the error bars had been over estimated ( ganga , ratra , & sugiyama 1996 ) . in any case , it is very unlikely that the @xmath0 measurement errors are described by a gaussian distribution .",
    "( note that the probability is a little higher for the @xmath0 = 67 km s@xmath1 mpc@xmath1 case , but even here a gaussian distribution is a very poor fit . )",
    "the fact that the error distribution of hubble constant measurements is non - gaussian does not necessarily imply an underlying non - gaussianity in the measurement errors .",
    "rather , the distribution tells us something about the observers ability to correctly estimate systematic and statistical uncertainties .",
    "figure 2 indicates that the distribution of hubble constant measurement errors has a more extended tail than is predicted by a gaussian probability distribution .",
    "perhaps the most well - known distribution with an extended tail is the cauchy , or lorentzian , or breit - wigner distribution , @xmath42 we also consider the case @xmath38 where the scale factor @xmath39 is allowed to vary while @xmath24 is minimized .",
    "figure 3 shows the data and best - fit cauchy distributions , and numerical values are listed in table 1 .",
    "unlike the gaussian case , the cauchy distribution can not be rejected ; it is acceptable at 9.9 % or 8.7 % depending on whether @xmath39 is fixed to unity or allowed to vary ( and it does significantly better at @xmath0 = 67 km s@xmath1 mpc@xmath1 ) .",
    "however , it is clear from fig .",
    "3 that the cauchy distribution has greater probability in the extended tails than does the hubble constant measurements error distribution .",
    "the cauchy distribution has a similar central peak , with a 50 % chance that @xmath43 , but a 95.4 % chance that @xmath44 instead of 7.0 as observed . it would therefore be beneficial to search for a distribution that has broader tails than the gaussian one but narrower than the cauchy case .",
    "a cauchy distribution with @xmath45 would result if the errors were gaussian distributed , observers took measurements free of systematic errors , divide their data into two parts , used each half to produce two independent estimates of the hubble constant , @xmath46 and @xmath47 , and produced a mean estimate @xmath48 with an error estimate ( standard deviation of the mean ) of @xmath49 .",
    "if @xmath46 and @xmath47 are drawn from an underlying gaussian distribution centered on the true value @xmath50 then @xmath51 is distributed like a cauchy distribution with @xmath52 .",
    "that gives a 50 % chance that @xmath53 is within 1 @xmath54 of the true value and a 95 % chance that @xmath53 is within 12.7 @xmath54 of the true value .",
    "the large tails result because in a gaussian distribution there is an appreciable chance that @xmath55 will be significantly less than the true sigma for the distribution . in this scenario the observer is really using the self - consistency of her observations to set the error bars .",
    "if one measures the distance to two galaxies using cepheids , and gets two values for the hubble constant that are close to each other , one may well be tempted to think that one s method has the high degree of accuracy implied by the observed value of @xmath49 .",
    "indeed , if one estimated the errors by other means ( estimated uncertainties in measuring the observed quantities required to measure the hubble constant , along with standard propagation of errors ) and one got an error significantly larger than @xmath55 then one might be suspicious that one should be so lucky as to obtain such a small value of @xmath55 . yet rarely , such lucky coincidences do occur and it is precisely these cases that cause the large tails in the cauchy distribution .",
    "the cauchy distribution with @xmath45 is acceptable at 9.9 % , but not a good fit and there are other distributions that are better fits .",
    "as we have noted , a cauchy distribution with @xmath45 would result from a true gaussian distribution if the observer divided his data into two parts , used the data itself to set error bars , and made the mistake of assuming the errors should be distributed according to a gaussian distribution rather than the student @xmath4 distribution ( which for the case of two data points is the @xmath56 student @xmath4 distribution , or the cauchy distribution ) .",
    "this prompts us to investigate the general student @xmath4 distributions .",
    "student s @xmath4 distribution is @xmath57 \\over\\sqrt{\\pi n}\\ ,     \\gamma(n/2 ) }     { 1 \\over \\left(1 + |n_\\sigma|^2/n\\right)^{(n+1)/2 } } , \\ ] ] where @xmath58 is positive and @xmath59 the gamma function .",
    "we also consider the distribution @xmath60 where the scale factor @xmath39 is allowed to vary while @xmath24 is minimized . when @xmath61 student s @xmath4 distribution becomes the gaussian distribution and for @xmath56 it is the cauchy distribution . for @xmath62 student",
    "s @xmath4 distribution has narrower tails than the cauchy case but broader ones than the gaussian distribution , just as wanted .",
    "we have fit student s @xmath4 distribution to the @xmath0 measurement errors data while allowing @xmath58 to take on integer values between 2 and 6 ( and sometimes going up to 30 ) , so in this case we have one additional parameter and hence one less degree of freedom .",
    "we find @xmath63 always minimizes the value of @xmath24 and so show this case in fig .  4 and table 1 . from table 1",
    "we see that if the scale factor @xmath39 is held at unity student s @xmath4 distribution is an unlikely fit to the data , especially if @xmath0 = 71 km s@xmath1 mpc@xmath1 .",
    "however , if @xmath39 is allowed to vary as @xmath24 is minimized , student s @xmath4 distribution with @xmath63 is an excellent fit to the @xmath0 measurements error distribution , and fig .",
    "4@xmath18 shows that there is very good agreement between the expected and measured counts in the last bin .",
    "the final probability density distribution we consider is the double exponential or laplace distribution , @xmath64 this falls off less rapidly than the gaussian distribution but faster than the cauchy distribution .",
    "the sample median is the best estimator for the mean of this distribution ( eadie et al .",
    "the results of the fit are shown in fig .  5 and listed in table 1 . as in the case for student s @xmath4 distribution with @xmath63 ,",
    "when @xmath39 is held fixed at unity the double exponential is an unacceptable fit to the @xmath0 measurements error distribution but when @xmath39 is allowed to vary it is an excellent fit to the data .    in the first paragraph of this section we noted that in fig .  1@xmath18 68.3 % and 95.4 % of the probability lies in the range @xmath19 and @xmath20 , respectively , and the @xmath65 and @xmath66",
    "ranges include 48 % and 69 % of the data points , respectively .",
    "( see the appendix for the corresponding numbers for the @xmath0 = 67 km s@xmath1 mpc@xmath1 case . )",
    "tables 2 and 3 show the related limits for the various probability density distributions we consider in this paper .",
    "these numerical values provide another indication of the non - gaussianity of the hubble constant measurement error distribution .",
    "our analysis of a perhaps unique ( because of its size ) data set , the measurement errors of all available estimates of the hubble constant , makes for some interesting conclusions . if all observers have done perfect jobs at estimating their errors and the true errors were gaussian , as might be expected , then the distributions in fig",
    ".  1 should be gaussian with standard deviation of unity .",
    "first , and perhaps not totally unexpectedly , the errors in the hubble constant are not gaussianly distributed , even if the scale factor @xmath39 is allowed to vary when minimizing @xmath24 . at the minimum value of @xmath24 , @xmath67 , suggesting it might be reasonable to roughly double the magnitude of @xmath0 measurement error bars .",
    "early observers using inferior equipment or techniques would have larger errors , but knowing that their methods were uncertain should have established larger error bars .",
    "as methods improved the measurements become more accurate but the stated error bars become smaller .",
    "early or late observers are at no relative disadvantage relative to others .",
    "indeed , each observer has freedom to state her error bars and has a priori an equal chance of having the true value occur within one standard deviation of their result . over - optimism",
    "would produce error bars that were too small while over - conservatism would produce error bars that were too large . which occurs in practice ?",
    "the results here suggest that astronomers were over - optimistic by almost a factor of 2 .",
    "in some case there were systematic errors of which the observers were simply unaware ( such as mistaking hii regions for bright stars ) .",
    "in other cases , standard candles were not as standard as imagined , leaving some steps in the distance ladder wrong by more than people thought .",
    "also , using self consistency in the data as a check on the errors can lead to large tails because it occasionally induces one to be over - optimistic ( the student @xmath4 effect ) . and the real data may have non - gaussian tails ( say in the luminosity of standard candles ) . in general over - conservatism ( the urge to be right ) always competes with over - optimism ( the urge to have more interesting limits ) . in the case of the hubble constant astronomers",
    "were over - optimistic . in a history - of - science context , it might be of interest to more closely examine the most deviant measurements of fig .  1 , those that have @xmath21 larger than say 7 , to understand why these are so deviant , but this is not our purpose here .",
    "the hubble constant measurement history suggests that to be really sure ( 95.4  % ) you have to go to 7 @xmath2 .",
    "this may explain why some people are cautious upon hearing of a three standard deviation result .",
    "it s not that they believe the errors but want to be more sure than 99.7  % .",
    "it s that they suspect there is a large chance ( @xmath68 50  % ) that the error bars may have been underestimated by a factor of 2 or 3 and the chance it is really correct is consequently really significantly less than 99.7  % .",
    "second , an @xmath63 student s @xmath4 distribution , with @xmath69 , or a double exponential distribution , with @xmath70 , are excellent fits to the @xmath0 measurement errors distribution , with @xmath0 = 67 km s@xmath1 mpc@xmath1 having a somewhat higher probability than @xmath0 = 71 km s@xmath1 mpc@xmath1 .",
    "the hubble constant measurement history gives an interesting example where we can access how trustworthy quoted errors might be in fundamental measurements .",
    "it would be interesting to study comparative examples from other fields .",
    "in particular , it would be interesting to know whether the @xmath63 student @xmath4 distribution or the double exponential distribution also provides a good description of the measurement errors of other quantities .",
    "we are grateful to j.  huchra for the compilation of @xmath0 measurements and acknowledge useful discussions with a.  kosowsky .",
    "gc and br acknowledge support from nsf career grant ast-9875031 and doe epscor grant de - fg02 - 00er45824 .",
    "jrg acknowledges support from nsf grant ast-9900772 .",
    "in the main body of the paper we assumed @xmath0 = 71 km s@xmath1 mpc@xmath1 , the central value from the wmap analysis . the wmap @xmath0 error bars are @xmath71 km s@xmath1 mpc@xmath1 ( bennett et al .",
    "@xmath0 is pinned down to only about 10 % at two standard deviations ( gott et al .",
    "2001 ) so it is reasonable to find out how our conclusions depend on the value of @xmath0 .",
    "in this appendix we use @xmath0 = 67 km s@xmath1 mpc@xmath1 , the central value from the gott et al .",
    "( 2001 ) median statistics analysis of a subset ( 331 measurements prior to mid 1999 ) of the 461 measurements used here .",
    "= 68 km s@xmath1 mpc@xmath1 ( using all 461 measurements ) ; the small shift in the median after a 40 % increase in the number of measurements considered is great tribute to its robustness .",
    "see gott et al .",
    "( 2001 ) , podariu et al .",
    "( 2001 ) , avelino , martins , & pinto ( 2002 ) , and chen & ratra ( 2003 ) for other cosmological applications of median statistics . ]",
    "figure 6 shows the @xmath0 measurements error distribution for the case when @xmath0 = 67 km s@xmath1 mpc@xmath1 .",
    "this distribution has a somewhat less prominent central peak than the @xmath0 = 71 km s@xmath1 mpc@xmath1 case . in the signed error distribution of fig .",
    "6@xmath15 68.3 % and 95.4 % of the probability lies in the range @xmath72 and @xmath73 , respectively , while for the absolute magnitude error distribution of fig .",
    "6@xmath18 the corresponding limits are @xmath74 and @xmath75 , respectively . in fig .",
    "6@xmath18 the @xmath76 and @xmath66 ranges include 51 % and 72 % of the data points , respectively .",
    "again , these are at odds with what is expected for a gaussian distribution .",
    "table 1 also lists the numerical fitting results for the @xmath0 = 67 km s@xmath1 mpc@xmath1 case . as in the case when @xmath0 = 71 km s@xmath1 mpc@xmath1 , here again the @xmath63 student s @xmath4 distribution and the double exponential distribution provide excellent fits to the @xmath0 error histogram when the scale factor @xmath39 is allowed to vary when minimizing @xmath24 .",
    "these two distributions are shown in figs .",
    ". it might be significant that the @xmath0 = 67 km s@xmath1 mpc@xmath1 case always has a lower @xmath24 than the @xmath0 = 71 km s@xmath1 mpc@xmath1 case , indicating perhaps that the median statistics value determined from a large fraction of the data is more robust  time will tell . in any case , a comparison of the entries in table 1 shows that the results presented here are robust with respect to small changes in the value of @xmath0 .",
    "lccccccccccccc gaussian & & 1 & 19.8 & 20 & @xmath77 & & 1 & 15.0 & 20 & @xmath77 + gaussian & & 1.8 & 2.63 & 19 & @xmath77 & & 1.7 & 1.92 & 19 & 0.94 + cauchy & & 1 & 1.42 & 20 & 9.9 & & 1 & 1.10 & 20 & 35 + cauchy & & 1.1 & 1.46 & 19 & 8.7 & & 1.0 & 1.15 & 19 & 29 + @xmath63 student s @xmath4 & & 1 & 2.58 & 19 & @xmath77 & & 1 & 1.56 & 19 & 5.7 + @xmath63 student s @xmath4 & & 1.3 & 0.717 & 18 & 80 & & 1.2 & 0.326 & 18 & 99.7 + double exponential & & 1 & 7.12 & 20 & @xmath77 & & 1 & 5.11 & 20 & @xmath77 + double exponential & & 1.5 & 0.501 & 19 & 96 & & 1.6 & 0.325 & 19 & 99.7    lcccccccc gaussian & & 1 & 1.0 & 2.0 & & 1 & 1.0 & 2.0 + gaussian & & 1.8 & 1.8 & 3.6 & & 1.7 & 1.7 & 3.4 + cauchy & & 1 & 1.8 & 14 & & 1 & 1.8 & 14 + cauchy & & 1.1 & 2.0 & 15 & & 1.0 & 1.8 & 14 + @xmath63 student s @xmath4 & & 1 & 1.3 & 4.5 & & 1 & 1.3 & 4.5 + @xmath63 student s @xmath4 & & 1.3 & 1.7 & 5.9 & & 1.2 & 1.6 & 5.4 + double exponential & & 1 & 1.2 & 3.1 & & 1 & 1.2 & 3.1 + double exponential & & 1.5 & 1.7 & 4.6 & & 1.6 & 1.8 & 4.9 + observed & & & 1.9 & 7.0 & & & 1.7 & 7.5    lcccccccc gaussian & & 1 & 0.68 & 0.95 & & 1 & 0.68 & 0.95 + gaussian & & 1.8 & 0.42 & 0.73 & & 1.7 & 0.44 & 0.76 + cauchy & & 1 & 0.50 & 0.71 & & 1 & 0.50 & 0.71 + cauchy & & 1.1 & 0.47 & 0.68 & & 1.0 & 0.50 & 0.71 + @xmath63 student s @xmath4 & & 1 & 0.58 & 0.82 & & 1 & 0.58 & 0.82 + @xmath63 student s",
    "@xmath4 & & 1.3 & 0.48 & 0.74 & & 1.2 & 0.51 & 0.76 + double exponential & & 1 & 0.63 & 0.87 & & 1 & 0.63 & 0.87 + double exponential & & 1.5 & 0.49 & 0.74 & & 1.6 & 0.47 & 0.71 + observed & & & 0.48 & 0.69 & & & 0.51 & 0.72"
  ],
  "abstract_text": [
    "<S> we construct the error distribution of hubble constant ( @xmath0 ) measurements from huchra s compilation of 461 measurements of @xmath0 and the wmap experiment central value @xmath0 = 71 km s@xmath1 mpc@xmath1 . </S>",
    "<S> this error distribution is non - gaussian , with significantly larger probability in the tails of the distribution than predicted by a gaussian distribution . </S>",
    "<S> the 95.4 % confidence limits are 7.0 @xmath2 in terms of the quoted errors . </S>",
    "<S> it is remarkably well described by either a widened @xmath3 student s @xmath4 distribution or a widened double exponential distribution . </S>",
    "<S> these conclusions are unchanged if we use instead the central value @xmath0 = 67 km s@xmath1 mpc@xmath1 found from a median statistics analysis of a major subset of @xmath0 measurements used here . </S>"
  ]
}