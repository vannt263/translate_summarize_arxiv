{
  "article_text": [
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ this is a late parrot !",
    "it s a stiff !",
    "bereft of life , it rests in peace ! if you had nt nailed him to the perch he would be pushing up the daisies !",
    "its metabolical processes are of interest only to historians !",
    "it s hopped the twig !",
    "it s shuffled off this mortal coil !",
    "it s rung down the curtain and joined the choir invisible !",
    "this is an ex - parrot ! _  monty python , `` pet shop '' _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    a mechanism for automatically generating multiple paraphrases of a given sentence would be of significant practical import for text - to - text generation systems .",
    "applications include summarization and rewriting @xcite : both could employ such a mechanism to produce candidate sentence paraphrases that other system components would filter for length , sophistication level , and so forth . not surprisingly , therefore , paraphrasing has been a focus of generation research for quite some time @xcite .",
    "one might initially suppose that sentence - level paraphrasing is simply the result of word - for - word or phrase - by - phrase substitution applied in a domain- and context - independent fashion .",
    "however , in studies of paraphrases across several domains , this was generally not the case .",
    "for instance , consider the following two sentences ( similar to examples found in ):    0.9 after the latest fed rate cut , stocks rose across the board .",
    "+    observe that `` fed '' ( federal reserve ) and `` greenspan '' are interchangeable only in the domain of us financial matters .",
    "also , note that one can not draw one - to - one correspondences between single words or phrases .",
    "for instance , nothing in the second sentence is really equivalent to `` across the board '' ; we can only say that the entire clauses `` stocks rose across the board '' and `` winners strongly outpaced losers '' are paraphrases .",
    "this evidence suggests two consequences : ( 1 ) we can not rely solely on generic domain - independent lexical resources for the task of paraphrasing , and ( 2 ) _ sentence - level _ paraphrasing is an important problem extending beyond that of paraphrasing smaller lexical units .",
    "_ our work presents a novel knowledge - lean algorithm that uses _ multiple - sequence alignment _ ( msa ) to _ learn _ to generate sentence - level paraphrases essentially from unannotated corpus data alone .",
    "_ in contrast to previous work using msa for generation , we need neither parallel data nor explicit information about sentence semantics .",
    "rather , we use two _ comparable corpora _ , in our case , collections of articles produced by two different newswire agencies about the same events .",
    "the use of related corpora is key : we can capture paraphrases that on the surface bear little resemblance but that , by the nature of the data , must be descriptions of the same information . note that we also acquire paraphrases from each of the individual corpora ; but the lack of clues as to sentence equivalence in single corpora means that we must be more conservative , only selecting as paraphrases items that are structurally very similar .",
    "our approach has three main steps .",
    "first , working on each of the comparable corpora separately , we compute _ lattices _  compact graph - based representations  to find commonalities within ( automatically derived ) groups of structurally similar sentences .",
    "next , we identify pairs of lattices from the two different corpora that are paraphrases of each other ; the identification process checks whether the lattices take similar arguments . finally , given an input sentence to be paraphrased , we match it to a lattice and use a paraphrase from the matched lattice s mate to generate an output sentence .",
    "the key features of this approach are :    * focus on paraphrase generation . * in contrast to earlier work , we not only extract paraphrasing rules , but also automatically determine which of the potentially relevant rules to apply to an input sentence and produce a revised form using them",
    ".    * flexible paraphrase types . * previous approaches to paraphrase acquisition focused on certain rigid types of paraphrases , for instance , limiting the number of arguments .",
    "in contrast , our method is not limited to a set of _ a priori_-specified paraphrase types .    * use of comparable corpora and minimal use of knowledge resources . *",
    "in addition to the advantages mentioned above , comparable corpora can be easily obtained for many domains , whereas previous approaches to paraphrase acquisition ( and the related problem of phrase - based machine translation ) required parallel corpora .",
    "we point out that one such approach , recently proposed by , also represents paraphrases by lattices , similarly to our method , although their lattices are derived using parse information .",
    "moreover , our algorithm does not employ knowledge resources such as parsers or lexical databases , which may not be available or appropriate for all domains  a key issue since paraphrasing is typically domain - dependent .",
    "nonetheless , our algorithm achieves good performance .",
    "previous work on automated paraphrasing has considered different levels of paraphrase granularity . learning synonyms via distributional similarity",
    "has been well - studied . and identify phrase - level paraphrases , while and acquire structural paraphrases encoded as templates .",
    "these latter are the most closely related to the sentence - level paraphrases we desire , and so we focus in this section on template - induction approaches .",
    "extract inference rules , which are related to paraphrases ( for example , x wrote y implies x is the author of y ) , to improve question answering .",
    "they assume that _ paths _ in dependency trees that take similar arguments ( leaves ) are close in meaning .",
    "however , only two - argument templates are considered .",
    "also use dependency - tree information to extract templates of a limited form ( in their case , determined by the underlying information extraction application ) .",
    "like us ( and unlike lin and pantel , who employ a single large corpus ) , they use articles written about the same event in different newspapers as data .",
    "our approach shares two characteristics with the two methods just described : pattern comparison by analysis of the patterns respective arguments , and use of non - parallel corpora as a data source .",
    "however , _ extraction _ methods are not easily extended to _ generation _ methods .",
    "one problem is that their templates often only match small fragments of a sentence .",
    "while this is appropriate for other applications , deciding whether to use a given template to generate a paraphrase requires information about the surrounding context provided by the entire sentence .",
    "[ [ overview ] ] overview + + + + + + + +    we first sketch the algorithm s broad outlines .",
    "the subsequent subsections provide more detailed descriptions of the individual steps .",
    "the major goals of our algorithm are to learn :    * recurring patterns in the data , such as x ( injured / wounded ) y people , z seriously , where the capital letters represent variables ; * pairings between such patternsthat represent paraphrases , for example , between the patternx ( injured / wounded ) y people , z of them seriously and the patterny were ( wounded / hurt ) by x , among them z were in serious condition .",
    "figure  [ fig : arch ] illustrates the main stages of our approach . during training ,",
    "patterninduction is first applied independently to the two datasets making up a pair of comparable corpora .",
    "individual patternsare learned by applying _ multiple - sequence alignment _ to clustersof sentences describing approximately similar events ; these patternsare represented compactly by _ lattices _ ( see figure [ fig : lattice ] ) .",
    "we then check for latticesfrom the two different corpora that tend to take the same arguments ; these latticepairs are taken to be paraphrase patterns .",
    "once training is done , we can generate paraphrases as follows : given the sentence `` the surprisebombing injured twenty people , five of them seriously '' , we match it to the lattice x ( injured / wounded ) y people , z of them seriously which can be rewritten as y were ( wounded / hurt ) by x , among them z were in serious condition , and so by substituting arguments we can generate `` twenty were wounded by the surprisebombing , among them five were in serious condition '' or `` twenty were hurt by the surprisebombing , among them five were in serious condition '' .",
    "our first step is to cluster sentences into groups from which to learn useful patterns ; for the multiple - sequence techniques we will use , this means that the sentences within clustersshould describe similar events and have similar structure , as in the sentences of figure [ fig : cluster ] .",
    "this is accomplished by applying hierarchical complete - link clustering to the sentences using a similarity metric based on word n - gram overlap ( @xmath0 ) .",
    "the only subtlety is that we do not want mismatches on sentence details ( e.g. , the location of a raid ) causing sentences describing the same type of occurrence ( e.g. , a raid ) from being separated , as this might yield clusterstoo fragmented for effective learning to take place .",
    "( moreover , variability in the _ arguments _ of the sentences in a cluster is needed for our learning algorithm to succeed ; see below . )",
    "we therefore first replace all appearances of dates , numbers , and proper names with generic tokens .",
    "clusterswith fewer than ten sentences are discarded .      in order to learn patterns ,",
    "we first compute a _ multiple - sequence alignment _ ( msa ) of the sentences in a given cluster .",
    "pairwise msa takes two sentences and a scoring function giving the similarity between words ; it determines the highest - scoring way to perform insertions , deletions , and changes to transform one of the sentences into the other .",
    "pairwise msa can be extended efficiently to multiple sequences via the iterative pairwise alignment , a polynomial - time method commonly used in computational biology @xcite .",
    "the results can be represented in an intuitive form via a word _ lattice _ ( see figure [ fig : lattice ] ) , which compactly represents ( n - gram ) structural similarities between the cluster s sentences .    to transform latticesinto generation - suitable patternsrequires some understanding of the possible varieties of latticestructures .",
    "the most important part of the transformation is to determine which words are actually instances of arguments , and so should be replaced by _ slots _",
    "( representing variables ) .",
    "the key intuition is that because the sentences in the clusterrepresent the same _ type _ of event , such as a bombing , but generally refer to different _ instances _ of said event ( e.g. a bombing in jerusalem versus in gaza ) , areas of large variability in the latticeshould correspond to arguments .    to quantify this notion of variability ,",
    "we first formalize its opposite : commonality .",
    "we define _ backbone _ nodes as those shared by more than 50% of the cluster s sentences .",
    "the choice of 50% is not arbitrary  it can be proved using the pigeonhole principle that our strict - majority criterion imposes a unique linear ordering of the backbone nodes that respects the word ordering within the sentences , thus guaranteeing at least a degree of well - formedness and avoiding the problem of how to order backbone nodes occurring on parallel `` branches '' of the lattice .",
    "once we have identified the backbonenodes as points of strong commonality , the next step is to identify the regions of variability ( or , in latticeterms , many parallel disjoint paths ) between them as ( probably ) corresponding to the arguments of the propositions that the sentences represent .",
    "for example , in the top of figure [ fig : lattice ] , the words  southern city , `` settlement of name'',``coastal resort of name '' , etc . all correspond to the location of an event and could be replaced by a single slot .",
    "figure [ fig : lattice ] shows an example of a latticeand the derived slotted lattice ; we give the details of the slot - induction process in the appendix .",
    "now , if we were using a parallel corpus , we could employ sentence - alignment information to determine which lattices correspond to paraphrases . since we do not have this information , we essentially approximate the parallel - corpus situation by correlating information from descriptions of ( what we hope are ) the same event occurring in the two different corpora .",
    "our method works as follows .",
    "once latticesfor each corpus in our comparable - corpus pair are computed , we identify latticeparaphrase pairs , using the idea that paraphrases will tend to take the same values as arguments .",
    "more specifically , we take a pair of latticesfrom different corpora , look back at the sentence clusters from which the two lattices were derived , and compare the slot values of those cross - corpus sentence pairs that appear in articles written on the _ same day _ on the same topic ; we pair the latticesif the degree of matching is over a threshold tuned on held - out data .",
    "for example , suppose we have two ( linearized ) lattices bombed slot2 and slot3 was bombed by slot4 drawn from different corpora .",
    "if in the first lattice s sentence cluster we have the sentence `` the plane bombed the town '' , and in the second lattice s sentence cluster we have a sentence written on the same day reading `` the town was bombed by the plane '' , then the corresponding lattices may well be paraphrases , where slot1 is identified with slot4 and slot2 with slot3 .    to compare the set of argument values of two lattices ,",
    "we simply count their word overlap , giving double weight to proper names and numbers and discarding auxiliaries ( we purposely ignore order because paraphrases can consist of word re - orderings ) .",
    "given a sentence to paraphrase , we first need to identify which , if any , of our previously - computed sentence clustersthe new sentence belongs most strongly to .",
    "we do this by finding the best alignment of the sentence to the existing lattices .",
    "if a matching latticeis found , we choose one of its comparable - corpus paraphrase latticesto rewrite the sentence , substituting in the argument values of the original sentence .",
    "this yields as many paraphrases as there are lattice paths .",
    "all evaluations involved judgments by native speakers of english who were not familiar with the paraphrasing systems under consideration .",
    "we implemented our system on a pair of comparable corpora consisting of articles produced between september 2000 and august 2002 by the agence france - presse ( afp ) and reuters news agencies . given our interest in domain - dependent paraphrasing , we limited attention to 9 mb of articles , collected using a tdt - style document clustering system , concerning individual acts of violence in israel and army raids on the palestinian territories . from this data",
    "( after removing 120 articles as a held - out parameter - training set ) , we extracted 43 slotted latticesfrom the afp corpus and 32 slotted latticesfrom the reuters corpus , and found 25 cross - corpus matching pairs ; since latticescontain multiple paths , these yielded 6,534 template pairs .      before evaluating the quality of the rewritings produced by our templates and lattices , we first tested the quality of a random sample of just the template pairs . in our instructions to the judges , we defined two text units ( such as sentences or snippets ) to be paraphrases if one of them can generally be substituted for the other without great loss of information ( but not necessarily vice versa ) . given a pair of _ templates _ produced by a system , the judges marked them as paraphrases if for many instantiations of the templates variables , the resulting text units were paraphrases .",
    "( several labelled examples were provided to supply further guidance ) .",
    "to put the evaluation results into context , we wanted to compare against another system , but we are not aware of any previous work creating templates precisely for the task of generating paraphrases .",
    "instead , we made a good - faith effort to adapt the dirt system to the problem , selecting the 6,534 highest - scoring templates it produced when run on our datasets .",
    "( the system of was unsuitable for evaluation purposes because their paraphrase extraction component is too tightly coupled to the underlying information extraction system . )",
    "it is important to note some important caveats in making this comparison , the most prominent being that dirt was not designed with sentence - paraphrase generation in mind  its templates are much shorter than ours , which may have affected the evaluators judgments  and was originally implemented on much larger data sets.tide@xmath1 n : nn : n '' , which we transformed into `` y tide of x '' so that its output format would be the same as ours . ]",
    "the point of this evaluation is simply to determine whether another corpus - based paraphrase - focused approach could easily achieve the same performance level .    in brief",
    ", the dirt system works as follows .",
    "dependency trees are constructed from parsing a large corpus .",
    "leaf - to - leaf paths are extracted from these dependency trees , with the leaves serving as slots .",
    "then , pairs of paths in which the slots tend to be filled by similar values , where the similarity measure is based on the mutual information between the value and the slot , are deemed to be paraphrases .",
    "we randomly extracted 500 pairs from the two algorithms output sets .",
    "of these , 100 paraphrases ( 50 per system ) made up a `` common '' set evaluated by all four judges , allowing us to compute agreement rates ; in addition , each judge also evaluated another `` individual '' set , seen only by him- or herself , consisting of another 100 pairs ( 50 per system ) .",
    "the `` individual '' sets allowed us to broaden our sample s coverage of the corpus .",
    "the pairs were presented in random order , and the judges were not told which system produced a given pair .",
    "as figure  [ msa - dirt - accuracy ] shows , our system outperforms the dirt system , with a consistent performance gap for all the judges of about 38% , although the absolute scores vary ( for example , judge 4 seems lenient ) .",
    "the judges assessment of correctness was fairly constant between the full 100-instance set and just the 50-instance common set alone .    in terms of agreement , the kappa value ( measuring pairwise agreement discounting chance occurrences ) on the common set was 0.54 , which corresponds to moderate agreement  .",
    "multiway agreement is depicted in figure  [ msa - dirt - accuracy ]  there , we see that in 86 of 100 cases , at least three of the judges gave the same correctness assessment , and in 60 cases all four judges concurred .",
    "finally , we evaluated the quality of the paraphrase sentences generated by our system , thus ( indirectly ) testing all the system components : pattern selection , paraphrase acquisition , and generation .",
    "we are not aware of another system generating sentence - level paraphrases .",
    "therefore , we used as a baseline a simple paraphrasing system that just replaces words with one of their randomly - chosen wordnet synonyms ( using the most frequent sense of the word that wordnet listed synonyms for ) .",
    "the number of substitutions was set proportional to the number of words our method replaced in the same sentence .",
    "the point of this comparison is to check whether simple synonym substitution yields results comparable to those of our algorithm .    [ cols=\"<,<\",options=\"header \" , ]     for this experiment , we randomly selected 20 afp articles about violence in the middle east published later than the articles in our training corpus .",
    "out of 484 sentences in this set , our system was able to paraphrase 59 ( 12.2% ) .",
    "( we chose parameters that optimized precision rather than recall on our small held - out set . )",
    "we found that after proper name substitution , only seven sentences in the test set appeared in the training set , which implies that latticesboost the generalization power of our method significantly : from seven to 59 sentences .",
    "interestingly , the coverage of the system varied significantly with article length .",
    "for the eight articles of ten or fewer sentences , we paraphrased 60.8% of the sentences per article on average , but for longer articles only 9.3% of the sentences per article on average were paraphrased .",
    "our analysis revealed that long articles tend to include large portions that are unique to the article , such as personal stories of the event participants , which explains why our algorithm had a lower paraphrasing rate for such articles .",
    "all 118 instances ( 59 per system ) were presented in random order to two judges , who were asked to indicate whether the meaning had been preserved .",
    "of the paraphrases generated by our system , the two evaluators deemed 81.4% and 78% , respectively , to be valid , whereas for the baseline system , the correctness results were 69.5% and 66.1% , respectively .",
    "agreement according to the kappa statistic was 0.6 .",
    "note that judging full sentences is inherently easier than judging templates , because template comparison requires considering a variety of possible slot values , while sentences are self - contained units .",
    "figure [ fig : wordnet ] shows two example sentences , one where our msa - based paraphrase was deemed correct by both judges , and one where both judges deemed the msa - generated paraphrase incorrect .",
    "examination of the results indicates that the two systems make essentially orthogonal types of errors .",
    "the baseline system s relatively poor performance supports our claim that whole - sentence paraphrasing is a hard task even when accurate word - level paraphrases are given .",
    "we presented an approach for generating sentence level paraphrases , a task not addressed previously .",
    "our method learns structurally similar patterns of expression from data and identifies paraphrasing pairs among them using a comparable corpus .",
    "a flexible pattern - matching procedure allows us to paraphrase an unseen sentence by matching it to one of the induced patterns .",
    "our approach generates both lexical and structural paraphrases .",
    "another contribution is the induction of msa lattices from non - parallel data .",
    "lattices have proven advantageous in a number of nlp contexts , but were usually produced from arallel data , which may not be readily available for many applications .",
    "we showed that word lattices can be induced from a type of corpus that can be easily obtained for many domains , broadening the applicability of this useful representation .",
    "in this appendix , we describe how we insert slots into latticesto form slotted lattices .",
    "recall that the backbone nodes in our latticesrepresent words appearing in many of the sentences from which the lattice was built . as mentioned above",
    ", the intuition is that areas of high variability between backbone nodes may correspond to arguments , or slots .",
    "but the key thing to note is that there are actually two different phenomena giving rise to multiple parallel paths : _ argument variability _",
    ", described above , and _",
    "synonym variability_. for example , figure [ fig : variability](b ) contains parallel paths corresponding to the synonyms `` injured '' and `` wounded '' .",
    "note that we want to _ remove _ argument variability so that we can generate paraphrases of sentences with arbitrary arguments ; but we want to _ preserve _ synonym variability in order to generate a variety of sentence rewritings .    to distinguish these two situations , we analyze the _ split level _ of backbonenodes that begin regions with multiple paths .",
    "the basic intuition is that there is probably more variability associated with arguments than with synonymy : for example , as datasets increase , the number of locations mentioned rises faster than the number of synonyms appearing .",
    "we make use of a _ synonymy threshold _",
    "@xmath2 ( set by held - out parameter - tuning to 30 ) , as follows .",
    "* if no more than @xmath2% of all the edges out of a backbonenode lead to the same next node , we have high enough variability to warrant inserting a slot node .",
    "* otherwise , we incorporate reliable synonyms , identified only single - word synonyms , phrase - level synonyms can similarly be acquired by considering chains of nodes connecting backbone nodes . ] into the backbone structure by preserving all nodes that are reached by at least @xmath2% of the sentences passing through the two neighboring backbonenodes .",
    "furthermore , all backbonenodes labelled with our special generic tokens are also replaced with slotnodes , since they , too , probably represent arguments ( we condense adjacent slotsinto one ) .",
    "nodes with in - degree lower than the synonymy threshold are removed under the assumption that they probably represent idiosyncrasies of individual sentences .",
    "see figure [ fig : variability ] for examples ."
  ],
  "abstract_text": [
    "<S> we address the text - to - text generation problem of sentence - level paraphrasing  a phenomenon distinct from and more difficult than word- or phrase - level paraphrasing . </S>",
    "<S> our approach applies _ multiple - sequence alignment _ to sentences gathered from unannotated comparable corpora : it learns a set of paraphrasing patterns represented by _ word lattice _ </S>",
    "<S> pairs and automatically determines how to apply these patterns to rewrite new sentences . </S>",
    "<S> the results of our evaluation experiments show that the system derives accurate paraphrases , outperforming baseline systems . </S>"
  ]
}