{
  "article_text": [
    "we consider the gaussian regression framework @xmath4 where @xmath5 is an unknown vector of @xmath6 and the @xmath7 are independent centered gaussian random variables with common variance @xmath2 . throughout the paper",
    ", @xmath8 is assumed to be unknown which corresponds to the practical case .",
    "our aim is to estimate @xmath0 from the observation of @xmath1 . for specific forms of @xmath0 , this setting allows to deal simultaneously with the following problems .",
    "[ ex - sd ] the vector @xmath0 is of the form @xmath9 where @xmath10 are distinct points of a set @xmath11 and @xmath12 is an unknown mapping from @xmath11 into @xmath13 .",
    "[ ex - vs ] the vector @xmath0 is assumed to be of the form @xmath14 where @xmath15 is a @xmath16 matrix , @xmath17 is an unknown @xmath18-dimensional vector and @xmath18 some integer larger than 1 ( and possibly larger than @xmath19 ) .",
    "the columns of the matrix @xmath15 are usually called predictors .",
    "when @xmath18 is large , one may assume that the decomposition  ( [ sv ] ) is sparse in the sense that only few @xmath20 are non - zero .",
    "estimating @xmath0 or finding the predictors associated to the non - zero coordinates of @xmath17 are classical issues .",
    "the latter is called variable selection .",
    "our estimation strategy is based on estimator selection .",
    "more precisely , we start with an arbitrary collection @xmath21 of estimators of @xmath0 based on @xmath1 and aim at selecting the one with the smallest euclidean risk by using the same observation @xmath1 . the way the estimators @xmath22 depend on @xmath1 may be arbitrary and possibly unknown . for example",
    ", the @xmath22 may be obtained from the minimization of a criterion , a bayesian procedure or the guess of some experts .",
    "the problem of choosing some best estimator among a family of candidate ones is central in statistics .",
    "let us present some examples .",
    "[ ex - ctp ] many statistical procedures depend on a ( possibly multi - dimensional ) parameter @xmath23 that needs to be tuned in view of obtaining an estimator with the best possible performance .",
    "for example , in the context of linear regression as described in example  [ ex - vs ] , the lasso estimator ( see tibshirani  @xcite and chen _ et al . _",
    "@xcite ) defined by @xmath24 with @xmath25}\\ ] ] depends on the choice of the parameter @xmath26 .",
    "selecting this parameter among a grid @xmath27 amounts to selecting a ( suitable ) estimator among the family @xmath28 .",
    "another dilemma for statisticians is the choice of a procedure to solve a given problem . in the context of example  [ ex - ctp ] , there exist many competitors to the lasso estimator and one may alternatively choose a procedure based on ridge regression ( see hoerl and kennard  @xcite ) , random forest or pls ( see tenenhaus  @xcite , helland  @xcite and helland  @xcite ) .",
    "similarly , for the problem of signal denoising as described in example  [ ex - sd ] , popular approaches include spline smoothing , wavelet decompositions and kernel estimators",
    ". the choice of a kernel may be possibly tricky .",
    "consider the problem described in example  [ ex - sd ] with @xmath29 .",
    "for a kernel @xmath30 and a bandwidth @xmath31 , the nadaraya - watson estimator ( see nadaraya  @xcite and watson  @xcite ) @xmath32 is defined as @xmath33 where for @xmath34 @xmath35 there exist many possible choices for the kernel @xmath30 , such as the gaussian kernel @xmath36 , the uniform kernel @xmath37 , etc .",
    "given a ( finite ) family @xmath38 of candidate kernels @xmath30 and a grid @xmath39 of possible values of @xmath40 , one may consider the problem of selecting the best kernel estimator among the family @xmath41 .      a common way to address the above issues is to use some cross - validation scheme such as leave - one - out or @xmath42-fold .",
    "even though these resampling techniques are widely used in practice , little is known on their theoretical performances . for more details",
    ", we refer to arlot and celisse  @xcite for a survey on cross - validation technics applied to model selection .",
    "compared to these approaches , as we shall see , the procedure we propose is less time consuming and easier to implement .",
    "moreover , it does not require to know how the estimators depend on the data @xmath1 and we can therefore handle the following problem .",
    "a statistician is given a collection @xmath28 of estimators from a family @xmath43 of experts @xmath23 , each of which keeping secret the way his / her estimator @xmath22 depends on the observation @xmath1 .",
    "the problem is then to find which expert @xmath23 is the closest to the truth .",
    "given a selection rule among @xmath3 , an important issue is to compare the risk of the selected estimator to those of the candidate ones .",
    "results in this direction are available in the context of model selection , which can be seen as a particular case of estimator selection .",
    "more precisely , for the purpose of selecting a suitable model one starts with a collection @xmath44 of those , typically linear spaces chosen for their approximation properties with respect to @xmath0 , and one associates to each model @xmath45 a suitable estimator @xmath46 with values in @xmath47 . selecting a model",
    "then amounts to selecting an estimator among the collection @xmath48 . for this problem ,",
    "selection rules based on the minimization of a penalized criterion have been proposed in the regression setting by yang  @xcite , baraud  @xcite , birg and massart  @xcite and baraud _ et al _  @xcite .",
    "another way , usually called lepski s method , appears in a series of papers by lepski  @xcite and was originally designed to perform model selection among collections of nested models . finally , we mention that other procedures based on resampling have interestingly emerged from the work of arlot  @xcite and clisse  @xcite .",
    "a common feature of those approaches lies in the fact that the proposed selection rules apply to specific collections of estimators only .",
    "an alternative to _ estimator selection _ is _ aggregation _ which aims at designing a suitable combination of given estimators in order to outperform each of these separately ( and even the best combination of these ) up to a remaining term .",
    "aggregation techniques can be found in catoni  @xcite , juditsky and nemirovski  @xcite , nemirovski  @xcite , yang  @xcite ,  @xcite ,  @xcite , tsybakov  @xcite , wegkamp  @xcite , birg  @xcite , rigollet and tsybakov  @xcite , bunea , tsybakov and wegkamp  @xcite and goldenshluger  @xcite for @xmath49-losses .",
    "most of the aggregation procedures are based on a sample splitting , one part of the data being used for building the estimators , the remaining part for selecting among these .",
    "such a device requires that the observations be i.i.d . or at least that one has at disposal two independent copies of the data .",
    "from this point of view our procedure differs from classical _ aggregation _ procedures since we use the whole data @xmath1 to build and select . in the gaussian regression setting that is considered here , we mention the results of leung and barron  @xcite for the problem of mixing least - squares estimators .",
    "their procedure uses the same data @xmath1 to estimate and to aggregate but requires the variance to be known .",
    "giraud  @xcite extends their results to the case where it is unknown .",
    "our approach for solving the problem of estimator selection is new .",
    "we introduce a collection @xmath44 of linear subspaces of @xmath6 for approximating the estimators in @xmath3 and use a penalized criterion to compare them . as already mentioned and as we shall see , this approach requires no assumption on the family of estimators at hand and is easy to implement , an r - package being available on    .",
    "a general way of comparing estimators in various statistical settings has been described in baraud  @xcite .",
    "however , the procedure proposed there is mainly abstract and inadequate in the gaussian framework we consider .",
    "we prove a non - asymptotic risk bound for the estimator we select and show that this bound is optimal in the sense that it essentially can not be improved ( except for numerical constants maybe ) by any other selection rule .",
    "for the sakes of illustration and comparison , we apply our procedure to various problems among which aggregation , model selection , variable selection and selection among linear estimators . in each of these cases ,",
    "our approach allows to recover classical results in the areas as well as to establish new ones . in the context of aggregation",
    "we compute the aggregation rates for the unknown variance case .",
    "these rates turn out to be the same as those for the known variance case . for selecting an estimator among a family of linear ones",
    ", we propose a new procedure and establish a risk bound which requires almost no assumption on the considered family .",
    "finally , our approach provides a way of selecting a suitable variable selection procedure among a family of candidate ones .",
    "it thus provides an alternative to cross - validation for which little is known .",
    "the paper is organized as follows . in section  [",
    "sect - main ] we present our selection rule and the theoretical properties of the resulting estimator . for illustration , we show in sections  [ sect - aggreg ] ,  [ sect - linear ] and  [ sect - vs ] respectively , how the procedure can be used to aggregate preliminary estimators , select a linear estimator among a finite collection of candidate ones , or solve the problem of variable selection .",
    "section  [ sec : numerique ] is devoted to two simulation studies .",
    "one aims at comparing the performance of our procedure to the classical @xmath42-fold in view of selecting a tuning parameter among a grid . in the other",
    ", we evaluate the performance of the variable selection procedure we propose to some classical ones such as the lasso , random forest , and others based on ridge and pls regression .",
    "finally , the proofs are postponed to section  [ sect - proof ] .    throughout the paper @xmath50 denotes a constant that may vary from line to line .",
    "given a collection @xmath51 of estimators of @xmath0 based on @xmath1 , the selection rule we propose is based on the choices of a family @xmath44 of linear subspaces of @xmath6 , a collection @xmath52 of ( possibly random ) subsets of @xmath44 , a weight function @xmath53 and a penalty function @xmath54 , both from @xmath44 into @xmath55 .",
    "we introduce those objects below and refer to sections  [ sect - aggreg ] ,  [ sect - linear ] and  [ sect - vs ] for examples .",
    "the collection @xmath56 can be arbitrary .",
    "in particular , @xmath3 need not be finite nor countable and it may consist of a mix of estimators based on the minimization of a criterion , a bayes procedure or the guess of some experts . the dependency of these estimators with respect to @xmath1 need not be known .",
    "nevertheless , we shall see on examples how we can use this information , when available , to improve the performance of our estimation procedure .",
    "let @xmath44 be a family of linear spaces of @xmath6 satisfying the following .",
    "[ h0 ] the family @xmath44 is finite or countable and for all @xmath45 , @xmath58 .    to each estimator @xmath59",
    ", we associate a ( possibly random ) subset @xmath60 .",
    "typically , the family @xmath44 should be chosen to possess good approximation properties with respect to the elements of @xmath3 and @xmath57 with respect to @xmath22 specifically .",
    "one may take @xmath61 but for computational reasons it will be convenient to allow @xmath57 to be smaller .",
    "the choices of @xmath57 may be made on the basis of the observation @xmath22 .",
    "we provide examples of @xmath44 and @xmath57 in various statistical settings described in sections  [ sect - aggreg ] to  [ sect - vs ] .",
    "we consider a function @xmath53 from @xmath44 into @xmath55 and assume    [ h1 ] @xmath63    whenever @xmath44 is finite , inequality  ( [ sigma ] ) automatically holds true .",
    "however , in practice @xmath64 should be kept to a reasonable size .",
    "when @xmath65 , @xmath66 can be interpreted as a prior distribution on @xmath44 and gives thus a bayesian flavor to the procedure we propose . to the weight function @xmath53 ,",
    "we associate the function @xmath62 mapping @xmath44 into @xmath55 and defined by @xmath67}=e^{-\\delta(s ) } \\label{edk.eq}\\ ] ] where @xmath68 denotes the positive part of @xmath69 and @xmath70 are two independent @xmath71 random variables with respectively @xmath72 and @xmath73 degrees of freedom .",
    "this function can be easily computed from the quantiles of the fisher distribution as we shall see in section  [ calcpen.st ] . from a more theoretical point of view , it is shown in baraud _",
    "et al _  @xcite that under assumption  [ h4 ] below , there exists a positive constant @xmath50 ( depending on @xmath74 only ) such that @xmath75    [ h4 ] there exists @xmath76 such that for all @xmath77 , @xmath78      the selection procedure we propose involves a penalty function @xmath54 from @xmath44 into @xmath55 with the following property .",
    "[ h2 ] the penalty function @xmath54 satisfies for some @xmath79 , @xmath80    whenever equality holds in  ( [ pen ] ) , it derives from  ( [ eq : oom ] ) that @xmath81 measures the complexity of the model @xmath47 in terms of dimension and weight .    denoting @xmath82 the projection operator onto a linear space @xmath83 , given the families @xmath57 , the penalty function @xmath54 and some positive number @xmath84",
    ", we define @xmath85},\\ ] ] where @xmath86      for all @xmath87 let us set @xmath88}.\\ ] ] this quantity corresponds to an accuracy index for the estimator @xmath22 with respect to the family @xmath57 .",
    "the following result holds .",
    "[ main ] let @xmath89 .",
    "assume that assumptions  [ h0 ] ,  [ h1 ] and  [ h2 ] hold .",
    "there exists a constant @xmath50 ( given by  ( [ cste.eq ] ) ) depending on @xmath30 and @xmath84 only such that for any @xmath90 in @xmath3 satisfying @xmath91 we have the following bounds @xmath92 } & \\le & \\e{\\left[{\\inf_{\\lambda\\in\\lambda}{\\left[{{\\left\\|{f-\\widehat        f_{\\lambda}}\\right\\|}^{2}+a(\\widehat      f_{\\lambda},\\ss_{\\lambda})}\\right]}}\\right]}+\\sigma\\sigma^{2}+\\delta \\label{eqbase}\\\\   & \\le & \\inf_{\\lambda\\in\\lambda}{\\left\\{{\\e{\\left[{{\\left\\|{f-\\widehat         f_{\\lambda}}\\right\\|}^{2}}\\right]}+\\e{\\left[{a(\\widehat       f_{\\lambda},\\ss_{\\lambda})}\\right]}}\\right\\}}+\\sigma\\sigma^{2 } + \\delta \\;\\;\\label{eq3}\\end{aligned}\\ ] ] ( provided that the quantity involved in the expectation in  ( [ eqbase ] ) is measurable ) .",
    "furthermore , if equality holds in  ( [ pen ] ) and assumption  [ h4 ] is satisfied , for each @xmath87    * if the set @xmath57 is non - random ,    @xmath93}}\\nonumber\\\\ & \\le&\\!\\!\\!\\!\\e{\\left[{{\\left\\|{f-\\widehat f_{\\lambda}}\\right\\|}^{2}}\\right]}+\\inf_{s\\in\\ss_{\\lambda}}{\\left[{\\e{\\left[{{\\left\\|{\\widehat f_{\\lambda}-\\pi_{s}\\widehat f_{\\lambda}}\\right\\|}^{2}}\\right]}+(\\dim(s)\\vee \\delta(s))\\sigma^{2}}\\right ] } \\label{bornea1}\\end{aligned}\\ ] ]    * if there exists a ( possibly random ) linear space @xmath94 such that @xmath95 with probability 1 ,    @xmath96}\\le \\e{\\left[{{\\left\\|{f-\\widehat f_{\\lambda}}\\right\\|}^{2}}\\right]}+\\e{\\left[{\\dim(\\widehat s_{\\lambda})\\vee \\delta(\\widehat s_{\\lambda})}\\right]}\\sigma^{2},\\ ] ]    where @xmath97 is a positive constant only depending on @xmath74 and @xmath30 .",
    "let us now comment theorem  [ main ] .",
    "it turns out that inequality  ( [ eqbase ] ) leaves no place for a substantial improvement in the sense that the bound we get is essentially optimal and can not be improved ( apart from constants ) by any other selection rule among @xmath3 . to see this ,",
    "let us assume for simplicity that @xmath3 is finite so that a measurable minimizer of @xmath98 always exists and @xmath99 can be chosen as 0 .",
    "let @xmath100 , @xmath101 ( to fix up the ideas ) , @xmath44 a family of linear spaces satisfying the assumptions of theorem  [ main ] and @xmath54 , the penalty function achieving equality in  ( [ pen ] ) .",
    "besides , assume that @xmath44 contains a linear space @xmath47 such that @xmath102 and associate to @xmath47 the weight @xmath103 . if @xmath61 for all @xmath23 , we deduce from  ( [ eq : oom ] ) and  ( [ eqbase ] ) that for some universal constant @xmath97 , whatever @xmath3 and @xmath104 @xmath105}}\\nonumber\\\\ & \\le & \\e{\\left[{\\inf_{\\lambda\\in\\lambda}{\\left[{{\\left\\|{f-\\widehat f_{\\lambda}}\\right\\|}^{2}+\\inf_{s\\in\\ss}{\\left({{\\left\\|{\\widehat f_{\\lambda}-\\pi_{s}\\widehat f_{\\lambda}}\\right\\|}^{2}+{\\mathop{\\rm pen}\\nolimits}(s)\\widehat \\sigma^{2}_{s}}\\right)}}\\right]}}\\right]}\\nonumber\\\\ & \\le & \\e{\\left[{\\inf_{\\lambda\\in\\lambda}{\\left[{{\\left\\|{f-\\widehat f_{\\lambda}}\\right\\|}^{2}+{\\left\\|{\\widehat f_{\\lambda}-\\pi_{s}\\widehat f_{\\lambda}}\\right\\|}^{2}+\\dim(s)\\widehat \\sigma^{2}_{s}}\\right]}}\\right]}.\\label{1000borne}\\end{aligned}\\ ] ] in the opposite direction , the following result holds .",
    "[ prop - opt ] there exists a universal constant @xmath50 , such that for any finite family @xmath28 of estimators and any selection rule @xmath106 based on @xmath1 among @xmath43 , there exists @xmath107 such that @xmath108}\\ge \\e{\\left[{\\inf_{\\lambda\\in\\lambda}{\\left[{{\\left\\|{f-\\widehat f_{\\lambda}}\\right\\|}^{2}+{\\left\\|{\\widehat f_{\\lambda}-\\pi_{s}\\widehat f_{\\lambda}}\\right\\|}^{2}+\\dim(s)\\sigma^{2}}\\right]}}\\right]}.\\ ] ]    we see that , up to the estimator @xmath109 in place of @xmath2 and numerical constants , the left - hand sides of  ( [ 1000borne ] ) and  ( [ borne1000 ] ) coincide .    in view of commenting  ( [ eq3 ] )",
    "further , we continue assuming that @xmath3 is finite so that we can keep @xmath110 in  ( [ eq3 ] ) .",
    "a particular feature of  ( [ eq3 ] ) lies in the fact that the risk bound pays no price for considering a large collection @xmath3 of estimators .",
    "in fact , it is actually decreasing with respect to @xmath3 ( or equivalently @xmath43 ) for the inclusion .",
    "this means that if one adds a new estimator to the collection @xmath3 ( without changing neither @xmath44 nor the families @xmath57 associated to the former estimators ) , the risk bound for @xmath90 can only be improved .",
    "in contrast , the computation of the estimator @xmath90 is all the more difficult that @xmath111 is large .",
    "more precisely , if the cardinalities of the families @xmath57 are not too large , the computation of @xmath90 requires around @xmath111 steps .",
    "the selection rule we use does not require to know how the estimators depend on @xmath1 .",
    "in fact , as we shall see , a more important piece of information is the ranges of the estimators @xmath112 as @xmath1 varies in @xmath6 .",
    "a situation of special interest occurs when each @xmath22 belongs to some ( possibly random ) linear space @xmath113 in @xmath44 with probability one . by taking @xmath57 such that @xmath94 for all @xmath23",
    ", we deduce from theorem  [ main ] by using  ( [ eq3 ] ) and  ( [ bornea2 ] ) the following corollary .",
    "[ maincor ] assume that the assumptions of theorem  [ main ] are satisfied , that assumption  [ h4 ] holds and that equality holds in  ( [ pen ] ) . if for all @xmath87 there exists a ( possibly random ) linear space @xmath114 such that @xmath95 with probability 1 , then @xmath90 satisfies @xmath115}\\le \\inf_{\\lambda\\in\\lambda}{\\left[{\\e{\\left[{{\\left\\|{f-\\widehat f_{\\lambda}}\\right\\|}^{2}}\\right]}+\\e{\\left[{\\dim(\\widehat s_{\\lambda})\\vee \\delta(\\widehat s_{\\lambda})}\\right]}\\sigma^{2}}\\right]}+\\delta,\\ ] ] for some @xmath50 depending on @xmath30 and @xmath74 only .",
    "one may apply this result in the context of model selection .",
    "one starts with a collection of models @xmath116 and associate to each @xmath117 an estimator @xmath118 with values in @xmath117 . by taking @xmath119 ( here @xmath120 ) and @xmath121 for all @xmath122",
    ", our selection procedure leads to an estimator @xmath123 which satisfies @xmath124}\\le \\inf_{m\\in\\m}{\\left[{\\e{\\left[{{\\left\\|{f-\\widehat f_{m}}\\right\\|}^{2}}\\right]}+{\\left({\\dim(s_{m})\\vee \\delta(s_{m})}\\right)}\\sigma^{2}}\\right]}.\\ ] ] when @xmath125 for all @xmath122 , our selection rule becomes @xmath126}\\ ] ] and turns out to coincide with that described in baraud _",
    "interestingly , corollary  [ maincor ] shows that this selection rule can still be used for families @xmath3 of ( non - linear ) estimators of the form @xmath127 where the @xmath128 are chosen randomly among @xmath44 on the basis of @xmath1 , doing thus as if the linear spaces @xmath128 were non - random .",
    "an estimator of the form @xmath127 can be interpreted as resulting from a model selection procedures among the family of projection estimators @xmath129 and hence ,  ( [ critsm ] ) can be used to choose some best model selection rule among a collection of candidate ones .",
    "in this section , we consider the problems of _ model selection aggregation _ ( ms ) , _ convex aggregation _ ( @xmath130 ) and _ linear aggregation _ ( l ) defined below .",
    "given @xmath131 preliminary estimators of @xmath0 , denoted @xmath132 , our aim is to build an estimator @xmath133 based on @xmath1 whose risk is as close as possible to @xmath134 where @xmath135 and , according to the aggregation problem at hand , @xmath43 is one of the three sets @xmath136 when @xmath137 , @xmath138 is the set @xmath139 consisting of the initial estimators .",
    "when @xmath140 , @xmath138 is the convex hull of the @xmath141 . in the literature , one may also find @xmath142^{m},\\ \\sum_{j=1}^{m}\\lambda_{j}\\le 1}\\right\\}}\\ ] ] in place of @xmath143 in which case @xmath138 is the convex hull of @xmath144 .",
    "finally , when @xmath145 , @xmath138 is the linear span of the @xmath141 .",
    "each of these three aggregation problems are solved _ separately _ if for each @xmath146 one can design an estimator @xmath147 satisfying @xmath148}-c\\inf_{g\\in\\ff_{\\lambda}}{\\left\\|{f - g}\\right\\|}^{2}\\le c'\\psi_{n,\\lambda}\\sigma^{2}\\ ] ] with @xmath149 , @xmath150 free of @xmath151 and @xmath152 these problems have only been considered when the variance is known .",
    "the quantity @xmath153 then corresponds to the best possible upper bound in  ( [ aggreg1 ] ) over all possible @xmath104 and preliminary estimators @xmath141 and is called the _ optimal rate of aggregation_. for a more precise definition , we refer the reader to tsybakov  @xcite .",
    "@xcite considered the problem of solving these three problems _ simultaneously _ by building an estimator @xmath133 which satisfies  ( [ aggreg1 ] ) simultaneously for all @xmath146 and some constant @xmath154 .",
    "this is an interesting issue since it is impossible to know in practice which aggregation device should be used to achieve the smallest risk bound : as @xmath43 grows ( for the inclusion ) , the bias @xmath134 decreases while the rate @xmath153 increases .",
    "the aim of this section is to show that our procedure provides a way of solving ( or nearly solving ) the three aggregation problems both _ separately _ and _ simultaneously _ when the variance is unknown .    throughout this section",
    ", we consider the family @xmath155 consisting of the @xmath117 defined for each @xmath156 and @xmath157 as the linear span of the @xmath141 for @xmath158 . along this section",
    ", we shall use the weight function @xmath53 defined on @xmath155 by @xmath159},\\ ] ] take @xmath101 and @xmath160 taking thus @xmath100 .",
    "the choices of @xmath84 and @xmath30 is only to fix up the ideas .",
    "note that @xmath53 satisfies assumption  [ h1 ] with @xmath161 . to avoid trivialities , we assume all along @xmath162 .",
    "problem ( l ) is the easiest to solve .",
    "let us take @xmath163 with @xmath145 and @xmath164 and @xmath165 for all @xmath166 .",
    "minimizing @xmath167 over @xmath168 amounts to minimizing @xmath169 over @xmath170 and hence , the resulting estimator is merely @xmath171 .",
    "the risk of @xmath172 satisfies @xmath173}\\le \\inf_{g\\in \\ff_{\\lambda}}{\\left\\|{f - g}\\right\\|}^{2}+ m\\sigma^{2}.\\ ] ] whatever @xmath19 and @xmath174 which solves the problem of _ linear aggregation_.      to tackle problem ( ms ) , we take @xmath163 with @xmath137 , that is , @xmath175 , @xmath176 and associate to each @xmath177 the collection @xmath57 reduced to @xmath178 .",
    "note that @xmath179 and @xmath180 for all @xmath181 , so that under the assumption that @xmath182 we may apply corollary  [ maincor ] with @xmath110 ( since @xmath138 is finite ) , @xmath183 and get that for some constant @xmath184 the resulting estimator @xmath185 satisfies @xmath186}\\le \\inf_{g\\in \\ff_{\\lambda}}{\\left\\|{f - g}\\right\\|}^{2}+ \\log(m)\\sigma^{2}.\\ ] ] this risk bound is of the form  ( [ aggreg1 ] ) except for the constant @xmath50 which is not equal to 1 .",
    "we do not know whether problem ( ms ) can be solved or not with @xmath149 when the variance @xmath2 is unknown and @xmath174 is large ( possibly larger than @xmath19 ) .      for this problem",
    ", we emphasize the aggregation rate with respect to the quantity @xmath187 if @xmath188 , take again the estimator @xmath172 . since the convex hull of the @xmath141 is a subset of the linear space @xmath189 , for @xmath140 we have @xmath190}\\le \\inf_{g\\in \\ff_{\\lambda}}{\\left\\|{f - g}\\right\\|}^{2}+ m\\sigma^{2}.\\ ] ]",
    "let us now turn to the case @xmath191 .",
    "more precisely , assume that @xmath192 and set @xmath193 .",
    "we consider the family of estimators @xmath163 with @xmath140 and @xmath194 the set @xmath143 being compact , @xmath195 admits a minimum @xmath196 over @xmath143 and we set @xmath197 .",
    "[ prop - convex ] there exists a universal constant @xmath154 such that @xmath198}-c\\inf_{g\\in\\ff_{\\lambda}}{\\left\\|{f - g}\\right\\|}^{2}\\le c\\sqrt{nl^{2}\\log(em/\\sqrt{nl^{2}})}\\sigma^{2}.\\ ] ]    this risk bound is of the form  ( [ aggreg1 ] ) except for the constant @xmath50 which is not equal to 1 .",
    "again , we do not know whether problem ( @xmath130 ) can be solved or not with @xmath149 when the variance @xmath2 is unknown and @xmath174 possibly larger than @xmath19 .",
    "consider now three estimators @xmath199 with values respectively in @xmath189 , @xmath200 and the convex hull @xmath201 of the @xmath141 ( we use a new notation for this convex hull to avoid ambiguity ) .",
    "one may take the estimators defined in section  [ aggregm ] but any others would suit .",
    "the aim of this section is to select the one with the smallest risk to estimate @xmath0 .",
    "to do so , we apply our selection procedure with @xmath202 , taking thus @xmath203 , and associate to each of these three estimators the families @xmath204 defined by  ( [ mla ] ) ,  ( [ mmsa ] ) and  ( [ mca ] ) respectively and choose @xmath205 .    [ prop - simult ] assume that  ( [ hm ] ) holds and that @xmath182 .",
    "there exists a universal constant @xmath184 such that whatever @xmath206 and @xmath207 with values in @xmath189 , @xmath200 and @xmath201 respectively , the selected estimator @xmath90 satisfies for all @xmath104 , @xmath208}\\le \\inf_{\\lambda\\in { \\left\\{{{\\rm l},{\\rm ms},\\cv}\\right\\ } } } { \\left [ { \\e{\\left[{{\\left\\|{f-\\widehat f_{\\lambda}}\\right\\|}^{2}}\\right ] } + b_{\\lambda}}\\right]},\\ ] ] where @xmath209}.\\ ] ] in particular , if @xmath206 and @xmath207 fulfills  ( [ aggreg1 ] ) , then @xmath208}\\le \\inf_{\\lambda\\in { \\left\\{{{\\rm l},{\\rm ms},\\cv}\\right\\ } } } { \\left [ { \\inf_{g\\in\\ff_{\\lambda}}{\\left\\|{f - g}\\right\\|}^{2}+b_{\\lambda}}\\right]},\\ ] ] where @xmath210 stands for @xmath138 when @xmath211 .",
    "in this section , we consider the situation where the estimators @xmath22 are linear , that is , are of the form @xmath212 for some known and deterministic @xmath213 matrix @xmath214 . as mentioned before , this setting covers many popular estimation procedures including kernel ridge estimators , spline smoothing , nadaraya estimators , @xmath23-nearest neighbors , projection estimators , low - pass filters , etc . in some cases",
    "@xmath215 is symmetric ( e.g. kernel ridge , spline smoothing , projection estimators ) , in some others @xmath215 is non - symmetric and non - singular ( as for nadaraya estimators ) and sometimes @xmath215 can be both singular and non - symmetric ( low pass filters , @xmath23-nearest neighbors ) .",
    "a common feature of those procedures lies in the fact that they depend on a tuning parameter ( possibly multidimensional ) and their practical performances can be quite poor if this parameter is not suitably calibrated .",
    "a series of papers have investigated the calibration of some of these procedures . to mention a few of them ,",
    "cao and golubev  @xcite focus on spline smoothing , zhang  @xcite on kernel ridge regression , goldenshluger and lepski  @xcite on kernel estimators and arlot and bach  @xcite propose a procedure to select among symmetric linear estimator with spectrum in @xmath216 $ ] .",
    "the procedure we present can handle all these cases in an unified framework . throughout the section ,",
    "we assume that @xmath43 is finite .      to apply our selection procedure , we need to associate to each @xmath215 a suitable collection of approximation spaces @xmath57 . to do so",
    ", we introduce below a linear space @xmath217 which plays a key role in our analysis .    for the sake of simplicity , let us first consider the case where @xmath215 is non - singular .",
    "then @xmath217 is defined as the linear span of the right - singular vectors of @xmath218 associated to singular values smaller than 1 .",
    "when @xmath215 is symmetric , @xmath217 is merely the linear span of the eigenvectors of @xmath215 associated to eigenvalues not smaller than 1/2 . if none of the singular values are smaller than 1 , then @xmath219 .",
    "let us now extend the definition of @xmath217 to singular operators @xmath215 .",
    "let us recall that @xmath220 where @xmath221 stands for the transpose of @xmath215 and @xmath222 for its range .",
    "the operator @xmath215 then induces a one to one operator between @xmath223 and @xmath224 .",
    "write @xmath225 for the inverse of this operator from @xmath224 to @xmath222 .",
    "the orthogonal projection operator from @xmath6 onto @xmath222 induces a linear operator from @xmath224 into @xmath222 , denoted @xmath226 .",
    "then @xmath217 is defined as the linear span of the right - singular vectors of @xmath227 associated to singular values smaller than 1 . again",
    "if this set is empty , @xmath219 .",
    "when @xmath215 is non - singular or symmetric , we recover the definition of @xmath217 given above .    for each @xmath87 , take @xmath57 such that @xmath228 . from a theoretical point of view",
    ", it is enough to take @xmath229 but practically it may be wise to use a larger set and by doing so , to possibly improve the approximation of @xmath22 by elements of @xmath57 .",
    "one may for example take @xmath230 where @xmath231 is the linear span of the right - singular vectors associated to the @xmath232 smallest singular values of @xmath227 .",
    "take @xmath233 and @xmath53 of the form @xmath234 where @xmath235 satisfies assumption  [ h1 ] with @xmath236 .",
    "one may take @xmath237 even though this choice is not necessarily the best . finally , for some @xmath79 , take @xmath238 for all @xmath45 and select @xmath90 by minimizing the criterion given by  ( [ eq : criterion ] ) , taking thus @xmath110 in  ( [ def - est ] ) .",
    "the following holds .",
    "[ linear estimators ] let @xmath79 , @xmath239 and @xmath240 .",
    "if assumption  [ h0 ] holds and @xmath241 for all @xmath45 , the estimator @xmath90 satisfies @xmath242}\\le \\inf_{\\lambda}\\e{\\left[{{\\left\\|{f-\\widehat f_{\\lambda}}\\right\\|}^{2}}\\right]}+\\sigma^{2},\\ ] ] for some @xmath50 depending on @xmath243 and @xmath74 only .",
    "the problem of selecting some best linear estimator among a family of those have also been considered in arlot and bach  @xcite in the gaussian regression framework , and in goldenshluger and lepski  @xcite in the multidimensional gaussian white noise model .",
    "arlot and bach proposed a penalized procedure based on random penalties . unlike ours , their approach requires that the operators be symmetric with eigenvalues in @xmath216 $ ] and that the cardinality of @xmath43 is at most polynomial with respect to @xmath19 .",
    "goldenshluger and lepski proposed a selection rule among families of kernel estimators to solve the problem of structural adaptation .",
    "their approach requires suitable assumptions on the kernels while ours requires nothing .",
    "nevertheless , we restrict to the case of the euclidean loss whereas goldenshluger and lepski considered more general @xmath49 ones .",
    "throughout this section , we consider the problem of variable selection introduced in example  [ ex - vs ] and assume that @xmath244 in order to avoid trivialities .",
    "when @xmath18 is small enough ( say smaller than 20 ) , this problem can be solved by using a suitable variable selection procedure that explores all the subsets of @xmath245 .",
    "for example , one may use the penalized criterion introduced in birg and massart  @xcite when the variance is known , and the one in baraud _",
    "@xcite when it is not .",
    "when @xmath18 is larger , such an approach can no longer be applied since it becomes numerically intractable . to overcome this problem , algorithms based on the minimization of convex criteria",
    "have been proposed among which are the lasso , the dantzig selector of cands and tao  @xcite , the elastic net of zou and hastie  @xcite .",
    "an alternative to those criteria is the forward - backward algorithm described in zhang  @xcite , among others .",
    "since there seems to be no evidence that one of these procedures outperforms all the others , it may be reasonable to mix them all and let the data decide which is the more appropriate to solve the problem at hand . as enlarging @xmath3",
    "can only improve the risk bound of our estimator , only the cpu resources should limit the number of candidate estimators .",
    "the procedure we propose could not only be used to select among those candidate procedures but also to select the tuning parameters they depend on . from this point of view",
    ", it provides an alternative to the cross - validation techniques which are quite popular but offer little theoretical guarantees .",
    "start by choosing a family @xmath246 of variable selection procedures .",
    "examples of such procedures are the lasso , the dantzig selector , the elastic net , among others .",
    "if necessary , associate to each @xmath247 a family of tuning parameters @xmath248 .",
    "for example , in order to use the lasso procedure one needs to choose a tuning parameter @xmath31 among a grid @xmath249 .",
    "if a selection procedure @xmath250 requires no choice of tuning parameters , then one may take @xmath251 .",
    "let us denote by @xmath252 the subset of @xmath245 corresponding to the predictors selected by the procedure @xmath250 for the choice of the tuning parameter @xmath40 . for @xmath253",
    ", let @xmath117 be the linear span of the column vectors @xmath254 for @xmath158 ( with the convention @xmath255 ) . for @xmath256 and @xmath257 ,",
    "associate to the subset @xmath258 an estimator @xmath259 of @xmath0 with values in @xmath260 ( one may for example take the projection of @xmath1 onto the random linear space @xmath260 but any other choice would suit ) .",
    "finally , consider the family @xmath21 of these estimators by taking @xmath261 and set @xmath262 .",
    "all along we assume that @xmath43 is finite ( so that we take @xmath110 in  ( [ def - est ] ) ) .      throughout",
    ", we shall restrict ourselves to subsets of predictors with cardinality not larger than some @xmath263 . in view of approximating the estimators @xmath22",
    ", we suggest the collection @xmath44 given by @xmath264 we associate to @xmath44 the weight function @xmath53 defined for @xmath45 by @xmath265}+\\log(1+d)\\ \\ { \\rm with}\\ \\",
    "d=\\dim(s ) . \\label{delta.eq}\\ ] ] since @xmath266 assumption  [ h1 ] is satisfied with @xmath267 .",
    "let us now turn to the choices of the @xmath268 .",
    "the criterion given by  ( [ eq : criterion ] ) can not be computed when @xmath61 for all @xmath23 as soon as @xmath18 is too large .",
    "in such a case , one must consider a smaller subset of @xmath44 and we suggest for @xmath269 @xmath270 ( where the @xmath117 are defined above ) , or preferably @xmath271 whenever this latter family is not too large . note that these two families are random .",
    "our choices of @xmath53 and @xmath57 ensure that @xmath272 for all @xmath87 and that @xmath273 hence , by applying corollary  [ maincor ] with @xmath274 , we get the following result .",
    "[ cor - vs1 ] let @xmath79 , @xmath239 and @xmath275 be some positive integer satisfying @xmath276 .",
    "let @xmath277 be a ( finite ) collection of random subsets of @xmath245 with cardinality not larger than @xmath275 based on the observation @xmath1 and @xmath278 a family of estimators @xmath0 , also based on @xmath1 , such that @xmath279 . by applying our selection procedure",
    ", the resulting estimator @xmath90 satisfies @xmath208}\\le \\inf_{\\lambda\\in\\lambda}{\\left[{\\e{\\left[{{\\left\\|{f-\\widehat f_{\\lambda}}\\right\\|}^{2}}\\right]}+\\e{\\left[{\\dim(s_{\\widehat m(\\lambda)})}\\right]}\\log(p)\\sigma^{2}}\\right]},\\ ] ] where @xmath50 is a constant depending on the choices of @xmath30 and @xmath74 only .    again , note that the risk bound we get is non - increasing with respect to @xmath43 .",
    "this means that if one adds a new variable selection procedure or considers more tuning parameters to increase @xmath43 , the risk bound we get can only be improved .    without additional information on the estimators",
    "@xmath22 it is difficult to compare @xmath280}\\sigma^{2}$ ] and @xmath281}$ ] .",
    "if @xmath22 is of the form @xmath282 for some deterministic subset @xmath45 it is well - known that @xmath283}={\\left\\|{f-\\pi_{s}f}\\right\\|}^{2}+\\dim(s)\\sigma^{2}\\ge \\dim(s)\\sigma^{2}.\\ ] ] under the assumption that @xmath284 and that @xmath285 belongs to @xmath286 with probability close enough to 1 , we can compare the risk of the estimator @xmath90 to the cardinality of @xmath287 .",
    "[ cor - vs2 ] assume that the assumptions of corollary  [ cor - vs1 ] hold and that @xmath288 for all @xmath87 .",
    "if @xmath284 for some non - void subset @xmath289 with cardinality not larger than @xmath275 , then @xmath208}\\le \\log(p)|m^*|\\sigma^2+r_{n}(m^{*})\\ ] ] where @xmath50 is a constant depending on @xmath30 and @xmath74 only , and @xmath290}}\\right)}^{1/2}.\\ ] ]    @xcite gives sufficient conditions on the design @xmath15 to ensure that @xmath291}$ ] is exponentially small with respect to @xmath19 when the family @xmath286 is obtained by using the lars - lasso algorithm with different values of the tuning parameter .",
    "in the linear regression setting described in example  [ ex - vs ] , we carry out a simulation study to evaluate the performances of our procedure to solve the two following problems .",
    "we first consider the problem , described in example  [ ex - ctp ] , of tuning the smoothing parameter of the lasso procedure for estimating @xmath0 .",
    "the performances of our procedure are compared with those of the @xmath42-fold cross - validation method .",
    "secondly , we consider the problem of variable selection .",
    "we solve it by using our criterion in view of selecting among a family @xmath246 of candidate variable selection procedures .",
    "our simulation study is based on a large number of examples which have been chosen in view of covering a large variety of situations .",
    "most of these have been found in the literature in the context of example  [ ex - vs ] either for estimation or variable selection purposes when the number @xmath18 of predictors is large .",
    "the section is organized as follows .",
    "the simulation design is given in the following section .",
    "then , we describe how our procedure is applied for tuning the lasso and performing variable selection . finally , we give the results of the simulation study .",
    "one example is determined by the number of observations @xmath19 , the number of variables @xmath18 , the @xmath292 matrix @xmath15 , the values of the parameters @xmath17 , and the ratio signal / noise @xmath293 .",
    "it is denoted by @xmath294 , and the set of all considered examples is denoted @xmath295 . for each example , we carry out 400 simulations of @xmath1 as a gaussian random vector with expectation @xmath296 and variance @xmath297 , where @xmath298 is the @xmath213 identity matrix , and @xmath299",
    ".    the collection @xmath295 is composed of several collections @xmath300 for @xmath301 where each collection @xmath300 is characterized by a vector of parameters @xmath302 , and a set @xmath303 of matrices @xmath15 : @xmath304 where @xmath305 and @xmath306 consists of pairs @xmath307 such that @xmath18 is smaller , equal or greater than @xmath19 .",
    "the examples are described in further details in section  [ simex.st ] .",
    "they are inspired by examples found in tibshirani  @xcite , zou and hastie  @xcite , zou  @xcite , and huang et al .",
    "@xcite for comparing the lasso method to the ridge , adaptive lasso and elastic net methods .",
    "they make up a large variety of situations .",
    "they include cases where    * the covariates are not , moderately or strongly correlated , * the covariates with zero coefficients are weakly or highly correlated with covariates with non - zero coefficients , * the covariates with non - zero coefficients are grouped and correlated within these groups , * the lasso method is known to be inconsistent , * few or many effects are present .",
    "we consider here the problem of tuning the smoothing parameter of the lasso estimator as described in example  [ ex - ctp ] . instead of considering the lasso estimators for a fixed grid @xmath43 of smoothing parameters @xmath23 , we rather focus on the sequence @xmath308 of estimators given by the @xmath275 first steps of the lars - lasso algorithm proposed by efron _ et al . _",
    "hence , the tuning parameter is here the number @xmath309 of steps . in our simulation study",
    ", we compare the performance of our criterion to that of the @xmath42-fold cross - validation for the problem of selecting the best estimator among the collection @xmath310 .",
    "we recall that our selection procedure relies on the choices of families @xmath44 , @xmath311 for @xmath312 , a weight function @xmath53 , a penalty function @xmath54 and two universal constants @xmath79 and @xmath240 .",
    "we choose the family @xmath44 defined by  ( [ sm ] ) .",
    "we associate to @xmath313 the family @xmath314 where the @xmath315 are defined in section  [ sect - irm ] and @xmath316 is the set of indices corresponding to the predictors retuned by the lars - lasso algorithm at step @xmath317 .",
    "we take @xmath318 with @xmath319 defined by  ( [ delta.eq ] ) and @xmath100 .",
    "this value of @xmath30 is consistent with what is suggested in baraud _",
    "_  @xcite .",
    "the choice of @xmath84 is based on the following considerations .",
    "first , choosing @xmath84 around one seems reasonable since it weights similarly the term @xmath320 which measures how well the estimator fits the data and the approximation term @xmath321 involved in our criterion  ( [ eq : criterion ] ) .",
    "second , simple calculation shows that the constant @xmath322 involved in theorem  [ main ] is minimum for @xmath84 close to 0.6 .",
    "we therefore carried out our simulations for @xmath84 varying from 0.2 to 1.5 .",
    "the results being very similar for @xmath84 between 0.5 and 1.2 , we choose @xmath323 .",
    "we denote by @xmath324 the resulting estimator of @xmath0 .      for each @xmath325 ,",
    "the prediction error is estimated using a @xmath42-fold cross - validation procedure , with @xmath326 .",
    "the estimator @xmath327 is chosen by minimizing the estimated prediction error .",
    "the simulations were carried out with r ( www.r-project.org ) using the library elasticnet .",
    "for each example @xmath328 , we estimate on the basis of 400 simulations the oracle risk @xmath329 and the euclidean risks @xmath330 and @xmath331 of @xmath324 and @xmath327 respectively .",
    "the results presented in table  [ tb1 ] show that our procedure tends to choose a better estimator than the cv in the sense that the ratios @xmath332 are closer to one than @xmath333 .",
    ".[tb1 ] mean , standard - error and quantiles of the ratios @xmath334 calculated over all @xmath335 such that @xmath336 .",
    "the number of such examples equals 654 , see section  [ simex.st ] .",
    "[ cols= \" > , > , > , > , > , > , > , > \" , ]",
    "throughout this section , we use the following notations . for all @xmath87 and @xmath337 , we write @xmath338 where @xmath339 for",
    "all @xmath87 , let @xmath340 be such that @xmath341 we also write @xmath342 and @xmath343 for the linear space generated by @xmath47 and @xmath0 .",
    "it follows the facts that for all @xmath87 and @xmath337 @xmath344 and simple algebra that @xmath345 for @xmath346 and @xmath45 , let us set @xmath347 if @xmath348 and @xmath349 otherwise . for all @xmath23 and @xmath47",
    ", we have @xmath350 and @xmath351 hence , by using  ( [ pen ] ) and  ( [ penn ] ) we get @xmath352 where @xmath353 for each @xmath45 , @xmath354 and since the variable @xmath355 is independent of @xmath356 and is stochastically larger than @xmath357 , we deduce from the definition of @xmath358 and  ( [ sigma ] ) , that on the one hand @xmath359 .    on the other hand , since @xmath47 is arbitrary among @xmath57 and since @xmath360 we deduce from  ( [ eq1 ] ) that for all @xmath87 , @xmath361}\\ ] ] with @xmath362 and  ( [ eq3 ] ) follows by taking the expectation on both sides of  ( [ eq10 ] ) .",
    "note that provided that @xmath363}\\ ] ] is measurable , we have actually proved the stronger inequality @xmath364}\\le \\e{\\left[{\\inf_{\\lambda\\in\\lambda}{\\left\\{{{\\left\\|{f-\\widehat f_{\\lambda}}\\right\\|}^{2}+a(\\widehat f_{\\lambda},\\ss_{\\lambda})}\\right\\}}}\\right]}+\\sigma^{2}\\sigma + \\delta.\\ ] ] let us now turn to the second part of the theorem , fixing some @xmath87 .",
    "since equality holds in  ( [ pen ] ) , under assumption  [ h4 ] by  ( [ eq : oom ] ) @xmath365 if @xmath57 is non - random , for some @xmath366 and all @xmath337 , @xmath93}}\\\\ & \\le&\\e{\\left[{{\\left\\|{\\widehat f_{\\lambda}-\\pi_{s}\\widehat f_{\\lambda}}\\right\\|}^{2}}\\right]}+(\\dim(s)\\vee \\delta(s))\\e{\\left[{\\widehat\\sigma_{s}^{2}}\\right]},\\\\ & = & \\e{\\left[{{\\left\\|{\\widehat f_{\\lambda}-\\pi_{s}\\widehat",
    "f_{\\lambda}}\\right\\|}^{2}}\\right]}+{\\dim(s)\\vee \\delta(s)\\over n-\\dim(s)}{\\left[{{\\left\\|{f-\\pi_{s}f}\\right\\|}^{2}+(n-\\dim(s))\\sigma^{2}}\\right]}.\\end{aligned}\\ ] ] since @xmath367 , we have @xmath368}\\le 2\\e{\\left[{{\\left\\|{f-\\widehat f_{\\lambda}}\\right\\|}^{2}}\\right]}+2\\e{\\left[{{\\left\\|{\\widehat f_{\\lambda}-\\pi_{s}\\widehat f_{\\lambda}}\\right\\|}^{2}}\\right]},\\ ] ] and under assumption  [ h4 ] , @xmath369 , and hence for all @xmath337 @xmath370}&\\le & { \\left({1+{2\\kappa\\over 1-\\kappa}}\\right)}\\e{\\left[{{\\left\\|{f-\\widehat f_{\\lambda}}\\right\\|}^{2}}\\right]}+{2\\kappa\\over 1-\\kappa}\\e{\\left[{{\\left\\|{\\widehat f_{\\lambda}-\\pi_{s}\\widehat f_{\\lambda}}\\right\\|}^{2}}\\right]}\\\\ & & \\ \\ + \\ \\ { \\left({\\dim(s)\\vee \\delta(s)}\\right)}\\sigma^{2}.\\end{aligned}\\ ] ] which leads to  ( [ bornea1 ] ) .",
    "let us turn to the proof of  ( [ bornea2 ] ) .",
    "we set @xmath371 . since with probability one @xmath372 , @xmath373}\\le \\e{\\left[{{\\mathop{\\rm pen}\\nolimits}(\\widehat s_{\\lambda})\\widehat \\sigma_{\\lambda}^{2}}\\right]}\\ ] ] and it suffices thus to bound the right - hand side . since equality holds in  ( [ pen ] ) and",
    "since @xmath95 @xmath374}\\\\ & \\le & 2k{\\ed(\\widehat s_{\\lambda})\\over n-\\dim(\\widehat s_{\\lambda})}{\\left[{{\\left\\|{f-\\widehat f_{\\lambda}}\\right\\|}^{2}+({\\left\\|{\\eps}\\right\\|}^{2}-2n\\sigma^{2})_{+}+2n\\sigma^{2}}\\right]}.\\end{aligned}\\ ] ] under assumption  [ h4 ] , @xmath375 and we deduce from  ( [ eq : oom ] ) that for some constant @xmath50 depending only on @xmath30 and @xmath74 @xmath376 and the result follows from the fact that @xmath377\\le 3\\sigma^{2}$ ] for all @xmath19 .      for all @xmath87 and @xmath378 , @xmath379 and hence , @xmath380}.\\ ] ] besides , since the minimax rate of estimation over @xmath47 is of order @xmath381 , for some universal constant @xmath50 , @xmath382}\\ge \\dim(s)\\sigma^{2}.\\ ] ] putting these bounds together lead to the result .      under  ( [ hm ] )",
    ", it is not difficult to see that @xmath383 so that @xmath44 is not empty and since for all @xmath384 @xmath385}\\le |m|(1+\\log m)\\le { n\\over 2},\\ ] ] assumptions  [ h0 ] to  [ h2 ] are satisfied with @xmath183 .",
    "besides , the set @xmath143 being compact , @xmath195 admits a minimum over @xmath143 ( we shall come back the minimization of this criterion at the end of the subsection ) and hence we can take @xmath110 . by applying theorem  [ main ] and using  ( [ bornea1 ] )",
    ", the resulting estimator @xmath197 satisfies for some universal constant @xmath184 @xmath386}\\le \\inf_{g\\in\\ff_{\\lambda}}{\\left\\{{{\\left\\|{f - g}\\right\\|}^{2}+\\overline a(g,\\ss)}\\right\\}},\\ ] ] where @xmath387}.\\ ] ] we bound @xmath388 from above by using the following approximation result below the proof of which can be found in makovoz  @xcite ( more precisely , we refer to the proof of his theorem  2 ) .    for all @xmath389 in the convex hull @xmath138 of the @xmath141 and all @xmath390",
    ", there exists @xmath156 such that @xmath391 and @xmath392    by using this lemma and the fact that @xmath393 for all @xmath394 , we get @xmath395}\\sigma^{2}.\\ ] ] taking for @xmath396 the integer part of @xmath397 which belongs to @xmath398 $ ] under  ( [ hm ] ) , we get @xmath399 for some universal constant @xmath150 which together with  ( [ conv1 ] ) leads to the risk bound @xmath198}-c\\inf_{g\\in\\ff_{\\lambda}}{\\left\\|{f - g}\\right\\|}^{2}\\le c\\sqrt{nl^{2}\\log(em/\\sqrt{nl^{2}})}\\sigma^{2}.\\ ] ]    concerning the computation of @xmath400 , note that @xmath401}\\\\ & = & \\inf_{s\\in\\ss_{\\cv}}{\\left\\{{{\\left[{\\inf_{\\lambda\\in\\lambda}{\\left ( { { \\left\\|{y-\\pi_{s}f_{\\lambda}}\\right\\|}^{2}+\\alpha{\\left\\|{f_{\\lambda}-\\pi_{s } f_{\\lambda}}\\right\\|}^{2}}\\right)}}\\right]}+{\\mathop{\\rm pen}\\nolimits}(s)\\,\\widehat\\sigma^2_{s}}\\right\\}},\\end{aligned}\\ ] ] and hence , one can solve the problem of minimizing @xmath167 over @xmath87 by proceeding into two steps .",
    "first , for each @xmath47 in the finite set @xmath402 minimize the convex criterion @xmath403 over the convex ( and compact set ) @xmath143 . denote by @xmath404 the resulting minimizers . then , minimize the quantity @xmath405 for @xmath47 varying among @xmath402 .",
    "denoting by @xmath406 such a minimizer , we have that @xmath407 .      by applying theorem",
    "[ main ] , we obtain that the selected estimator @xmath90 satisfies @xmath208}\\le \\inf_{\\lambda\\in { \\left\\{{{\\rm l},{\\rm ms},\\cv}\\right\\ } } } { \\left[{\\e{\\left[{{\\left\\|{f-\\widehat f_{\\lambda}}\\right\\|}^{2}}\\right]}+\\e{\\left[{a(\\widehat f_{\\lambda},\\ss_{\\lambda})}\\right]}}\\right]}.\\ ] ] let us now bound @xmath408}$ ] for each @xmath87 .    if @xmath409 , by using  ( [ bornea1 ] ) and the fact that @xmath410 , we have @xmath411}\\le \\e{\\left[{{\\left\\|{f-\\widehat f_{{\\rm l}}}\\right\\|}^{2}}\\right]}+m\\sigma^{2}.\\ ] ]",
    "if @xmath412 , we may use  ( [ bornea2 ] ) since with probability one @xmath413 and since @xmath414 for all @xmath181 , we get @xmath415}\\le \\e{\\left[{{\\left\\|{f-\\widehat f_{{\\rm ms}}}\\right\\|}^{2}}\\right]}+\\log(m)\\sigma^{2}.\\ ] ] finally , let us turn to the case @xmath416 and denote by @xmath389 the best approximation of @xmath0 in @xmath201 . since @xmath417 , for all @xmath418 , @xmath419 and hence by using  ( [ bornea1 ] ) @xmath420}\\le \\e{\\left[{{\\left\\|{f-\\widehat f_{\\cv}}\\right\\|}^{2}}\\right]}+\\overline a(g,\\ss_{\\cv})\\ ] ] where @xmath421 is given by  ( [ def - abar ] ) . by arguing as in section  ( [ sect - ca ] )",
    ", we deduce that under  ( [ hm ] ) @xmath420}\\le \\e{\\left[{{\\left\\|{f-\\widehat     f_{\\cv}}\\right\\|}^{2}}\\right]}+\\sqrt{nl^{2}\\log(em/\\sqrt{nl^{2}})}\\sigma^{2}.\\ ] ] by putting these bounds together we get the result .",
    "since assumptions  [ h0 ] to  [ h2 ] are fulfilled and @xmath3 is finite , we may apply theorem  [ main ] and take @xmath110 . by using  ( [ bornea1 ] ) , we have for some @xmath50 depending on @xmath243 and @xmath74 , @xmath422}}\\\\ & \\leq & \\inf_{\\lambda\\in\\lambda } { \\left\\{{\\e{\\left[{{\\left\\|{f-\\widehat f_{\\lambda}}\\right\\|}^2}\\right]}+\\e{\\left[{{\\left\\|{\\widehat f_{\\lambda}-\\pi_{s_{\\lambda}}\\widehat f_{\\lambda}}\\right\\|}^2}\\right]}+a(1+\\dim(s_{\\lambda}))\\sigma^2}\\right\\}}.\\end{aligned}\\ ] ] for all @xmath87 , @xmath423}&=&{\\left\\|{f - a_{\\lambda}f}\\right\\|}^2+\\e{\\left[{{\\left\\|{a_{\\lambda}\\eps}\\right\\|}^2}\\right]}\\\\ & = & { \\left\\|{f - a_{\\lambda}f}\\right\\|}^2+{\\rm tr}(a_{\\lambda}^*a_{\\lambda})\\sigma^2\\\\ & \\ge & \\max{\\left\\{{{\\left\\|{f - a_{\\lambda}f}\\right\\|}^2,{\\rm tr}(a_{\\lambda}^*a_{\\lambda})\\sigma^2}\\right\\}}\\end{aligned}\\ ] ] and @xmath424}&=&{\\left\\|{(i-\\pi_{s_{\\lambda}})a_{\\lambda}f}\\right\\|}^2+\\e{\\left[{{\\left\\|{(i-\\pi_{s_{\\lambda}})a_{\\lambda}\\eps}\\right\\|}^2}\\right]},\\\\ & \\le & 2\\max{\\left\\{{{\\left\\|{(i-\\pi_{s_{\\lambda}})a_{\\lambda}f}\\right\\|}^2,\\e{\\left[{{\\left\\|{a_{\\lambda}\\eps}\\right\\|}^2}\\right]}}\\right\\}}\\\\ & = & 2\\max{\\left\\{{{\\left\\|{(i-\\pi_{s_{\\lambda}})a_{\\lambda}f}\\right\\|}^2,{\\rm tr}(a_{\\lambda}^{*}a_{\\lambda})\\sigma^{2}}\\right\\}}\\end{aligned}\\ ] ] and hence , corollary  [ linear estimators ] follows from the next lemma .",
    "[ lemme lineaire ] for all @xmath87 we have @xmath425    writing @xmath426 and using the fact that @xmath427 and the definition of @xmath226 , we obtain @xmath428 where @xmath429 are the singular values of @xmath227 counted with their multiplicity and @xmath430 is an orthonormal family of right - singular vectors associated to @xmath431 .",
    "if @xmath432 , then @xmath433 and we have @xmath434",
    ". otherwise , @xmath435 , we may consider @xmath436 as the largest @xmath232 such that @xmath437 and derive that @xmath438 which proves the assertion  @xmath439 .",
    "for the bound  @xmath440 , we set @xmath441 and note that @xmath442 induces a semi - positive quadratic form on @xmath222 . as a consequence the quadratic form @xmath443 is dominated by the quadratic form @xmath444 on @xmath222 .",
    "furthermore @xmath445 where @xmath446 is the inverse of the linear operator @xmath447 induced by @xmath448 restricted on @xmath223 .",
    "we then have that the quadratic form induced by @xmath446 is dominated by the quadratic form @xmath449 on @xmath222 .",
    "in particular the sequence of the eigenvalues of @xmath446 is dominated by the sequence @xmath450 so @xmath451 which conclude the proof of lemma  [ lemme lineaire ] .      along the section , we write @xmath452 for @xmath453 and @xmath113 for @xmath454 for short . by using  ( [ eqbase ] ) with @xmath110 and since @xmath455 , we have @xmath456}\\le \\e{\\left[{\\inf_{\\lambda\\in\\lambda}\\|f-\\pi_{\\widehat s_{\\lambda}}y\\|^2+{\\mathop{\\rm pen}\\nolimits}(\\widehat s_{\\lambda})\\widehat \\sigma^2_{\\widehat s_{\\lambda}}}\\right]}+(1+\\log(p+1))\\sigma^2,\\ ] ] for some constant @xmath184 depending on @xmath30 only . writing @xmath457 for the event @xmath458",
    ", we have @xmath459}\\le a_{n}+r'_{n}\\ ] ] where @xmath460}\\\\ r'_{n}&= & \\e{\\left[{\\inf_{\\lambda\\in\\lambda}{\\left\\{{\\|f-\\pi_{\\widehat s_{\\lambda}}y\\|^2+{\\mathop{\\rm pen}\\nolimits}(\\widehat s_{\\lambda})\\widehat \\sigma^2_{\\widehat s_{\\lambda}}}\\right\\}}{\\bf 1}_{b}}\\right]}.\\end{aligned}\\ ] ]    let us bound @xmath461 from above .",
    "note that @xmath462 and @xmath463 and since @xmath464 , by using  ( [ eq : oom ] ) we get @xmath465 for some constant @xmath150 depending on @xmath30 and @xmath74 only .",
    "let us now turn to @xmath466 . for all @xmath87 , @xmath467 and @xmath468 since for all @xmath45 , @xmath469 , by using  ( [ eq : oom ] )",
    "again , there exists some positive constant @xmath470 depending on @xmath30 and @xmath74 only such that for all @xmath87 , @xmath471 and hence , @xmath472 some calculation shows that @xmath473}\\le{\\left({{\\left\\|{f}\\right\\|}^{2}+2n\\sigma^{2}}\\right)}^{2}$ ] and hence , by cauchy - schwarz inequality @xmath474 the result follows by putting the bounds on @xmath461 and @xmath466 together .",
    "arlot , s. ( 2007 ) . .",
    "phd thesis , university paris xi .",
    "arlot , s. ( 2009 ) .",
    "model selection by resampling penalization .",
    ", 3:557624 .",
    "arlot , s. and bach , f. ( 2009 ) .",
    "data - driven calibration of linear estimators with minimal penalties . , 22:4654 .",
    "arlot , s. and celisse , a. ( 2010 ) . a survey of cross - validation procedures for model selection .",
    ", 4:4079 .",
    "baraud , y. ( 2000 ) .",
    "model selection for regression on a fixed design .",
    ", 117(4):467493 .",
    "baraud , y. ( 2010 ) .",
    "estimator selection with respect to hellinger - type risks . .",
    "baraud , y. , giraud , c. , and huet , s. ( 2009 ) .",
    "gaussian model selection with an unknown variance .",
    ", 37(2):630672 .",
    "birg , l. ( 2006 ) .",
    "model selection via testing : an alternative to ( penalized ) maximum likelihood estimators .",
    ", 42(3):273325 .",
    "birg , l. and massart , p. ( 2001 ) .",
    "gaussian model selection .",
    ", 3(3):203268 .",
    "boulesteix , a. and strimmer , k. ( 2006 ) . .",
    ", 8(1):3244 .",
    "breiman , l. ( 2001 ) .",
    "random forests . , 45:532 .",
    "bunea , f. , tsybakov , a.  b. , and wegkamp , m.  h. ( 2007 ) .",
    "aggregation for gaussian regression .",
    ", 35(4):16741697 .",
    "cands , e. and tao , t. ( 2007 ) .",
    "the dantzig selector : statistical estimation when @xmath18 is much larger than @xmath19 .",
    ", 35(6):23132351 .",
    "cao , y. and golubev , y. ( 2006 ) . on oracle inequalities related to smoothing splines .",
    ", 15(4):398414 ( 2007 ) .",
    "catoni , o. ( 1997 ) . mixture approach to universal model selection . technical report , ecole normale suprieure , france .",
    "catoni , o. ( 2004 ) . statistical learning theory and stochastic optimization . in _",
    "lecture notes from the 31st summer school on probability theory held in saint - flour , july 825 , 2001_. springer - verlag , berlin .",
    "celisse , a. ( 2008 ) . .",
    "phd thesis , university paris xi .",
    "chen , s.  s. , donoho , d.  l. , and saunders , m.  a. ( 1998 ) .",
    "atomic decomposition by basis pursuit .",
    ", 20(1):3361 ( electronic ) .",
    "daz - uriarte , r. and alvares  de andrs , s. ( 2006 ) . .",
    ", 7(3 ) .",
    "efron , b. , hastie , t. , johnstone , i. , and tibshirani , r. ( 2004 ) .",
    "least angle regression .",
    ", 32(2):407499 . with discussion , and a rejoinder by the authors .",
    "genuer , r. , poggi , j .-",
    "m . , and tuleau - malot , c. ( 2010 ) .",
    "variable selection using random forests .",
    ", to appear .",
    "giraud , c. ( 2008 ) .",
    "mixing least - squares estimators when the variance is unknown .",
    ", 14(4):10891107 .",
    "goldenshluger , a. ( 2009 ) . a universal procedure for aggregating estimators .",
    ", 37(1):542568 .",
    "goldenshluger , a. and lepski , o. ( 2009 ) .",
    "structural adaptation via @xmath475-norm oracle inequalities .",
    ", 143(1 - 2):4171 .",
    "helland , i. ( 2006 ) .",
    "partial least squares regression . in kotz ,",
    "s. , balakrishnan , n. , read , c. , vidakovic , b. , and johnston , n. , editors , _ encyclopedia of statistical sciences ( 2nd ed . )",
    "_ , volume  9 , pages 59575962 , new york .",
    "helland , i. ( 2001 ) .",
    "some theoretical aspects of partial least squares regression . , 58:97107 .",
    "hoerl , a. and kennard , r. ( 1970 ) .",
    "ridge regression : bayes estimation for nonorthogonal problems .",
    ", 12:5567 .",
    "hoerl , a. and kennard , r. ( 2006 ) . ridge regression . in kotz , s. , balakrishnan , n. ,",
    "read , c. , vidakovic , b. , and johnston , n. , editors , _ encyclopedia of statistical sciences ( 2nd ed . )",
    "_ , volume  11 , pages 72737280 , new york .",
    "huang , j. , ma , s. , and zhang , c .- h .",
    "( 2008 ) . , 4(1603 - 1618 ) .",
    "juditsky , a. and nemirovski , a. ( 2000 ) .",
    "functional aggregation for nonparametric regression .",
    ", 28(3):681712 .",
    "lepski , o.  v. ( 1990 ) . a problem of adaptive estimation in gaussian white noise .",
    ", 35(3):459470 .",
    "lepski , o.  v. ( 1991 ) .",
    "asymptotically minimax adaptive estimation . i.",
    "upper bounds . optimally adaptive estimates . , 36(4):645659 .",
    "lepski , o.  v. ( 1992a ) .",
    "asymptotically minimax adaptive estimation .",
    "schemes without optimal adaptation . adaptive estimates . , 37(3):468481 .",
    "lepski , o.  v. ( 1992b ) . on problems of adaptive estimation in white gaussian noise . in _ topics in nonparametric",
    "estimation _ , volume  12 of _ adv .",
    "soviet math .",
    "_ , pages 87106 .",
    "soc . , providence , ri .",
    "leung , g. and barron , a.  r. ( 2006 ) .",
    "information theory and mixing least - squares regressions .",
    ", 52(8):33963410 .",
    "makovoz , y. ( 1996 ) .",
    "random approximants and neural networks . , 85(1):98109 .",
    "nadaraya , e.  a. ( 1964 ) . on estimating regression .",
    ", 9(1):141142 .",
    "nemirovski , a. ( 2000 ) .",
    "topics in non - parametric statistics . in _",
    "lectures on probability theory and statistics ( saint - flour , 1998 ) _ , volume 1738 of _ lecture notes in math .",
    "_ , pages 85277 .",
    "springer , berlin .",
    "rigollet , p. and tsybakov , a.  b. ( 2007 ) .",
    "linear and convex aggregation of density estimators .",
    ", 16(3):260280 .",
    "strobl , c. , boulesteix , a .-",
    "l . , kneib , t. , augustin , t. , and zeileis , a. ( 2008 ) .",
    "conditional variable importance for random forests . , 9(307 ) .",
    "strobl , c. , boulesteix , a .-",
    ", zeileis , a. , and hothorn , t. ( 2007 ) .",
    "bias in random forest variable importance measures : illustrations , sources and a solution . , 8(25 ) .",
    "tenenhaus , m. ( 1998 ) . .",
    "ditions technip , paris .",
    "thorie et pratique .",
    "[ theory and application ] .",
    "tibshirani , r. ( 1996 ) .",
    "regression shrinkage and selection via the lasso .",
    ", 58(1):267288 .",
    "tsybakov , a.  b. ( 2003 ) .",
    "optimal rates of aggregation . in _ proceedings of the 16th annual conference on learning theory ( colt ) and 7th annual workshop on kernel machines _ , pages 303313 .",
    "lecture notes in artificial intelligence 2777 , springer - verlag , berlin .",
    "watson , g.  s. ( 1964 ) .",
    "smooth regression analysis .",
    ", 26:359372 .",
    "wegkamp , m. ( 2003 ) .",
    "model selection in nonparametric regression .",
    ", 31:252273 .",
    "yang , y. ( 1999 ) .",
    "model selection for nonparametric regression .",
    ", 9:475499 .",
    "yang , y. ( 2000a ) . combining different procedures for adaptive regression .",
    ", 74(1):135161 .",
    "yang , y. ( 2000b ) . mixing strategies for density estimation . , 28(1):7587 .",
    "yang , y. ( 2001 ) . adaptive regression by mixing .",
    ", 96(454):574588 .",
    "zhang , t. ( 2005 ) .",
    "learning bounds for kernel regression using effective data dimensionality .",
    ", 17(9):20772098 .",
    "zhang , t. ( 2008 ) .",
    "adaptive forward - backward greedy algorithm for learning sparse representations .",
    "technical report , rutgers university , nj .    zhao , p. and yu , b. ( 2006 ) . on model selection consistency of lasso .",
    "7(nov):25412563 .",
    "zou , h. ( 2006 ) .",
    "the adaptive lasso and its oracle properties . , 101(476):14181429 .",
    "zou , h. and hastie , t. ( 2005 ) .",
    "regularization and variable selection via the elastic net . , 67(2):301320 .",
    "the penalty @xmath476 , defined at equation  ( [ edk.eq ] ) , is linked to the edkhi function introduced in baraud _ al _  @xcite ( see definition  3 ) , via the following formula : @xmath477 therefore , according to the result given in section  6.1 in baraud _",
    "@xcite , @xmath476 is the solution in @xmath478 of the equation @xmath479      the collection @xmath295 is composed of several collections @xmath480 that are detailed below .",
    "the collections @xmath481 to @xmath482 are composed of examples where @xmath15 is generated as @xmath19 independent centered gaussian vectors with covariance matrix @xmath50 . for each @xmath483 , we define a @xmath484 matrix @xmath485 and a @xmath18-vector of parameters @xmath302 .",
    "we denote by @xmath303 the set of 5 matrices @xmath15 simulated as @xmath19-i.i.d @xmath486 .",
    "the collection @xmath300 is then defined as follows : @xmath487 where @xmath305 and @xmath488 in section  [ cvedkhi.st ] , and @xmath489 in section  [ selvar.st ] .",
    "the matrix @xmath50 equals the @xmath484 identity matrix denoted @xmath490 .",
    "the parameters @xmath17 satisfy @xmath491 for @xmath492 , @xmath493 for @xmath494 , @xmath495 for @xmath496 , @xmath497 for @xmath498 .",
    "the matrix @xmath50 satisfies @xmath513 for @xmath514 , @xmath515 for @xmath516 , @xmath517 for @xmath518 , with @xmath519 and @xmath520 .",
    "the parameters @xmath17 satisfy @xmath491 for @xmath521 , @xmath522 for @xmath523 .                  in this last example , we denote by @xmath545 the set of 5 matrices @xmath15 simulated as follows . for @xmath546 , we denote by @xmath547 the column @xmath548 of @xmath15 .",
    "let @xmath549 be generated as @xmath19 i.i.d .",
    "@xmath550 and let @xmath551 be generated as @xmath19 i.i.d . @xmath552 .",
    "then for @xmath553 , @xmath554 , for @xmath555 , @xmath556 , for @xmath557 , @xmath558 , for @xmath492 , @xmath559 .",
    "the parameters @xmath17 are as in collection @xmath510 . the collection @xmath544 is defined as the set of examples @xmath560 for @xmath561 , @xmath562 , and @xmath563 .    the collection @xmath295 is thus composed of 660 examples for @xmath306 chosen as in  ( [ i3.eq ] ) , and 825 for @xmath306 chosen as in  ( [ i2.eq ] ) . for some of the examples ,",
    "the lasso estimators were highly biased leading to high values of the ratio @xmath564 , see equation  ( [ oracl.eq ] ) .",
    "we only keep the examples for which the lasso estimator improves the risk of the naive estimator @xmath1 by a factor at least @xmath565 .",
    "this convention leads us to remove 171 examples over 825 .",
    "these pathological examples are coming from the collections @xmath481 , @xmath510 and @xmath512 for @xmath566 and @xmath567 , and from collections @xmath499 and @xmath507 when @xmath568 .",
    "the examples of collection @xmath512 were chosen by zou to illustrate that the lasso estimators may be highly biased .",
    "all the other examples , correspond to matrices @xmath15 that are nearly orthogonal .        _ the lasso procedure _ is described in section  [ cvedkhi.st ] .",
    "the collection @xmath571 where @xmath572 is the set of indices corresponding to the predictors returned by the lars - lasso algorithm at step @xmath573 ( see section  [ cvedkhi.st ] ) .    _",
    "the ridge procedure _ is based on the minimization of @xmath574 with respect to @xmath17 , for some positive @xmath40 , see for example hoerl and kennard  @xcite .",
    "tibshirani  @xcite noted that in the case of a large number of small effects , ridge regression gives better results than the lasso for variable selection . for each @xmath575 ,",
    "the regression coefficients @xmath576 are calculated and a collection of predictors sets is built as follows .",
    "let @xmath577 be such that @xmath578 then , the collection @xmath579 is defined as @xmath580 .",
    "_ the elastic net procedure _ proposed by zou and hastie  @xcite mixes the @xmath581 and @xmath582 penalties of the lasso and the ridge procedures .",
    "let @xmath583 be a grid of values for the tuning parameter @xmath40 of the @xmath582 penalty .",
    "we choose @xmath584 where @xmath585 denotes the collection of the active sets of cardinality less than @xmath275 , selected by the elastic net procedure when the @xmath582-smoothing parameter equals @xmath40 . for each @xmath575",
    "the collection @xmath585 can be conveniently computed by first calculating the ridge regression coefficients and then applying the lars - lasso algorithm , see zou and hastie  @xcite .    _ the partial least squares regression _",
    "( plsr1 ) aims to reduce the dimensionality of the regression problem by calculating a small number of components that are usefull for predicting @xmath1 .",
    "several applications of this procedure for analysing high - dimensional genomic data have been reviewed by boulesteix and strimmer  @xcite .",
    "in particular , it can be used for calculating subsets of covariates as we did for the ridge procedure .",
    "the plsr1 procedure constructs , for a given @xmath40 , uncorrelated latent components @xmath586 that are highly correlated with the response @xmath1 , see helland  @xcite .",
    "let @xmath587 be a grid a values for the tuning parameter @xmath40 .",
    "for each @xmath588 , we write @xmath589 for the pls regression coefficients calculated with the first @xmath40 components .",
    "we then set @xmath590 , where @xmath591 is build from @xmath589 as for the ridge procedure .    _",
    "the adaptive lasso procedure _ proposed by zou  @xcite starts with a preliminary estimator @xmath592 .",
    "then one applies the lasso procedure replacing the parameters @xmath593 in the @xmath581 penalty by the weighted parameters @xmath594 for some positive @xmath595 .",
    "the idea is to increase the penalty for coefficients that are close to zero , reducing thus the bias in the estimation of @xmath0 and improving the variable selection accuracy .",
    "zou showed that , if @xmath592 is a @xmath596-consistent estimator of @xmath17 , then the adaptive lasso procedure is consistent in situations where the lasso is not .",
    "a lot of work has been done around this subject , see huang et al .",
    "@xcite for example .      - using the ridge estimator , @xmath598 as preliminary estimator . for each @xmath599",
    ", the adaptive lasso procedure is applied for calculating the active sets , @xmath600 , of cardinality less than @xmath275 .",
    "the collection @xmath601 is thus defined as @xmath602 .      _",
    "the random forest algorithm _ was proposed by breiman  @xcite for classification and regression problems .",
    "the procedure averages several regression trees calculated on bootstrap samples .",
    "the algorithm returns measures of variable importance that may be used for variable selection , see for example daz - uriarte and alvares de andrs  @xcite , genuer et al .",
    "@xcite , strobl et al .",
    "@xcite .",
    "let us denote by @xmath40 the number of variables randomly chosen at each split when constructing the trees and @xmath605 for each @xmath606 , we consider the set of indices @xmath607 where @xmath608 are the ranks of the variable importance measures .",
    "two importance measures are proposed . the first one is based on the decrease in the mean square error of prediction after permutation of each of the variables .",
    "it leads to the collection @xmath609 .",
    "the second one is based on the decrease in node impurities , and leads similarly to the collection @xmath610 .",
    "we have to choose @xmath275 , the largest number of predictors considered in the collection @xmath613 . for all methods , except the exhaustive method",
    ", @xmath275 may be large , say @xmath614 . nevertheless , for saving computing time , we chose @xmath275 large enough such that the dimension of the estimated subset is always smaller than @xmath275 .",
    "for the exhaustive method , @xmath275 must be chosen in order to make the calculation feasible : @xmath615 for @xmath616 , @xmath617 for @xmath618 and @xmath619 for @xmath620 ."
  ],
  "abstract_text": [
    "<S> we consider the problem of estimating the mean @xmath0 of a gaussian vector @xmath1 with independent components of common unknown variance @xmath2 . </S>",
    "<S> our estimation procedure is based on estimator selection . more precisely , we start with an arbitrary and possibly infinite collection @xmath3 of estimators of @xmath0 based on @xmath1 and , with the same data @xmath1 , aim at selecting an estimator among @xmath3 with the smallest euclidean risk . </S>",
    "<S> no assumptions on the estimators are made and their dependencies with respect to @xmath1 may be unknown . </S>",
    "<S> we establish a non - asymptotic risk bound for the selected estimator . </S>",
    "<S> as particular cases , our approach allows to handle the problems of aggregation and model selection as well as those of choosing a window and a kernel for estimating a regression function , or tuning the parameter involved in a penalized criterion . </S>",
    "<S> we also derive oracle - type inequalities when @xmath3 consists of linear estimators . for illustration </S>",
    "<S> , we carry out two simulation studies . </S>",
    "<S> one aims at comparing our procedure to cross - validation for choosing a tuning parameter . </S>",
    "<S> the other shows how to implement our approach to solve the problem of variable selection in practice .    , </S>"
  ]
}