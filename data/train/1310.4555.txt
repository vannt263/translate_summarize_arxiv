{
  "article_text": [
    "kinetic transport for a gas or plasma involves particle interactions such as collisions , excitation / deexcitation and ionization / recombination .",
    "simulation of these interactions is most often performed using the direct simulation monte carlo ( dsmc ) method @xcite or one of its variants , in which the actual particle distribution is represented by a relatively small number of numerical particles , each of which is characterized by state variables , such as position @xmath0 and energy @xmath1 .",
    "interactions between the numerical particles are performed by random selection of the interacting particles and the interaction parameters , depending on the interaction rates . correctly sampling these interactions",
    "involves several computational challenges : first the number @xmath2 of particles can be large ( e.g. , @xmath3 ) and the number of possible interaction events can be even larger ( e.g. , @xmath4 for @xmath5 or @xmath6 ) .",
    "second , the interaction probabilities vary throughout the simulation since interactions change the state of the interacting particles .",
    "these two difficulties are routinely overcome using acceptance - rejection sampling .",
    "third , the interaction rates can be nearly singular , for example in a recombination event between an ion and two electrons ( described in more detail in section [ section : example2 ] ) .",
    "this creates a wide range of interaction rates that makes acceptance - rejection computationally intractable .",
    "figure [ figure : challenge ] illustrates these challenges and how different methods can handle them .",
    "the sampling method presented here , which we call reduced rejection , was developed to overcome the challenges of a large number of interaction events with fluctuating and singular rates .",
    "[ ! htp ]     simulation of kinetics requires sampling methods that generate independent samples .",
    "this rules out markov chain monte carlo schemes , such as metropolis ",
    "hastings , gibbs sampling , and slice sampling .",
    "although these methods are very powerful and are used very often , this paper focuses on sampling methods that generate independent samples .",
    "there are several efficient algorithms for simulation of discrete random variables , notably marsaglia s table method @xcite and the alias method @xcite .",
    "however , these methods require pre - processing time and , therefore , are not efficient for sampling from a random variable whose probability function changes during the simulation . for continuous random variables",
    "there are several different algorithms ; nevertheless , each of these algorithms has its own constraints .",
    "for example , inverse transform sampling method requires knowledge of the cumulative distribution function and evaluation of its inverse , box - muller only applies to a normal distribution , and ziggurat algorithm @xcite can be used for random variables that have monotone decreasing ( or symmetric unimodal ) density function .    the algorithm of choice for general ( both continuous and discrete ) random variables that generates independent samples and does not require preprocessing time is acceptance - rejection method ( see for example @xcite ) .",
    "let @xmath7 be a real - valued function on the sample space .",
    "let @xmath8 $ ] denote the expectation of function @xmath7 . by sampling according to function @xmath7 we mean to sample using the probability distribution function @xmath9 $ ] .",
    "we say function @xmath7 _ encloses _ function @xmath10 if @xmath11 for all @xmath0 in the sample space . the idea of acceptance - rejection method is to find a proposal function @xmath7 that encloses function @xmath10 .",
    "suppose we already have a mechanism to sample according to @xmath7 , then acceptance - rejection algorithm enables us to sample according to @xmath10 .",
    "in most cases the constant function is used as the proposal function @xmath7 .",
    "the main drawback of acceptance - rejection method is that it might reject many samples .",
    "indeed the ratio of the number of rejected samples to the number of accepted samples is approximately equal to the ratio of the area between curves @xmath7 and @xmath10 to the area under the curve @xmath10 .    for many given distributions , finding a good proposal function that encloses it without leading to many rejected samples",
    "is difficult .",
    "one extension to acceptance - rejection method is adaptive rejection sampling @xcite .",
    "the basic idea of adaptive rejection sampling is to construct proposal function @xmath7 that encloses the given distribution by concatenating segments of one or more exponential distributions .",
    "as the algorithm proceeds , it successively updates the proposal function @xmath7 to correspond more closely to the given distribution .",
    "another extension to acceptance - rejection method is economical method @xcite .",
    "this method is basically a generalization of alias method for continuous distributions . in this method",
    ", one needs to define a specific transformation that maps @xmath12 to @xmath13 .",
    "although this method produces no rejection , finding the required transformation is difficult in general .",
    "in the reduced rejection method we sample according to a given function @xmath10 based on a proposal function @xmath7 .",
    "in contrast to the acceptance - rejection method , reduced rejection sampling does not require @xmath7 to enclose @xmath10 ( i.e. it allows @xmath14 for some @xmath0 ) .",
    "on the other hand , reduced rejection sampling requires some extra knowledge about the functions @xmath10 and @xmath7 .",
    "the reduced rejection sampling method can be applied to a wide range of sampling problems ( for both continuous and discrete random variables ) and in many examples is more efficient than customary methods ( three examples are provided in sections [ section : example1 ] , [ section : example2 ] and [ section : example3 ] ) . in particular ,",
    "reduced rejection sampling requires no pre - processing time and consequently is suitable for simulations in which @xmath10 is changing constantly ( see section [ section : comparison ] for an elaboration on this point and sections [ section : example2 ] and [ section : example3 ] for examples of simulations with fluctuating @xmath10 ) .",
    "also in situations where @xmath10 has singularities or is highly peaked in certain regions , reduced rejection sampling can be very efficient .",
    "the next section describes the reduced rejection sampling and proves its validity .",
    "section [ section : comparison ] compares reduced rejection sampling to other methods ( including other generalizations of acceptance - rejection ) , highlights advantages of reduced rejection sampling in comparison to other methods , and points out some of the challenges in applying reduced rejection sampling . in section [ section : example1 ] , reduced rejection sampling is demonstrated on a simple example . in section [ section : example2 ] , reduced rejection sampling is applied to an example motivated from plasma physics , for which other sampling methods can not be used efficiently . in section [ section :",
    "example3 ] , we make some comments on how to apply reduced rejection in the context of stochastic chemical kinetics . in the appendix",
    ", we provide flow charts for the reduced rejection algorithm .",
    "consider a sample space @xmath15 with lebesgue measure @xmath16 on @xmath15 , and two functions @xmath17 .",
    "denote @xmath18=\\int_\\omega q(x)d\\mu(x ) , \\qquad i[p]=\\int_\\omega p(x)d\\mu(s).\\ ] ] by sampling from @xmath15 according to @xmath10 we mean sampling from @xmath15 using probability distribution function @xmath19 $ ] .",
    "partition sample space @xmath15 into two sets @xmath20 and @xmath21 : @xmath22 reduced rejection sampling is a method for sampling from @xmath15 according to @xmath10 using an auxiliary function @xmath7 .",
    "it depends on the following :    * the values of @xmath8 $ ] , @xmath23 $ ] and @xmath24 .",
    "note that the last value is needed only for `` algorithm ii '' , see subsection [ subsection : algorithm ] . * a mechanism to sample from @xmath15 according to @xmath7 . * a mechanism to sample from @xmath25 according to @xmath26 .    whereas the acceptance - rejection method for sampling from @xmath10 requires a function @xmath7 that encloses @xmath10 ( i.e. , @xmath27 for all @xmath28 ) ,",
    "the reduced rejection sampling algorithm is a generalization of the acceptance - rejection method , that allows @xmath14 for some @xmath0 .",
    "the reduced rejection sampling algorithm is detailed in section [ subsection : algorithm ] , and its validity as a method for sampling from @xmath15 according to @xmath10 is demonstrated in section [ subsection : proof ] .",
    "the reduced rejection sampling method consists of two algorithms ( i.e. , two different algorithms ) depending on the relative values of @xmath23 $ ] and @xmath8 $ ] .",
    "the outcome of each algorithm is a value @xmath29 that is an independent sample from @xmath15 according to @xmath10 .",
    "* algorithm i : * @xmath30 \\geq i[q]}$ ] .    perform the following steps :    * with probability @xmath31-i[q])/i[p]$ ] , sample @xmath32 from @xmath25 according to @xmath26 and accept @xmath33 .",
    "* otherwise ( with probability @xmath34/i[p]$ ] ) , sample @xmath32 from @xmath15 according to @xmath7 . 1 .",
    "if @xmath35 , accept @xmath33 .",
    "2 .   if @xmath36 , accept @xmath33 with probability @xmath37 .",
    "* if @xmath32 was not accepted , then sample a new value of @xmath38 from @xmath25 according to @xmath26 and accept @xmath39 .",
    "* algorithm ii : * @xmath30 < i[q]}$ ] .    perform the following steps until a value @xmath29 is accepted :    * sample @xmath32 from @xmath15 according to @xmath7 . * if @xmath35 , accept @xmath33 . * if @xmath36 , accept @xmath33 with probability @xmath40 , * if @xmath32 was not accepted , then 1 .   with probability",
    "@xmath41 select @xmath38 from @xmath25 according to @xmath26 and accept @xmath39 , in which @xmath42-i[p]+\\int_{\\mathcal{l}}(p(x)-q(x))d\\mu(x)}.\\label{equation : algorithmii } \\end{aligned}\\ ] ] 2 .   otherwise ( i.e. , with probability @xmath43 )",
    ", return to ( i ) without accepting a value of @xmath29 .",
    "figures [ figure : algorithmi ] and [ figure : algorithmii ] in appendix [ section : appendix ] , illustrate flow charts of algorithms i and ii .    as described in algorithms i and",
    "ii , reduced rejection samples from @xmath44 through the following steps : on @xmath25 , treat @xmath44 as a mixture @xmath45 and sample from @xmath46 and @xmath47 with the correct probabilities ; and on @xmath48 , sample from @xmath44 by sampling from @xmath46 and accepting the sample with probability @xmath49 .",
    "rejected samples in @xmath48 correspond to the region @xmath50 in figure [ figure : rrsampling ] , and the region @xmath51 is where @xmath46 does not enclose @xmath44 .",
    "if @xmath52 ( i.e. , algorithm i ) then all of the rejected samples can be replaced by samples from @xmath51 ; if @xmath53 ( i.e. , algorithm ii ) then a portion of the rejected samples can be replaced by samples from @xmath51 , and for the remainder , the algorithm is repeated as in acceptance - rejection .    [ !",
    "htp ]   is where @xmath46 does not enclose @xmath44 , and region @xmath50 is where samples are rejected .",
    "rejected samples from region @xmath50 can be replaced by samples from region @xmath51 if @xmath52 ; otherwise ( if @xmath53 ) , some of the rejected samples lead to repetition of the algorithm.,title=\"fig:\",width=453 ]      in this subsection we show the correctness of the reduced rejection sampling method .",
    "as the method is different for algorithms i and ii , we prove the correctness for each algorithm separately .",
    "* proof for algorithm i : * for each @xmath54 , show that the algorithm of algorithm i returns @xmath29 with probability @xmath55 $ ] .",
    "if @xmath56 , then part ( ii ) must have been selected , @xmath29 must have been sampled in ( ii ) and it must have been accepted in case ( ii.b ) .",
    "therefore , the probability of returning @xmath29 is @xmath57 \\",
    "\\pr[z\\hbox { sampled in ( ii ) } ] \\",
    "\\pr[z \\hbox { accepted in ( ii.b ) } ]   & = & \\frac{i[q]}{i[p]}\\times\\frac{q(z)d\\mu(z)}{i[q]}\\times\\frac{p(z)}{q(z)}\\nonumber \\\\ & = & \\frac{p(z)d\\mu(z)}{i[p]}.\\label{equation : ismall}\\end{aligned}\\ ] ] also note that for every @xmath36 , after @xmath32 is selected in ( ii.b ) with probability @xmath58}d\\mu(x)$ ] , the probability of reaching ( iii ) is @xmath59 .",
    "thus the total probability of reaching ( iii ) after selecting ( ii ) is @xmath60= \\int_{\\mathcal{s}}\\frac{q(x)-p(x)}{q(x)}\\frac{q(x)}{i[q]}d\\mu(x)= \\frac{\\int_{\\mathcal{s}}(q(x)-p(x))d\\mu(x)}{i[q]}.\\label{equation : isampleb}\\ ] ]    next suppose that @xmath61 .",
    "the probability that @xmath29 is returned from ( i ) is @xmath62 = & \\pr[\\hbox{(i ) selected } ] \\",
    "\\pr[z \\hbox { sampled in ( i ) } ] \\notag \\\\ = & \\frac{(i[p]-i[q])}{i[p ] } \\times \\frac{(p(z)-q(z))d\\mu(z)}{\\int_{\\mathcal{l}}(p(x)-q(x))d\\mu(x)}. \\label{equation : i0}\\end{aligned}\\ ] ] the probability that @xmath29 was returned from ( ii.a ) is @xmath63 = & \\pr[\\hbox{(ii ) selected } ] \\",
    "\\pr[z \\hbox { sampled in ( ii.a)}]\\notag \\\\ = & \\frac{i[q]}{i[p]}\\times\\frac{q(z)d\\mu(z)}{i[q]}\\notag \\\\ = & \\frac{q(z)d\\mu(z)}{i[p]}. \\label{equation : i1}\\end{aligned}\\ ] ] also , using equation , the probability that @xmath29 was returned from ( iii ) is @xmath64 \\notag \\\\ = & \\pr[\\hbox{(ii ) selected } ] \\ \\pr[\\hbox{reaching   ( iii ) $ \\vert$   ( ii ) selected } ] \\",
    "\\pr[z \\hbox { sampled from $ \\mathcal{l}$ in ( iii)}]\\nonumber \\\\ = & \\frac{i[q]}{i[p]}\\times\\left(\\frac{\\int_{\\mathcal{s}}(q(x)-p(x))d\\mu(x)}{i[q]}\\right)\\times\\frac{(p(z)-q(z))d\\mu(z)}{\\int_{\\mathcal{l}}(p(x)-q(x))d\\mu(x ) } \\notag\\\\   = & \\frac{\\int_{\\mathcal{s}}(q(x)-p(x))d\\mu(x)}{i[p]}\\frac{(p(z)-q(z))d\\mu(z)}{\\int_{\\mathcal{l}}(p(x)-q(x))d\\mu(x)}. \\label{equation : i2}\\end{aligned}\\ ] ] finally , using equations , , and , the probability of returning @xmath29 is @xmath65+\\pr[z \\hbox { returned from ( ii.a)}]+\\pr[z \\hbox { returned from ( iii ) } ] \\notag\\\\ = & \\frac{(i[p]-i[q])}{i[p]}\\frac{(p(z)-q(z))d\\mu(z)}{\\int_{\\mathcal{l}}(p(x)-q(x))d\\mu(x ) } + \\frac{q(z)d\\mu(z)}{i[p ] } + \\frac{(p(z)-q(z))d\\mu(z)}{\\int_{\\mathcal{l}}(p(x)-q(x))d\\mu(x)}\\frac{\\int_{\\mathcal{s}}(q(x)-p(x))d\\mu(x)}{i[p ] } \\notag\\\\ = & \\frac{(p(z)-q(z))d\\mu(z)}{i[p]\\int_{\\mathcal{l}}(p(x)-q(x))d\\mu(x)}\\left(i[p]-i[q]+\\int_{\\mathcal{s}}(q(x)-p(x))d\\mu(x)\\right)+\\frac{q(z)d\\mu(z)}{i[p ] } \\notag\\\\ = & \\frac{(p(z)-q(z))d\\mu(z)}{i[p]\\int_{\\mathcal{l}}(p(x)-q(x))d\\mu(x)}\\left(\\int_{\\mathcal{l}}(p(x)-q(x))d\\mu(x)\\right)+\\frac{q(z)d\\mu(z)}{i[p ] } \\notag\\\\ = &   \\frac{p(z)d\\mu(z)}{i[p]}. \\label{equation : ilarge}\\end{aligned}\\ ] ]    hence , by and , whether @xmath56 or @xmath61 , the probability of returning @xmath29 is equal to @xmath55 $ ] .",
    "this completes the proof for algorithm i.    * proof for algorithm ii : * for each @xmath54 , show that the algorithm in algorithm ii returns @xmath29 with probability @xmath55 $ ] .",
    "the algorithm consists of some number of cycles , each consisting of steps ( i)-(iv ) , until a value @xmath29 is accepted .",
    "we first calculate the probability that @xmath29 is accepted within one of the cycles .",
    "suppose that @xmath56 . then @xmath29 must be sampled in ( i ) and accepted in ( iii ) .",
    "thus , the probability of returning @xmath29 in ( iii ) is @xmath66 = & \\pr[z \\hbox { sampled in ( i ) } ] \\     \\pr[z \\hbox { accepted in ( iii ) } ] \\notag\\\\ = & \\frac{q(z)d\\mu(z)}{i[q]}\\times\\frac{p(z)}{q(z)}\\notag\\\\ = & \\frac{p(z)d\\mu(z)}{i[q]}. \\label{equation : iismall}\\end{aligned}\\ ] ] also note that for every @xmath36,which is chosen with probability @xmath67}$ ] , the probability that it is not accepted in ( iii ) is @xmath59 . thus the total probability of not returning an element of @xmath48 in ( iii ) , which is the same as the probability of reaching ( iv ) , is @xmath68 = \\int_{\\mathcal{s}}\\frac{q(x)-p(x)}{q(x)}\\frac{q(x)}{i[q]}d\\mu(x ) = \\int_{\\mathcal{s}}\\frac{q(x)-p(x)}{i[q]}d\\mu(x ) .",
    "\\label{equation : notreturn}\\ ] ]    next suppose that @xmath61 .",
    "the probability that @xmath29 is accepted in ( ii ) is @xmath69=\\frac{q(z)d\\mu(z)}{i[q]}. \\label{equation : ai}\\ ] ] for @xmath29 to be returned from ( iv.a ) , the algorithm must reach ( iv ) , then go to ( iv.a ) and then select @xmath29 in ( iv.a ) .",
    "this has probability @xmath70 \\notag \\\\ = &      \\pr[\\hbox{reach ( iv ) } ]   \\           \\pr[\\hbox{go to ( iv.a ) } ]   \\",
    "\\pr[\\hbox{z sampled   in ( iv.a ) } ]   \\notag\\\\   = & \\left(\\int_{\\mathcal{s}}\\frac{q(x)-p(x)}{i[q]}d\\mu(x)\\right)\\times \\frac{\\int_{\\mathcal{l}}(p(x)-q(x))d\\mu(x)}{\\int_{\\mathcal{s}}(q(x)-p(x))d\\mu(x ) } \\times   \\frac{(p(z)-q(z))d\\mu(z)}{\\int_{\\mathcal{l}}(p(x)-q(x))d\\mu(x ) } \\nonumber \\\\ = & \\frac{(p(z)-q(z))d\\mu(z)}{i[q]}.   \\label{equation : bi}\\end{aligned}\\ ] ] now using equations and , the probability of returning @xmath29 in a cycle is @xmath71= & \\pr[z \\hbox { returned from ( ii)}]+\\pr[z \\hbox { returned from ( iv.a)}]\\notag\\\\ = & \\frac{q(z)d\\mu(z)}{i[q]}+\\frac{(p(z)-q(z))d\\mu(z)}{i[q]}\\notag\\\\ = & \\frac{p(z)d\\mu(z)}{i[q]}. \\label{equation : iilarge}\\end{aligned}\\ ] ] equations and imply that , whether @xmath56 or @xmath72 , the probability that @xmath29 is returned in a cycle is @xmath73 $ ] . integrating over all the samples in @xmath15 , we deduce that the probability that a sample is returned in a cycle is @xmath23/i[q]$ ] .",
    "consequently , probability that no sample point is returned in a cycle is @xmath74/i[q]$ ] .",
    "because the cycle is repeated until a sample point is returned , we conclude that the probability that the algorithm returns @xmath29 is equal to @xmath75}{i[q]})^{k-1}\\frac{p(z)d\\mu(z)}{i[q]}=\\frac{p(z)d\\mu(z)}{i[p]}.\\ ] ] this completes the proof for algorithm ii .",
    "note that the efficiency of algorithm ii is nominally the same as acceptance rejection , i.e. the probability of a rejection is @xmath76/i[q]$ ] .",
    "actually it can be significantly better because @xmath8 $ ] can be smaller , since @xmath77 is allowed .",
    "also , note that if @xmath23=i[q]$ ] , then algorithms i and ii are the same .",
    "one of the important features of reduced rejection sampling is that it requires no preprocessing time .",
    "this is particularly useful for _ dynamic simulation _ ; i.e. , simulation in which the probability distribution function @xmath10 may change after each sample ( see section [ section : example2 ] for an example from plasma physics ) . for dynamic simulation ,",
    "fast discrete sampling methods such as marsaglia s table method or the alias method , are not suitable as they require preprocessing time after each change in @xmath10 .",
    "although , the acceptance - rejection method requires no preprocessing time and can be used for dynamic simulation , it may require changes in @xmath7 if @xmath10 changes , which is usually not difficult , and it becomes very inefficient when the ratio of the area under function @xmath10 to the area under proposal function @xmath7 is small .",
    "moreover , adaptive rejection sampling is not efficient , because the process of adapting @xmath7 to @xmath10 starts over whenever @xmath10 changes .",
    "the reduced rejection sampling method can be thought as an extension of the acceptance - rejection method . in particular when the proposal function @xmath7 encloses @xmath10 ( i.e. , @xmath78 so that @xmath79 ) the reduced rejection sampling method reduces to acceptance - rejection method .",
    "the advantage of reduced rejection sampling over acceptance - rejection method is that the proposal function @xmath7 does not need to enclose function @xmath10 ; i.e. , it allows @xmath80 for some @xmath0 .",
    "this is very useful in dynamic simulation as it can accommodate changes in @xmath10 without requiring changes in @xmath7 .",
    "moreover , reduced rejection sampling may result in less unwanted samples than acceptance - rejection does , especially if @xmath10 has singularities or is highly peaked .",
    "there are several challenges in implementing the reduced rejection sampling method .",
    "the main challenge is the need to sample from set @xmath15 according to @xmath7 and from set @xmath25 according to @xmath26 , which can be performed by various sampling methods .",
    "another challenge in using reduced rejection sampling is the need to know the values of @xmath8 $ ] , @xmath23 $ ] and @xmath24 ( but note that the last value is only for algorithm ii ) . in many situations , these values",
    "are readily available or can be calculated during the simulation .",
    "in this section , reduced rejection sampling method is applied to a simple problem .",
    "let @xmath81 and sample according to @xmath82{1-x}}\\label{equation : ex1}\\ ] ] which has singularities at 0 and 1 . using inverse transform sampling",
    ", it is easy to sample according to @xmath83 or @xmath84{1-x}$ ] , but inverse transform can not be easily applied to as it requires finding the root of a eighth degree polynomial .",
    "we apply reduced rejection sampling to this problem by setting @xmath85 . observe that @xmath86 . as mentioned earlier , inverse",
    "transform sampling is easily used to sample according to @xmath7 and according to @xmath26 .",
    "the reduced rejection sampling is very fast and yields no unwanted sample points .",
    "this example is equivalent to sampling from a mixture and can be extended to sampling from a probability density @xmath10 that is a sum @xmath87 , if there is a method for sampling from each @xmath88 separately and the integrals @xmath89 $ ] are all known .",
    "in this section , we apply reduced rejection sampling to an idealized problem motivated by plasma physics . as discussed in subsection [ subsection : numerical result ] , the unique features of this problem makes other sampling methods inefficient to use .      the example presented here is a simplified version of simulation for recombination by impact of two electrons with an ion , in which one of the electrons is absorbed into the atom and the other electron is scattered . for incident",
    "electron energies @xmath90 and @xmath91 , the recombination rate is proportional to @xmath92 @xcite , which can become singular if electrons of low energy are present .",
    "this is an obstacle to kinetic simulation of recombination by electron impact in a plasma .",
    "our goal is to simulate the evolution of the following system : consider @xmath2 particles labeled @xmath93 .",
    "to each particle @xmath94 we associate a number @xmath95 , called the state of particle @xmath94 ( and corresponding to electron energy in the recombination problem ) . occasionally , where it does not cause confusion , we use @xmath96 to refer to particle @xmath94 .",
    "we refer to the set @xmath97 as the configuration of the system . for every pair of states @xmath96 and @xmath98",
    ", @xmath99 is a random variable with an exponential distribution with parameter @xmath100 , in which @xmath101 is a fixed constant between 0 and 1 .",
    "@xmath99 is the time for interaction between particle @xmath94 and @xmath102 which randomly occurs with rate @xmath103 .",
    "after an interaction occurs , say for the pair @xmath104 , the values of states @xmath105 and @xmath106 are replaced by new values @xmath107 and @xmath108 ; consequently , the distribution of @xmath99 changes if either of @xmath94 and @xmath102 is equal to @xmath109 or @xmath110 .",
    "we will consider a simple updating mechanisms for the states after each interaction . in the simulations presented below ,",
    "the updated values of @xmath107 and @xmath108 are chosen independently and uniformly at random from @xmath111 , without dependence on @xmath105 and @xmath106 .",
    "this choice has been made for simplicity and because the stationary distribution can be calculated for this choice ( see section [ subsection : theoretical result ] ) , but we expect that reduced rejection sampling would work equally well for more complex interaction rules .",
    "indeed the algorithm [ algorithm : process ] described below and the more detailed algorithm presented in section [ subsection : use of reduced rejection ] do not depend on the interaction rules .",
    "first we make some notation and observations . set @xmath112 and @xmath113 .",
    "let @xmath114 denotes an exponential random variable with parameter @xmath115 ( with rate @xmath116 ) ; then @xmath117 for any scalar @xmath16 .",
    "we will use @xmath118 in the following algorithm , which is a variant of the kinetic monte carlo ( kmc ) algorithm ( also known as the residence - time algorithm or the n - fold way or the bortz - kalos - lebowitz ( bkl ) algorithm @xcite ) , that simulates the system described above .",
    "this algorithm chooses interactions , by choosing two particles separately out of the @xmath2 number of particles , rather than choosing a pair of particles out of the @xmath119 number of pairs .",
    "[ algorithm : process ]    1 .",
    "start from @xmath120 2 .",
    "choose time @xmath121 by sampling from an exponential distribution with rate @xmath122 .",
    "3 .   choose index @xmath109 with probability @xmath123 .",
    "4 .   choose index @xmath110 with probability @xmath124 .",
    "5 .   at time",
    "@xmath125 interaction between particles @xmath109 and @xmath110 occurs .",
    "update states @xmath105 and @xmath106 according to the updating mechanism and the update value of @xmath126 .",
    "set @xmath127 and start over from 2 .",
    "we use reduced rejection sampling in subsection [ subsection : numerical result ] to perform steps 3 and 4 in the above algorithm .",
    "we also explain why other methods of sampling would be inefficient in this circumstances . to verify that our simulation is working properly",
    ", we perform the following test .",
    "let @xmath128 be a real - valued function on configuration space , with expectation @xmath129 $ ] of @xmath130 over configurations of the system .",
    "for a simple interaction rule and some functions @xmath130 we can find the value of @xmath129 $ ] analytically , as shown in subsection [ subsection : theoretical result ] .",
    "consequently , the difference between the numerical and analytic results provides a measure of the accuracy of the simulation as discussed at the end of subsection [ subsection : numerical result ] .",
    "think of the system s evolution as a random walk over the configurations of the system .",
    "suppose the updating process is that if states @xmath105 and @xmath106 interact , then states @xmath107 and @xmath108 are chosen , independently , uniformly at random from @xmath111 . in this section ,",
    "we find the stationary distribution for this random walk and the value of @xmath129 $ ] for two functions @xmath130 .",
    "let @xmath131 denote the probability of going from configuration @xmath132 to configuration @xmath133 , and @xmath134 as the probability of going from values @xmath135 to values @xmath136 in an interaction .",
    "these satisfy @xmath137 that is , the probability of getting @xmath107 and @xmath108 after @xmath105 and @xmath106 interact , is the same as probability of getting @xmath105 and @xmath106 after @xmath107 and @xmath108 interact .",
    "furthermore , for this updating mechanism , the random walk is completely mixing ; that is , it can go from any configuration to any other configuration ( if for example the updating mechanism had additional constraints , such as @xmath138 , then we would not have a mixing random walk since we could reach only those configurations that have the same sum of states as the starting configuration ) .",
    "for every configuration @xmath139 , set @xmath140 where @xmath141 is the normalizing constant so that @xmath142 .",
    "suppose the current configuration of the system is @xmath143 . according to steps 3 and 4 in algorithm [ algorithm : process ] , the probability of interaction occurring between states @xmath105 and @xmath106 is proportional to @xmath144 .",
    "let @xmath145 .",
    "then @xmath146 for the ease of explanation , relabel the states of @xmath133 so that @xmath147 where @xmath148 for @xmath149 .",
    "similarly , @xmath150 because of and since @xmath148 , for @xmath149 , it is straightforward to verify the detailed balance equation @xmath151 therefore , @xmath152 is the ( unique ) stationary distribution of the random walk . since the updating mechanism is completely mixing , the normalizing constant @xmath141 for distribution @xmath152 is the integral @xmath153 hence for any function @xmath128 , @xmath154=\\frac{(\\alpha+1)^{n-2}}{\\binom{n}{2}}\\int_{(0,1)^n}g(x_1,\\ldots , x_n)(x_1\\cdots x_n)^\\alpha\\left(\\sum_{i , j}\\frac{1}{(x_ix_j)^\\alpha}\\right)dx_1\\cdots dx_n.\\ ] ] some tedious algebra leads to the following proposition :    [ proposition : g ] using the above notations and assumptions :    * @xmath129=\\frac{\\alpha+1}{\\alpha+2}(n-2)+1 $ ] when @xmath155 . *",
    "@xmath129=\\frac{\\alpha+1}{\\alpha+3}(n-2)+\\frac{2}{3}$ ] when @xmath156 .      in this section",
    "we make some remarks about the challenges involved in simulating this system .",
    "the main challenge of sampling in this dynamic simulation is that the @xmath157 s are changing after each interaction .",
    "consequently , the sampling method should require small or zero preprocessing time .",
    "for this reason , discrete sampling methods such as marsaglia s table method or the alias method are not very efficient for this problem .",
    "next consider using acceptance - rejection method based on uniform sampling from @xmath158 to @xmath159 for the proposal distribution ( i.e. , @xmath46 constant ) .",
    "as mentioned earlier , the changing distribution property of the problem is not very detrimental for acceptance - rejection . on the other hand ,",
    "the singularity in the rates at @xmath160 can lead to a large constant for @xmath46 , for which there will be many rejected samples , so that the method is inefficient .",
    "moreover , there seems to be no other clear choice for the proposal distribution @xmath46 other than a constant .",
    "note that the sampling is from a discrete set of probabilities @xmath161 with little control over their values ; for example the @xmath157 s are not monotonically ordered .",
    "this is quite different from sampling a single random variable from the density @xmath162 .      in this section",
    "we explain how to use reduced rejection sampling to perform steps 3 and 4 in algorithm [ algorithm : process ] .",
    "reduced rejection sampling can be readily used in this dynamic simulation . even though the values of the @xmath157 s change after each interaction",
    ", they do not change drastically ; in each interaction at most two of the @xmath157 s change .",
    "starting at time @xmath163 , we set @xmath164 .",
    "after each interaction , we update the values of @xmath165 s to @xmath166 , but do not change the values of @xmath167 s . note that we can easily update the value of @xmath23 $ ] after each interaction and keep track of set @xmath168 by comparing the updated values of @xmath165 s to their corresponding values of @xmath167 s .",
    "moreover , the size of set @xmath25 changes by at most 2 after each interaction ( but it can also decrease after some interactions ) .",
    "we use marsaglia s table method to sample according to @xmath167 s .",
    "since we do not update @xmath167 s after each interaction , the preprocessing time in marsaglia s table method is only required for the first sampling and not for the subsequent samplings .",
    "to sample from set @xmath25 according to @xmath169 , we use acceptance - rejection with uniform distribution for the proposal distribution .",
    "as long as the size of set @xmath25 is not too big , the sampling from @xmath25 is not very time consuming . to prevent @xmath25 from getting too large",
    ", we reset the values of @xmath167 s to @xmath164 , which sets @xmath25 to be empty , whenever the size of @xmath25 exceeds a predetermined number @xmath170 .",
    "the size of @xmath170 is important for the performance of the algorithm",
    ". if @xmath170 is too small , then there are many updates of the @xmath167 s , each of which requires preprocessing time for marsaglia s table method . on the other hand , if @xmath170 is too big , then @xmath25 is large and costly to sample from by acceptance - rejection .",
    "our computational experience shows that setting @xmath170 equal to a multiple of @xmath171 is a good choice .",
    "it might be better for the reinitialization criterion to be based on the efficiency of the sampling from @xmath25 ( i.e. , the fraction of rejected samples when using acceptance rejection on @xmath25 ) , rather than the size of @xmath25 .",
    "we simulated the evolution of the system under the conditions outlined in subsection [ subsection : theoretical result ] with @xmath172 , and @xmath173 .",
    "we start with a random configuration at time @xmath174 .",
    "the simulation is based the reduced rejection sampling method , using marsaglia s table method and the acceptance - rejection method as described above .",
    "after each interaction , we evaluate function @xmath155 and take the average to get an estimate for @xmath129 $ ] . each result",
    "is produced by taking an average of five independent runs .",
    "figure [ figure : linear10000result ] compares the results for @xmath129 $ ] from reduced rejection sampling with those from the acceptance - rejection method .",
    "the results of figure [ figure : linear10000result ] show excellent agreement between the values of @xmath129 $ ] as a function of the number of interactions from the two methods , which provides a validity check for reduced rejection sampling .",
    "the advantage of reduced rejection sampling is demonstrated in figure [ figure : linear10000loglog ] which shows a log - log plot of the processing time , as a function of the number of interactions , for reduced rejection sampling and acceptance - rejection .",
    "the results show that reduced rejection sampling is much faster than the acceptance - rejection method .",
    "in fact , for @xmath159 interactions , the computational time scales like @xmath175 for reduced rejection sampling , and like @xmath176 for acceptance - rejection , in the range @xmath177 . for small values of @xmath159 ,",
    "the initial pre - processing step of marsaglia s table method dominates the computational time . for @xmath178 , however , the pre - processing time ( including the multiple pre - processing steps due to reinitialization ) is not a significant part of the computational time .",
    "the average number of reinitializations for reduced rejection sampling is @xmath179 for @xmath180 , respectively .",
    "another interesting advantage of the reduced rejection sampling is that the variance of the processing time for independent runs is much smaller in the reduced rejection sampling than it is in the acceptance - rejection method .",
    "[ ! htp ] $ ] using different number of interactions . here",
    "@xmath155 , @xmath172 , and @xmath173 .",
    "also @xmath181 for the reduced rejection sampling .",
    "the theoretical value of @xmath129 $ ] is 5999.8 . the estimated value of @xmath129 $ ] after @xmath182 interactions using reduced rejection sampling and acceptance - rejection methods were , respectively , 5994.59 and 5996.35 .",
    "the reported result is the average of 5 independent runs.,title=\"fig:\",width=453 ]    [ ! htp ] , @xmath172 and @xmath173 .",
    "also @xmath181 for the reduced rejection sampling .",
    "the reported processing time is the average of 5 independent runs.,title=\"fig:\",width=453 ]",
    "in this section we describe how we can use reduced rejection in the context of stochastic chemical kinetics .",
    "stochastic simulation in chemical kinetics is a monte carlo procedure to numerically simulate the time evolution of a well - stirred chemically reacting system .",
    "the first stochastic simulation algorithm , called the direct method , was presented in @xcite .",
    "the direct method is computationally expensive and there have been many adaptations of this algorithm to achieve greater speed in simulation .",
    "the first - reaction method , also in @xcite , is an equivalent formulation of the direct method .",
    "the next - reaction method @xcite is an improvement over the first - reaction method , using a binary - tree structure to store the reaction times .",
    "the modified direct method @xcite and sorting direct method @xcite speed up the direct method by indexing the reactions in such a way that reactions with larger propensity function tend to have a lower index value .",
    "recently , some new stochastic simulation algorithms , called partial - propensity methods , were introduced that work only for elementary chemical reactions ( i.e. reactions with at most two different reactants ) ( see @xcite ) .",
    "nevertheless , note that it is possible to decompose any non - elementary reaction into combination of elementary reactions .",
    "there are also approximate stochastic simulation algorithms , such as tau - leaping and slow - scale , that provide better computational efficiency in exchange for sacrificing some of the exactness in the direct method ( see @xcite and the references therein for more details ) .",
    "next we give a brief review of stochastic simulation in chemical kinetics .",
    "an excellent reference with more detailed explanation is @xcite . using the same notation and terminology as in @xcite ,",
    "consider a well - stirred system of molecules of @xmath2 chemical species @xmath183 , which interact through @xmath170 chemical reactions @xmath184 .",
    "let @xmath185 denote the number of molecules of species @xmath186 in the system at time @xmath187 .",
    "the goal is to estimate the state vector @xmath188 given the system is initially in state @xmath189 .",
    "similar to section [ section : example2 ] , when the system is in state @xmath190 , the time for reaction @xmath191 to occur is given by an exponential distribution whose rate is the propensity function @xmath192 .",
    "when reaction @xmath191 occurs , the state of the system changes from @xmath190 to @xmath193 , where @xmath194 is the change in the number of @xmath186 molecules when one reaction @xmath191 occurs .    estimating the propensity functions in general is not an easy task . as noted ,",
    "the value of the propensity functions depend on the state of the system .",
    "for example , if @xmath195 and @xmath191 are , respectively , the unimolecular reaction @xmath196 and bimolecular reaction @xmath197 , then @xmath198 and @xmath199 for some constants @xmath200 and @xmath201 .",
    "therefore , the propensity functions of the reactions are changing throughout the simulation .",
    "moreover , if for some chemical species the magnitude of their population differ drastically from others , we expect the value of propensity functions to be very non - uniform",
    ".    for every state @xmath190 , define @xmath202 to simulate the chemical kinetics of the system the following algorithm is used , which resembles algorithm [ algorithm : process ] in section [ section : example2 ] .    [ algorithm : chemicalprocess ]    1 .",
    "start from time @xmath174 and state @xmath203 .",
    "2 .   choose time @xmath121 by sampling from an exponential distribution with rate @xmath204 .",
    "3 .   choose index @xmath109 with probability @xmath205 .",
    "4 .   at time",
    "@xmath125 reaction @xmath206 occurs .",
    "5 .   update time @xmath127 , state @xmath207 and start over from 2 .    in the original direct method",
    "@xcite step 3 in the above algorithm [ algorithm : chemicalprocess ] is performed by choosing number @xmath208 uniformly at random in the unit interval and setting @xmath209 however , when we have many reactions with a wide range of propensity function values presented in the system , a scenario that is very common in biological models , the above procedure of using partial sums becomes computationally expensive . as noted earlier , some methods , such as the modified direct method @xcite and sorting direct method @xcite , index the reactions in a smart way so that they can save on the average number of terms summed in equation and consequently achieve computational efficiency .",
    "we propose a different approach to performing step 3 in algorithm [ algorithm : chemicalprocess ] using the acceptance - rejection or reduced rejection method .",
    "the approach is very similar to what was done in section [ section : example2 ] . to be specific , we can use acceptance - rejection for step 3 in the following way : let @xmath210 until an index is accepted , select index @xmath109 uniformly at random from @xmath211 and accept it with probability @xmath212 ; otherwise , discard @xmath109 and repeat .",
    "when an index is accepted step 3 in algorithm [ algorithm : chemicalprocess ] is completed . typically for chemical reactions @xmath191 ,",
    "most of @xmath194 s are zero ; therefore , we can efficiently update the value of @xmath213 at each iteration of algorithm [ algorithm : chemicalprocess ] .    however , as in section [ section : example2 ]",
    ", if the values of @xmath214 s are very non - uniform ( for example , when the population of some chemical species differ drastically from that of other species in the system ) the acceptance - rejection method becomes inefficient due to rejection of many samples . in these circumstances ,",
    "the reduced rejection algorithm can be readily used in a very similar way as it was used in section [ section : example2 ] .",
    "we expect that the use of the reduced rejection algorithm in these circumstances would greatly improve the computational efficiency of the exact stochastic simulation algorithms .",
    "in this paper we introduce a new reduced rejection sampling method that can be used to generate independent samples for a discrete or continuous random variable .",
    "the strength of this algorithm is most evident for applications in which acceptance - rejection method is inefficient ; namely , the probability distribution of the random variable is highly peaked in certain regions or has singularities .",
    "it is also useful when the probabilities are fluctuating , so that discrete methods that requiring preprocessing are inefficient . in particular ,",
    "the reduced rejection sampling method is expected to perform well on kinetic simulation of electron - impact recombination in a plasma , which is difficult to simulate by other methods .",
    "the preliminary examples in this paper are meant to illustrate these advantages of the reduced rejection sampling method .",
    "they provide evidence of improvement in computation time using the reduced rejection sampling versus acceptance - rejection method .",
    "these examples also provide some insights on implementation of the method .",
    "one possible direction for future research is the nested use of reduced rejection sampling methods . for the most difficult step - sampling from @xmath25 according to @xmath26 - we propose to apply the reduced rejection sampling method again using a new proposal function .",
    "in essence , this would use one reduced rejection sampling method inside another reduced rejection sampling method .",
    "[ ! htp ]   \\geq i[q]$].,title=\"fig:\",width=604 ]                              g.  marsaglia and w. w.  tsang , `` a fast , easily implemented method for sampling from decreasing or symmetric unimodal density functions '' , _ siam journ .",
    "scient . and statis . computing _ ,",
    "* 5 * , 2 ( 1984 ) , 349359 .",
    "mccollum , g.d .",
    "peterson , c.d .",
    "cox , m.l .",
    "simpson , n.f .",
    "samatova ,  the sorting direct method for stochastic simulation of biochemical systems with varying reaction execution behavior \" , _ comput . bio .",
    "_ , * 30 * , ( 2006 ) , 3949 ."
  ],
  "abstract_text": [
    "<S> in this paper we present a method to generate independent samples for a general random variable , either continuous or discrete . </S>",
    "<S> the algorithm is an extension of the acceptance - rejection method , and it is particularly useful for kinetic simulation in which the rates are fluctuating in time and have singular limits , as occurs for example in simulation of recombination interactions in a plasma . </S>",
    "<S> although it depends on some additional requirements , the new method is easy to implement and rejects less samples than the acceptance - rejection method . </S>"
  ]
}