{
  "article_text": [
    "there are several factors that motivated us to investigate the plausibility of using programmable gpus for numerical simulations on structured grids .",
    "these factors are the gpus    * high flops count , * compatible price performance , and * better than the cpu rate of performance increase over time .    to be specific , nvidia s nv30 graphics card is advertised to have a theoretical operation count of @xmath0 gflops / s ( see the nv30 preview @xcite or table [ table1 ] , where we summarize the results of a gpu - cpu comparison in performing graphics related operations ) . taking into account the price of the nv30 graphics card we get the low @xmath1 cents per m flop .",
    "also , the current development tendency shows a stable doubling of the gpu s performance every six months , compared to doubling the cpu s performance for every @xmath2 months ( a fact valid for the last @xmath3 years , known as _ moore s law _ ) .",
    "review of the literature on using gpus for non - graphics applications ( subsection [ literature ] ) shows that scientists report speedups of the gpu compared to the cpu for low precision operations .",
    "several of the non - graphics applications that make use of the recently available @xmath4-bit floating point arithmetic report performance that is comparable to the cpu s performance .",
    "thus , advertisements or scientific papers that report speedups of one or above one order of magnitude in favor of the gpu are restricted to low precision operations .",
    "for example , the advertised @xmath0 gflops / s in @xcite can not be achieved for @xmath4-bit floating point operations ( figure [ pipeline ] , right gives the nv30 graphics card specifications ) .",
    "such advertisements and reports about `` extremely '' high performances , combined with vagueness about the precision of the computations motivated us to develop simple gpu benchmarking software .",
    "benchmarking the nv30 fragment processors with simple vector operations showed performance of @xmath5 gflops / s which is @xmath6 of the theoretically possible maximal performance .",
    "this is the speed that we also achieved on two probability - based applications that we implemented on the gpu .    [ cols=\"^,^,^ \" , ]      communication rates between the cpu and the gpu for lattices of different sizes .",
    "the reading is done using the glreadpixels function , writing the boundary is done using the gldrawpixels function , and writing the whole domain is done using the gltexsubimage2d function .",
    "[ communication_performance ]",
    "our analysis and benchmarking were mainly concentrated on the nv30 fragment processors floating point operations performance .",
    "the results show that it is feasible to use gpus for numerical simulations .",
    "we demonstrated this by banchmarking the gpu s performance on simple vector operations and by implementing two probability - based simulations , namely the ising and the percolation models .",
    "for these two applications , the applications cited in the literature overview , our vector operations benchmarks , and various nvidia distributed shaders we observed that the fragment processors ( nvidia nv30 ) floating point performance is comparable to the performance of a pentium 4 processor running at 2.8 ghz . by comparable we mean that the gpu may be @xmath7 times faster for certain applications . for example , we achieved @xmath6 of the theoretically possible maximal performance .",
    "such performance made the gpu s implementation run twice faster than the cpu s .",
    "a modification of the algorithm that removes the conditional if / else statements related to the checkerboard type traversal of the 2d lattices would contribute an additional speedup of at least two times .",
    "these results make the use of the gpu as a coprocessor appealing .",
    "also , gpus tend to have a higher rate of performance increase over time than the cpus , thus making the study of non - graphics applications on the gpu valuable research for the future .",
    "speedups of approximately one order of magnitude in favor of the gpu are observed only in certain applications involving low precision computations .",
    "a reason for this difference is the larger traffic involved in higher precision computations which traffic makes the gpus local memory bandwidth a computational bottleneck .",
    "finally , we note that the acceleration graphics ports provide enough bandwidth for the cpu - gpu communications to make the use of parallel gpus computations feasible .",
    "we would like to thank beverly tomov from cold spring harbor laboratory , ny , for her attentive editing and remarks .",
    "we thank michael creutz from brookhaven national laboratory , ny , for the discussions with him , his suggestions , and advice about the models implemented .            nolan goodnight , gregory lewin , david luebke , and kevin skadron , _ a multigrid solver for boundary - value problems using programmable graphics hardware _ , proceedings of the acm siggraph / eurographics conference on graphics hardware ( 2003 ) , pp .",
    "102 - 111 .",
    "r. sasik , t. hwa , n. iranfar , and w.f .",
    "loomis , _ percolation clustering : a novel approach to the clustering of gene expression patterns in dictyostelium development _",
    ", pacific symposium on biocomputing 6:335 - 347 ( 2001 ) ."
  ],
  "abstract_text": [
    "<S> the latest graphics processing units ( gpus ) are reported to reach up to 200 billion floating point operations per second ( @xmath0 gflops @xcite ) and to have price performance of @xmath1 cents per m flop . </S>",
    "<S> these facts raise great interest in the plausibility of extending the gpus use to non - graphics applications , in particular numerical simulations on structured grids ( lattice ) . </S>",
    "<S> we review previous work on using gpus for non - graphics applications , implement probability - based simulations on the gpu , namely the ising and percolation models , implement vector operation benchmarks for the gpu , and finally compare the cpu s and gpu s performance . </S>",
    "<S> a general conclusion from the results obtained is that moving computations from the cpu to the gpu is feasible , yielding good time and price performance , for certain lattice computations . </S>",
    "<S> preliminary results also show that it is feasible to use them in parallel . </S>"
  ]
}