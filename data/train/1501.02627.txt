{
  "article_text": [
    "in many fields it is common to have access to information about sums of random variables and to desire information about those variables themselves . in mass spectrometry , when two ( or more ) analytes with similar mass - to - charge are measured , the intensity of the resulting peak is a function of the sum of abundances of those analytes ( this problem occurs not only in the mass spectrometry of small molecules , but also in measuring isotope measurement in elemental and nuclear mass spectrometry ) . in transcriptomics ,",
    "the abundance of a particular non - unique read ( _ i.e. _ , an rna sequence that maps to multiple locations in the transcriptome or genome ) provides information about the sum of the abundances of all transcripts that contain the read ( each transcript weighted by how many copies of the read it carries ) .",
    "proteomics has its own version of non - unique reads , shared peptides which can be found in multiple proteins ( not only are shared peptides the principal source of difficulty in protein inference@xcite , they are also responsible for the difficulty evaluating putatative sets of discovered proteins@xcite ) . in population genetics , the prior knowledge about population structure can suggest an expected number of individuals with a particular genotype , which in turn yields probabilistic information about the individuals whose aggregate genotypes are expected to produce that sum ( inference is particularly pronounced in polyploids , which increase the dimensionality of the problem@xcite ) .    in all of these fields ,",
    "the information on sums of random variables presents a singular obstacle to computational biology . and",
    "regardless of how infrequently we as scientists directly discuss our current inability to effectively utilize information about sums of random variables , the perception of our limited ability to meet the challenge has become firmly entrenched in our collective unconscious ; the silent agreement on our inability to turn the sausage grinder backwards and convert the sausage ( information about the sum of several random variables ) back into the pigs ( information about those random variables that contributed to the sum ) is so well - established that it not only defines the way that we address these data ( _ i.e. _ , mass spectra peaks containing multiple analytes , counts of non - unique reads , _ etc . _ ) , but it also causes us to discard data and even limit research directions we might otherwise consider .",
    "for instance , in mass spectrometry , substantial effort is invested in chromatography@xcite and other separation techniques@xcite , which aim to distinguish and separate analytes so that they will not be measured by the mass spectrometer simultaneously ( thereby reducing the chances of analytes resulting in overlapping peaks ) ; the task of decomposing this useful aggregate information from overlapping peaks back into information about its contributing parts is eschewed in favor of significant investments in instrumentation ( _ e.g. _ , more and more advanced separation technologies and higher - resolution mass spectrometers@xcite ) , and still it is common practice to discard shared isotope peaks and shared peptides even though they may comprise a large percent of the data and contain additional information@xcite .",
    "likewise , in genomics and transcriptomics it is common to simply discard all data from non - unique reads@xcite . in burgeoning fields such as metagenomics ,",
    "this loss of data and subsequent loss of information can be even more pronounced , for instance when all data that map to two or more species of interest are discarded . in some cases",
    ", recovering this lost information will be the key to making strong conclusions , such as distinguishing between two closely related species ( or bacterial strains ) in a metagenomic mixture .",
    "fast fourier transform- ( fft-)based convolution can be used to dramatically improve the efficiency of computing the sum - product addition of two discrete random variables . for three discrete random variables where @xmath6 and where @xmath7 and @xmath8 , then the probability mass function ( pmf ) of @xmath9 can be computed via the convolution of the pmfs of @xmath10 and @xmath11 , denoted @xmath12 and @xmath13 respectively .",
    "note that it is sufficient to compute @xmath14 , because the result will be scaled so that its sum is of unity ( @xmath15}$ ] ) : @xmath16 & \\propto & \\sum_\\ell \\sum_r \\pr(l=\\ell ) \\pr(r = r ) \\pr(m=\\ell+r ) \\\\ & = & \\sum_\\ell \\pr(l=\\ell ) \\pr(r = m-\\ell)\\\\ & = & { \\operatorname{pmf}}_{l } * { \\operatorname{pmf}}_{r } \\\\\\end{aligned}\\ ] ] where @xmath17 performs the convolution between the two @xmath18length vectors storing each pmf .    whereas naive convolution would compute @xmath19 in @xmath20 steps",
    ", fft exploits the bijection of this convolution to the product between two polynomials ( where the vectors being convolved are the coefficients of the polynomials being multiplied and the coefficients of their product forms the vector result ) ; this bijection enables the use of alternative forms for representing the polynomials ( each order @xmath21 polynomial can be represented through @xmath1 unique points through which it passes ) , which in turn permits elegant divide and conquer algorithms such as the cooley - tukey fft to compute fast convolution in @xmath22 steps .",
    "subtraction in the sum - product ( _ i.e. _ , computing @xmath23 ) scheme can be performed by first negating @xmath24 ( this is done by reversing reversing the vector storing the pmf @xmath25 = { \\operatorname{pmf}}_{r}[-r]$ ] ) , and then adding @xmath26 as before via fft convolution @xmath27 . also , the runtime constant on fft convolution algorithms is generally very low , partly due to the nature of elegance of the algorithms and partly because implementations have been optimized heavily due to the ubiquity of convolution in signal processing .",
    "the task of processing information about the sum of @xmath2 discrete variables ( each with @xmath1 bins ) to retrieve information on the individual variables can be performed naively in @xmath28 steps by simply enumerating the exponentially many possible outcomes ; however , such brute - force techniques are wildly inefficient when either @xmath2 or @xmath1 become large .",
    "fortunately , recent work proposes methods to decompose larger problems ( _ e.g. _ , into multiple sums and differences of pairs of discrete variables of the form @xmath29 and @xmath30 , very fast inference can be achieved .",
    "this has been derived for binary variables ( @xmath31 ) in @xmath32@xcite , and was independently discovered for arbitrary discrete distributions ( _ i.e. _ , where @xmath33 ) and to multidimensional distributions ( via matrix convolution , which can be decomposed into one - dimensional convolutions by the row - column algorithm ) using the probabilistic convolution tree@xcite ( * algorithm  [ algorithm : convolution - tree ] * ) . in the general case ,",
    "distributions on all individual variables conditional on information about the sum can be computed in @xmath34 steps ( whenever @xmath22 fast convolution is available ) : @xmath35 + \\left [ \\sum_{u=1}^{log(n ) } u \\right]\\\\ & = & n k   \\left [ \\log(n )",
    "\\log(k ) \\right ] + \\left [ \\frac{\\log(n)(\\log(n)+1)}{2 } \\right]\\\\ & \\in & o(n k\\log(n k)\\log(n).\\\\\\end{aligned}\\ ] ] in practice , this can be significantly faster than the @xmath36 steps required by dynamic programming when fast convolution is not available .",
    "for instance , when an observed transcript fragment could originate from @xmath37 species , and where the abundance of each species is discretized into @xmath38 bins , then fast convolution makes inference more than 1800 times faster ( the difference between one algorithm taking 1 second and the other taking 30 minutes ) , and the disparity only grows for problems with larger values of @xmath2 and @xmath1 .",
    "it should be noted that these are only approximate runtimes calculated from the big o form ; in practice , it is fairly likely that methods based on fast convolution will be significantly faster , because of the method s inherent properties and the fact that very optimized implementations exist .    @xmath39 ~ ] $ ] @xmath40 $ ] @xmath41 $ ]    @xmath42 * _ { param } currentlayer[j+1]$ ]",
    "@xmath43 @xmath44    @xmath45 ~ ] $ ] @xmath46 $ ] @xmath41 $ ]    @xmath47 $ ]    @xmath48 $ ] @xmath49 $ ]    @xmath50 ~*_{param}~ ( -rhs)$ ] + @xmath51 @xmath52 ~*_{param}~ ( -lhs)$ ] @xmath53    @xmath54 @xmath55    @xmath56 @xmath57 @xmath58    @xmath59[j]$ ] @xmath60[0]$ ] @xmath61 +      qualitatively , max - product inference is a close cousin to sum - product inference .",
    "where sum - product inference considers each of the exponentially many joint events and allows each to contribute to the result ( in hidden markov models , this is analogous to the forward - backward algorithm ) , max - product inference allows only the highest - quality joint events to contribute ( in hidden markov models , this defines the viterbi path ) .",
    "both inference methods have complementary advantages and disadvantages : the advantage of sum - product inference is its democratized equal weighting of all joint events , the variety of which can provide a rich description of any high - probability joint events suggested by the data ; however , this can also have disadvantages in that many low - quality joint events ( _ i.e. _ , those with low joint probability ) may shape the result as much as a small number of high - quality results .",
    "likewise , in sum - product inference , multiple mutually exclusive joint events can simultaneously contribute to the result , raising the potential to erroneously infer implausible conclusions , because both may be plausible before considering the other .",
    "it is because of these disadvantages in sum - product inference that max - product inference is widely used , because it forces the inferences to be jointly plausible ( not simply individually , but as a whole ) , and because it drowns out noise from low - quality configurations that can diffuse and lower the certainty of conclusions in sum - product inference .",
    "specifically , efficient max - product inference on sums of random variables would be quite useful ; in addition to the examples of shared peptides , non - unique reads , _ etc . _",
    "found throughout computational biology ( wherein we have information about the sum of variables , but want to draw conclusions about the variables themselves ) , more efficient max - product inference would make possible new inference algorithms on specialized classes of hidden markov models ( hmms ) where the transition probabilities from the state at index @xmath62 to the state at index @xmath63 depend on a function of either @xmath64 or @xmath65 or @xmath66 .",
    "such hmms have applications in finance and time series analysis , where the @xmath1 states at any layer are high - resolution discretizations of some quantity or price , and where the probability of a price moving up or down is influenced by the quantity up or down it moved since the previous time point . in a general hmm with @xmath1 states and @xmath2 layers of those states , performing either sum - product ( via the forward - backward algorithm ) or max - product ( via the viterbi algorithm ) inference requires @xmath67 steps ; however , performing sum - product inference on the specialized class of hmms mentioned above would require only @xmath68 steps , because each layer can be processed as a two - node convolution tree .",
    "but finding the viterbi path on such a model in @xmath68 steps is not currently feasible , because doing so would require performing max - convolution ( where the max of all valid pairings is chosen rather than the sum over all valid pairings ) in @xmath0 steps .",
    "however , despite the promise of max - product inference on sums of random variables , a fast practical solution that utilizes @xmath0 max - convolution ( _ i.e. _ , one with speed roughly comparable to fft - based standard convolution ) is not yet available for the general max - convolution problem problem .",
    "one special cases for use only when @xmath31@xcite , can solve the problem in @xmath69 time by sorting the @xmath2 variables in descending order of probability @xmath70 and then exploiting the property that any case where the number of `` true '' variables @xmath71 must prefer the first @xmath72 variables in the sorted order .",
    "this method understandably fails when @xmath33 because there is no guarantee of an ordering that will satisfy all dimensions ( when @xmath31 , increasing the probability of @xmath73 has a useful effect of decreasing the probability of @xmath74 by the same amount in order to preserve the unitary value of the sum ) .",
    "when @xmath75 , a similar idea to the sorting approach can be used , but not without approximation or some method for exploring or optimizing the exponential space of joint events@xcite .    adapting the probabilistic convolution tree algorithm from * algorithm  [ algorithm : convolution - tree ] * ( by simply replacing all uses of @xmath17 with @xmath76 when adding pairs of variables ) achieves only an @xmath36 runtime for max - product inference because additions and subtractions between individual pairs of random variables will require @xmath77 time without a faster algorithm for max - convolution .",
    "it is tempting to try to derive an fft equivalent to max - convolution , a subtle difference makes this challenging : where standard convolution uses the operations @xmath78 on real - valued numbers ( a `` ring '' ) , max - convolution employs @xmath79 ( alternatively applying a log - transformation to the probabilities being convolved will negate them and thus change the problem to the equivalent @xmath80 operations , called min - convolution , infimal convolution , or convolution on the `` tropical semiring '' ) .",
    "regardless of which form is used , the employment of the @xmath81 ( or @xmath82 in the min - convolution case ) downgrades the operation from a ring to a `` semiring '' because the @xmath81 and @xmath82 operations have no inverse .",
    "thus , the max - product addition of two discrete random variables takes a different form , which is no longer bijective to polynomial multiplication : @xmath16 & \\propto & \\max_\\ell \\max_r \\pr(l=\\ell ) \\pr(r = r ) \\pr(m=\\ell+r ) \\\\ & = & \\max_\\ell \\pr(l=\\ell ) \\pr(r = m-\\ell)\\\\ & = & { \\operatorname{pmf}}_l * _ { \\max } { \\operatorname{pmf}}_r \\\\\\end{aligned}\\ ] ] where @xmath76 is the max - convolution operator .",
    "the loss of the bijective polynomial representation prevents the exploitation of the lagrange form of polynomials , and thus there is no known @xmath22 algorithm for performing max - convolution .",
    "excluding such highly specialized methods as the rank method of babai@xcite ( which achieves a runtime of @xmath83 time but only when the vector of length @xmath84 contains elements with value @xmath85 or @xmath86 ) , the two most sophisticated max - convolution algorithms applicable to probabilistic inference are from bussieck _ _ et al.__@xcite and bremner _",
    "_ et al.__@xcite .",
    "the method from bussieck has a @xmath87 runtime in the worst case , but under a certain distribution values in the two vectors being convolved , the authors demonstrate an expected runtime of @xmath0@xcite .",
    "the approach works by starting the result @xmath88 \\gets -\\infty$ ] and then proceeds by sorting the two vectors being convolved ( @xmath10 and @xmath11 ) in descending order .",
    "their method then proceeds through both lists head - first to generate the first @xmath22 sorted terms of @xmath89 { \\operatorname{pmf}}_r[r]$ ] .",
    "each of these terms is used to update the appropriate index in the result vector @xmath90 \\gets \\max({\\operatorname{pmf}}_m'[\\ell+r ] , { \\operatorname{pmf}}_l[\\ell ] { \\operatorname{pmf}}_r[r ] ) $ ] .",
    "thus far , the algorithm is @xmath91 , but there may be indices of @xmath92 that have not yet been set ( they are still equal to @xmath93 ) ; each of these must be computed , and each such direct computation takes @xmath94 time ( if there are @xmath95 such unset indices , then the overall runtime becomes @xmath87 ) . despite the significant achievement posed by the construction of this algorithm , the authors suggest that the runtime constant is quite high due to the overhead of the sophisticated algorithms used to sort the largest @xmath22 values while neither sorting nor even _ generating _ all @xmath96 values ; they suggest that their result is of mostly theoretical import , and suggest using other methods in practice .",
    "the method of bremner _",
    "( which was subsequently extended by williams@xcite ) draws a relationship between min - convolution and the necklace alignment problem , wherein two two collections of beads , each on its own circular string , are rotated to optimally align@xcite .",
    "their method is the most sophisticated in existence , and consists of a highly complicated exploitation of similarity to the all - pairs shortest paths problem to achieve a method with a subquadratic worst - case runtime of @xmath97 for each max - convolution ( the runtime of the bremner _",
    "_ method can also be improved using a more recent method for solving the all - pairs shortest paths problem , decreasing the runtime to @xmath98@xcite ) .",
    "even if it were possble to be implemented with runtime constant as optimized as fft , the cost of using a max - convolution tree to solve the previously mentioned metagenomic max - product inference problem on @xmath37 variables where each has @xmath38 states would be over 166 times slower than the cost of solving an equally sized sum - product problem with fft convolution ( the number of steps required was calculated numerically to avoid computing a closed form of the computational cost ) .",
    "thus , even with significant mathematical sophistication of these two state - of - the - art methods , practically efficient max - product inference may be out of reach for even moderately sized problems .",
    "here i will introduce a numerical method for estimating the max - convolution in @xmath22 time , which can be applied easily using existing high - performance numerical software libraries .",
    "this is essentially performed by transforming both the inputs and outputs of the fft to achieve @xmath99norm convolution , which in turn is used as an approximation for max - convolution via the chebyshev norm .    for a max - convolution between @xmath12 and @xmath13 @xmath100 = \\max_\\ell~ { \\operatorname{pmf}}_l[\\ell ] { \\operatorname{pmf}}_r[m-\\ell],\\ ] ] at each @xmath72 value ,",
    "the shifted products terms can be rewritten as a simple vector @xmath101 where elements are defined by @xmath102 = { \\operatorname{pmf}}_l[\\ell ] { \\operatorname{pmf}}_r[m-\\ell].\\ ] ] furthermore , because pmfs consist of nonnegative real values ( or machine - precision representations ) , then this can be rewritten using the chebyshev norm , which computes the maximum absolute value in the vector @xmath101 . because @xmath101 comes from the product of pmf terms , it is also nonnegative and thus absolute values can be ignored in both the computation and the result : @xmath103 & = & \\max_\\ell u^{(m)}[\\ell ] \\\\     { \\operatorname{pmf}}_m'[m ] & = & \\lim_{p \\to \\infty } { \\left ( \\sum_\\ell { u^{m)}[\\ell]}^p \\right)}^\\frac{1}{p}. \\\\\\end{aligned}\\ ] ] and then each element of @xmath104 $ ] can be expanded back into its original factors : @xmath100 = \\lim_{p \\to \\infty } { \\left ( \\sum_\\ell { { \\operatorname{pmf}}_l[\\ell]}^p { { \\operatorname{pmf}}_r[m-\\ell]}^p \\right)}^\\frac{1}{p}.\\ ] ] at this point , a sufficiently large value @xmath105 is used in place of the limit @xmath106 : @xmath100 \\approx { \\left ( \\sum_\\ell { { \\operatorname{pmf}}_l[\\ell]}^{p^ * } { { \\operatorname{pmf}}_r[m-\\ell]}^{p^ * } \\right)}^\\frac{1}{p^*}.\\ ] ]    at this point , it can be observed that every time elements of the pmfs @xmath107 $ ] and @xmath108 $ ] appear , they are raised to the @xmath105 power ; thus , it is possible to change variables and let @xmath109 = & { { \\operatorname{pmf}}_l[\\ell]}^{p^*}\\\\    \\forall r &   v_r[r ] = & { { \\operatorname{pmf}}_r[r]}^{p^*},\\\\\\end{aligned}\\ ] ] yielding @xmath100 \\approx { \\left ( \\sum_\\ell v_l[\\ell ] v_r[m-\\ell ] \\right)}^\\frac{1}{p^*}.\\ ] ]    a similar strategy can be made for @xmath92 ; it is possible to introduce another vector @xmath110 such that every element @xmath111 $ ] is the result of @xmath112 & \\approx & { v_m[m]}^{\\frac{1}{p^*}}\\\\    v_m[m ] & = & \\sum_\\ell v_l[\\ell ] v_r[m-\\ell]\\\\\\end{aligned}\\ ] ] and thus it becomes clear that @xmath110 is the result of standard convolution ( _ not _ a max - convolution ) between @xmath113 and @xmath114 .",
    "this suggests a numerical algorithm that can make use of existing fft convolution libraries to compute @xmath115 in @xmath0 steps ( * algorithm  [ algorithm : max - convolution - original ] * ) .",
    "@xmath116 \\gets { { \\operatorname{pmf}}_l[\\ell]}^{p^*}$ ]    @xmath117 \\gets { { \\operatorname{pmf}}_r[r]}^{p^*}$ ]    @xmath118    @xmath119 \\gets { v_m[m]}^{\\frac{1}{p^*}}$ ]    @xmath92      the main caveat for numerical methods is often the loss of precision due to underflow when raising small probabilities to the power @xmath105 ( in this case , no overflow occurs because the inputs are probabilities ) ; such losses may not be undone later when raising to the @xmath120 power .",
    "one way to limit unnecessary loss of precision is to recognize that since @xmath92 will be normalized after it is estimated , then it can be scaled arbitrarily during computation without altering the final result .",
    "for this reason it can be beneficial to scale a vector by dividing by its maximum element before raising it to the @xmath105 or @xmath120 power ; this will start the dominant elements close to 1 , and thus allow them to lose little information to underflow . using this strategy yields a slightly modified algorithm ( * algorithm  [ algorithm : max - convolution - revised ] * ) .",
    "@xmath121 $ ] @xmath122 $ ]    @xmath116 \\gets { \\left ( \\frac{{\\operatorname{pmf}}_l[\\ell]}{{\\operatorname{pmf}}_l [ \\ell_{\\max } ] } \\right ) } ^{p^*}$ ]    @xmath117 \\gets { \\left ( \\frac{{\\operatorname{pmf}}_r[r]}{{\\operatorname{pmf}}_r [ r_{\\max } ] } \\right ) } ^{p^*}$ ]    @xmath118    @xmath123 $ ]    @xmath119 \\gets { \\left ( \\frac{v_m[m]}{v_m[m_{\\max } ] } \\right ) } ^{\\frac{1}{p^*}}$ ]    @xmath124 { \\operatorname{pmf}}_r[r_{\\max } ] ~        { \\operatorname{pmf}}_m'$ ]    note that , like standard implementations of fast convolution @xmath17 , in implementing the fast @xmath76 operator it is possible to automatically choose between a naive implementation or the fast numerical implementation depending on the size of the problem ; on very small problems ( _ e.g. _ @xmath125 ) , the naive operation will have less overhead and can be a bit faster ( the specific threshold can be chosen roughly by comparing the expected running time from the fast numerical method @xmath126 ( where @xmath127 is double the next integer power of two and the log is base two ) to the @xmath77 ) ; this can reduce numerical error further .",
    "i briefly compare the speed and accuracy of the fast numerical max - convolution method as compared to naive max - convolution .",
    "both methods are implemented in the python programming language using floating point math and the numpy package ( the fast numerical max - convolution method is implemented from * algorithm  [ algorithm : max - convolution - revised]*.      the speed of naive max - convolution is compared to the fast numerical estimate . for each @xmath128 ,",
    "random pairs of vectors with uniform elements ( _ i.e. _ , each element is drawn from @xmath129 ) .",
    "the result of the max - convolution @xmath130 is computed via @xmath87 naive max - convolution and the fast numerical method .",
    "* figure  [ figure : max - convolution - speed ] * demonstrates an substantial speedup in practice .    , width=384 ]      a cursory empirical test of the numerical stability as a result of @xmath105 and the vector length @xmath1 was performed . in * figure",
    "[ figure : max - convolution - accuracy ] * , the numerical stability was demonstrated on 64 random pairs of vectors for each length @xmath131 . for all @xmath132 , and the relative absolute error of each element in the result of the max - convolution",
    "is computed @xmath133 -    exact[m]}{exact[m]}|$ ] , where @xmath134 $ ] and @xmath135 $ ] refer to the value at index @xmath72 of the numerical and naive results respectively .",
    "( * figure  [ figure : max - convolution - accuracy ] * ) demonstrates the relationship between @xmath105 , @xmath1 , the relative absolute error and the magnitude of the exact result .",
    "[ cols=\"^,^,^ \" , ]     qualitatively , optimizing the numerical performance involves satisfying competing ideals : when @xmath105 is large , the problem solved converges to the max - convolution inference , but underflow becomes significant . when @xmath105 it s too small , non - maximal terms contribute to the result ( more similar to a standard convolution ) .",
    "although more sophisticated numerical analysis would almost certainly yield larger improvements to the method , a simple improvement is exploited : generally the relative error is fairly low , but only becomes high in these experiments when the exact value at that index is close to zero ( this is intuitive from the formula for relative absolute error ) . since underflow is the only numerical consideration ( because the values are normalized to the maximum ) , then it follows that a result that is not close to zero at some index is convergent when @xmath105 is substantially large ( if it suffered from too much underflow , then it would approach zero quickly ) .",
    "therefore , when using a high value of @xmath105 , indices where the numerical solution is close to zero indicate that the potential for numerical error , and suggest that a smaller value of @xmath105 could be used for those indices .",
    "this yields a further improvement where a more accurate result can be constructed from two calls of * algorithm  [ algorithm : max - convolution - revised ] * ; this improved method is shown in * algorithm  [ algorithm : max - convolution - piecewise ] * , and runs in roughly twice as many steps as * algorithm  [ algorithm : max - convolution - revised ] * ( still @xmath136 ) .",
    "note that this piecewise method could be trivially extended to use more than two values of @xmath105 , increasing accuracy at the expense of additional runtime ( although , assuming the results using the different values of @xmath105 are computed in decreasing order , then the routine could potentially terminate once the result has been estimated at all indices with adequate numeric stability ) .",
    "@xmath137 @xmath138 @xmath139    @xmath140 \\gets maxconvolutionrevised({\\operatorname{pmf}}_l , { \\operatorname{pmf}}_r , p_{lower}^*)$ ] @xmath141 \\gets maxconvolutionrevised({\\operatorname{pmf}}_l , { \\operatorname{pmf}}_r , p_{higher}^*)$ ]    @xmath142 \\gets f_{aggressive}[m]$ ] @xmath142 \\gets f_{stable}[m]$ ]    @xmath143",
    "lastly , a three - way piecewise implementation of max - convolution similar to the one shown in * algorithm  [ algorithm : max - convolution - piecewise ] * but using @xmath144 ( the python code of this three - way piecewise method is given in the accompanying python demonstration code ) is used to solve a simulated probabilistic generalization of the the subset sum problem . in this problem ,",
    "@xmath145 people go shopping and each person @xmath146 buys exactly one of two items ( the price of the item they do purchase is @xmath147 and the price of the item they do not purchase is @xmath148 ) , where the costs of both items for each person @xmath149 are unknown to us . then , given fuzzy knowledge about the costs of these items with all prices discretized into @xmath150 bins ( _ i.e. _ , @xmath151 $ ] where @xmath152 ) and given fuzzy knowledge about the total amount spent ( @xmath153 , where @xmath154 ) , we try to infer the amount spent by each person ( _ i.e. _ , for each person @xmath149 , estimating @xmath147 ) .",
    "data are generated as follows : at each variable @xmath155 , two means are randomly sampled @xmath156 , and a discretized gaussian pmf is centered about each mean ( the standard deviations of the gaussians are each sampled @xmath157 ) .",
    "a vector proportional to the pmf @xmath158 $ ] is computed using the sum of these gaussians with a vector of uniform noise @xmath159 \\sim uniform(0 , 0.0001)$ ] .",
    "the likelihood distribution on the sum @xmath154 is generated by adding a gaussian with mean @xmath160 and variance @xmath161 ( _ i.e. _ , @xmath162 the possible number of outcomes for @xmath9 ) , plus point - wise samples of uniform noise @xmath163 \\sim uniform(0 , 0.0001)$ ] .    on each problem instance ,",
    "likelihoods for all inputs @xmath164 are computed twice , once using naive max - convolution and once using the numerical method .",
    "note that even though these values of @xmath2 and @xmath1 do not appear particularly large , the full max - convolution tree that they produce will will compute several max - convolutions on the order of @xmath94 and a few on the order of @xmath165 , which in this case can be @xmath166 .",
    "for this reason , computing the likelihood curve with the naive result requires 159 seconds , while the fast numerical approach takes 0.935 seconds to compute a highly similar result .    a single likelihood distribution @xmath167 for one particular @xmath168 , which was computed using the naive method , the fast numerical method is plotted in * figure  [ figure : max - convolution - tree - accuracy]*. this figure also demonstrates the utility of max - product inference by also showing the result from sum - product inference , which is much less informative ( and does not have a mode close to the correct answer .",
    "although its ethos may ultimately limit the utility of this method to numerical settings where small errors are tolerable ( as opposed to the to more general theoretical papers previously mentioned@xcite ) , numerical method proposed here gives a simple and very fast estimate of the max - convolution result , which could allow use of numerical max - convolution ( or max - product inference on the sums and differences between discrete distributions ) in settings where it is currently far too computationally expensive .",
    "the largest caveat to the method is the inaccuracy that can occur due to numerical bottlenecks ( _ e.g. _ , underflow ) ; however , for many problems ( _ e.g. _ , practical applications of the max - product inference problem in * figure  [ figure : max - convolution - tree - accuracy ] * ) , the numerical method is sufficient to perform high - quality inference , but in a dramatically faster time .     from the probabilistic generalization of the subset - sum problem is shown .",
    "this distribution is computed via a probabilistic max - convolution tree ( the problem is solved twice , once with naive max - convolution and once via fast numerical max - convolution ) .",
    "the true mode value @xmath147 for that @xmath149 is indicated by the bar beneath the largest mode . to compare the results of different types of inference ,",
    "the sum - product result from the convolution tree ( using the standard convolution operator ) is also plotted ; note that sum - product inference produces a less discriminative likelihood curve because many possible joint events have diffused into it .",
    "[ figure : max - convolution - tree - accuracy],width=384 ]    furthermore , the connection between the max - convolution problem and the all - pairs shortest path problem from graph theory@xcite means that the fast numerical method may be used to compute fast numerical approximations to that important computer science problem .",
    "such fast numerical estimates could complement theoretical solutions to that problem .    a more in - depth theoretical analysis of the algorithm s error",
    "would likely yield multiple opportunities to modify the algorithm in order to decrease error .",
    "for example , one possible improvement could be performed by using log - transformed real values : in log - transformed space , raising to the power @xmath105 would be equivalent to scaling by @xmath105 , and would not produce significant underflow . furthermore , the operations required by fft convolution could be performed in log - space by translating the ring ( @xmath169,@xmath170 ) on real values to its equivalent ring @xmath171 on log - transformed values , where the operation @xmath172 is performed by dividing out the greater of the two arguments @xmath173 ( w.l.o.g . ) and then computing @xmath174 via taylor series ; performing the cooley - tukey fft on log - transformed values ( or possibly using a different fft algorithm that is particularly well suited for log - transformed values ) could represent one route for improving the accuracy .    in a similar vein , more sophisticated techniques for locally choosing among a small number of values for @xmath105 ( compared to a simple piecewise function on two possible values of @xmath105 ) .",
    "for instance , under roughly uniform distributions of values in the two vectors being max - convolved , the values closest to zero ( and thus having higher chance of having high relative absolute error ) will occur more commonly at the first and last indices of the numerical estimate ( because those indices take the maximum over smaller collections of elements , and are thus more likely to be smaller values ) .",
    "similar attention could be put toward scaling the vectors prior to taking elements to the @xmath105 ( compared to the current procedure of dividing by the maximum element value ) may minimize the underflow on a large number of points with large values .",
    "a most exciting possibility would be that having an accurate estimate of the max - convolution result somehow could be used to compute a more accurate result ( _ e.g. _ , using approximate results from different @xmath105 and exploiting the property @xmath175 when @xmath176 and @xmath177 ) .",
    "such directions of future research could possibly solving the max - convolution iteratively over a bounded or constant number of subproblems where each subproblem requires @xmath0 , by first computing initial estimates of the max - convolution result with the numerical method presented here , and then using those initial estimates to parameterize a subsequent call to the numerical method in a manner reminiscent of the qr algorithm for eigendecomposition@xcite . from * algorithm  [ algorithm : max - convolution - piecewise ]",
    "* , it seems highly likely that there will be more ways by which an initial result can be used to obtain a higher - accuracy result .",
    "furthermore , even pursuing methods for obtaining very high accuracy with large values of @xmath105 may not always be of substantial interest .",
    "indeed , even if no improvement to accuracy is ever presented , the design and parameterization of machine learning methods ( including graphical models ) has traditionally been empirically driven , and the use of exact max - product inference is hardly sacrosanct in every application ( as opposed to inference somewhere between sum - product and max - product ) . from this perspective , rather than choosing @xmath105 as a static constant value @xmath178 ( _ i.e. _ , performing sum - product inference ) or @xmath179 ( _ i.e. _ , performing max - product inference ) , @xmath105 could be viewed as a hyperparameter that is used to position inference on a continuum somewhere between all joint events contributing equally to the end result ( sum - product ) and only the best joint event contributing ( max - product ) , and intermediate values of @xmath105 would establish a preference for the top few joint events . in this sense",
    ", the value chosen for @xmath105 could be driven by the data , and the problem of @xmath180-norm convolution ( where a finite @xmath180 is desired , rather than max - convolution where @xmath181 ) can already be solved with very high accuracy by the proposed numerical method for any moderate choice of @xmath105 .",
    "a simple illustration of the fast numeric max - convolution method in python ( using the numpy package ) , including a three - way piecewise implementation , is available at https://bitbucket.org/orserang/fast-numerical-max-convolution .",
    "thanks to mattias franberg and ryan emerson for the helpful comments .",
    "bremner , d. , chan , t.m . ,",
    "demaine , e.d . , erickson , j. , hurtado , f. , iacono , j. , langerman , s. , taslakian , p. : necklaces , convolutions , and @xmath182 . in : algorithms  esa 2006 , pp .",
    "springer ( 2006 )      dost , b. , bandeira , n. , li , x. , shen , z. , briggs , s. , bafna , v. : shared peptides in mass spectrometry based proteomics . in : s.  batzoglou ( ed . ) proceedings of the thirteenth annual international conference on computational molecular biology , vol .",
    "13 , pp . 356371 ( 2009 )          james , a.t . , martin , a.j.p . :",
    "gas - liquid partition chromatography : the separation and micro - estimation of volatile fatty acids from formic acid to dodecanoic acid .",
    "biochemical journal * 50*(5 ) , 679 ( 1952 )    lefranois , p. , euskirchen , g.m . ,",
    "auerbach , r.k . ,",
    "rozowsky , j. , gibson , t. , yellman , c.m . , gerstein , m. , snyder , m. : efficient yeast chip - seq using multiplex short - read dna sequencing .",
    "bmc genomics * 10*(1 ) , 37 ( 2009 )    polacco , b.j . , purvine , s.o . ,",
    "zink , e.m . , lavoie , s.p . ,",
    "lipton , m.s . , summers , a.o . , miller ,",
    "s.m . : discovering mercury protein modifications in whole proteomes using natural isotope distributions observed in liquid chromatography - tandem mass spectrometry .",
    "molecular & cellular proteomics * 10*(8 ) , m110004,853 ( 2011 )    pringle , s.d . , giles , k. , wildgoose , j.l . ,",
    "williams , j.p . ,",
    "slade , s.e .",
    ", thalassinos , k. , bateman , r.h . , bowers , m.t . ,",
    "scrivens , j.h . :",
    "an investigation of the mobility separation of some peptide and protein ions using a new hybrid quadrupole / travelling wave ims / oa - tof instrument . international journal of mass spectrometry * 261*(1 ) , 112 ( 2007 )          serang , o. , moruz , l. , r. , m.h . ,",
    "kll , l. : recognizing uncertainty increases robustness and reproducibility of mass spectrometry - based protein inferences .",
    "journal of proteome research * 11*(12 ) , 558691 ( 2012 )    serang , o. , noble , w.s . : faster mass spectrometry - based protein inference : junction trees are more efficient than sampling and marginalization by enumeration . computational biology and bioinformatics , ieee / acm transactions on * 9*(3 ) , 809817 ( 2012 )"
  ],
  "abstract_text": [
    "<S> observations depending on sums of random variables are common throughout many fields ; however , no efficient solution is currently known for performing max - product inference on these sums of general discrete distributions ( max - product inference can be used to obtain _ maximum a posteriori _ estimates ) . the limiting step to max - product inference </S>",
    "<S> is the max - convolution problem ( sometimes presented in log - transformed form and denoted as `` infimal convolution '' , `` min - convolution '' , or `` convolution on the tropical semiring '' ) , for which no @xmath0 method is currently known . here </S>",
    "<S> i present a @xmath0 numerical method for estimating the max - convolution of two nonnegative vectors ( _ e.g. _ , two probability mass functions ) , where @xmath1 is the length of the larger vector . </S>",
    "<S> this numerical max - convolution method is then demonstrated by performing fast max - product inference on a convolution tree , a data structure for performing fast inference given information on the sum of @xmath2 discrete random variables in @xmath3 steps ( where each random variable has an arbitrary prior distribution on @xmath1 contiguous possible states ) . </S>",
    "<S> the numerical max - convolution method can be applied to specialized classes of hidden markov models to reduce the runtime of computing the viterbi path from @xmath4 to @xmath5 , and has potential application to the all - pairs shortest paths problem . </S>"
  ]
}