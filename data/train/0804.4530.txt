{
  "article_text": [
    "we consider games played between two players on graphs . at every round of the game , each of the two players selects a move ; the moves of the players then determine the transition to the successor state .",
    "a play of the game gives rise to a path on the graph .",
    "we consider two basic goals for the players : _ reachability , _ and _ safety .",
    "_ in the reachability goal , player  1 must reach a set of target states or , if randomization is needed to play the game , then player  1 must maximize the probability of reaching the target set . in the safety goal ,",
    "player  1 must ensure that a set of target states is never left or , if randomization is required , then player  1 must ensure that the probability of leaving the target set is as low as possible .",
    "the two goals are dual , and the games are determined : the maximal probability with which player  1 can reach a target set is equal to one minus the maximal probability with which player  2 can confine the game in the complement set @xcite .",
    "these games on graphs can be divided into two classes : _ turn - based _ and _ concurrent . _ in turn - based games , only one player has a choice of moves at each state ; in concurrent games , at each state both players choose a move , simultaneously and independently , from a set of available moves .    for turn - based games , the solution of games with reachability and safety goals has long been known .",
    "if the move played determines uniquely the successor state , the games can be solved in linear - time in the size of the game graph .",
    "if the move played determines a probability distribution over the successor state , the problem of deciding whether a safety of reachability can be won with probability greater than @xmath2 $ ] is in np @xmath3 co - np @xcite , and the exact value of a game can be computed by strategy improvement algorithms @xcite .",
    "these results all hinge on the fact that turn - based reachability and safety games can be optimally won with deterministic , and memoryless , strategies .",
    "these strategies are functions from states to moves , so they are finite in number , and guarantees the termination of the algorithms .",
    "the situation is different for the concurrent case , where randomization is needed even in the case in which the moves played by the players uniquely determine the successor state .",
    "the _ value _ of the game is defined , as usual , as the sup - inf value : the supremum , over all strategies of player  1 , of the infimum , over all strategies of player  2 , of the probability of achieving the safety or reachability goal . in concurrent reachability games ,",
    "players are only guaranteed the existence of @xmath4-optimal strategies , that ensure that the value of the game is achieved within a specified @xmath5 @xcite ; these strategies ( which depend on @xmath4 ) are memoryless , but in general need randomization @xcite . however , for concurrent safety games memoryless optimal strategies exist  @xcite .",
    "thus , these strategies are mappings from states , to probability distributions over moves .",
    "while complexity results are available for the solution of concurrent reachability and safety games , practical algorithms for their solution , that can provide both a value , and an estimated error , have so far been lacking .",
    "the question of whether the value of a concurrent reachability or safety game is at least @xmath2 $ ] can be decided in pspace via a reduction to the theory of the real closed field @xcite .",
    "this yields a binary - search algorithm to approximate the value .",
    "this approach is theoretical , but complex due to the complex decision algorithms for the theory of reals .    thus far , the only practical approach to the solution of concurrent safety and reachability games has been via value iteration , and via strategy improvement for reachability games . in @xcite",
    "it was shown how to construct a series of valuations that approximates from below , and converges , to the value of a reachability game ; the same algorithm provides valuations converging from above to the value of a safety game . in @xcite , it was shown how to construct a series of strategies for reachability games that converge towards optimality . neither scheme is guaranteed to terminate , not even strategy improvement , since in general only @xmath4-optimal strategies are guaranteed to exist .",
    "both of these approximation schemes lead to practical algorithms . the problem with both schemes ,",
    "however , is that they provide only _ lower _ bounds for the value of reachability games , and only _ upper _ bounds for the value of safety games . as no bounds are available for the speed of convergence of these algorithms , the question of how to derive the matching bounds",
    "has so far been open .    in this paper , we present the first strategy improvement algorithm for the solution of concurrent safety games . given a safety goal for player  1 , the algorithm computes a sequence of memoryless , randomized strategies @xmath6 for player  1 that converge towards optimality .",
    "albeit memoryless randomized optimal strategies exist for safety goals @xcite , the strategy improvement algorithm may not converge in finitely many iterations : indeed , optimal strategies may require moves to be played with irrational probabilities , while the strategies produced by the algorithm play moves with probabilities that are rational numbers .",
    "the main significance of the algorithm is that it provides a converging sequence of _ lower _ bounds for the value of a safety game , and dually , of _ upper _ bounds for the value of a reachability game . to obtain such bounds",
    ", it suffices to compute the value @xmath7 provided by @xmath8 at a state @xmath9 , for @xmath10 .",
    "once @xmath8 is fixed , the game is reduced to a markov decision process , and the value @xmath7 of the safety game can be computed at all @xmath9 e.g.  via linear programming @xcite .",
    "thus , together with the value or strategy improvement algorithms of @xcite , the algorithm presented in this paper provides the first practical way of computing converging lower and upper bounds for the values of concurrent reachability and safety games .",
    "we also present a detailed analysis of termination criteria for turn - based stochastic games , and obtain an improved upper bound for termination for turn - based stochastic games .    the strategy improvement algorithm for reachability games of @xcite",
    "is based on locally improving the strategy on the basis of the valuation it yields .",
    "this approach does not suffice for safety games : the sequence of strategies obtained would yield increasing values to player  1 , but these value would not necessarily converge to the value of the game . in this paper , we introduce a novel , and non - local , improvement step , which augments the standard value - based improvement step .",
    "the non - local step involves the analysis of an appropriately - constructed turn - based game . as value iteration for safety games converges from above , while our sequences of strategies yields values that converge from below , the proof of convergence for our algorithm can not be derived from a connection with value iteration , as was the case for reachability games .",
    "thus , we developed new proof techniques to show both the monotonicity of the strategy values produced by our algorithm , and to show convergence to the value of the game .",
    "[ [ notation . ] ] notation .",
    "+ + + + + + + + +    for a countable set  @xmath11 , a _ probability distribution _ on @xmath11 is a function @xmath12 $ ] such that @xmath13 .",
    "we denote the set of probability distributions on @xmath11 by @xmath14 . given a distribution @xmath15 , we denote by @xmath16 the support set of @xmath17 .    a ( two - player ) _ concurrent game structure _ @xmath18 consists of the following components :    * a finite state space @xmath19 and a finite set @xmath20 of moves or actions . * two move assignments @xmath21 .",
    "for @xmath22 , assignment @xmath23 associates with each state @xmath24 a nonempty set @xmath25 of moves available to player @xmath26 at state @xmath9 .",
    "* a probabilistic transition function @xmath27 that gives the probability @xmath28 of a transition from @xmath9 to @xmath29 when player  1 chooses at state @xmath9 move @xmath30 and player  2 chooses move @xmath31 , for all @xmath32 and @xmath33 , @xmath34 .",
    "we denote by @xmath35 the size of transition function , i.e. , @xmath36 .",
    "we denote by @xmath37 the size of the game graph , and @xmath38 . at every state @xmath39 ,",
    "player  1 chooses a move @xmath40 , and simultaneously and independently player  2 chooses a move @xmath41 .",
    "the game then proceeds to the successor state @xmath29 with probability @xmath42 , for all @xmath43 .",
    "a state @xmath9 is an _ absorbing state _ if for all @xmath33 and @xmath34 , we have @xmath44 . in other words , at an absorbing state @xmath9 for all choices of moves of the two players , the successor state is always @xmath9 .    a _ turn - based stochastic game graph _ ( _ @xmath45-player game graph _ ) @xmath46 consists of a finite directed graph @xmath47 , a partition @xmath48 , @xmath49 , @xmath50 of the finite set @xmath19 of states , and a probabilistic transition function @xmath17 : @xmath51 , where @xmath52 denotes the set of probability distributions over the state space  @xmath19 .",
    "the states in @xmath53 are the _",
    "player-@xmath54 _ states , where player  @xmath54 decides the successor state ; the states in @xmath49 are the _",
    "player-@xmath55 _ states , where player  @xmath55 decides the successor state ; and the states in @xmath56 are the _ random or probabilistic _ states , where the successor state is chosen according to the probabilistic transition function  @xmath17 .",
    "we assume that for @xmath57 and @xmath43 , we have @xmath58 iff @xmath59 , and we often write @xmath60 for @xmath61 . for technical convenience",
    "we assume that every state in the graph @xmath47 has at least one outgoing edge . for a state @xmath39",
    ", we write @xmath62 to denote the set @xmath63 of possible successors .",
    "we denote by @xmath35 the size of the transition function , i.e. , latexmath:[$|\\trans|=\\sum_{s\\in s_r , t\\in s }     bits required to specify the transition probability @xmath61 .",
    "we denote by @xmath37 the size of the game graph , and @xmath65 .",
    "[ [ plays . ] ] plays .",
    "+ + + + + +    a _ play _ @xmath66 of @xmath67 is an infinite sequence @xmath68 of states in @xmath19 such that for all @xmath69 , there are moves @xmath70 and @xmath71 with @xmath72 .",
    "we denote by @xmath73 the set of all plays , and by @xmath74 the set of all plays @xmath75 such that @xmath76 , that is , the set of plays starting from state  @xmath9 .",
    "[ [ selectors - and - strategies . ] ] selectors and strategies .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + +    a _ selector _",
    "@xmath77 for player @xmath78 is a function @xmath79 such that for all states @xmath24 and moves @xmath80 , if @xmath81 , then @xmath82 .",
    "a selector @xmath77 for player @xmath26 at a state @xmath9 is a distribution over moves such that if @xmath83 , then @xmath82 . we denote by @xmath84 the set of all selectors for player  @xmath78 , and similarly , we denote by @xmath85 the set of all selectors for player  @xmath26 at a state @xmath9 .",
    "the selector @xmath77 is _ pure _ if for every state @xmath24 , there is a move @xmath80 such that @xmath86 . a _",
    "strategy _ for player @xmath87 is a function @xmath88 that associates with every finite , nonempty sequence of states , representing the history of the play so far , a selector for player  @xmath26 ; that is , for all @xmath89 and @xmath24 , we have @xmath90 . the strategy @xmath91 is _ pure _ if it always chooses a pure selector ; that is , for all @xmath92 , there is a move @xmath80 such that @xmath93 . a _",
    "strategy is independent of the history of the play and depends only on the current state .",
    "memoryless strategies correspond to selectors ; we write @xmath94 for the memoryless strategy consisting in playing forever the selector @xmath77 .",
    "a strategy is _ pure memoryless _ if it is both pure and memoryless . in a turn - based stochastic game ,",
    "a strategy for player  1 is a function @xmath95 , such that for all @xmath89 and for all @xmath96 we have @xmath97 .",
    "memoryless strategies and pure memoryless strategies are obtained as the restriction of strategies as in the case of concurrent game graphs .",
    "the family of strategies for player  2 are defined analogously .",
    "we denote by @xmath98 and @xmath99 the sets of all strategies for player @xmath54 and player @xmath55 , respectively .",
    "we denote by @xmath100 and @xmath101 the sets of memoryless strategies and pure memoryless strategies for player  @xmath26 , respectively .",
    "[ [ destinations - of - moves - and - selectors . ] ] destinations of moves and selectors .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    for all states @xmath24 and moves @xmath33 and @xmath34 , we indicate by @xmath102 the set of possible successors of @xmath9 when the moves @xmath30 and @xmath31 are chosen . given a state @xmath9 , and selectors @xmath103 and @xmath104 for the two players , we denote by @xmath105 the set of possible successors of @xmath9 with respect to the selectors @xmath103 and @xmath104 .",
    "once a starting state @xmath9 and strategies @xmath106 and @xmath107 for the two players are fixed , the game is reduced to an ordinary stochastic process .",
    "hence , the probabilities of events are uniquely defined , where an _ event _ @xmath108 is a measurable set of plays . for an event @xmath108 ,",
    "we denote by @xmath109 the probability that a play belongs to @xmath110 when the game starts from @xmath9 and the players follows the strategies @xmath106 and  @xmath107 .",
    "similarly , for a measurable function @xmath111 , we denote by @xmath112 the expected value of @xmath113 when the game starts from @xmath9 and the players follow the strategies @xmath106 and  @xmath107 . for @xmath114 , we denote by @xmath115 the random variable denoting the @xmath26-th state along a play .    [ [ valuations . ] ] valuations .",
    "+ + + + + + + + + + +    a _ valuation _ is a mapping @xmath116 $ ] associating a real number @xmath117 $ ] with each state @xmath9 . given two valuations @xmath118 ,",
    "we write @xmath119 when @xmath120 for all states @xmath24 . for an event @xmath110 , we denote by @xmath121 the valuation @xmath122 $ ] defined for all states @xmath24 by @xmath123 .",
    "similarly , for a measurable function @xmath124 $ ] , we denote by @xmath125 the valuation @xmath122 $ ] defined for all @xmath24 by @xmath126 .",
    "[ [ reachability - and - safety - objectives . ] ] reachability and safety objectives .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    given a set @xmath127 of _ safe _ states , the objective of a safety game consists in never leaving @xmath0 .",
    "therefore , we define the set of winning plays as the set @xmath128 . given a subset @xmath129 of _ target _ states , the objective of a reachability game consists in reaching @xmath130 .",
    "correspondingly , the set winning plays is @xmath131 of plays that visit @xmath130 .",
    "for all @xmath127 and @xmath129 , the sets @xmath132 and @xmath133 is measurable .",
    "an objective in general is a measurable set , and in this paper we would consider only reachability and safety objectives . for an objective @xmath134 , the probability of satisfying @xmath134 from a state @xmath24 under strategies @xmath106 and @xmath107 for players  1 and  2 , respectively , is @xmath135 .",
    "we define the _ value _ for player  1 of game with objective @xmath134 from the state @xmath24 as @xmath136 i.e. , the value is the maximal probability with which player  1 can guarantee the satisfaction of @xmath134 against all player  2 strategies .",
    "given a player-1 strategy @xmath106 , we use the notation @xmath137 a strategy @xmath106 for player  1 is _ optimal _ for an objective @xmath134 if for all states @xmath24 , we have @xmath138 for @xmath139 , a strategy @xmath106 for player  1 is _ @xmath140-optimal _ if for all states @xmath24 , we have @xmath141 the notion of values and optimal strategies for player  2 are defined analogously .",
    "reachability and safety objectives are dual , i.e. , we have @xmath142 . the quantitative determinacy result of",
    "@xcite ensures that for all states @xmath24 , we have @xmath143    [ thrm : memory - determinacy ] for all concurrent game graphs @xmath144 , for all @xmath145 , such that @xmath146 , the following assertions hold .    1 .",
    "@xcite memoryless optimal strategies exist for safety objectives @xmath132 .",
    "@xcite for all @xmath147 , memoryless @xmath140-optimal strategies exist for reachability objectives @xmath133 .",
    "3 .   @xcite if @xmath144 is a turn - based stochastic game graph , then pure memoryless optimal strategies exist for reachability objectives @xmath133 and safety objectives @xmath132 .",
    "to develop our arguments , we need some facts about one - player versions of concurrent stochastic games , known as _ markov decision processes _ ( mdps ) @xcite . for @xmath78 , a _",
    "player-@xmath26 mdp _",
    "( for short , @xmath26-mdp ) is a concurrent game where , for all states @xmath24 , we have @xmath148 . given a concurrent game @xmath144 , if we fix a memoryless strategy corresponding to selector @xmath103 for player  1 , the game is equivalent to a 2-mdp @xmath149 with the transition function @xmath150 for all @xmath24 and @xmath34 .",
    "similarly , if we fix selectors @xmath103 and @xmath104 for both players in a concurrent game @xmath144 , we obtain a markov chain , which we denote by @xmath151 .    [",
    "[ end - components . ] ] end components .",
    "+ + + + + + + + + + + + + + +    in an mdp , the sets of states that play an equivalent role to the closed recurrent classes of markov chains @xcite are called `` end components '' @xcite .    an _ end component _ of an @xmath26-mdp @xmath144 , for @xmath87 ,",
    "is a subset @xmath152 of the states such that there is a selector @xmath77 for player  @xmath26 so that @xmath153 is a closed recurrent class of the markov chain @xmath154 .",
    "it is not difficult to see that an equivalent characterization of an end component @xmath153 is the following . for each state @xmath155",
    ", there is a subset @xmath156 of moves such that :    1 .   _",
    "( closed ) _ if a move in @xmath157 is chosen by player @xmath26 at state @xmath9 , then all successor states that are obtained with nonzero probability lie in @xmath153 ; and 2 .   _ ( recurrent ) _ the graph @xmath158 , where @xmath159 consists of the transitions that occur with nonzero probability when moves in @xmath160 are chosen by player @xmath26 , is strongly connected .    given a play @xmath161 , we denote by @xmath162 the set of states that occurs infinitely often along @xmath66 . given a set @xmath163 of subsets of states , we denote by @xmath164 the event @xmath165 .",
    "the following theorem states that in a 2-mdp , for every strategy of player  2 , the set of states that are visited infinitely often is , with probability  1 , an end component .",
    "corollary  [ coro : prob1 ] follows easily from theorem  [ theo - ec ] .",
    "@xcite [ theo - ec ] for a player-1 selector @xmath103 , let @xmath166 be the set of end components of a 2-mdp @xmath149 . for all player-2 strategies @xmath107 and all states @xmath24 , we have @xmath167 .",
    "[ coro : prob1 ] for a player-1 selector @xmath103 , let @xmath166 be the set of end components of a 2-mdp @xmath149 , and let @xmath168 be the set of states of all end components . for all player-2 strategies",
    "@xmath107 and all states @xmath24 , we have @xmath169 .    [",
    "[ subsec : mdpreach ] ] mdps with reachability objectives .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    given a 2-mdp with a reachability objective @xmath133 for player  2 , where @xmath170 , the values can be obtained as the solution of a linear program  @xcite .",
    "the linear program has a variable @xmath171 for all states @xmath24 , and the objective function and the constraints are as follows : @xmath172 @xmath173 the correctness of the above linear program to compute the values follows from  @xcite .",
    "in this section we present a strategy improvement algorithm for concurrent games with safety objectives . the algorithm will produce a sequence of selectors @xmath174 for player 1 , such that :    1 .",
    "[ l - improve-1 ] for all @xmath114 , we have @xmath175 ; 2 .",
    "[ l - improve-3 ] if there is @xmath114 such that @xmath176 , then @xmath177 ; and 3 .",
    "[ l - improve-2 ] @xmath178 .",
    "condition  [ l - improve-1 ] guarantees that the algorithm computes a sequence of monotonically improving selectors .",
    "condition  [ l - improve-3 ] guarantees that if a selector can not be improved , then it is optimal .",
    "condition  [ l - improve-2 ] guarantees that the value guaranteed by the selectors converges to the value of the game , or equivalently , that for all @xmath139 , there is a number @xmath26 of iterations such that the memoryless player-1 strategy @xmath179 is @xmath140-optimal .",
    "note that for concurrent safety games , there may be no @xmath180 such that @xmath176 , that is , the algorithm may fail to generate an optimal selector .",
    "this is because there are concurrent safety games such that the values are irrational  @xcite .",
    "we start with a few notations    * the @xmath181 operator and optimal selectors . * given a valuation @xmath182 , and two selectors @xmath183 and @xmath184 , we define the valuations @xmath185 , @xmath186 , and @xmath187 as follows , for all states @xmath24 : @xmath188 intuitively , @xmath189 is the greatest expectation of @xmath182 that player  1 can guarantee at a successor state of @xmath9 . also note that given a valuation @xmath182 , the computation of @xmath187 reduces to the solution of a zero - sum one - shot matrix game , and can be solved by linear programming .",
    "similarly , @xmath190 is the greatest expectation of @xmath182 that player  1 can guarantee at a successor state of @xmath9 by playing the selector @xmath103 .",
    "note that all of these operators on valuations are monotonic : for two valuations @xmath191 , if @xmath119 , then for all selectors @xmath183 and @xmath184 , we have @xmath192 , @xmath193 , and @xmath194 . given a valuation @xmath182 and a state @xmath9 , we define by @xmath195 the set of optimal selectors for @xmath182 at state @xmath9 .",
    "for an optimal selector @xmath196 , we define the set of counter - optimal actions as follows : @xmath197 observe that for @xmath196 , for all @xmath198 we have @xmath199 .",
    "we define the set of optimal selector support and the counter - optimal action set as follows : @xmath200 i.e. , it consists of pairs @xmath201 of actions of player  1 and player  2 , such that there is an optimal selector @xmath103 with support @xmath11 , and @xmath202 is the set of counter - optimal actions to @xmath103 .",
    "* turn - based reduction . * given a concurrent game @xmath203 and a valuation @xmath182 we construct a turn - based stochastic game @xmath204 as follows :    1 .",
    "the set of states is as follows : @xmath205 2 .",
    "the state space partition is as follows : @xmath206 ; @xmath207 ; and @xmath208 3 .",
    "the set of edges is as follows : @xmath209 4 .",
    "the transition function @xmath210 for all states in @xmath211 is uniform over its successors .",
    "intuitively , the reduction is as follows . given the valuation @xmath182 , state @xmath9 is a player  1 state where player  1 can select a pair @xmath201 ( and move to state @xmath212 ) with @xmath213 and @xmath214 such that there is an optimal selector @xmath103 with support exactly @xmath11 and the set of counter - optimal actions to @xmath103 is the set @xmath202 . from a player  2 state @xmath212 ,",
    "player  2 can choose any action @xmath215 from the set @xmath202 , and move to state @xmath216 .",
    "a state @xmath216 is a probabilistic state where all the states in @xmath217 are chosen uniformly at random .",
    "given a set @xmath218 we denote by @xmath219 .",
    "we refer to the above reduction as @xmath220 , i.e. , @xmath221 .",
    "* value - class of a valuation . * given a valuation @xmath182 and a real @xmath222 , the _ value - class _ @xmath223 of value @xmath224 is the set of states with valuation @xmath224 , i.e. , @xmath225      [ [ ordering - of - strategies . ] ] ordering of strategies .",
    "+ + + + + + + + + + + + + + + + + + + + + + +    let @xmath144 be a concurrent game and @xmath0 be the set of safe states .",
    "let @xmath226 .",
    "given a concurrent game graph @xmath144 with a safety objective @xmath132 , the set of _ almost - sure winning _ states is the set of states @xmath9 such that the value at @xmath9 is  @xmath54 , i.e. , @xmath227 is the set of almost - sure winning states .",
    "an optimal strategy from @xmath228 is referred as an almost - sure winning strategy .",
    "the set @xmath228 and an almost - sure winning strategy can be computed in linear time by the algorithm given in  @xcite .",
    "we assume without loss of generality that all states in @xmath229 are absorbing .",
    "we define a preorder @xmath230 on the strategies for player 1 as follows : given two player 1 strategies @xmath106 and @xmath231 , let @xmath232 if the following two conditions hold : ( i )  @xmath233 ; and ( ii )  @xmath234 for some state @xmath39 . furthermore , we write @xmath235 if either @xmath232 or @xmath236 .",
    "we first present an example that shows the improvements based only on @xmath237 operators are not sufficient for safety games , even on turn - based games and then present our algorithm .",
    "[ examp : conc - safety ] consider the turn - based stochastic game shown in fig  [ fig : example - tbs ] , where the @xmath238 states are player  1 states , the @xmath239 states are player  2 states , and @xmath240 states are random states with probabilities labeled on edges .",
    "the safety goal is to avoid the state @xmath241 .",
    "consider a memoryless strategy @xmath106 for player  1 that chooses the successor @xmath242 , and the counter - strategy @xmath107 for player  2 chooses @xmath243 .",
    "given the strategies @xmath106 and @xmath107 , the value at @xmath244 and @xmath245 is @xmath246 , and since all successors of @xmath247 have value @xmath246 , the value can not be improved by @xmath237",
    ". however , note that if player  2 is restricted to choose only value optimal selectors for the value @xmath246 , then player  1 can switch to the strategy @xmath248 and ensure that the game stays in the value class @xmath246 with probability  1 . hence switching to @xmath249 would force player  2 to select a counter - strategy that switches to the strategy @xmath250 , and thus player  1 can get a value @xmath251 .    [",
    "[ informal - description - of - algorithmalgorithmstrategy - improve - safe . ] ] informal description of algorithm  [ algorithm : strategy - improve - safe ] .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we now present the strategy improvement algorithm ( algorithm  [ algorithm : strategy - improve - safe ] ) for computing the values for all states in @xmath252 .",
    "the algorithm iteratively improves player-1 strategies according to the preorder @xmath230 .",
    "the algorithm starts with the random selector @xmath253 that plays at all states all actions uniformly at random . at iteration @xmath254",
    ", the algorithm considers the memoryless player-1 strategy @xmath255 and computes the value @xmath256 .",
    "observe that since @xmath255 is a memoryless strategy , the computation of @xmath256 involves solving the 2-mdp @xmath257 .",
    "the valuation @xmath256 is named @xmath258 . for all states @xmath9 such that @xmath259 , the memoryless strategy at @xmath9 is modified to a selector that is value - optimal for @xmath258 .",
    "the algorithm then proceeds to the next iteration .",
    "if @xmath260 , then the algorithm constructs the game @xmath261 , and computes @xmath262 as the set of almost - sure winning states in @xmath263 for the objective @xmath264 .",
    "let @xmath265 .",
    "if @xmath266 is non - empty , then a selector @xmath267 is obtained at @xmath266 from an pure memoryless optimal strategy ( i.e. , an almost - sure winning strategy ) in @xmath263 , and the algorithm proceeds to iteration @xmath254 . if @xmath268 and @xmath266 is empty , then the algorithm stops and returns the memoryless strategy @xmath255 for player  1 .",
    "unlike strategy improvement algorithms for turn - based games ( see @xcite for a survey ) , algorithm  [ algorithm : strategy - improve - safe ] is not guaranteed to terminate , because the value of a safety game may not be rational .",
    "aaa = aaa = aaa = aaa = aaa = aaa = aaa = aaa + a concurrent game structure @xmath144 with safe set @xmath0 .",
    "+ a strategy @xmath269 for player  1 .",
    "compute @xmath270 .",
    "let @xmath271 and @xmath272 .",
    "compute @xmath273 .",
    "do \\ { *   + 3.1 .",
    "let @xmath274 .",
    "+ 3.2 * if * @xmath275 , * then * + 3.2.1 let @xmath103 be a player-1 selector such that for all states @xmath276 , + we have @xmath277 .",
    "+ 3.2.2 the player-1 selector @xmath267 is defined as follows : for each state @xmath278 , let + @xmath279 + 3.3 * else * + 3.3.1 let@xmath261 + 3.3.2 let @xmath262 be the set of almost - sure winning states in @xmath263 for @xmath264 and + @xmath280 be a pure memoryless almost - sure winning strategy from the set @xmath262 .",
    "+ 3.3.3 * if * ( @xmath281 ) + 3.3.3.1 let @xmath282 + 3.3.3.2 the player-1 selector @xmath267 is defined as follows : for @xmath278 , let + @xmath283 + 3.4 .",
    "compute @xmath284 .",
    "let @xmath285 .",
    "+ @xmath286 and @xmath287 .",
    "return * @xmath288 .",
    "[ lemm : stra - improve - safe1 ] let @xmath289 and @xmath267 be the player-1 selectors obtained at iterations @xmath26 and @xmath254 of algorithm  [ algorithm : strategy - improve - safe ] .",
    "let @xmath290 .",
    "let @xmath291 and @xmath292 .",
    "then @xmath293 for all states @xmath39 ; and therefore @xmath294 for all states @xmath39 , and @xmath295 for all states @xmath296 .",
    "consider the valuations @xmath258 and @xmath297 obtained at iterations @xmath26 and @xmath254 , respectively , and let @xmath298 be the valuation defined by @xmath299 for all states @xmath24 .",
    "the counter - optimal strategy for player  2 to minimize @xmath297 is obtained by maximizing the probability to reach @xmath130 .",
    "let @xmath300 in other words , @xmath301 , and we also have @xmath302 .",
    "we now show that @xmath303 is a feasible solution to the linear program for mdps with the objective @xmath133 , as described in section  [ sec : mdp ] . since @xmath304",
    ", it follows that for all states @xmath39 and all moves @xmath34 , we have @xmath305 for all states",
    "@xmath306 , we have @xmath307 and @xmath308 , and since @xmath302 , it follows that for all states @xmath306 and all moves @xmath34 , we have @xmath309    since for @xmath276 the selector @xmath310 is obtained as an optimal selector for @xmath311 , it follows that for all states @xmath296 and all moves @xmath34 , we have @xmath312 in other words , @xmath313 .",
    "hence for all states @xmath296 and all moves @xmath34 , we have @xmath314 since @xmath302 , for all states @xmath296 and all moves @xmath34 , we have @xmath315 hence it follows that @xmath303 is a feasible solution to the linear program for mdps with reachability objectives .",
    "since the reachability valuation for player  2 for @xmath133 is the least solution ( observe that the objective function of the linear program is a minimizing function ) , it follows that @xmath316 .",
    "thus we obtain @xmath294 for all states @xmath24 , and @xmath295 for all states @xmath276 .",
    "recall that by example  [ examp : conc - safety ] it follows that improvement by only step  3.2 is not sufficient to guarantee convergence to optimal values .",
    "we now present a lemma about the turn - based reduction , and then show that step 3.3 also leads to an improvement .",
    "finally , in theorem  [ thrm : safe - termination ] we show that if improvements by step 3.2 and step 3.3 are not possible , then the optimal value and an optimal strategy is obtained .",
    "[ lemm : stra - improve - safetb ] let @xmath144 be a concurrent game with a set @xmath0 of safe states .",
    "let @xmath182 be a valuation and consider @xmath221 .",
    "let @xmath317 be the set of almost - sure winning states in @xmath318 for the objective @xmath264 , and let @xmath280 be a pure memoryless almost - sure winning strategy from @xmath317 in @xmath318 .",
    "consider a memoryless strategy @xmath106 in @xmath144 for states in @xmath319 as follows : if @xmath320 , then @xmath321 such that @xmath322 and @xmath323 .",
    "consider a pure memoryless strategy @xmath107 for player  2 . if for all states @xmath324 , we have @xmath325 , then for all @xmath324 , we have @xmath326 .",
    "we analyze the markov chain arising after the player fixes the memoryless strategies @xmath106 and @xmath107 .",
    "given the strategy @xmath107 consider the strategy @xmath327 as follows : if @xmath320 and @xmath328 , then at state @xmath212 choose the successor @xmath216 . since @xmath280 is an almost - sure winning strategy for @xmath264 , it follows that in the markov chain obtained by fixing @xmath280 and @xmath327 in @xmath318 , all closed connected recurrent set of states that intersect with @xmath317 are contained in @xmath317 , and from all states of @xmath317 the closed connected recurrent set of states within @xmath317 are reached with probability  1 .",
    "it follows that in the markov chain obtained from fixing @xmath106 and @xmath107 in @xmath144 all closed connected recurrent set of states that intersect with @xmath329 are contained in @xmath330 , and from all states of @xmath319 the closed connected recurrent set of states within @xmath330 are reached with probability  1 .",
    "the desired result follows .",
    "[ lemm : stra - improve - safe2 ] let @xmath289 and @xmath267 be the player-1 selectors obtained at iterations @xmath26 and @xmath254 of algorithm  [ algorithm : strategy - improve - safe ] .",
    "let @xmath331 , and @xmath332 .",
    "let @xmath291 and @xmath292 .",
    "then @xmath294 for all states @xmath39 , and @xmath295 for some state @xmath333 .",
    "we first show that @xmath334 .",
    "let @xmath335 .",
    "let @xmath299 for all states @xmath24 .",
    "since @xmath304 , it follows that for all states @xmath39 and all moves @xmath34 , we have @xmath305 the selector @xmath336 chosen for @xmath267 at @xmath337 satisfies that @xmath338 .",
    "it follows that for all states @xmath39 and all moves @xmath34 , we have",
    "@xmath339 it follows that the maximal probability with which player  2 can reach @xmath130 against the strategy @xmath340 is at most @xmath298 .",
    "it follows that @xmath341 .",
    "we now argue that for some state @xmath337 we have @xmath342 .",
    "given the strategy @xmath340 , consider a pure memoryless counter - optimal strategy @xmath107 for player  2 to reach @xmath130 . since the selectors @xmath310 at states @xmath343 are obtained from the almost - sure strategy @xmath344 in the turn - based game @xmath263 to satisfy @xmath264 , it follows from lemma  [ lemm : stra - improve - safetb ] that if for every state @xmath337 , the action @xmath345 , then from all states @xmath337 , the game stays safe in @xmath0 with probability  1 .",
    "since @xmath340 is a given strategy for player  1 , and @xmath107 is counter - optimal against @xmath340 , this would imply that @xmath346 .",
    "this would contradict that @xmath227 and @xmath347 .",
    "it follows that for some state @xmath348 we have @xmath349 , and since @xmath350 we have @xmath351 in other words , we have @xmath352 define a valuation @xmath353 as follows : @xmath354 for @xmath355 , and @xmath356 . hence @xmath357 , and given the strategy @xmath340 and the counter - optimal strategy @xmath107 , the valuation @xmath353 satisfies the inequalities of the linear - program for reachability to @xmath130 .",
    "it follows that the probability to reach @xmath130 given @xmath340 is at most @xmath353 . since @xmath357",
    ", it follows that @xmath294 for all @xmath39 , and @xmath358 .",
    "this concludes the proof .",
    "we obtain the following theorem from lemma  [ lemm : stra - improve - safe1 ] and lemma  [ lemm : stra - improve - safe2 ] that shows that the sequences of values we obtain is monotonically non - decreasing .",
    "[ thrm : safe - mono ] for @xmath359 , let @xmath360 and @xmath267 be the player-1 selectors obtained at iterations @xmath26 and @xmath254 of algorithm  [ algorithm : strategy - improve - safe ] .",
    "if @xmath361 , then @xmath362 .",
    "[ thrm : safe - termination ] let @xmath258 be the valuation at iteration @xmath26 of algorithm  [ algorithm : strategy - improve - safe ] such that @xmath363 .",
    "if @xmath364 , and @xmath365 , then @xmath179 is an optimal strategy and @xmath366 .",
    "we show that for all memoryless strategies @xmath106 for player  1 we have @xmath367 . since memoryless optimal strategies exist for concurrent games with safety objectives ( theorem  [ thrm : memory - determinacy ] ) the desired result follows .",
    "let @xmath327 be a pure memoryless optimal strategy for player  2 in @xmath263 for the objective complementary to @xmath264 , where @xmath368 .",
    "consider a memoryless strategy @xmath106 for player  1 , and we define a pure memoryless strategy @xmath107 for player  2 as follows .    1 .   if @xmath369 , then @xmath370 , such that @xmath371 ; ( such a @xmath215 exists since @xmath369 ) .",
    "if @xmath372 , then let @xmath373 , and consider @xmath202 such that @xmath374 . then we have @xmath375 , such that @xmath376 .    observe that by construction of @xmath107 , for all @xmath377 , we have @xmath378 . we first show that in the markov chain obtained by fixing @xmath106 and @xmath107 in @xmath144 , there is no closed connected recurrent set of states @xmath153 such that @xmath379 .",
    "assume towards contradiction that @xmath153 is a closed connected recurrent set of states in @xmath380 .",
    "the following case analysis achieves the contradiction .    1 .",
    "suppose for every state @xmath155 we have @xmath372 .",
    "then consider the strategy @xmath280 in @xmath263 such that for a state @xmath155 we have @xmath320 , where @xmath381 , and @xmath374 .",
    "since @xmath153 is closed connected recurrent states , it follows by construction that for all states @xmath155 in the game @xmath263 we have @xmath382 , where @xmath383 .",
    "it follows that for all @xmath155 in @xmath263 we have @xmath384 .",
    "since @xmath327 is an optimal strategy , it follows that @xmath385 .",
    "this contradicts that @xmath386 .",
    "2 .   otherwise for some state @xmath387 we have @xmath388 .",
    "let @xmath389 , i.e. , @xmath224 is the least value - class with non - empty intersection with @xmath153 .",
    "hence it follows that for all @xmath390 , we have @xmath391 . observe that since for all @xmath155 we have @xmath378 , it follows that for all @xmath392 either ( a )  @xmath393 ; or ( b )  @xmath394 , for some @xmath390 .",
    "since @xmath395 is the least value - class with non - empty intersection with @xmath153 , it follows that for all @xmath392 we have @xmath396 .",
    "it follows that @xmath397 .",
    "consider the state @xmath387 such that @xmath398 . by the construction of @xmath399",
    ", we have @xmath400 .",
    "hence we must have @xmath401 , for some @xmath402 .",
    "thus we have a contradiction .",
    "it follows from above that there is no closed connected recurrent set of states in @xmath403 , and hence with probability  1 the game reaches @xmath404 from all states in @xmath380 . hence the probability to satisfy",
    "@xmath132 is equal to the probability to reach @xmath228 . since for all states @xmath405 we have @xmath378",
    ", it follows that given the strategies @xmath106 and @xmath107 , the valuation @xmath258 satisfies all the inequalities for linear program to reach @xmath228 .",
    "it follows that the probability to reach @xmath228 from @xmath9 is atmost @xmath406 .",
    "it follows that for all @xmath405 we have @xmath407 . the result follows .",
    "* convergence .",
    "* we first observe that since pure memoryless optimal strategies exist for turn - based stochastic games with safety objectives ( theorem  [ thrm : memory - determinacy ] ) , for turn - based stochastic games it suffices to iterate over pure memoryless selectors .",
    "since the number of pure memoryless strategies is finite , it follows for turn - based stochastic games algorithm  [ algorithm : strategy - improve - safe ] always terminates and yields an optimal strategy . for concurrent games",
    ", we will use the result that for @xmath147 , there is a _ @xmath408-uniform memoryless _ strategy that achieves the value of a safety objective with in @xmath140 .",
    "we first define @xmath408-uniform memoryless strategies .",
    "a selector @xmath77 for player  1 is _",
    "if for all @xmath409 and all @xmath410 there exists @xmath411 such that @xmath412 and @xmath413 , i.e. , the moves in the support are played with probability that are multiples of @xmath414 with @xmath415 .",
    "[ lemm : kuniform ] for all concurrent game graphs @xmath144 , for all safety objectives @xmath132 , for @xmath218 , for all @xmath147 , there exist @xmath408-uniform selectors @xmath77 such that @xmath94 is an @xmath140-optimal strategy for @xmath416 , where @xmath417 .    _",
    "( sketch ) .",
    "_ for a rational @xmath224 , using the results of  @xcite , it can be shown that whether @xmath418 can be expressed in the quantifier free fragment of the theory of reals . then using the formula in the theory of reals and theorem  13.12 of  @xcite",
    ", it can be shown that if there is a memoryless strategy @xmath106 that achieves value at least @xmath224 , then there is a @xmath408-uniform memoryless strategy @xmath419 that achieves value at least @xmath420 , where @xmath416 , for @xmath417 .    * strategy improvement with @xmath408-uniform selectors .",
    "* we first argue that if we restrict algorithm  [ algorithm : strategy - improve - safe ] such that every iteration yields a @xmath408-uniform selector , then the algorithm terminates .",
    "if we restrict to @xmath408-uniform selectors , then a concurrent game graph @xmath144 can be converted to a turn - based stochastic game graph , where player  1 first chooses a @xmath408-uniform selector , then player  2 chooses an action , and then the transition is determined by the chosen @xmath408-uniform selector of player  1 , the action of player  2 and the transition function @xmath17 of the game graph @xmath144 . then by termination of turn - based stochastic games it follows that the algorithm will terminate . given @xmath408 ,",
    "let us denote by @xmath421 the valuation of algorithm  [ algorithm : strategy - improve - safe ] at iteration @xmath26 , where the selectors are restricted to be @xmath408-uniform , and @xmath258 is the valuation of algorithm  [ algorithm : strategy - improve - safe ] at iteration @xmath26 .",
    "since @xmath258 is obtained without any restriction , it follows that for all @xmath422 , for all @xmath114 , we have @xmath423 . from lemma  [ lemm : kuniform ]",
    "it follows that for all @xmath147 , there exists a @xmath422 and @xmath359 such that for all @xmath9 we have @xmath424 .",
    "this gives us the following result .",
    "let @xmath258 be the valuation obtained at iteration @xmath26 of algorithm  [ algorithm : strategy - improve - safe ] .",
    "then the following assertions hold .    1",
    ".   for all @xmath147 , there exists @xmath26 such that for all @xmath9 we have @xmath425 .",
    "2 .   @xmath426 .",
    "* complexity .",
    "* algorithm  [ algorithm : strategy - improve - safe ] may not terminate in general .",
    "we briefly describe the complexity of every iteration . given a valuation @xmath258 , the computation of @xmath427 involves solution of matrix games with rewards @xmath258 and can be computed in polynomial time using linear - programming . given @xmath258 and @xmath268 , the set @xmath428 and @xmath429 can be computed by enumerating the subsets of available actions at @xmath9 and then using linear - programming : for example to check @xmath430 it suffices to check that there is an selector @xmath103 such that @xmath103 is optimal ( i.e. for all actions @xmath431 we have @xmath432 ) ; for all @xmath433 we have @xmath434 , and for all @xmath435 we have @xmath436 ; and to check @xmath202 is the set of counter - optimal actions we check that for @xmath437 we have @xmath438 ; and for @xmath439 we have @xmath440 .",
    "all the above can be solved by checking feasibility of a set of linear inequalities .",
    "hence @xmath441 can be computed in time polynomial in size of @xmath144 and @xmath258 and exponential in the number of moves .",
    "the set of almost - sure winning states in turn - based stochastic games with safety objectives can be computed in linear - time  @xcite .",
    "in this section we present termination criteria for strategy improvement algorithms for concurrent games for @xmath140-approximation , and then present an improved termination condition for turn - based games .    * termination for concurrent games . * a strategy improvement algorithm for reachability games was presented in  @xcite .",
    "we refer to the algorithm of  @xcite as the _ reachability strategy improvement algorithm_. the reachability strategy improvement algorithm is simpler than algorithm  [ algorithm : strategy - improve - safe ] : it is similar to algorithm  [ algorithm : strategy - improve - safe ] and in every iteration only step  3.2 is executed ( and step 3.3 need not be executed ) . applying the reachability strategy improvement algorithm of  @xcite for player  2 , for a reachability objective @xmath133",
    ", we obtain a sequence of valuations @xmath442 such that ( a ) @xmath443 ; ( b ) if @xmath444 , then @xmath445 ; and ( c ) @xmath446 . given a concurrent game @xmath144 with @xmath127 and @xmath447 , we apply the reachability strategy improvement algorithm to obtain the sequence of valuation @xmath442 as above , and we apply algorithm  [ algorithm : strategy - improve - safe ] to obtain a sequence of valuation @xmath448 .",
    "the termination criteria are as follows :    1 .",
    "if for some @xmath26 we have @xmath444 , then we have @xmath445 , and @xmath449 , and we obtain the values of the game ; 2 .   if for some @xmath26 we have @xmath450 , then we have @xmath451 , and @xmath366 , and we obtain the values of the game ; and 3 .   for @xmath147 , if for some @xmath359",
    ", we have @xmath452 , then for all @xmath24 we have @xmath453 and @xmath454 ( i.e. , the algorithm can stop for @xmath140-approximation ) .",
    "observe that since @xmath442 and @xmath448 are both monotonically non - decreasing and @xmath455 , it follows that if @xmath452 , then forall @xmath456 we have @xmath457 and @xmath458 .",
    "this establishes that @xmath459 and @xmath460 ; and the correctness of the stopping criteria ( 3 ) for @xmath140-approximation follows .",
    "we also note that instead of applying the reachability strategy improvement algorithm , a value - iteration algorithm can be applied for reachability games to obtain a sequence of valuation with properties similar to @xmath461 and the above termination criteria can be applied .",
    "let @xmath144 be a concurrent game graph with a safety objective @xmath132 .",
    "algorithm  [ algorithm : strategy - improve - safe ] and the reachability strategy improvement algorithm for player  2 for the reachability objective @xmath462 yield sequence of valuations @xmath448 and @xmath461 , respectively , such that ( a )  for all @xmath359 , we have @xmath463 ; and ( b )  @xmath464 .    * termination for turn - based games . * for turn - based stochastic games",
    "algorithm  [ algorithm : strategy - improve - safe ] and as well as the reachability strategy improvement algorithm terminates .",
    "each iteration of the reachability strategy improvement algorithm of  @xcite is computable in polynomial time , and here we present a termination guarantee for the reachability strategy improvement algorithm . to apply the reachability strategy improvement algorithm we assume the objective of player  1 to be a reachability objective @xmath133 , and the correctness of the algorithm relies on the notion of _ proper strategies_. let @xmath465 .",
    "then the notion of proper strategies and its properties are as follows .",
    "a player-1 strategy @xmath106 is _ proper _ if for all player-2 strategies @xmath107 , and for all states @xmath466 , we have @xmath467 . a player-1 selector @xmath103 is _ proper _ if the memoryless player-1 strategy @xmath468 is proper .",
    "let @xmath144 be a turn - based stochastic game with reachability objective @xmath133 for player  1 .",
    "let @xmath470 be the initial selector , and @xmath289 be the selector obtained at iteration @xmath26 of the reachability strategy improvement algorithm .",
    "if @xmath289 is a pure , proper selector , then the following assertions hold :    1 .   for all @xmath359",
    ", we have @xmath289 is a pure , proper selector ; 2 .   for all @xmath359",
    ", we have @xmath443 , where @xmath471 and @xmath472 ; and 3 .   if @xmath444 , then @xmath473 , and there exists @xmath26 such that @xmath444 .",
    "the strategy improvement algorithm of condon  @xcite works only for _ halting games _ , but the reachability strategy improvement algorithm works if we start with a pure , proper selector for reachability games that are not halting . hence to use the reachability strategy improvement algorithm to compute values we need to start with a pure , proper selector .",
    "we present a procedure to compute a pure , proper selector , and then present termination bounds ( i.e. , bounds on @xmath26 such that @xmath444 ) . the construction of pure , proper selector is based on the notion of _ attractors _ defined below",
    ".    _ attractor strategy .",
    "_ let @xmath474 , and for @xmath359 we have @xmath475 since for all @xmath476 we have @xmath477 , it follows that from all states in @xmath478 player  1 can ensure that @xmath130 is reached with positive probability .",
    "it follows that for some @xmath114 we have @xmath479 .",
    "the pure _ attractor _",
    "selector @xmath480 is as follows : for a state @xmath481 we have @xmath482 , where @xmath483 ( such a @xmath29 exists by construction ) .",
    "the pure memoryless strategy @xmath484 ensures that for all @xmath359 , from @xmath485 the game reaches @xmath486 with positive probability . hence there is no end - component @xmath153 contained in @xmath487 in the mdp @xmath488 .",
    "it follows that @xmath480 is a pure selector that is proper , and the selector @xmath480 can be computed in @xmath489 time .",
    "this completes the reachability strategy improvement algorithm for turn - based stochastic games .",
    "we now present the termination bounds .",
    "_ termination bounds .",
    "_ we present termination bounds for binary turn - based stochastic games .",
    "a turn - based stochastic game is binary if for all @xmath490 we have @xmath491 , and for all @xmath57 if @xmath492 , then for all @xmath493 we have @xmath494 , i.e. , for all probabilistic states there are at most two successors and the transition function @xmath17 is uniform .",
    "the results follow as a special case of lemma  2 of  @xcite .",
    "lemma  2 of  @xcite holds for halting turn - based stochastic games , and since markov chains reaches the set of closed connected recurrent states with probability  1 from all states the result follows .      since pure memoryless optimal strategies exist for both players ( theorem  [ thrm : memory - determinacy ] ) , we fix pure memoryless optimal strategies @xmath106 and @xmath107 for both players .",
    "the markov chain @xmath500 can be then reduced to an equivalent markov chains with @xmath501 states ( since we fix deterministic successors for states in @xmath502 , they can be collapsed to their successors ) .",
    "the result then follows from lemma  [ lemm : mc - bound ] .    from lemma  [ lemm : tb - bound ]",
    "it follows that at iteration  @xmath26 of the reachability strategy improvement algorithm either the sum of the values either increases by @xmath503 or else there is a valuation @xmath504 such that @xmath444 .",
    "since the sum of values of all states can be at most @xmath495 , it follows that algorithm terminates in at most @xmath505 steps .",
    "moreover , since the number of pure memoryless strategies is at most @xmath506 , the algorithm terminates in at most @xmath506 steps .",
    "it follows from the results of  @xcite that a turn - based stochastic game graph @xmath144 can be reduced to a equivalent binary turn - based stochastic game graph @xmath507 such that the set of player  1 and player  2 states in @xmath144 and @xmath507 are the same and the number of probabilistic states in @xmath507 is @xmath508 , where @xmath35 is the size of the transition function in @xmath144 .",
    "thus we obtain the following result .",
    "let @xmath144 be a turn - based stochastic game with a reachability objective @xmath133 , then the reachability strategy improvement algorithm computes the values in time @xmath509 where @xmath510 is polynomial function .",
    "the results of  @xcite presented an algorithm for turn - based stochastic games that works in time @xmath511 .",
    "the algorithm of  @xcite works only for turn - based stochastic games , for general turn - based stochastic games the complexity of the algorithm of  @xcite is better .",
    "however , for turn - based stochastic games where the transition function at all states can expressed in constant bits we have @xmath512 . in these cases the reachability strategy improvement algorithm ( that works for both concurrent and turn - based stochastic games ) works in time @xmath513 as compared to the time @xmath514 of the algorithm of  @xcite .",
    "a.  bianco and l.  de  alfaro .",
    "model checking of probabilistic and nondeterministic systems . in _",
    "fsttcs 95 : software technology and theoretical computer science _",
    ", volume 1026 of _ lecture notes in computer science _ , pages 499513 .",
    "springer - verlag , 1995 .",
    "a.  condon . on algorithms for simple stochastic games . in _ advances in computational complexity",
    ", volume  13 of _ dimacs series in discrete mathematics and theoretical computer science _ , pages 5173 .",
    "american mathematical society , 1993 ."
  ],
  "abstract_text": [
    "<S> we consider concurrent games played on graphs . at every round of the game , </S>",
    "<S> each player simultaneously and independently selects a move ; the moves jointly determine the transition to a successor state . </S>",
    "<S> two basic objectives are the safety objective : `` stay forever in a set @xmath0 of states '' , and its dual , the reachability objective , `` reach a set @xmath1 of states '' . </S>",
    "<S> we present in this paper a strategy improvement algorithm for computing the _ value _ of a concurrent safety game , that is , the maximal probability with which player  1 can enforce the safety objective . </S>",
    "<S> the algorithm yields a sequence of player-1 strategies which ensure probabilities of winning that converge monotonically to the value of the safety game .    </S>",
    "<S> the significance of the result is twofold . </S>",
    "<S> first , while strategy improvement algorithms were known for markov decision processes and turn - based games , as well as for concurrent reachability games , this is the first strategy improvement algorithm for concurrent safety games . </S>",
    "<S> second , and most importantly , the improvement algorithm provides a way to approximate the value of a concurrent safety game _ from below _ ( the known value - iteration algorithms approximate the value from above ) . </S>",
    "<S> thus , when used together with value - iteration algorithms , or with strategy improvement algorithms for reachability games , our algorithm leads to the first practical algorithm for computing converging upper and lower bounds for the value of reachability and safety games . </S>"
  ]
}