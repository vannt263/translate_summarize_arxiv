{
  "article_text": [
    "let @xmath0 be the unique solution of the sde @xmath1 where @xmath2 and @xmath3 are @xmath4 functions with bounded derivatives , @xmath5 is a ( multi - dimensional ) brownian motion and @xmath6 a one - dimensional infinite activity pure jump lvy process with lvy measure @xmath7 . in this paper",
    "we are interested in the weak approximation of @xmath8 using random partitions of the time interval .    the traditional approach , analysed , e.g. ,  in jacod et al @xcite and protter - talay @xcite , consists in approximating @xmath9 using the euler scheme with a uniformly spaced time grid .",
    "it suffers from two difficulties : first , for a general lvy measure @xmath10 there is no available algorithm to simulate the increments of the driving lvy process and second , a large jump of @xmath6 occurring between two discretization points can lead to a large discretization error .    with the aim of resolving these problems , rubenthaler @xcite",
    "( see also bruti - liberati and platen @xcite and mordecki et al @xcite in the context of finite intensity lvy processes ) introduced the idea of replacing the driving process @xmath6 by a suitable compound poisson approximation and placing the discretization points at the jump times of the compound poisson process .",
    "this approach is problematic when the jump activity of the driving lvy process @xmath6 is strong , that is , the lvy measure has a strong singularity at zero .    in kohatsu - tankov @xcite ,",
    "the authors introduce and analyze a new approximation scheme in the case @xmath11 , building on the ideas of rubenthaler and asmussen - rosinski @xcite .",
    "the idea is to replace the driving process @xmath6 by an approximating process @xmath12 , which incorporates all jumps of @xmath6 bigger than @xmath13 and approximates the jumps of @xmath6 smaller than @xmath13 with a suitable chosen brownian motion , matching the second moment of @xmath6 .",
    "the solution to the contiunuous sde between the jump times can then be approximated with a suitable high order scheme .",
    "more recently , a similar approximation was used in the context of multilevel monte carlo schemes for lvy - driven sdes @xcite .",
    "although the previous approach improves the rates of convergence obtained with rubenthaler s scheme , there are limits on how well the small jumps of a lvy process can be approximated by a brownian motion ( think of non - symmetric lvy processes ) . in tankov @xcite ,",
    "the author presented a new scheme in the case @xmath11 based on approximating @xmath14 by a finite intensity lvy process , which incorporates all jumps bigger than @xmath13 and matches a given number of moments of @xmath6 with an additional compound poisson term .",
    "the main advantages of this approach are that the schemes are very easy to implement , because the driving process is piecewise deterministic , and that one can , in specific cases , obtain arbitrarily high order of convergence by matching a sufficiently large number of moments of @xmath15    in this paper we are interested in two aspects of approximation schemes for lvy driven sde s .",
    "first , in many of the previously mentioned schemes one assumes that there is no brownian motion component in the equation ( [ equ_x_intro ] ) ( i.e.  @xmath11 ) . the reason for this was that the speed of convergence of the approximating scheme for the jump component is fast and therefore it was not clear how to match this speed with the approximation of the brownian component without wasting computing resources .",
    "furthermore the fact that the equation does not have a brownian component facilitates the error analysis and the implementation of the scheme because the sde between jumps is deterministic , as in @xcite , or can be treated as a deterministic equation perturbed by a small noise term as in @xcite . on the other hand ,",
    "recent developments in the area of weak approximations for continuous sde s @xcite allow for high order approximations of the brownian component .",
    "therefore one may expect that the right combination of these approximation techniques with suitable jump adapted approximation schemes for pure jump sde s can be achieved .",
    "our second goal is a systematic study of the new moment - matching approximation schemes introduced in @xcite , with the objective of designing optimal compound poisson approximations and studying their convergence in a more general setting .    in this article , we show that the mathematical framework developed in tanaka - kohatsu @xcite is the appropriate tool in order to deal with the general situation ( @xmath16 ) .",
    "however , it needs to be adapted to the present setting where the partition is random while in @xcite , the partition is fixed .",
    "this framework is based on semigroup decompositions , which allow the study of a complex generator by decomposing it into simple components .",
    "the error estimate is obtained by a local analysis of each component .    in the resulting error bound ,",
    "the contributions of the compound poisson approximation and of the discretization scheme for the brownian part are separate and tractable .",
    "this allows to balance the two contributions by an appropriate choice of the order of the discretization scheme for the brownian part , in order to minimize the computational time . on the other hand",
    ", this decomposition enables us to formulate the problem of choosing the compound poisson approximation as an optimization problem ( minimizing the error bound ) .",
    "we characterize the optimal approximating process in the general case and provide explicit representation in specific situations .",
    "often , the optimal solution is to keep all the jumps bigger than @xmath13 and add an additional compound poisson process to match the moment structure of the small jumps . under a regularity assumption on the lvy measure ,",
    "we show that this methodology can be used to construct approximations with arbitrarily high order of convergence",
    ".    an interesting consequence of our analysis is that the asmussen - rosinski approach is not the optimal procedure to approximate the small jumps in the setting of weak convergence .",
    "we give a better procedure , which uses lvy measures with point masses to approximate the small jumps ( see remark [ asro ] ) .    in order to correctly describe the optimality aspect ,",
    "let @xmath17 be the unique solution of @xmath18 but using @xmath19 as driving process instead of @xmath6 .",
    "@xmath19 is a finite activity lvy process with lvy measure @xmath20 , which may have a wiener component .",
    "furthermore , let @xmath21 be a computable approximation of @xmath22 which shares the same jump times as @xmath23 the first objective is to find an upper bound for the difference @xmath24-\\mathbb{e}[f(\\bar{x}_{1})]$ ] in terms of @xmath25 ( the average number of partition intervals ) and the moments of @xmath26 and @xmath27 this part assumes then that the brownian component can be simulated exactly .    in the second part , we approximate the brownian component and analyze the error @xmath28-\\mathbb{e}[f(\\widehat{x}_{1})]$ ] . to analyze @xmath29 we extend the operator approach developed in @xcite to jump - adapted random partitions .    in conclusion",
    ", we find that we can express an upper bound for @xmath30 in terms of the moments of @xmath26 and @xmath31 and an upper bound for @xmath32 in terms of @xmath33 now , for fixed @xmath34 ( and , hence , @xmath32 ) we consider @xmath20 as a variable and minimize the upper bound for @xmath35 obtaining an optimal lvy measure @xmath20 for the approximating finite intensity process @xmath19 .",
    "once the optimal error is known as a function of @xmath36 ( this is done as a worse case analysis or in asymptotic form ) one can identify the order of the approximation that is needed for the brownian component .",
    "the paper is structured as follows . in section [ sec : prelim ] , we introduce the notation . in section [ sec : weakerror ] , we start introducing the assumptions in order to study the weak error of the approximations and we give the main error estimate , which will be the base for the study of optimal approximations . the expansion of the error is given in terms of @xmath37 and the moments of @xmath38    the proof of the main error estimate is given in sections [ sec : d ] and [ sec : hatd ] , which analyze , respectively , @xmath30 and @xmath32 . in section [ sec : optimal ] , we formulate the problem of finding the optimal compound poisson approximation of @xmath6 as an optimization problem , characterize its solution and prove an existence result .",
    "explicit examples of solutions are given in section [ sec : explicit ] , and section [ sec:6.2 ] analyzes the convergence rates of the resulting scheme .",
    "specific algorithms and numerical illustrations are provided in section [ numerics.sec ] .",
    "finally , in the appendix we gather some technical lemmas .    throughout the article we use the einstein notation of summation over double indices .",
    "@xmath39 denotes the point mass measure at @xmath40 .",
    "various positive constants are denoted by @xmath41 or @xmath42 with the dependence on various parameters .",
    "their exact values may change from one line to the next without further mentioning .",
    "let the process @xmath43}$ ] be the unique solution of the following @xmath44-dimensional sde @xmath45 where @xmath46 and @xmath47 are @xmath48 functions with bounded derivatives , @xmath49}$ ] is a @xmath50-dimensional standard brownian motion and @xmath51}$ ] is a one dimensional lvy process ( independent of @xmath5 ) with the following representation@xmath52 where @xmath7 is an infinite activity lvy measure , that is @xmath53 and @xmath54 is a poisson random measure on @xmath55 with intensity @xmath56 .",
    "let @xmath57}$ ] be the approximating process , which is the solution of the sde@xmath58 where @xmath59}$ ] is a lvy process ( independent of @xmath5 ) with the following representation@xmath60 where @xmath61 , @xmath62 and @xmath63 is a poisson random measure on @xmath55 with intensity @xmath64 and @xmath65}$ ] is a standard @xmath50-dimensional brownian motion independent of all the other processes .",
    "we assume that @xmath66 belongs to a set of possible approximation parameters denoted by @xmath67 . without loss of generality we may sometimes abuse the notation and write @xmath68 to denote the lvy measure for which there exists @xmath69 and @xmath70 so that @xmath71 .    note",
    "that , if we define @xmath72 then we can write@xmath73    sometimes , the following flow notation will be useful@xmath74    define the process@xmath75 and the following operator@xmath76.\\ ] ]    we consider the following stopping times@xmath77\\right )   \\neq0\\},\\quad i\\in\\mathbb{n},\\\\ \\bar t_{0 }   &   = 0.\\end{aligned}\\ ] ] and the associated _ jump _ operators@xmath78,\\quad i\\in\\mathbb{n}\\\\ ( \\bar s^{0}f)\\left (   x\\right )    &   = f\\left (   x\\right )   .\\end{aligned}\\ ] ]    note that the stopping times @xmath79 are well defined because @xmath80 and that @xmath81 is independent of @xmath82 because the jump sizes of a compound poisson process are identically distributed .",
    "still , we will keep this notation as it will help to keep track of the number of jumps .",
    "we will also assume that there exist a process @xmath83}$ ] satisfying the following stochastic representation condition .",
    "[ @xmath84assume that @xmath85 satisfies@xmath86   & = \\mathbb{e}[\\boldsymbol{1}_{\\{1<\\bar t_{1 } \\}}\\bar s^{0}\\widehat{p}_{1}f\\left (   x\\right )   ] , \\\\",
    "\\mathbb{e}[\\boldsymbol{1}_{\\{\\bar t_{i}<1<\\bar t_{i+1}\\}}f(\\widehat{x}_{1 } ) ] &   = \\mathbb{e}[\\boldsymbol{1}_{\\{\\bar t_{i}<1<\\bar t_{i+1}\\}}\\bar s^{0}\\widehat{p}_{\\bar t_{1}\\wedge1}\\bar s^{1}\\widehat{p}_{\\bar t_{2}-\\bar t_{1}}\\cdots\\bar s^{i}\\widehat{p}_{1-\\bar t_{i}}f\\left (   x\\right )   ] , \\end{aligned}\\ ] ] for @xmath87 where @xmath88 is a linear operator .",
    "the process @xmath85 and the linear operator @xmath88 correspond to the scheme chosen to approximate the solution of equation @xmath89 between jumps .",
    "recall that for each multi - index of order @xmath90 , @xmath91 we define @xmath92 .",
    "we also use the following notation @xmath93 for any function @xmath94 .",
    "we introduce the following spaces of functions .    * @xmath95 the set of @xmath96 functions @xmath97 such that for each multi - index @xmath98 with @xmath99 @xmath100 for some positive constant @xmath101    we will use the notation @xmath102 in each @xmath103 we consider the norm@xmath104    * @xmath105 the set of @xmath96 functions @xmath97 such that for each multi - index @xmath98 with @xmath106 @xmath107 for some positive constant @xmath101    [ @xmath108@xmath109 @xmath110 and @xmath111    [ @xmath112@xmath113 and @xmath114 for all @xmath115    in fact , all the results up to section [ sec : hatd ] only use moments up to power @xmath116 when we assume @xmath117 . still , in applications , in order for the continuous high - order scheme to satisfy the assumption @xmath118 ( see below ) , the moments of order at least @xmath119 are required .",
    "for this reason , we prefer this version of the assumptions .",
    "our next objective is to establish the main error estimate of this paper . in order to do this ,",
    "we need to introduce a modification of the framework introduced in @xcite in the next section .",
    "the error estimate will then be given in section [ sec : mainerr ] .",
    "to simplify the notation , we define the non commutative product of operators as follows .",
    "given a finite number of linear operators @xmath120 we define@xmath121 suppose we are given two sequences of linear operators @xmath122 and @xmath123 , @xmath124 $ ] .",
    "furthermore , assume that for each @xmath125 approximates @xmath126 in some sense to be defined later ( see assumption [ ass:6 ] ) .",
    "given a partition @xmath127 we define its norm as @xmath128 .",
    "now , we would like to estimate the following quantity@xmath129 in order to achieve this goal , we will make use of the following expansion@xmath130 hence , if we have a good norm estimates of @xmath131 and @xmath132 then we can expect that @xmath133 approximates well @xmath134    from now on , @xmath135 is a linear operator for @xmath136 $ ] and @xmath137 is a linear operator for @xmath136.$ ]    [ @xmath138for all @xmath87 if @xmath139 with @xmath140 then @xmath141 and@xmath142   } \\left\\vert q_{t}^{i}f\\right\\vert _ { c_{p}}\\leq k\\left (   \\mathcal{a}\\right )   \\left\\vert f\\right\\vert _ { c_{p}},\\ ] ] for some constant @xmath143 furthermore , we assume @xmath144 whenever @xmath145 and @xmath146 .",
    "[ @xmath147for all @xmath87 @xmath148 satisfies @xmath149 and for each @xmath150@xmath151 for some positive constants @xmath152 and @xmath153    for @xmath154   \\rightarrow\\mathbb{r}_{\\mathbb{+}}$ ] denotes an increasing function satisfying @xmath155 usually , we have @xmath156    [ ass:6 ] @xmath157 for all @xmath87 define @xmath158 for each @xmath140 there exists a constant @xmath159 such that if @xmath160 with @xmath161 then@xmath162 for all @xmath163   .$ ]    [ ass:7 ] @xmath164  if @xmath165 one has that for @xmath166@xmath167   ^{n - k}}\\left\\vert \\prod_{i = k+1}^{n}\\bar p_{t_{i}}^{i}f\\right\\vert _ { c_{p}^{m}}\\leq c\\left ( \\mathcal{a } \\right )   \\left\\vert f\\right\\vert _ { c_{p}^{m}}.\\ ] ]    [ l_bounded_product_q]under assumption @xmath168 the operators @xmath169 satisfy @xmath170 for any positive function @xmath171 and @xmath172 for some positive constant @xmath41 .",
    "let @xmath173 for @xmath174 using assumption @xmath168 the monotonicity of the operators @xmath169 and that these operators are the identity on constants , we have@xmath175 with constants @xmath152 and @xmath176 that do not depend on @xmath177 since @xmath178 by induction follows that@xmath179    [ th_approx_operators]assume @xmath180 for @xmath181 and @xmath148 and @xmath182 then for any @xmath183 there exists a constant @xmath184 such that@xmath185    let @xmath186 using the expansion @xmath187 , we have@xmath188 using assumption @xmath189 and @xmath190 we obtain @xmath191 now , lemma [ l_bounded_product_q ] yields@xmath192 finally , adding up the estimates @xmath193      [ theomain]let @xmath83}$ ] be a process satisfying assumption @xmath194 assume that the operators @xmath195 and @xmath196 satisfy assumptions @xmath180 and @xmath197 .",
    "i ) : :    assume @xmath198and    @xmath199 then there exist positive constants    @xmath200 and @xmath201 ,    @xmath202 such that @xmath203-\\mathbb{e}[f(\\widehat{x}_{1})]|\\\\    &   \\leq c_{1}\\left (   x\\right )   \\left\\vert \\int_{\\left\\vert y\\right\\vert    > 1}y(\\nu-\\bar\\nu)\\left (   dy\\right )   -\\bar\\mu\\right\\vert + c_{2}(x)\\left\\vert    \\int_{\\mathbb{r}}y^{2}(\\nu-\\bar\\nu)\\left (   dy\\right )   -\\bar\\sigma    ^{2}\\right\\vert",
    "\\\\    &   + \\sum_{i=3}^{n}c_{i}\\left (   x\\right )   \\left\\vert \\int_{\\mathbb{r}}y^{i}(\\nu-\\bar\\nu)\\left (   dy\\right )   \\right\\vert \\\\    &   + c_{n+1}\\left (   x\\right )   \\int_{\\mathbb{r}}\\left\\vert y\\right\\vert    ^{n+1}\\left\\vert \\nu-\\bar\\nu\\right\\vert \\left (   dy\\right )   + k\\left (    x,\\mathcal{a},m\\right )   \\left\\vert f\\right\\vert _ { c_{p}^{2(m+1)}}\\bar    \\lambda^{-m}.\\end{aligned}\\ ] ] ii ) : :    assume @xmath204and    @xmath205    then there exist positive constants @xmath200    and @xmath201 , @xmath202 such that    @xmath203-\\mathbb{e}[f(\\widehat{x}_{1})]|\\\\    &   \\leq c_{1}\\left (   x\\right )   \\left\\vert f\\right\\vert _ { c_{p}^{1}}\\left\\vert    \\int_{\\left\\vert y\\right\\vert > 1}y(\\nu-\\bar\\nu)\\left (   dy\\right )   -\\bar    \\mu\\right\\vert + c_{2}(x)\\left\\vert f\\right\\vert _ { c_{p}^{2}}\\left\\vert    \\int_{\\mathbb{r}}y^{2}(\\nu-\\bar\\nu)\\left (   dy\\right )   -\\bar\\sigma    ^{2}\\right\\vert \\\\    &   + \\sum_{i=3}^{n}c_{i}\\left (   x\\right )   \\left\\vert f\\right\\vert _ { c_{p}^{i}}\\left\\vert \\int_{\\mathbb{r}}y^{i}(\\nu-\\bar\\nu)\\left (   dy\\right )   \\right\\vert    \\\\    &   + c_{n+1}\\left (   x\\right )   \\left\\vert f\\right\\vert _ { c_{p}^{n+1}}\\{\\int_{\\mathbb{r}}\\left\\vert y\\right\\vert ^{n+1}\\left\\vert \\nu-\\bar    \\nu\\right\\vert \\left (   dy\\right )   + \\int_{\\mathbb{r}}\\left\\vert y\\right\\vert    ^{n+p+1}\\left\\vert \\nu-\\bar\\nu\\right\\vert \\left (   dy\\right )   \\}\\\\    &   + k\\left (   x,\\mathcal{a},m\\right )   \\left\\vert f\\right\\vert _ { c_{p}^{2(m+1)}}\\bar\\lambda^{-m}.\\end{aligned}\\ ] ]    follows from theorems [ th_e[f(x1)-f(x1eps ) ] ] and [ th_e[f(x1eps)-f(x1epsapprox ) ] ] .",
    "the first simple example of application of the above result is to parametrize the set @xmath67 by a parameter @xmath206 $ ] so that : @xmath207 take @xmath208 to be the operator associated with a one step euler scheme , so that the overall approximation consists in applying the euler scheme between the jumps of @xmath209 .",
    "then the above result reads@xmath210-\\mathbb{e}[f(\\widehat{x}_{1}^{\\varepsilon})]|\\leq c_{3}(x)\\int_{|y|\\leq\\varepsilon}\\left\\vert y\\right\\vert ^{3}\\nu\\left (   dy\\right )   + k\\left (   x\\right )   \\left\\vert f\\right\\vert _ { c_{p}^{4}}\\lambda_{\\varepsilon}^{-1}.\\ ] ]    when @xmath11 , this result corresponds to theorem 2 in @xcite .    in the particular case of an @xmath98-stabe - like lvy process with lvy density near zero",
    ", one obtains that the best convergence rate is @xmath211 for @xmath212 and the worse case is @xmath213 for @xmath214 .",
    "note that we could have applied high order schemes for wiener driven sdes in order to improve the last term above to @xmath215 .",
    "additional examples , algorithms , and numerical illustrations will be given in section [ numerics.sec ] .",
    "thoughout this section we will use the notation @xmath217 $ ] .",
    "some auxiliary properties of this function @xmath218 are established in lemma [ l_u(t , x ) ] .",
    "[ th_e[f(x1)-f(x1eps ) ] ]    i ) : :    assume @xmath219 and    @xmath220 then , we have the following    expansion@xmath221=\\int    _ { 0}^{1}\\bar b_{t}^{1}dt\\left\\ { \\int_{\\left\\vert y\\right\\vert > 1}y(\\nu-\\bar    \\nu)\\left (   dy\\right )   -\\bar\\mu\\right\\ } \\nonumber\\\\    &   + \\int_{0}^{1}\\bar b_{t}^{2}dt\\left (   \\int_{\\mathbb{r}}y^{2}(\\nu-\\bar    \\nu)\\left (   dy\\right )   -\\bar\\sigma^{2}\\right ) \\\\    &   + \\sum_{i=3}^{n}\\int_{0}^{1}\\bar b_{t}^{i}dt\\int_{\\mathbb{r}}y^{i}(\\nu    -\\bar\\nu)\\left (   dy\\right )   + \\int_{0}^{1}\\bar b_{t}^{n+1}dt,\\label{eq_expansion_e[x1-x1eps]3}\\ ] ]    where@xmath222 , \\quad i=1, ...",
    ",n,\\\\    \\bar b_{t}^{n+1 }   &   : = \\mathbb{e}\\bigg [ \\sum_{|\\alpha|=n+1}\\int_{\\mathbb{r}}\\left (   \\int_{0}^{1}\\frac{\\partial^{|\\alpha|}}{\\partial x^{\\alpha}}u\\left (    t,\\bar x_{t}+\\theta yh\\left (   \\bar x_{t}\\right )   \\right )   \\frac{\\left (    1-\\theta\\right )   ^{|\\alpha|-1}}{n!}d\\theta\\right ) \\\\    &   \\times h^{\\alpha}\\left (   \\bar x_{t}\\right )   y^{n+1}(\\nu-\\bar\\nu)\\left (    dy\\right )   \\bigg ] , \\end{aligned}\\ ] ] and @xmath223 where the constants    @xmath224 do not depend on    @xmath225 ii ) : :    assume @xmath226 and    @xmath227 . then we have that the expansion    @xmath2283}\\right )   $ ] also holds with    @xmath229    and@xmath230 where the    constants @xmath224 do not    depend on @xmath225    to simplify the notation we will give the proof in the case @xmath231 note that @xmath232=\\mathbb{e}[f\\left (   x_{1}\\left ( 0,x\\right )   \\right )   ] = u\\left (   0,x\\right )   $ ] and @xmath233=\\mathbb{e}[u\\left (   0,x\\right )   -u\\left (   1,\\bar x_{1}\\right )   ] .\\ ] ] applying it formula to @xmath234 and taking into account the equation satisfied by @xmath235 ( see lemma [ l_u(t , x ) ] ) , we have@xmath236\\\\ &   = \\mathbb{e}\\left [ \\int_{0}^{1}\\frac{\\partial u}{\\partial x}\\left (   t,\\bar x_{t}\\right )   h\\left (   \\bar x_{t}\\right )   \\left\\ { \\int_{\\left\\vert y\\right\\vert > 1}y(\\nu-\\bar\\nu)\\left (   dy\\right )   -\\bar\\mu\\right\\ } dt\\right ] \\\\ &   + \\mathbb{e}\\left [ \\int_{0}^{1}\\int_{\\mathbb{r}}\\left\\ { u\\left (   t,\\bar x_{t}+h\\left (   \\bar x_{t}\\right )   y\\right )   -u\\left (   t,\\bar x_{t}\\right ) -\\frac{\\partial u}{\\partial x}\\left (   t,\\bar x_{t}\\right )   h\\left (   \\bar x_{t}\\right )   y\\right\\ } ( \\nu-\\bar\\nu)\\left (   dy\\right )   dt\\right ] \\\\ &   -\\mathbb{e}\\left [ \\frac{\\bar\\sigma^{2}}{2}\\int_{0}^{1}\\frac{\\partial^{2}u}{\\partial x^{2}}\\left (   t,\\bar x_{t}\\right )   h^{2}\\left (   \\bar x_{t}\\right ) dt\\right ] .\\end{aligned}\\ ] ] making a taylor expansion of order @xmath237 we obtain@xmath238 \\\\ &   = \\sum_{i=2}^{n}\\mathbb{e}\\left [ \\int_{0}^{1}\\int_{\\mathbb{r}}\\frac { \\partial^{i}}{\\partial x^{i}}u\\left (   t,\\bar x_{t}\\right )   h^{i}\\left (   \\bar x_{t}\\right )   y^{i}(\\nu-\\bar\\nu)\\left (   dy\\right )   dt\\right ] \\\\ &   + \\mathbb{e}\\left [ \\int_{0}^{1}\\int_{\\mathbb{r}}\\left (   \\int_{0}^{1}\\frac{\\partial^{n+1}}{\\partial x^{n+1}}u\\left (   t,\\bar x_{t}+\\theta yh\\left ( \\bar x_{t}\\right )   \\right )   \\frac{\\left (   1-\\theta\\right )   ^{n}}{n!}d\\theta\\right )   \\times h^{n+1}\\left (   \\bar x_{t}\\right )   y^{n+1}(\\nu-\\bar \\nu)\\left (   dy\\right )   dt\\right]\\end{aligned}\\ ] ] hence , collecting terms , we have @xmath236\\\\ &   = \\int_{0}^{1}\\mathbb{e}\\left [ \\frac{\\partial u}{\\partial x}\\left (   t\\bar x_{t}\\right )   h\\left (   \\bar x_{t}\\right )   \\right ] dt\\left\\ { \\int_{\\left\\vert y\\right\\vert > 1}y(\\nu-\\bar\\nu)\\left (   dy\\right )   + \\int_{\\left\\vert y\\right\\vert \\leq1}y\\bar\\nu\\left (   dy\\right )   -\\bar\\mu\\right\\ } \\\\ &   + \\left (   \\int_{\\mathbb{r}}y^{2}(\\nu-\\bar\\nu)\\left (   dy\\right )   -\\bar \\sigma^{2}\\right )   \\mathbb{e}\\left [ \\int_{0}^{1}\\frac{1}{2!}\\frac{\\partial ^{2}}{\\partial x^{2}}u\\left (   t,\\bar x_{t}\\right )   h^{i}\\left (   \\bar x_{t}\\right )   dt\\right ] \\\\ &   + \\sum_{i=3}^{n}\\int_{0}^{1}\\mathbb{e}\\left [ \\frac{1}{i!}\\frac{\\partial^{i}}{\\partial x^{i}}u\\left (   t,\\bar x_{t}\\right )   h^{i}\\left (   \\bar x_{t}\\right ) \\right ] dt\\int_{\\mathbb{r}}y^{i}(\\nu-\\bar\\nu)\\left (   dy\\right ) \\\\ &   + \\int_{0}^{1}\\mathbb{e}\\bigg [ \\int_{\\mathbb{r}}\\left (   \\int_{0}^{1}\\frac{\\partial^{n+1}}{\\partial x^{n+1}}u\\left (   t,\\bar x_{t}+\\lambda yh\\left ( \\bar x_{t}\\right )   \\right )   \\frac{\\left (   1-\\theta\\right )   ^{n}}{n!}d\\theta\\right )   \\times h^{n+1}\\left (   \\bar x_{t}\\right )   y^{n+1}(\\nu-\\bar \\nu)\\left (   dy\\right ) \\bigg ] dt\\end{aligned}\\ ] ] and we obtain the expansion @xmath2393}\\right ) .$ ] under the assumption @xmath240 using lemmas [ l_lp_uniform_bounds ] and [ l_u(t , x ) ] , one obtains the first inequality in ( [ star ] ) .",
    "similarly , if we assume @xmath241 using lemmas [ l_lp_uniform_bounds ] and [ l_u(t , x ) ] , one obtains the second inequality in ( [ star ] ) .",
    "[ l_expectation_operator]for @xmath243 one has that @xmath244=\\mathbb{e}[\\boldsymbol{1}_{\\{\\bar t_{i}<1<\\bar t_{i+1}\\}}\\bar p_{1-\\bar t_{i } } f(\\bar x_{\\bar t_{i}})].\\ ] ]    define @xmath245 then , on the set @xmath246 @xmath247\\\\ &   = \\mathbb{e}\\bigg [ f(x+\\int_{t}^{1}\\bar b(\\bar x_{s}(t , x))ds\\\\ &   + \\int_{t}^{1}\\sigma(\\bar x_{s}(t , x))db_{s}+\\bar\\sigma\\int_{t}^{1}h(\\bar x_{s}(t , x))dw_{s})\\bigg |\\bar{\\mathcal{h}}^{i , i}\\bigg ] \\bigg |_{t=\\bar t_{i},x=\\bar x_{\\bar t_{i}}}\\\\ &   = \\mathbb{e}[f\\left (   \\bar y_{1}\\left (   t , x\\right )   \\right )   ] |_{t=\\bar t_{i},x=\\bar x_{\\bar t_{i}}},\\end{aligned}\\ ] ] where in the last equality we have used that @xmath248 satisfies the same sde as @xmath249 on @xmath250 . now applying lemma [ l_law_of_x ] and the definition of @xmath251 we obtain the result .    applying the previous lemma with @xmath252 and using",
    "that @xmath253 is the identity operator we obtain that@xmath254=\\mathbb{e}[\\boldsymbol{1}_{\\{1<\\bar t_{1}\\}}\\bar s^{0}\\bar p_{1}f\\left (   x\\right )   ] .\\ ] ]    [ pr_sr_xeps_1]for @xmath87 the following equality holds .",
    "@xmath255=\\mathbb{e}[\\boldsymbol{1}_{\\{\\bar t_{i}<1<\\bar t_{i+1}\\}}\\bar s^{0}\\bar p_{\\bar t_{1}}\\bar s^{1}\\bar p_{\\bar t_{2}-\\bar t_{1}}\\cdots\\bar s^{i}\\bar p_{1-\\bar t_{i}}f\\left (   x\\right )   ] .\\ ] ]    define @xmath256 by lemma [ l_expectation_operator ] and the definition of the operator @xmath81 we have that@xmath257\\\\ &   = \\mathbb{e}[\\boldsymbol{1}_{\\{\\bar t_{i}<1<\\bar t_{i+1}\\}}\\bar p_{1-\\bar t_{i}}f(\\bar x_{\\bar t_{i } } ) ] \\\\ &   = \\mathbb{e}[\\boldsymbol{1}_{\\{\\bar t_{i}<1<\\bar t_{i+1}\\}}\\mathbb{e}[\\bar p_{1-\\bar t_{i}}f(\\bar x_{\\bar t_{i}-}+h(\\bar x_{\\bar t_{i}-})\\delta\\bar z_{\\bar t_{i } } ) |\\bar{\\mathcal{g}}^{i , i}]]\\\\ &   = \\mathbb{e}[\\boldsymbol{1}_{\\{\\bar t_{i}<1<\\bar t_{i+1}\\}}\\bar s^{i}\\bar p_{1-t}f(x)|_{t=\\bar t_{i},x=\\bar x_{\\bar t_{i}-}}]\\\\ &   = \\mathbb{e}[\\boldsymbol{1}_{\\{\\bar t_{i}<1<\\bar t_{i+1}\\}}\\bar s^{i}\\bar p_{1-\\bar t_{i}}f(\\bar x_{\\bar t_{i}-}(\\bar t_{i-1},\\bar x_{\\bar t_{i-1 } } ) ) ] \\\\ &   = \\mathbb{e}[\\boldsymbol{1}_{\\{\\bar t_{i}<1<\\bar t_{i+1}\\}}\\bar s^{i}\\bar p_{1-\\bar t_{i}}f(\\bar y_{\\bar t_{i}}(\\bar t_{i-1},\\bar x_{\\bar t_{i-1 } } ) ) ] .\\end{aligned}\\ ] ] where in the last equality we have used that @xmath258    reasoning analogously to the proof of lemma [ l_expectation_operator ] , one has that@xmath259 \\\\ &   = \\mathbb{e}\\left [ \\boldsymbol{1}_{\\{\\bar t_{i}<1<\\bar t_{i+1}\\}}\\mathbb{e}[\\bar s^{i}\\bar p_{1-\\bar t_{i}}f(\\bar y_{\\bar t_{i}}(\\bar t_{i-1},\\bar x_{\\bar t_{i-1}}))|\\bar{\\mathcal{h}}^{i , i-1}]\\right ] \\\\ &   = \\mathbb{e}\\left [ \\boldsymbol{1}_{\\{\\bar t_{i}<1<\\bar t_{i+1}\\}}\\mathbb{e}[\\bar s^{i}\\bar p_{1-t_{i}}f(\\bar y_{t_{i}}\\left (   t_{i-1},x\\right ) ) ] |_{t_{i}=\\bar t_{i},t_{i-1}=\\bar t_{i-1},x=\\bar x_{\\bar t_{i-1}}}\\right ] \\\\ &   = \\mathbb{e}\\left [ \\boldsymbol{1}_{\\{\\bar t_{i}<1<\\bar t_{i+1}\\}}\\bar p_{\\bar t_{i}-\\bar t_{i-1}}\\bar s^{i}\\bar p_{1-\\bar t_{i}}f(x_{\\bar t_{i-1}})]\\right ] .\\end{aligned}\\ ] ] iterating this procedure the result follows .",
    "now we need the following technical result .",
    "[ pr_sum_series]we have",
    "that @xmath260 \\leq c\\left (   m\\right )   \\bar\\lambda^{-m}.\\ ] ]    from lemma 11 in @xcite , one has that@xmath261 \\leq c\\left (   m\\right )   \\bar\\lambda^{-m},\\ ] ] where @xmath262 and @xmath263 is a constant that only depends on @xmath264 we can write@xmath265    &   = \\sum_{i=0}^{\\infty}\\mathbb{e}\\left [ \\boldsymbol{1}_{\\{\\bar t_{i } < 1<\\bar t_{i+1}\\}}\\int_{0}^{1}\\left (   t-\\eta\\left (   t\\right )   \\right ) ^{m}dt\\right ] \\\\ &   = \\sum_{i=0}^{\\infty}\\sum_{k=1}^{i+1}\\mathbb{e}[\\boldsymbol{1}_{\\{\\bar t_{i}<1<\\bar t_{i+1}\\}}\\int_{\\bar t_{k-1}}^{\\bar t_{k}\\wedge1}\\left ( t-\\eta\\left (   t\\right )   \\right )   ^{m}dt],\\end{aligned}\\ ] ] and the result follows by integration",
    ".    the main result of this section is the following .    [ th_e[f(x1eps)-f(x1epsapprox)]]let @xmath266}$ ] be the process defined in @xmath267 and @xmath268}$ ] a process satisfying assumption @xmath194 if the operators @xmath269 and @xmath270 associated to these processes satisfy assumptions @xmath180 and @xmath271 with @xmath272 . then for any @xmath273 there exists a constant @xmath184 such that@xmath274-\\mathbb{e}[f(\\widehat{x}_{1})]\\right\\vert \\leq k\\left (   x,\\mathcal{a},m\\right )   \\left\\vert f\\right\\vert _ { c_{p}^{2(m+1)}}\\bar\\lambda^{-m}\\ ] ]    we can write@xmath275-\\mathbb{e}[f(\\widehat{x}_{1})]=\\mathbb{e}\\left [ \\sum_{i=0}^{\\infty}[\\boldsymbol{1}_{\\{\\bar t_{i}<1<\\bar t_{i+1}\\}}(f(\\bar x_{1})-f(\\widehat{x}_{1}))\\right ] .\\ ] ] by proposition [ pr_sr_xeps_1 ] and assumption @xmath276 , we have@xmath277 \\\\ &   = \\sum_{i=0}^{\\infty}\\mathbb{e}\\bigg [ \\boldsymbol{1}_{\\{\\bar t_{i}<1<\\bar t_{i+1}\\}}(\\bar s^{0}\\bar p_{\\bar t_{1 } } \\bar s^{1}\\bar p_{\\bar t_{2}-\\bar t_{1 } } \\cdots\\bar s^{i}\\bar p_{1-\\bar t_{i}}\\\\ &   -\\bar s^{0}\\widehat{p}_{\\bar t_{1}}\\bar s^{1}\\widehat{p}_{\\bar t_{2}-\\bar t_{1 } } \\cdots\\bar s^{i}\\widehat{p}_{1-\\bar t_{i } } ) f\\left (   x\\right ) \\bigg ] \\\\ &   = \\sum_{i=0}^{\\infty}\\mathbb{e}\\left [ \\boldsymbol{1}_{\\{\\bar t_{i}<1<\\bar t_{i+1}\\}}\\left (   \\prod_{k=1}^{i+1}\\bar p_{\\bar t_{k}\\wedge1-\\bar t_{k-1}}^{k}-\\prod_{k=1}^{i+1}q_{\\bar t_{k}\\wedge1-\\bar t_{k-1}}^{k}\\right )   f\\left ( x\\right )   \\right]\\end{aligned}\\ ] ] then , by theorem [ th_approx_operators ] , we obtain that@xmath278-\\mathbb{e}[f(\\widehat{x}_{1})]\\right\\vert \\\\ &   \\leq\\sum_{i=0}^{\\infty}\\left\\vert \\mathbb{e}\\left [ \\boldsymbol{1}_{\\{\\bar t_{i}<1<\\bar t_{i+1}\\}}\\left (   \\prod_{k=1}^{i+1}\\bar p_{\\bar t_{k}\\wedge1-\\bar t_{k-1}}^{k}-\\prod_{k=1}^{i+1}q_{\\bar t_{k}\\wedge1-\\bar t_{k-1 } } ^{k}\\right ) f\\left (   x\\right )   \\right ] \\right\\vert \\\\ &   \\leq k\\left (   x,\\mathcal{a},m\\right )   \\left\\vert f\\right\\vert _ { c_{p}^{2(m+1)}}\\sum_{i=0}^{\\infty}\\sum_{k=1}^{i+1}\\mathbb{e}\\left [ \\boldsymbol{1}_{\\{\\bar t_{i}<1<\\bar t_{i+1}\\}}\\left (   \\bar t_{k}\\wedge1-\\bar t_{k-1}\\right )   \\delta_{m}\\left (   \\bar t_{k}\\wedge1-\\bar t_{k-1}\\right ) \\right ] , \\end{aligned}\\ ] ] then the result follows by proposition [ pr_sum_series]@xmath279",
    "in this section , we discuss the optimization of the error bound in theorem [ theomain ] , i ) with respect to the choice of the approximating lvy process @xmath209 .",
    "we would like to choose the parameters @xmath69 and @xmath70 and the lvy measure @xmath280 in order to make the first four terms in the expansion small , that is , we concentrate on @xmath281 our approach is to take @xmath282 so that the expansion becomes @xmath283 ( see remark [ asro ] for an alternative choice of @xmath70 ) .",
    "next , we choose the lvy measure @xmath280 in the class of measures for which the first sum is equal to zero and then optimize over @xmath280 in this class with fixed intensity @xmath284 in order to make the last term as small as possible .",
    "we will denote by @xmath285 the set of all positive finite measures on @xmath286 .",
    "the problem of finding the optimal approximating lvy measure then takes the following form .",
    "[ @xmath287let @xmath7 be a lvy measure on @xmath286 admitting the first @xmath288 moments@xmath289 where @xmath237 and define @xmath290 . for any @xmath291 define the functional @xmath292",
    "the problem @xmath293 consists in finding @xmath294 under the constraints @xmath295 where @xmath296 , where we set by convention @xmath297 .",
    "the computation of @xmath298 for @xmath299 is a classical problem , known as the hamburger problem .",
    "a summary of known results on this problem is provided in appendix a.    in explicit examples of section [ sec : explicit ] , and in the general treatment of section [ sec:6.2 ] , we shall see that for the solutions of @xmath300 that we will find , the term @xmath301 appearing in theorem [ theomain ] , ii ) will always be of a lower order as @xmath302 than @xmath303 .",
    "therefore , the convergence rates of our schemes will be the same under @xmath219 and under @xmath304 .",
    "[ propexistsolution]the problem @xmath300 admits a solution .    by corollary [ exist.cor ] , there exist at least one measure satisfying the constraints . for @xmath305 , we define by @xmath306 the set of all such measures . for @xmath307 , we define by @xmath308 the set of all measures @xmath291 satisfying @xmath309 and @xmath310 , where @xmath311 it is clear that minimum in is the same as the minimum over the set @xmath306 for any @xmath312 .",
    "define@xmath313 by chebyshev s inequality we have that @xmath314 which yields the tightness of @xmath315 by prokhorov s theorem , we have that the set @xmath316 is relatively sequentially compact but , as @xmath316 is closed ( see e.g. , chapter vii in doob @xcite ) , we also have that is sequentially compact .",
    "the set @xmath317 is bounded from below and , hence , it has an infimum , say @xmath318 .",
    "then , by the basic properties of the the infimum , we can find a sequence of real numbers of the form @xmath319 converging to @xmath320 as @xmath321 is sequentially compact we can always find a sequence @xmath322 that converges weakly to some @xmath323 but @xmath324 being a subsequence of the convergent sequence @xmath325 must converge to @xmath320 hence , we only need to prove the lower semicontinuity of the functional @xmath326 that is , if @xmath327 converges weakly to @xmath20 then @xmath328    let @xmath329 . by the hahn decomposition theorem",
    ", there exist disjoint measurable sets @xmath330 and @xmath331 such that @xmath332 , @xmath26 is nonnegative on @xmath330 and nonpositive on @xmath331 .",
    "the functional @xmath333 can be alternatively written as @xmath334 where @xmath335 is the space of bounded measurable functions endowed with the essential supremum norm .",
    "this implies that @xmath336 where @xmath337 is the space of continuous functions with compact support .",
    "fix @xmath338 . by the monotone convergence theorem there exists @xmath339 such that @xmath340 since the measure @xmath341 is a finite measure on @xmath286 , both measures in its jordan decomposition are also finite and hence inner regular ( see e.g. v.16 in @xcite ) .",
    "therefore , we can find two closed sets @xmath342 and @xmath343 such that @xmath344 is positive on @xmath345 , negative on @xmath346 and @xmath347 . by lusin s theorem , we can find an interpolation between @xmath348 and @xmath349 .",
    "that is , a function @xmath350 with @xmath351 such that @xmath352 for @xmath353 , @xmath354 for @xmath355 and @xmath356 for @xmath357 with    @xmath358    therefore , finally @xmath359 which , together with means that @xmath360 because the choice of @xmath13 was arbitrary .    for a sequence @xmath361 which converges weakly to @xmath20",
    ", we have , for every @xmath350 with @xmath351 : @xmath362 now , taking the @xmath363 with respect to @xmath364 in the left - hand side , we obtain the desired result .",
    "the following result provides a characterization of the solutions of @xmath365 , which will be useful in finding explicit representations for small @xmath288 .",
    "[ propcharacsolution]the measure @xmath20 is a solution of @xmath366 if and only if it satisfies the constraints @xmath367 , and there exists a piecewise polynomial function @xmath368 such that @xmath369 for all @xmath40 , a function @xmath370 $ ] and a positive measure @xmath371 on @xmath286 such that @xmath372    [ remark_noatom]if the measure @xmath7 is absolutely continuous with respect to lebesgue s measure , the expression @xmath373 simplifies to @xmath374 moreover , in the case @xmath375 @xmath376 is a polynomial and the measure @xmath371 may always be taken to be an atomic measure with at most @xmath377 atoms ( because a positive polynomial of degree @xmath378 has at most @xmath377 distinct roots ) .",
    "a measure @xmath379 which satisfies the constraints @xmath367 is a solution of @xmath366 if and only if there exists a vector of lagrange multipliers @xmath380 such that @xmath379 minimizes the lagrangian @xmath381 over all measures @xmath291 , and @xmath382 .",
    "the lagrangian for this problem takes the form ( dropping the terms which do not depend on @xmath20 ) : @xmath383 set @xmath384 .",
    "let @xmath385 be such that @xmath386 and consider the family of measures @xmath387 where @xmath388 then , for any @xmath389 , @xmath390 where @xmath391 .",
    "for @xmath392 we have that@xmath393 therefore , necessarily @xmath369 for all @xmath40 .",
    "now , as before , let the jordan decomposition of @xmath394 be given by @xmath395 , where @xmath396 and @xmath397 are supported on disjoint measurable sets .",
    "then , @xmath398 where @xmath41 denotes the terms which do not depend on @xmath399 and @xmath400 .",
    "then , it is clear that at optimum ,    * @xmath399 should be equal to a measure with support @xmath401 .",
    "therefore in general , there will be no uniqueness .",
    "* @xmath402 on @xmath403 .",
    "* @xmath404 on @xmath405 .",
    "this follows because @xmath396 and @xmath397 are supported on disjoint measurable sets and @xmath406 .",
    "* @xmath400 satisfies @xmath407 on @xmath408 .    combining these observations",
    ", we complete the proof .",
    "let @xmath375 and @xmath7 be absolutely continuous . to find an optimal measure for the problem @xmath300 we can use the following procedure .",
    "use the following parametrization for @xmath409 and @xmath410 @xmath411  solve the following system of @xmath288 nonlinear equations for @xmath412 and @xmath413@xmath414 obviously , in general , the solution to this system can only be approximated numerically and this does not seem an easy task . for @xmath415 , the solutions are quite explicit ; they are discussed in the following section .    to complete the analysis we need to quantify the dependence of the optimal value of the error @xmath416 on @xmath417 when @xmath417 tends to infinity .",
    "this is achieved in the following section for small values of @xmath288 and in section [ sec:6.2 ] for general @xmath288 , under a regularity assumption on the lvy measure .      throughout this section",
    "we assume that the measure @xmath7 is absolutely continuous with respect to the lebesgue measure .",
    "[ [ the - case - n2 . ] ] the case @xmath418 .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + +    we use the characterization of proposition [ propcharacsolution ] ( see also remark [ remark_noatom ] ) .",
    "the function @xmath419 is necessarily of the form @xmath420 for some @xmath421 ( otherwise the infimum of the lagrangian would be @xmath422 ) , and therefore the optimal solution is given by@xmath423 where @xmath424 solves @xmath425 the approximation error @xmath426 is given by @xmath427 which can go to zero at an arbitrarily slow rate as @xmath428 .    [ [ the - case - n3 . ] ] the case @xmath429 .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + +    the function @xmath419 is now of the form @xmath430 , and the positivity constraint implies that @xmath419 is necessarily of the form @xmath431 or , in other words , @xmath432 , for some @xmath338 .",
    "it is now easy to see that an optimal solution is given by @xmath433 where @xmath424 solves @xmath434 and@xmath435 the approximation error @xmath436 satisfies @xmath437 as @xmath428 , since @xmath438 and @xmath439 however , the scheme with @xmath440 achieves a better rate with the same computational cost .",
    "[ [ the - case - n4 . ] ] the case @xmath440 .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + +    the function @xmath419 is now of the form @xmath441 and from the positivity constraint we then deduce that @xmath442 for some @xmath338 . analyzing the function",
    "@xmath443 it is easy to check that @xmath444 hence , the optimal solution is of the form @xmath445 where the constants @xmath446 and @xmath447 are determined from the moment constraints and satisfy @xmath448 and @xmath424 is found from the intensity constraint @xmath449 where@xmath450 note that @xmath451 is strictly decreasing , continuous , and satisfies @xmath452 and @xmath453 , which ensures the existence of a unique solution for @xmath454 also note that @xmath455 which ensures the non negativity of @xmath456    the worst case convergence rate can be estimated similarly to the case @xmath429 and satisfies @xmath457 as @xmath458 . as we shall see in the next section , in the presence of a more detailed information about the explosion of the lvy measure at zero , this convergence rate can be refined .",
    "@xmath459 [ asro ]    1 .",
    "the calculations of this section make it clear that as far as weak approximations are concerned , the asmussen - rosinski approach of approximating the small jumps of a lvy process with a brownian motion is not necessarily the only answer .",
    "in fact , the case @xmath429 studied above leads to an approximation which is asymptotically equivalent to the asmussen - rosinski method and the case @xmath440 leads to a scheme which converges at a faster rate , for the same computational cost .",
    "2 .   instead of taking @xmath460",
    ", one may choose @xmath461 which makes the second term in equal to zero , which leads , for @xmath462 , to the following optimization problem for @xmath20 : @xmath463under the constraints @xmath464this problem assumes the use of the asmussen - rosinski approach to match the second moment of @xmath465 .",
    "the analysis of this problem can be carried out using the same tools described above and leads to similar results .",
    "the notion of regular variation provides a convenient tool to study the convergence of our moment matching schemes even in the cases when @xmath288 is large and an explicit solution of @xmath466 is not available .",
    "we refer to @xcite for background on regular variation .    as usual , we denote by @xmath467 the class of regularly varying functions with index @xmath98 ( at zero or at infinity depending on the context ) . the following assumption , which is satisfied by many parametric lvy models used in practice ( stable , tempered stable / cgmy , normal inverse gaussian , generalized hyperbolic etc . ) may be used to quantify the rate of explosion of the lvy measure near zero .",
    "there exists @xmath468 , positive constants @xmath469 and @xmath470 with @xmath471 and a function @xmath472 ( at zero ) such that the lvy measure @xmath473 satisfies @xmath474    [ theomainregvar]let @xmath288 be even and let the lvy measure @xmath465 satisfy the assumption @xmath475 .",
    "then there exists a function @xmath476 with @xmath477 as @xmath478 such that the error bound @xmath479 defined by satisfies @xmath480for all @xmath417 sufficiently large , and for some constants @xmath481 with @xmath482 . the function @xmath364 is given explicitly by @xmath483 , where @xmath484 is a generalized inverse of the function @xmath485 appearing in assumption @xmath475 .",
    "@xmath459    1 .",
    "the regular variation implies that as @xmath428 , the error goes to zero as @xmath486 times a slowly varying factor ( such as logarithm ) . to compute the explicit convergence rate , the exact form of the regularly varying function @xmath485",
    "must be known .",
    "for example , if @xmath487 then @xmath488 for some strictly positive constant @xmath41 .",
    "2 .   in the case",
    "@xmath440 it can be shown using similar methods that @xmath489 for some strictly positive constant @xmath41 .    throughout the proof ,",
    "we let @xmath490 . to obtain an upper bound on the error",
    ", we construct a , possibly suboptimal , measure satisfying the constraints which attains the desired rate .",
    "let @xmath338 , and define @xmath491 where @xmath492 is the solution ( minimizer ) of the moment problem @xmath493 where we define @xmath494 .",
    "then , @xmath495 where @xmath496 by proposition [ momprobeven ] , @xmath497 on the other hand , the matrix @xmath498 is ( nonnegative ) positive definite , because it is a moment matrix of a measure .",
    "therefore , by sylvester s criterion we can write @xmath499 and also @xmath500 but @xmath501 and therefore @xmath502 by integration by parts and karamata s theorem ( theorem 1.5.11 in bgt.87 ) , we show that @xmath503 } |y|^{p}{\\nu}(dy)}{\\varepsilon^p \\int_{(\\varepsilon,\\infty ) } \\nu(dz ) } = \\frac{\\alpha}{p-\\alpha},\\quad \\text{for all $ p>\\alpha$}.   \\label{karamata}\\end{aligned}\\ ] ] and so @xmath504 the matrix @xmath505 is positive definite because @xmath506 therefore , @xmath507 and there exits a constant @xmath508 such that @xmath509 for @xmath13 sufficiently small .    to sum up , we have found that there exist two positive constants @xmath510 and @xmath511 such that for @xmath13 sufficiently small , @xmath512 let @xmath513 and @xmath514 .",
    "this function satisfies @xmath515 , and since @xmath516 as @xmath517 , by theorem 1.5.12 in @xcite , we also get that @xmath518 as @xmath428 .",
    "now , for a given @xmath519 , consider the measure with @xmath520 , and possibly an additional atom at @xmath521 to satisfy the intensity constraint .",
    "this measure satisfies the constraints of problem @xmath466 and , by , has error bounded by @xmath522 so that the upper bound of the theorem holds with @xmath523 .    to compute the lower bound ,",
    "observe that @xmath524and the explicit optimal solution for the problem in the right - hand side is given by @xmath525where @xmath526 and @xmath527 $ ] are such that @xmath528 ( cf proposition [ propcharacsolution ] ) , which means that in particular @xmath529 introduced above .",
    "on the other hand , the error functional associated to this solution satisfies @xmath530which proves the lower bound .",
    "according to section [ sec : optimal ] , our approach to find an optimal approximation for the lvy measure starts by setting @xmath531 and @xmath532 hence , the solution of equation @xmath533 between jumps satisfies the following equation@xmath534where@xmath535this implies that the drift term of the continuous part will depend on @xmath20 through the parameter @xmath536 therefore , once we have fixed @xmath20 the optimal approximation of the lvy measure @xmath537 we need to choose a weak approximation method to solve equation @xmath538 we will consider the following approaches :    * * weak taylor approximations * : these methods are based on the it - taylor expansion of the solution of @xmath539 this expansion is the stochastic analogue of the classical taylor expansion , where the role of polynomials is played by multiple iterated stochastic integrals . truncating the expansion at a certain degree of the iterated integrals we obtain an approximation method with global order of convergence related to that degree , see proposition 5.11.1 in @xcite .",
    "we will consider the weak taylor approximations with global order of convergence 1,2 and 3 , which we will denote by wt1 , wt2 and wt3 .",
    "although the method is conceptually simple to understand , it presents some difficulties in the implementation as we need to sample from the joint law of multiple stochastic integrals of different orders .",
    "this makes the method less appealing from a practical point of view , especially when the driving brownian motion is multi - dimensional . *",
    "* kusuoka - lyons - victoir methods * : these methods are also based on stochastic taylor expansions .",
    "the idea is to approximate the expectation under the wiener measure by the expectation under a probability measure supported on a finite number of paths of finite variation . by construction ,",
    "the expectations of the iterated stratonovich integrals , up to a certain degree , under this new measure match the expectations of the corresponding iterated integrals under the wiener measure . using the stratonovich - taylor formula one can deduce that the approximations obtained have global order of convergence depending on the degree of the iterated integrals taken into account , see @xcite . in particular we will consider the approximation schemes of degree 3 and 5 , denoted by klv3 and klv5 , which give , respectively , global order of convergence 1 and 2 .",
    "deriving and implementing these methods is not straightforward , see @xcite for an account on these issues . * * ninomiya - victoir method * : the ninomiya - victoir method can be seen as a stochastic splitting method .",
    "the idea is to find suitable small time approximations of the semigroup associated to the solution of equation @xmath538 these approximations are written in terms of weighted products ( compositions ) of simpler semigroups associated to the so called coordinate processes and are deduced using formal taylor expansions of the semigroups involved .",
    "the main difference with respect to the classical splitting methods is that , in the stochastic case , we need to find appropriate stochastic representations of the semigroups in order to implement the monte carlo method .",
    "these representations involve solving or approximating odes with random coefficients .",
    "we will consider the algorithm given by ninomiya and victoir in @xcite , which has global order of convergence 2 .",
    "having fixed an optimal lvy measure and a weak approximation scheme for the continuous part we can apply the following algorithm to obtain a sample of @xmath540    ' '' ''    * algorithm to generate a weak approximation of * @xmath541    ' '' ''    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ * requires * :    the initial condition @xmath542    the optimal lvy measure @xmath543    the weak approximation method @xmath544 to solve @xmath545,y\\in \\mathbb{r}^{d}$ ]    * compute * @xmath546 and @xmath547    * set * @xmath548    * simulate * the next jump time @xmath549    * while * @xmath550",
    "* do *    * \\ { *    * compute * @xmath551    * simulate * @xmath552 a jump from the poisson random measure    with lvy measure @xmath20    * set * @xmath553    * set * @xmath554    * simulate * the next jump time @xmath549    * } *    * compute * @xmath555    * set * @xmath556    * return * @xmath557 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    ' '' ''    applying , independently , the previous algorithm @xmath558 times we obtain a sequence @xmath559 and the monte carlo estimator of @xmath560 $ ] is given by @xmath561    we end this section with some numerical examples .",
    "we evaluate @xmath560 $ ] , where @xmath9 is the solution of equation with @xmath562 and @xmath563 . to approximate the lvy process",
    ", we use the optimal schemes presented in section [ sec : explicit ] with @xmath418 , @xmath429 and @xmath440 , and denoted , respectively , by oa2 , oa3 and oa4 in the examples below . for solving the continuous sde between the times of jumps , we use the schemes wt1 , wt2 , wt3 , klv3 , klv5 and nv mentioned above .",
    "finally , the process @xmath6 is taken to be a cgmy process , which is a lvy process with no diffusion component and lvy density of the form @xmath564the third component of the characteristic triplet is chosen in such way that @xmath6 becomes a martingale .",
    "an algorithm for simulating the increments of @xmath6 is available @xcite , which makes it possible to compare our methods to the traditional euler scheme .",
    "also , this process satisfies the assumption @xmath475 of the previous section , and allows us to illustrate the dependence of the convergence rates on the parameter @xmath565 .",
    "actually , combining theorems [ theomain ]  and [ theomainregvar ] we have the following result .",
    "assume the hypotheses in theorems [ theomain ] , ii )  and theomainregvar , and choose @xmath566 and @xmath567 then , for @xmath288 even , we have that there exist positive constants @xmath568 and a slowly varying function @xmath569 such that @xmath570-\\mathbb{e}[f(\\widehat{x}_{1})]| \\\\ & \\leq & c\\left ( x\\right ) \\left\\vert f\\right\\vert _ { c_{p}^{n+1}}l(\\lambda ) \\lambda ^{1-\\frac{n}{\\alpha } } + k\\left ( x,\\mathcal{a},m\\right ) \\left\\vert f\\right\\vert _ { c_{p}^{2(m+1)}}\\lambda ^{-m},\\end{aligned}\\]]where",
    "@xmath571    we use @xmath572 simulation paths in all examples .",
    "for the euler scheme , all values are computed using the same set of paths with @xmath573 and @xmath574 discretization intervals . for the optimal schemes ,",
    "different paths are used for each point on the graph , and the different points are obtained by choosing the values of the parameter @xmath13 which correspond to the values of @xmath575 in the range @xmath576 $ ] . also , the computing time for each point has been normalized by the standard deviation of the mc estimate , so that the times for all points correspond to the time required to get a standard deviation of 0.001 .",
    "the variance of the mc estimate is about the same for all values computed with the optimal schemes . for the euler scheme ,",
    "the variance may be different , because , on one hand , the simulation method from @xcite makes use of a probability change which increases variance , and on the other hand , we use a variance reduction techique for the euler scheme ( by taking @xmath577 $ ] as control variate ) but not for the other schemes . in all the numerical examples below",
    "we take @xmath578 , @xmath579 , @xmath580 and @xmath581 .",
    "furthermore , for data set i , we take @xmath582 and @xmath583 ( finite variation jumps ) and for data set ii we take @xmath584 and @xmath585 ( infinite variation jumps ) .",
    "these two choices yield approximately the same variance of @xmath586 and allow us to understand the effect of @xmath565 on the convergence rate .    for our first example , we take @xmath587 and @xmath588 . in this case , @xmath9 is simply the stochastic exponential of @xmath589 , and the exact value of @xmath560 $ ] can be computed explicitly : @xmath590=e^{\\gamma_0 } $ ] .",
    "figure [ klv_hx_fx ] plots the errors of the klv schemes of different degrees and the nv scheme on a log - log scale for data sets i and ii . in this case , the three approximations of the lvy measure , oa2 , oa3 and oa4 , have very similar performance and we only plot the results for oa2 .",
    "this happens because with the choice @xmath591 , we have @xmath592=\\mathbb{e}[f(x_{1})]$ ] as soon as the approximation scheme for the lvy measure preserves the expectation of the lvy process , which is the case for all three approximation schemes oa1 , oa2 and oa3 .",
    "in other words , for this choice of @xmath364 and @xmath3 , the approximation of the lvy measure does not introduce any error .",
    "the error is therefore exclusively determined by the approximation scheme which is used between the jump times . however , in this case , the klv and nv methods perfom so well that all the errors are below the statistical error due to the monte carlo method and it is not even possible to identify the actual order of convergence .     and @xmath588 .",
    "left : parameters from data set i. right : parameters from data set ii .",
    ", title=\"fig:\",scaledwidth=55.0% ] and @xmath588 . left : parameters from data set i. right : parameters from data set ii .",
    ", title=\"fig:\",scaledwidth=55.0% ]    in our second example , we take @xmath587 still and @xmath593 .",
    "the exact value of @xmath560 $ ] can also be computed explicitly and is now equal to @xmath594 & = \\mathbb{e}[\\mathcal{e}(2z+[z , z])_{t}]=\\exp \\{e[2z_{t}]+e[[z , z]_{t}]\\ } \\\\ & = \\exp \\left\\ { 2\\gamma_0 t+\\sigma ^{2}t+t\\int_{\\mathbb{r}}y^{2}\\nu ( dy)\\right\\ } \\\\ & = \\exp \\left\\ { 2\\gamma_0 t+\\sigma ^{2}t+tc\\gamma ( 2-\\alpha ) ( \\lambda _ { + } ^{\\alpha -2}+\\lambda _ { -}^{\\alpha -2})\\right\\ } .\\end{aligned}\\]]figure [ wt_hx_fx2 ] plots the errors of the weak taylor schemes of different orders on a log - log scale for data sets i and ii , together with the theoretical error rates . in this case , one can clearly see the difference between the three schemes for approximating the lvy measure ( oa2 , oa3 and oa4 ) as well as the effect of the parameter @xmath565 .    for @xmath595 ( upper three graphs ) ,",
    "the error of approximating the lvy measure is of order of @xmath596 for oa2 , @xmath597 for oa3 and @xmath598 for oa4 .",
    "therefore , in these graphs , the global error is dominated by the one of approximating the diffusion part : we observe a clear improvement going from wt1 to wt2 and wt3 , and no visible change going from oa2 to oa3 and oa4 .    on the other hand , in the lower left graph , which corresponds to @xmath585 and @xmath418 , the error of approximating the lvy measure",
    "is of order of @xmath599 , which dominates the error of approximating the continuous sde for any of the three weak taylor schemes , and determines the slope of the curves in this graph . in this context , using the optimal scheme with @xmath429 ( lower middle graph ) or @xmath440 ( lower right graph ) leads to an substantial improvement of performance . in this case",
    ", we observe similar behavior for @xmath429 and @xmath440 because the lvy measure of @xmath6 is locally symmetric near zero , which means that @xmath600-moment scheme and @xmath601-moment scheme actually have the same convergence rate .",
    "the theoretical error rate of the euler scheme is always @xmath602 , which corresponds to the straight solid line on the graphs .",
    "the observed convergence rates appears slower than the theoretical prediction due to our variance reduction method , which has better performance when the number of discretization dates is small .     and @xmath593 .",
    "top : parameters from data set i. bottom : parameters from data set ii .",
    ", title=\"fig:\",scaledwidth=40.0% ] and @xmath593 .",
    "top : parameters from data set i. bottom : parameters from data set ii .",
    ", title=\"fig:\",scaledwidth=40.0% ] and @xmath593 .",
    "top : parameters from data set i. bottom : parameters from data set ii .",
    ", title=\"fig:\",scaledwidth=40.0% ]     and @xmath593 .",
    "top : parameters from data set i. bottom : parameters from data set ii .",
    ", title=\"fig:\",scaledwidth=40.0% ] and @xmath593 .",
    "top : parameters from data set i. bottom : parameters from data set ii .",
    ", title=\"fig:\",scaledwidth=40.0% ] and @xmath593 .",
    "top : parameters from data set i. bottom : parameters from data set ii .",
    ", title=\"fig:\",scaledwidth=40.0% ]",
    "in this section we present an auxiliary problem related with the moment matching of finite measures .    we define @xmath603 where @xmath604 are fixed real numbers .",
    "we want to compute @xmath605 , i.e. , the smallest intensity for which the moment constraints are feasible . this problem is very similar to the classical truncated hamburger moment problem and goes back to the works of chebyshev , markov and stieltjes .",
    "the known results on an infinite interval can be summarized as follows k - n77 :    [ momprobeven ] let @xmath606 and let @xmath607 be given .",
    "there exists a measure @xmath608 with @xmath609 if and only if the matrix @xmath610 is nonnegative definite .",
    "[ sylv.cor ] let @xmath606 , and let @xmath607 be given such that @xmath611 for some nonnegative measure @xmath7 .",
    "then there exists a measure @xmath608 with @xmath609 if and only if @xmath612 .    using proposition [ momprobeven ] ,",
    "it is enough to check that the the matrix @xmath610 is nonnegative definite . by the definition of @xmath613 for @xmath614 we have that the matrix @xmath615 is nonnegative definite .",
    "hence , by the sylvester s criterion applied to the lower right corner minors of the matrix @xmath610 , we have that in order for it to be nonnegative definite it is sufficient that @xmath612 .    [ exist.cor ] for @xmath616 as in corollary [ sylv.cor ] , the set of values @xmath617",
    "for which there exists a measure @xmath608 with @xmath609 is of the form @xmath618 .",
    "the case when @xmath288 is odd can be deduced from the previous one .",
    "let @xmath619 .",
    "there exists a measure @xmath608 with @xmath609 if and only if the matrix @xmath620 is nonnegative definite for some @xmath621 and @xmath622 .",
    "a simple matrix algebra computation then yields the following solutions for small @xmath288:@xmath623",
    "in this section we will assume the notation established in the first section .",
    "[ l_lp_uniform_bounds]assume that , for some @xmath140@xmath624 @xmath625.then , there exists a constant @xmath626 which does not depend on @xmath627 such that @xmath628 & \\leq c\\left ( 1+\\left\\vert x\\right\\vert ^{p}\\right ) , \\\\ \\mathbb{e}\\left [ \\sup_{0\\leq t\\leq1}\\left\\vert \\bar x_{t}\\right\\vert ^{p}\\right ] & \\leq c\\left ( 1+\\left\\vert x\\right\\vert ^{p}\\right ) .\\end{aligned}\\ ] ]      [ l_derivatives_lp_finiteness]let @xmath629 and for an integer @xmath630 assume @xmath631 @xmath632 .",
    "then for any multi - index @xmath98 with @xmath633 we have @xmath634}\\left\\vert \\frac{\\partial^{\\alpha}}{\\partial x^{\\alpha}}x_{1}\\left ( t , x\\right ) \\right\\vert ^{p}\\right ] < \\infty .\\ ] ]                        the derivative @xmath655 satisfies@xmath656.\\ ] ] the interchange of the derivative and the expectations is justified using lemma [ l_derivatives_lp_finiteness ] .",
    "furthermore , one obtains by a direct estimation the boundedness under @xmath117 or the polynomial growth under @xmath648 using lemmas l_lp_uniform_bounds and [ l_derivatives_lp_finiteness ] .",
    "the other derivatives with respect to @xmath657 are obtained by successive differentiation under the expectation and the derivative with respect to @xmath658 is obtained from it s formula applied to @xmath659 using lemma [ l_law_of_x ] .",
    "p. tankov .",
    "_ high order weak approximation schemes for lvy - driven sdes _ , in _ proceedings of the 9@xmath660 international conference on monte carlo and quasi - monte carlo methods in scientific computing _ , springer , 2011 ( to appear ) ."
  ],
  "abstract_text": [
    "<S> we consider a general class of high order weak approximation schemes for stochastic differential equations driven by lvy processes with infinite activity . </S>",
    "<S> these schemes combine a compound poisson approximation for the jump part of the lvy process with a high order scheme for the brownian driven component , applied between the jump times . </S>",
    "<S> the overall approximation is analyzed using a stochastic splitting argument . </S>",
    "<S> the resulting error bound involves separate contributions of the compound poisson approximation and of the discretization scheme for the brownian part , and allows , on one hand , to balance the two contributions in order to minimize the computational time , and on the other hand , to study the optimal design of the approximating compound poisson process . </S>",
    "<S> for driving processes whose lvy measure explodes near zero in a regularly varying way , this procedure allows to construct discretization schemes with arbitrary order of convergence .    </S>",
    "<S> * key words : * lvy - driven stochastic differential equations , high order discretization schemes , weak approximation , regular variation    * 2010 mathematics subject classification : * 65c30 , 60g51 </S>"
  ]
}