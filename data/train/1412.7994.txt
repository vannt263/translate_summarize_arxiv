{
  "article_text": [
    "a lattice @xmath9 is defined as the set of all integer combinations of some linearly independent vectors @xmath10 .",
    "the matrix @xmath11 is called a basis of @xmath9 , and we write @xmath12 for the lattice generated by @xmath13 .    perhaps the most central computational problem on lattices is the shortest vector problem ( @xmath14 ) .",
    "given a basis for a lattice @xmath15 , @xmath14 is to compute a non - zero vector in @xmath9 of minimum euclidean norm .",
    "starting in the 80s , the use of approximate and exact solvers for @xmath14 ( and other lattice problems ) gained prominence for their applications in algorithmic number theory  @xcite , coding over gaussian channels  @xcite , cryptanalysis  @xcite , combinatorial optimization and integer programming  @xcite . over the past decade and a half",
    ", the study of lattice problems greatly increased due to newly found applications in cryptography .",
    "many powerful cryptographic primitives , such as fully homomorphic encryption  @xcite , now have their security based on the _ worst - case _ hardness of approximating the decision version of @xmath14 ( and other lattice problems ) to within polynomial factors  @xcite .    from the computational complexity perspective",
    ", much is known about @xmath14 in both its exact and approximate versions . on the hardness side , was shown to be np - hard to approximate within any constant factor ( under randomized reductions ) and hard to approximate to within @xmath16 for some constant @xmath17 under reasonable complexity assumptions  @xcite . from the perspective of polynomial - time algorithms ,",
    "the celebrated lll basis reduction gives a @xmath18 approximation algorithm for @xmath14  @xcite , and schnorr s block reduction algorithm  @xcite , with subsequent refinements  @xcite , gives a @xmath19 approximation in @xmath20 time allowing for a smooth tradeoff between time and approximation quality .    as one would expect from the hardness results above , all known algorithms for",
    "solving exact , including the ones we present here , require at least exponential time and sometimes also exponential space ( and the same is true even for polynomial approximation factors ) .",
    "we mention in passing that despite running in exponential time , these algorithms have practical importance in addition to the obvious theoretical importance .",
    "for instance , they are used for assessing the practical security of lattice - based cryptographic primitives , they are used as subroutines in the best current approximation algorithms ( variants of block reduction ) , and they are used in some applications where low - dimensional lattices naturally arise .    while the state of the art for polynomial - time approximation of lattice problems has remained relatively static over the last two decades , the situation for exact algorithms has been markedly different . indeed",
    ", three major ( and very different ) classes of algorithms for @xmath14 have been developed .    the first class , developed by kannan  @xcite and refined by many others  @xcite , is based on combining strong basis reduction with exhaustive enumeration inside euclidean balls .",
    "the fastest current algorithm in this class solves @xmath14 in @xmath21 time while using @xmath22 space  @xcite .",
    "the next landmark algorithm , developed by ajtai , kumar , and sivakumar  @xcite ( henceforth aks ) , is the most similar to this work .",
    "aks devised a method based on `` randomized sieving , '' whereby exponentially many randomly generated lattice vectors are iteratively combined to create shorter and shorter vectors , to give the first @xmath18-time ( and space ) randomized algorithm for @xmath14 .",
    "many extensions and improvements of their sieving technique have been proposed , both provable  @xcite and heuristic  @xcite , where the fastest provable sieving algorithm  @xcite for exact @xmath14 requires @xmath23 time and @xmath24 space .",
    "it was observed by @xcite that aks can be modified to obtain a @xmath25-time and @xmath26-space algorithm for approximating @xmath14 to within some large constant factor .",
    "here @xmath27 corresponds to the best known upper bound on the @xmath1-dimensional `` kissing number '' ( the maximum number of points one can place on the unit sphere such that the pairwise distances are @xmath28 ) due to kabatjanski and leventen  @xcite .    the most recent breakthrough , due to micciancio and voulgaris  @xcite ( henceforth mv ) and built upon the approach of sommer , feder , and shalvi  @xcite , is a deterministic @xmath2-time and @xmath3-space algorithm for @xmath14 .",
    "it uses the _ voronoi cell _ of the lattice  the centrally symmetric polytope corresponding to the points closer to the origin than to any other lattice point .    [",
    "[ main - contribution . ] ] main contribution .",
    "+ + + + + + + + + + + + + + + + + +    as our main result , we give a randomized @xmath0-time and space algorithm for exact svp , improving on the @xmath2 deterministic running time of mv . a second main result is a much faster @xmath7-time ( and space ) algorithm that approximates the decision version of @xmath14 to within a small constant factor .",
    "our @xmath0-time algorithm actually solves a more difficult problem , namely , that of generating many discrete gaussian samples from a lattice with arbitrary parameter , as we describe below . we feel that this is even more interesting than the improved running time for @xmath14 , and it should have further applications . as far as we are aware , outside of security reductions having access to powerful oracles",
    ", this is the first provable algorithm to use the discrete gaussian _ directly _ to solve a classical lattice problem .    [ [ discrete - gaussian - samplers . ] ] discrete gaussian samplers .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + +    our first main technical contribution is a general discrete gaussian sampler , which will directly imply our @xmath14 algorithm .",
    "below , we give an informal description of this result .",
    "( see section  [ sec : main - dgs ] for the details . )",
    "define @xmath29 and @xmath30 for any discrete set @xmath31 .",
    "the discrete gaussian distribution @xmath32 over the lattice @xmath15 with parameter @xmath33 is the distribution satisfying @xmath34 = \\rho_s({\\ensuremath{\\mathbf{x}}})/\\rho_s({\\mathcal{l}}),\\quad   \\forall { \\ensuremath{\\mathbf{x } } } \\in { \\mathcal{l}}\\text{.}\\ ] ] see figure  [ fig : dgs ] for an illustration .",
    "the parameter @xmath33 determines the `` width '' of the discrete gaussian .",
    "note that as @xmath33 becomes smaller , @xmath32 becomes more and more concentrated on short lattice vectors .",
    "hence it should not come as a surprise that being able to obtain sufficiently many samples from @xmath32 for an arbitrary @xmath33 leads to a solution to .",
    "we will discuss this relatively natural reduction below , but first let us describe our main technical contributions , the gaussian samplers .",
    "there is an algorithm that takes as input a lattice @xmath35 and _ any parameter _ @xmath36 and outputs @xmath4 i.i.d .",
    "samples from @xmath32 using @xmath0 time and space .",
    "[ thm : gen - dgs - inf ]    notice the amortized aspect of the algorithm : we obtain @xmath4 vectors in about @xmath37 time .",
    "we do not know how to reduce the time to @xmath38 even if all we want is just one vector ! ( but see below for a faster algorithm that works for large parameters . ) improving the running time of the algorithm ( while still outputting a sufficiently large number of samples ) would immediately translate into an improved algorithm .    as we explain below , a closer inspection of the technique used in our algorithm",
    "suggests that with some refinement it might be able to achieve a running time of @xmath4 .",
    "indeed , we actually do achieve this , but only for sufficiently large parameters @xmath33 .",
    "in fact , for such parameters , we actually manage to output @xmath4 samples in @xmath7 time .",
    "this is our second main technical contribution .",
    "there is an algorithm that takes as input a lattice @xmath35 and a parameter @xmath39 _ above the smoothing parameter _ of @xmath9 and outputs @xmath4 i.i.d .",
    "samples from @xmath32 using @xmath7 time and space .",
    "[ thm : smooth - dgs - inf ]    the smoothing parameter is the value of @xmath33 above which @xmath32 `` looks like '' a continuous gaussian in a certain precise mathematical sense .",
    "( see definition  [ def : smooth ] . )",
    "while sampling above smoothing appears to be insufficient to solve exact lattice problems , it is enough to solve major lattice problems approximately .",
    "indeed , we show how this is sufficient to approximate the decision version of to within a constant factor in time @xmath7 ( with the constant being roughly @xmath8 ) .",
    "this holds the record for the fastest provable running time of a hard lattice problem .",
    "we note that theorem  [ thm : smooth - dgs - inf ] actually implies a slightly stronger result ; we can obtain @xmath4 samples from the gaussian over a lattice _ shift _",
    ", @xmath40 , in @xmath41 time and space , as long as @xmath33 is above smoothing .",
    "however , we know of no application of this slightly stronger result .",
    "( see section  [ sec : shifted ] for a proof sketch . )",
    "the task of discrete gaussian sampling is by no means new .",
    "it by now has a long history within cryptography @xcite . by analyzing an algorithm of klein  @xcite , gentry , peikert , and",
    "vaikuntanathan  @xcite first showed how to solve in polynomial time for large parameters .",
    "( we remark that klein analyzed this algorithm for very small parameters and used it to solve the bdd problem . for such parameters ,",
    "the algorithm does not produce samples distributed according to the discrete gaussian distribution . )",
    "has been used extensively to improve reductions from worst - case lattice problems ( such as approximate decisional @xmath14 ) to the average - case short integer solution ( sis ) and learning with errors ( lwe ) problems @xcite , and as a core subroutine for instantiating certain cryptographic primitives  @xcite . in all previous works ,",
    "the procedure either samples at very high parameters or requires _ a priori knowledge of a relatively short lattice basis_typically only available when a user is able to generate the lattice themselves , such as in certain trapdoor schemes  or access to _ powerful oracles _ , such as sis or lwe oracles .",
    "furthermore , even with oracles and a short basis , none of the algorithms from prior work could be used to sample below the _ smoothing parameter _ of the lattice .",
    "the reason that we are able to achieve this is because of our observation that , if we allow ourselves exponential time , we can carefully combine vectors sampled from a discrete gaussian together to obtain vectors whose distribution is _ exactly _ a discrete gaussian with a smaller parameter .",
    "( see lemma  [ lem : sumofgaussians ] and the proof overview below . )",
    "all prior work only obtained a distribution that is _ statistically close _ to the discrete gaussian , with error that is unbounded below the smoothing parameter .",
    "we note that our approach is similar to that of the aks algorithm at a high level .",
    "in particular , like aks , we use a sieve algorithm that starts with a large collection of randomly selected vectors and proceeds to combine them together in pairs to find short lattice vectors . the major important difference between our approach and that of the aks algorithm and its derivatives is that we maintain complete control over the distribution of the lattice points that we generate at each step . while prior work is focused ( quite naturally ) on controlling the _ lengths _ of the vectors after each step , our algorithm actually completely ignores their lengths  choosing whether to combine two vectors based only on their _ coset _ mod a sublattice",
    ".    indeed , we view our @xmath0-time algorithm as an efficient discrete gaussian sampler that consequently yields an efficient solution to , rather than as a sieve algorithm for .",
    "it is the simplicity and elegance of the discrete gaussian distribution that allows us to side - step many of the complications that arise with other sieve algorithms ( such as the `` perturbation '' step ) .",
    "indeed , the @xmath0-time algorithm is quite simple ; the most technical tool that it uses is a simple subroutine that we call the `` square sampler '' ( described below ) .",
    "one negative aspect of our approach is that it has a clear lower bound .",
    "it seems that we can not use this approach to find any algorithm that runs in time less than @xmath4 . and",
    ", the quoted running time of each algorithm ( @xmath0 and @xmath41 respectively ) is essentially tight in both theory and practice  for large ( and relevant ) parameters , our sieves yield essentially nothing when their input consists of fewer than @xmath37 or @xmath4 vectors respectively .",
    "this is in contrast to aks - style algorithms , which seem to perform well heuristically  @xcite .",
    "we now include a high - level description of our proofs , first that of theorem  [ thm : gen - dgs - inf ] and then that of the more refined theorem  [ thm : smooth - dgs - inf ] .",
    "we end with a brief discussion of how to use gaussian samples to solve as well as other applications .",
    "[ [ a-2non - time - combiner - for - dgs . ] ] a @xmath0-time combiner for dgs .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    recall that efficient algorithms are known for sampling from the discrete gaussian at very high parameters @xcite .",
    "it therefore suffices to find a way to efficiently _ convert _ samples from the discrete gaussian with a high parameter to samples with a parameter lowered by a constant factor . by repeating this `` conversion '' many times",
    ", we can obtain samples with much lower parameters .",
    "note that this is trivial to do for the continuous gaussian : if we divide a vector sampled from the continuous gaussian distribution by @xmath42 , the result is distributed as a continuous gaussian with half the width . of course",
    ", half of a lattice vector is not typically in the lattice , so this method fails spectacularly when applied to the discrete gaussian .",
    "but , we can try to fix this by _ conditioning _ on the result staying in the lattice .",
    "i.e. , we can sample many vectors from @xmath43 , keep those that are in the `` doubled lattice '' @xmath44 , and divide them by two .",
    "this method does work , but it is terribly inefficient  there are @xmath37 cosets of @xmath44 , and for some typical parameters , a sample from @xmath43 will land in @xmath44 with probability as small as @xmath45 .",
    "i.e. , our `` loss factor , '' the ratio of the number of output vectors to the number of input vectors , can be as bad as @xmath45 for a single step . if we wish to iterate this @xmath46 times , we could need @xmath47 input vectors for each output vector , resulting in a very slow algorithm",
    "!    we can be much more efficient , however , if we instead look for _ pairs _ of vectors sampled from @xmath43 whose _ sum _ is in @xmath44 , or equivalently pairs of vectors that lie in the same coset @xmath48 mod @xmath44 .",
    "taking our intuition from the continuous gaussian , we might hope that the _ average _ of two such vectors will be distributed as @xmath49 .",
    "this suggests an _ amortized _ algorithm , in which we sample many vectors from @xmath32 , place them in `` buckets '' according to their coset mod @xmath44 , and then take the average of disjoint pairs of elements in the same bucket .",
    "we call such an algorithm a `` combiner . ''",
    "the most natural combiner to consider is the `` greedy combiner , '' which simply pairs as many vectors in each bucket as it can , leaving at most one unpaired vector per bucket .",
    "since there are @xmath37 cosets , if we take , say , @xmath50 samples from @xmath43 , almost all of the resulting vectors will be paired .",
    "a lemma due to peikert ( @xcite ) shows that the resulting distribution will be statistically close to the desired distribution , @xmath49 , _ provided that the parameter @xmath33 is above the smoothing parameter_. at this point , we can already build a roughly @xmath37-time algorithm for that works for such parameters .",
    "( namely , use prior work to sample at some very high parameter and iteratively apply the combiner described above . ) while this is not our main result ( it is strictly weaker ) , we note that we have not seen this observation mentioned elsewhere .- time algorithm for @xmath51 above the smoothing parameter by instantiating the oracles in  @xcite . ] but , in order to move _ below smoothing _",
    "( which is necessary , e.g. , for solving ) , we need to do something else .",
    "@xmath52{rotationlegend } } \\includegraphics[width=.40 \\textwidth]{rotationprime1 } \\quad \\raisebox{10 ex}{\\includegraphics[width = .1 \\textwidth]{arrow } } \\quad \\includegraphics[width=0.40 \\textwidth]{rotationprime2}\\ ] ]    in particular , below the smoothing parameter , combining discrete gaussian vectors `` greedily '' as above will not typically give a result that is statistically close to a gaussian distribution .",
    "however , all is not lost .",
    "recall that our algorithm works by picking pairs of vectors sampled independently from @xmath43 that are in the same coset @xmath48 mod @xmath44 , and then taking the average of each pair .",
    "so , the algorithm effectively samples a vector @xmath53 from _ some _ distribution over the @xmath54-dimensional lattice of pairs of vectors from @xmath9 that are in the same coset mod @xmath44 , @xmath55 and then outputs @xmath56 .",
    "we claim that _ assuming that that distribution is _",
    "@xmath57 , the output @xmath56 is distributed _ exactly _ as @xmath49 .",
    "this fact , shown in lemma  [ lem : sumofgaussians ] , has a straightforward proof , yet we have not seen this observation before .",
    "( it is closely related to riemann s theta relations , as described in  ( * ? ? ?",
    "* chapter 1 , section 5 ) ; see also  @xcite ) the idea is the following .",
    "it is not difficult to show that if we apply the @xmath58 rotation given by @xmath59 to @xmath60 , we obtain the _ product _ lattice @xmath61 .",
    "( figure  [ fig : rotation ] shows the one - dimensional case . ) note that the rotation of a discrete gaussian is again a discrete gaussian , and the discrete gaussian over a product lattice is a product distribution .",
    "therefore , if @xmath53 is distributed according to @xmath62 , the distribution of @xmath63 is according to @xmath64 which is a product distribution , and hence @xmath65 is distributed according to @xmath66 , as claimed .",
    "however , note that if the combiner just greedily paired as many vectors from each coset as possible , it would _ not _ yield samples from @xmath57 . in particular , the probability that a sample from @xmath62 will land in @xmath67 for some coset @xmath48 is proportional to the `` squared weight '' of the coset @xmath68 .",
    "but , the greedy approach pairs vectors from @xmath69 with probability roughly proportional to @xmath70 .",
    "( figure  [ fig : greedybargraph ] shows how the resulting distributions differ in the one - dimensional case . ) for parameters above smoothing , these distributions are roughly the same , but to go below smoothing ( and to avoid the statistical error resulting from the greedy approach ) , we need a way to sample pairs from this `` squared distribution '' directly .",
    "this mismatch between the `` squared distribution '' that we want and the `` unsquared '' distribution that we get is the primary technical challenge that we must overcome to build our general discrete gaussian combiner . to solve it",
    ", we present a generic solution for `` converting any probability distribution to its square '' relatively efficiently , which we call the `` square sampler . ''",
    "informally , the square sampler is given access to samples from some probability distribution that assigns respective ( unknown ) probabilities @xmath71 to the elements in some ( large ) finite set @xmath72 .",
    "it uses this to efficiently sample a large collection of _ independent _ coin flips @xmath73 such that @xmath74 with probability proportional to @xmath75 .",
    "then , using these coins , it applies rejection sampling to the input samples ( accepting the @xmath76th instance of input value @xmath77 if @xmath74 ) in order to obtain the desired `` squared distribution . '' if @xmath78 = t p_i$ ] for some proportionality factor @xmath79 , it is not hard to see that the expected `` loss factor '' of this process is @xmath80 .",
    "we therefore take @xmath79 to be as large as possible by setting @xmath81 ( if we took @xmath79 to be any larger , we would need a coin that lands on heads with probability greater than one ! ) , making the loss factor of the square sampler approximately @xmath82 .",
    "( see section  [ sec : squaresampler ] and theorem  [ thm : squaresampler ] in particular . )    in particular , when combining discrete gaussian vectors , the loss factor is approximately the _ collision probability _ over the cosets @xmath48 of @xmath44 , @xmath83 , divided by the maximal probability of a single coset . as a result ,",
    "if one coset has a @xmath84 fraction of the total weight and the other cosets split the remaining weight roughly evenly , then the loss factor is roughly @xmath85 _ for a single step of the combiner_. this looks terrible for us , as it could be the case that @xmath46 applications of the combiner could yield a loss factor of @xmath86 !",
    "surprisingly , we show that the product of all loss factors for an arbitrarily long sequence of applications of the combiner is at worst @xmath84 ( ignoring loss due to other factors ) .",
    "i.e. , the accumulated loss factor can be no worse than essentially the worst - case loss factor in a single step ! ) , we do not yet have good intuitive understanding of it .",
    "indeed , we have found ourselves referring to it as the `` magic cancellation . '' ] as a result , our general combiner always returns @xmath4 vectors when its input is @xmath5 vectors sampled from the discrete gaussian .",
    "( see corollary  [ cor : pipeline ] for the formal analysis of repeated application of our combiner . )",
    "[ [ a-2n2on - time - combiner - for - dgs - above - smoothing . ] ] a @xmath7-time combiner for dgs above smoothing .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    recall that the general combiner described above starts with many vectors and then repeatedly takes the average of pairs of vectors that lie in the same coset of @xmath44 .",
    "we observed that this combiner necessarily needs over @xmath87 vectors `` just to get started '' because it works over the @xmath37 cosets of @xmath44 . to get a faster combiner",
    ", we therefore try pairing vectors according to the cosets of some sublattice @xmath88 that `` lies between '' @xmath9 and @xmath44 such that @xmath89 . if we simply take many samples from @xmath43 , group them according to their cosets mod @xmath88 , and sum them together ( taking averages is a bit less natural in this context ) , analogy with the continuous gaussian suggests that the resulting vectors will be distributed as roughly @xmath90 .",
    "note that the parameter has increased , which is not what we wanted , but we are now sampling from a sparser lattice .",
    "in particular , suppose that we apply this combiner twice , so that in the second step we obtain vectors from some sublattice @xmath91 .",
    "we then expect to obtain samples from roughly @xmath92 .",
    "so , intuitively , if we take @xmath91 to be a sublattice of @xmath44 , we have `` made progress , '' even though we have doubled the parameter . our running time will be proportional to the index of @xmath88 over @xmath9 ( assuming that the index of @xmath91 over @xmath88 is the same ) , so we should take the index of @xmath88 over @xmath9 to be as small as possible . more specifically ,",
    "we can build a `` tower '' of progressively sparser lattices @xmath93 with the index of @xmath94 over @xmath95 taken to be slightly larger than @xmath4 .",
    "if we take @xmath96 to be the lattice from which we wish to obtain samples with parameter @xmath33 and @xmath97 to be a dense lattice from which we can sample efficiently with parameter @xmath98 , we can hope that iteratively applying such a combiner `` up the tower '' will yield a sampling algorithm .",
    "as in the description of our @xmath37-time combiner , the lemma from  @xcite shows that the above approach , when instantiated with the `` greedy combiner , '' will yield an algorithm that can output vectors whose distribution is statistically close to the discrete gaussian for parameters @xmath33 that are above the smoothing parameter .",
    "though this statistical distance can be made small , it is large enough to break applications such as our approximation algorithm for decision @xmath14 .    to avoid this error",
    ", the natural hope is that the same combiner used in the @xmath37-time algorithm above ( the one with the `` square sampler '' ) will suffice .",
    "unfortunately , this gives the wrong distribution .",
    "in particular , we obtain a distribution in which the cosets of @xmath88 over @xmath44 have weight that is proportional to the _ square _ of their weights over the discrete gaussian .",
    "( see lemma  [ lem : anysublattice ] .",
    "note that when @xmath99 there is only one such coset , which is why our @xmath37-time combiner does not run into this problem . ) in some sense , this is the `` inverse '' of the problem that the square sampler solves . and , indeed",
    ", we solve it by building a `` square - root sampler''based on a clever trick ( used implicitly in  @xcite and discussed in  @xcite ) that allows one to flip a coin with probability @xmath100 given black - box access to a coin with unknown probability @xmath101 .",
    "( see claim  [ clm : sqrt ] for the trick and theorem  [ thm : sqrtsampler ] for the square - root sampler . )",
    "so , our combiner works by `` squaring the weights '' of the cosets mod @xmath88 of input vectors ; pairing them according to these squared weights and summing the pairs ; and then `` taking the square root of the weights '' of the cosets mod @xmath44 of the resulting output vectors .",
    "this completes the description of the proof of theorem  [ thm : smooth - dgs - inf ] .",
    "the above only works above the smoothing parameter because the required size of the input to the square - root sampler depends on @xmath102 , where @xmath103 is the probability of landing in the coset with minimal weight .",
    "we therefore only know how to use the square - root sampler to efficiently sample above the smoothing parameter , where the minimal weight is roughly equal to the maximal weight . indeed , in this regime ,",
    "both the square - root sampler and the square sampler incur `` almost no loss , '' so that we obtain an algorithm that runs in time @xmath41 and returns @xmath4 samples from the discrete gaussian .",
    "but , below this , @xmath102 can be arbitrarily large .",
    "( intuitively , some sort of dependence on @xmath102 is necessary for a square - root sampler because a coset whose weight is negligible could have significant weight after we `` take the square root . ''",
    "so , we should expect that any square - root sampler would need at least enough samples to `` see '' such a coset . )",
    "however , we again stress that our techniques do not incur error that depends on `` how smooth the distribution is . ''",
    "this leaves open the possibility that our algorithm might be modified to work even _ below _ smoothing .",
    "the only bottleneck is that the square - root sampler requires very large input in such cases .",
    "but , we note that the way that we currently use the square - root sampler might not be optimal .",
    "more specifically , we observe the rather strange behavior of our current algorithm : when the algorithm `` takes the square root '' of some coset weights , it typically `` squares '' the weights of some ( different ! ) cosets immediately afterwards .",
    "so , while the second step is not the exact inverse of the first , it does still seem that the square - root step is a bit counterproductive .",
    "this suggests that there is room for improvement in this algorithm , and we have made some progress to that end by proving a correlation inequality that we believe should play a central role in such an improved algorithm  @xcite .",
    "finally , we note that this algorithm can actually be used to obtain @xmath4 samples from the _ shifted _ discrete gaussian @xmath40 for any @xmath104 and parameter @xmath33 above smoothing in @xmath7 time .",
    "we know of no applications for this , but we present a proof sketch in section  [ sec : shifted ] for completeness .",
    "[ [ reduction - from - svp - to - dgs . ] ] reduction from svp to dgs .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + +    as we mentioned above , if we could efficiently sample from @xmath32 at the _",
    "right parameter _ , we can hope that a @xmath32 sample will hit a shortest non - zero vector of @xmath9 with reasonable probability .",
    "one can quickly see here that there is an important trade - off in the choice of @xmath33 . for @xmath33 too small",
    ", the discrete gaussian becomes completely concentrated on @xmath105 , whereas for @xmath33 too large , the distribution becomes too diffuse over @xmath9 and will rarely hit a shortest vector . by properly choosing @xmath33 , we show that the discrete gaussian yields a shortest non - zero lattice vector with probability @xmath106 .",
    "( in section  [ sec : boundonmass ] , we note that the optimal parameter has a nice interpretation in terms of the smoothing parameter . )",
    "since our algorithm returns @xmath4 vectors in time @xmath5 , we obtain a @xmath5-time algorithm for .    in order to obtain this bound",
    ", we use the result of kabatjanski and leventen that achieves the current best upper bound on the kissing number  @xcite .",
    "( the kissing number bounds from above the maximal number of shortest non - zero vectors in a lattice . note that the reciprocal of the latter is a natural upper bound on the above probability . ) at a high level , this is essentially the same problem faced by the randomized sieving algorithms , and our techniques are very similar to those developed there ( in particular those in  @xcite ) .",
    "[ [ reduction - from - decision - svp - to - dgs - above - smoothing . ] ] reduction from decision svp to dgs above smoothing .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    in order to approximate the length of the shortest non - zero lattice vector to within a constant factor , we note ( in lemma  [ lem : etalambda1 ] ) that it suffices to approximate the smoothing parameter of the dual lattice ( for exponentially small @xmath107 ) to within a constant factor . of course",
    ", if we had a @xmath4-time discrete gaussian sampler that worked above smoothing and always failed below smoothing , then it would be trivial to use this to approximate the smoothing parameter .",
    "however , while our @xmath41-time sampler does in fact always work above smoothing , it is not a priori clear how it behaves when asked to provide samples below smoothing .",
    "we handle this problem as follows .",
    "first , while we can not guarantee that our sampler always fails below smoothing , we show ( with a bit more work ) that it always either fails or outputs valid discrete gaussian samples .",
    "we call such a sampler `` honest . ''",
    "( see definition  [ def : hdgs ] . )",
    "second , we show a simple test that can distinguish between the discrete gaussian distribution with parameter slightly above smoothing and the discrete gaussian with parameter below smoothing .",
    "( see lemma  [ lem : smoothcovariance ] . ) with this , we obtain an @xmath108-approximation algorithm for the smoothing parameter that runs in @xmath7 time .    [",
    "[ further - applications . ] ] further applications .",
    "+ + + + + + + + + + + + + + + + + + + + +    another fundamental problem on lattices is the closest vector problem ( cvp ) , in which we must find a closest lattice vector to some target vector @xmath109 .",
    "@xmath110 is known to be at least as hard as @xmath14 , as there is a polynomial - time approximation - preserving reduction from @xmath14 to @xmath110  @xcite .",
    "furthermore , almost all of the major lattice problems reduce to @xmath110 in this way  @xcite .    the fastest exact algorithm for @xmath110",
    "is again the @xmath2-time and @xmath3-space algorithm due to micciancio and voulgaris  @xcite ( which in fact more directly solves @xmath110 than @xmath14 ) . for approximation factor @xmath111 for @xmath112 ,",
    "randomized sieving techniques have been shown capable of solving @xmath113-approximate @xmath110 in @xmath114 time and space  @xcite , though little effort has been made to optimize the constant in the exponent .",
    "based on an embedding trick of kannan  @xcite and standard concentration bounds on the discrete gaussian , we show how to use our sampler to solve @xmath6-approximate in time @xmath0 . as mentioned above , the reductions of  @xcite show that this yields the same approximation factor and running time for almost all lattice problems .",
    "also , our algorithm from theorem  [ thm : smooth - dgs - inf ] gives @xmath41-time algorithms for @xmath115- ( corollary  [ cor : bdd ] ) and @xmath116-approximate ( corollary  [ cor : sivp ] ) .",
    "our work raises many questions and potential avenues for improvement .",
    "firstly , we suspect that the algorithm from theorem  [ thm : smooth - dgs - inf ] can be modified to work for an arbitrary parameter @xmath33 with the same running time of roughly @xmath4 ( at least to sample a single vector )",
    ". such a result would subsume theorem  [ thm : gen - dgs - inf ] and would lead to an improved algorithm for , as well as other problems .",
    "we have made some modest progress towards proving this , but a solution still seems far .",
    "another central open problem is whether @xmath14 can be solved in singly exponential time but only polynomial space .",
    "the best running time known for polynomial - space algorithms is the @xmath117 obtained by enumeration - based methods  @xcite .",
    "finally , this work shows that discrete gaussian sampling is a lattice problem of central importance .",
    "however , for parameters below smoothing is not nearly as well - understood as many other lattice problems , and many natural questions remain open .",
    "for example , is there a dimension - preserving reduction from to ? ( this question was answered by @xcite after a preliminary version of this work appeared . ) is ( centered ) np - hard ?    [ [ follow - up - work . ] ] follow - up work . + + + + + + + + + + + + + + +    in a follow - up work by three of us  @xcite we generalize theorem  [ thm : gen - dgs - inf ] by presenting a @xmath0-time algorithm to sample from the _ shifted _ discrete gaussian @xmath40 for any @xmath104 and @xmath118 . as an application of that algorithm",
    ", we show in  @xcite how to obtain a @xmath0-time algorithm for _ exact _ ( which is a harder problem than @xmath14 , as follows from the dimension - preserving reduction in  @xcite ) . while those results are stronger than some of the results presented in this paper , the proofs in  @xcite are also significantly more involved .",
    "[ [ organization . ] ] organization .",
    "+ + + + + + + + + + + + +    in section  [ sec : prelims ] , we overview the necessary background material and give the basic definitions used throughout the paper . in section  [ sec : main - dgs ] , we give our general @xmath0-time dgs sampler ( theorem  [ thm : dgs ] ) . in section  [ sec :",
    "svp ] , we prove our bound on the number of discrete gaussian samples needed for @xmath14 ( lemma  [ lem : gaussian - sum - bound ] and proposition  [ prop : boundonmass - kl ] ) and give our reduction from to ( theorem  [ thm : svptodgs ] ) . in section  [ sec : abovesmooth ] , we give our @xmath7-time dgs sampler for parameters above smoothing ( theorem  [ thm : smoothdgs ] ) . in section  [ sec : gapsvp ] , we show our reduction from to above smoothing ( theorem  [ thm : gapsvp ] ) .",
    "finally , in section  [ sec : other ] , we show our @xmath0-time algorithm for @xmath6-approximate @xmath110 ( theorem  [ thm : cvptodgs ] ) and our @xmath41-time algorithms for @xmath115- ( corollary  [ cor : bdd ] ) and @xmath116-approximate ( corollary  [ cor : sivp ] ) .",
    "let @xmath119 . except where we specify otherwise , we use @xmath120 , @xmath121 , and @xmath122 to denote universal positive constants , which might differ from one occurrence to the next .",
    "we use bold letters @xmath123 for vectors and denote a vector s coordinates with indices @xmath124 . throughout the paper",
    ", @xmath1 will always be the dimension of the ambient space @xmath125 .",
    "a rank @xmath126 lattice @xmath35 is the set of all integer linear combinations of @xmath126 linearly independent vectors @xmath127 .",
    "@xmath13 is called a basis of the lattice and is not unique .",
    "formally , a lattice is represented by a basis @xmath13 for computational purposes , though for simplicity we often do not make this explicit . if @xmath128 , we say that the lattice has full rank , and we often assume this as results for full - rank lattices naturally imply results for arbitrary lattices .    given a basis , @xmath129 , we write @xmath130 to denote the lattice with basis @xmath129 .",
    "the length of a shortest non - zero vector in the lattice is written @xmath131 . for a vector @xmath104 , we write @xmath132 to denote the distance between @xmath109 and the lattice , @xmath133 .    for a lattice @xmath35 ,",
    "the dual lattice , denoted @xmath134 , is defined as the set of all points in @xmath135 that have integer inner products with all lattice points , @xmath136 similarly , for a lattice basis @xmath137 , we define the dual basis @xmath138 to be the unique set of vectors in @xmath135 satisfying @xmath139 .",
    "it is easy to show that @xmath134 is itself a rank @xmath126 lattice and @xmath140 is a basis of @xmath134 .    for a lattice @xmath9 ,",
    "the @xmath77th successive minimum of @xmath9 is @xmath141    in other words , the @xmath77th successive minimum of @xmath9 is the smallest value @xmath142 such that there are @xmath77 linearly independent vectors in @xmath9 of length at most @xmath142 .      for any @xmath118",
    ", we define the function @xmath143 as @xmath144 . when @xmath145 , we simply write @xmath146 . for a discrete set @xmath147",
    "we define @xmath148 .    for a lattice @xmath35 and a vector @xmath104 ,",
    "let @xmath149 be the probability distribution over @xmath150 such that the probability of drawing @xmath151 is proportional to @xmath152 .",
    "we call this the discrete gaussian distribution over @xmath150 with parameter @xmath33 .",
    "we make frequent use of the discrete gaussian over the cosets of a sublattice .",
    "if @xmath153 is a sublattice of @xmath9 , then the set of cosets , @xmath154 is the set of translations of @xmath88 by lattice vectors , @xmath155 for some @xmath156 .",
    "it is easily seen from the poisson summation formula that for any @xmath157 , @xmath158 , i.e. , the zero coset has maximal weight ( see , e.g. , @xcite ) .",
    "we use this fact throughout the paper .",
    "in particular , it follows that @xmath159 .",
    "banaszczyk proved the following two bounds on the discrete gaussian  @xcite .",
    "[ lem : banaszczyk ] for any lattice @xmath160 and @xmath161 , @xmath162    [ lem : banaszczyktail ] for any lattice @xmath160 , @xmath163 , @xmath164 , and @xmath165 , @xmath166 < \\frac{\\rho_s({\\mathcal{l}})}{\\rho_s({\\mathcal{l}}+ { \\ensuremath{\\mathbf{u}}})}\\big ( \\sqrt{2 \\pi e t^2 } \\exp(-\\pi t^2 ) \\big)^n \\ ; .\\ ] ]    [ def : smooth ] for a lattice @xmath35 and @xmath167 , we define the smoothing parameter @xmath168 as the unique value satisfying @xmath169 .    we note that if @xmath153 , then @xmath170 , and we have @xmath171 .",
    "the name smoothing parameter comes from the following fact .",
    "[ clm : smooth ] for any lattice @xmath35 and @xmath172 , if @xmath173 , then for all @xmath104 , @xmath174    finally , we will need the following basic bounds on the smoothing parameter , the first of which is essentially the same as ( * ? ? ?",
    "* lemma 2.4 ) .",
    "[ lem : doublesmooth ] for any lattice @xmath35 , @xmath172 , and @xmath175 , we have @xmath176 .",
    "suppose without loss of generality that @xmath177 .",
    "then , @xmath178    [ lem : etalambdansqrt ] for any lattice @xmath35 and @xmath179 , @xmath180    suppose @xmath181 .",
    "then there exists a @xmath164 such that @xmath182 .",
    "then , using lemma  [ lem : banaszczyktail ] , @xmath183 using claim  [ clm : smooth ] , this gives @xmath184 which is a contradiction .",
    "[ lem : subgaussianity ] for any lattice @xmath35 , @xmath163 , @xmath185 , and unit vector @xmath186 , @xmath187 \\leq 2 e^{-\\pi t^2/s^2 } \\ ; .\\ ] ]      given a basis , @xmath188 , we define its gram - schmidt orthogonalization @xmath189 by @xmath190 here , @xmath191 is the orthogonal projection on the subspace @xmath192 and @xmath193 denotes the subspace orthogonal to @xmath194 .",
    "the following problem plays a central role in this paper .",
    "[ def : dgs ] for @xmath195 , @xmath196 a function that maps lattices to non - negative real numbers , and @xmath197 , @xmath198 ( the discrete gaussian sampling problem ) is defined as follows : the input is a basis @xmath13 for a lattice @xmath35 and a parameter @xmath199 .",
    "the goal is to output a sequence of @xmath200 vectors whose joint distribution is @xmath107-close to @xmath201 .",
    "we omit the parameter @xmath107 if @xmath202 , the parameter @xmath203 if @xmath204 , and the parameter @xmath200 if @xmath205 .",
    "we stress that @xmath107 bounds the statistical distance between the _ joint _ distribution of the output vectors and @xmath200 independent samples of @xmath32 .    for our applications , we consider the following lattice problems .",
    "the search problem @xmath14 ( shortest vector problem ) is defined as follows : the input is a basis @xmath13 for a lattice @xmath35 .",
    "the goal is to output a vector @xmath156 with @xmath206 .",
    "for @xmath207 ( the approximation factor ) , the decision problem @xmath208 is defined as follows : the input is a basis @xmath13 for a lattice @xmath35 and a number @xmath209 .",
    "the goal is to output yes if @xmath210 and no if @xmath211 .    for @xmath207 ( the approximation factor ) , the search problem @xmath212 ( closest vector problem ) is defined as follows : the input is a basis @xmath13 for a lattice @xmath35 and a target vector @xmath104 .",
    "the goal is to output a vector @xmath156 with @xmath213 .    for @xmath214 ( the approximation factor ) ,",
    "the search problem @xmath215 ( bounded distance decoding ) is defined as follows : the input is a basis @xmath13 for a lattice @xmath35 and a target vector @xmath104 with @xmath216 .",
    "the goal is to output a closest lattice vector to @xmath109 .",
    "note that , while our other problems become more difficult as the approximation factor @xmath113 becomes smaller , @xmath215 becomes more difficult as @xmath217 gets larger . for convenience ,",
    "when we discuss the running time of algorithms solving the above problems , we ignore polynomial factors in the bit - length of the individual input basis vectors .",
    "( i.e. , we consider only the dependence on the ambient dimension @xmath1 . )",
    "the following theorem was proven by ajtai , kumar , and sivakumar  @xcite , building on work of schnorr  @xcite .",
    "( while we use the aks algorithm repeatedly in the sequel for convenience , we note that we could instead use the conceptually simpler algorithm from  @xcite to obtain asymptotically identical results . )",
    "[ thm : bkz ] there is an algorithm that takes as input a lattice @xmath35 and @xmath218 and outputs an @xmath19-reduced basis of @xmath9 in time @xmath219 , where we say that a basis @xmath188 of a lattice @xmath9 is @xmath113-reduced for some @xmath220 if    1 .",
    "@xmath221 ; and 2 .",
    "@xmath222 is a @xmath113-reduced basis of @xmath223 .    in order to initialize our algorithm",
    ", we will need to use a gaussian sampler such as the one given by gentry , peikert , and vaikuntanathan  @xcite . for convenience , we use the following modest strengthening of this result , which provides exact samples and gives slightly better bounds on the parameter @xmath33 .",
    "[ thm : gpv ] there is a probabilistic polynomial - time algorithm that takes as input a basis @xmath13 for a lattice @xmath35 and @xmath224 and outputs a vector that is distributed exactly as @xmath43 , where @xmath225 .",
    "ideally , we would like to use theorem  [ thm : bkz ] and theorem  [ thm : gpv ] to solve @xmath226 for @xmath227 in time @xmath228 .",
    "unfortunately , this does not work .",
    "the problem is that theorem  [ thm : gpv ] only allows us to sample from @xmath43 if _ all _ of the gram - schmidt vectors are smaller than @xmath229 .",
    "we can not hope to achieve this even for @xmath230 .",
    "indeed , there may not even be such a basis !",
    "instead , we show that we can sample from a sublattice for which we can find such a basis , and we show that this sublattice contains all of the `` short '' lattice points .",
    "[ prop : startgauss ] there is an algorithm that takes as input a lattice @xmath35 with @xmath231 , @xmath232 , @xmath233 ( the desired number of output vectors ) , and @xmath163 and outputs a sublattice @xmath234 and @xmath235 independent samples from @xmath236 in time @xmath237 .",
    "the sublattice @xmath234 contains all vectors in @xmath9 of length at most @xmath238 .",
    "furthermore , if @xmath239 then @xmath240 .    on input",
    "@xmath9 the algorithm first runs the procedure from theorem  [ thm : bkz ] on @xmath9 with parameter @xmath241 , receiving output @xmath188 .",
    "let @xmath242 be the corresponding gram - schmidt vectors , and let @xmath46 be maximal such that @xmath243 for all @xmath244 .",
    "the algorithm then runs the procedure from theorem  [ thm : gpv ] @xmath235 times on input @xmath245 and @xmath39 and outputs the result together with @xmath246 .",
    "the running time is clear .",
    "it follows immediately from theorem  [ thm : gpv ] that the output has the correct distribution .",
    "if @xmath247 , then we are done .",
    "otherwise , let @xmath248 . by theorem  [ thm : bkz ] , we have that @xmath249 the main result follows by noting that @xmath250 implies that @xmath251 .    finally , we note that @xmath252 for all @xmath77 .",
    "it follows that , if @xmath253 , then @xmath247 , and therefore @xmath240 .",
    "the second statement then follows from lemma  [ lem : etalambdansqrt ] .",
    "the poisson distribution with parameter @xmath254 is the distribution defined by @xmath255 = \\frac{\\lambda^r}{r!}\\cdot e^{-\\lambda}\\ ] ] for all @xmath256 .    intuitively , the poisson distribution is the distribution obtained by , e.g. , counting the number of decay events over some fixed time period in some large , homogenous radioactive source .",
    "the parameter @xmath257 is just the expected count .",
    "[ lem : glynn ] for @xmath254 let @xmath258 be a @xmath259 random variable .",
    "then ,    * for any @xmath260 , @xmath261 * and for any @xmath262 , @xmath263    [ cor : poistail ] for any @xmath264 , there exist @xmath265 such that the following holds for all @xmath266 . if @xmath258 is a @xmath259 random variable for some @xmath267 then @xmath268 and similarly , if @xmath269 then @xmath270    stirling s approximation implies the inequality @xmath271 valid for all @xmath272 , which together with lemma  [ lem : glynn ] implies in both cases the upper bound @xmath273 the function @xmath274 is non - negative and strictly convex on @xmath275 and obtains its minimum of @xmath276 at @xmath277 . as a result",
    ", it is uniformly bounded away from @xmath276 for all @xmath278 satisfying @xmath279 .",
    "we will also need the chernoff - hoeffding bound  @xcite .",
    "[ lem : chernoff ] let @xmath280 be independent and identically distributed random variables with @xmath281 and expectation @xmath282 . then , for any @xmath283 , @xmath284 \\leq \\exp(-c",
    "\\delta^2 n\\mu / a ) \\ ; , \\ ] ] and @xmath285 \\leq \\exp(-c \\delta^2 n\\mu / a ) \\ ; .\\ ] ]    [ lem : independence ] let @xmath254 and @xmath286^n$ ] with @xmath287 . consider the process that first samples @xmath288 and then samples @xmath289 independently with @xmath290 = p_i$ ] . for each @xmath77",
    ", let @xmath291 be the number of occurrences of @xmath77 in the sequence @xmath289 . then , @xmath291 is distributed as @xmath292 independently of the other @xmath293 .",
    "considering the joint distribution , we have @xmath294 & = \\pr[r = \\length{{\\ensuremath{\\mathbf{a}}}}_1 ] \\cdot \\pr[{\\ensuremath{\\mathbf{y } } } = { \\ensuremath{\\mathbf{a } } } | r = \\length{{\\ensuremath{\\mathbf{a}}}}_1]\\\\   & =   \\lambda^{\\length{{\\ensuremath{\\mathbf{a}}}}_1 } e^{-\\lambda } \\prod_i \\frac{p_i^{a_i}}{a_i ! } \\\\ & = \\prod_i \\left(\\frac{(\\lambda p_i)^{a_i}}{a_i ! } \\cdot e^{- \\lambda p_i } \\right ) \\ ; , \\end{aligned}\\ ] ] as needed .",
    "[ clm : poissontobinom ] for @xmath295 and @xmath296 , consider the procedure obtained by sampling @xmath142 from @xmath259 and then outputting @xmath297 with probability @xmath298 and @xmath276 otherwise .",
    "the output of this procedure is within statistical distance @xmath299 of the bernoulli distribution @xmath300 .",
    "if @xmath258 is distributed like @xmath259 , the statistical distance is given by @xmath301 & =   \\operatorname*{\\mathbb{e}}[\\max\\{0,\\ x/\\kappa - 1 \\ } ] \\\\",
    "& \\le   \\kappa^{-1 } \\operatorname*{\\mathbb{e}}[1_{x > \\kappa } \\cdot x ] \\\\ & = \\kappa^{-1 } \\sum_{r = \\floor{\\kappa}+1}^\\infty r \\lambda^r \\exp(-\\lambda ) / r !   \\\\",
    "& = \\kappa^{-1 } \\lambda \\sum_{r = \\floor{\\kappa}}^\\infty \\lambda^r \\exp(-\\lambda ) / r ! \\ ; , \\end{aligned}\\ ] ] which is at most @xmath299 by lemma  [ lem : glynn ] and our choice of parameters .",
    "recall that a naive bucketing procedure does not weight cosets in the way that we would like .",
    "in particular , the resulting number of vectors in the cosets is distributed with probabilities proportional to @xmath70 , while we would like the probabilities to be proportional to @xmath302 .",
    "theorem  [ thm : squaresampler ] shows how to use samples from any multinomial distribution to sample from the `` squared distribution '' ( with small error ) .",
    "the `` square sampler '' that we present in theorem  [ thm : squaresampler ] needs to compute an estimate of the maximal probability @xmath303 , given samples from some probability distribution with respective probabilities @xmath71 .",
    "the following proposition shows that there is a relatively efficient way of estimating @xmath303 .",
    "the proposition is included here for completeness . in our application , we will know which of the elements @xmath304 has maximal probability , so we could instead simply estimate @xmath303 directly .    [ prop : estimatinglinfty ] there is an algorithm that takes as input @xmath305 ( the confidence parameter ) and a sequence of @xmath235 elements from @xmath306 and outputs a value @xmath307 such that , if the input consists of @xmath308 independent samples from the distribution that assigns probability @xmath75 to element @xmath77 , then @xmath309 except with probability at most @xmath310 , where @xmath311 .",
    "the algorithm runs in time @xmath312 .",
    "the algorithm is the following .",
    "initialize @xmath313 .",
    "sample @xmath142 from @xmath314 and read the next @xmath142 elements in the input sequence ( or fail if there are not enough elements remaining ) .",
    "count how many times each @xmath315 appears in this subsequence . if there exists an @xmath77 appearing at least @xmath316 times , output @xmath101 and stop .",
    "otherwise , divide @xmath101 by @xmath42 and repeat .",
    "the running time is clear . by lemma  [ lem : independence ] , at each iteration the number of times @xmath77 appears is distributed like @xmath317 independently of everything else .",
    "consider now the iterations with @xmath318 . since @xmath319 , there are at most @xmath320 such iterations , and in each there are @xmath321 possible values of @xmath77 .",
    "therefore , by corollary  [ cor : poistail ] and a union bound , the probability that there exists an iteration with @xmath318 and an @xmath77 that appears there at least @xmath316 times is at most @xmath322 . finally , consider the iteration in which @xmath323 , and let @xmath77 be the index achieving @xmath324 .",
    "then by corollary  [ cor : poistail ] again , with all but probability @xmath325 , the item @xmath77 appears at least @xmath316 times . to summarize , assuming none of the bad events happens , the output satisfies @xmath326 as desired .    for a vector @xmath286^n$ ] with @xmath287 , let @xmath327 where @xmath328 .",
    "[ thm : squaresampler ] there is an algorithm that takes as input @xmath329 ( the confidence parameter ) and @xmath235 elements from @xmath306 and outputs a sequence of elements from the same set such that    1 .",
    "[ item : squareruntime ] the running time is @xmath312 ; 2 .   [",
    "item : squareinputoutput ] each @xmath330 appears at least twice as often in the input as in the output ; and 3 .",
    "[ item : squaredistribution ] if the input consists of @xmath331 independent samples from the distribution that assigns probability @xmath75 to element @xmath77 , then the output is within statistical distance @xmath332 of @xmath200 independent samples with respective probabilities @xmath333 where @xmath334 is a random variable .",
    "the algorithm first runs the procedure from proposition  [ prop : estimatinglinfty ] on the first @xmath335 elements from its input sequence , receiving as output @xmath307 .",
    "the algorithm then reads the remaining elements in sequence .",
    "if it ever reads the last element of the input , it fails . for @xmath336 , the algorithm samples @xmath142 according to @xmath337 and takes the next @xmath142 unused elements in the input . for @xmath338 ,",
    "let @xmath339 be the number of times element @xmath77 appears in the @xmath76th such subsequence .",
    "for each @xmath340 let @xmath73 be @xmath297 with probability @xmath341 and @xmath276 otherwise .",
    "( to achieve the correct running time , we do not actually explicitly store these values when @xmath342 . )    finally , the algorithm looks through the next @xmath343 elements , one element at a time ( or it fails if there are not @xmath343 elements remaining ) .",
    "when it sees element @xmath77 , it adds it to its output if @xmath74 where @xmath344 is the smallest index such that @xmath73 is unused ( or it fails if there is no unused @xmath73 ) .",
    "the running time of the algorithm is clear .",
    "we first prove that the output elements have the correct distribution , ignoring failure . by proposition  [ prop : estimatinglinfty ]",
    ", we can assume that @xmath345 , introducing statistical distance at most @xmath310 .",
    "then , by lemma  [ lem : independence ] , the @xmath339 are distributed independently as @xmath346 . by claim  [ clm : poissontobinom ] , each @xmath73 is within statistical distance @xmath347 of @xmath348 .",
    "so , we assume that the @xmath73 are independently distributed exactly as @xmath348 , introducing statistical distance that is at most @xmath349 .",
    "then , in the final stage of the algorithm , the probability of outputting @xmath77 at each step is @xmath350 .",
    "hence , the individual output samples have the correct distribution . by the chernoff - hoeffding bound ( lemma  [ lem : chernoff ] )",
    ", the size of the output will be at least @xmath351 except with probability at most @xmath352 .",
    "we now prove that the algorithm rarely fails .",
    "the number of inputs used in the first stage is distributed as @xmath353 , which by corollary  [ cor : poistail ] is at most @xmath335 except with probability at most @xmath354 . applying the chernoff - hoeffding bound again",
    ", we have that the number of coins @xmath73 used for a fixed @xmath77 is at most @xmath355 except with probability at most @xmath356 .",
    "so , the algorithm fails with probability at most @xmath357 .",
    "finally , we note that each @xmath77 appearing in the output corresponds to two copies of @xmath77 appearing in the input : one corresponding to some value @xmath358 and another sampled in the final stage .",
    "ideally , we would like the average of two vectors sampled from @xmath32 to be distributed as @xmath359 for some @xmath360 . unfortunately , this is false for the simple reason that the average of two lattice vectors may not be in the lattice !",
    "the following lemma shows that we do obtain the desired distribution if we condition on the result being in the lattice . the number of vectors that we output will depend on the expression @xmath361 where @xmath69 ranges over all cosets of @xmath44 over @xmath9 , so we analyze this expression as well .",
    "( note that , for two lattice vectors @xmath362 and @xmath363 , we have @xmath364 if and only if @xmath362 and @xmath363 are in the same coset over @xmath44 .",
    "so , the cosets of @xmath44 arise naturally in this context . )",
    "[ lem : sumofgaussians ] let @xmath35 and @xmath163 .",
    "then for all @xmath156 , @xmath365   = \\pr_{{\\ensuremath{\\mathbf{x } } } \\sim d_{{\\mathcal{l } } , s/\\sqrt{2}}}[{\\ensuremath{\\mathbf{x } } } = { \\ensuremath{\\mathbf{y } } } ] \\ ; .\\end{aligned}\\ ] ] furthermore , @xmath366    multiplying the left - hand side of   by @xmath367   = \\rho_s({\\mathcal{l}})^{-2 } \\sum_{{\\ensuremath{{\\ensuremath{\\mathbf{c}}}}}\\in { \\mathcal{l}}/(2{\\mathcal{l}})}\\rho_s({\\ensuremath{{\\ensuremath{\\mathbf{c}}}}})^2 $ ] we get for any @xmath156 , @xmath368    & = \\frac{1}{\\rho_s({\\mathcal{l}})^2}\\cdot\\sum_{{\\ensuremath{\\mathbf{x } } } \\in { \\mathcal{l } } } \\rho_s({\\ensuremath{\\mathbf{x } } } ) \\rho_s(2{\\ensuremath{\\mathbf{y } } } - { \\ensuremath{\\mathbf{x}}})\\\\ & = \\frac{\\rho_{s/\\sqrt{2}}({\\ensuremath{\\mathbf{y}}})}{\\rho_s({\\mathcal{l}})^2}\\cdot\\sum_{{\\ensuremath{\\mathbf{x } } } \\in { \\mathcal{l } } } \\rho_{s/\\sqrt{2}}({\\ensuremath{\\mathbf{x } } } - { \\ensuremath{\\mathbf{y}}})\\\\ & = \\frac{\\rho_{s/\\sqrt{2}}({\\ensuremath{\\mathbf{y}}})}{\\rho_s({\\mathcal{l}})^2}\\cdot \\rho_{s/\\sqrt{2}}({\\mathcal{l } } ) \\\\ & = \\rho_s({\\mathcal{l}})^{-2}\\cdot \\rho_{s/\\sqrt{2}}({\\mathcal{l}})^2 \\pr_{{\\ensuremath{\\mathbf{x } } } \\sim d_{{\\mathcal{l } } , s/\\sqrt{2}}}[{\\ensuremath{\\mathbf{x } } } = { \\ensuremath{\\mathbf{y } } } ] \\ ; .\\end{aligned}\\ ] ] hence both sides of   are proportional to each other . since they are probabilities , they are actually equal .",
    "in particular , the ratio between them , @xmath369 , is one .",
    "[ prop : combiner ] there is an algorithm that takes as input a lattice @xmath35 , @xmath329 ( the confidence parameter ) , and a sequence of vectors from @xmath9 , and outputs a sequence of vectors from @xmath9 such that , if the input consists of @xmath370 independent samples from @xmath43 for some @xmath163 , then the output is within statistical distance @xmath371 of @xmath200 independent samples from @xmath49 where @xmath200 is a random variable with @xmath372 the running time of the algorithm is at most @xmath373 .",
    "let @xmath374 be the input vectors .",
    "for each @xmath77 , let @xmath375 be the coset of @xmath376 .",
    "the combiner runs the algorithm from theorem  [ thm : squaresampler ] with input @xmath377 and @xmath378 , receiving output @xmath379 .",
    "( formally , we must encode the cosets as integers in @xmath380 . ) finally , for each @xmath381 , it chooses a pair of unpaired vectors @xmath382 with @xmath383 and outputs @xmath384 .",
    "the running time of the algorithm follows from item  [ item : squareruntime ] of theorem  [ thm : squaresampler ] .",
    "furthermore , we note that by item  [ item : squareinputoutput ] of the same theorem , there will always be a pair of indices @xmath385 for each @xmath77 as above .",
    "to prove correctness , we observe that for @xmath386 and @xmath387 , @xmath388 = \\frac{\\rho_s({\\ensuremath{{\\ensuremath{\\mathbf{c}}}}})}{\\rho_s({\\mathcal{l } } ) } \\cdot \\pr_{{\\ensuremath{\\mathbf{x } } } \\sim d_{{\\ensuremath{{\\ensuremath{\\mathbf{c}}}}},s}}[{\\ensuremath{\\mathbf{x } } } = { \\ensuremath{\\mathbf{y } } } ] \\;.\\ ] ] in particular , we have that @xmath389 = \\rho_s({\\ensuremath{{\\ensuremath{\\mathbf{c}}}}})/\\rho_s({\\mathcal{l}})$ ] , and @xmath44 is the coset with the highest probability . then , the cosets @xmath378 satisfy the conditions necessary for item  [ item : squaredistribution ] of theorem  [ thm : squaresampler ] with @xmath390 .    applying the theorem , up to statistical distance @xmath391",
    ", we have that the output vectors are independent , and @xmath392 where the equality follows from lemma  [ lem : sumofgaussians ] . furthermore , we have @xmath393 = \\rho_s({\\ensuremath{{\\ensuremath{\\mathbf{c}}}}})^2/\\sum_{{\\ensuremath{{\\ensuremath{\\mathbf{c } } } } } ' } \\rho_s({\\ensuremath{{\\ensuremath{\\mathbf{c}}}}}')^2 $ ] for any coset @xmath386 .",
    "therefore , for any @xmath156 , @xmath394 & = \\frac{1}{\\sum \\rho_s({\\ensuremath{{\\ensuremath{\\mathbf{c}}}}})^2 } \\cdot \\sum_{{\\ensuremath{{\\ensuremath{\\mathbf{c}}}}}\\in { \\mathcal{l}}/(2{\\mathcal{l } } ) } \\rho_s({\\ensuremath{{\\ensuremath{\\mathbf{c}}}}})^2 \\cdot \\pr_{({\\ensuremath{\\mathbf{x}}}_j , { \\ensuremath{\\mathbf{x}}}_k ) \\sim d_{{\\ensuremath{{\\ensuremath{\\mathbf{c } } } } } , s}^2}[({\\ensuremath{\\mathbf{x}}}_j + { \\ensuremath{\\mathbf{x}}}_k)/2   = { \\ensuremath{\\mathbf{y}}}]\\\\ & = \\pr_{({\\ensuremath{\\mathbf{x}}}_1 , { \\ensuremath{\\mathbf{x}}}_2 ) \\sim d_{{\\mathcal{l } } , s}^2}[({\\ensuremath{\\mathbf{x}}}_1 + { \\ensuremath{\\mathbf{x}}}_2)/2 = { \\ensuremath{\\mathbf{y } } } ~|~ { \\ensuremath{\\mathbf{x}}}_1 + { \\ensuremath{\\mathbf{x}}}_2 \\in 2{\\mathcal{l } } ]   \\ ; .\\end{aligned}\\ ] ] the result then follows from lemma  [ lem : sumofgaussians ] .    by calling the algorithm from proposition  [ prop : combiner ] repeatedly , we obtain a general discrete gaussian combiner .",
    "[ cor : pipeline ] there is an algorithm that takes as input a lattice @xmath35 , @xmath395 ( the step parameter ) , @xmath329 ( the confidence parameter ) , and @xmath396 vectors in @xmath9 such that , if the input vectors are distributed as @xmath43 for some @xmath163 , then the output is a sequence of @xmath4 vectors whose distribution is within statistical distance @xmath397 of independent samples from @xmath398 .",
    "the algorithm runs in time @xmath399 .",
    "let @xmath400 be the sequence of input vectors . for @xmath401 ,",
    "the algorithm calls the procedure from proposition  [ prop : combiner ] with input @xmath9 , @xmath377 , and @xmath402 , receiving an output sequence @xmath403 of some length @xmath404 .",
    "finally , the algorithm outputs the first @xmath4 vectors of @xmath405 ( or fails if there are not enough vectors ) .",
    "the running time is clear .",
    "fix @xmath9 , @xmath33 , and @xmath406 . for convenience ,",
    "let @xmath407 .",
    "note that by lemma  [ lem : banaszczyk ] we have that @xmath408 for all @xmath77 , a fact that we use repeatedly below .",
    "we wish to prove by induction that @xmath402 is within statistical distance @xmath409 of @xmath410 with @xmath411 for all @xmath77 .",
    "since @xmath412 and @xmath413 , it follows that   holds when @xmath414 .",
    "suppose that @xmath402 has the correct distribution and   holds for some @xmath77 with @xmath415 .",
    "notice that the right - hand side of   is at least @xmath416 and that the latter is precisely the lower bound on @xmath417 appearing in proposition  [ prop : combiner ] .",
    "we can therefore apply the proposition and the induction hypothesis , and obtain that ( up to statistical distance at most @xmath418 ) , @xmath403 has the correct distribution with @xmath419 as needed .",
    "the result follows by noting that @xmath420 .",
    "[ thm : dgs ] there is an algorithm that solves @xmath421 in time @xmath422 for any @xmath423 .    on input @xmath35 a lattice , @xmath163 , and @xmath423",
    ", the algorithm behaves as follows .",
    "first , it runs the sampler from proposition  [ prop : startgauss ] on @xmath9 with parameters @xmath142 , @xmath424 , and @xmath425 , with @xmath142 and @xmath406 to be set in the analysis .",
    "it receives as output a sublattice @xmath426 and vectors @xmath427 .",
    "it then runs the combiner from corollary  [ cor : pipeline ] with input @xmath234 , @xmath428 , @xmath377 , and @xmath429 and outputs the result .",
    "the running time of the first stage of the algorithm is @xmath237 by proposition  [ prop : startgauss ] , and by corollary  [ cor : pipeline ] , the running time of the second stage is @xmath430 . setting @xmath431 and @xmath432 , it follows that the running time is as claimed . applying the proposition and corollary again",
    ", we have that the output is within statistical distance @xmath433 of @xmath434 .",
    "furthermore , we have that @xmath234 contains all vectors of length at most @xmath435 .",
    "it remains to prove that @xmath236 is within statistical distance @xmath433 of @xmath43 .",
    "notice that @xmath436 is a restriction of the distribution @xmath43 to @xmath88 and hence the statistical distance between these two distributions is @xmath437",
    "< \\pr_{{\\ensuremath{\\mathbf{x } } } \\sim d_{{\\mathcal{l } } , s}}[\\length{{\\ensuremath{\\mathbf{x } } } } > \\sqrt{\\kappa } s ] <",
    "\\exp(-\\omega(\\kappa ) ) \\ ; , \\ ] ] as needed , where we used lemma  [ lem : banaszczyktail ] .",
    "in this section , we prove a bound on the gaussian mass of a lattice that follows from an upper bound on the kissing number due to kabatjanski and leventen  @xcite .",
    "in particular , we use the following lemma from  @xcite based on  @xcite . for convenience , we define @xmath438 , and we use this notation throughout this section .    [",
    "lem : kl ] let @xmath439 be a lattice with @xmath440 . then for any @xmath441",
    ", the number of lattice vectors of length at most @xmath142 is at most @xmath442 .",
    "we now use lemma  [ lem : kl ] to bound @xmath443 .",
    "[ lem : gaussian - sum - bound ] let @xmath35 be a lattice of rank at least one . then for any @xmath444 , @xmath445 and for @xmath446 , we have @xmath447    we note that an easy calculation shows that the right - hand side of eq .",
    "is never smaller than the right - hand side of eq .  .",
    "in particular , this means that eq",
    ".   actually applies for all @xmath33 .",
    "we assume without loss of generality that @xmath9 is normalized so that @xmath440 .",
    "let @xmath448 .",
    "for @xmath449 , define @xmath450 . by lemma  [ lem : kl ] , @xmath451 , and , for any vector @xmath452 , @xmath453 .",
    "therefore , @xmath454 so , we have @xmath455 where we have used the fact that @xmath456 decays geometrically when @xmath77 is at least , say , @xmath457 , and so the sum up to that point is the same as the infinite sum up to a constant factor .",
    "note that for any @xmath458 , the maximum of @xmath459 over the interval @xmath449 is obtained at @xmath460 if this value is at least @xmath297 or at @xmath461 otherwise .",
    "the result follows .",
    "[ prop : boundonmass - kl ] let @xmath35 be a lattice of rank at least one .",
    "let @xmath462 then , @xmath463 \\geq e^{-\\beta^2 n/(2e ) - o(n ) } \\approx 1.38^{-n - o(n ) } \\ ; .\\ ] ]    by lemma  [ lem : gaussian - sum - bound ] , we have that @xmath464 .",
    "therefore , @xmath463",
    "\\geq e^{-\\pi / s^2}/\\rho_s({\\mathcal{l } } ) \\geq   e^{-\\pi / s^2 - o(n ) } = e^{-\\beta^2 n/(2e ) - o(n ) }   \\ ; , \\ ] ] as needed .",
    "an easy calculation shows that the probability in proposition  [ prop : boundonmass - kl ] is maximized to within a factor of two when @xmath465 .",
    "i.e. , for any shortest non - zero vector @xmath156 , @xmath466 \\leq 2 \\pr _ { { \\ensuremath{\\mathbf{x } } } \\sim d_{{\\mathcal{l } } , 1/\\eta_1({\\mathcal{l}}^*)}}[{\\ensuremath{\\mathbf{x } } } = { \\ensuremath{\\mathbf{y } } } ] =    \\exp(-\\pi \\eta_1({\\mathcal{l}}^*)^2 \\cdot \\lambda_1({\\mathcal{l}})^2 ) \\ ; .\\ ] ]      [ thm : svptodgs ] there is a reduction from @xmath14 to @xmath467 .",
    "the reduction makes @xmath468 calls to the @xmath51 oracle , preserves the dimension of the lattice , and runs in time @xmath469 .",
    "let @xmath470 be an oracle solving @xmath467 .",
    "we construct an algorithm solving @xmath14 as follows .",
    "it first runs the procedure from theorem  [ thm : bkz ] on @xmath9 with @xmath471 .",
    "let @xmath126 be the length of the first basis vector in the output . for @xmath472 ,",
    "the algorithm calls @xmath470 on @xmath9 with parameter @xmath473 .",
    "let @xmath474 be a shortest non - zero vector in the output .",
    "finally , the algorithm outputs a shortest vector among the @xmath474 .",
    "the running time of the algorithm is clear . by theorem  [ thm : bkz ]",
    ", we have @xmath475 .",
    "it follows that there exists some @xmath77 such that @xmath476 where @xmath477 ( i.e. , @xmath478 is the parameter from proposition  [ prop : boundonmass - kl ] ) .",
    "we assume that the output of @xmath470 is exactly @xmath479 when called on @xmath480 , incurring statistical distance at most @xmath481 . for such @xmath77 , by lemma  [ lem : banaszczyk ] we have @xmath482   & \\geq 1.01^{-n } \\cdot \\pr_{{\\ensuremath{\\mathbf{x } } } \\sim d_{{\\mathcal{l}},\\hat{s}}}[\\length{{\\ensuremath{\\mathbf{x } } } } = \\lambda_1({\\mathcal{l}})]\\\\ & \\geq 1.4^{-n - o(n ) }   & \\text{(proposition~\\ref{prop : boundonmass - kl } ) } \\ ; .\\end{aligned}\\ ] ] the result follows by noting that @xmath483 , so @xmath4 samples from @xmath484 will contain a shortest vector with probability at least @xmath485 .",
    "[ cor : svp ] there is an algorithm that solves in time @xmath5 .",
    "combine the reduction from theorem  [ thm : svptodgs ] with the algorithm from theorem  [ thm : dgs ] .",
    "in this section we present a @xmath4-time algorithm for @xmath226 with @xmath196 approximately the smoothing parameter . for our applications ( in particular for solving @xmath486 ) we will need a slightly stronger guarantee from the algorithm .",
    "namely , when asked to produce samples with too small a parameter @xmath487 , the output should still consist of discrete gaussian samples of the desired parameter , but potentially less of them ( or even none at all ) .",
    "we make this property formal in the following slight modification of definition  [ def : dgs ] .",
    "[ def : hdgs ] for @xmath488 , @xmath196 a function that maps lattices to non - negative real numbers , and @xmath256 , @xmath489 ( the _ honest _ discrete gaussian sampling problem ) is defined as follows : the input is a basis @xmath13 for a lattice @xmath35 and a parameter @xmath163 .",
    "the goal is for the output distribution to be @xmath107-close to @xmath490 for some independent random variable @xmath491 . if @xmath199 , then @xmath492 must equal @xmath200 .",
    "[ clm : sqrt ] there is an algorithm that , given black - box access to the bernoulli distribution @xmath493 for unknown @xmath494 , outputs @xmath495 such that the distribution of @xmath496 is exactly @xmath497 .",
    "the expected running time of the algorithm is @xmath498 .",
    "furthermore , if the algorithm s input is restricted to @xmath499 independent samples from the bernoulli distribution @xmath493 for unknown @xmath500 , then the distribution of @xmath496 is within statistical distance @xmath501 of @xmath497 , and the algorithm runs in time @xmath502 .",
    "the algorithm repeatedly samples from @xmath493 until the first time that it sees a @xmath297 .",
    "let @xmath46 be the number of times that zero appears before this first one .",
    "( e.g. , if the input sequence is @xmath503 , @xmath504 . )",
    "then , the algorithm tosses @xmath505 unbiased coins and outputs @xmath297 if exactly half of them are heads and 0 otherwise .",
    "( in the case when the algorithm s input length is restricted , it outputs @xmath276 if there are no ones in its input sequence . )",
    "correctness is immediate from the following identity , which can be obtained by taking the taylor expansion of @xmath506 around @xmath313 and then multiplying by @xmath101 , @xmath507 the expected running time is clearly proportional to @xmath508 .    for a vector @xmath286^n$ ] with @xmath287",
    ", let @xmath509 where @xmath510 .",
    "[ thm : sqrtsampler ] there is an algorithm that takes as input @xmath329 ( the confidence parameter ) , @xmath511 ( an upper bound on the ratio @xmath512 ) , and @xmath235 elements from @xmath306 and outputs a sequence of elements from the same set such that    1 .",
    "[ item : sqrtruntime ] the running time is @xmath513 ; 2 .   [",
    "item : sqrtinputoutput ] each @xmath330 appears at least as often in the input as in the output ; and 3 .",
    "[ item : sqrtdistribution ] if the input consists of @xmath514 independent samples from the distribution that assigns probability @xmath515 to element @xmath77 with @xmath516 , then the output is within statistical distance @xmath517 of @xmath200 independent samples with respective probabilities @xmath518 where @xmath519 .",
    "the algorithm first runs the procedure from proposition  [ prop : estimatinglinfty ] on the first @xmath335 elements from its input sequence , receiving as output @xmath307 .",
    "if @xmath520 , it fails . otherwise , for @xmath521 , the algorithm samples",
    "@xmath142 from @xmath522 and takes the next @xmath142 unused elements in the input ( or fails if there are not @xmath142 unused elements remaining ) . for @xmath338 ,",
    "let @xmath339 be the number of times element @xmath77 appears in the @xmath76th such subsequence .",
    "for each @xmath340 let @xmath73 be @xmath297 with probability @xmath523 and @xmath276 otherwise .",
    "let @xmath524 .",
    "then , for @xmath338 and @xmath525 , let @xmath526 be the output of the procedure from claim  [ clm : sqrt ] on input @xmath527 .",
    "finally , the algorithm repeats the following at most @xmath528 times : it samples @xmath330 uniformly at random , and adds it to its output if @xmath529 where @xmath530 is the smallest index such that @xmath526 is unused ( or it fails if there is no unused @xmath526 ) .",
    "the algorithm stops as soon as its output contains @xmath200 samples .",
    "it fails if the output contains fewer than @xmath200 samples when the loop ends .",
    "note that the number of coins @xmath73 and @xmath526 is less than @xmath531 .",
    "it follows that the running time is as claimed .",
    "note also that each @xmath77 appearing in the output corresponds to some @xmath358 , which in turn corresponds to at least one element @xmath77 in the input .",
    "we first prove that the output elements have the correct distribution , ignoring failure . by proposition  [ prop : estimatinglinfty ]",
    ", we can assume that @xmath345 , introducing statistical distance at most @xmath310 .",
    "by lemma  [ lem : independence ] and claim  [ clm : poissontobinom ] , we can further assume that @xmath73 are independent and distributed exactly as @xmath348 , introducing statistical distance that is at most @xmath532 .",
    "applying claim  [ clm : sqrt ] , we have that @xmath526 is within statistical distance @xmath533 of @xmath534 .",
    "so , the outputs have the correct distribution .",
    "we now prove that the algorithm rarely fails .",
    "the number of inputs used in the second stage is distributed as @xmath535 , which by corollary  [ cor : poistail ] is at most @xmath335 except with probability at most @xmath325 . applying the chernoff - hoeffding bound ( lemma  [ lem : chernoff ] )",
    ", we have that the number of coins @xmath526 used for a fixed @xmath77 is at most @xmath536 except with probability at most @xmath356 ( where we have used the fact that @xmath537 ) .",
    "finally , applying the chernoff - hoeffding bound again , we have that after @xmath538 steps , the size of the output will be at least @xmath539 except with probability at most @xmath356 .",
    "so , the algorithm fails with probability at most @xmath540 .    in order to create an `` honest '' discrete gaussian sampler as in definition  [ def : hdgs ] that uses the square - root sampler",
    ", we will need a way to `` check @xmath541 , ''",
    "so that we only use the square - root sampler when we can be sure that item  [ item : sqrtdistribution ] applies .",
    "we achieve this with the following simple claim .",
    "[ clm : tcheck ] there is an algorithm that takes as input a number @xmath511 and @xmath235 independent samples from the distribution that assigns probability @xmath515 to each element @xmath330 , runs in time @xmath542 , and satisfies    1 .",
    "[ item : smalltcheck ] if @xmath543 , then the algorithm outputs no with probability at least @xmath544 ; and 2 .",
    "[ item : bigtcheck ] if @xmath545 , then the algorithm outputs yes with probability at least @xmath546 .",
    "the algorithm is quite simple . on input @xmath547 ,",
    "let @xmath548 and @xmath549 .",
    "the algorithm outputs no if @xmath550 and yes otherwise .",
    "the running time of the algorithm is clear .",
    "let @xmath311 and @xmath551 .",
    "suppose @xmath552 .",
    "then , by the chernoff - hoeffding bound ( lemma  [ lem : chernoff ] ) , we have @xmath553 except with probability at most @xmath554 .",
    "similarly , we have that @xmath555 except with probability at most @xmath556 .",
    "item  [ item : smalltcheck ] follows .",
    "now , suppose @xmath545 .",
    "then , for any @xmath77 , by the chernoff - hoeffding bound we have @xmath557 except with probability at most @xmath558 . item  [ item : bigtcheck ] then follows by union bound .",
    "the following lemma generalizes the first part of lemma  [ lem : sumofgaussians ] . in particular , we recover lemma  [ lem : sumofgaussians ] when @xmath99 .",
    "( note that , since we require that @xmath559 , we have that the sum of two lattice vectors @xmath560 is in @xmath88 if and only if @xmath362 and @xmath363 are in the same coset of @xmath88 over @xmath9 . )    [ lem : anysublattice ] let @xmath35 be a lattice , and let @xmath153 be a sublattice with @xmath559 . then for any @xmath561 and @xmath163 , we have @xmath562 = \\frac{\\rho_{\\sqrt{2 } s}(2{\\mathcal{l}}+ { \\ensuremath{\\mathbf{y}}})^2}{{{p}_{\\mathsf{col } } } } \\cdot \\pr_{{\\ensuremath{\\mathbf{x } } } \\sim d_{2 { \\mathcal{l}}+ { \\ensuremath{\\mathbf{y } } } , \\sqrt{2 } s}}[{\\ensuremath{\\mathbf{x } } } = { \\ensuremath{\\mathbf{y } } } ] \\ ; , \\ ] ] where @xmath563 .    it suffices to show that the probability on the left - hand side is proportional to @xmath564 .",
    "indeed , @xmath565   & = \\frac{1}{\\rho_s({\\mathcal{l}})^2}\\cdot\\sum_{{\\ensuremath{\\mathbf{x } } } \\in { \\mathcal{l } } } \\rho_s({\\ensuremath{\\mathbf{x } } } ) \\rho_s({\\ensuremath{\\mathbf{y } } } - { \\ensuremath{\\mathbf{x}}})\\\\ & = \\frac{\\rho_{\\sqrt{2 } s}({\\ensuremath{\\mathbf{y}}})}{\\rho_s({\\mathcal{l}})^2}\\cdot\\sum_{{\\ensuremath{\\mathbf{x } } } \\in { \\mathcal{l } } } \\rho_{\\sqrt{2}s}(2{\\ensuremath{\\mathbf{x } } } - { \\ensuremath{\\mathbf{y}}})\\\\ & = \\frac{\\rho_{\\sqrt{2 } s}({\\ensuremath{\\mathbf{y}}})}{\\rho_s({\\mathcal{l}})^2}\\cdot\\rho_{\\sqrt{2}s}(2{\\mathcal{l}}+ { \\ensuremath{\\mathbf{y } } } ) \\ ; .\\end{aligned}\\ ] ]    [ prop : sqrtcombiner ] there is an algorithm that takes as input a lattice @xmath35 , a sublattice @xmath153 of index @xmath566 with @xmath559 , @xmath329 ( the confidence parameter ) , and a sequence of vectors from @xmath9 such that , if the input consists of @xmath567 independent samples from @xmath43 for some @xmath163 , then    1 .",
    "the running time of the algorithm is @xmath568 ; 2 .   [ item : sqrtcombinerdistrib ] the output distribution is @xmath391-close to @xmath200 independent samples from @xmath569 where @xmath570 is an independent random variable ; and 3 .",
    "[ item : goodt ] if @xmath571 and @xmath572 , then @xmath573 where @xmath574 .",
    "let @xmath374 be the input vectors , and for each @xmath77 , let @xmath575 be the coset of @xmath376 .",
    "the algorithm first applies the square sampler in a manner similar to that of the algorithm from proposition  [ prop : combiner ] .",
    "namely , the algorithm runs the procedure from theorem  [ thm : squaresampler ] with input @xmath377 and @xmath378 , receiving output @xmath576 . for each @xmath577",
    ", it chooses a pair of unpaired vectors @xmath382 with @xmath383 and sets @xmath578 .",
    "let @xmath579 .",
    "the algorithm now applies two tests to `` check that the distribution is sufficiently smooth . ''",
    "first , it checks if @xmath580 . if not , it halts and outputs nothing . next , let @xmath581 be the coset of @xmath582 .",
    "the algorithm runs the procedure from claim  [ clm : tcheck ] on the first @xmath583 such cosets with parameter @xmath584 .",
    "it halts and outputs nothing if this procedure outputs no .",
    "the algorithm now applies the square - root sampler to the remaining cosets .",
    "namely , it runs the procedure from theorem  [ thm : sqrtsampler ] with input @xmath377 , @xmath585 , and @xmath586 , receiving output @xmath587 .",
    "if @xmath588 , it halts and outputs nothing . otherwise , for each @xmath589 , it chooses an unused vector @xmath590 with @xmath591 and @xmath592 and adds it to its output .",
    "the running time of the algorithm follows from item  [ item : squareruntime ] of theorem  [ thm : squaresampler ] and the corresponding item  [ item : sqrtruntime ] of theorem  [ thm : sqrtsampler ] .",
    "furthermore , we note that by item  [ item : squareinputoutput ] of theorem  [ thm : squaresampler ] , the first step of the above algorithm will always be able to find unused @xmath385 satisfying @xmath383 , and by item  [ item : sqrtinputoutput ] of theorem  [ thm : sqrtsampler ] , the second step will always be able to find an unused @xmath76 satisfying @xmath592 .",
    "we now prove item  [ item : sqrtcombinerdistrib ] .",
    "note that , since the index of @xmath88 over @xmath9 is @xmath593 , the maximal probability of a coset must be at least @xmath594 .",
    "it follows that @xmath595 satisfy the conditions necessary for item  [ item : squaredistribution ] of theorem  [ thm : squaresampler ] .",
    "applying the theorem , we have that ( up to statistical distance @xmath391 ) the output vectors @xmath596 are independent and @xmath597 furthermore , they assign to each @xmath561 the probability @xmath394 & = \\frac{1}{\\sum_{{\\ensuremath{{\\ensuremath{\\mathbf{c}}}}}\\in { \\mathcal{l}}/{\\mathcal{l } } ' } \\rho_s({\\ensuremath{{\\ensuremath{\\mathbf{c}}}}})^2 } \\cdot \\sum_{{\\ensuremath{{\\ensuremath{\\mathbf{c}}}}}\\in { \\mathcal{l}}/{\\mathcal{l } } ' }   \\rho_s({\\ensuremath{{\\ensuremath{\\mathbf{c}}}}})^2 \\cdot \\pr_{({\\ensuremath{\\mathbf{x}}}_1 , { \\ensuremath{\\mathbf{x}}}_2 ) \\sim d_{{\\ensuremath{{\\ensuremath{\\mathbf{c } } } } } , s}^2}[{\\ensuremath{\\mathbf{x}}}_1 + { \\ensuremath{\\mathbf{x}}}_2 = { \\ensuremath{\\mathbf{y}}}]\\\\ & = \\pr_{({\\ensuremath{\\mathbf{x}}}_1 , { \\ensuremath{\\mathbf{x}}}_2 ) \\sim d_{{\\mathcal{l}},s}^2}[{\\ensuremath{\\mathbf{x}}}_1 + { \\ensuremath{\\mathbf{x}}}_2 = { \\ensuremath{\\mathbf{y}}}\\ |\\ { \\ensuremath{\\mathbf{x}}}_1 + { \\ensuremath{\\mathbf{x}}}_2 \\in { \\mathcal{l } } ' ] \\\\ & = \\frac{\\rho_{\\sqrt{2 } s}(2{\\mathcal{l}}+ { \\ensuremath{\\mathbf{y}}})^2}{\\sum_{{\\ensuremath{\\mathbf{d } } } \\in { \\mathcal{l}}'/(2{\\mathcal{l } } ) } \\rho_{\\sqrt{2 } s}({\\ensuremath{\\mathbf{d}}})^2 } \\cdot \\pr_{{\\ensuremath{\\mathbf{x } } } \\sim d_{2 { \\mathcal{l}}+ { \\ensuremath{\\mathbf{y } } } , \\sqrt{2 } s}}[{\\ensuremath{\\mathbf{x } } } = { \\ensuremath{\\mathbf{y } } } ] \\ ; , \\end{aligned}\\ ] ] where we have used lemma  [ lem : anysublattice ] . in particular , the distribution of each @xmath598 is given by @xmath599 & =   \\frac{\\rho_{\\sqrt{2 } s}({\\ensuremath{\\mathbf{d}}})^2}{\\sum_{{\\ensuremath{\\mathbf{d } } } ' \\in { \\mathcal{l}}'/(2{\\mathcal{l } } ) } \\rho_{\\sqrt{2 } s}({\\ensuremath{\\mathbf{d'}}})^2 }   \\ ; , \\end{aligned}\\ ] ] for @xmath600 .",
    "the highest probability is obtained at @xmath601 , and we denote it by @xmath324 .    since the algorithm outputs nothing otherwise ( in which case item  [ item : sqrtcombinerdistrib ] trivially holds ) , we only need to consider the case when @xmath602 so we assume this below .",
    "in addition , by item  [ item : smalltcheck ] of claim  [ clm : tcheck ] , the algorithm will halt after the second `` smoothness test '' with probability at least @xmath603 unless @xmath604",
    "so , we can also assume that eq .   holds .",
    "using eq .   and",
    "the fact that the index of @xmath88 over @xmath44 is @xmath605 , we see that @xmath606 . combining this with eq .",
    ", we see that the conditions for item  [ item : sqrtdistribution ] of theorem  [ thm : sqrtsampler ] are satisfied .",
    "let @xmath607 be the vectors `` chosen by the square - root sampler . ''",
    "applying the theorem , up to statistical distance @xmath391 , we have that the @xmath608 are independently distributed , and @xmath609 as needed , where we have used eq .  .",
    "furthermore , we have that for any coset @xmath600 , @xmath610 = \\frac{\\rho_{\\sqrt{2 } s}({\\ensuremath{\\mathbf{d}}})}{\\sum_{{\\ensuremath{\\mathbf{d } } } ' \\in { \\mathcal{l}}'/(2{\\mathcal{l } } ) } \\rho_{\\sqrt{2 } s}({\\ensuremath{\\mathbf{d } } } ' ) } = \\frac{\\rho_{\\sqrt{2 } s}({\\ensuremath{\\mathbf{d}}})}{\\rho_{\\sqrt{2 } s}({\\mathcal{l } } ' ) } \\ ; .\\ ] ] therefore , for any @xmath561 , we have @xmath611 = \\frac{\\rho_{\\sqrt{2 } s}(2{\\mathcal{l}}+ { \\ensuremath{\\mathbf{y}}})}{\\rho_{\\sqrt{2 } s}({\\mathcal{l } } ' ) } \\cdot \\pr_{{\\ensuremath{\\mathbf{x } } } \\sim d_{2 { \\mathcal{l}}+ { \\ensuremath{\\mathbf{y } } } , \\sqrt{2 } s}}[{\\ensuremath{\\mathbf{x } } } = { \\ensuremath{\\mathbf{y}}}]\\ = \\pr_{{\\ensuremath{\\mathbf{x } } } \\sim d_{{\\mathcal{l } } ' , \\sqrt{2 } s}}[{\\ensuremath{\\mathbf{x } } } = { \\ensuremath{\\mathbf{y } } } ] \\ ; , \\ ] ] as needed .    finally , we prove item  [ item : goodt ] .",
    "suppose that @xmath33 satisfies @xmath571 and @xmath572 .",
    "note that by claim  [ clm : smooth ] , we have that @xmath612 combining this with eq .   shows that the algorithm will not halt after the first `` smoothness test '' except with probability at most @xmath391 . similarly , since @xmath613 , @xmath614 by applying item  [ item : bigtcheck ] of claim  [ clm : tcheck ] , we see that the algorithm also will not halt after the second `` smoothness test '' except with negligible probability .",
    "therefore , item  [ item : goodt ] holds .    we are going to apply proposition  [ prop : sqrtcombiner ] repeatedly , to a `` tower '' of lattices @xmath93 , as defined next .",
    "[ def : tower ] for an integer @xmath615 satisfying @xmath616 , we say that @xmath93 is a _ tower of lattices in @xmath125 of index @xmath617 _ if for all @xmath77 we have @xmath618 , @xmath619 , and the index of @xmath94 in @xmath95 is @xmath617 .",
    "we next observe that it is easy to construct a tower with any desired final lattice @xmath96 .",
    "in fact , one can even choose @xmath620 , the second - to - last lattice in the tower .",
    "[ clm : buildingtower ] there is a polynomial - time algorithm that given integers @xmath621 and @xmath622 , as well as two lattices @xmath9 and @xmath88 in @xmath125 satisfying @xmath623 with the index of @xmath9 in @xmath88 being @xmath617 , outputs a tower of lattices @xmath624 of index @xmath617 with @xmath625 , @xmath626 , and @xmath627 .",
    "let @xmath628 be a basis of @xmath9 chosen so that @xmath629 is a basis of @xmath88 .",
    "it is not difficult to see that such a basis exists . then define the tower by `` cyclically halving @xmath615 coordinates , '' namely , @xmath630 etc .",
    "it is easy to check that this satisfies all the required properties .",
    "[ cor : sqrtpipeline ] there is an algorithm that takes as input a tower of lattices @xmath631 in @xmath125 of index @xmath566 , @xmath329 ( the confidence parameter ) , and @xmath632 vectors in @xmath97 such that ,    1 .",
    "[ item : towerruntime ] the algorithm runs in time @xmath633 ; 2 .",
    "[ item : anyparameter ] if the input vectors are distributed as @xmath634 for some @xmath33 , then the output is @xmath635-close to @xmath200 independent samples from @xmath636 where @xmath637 is an independent random variable ; and 3 .",
    "[ item : sabovesmooth ] if @xmath638 and @xmath639 , then @xmath640 .",
    "let @xmath400 be the sequence of input vectors . for @xmath401 ,",
    "the algorithm calls the procedure from proposition  [ prop : sqrtcombiner ] with input @xmath641 , @xmath642 , @xmath377 , and @xmath643 , receiving output @xmath403 .",
    "if @xmath403 is empty , it halts and outputs nothing .",
    "finally , the algorithm outputs the first @xmath4 vectors in @xmath405 .",
    "the running time is clear .",
    "define @xmath644 . since @xmath645 for @xmath646",
    ", we have by induction using item  [ item : sqrtcombinerdistrib ] of proposition  [ prop : sqrtcombiner ] that for @xmath647 , up to statistical distance @xmath409 , @xmath402 is distributed like @xmath648 independent random samples from @xmath649 where @xmath650 is an independent random variable . here for convenience , if the algorithm aborts at some stage",
    "@xmath76 , we define @xmath402 for @xmath651 as the empty set . since @xmath652 , item  [ item : anyparameter ] follows .",
    "finally , suppose @xmath653 . since @xmath619",
    ", we have that @xmath654 .",
    "it follows that @xmath655 for all @xmath656 .",
    "item  [ item : sabovesmooth ] then follows immediately from item  [ item : goodt ] of proposition  [ prop : sqrtcombiner ] .",
    "[ thm : smoothdgs ] let @xmath203 be the function that maps a lattice @xmath9 to @xmath657 .",
    "then , there is an algorithm that solves @xmath658 in time @xmath659 for any @xmath423 .",
    "we first present an algorithm that works for @xmath660 and then modify it to achieve the desired @xmath661 . on input @xmath35 a lattice of rank @xmath1 and @xmath163",
    ", the algorithm behaves as follows .",
    "it first applies the algorithm from claim  [ clm : buildingtower ] with parameters @xmath662 and @xmath663 to be set in the analysis , the lattice @xmath9 , and an arbitrary choice of @xmath88 satisfying the properties there .",
    "it obtained a tower of lattices @xmath631 of index @xmath617 such that @xmath625 and @xmath627 .",
    "the algorithm then runs the sampler from proposition  [ prop : startgauss ] on @xmath97 with parameters @xmath142 ( to be set in the analysis ) , @xmath664 , and @xmath665 .",
    "it receives as output a sublattice @xmath666 and vectors @xmath667 .",
    "if @xmath668 , it outputs nothing and halts . otherwise , it runs the procedure from corollary  [ cor : sqrtpipeline ] with input @xmath93 , @xmath377 , and @xmath669 and outputs the result .",
    "let @xmath670 , @xmath671 , and @xmath672 .",
    "applying proposition  [ prop : startgauss ] and item  [ item : anyparameter ] of corollary  [ cor : sqrtpipeline ] , we have that the output will be distributed as @xmath673 for some @xmath637 up to statistical distance @xmath433 , as needed .",
    "we wish to show that , if @xmath674 , then we have @xmath640 .",
    "note that @xmath675 therefore , by proposition  [ prop : startgauss ] , we have that @xmath676 , so that the algorithm will not halt after running the sampler from proposition  [ prop : startgauss ] .",
    "furthermore , since @xmath677 , we have @xmath678 , and since @xmath679 , we obviously also have @xmath680 therefore , by item  [ item : sabovesmooth ] of corollary  [ cor : sqrtpipeline ] , we have that @xmath640 as needed .",
    "now , consider the running time .",
    "the tower of lattices can be built in polynomial time .",
    "the procedure from proposition  [ prop : startgauss ] runs in time @xmath237 , and the procedure from corollary  [ cor : sqrtpipeline ] runs in time @xmath633 .",
    "it follows that the running time is as claimed .",
    "we now show how to modify the above algorithm to work for @xmath681 .",
    "the bottleneck in the above proof is the condition in eq .",
    "needed for item  [ item : sabovesmooth ] of corollary  [ cor : sqrtpipeline ] to apply .",
    "the trouble is that we used the trivial inequality @xmath682 in order to show that this holds , even though @xmath620 is a superlattice of @xmath96 of index greater than @xmath4 , and so one might expect a gap of about @xmath683 between these two quantities . indeed , lemma  [ lem : supersmoothlattice ] below shows how to randomly choose such a superlattice @xmath620 such that @xmath684 holds with constant positive probability .",
    "so we now use the same procedure as above , except we apply the algorithm in claim  [ clm : buildingtower ] with that choice of @xmath620 .",
    "assuming @xmath620 satisfies this constraint , the constraint   holds , whenever @xmath685 and hence the algorithm would be successful .",
    "this almost completes the proof , except for one minor caveat : as described above , our algorithm successfully outputs @xmath4 vectors ( in the `` good '' case of @xmath686 ) only with some constant positive probability , whereas our goal is to be successful with probability @xmath687 .",
    "this can easily be mended by repeating the algorithm @xmath377 times , each time choosing an independent @xmath620 .",
    "[ lem : supersmoothlattice ] there is a probabilistic polynomial - time algorithm that takes as input a lattice @xmath35 of rank @xmath1 and an integer @xmath615 with @xmath688 and returns a superlattice @xmath689 of index @xmath617 with @xmath690 such that for any @xmath172 , we have @xmath691 with probability at least @xmath481 , where @xmath692 .",
    "the algorithm simply selects a superlattice @xmath689 of index @xmath617 with @xmath690 uniformly at random",
    ". it will be convenient to equivalently work in the dual and to instead pick @xmath693 of index @xmath617 with @xmath694 . in more detail ,",
    "let @xmath695 be a basis of the dual lattice @xmath134 .",
    "this defines a group isomorphism @xmath696 given by @xmath697 .",
    "the algorithm picks a random subspace @xmath698 of dimension @xmath699 and sets @xmath700 to be the union of the cosets corresponding to the points in @xmath701 .",
    "( it can do this efficiently by , e.g. , taking a basis @xmath702 of @xmath701 , and taking the lattice generated by @xmath703 where @xmath704 is any coset representative of @xmath705 . )",
    "it then returns the primal lattice @xmath88 .",
    "it is clear that the algorithm runs in polynomial time and that @xmath9 has index @xmath617 over @xmath88 with @xmath706 as needed .",
    "note that all vectors in @xmath707 have equal probability @xmath708 of being in the subspace @xmath701 .",
    "therefore , for any dual coset @xmath709 with @xmath710 , we have @xmath711 = ( 2^{n - a}-1)/(2^n-1)$ ] .",
    "then , assuming without loss of generality that @xmath712 , we have @xmath713 & = \\sum_{{\\ensuremath{{\\ensuremath{\\mathbf{c}}}}}^ * \\in { \\mathcal{l}}^*/(2{\\mathcal{l}}^ * ) } \\pr[{\\ensuremath{{\\ensuremath{\\mathbf{c}}}}}^ * \\in { \\mathcal{l}}^{\\prime * } ] \\rho_{\\sqrt{2}}({\\ensuremath{{\\ensuremath{\\mathbf{c}}}}}^*)\\\\ & = \\rho_{\\sqrt{2}}(2{\\mathcal{l}}^ * ) + \\frac{2^{n - a}-1}{2^n-1 } \\cdot   \\sum_{{\\ensuremath{{\\ensuremath{\\mathbf{c}}}}}^ * \\in { \\mathcal{l}}^*/(2{\\mathcal{l}}^ * ) \\setminus \\ { 2{\\mathcal{l}}^*\\ } } \\rho_{\\sqrt{2}}({\\ensuremath{{\\ensuremath{\\mathbf{c}}}}}^*)\\\\ & < 1 + { \\varepsilon}^2 + 2^{-a}\\rho_{\\sqrt{2}}({\\mathcal{l}}^ * ) & \\text{(lemma~\\ref{lem : doublesmooth})}\\\\ & \\leq 1 + { \\varepsilon}^2 + 2^{n/2-a}(1+{\\varepsilon } ) & \\text{(lemma~\\ref{lem : banaszczyk } ) } \\ ; .\\end{aligned}\\ ] ] by markov s inequality , @xmath714 with probability at least @xmath481 , and the result follows .",
    "here we observe that the sampler described above can actually be used to obtain @xmath4 samples from the _ shifted _ discrete gaussian @xmath40 in @xmath41 time for any parameter @xmath715 with @xmath716 .",
    "( section  [ sec : approx - cvp ] describes essentially the same reduction in a slightly different context . )",
    "we present a brief proof sketch here in case this finds applications in future work . the idea is to call the sampler from theorem  [ thm : smoothdgs ] repeatedly on the lattice @xmath717 , where @xmath718 .",
    "note that the lattice hyperplane @xmath719 is simply a copy of @xmath720 shifted by @xmath721 , so that @xmath722 .",
    "we therefore simply return the first @xmath1 coordinates of the first @xmath4 vectors in @xmath723 that occur in the output .    to prove that this algorithm works , we simply need to show that ( 1 ) @xmath724 , so that the call to the algorithm from theorem  [ thm : smoothdgs ] will be valid as long as @xmath715 ; and ( 2 ) when @xmath33 is above smoothing , a vector sampled from @xmath62 will land in @xmath723 with relatively high probability , so that we will not have to make too many calls to the algorithm from theorem  [ thm : smoothdgs ] in order to find @xmath4 vectors in @xmath723 .",
    "both claims follow from standard calculations .",
    "( as described above , the algorithm achieves @xmath725 and makes a constant number of calls to the centered dgs oracle .",
    "if we instead set @xmath726 for @xmath305 and make @xmath727 oracle calls , we can obtain @xmath728 . )",
    "in this section we present our @xmath729 algorithm . the main idea is to approximate the smoothing parameter of @xmath134 and then use lemma  [ lem : etalambda1 ] to relate it to @xmath131 . to distinguish a parameter above smoothing from a parameter below smoothing , we call the @xmath730 oracle with the given parameter",
    ". it is below smoothing if the oracle does not produce enough samples or if a statistical test on the output ( lemma  [ lem : smoothcovariance ] ) fails .    [",
    "lem : etalambda1 ] for any lattice @xmath35 and @xmath172 , if @xmath731 , we have @xmath732 and if @xmath733 , we have @xmath734 where @xmath438 .    as will be apparent from the proof and the remark after lemma  [ lem : gaussian - sum - bound ] , eq",
    ".   actually holds for all @xmath172 .    throughout the proof",
    "we assume without loss of generality that @xmath440 .",
    "for the lower bound in both cases , let @xmath735 .",
    "then , @xmath736 , as needed .",
    "let @xmath731 , and let @xmath33 be the expression in the right - hand side of  . then , noting that @xmath737 , by eq .",
    "in lemma  [ lem : gaussian - sum - bound ] , we have @xmath738 as needed .",
    "now , let @xmath733 , and let @xmath33 be the expression in the right - hand side of  . then , noting that @xmath739 , by eq .   in lemma",
    "[ lem : gaussian - sum - bound ] , we have @xmath740 as needed .    for a matrix @xmath741 ,",
    "the spectral norm of @xmath235 is defined as @xmath742    for a symmetric matrix @xmath235 ( the only case that interests us ) , @xmath743 is equivalently the largest absolute value of an eigenvalue of @xmath235 .",
    "[ lem : smoothcovariance ] for any lattice @xmath35 with @xmath744 and any @xmath745 , @xmath746\\big\\|    \\leq \\frac{1}{\\pi } \\cdot   \\frac{{\\varepsilon}}{1+{\\varepsilon } } \\cdot \\big(\\log \\frac{2(1+{\\varepsilon})}{{\\varepsilon } } + 1 \\big ) \\ ; , \\ ] ] where @xmath747 is the @xmath748 identity matrix .    for the upper bound , see ( * ? ? ? * lemma 4.4 ) .",
    "for the lower bound , from the same source , we have @xmath749 = \\eta_{\\varepsilon}({\\mathcal{l}})^2 \\cdot \\operatorname*{\\mathbb{e}}_{{\\ensuremath{\\mathbf{y } } } \\sim d_{{\\mathcal{l}}^ * , 1/\\eta_{\\varepsilon}({\\mathcal{l}})}}[{\\ensuremath{\\mathbf{y}}}{\\ensuremath{\\mathbf{y}}}^t ] \\ ; .\\ ] ] note that for any positive semidefinite matrix @xmath750 , we have @xmath751 .",
    "therefore , @xmath752 \\big\\| & \\geq \\frac{\\eta_{\\varepsilon}({\\mathcal{l}})^2}{n } \\cdot { \\mbox{\\rm tr}}\\big(\\operatorname*{\\mathbb{e}}_{{\\ensuremath{\\mathbf{y } } } \\sim d_{{\\mathcal{l}}^ * , 1/\\eta_{\\varepsilon}({\\mathcal{l}})}}[{\\ensuremath{\\mathbf{y}}}{\\ensuremath{\\mathbf{y}}}^t ] \\big ) \\\\ & \\geq \\frac{\\eta_{\\varepsilon}({\\mathcal{l}})^2}{n } \\cdot \\frac{{\\varepsilon}\\lambda_1({\\mathcal{l}}^*)^2}{1+{\\varepsilon } } \\ ; .\\end{aligned}\\ ] ] applying lemma  [ lem : etalambda1 ] gives us the desired lower bound when @xmath753 .",
    "for @xmath754 , we obtain the result whenever @xmath755 , so it suffices to consider the case @xmath756 .",
    "let @xmath757 satisfy @xmath758 .",
    "then , applying lemma  [ lem : subgaussianity ] , we have @xmath759   & = 2\\int_0^\\infty r \\cdot \\pr[\\abs{\\inner{{\\ensuremath{\\mathbf{x } } } , { \\ensuremath{\\mathbf{v } } } } } \\geq r\\lambda_1({\\mathcal{l}}^ * ) ] { \\rm d } r \\\\ & = \\frac{\\pr[\\abs{\\inner{{\\ensuremath{\\mathbf{x } } } , { \\ensuremath{\\mathbf{v } } } } } \\geq 1]}{\\lambda_1({\\mathcal{l}}^*)^2}+2\\int_{1/\\lambda_1({\\mathcal{l}}^*)}^\\infty r \\pr[\\abs{\\inner{{\\ensuremath{\\mathbf{x } } } , { \\ensuremath{\\mathbf{v } } } } } \\geq r\\lambda_1({\\mathcal{l}}^ * ) ] { \\rm d } r \\\\ & \\leq \\frac{2e^{-\\pi/(\\lambda_1({\\mathcal{l}}^*)\\eta_{\\varepsilon}({\\mathcal{l}}))^2}}{\\lambda_1({\\mathcal{l}}^*)^2 } + 4\\int_{1/\\lambda_1({\\mathcal{l}}^*)}^\\infty r e^{-\\pi r^2/ \\eta_{\\varepsilon}({\\mathcal{l}})^2 } { \\rm d } r   \\\\ & = 2e^{-\\pi/(\\lambda_1({\\mathcal{l}}^*)\\eta_{\\varepsilon}({\\mathcal{l}}))^2 } \\cdot \\big ( \\frac{1}{\\lambda_1({\\mathcal{l}}^*)^2 } + \\frac{\\eta_{\\varepsilon}({\\mathcal{l}})^2}{\\pi } \\big)\\\\ & \\leq 2\\frac{\\pi + 1/\\pi}{e^{\\pi^2 } } \\cdot \\eta_{\\varepsilon}({\\mathcal{l}})^2 \\ ; , \\end{aligned}\\ ] ] where we have used the fact that @xmath760 is increasing for @xmath761 .",
    "therefore , @xmath762\\big\\| \\geq \\frac{1}{2\\pi } - 2\\frac{\\pi + 1/\\pi}{e^{\\pi^2 } } > \\frac{1}{\\pi n } \\cdot \\frac{{\\varepsilon}}{1+{\\varepsilon } } \\ ; , \\ ] ] as needed .    we will also need a form of the matrix chernoff bound .",
    "in particular , we use a less general version of  ( * ? ? ? * theorem 5.29 ) .",
    "[ lem : matrixchernoff ] let @xmath763 be independent and identically distributed random symmetric matrices in @xmath764 with @xmath765 and expectation @xmath282 .",
    "then , for any @xmath766 , @xmath767 \\leq 2n \\exp(-cn t^2/a^2 ) \\ ; .\\ ] ]    [ thm : gapsvp ] for any @xmath768 $ ] , there is a reduction from @xmath208 to @xmath769 where @xmath770 and @xmath771 where @xmath438 .",
    "the reduction preserves dimension , makes a single call to the @xmath730 oracle , and runs in time @xmath772 .    on input a lattice @xmath35 and @xmath209 ,",
    "the reduction calls the @xmath730 oracle with input @xmath134 and parameter @xmath118 to be set in the analysis .",
    "if the oracle outputs fewer than @xmath200 vectors , the reduction immediately outputs yes ( i.e. , the reduction guesses that @xmath210 ) .",
    "otherwise , it receives as output @xmath773 .",
    "let @xmath774 be the sample covariance .",
    "if    @xmath775 the reduction outputs no ( i.e. , it guesses that @xmath211 ) .",
    "otherwise , it outputs yes .",
    "the running time is clear .",
    "let @xmath776 suppose @xmath210 .",
    "then , by the lower bound in lemma  [ lem : etalambda1 ] , we have @xmath777 . by the definition of @xmath730",
    ", we have that the output of the oracle is statistically close to @xmath778 for some independent random variable @xmath779 .",
    "so , we assume that the oracle outputs exactly this distribution , introducing statistical distance at most 1/4 .",
    "if @xmath780 , then the reduction correctly outputs yes .",
    "conditioning on @xmath781 and using lemma  [ lem : smoothcovariance ] , we have @xmath782 \\big\\| > \\frac{{\\varepsilon}}{2 \\pi n}\\cdot \\log(1/{\\varepsilon } )   \\ ; , \\ ] ] where we have used the fact that the lower bound in lemma  [ lem : smoothcovariance ] is monotonically increasing .",
    "so , in order to show that the reduction will output yes , it suffices to show that @xmath783 is concentrated around its mean . by lemma  [ lem : banaszczyk ] and",
    "union bound , we can assume that @xmath784 , introducing only negligible statistical distance .",
    "assuming that this is the case , we can apply lemma  [ lem : matrixchernoff ] with @xmath785 and @xmath786 , and we have that @xmath787\\big\\| \\geq t \\big ] \\leq 2n\\exp(-cm t^2/a^2 ) \\leq \\exp(-cn ) \\ ; , \\ ] ] where we have used the fact that @xmath788 .",
    "it follows that the reduction correctly outputs yes with all but negligible probability .",
    "now , suppose @xmath211 .",
    "then , by eq .   of lemma  [ lem : etalambda1 ] with @xmath107",
    "there taken to be @xmath481 , we have @xmath789 . in this regime , by the definition of @xmath730 , we have that the output of the oracle is within statistical distance @xmath790 of @xmath791 .",
    "so , we can assume that the output is exactly @xmath791 , introducing statistical distance at most @xmath790 .",
    "applying lemma  [ lem : etalambda1 ] again , we have @xmath792 where we have used lemma  [ lem : doublesmooth ] , the fact that @xmath793 , and the observation that eq",
    ".   applies for all @xmath172 . applying lemma  [ lem : smoothcovariance ] , we have that @xmath782 \\big\\| < \\frac{{\\varepsilon}}{\\pi n^2 } \\cdot \\big ( \\log \\frac{2(n^2+{\\varepsilon})}{{\\varepsilon } } + 1\\big ) <",
    "\\frac{{\\varepsilon}}{20 n } \\cdot \\log(1/{\\varepsilon } ) \\ ; , \\ ] ] for sufficiently large @xmath1 ( where we have used the fact that the upper bound in lemma  [ lem : smoothcovariance ] is monotonically increasing ) . finally , applying lemma  [ lem : matrixchernoff ] as above shows that the oracle correctly outputs no with all but negligible probability .",
    "[ cor : gapsvp ] there is a randomized algorithm that solves @xmath208 for @xmath794 in time @xmath7 .",
    "combine the algorithm from theorem  [ thm : smoothdgs ] with the reduction from theorem  [ thm : gapsvp ] with @xmath795 .",
    "[ thm : cvptodgs ] for @xmath796 , there is a reduction from @xmath212 to @xmath467 .",
    "the reduction makes @xmath797 calls to the @xmath51 oracle on an @xmath798-dimensional lattice and runs in time @xmath799 .    on input @xmath800 and @xmath104 , the reduction behaves as follows .",
    "it first uses babai s nearest plane algorithm  @xcite to approximate the distance to the lattice @xmath132 , receiving as output @xmath801 .",
    "fix @xmath802 .",
    "then for @xmath803 , let @xmath804 , and let @xmath805 be the @xmath798-dimensional lattice generated by @xmath806 for @xmath807 $ ] , and the additional basis vector @xmath808 .",
    "the reduction calls the @xmath51 oracle on @xmath805 with parameter @xmath809 , and let @xmath810 be the shortest vector among the returned vectors whose last coordinate is @xmath809 .",
    "finally , the reduction outputs the first @xmath1 coordinates of @xmath811 where @xmath76 is such that @xmath810 is shortest .",
    "the running time of the algorithm is clear .",
    "as was shown in  @xcite , we have @xmath812 .",
    "thus , there exists a @xmath76 such that @xmath813 , where @xmath814",
    ". let @xmath815 be the set of vectors from which we choose @xmath810 .",
    "we note that it suffices to show that a sample from @xmath816 will land in @xmath192 and have length at most @xmath817 with probability at least @xmath818 . indeed ,",
    "if this is the case , then the algorithm will find a vector in @xmath192 of length at most @xmath817 with constant probability , and its output will be a @xmath113-approximate closest vector .",
    "we first consider the probability that a vector lands in @xmath192 , @xmath819 .",
    "for the denominator , using the fact that @xmath820 for any @xmath821 , we have @xmath822 turning to the numerator , @xmath823 thus , we have @xmath824   \\geq e^{-\\pi n/\\alpha^2}/100 \\ ; .\\ ] ]    set @xmath825 .",
    "recall from lemma  [ lem : banaszczyktail ] that @xmath826 \\leq \\big ( \\sqrt{2 \\pi e t^2 } \\exp(-\\pi",
    "t^2 ) \\big)^n \\ ; .\\ ] ] then , combining and , plugging in the values for @xmath217 , @xmath541 , and @xmath113 , and assuming @xmath1 is sufficiently large , gives @xmath827 & \\geq e^{-\\pi n/\\alpha^2}/100 - ( 2\\pi e t^2)^{n/2 } \\cdot",
    "e^{-\\pi t^2 n } \\\\ & \\geq 2^{-n/2 - o(1 ) } \\ ; , \\end{aligned}\\ ] ] where we have used the fact that @xmath828 and @xmath829 .",
    "the result follows from the fact that @xmath830 .",
    "we note that the above proof actually yields a more general statement .",
    "in particular , for any @xmath831 , there is a reduction from @xmath212 to @xmath832 where @xmath833 and @xmath834 we recover theorem  [ thm : cvptodgs ] by setting @xmath835 .",
    "lyubashevsky and micciancio show a polynomial - time reduction from @xmath837 to @xmath208  @xcite . by combining this with theorem  [ thm : gapsvp ]",
    ", we immediately get a solution to @xmath215 for @xmath838 .",
    "but , we can improve this to @xmath839 by using the following ( slightly modified ) theorem from @xcite that shows how to solve a variant of directly using discrete gaussian samples .",
    "[ thm : cvpp ] for any @xmath840 , let @xmath841 then , there exists a reduction from @xmath842 to @xmath843 where @xmath844 and @xmath842 is the problem of solving @xmath110 for target vectors that are guaranteed to be within a distance @xmath845 of the lattice .",
    "the reduction preserves the dimension , makes a single call to the @xmath51 oracle , and runs in time @xmath772 .",
    "let @xmath847 , and let @xmath848 as above . by eq .   of lemma  [ lem :",
    "etalambda1 ] , any algorithm that solves @xmath842 is also a solution to @xmath215 with @xmath849 applying theorem  [ thm : cvpp ] gives a reduction from @xmath215 to @xmath850 with @xmath851 that runs in time @xmath772 .",
    "finally , we note that lemma  [ lem : etalambda1 ] implies that , @xmath852 for sufficiently large @xmath1 .",
    "therefore , theorem  [ thm : smoothdgs ] gives a solution to @xmath853 with the desired running time .",
    "mikls ajtai .",
    "generating hard instances of lattice problems . in _ complexity of computations and proofs",
    "_ , volume  13 of _ quad . mat .",
    "_ , pages 132 .",
    ", seconda univ .",
    "napoli , caserta , 2004 .",
    "preliminary version in stoc96 .",
    "daniel dadush , oded regev , and noah stephens - davidowitz . on the closest vector problem with a distance guarantee . in _",
    "ieee 29th conference on computational complexity _ , pages 98109 , 2014 .",
    "full version available at http://arxiv.org/abs/1409.8063 .",
    "guillaume hanrot and damien stehl .",
    "improved analysis of kannan s shortest lattice vector algorithm ( extended abstract ) . in _ advances in cryptology ",
    "crypto 2007 _ , volume 4622 of _ lecture notes in comput .",
    "_ , pages 170186 .",
    "springer , berlin , 2007 .",
    "vadim lyubashevsky and daniele micciancio . on bounded distance decoding , unique shortest vectors , and the minimum distance problem . in _ advances in cryptology - crypto 2009",
    "_ , pages 577594 .",
    "springer , 2009 .",
    "daniele micciancio and chris peikert .",
    "trapdoors for lattices : simpler , tighter , faster , smaller . in _ advances in cryptology   eurocrypt 2012 _ , volume 7237 of _ lecture notes in computer science _ , pages 700718 .",
    "springer , 2012 .",
    "oman vershynin .",
    "ntroduction to the non - asymptotic analysis of random matrices . in y.c .",
    "eldar and g. kutyniok , editors , _ compressed sensing : theory and applications _ , pages 210268 .",
    "cambridge univ press , 2012 .",
    "xiaoyun wang , mingjie liu , chengliang tian , and jingguo bi . improved nguyen - vidick heuristic sieve algorithm for shortest vector problem . in _ proceedings of the 6th acm symposium on information ,",
    "computer and communications security _ , asiaccs 11 , pages 19 , new york , ny , usa , 2011 .",
    "feng zhang , yanbin pan , and gengran hu .",
    "a three - level sieve algorithm for the shortest vector problem . in tanja lange ,",
    "kristin lauter , and petr lisonek , editors , _ selected areas in cryptography ",
    "sac 2013 _ , lecture notes in computer science , pages 2947 .",
    "springer berlin heidelberg , 2014 ."
  ],
  "abstract_text": [
    "<S> we give a randomized @xmath0-time and space algorithm for solving the shortest vector problem ( svp ) on @xmath1-dimensional euclidean lattices . </S>",
    "<S> this improves on the previous fastest algorithm : the deterministic @xmath2-time and @xmath3-space algorithm of micciancio and voulgaris ( stoc 2010 , siam j.  comp .  2013 ) .    </S>",
    "<S> in fact , we give a conceptually simple algorithm that solves the ( in our opinion , even more interesting ) problem of discrete gaussian sampling ( ) . more specifically , we show how to sample @xmath4 vectors from the discrete gaussian distribution at _ any parameter _ in @xmath0 time and space . </S>",
    "<S> ( prior work only solved for very large parameters . ) our result then follows from a natural reduction from to . </S>",
    "<S> we also show that our algorithm implies a @xmath5-time algorithm that approximates the closest vector problem to within a factor of @xmath6 .    </S>",
    "<S> in addition , we give a more refined algorithm for above the so - called _ smoothing parameter _ of the lattice , which can generate @xmath4 discrete gaussian samples in just @xmath7 time and space . among other things , this implies a @xmath7-time and space algorithm for @xmath8-approximate decision .    </S>",
    "<S> * keywords . * </S>",
    "<S> discrete gaussian , shortest vector problem , lattice problems . </S>"
  ]
}