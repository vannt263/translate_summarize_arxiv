{
  "article_text": [
    "let @xmath0 be a second - order zero - mean stationary process and its covariogram be defined @xmath1 assume the spectral density @xmath2 of @xmath3 , with @xmath4 exists and represents a continuous function on @xmath50,\\pi]$ ] .",
    "consequently , the spectral density of @xmath3 should satisfy the asymptotic property , @xmath6 with @xmath7 called the `` memory parameter '' and @xmath8 .",
    "if @xmath9 , the process @xmath3 is a so - called long - memory process , if not @xmath3 is called a short memory process ( see doukhan _",
    "et al . _ , 2003 , for more details ) .",
    "+ this paper deals with two semi - parametric frameworks which are :    * * assumption a1 : * @xmath3 is a zero mean stationary gaussian process with spectral density satisfying @xmath100,\\pi],\\ ] ] with @xmath11 and @xmath12 where @xmath13 , @xmath14 and @xmath15 \\to { \\mathbb{r}}^+~\\mbox{such that}~|g(\\lambda)-g(0)|\\leq c_{d ' } \\cdot |\\lambda|^{d'}~~\\mbox{for all}~\\lambda \\in [ -\\pi,\\pi ] \\big \\}.\\ ] ] * * assumption a1 : * @xmath3 is a zero - mean stationary gaussian process with spectral density satisfying @xmath100,\\pi],\\ ] ] with @xmath11 and @xmath16 where @xmath13 , @xmath17 and @xmath18 \\to { \\mathbb{r}}^+~\\mbox{such that}~g(\\lambda)=g(0)+c_{d ' } \\ , |\\lambda|^{d'}+o\\big (     a great number of earlier works concerning the estimation of the long range parameter in a semi - parametric framework ( see for instance giraitis _",
    "et al . _ , 1997 , 2000 ) are based on assumption a1 or equivalent assumption on @xmath2 .",
    "another expression ( see robinson , 1995 , moulines and soulier , 2003 or moulines _ et al . _ , 2007 ) is @xmath19 with @xmath20 a function such that @xmath21 and @xmath22 ) .",
    "it is obvious that for @xmath23 such an assumption corresponds to assumption a1 with @xmath24 .",
    "moreover , following arguments developed in giraitis _",
    "_ , 1997 , 2000 , if @xmath25 with @xmath26 is such that @xmath20 is @xmath27 times differentiable around @xmath28 with @xmath29 satisfying a lipschitzian condition of degree @xmath30 around @xmath31 , then @xmath32.so for our purpose , @xmath33 is a more pertinent parameter than @xmath34 ( which is often used in no - parametric literature ) . finally , the assumption a1 is a necessary condition to study the following adaptive estimator of @xmath35 .",
    "we have @xmath36 .",
    "fractional gaussian noises ( with @xmath37 ) and farima[p , d , q ] processes ( with also @xmath37 ) represent the first and well known examples of processes satisfying assumption a1 ( and therefore assumption a1 ) .    in andrews and sun ( 2004 ) ,",
    "an adaptive procedure covers a more general class of functions than @xmath38 , _",
    "i.e. _ @xmath39 defined by : @xmath40 \\to { \\mathbb{r}}^+~\\mbox{such that , as $ \\lambda \\to 0$}\\\\ g(\\lambda)=g(0)+\\sum_{i=0}^k c'_i \\lambda^{2i}+ c_{d ' } \\ , |\\lambda|^{d'}+o\\big (    unfortunately , the adaptive wavelet based estimator defined below , as local or global log - periodogram estimators , is unable to be adapted to such a class ( and therefore , when @xmath41 , its convergence rate will be the same than if the spectral density is included in @xmath42 , at the contrary to andrew and sun estimator ) .",
    "this work is to provide a wavelet - based semi - parametric estimation of the parameter @xmath35 .",
    "this method has been introduced by flandrin ( 1989 ) and numerically developed by abry _",
    "( 1998 , 2001 ) and veitch",
    "_ et al . _",
    "asymptotic results are reported in bardet _",
    "et al . _ ( 2000 ) and more recently in moulines _ et al . _",
    "taking into account these papers , two points of our work can be highlighted  : first , a central limit theorem based on conditions which are weaker than those in bardet _ et al . _ ( 2000 ) .",
    "secondly , we define an auto - driven estimator @xmath43 of @xmath35 ( its definition being different than in veitch _",
    "et al . _ , 2003 ) .",
    "this results in a central limit theorem followed by @xmath43 and this estimator is proved rate optimal up to a logarithm factor ( see below ) . below we shall develop this point .",
    "+   + define the usual sobolev space @xmath44 for @xmath45 and @xmath46 , @xmath47)\\ , /\\",
    ", \\sum_{\\ell\\in{\\mathbb{z}}}(1+|\\ell|)^{\\beta }    \\right \\}.\\end{aligned}\\ ] ] let @xmath48 be a `` mother '' wavelet satisfying the following assumption : +   + * assumption @xmath49  :* _ @xmath50 with @xmath51$]-support and such that _    1 .   _",
    "@xmath48 is included in the sobolev class @xmath52 with @xmath46 _ ; 2 .   _",
    "@xmath53 and @xmath54_.     + a consequence of the first point of this assumption is : for all @xmath55 , @xmath56 , where @xmath57 is the fourier transform of @xmath48 .",
    "a useful consequence of the second point is @xmath58 for @xmath59 with @xmath60 a real number not depending on @xmath61 .",
    "+   + the function @xmath48 is a smooth compactly supported function ( the interval @xmath51 $ ] is meant for better readability , but the following results can be extended to another interval ) with its @xmath62 first vanishing moments . if @xmath63 and @xmath64 in assumptions a1 , assumption @xmath49 can be replaced by a weaker assumption : +   + * assumption @xmath65  :* _ @xmath50 with @xmath51$]-support and such that _    1 .   _",
    "@xmath48 is included in the sobolev class @xmath66 with @xmath46 _ ; 2 .   _",
    "@xmath53 and @xmath54_.    the choice of a wavelet satisfying assumption @xmath49 is quite restricted because of the required smoothness of @xmath48 .",
    "for instance , the function @xmath67 and @xmath68 satisfies assumption @xmath49 .",
    "the class of `` wavelet '' checking assumption @xmath65 is larger .",
    "for instance , @xmath48 can be a dilated daubechies `` mother '' wavelet of order @xmath69 with @xmath70 to ensure the smoothness of the function @xmath48.it is also possible to apply the following theory to `` essentially '' compactly supported `` mother '' wavelet like the lemari - meyer wavelet .",
    "note that it is not necessary to choose @xmath48 being a `` mother '' wavelet associated to a multi - resolution analysis of @xmath71 as in the recent paper of moulines _",
    "the whole theory can be developed without this assumption , in which case the choice of @xmath48 is larger .",
    "if @xmath72 is a continuous - time process for @xmath73 , the `` classical '' wavelet coefficient @xmath74 of the process @xmath75 for the scale @xmath76 and the shift @xmath77 is @xmath78 however , this formula ( [ coeff_d ] ) of a wavelet coefficient can not be computed from a time series .",
    "the support of @xmath48 being @xmath51 $ ] , let us take the following approximation of formula ( [ coeff_d ] ) and define the wavelet coefficients of @xmath0 by @xmath79 for @xmath80 .",
    "note that this approximation is the same as the wavelet coefficient computed from mallat algorithm for an orthogonal discrete wavelet basis ( for instance with daubechies mother wavelet ) .    here",
    "a continuous wavelet transform is considered .",
    "the discrete wavelet transform where @xmath81 , in other words numerically very interesting ( using mallat cascade algorithm ) is just a particular case .",
    "the main point in studing a continuous transform is to offer a larger number of `` scales '' for computing the data - driven optimal bandwidth ( see below ) .    under assumption a1 , for all @xmath82 ,",
    "the asymptotic behavior of the variance of @xmath83 is a power law in scale @xmath76 ( when @xmath84 ) . indeed , for all @xmath85",
    ", @xmath86 is a gaussian stationary process and ( see section more details in [ clt ] ) : @xmath87 with a constant @xmath88 such that , @xmath89 is the fourier transform of @xmath48 ( the existence of @xmath90 is established in section [ prvs ] ) .",
    "note that ( [ asymcoeff ] ) is also checked without the gaussian hypothesis in assumption a1 ( the existence of the second moment order of @xmath3 is sufficient ) .",
    "+ the principle of the wavelet - based estimation of @xmath35 is linked to this power law @xmath91 .",
    "indeed , let @xmath92 be a sampled path of @xmath3 and define @xmath93 a sample variance of @xmath94 obtained from an appropriate choice of shifts @xmath77 , _",
    "i.e. _ @xmath95 } \\sum _ { k=1}^{[n / a]}e^2(a , k-1).\\end{aligned}\\ ] ] then , when @xmath96 satisfies @xmath97 , a central limit theorem for @xmath98 can be proved .",
    "more precisely we get @xmath99 with @xmath100 and @xmath101 . as a consequence , using different scales @xmath102 where @xmath103 with @xmath104 a `` large enough '' scale , a linear regression of @xmath105 by @xmath106 provides an estimator @xmath107 which satisfies at the same time a central limit theorem with a convergence rate @xmath108 .   + but the main problem is : how to select a large enough scale @xmath104 considering that the smaller @xmath104 , the faster the convergence rate of @xmath107 .",
    "an optimal solution would be to chose @xmath104 larger but closer to @xmath109 , but the parameter @xmath33 is supposed to be unknown . in veitch _",
    "( 2003 ) , an automatic selection procedure is proposed using a chi - squared goodness of fit statistic .",
    "this procedure is applied successfully on a large number of numerical examples without any theoretical proofs however .",
    "our present method is close to the latter . roughly speaking",
    ", the `` optimal '' choice of scale @xmath110 is based on the `` best '' linear regression among all the possible linear regressions of @xmath111 consecutive points @xmath112 , where @xmath111 is a fixed integer number .",
    "formally speaking , a contrast is minimized and the chosen scale @xmath113 satisfies : @xmath114 thus , the adaptive estimator @xmath115 of @xmath35 for this scale @xmath113 is such that  : @xmath116 with @xmath117 .",
    "consequently , the minimax rate of convergence @xmath118 , up to a logarithm factor , for the estimation of the long memory parameter @xmath35 in this semi - parametric setting ( see giraitis _",
    "et al . _ , 1997 ) is given by @xmath115 .",
    "+   + such a rate of convergence can also be obtained by other adaptive estimators ( for more details see below ) .",
    "however , @xmath115 has several `` theoretic '' advantages : firstly , it can be applied to all @xmath119 and @xmath120 ( which are very general conditions covering long and short memory , in fact larger conditions than those usually required for adaptive log - periodogram or local whittle estimators ) with a nearly optimal convergence rate .",
    "secondly , @xmath121 satisfies a central limit theorem and sharp confidence intervals for @xmath35 can be computed ( in such a case , the asymptotic @xmath122 is replaced by @xmath123 , for more details see below ) .",
    "finally , under additive assumptions on @xmath48 ( @xmath48 is supposed to have its first @xmath62 vanishing moments ) , @xmath115 can also be applied to a process with a polynomial trend of degree @xmath124 .",
    "+   + we then give a several simulations in order to appreciate empirical properties of the adaptive estimator @xmath115 .",
    "first , using a benchmark composed of @xmath125 different `` test '' processes satisfying assumption a1 ( see below ) , the central limit theorem satisfied by @xmath115 is empirically checked .",
    "the empirical choice of the parameter @xmath111 is also studied . moreover ,",
    "the robustness of @xmath115 is successfully tested .",
    "finally , the adaptive wavelet - based estimator is compared with several existing adaptive estimators of the memory parameter from generated paths of the @xmath125 different `` test '' processes ( giraitis - robinson - samarov adaptive local log - periodogram , moulines - soulier adaptive global log - periodogram , robinson local whittle , abry - taqqu - veitch data - driven wavelet based , bhansali - giraitis - kokoszka far estimators ) .",
    "the simulations results of @xmath115 are convincing .",
    "the convergence rate of @xmath115 is often ranges among the best of the @xmath125 test processes ( however the robinson local whittle estimator @xmath126 provides more uniformly accurate estimations of @xmath35 ) .",
    "three other numerical advantages are offered by the adaptive wavelet - based estimator ( and not by @xmath126 ) .",
    "firstly , it is a very low consuming time estimator .",
    "secondly it is a very robust estimator : it is not sensitive to possible polynomial trends and seems to be consistent in non - gaussian cases .",
    "finally , the graph of the log - log regression of sample variance of wavelet coefficients is meaningful and may lead us to model data with more general processes like locally fractional gaussian noise ( see bardet and bertrand , 2007 ) .",
    "+   + the central limit theorem for sample variance of wavelet coefficient is subject of section [ clt].section [ adaptive ] is concerned with the automatic selection of the scale as well as the asymptotic behavior of @xmath115 .",
    "finally simulations are given in section [ simul ] and proofs in section [ prvs ] .",
    "the following asymptotic behavior of the variance of wavelet coefficients is the basis of all further developments . the first point that explains",
    "all that follows is the    [ vard ] under assumption a1 and assumption @xmath49 , for @xmath85 , @xmath86 is a zero mean gaussian stationary process and it exists @xmath127 not depending on @xmath76 such that , for all @xmath128 , @xmath129    please see section [ prvs ] for the proofs .",
    "the paper of moulines _",
    "( 2007 ) gives similar results for multi - resolution wavelet analysis .",
    "the special case of long memory process can also be studied with weaker assumption @xmath65 ,    [ vard ] under assumption @xmath65 and assumption a1 with @xmath64 and @xmath130 , for @xmath131 , @xmath86 is a zero mean gaussian stationary process and ( [ ea1 ] ) holds .",
    "two corollaries can be added to both those properties .",
    "first , under assumption a1 a more precise result can be established .",
    "[ cor1 ] under :    * assumption a1 and assumption @xmath49 ; * * or * assumption a1 with @xmath64 , @xmath132 and assumption @xmath65 ;    then @xmath86 is a zero mean gaussian stationary process and @xmath133    this corollary is key point for the estimation of an appropriated sequence of scale @xmath134 . indeed ,",
    "when @xmath16 , then @xmath135 for all @xmath136 satisfying @xmath137 .",
    "therefore , assumption a1 is required for obtaining the optimal choice of @xmath104 , _",
    "i.e. _ @xmath138 ( see below for more details ) .",
    "the following corollary generalizes the above properties [ vard ] and [ vard ] .",
    "[ cor2 ] properties [ vard ] and [ vard ] are also checked when the gaussian hypothesis of @xmath3 is replaced by @xmath139 for all @xmath140 .    in this paper ,",
    "the gaussian hypothesis has been taken into account merely to insure the convergence of the sample variance ( [ def : tn ] ) of wavelet coefficients following a central limit theorem ( see below ) .",
    "such a convergence can also be obtained for more general processes using a different proof of the central limit theorem , for instance for linear processes ( see a forthcoming work ) .",
    "[ tlclog ] define @xmath151 and @xmath152 .",
    "let @xmath153 be such that @xmath154 and @xmath155 . under assumption a1 and assumption @xmath49 , @xmath156 with @xmath157 the covariance matrix such that @xmath158    the same result under weaker assumptions on @xmath48 can be also established when @xmath3 is a long memory process .",
    "[ tlclog2 ] define @xmath151 and @xmath159 .",
    "let @xmath153 be such that @xmath154 and @xmath155 . under assumption @xmath65 and assumption a1 with @xmath9 and @xmath160 , the clt ( [ cltsn ] ) holds .    these results can be easily generalized for processes with polynomial trends if @xmath48 is considered having its first @xmath62 vanishing moments .",
    "i.e ,    [ cor3 ] given the same hypothesis as in proposition [ tlclog ] or [ tlclog2 ] and if @xmath48 is such that @xmath161 is satisfying , @xmath162 the clt ( [ cltsn ] ) also holds for any process @xmath163 such that for all @xmath164 , @xmath165 with @xmath166 is a polynomial function and @xmath167 are real numbers .",
    "the clt ( [ cltsn ] ) implies the following clt for the vector @xmath168 , @xmath169 and therefore , @xmath170 with @xmath171 , @xmath172 and @xmath173 .",
    "therefore , a log - log regression of @xmath174 on scales @xmath175 provides an estimator @xmath176 of @xmath177 such that @xmath178 which satisfies the following clt ,    [ tlcd ] under the assumptions of the proposition [ tlclog ] , @xmath179 with @xmath180 and @xmath181 given by ( [ cov ] ) .",
    "moreover , under assumption a1 and if @xmath182 , @xmath183 is a semi - parametric estimator of @xmath35 and its asymptotic mean square error can be minimized with an appropriate scales sequence @xmath110 reaching the well - known minimax rate of convergence for memory parameter @xmath35 in this semi - parametric setting ( see for instance giraitis _",
    "et al . _ , 1997 and 2000 ) . indeed ,    [ hatd ] let @xmath3 satisfy assumption a1 with @xmath184 and @xmath48 the assumption @xmath49 .",
    "let @xmath110 be a sequence such that @xmath185 $ ] . then , the estimator @xmath107 is rate optimal in the minimax sense , _",
    "i.e. _ @xmath186<+\\infty.\\ ] ]    as far as we know , there are no theoretic results of optimality in case of @xmath187 , but according to the usual following non - parametric theory , such minimax results can also be obtained . moreover , in case of long - memory processes ( if @xmath9 ) , under assumption a1 for @xmath3 and assumption @xmath65 for @xmath48 , the estimator @xmath107 is also rate optimal in the minimax sense .    in the previous propositions [ tlclog ] and [ tlcd ] , the rate of convergence of scale @xmath104 obeys to the following condition , @xmath188 now , for better readability , take @xmath189 .",
    "then , the above condition goes as follow : @xmath190 thus an optimal choice ( leading to a faster convergence rate of the estimator ) is obtained for @xmath191 with @xmath192 . but @xmath193 depends on @xmath33 which is unknown . to solve this problem , veitch _",
    "( 2003 ) suggest a chi - square - based test ( constructed from a distance between the regression line and the different points @xmath194 ) .",
    "it seems to be an efficient and interesting numerical way to estimate @xmath35 , but without theoretical proofs ( contrary to global or local log - periodogram procedures which are proved to reach the minimax convergence rate , see for instance moulines and soulier , 2003 ) .",
    "+   + we suggest a new procedure for the data - driven selection of optimal scales ,",
    "_ i.e. _ optimal @xmath195 .",
    "let us consider an important parameter , the number of considered scales @xmath196 and set @xmath197 . for @xmath198",
    ", define also    * the vector @xmath199 ; * the matrix @xmath200 ; * the contrast , @xmath201    @xmath202 corresponds to a squared distance between the @xmath111 points @xmath203 and a line .",
    "the point is to minimize this contrast for these three parameters .",
    "it is obvious that for a fixed @xmath204 @xmath205 is minimized from the previous least square regression and therefore , @xmath206 with @xmath207 obtained as in relation ( [ hatd ] ) .",
    "however , since @xmath208 has to be obtained from numerical computations , the interval @xmath209 can be discretized as follows , @xmath210}{\\log n } \\big \\}.\\ ] ] hence , if @xmath211 , it exists @xmath212\\}$ ] such that @xmath213 .",
    "[ defan ] this choice of discretization is implied by the following proof of the consistency of @xmath208 .",
    "if the interval @xmath209 is stepped in @xmath214 points , with @xmath45 , the used proof can not attest this consistency .",
    "finally , it is the same framework as the usual discrete wavelet transform ( see for instance veitch _",
    "et al . _ , 2003 ) but less restricted since @xmath215 may be replaced in the previous expression of @xmath216 by any negligible function of @xmath217 compared to functions @xmath214 with @xmath45 ( for instance , @xmath218 or @xmath219 can be used ) .",
    "consequently , take @xmath220 then , minimize @xmath221 for variables @xmath222 is equivalent to minimize @xmath223 for variable @xmath211 , that is @xmath224 from this central limit theorem derives    [ hatalpha ] let @xmath3 satisfy assumption a1 and @xmath48 assumption @xmath49 ( or assumption @xmath65 if @xmath64 and @xmath130 ) .",
    "then , @xmath225    this proves also the consistency of an estimator @xmath226 of the parameter @xmath33 ,    [ hatd ] taking the hypothesis of proposition [ hatalpha ] , we have @xmath227    the estimator @xmath208 defines the selected scale @xmath228 such that @xmath229 . from a straightforward application of the proof of proposition [ hatalpha ] ( see the details in the proof of theorem [ tilded ] ) , the asymptotic behavior of @xmath228 can be specified , that is , @xmath230 for all positive real numbers @xmath231 and @xmath232 such that @xmath233 and @xmath234 .",
    "consequently , the selected scale is asymptotically equal to @xmath235 up to a logarithm factor .",
    "+   + finally , proposition [ hatalpha ] can be used to define an adaptive estimator of @xmath35 .",
    "first , define the straightforward estimator @xmath236 which should minimize the mean square error using @xmath228 .",
    "however , the estimator @xmath237 does not attest a clt since @xmath238 and therefore it can not be asserted that @xmath239 . to establish a clt satisfied by an adaptive estimator @xmath115 of @xmath35 , an adaptive scale sequence @xmath240 has to be defined to ensure @xmath241 .",
    "the following theorem provides the asymptotic behavior of such an estimator ,    [ tilded ] let @xmath3 satisfy assumption a1 and @xmath48 assumption @xmath49 ( or assumption @xmath65 if @xmath64 and @xmath130 ) .",
    "define , @xmath242 @xmath243 , @xmath244    both the adaptive estimators @xmath237 and @xmath121 converge to @xmath35 with a rate of convergence rate equal to the minimax rate of convergence @xmath245 up to a logarithm factor ( this result being classical within this semi - parametric framework ) .",
    "unfortunately , our method can not prove that the mean square error of both these estimators reaches the optimal rate and therefore to be oracles .    to conclude this theoretic approach , the main properties satisfied by the estimators @xmath237 and @xmath115 can be summarized as follows :",
    "both the adaptive estimators @xmath237 and @xmath121 converge at @xmath35 with a rate of convergence rate equal to the minimax rate of convergence @xmath245 up to a logarithm factor for all @xmath119 and @xmath120 ( this being very general conditions covering long and short memory , even larger than usual conditions required for adaptive log - periodogram or local whittle estimators ) whith @xmath3 considered a gaussian process .",
    "the estimator @xmath115 satisfies the clt ( [ cltd2 ] ) and therefore sharp confidence intervals for @xmath35 can be computed ( in which case , the asymptotic matrix @xmath246 is replaced by @xmath247 ) .",
    "this is not applicable to an adaptive log - periodogram or local whittle estimators .",
    "the main property [ vard ] is also satisfied without the gaussian hypothesis .",
    "therefore , adaptive estimators @xmath248 and @xmath115 can also be interesting estimators of @xmath35 for non - gaussian processes like linear or more general processes ( but a clt similar to theorem [ tlclog ] has to be established ... ) .",
    "4 .   under additive assumptions on @xmath48 ( @xmath48",
    "is supposed to have its first @xmath62 vanishing moments ) , both estimators @xmath237 and @xmath115 can also be used for a process @xmath3 with a polynomial trend of degree @xmath124 , which again can not be yielded with an adaptive log - periodogram or local whittle estimators .",
    "the adaptive wavelet basis estimators @xmath237 and @xmath115 are new estimators of the memory parameter @xmath35 in the semi - parametric frame . different estimators of this kind are also reported in other research works to have proved optimal . in this paper , some theoretic advantages of adaptive wavelet basis estimators have been highlighted .",
    "but what about concrete procedure and results of such estimators applied to an observed sample ? the following simulations will help to answer this question .",
    "+ first , the properties ( consistency , robustness , choice of the parameter @xmath111 and mother wavelet function @xmath48 ) of @xmath249 and @xmath115 are investigated .",
    "secondly , in cases of gaussian long - memory processes ( with @xmath250 and @xmath63 ) , the simulation results of the estimator @xmath237 are compared to those obtained with the best known semi - parametric long - memory estimators .",
    "+   + to begin with , the simulations conditions have to be specified .",
    "the results are obtained from @xmath251 generated independent samples of each process belonging to the following `` benchmark '' .",
    "the concrete procedures of generation of these processes are obtained from the circulant matrix method , as detailed in doukhan _",
    "the simulations are realized for different values of @xmath35 , @xmath217 and processes which satisfy assumption a1 and therefore assumption a1 ( the article of moulines _",
    "et al . _ , 2007 ,",
    "gives a lot of details on this point ) :    1 .",
    "the fractional gaussian noise ( fgn ) of parameter @xmath252 ( for @xmath253 ) and @xmath254 .",
    "the spectral density @xmath255 of a fgn is such that @xmath256 is included in @xmath257 ( thus @xmath37 ) ; 2 .",
    "the farima[p , d , q ] process with parameter @xmath69 such that @xmath258 ( therefore @xmath253 ) , the innovation variance @xmath259 satisfying @xmath254 and @xmath260 .",
    "the spectral density @xmath261 of such a process is such that @xmath262 is included in the set @xmath257 ( thus @xmath37 ) ; 3 .",
    "the gaussian stationary process @xmath263 , such that its spectral density is @xmath264$},\\end{aligned}\\ ] ] with @xmath265 and @xmath266 .",
    "therefore @xmath267 with @xmath266 .    in the long memory frame , a `` benchmark '' of processes",
    "is considered for @xmath268 :    * fgn processes with parameters @xmath252 and @xmath254 ; * farima[0,d,0 ] processes with @xmath269 and standard gaussian innovations ; * farima[1,d,0 ] processes with @xmath269 , standard gaussian innovations and ar coefficient @xmath270 ; * farima[1,d,1 ] processes with @xmath269 , standard gaussian innovations and ar coefficient @xmath271 and ma coefficient @xmath272 ; * @xmath263 gaussian processes with @xmath273 .",
    "below , we give the different properties of the adaptive wavelet based method .",
    "+   + * choice of the mother wavelet @xmath48 : * for short memory processes ( @xmath274 ) , let the wavelet @xmath275 be such that @xmath276 with @xmath68 .",
    "it satisfies assumption @xmath49 .",
    "lemari - meyer wavelets can be also investigated but this will lead to quite different theoretic studies since its support is not bounded ( but `` essentially '' compact ) . + for long memory processes ( @xmath64 ) , let the mother wavelet @xmath277 be such that @xmath278 which satisfies assumption @xmath65 .",
    "note that daubechies mother wavelet or @xmath275 lead to `` similar '' results ( but not as good ) .",
    "+   + * choice of the parameter @xmath111 : * this parameter is very important to estimate the `` beginning '' of the linear part of the graph drawn by points @xmath279 . on the one hand , if @xmath111 is a too small a number ( for instance @xmath280 ) , another small linear part of this graph ( even before the `` true '' beginning @xmath235 ) may be chosen ; consequently , the @xmath281 ( square root of the mean square error ) of @xmath208 and therefore of @xmath282 or @xmath115 will be too large .",
    "on the other hand , if @xmath111 is a too large a number ( for instance @xmath283 for @xmath284 ) , the estimator @xmath208 will certainly satisfy @xmath285 since it will not be possible to consider @xmath111 different scales larger than @xmath235 ( if @xmath273 therefore @xmath286 , then @xmath104 has to satisfy : @xmath287 is a large number and @xmath288 ; this is not really possible ) .",
    "moreover , it is possible that a `` good '' choice of @xmath111 depends on the `` flatness '' of the spectral density @xmath2 , _",
    "i.e. _ on @xmath33 .",
    "we have proceeded to simulations for each different values of @xmath111 ( and @xmath217 and @xmath35 ) .",
    "only @xmath281 of estimators are presented .",
    "the results are specified in table 1 .",
    "+   +    [ table1 ]    @xmath289     ._consistency of estimators @xmath237 , @xmath115 , @xmath208 , @xmath290 following @xmath111 from simulations of the different long - memory processes of the benchmark",
    ". for each value of @xmath217 ( @xmath291 , @xmath292 and @xmath293 ) , of @xmath35 ( @xmath294 , @xmath295 , @xmath296 , @xmath297 and @xmath298 ) and @xmath111 ( @xmath125 , @xmath299 , @xmath300 , @xmath301 , @xmath302 and @xmath303 ) , @xmath251 independent samples of each process are generated .",
    "the @xmath281 of each estimator is obtained from a mean of @xmath281 obtained for the different values of @xmath35 . _ [ cols=\"^,^,^,^,^,^,^,^\",options=\"header \" , ]     these simulations allow to distinguish four `` clusters '' of estimators .",
    "* @xmath304 is obtained from a bic - criterium hierarchical model selection ( from @xmath305 to @xmath306 parameters , corresponding to the length of the approximation of the fourier expansion of the spectral density ) using whittle estimation . for these simulations ,",
    "the bic criterion is generally minimal for @xmath125 to @xmath307 parameters to be estimated .",
    "simulation results are not very satisfactory except for @xmath308 ( close to the short memory ) .",
    "moreover , this procedure is rather time - consuming . * @xmath309 offers good results for fgn and farima@xmath310 .",
    "however , this estimator does not converge fast enough for the other processes .",
    "* estimators @xmath311 and @xmath312 have similar properties .",
    "they ( especially @xmath312 ) are very interesting because they offer the same fairly good rates of convergence for all processes of the benchmark . * being built on similar principles , estimators @xmath313 and @xmath237 have similar behavior as well .",
    "their convergence rates are the fastest for fgn and farima@xmath310 and are almost close to fast ones for the other processes .",
    "their times of computing , especially for @xmath313 for which the computations of wavelet coefficients with that the mallat algorithm , are the shortest .",
    "* conclusion : * which estimator among those studied above has to be chosen in a practical frame , _",
    "i.e. _ an observed time series ? we propose the following procedure for estimating an eventual long memory parameter :    1 .",
    "firstly , since this procedure is very low time consuming and applicable to processes with smooth trends , draw the log - log regression of wavelet coefficients variances onto scales .",
    "if a linear zone appears in this graph , consider the estimator @xmath237 ( or @xmath313 ) of @xmath35 .",
    "2 .   if a linear zone appears in the previous graph and if the observed time series seems to be without a trend , compute @xmath312 .",
    "3 .   compare both the estimated value of @xmath35 from confidence intervals ( available for @xmath237 or @xmath313 and @xmath312 ) .",
    "the arguments of this proof are similar to those of abry _ et al . _",
    "( 1998 ) or moulines _ et al . _",
    "first , for @xmath131 , @xmath314 now , it is well known that if @xmath315 the sobolev space with parameters @xmath316 and @xmath46 , then @xmath317 with @xmath318 only depending on @xmath319 and @xmath320 ( see for instance devore and lorentz , 1993 ) . therefore if @xmath48 satisfies assumption @xmath49 and @xmath3 assumption a1 , for all @xmath316 , since @xmath321 , @xmath322 since @xmath323 for all @xmath324 .",
    "consequently , if @xmath48 satisfies assumption @xmath49 , for all @xmath325 , for all @xmath85 , there exists @xmath326 not depending on @xmath76 such that @xmath327 but from assumption @xmath49 , for all @xmath328 , @xmath329 because assumption @xmath49 implies that @xmath330 when @xmath59 and there exists @xmath331 such that @xmath332 .",
    "moreover , for all @xmath331 , @xmath333 with @xmath8 and @xmath334 not depending on @xmath76 . as a consequence , under assumption a1 , for all @xmath335 , all @xmath336 and all @xmath337 , @xmath338 now , by choosing @xmath339 such that @xmath340 , the inequality ( [ ea1 ] ) is obtained .",
    "@xmath341 + using the proof of previous property [ vard ] , with assumption @xmath65 , @xmath48 is included in a sobolev space @xmath342 , inequality ( [ induction ] ) is checked with @xmath343 and ( [ fourier ] ) is replaced by @xmath344 since @xmath345 .",
    "therefore , inequality ( [ fourier2 ] ) is replaced by @xmath346 the end of the proof is similar to the end of the previous proof , but now @xmath347 exists for @xmath348 and @xmath349 finally , under assumption a1 , for all @xmath85 , since @xmath350 , @xmath351 which achieves the proof .",
    "@xmath341 + both these proofs provide main arguments to establish ( [ equid ] ) . for better readability",
    ", we will consider only assumption a1 and assumption @xmath49 ( the long memory process being similar ) .",
    "the main difference consists in specifying the asymptotic behavior of @xmath352 .",
    "but , @xmath353 the asymptotic behavior of @xmath354 when @xmath355 ( @xmath48 is considered to satisfy assumption @xmath49 ) , this behavior induces that @xmath356 for all @xmath336 .",
    "moreover , @xmath357 and @xmath358 . finally , using @xmath359 , we obtain @xmath360 $ ] , @xmath361 when @xmath84 .",
    "therefore , from lebesgue theorem ( checked from the asymptotic behavior of @xmath362 ) , @xmath363 as a consequence , from ( [ som1 ] ) , ( [ som2 ] ) , ( [ som3 ] ) , ( [ som4 ] ) and ( [ som5 ] ) , the corollary is proven .",
    "@xmath341 + this proof can be decomposed into three steps : * step 1 * , * step 2 * and * step 3*. +   + * step 1*. in this part , @xmath364 is proven to converge at an asymptotic covariance matrix @xmath365 .",
    "first , for all @xmath366 , @xmath367}\\frac 1 { [ n / r_ja_n ] } \\sum_{p=1}^{[n / r_ia_n ] } \\sum_{q=1}^{[n / r_ja_n ] } \\big ( { \\mbox{cov}}(\\tilde e(r_ia_n , p),\\tilde e(r_ja_n , q ) \\big ) ^2,\\end{aligned}\\ ] ] because @xmath3 is a gaussian process .",
    "therefore , by considering only @xmath368 and @xmath369 , for @xmath217 and @xmath104 large enough , @xmath370 now , for @xmath371\\}\\times \\{1,\\ldots,[n / r_ia_n]\\}$ ] , @xmath372 using the same expansion as in ( [ fourier2 ] ) , under assumption @xmath49 the previous equality becomes , for all @xmath373 , @xmath374 with @xmath375 not depending on @xmath104 and due the asymptotic behaviors of @xmath376 when @xmath377 and @xmath378 .",
    "now , under assumption a1 , @xmath379 since @xmath380 from assumption @xmath49 .",
    "finally , from ( [ ineq1 ] ) and ( [ ineq2 ] ) , we have @xmath8 not depending on @xmath217 such that for all @xmath381 , @xmath382 it remains to evaluate @xmath383 .",
    "thus , if @xmath384 , using an integration by parts , @xmath385_{-\\pi a_n}^{\\pi   a_n } \\right .    \\\\",
    "\\nonumber & + & \\left .",
    "\\frac 1 { i(r_ip - r_jq)}\\int _",
    "{ -\\pi a_n}^{\\pi a_n } \\hspace{-3 mm } du \\,\\frac { \\partial}{\\partial u } \\big ( \\frac { \\widehat \\psi ( ur_i)\\overline{\\widehat \\psi } ( ur_j)}{u^d}\\big ) e^{-iu(r_ip - r_jq)}\\right | \\\\ \\nonumber   & & \\hspace{-6 cm } \\leq   \\frac { 1 } { |r_ip - r_jq| } \\int_{-\\infty}^{\\infty }   \\left ( \\frac d { |u|^{d+1}}\\big |\\widehat \\psi ( ur_i)\\overline{\\widehat \\psi } ( ur_j)\\big | + \\frac 1 { |u|^{d}}\\big |   \\frac { \\partial}{\\partial u } \\big ( \\widehat \\psi ( ur_i)\\overline{\\widehat \\psi } ( ur_j ) \\big ) \\big | \\right ) du \\\\ \\label{equi1 } & & \\hspace{-6 cm } \\leq c \\frac { 1 } { |r_ip - r_jq| } \\end{aligned}\\ ] ] with @xmath386 not depending on @xmath217 , since :      moreover , if @xmath394 , from cauchy - schwartz inequality and property [ vard ] , for @xmath104 large enough @xmath395 therefore , using ( [ ineq3 ] ) , ( [ equi1 ] ) and ( [ ineg_cov2 ] ) and the inequality @xmath396 for all @xmath397 , we have @xmath8 such that for @xmath104 large enough , @xmath398 hence , with ( [ covsn ] ) , @xmath399}\\frac 1 { [ n / r_ja_n ] } \\sum_{p=1}^{[n / r_ia_n ] } \\sum_{q=1}^{[n / r_ja_n]}\\big ( \\frac { 1 } { ( 1+|r_ip - r_jq|)^2}+ \\frac { 1 } { a_n^{2d ' } } \\big ) \\end{aligned}\\ ] ] but , from the theorem of comparison between sums and integrals , @xmath400 } \\sum_{q=1}^{[n / r_ja_n ] } ( 1+|r_ip - r_jq|)^{-2 } & \\leq   & \\frac 1 { r_ir_j } \\int _ 0^{n / a_n } \\int _ 0^{n / a_n}\\frac { du\\ , dv}{(1+|u - v|)^{2 } } \\\\ & \\leq   & \\frac 2 { r_ir_j } \\int _",
    "0^{n / a_n }   \\frac { n / a_n \\ , dw}{(1+w)^{2}}\\\\ & \\leq & \\frac 2 { r_ir_j } \\cdot \\frac n { a_n}.\\end{aligned}\\ ] ] as a consequence , if @xmath104 is such that @xmath401 then @xmath402 .",
    "more precisely , since this covariance is a sum of positive terms , if @xmath403 , @xmath404 a non null ( from ( [ covsn1 ] ) ) symmetric matrix with @xmath405 that can be specified . indeed , from the previous computations , if @xmath403 , @xmath406 } \\sum_{q=1}^{[n / r_ja_n]}\\big ( \\frac { ( r_ir_j)^{(1-d)/2 } } { k_{(\\psi , d ) } } \\int _ 0 ^\\infty du \\ , \\frac { \\widehat \\psi ( ur_i)\\overline{\\widehat \\psi } ( ur_j ) } { u^d}\\cos ( u(r_ip - r_jq ) ) \\big ) ^2 \\\\ & = & \\lim_{n\\to \\infty}\\frac { 8(r_ir_j)^{2-d } a_n } { k^2_{(\\psi , d)}n } \\sum_{m=-[n / d_{ij}a_n]+1}^{[n / d_{ij}a_n]-1}(\\frac n { d_{ij}a_n}-|m|\\big ) \\big (   \\int _ 0 ^\\infty du \\ , \\frac { \\widehat \\psi ( ur_i)\\overline{\\widehat \\psi } ( ur_j ) } { u^d}\\cos ( u \\,d_{ij } m ) \\big ) ^2\\\\ & = & \\frac { 8(r_ir_j)^{2-d } } { k^2_{(\\psi , d)}d_{ij } } \\sum_{m=-\\infty}^{\\infty}\\big ( \\int _ 0 ^\\infty \\frac { \\widehat \\psi ( ur_i)\\overline{\\widehat \\psi } ( ur_j ) } { u^d}\\cos ( u \\,d_{ij } m)\\ , du \\big ) ^2,\\end{aligned}\\ ] ] with @xmath407 .",
    "therefore , the matrix @xmath365 depends only on on @xmath408 . +   + * step 2*.generaly speaking , the above result is not sufficient to obtain the central limit theorem , @xmath409 however , each @xmath410 is a quadratic form of a gaussian process .",
    "_ mutatis mutandis _ , it is exactly the same framework ( _ i.e. _ a lindeberg central limit theorem ) as that of proposition 2.1 in bardet ( 2000 ) , and ( [ tlc1 ] ) is checked .",
    "moreover , if @xmath411 is such that @xmath412 then using the asymptotic behavior of @xmath413 provided in property [ vard ] , @xmath414 as a consequence , under those assumptions , @xmath415   + * step 3*. the logarithm function @xmath416 is @xmath417 on @xmath418 . as a consequence , using the delta - method , the central limit theorem ( [ cltsn ] ) for the vector @xmath419 follows with the same asymptotical covariance matrix @xmath181 ( because the jacobian matrix of the function in @xmath420 is the identity matrix ) .",
    "@xmath341 +   + there is a perfect identity between this proof and that of proposition [ tlclog ] , both of which are based on the approximations of fourier transforms provided in the proof of property [ vard ] .",
    "@xmath341 + it is clear that @xmath421 for all @xmath422 , with @xmath423 satisfying proposition [ tlclog ] and [ tlclog2 ] .",
    "but , any wavelet coefficient of @xmath424 is obviously null from the assumption on @xmath48 .",
    "therefore the statistic @xmath425 is the same for @xmath3 and @xmath426 .",
    "@xmath341 + let @xmath427 be a fixed positive real number , such that @xmath428 .",
    "+   + * i. * first , a bound of @xmath429 is provided .",
    "indeed , @xmath430}^{\\log [ n/\\ell]}\\pr\\big ( \\widehat q_n(\\alpha^*+\\varepsilon/2 ) > \\widehat q_n\\big ( \\frac { k}{\\log n } \\big)\\big ) .\\end{aligned}\\ ] ] but , for @xmath431 , @xmath432 with @xmath433 for all @xmath434 , _ i.e. _ @xmath435 is the matrix of an orthogonal projection on the orthogonal subspace ( in @xmath436 ) generated by @xmath437 ( and @xmath438 is the identity matrix in @xmath436 ) . from the expression of @xmath437 , it is obvious that for all @xmath434 , @xmath439 with the matrix @xmath440 as in proposition [ tlcd ] .",
    "thereby , @xmath441 with @xmath442 for all @xmath434 . from proposition [ tlclog ] , for all @xmath443 , the asymptotic law of @xmath444 is a gaussian law with covariance matrix @xmath445 . moreover , the rank of the matrix is @xmath446 is @xmath447 ( this is the rank of @xmath448 ) and we have    @xmath449 , not depending on @xmath217 ) such that @xmath450 is a non - negative matrix ( @xmath451 ) . as a consequence , for a large enough @xmath217 , @xmath452 with @xmath453 .",
    "moreover , from markov inequality , @xmath454 with @xmath455 and @xmath456 .",
    "like @xmath457 does not depend on @xmath217 , we obtain that @xmath458 not depending on @xmath217 , such that for large enough @xmath217 , @xmath459 and therefore , the inequality ( [ qn ] ) becomes , for @xmath217 large enough , @xmath460}^{\\log [ n/\\ell ] } n ^{-\\frac { ( \\ell-2 ) } 4\\big ( \\big ( \\frac { k}{\\log n}\\big ) -(\\alpha^*+\\varepsilon/2)\\big ) } \\\\ \\label{borne1 } & \\geq & 1 - m_1 \\cdot \\log n \\cdot n^{-\\frac { ( \\ell-2 ) } { 12 } \\varepsilon } .\\end{aligned}\\ ] ] * ii . * secondly , a bound of @xmath461 is provided . following the above arguments and notations , @xmath462 + 1}\\pr\\big ( \\widehat q_n(\\alpha^*+\\frac { 1-\\alpha^ * } { 2 \\alpha^ * } \\varepsilon ) >",
    "\\widehat q_n\\big ( \\frac { k}{\\log n } \\big)\\big ) , \\end{aligned}\\ ] ] and as above , @xmath463 now , in the case @xmath189 with @xmath464 , the sample variance of wavelet coefficients is biased . in this case , from the relation of corollary [ cor1 ] under assumption a1 , @xmath465 with @xmath466 when @xmath467 for all @xmath468 and @xmath469 . as a consequence , for large enough @xmath217 , @xmath470 with @xmath471 , because the vector @xmath472 is not in the orthogonal subspace of the subspace generated by the matrix @xmath473 .",
    "then , the relation ( [ tlc7 ] ) becomes , @xmath474 with @xmath475 , because @xmath455 and @xmath476 for all @xmath477 .",
    "hence , from the inequality ( [ qn2 ] ) , for large enough @xmath217 , @xmath478 the inequalities ( [ borne1 ] ) and ( [ borne2 ] ) imply that @xmath479 .",
    "@xmath341 +   + the central limit theorem of ( [ cltd2 ] ) can be established from the following arguments .",
    "first , @xmath480 . following the previous proof",
    ", there is for all @xmath427 , @xmath481 consequently , if @xmath482 with @xmath483 then , @xmath484 now , from corollary [ hatd ] , @xmath485 .",
    "therefore , @xmath486 .",
    "thus , with @xmath487 , @xmath488 which implies @xmath489 . + secondly , for @xmath490 , @xmath491 with @xmath492 the probability density function of @xmath208 and @xmath493 .",
    "+   + to prove the second part of ( [ cltd2 ] ) , we infer deduces from above that @xmath494 with @xmath495 . therefore , @xmath496 , @xmath497 this inequality and the previous central limit theorem result in : for all @xmath498 , and @xmath427 , @xmath499 * acknowledgments .",
    "* the authors are very grateful to anonymous referees for many relevant suggestions and corrections that strongly improve the content of the paper ."
  ],
  "abstract_text": [
    "<S> this work is intended as a contribution to a wavelet - based adaptive estimator of the memory parameter in the classical semi - parametric framework for gaussian stationary processes . </S>",
    "<S> in particular we introduce and develop the choice of a data - driven optimal bandwidth . </S>",
    "<S> moreover , we establish a central limit theorem for the estimator of the memory parameter with the minimax rate of convergence ( up to a logarithm factor ) . </S>",
    "<S> the quality of the estimators are attested by simulations . </S>"
  ]
}