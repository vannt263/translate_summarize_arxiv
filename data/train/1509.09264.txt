{
  "article_text": [
    "matrix inversion has quite numerous applications in statistics , cryptography , computer graphics , etc .",
    "it is hard to imagine a computation system or a scientific programming environment without a library or a function that calculates the inverse of a matrix .    in many applications ,",
    "where the inverse of a matrix appears , there is no actual need for direct computation of the inverse , as the corresponding problem can be solved by computing the solution of a matrix equation or , in particular cases , a system of linear equations .",
    "there are some problems , however , where the inverse of a matrix is indeed required .",
    "even though the problem of computing matrix inverse has been extensively studied and described in many numerical monographs , there is a variety of new papers dealing with the subject . in the last couple of years",
    ", a number of algorithms for inverting structured matrices ( block , banded ) were proposed .",
    "this paper focuses on recursive algorithms for inverting tridiagonal matrices .",
    "the existing recursive algorithms for computing the inverse of this kind of matrices are not very popular and are not commonly used .",
    "this is mostly because there are wide classes of matrices for which these algorithms can not be applied due to their instability or other kind of limitations ( it will be described in more detail throughout the paper ) . in this paper , we analyse the reasons of these disadvantages and search for the way to eliminatethem . as a result ,",
    "new formulae for the elements of the inverse of a tridiagonal matrix @xmath0 are proposed , which allow a very fast and accurate computation of @xmath4 , and can be applied to any non - singular tridiagonal matrix . to our knowledge , it is the only method that guarantees , in general , very small , both left and right , residual errors .",
    "let@xmath7be a tridiagonal matrix , i.e.  a matrix whose elements satisfy @xmath8 if @xmath9 ( @xmath10 ) .",
    "the most common algorithms for evaluating @xmath11 are based on solving the matrix equation @xmath12 ( or @xmath2 ; throughout the paper , by @xmath3 , we shall denote the numerically computed inverse of a tridiagonal matrix @xmath0 , or the inverse which is to be computed , and by @xmath13 , the identity matrix ) . if the above equation is solved using , for example , gaussian elimination with partial pivoting or using orthogonal transformations ( e.g. , givens rotations ) , the computed inverse satisfies ( c.f .",
    "@xcite ) @xmath14 where @xmath15 is the machine precision , @xmath16 is a small constant , and @xmath17 is the condition number of @xmath0 . in such a case , it is easy to verify that the relative error @xmath18 satisfies a similar bound : @xmath19 however , the bound is not so favourable in the case of the second residual error .",
    "we have , as @xmath20 , @xmath21      & = { \\,}\\| a^{-1}\\big(ax - i\\big ) a \\|{\\,}\\leq{\\,}{\\mathrm{cond}(a ) } \\| ax - i{\\hspace*{0.1ex}}\\| { \\,}\\leq{\\,}{\\varepsilon}{\\hspace*{0.1ex}}k{\\hspace*{0.1ex}}{\\mathrm{cond}(a)}^2 .\\end{aligned}\\ ] ] indeed , consider the following tridiagonal matrix @xmath22 whose elements @xmath23 for @xmath24 , @xmath25 are listed column - wise in the following table : @xmath26 the condition number @xmath27 .",
    "if the inverse @xmath3 is computed in the double precision arithmetic ( @xmath28 ) using the _ matlab _  command ` x = a\\eye(10 ) ` , i.e.  using the gaussian elimination with partial pivoting , then we will obtain the result that satisfies @xmath29 for the left residual error , however , we have @xmath30 and there are elements larger than @xmath31 outside the main diagonal in the matrix @xmath32 .",
    "such an inverse may not always be considered as a very satisfactory one . this problem has been already considered by higham in @xcite and @xcite , who proposed a symmetric algorithm for computing @xmath11 based on @xmath33 factorisation . however , as the algorithm uses no pivoting strategy , it is not stable in general .",
    "the asymptotic complexity of the most common algorithms for inverting tridiagonal matrices equals : @xmath34 in the case of gaussian elimination without pivoting , @xmath35 in the case of gaussian elimination with partial pivoting , and @xmath36 in the case of the algorithm based on givens rotations . in the present paper",
    ", we will propose some new formulae which allow to construct a stable , always applicable algorithm with the smallest possible asymptotic complexity:@xmath37 .",
    "the authors do not know the formal proof of stability of the algorithm yet , but strong justification is given to support the conjecture that the inverse @xmath3 of a tridiagonal matrix @xmath38 computed using the proposed method satisfies @xmath39 where @xmath40 .    in the incoming section , we shall present a short review of the recursive approach to the problem of inverting a tridiagonal matrix .",
    "we shall also recall the basic facts from the theory of difference equations that will help to explain the reasons several recently proposed recursive algorithms for inverting tridiagonal matrices fail ( or are unstable ) for some important classes of matrices .    in section [ secnewalg ] , we present a new efficient method , also based on some recursions , which can be successfully applied to invert any non - singular tridiagonal matrix .",
    "in this section , we review several algorithms for recursive computation of the inverse of a tridiagonal matrix .",
    "showing the strong and the weak sides of these algorithms will lead us to the main result of the paper .",
    "it is well known that for@xmath41and @xmath42the product@xmath43 can be interpreted as the linear combination of columns of the matrix @xmath44 , @xmath45 where @xmath46 denotes the @xmath47-th column of @xmath44 .",
    "such interpretation is very convenient if @xmath48 has only a few non - zero elements .",
    "let @xmath7be a tridiagonal matrix ( @xmath8 if @xmath9 ) .",
    "then , the equation @xmath49 implies the following relations : @xmath50    a_{n-1,n } x_{n-1 } + a_{n , n } x_n & = i_n\\,.\\end{aligned}\\ ] ] thus , if we know the last column @xmath51 of @xmath3 , then we can easily recursively compute the whole inverse matrix @xmath3 : @xmath52 where we assume that@xmath53 ( @xmath54 ) .",
    "the above simple observation laid the basis of two recent algorithms , @xcite and @xcite , for inverting tridiagonal matrices . in @xcite , the last column @xmath51 is computed using the lu factorisation without pivoting , while in @xcite , the miller algorithm  a classical algorithm for computing the _ minimal _ solutions of second order difference equations  has been rediscovered .",
    "let us test the stability of the above scheme for a very small and very well conditioned matrix @xmath55 for the inverse matrix @xmath3 computed in the double precision arithmetic using the algorithm @xcite or @xcite ( the result does not depend on which of the above ways the last column of @xmath3 is calculated ) , we have @xmath56 while @xmath57 . if the inverse is evaluated in _",
    "matlab _ : ` x = a\\eye(6 ) ` , the residual errors satisfy @xmath58 .",
    "the explanation of such a huge instability of the simple recursive algorithms based on ( [ simplrecalg ] ) is quite simple if we recall some basic facts of the theory of linear second order difference equations ( see , for example , @xcite or @xcite ) .",
    "the three term ( second order ) homogenous recurrence ( difference ) equation can be written in general form as follows : @xmath59 where @xmath60 ( @xmath61 ) are known coefficients , and @xmath62 is a solution we are looking for .",
    "the equation ( [ gendiffeq ] ) has two - dimensional space of solutions . if there exist two linearly independent solutions @xmath63 and @xmath64 such that @xmath65 then @xmath63 is called a _ minimal _ , and @xmath64 is called a _",
    "dominant _ solution .",
    "the _ forward recursion _",
    "algorithm , @xmath66 is stable for dominant solutions only , while the _ backward recursion _ , @xmath67 is stable only for minimal solutions ( at the present point , we do not consider the problem of obtaining the initial values for the above recurrences ) . if for any pair@xmath63,@xmath64of independent solutions we have @xmath68 where @xmath69 , then both forward and backward recursion algorithms are stable ( in the asymptotic sense ) .    in practice , we are usually interested in computing only a part of a solution of the recurrence equation ( [ gendiffeq ] ) , i.e.  the values of @xmath70 for @xmath71 for some @xmath72 .",
    "note that the starting point ( @xmath73 ) and the main ( forward ) direction of the recursion ( @xmath74 ) is only a convention .",
    "the space of all minimal solutions of the linear second order difference equation is one - dimensional",
    ". an important property of the minimal solutions and the algorithm for computing the values of such a solution is given in the following theorem .",
    "[ twmiller ] assume that a three term recurrence equation ( [ gendiffeq ] ) has a minimal solution @xmath63 which satisfies a normalising condition @xmath75 for @xmath76 , define the values @xmath77}_k$ ] ( @xmath78 ) as follows : @xmath79}_{n+1 } = 0 , \\quad x^{[n]}_n = d \\neq 0,\\\\[0.75ex ]    & x^{[n]}_{k-1 } =    - a_{-{\\hspace*{-0.1ex}}1}(k)^{{\\hspace*{-0.1ex}}-1}\\big(a_{0}(k){\\,}x^{[n]}_k    + a_{1}(k){\\,}x^{[n]}_{k+1}\\big ) \\qquad ( n \\geq k > 1){\\hspace*{0.1ex}}. \\vspace{0.75ex}\\end{aligned}\\ ] ] then , for each @xmath80 @xmath81}_k = { \\,}{\\hspace*{0.2ex}}u_k{\\hspace*{0.1ex}},\\ ] ] where @xmath82}_k \\vspace{-0.5ex}\\ ] ] ( we assume that @xmath77}_k=0 $ ] if @xmath83 ) .",
    "see @xcite or @xcite .",
    "let us consider again the recursion ( [ simplrecalg ] ) , but only for the elements of the first row of the inverse matrix @xmath3 .",
    "we may of course write that @xmath84 . additionally , if @xmath53 for all @xmath54 , then , theoretically , @xmath85 ( this will be justified in the later part of the paper ; see also @xcite ) .",
    "consequently , we have @xmath86 and ( also from the equation @xmath2 ) @xmath87 . comparing the above formulae to the ones of theorem [ twmiller ]",
    ", we may conclude that if the recurrence equation for the first row of the inverse matrix @xmath3 has minimal and dominant solutions , then the elements @xmath88 of the first row of @xmath3 behave more like a minimal solution than like a dominant one .",
    "therefore , the recursion ( [ simplrecalg ] ) which is the backward recursion for @xmath89 ( @xmath90 ) is stable for all elements of the first row of the matrix @xmath3 . in the case of the @xmath91-th row , we have @xmath92 note that this recurrence is exactly the same as the recurrence ( [ row1simplrec ] ) , only the starting value @xmath93 is replaced by @xmath94 .",
    "this implies that the recursion ( [ simplrecalg ] ) is stable for all elements in the upper triangle of @xmath3 . in the lower triangle of @xmath3 ,",
    "the situation is reversed .",
    "the recursion ( [ simplrecalg ] ) is the forward recursion algorithm for the elements @xmath95 ( @xmath96 , @xmath97 ) , and therefore may be unstable if the corresponding recurrence equation , @xmath98 has minimal and dominant solutions . indeed , in the case of the matrix @xmath3 computed using ( [ simplrecalg ] ) , where @xmath0 is given by ( [ matrix2016 ] ) , we have @xmath99 ( for readability , diagonal elements are underlined ) .",
    "the recursion ( [ simplrecalg ] ) was obtained from the matrix equation @xmath2 .",
    "the second , twin equation , @xmath1 , implies the analogous recurrence for rows of the matrix @xmath3 . from the discussion above , the important result follows .",
    "[ wntowardsdiag ] if the elements of the inverse @xmath3 of a tridiagonal matrix are computed recursively , then , in general , the algorithm is stable only if the recurrences are carried out towards the main diagonal of @xmath3 .",
    "we end this subsection by formulating the algorithm for stable recursive computation of the last column of @xmath3 ( the algorithm is a consequence of theorem [ twmiller ] and the equation @xmath100 , and is a particular case of the _ miller backward recursion algorithm _ ) : @xmath101    y_{k+1 } & = { \\hspace*{0.1ex}}- a_{k , k+1}^{-1}\\big(a_{k , k-1}{\\hspace*{0.1ex}}y_{k-1 } + a_{k , k}{\\hspace*{0.1ex}}y_{k}\\big )               \\quad\\ , ( 1\\leq k < n ) , \\\\[0.75ex ]    x_{k , n } & = { \\hspace*{0.1ex}}f y_{k }               \\quad\\ , ( 1 \\leq k \\leq n),\\end{aligned}\\ ] ] where the normalising factor @xmath102 .      by corollary [ wntowardsdiag ] , we conclude that a single recursion , like ( [ simplrecalg ] ) , can not lead to a stable algorithm for inverting tridiagonal matrices . however ,",
    "as the algorithm ( [ simplrecalg ] ) computes the upper triangle of the inverse matrix @xmath3 correctly , we may use a similar scheme to compute the elements in the lower triangle , namely , compute the first column @xmath103 using the miller algorithm , and then compute recursively the elements below the diagonal in the consecutive columns @xmath104 ( @xmath105 ) .",
    "the scheme looks as follows : compute the columns @xmath103 and @xmath51 using the miller algorithm , and then set @xmath106 we assume that @xmath53 and @xmath107 for @xmath54 .",
    "the above algorithm appears to be presented in print for the first time , but it is still not a very good one as we shall justify below .",
    "observe that the formulae ( [ twowayrecalg ] ) are based on the equations @xmath108 ( throughout the paper we assume than @xmath109 and @xmath110 if @xmath111 , or @xmath112 , or @xmath113 , or @xmath114 ) , while the miller algorithm for computing the columns @xmath103 and @xmath51 is based on the equations @xmath115 theoretically , in our applications of the miller algorithm we use the above equations only for @xmath116 .",
    "however , as @xmath117 , all the equations ( [ axscalarrec ] ) for @xmath118 and @xmath119 are in fact the same recurrence equations ( an analogous observation is true for @xmath120 and @xmath121 ) .",
    "the normalising factors used for computing the columns @xmath103 and @xmath51 by the miller algorithm are derived from the two additional equations , @xmath122 but the remaining equations , @xmath123 for @xmath124 , are nowhere used in the above algorithm . in other words ,",
    "the presented two - way recursion algorithm computes the matrix @xmath3 which satisfy the following system of matrix equations @xmath125 where @xmath126 and @xmath127 are diagonal matrices for which we only know that @xmath128 .",
    "an interesting question arises : does the algorithm based on ( [ twowayrecalg ] ) computes the actual inverse of a tridiagonal matrix @xmath0 , assuming that the computations are exact ?",
    "the answer to the above question is delivered by the following theorem .",
    "[ twtwoonesonly ] let @xmath38 be a non - singular tridiagonal matrix such that @xmath107 and @xmath53 for @xmath54 . if a matrix @xmath3 is a solution of the system of matrix equations @xmath129 where @xmath126 and @xmath127 are diagonal matrices and @xmath127 satisfy @xmath128 , then @xmath130 and , consequently , @xmath4 .    the equations ( [ dheqs ] ) imply that @xmath131 equating the corresponding elements of the matrices @xmath132 and @xmath133 in a proper order , and recalling that @xmath134 and @xmath135 for @xmath136 , we obtain , in sequence , @xmath137 similarly , but using the condition @xmath138 , we get @xmath139 if @xmath140 is",
    "even , then from ( [ dh1forward ] ) and ( [ dh1backward ] ) we immediately obtain that @xmath141 for @xmath142 .",
    "if @xmath140 is odd , then at least one of the diagonal elements of @xmath0 is different from 0 ( otherwise there would be @xmath143 ) . for simplicity , assume that @xmath144 .",
    "then , @xmath145 by combining the above result with ( [ dh1forward ] ) , again , we get @xmath141 for @xmath142 .",
    "the above theorem implies that in theory , the algorithm based on the recursions ( [ twowayrecalg ] ) computes the correct inverse @xmath3 of a tridiagonal matrix @xmath0 with non - zero elements on the sub- and super- diagonals .",
    "the situation is a little different in practice .",
    "the numerical performance of this algorithm is far from perfection . as the lower and the upper triangles of @xmath3",
    "are computed completely independently , the values of magnitude close to @xmath146 may appear for more detailed explanation .",
    "] along the main diagonals of the residual matrices @xmath147 and @xmath148 . thus",
    ", the search for a better algorithm has to be continued .",
    "before it is done , we shall consider for a moment the complexity of the methods presented so far .",
    "it is readily seen that the algorithm based on ( [ twowayrecalg ] ) , like the one based on ( [ simplrecalg ] ) , has the complexity equal to @xmath34 . however , if we take a closer look on the equations in ( [ twowayrecalg ] ) , we can see that the recurrence for the elements in the upper triangle of @xmath3 ( the first two lines ) is exactly the same for each value of the row index @xmath91 , only the initial values are different .",
    "thus , we may carry out the recurrence only once , and then scale the result according to the corresponding initial values : @xmath149 the above scheme uses @xmath150 arithmetic operations . if a similar modification is done for the computation of the lower triangle of @xmath3 , the complexity of the whole algorithm will drop to the smallest possible  as the inverse of a tridiagonal matrix has , in general , @xmath151 different elements  asymptotic value : @xmath37 .      in order to improve the numerical properties of the recursive algorithm for inverting tridiagonal matrices",
    ", we should include the known dependence between the elements in the upper triangle of the inverse matrix @xmath3 and the elements in the lower triangle .",
    "the simple formula that relates these elements was given in @xcite .",
    "[ lmlewissymm ] if @xmath38 is a tridiagonal matrix such that @xmath152 for @xmath153 , and @xmath154 , then @xmath155    see @xcite .",
    "the formula ( [ lewissymm ] ) is a direct consequence ( cf .",
    "@xcite ) of the equations @xmath156 i.e.  is implied , in particular , by the equations ( [ missingones ] ) that were not used in the algorithm presented in the previous subsection . now , the following algorithm with a very favourable numerical properties can be formulated : compute the last column @xmath51 using the miller algorithm ; compute the remaining elements in the upper triangle of @xmath3 using ( [ fasttwowayrec ] ) for @xmath157 , instead of for @xmath158 ; compute the lower triangle of @xmath3 using ( [ lewissymm ] ) .",
    "this algorithm works very well if the two following conditions are satisfied : if @xmath53 for all @xmath159 , and if the values @xmath160 ( @xmath161 ) in ( [ fasttwowayrec ] ) do not grow too large , causing floating - point overflow ( which is , unfortunately , a quite frequent case ) .",
    "a very similar algorithm was formulated in @xcite , where only the last column @xmath51 is computed in a slightly different ( but mathematically and numerically equivalent ) way .",
    "note that due to ( [ lewissymm ] ) , the complexity of the above scheme grew to @xmath162 .",
    "the complexity , however , can be reduced back to @xmath163 if we apply the relation ( [ lewissymm ] ) in a little different way .",
    "the following theorem is one of the main results of @xcite :    [ twlewis ] assume that @xmath0 is a non - singular tridiagonal matrix which satisfies @xmath53 and @xmath107 for @xmath159 .",
    "let the sequences @xmath164 , @xmath165 , @xmath166 be defined in the following way : @xmath167 then , the inverse @xmath154 satisfies @xmath168    it is readily seen that the equations ( [ alglewis(1)])([alglewis(2 ) ] ) allow to compute the inverse matrix @xmath4 using only @xmath37 arithmetic operations .",
    "more importantly , the algorithm based on theorem [ twlewis ] uses every single scalar equation ( cf .",
    "( [ xascalarrec ] ) , ( [ axscalarrec ] ) and ( [ notmissingones ] ) ) that results from the system of matrix equations @xmath169 .",
    "therefore , we should expect the residual errors @xmath170 and @xmath171 to be very small .",
    "numerical experiments confirm that .",
    "unfortunately , the algorithm ( [ alglewis(1)])([alglewis(2 ) ] ) can not be applied for a quite wide class of tridiagonal matrices .",
    "the first problem is that the quantities @xmath172 ( @xmath71 ) and @xmath160 ( @xmath90 ) may grow very fast causing the floating - point overflow . in other words",
    ", the procedure will fail if @xmath173 with respect to the given floating - point arithmetic ( it is quite easy to show , c.f .",
    "@xcite , that in theory , we always have @xmath85 if the assumptions of theorem [ twlewis ] are satisfied ) .",
    "note that all algorithms presented so far in this paper suffer the high risk of floating - point overflow .",
    "another problem is the assumption that all sub- and super- diagonal elements are different from 0 . in @xcite",
    "the following solution is suggested .",
    "assume that @xmath174 for some @xmath175 .",
    "then , we have @xmath176 where @xmath177 , @xmath178 are tridiagonal matrices , and @xmath179 has only one non - zero element @xmath180 ( if we assume that @xmath152 ) .",
    "consequently , @xmath181_k \\big([(g^{-1})^t]_1)^t , \\ ] ] where by @xmath182_k$ ] we denote the @xmath47-th column of the corresponding matrix .",
    "the matrices @xmath183 and @xmath184 are inverted by the algorithm ( [ alglewis(1)])([alglewis(2 ) ] ) ( using the above block form again whenever necessary ) .",
    "the numerical drawback of this approach is that the inverses of @xmath184 and @xmath183 are computed independently . as a result",
    ", the elements of the residual matrices @xmath147 and @xmath148 that lay along the lines corresponding to the borders of the upper - right block in ( [ blockinv ] ) may have the magnitude of order @xmath185 ( see the beginning of section [ secnewalg ] for more detailed explanation ) .",
    "note that there are matrices for which @xmath186 is close to @xmath187 .    the equations ( [ alglewis(2 ) ] )",
    "delivers compact closed - form formulae for the elements of the inverse matrix @xmath3 .",
    "the following known characterisation of the inverse of a tridiagonal matrix follows immediately .",
    "[ twrank1 ] if @xmath38 is a tridiagonal matrix and @xmath154 , then all matrices @xmath188 ( @xmath71 ) have rank not greater than 1 .",
    "if @xmath53 and @xmath107 for @xmath159 , the assertion is obtained readily from ( [ alglewis(2 ) ] ) , otherwise from ( [ blockinv ] ) and ( [ alglewis(2 ) ] ) .",
    "alternative proofs of the above property can be found in @xcite and @xcite .",
    "we start this section by explaining why using all the scalar equations that result from the condition @xmath169 is crucial for obtaining an algorithm for inverting tridiagonal matrices that guarantees very small residual errors @xmath170 and @xmath171 .",
    "assume that for some @xmath189 and @xmath190 the elements @xmath191 , @xmath95 , @xmath192 of the inverse matrix @xmath3 were evaluated as @xmath193 ( @xmath194 ) for some real number @xmath195 , and that the quantity @xmath196 was computed from the equation @xmath197 in such a case , we have @xmath198 where @xmath199 ( for simplicity we ignore the terms of order @xmath200 ) .",
    "consequently , @xmath201 on the other hand , if at least one of the elements @xmath191 , @xmath95 , and @xmath192 was computed independently of the other two , then the best we may in general expect is that @xmath202 ( @xmath61 ) , where @xmath203 denote the exact values of the considered elements of @xmath11 , and @xmath204 for some small @xmath205 .",
    "we have , of course , @xmath206 , however , if @xmath203 ( @xmath61 ) are replaced by their computed values @xmath207 , then we obtain @xmath208 by comparing the inequalities ( [ scalarreserrgoodrec ] ) and ( [ scalarreserrbadrec ] ) , we conclude that the algorithm that does not _ fully _ exploit the equations @xmath169 may compute the inverse for which the residual error @xmath6 ( or @xmath5 ) is about @xmath187 times larger than in the case of the algorithm that does . finding an algorithm which is a complete reflection of the equations @xmath169 , and can be applied for an arbitrary tridiagonal matrix @xmath0 ( and is stable ) is not easy , and  to our knowledge  has not been succeeded yet .    before we take care of the problem described above , we will solve a little less difficult one , related to the possible occurrence of the floating point overflow when computing the solutions of the recurrence relations .",
    "assume that the solution of the difference equation ( [ gendiffeq ] ) we are looking for satisfies the condition @xmath209 for all @xmath210 .",
    "if we define @xmath211 , then the equation ( [ gendiffeq ] ) can be written in the equivalent form : @xmath212 once the ratios @xmath213 are computed , and the value @xmath214 or @xmath215 ( for some @xmath216 ) is known , we may easily compute all other values @xmath70 : @xmath217 this approach is well know in the theory of the second order difference equations and was described in detail in @xcite .",
    "it practically eliminates the risk of the floating point overflow when computing the solution @xmath218 of ( [ gendiffeq ] ) .",
    "the use of ratios in the problem of inverting tridiagonal matrices was already suggested ( but not strictly formulated given there are numerically unstable in general case . ] ) in @xcite , where the algorithm , in a sense similar to ( [ alglewis(1)])([alglewis(2 ) ] ) , for inverting tridiagonal symmetric positive definite matrices with all negative sub- and super- diagonal elements was proposed .",
    "let us consider the lower triangle of @xmath154 , and assume that @xmath219 ( @xmath142 , @xmath220 . from the equation @xmath49 , we have ( for @xmath221 and @xmath222 ) @xmath223    & a_{k-1,k}{\\,}x_{s , k-1 } + a_{k , k}{\\,}x_{s , k } +      a_{k+1,k}{\\,}x_{s , k+1 } = 0{\\hspace*{0.1ex}}. \\vspace{-0.5ex}\\end{aligned}\\ ] ] now ,",
    "setting @xmath224 ( @xmath54 ) , we immediately obtain @xmath225 from the equation @xmath12 , we have @xmath226 where @xmath227 ( @xmath175 , @xmath228 ) .",
    "note that the ratios @xmath213 do not depend on the column index @xmath229 , and @xmath230 ( @xmath159 ) do not depend on the row index @xmath91 ( this fact may be considered as another proof of theorem [ twrank1 ] ) .    for the upper triangle of @xmath3 , we define @xmath231 ( @xmath54 , @xmath232 ) and @xmath233 ( @xmath175 , @xmath234 ) . for these ratios ,",
    "the recurrences similar to ( [ colratiosrec ] ) and ( [ rowratiosrec ] ) can be also derived .",
    "however , the upper triangle ratios should not be computed independently of the lower triangle ones for the reasons described earlier  the residual errors may depend on @xmath235 in such a case .",
    "the following result should be applied instead .",
    "[ lmratiossymm ] let @xmath0 be a tridiagonal matrix satisfying @xmath53 , @xmath107 for @xmath159 .",
    "if the ratios @xmath213 , @xmath236 ( @xmath237 ) , and @xmath230 , @xmath238 ( @xmath159 ) are defined as above , then @xmath239      observe that from the equation @xmath240 we immediately obtain that @xmath241 .",
    "if a tridiagonal matrix @xmath0 satisfies the assumptions of lemma [ lmratiossymm ] , and its inverse has only non - zero elements , then from ( [ starteq ] ) and ( [ colratiosrec])([ratiossymm ] ) , we obtain the following set of algorithms for inverting the matrix @xmath0 .            using the ratios @xmath213 , @xmath230 , @xmath238 , and @xmath236 , compute all other elements of @xmath3 from the adjacent ones , in an ( theoretically ) arbitrary order , using only one multiplication or division for each element .",
    "clearly , the above algorithm uses @xmath37 arithmetic operations . in figure",
    "[ figorder ] , we present some exemplary orders the inverse matrix @xmath3 may be computed in .",
    "the first of them is presented as an analogy to the algorithm based on ( [ simplrecalg ] ) .",
    "this time , however , no instability occurs , as the recurrences for ratios are carried out in the stable direction ( the arrows on the graphs correspond to step 4 of algorithm [ algkw ] ) .",
    "the last example is presented just for fun .",
    "note that with these two orders , the algorithm fails if @xmath173 .",
    "there is no such problem with the two remaining suggested orders .",
    "the lower left one is much better suited in the case matrices are stored column - wise in the computer memory ( one may use , of course , its row analogy if matrices are stored row - wise ) .",
    "may be computed when using algorithm [ algkw ] .",
    "the initial element is coloured purple.,title=\"fig:\",width=188,height=188 ]   may be computed when using algorithm [ algkw ] .",
    "the initial element is coloured purple.,title=\"fig:\",width=188,height=188 ] +   may be computed when using algorithm [ algkw ] .",
    "the initial element is coloured purple.,title=\"fig:\",width=188,height=188 ]   may be computed when using algorithm [ algkw ] .",
    "the initial element is coloured purple.,title=\"fig:\",width=188,height=188 ]    in the remaining part of the paper we shall use and extend the following formulae , which corresponds to the lower left diagram in figure [ figorder ] : @xmath243 { \\displaystyle}\\hspace*{-1.225ex }    { \\renewcommand{\\arraystretch}{1.5 }     \\left.\\begin{array}{l }       x_{j , k-1 } = q_k{\\hspace*{0.2ex}}x_{j , k } \\quad ( k\\leq j\\leq n ) \\\\",
    "x_{k-1,k-1 } = r_{k-1}^{-1 } x_{k , k-1 }     \\end{array}\\right\\ } } \\quad ( n \\geq k > 1){\\hspace*{0.1ex } } , \\\\[2.5ex ] { \\displaystyle}x_{j , k+1 } = \\big(a_{k+1,k}^{-1 } a_{k , k+1 } r_{k}\\big ) x_{j , k }     \\quad   ( 1\\leq j\\leq k,\\,\\,\\,1 \\leq k < n){\\hspace*{0.1ex}}.\\\\[-1ex ] \\end{array}\\quad\\right\\ } \\label{kwbasic } \\vspace{0.75ex}\\ ] ] the above scheme is valid only if the inverse matrix @xmath3 has only non - zero elements in its lower triangle ( note that this implies that @xmath244 for @xmath175 , and that each ratio that appears in ( [ kwbasic ] ) is finite  for proof , see theorem [ twzeroblocks ] below ) .",
    "now , we shall investigate the numerical properties of the algorithm based on ( [ kwbasic ] ) .",
    "consider the elements @xmath191 , @xmath95 , @xmath192 ( @xmath245 ) of @xmath3 .",
    "they are computed as follows @xmath246 , @xmath247 , which means that the numerically computed values satisfy @xmath248 and @xmath249 , where @xmath250 . from the first set of equations in ( [ kwbasic ] ) , we readily obtain that for the numerically computed ratios @xmath230 and @xmath251 the following equality holds : @xmath252 ( again , we ignore the terms of order @xmath200 ) . by combining the two above results",
    ", we get @xmath253 every three adjacent elements in a column of the lower triangle of @xmath3 are computed exactly as follows : @xmath254    x_{k , j}{\\,}&={\\,}x_{k+1,k}/r_k{\\hspace*{0.1ex}}q_{k}{\\hspace*{0.1ex}}q_{k-1}{\\hspace*{-0.1ex}}\\cdots{\\hspace*{0.1ex}}q_{j+1 } , \\\\[0.5ex ]    x_{k+1,j}{\\,}&={\\,}x_{k+1,k}{\\hspace*{0.1ex}}q_{k}{\\hspace*{0.1ex}}q_{k-1}{\\hspace*{-0.1ex}}\\cdots{\\hspace*{0.1ex}}q_{j+1 } .",
    "\\vspace{0.5ex}\\end{aligned}\\ ] ] therefore , a bound analogous to ( [ kwlowroweqest ] ) can be obtained with the leading factor @xmath255 replaced with some @xmath256 that depends linearly on the column index @xmath229 .",
    "obviously , @xmath257 for some @xmath258 .",
    "similar @xmath259-dependent bounds may be obtained ( in a little more tedious way ) for all remaining elements of the matrices @xmath260 and @xmath147 if we recall the relations ( [ ratiossymm ] ) .        from the above lemma ( or its several obvious generalisations )",
    ", we immediately obtain that the inverse matrix @xmath3 computed numerically using the algorithm ( [ kwbasic ] ) satisfies ( for simplicity , we restrict our attention to the @xmath266 norm only ) @xmath267 where @xmath40 .",
    "note that a similar ( even sharper ) bound for @xmath268 holds in the case of the unstable recursive algorithm based on ( [ simplrecalg ] ) .",
    "the important difference is the relation between @xmath269 and @xmath270 .",
    "we are convinced that if the inverse @xmath3 is computed using ( [ kwbasic ] ) , then @xmath271 for some @xmath272 .",
    "possibly , the difference between @xmath270 and @xmath269 may be larger than the right hand side of ( [ xnorm ] ) , but then @xmath273 . our presumption is based on the fact that we are computing minimal solutions of the difference equations which correspond to the equations @xmath274 ( the recurrences for ratios are carried out in the stable , towards - the - diagonal direction ) . in such a case ,",
    "if we are moving away ( along a row or a column ) from the main diagonal , the elements of @xmath3 should not grow faster ( or decrease slower ) in modulus than the elements of @xmath11 , which , we think , implies the inequality ( [ xnorm ] )",
    ". however , a formal proof of that assumption may be difficult .",
    "[ cjkwerr ] if @xmath38 is a non - singular matrix whose inverse has only non - zero elements in the lower triangle , and @xmath275 for some constant @xmath276 , then the inverse @xmath3 computed numerically by the algorithm ( [ kwbasic ] ) satisfies @xmath277 for some @xmath278 .",
    "[ rmlewiserr ] in the case of the algorithm of lewis ( c.f .",
    "@xcite or theorem [ twlewis ] ) , bounds similar to ( [ kwlowroweqest ] ) can be derived for all non - diagonal elements of the residual matrices @xmath279 and @xmath280 . for the diagonal elements , only @xmath259-dependent bounds hold .",
    "this implies that the numerical properties of the algorithm ( [ alglewis(1)])([alglewis(2 ) ] ) and the new algorithm based on ( [ kwbasic ] ) are comparable .",
    "recall that if the lewis algorithm is used together with ( [ blockinv ] ) , then moduli of some elements of the residual matrices may be almost as large as @xmath281 .",
    "[ rm2ndnorm ] the important step in justifying conjecture [ cjkwerr ] is lemma [ lmsumnorm ] which is not true in the case of the second matrix norm in general . the assertion of lemma [ lmsumnorm ] is an immediate consequence of the fact that @xmath282 if @xmath283 . in the case of the @xmath284 norm ,",
    "taking the moduli of all elements of a matrix may increase the norm by a factor proportional to @xmath285 ( @xmath140 is the matrix size ) .",
    "however , if @xmath286 , where @xmath0 is a tridiagonal matrix , or @xmath287 , then @xmath288 , and the inequality similar to ( [ sumnormest ] ) is satisfied with the factor @xmath289 replaced by @xmath255 .",
    "consequently , we suspect the error estimation ( [ mainerrest ] ) to be also true in the case of the second matrix norm .",
    "the last thing to do is to extend the scheme given in ( [ kwbasic ] ) so that it can be applied to an arbitrary non - singular tridiagonal matrix . obviously ,",
    "unlike , e.g. , ( [ blockinv ] ) , the extension should preserve the very favourable numerical properties of the formulae ( [ kwbasic ] ) , and also should not increase the computational complexity . to achieve the goal ,",
    "several conditions need to be fulfilled .",
    "the computations may include only one initial element , all other elements of the inverse matrix @xmath3 should be computed from another element ( adjacent if possible ) by only one multiplication ( or division ) , @xmath290 , in such a way that each relation between two elements is a direct consequence of the equations @xmath291 .",
    "of course , the number of additional arithmetic operations should depend linearly on the matrix size @xmath140 .",
    "[ twzeroblocks ] let @xmath38 be a non - singular tridiagonal matrix , and @xmath4 .",
    "+ * a ) * if @xmath292 , then @xmath293 for all @xmath294 and @xmath228 ( @xmath295 ) . +",
    "* b ) * if @xmath296 , then @xmath293 for all @xmath297 and @xmath232 ( @xmath298 ) . +",
    "* c ) * if @xmath299 or , equivalently , @xmath300 , then @xmath301 for @xmath302 , and @xmath303 for @xmath304 ( @xmath298 ) . +",
    "* d ) * if @xmath305 or , equivalently , @xmath306 , then @xmath303 for @xmath307 , and @xmath301 for @xmath308 ( @xmath295 ) . +",
    "* e ) * each block of zeros in the inverse matrix @xmath3 may consist only of the four different types of blocks described above .      before we proceed , one more problem , related to the computation of ratios , has to be solved .",
    "observe that if @xmath309 , then @xmath310 . in this case , the relation ( [ ratiossymm ] ) remains true , but it can not be used to compute the ratio @xmath311 .",
    "the following lemma delivers a scheme that allows to compute all the ratios in every case , without sacrificing the relation ( [ ratiossymm ] ) .",
    "the formulae ( [ ratiossmartqlru ] ) and ( [ ratiossmartqurl ] ) follow immediately from the definition of the ratios and the relation ( [ ratiossymm ] ) if we additionally assume the standard convention that @xmath323 for @xmath324 , @xmath325 for @xmath326 , and @xmath327 for @xmath328 .      by ( [ ratiossymm ] ) , the last terms in ( [ sqlru ] ) and ( [ tqurl ] ) can be replaced with @xmath331 and @xmath332 , respectively .",
    "the method proposed in this paper has slightly better numerical properties if the  leading  ratios  the ones that appear in ( [ sqlru ] ) and ( [ tqurl ] )  belong to the same triangle , i.e. if the leading pairs are : @xmath333 and @xmath334 , or : @xmath335 and @xmath336 .",
    "[ twkwext ] let @xmath38 be a non - singular tridiagonal matrix , and @xmath4 .",
    "let us assume that the ratios @xmath337 , @xmath238 ( @xmath159 ) , and @xmath236 , @xmath338 ( @xmath237 ) are given by ( [ ratiossmartqlru ] ) and ( [ ratiossmartqurl ] ) . + * a ) * for @xmath159 , if @xmath339 , then @xmath340 otherwise , for @xmath341 , we have @xmath342 in addition , if @xmath343 , then for @xmath344 , @xmath345 * b ) * for @xmath159 , the diagonal element @xmath346 satisfies @xmath347 * c ) * for @xmath237 , if @xmath348 , then @xmath349 otherwise , for @xmath350 we have @xmath351 in addition , if @xmath352 , then for @xmath353 @xmath354    the formulae ( [ kwextlower])([kwextuppermissing ] ) result from the equations @xmath355 , the relations ( [ lewissymm ] ) and ( [ ratiossymm ] ) , and from theorem [ twzeroblocks ] . the complete proof is not very difficult , but is quite long",
    ". therefore , we shall justify only the one before last formula in ( [ kwextlowermissing ] ) , which  we think  is the most difficult one to prove .    from theorem [ twzeroblocks ] , we conclude that if @xmath356 and @xmath357 , then @xmath358 and @xmath110 for @xmath359 , @xmath220 .",
    "this implies that the closest non - zero element to @xmath360 in the lower triangle is @xmath361 , but there is no  multiplicative path  between these two elements that would lead through the lower triangle only .",
    "however , in this case , we also ( by theorem [ twzeroblocks ] ) know that @xmath362 , @xmath363 , and that @xmath364 , @xmath365 , and @xmath366 , as @xmath0 is non - singular and @xmath152 .",
    "consequently , from the equations @xmath1 and @xmath367 , and from ( [ lewissymm ] ) , we have @xmath368    all other equations of theorem [ twkwext ] can be proved in an analogous way .",
    "what still may need a little more explanation is that , e.g. , the second and fourth equations of ( [ kwextlowermissing ] ) refer to the element @xmath369 which does not exist if @xmath370 .",
    "however , it can be proved ( using .",
    "e.g. , theorem [ twzeroblocks ] ) that if @xmath0 is a non - singular matrix , then the conditions required by these two equations can be satisfied only for @xmath371 .    the formulae ( [ kwextlower])([kwextuppermissing ] ) may be considered as a detailed description of the new algorithm which can invert any non - singular tridiagonal matrix @xmath0 if we add one initial step : @xmath372 .",
    "the algorithm is not as elegant as , e.g , pivoting in the case of gaussian elimination , but has a very important feature : has the same complexity as its basic version , i.e.  @xmath37 .",
    "what is even more important , the new extended algorithm has the same numerical properties as the one given by ( [ kwbasic ] ) .",
    "some doubts may be related to the last formulae of ( [ kwextlowermissing ] ) and ( [ kwextdiag ] ) , where we , in fact , use another starting element . however ,",
    "these two cases correspond to the situation , where @xmath0 is a block diagonal matrix , and so @xmath373 in this particular case only , the matrices @xmath183 and @xmath184 can be inverted independently , as they are in no way related in the equations @xmath274 .",
    "note that with the proposed scheme , there is no need for special treatment of cases analogous to ( [ blockdiaginv ] ) , as the initial element for the inverse matrix @xmath374 is computed  as one could say  on the way ."
  ],
  "abstract_text": [
    "<S> if @xmath0 is a tridiagonal matrix , then the equations @xmath1 and @xmath2 defining the inverse @xmath3 of @xmath0 are in fact the second order recurrence relations for the elements in each row and column of @xmath3 . </S>",
    "<S> thus , the recursive algorithms should be a natural and commonly used way for inverting tridiagonal matrices  but they are not . </S>",
    "<S> even though a variety of such algorithms were proposed so far , none of them can be applied to numerically invert an arbitrary tridiagonal matrix . moreover , </S>",
    "<S> some of the methods suffer a huge instability problem . in this paper , we investigate these problems very thoroughly . </S>",
    "<S> we locate and explain the different reasons the recursive algorithms for inverting such matrices fail to deliver satisfactory ( or any ) result , and then propose new formulae for the elements of @xmath4 that allow to construct the asymptotically fastest possible algorithm for computing the inverse of an arbitrary tridiagonal matrix @xmath0 , for which both residual errors , @xmath5 and @xmath6 , are always very small . </S>"
  ]
}