{
  "article_text": [
    "we address one of the fundamental problems in uncertainty quantification ( uq ) : the mapping of the probability distribution of a random variable through a nonlinear function .",
    "let us assume that we are concerned with a specific physical or engineering model which is computationally expensive .",
    "the model is defined by the map @xmath0 .",
    "it takes a parameter @xmath1 as input , and produces an output @xmath2 , @xmath3 . in this paper",
    "we restrict ourselves to a proof - of - principle one - dimensional case .",
    "let us assume that @xmath1 is a random variable distributed with probability density function ( pdf ) @xmath4 .",
    "the uncertainty quantification problem is the estimation of the pdf @xmath5 of the output variable @xmath6 , given @xmath4 .",
    "formally , the problem can be simply cast as a coordinate transformation and one easily obtains @xmath7 where @xmath8 is the jacobian of @xmath9 .",
    "the sum over all @xmath10 such that @xmath11 takes in account the possibility that @xmath12 may not be injective .",
    "if the function @xmath12 is known exactly and invertible , eq.([py ] ) can be used straightforwardly to construct the pdf @xmath13 , but this is of course not the case when the mapping @xmath12 is computed via numerical simulations .",
    "several techniques have been studied in the last couple of decades to tackle this problem .",
    "generally , the techniques can be divided in two categories : intrusive and non - intrusive @xcite .",
    "intrusive methods modify the original , _ deterministic _ , set of equations to account for the stochastic nature of the input ( random ) variables , hence eventually dealing with stochastic differential equations , and employing specific numerical techniques to solve them .",
    "classical examples of intrusive methods are represented by polynomial chaos expansion @xcite , and stochastic galerkin methods @xcite .",
    "on the other hand , the philosophy behind non - intrusive methods is to make use of the deterministic version of the model ( and the computer code that solves it ) as a black - box , which returns one deterministic output for any given input .",
    "an arbitrary large number of solutions , obtained by sampling the input parameter space , can then be collected and analyzed in order to reconstruct the pdf @xmath13 .",
    "the paradigm of non - intrusive methods is perhaps best represented by monte carlo ( mc ) methods @xcite : one can construct an ensemble of input parameters @xmath14 ( @xmath15 typically large ) distributed according to the pdf @xmath16 , run the corresponding ensemble of simulations @xmath17 , and process the outputs @xmath18 .",
    "mc methods are probably the most robust of all the non - intrusive methods .",
    "their main shortcoming is the slow convergence of the method , with a typical convergence rate proportional to @xmath19 . for many applications quasi - monte carlo ( qmc ) methods @xcite",
    "are now preferred to mc methods , for their faster convergence rate .",
    "in qmc the pseudo - random generator of samples is replaced by more uniform distributions , obtained through so - called quasi - random generators @xcite .",
    "it is often said that mc and qmc do not suffer the ` curse of dimensionality'@xcite , in the sense that the convergence rate ( but not the actual error ! ) is not affected by the dimension @xmath20 of the input parameter space .",
    "therefore , they represent the standard choice for large dimensional problems . on the other hand , when the dimension @xmath20 is not very large , collocation methods @xcite are usually more efficient .    collocation methods recast an uq problem as an interpolation problem . in collocation methods ,",
    "the function @xmath21 is sampled in a small ( compared to the mc approach ) number of points ( ` collocation points ' ) , and an interpolant is constructed to obtain an approximation of @xmath12 over the whole input parameter space , from which the pdf @xmath13 can be estimated .",
    "the question then arises on how to effectively choose the collocation points .",
    "recalling that every evaluation of the function @xmath12 amounts to performing an expensive simulation , the challenge resides in obtaining an accurate approximation of @xmath22 with the least number of collocation points .",
    "+ as the name suggests , collocation methods are usually derived from classical quadrature rules @xcite .",
    "the type of pdf @xmath23 can guide the choice of the optimal quadrature rule to be used ( i.e. , gauss - hermite for a gaussian probability , gauss - legendre for a uniform probability , etc .",
    "@xcite ) . furthermore ,",
    "because quadratures are associated with polynomial interpolation , it becomes natural to define a global interpolant in terms of a lagrange polynomial @xcite .",
    "also , choosing the collocation points as the abscissas of a given quadrature rule makes sense particularly if one is only interested in the evaluation of the statistical moments of the pdf ( i.e. , mean , variance , etc . )",
    "@xcite .",
    "on the other hand , there are several applications where one is interested in the approximation of the full pdf @xmath22 .",
    "for instance , when @xmath12 is narrowly peaked around two or more distinct values , its mean does not have any statistical meaning . in such cases",
    "one can wonder whether a standard collocation method based on quadrature rules still represents the optimal choice , in the sense of the computational cost to obtain a given accuracy .    from this perspective",
    ", a downside of collocation methods is that the collocation points are chosen a priori , without making use of the knowledge of @xmath21 acquired at previous interpolation levels .",
    "for instance , the clenshaw - curtis ( cc ) method uses a set of points that contains nested subset , in order to re - use all the previous computations , when the number of collocation points is increased .",
    "however , since the abscissas are unevenly spaced and concentrated towards the edge of the domain ( this is typical of all quadrature rules , in order to overcome the runge phenomenon @xcite ) , it is likely that the majority of the performed simulations will not contribute significantly in achieving a better approximation of @xmath22 .",
    "stated differently , one would like to employ a method where each new sampling point is chosen in such a way to result in the fastest convergence rate for the approximated @xmath22 , in contrast to a set of points defined a priori .",
    "as a matter of fact , because the function @xmath12 is unknown , a certain number of simulations will always be redundant , in the sense that they will contribute very little to the convergence of @xmath5 .",
    "the rationale for this work is to devise a method to minimize such a redundancy in the choice of sampling points while achieving fastest possible convergence of @xmath5 .",
    "clearly , this suggests to devise a strategy that chooses collocation points _ adaptively _ , making use of the knowledge of the interpolant of @xmath21 , which becomes more and more accurate as more points are added .",
    "a well known adaptive sampling algorithm is based on the calculation of the so - called hierarchical surplus ( * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* see e.g ) .",
    "this is defined as the difference , between two levels of refinement , in the solution obtained by the interpolant .",
    "although this algorithm is quite robust , and it is especially efficient in detecting discontinuities , it has the obvious drawback that it can be prematurely terminated , whenever the interpolant happens to exactly pass through the true solution on a point where the hierarchical surplus is calculated , no matter how inaccurate the interpolant is in close - by regions ( see figure [ fig : hierarchical_example ] for an example ) .",
    "the goal of this paper is to describe an alternative strategy for the adaptive selection of sampling points .",
    "the objective in devising such strategy is to have a simple and robust set of rules for choosing the next sampling point .",
    "the paper is concerned with a proof - of - principle demonstration of our new strategy , and we will focus here on one dimensional cases and on the case of uniform @xmath4 only , postponing the generalization to multiple dimensions to future work .",
    "it is important to appreciate that the stated goal of this work is different from the traditional approach followed in the overwhelming majority of works that have presented sampling methods for uq in the literature .",
    "indeed , it is standard to focus on the convergence of the nonlinear unknown function @xmath21 , trying to minimize the interpolation error on @xmath21 , for a given number of sampling points . on the other hand",
    ", we will show that the convergence rates of @xmath21 and of its cumulative distribution function can be quite different .",
    "our new strategy is designed to achieve the fastest convergence on the latter quantity , which is ultimately the observable quantity of an experiment .",
    "the paper is organized as follows . in section 2",
    "we define the mathematical methods used for the construction of the interpolant and show our adaptive strategy to choose a new collocation points . in section 3",
    "we present some numerical examples and comparisons with the clenshaw - curtis collocation method , and the adaptive method based on hierarchical surplus .",
    "finally , we draw our conclusions in section 4 .",
    "in section 3 , we compare our method with the cc method , which is the standard appropriate collocation method for a uniform @xmath23 . here , we recall the basic properties of cc , for completeness .",
    "the clenshaw - curtis ( cc ) quadrature rule uses the extrema of a chebyshev polynomial ( the so - called ` extrema plus end - points ' collocation points in @xcite ) as abscissas .",
    "they are particularly appealing to be used as collocation points in uq , because a certain subset of them are nested .",
    "specifically , they are defined , in the interval @xmath24 $ ] as : @xmath25 one can notice that the the set of @xmath26 points is fully contained in the set of @xmath27 points ( with @xmath28 an arbitrary integer , referred to as the level of the set ) .",
    "in practice this means that one can construct a nested sequence of collocation points with @xmath29 , re - using all the previous evaluations of @xmath12 .",
    "collocation points based on quadratures are optimal to calculate moments : on the left - hand side is a label , such that @xmath30 is the mean , @xmath31 is the variance , and so on . on the right - hand side it is an exponent . ] @xmath32 where we used the identity relation , @xmath33 it is known that integration by quadrature is very accurate ( for smooth enough integrand ) , and the moments can be readily evaluated , without the need to construct an interpolant : @xmath34 where the weights @xmath35 can be computed with standard techniques ( see , e.g. @xcite ) .",
    "the interpolant for the cc method is the lagrange polynomial .",
    "the hierarchical surplus algorithm is widely used for interpolation on sparse grids .",
    "it is generally defined as the difference between the value of an interpolant at the current and previous interpolation levels @xcite .",
    "the simplest algorithm prescribes a certain tolerance and looks for all the point at the new level where the hierarchical surplus is larger than the tolerance .",
    "the new sampling points ( at the next level ) will be the neighbors of the points where this condition is met . in one - dimension ,",
    "the algorithm is extremely simple because the neighbors are only two points , that one can define in such a way that cells are always halved . in this work",
    ", we compare our new method with a slightly improved version of the hierarchical surplus algorithm .",
    "the reason is because we do not want our comparisons to be dependent on the choice of an arbitrary tolerance level , and we want to be able to add new points two at the time .",
    "hence , we define a new interpolation level by adding only the two neighbors of the point with the largest hierarchical surplus .",
    "all the previous hierarchical surpluses that have been calculated , but for which new points have not been added yet are kept .",
    "the pseudo - code of the algorithm follows .",
    "the interpolant is understood to be piece - wise linear interpolation , and the grid is @xmath36 $ ] .    calculate the interpolant on the grid @xmath37 .",
    "define @xmath38 and add them on the grid calculate the interpolant on the new grid calculate the hierarchical surplus on the last two entries of @xmath39 and store them in the vector @xmath40 find the largest hierarchical surplus in @xmath40 , remove it from @xmath40 and remove the corresponding @xmath10 from @xmath41 append the two neighbors to @xmath41 and add them to the grid      we use a multiquadric biharmonic radial basis function ( rbf ) with respect to a set of points @xmath42 , with @xmath43 , defined as : @xmath44 where @xmath45 are free parameters ( referred to as shape parameters ) . the function @xmath21 is approximated by the interpolant @xmath46 defined as @xmath47 the weights @xmath48 are obtained by imposing that @xmath49 for each sampling point in the set , namely the interpolation error is null at the sampling points .",
    "this results in solving a linear system for @xmath50 of the form @xmath51 , with @xmath52 a real symmetric @xmath53 matrix .",
    "we note that , by construction , the linear system will become more and more ill - conditioned with increasing @xmath15 , for fixed values of @xmath54 .",
    "this can be easily understood because when two points become closer and closer the corresponding two rows in the matrix @xmath52 become less and less linearly independent . to overcome this problem one needs to decrease the corresponding values of @xmath54 . in turns",
    ", this means that the interpolant @xmath46 will tend to a piece - wise linear interpolant for increasingly large @xmath15 .",
    "we focus , as the main diagnostic of our method , on the cumulative distribution function ( cdf ) @xmath55 , which is defined as @xmath56 where @xmath57 . as it is well known , the interpretation of the cumulative distribution function is that , for a given value @xmath58 , @xmath59 is the probability that @xmath60 .",
    "of course , the cdf @xmath55 contains all the statistical information needed to calculate any moment of the distribution , and can return the probability density function @xmath13 , upon differentiation .",
    "moreover , the cdf is always well defined between 0 and 1 .",
    "the following two straightforward considerations will guide the design of our _ adaptive selection strategy_. a first crucial point , already evident from eq .",
    "( [ py ] ) , is whether or not @xmath21 is bijective .",
    "when @xmath21 is bijective this translates to the cdf @xmath55 being continuous , while a non - bijective function @xmath21 produces a cdf @xmath55 which is discontinuous .",
    "it follows that intervals in @xmath10 where @xmath21 is constant ( or nearly constant ) will map into a single value @xmath61 ( or a very small interval in @xmath62 ) where the cdf will be discontinuous ( or ` nearly ' discontinuous ) .",
    "secondly , an interval in @xmath10 with a large first derivative of @xmath21 will produce a nearly flat cdf @xmath55 .",
    "this is again clear by noticing that the jacobian @xmath63 in eq .",
    "( [ py ] ) ( @xmath64 in one dimension ) is in the denominator , and therefore the corresponding @xmath13 will be very small , resulting in a flat cdf @xmath55 .",
    "+ loosely speaking one can then state that regions where @xmath21 is flat will produce large jumps in the cdf @xmath55 and , conversely , regions where the @xmath21 has large jumps will map in to a nearly flat cdf @xmath55 . from this simple considerations one can appreciate how important it is to have an interpolant that accurately capture both regions with _ very large _ and _ very small _ first derivative of @xmath21 . moreover , since the cdf @xmath55 is an integrated quantity , interpolation errors committed around a given @xmath62 will propagate in the cdf for all larger @xmath62 values .",
    "for this reason , it is important to achieve a global convergence with interpolation errors that are of the same order of magnitude along the whole domain .",
    "+ the adaptive section algorithm works as follows .",
    "we work in the interval @xmath36 $ ] ( every other interval where the support of @xmath21 is defined can be rescaled to this interval ) . we denote with @xmath42 the sampling set which we assume is always sorted , such that @xmath65 .",
    "we start with 3 points : @xmath66 , @xmath67 , @xmath68 . for the robustness and the simplicity of the implementation we choose to select a new sampling point always at equal distance between two existing points .",
    "one can decide to limit the ratio between the largest and smallest distance between adjacent points : @xmath69 ( with @xmath70 ) , where @xmath71 is the distance between the points @xmath72 and @xmath73 .",
    "this avoids to keep refining small intervals when large intervals might still be under - resolved , thus aiming for the above mentioned global convergence over the whole support . at each iteration",
    "we create a list of possible new points , by halving every interval , excluding the points that would increase the value of @xmath74 above the maximum desired ( note that @xmath74 will always be a power of 2 ) .",
    "we calculate the first derivative of @xmath46 at these points , and alternatively choose the point with largest / smallest derivative as the next sampling point .",
    "notice that , by the definition of the interpolant , eq .",
    "( [ interpolant ] ) , its first derivative can be calculated exactly as : @xmath75 without having to recompute the weights @xmath48 . at each iteration",
    "the shape parameters @xmath45 are defined at each points , as @xmath76 , i.e. they are linearly rescaled with the smallest distance between the point @xmath73 and its neighbors .",
    "the pseudo - code of the algorithm follows .",
    "@xmath77 exclude points in @xmath78 such that @xmath79 calculate @xmath80 through ( [ first_derivative ] ) at @xmath81 alternatively choose @xmath78 with largest / smallest values of @xmath82 as new collocation point calculate new weights @xmath48",
    "in this section we present and discuss four numerical examples where we apply our adaptive selection strategy . in this work we focus on a single input parameter and the case of constant probability @xmath83 in the interval",
    "@xmath84 $ ] , and we compare our results against the clenshaw - curtis , and the hierarchical surplus methods . we denote with @xmath85 the interpolant obtained with a set of @xmath86 points ( hence the iterative procedure starts with @xmath87 ) . a possible way to construct the cdf @xmath55 from a given interpolant @xmath85 would be to generate a sample of points in the domain @xmath24 $ ] , randomly distributed according to the pdf @xmath16 , collecting the corresponding values calculated through eq .",
    "( [ interpolant ] ) , and constructing their cdf . because here we work with a constant @xmath16 , it is more efficient to simply define a uniform grid in the domain @xmath88 $ ] where to compute @xmath85 . in the following",
    "we will use , in the evaluation of the cdf @xmath55 , a grid in @xmath89 with @xmath90 points equally spaced in the interval @xmath91 $ ] , and a grid in @xmath92 with @xmath93 points equally spaced in the interval @xmath24 $ ] .",
    "we define the following errors : @xmath94 where @xmath95 denotes the l@xmath96 norm .",
    "it is important to realize that the accuracy of the numerically evaluated cdf @xmath55 will always depend on the binning of @xmath97 , i.e. the points at which the cdf is evaluated . as we will see in the following examples , the error @xmath98 saturates for large @xmath99 , which thus is an artifact of the finite bin size .",
    "we emphasize that , differently from most of the previous literature , our strategy focuses on converging rapidly in @xmath100 , rather than in @xmath101 .",
    "of course , a more accurate interpolant will always result in a more accurate cdf , however the relationship between a reduction in @xmath102 and a corresponding reduction in @xmath98 is not at all trivial .",
    "this is because the relation between @xmath16 and @xmath13 is mediated by the jacobian of @xmath21 , and it also involves the bijectivity of @xmath12 .",
    "+ finally , we study the convergence of the mean @xmath103 , see equation [ eq : mean ] , and the variance @xmath104 , which is defined as @xmath105 these will be calculated by quadrature for the cc methods , and with an integration via trapezoidal method for the adaptive methods .",
    "+ we study two analytical test cases :    * case 1 : @xmath106 ; * case 2 : @xmath107 ;    and two test cases where an analytical solution is not available , and the reference @xmath21 will be calculated as an accurate numerical solution of a set of ordinary differential equations :    * case 3 : lotka - volterra model ( predator - prey ) ; * case 4 : van der pol oscillator .    while case 1 and 2 are more favorable to the cc method , because the functions are smooth and analytical , hence a polynomial interpolation is expected to produce accurate results , the latter two cases mimic applications of real interest , where the model does not produce analytical results , although @xmath21 might still be smooth ( at least piece - wise , in case 4 ) .      in this case",
    "@xmath21 is a bijective function , with one point ( @xmath108 ) where the first derivative vanishes .",
    "figure [ fig : case_1_f ] shows the function @xmath21 ( top panel ) and the corresponding cdf @xmath55 ( bottom panel ) , which in this case can be derived analytically .",
    "hence , we use the analytical expression of cdf @xmath55 to evaluate the error @xmath98 . the convergence of @xmath98 and @xmath102 is shown in figure [ fig : case_1_err ] ( top and bottom panels , respectively ) . here and in all the following figures blue squares denote the new adaptive selection method , red dots are for the cc methods , and black line is for the hierarchical surplus method .",
    "we have run the cc method only for @xmath109 ( i.e. the points at which the collocation points are nested ) , but for a better graphical visualization the red dots are connected with straight lines .",
    "one can notice that the error for the new adaptive method is consistently smaller than for the cc method . from the top panel , one can appreciate the saving in computer power that can be achieved with our new method .",
    "although the difference with cc is not very large until @xmath110 , at @xmath111 there is an order of magnitude difference between the two .",
    "it effectively means that in order to achieve the same error @xmath112 , the cc method would run at least twice the number of simulations .",
    "the importance of focusing on the convergence of the cdf , rather than on the interpolant , is clear in comparing our method with the hierarchical surplus method .",
    "for instance , for @xmath113 , the two methods have a comparable error @xmath101 , but our method has achieved almost an order of magnitude more accurate solution in @xmath55 . effectively , this means that our method has sampled the new points less redundantly . in this case",
    "@xmath21 is an anti - symmetric function with zero mean .",
    "hence , any method that chooses sampling points symmetrically distributed around zero would produce the correct first moment @xmath103 .",
    "we show in figure [ fig : case_1_sigma ] the convergence of @xmath114 , as the absolute value of the different with the exact value @xmath115 , in logarithmic scale .",
    "blue , red , and black lines represent the new adaptive method , the cc , and the hierarchical surplus methods , respectively ( where again for the cc , simulations are only performed where the red dots are shown ) .",
    "the exact value is @xmath116 .",
    "as we mentioned , the cc method is optimal to calculate moments , since it uses quadrature .",
    "although in our method the error does not decrease monotonically , it is comparable with the result for cc .      in this case",
    "the function @xmath21 is periodic , and it presents , in the domain @xmath36 $ ] three local minima ( @xmath117 ) and three local maxima ( @xmath118 ) .",
    "the function and the cdf @xmath55 are shown in figure [ fig : case_2_f ] ( top and bottom panel , respectively ) .",
    "figure [ fig : case_2_err ] shows the error for this case ( from now on the same format of figure [ fig : case_1_err ] will be used ) .",
    "the first consideration is that the hierarchical surplus method is the less accurate of the three .",
    "second , @xmath102 is essentially the same for the cc and the new method , up to @xmath119 . for @xmath120",
    "the cc methods achieve a much accurate solution as compared to the new adaptive method , whose error has a much slower convergence .",
    "however , looking at the error in the cdf in top panel of figure [ fig : case_2_err ] , the two methods are essentially equivalent .",
    "this example demonstrates that , in an uq framework , the primary goal in constructing a good interpolant should not be to minimize the error of the interpolant with respect to the true @xmath21 , but rather to achieve the fastest possible convergence on the cdf @xmath121 . although , the two effects are intuitively correlated , they are not into a linear relationship .",
    "in other words , not all sample points in @xmath10 count equally in minimizing @xmath98 .",
    "the convergence of @xmath122 ( exact value @xmath123 ) and @xmath114 ( exact value @xmath124 ) is shown in figures [ fig : case_2_mu ] and [ fig : case_2_sigma ] , respectively .",
    "it is interesting to notice that our method presents errors that are always smaller than the cc method , although the errors degrade considerably in the regions between two cc points , where the two adaptive methods yield comparable results .",
    "the lotka - volterra model @xcite is a well - studied model that exemplifies the interaction between two populations ( predators and preys ) .",
    "this case is more realistic than cases 1 and 2 , as the solution of the model can not be written in analytical form .",
    "as such , both the @xmath21 and the cdf @xmath55 used to compute the errors are calculated numerically .",
    "we use the following simple model : @xmath125 where @xmath126 and @xmath127 denote the population size for each species ( say , horses and lions ) as function of time .",
    "the ode is easily solved in matlab , with the ` ode45 ` routine , with an absolute tolerance set equal to @xmath128 .",
    "we use , as initial conditions , @xmath129 , and we solve the equations for @xmath130 $ ] . clearly , the solution of the model depends on the input parameter @xmath10 .",
    "we define our test function @xmath21 to be the result of the model for the @xmath131 population at time @xmath132 : @xmath133 the resulting function @xmath21 , and the computed cdf @xmath55 are shown in figure [ fig : case_3_f ] ( top and bottom panel , respectively ) .",
    "we note that , although @xmath21 can not be expressed as an analytical function , it is still smooth , and hence it does not present particular difficulties in being approximated through a polynomial interpolant .",
    "indeed the error @xmath102 undergoes a fast convergence both for the adaptive methods and for the cc method ( figure [ fig : case_3_err ] ) .",
    "once again , the new adaptive method is much more powerful than the cc method in achieving a better convergence rate , and thus saving computational power , while the hierarchical surplus method is the worst of the three .",
    "convergence of @xmath122 and @xmath114 are shown in figures [ fig : case_3_mu ] and [ fig : case_3_sigma ] , respectively .",
    "similar to previous cases , the cc presents a monotonic convergence , while this is not the case for the adaptive methods . only for @xmath120 ,",
    "the cc method yields much better results than the new method .",
    "our last example is the celebrated van der pol oscillator@xcite , which has been extensively studied as a textbook case of a nonlinear dynamical system . in this respect",
    "this test case is very relevant to uncertainty quantification , since real systems often exhibit a high degree of nonlinearity .",
    "similar to case 3 , we define our test function @xmath21 as the output of a set of two odes , which we solve numerically with matlab .",
    "the model for the van der pol oscillator is : @xmath134 the initial conditions are @xmath135 , @xmath136 .",
    "the model is solved for time @xmath137 $ ] , and the function @xmath21 is defined as @xmath138 the so - called nonlinear damping parameter is rescaled such that for @xmath36 $ ] , it ranges between 50 and 250 .",
    "the function @xmath21 and the corresponding cdf @xmath55 are shown in figure [ fig : case_4_f ] .",
    "this function is clearly much more challenging than the previous ones .",
    "it is divided in two branches , where it takes values @xmath139 and @xmath140 , and it presents discontinuities where it jumps from one branch to the other .",
    "correspondingly , cdf @xmath55 presents a flat plateau for @xmath141 , which is the major challenge for both methods . in figure",
    "[ fig : case_4_err ] we show the errors @xmath102 and @xmath98 .",
    "the overall convergence rate of the cc and the new method is similar . for this case ,",
    "the hierarchical surplus method yields a better convergence , but only for @xmath142 . as we commented before",
    ", the mean @xmath122 has no statistical meaning in this case , because the output is divided into two separate regions . the convergence for @xmath114 is presented in figure [ fig : case_4_sigma ] .",
    "we have presented a new adaptive algorithm for the selection of sampling points for non - intrusive stochastic collocation in uncertainty quantification ( uq ) .",
    "the main idea is to use a radial basis function as interpolant , and to refine the grid on points where the interpolant presents large and small first derivative .",
    "+ in this work we have focused on 1d and uniform probability @xmath16 , and we have shown four test cases , encompassing analytical and non - analytical smooth functions , which are prototype of a very wide class of functions . in all cases the new adaptive method improved the efficiency of both the ( non - adaptive ) clenshaw - curtis collocation method , and of the adaptive algorithm based on the calculation of the hierarchical surplus ( note that the method used in this paper is a slight improvement of the classical algorithm ) .",
    "the strength of our method is the ability to select a new sampling point making full use of the interpolant resulting from all the previous evaluation of the function @xmath21 , thus seeking the most optimal convergence rate for the cdf @xmath55 .",
    "we have shown that there is no one - to - one correspondence between a reduction in the interpolation error @xmath102 and a reduction in the cdf error @xmath98 .",
    "for this reason , collocation methods that choose the distribution of sampling points a priori can perform poorly in attaining a fast convergence rate in @xmath98 , which is the main goal of uq .",
    "moreover , in order to maintain the nestedness of the collocation points the cc method requires larger and larger number of simulations ( @xmath143 moving from level @xmath28 to level @xmath144 ) , which is in contrast with our new method where one can add one point at the time . + we envision many possible research directions to further investigate our method .",
    "the most obvious is to study multi - dimensional problems .",
    "we emphasize that the radial basis function is a mesh - free method and as such we anticipate that this will largely alleviate the curse of dimensionality that afflicts other collocation methods based on quadrature points ( however , see @xcite for methods related to the construction of sparse grids , which have the same aim ) .",
    "moreover , it will be interesting to explore the versatility of rbf in what concerns the possibility of choosing an optimal shape parameter @xmath54 @xcite .",
    "recent work @xcite investigated the role of the shape parameter @xmath54 in interpolating discontinuous functions , which might be very relevant in the context of uq , when the continuity of @xmath21 can not be assumed a priori .",
    "finally , a very appealing research direction , would be to simultaneously exploit quasi - monte carlo and adaptive selection methods for extremely large dimension problems .",
    "a. a. and c. r. are supported by fom project no .",
    "12cser058 and 12pr304 , respectively .",
    "we would like to remember dr.ir .",
    "j.a.s . witteveen ( @xmath145 2015 ) for the useful discussions we had about uncertainty quantification .",
    "43 natexlab#1#1[1]`#1 ` [ 2]#2 [ 1]#1 [ 1]http://dx.doi.org/#1 [ ] [ 1]pmid:#1 [ ] [ 2]#2 , , ( ) .",
    ", , , ( ) . , , , , , , in : , , pp . . , , , ( ) .",
    ", , , , ( ) . , , , , in : , gcms 11 , , , , pp . .",
    ", , , ( ) . , , , .",
    ", , , , . , , ( ) .",
    ", , , , . , , , , .",
    ", , , ( ) . , , ( ) .",
    ", , , in : , , , pp . . , , , ( ) .",
    ", , , ( ) .",
    ", , , ( ) . , , , , ( ) .",
    ", , , ( ) . , , ( ) .",
    ", , ( ) . , , , ( ) .",
    ", , , ( ) . , , , in : , , , pp . .",
    ", , , ( ) .",
    ", , ( ) . , , , ( ) .",
    ", , in : , , , pp . . , ,",
    "( ) . , , , ( ) . , , , , , , ( ) . , , , , ( )",
    ", , , ( ) . , , , ( ) .",
    ", , , ( ) . , , , ( ) .",
    "( in black ) goes exactly through the red straight line at the points @xmath146 .",
    "calculating the piece - wise linear interpolant between two ( @xmath147 ) , three ( @xmath148 ) , and five ( @xmath149 ) points would result in a null hierarchical surplus on these points.,width=10 ]       ( top ) @xmath102 ( bottom ) as function of number of sampling points @xmath15 .",
    "blue squares : new adaptive selection method .",
    "red dots : clenshaw - curtis .",
    "black curve : adaptive method based on hierarchical surplus.,width=10 ]         ( top ) @xmath102 ( bottom ) as function of number of sampling points @xmath15 .",
    "blue squares : new adaptive selection method .",
    "red dots : clenshaw - curtis .",
    "black curve : adaptive method based on hierarchical surplus.,width=10 ]           ( top ) @xmath102 ( bottom ) as function of number of sampling points @xmath15 .",
    "blue squares : new adaptive selection method .",
    "red dots : clenshaw - curtis .",
    "black curve : adaptive method based on hierarchical surplus.,width=10 ]           ( top ) @xmath102 ( bottom ) as function of number of sampling points @xmath15 .",
    "blue squares : new adaptive selection method .",
    "red dots : clenshaw - curtis .",
    "black curve : adaptive method based on hierarchical surplus.,width=10 ]"
  ],
  "abstract_text": [
    "<S> we present a simple and robust strategy for the selection of sampling points in uncertainty quantification . </S>",
    "<S> the goal is to achieve the fastest possible convergence in the cumulative distribution function of a stochastic output of interest . </S>",
    "<S> we assume that the output of interest is the outcome of a computationally expensive nonlinear mapping of an input random variable , whose probability density function is known . </S>",
    "<S> we use a radial function basis to construct an accurate interpolant of the mapping . </S>",
    "<S> this strategy enables adding new sampling points one at a time , _ </S>",
    "<S> adaptively_. this takes into full account the previous evaluations of the target nonlinear function . </S>",
    "<S> we present comparisons with a stochastic collocation method based on the clenshaw - curtis quadrature rule , and with an adaptive method based on hierarchical surplus , showing that the new method often results in a large computational saving . </S>"
  ]
}