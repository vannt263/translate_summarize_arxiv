{
  "article_text": [
    "objects can be given literally , like the literal four - letter genome of a mouse , or the literal text of _ war and peace _ by tolstoy .",
    "for simplicity we take it that all meaning of the object is represented by the literal object itself .",
    "objects can also be given by name , like `` the four - letter genome of a mouse , '' or `` the text of _ war and peace _ by tolstoy . ''",
    "there are also objects that can not be given literally , but only by name , and that acquire their meaning from their contexts in background common knowledge in humankind , like `` home '' or `` red . ''",
    "to make computers more intelligent one would like to represent meaning in computer - digestable form . long - term and labor - intensive efforts like the _ cyc _ project @xcite and the _ wordnet _ project @xcite try to establish semantic relations between common objects , or , more precisely , _",
    "names _ for those objects .",
    "the idea is to create a semantic web of such vast proportions that rudimentary intelligence , and knowledge about the real world , spontaneously emerge .",
    "this comes at the great cost of designing structures capable of manipulating knowledge , and entering high quality contents in these structures by knowledgeable human experts . while the efforts are long - running and large scale ,",
    "the overall information entered is minute compared to what is available on the world - wide - web .",
    "the rise of the world - wide - web has enticed millions of users to type in trillions of characters to create billions of web pages of on average low quality contents .",
    "the sheer mass of the information about almost every conceivable topic makes it likely that extremes will cancel and the majority or average is meaningful in a low - quality approximate sense .",
    "we devise a general method to tap the amorphous low - grade knowledge available for free on the world - wide - web , typed in by local users aiming at personal gratification of diverse objectives , and yet globally achieving what is effectively the largest semantic electronic database in the world . moreover , this database is available for all by using any search engine that can return aggregate page - count estimates for a large range of search - queries , like google .",
    "previously , we and others developed a compression - based method to establish a universal similarity metric among objects given as finite binary strings @xcite , which was widely reported @xcite .",
    "such objects can be genomes , music pieces in midi format , computer programs in ruby or c , pictures in simple bitmap formats , or time sequences such as heart rhythm data .",
    "this method is feature - free in the sense that it does nt analyze the files looking for particular features ; rather it analyzes all features simultaneously and determines the similarity between every pair of objects according to the most dominant shared feature .",
    "the crucial point is that the method analyzes the objects themselves .",
    "this precludes comparison of abstract notions or other objects that do nt lend themselves to direct analysis , like emotions , colors , socrates , plato , mike bonanno and albert einstein .",
    "while the previous method that compares the objects themselves is particularly suited to obtain knowledge about the similarity of objects themselves , irrespective of common beliefs about such similarities , here we develop a method that uses only the name of an object and obtains knowledge about the similarity of objects , a quantified relative google semantics , by tapping available information generated by multitudes of web users .",
    "here we are reminded of the words of d.h .",
    "rumsfeld @xcite `` a trained ape can know an awful lot / of what is going on in this world / just by punching on his mouse / for a relatively modest cost ! '' in this paper , the google semantics of a word or phrase",
    "consists of the set of web pages returned by the query concerned .",
    "while the theory we propose is rather intricate , the resulting method is simple enough .",
    "we give an example : at the time of doing the experiment , a google search for `` horse '' , returned 46,700,000 hits .",
    "the number of hits for the search term `` rider '' was 12,200,000 .",
    "searching for the pages where both `` horse '' and `` rider '' occur gave 2,630,000 hits , and google indexed 8,058,044,651 web pages . using these numbers in the main formula we derive below , with @xmath0 , this yields a normalized google distance between the terms `` horse '' and `` rider '' as follows : @xmath1 in the sequel of the paper we argue that the ngd is a normed semantic distance between the terms in question , usually ( but not always , see below ) in between 0 ( identical ) and 1 ( unrelated ) , in the cognitive space invoked by the usage of the terms on the world - wide - web as filtered by google .",
    "because of the vastness and diversity of the web this may be taken as related to the current use of the terms in society .",
    "we did the same calculation when google indexed only one - half of the number of pages : 4,285,199,774 .",
    "it is instructive that the probabilities of the used search terms did nt change significantly over this doubling of pages , with number of hits for `` horse '' equal 23,700,000 , for `` rider '' equal 6,270,000 , and for `` horse , rider '' equal to 1,180,000 .",
    "the @xmath2 we computed in that situation was @xmath3 .",
    "this is in line with our contention that the relative frequencies of web pages containing search terms gives objective information about the semantic relations between the search terms .",
    "if this is the case , then the google probabilities of search terms and the computed ngd s should stabilize ( become scale invariant ) with a growing google database .",
    "there is a great deal of work in both cognitive psychology @xcite , linguistics , and computer science , about using word ( phrases ) frequencies in text corpora to develop measures for word similarity or word association , partially surveyed in @xcite , going back to at least @xcite .",
    "one of the most successful is latent semantic analysis ( lsa ) @xcite that has been applied in various forms in a great number of applications .",
    "we discuss lsa and its relation to the present approach in appendix  [ app.lsa ] . as with lsa",
    ", many other previous approaches of extracting corollations from text documents are based on text corpora that are many order of magnitudes smaller , and that are in local storage , and on assumptions that are more refined , than what we propose . in contrast ,",
    "@xcite and the many references cited there , use the web and google counts to identify lexico - syntactic patterns or other data .",
    "again , the theory , aim , feature analysis , and execution are different from ours , and can not meaningfully be compared .",
    "essentially , our method below automatically extracts semantic relations between arbitrary objects from the web in a manner that is feature - free , up to the search - engine used , and computationally feasible .",
    "this seems to be a new direction altogether .",
    "the main thrust is to develop a new theory of semantic distance between a pair of objects , based on ( and unavoidably biased by ) a background contents consisting of a database of documents .",
    "an example of the latter is the set of pages constituting the world - wide - web .",
    "similarity relations between pairs of objects is distilled from the documents by just using the number of documents in which the objects occur , singly and jointly ( irrespective of location or multiplicity ) . for us ,",
    "the google semantics of a word or phrase consists of the set of web pages returned by the query concerned .",
    "note that this can mean that terms with different meaning have the same semantics , and that opposites like `` true '' and `` false '' often have a similar semantics .",
    "thus , we just discover associations between terms , suggesting a likely relationship .",
    "as the web grows , the google semantics may become less primitive .",
    "the theoretical underpinning is based on the theory of kolmogorov complexity @xcite , and is in terms of coding and compression .",
    "this allows to express and prove properties of absolute relations between objects that can not even be expressed by other approaches .",
    "the theory , application , and the particular ngd formula to express the bilateral semantic relations are ( as far as we know ) not equivalent to any earlier theory , application , and formula in this area .",
    "the current paper is a next step in a decade of cumulative research in this area , of which the main thread is @xcite with @xcite using the related approach of @xcite .",
    "we first start with a technical introduction outlining some notions underpinning our approach : kolmogorov complexity , information distance , and compression - based similarity metric ( section  [ sect.tech ] ) .",
    "then we give a technical description of the google distribution , the normalized google distance , and the universality of these notions ( section  [ sect.google ] ) .",
    "while it may be possible in principle that other methods can use the entire world - wide - web to determine semantic similarity between terms , we do not know of a method that both uses the entire web , or computationally can use the entire web , and ( or ) has the same aims as our method .",
    "to validate our method we therefore can not compare its performance to other existing methods .",
    "ours is a new proposal for a new task .",
    "we validate the method in the following way : by theoretical analysis , by anecdotical evidence in a plethora of applications , and by systematic and massive comparison of accuracy in a classification application compared to the uncontroversial body of knowledge in the wordnet database . in section  [ sect.google ] we give the theoretic underpinning of the method and prove its universality . in section  [ sect.exp ]",
    "we present a plethora of clustering and classification experiments to validate the universality , robustness , and accuracy of our proposal . in section  [ sect.validation ]",
    "we test repetitive automatic performance against uncontroversial semantic knowledge : we present the results of a massive randomized classification trial we conducted to gauge the accuracy of our method to the expert knowledge as implemented over the decades in the wordnet database .",
    "the preliminary publication @xcite of this work on the web archives was widely reported and discussed , for example @xcite .",
    "the actual experimental data can be downloaded from @xcite .",
    "the method is implemented as an easy - to - use software tool available on the web @xcite , available to all .",
    "the application of the theory we develop is a method that is justified by the vastness of the world - wide - web , the assumption that the mass of information is so diverse that the frequencies of pages returned by google queries averages the semantic information in such a way that one can distill a valid semantic distance between the query subjects .",
    "it appears to be the only method that starts from scratch , is feature - free in that it uses just the web and a search engine to supply contents , and automatically generates relative semantics between words and phrases .",
    "a possible drawback of our method is that it relies on the accuracy of the returned counts .",
    "as noted in @xcite , the returned google counts are inaccurate , and especially if one uses the boolean or operator between search terms , at the time of writing .",
    "the and operator we use is less problematic , and we do not use the or operator . furthermore , google apparently estimates the number of hits based on samples , and the number of indexed pages changes rapidly . to compensate for the latter effect",
    ", we have inserted a normalizing mechanism in the complearn software .",
    "generally though , if search engines have peculiar ways of counting number of hits , in large part this should not matter , as long as some reasonable conditions hold on how counts are reported .",
    "linguists judge the accuracy of google counts trustworthy enough : in @xcite ( see also the many references to related research ) it is shown that web searches for rare two - word phrases correlated well with the frequency found in traditional corpora , as well as with human judgments of whether those phrases were natural . thus , google is the simplest means to get the most information .",
    "note , however , that a single google query takes a fraction of a second , and that google restricts every ip address to a maximum of ( currently ) 500 queries per day ",
    "although they are cooperative enough to extend this quotum for noncommercial purposes .",
    "the experimental evidence provided here shows that the combination of google and our method yields reasonable results , gauged against common sense ( ` colors ' are different from ` numbers ' ) and against the expert knowledge in the wordnet data base .",
    "a reviewer suggested downscaling our method by testing it on smaller text corpora .",
    "this does not seem useful .",
    "clearly perfomance will deteriorate with decreasing data base size .",
    "a thought experiment using the extreme case of a single web page consisting of a single term suffices . practically addressing this issue",
    "is begging the question . instead ,",
    "in section  [ sect.google ] we theoretically analyze the relative semantics of search terms established using all of the web , and its universality with respect to the relative semantics of search terms using subsets of web pages .",
    "the basis of much of the theory explored in this paper is kolmogorov complexity . for an introduction and details",
    "see the textbook @xcite . here",
    "we give some intuition and notation .",
    "we assume a fixed reference universal programming system .",
    "such a system may be a general computer language like lisp or ruby , and it may also be a fixed reference universal turing machine in a given standard enumeration of turing machines .",
    "the latter choice has the advantage of being formally simple and hence easy to theoretically manipulate .",
    "but the choice makes no difference in principle , and the theory is invariant under changes among the universal programming systems , provided we stick to a particular choice .",
    "we only consider universal programming systems such that the associated set of programs is a prefix code  as is the case in all standard computer languages .",
    "the _ kolmogorov complexity _ of a string @xmath4 is the length , in bits , of the shortest computer program of the fixed reference computing system that produces @xmath4 as output .",
    "the choice of computing system changes the value of @xmath5 by at most an additive fixed constant .",
    "since @xmath5 goes to infinity with @xmath4 , this additive fixed constant is an ignorable quantity if we consider large @xmath4 .",
    "one way to think about the kolmogorov complexity @xmath5 is to view it as the length , in bits , of the ultimate compressed version from which @xmath4 can be recovered by a general decompression program . compressing @xmath4 using the compressor _ gzip _ results in a file @xmath6 with ( for files that contain redundancies ) the length @xmath7 .",
    "using a better compressor _ bzip2 _ results in a file @xmath8 with ( for redundant files ) usually @xmath9 ; using a still better compressor like _ ppmz _ results in a file @xmath10 with ( for again appropriately redundant files ) @xmath11 .",
    "the kolmogorov complexity @xmath5 gives a lower bound on the ultimate value : for every existing compressor , or compressors that are possible but not known , we have that @xmath5 is less or equal to the length of the compressed version of @xmath4 .",
    "that is , @xmath5 gives us the ultimate value of the length of a compressed version of @xmath4 ( more precisely , from which version @xmath4 can be reconstructed by a general purpose decompresser ) , and our task in designing better and better compressors is to approach this lower bound as closely as possible .",
    "in @xcite we considered the following notion : given two strings @xmath4 and @xmath12 , what is the length of the shortest binary program in the reference universal computing system such that the program computes output @xmath12 from input @xmath4 , and also output @xmath4 from input @xmath12 .",
    "this is called the _ information distance _ and denoted as @xmath13 .",
    "it turns out that , up to a negligible logarithmic additive term , @xmath14 where @xmath15 is the binary length of the shortest program that produces the pair @xmath16 and a way to tell them apart .",
    "this distance @xmath13 is actually a metric : up to close precision we have @xmath17 , @xmath18 for @xmath19 , @xmath20 and @xmath21 , for all @xmath22 .",
    "we now consider a large class of _ admissible distances _ : all distances ( not necessarily metric ) that are nonnegative , symmetric , and _",
    "computable _ in the sense that for every such distance @xmath23 there is a prefix program that , given two strings @xmath4 and @xmath12 , has binary length equal to the distance @xmath24 between @xmath4 and @xmath12 .",
    "then , @xmath25 where @xmath26 is a constant that depends only on @xmath23 but not on @xmath16 , and we say that @xmath13 _ minorizes _",
    "@xmath24 up to an additive constant .",
    "we call the information distance @xmath27 _ universal _ for the family of computable distances , since the former minorizes every member of the latter family up to an additive constant .",
    "if two strings @xmath4 and @xmath12 are close according to _ some _ computable distance @xmath23 , then they are at least as close according to distance @xmath27 . since every feature in which we can compare two strings can be quantified in terms of a distance , and every distance can be viewed as expressing a quantification of how much of a particular feature the strings do not have in common ( the feature being quantified by that distance ) , the information distance determines the distance between two strings minorizing the _ dominant _ feature in which they are similar .",
    "this means that , if we consider more than two strings , the information distance between every pair may be based on minorizing a different dominating feature .",
    "if small strings differ by an information distance which is large compared to their sizes , then the strings are very different . however",
    ", if two very large strings differ by the same ( now relatively small ) information distance , then they are very similar .",
    "therefore , the information distance itself is not suitable to express true similarity . for that we must define a relative information distance : we need to normalize the information distance .",
    "such an approach was first proposed in @xcite in the context of genomics - based phylogeny , and improved in @xcite to the one we use here .",
    "normalized information distance ( nid ) _ has values between 0 and 1 , and it inherits the universality of the information distance in the sense that it minorizes , up to a vanishing additive term , every other possible normalized computable distance ( suitably defined ) . in the same way as before we can identify the computable normalized distances with computable similarities according to some features , and the nid discovers for every pair of strings the feature in which they are most similar , and expresses that similarity on a scale from 0 to 1 ( 0 being the same and 1 being completely different in the sense of sharing no features ) . considering a set of strings ,",
    "the feature in which two strings are most similar may be a different one for different pairs of strings .",
    "the nid is defined by    @xmath28    it has several wonderful properties that justify its description as the most informative metric @xcite .",
    "the nid is uncomputable since the kolmogorov complexity is uncomputable .",
    "but we can use real data compression programs to approximate the kolmogorov complexities @xmath29 .",
    "a compression algorithm defines a computable function from strings to the lengths of the compressed versions of those strings .",
    "therefore , the number of bits of the compressed version of a string is an upper bound on kolmogorov complexity of that string , up to an additive constant depending on the compressor but not on the string in question .",
    "thus , if @xmath30 is a compressor and we use @xmath31 to denote the length of the compressed version of a string @xmath4 , then we arrive at the _ normalized compression distance _ : @xmath32 where for convenience we have replaced the pair @xmath33 in the formula by the concatenation @xmath34 .",
    "this transition raises several tricky problems , for example how the ncd approximates the nid if @xmath30 approximates @xmath35 , see @xcite , which do not need to concern us here .",
    "thus , the ncd is actually a family of compression functions parameterized by the given data compressor @xmath30 .",
    "the nid is the limiting case , where @xmath5 denotes the number of bits in the shortest code for @xmath4 from which @xmath4 can be decompressed by a general purpose computable decompressor .",
    "every text corpus or particular user combined with a frequency extractor defines its own relative frequencies of words and phrases usage . in the world - wide - web and google setting there are millions of users and text corpora , each with its own distribution . in the sequel ,",
    "we show ( and prove ) that the google distribution is universal for all the individual web users distributions .",
    "the number of web pages currently indexed by google is approaching @xmath36 .",
    "every common search term occurs in millions of web pages .",
    "this number is so vast , and the number of web authors generating web pages is so enormous ( and can be assumed to be a truly representative very large sample from humankind ) , that the probabilities of google search terms , conceived as the frequencies of page counts returned by google divided by the number of pages indexed by google , approximate the actual relative frequencies of those search terms as actually used in society . based on this premise , the theory we develop in this paper states that the relations represented by the normalized google distance approximately capture the assumed true semantic relations governing the search terms .",
    "the ngd formula only uses the probabilities of search terms extracted from the text corpus in question .",
    "we use the world - wide - web and google , but the same method may be used with other text corpora like the king james version of the bible or the oxford english dictionary and frequency count extractors , or the world - wide - web again and yahoo as frequency count extractor . in these cases",
    "one obtains a text corpus and frequency extractor biased semantics of the search terms . to obtain the true relative frequencies of words and phrases in society is a major problem in applied linguistic research .",
    "this requires analyzing representative random samples of sufficient sizes .",
    "the question of how to sample randomly and representatively is a continuous source of debate .",
    "our contention that the web is such a large and diverse text corpus , and google such an able extractor , that the relative page counts approximate the true societal word- and phrases usage , starts to be supported by current real linguistics research @xcite .",
    "let the set of singleton _ google search terms _ be denoted by @xmath37 . in the sequel we use both singleton search terms and doubleton search terms @xmath38 .",
    "let the set of web pages indexed ( possible of being returned ) by google be @xmath39 .",
    "the cardinality of @xmath39 is denoted by @xmath40 , and at the time of this writing @xmath41 ( and presumably greater by the time of reading this ) .",
    "assume that a priori all web pages are equi - probable , with the probability of being returned by google being @xmath42 .",
    "a subset of @xmath39 is called an _ event_. every _ search term _",
    "@xmath4 usable by google defines a _",
    "singleton google event _ @xmath43 of web pages that contain an occurrence of @xmath4 and are returned by google if we do a search for @xmath4 .",
    "let @xmath44 $ ] be the uniform mass probability function .",
    "the probability of an event @xmath45 is @xmath46 .",
    "similarly , the _ doubleton google event _",
    "@xmath47 is the set of web pages returned by google if we do a search for pages containing both search term @xmath4 and search term @xmath12 .",
    "the probability of this event is @xmath48 .",
    "we can also define the other boolean combinations : @xmath49 and @xmath50 , each such event having a probability equal to its cardinality divided by @xmath51 . if @xmath52 is an event obtained from the basic events @xmath53 , corresponding to basic search terms @xmath54 , by finitely many applications of the boolean operations , then the probability @xmath55 .",
    "google events capture in a particular sense all background knowledge about the search terms concerned available ( to google ) on the web .",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ the google event @xmath45 , consisting of the set of all web pages containing one or more occurrences of the search term @xmath4 , thus embodies , in every possible sense , all direct context in which @xmath4 occurs on the web .",
    "this constitutes the google semantics of the term . _",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    it is of course possible that parts of this direct contextual material link to other web pages in which @xmath4 does not occur and thereby supply additional context . in our approach",
    "this indirect context is ignored .",
    "nonetheless , indirect context may be important and future refinements of the method may take it into account .",
    "the event @xmath45 consists of all possible direct knowledge on the web regarding @xmath4 .",
    "therefore , it is natural to consider code words for those events as coding this background knowledge .",
    "however , we can not use the probability of the events directly to determine a prefix code , or , rather the underlying information content implied by the probability .",
    "the reason is that the events overlap and hence the summed probability exceeds 1 . by the kraft inequality @xcite",
    "this prevents a corresponding set of code - word lengths .",
    "the solution is to normalize : we use the probability of the google events to define a probability mass function over the set @xmath56 of google search terms , both singleton and doubleton terms",
    ". there are @xmath57 singleton terms , and @xmath58 doubletons consisting of a pair of non - identical terms .",
    "define @xmath59 counting each singleton set and each doubleton set ( by definition unordered ) once in the summation .",
    "note that this means that for every pair @xmath60 , with @xmath19 , the web pages @xmath61 are counted three times : once in @xmath62 , once in @xmath63 , and once in @xmath64 .",
    "since every web page that is indexed by google contains at least one occurrence of a search term , we have @xmath65 .",
    "on the other hand , web pages contain on average not more than a certain constant @xmath66 search terms .",
    "therefore , @xmath67 .",
    "define @xmath68 then , @xmath69 .",
    "this @xmath70-distribution changes over time , and between different samplings from the distribution .",
    "but let us imagine that @xmath70 holds in the sense of an instantaneous snapshot",
    ". the real situation will be an approximation of this . given the google machinery , these are absolute probabilities which allow us to define the associated prefix code - word lengths ( information contents ) for both the singletons and the doubletons .",
    "the _ google code _",
    "@xmath71 is defined by @xmath72      in contrast to strings @xmath4 where the complexity @xmath31 represents the length of the compressed version of @xmath4 using compressor @xmath30 , for a search term @xmath4 ( just the name for an object rather than the object itself ) , the google code of length @xmath73 represents the shortest expected prefix - code word length of the associated google event @xmath45 . the expectation is taken over the google distribution @xmath70 . in this sense we can use the google distribution as a compressor for the google semantics associated with the search terms .",
    "the associated ncd , now called the _ normalized google distance ( ngd ) _ is then defined by , and can be rewritten as the right - hand expression : @xmath74 where @xmath75 denotes the number of pages containing @xmath4 , and @xmath76 denotes the number of pages containing both @xmath4 and @xmath12 , as reported by google .",
    "this @xmath77 is an approximation to the @xmath78 of using the prefix code - word lengths ( google code ) generated by the google distribution as defining a compressor approximating the length of the kolmogorov code , using the background knowledge on the web as viewed by google as conditional information . in practice , use the page counts returned by google for the frequencies , and we have to choose @xmath79 . from the right - hand side term in it is apparent that by increasing @xmath79 we decrease the ngd , everything gets closer together , and by decreasing @xmath79 we increase the ngd , everything gets further apart .",
    "our experiments suggest that every reasonable ( @xmath51 or a value greater than any @xmath75 ) value can be used as normalizing factor @xmath79 , and our results seem in general insensitive to this choice . in our software , this parameter @xmath79 can be adjusted as appropriate , and we often use @xmath51 for @xmath79 .",
    "the following are the main properties of the ngd ( as long as we choose parameter @xmath65 ) :    1 .",
    "the _ range _ of the ngd is in between 0 and @xmath80 ( sometimes slightly negative if the google counts are untrustworthy and state @xmath81 , see section  [ sect.materials ] ) ; 1 .",
    "if @xmath82 or if @xmath19 but frequency @xmath83 , then @xmath84 .",
    "that is , the semantics of @xmath4 and @xmath12 in the google sense is the same .",
    "if frequency @xmath85 , then for every search term @xmath12 we have @xmath86 , and the @xmath87 , which we take to be 1 by definition .",
    "the @xmath77 is always nonnegative and @xmath88 for every @xmath4 .",
    "for every pair @xmath16 we have @xmath89 : it is symmetric .",
    "however , the ngd is _ not a metric _ : it does not satisfy @xmath90 for every @xmath19 . as before , let @xmath45 denote the set of web pages containing one or more occurrences of @xmath4 .",
    "for example , choose @xmath19 with @xmath91 .",
    "then , @xmath92 and @xmath84 . nor",
    "does the ngd satisfy the triangle inequality @xmath93 for all @xmath22 .",
    "for example , choose @xmath94 , @xmath95 , @xmath96 , @xmath97 , and @xmath98 .",
    "then , @xmath99 , @xmath100 , and @xmath86 .",
    "this yields @xmath101 and @xmath102 , which violates the triangle inequality for all @xmath79 .",
    "the ngd is _ scale - invariant _ in the following sense : assume that when the number @xmath79 of pages indexed by google ( accounting for the multiplicity of different search terms per page ) grows , the number of pages containing a given search term goes to a fixed fraction of @xmath79 , and so does the number of pages containing a given conjunction of search terms .",
    "this means that if @xmath79 doubles , then so do the @xmath103-frequencies .",
    "for the ngd to give us an objective semantic relation between search terms , it needs to become stable when the number @xmath79 grows unboundedly .      a central notion in the application of compression to learning",
    "is the notion of `` universal distribution , '' see @xcite .",
    "consider an effective enumeration @xmath104 of probability mass functions with domain @xmath37 .",
    "the list @xmath105 can be finite or countably infinite .",
    "[ def.unimult ] a probability mass function @xmath106 occurring in @xmath105 is _ universal _ for @xmath105 , if for every @xmath107 in @xmath105 there is a constant @xmath108 and @xmath109 , such that for every @xmath110 we have @xmath111 . here",
    "@xmath112 may depend on the indexes @xmath113 , but not on the functional mappings of the elements of list @xmath105 nor on @xmath4 .",
    "if @xmath106 is universal for @xmath105 , then it immediately follows that for every @xmath107 in @xmath105 , the prefix code - word length for source word @xmath4 , see @xcite , associated with @xmath106 , minorizes the prefix code - word length associated with @xmath107 , by satisfying @xmath114 , for every @xmath110 .    in the following we consider partitions of the set of web pages , each subset in the partition together with a probability mass function of search terms .",
    "for example , we may consider the list @xmath115 of _ web authors producing pages _ on the web , and consider the set of web pages produced by each web author , or some other partition .",
    "`` web author '' is just a metaphor we use for convenience .",
    "let web author @xmath116 of the list @xmath117 produce the set of web pages @xmath118 and denote @xmath119 .",
    "we identify a web author @xmath116 with the set of web pages @xmath118 he produces . since we have no knowledge of the set of web authors , we consider every possible partion of @xmath39 into one of more equivalence classes , @xmath120 , @xmath121 ( @xmath122 ) , as defining a realizable set of web authors @xmath123 .",
    "consider a partition of @xmath39 into @xmath124 .",
    "a search term @xmath4 usable by google defines an event @xmath125 of web pages produced by web author @xmath116 that contain search term @xmath4 .",
    "similarly , @xmath126 is the set of web pages produced by @xmath116 that is returned by google searching for pages containing both search term @xmath4 and search term @xmath12 .",
    "let @xmath127 note that there is an @xmath128 such that @xmath129 .",
    "for every search term @xmath110 define a probability mass function @xmath130 , the _ individual web author s google distribution _",
    ", on the sample space @xmath131 by @xmath132 then , @xmath133 .",
    "let @xmath124 be any partition of @xmath39 into subsets ( web authors ) , and let @xmath134 be the corresponding individual google distributions .",
    "then the google distribution @xmath70 is universal for the enumeration @xmath135 .",
    "we can express the overall google distribution in terms of the individual web author s distributions : @xmath136 consequently , @xmath137 . since also @xmath138",
    ", we have shown that @xmath139 is universal for the family @xmath135 of individual web author s google distributions , according to definition  [ def.unimult ] .",
    "let us show that , for example , the uniform distribution @xmath140 ( @xmath141 ) over the search terms @xmath110 is not universal , for @xmath142 . by the requirement @xmath143 ,",
    "the sum taken over the number @xmath144 of web authors in the list @xmath117 , there is an @xmath116 such that @xmath145 .",
    "taking the uniform distribution on say @xmath146 search terms assigns probability @xmath147 to each of them . by the definition of universality of a probability mass function for the list of individual google probability mass functions",
    "@xmath130 , we can choose the function @xmath130 freely ( as long as @xmath148 , and there is another function @xmath149 to exchange probabilities of search terms with ) .",
    "so choose some search term @xmath4 and set @xmath150 , and @xmath151 for all search terms @xmath152 .",
    "then , we obtain @xmath153 . this yields the required contradiction for @xmath154 .",
    "every individual web author produces both an individual google distribution @xmath130 , and an _ individual prefix code - word length _ @xmath155 associated with @xmath130 ( see @xcite for this code ) for the search terms .    the associated _ individual normalized google distance _ @xmath156 of web author @xmath116",
    "is defined according to , with @xmath155 substituted for @xmath71 .",
    "these google distances @xmath156 can be viewed as the individual semantic distances according to the bias of web author @xmath116 .",
    "these individual semantics are subsumed in the general google semantics in the following sense : the normalized google distance is _ universal _ for the family of individual normalized google distances , in the sense that it is as about as small as the least individual normalized google distance , with high probability .",
    "hence the google semantics as evoked by all of the web society in a certain sense captures the biases or knowledge of the individual web authors . in theorem  [ theo.ngd ]",
    "we show that , for every @xmath157 , the inequality @xmath158 with @xmath159 is satisfied with @xmath130-probability going to 1 with growing @xmath160 .    to interpret , we observe that in case @xmath73 and @xmath161 are large with respect to @xmath162 , then @xmath163 . if moreover @xmath164 is large with respect to @xmath162 , then approximately @xmath165",
    ". let us estimate @xmath166 for this case under reasonable assumptions .",
    "without loss of generality assume @xmath167 . if @xmath168 , the number of pages returned on query @xmath4 , then @xmath169 .",
    "thus , approximately @xmath170",
    ". the uniform expectation of @xmath171 is @xmath172 , and @xmath79 divided by that expectation of @xmath171 equals @xmath173 , the number of web authors producing web pages .",
    "the uniform expectation of @xmath75 is @xmath174 , and @xmath79 divided by that expectation of @xmath75 equals @xmath57 , the number of google search terms we use .",
    "thus , approximately , @xmath175 , and the more the number of search terms exceeds the number of web authors , the more @xmath166 goes to 0 in expectation .    to understand",
    ", we may consider the codelengths involved as the google database changes over time .",
    "it is reasonable to expect that both the total number of pages as well as the total number of search terms in the google database will continue to grow for some time . in this period ,",
    "the sum total probability mass will be carved up into increasingly smaller pieces for more and more search terms .",
    "the maximum singleton and doubleton codelengths within the google database will grow .",
    "but the universality property of the google distribution implies that the google distribution s code length for almost all particular search terms will only exceed the best codelength among any of the individual web authors as in .",
    "the size of this gap will grow more slowly than the codelength for any particular search term over time .",
    "thus , the coding space that is suboptimal in the google distribution s code is an ever - smaller piece ( in terms of proportion ) of the total coding space .",
    "[ theo.ngd ] for every web author @xmath176 , the @xmath130-probability concentrated on the pairs of search terms for which holds is at least @xmath177 .",
    "the prefix code - word lengths @xmath155 associated with @xmath130 satisfy @xmath178 and @xmath179 .",
    "substituting @xmath180 by @xmath181 in the middle term of , we obtain @xmath182 _ markov s inequality _ says the following : let @xmath183 be any probability mass function ; let @xmath103 be any nonnegative function with @xmath183-expected value @xmath184 . for @xmath185",
    "we have @xmath186 .",
    "fix web author @xmath176 .",
    "we consider the conditional probability mass functions @xmath187 and @xmath188 over singleton search terms in @xmath37 ( no doubletons ) : the @xmath189-expected value of @xmath190 is @xmath191 since @xmath192 is a probability mass function summing to @xmath193 .",
    "then , by markov s inequality @xmath194 since the probability of an event of a doubleton set of search terms is not greater than that of an event based on either of the constituent search terms , and the probability of a singleton event conditioned on it being a singleton event is at least as large as the unconditional probability of that event , @xmath195 and @xmath196 . if @xmath197 , then @xmath198 and the search terms @xmath4 satisfy the condition of .",
    "moreover , the probabilities satisfy @xmath199 .",
    "together , it follows from that @xmath200 and therefore @xmath201 for the @xmath4 s with @xmath202 we have @xmath203 .",
    "substitute @xmath204 for @xmath205 ( there is @xmath130-probability @xmath206 that @xmath207 ) and @xmath208 in , both in the @xmath209-term in the numerator , and in the @xmath210-term in the denominator . noting that the two @xmath130-probabilities @xmath211 are independent , the total @xmath130-probability that both substitutions are justified is at least @xmath177 .",
    "therefore , the google normalized distance minorizes every normalized compression distance based on a particular user s generated probabilities of search terms , with high probability up to an error term that in typical cases is ignorable .",
    "we used our software tool available from http://www.complearn.org , the same tool that has been used in our earlier papers @xcite to construct trees representing hierarchical clusters of objects in an unsupervised way .",
    "however , now we use the normalized google distance ( ngd ) instead of the normalized compression distance ( ncd ) .",
    "the method works by first calculating a distance matrix whose entries are the pairswise ngd s of the terms in the input list",
    ". then calculate a best - matching unrooted ternary tree using a novel quartet - method style heuristic based on randomized hill - climbing using a new fitness objective function for the candidate trees .",
    "let us briefly explain what the method does ; for more explanation see @xcite .",
    "given a set of objects as points in a space provided with a ( not necessarily metric ) distance measure , the associated _ distance matrix _ has as entries the pairwise distances between the objects .",
    "regardless of the original space and distance measure , it is always possible to configure @xmath212 objects is @xmath212-dimensional euclidean space in such a way that the associated distances are identical to the original ones , resulting in an identical distance matrix .",
    "this distance matrix contains the pairwise distance relations according to the chosen measure in raw form .",
    "but in this format that information is not easily usable , since for @xmath213 our cognitive capabilities rapidly fail . just as the distance matrix is a reduced form of information representing the original data set , we now need to reduce the information even further in order to achieve a cognitively acceptable format like data clusters . to extract a hierarchy of clusters from the distance matrix , we determine a dendrogram ( ternary tree ) that agrees with the distance matrix according to a fidelity measure .",
    "this allows us to extract more information from the data than just flat clustering ( determining disjoint clusters in dimensional representation ) .",
    "this method does not just take the strongest link in each case as the `` true '' one , and ignore all others ; instead the tree represents all the relations in the distance matrix with as little distortion as is possible . in the particular examples we give below , as in all clustering examples we did but not depicted , the fidelity was close to 1 , meaning that the relations in the distance matrix are faithfully represented in the tree .",
    "the objects to be clustered are search terms consisting of the names of colors , numbers , and some tricky words .",
    "the program automatically organized the colors towards one side of the tree and the numbers towards the other , figure  [ fig.colors ] .",
    "it arranges the terms which have as only meaning a color or a number , and        nothing else , on the farthest reach of the color side and the number side , respectively .",
    "it puts the more general terms black and white , and zero , one , and two , towards the center , thus indicating their more ambiguous interpretation .",
    "also , things which were not exactly colors or numbers are also put towards the center , like the word `` small '' .",
    "as far as the authors know there do not exist other experiments that create this type of semantic distance automatically from the web using google or similar search engines .",
    "thus , there is no baseline to compare against ; rather the current experiment can be a baseline to evaluate the behavior of future systems .",
    "in the example of figure  [ fig.painters ] , the names of fifteen paintings by steen , rembrandt , and bol were entered .",
    "we use the full name as a single google search term ( also in the next experiment with book titles ) . in the experiment",
    ", only painting title names were used ; the associated painters are given below .",
    "we do not know of comparable experiments to use as baseline to judge the performance ; this is a new type of contents clustering made possible by the existence of the web and search engines .",
    "the painters and paintings used are as follows :    * rembrandt van rijn : * _ hendrickje slapend ; portrait of maria trip ; portrait of johannes wtenbogaert ; the stone bridge ; the prophetess anna _ ;    * jan steen : * _ leiden baker arend oostwaert ; keyzerswaert ; two men playing backgammon ; woman at her toilet ; prince s day ; the merry family _ ;    * ferdinand bol : * _ maria rey ; consul titus manlius torquatus ; swartenhout ; venus and adonis _ .",
    "another example is english novelists .",
    "the authors and texts used are :        * william shakespeare : * _ a midsummer night s dream ; julius caesar ; love s labours lost ; romeo and juliet _ .",
    "* jonathan swift : * _ the battle of the books ; gulliver s travels ; tale of a tub ; a modest proposal _ ;    * oscar wilde : * _ lady windermere s fan ; a woman of no importance ; salome ; the picture of dorian gray_.    the clustering is given in figure  [ fig.englishnov ] , and to provide a feeling for the figures involved we give the associated ngd matrix in figure  [ fig.distmatr ] .",
    "the @xmath214 value in figure  [ fig.englishnov ] gives the fidelity of the tree as a representation of the pairwise distances in the ngd matrix ( @xmath215 is perfect and @xmath216 is as bad as possible .",
    "for details see @xcite ) .",
    "the question arises why we should expect this .",
    "are names of artistic objects so distinct ?",
    "( yes . the point also being that the distances from every single object to all other objects are involved .",
    "the tree takes this global aspect into account and therefore disambiguates other meanings of the objects to retain the meaning that is relevant for this collection . )",
    "is the distinguishing feature subject matter or title style ? in these experiments with objects belonging to the cultural heritage it is clearly a subject matter . to stress the point we used `` julius caesar '' of shakespeare .",
    "this term occurs on the web    * training data *",
    "+    l l l l l + _ positive training _ & ( 22 cases ) + avalanche&bomb threat&broken leg&burglary&car collision + death threat&fire&flood&gas leak&heart attack + hurricane&landslide&murder&overdose&pneumonia + rape&roof collapse&sinking ship&stroke&tornado + train wreck&trapped miners +   + _ negative training _ &",
    "( 25 cases ) + arthritis&broken dishwasher&broken toe&cat in tree&contempt of court + dandruff&delayed train&dizziness&drunkenness&enumeration + flat tire&frog&headache&leaky faucet&littering + missing dog&paper cut&practical joke&rain&roof leak + sore throat&sunset&truancy&vagrancy&vulgarity +   + _ anchors _ & ( 6 dimensions ) + crime&happy&help&safe&urgent + wash +   +    l l l + & positive tests & negative tests + positive&assault , coma,&menopause , prank call , + predictions&electrocution , heat stroke,&pregnancy , traffic jam + & homicide , looting , & + & meningitis , robbery , & + & suicide & + negative&sprained ankle&acne , annoying sister , + predictions&&campfire , desk , + & & mayday , meal +   * accuracy * & 15/20 = 75.00% & +    * training data *   +    l",
    "l l l l + _ positive training _ & ( 21 cases ) + 11&13&17&19&2 + 23&29&3&31&37 + 41&43&47&5&53 + 59&61&67&7&71 + 73 +   + _ negative training _ & ( 22 cases ) + 10&12&14&15&16 + 18&20&21&22&24 + 25&26&27&28&30 + 32&33&34&4&6 + 8&9 +   + _ anchors _ & ( 5 dimensions ) + composite&number&orange&prime&record +",
    "+    l l l + & positive tests & negative tests + positive&101 , 103,&110 + predictions&107 , 109 , & + & 79 , 83 , & + & 89 , 91 , & + & 97 & + negative&&36 , 38 , + predictions&&40 , 42 , + & & 44 , 45 , + & & 46 , 48 , + & & 49 +   * accuracy * & 18/19 = 94.74% & +    overwhelmingly in other contexts and styles . yet the collection of the other objects used , and the semantic distance towards those objects , given by the ngd formula , singled out the semantics of `` julius caesar '' relevant to this experiment .",
    "term co - occurrence in this specific context of author discussion is not swamped by other uses of this common english term because of the particular form of the ngd and the distances being pairwise . using book titles which are common words , like `` horse '' and `` rider '' by author x , supposing they exist",
    ", this swamping effect will presumably arise .",
    "does the system gets confused if we add more artists ?",
    "( representing the ngd matrix in bifurcating trees without distortion becomes more difficult for , say , more than 25 objects .",
    "see @xcite . )",
    "what about other subjects , like music , sculpture ?",
    "( presumably , the system will be more trustworthy if the subjects are more common on the web . )",
    "these experiments are representative for those we have performed with the current software .",
    "we did not cherry - pick the best outcomes . for example , all experiments with these three english writers , with different selections of four works of each , always yielded a tree so that we could draw a convex hull around the works of each author , without overlap .",
    "interestingly , a similar experiment with russian authors gave worse results .",
    "the readers can do their own experiments to satisfy their curiosity using our publicly available software tool at http://clo.complearn.org/ , also used in the depicted experiments .",
    "each experiment can take a long time , hours , because of the googling , network traffic , and tree reconstruction and layout .",
    "do nt wait , just check for the result later .",
    "on the web page http://clo.complearn.org/clo/listmonths/t.html the onging cumulated results of all ( in december 2005 some 160 ) experiments by the public , including the ones depicted here , are recorded .",
    "we augment the google method by adding a trainable component of the learning system . here",
    "we use the support vector machine ( svm ) as a trainable component . for the svm method used in this paper",
    ", we refer to the exposition @xcite .",
    "we use libsvm software for all of our svm experiments .",
    "the setting is a binary classification problem on examples represented by search terms .",
    "we require a human expert to provide a list of at least 40 _ training words _ , consisting of at least 20 positive examples and 20 negative examples , to illustrate the contemplated concept class .",
    "the expert also provides , say , six _ anchor words _",
    "@xmath217 , of which half are in some way related to the concept under consideration .",
    "then , we use the anchor words to convert each of the 40 training words @xmath218 to 6-dimensional _ training vectors _",
    "the entry @xmath220 of @xmath221 is defined as @xmath222 ( @xmath223 , @xmath224 ) .",
    "the training vectors are then used to train an svm to learn the concept , and then test words may be classified using the same anchors and trained svm model .    in figure  [ fig.emergencies ] , we trained using a list of `` emergencies '' as positive examples , and a list of `` almost emergencies '' as negative examples .",
    "the figure is self - explanatory .",
    "the accuracy on the test set is 75% . in figure",
    "[ fig.primes ] the method learns to distinguish prime numbers from non - prime numbers by example .",
    "the accuracy on the test set is about 95% .",
    "this example illustrates several common features of our method that distinguish it from the strictly deductive techniques .",
    "yet another potential application of the ngd method is in natural language translation .",
    "( in the experiment below we do nt use svm s to obtain our result , but determine correlations instead . )",
    "suppose we are given a system that tries to infer a translation - vocabulary among english and spanish .",
    "assume that the system has already determined that there are five words that appear in two different matched sentences , but the permutation associating the english and spanish words is , as yet , undetermined .",
    "this setting can arise in real situations , because english and spanish have different rules for word - ordering . at the outset",
    "we assume a pre - existing vocabulary of eight english words with their matched spanish translation .",
    "can we infer the correct permutation mapping the unknown words using the pre - existing vocabulary as a basis ?",
    "we start by forming an ngd matrix using the additional english words of which the translation is known , figure  [ fig.estrans1 ] .",
    "we label the columns by the translation - known english words , the rows by the translation - unknown english words .",
    "the entries of the matrix are the ngd s between the english words labeling the columns and rows .",
    "this constitutes the english basis matrix .",
    "next , consider the known spanish words corresponding to the known english words .",
    "form a new matrix with the known spanish words labeling the columns in the same order as the known english words .",
    "label the rows of the new matrix by choosing one of the many possible permutations of the unknown spanish words . for each permutation ,",
    "form the ngd matrix for the spanish words , and compute the pairwise correlation of this sequence of values to each of the values in the given english word basis matrix .",
    "choose the permutation with the highest positive correlation . if there is no positive correlation report a failure to extend the vocabulary .",
    "in this example , the computer inferred the correct permutation for the testing words , see figure  [ fig.estrans2 ] .    [",
    "cols=\">,<\",options=\"header \" , ]",
    "wordnet @xcite is a semantic concordance of english .",
    "it focusses on the meaning of words by dividing them into categories .",
    "we use this as follows . a category we want to learn , the concept ,",
    "is termed , say , `` electrical '' , and represents anything that may pertain to electronics . the negative examples are constituted by simply everything else .",
    "this category represents a typical expansion of a node in the wordnet hierarchy . in an experiment we ran ,",
    "the accuracy on the test set is 100% : it turns out that `` electrical terms '' are unambiguous and easy to learn and classify by our method .",
    "the information in the wordnet database is entered over the decades by human experts and is precise .",
    "the database is an academic venture and is publicly accessible .",
    "hence it is a good baseline against which to judge the accuracy of our method in an indirect manner .",
    "while we can not directly compare the semantic distance , the ngd , between objects , we can indirectly judge how accurate it is by using it as basis for a learning algorithm .",
    "in particular , we investigated how well semantic categories as learned using the ngd ",
    "svm approach agree with the corresponding wordnet categories .",
    "for details about the structure of wordnet we refer to the official wordnet documentation available online .",
    "we considered 100 randomly selected semantic categories from the wordnet database .",
    "for each category we executed the following sequence .",
    "first , the svm is trained on 50 labeled training samples .",
    "the positive examples are randomly drawn from the wordnet database in the category in question .",
    "the negative examples are randomly drawn from a dictionary .",
    "while the latter examples may be false negatives , we consider the probability negligible . per experiment we used a total of six anchors ,",
    "three of which are randomly drawn from the wordnet database category in question , and three of which are drawn from the dictionary .",
    "subsequently , every example is converted to 6-dimensional vectors using ngd .",
    "the @xmath116th entry of the vector is the ngd between the @xmath116th anchor and the example concerned ( @xmath224 ) .",
    "the svm is trained on the resulting labeled vectors .",
    "the kernel - width and error - cost parameters are automatically determined using five - fold cross validation .",
    "finally , testing of how well the svm has learned the classifier is performed using 20 new examples in a balanced ensemble of positive and negative examples obtained in the same way , and converted to 6-dimensional vectors in the same manner , as the training examples .",
    "this results in an accuracy score of correctly classified test examples .",
    "we ran 100 experiments .",
    "the actual data are available at @xcite .",
    "a histogram of agreement accuracies is shown in figure  [ fig.wordnethisto ] .",
    "on average , our method turns out to agree well with the wordnet semantic concordance made by human experts .",
    "the mean of the accuracies of agreements is 0.8725 .",
    "the variance is @xmath225 , which gives a standard deviation of @xmath226 .",
    "thus , it is rare to find agreement less than 75% .",
    "the total number of google searches involved in this randomized automatic trial is upper bounded by @xmath227 .",
    "a considerable savings resulted from the fact that we can re - use certain google counts .",
    "for every new term , in computing its 6-dimensional vector , the ngd computed with respect to the six anchors requires the counts for the anchors which needs to be computed only once for each experiment , the count of the new term which can be computed once , and the count of the joint occurrence of the new term and each of the six anchors , which has to be computed in each case .",
    "altogether , this gives a total of @xmath228 for every experiment , so @xmath229 google searches for the entire trial .",
    "it is conceivable that other scores instead of the ngd used in the construction of 6-dimensional vectors work competetively . yet , something simple like `` the number of words used in common in their dictionary definition '' ( google indexes dictionaries too ) is begging the question and unlikely to be successful . in @xcite the ncd abbroach , compression of the literal objects , was compared with a number of alternative approaches like the euclidean distance between frequency vectors of blocks .",
    "the alternatives gave results that were completely unacceptable . in the current setting , we can conceive of euclidean vectors of word frequencies in the set of pages corresponding to the search term . apart from the fact that google does not support automatical analysis of all pages reported for a search term",
    ", it would be computationally infeasible to analyze the millions of pages involved .",
    "thus , a competetive nontrivial alternative to compare the present technique against is an interesting open question .",
    "a comparison can be made with the _ cyc _ project @xcite .",
    "cyc , a project of the commercial venture cycorp , tries to create artificial common sense .",
    "cyc s knowledge base consists of hundreds of microtheories and hundreds of thousands of terms , as well as over a million hand - crafted assertions written in a formal language called cycl  @xcite .",
    "cycl is an enhanced variety of first - order predicate logic .",
    "this knowledge base was created over the course of decades by paid human experts .",
    "it is therefore of extremely high quality .",
    "google , on the other hand , is almost completely unstructured , and offers only a primitive query capability that is not nearly flexible enough to represent formal deduction .",
    "but what it lacks in expressiveness google makes up for in size ; google has already indexed more than eight billion pages and shows no signs of slowing down .",
    "we thank the referees and others for comments on presentation .",
    "the basis assumption of latent semantic analysis is that `` the cognitive similarity between any two words is reflected in the way they co - occur in small subsamples of the language . '' in particular",
    ", this is implemented by constructing a matrix with rows labeled by the @xmath230 documents involved , and the columns labeled by the @xmath144 attributes ( words , phrases ) .",
    "the entries are the number of times the column attribute occurs in the row document .",
    "the entries are then processed by taking the logarithm of the entry and dividing it by the number of documents the attribute occurred in , or some other normalizing function .",
    "this results in a sparse but high - dimensional matrix @xmath231 .",
    "a main feature of lsa is to reduce the dimensionality of the matrix by projecting it into an adequate subspace of lower dimension using singular value decomposition @xmath232 where @xmath233 are orthogonal matrices and @xmath23 is a diagonal matrix .",
    "the diagonal elements @xmath234 ( @xmath235 ) satisfy @xmath236 , and the closest matrix @xmath237 of dimension @xmath238 in terms of the so - called frobenius norm is obtained by setting @xmath239 for @xmath240 . using @xmath237 corresponds to using the most important dimensions .",
    "each attribute is now taken to correspond to a column vector in @xmath237 , and the similarity between two attributes is usually taken to be the cosine between their two vectors . to compare lsa to our proposed method",
    ", the documents could be the web pages , the entries in matrix @xmath231 are the frequencies of a search terms in each web page .",
    "this is then converted as above to obtain vectors for each search term .",
    "subsequently , the cosine between vectors gives the similarity between the terms .",
    "lsa has been used in a plethora of applications ranging from data base query systems to synonymy answering systems in toefl tests .",
    "comparing its performance to our method is problematic for several reasons .",
    "first , the numerical quantity measuring the semantic distance between pairs of terms can not directly be compared , since they have quite different epistimologies .",
    "indirect comparison could be given using the method as basis for a particular application , and comparing accuracies .",
    "however , application of lsa in terms of the web using google is computationally out of the question , because the matrix @xmath231 would have @xmath36 rows , even if google would report frequencies of occurrences in web pages and identify the web pages properly .",
    "one would need to retrieve the entire google data base , which is many terabytes .",
    "moreover , as noted in section  [ sect.materials ] , each google search takes a significant amount of time , and we can not automatically make more than a certain number of them per day .",
    "an alternative interpretation by considering the web as a single document makes the matrix @xmath231 above into a vector and appears to defeat the lsa process altogether .",
    "summarizing , the basic idea of our method is similar to that of lsa in spirit .",
    "what is novel is that we can do it with selected terms over a very large document collection , whereas lsa involves matrix operations over a closed collection of limited size , and hence is not possible to apply in the web context .",
    "99    j.p .",
    "bagrow , d. ben - avraham , on the google - fame of scientists and other populations , _ aip conference proceedings _",
    "779:1(2005 ) , 8189 .",
    "bennett , p. gcs , m. li , p.m.b .",
    "vitnyi , w. zurek , information distance , _ ieee trans .",
    "information theory _ , 44:4(1998 ) , 14071423 .",
    "bennett , m. li , b. ma , chain letters and evolutionary histories , _ scientific american _ , june 2003 , 7681 .",
    "c.j.c . burges . a tutorial on support vector machines for pattern recognition , _ data mining and knowledge discovery _ , 2:2(1998),121167 .    automatic meaning discovery using google : 100 experiments in learning wordnet categories , 2004 , http://www.cwi.nl/@xmath241cilibrar/googlepaper/appendix.pdf    r. cilibrasi , complearn home , http://www.complearn.org/    r.  cilibrasi , r.  de  wolf , p.  vitanyi .",
    "algorithmic clustering of music based on string compression , _ computer music j. _ , 28:4(2004 ) , 49 - 67 .",
    "r.  cilibrasi , p.  vitanyi .",
    "clustering by compression , _ ieee trans .",
    "information theory _ , 51:4(2005 ) , 1523- 1545 .",
    "r.  cilibrasi , p.  vitanyi , automatic meaning discovery using google , http://xxx.lanl.gov/abs/cs.cl/0412098 ( 2004 ) .",
    "r.  cilibrasi , p.  vitanyi , a new quartet tree heuristic for hierarchical clustering , http://arxiv.org/abs/cs.ds/0606048    p. cimiano , s. staab , learning by googling , _ sigkdd explorations _ , 6:2(2004 ) , 2433 .",
    "t.m . cover and j.a .",
    "thomas , _ elements of information theory _ , wiley , new york , 1991",
    ".    j .- p .",
    "delahaye , classer musiques , langues , images , textes et genomes ,",
    "_ pour la science _ , 317(march 2004 ) , 98103 .",
    "the basics of google search , http://www.google.com/help/basics.html .",
    "kraft , a device for quantizing , grouping and coding amplitude modulated pulses .",
    "master s thesis , dept . of electrical engineering ,",
    "cambridge , mass . , 1949 .",
    "d. graham - rowe , a search for meaning , _ new scientist _ , 29 january 2005 , p.21 .",
    ", from january 29 , 2005 : http://science.slashdot.org + /article.pl?sid=05/01/29/1815242&tid=217&tid=14 chih - chung chang and chih - jen lin , libsvm : a library for support vector machines , 2001 .",
    "software available at http://www.csie.ntu.edu.tw/@xmath242cjlin/libsvm    p. cimiano , s. staab , learning by googling , _ acm sigkdd explorations newsletter _",
    ", 6:2 ( december 2004 ) , 24  33    h. muir , software to unzip identity of unknown composers , _ new scientist _ , 12 april 2003 .",
    "k. patch , software sorts tunes , _ technology research news _ ,",
    "april 23/30 , 2003 .",
    "d.  b. lenat .",
    "cyc : a large - scale investment in knowledge infrastructure , _ comm .",
    "acm _ , 38:11(1995),3338 .",
    "f keller , m lapata , using the web to obtain frequencies for unseen bigrams , _ computational linguistics _ , 29:3(2003 ) , 459484 .",
    "three approaches to the quantitative definition of information , _ problems inform . transmission _ , 1:1(1965 ) , 17 .",
    "m. li , j.h .",
    "badger , x. chen , s. kwong , p. kearney , and h. zhang , an information - based sequence distance and its application to whole mitochondrial genome phylogeny , _ bioinformatics _ , 17:2(2001 ) , 149154 .",
    "m.  li , x.  chen , x.  li , b.  ma , p.  vitanyi .",
    "the similarity metric , _ iaa eee trans .",
    "information theory _ , 50:12(2004 ) , 3250- 3264 .",
    "m. li , p. m.  b. vitanyi .",
    "_ an introduction to kolmogorov complexity and its applications _ , 2nd ed . , springer - verlag , new york , 1997 .",
    "m.  li and p.m.b .",
    "algorithmic complexity , pp .",
    "376382 in : _ international encyclopedia of the social & behavioral sciences _ , n.j .",
    "smelser and p.b .",
    "baltes , eds . ,",
    "pergamon , oxford , 2001/2002 .",
    "m. li and p.m.b .",
    "vitnyi , reversibility and adiabatic computation : trading time and space for energy , _ proc .",
    "royal society of london , series a _ , 452(1996 ) , 769 - 789 .",
    "s.  l. reed , d.  b. lenat .",
    "mapping ontologies into cyc . _",
    "aaai conference 2002 workshop on ontologies for the semantic web _ , edmonton , canada .",
    "rumsfeld , the digital revolution , originally published june 9 , 2001 , following a european trip . in : h.",
    "seely , the poetry of d.h .",
    "rumsfeld , 2003 , http://slate.msn.com/id/2081042/    c.  e. shannon . a mathematical theory of communication .",
    "bell systems technical j. _ , 27(1948 ) , 379423 and 623656 .",
    "miller et.al , wordnet , a lexical database for the english language , cognitive science lab , princeton university , http://www.cogsci.princeton.edu/@xmath242wn    e. terra and c. l. a. clarke .",
    "frequency estimates for statistical word similarity measures .",
    "hlt / naacl 2003 , edmonton , alberta , may 2003 .",
    "37/162    m.e .",
    "lesk , word - word associations in document retrieval systems , _ american documentation _ , 20:1(1969 ) , 2738 .",
    "n . tan , v. kumar , j. srivastava , selecting the right interestingness measure for associating patterns .",
    "acm - sigkdd conf .",
    "knowledge discovery and data mining _",
    ", 2002 , 491502 .",
    "t. landauer and s. dumais , a solution to plato s problem : the latent semantic analysis theory of acquisition , induction and representation of knowledge , _ psychol .",
    "_ , 104(1997 ) , 211240 .",
    "corpus collosal : how well does the world wide web represent human language ? _ the economist _ , january 20 , 2005 .",
    "http://www.economist.com/science + /displaystory.cfm?story_id=3576374    e. keogh , s. lonardi , and c.a .",
    "rtanamahatana , toward parameter - free data mining , in : _ proc .",
    "10th acm sigkdd intnl conf .",
    "knowledge discovery and data mining _ , 2004 , 206215 .    c. costa santos , j. bernardes , p.m.b .",
    "vitanyi , l. antunes , clustering fetal heart rate tracings by compression , in : _ proc .",
    "19th ieee symp .",
    "computer - based medical systems _ , 2006 , 685 - 690 .",
    "rudi cilibrasi received his b.s . with honors from the california institute of technology in 1996 .",
    "he has programmed computers for over two decades , both in academia , and industry with various companies in silicon valley , including microsoft , in diverse areas such as machine learning , data compression , process control , vlsi design , computer graphics , computer security , and networking protocols .",
    "he is now a phd student at the centre for mathematics and computer science ( cwi ) in the netherlands , and expects to receive his phd soon on the circle of ideas of which this paper is representative .",
    "he helped create the first publicly downloadable normalized compression / google distance software , and is maintaining http://www.complearn.org now .",
    "home page : http://www.cwi.nl/@xmath241cilibrar/    paul m.b .",
    "vitnyi is a fellow of the centre for mathematics and computer science ( cwi ) in amsterdam and is professor of computer science at the university of amsterdam .",
    "he serves on the editorial boards of distributed computing ( until 2003 ) , information processing letters , theory of computing systems , parallel processing letters , international journal of foundations of computer science , journal of computer and systems sciences ( guest editor ) , and elsewhere .",
    "he has worked on cellular automata , computational complexity , distributed and parallel computing , machine learning and prediction , physics of computation , kolmogorov complexity , quantum computing .",
    "together with ming li they pioneered applications of kolmogorov complexity and co - authored `` an introduction to kolmogorov complexity and its applications , '' springer - verlag , new york , 1993 ( 2nd edition 1997 ) , parts of which have been translated into chinese , russian and japanese .",
    "home page : http://www.cwi.nl/@xmath241paulv/"
  ],
  "abstract_text": [
    "<S> words and phrases acquire meaning from the way they are used in society , from their relative semantics to other words and phrases . for computers the equivalent of ` society ' is ` database , ' and the equivalent of ` use ' is ` way to search the database . ' </S>",
    "<S> we present a new theory of similarity between words and phrases based on information distance and kolmogorov complexity . to fix thoughts we use the world - wide - web as database , and google as search engine . </S>",
    "<S> the method is also applicable to other search engines and databases . </S>",
    "<S> this theory is then applied to construct a method to automatically extract similarity , the google similarity distance , of words and phrases from the world - wide - web using google page counts . </S>",
    "<S> the world - wide - web is the largest database on earth , and the context information entered by millions of independent users averages out to provide automatic semantics of useful quality . </S>",
    "<S> we give applications in hierarchical clustering , classification , and language translation . </S>",
    "<S> we give examples to distinguish between colors and numbers , cluster names of paintings by 17th century dutch masters and names of books by english novelists , the ability to understand emergencies , and primes , and we demonstrate the ability to do a simple automatic english - spanish translation . </S>",
    "<S> finally , we use the wordnet database as an objective baseline against which to judge the performance of our method . </S>",
    "<S> we conduct a massive randomized trial in binary classification using support vector machines to learn categories based on our google distance , resulting in an a mean agreement of 87% with the expert crafted wordnet categories .    </S>",
    "<S> _ index terms_    accuracy comparison with wordnet categories , automatic classification and clustering , automatic meaning discovery using google , automatic relative semantics , automatic translation , dissimilarity semantic distance , google search , google distribution via page hit counts , google code , kolmogorov complexity , normalized compression distance ( ncd ) , normalized information distance ( nid ) , normalized google distance ( ngd ) , meaning of words and phrases extracted from the web , parameter - free data - mining , universal similarity metric </S>"
  ]
}