{
  "article_text": [
    "linear inverse problems arise throughout engineering and the mathematical sciences . in most applications , these problems are ill - conditioned or under - determined , so we must apply additional regularizing constraints in order to obtain interesting solutions .",
    "most modern approaches use the sparsity of the solution as a regularizer @xcite . in this paper",
    "we first give a brief view of these algorithms for sparse approximation .",
    "then we propose a new two - stage sparse representation method solving linear inverse problem .",
    "roughly there are four class of approaches to solve linear inverse problems : greedy , convex relaxation , proximal and combinatorial methods .",
    "orthogonal matching pursuit ( omp ) @xcite is one of the important greedy algorithms .",
    "omp finds one atom at a time for approximating the solution of the @xmath0 problem :    @xmath1    where @xmath2 is a target signal , @xmath3 is some error tolerance .",
    "we refer to the vector @xmath4 as representing coefficient of @xmath2 respect to the dictionary @xmath5 .",
    "we say @xmath4 is @xmath6-sparse when @xmath7 .",
    "the dictionary @xmath8 is a real matrix whose columns have unit euclidean norm : @xmath9=1 for @xmath10 .",
    "omp accumulates the vectors which have the least residual with the representing coefficients .",
    "the accuracy is restricted since omp does not consider that the multiple correlated atoms might be jointly selected .",
    "the other greedy algorithms , including st - omp @xcite , romp @xcite , etc , use @xmath11-norm to replace the np - hard @xmath0-norm minimization : @xmath12 they work well when @xmath4 is very sparse , but will deviate from the ideal solution of eqn .",
    "( [ eqn_1 ] ) when the number of non - zero entries in @xmath4 increases , as illustrated in the paper @xcite .",
    "cosamp @xcite , as a combinatorial method for solving @xmath13 , is a widely used method which avoids the pure greedy nature of omp which can never remove any atom once they are selected .",
    "it provides rigorous bounds on the runtime that are much better than the available results for interior - point methods @xcite - e.g. @xmath14-magic @xcite .",
    "the uniformity property of cosamp shows it can recover all signals given a fixed sampling matrix and the stability property guarantees its success when the samples are contaminated with noise .    the convex relaxation methods , as another branch , relaxes the @xmath15 form by @xmath13 .",
    "basis pursuit denoising ( bpdn ) @xcite , solves a regularization problem with a trade - off between having a small residual and making coefficients simple in the @xmath11 sense .",
    "basis pursuit ( bp ) series methods are far more complicated than omp series , because these methods obtain the global solution of the optimal problem in each iteration .",
    "@xmath14-magic @xcite is a collection of matlab routines which are based on standard interior - point methods .",
    "one class of algorithms within reformulates linear inverse problem as the second - order cone program , and solve it with log - barrier method , which use conjugate gradient method as inner core .",
    "least - angle regression ( lars ) @xcite , as an active set method , performs model select to find the optimal point iteratively .",
    "it produces a full piecewise linear solution path , which is useful in cross - validation or similar attempts to tune the model .    among the proximal methods , iterative shrinkage - thresholding algorithm ( ista ) @xcite solves the variant of the problem @xmath13 , @xmath16 where @xmath17 is a regularization parameter . roughly speaking ,",
    "each iteration comprises of a multiplication by @xmath5 and its adjoint , along with a scalar shrinkage step on the obtained result .",
    "a short survey on the applications of ista series can be found in @xcite .",
    "fista @xcite , as a quicker version of ista , is proposed recently .",
    "still fista needs many iterations for solving inverse problem if @xmath17 is small which is required for a good approximation of eqn .",
    "( [ eqn_2 ] ) .",
    "iht @xcite is based on the surrogate objective from @xcite : @xmath18 for @xmath19 , the above is a majorisation of the @xmath0 regularized sparse coding objective function . using a fixed threshold",
    ", the author show iht converges to a local minimum .",
    "it also needs many iterations and the per - iteration cost is about the same as matching pursuit .",
    "gisa @xcite cleverly extended the soft - thresholding operator for @xmath20-norm regularized sparse coding problem . instead of a fixed threshold ( e.g. @xmath21 in iht )",
    ", the authors derived the following non - linear equation for the threshold @xmath22 : @xmath23 using the above threshold gst can always find the correct solution to the @xmath20-minimization problem @xmath24 .",
    "the authors show one iteration of gisa is sufficient for image deconvolution .",
    "hence it is very efficient .    to reduce the iterations for a proximal approach , the linearized bregman ( lb ) algorithm @xcite",
    "is produced which is equivalent to gradient descent applied to a certain dual formulation .",
    "the analysis shows that lb has the exact regularization property ; namely , it converges to an exact solution of @xmath13 whenever its smooth parameter @xmath25 is greater than a certain value .",
    "the lb algorithm returns the solution to @xmath13 by solving the model : @xmath26    lb replaces the quadratic penalty in @xmath13 with a linear term and uses a mixture of @xmath11 and @xmath27 norm for the regularization .",
    "this key modification produces a strictly convex differentiable objective function .",
    "the lb method requires @xmath28 iterations to obtain an @xmath29-optimal solution , while accelerated linearized bregman method ( alb ) @xcite reduces the iteration complexity to @xmath30 while requiring almost the same computational effort on each iteration .",
    "alb converges much quicker than lb on three types of sensing matrices generated by the @xmath31 function @xcite , which are standard gaussian matrix , normalized gaussian matrix and bernoulli + 1/-1 matrix .",
    "the other merit is that the relative errors obtained by alb as a function of the iteration number are much smaller than lb does .",
    "how to quickly represent data has been an open problem to deal with in academic and industrial area .",
    "many algorithms similar to above representative approaches adopt one - stage coding technique , which encodes the original samples in a large projected space . among them many recast @xmath13 as a convex program with quadratic constraints , the computational cost for practical applications can be prohibitively high for large - scale problems .",
    "honglak lee @xcite , etc .",
    ", however , proposed an efficient sparse coding algorithm , iteratively solving two convex optimization problems : an @xmath14 regularized least squares problem and an @xmath32-constrained least square problem . in @xcite , an online tracking algorithm with two stage optimization",
    "was proposed to jointly minimize the target reconstruction error and maximize the discriminative power by selecting a sparse set of features .",
    "it is very effective in handling a number of challenging sequences .",
    "tsr @xcite proposed a robust and fast sparse representation method based on divide and conquer strategy .",
    "it divided the procedure of recognition into outlier detection stage and recognition stage .",
    "fsr @xcite , first uses ksvd method @xcite to construct an dictionary for sparse representation , then applies omp @xcite to the dictionary to generate coefficients , and forms a reduced dictionary with the coefficients for the sparse coding .",
    "it runs much quicker than @xmath14-magic solver , but the ksvd method assumes an overcomplete dictionary and has trivial problem when the dimension of the signal is larger than that of the dictionary .    as signals can be modelled by a small set of atoms in a dictionary , fsrm @xcite exploits the property and shows that the @xmath11-norm minimization problem can be reduced from a large and dense linear system to a small and sparse one .",
    "it exploits cosamp to generate sparse coefficients to do the next coding .",
    "experimental results with image recognition indicate fsrm achieves double - digit gain in speed with comparable accuracy compared with the @xmath14-magic solver , and solves the trivial problem fsr has .",
    "the above two methods shares the same two - stage structure .",
    "the core of both is to design a projected sparsifying basis in the first stage to represent the signals , and do sparse coding in the second stage .",
    "motivated by it , we propose a two - stage sparse representation ( tssr ) method to design the basis for rapid speed .",
    "tssr makes the sparse approximation computationally tractable without sacrificing stable convergence .",
    "it represents the data in a smaller space for discriminative representation and reduces the runtime dramatically .",
    "the remainder of the paper is organized as follows .",
    "section 2 presents the derivation of tssr for verification and the algorithm flowchart as well as the complexity analysis .",
    "section 3 gives an example to facilitate the method and shows the experimental results with discussion , and finally section 4 offers the conclusion .",
    "we first describe the derivation of tssr , which generates a new form of @xmath13 , then give an example to show the performance of the method .",
    "the signal @xmath33 can be approximated in two ways :    \\1 .",
    "@xmath5 is viewed as a dictionary , @xmath8 .",
    "@xmath34 is then the generated sparse coefficient vector over @xmath5 , and @xmath35 where @xmath36 $ ] .",
    "@xmath5 is supposed to satisfy the restricted isometry property ( rip ) of order @xmath37 @xcite , and @xmath38 is the residual .",
    "the illustration of eqn .",
    "( [ eqn_5 ] ) is shown as figure [ fig_1](a ) .",
    "we can generate a new space @xmath39 , here @xmath40 is viewed as a measurement dictionary , and @xmath41 $ ] represents a sparsity basis or dictionary in which @xmath42 expresses the @xmath43-th sparse vector .",
    "then we have another description of the signal @xmath2 over the @xmath39 other than eqn .",
    "( [ eqn_5 ] ) .",
    "the mechanism of tssr is illustrated in figure [ fig_1](b ) . to minimize the residual of the signal @xmath44 , @xmath45",
    ", whose another form is @xmath46 , we have @xmath47 here @xmath48 , @xmath49 , and @xmath50 is supposed to obey the gaussian distribution in the new space @xmath39 for sparse reconstruction . then we can obtain @xmath51 after measuring the signal @xmath2 , we can describe the intermediate representing coefficients @xmath52 as @xmath53 @xmath54 is a small constant , while in eqn .",
    "( [ eqn_6 ] ) @xmath55 @xmath56 . by introducing the residual signals to eqn .",
    "( [ eqn_8 ] ) and integrating eqn .",
    "( [ eqn_7 ] ) , we can obtain @xmath57 here @xmath58 is the same residual as @xmath38 in eqn .",
    "( [ eqn_5 ] ) and @xmath59 , for a small constant @xmath29 . with _",
    "bi - lipschitz _",
    "property @xcite , we can derive @xmath60 given @xmath61 .",
    "in fact , assume @xmath4 is @xmath6-sparse , from eqn .",
    "( [ eqn_9 ] ) , we have @xmath62 let @xmath63 , which is also a sparse vector , we have @xmath64 .",
    "recall that @xmath40 satisfies the rip of order @xmath6 with constant @xmath65 if @xmath66 we can derive the upper bound for @xmath67 using eqn .",
    "( [ eqn_11 ] ) and eqn .",
    "( [ eqn_12 ] ) .",
    "@xmath68 from eqn .",
    "( [ eqn_13 ] ) we have @xmath69 then the projection of @xmath52 onto @xmath70 as shown in figure [ fig_1](b ) is the approximation to that of @xmath2 onto @xmath40 .",
    "so the original problem @xmath13 is modified by @xmath71    since the @xmath72 is a constant , which has little effect on the solution of the object function , eqn .",
    "( [ eqn_15 ] ) then can be approximated by the following equation    @xmath73    @xmath74 and @xmath75 are small different constants .",
    "since the projection at the second stage is conducted on a much sparser space @xmath70 than @xmath5 in the first one , the runtime is greatly reduced .",
    "also the derivation above verifies the stability of tssr , which satisfies the rip condition under certain assumption .",
    "the space @xmath5 is approximated by the space @xmath39 as eqn .",
    "( [ eqn_6 ] ) indicates , and the @xmath40 and @xmath70 are designed as follows :    we can design @xmath40 to obey or nearly obey gaussian distribution with the training data through matrix computation .",
    "since gaussian distribution with bounded support is sub - gaussian @xcite , we can exploit the concentration property which only requires sub - gaussian .",
    "any distribution satisfying a concentration inequality @xcite will provide both the canonical rip and the universality with respect to a certain sparsity basis @xmath70 . here",
    "@xmath70 is represented as an identity @xmath76 matrix and @xmath77 , each @xmath6-dimensional subspace from @xmath78 is mapped to a unique @xmath6-dimensional hyperplane in @xmath79 . once @xmath80 has a sufficient amount of independence , the concentration of measure tends to be sub - gaussian in nature .",
    "then we can acquire signals @xmath81 that are sparse or compressible in practice .",
    "so @xmath70 is designed to approach an identity matrix after the first - stage implementation to have the property described above . by choosing @xmath6-dimensional subspaces spanned by sets of @xmath6 columns of @xmath40 , theorem 5.2 of @xcite",
    "establishes the rip for @xmath39 for each of the distributions .",
    "the transformation from the first stage to the second one can be motivated by concentration of measure principle @xcite and bi - lipschitz theory @xcite .",
    "further derivations are found in the appendix .",
    "the flowchart of tssr is described as follows ,    1 .",
    "input : a test signal @xmath33 , training signals @xmath82 , sparsity level @xmath6 , a measurement matrix @xmath48 with each column normalized .",
    "2 .   generate the features of @xmath83 over @xmath40 , using eqn .",
    "( [ eqn_6 ] ) , and form @xmath70 to approach an identity matrix , @xmath41 \\in\\mathbb{r}^{n\\times n}$ ] , in which @xmath42 expresses the @xmath43-th sparse vector .",
    "3 .   obtain the feature @xmath52 of @xmath2 over @xmath40 , using @xmath84 , where @xmath85 .",
    "4 .   reformulate the objective function @xmath13 as eqn .",
    "( [ eqn_16 ] ) .",
    "apply sparse coding to obtain representing coefficients @xmath86 over @xmath70 .",
    "since the @xmath70 is contained in a much sparser space than @xmath39 does , the implementing speed increases dramatically , while the solution @xmath4 approaches the original one solving @xmath13 .",
    "we give a brief description of the complexity analysis comparison between solving eqn .",
    "( [ eqn_2 ] ) and eqn .",
    "( [ eqn_16 ] ) . the computational cost for the former hinges on @xmath5 representing @xmath2 , while that for the latter lies in @xmath70 representing @xmath52 . for the @xmath5 is contained in the space generated by the original data , the implementation over it completely depends on the ability of coding .",
    "differently , the @xmath70 can be designed to approach an identity matrix or a sparse matrix which has maximum value at the diagonal and small number of minor values off - diagonal of it . to do the coding task with the same optimization method , the time to take one sweep over the columns of @xmath5 and @xmath70",
    "then need quite different cost .",
    "for instance , if we use cosamp @xcite for coding , we need @xmath87 and nearly @xmath88 flops for @xmath5 and @xmath70 respectively . as a result ,",
    "the recognition rate of the proposed tssr is much quicker than many one - stage methods .    to demonstrate the effectiveness of the method",
    ", we give an application to the method in the next section .",
    "an instance of tssr is shown in the following . in the first stage",
    ", we adopt the alb @xcite to generate sparse coefficients of samples over @xmath40 , which has merits of very small relative errors , and achieves the desired convergence with small number of iterations .",
    "@xmath40 can be formed by the training dataset @xmath83 which are normalized , as shown in figure [ fig_1 ] , and we can make @xmath40 obey or nearly obey the gaussian distribution .",
    "the sparse coefficients ( features )",
    "@xmath70 corresponds to training data @xmath83 , and forms a square matrix ( dictionary ) , in which each column is composed of the representatives of one sample . with sparse coding by alb for different datasets ,",
    "the generated @xmath70 is similar to an identity matrix or a sparse matrix which has maximum value at the diagonal and relatively small values off - diagonal of it .",
    "then we use cosamp @xcite to acquire the sparse coefficients @xmath52 of test data @xmath2 over @xmath40 and then represent @xmath52 over @xmath70 .",
    "the uniformity property of cosamp shows it can recover all signals given a fixed sampling matrix and the stability property guarantees its success in solving problem @xmath13 when the samples are contaminated with noise .",
    "cosamp performs signal estimation and residual update , and then generates @xmath6 ( sparsity level ) non - zero coefficients for @xmath52 @xcite .",
    "as section [ derivation ] shows , the problem @xmath13 then becomes searching for the sparsest solution on the basis @xmath70 as eqn .",
    "( [ eqn_16 ] ) shows .",
    "for classification we use the sparse representation classifier ( src ) @xcite which is known to have good robustness against signal corruption and noise .",
    "src minimizes the residuals between the test sample and training samples of different classes , and find the label of test sample which corresponds to the minimum residual .",
    "note that tssr structure can accommodate other sparse solvers as well as different measure matrices .",
    "we present experimental results on real data sets to demonstrate the efficiency and effectiveness of the proposed algorithm .",
    "all the experiments were carried out using matlab on a 3.0ghz machine with 2 g ram . the time to classify one image",
    "is averaged over 10 runs .",
    "the bold values indicate the best performances under specific condition .",
    "the parameter @xmath25 adopted in alb depends on the data @xcite , but a typical value is 1 to 10 times the estimate of @xmath89 . here",
    "we assume that an observed sample belongs to one certain class and can be well represented using samples from the same class .",
    "we present image recognition results with our tssr in comparison with several benchmark methods solving linear inverse problem : matching pursuit method ( cosamp ) , interior point method ( @xmath14-magic ) , active set method ( homotopy @xcite ) , proximal method ( fista ) , bregman method ( alb ) , and two - stage structure methods ( fsr and fsrm ) .",
    "the cmu pie database http : //vasc.ri.cmu.edu / idb / html / face/[http : //vasc.ri.cmu.edu",
    "/ idb / html / face/ ] contains 68 human subjects with 41,368 face images as a whole .",
    "we choose the five near frontal poses ( c05 , c07 , c09 , c27 , and c29 ) and use all the images under different illuminations and expressions , thus we get 170 images for each individual .",
    "each image is manually aligned to @xmath90 according to the eyes positions , with 256 gray levels per pixel .",
    "so each image can be represented by a 1024-dimensional vector in image space .",
    "no further preprocessing is done . for pie ,",
    "we randomly select 1 , 3 , 6 samples for training , and the other 30 for test ( i.e. , cases 1 - 30 , 3 - 30 , 6 - 30 ) to evaluate the performance . the results are shown in table 1 .",
    ".recognition accuracy and speed comparison using pie database .",
    "[ cols=\"^,^,^,^,^,^,^\",options=\"header \" , ]      +        we compare tssr with fsrm and fsr @xcite to evaluate their performance since they are all algorithms with two - stage structure and using sparsity level @xmath6 to represent coefficients . for fsr , the ksvd algorithm @xcite within it",
    "is constrained by the dimension of training vectors , and trivial solution would occur if the dimension of the training vector is larger than the size of the dictionary .",
    "so we performed all the experiments on the extended yaleb database , using half of samples for training and the rest for testing as fsr did @xcite .",
    "the speed comparison between fsr and tssr and the accuracy of all the algorithms under different @xmath6 can be seen in table 4 and figure [ fig_2 ] .",
    "we can find :    \\1 .",
    "tssr is more accurate than the other two methods .",
    "interestingly for the methods themselves , there is a trade - off between recognition accuracy and speed .",
    "each method obtains its highest accuracy at the predetermined value @xmath6 , i.e. , 13.2 .",
    "this may be due to precondition that the normalization of the original data satisfies the requirement of gaussian distribution , and eqn .",
    "( [ eqn_18 ] ) does its work .",
    "the tssr is the fastest among all the methods in most cases .",
    "as @xmath6 increases , the run time ratio of fsr to tssr becomes larger , which is from 1 ( @xmath91 ) to 61 ( @xmath92 ) .",
    "fsr needs a little less time than tssr when @xmath91 , which is much lower in accuracy than the others .",
    "fsrm runs faster than fsr for most cases .",
    "this is because ksvd adopted in fsr needs much time to generate coefficients for the second coding , while fsrm feeds the adopted @xmath14-magic with a much smaller input set in a two - stage process .",
    "meanwhile , tssr needs almost the same time to run for every @xmath6 , which is similar to fsrm .",
    "the reason is that tssr and fsrm have similar two - stage structure and both encode coefficients sparse enough which are generated in the first stage .",
    "extensive experiments on four biometrics databases have revealed some significant points , from which we can find the following :    \\1 .",
    "comparing with several classical methods for solving linear inverse problems , experiments on pie , ar and polyu palmprint databases show that tssr is an effective and efficient method .",
    "tssr almost uses the same time for different @xmath6 to run .",
    "this indicates the robustness of algorithm , which is not sensitive to the parameter @xmath6 .",
    "this happens as long as we can obtain a small number of good discriminative features in the first stage to do the second sparse coding .",
    "since tssr reaches the highest accuracy at about the predetermined value @xmath6 , we can first use eqn .",
    "( [ eqn_18 ] ) to get the initial @xmath6 for a dataset , then adjust @xmath6 in experiments to acquire the desired results .",
    "this may give us freedom and avoid trial and error .",
    "alb and fista can obtain quite high accuracy for all the three databases , which can be used in the situation when highest accuracy is desired and we can afford the time .",
    "tssr structure can accommodate different sparse solvers as well as different measure matrices , if @xmath40 is designed to obey or nearly obey gaussian distribution with the training data , while @xmath70 approximates an identity matrix .",
    "since the sparsity basis @xmath70 is approaching identity matrix but not exactly , this may have effect on recognition rate .",
    "so the design of @xmath70 is worth studying further .",
    "we have proposed a new method of two - stage sparse representation ( tssr ) for solving linear inverse problem .",
    "tssr makes the sparse approximation computationally tractable without sacrificing stable convergence .",
    "experimental results with image recognition indicate tssr is more efficient with comparable accuracy than several classic methods solving linear inverse problem .",
    "as the proposed method provides a good way to exploit the special structure of biometric datasets and is helpful for achieving rapid speed , it can be also applied to other recognition tasks .",
    "00    j.a .",
    "tropp , s.j .",
    "wright , computational methods for sparse solution of linear inverse problems , p ieee , 98 ( 2010 ) 948 - 958 .",
    "h. cheng , z. c. liu , l. yang , x.w .",
    ", sparse representation and learning in visual recognition : theory and applications , signal process , 93 ( 2013 ) 1408 - 1425 .    y.c .",
    "pati , r. rezaiifar , p.s .",
    "krishnaprasad , orthogonal matching pursuit : recursive function approximation with applications to wavelet decomposition , in : 27th asilomar conference on signals , systems and computers , 1993 , pp .",
    "40 - 44 .",
    "donoho , y. tsaig , i. drori , j.l .",
    "starck , sparse solution of underdetermined systems of linear equations by stagewise orthogonal matching pursuit , ieee t inform theory 58 ( 2012 ) 1094 - 1121 .",
    "d. needell , r. vershynin , signal recovery from incomplete and inaccurate measurements via regularized orthogonal matching pursuit , ieee j - stsp , 4 ( 2010 ) 310 - 316 .    m.d .",
    "plumbley , m. bevilacqua , sparse reconstruction for compressed sensing using stagewise polytope faces pursuit , in : 16th international conference on digital signal processing , 2009 , pp . 1 - 8",
    ".    d. needell , j.a .",
    "tropp , cosamp : iterative signal recovery from incomplete and inaccurate samples , appl comput harmon a , 26 ( 2009 ) 301 - 321 .",
    "wright , the interior - point revolution in optimization : history , recent developments , and lasting consequences , b am math soc , 42 ( 2005 ) 39 - 56 .    e.j .",
    "candes , j. romberg , @xmath14-magic : recovery of sparse signal via convex programming , ( 2005 ) pp . 19 .",
    "donoho , x. huo , uncertainty principles and ideal atomic decomposition , ieee t inform theory 47 ( 2001 ) 2845 - 2862 .    b. efron , t. hastie , i. johnstone , r. tibshirani , least angle regression , ann stat , 32 ( 2004 ) 407 - 451 .",
    "i. daubechies , m. defrise , c. de mol , an iterative thresholding algorithm for linear inverse problems with a sparsity constraint , commun pur appl math , 57 ( 2004 ) 1413 - 1457 .",
    "w. yin , stanley and goldfarb , donald and darbon , jerome , bregman iterative algorithms for @xmath11-minimization with applications to compressed sensing , siam j imaging sci , 1 ( 2008 ) 143 - 168 .",
    "a. beck , m. teboulle , a fast iterative shrinkage - thresholding algorithm for linear nverse problems , siam j imaging sci , 2 ( 2009 ) 183 - 202 .",
    "m. davenport , sub - gaussian random variables , url http://cnx.org/content/m37185/latest/ , 2011 .",
    "blumensath , t. and yaghoobi , m. and davies , m. e. , iterative hard thresholding and @xmath0 regularisation , icassp 07 , 877 - 880 .",
    "herrity , kk and gilbert , ac and tropp , ja , sparse approximation via iterative thresholding , assp ( 2006 ) , 624627 .",
    "zuo , wangmeng and meng , deyu and zhang , lei and feng , xiangchu and zhang , david , a generalized iterated shrinkage algorithm for non - convex sparse coding , iccv ( 2013 ) .",
    "w. yin , analysis and generalizations of the linearized bregman method , siam j imaging sci , 3 ( 2010 ) 856 - 877 .",
    "h. lee , a. battle , r. raina , and a. y. ng , efficient sparse coding algorithms , advances in neural information processing systems in nips2007 , 2007 .",
    "b. y. liu , l. yang , j. z. huang , and p. meer , robust and fast collaborative tracking with two stage sparse optimization , proc . of the 11th european conf . on computer vision , in : eccv2010 , 2010 , pp",
    ". 1 - 14 .",
    "r. he , b.g .",
    "zheng , and y.q .",
    "guo , two - stage sparse representation for robust recognition on large - scale database . in : aaai2010 , 2010 ,",
    "pp 475 - 480 .",
    "b. huang , s. ma , d. goldfarb , accelerated linearized bregman method , j sci comput arxiv:1106.5413v1 ( 2011 ) .",
    "huang , m .- h .",
    "yang , fast sparse representation with prototypes , in : cvpr 2010 , 2010 , pp .",
    "3618 - 3625 .",
    "m. aharon , m. elad , a. bruckstein , k - svd : design of dictionaries for sparse representation , in : proceedings of spars 2005 , 2005 , pp .",
    "9 - 12 .",
    "peng , j.w .",
    "li , fast sparse representation model for @xmath11-norm minimisation problem , electron lett , 48 ( 2012 ) 162 - 164 .",
    "candes , j.k .",
    "romberg , t. tao , stable signal recovery from incomplete and inaccurate measurements , commun pur appl math , 59 ( 2006 ) 1207 - 1223 .",
    "davenport , p.t .",
    "boufounos , m.b .",
    "wakin , r.g .",
    "baraniuk , signal processing with compressive measurements , ieee j - stsp , 4 ( 2010 ) 445 - 460 .",
    "m. talagrand , a new look at independence , ann probab , 24 ( 1996 ) 1 - 34 .",
    "j. wright , a. yang , a. ganesh , s. sastry , y. ma , robust face recognition via sparse representation , ieee t pattern anal , 31 ( 2009 ) 210 - 227 .",
    "georghiades , p.n .",
    "belhumeur , d. kriegman , from few to many : illumination cone models for face recognition under variable lighting and pose , ieee t pattern anal , 23 ( 2001 ) 643 - 660 .",
    "a.m. martinez , a.c .",
    "kak , pca versus lda , ieee t pattern anal , 23 ( 2001 ) 228 - 233 .",
    "d. zhang , w .- k .",
    "kong , j. you , m. wong , online palmprint identification , ieee t pattern anal , 25 ( 2003 ) 1041 - 1050 .",
    "donoho , y. tsaig , i. drori , j .- l .",
    "starck , fast solution of @xmath11-norm minimization problems when the solution may be sparse , ieee t inform theory , 54 ( 2008 ) 4789 - 4812 .",
    "lai , w. yin , augmented @xmath11 and nuclear - norm models with a globally linearly convergent algorithm , corr , arxiv:1201.4615 ( 2012 ) .",
    "t. strohmer , r.w.h .",
    "jr . , grassmannian frames with applications to coding and communication , appl comput harmon a , 14 ( 2003 ) 257 - 275 .",
    "benedetto , john j. , kolesar , j. d , geometric properties of grassmannian frames for r2 and r3 , eurasip j. adv .",
    "proc . , doi:10.1155/asp/2006/49850 , 2006 ( 2006 ) .",
    "candes , the restricted isometry property and its implications for compressed sensing , c. r. acad .",
    "paris ser .",
    ", 346 ( 2008 ) 589 - 592 .",
    "baraniuk , m.a .",
    "davenport , r.a .",
    "devore , m.b .",
    "wakin , a simple proof of the restricted isometry property for random matrices , constr approx , 28 ( 2008 ) 253 - 263 .",
    "suppose we have a signal @xmath93 , and a matrix @xmath8 in which the signal is presented .",
    "we want to obtain a sparse set of coefficients ( sparsity level k ) to represent the signal .",
    "assume that an observed sample belongs to one certain class and can be well represented using samples from the same class .",
    "we say that the matrix @xmath5 satisfies rip of order @xmath6 with constant @xmath94 if @xmath95    rip is a measure of closeness to an identity matrix for sparse vectors .",
    "define that @xmath96 selects @xmath6 columns from @xmath5 , the rip then suggests that every @xmath96 should behave like an isometry - not changing the length of the vector it multiplies .",
    "if @xmath97 is small enough , the norm @xmath98 can be constrained to a small enough value , in which case a sparse representation can be stably determined @xcite .",
    "we can generate random @xmath99 matrices @xmath5 by choosing the entries @xmath100 as independent and identically distributed ( i.i.d . ) random variables . for any @xmath101 ,",
    "the random variable @xmath102 is strongly concentrated about its expected value , that is , @xmath103 where the probability is taken over @xmath5 and @xmath104 is a constant , for any @xmath105 . this is called concentration of measure inequalities .    any distribution",
    "satisfying a concentration inequality will provide both the canonical rip and the universality with respect to a certain sparsifying basis @xmath40 . here",
    "@xmath40 is represented as a @xmath76 identity matrix and @xmath77 , with which we can acquire signals @xmath106 that are sparse or compressible in practice . by choosing @xmath6-dimensional subspaces spanned by sets of @xmath6 columns of @xmath40 , theorem 5.2 of @xcite",
    "establishes the rip for @xmath107 for each of the distributions .",
    "see @xcite for more details on sub - gaussian random variables .",
    "if the matrix @xmath107 satisfies the rip of order @xmath37 , then @xmath5 is a @xmath108-stable embedding of @xmath109 , where @xmath110 .",
    "we would require that @xmath107 satisfies the rip , and thus bound the error @xmath111 introduced by the embedding .",
    "see the appendix b.",
    "* theorem 1 * suppose that @xmath5 satisfies the rip of order 2@xmath6 with isometry constant @xmath112 .",
    "given measurements of the form @xmath113 , where @xmath114 , the solution @xmath115 to @xmath116 obeys @xmath117 where @xmath118 , @xmath119 is denoted as the vector @xmath4 with all but the @xmath6-largest entries set to zero .",
    "we can restate the rip in a more general form .",
    "let @xmath120 and @xmath121 be given , we say a mapping @xmath5 is a @xmath108-stable embedding of @xmath122 if @xmath123 for all @xmath124 , @xmath125 .",
    "a mapping satisfying the property is commonly called _ bi - lipschitz _ @xcite .    * lemma 1 @xcite : * let @xmath126 and @xmath127 be sets of points in @xmath79 .",
    "fix @xmath128 .",
    "let @xmath5 be an @xmath99 random matrix with _",
    "_ entries chosen from a distribution satisfying eqn .",
    "( [ eqn_21 ] ) .",
    "if @xmath129 then with probability exceeding @xmath130 , @xmath5 is a @xmath108-stable embedding of @xmath122 .",
    "with lemma 1 , we can derive that @xmath5 is a @xmath108-stable embedding of @xmath131 ."
  ],
  "abstract_text": [
    "<S> there are a large number of methods for solving under - determined linear inverse problem . </S>",
    "<S> many of them have very high time complexity for large datasets . we propose a new method called two - stage sparse representation ( tssr ) to tackle this problem . </S>",
    "<S> we decompose the representing space of signals into two parts , the measurement dictionary and the sparsifying basis . </S>",
    "<S> the dictionary is designed to approximate a sub - gaussian distribution to exploit its concentration property . </S>",
    "<S> we apply sparse coding to the signals on the dictionary in the first stage , and obtain the training and testing coefficients respectively . </S>",
    "<S> then we design the basis to approach an identity matrix in the second stage , to acquire the restricted isometry property ( rip ) and universality property . </S>",
    "<S> the testing coefficients are encoded over the basis and the final representing coefficients are obtained . </S>",
    "<S> we verify that the projection of testing coefficients onto the basis is a good approximation of the signal onto the representing space . </S>",
    "<S> since the projection is conducted on a much sparser space , the runtime is greatly reduced . for concrete realization </S>",
    "<S> , we provide an instance for the proposed tssr . </S>",
    "<S> experiments on four biometrics databases show that tssr is effective and efficient , comparing with several classical methods for solving linear inverse problem .    </S>",
    "<S> linear inverse problem , sparse representation , two - stage structure , mutual coherence , bi - lipschitz , concentration of measure </S>"
  ]
}