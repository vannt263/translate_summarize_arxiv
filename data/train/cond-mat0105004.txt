{
  "article_text": [
    ""
  ],
  "abstract_text": [
    "<S> we present an exact solution for the dynamics of on - line hebbian learning in neural networks , with restricted and unrealizable training sets . </S>",
    "<S> in contrast to other studies on learning with restricted training sets , unrealizability is here caused by structural mismatch , rather than data noise : the teacher machine is a perceptron with a reversed wedge - type transfer function , while the student machine is a perceptron with a sigmoidal transfer function . </S>",
    "<S> we calculate the glassy dynamics of the macroscopic performance measures , training error and generalization error , and the ( non - gaussian ) student field distribution . </S>",
    "<S> our results , which find excellent confirmation in numerical simulations , provide a new benchmark test for general formalisms with which to study unrealizable learning processes with restricted training sets .    on - line learning processes in artificial neural networks have been studied using statistical mechanical techniques for about a decade now ( see e.g. @xcite for reviews ) . </S>",
    "<S> initially , most dynamical studies were restricted to the regime where the number of training examples is larger than the number of learning steps , since this generally leads to gaussian field distributions and relatively simple non - glassy dynamics . in practical situations , however , it is usually difficult to acquire large training sets , and one is therefore often forced to recycle the data in the training set . </S>",
    "<S> the latter situation , characterized by the presence of disorder ( the composition of the training set ) and non - trivial dynamics , was studied in e.g. @xcite ( for binary weights ) , and in @xcite ( for continuous weights ) . </S>",
    "<S> these studies generally involve approximations at some stage . </S>",
    "<S> this motivated @xcite , where it was shown how for the special case of on - line hebbian learning the dynamics can be solved exactly ( for restricted training sets ) , providing an excellent benchmark for general theories and approximation schemes . </S>",
    "<S> some of the studies mentioned above involved learning from restricted but unrealizable training sets , where it is impossible for the student to achieve perfect performance , even if an infinitely large training set had been available . </S>",
    "<S> this could result from corruption by noise of realizable data ( as in e.g. @xcite ) , or from structural mismatch between teacher and student . </S>",
    "<S> a typical toy model to realize the latter situation is obtained by using a perceptron with a reversed wedge transfer function as a teacher machine to train an ordinary perceptron @xcite ( note : there is also a relation between simple perceptrons with reversed wedge transfer functions and the so - called parity machines ) . since all dynamical studies with restricted but unrealizable training sets </S>",
    "<S> have so far been carried out only for the data noise scenario , it would be of considerable interest to investigate exactly solvable models with restricted training sets but unrealizability due to structural mismatch . in this letter </S>",
    "<S> , we carry out such a study : we solve the dynamics of on - line hebbian learning from unrealizable restricted training sets , for a teacher - student scenario where teacher and student have different transfer functions ( a reverse - wedge and a sigmoidal one , respectively ) .    </S>",
    "<S> we investigate on - line learning in a ordinary student perceptron @xmath0 ( whose weight vector is denoted by @xmath1 ) , which tries to learn a task defined by a teacher perceptron @xmath2 ( whose weight vector is denoted by @xmath3 ) . </S>",
    "<S> the teacher is equipped with a reversed wedge transfer function , i.e. @xmath4 $ ] where @xmath5 and @xmath6 is the input vector , whereas @xmath7 $ ] with @xmath8 . </S>",
    "<S> the teacher s weight vector @xmath3 is normalized such that @xmath9 , with @xmath10 for each @xmath11 . </S>",
    "<S> it is clear that in the limits @xmath12 and @xmath13 ( where @xmath14 characterizes the width of the reverse wedge ) the task becomes realizable for the student , since @xmath15 $ ] and @xmath16 $ ] . </S>",
    "<S> we define the conventional order parameters @xmath17\\,\\equiv\\,\\mbox{\\boldmath $ j$}^{2}$ ] and @xmath18\\,\\equiv\\ , \\mbox{\\boldmath $ b$ } \\cdot \\mbox{\\boldmath $ j$}$ ] . </S>",
    "<S> one of the main quantities of interest is the generalization error @xmath19 , the probability of disagreement between teacher and student for input vectors taken randomly from the _ full _ set of all possible inputs : @xmath20 \\rangle_{\\mbox{\\boldmath $ \\xi$ } } = \\int_{0}^{a}\\!du~{\\rm erf}[r^{+}(u ) ] + \\int_{a}^{\\infty}\\!du~{\\rm erf}[r^{-}(u ) ] , \\label{gene}\\ ] ] where @xmath21 , @xmath22 , @xmath23 $ ] is the step function , and @xmath24 denotes averaging over all @xmath25 . </S>",
    "<S> it was shown in @xcite that the optimal normalized overlap @xmath26 ( giving the smallest value of the generalization error ) equals @xmath27 as long as the reversed wedge parameter @xmath14 is greater than @xmath28 ; @xmath29 suddenly changes from @xmath27 to @xmath30 at @xmath31 . </S>",
    "<S> for this model system , we use the following on - line hebbian learning rule @xmath32 where @xmath33 indicates the learning step , and @xmath34 and @xmath35 represent the learning rate and the weight decay , respectively . </S>",
    "<S> the student learns from data picked randomly from the restricted training set @xmath36 .    to calculate macroscopic physical observables , </S>",
    "<S> averaged over the disorder ( the composition of the training set ) at any time , we need to distinguish between two averaging procedures @xcite . </S>",
    "<S> the first is the average over all possible ` paths ' @xmath37 defining the actual sampling order from the training set : @xmath38 the second is averaging over all training sets : @xmath39\\rangle_{\\rm sets } & \\equiv & 2^{-pn } \\sum_{\\mbox{\\boldmath $ \\xi$}^{1 } } { \\cdots } \\sum_{\\mbox{\\boldmath $ \\xi$}^{p } } \\sum_{t_{a}^{1},{\\cdots } , t_{a}^{p } } \\prod_{\\mu } p(t_{a}^{\\mu}|\\mbox{\\boldmath $ \\xi$}^{\\mu } )   \\nonumber \\\\ \\mbox { } & & \\times~f [ ( \\mbox{\\boldmath $ \\xi$}^{1 } , t_{a}^{1 } ) , \\cdots , ( \\mbox{\\boldmath $ \\xi$}^{p } , t_{a}^{p } ) ] \\label{sets_ave}\\end{aligned}\\ ] ] the key to the full solvability of the present model , as in @xcite , is the fact that allows us to write @xmath40 ( the student s weight vector at @xmath41-th step ) in explicit form as @xmath42 where @xmath43 . </S>",
    "<S> the above averaging procedures can now be carried out exactly .       in the limit of @xmath61 , for @xmath62 ( ) , @xmath72 ( ) and @xmath73 ( ) . </S>",
    "<S> we chose @xmath74 . </S>",
    "<S> right : the corresponding normalized overlap @xmath26 which gives the generalization error in the left figure . </S>",
    "<S> the best possible values for the generalization error and the optimal normalized overlap are shown by thick lines . </S>",
    "<S> , title=\"fig : \" ]   in the limit of @xmath61 , for @xmath62 ( ) , @xmath72 ( ) and @xmath73 ( ) . </S>",
    "<S> we chose @xmath74 . </S>",
    "<S> right : the corresponding normalized overlap @xmath26 which gives the generalization error in the left figure . </S>",
    "<S> the best possible values for the generalization error and the optimal normalized overlap are shown by thick lines . </S>",
    "<S> , title=\"fig : \" ]       generated during on - line hebbian learning , from a teacher with a reversed wedge of width @xmath104 and for @xmath74 , @xmath105 and @xmath72 , at times @xmath106 . </S>",
    "<S> solid lines : the theoretical result ( [ pt ] ) . </S>",
    "<S> histograms : results obtained via computer simulations for systems of size @xmath107 . </S>",
    "<S> , title=\"fig : \" ]   generated during on - line hebbian learning , from a teacher with a reversed wedge of width @xmath104 and for @xmath74 , @xmath105 and @xmath72 , at times @xmath106 . </S>",
    "<S> solid lines : the theoretical result ( [ pt ] ) . </S>",
    "<S> histograms : results obtained via computer simulations for systems of size @xmath107 . </S>",
    "<S> , title=\"fig : \" ]   generated during on - line hebbian learning , from a teacher with a reversed wedge of width @xmath104 and for @xmath74 , @xmath105 and @xmath72 , at times @xmath106 . </S>",
    "<S> solid lines : the theoretical result ( [ pt ] ) . </S>",
    "<S> histograms : results obtained via computer simulations for systems of size @xmath107 . </S>",
    "<S> , title=\"fig : \" ]   generated during on - line hebbian learning , from a teacher with a reversed wedge of width @xmath104 and for @xmath74 , @xmath105 and @xmath72 , at times @xmath106 . </S>",
    "<S> solid lines : the theoretical result ( [ pt ] ) . </S>",
    "<S> histograms : results obtained via computer simulations for systems of size @xmath107 . </S>",
    "<S> , title=\"fig : \" ]    it follows from that @xmath87 is a symmetric function of @xmath108 , for all times and all values of the reversed wedge width @xmath14 . in the special cases @xmath109 and @xmath110 ( where the task becomes realizable ) we find our result ( [ pt ] ) reducing to that of @xmath111 : @xmath112 with @xmath113 for @xmath109 , and @xmath114 for @xmath110 . </S>",
    "<S> this is consistent with @xcite , where the parameter @xmath115 denoted the probability that a teacher output was corrupted by noise . </S>",
    "<S> here we find that , if the width of the reversed wedge is zero , the transfer function of the teacher is the inverse of that of the student , and the the output of the teacher can be regarded as a noisy output with flip probability @xmath114 . in contrast , in the general case @xmath116 , equation ( [ pt ] ) shows that the effect of structural non - realizability can not be described by an ` effective ' output noise . in we plot </S>",
    "<S> @xmath90 as given by equation ( [ pt ] ) , for @xmath104 , @xmath74 and @xmath72 , at different points in time , and we compare the result to the corresponding observations in numerical simulations ( histograms ) . </S>",
    "<S> one clearly observes how @xmath90 evolves from a gaussian distribution at @xmath117 to a manifestly non - gaussian one . </S>",
    "<S> finally , we calculate the training error @xmath118 , which measures the average fraction of errors made by the student on inputs taken from the training set . </S>",
    "<S> it is given by @xmath119 p_{t}(x|y , t_a)p(t_a|y).\\end{aligned}\\ ] ] by using and we can obtain the explicit form of @xmath118 as @xmath120 \\nonumber \\\\ & & + 4 { \\rm e}^{-\\frac{a^2}{2 } } \\int\\ ! \\frac{d\\hat{x}}{2\\pi \\hat{x } } { \\rm e}^{-\\frac{q}{2}\\hat{x}^{2 } + \\chi_{\\rm r}(\\hat{x } ) } { \\cos}(\\chi_{\\rm i}(\\hat{x } ) ) \\int_{0}^{\\hat{x}r}\\!\\ ! </S>",
    "<S> \\frac{d\\lambda}{\\sqrt{2\\pi } } { \\rm e}^{\\frac{\\lambda^2}{2 } } \\cos ( a\\lambda ) . </S>",
    "<S> \\label{eq : et}\\end{aligned}\\ ] ] in , we plot both the training error ( [ eq : et ] ) and the generalization error ( [ gene ] ) for four different values of the width @xmath14 of the teacher s reversed wedge , viz . </S>",
    "<S> @xmath121 .     and </S>",
    "<S> generalization errors @xmath19 as functions of time , for different values of @xmath79 . in all cases </S>",
    "<S> @xmath74 and @xmath72 , with initial conditions @xmath122 and @xmath123 . from the upper left panel to the lower right panel : @xmath124 and @xmath125 </S>",
    "<S> . in each panel , </S>",
    "<S> the upper three solid lines indicate our theoretical results of @xmath60 , together with the corresponding results of computer simulations : ( @xmath105 ) , ( @xmath126 ) and ( @xmath127 ) . </S>",
    "<S> the lower three lines are theoretical results for @xmath118 , compared to the results of computer simulations , with ( @xmath105 ) , ( @xmath126 ) and ( @xmath127 ) . </S>",
    "<S> all simulations are carried out for systems of size @xmath128.,title=\"fig : \" ]   and generalization errors @xmath19 as functions of time , for different values of @xmath79 . in all cases @xmath74 and @xmath72 , with initial conditions @xmath122 and @xmath123 . </S>",
    "<S> from the upper left panel to the lower right panel : @xmath124 and @xmath125 . in each panel </S>",
    "<S> , the upper three solid lines indicate our theoretical results of @xmath60 , together with the corresponding results of computer simulations : ( @xmath105 ) , ( @xmath126 ) and ( @xmath127 ) . </S>",
    "<S> the lower three lines are theoretical results for @xmath118 , compared to the results of computer simulations , with ( @xmath105 ) , ( @xmath126 ) and ( @xmath127 ) . </S>",
    "<S> all simulations are carried out for systems of size @xmath128.,title=\"fig : \" ]   and generalization errors @xmath19 as functions of time , for different values of @xmath79 . in all cases @xmath74 and @xmath72 , with initial conditions @xmath122 and @xmath123 . from the upper left panel to the lower right panel : @xmath124 and @xmath125 . in each panel , </S>",
    "<S> the upper three solid lines indicate our theoretical results of @xmath60 , together with the corresponding results of computer simulations : ( @xmath105 ) , ( @xmath126 ) and ( @xmath127 ) . </S>",
    "<S> the lower three lines are theoretical results for @xmath118 , compared to the results of computer simulations , with ( @xmath105 ) , ( @xmath126 ) and ( @xmath127 ) . </S>",
    "<S> all simulations are carried out for systems of size @xmath128.,title=\"fig : \" ]   and generalization errors @xmath19 as functions of time , for different values of @xmath79 . in all cases @xmath74 and @xmath72 , with initial conditions @xmath122 and @xmath123 . from the upper left panel to the lower right panel : @xmath124 and @xmath125 . in each panel , </S>",
    "<S> the upper three solid lines indicate our theoretical results of @xmath60 , together with the corresponding results of computer simulations : ( @xmath105 ) , ( @xmath126 ) and ( @xmath127 ) . </S>",
    "<S> the lower three lines are theoretical results for @xmath118 , compared to the results of computer simulations , with ( @xmath105 ) , ( @xmath126 ) and ( @xmath127 ) . </S>",
    "<S> all simulations are carried out for systems of size @xmath128.,title=\"fig : \" ]    in all case we find the theoretical results and the computer simulations to be in excellent agreement . in the limit @xmath129 </S>",
    "<S> we also observe that the asymptotic values of both @xmath60 and @xmath118 indeed approach @xmath130 ( see ) for increasing @xmath79 , as it should .    in conclusion , </S>",
    "<S> in this letter we have solved the dynamics of on - line hebbian learning with structurally unrealizable restricted training sets exactly , for the case where a standard perceptron is being trained by a teacher perceptron with a reversed wedge transfer function . </S>",
    "<S> although our solution applies only to hebbian learning ( as did the one in @xcite ) , we believe that our results provide a valuable new benchmark against which to test ( approximations made in ) more general formalisms , such as generating functional analysis @xcite , dynamical replica theory @xcite or the cavity method @xcite . + the authors would like to thank king s college london ( ji ) and the tokyo institute of technology ( accc ) for their hospitality . </S>"
  ]
}