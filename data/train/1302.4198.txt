{
  "article_text": [
    "classical time series analysis is based on the assumption of stationarity . however , many time series exhibit a nonstationary behavior .",
    "examples come from fields as diverse as finance , sound analysis and neuroscience .",
    "one way to model nonstationary behavior is provided by the theory of locally stationary processes introduced by dahlhaus ; cf .",
    "@xcite and  @xcite . intuitively speaking ,",
    "a process is locally stationary if over short periods of time ( i.e. , locally in time ) it behaves in an approximately stationary way .",
    "so far , locally stationary models have been mainly considered within a parametric context .",
    "usually , parametric models are analyzed in which the coefficients are allowed to change smoothly over time .",
    "there is a considerable amount of papers that deal with time series models with time - varying coefficients .",
    "dahlhaus et al .",
    "@xcite , for example , study wavelet estimation in autoregressive models with time - dependent parameters .",
    "dahlhaus and subba rao  @xcite analyze a class of arch models with time - varying coefficients .",
    "they propose a kernel - based quasi - maximum likelihood method to estimate the parameter functions ; a kernel - based normalized - least - squares method is suggested by fryzlewicz et al .",
    "@xcite . hafner and linton  @xcite",
    "provide estimation theory for a multivariate garch model with a time - varying unconditional variance .",
    "finally , a  diffusion process with a time - dependent drift and diffusion function is investigated in koo and linton  @xcite .    in this paper",
    ", we introduce a nonparametric framework which can be regarded as a natural extension of time series models with time - varying coefficients . in its most general form , the model is given by @xmath0 with @xmath1 = 0 $ ] , where @xmath2 and @xmath3 are random variables of dimension  @xmath4 and @xmath5 , respectively .",
    "the model variables are assumed to be locally stationary and the regression function as a whole is allowed to change smoothly over time . as usual in the literature on locally stationary processes ,",
    "the function @xmath6 does not depend on real time @xmath7 but rather on rescaled time @xmath8 .",
    "this goes along with the model variables forming a triangular array instead of a sequence . throughout the",
    ", we stick to an intuitive concept of local stationarity .",
    "a technically rigorous definition is given in section  [ section - loc - stat ] .",
    "there is a wide range of interesting nonlinear time series models that fit into the general framework ( [ model ] ) .",
    "an important example is the nonparametric autoregressive model @xmath9 with @xmath10=0 $ ] , which is analyzed in section  [ section - tvnar ] .",
    "as will be seen there , the process defined in ( [ model - ar ] ) is locally stationary and strongly mixing under suitable conditions on the function @xmath6 and the error terms @xmath11 .",
    "note that independently of the present work , kristensen  @xcite has developed results on local stationarity of the process given in ( [ model - ar ] ) under a set of assumptions similar to ours .    in section",
    "[ section - estimation ] , we develop estimation theory for the nonparametric regression function in the general framework ( [ model ] ) .",
    "as described there , the regression function is estimated by nonparametric kernel methods .",
    "we provide a complete asymptotic theory for our estimates .",
    "in particular , we derive uniform convergence rates and an asymptotic normality result . to do so",
    ", we split up the estimates into a variance part and a bias part . in order to control the variance part",
    ", we generalize results on uniform convergence rates for kernel estimates as provided , for example , in bosq  @xcite , masry @xcite and hansen  @xcite .",
    "the locally stationary behavior of the model variables also changes the asymptotic analysis of the bias part .",
    "in particular , it produces an additional bias term which can be regarded as measuring the deviation from stationarity .",
    "even though model ( [ model ] ) is theoretically interesting , it has an important drawback . estimating the time - varying regression function in ( [ model ] ) suffers from an even more severe curse of dimensionality problem than in the standard , strictly stationary setting with a time - invariant regression function .",
    "the reason is that in model ( [ model ] ) , we fit a fully nonparametric function @xmath12 locally around _",
    "each _ rescaled time point @xmath13 . compared to the standard case",
    ", this means that we additionally smooth in time direction and thus increase the dimensionality of the estimation problem by one .",
    "this makes the procedure even more data consuming than in the standard setting and thus infeasible in many applications .    in order to countervail this severe curse of dimensionality",
    ", we impose some structural constraints on the regression function in ( [ model ] ) .",
    "in particular , we consider additive models of the form @xmath14 with @xmath15 and @xmath16 = 0 $ ] . in section",
    "[ section - add ] , we will show that the component functions of this model can be estimated with two - dimensional nonparametric convergence rates , no matter how large the dimension @xmath5 . in order to do so , we extend the smooth backfitting approach of mammen et al .",
    "@xcite to our setting .",
    "heuristically speaking , a process @xmath17 is locally stationary if it behaves approximately stationary locally in time .",
    "this intuitive concept can be turned into a rigorous definition in different ways .",
    "one way is to require that locally around each rescaled time point @xmath13 , the process @xmath18 can be approximated by a stationary process @xmath19 in a stochastic sense ; cf .",
    ", for example , dahlhaus and subba rao  @xcite .",
    "this idea also underlies the following definition .",
    "[ def - loc - stat ] the process @xmath18 is locally stationary if for each rescaled time point @xmath20 $ ] there exists an associated process @xmath21 with the following two properties :    @xmath22 is strictly stationary with density @xmath23 ;    it holds that @xmath24 where @xmath25 is a process of positive variables satisfying @xmath26 < c$ ] for some @xmath27 and @xmath28 independent of @xmath13 , @xmath7 , and @xmath29 .",
    "@xmath30 denotes an arbitrary norm on @xmath31 .",
    "since the @xmath32th moments of the variables @xmath33 are uniformly bounded , it holds that @xmath34 . as a consequence of the above definition",
    ", we thus have @xmath35 the constant @xmath32 can be regarded as a measure of how well @xmath3 is approximated by @xmath36 : the larger @xmath32 can be chosen , the less mass is contained in the tails of the distribution of @xmath33 . thus ,",
    "if @xmath32 is large , then the bound @xmath37 will take rather moderate values for most of the time . in this sense ,",
    "the bound and thus the approximation of @xmath3 by @xmath36 is getting better for larger @xmath32 .",
    "in this section , we examine a large class of nonlinear autoregressive processes with a time - varying regression function that fit into the general framework ( [ model ] ) .",
    "we show that these processes are locally stationary and strongly mixing under suitable conditions on the model components . to shorten notation , we repeatedly make use of the following abbreviation : for any array of variables @xmath38 , we let @xmath39 for @xmath40 .",
    "we call an array @xmath41 a time - varying nonlinear autoregressive ( tvnar ) process if @xmath42 evolves according to the equation @xmath43 a tvnar process is thus an autoregressive process of form ( [ model - ar ] ) with errors @xmath44 . in the above definition , @xmath45 and @xmath46",
    "are smooth functions of rescaled time @xmath13 and @xmath47 .",
    "we stipulate that for @xmath48 , @xmath49 and @xmath50 .",
    "analogously , we set @xmath51 and @xmath52 for @xmath53 . furthermore , the variables @xmath54 are assumed to be i.i.d . with mean zero . for each @xmath55",
    ", we additionally define the associated process @xmath56 by @xmath57 where the rescaled time argument of the functions @xmath6 and @xmath58 is fixed at @xmath13 .",
    "as stipulated above , the functions @xmath6 and @xmath58 in ( [ tvnar ] ) do not change over time for @xmath59 .",
    "put differently , @xmath60 for all @xmath59 .",
    "we can thus assume that @xmath61 for @xmath62 .",
    "consequently , if there exists a process @xmath63 that satisfies the system of equations ( [ nar ] ) for @xmath64 , then this immediately implies the existence of a tvnar process @xmath65 satisfying ( [ tvnar ] ) . as will turn out , under appropriate conditions",
    "there exists a strictly stationary solution @xmath66 to ( [ nar ] ) for each @xmath55 , in particular for @xmath67 .",
    "we can thus take for granted that the tvnar process @xmath65 defined by ( [ tvnar ] ) exists .",
    "before we turn to the analysis of the tvnar process , we compare it to the framework of zhou and wu  @xcite and zhou  @xcite .",
    "their model is given by the equation @xmath68 , where @xmath69 with i.i.d .",
    "variables @xmath54 and @xmath70 is a measurable function . in their theory , the variables @xmath71 play the role of a stationary approximation at @xmath20 $ ] . under suitable assumptions",
    ", we can iterate equation ( [ nar ] ) to obtain that @xmath72 for some measurable function  @xmath73 . note , however , that @xmath74 in general .",
    "this is due to the fact that when iterating ( [ nar ] ) , we use the same functions @xmath12 and @xmath75 in each step .",
    "in contrast to this , different functions show up in each step when iterating the tvnar variables  @xmath42 .",
    "thus , the relation between the tvnar process @xmath65 and the approximations @xmath76 is in general different from that between the processes @xmath77 and @xmath78 in the setting of zhou and wu .",
    "we now list some conditions which are sufficient to ensure that the tvnar process is locally stationary and strongly mixing . to start with",
    ", the function @xmath6 is supposed to satisfy the following conditions :    [ m1 ] @xmath6 is absolutely bounded by some constant @xmath79 .",
    "[ m2 ] @xmath6 is lipschitz continuous with respect to rescaled time @xmath13 , that is , there exists a constant @xmath80 such that @xmath81 for all @xmath47 .",
    "[ m3 ] @xmath6 is continuously differentiable with respect to @xmath82 .",
    "the partial derivatives @xmath83 have the property that for some @xmath84 , @xmath85 an exact formula for the bound @xmath86 is given in ( [ delta ] ) in appendix  [ appa ] .    the function @xmath58 is required to fulfill analogous assumptions .",
    "[ sigma1 ] @xmath58 is bounded by some constant @xmath87 from above and by some constant @xmath88 from below , that is , @xmath89 for all @xmath13 and  @xmath82 .",
    "[ sigma2 ] @xmath58 is lipschitz continuous with respect to rescaled time @xmath13 .",
    "[ sigma3 ] @xmath58 is continuously differentiable with respect to @xmath82 .",
    "the partial derivatives @xmath90 have the property that for some @xmath84,@xmath91 for all @xmath55 and @xmath92 .",
    "finally , the error terms are required to have the following properties .",
    "[ e1 ] the variables @xmath54 are i.i.d .",
    "with @xmath93 = 0 $ ] and @xmath94 for some . moreover , they have an everywhere positive and continuous density  @xmath95 .",
    "[ e2 ] the density @xmath95 is bounded and lipschitz , that is , there exists a constant @xmath80 such that @xmath96 for all @xmath97 .    to show that the tvnar process is strongly mixing",
    ", we additionally need the following condition on the density of the error terms :    [ e3 ] let @xmath98 , @xmath99 be any constants with @xmath100 and @xmath101 .",
    "the density @xmath95 fulfills the condition @xmath102z + d_1\\bigr ) - f_{\\varepsilon } ( z)\\bigr| \\,dz \\le c_{d_0,d_1 } \\bigl ( d_0 + |d_1| \\bigr)\\ ] ] with @xmath103 only depending on the bounds @xmath104 and @xmath105 .",
    "we shortly give some remarks on the above conditions :    our set of assumptions can be regarded as a strengthening of the assumptions needed to show geometric ergodicity of nonlinear ar processes of the form @xmath106 .",
    "the main assumption in this context requires the functions @xmath6 and @xmath58 not to grow too fast outside a large bounded set .",
    "more precisely , it requires them to be dominated by linear functions with sufficiently small slopes ; cf .",
    "tjstheim @xcite , bhattacharya and lee  @xcite , an and huang  @xcite or chen and chen  @xcite , among others .",
    "[ m3 ] and  [ sigma3 ] are very close in spirit to this kind of assumption .",
    "they restrict the growth of @xmath6 and @xmath58 by requiring the derivatives of these functions to be small outside a large bounded set .    if we replace  [ m3 ] and  [ sigma3 ] with the stronger assumption that the partial derivatives @xmath107 and @xmath108 are globally bounded by some sufficiently small number",
    "@xmath109 , then some straightforward modifications allow us to dispense with the boundedness assumptions [ m1 ] and  [ sigma1 ] in the local stationarity and mixing proofs .",
    "condition  [ m3 ] implies that the derivatives @xmath110 are absolutely bounded .",
    "hence , there exists a constant @xmath111 such that @xmath112 for all @xmath113 and @xmath114 .",
    "similarly ,  [ sigma3 ] implies that the derivatives @xmath115 are absolutely bounded by some constant @xmath116 .",
    "as already noted ,  [ e3 ] is only needed to prove that the tvnar process is strongly mixing .",
    "it is , for example , fulfilled for the class of bounded densities @xmath95 whose first derivative @xmath117 is bounded , satisfies @xmath118 and declines monotonically to zero for values @xmath119 for some constant @xmath120 ; see also section  3 in fryzlewicz and subba rao  @xcite who work with assumptions closely related to  [ e3 ] .",
    "we now show that the tvnar process is locally stationary and strongly mixing under the assumptions listed above .",
    "in addition , we will see that the auxiliary processes @xmath76 have densities that vary smoothly over rescaled time @xmath13 . as will turn out , these three properties are central for the estimation theory developed in sections [ section - estimation ] and  [ section - add ] .",
    "the first theorem summarizes some properties of the tvnar process and of the auxiliary processes @xmath76 that are needed to prove the main results .",
    "[ theo - stat ] let  [ m1][m3 ] ,  [ sigma1][sigma3 ] and [ e1 ] be fulfilled . then",
    ":    [ theo - stat-1 ] for each @xmath55 , the process @xmath121 has a strictly stationary solution with @xmath54 independent of @xmath122 for @xmath40 ;    [ theo - stat-2 ] the variables @xmath123 have a density @xmath124 w.r.t .",
    "lebesgue measure ;    [ theo - stat-3 ] the variables @xmath125 have densities @xmath126 w.r.t .",
    "lebesgue measure .",
    "the next result states that @xmath65 can be locally approximated by @xmath76 .",
    "together with theorem  [ theo - stat ] , it shows that the tvnar process @xmath65 is locally stationary in the sense of definition  [ def - loc - stat ] .",
    "[ theo - loc - stat ] let  [ m1][m3 ] ,  [ sigma1][sigma3 ] and [ e1 ] be fulfilled .",
    "then @xmath127 where the variables @xmath33 have the property that @xmath26 < c$ ] for some @xmath27 and @xmath28 independent of @xmath13 , @xmath7 and @xmath29 .",
    "to get an idea of the proof of theorem  [ theo - loc - stat ] , consider the model @xmath128 for a moment .",
    "our arguments are based on a backward expansion of the difference @xmath129 . exploiting the smoothness conditions of  [ m2 ] and  [ m3 ] together with the boundedness of @xmath6",
    ", we obtain that @xmath130 where @xmath131 is the derivative of @xmath45 with respect to @xmath82 and @xmath132 is an intermediate point between @xmath133 and @xmath122 .",
    "to prove ( [ loc - stat - inequ ] ) , we have to show that the product @xmath134 is contracting in some stochastic sense as @xmath135 tends to infinity . the heuristic idea behind",
    "the proof is the following : using conditions [ m1 ] and  [ e1 ] , we can show that at least a certain fraction of the terms @xmath136 take a value in the region @xmath137 as @xmath135 grows large . since the derivative @xmath138 is small in this region according to [ m3 ] , this ensures that at least a certain fraction of the elements in the product @xmath134 are small in value .",
    "this prevents the product from exploding and makes it contract to zero as @xmath135 goes to infinity .",
    "next , we come to a result which shows that the densities of the approximating variables @xmath123 change smoothly over time .",
    "[ theo - smooth - dens ] let @xmath139 be the density of @xmath123 at .",
    "if  [ m1][m3 ] , [ sigma1][sigma3 ] and  [ e1 ] ,  [ e2 ] are fulfilled , then @xmath140 with some constant @xmath141 and @xmath142 continuously depending on @xmath82 .",
    "we finally characterize the mixing behavior of the tvnar process .",
    "to do so , we first give a quick reminder of the definitions of an @xmath143- and @xmath144-mixing array .",
    "let @xmath145 be a probability space , and let @xmath146 and @xmath147 be subfields of  @xmath148 .",
    "define @xmath149 moreover , for an array @xmath150 , define the coefficients @xmath151 where @xmath152 is the @xmath58-field generated by @xmath153 .",
    "the array @xmath154 is said to be @xmath143-mixing ( or strongly mixing ) if @xmath155 as @xmath156 .",
    "similarly , it is called @xmath144-mixing if @xmath157 .",
    "note that @xmath158-mixing implies @xmath143-mixing .",
    "the final result of this section shows that the tvnar process is @xmath144-mixing with coefficients that converge exponentially fast to zero .",
    "[ theo - mixing ] if  [ m1][m3 ] ,  [ sigma1][sigma3 ] and [ e1][e3 ] are fulfilled , then the tvnar process @xmath65 is geometrically @xmath144-mixing , that is , there exist positive constants @xmath159 and @xmath28 such that @xmath160 .",
    "the strategy of the proof is as follows : the ( conditional ) probabilities that show up in the definition of the @xmath158-coefficient in ( [ beta - coeff ] ) can be written in terms of the functions @xmath6 , @xmath58 and the error density @xmath95 .",
    "to do so , we derive recursive expressions of the model variables @xmath42 and of certain conditional densities of @xmath42 . rewriting the @xmath144-coefficient with the help of these expressions",
    "allows us to derive an appropriate bound for it .",
    "the overall strategy is thus similar to that of fryzlewicz and subba rao  @xcite who also derive bounds of mixing coefficients in terms of conditional densities .",
    "the specific steps of the proof , however , are quite different . the details together with the proofs of the other theorems can be found in appendix  [ appa ] .",
    "in this section , we consider kernel estimation in the general model ( [ model ] ) , @xmath161 with @xmath1 = 0 $ ] . note that @xmath162 is the conditional mean function in model ( [ model ] ) at the time point @xmath7 .",
    "the function @xmath6 is thus identified almost surely on the grid of points @xmath8 for @xmath163 .",
    "these points form a dense subset of the unit interval as the sample size grows to infinity . as a consequence , @xmath6",
    "is identified almost surely at all rescaled time points @xmath20 $ ] if it is continuous in time direction ( which we will assume in what follows ) .",
    "we restrict attention to nadaraya  watson ( nw ) estimation .",
    "it is straightforward to extend the theory to local linear ( or more generally local polynomial ) estimation .",
    "the nw estimator of model ( [ model ] ) is given by @xmath164 here and in what follows , we write @xmath165 and @xmath166 for any vector @xmath167 , that is , we use subscripts to indicate the time point of observation and superscripts to denote the components of the vector .",
    "@xmath168 denotes a one - dimensional kernel function and we use the notation @xmath169 . for convenience , we work with a product kernel and assume that the bandwidth @xmath170 is the same in each direction .",
    "our results can , however , be easily modified to allow for nonproduct kernels and different bandwidths .",
    "the estimate defined in ( [ nw ] ) differs from the nw estimator in the standard strictly stationary setting in that there is an additional kernel in time direction .",
    "we thus do not only smooth in the direction of the covariates @xmath3 but also in the time direction .",
    "this takes into account that the regression function is varying over time . in what follows",
    ", we derive the asymptotic properties of our nw estimate .",
    "the proofs are given in appendix  [ appb ] .",
    "the following three conditions are central to our results :    [ c1 ] the process @xmath18 is locally stationary in the sense of definition  [ def - loc - stat ] .",
    "thus , for each time point @xmath20 $ ] , there exists a strictly stationary process @xmath22 having the property that @xmath171 a.s . with @xmath26 \\le c$ ] for some @xmath27 .",
    "[ c2 ] the densities @xmath172 of the variables @xmath36 are smooth in @xmath13 . in particular , @xmath173 is differentiable w.r.t .",
    "@xmath13 for each @xmath167 , and the derivative @xmath174 is continuous .",
    "[ c3 ] the array @xmath175 is @xmath143-mixing .",
    "as seen in section  [ section - tvnar ] , these three conditions are essentially fulfilled for the tvnar process :  [ c1 ] and  [ c3 ] follow immediately from theorems  [ theo - loc - stat ] and  [ theo - mixing ] . moreover , theorem  [ theo - smooth - dens ] shows that the tvnar process satisfies a weakened version of  [ c2 ] which requires the densities @xmath23 to be continuous rather than differentiable in time direction .",
    "note that we could do with this weakened version of  [ c2 ] , however at the cost of getting slower convergence rates for the bias part of the nw estimate .",
    "in addition to the above three assumptions , we impose the following regularity conditions :    [ c4 ] @xmath173 is partially differentiable w.r.t .",
    "@xmath176 for each @xmath20 $ ] .",
    "the derivatives @xmath177 are continuous for @xmath178 .",
    "[ c5 ] @xmath179 is twice continuously partially differentiable with first derivatives @xmath180 and second derivatives @xmath181 for @xmath182 .",
    "[ c6 ] the kernel @xmath168 is symmetric about zero , bounded and has compact support , that is , @xmath183 for all @xmath184 with some @xmath185 .",
    "furthermore , @xmath168 is lipschitz , that is , @xmath186 for some @xmath80 and all @xmath187 .",
    "finally , note that throughout the paper the bandwidth @xmath170 is assumed to converge to zero at least at polynomial rate , that is , there exists a small @xmath188 such that @xmath189 for some constant @xmath190 .      as a first step in the analysis of the nw estimate ( [ nw ] )",
    ", we examine kernel averages of the general form @xmath191 with @xmath192 being an array of one - dimensional random variables .",
    "a wide range of kernel - based estimators , including the nw estimator defined in ( [ nw ] ) , can be written as functions of averages of the above form .",
    "the asymptotic behavior of such averages is thus of wider interest .",
    "for this reason , we investigate the properties of these averages for a general array of variables @xmath192 . later on",
    "we will employ the results with @xmath193 and @xmath194 .",
    "we now derive the uniform convergence rate of @xmath195 .",
    "to do so , we make the following assumptions on the components in ( [ kernel - average-1 ] ) :    [ k0 ] it holds that @xmath196 for some @xmath197 and @xmath28 .",
    "[ k1 ] the array @xmath198 is @xmath143-mixing .",
    "the mixing coefficients @xmath143 have the property that @xmath199 for some @xmath200 and @xmath201 .",
    "[ k2 ] let @xmath202 and @xmath203 be the densities of @xmath3 and@xmath204 , respectively . for any compact set @xmath205 , there exists a @xmath206 such that @xmath207 and . moreover , there existsa number @xmath208 such that for all @xmath209,@xmath210 .",
    "the next theorem generalizes uniform convergence results of hansen @xcite for the strictly stationary case to our setting .",
    "see kristensen  @xcite for related results .",
    "[ theo - stochastic - part ] assume that  [ k0][k2 ] are satisfied with @xmath211 and that the kernel @xmath168 fulfills  [ c6 ] .",
    "in addition , let the bandwidth satisfy @xmath212 with @xmath213 slowly diverging to infinity ( e.g. , @xmath214 ) and @xmath215 finally , let @xmath216 be a compact subset of @xmath31 .",
    "then it holds that @xmath217 , x \\in s } \\bigl| \\hat{\\psi}(u , x ) - { \\mathbb{e}}\\hat{\\psi}(u , x ) \\bigr| = o_p \\biggl(\\sqrt{\\frac{\\log t}{t h^{d+1 } } } \\biggr).\\ ] ]    the convergence rate in the above theorem is identical to the rate obtained for a @xmath218-dimensional nonparametric estimation problem in the standard strictly stationary setting .",
    "this reflects the fact that additionally smoothing in time direction , we essentially have a @xmath218-dimensional problem in our case .",
    "moreover , note that with ( [ beta ] ) and ( [ theta ] ) , we can compute that @xmath219 $ ] .",
    "in particular , @xmath220 if the mixing coefficients decay exponentially fast to zero , that is , if @xmath221 .",
    "restriction ( [ h ] ) on the bandwidth is thus a strengthening of the usual condition that @xmath222 .",
    "the next theorem characterizes the uniform convergence behavior of our nw estimate .",
    "[ theo - nw ] assume that  [ c1][c6 ] hold and that  [ k0][k2 ] are fulfilled both for @xmath193 and @xmath223 .",
    "let @xmath144 satisfy ( [ beta ] ) and suppose that @xmath224 , x \\in s } f(u , x ) > 0 $ ] .",
    "moreover , assume that the bandwidth @xmath170 satisfies @xmath225 with @xmath226 given in ( [ theta ] ) , @xmath227 , @xmath228 and @xmath32 introduced in  [ c1 ] .",
    "defining @xmath229 $ ] , it then holds that @xmath230    to derive the above result , we decompose the difference @xmath231 into a stochastic part and a bias part . using theorem [ theo - stochastic - part ] ,",
    "the stochastic part can be shown to be of the order @xmath232 .",
    "the bias term splits up into two parts , a standard component of the order @xmath233 and a nonstandard component of the order @xmath234 .",
    "the latter component results from replacing the variables @xmath3 by @xmath235 in the bias term .",
    "it thus captures how far these variables are from their stationary approximations @xmath235 .",
    "put differently , it measures the deviation from stationarity . as will be seen in appendix [ appb ] , handling this nonstationarity bias requires techniques substantially different from those needed to treat the bias term in a strictly stationary setting .",
    "note that the additional nonstationarity bias converges faster to zero for larger @xmath236 .",
    "this makes perfect sense if we recall from section  [ section - loc - stat ] that @xmath237 measures how well @xmath3 is locally approximated by @xmath235 : the larger @xmath237 , the smaller the deviation of @xmath3 from its stationary approximation and thus the smaller the additional nonstationarity bias .",
    "we conclude the asymptotic analysis of our nw estimate with a result on asymptotic normality .",
    "[ theo - normality ] assume that  [ c1][c6 ] hold and that  [ k0][k2 ] are fulfilled both for @xmath193 and @xmath223 .",
    "let @xmath238 and @xmath239 with @xmath228 .",
    "moreover , suppose that @xmath240 and that @xmath241 $ ] is continuous . finally , let @xmath242 to ensure that the bandwidth @xmath170 can be chosen to satisfy @xmath243 for a constant @xmath244",
    ". then @xmath245 where @xmath246 / f(u , x)$ ] and @xmath247 with @xmath248 and @xmath249 .",
    "the above theorem parallels the asymptotic normality result for the standard strictly stationary setting .",
    "in particular , the bias and variance expressions @xmath250 and @xmath251 are very similar to those from the standard case . by requiring that @xmath252 , we make sure that the additional nonstationarity bias is asymptotically negligible .",
    "we now put some structural constraints on the regression function @xmath6 in model ( [ model ] ) . in particular",
    ", we assume that for all rescaled time points @xmath20 $ ] and all points @xmath176 in a compact subset of @xmath31 , say @xmath253^d$ ] , the regression function can be split up into additive components according to @xmath254 .",
    "this means that for @xmath255^d$ ] , we have the additive regression model @xmath256 = m_0 \\biggl(\\frac{t}{t } \\biggr ) + \\sum _ { j=1}^d m_j \\biggl ( \\frac{t}{t},x^j \\biggr).\\ ] ]    to identify the component functions of model ( [ additive - model ] ) within the unit cube @xmath253^d$ ] , we impose the condition that @xmath257 for all @xmath258 and all rescaled time points @xmath20 $ ] . here , the functions @xmath259 are the marginals of the density @xmath260^d ) f(u , x)}{{\\mathbb{p}}(x_0(u ) \\in[0,1]^d)},\\ ] ] where as before @xmath261 is the density of the strictly stationary process @xmath22 .",
    "note that this normalization of the component functions varies over time in the sense that for each rescaled time point @xmath13 , we integrate with respect to a different density .    to estimate the functions @xmath262 , we adapt the smooth backfitting technique of mammen et al .",
    "@xcite to our setting . to do so",
    ", we first introduce the auxiliary estimates @xmath263^d } } } \\sum_{t=1}^t i\\bigl(x_{t , t } \\in[0,1]^d\\bigr ) k_h \\biggl(u , \\frac{t}{t } \\biggr ) \\prod_{j=1}^d k_h \\bigl(x^j , x_{t , t}^j \\bigr ) , \\\\",
    "\\hat{m}(u , x ) & = & \\frac{1}{{t_{[0,1]^d } } } \\sum_{t=1}^t i\\bigl(x_{t , t } \\in[0,1]^d\\bigr ) k_h \\biggl(u , \\frac{t}{t } \\biggr ) \\prod_{j=1}^d k_h \\bigl(x^j , x_{t , t}^j \\bigr ) y_{t , t } / \\hat{p}(u , x).\\end{aligned}\\ ] ] @xmath264 is a kernel estimate of the density @xmath265 , and @xmath266 is a @xmath218-dimensional nw smoother that estimates @xmath179 for @xmath255^d$ ] . in the above definitions , @xmath267^d}}= \\sum_{t=1}^t",
    "k_h \\biggl(u,\\frac{t}{t } \\biggr ) i\\bigl(x_{t , t } \\in[0,1]^d \\bigr)\\ ] ] is the number of observations in the unit cube @xmath253^d$ ] , where only time points close to @xmath13 are taken into account , and @xmath268\\bigr ) \\frac{k_h(v - w)}{\\int_0 ^ 1 k_h(s - w ) \\,ds}\\ ] ] is a modified kernel weight .",
    "this weight has the property that for all @xmath269 $ ] , which is needed to derive the asymptotic properties of the backfitting estimates .",
    "given the smoothers @xmath270 and @xmath271 , we define the smooth backfitting estimates @xmath272 , @xmath273 of the functions @xmath274 , @xmath275 at the time point @xmath276 $ ] as the minimizers of the criterion @xmath277 where the minimization runs over all additive functions @xmath278 whose components are normalized to satisfy @xmath279 for @xmath258 . here",
    ", @xmath280 is the marginal of the kernel density @xmath281 at the point @xmath282 .    according to ( [ proj ] ) ,",
    "the backfitting estimate @xmath283 is an @xmath284-projection of the full - dimensional nw estimate @xmath285 onto the subspace of additive functions , where the projection is done with respect to the density estimate @xmath286 .",
    "note that ( [ proj ] ) is a @xmath5-dimensional projection problem .",
    "in particular , rescaled time does not enter as an additional dimension .",
    "the projection is rather done separately for each time point @xmath20 $ ] .",
    "we thus fit a smooth backfitting estimate to the data separately around each point in time @xmath13 .    by differentiation",
    ", we can show that the minimizer of ( [ proj ] ) is characterized by the system of integral equations @xmath287 together with @xmath288 for @xmath178 . here ,",
    "@xmath289 and @xmath290 are kernel density estimates , and @xmath291 is a nw smoother defined as @xmath292^d } } } \\sum _ { t=1}^t i\\bigl(x_{t , t } \\in[0,1]^d\\bigr ) k_h \\biggl(u,\\frac{t}{t } \\biggr ) k_h\\bigl(x^j , x_{t , t}^j\\bigr ) , \\\\",
    "\\hat{p}_{j , k}\\bigl(u , x^j , x^k\\bigr ) & = & \\frac{1}{{t_{[0,1]^d } } } \\sum_{t=1}^t i \\bigl(x_{t , t } \\in[0,1]^d\\bigr ) k_h \\biggl(u , \\frac{t}{t } \\biggr)\\\\ & & \\qquad\\hspace*{22pt}{}\\times k_h\\bigl(x^j , x_{t , t}^j \\bigr ) k_h\\bigl(x^k , x_{t , t}^k\\bigr ) , \\\\",
    "\\hat{m}_j\\bigl(u , x^j\\bigr ) & = & \\frac{1}{{t_{[0,1]^d } } } \\sum _ { t=1}^t i\\bigl(x_{t , t } \\in[0,1]^d\\bigr ) k_h \\biggl(u,\\frac{t}{t } \\biggr)\\\\ & & \\qquad\\hspace*{22pt}{}\\times k_h\\bigl(x^j , x_{t , t}^j\\bigr ) y_{t , t } / \\hat{p}_j\\bigl(u , x^j\\bigr).\\end{aligned}\\ ] ] moreover , the estimate @xmath272 of the model constant at time point @xmath13 is given by @xmath293^d}}^{-1 } \\sum_{t=1}^t i(x_{t , t } \\in[0,1]^d ) k_h(u,\\frac{t}{t } ) y_{t , t}$ ] .",
    "we next summarize the assumptions needed to derive the asymptotic properties of the smooth backfitting estimates .",
    "first of all , the conditions of section  [ section - estimation ] must be satisfied for the kernel estimates that show up in the system of integral equations  ( [ int - equ ] ) .",
    "this is ensured by the following assumption.=-1    [ add1 ] conditions  [ c1][c6 ] are fulfilled together with  [ k0][k2 ] for @xmath294 and @xmath295 .",
    "the parameter @xmath144 satisfies the inequality @xmath296 and @xmath297 , x \\in[0,1]^d } f(u , x ) > 0 $ ] .",
    "in addition to  [ add1 ] , we need some restrictions on the admissible bandwidth . for convenience ,",
    "we stipulate somewhat stronger conditions than in section  [ section - estimation ] to get rid of the additional nonstationarity bias from the very beginning .",
    "[ add2 ] the bandwidth @xmath170 is such that ( i ) @xmath298 , ( ii ) @xmath299 with @xmath214 and @xmath300 and ( iii ) @xmath301 and @xmath302 with @xmath303 and @xmath32 given in  [ c1 ]",
    ".    condition ( ii ) is already known from section [ section - estimation ] . as will be seen in appendix  [ appc ] , ( iii )",
    "ensures that the additional nonstationarity bias is of smaller order than @xmath233 and can thus be asymptotically neglected .",
    "the expressions for @xmath144 and @xmath226 in  [ add1 ] and  [ add2 ] are calculated as follows : using the formulas ( [ beta ] ) and ( [ theta ] ) from theorem  [ theo - stochastic - part ] , we get a pair of expressions for @xmath144 and @xmath226 for each of the kernel estimates occurring in ( [ int - equ ] ) . combining these expressions yields the formulas in [ add1 ] and  [ add2 ] .    under the above assumptions",
    ", we can establish the following results , the proofs of which are given in appendix  [ appc ] .",
    "first , the backfitting estimates uniformly converge to the true component functions at the two - dimensional rates no matter how large the dimension @xmath5 of the full regression function .",
    "[ theorem - sbf-1 ] let @xmath304 $ ] .",
    "then under  [ add1 ] and [ add2 ] , @xmath305    second , the estimates are asymptotically normal if rescaled appropriately .",
    "[ theorem - sbf-2 ] suppose that  [ add1 ] and  [ add2 ] hold .",
    "in addition , let @xmath306 and @xmath307 to ensure that the bandwidth @xmath170 can be chosen to satisfy @xmath308^d}}h^6 \\rightarrow c_h$ ] for a constant @xmath244 .",
    "then for any @xmath309 , @xmath310^d}}h^2 } { \\left } [ \\matrix { \\tilde{m}_1\\bigl(u , x^1\\bigr ) - m_1 \\bigl(u , x^1\\bigr ) \\cr \\vdots \\cr \\tilde{m}_d \\bigl(u , x^d\\bigr ) - m_d\\bigl(u , x^d\\bigr ) } { \\right } ] { \\stackrel{d}{\\longrightarrow}}{n}(b_{u , x},v_{u , x}).\\ ] ] here , @xmath251 is a diagonal matrix whose diagonal entries are given by the expressions @xmath311 with @xmath312 .",
    "moreover , the bias term has the form @xmath313^t$ ] .",
    "the functions @xmath314 in this expression are defined as the minimizers of the problem @xmath315 ^ 2 p(u , x ) \\,dx,\\ ] ] where the minimization runs over all additive functions @xmath316 with @xmath317 , and the function @xmath144 is given in lemma  [ lemma - add - nw - bias ] of appendix  [ appc ] . moreover",
    ", the terms @xmath318 can be characterized by the equation @xmath319 , where the functions @xmath320 are again defined in lemma  [ lemma - add - nw - bias ] .",
    "in this paper , we have studied nonparametric models with a time - varying regression function and locally stationary covariates .",
    "we have developed a complete asymptotic theory for kernel estimates in these models .",
    "in addition , we have shown that the main assumptions of the theory are satisfied for a large class of nonlinear autoregressive processes with a time - varying regression function .",
    "our analysis can be extended in several directions .",
    "an important issue is bandwidth selection in our framework . as shown in theorem [ theo - normality ] , the asymptotic bias and variance expressions of our nw estimate are very similar in structure to those from a standard stationary random design .",
    "we thus conjecture that the techniques to choose the bandwidth in such a design can be adapted to our setting .",
    "in particular , using the formulas for the asymptotic bias and variance from theorem  [ theo - normality ] , it should be possible to select the bandwidth via plug - in methods .",
    "another issue concerns forecasting .",
    "the convergence results of theorems  [ theo - nw ] and  [ theorem - sbf-1 ] are only valid for rescaled time lying in a subset @xmath321 $ ] of the unit interval . for forecasting purposes , it would be important to provide convergence rates also in the boundary region @xmath322 $ ]",
    ". this can be achieved by using boundary - corrected kernels .",
    "another possibility is to work with one - sided kernels . in both cases",
    ", we have to ensure that the kernels have compact support and are lipschitz to get the theory to work .",
    "[ app ]    [ [ appa ] ]    in this appendix , we prove the results on the tvnar process from section  [ section - tvnar ] .",
    "to shorten notation , we frequently make use of the abbreviations @xmath323 , @xmath324 and @xmath325 .",
    "moreover , throughout the appendices , the symbol @xmath326 denotes a universal real constant which may take a different value on each occurrence .      before we come to the proofs of the theorems , we state some useful facts needed for the arguments later on .",
    "linearization of @xmath6 and @xmath58 .",
    "consider the function @xmath6 .",
    "the mean value theorem allows us to write @xmath327 where we have used the shorthands @xmath328 and @xmath329 for @xmath178 with the functions@xmath330 .",
    "the terms @xmath331 have the property that @xmath332 for @xmath258 with @xmath333 and @xmath334 .",
    "this is a straightforward consequence of the boundedness assumptions on @xmath6 and @xmath58 .",
    "see the supplement  @xcite for details",
    ".    repeating the above considerations for the function @xmath58 , we obtain analogous terms @xmath335 that are again bounded by @xmath336 for @xmath178 .",
    "recursive formulas for @xmath42 . for the proof of theorem",
    "[ theo - mixing ] , we rewrite @xmath42 in a recursive fashion : letting @xmath337 and @xmath338 be values of @xmath339 and @xmath340 , respectively , we recursively define the functions @xmath341 by @xmath342 and for @xmath343 by @xmath344 using analogous recursions for the function @xmath58 , we can additionally define functions @xmath345 for @xmath346 . with this notation at hand ,",
    "@xmath42 can represented as @xmath347 moreover , for @xmath348 we can write @xmath349 the term @xmath350 can be reformulated in the same way .",
    "formulas for conditional densities . throughout the appendix , the symbol @xmath351 is used to denote the density of @xmath352 conditional on @xmath353 .",
    "if the residuals @xmath54 have a density @xmath95 , then it can be shown that for @xmath354 , @xmath355 here , @xmath356 , @xmath357 , @xmath358 and @xmath359 are values of @xmath42 , @xmath360 , @xmath361 and @xmath362 , respectively .",
    "moreover , @xmath363 and @xmath364 is defined analogously .",
    "property  [ theo - stat-1 ] follows by standard arguments to be found , for example , in chen and chen  @xcite .",
    "property  [ theo - stat-2 ] immediately follows with the help of ( [ cond - dens ] ) . recalling that @xmath365 for @xmath366 , [ theo - stat-3 ]",
    "can again be shown by using  ( [ cond - dens ] ) .",
    "we apply the triangle inequality to get @xmath367 and bound the terms @xmath368 and @xmath369 separately . in",
    "what follows , we restrict attention to the term @xmath370 , the arguments for @xmath368 being analogous .    notation . throughout the proof",
    ", the symbol @xmath371 denotes the euclidean norm for vectors @xmath372 , and @xmath373 is the spectral norm for @xmath374 matrices @xmath375 . in addition ,",
    "furthermore , for @xmath377 , we define the family of matrices @xmath378 finally , as already noted at the beginning of the appendix , we make use of the shorthands @xmath323 , @xmath379 and @xmath325 .    backward iteration . by the smoothness conditions on @xmath6 and @xmath58 , @xmath380 with @xmath381 and @xmath382 for @xmath178 as introduced in ( [ mean - val - theo3 ] ) .",
    "the terms @xmath335 for @xmath383 are defined analogously . in matrix notation , we obtain @xmath384 with @xmath385 and @xmath386 iterating ( [ iterate1 ] ) @xmath135 times yields @xmath387 note that the rescaled time argument @xmath8 plays the same role as the argument @xmath13 and thus remains fixed when iterating backward .",
    "next define matrices @xmath388 by @xmath389 with @xmath390 . as shown in the preliminaries section of the appendix , @xmath391 for @xmath178",
    "therefore , the entries of the matrix @xmath388 are all weakly larger in absolute value than those of @xmath392 .",
    "this implies that @xmath393 with @xmath394 . using this together with the boundedness of @xmath6 and @xmath58 and the fact that @xmath395 with @xmath396    bounding @xmath397 and @xmath398 .",
    "the convergence behavior of @xmath397 and @xmath398 for @xmath399 mainly depends on the properties of the product @xmath400 .",
    "the behavior of the latter is described by the following lemma .",
    "[ lemmaa1 ] if @xmath86 is sufficiently small , in particular , if it satisfies ( [ delta ] ) , then there exists a constant @xmath27 such that for some @xmath159 , @xmath401 \\le c \\gamma^n.\\ ] ]    the proof of lemma  [ lemmaa1 ] is postponed until the arguments for theorem  [ theo - loc - stat ] are completed .",
    "the following statement is a direct consequence of lemma  [ lemmaa1 ] .",
    "there exists a constant @xmath27 such that @xmath402 \\le c \\gamma^n$ ] for some @xmath159 .",
    "in particular , @xmath403 as @xmath404 .",
    "in addition , it holds that :    @xmath405 , where the variables @xmath406 have the property that @xmath407 \\le c$ ] for a positive constant @xmath408 and all @xmath7 .    this can be seen as follows .",
    "first note that @xmath409 using the monotone convergence theorem and love s inequality with @xmath408 , we obtain @xmath407 \\le c { \\mathbb{e}}(1 + |\\varepsilon _ t|)^{\\rho } + \\sum_{r=0}^{\\infty } { \\mathbb{e}}[r_{t , r}^{\\rho}]$ ] . as the right - hand side of the previous inequality is finite by ( r ) , we arrive at ( v ) .",
    "\\(r ) and ( v ) imply that @xmath410 a.s . with variables",
    "@xmath406 whose @xmath32th moment is uniformly bounded by some finite constant @xmath326 .",
    "an analogous result can be derived for @xmath368 .",
    "this completes the proof .",
    "proof of lemma  [ lemmaa1 ] we want to show that the @xmath32th moment of the product @xmath400 converges exponentially fast to zero as @xmath404 .",
    "this is a highly nontrivial problem , and as far as we can see , it can not be solved by simply adapting techniques from related papers on models with time - varying coefficients .",
    "the problem is that the techniques used therein are either tailored to products of deterministic matrices ( see , e.g. , proposition 13 in moulines et al .",
    "@xcite ) or they heavily draw on the independence of the random matrices involved ( see , e.g. , proposition  2.1 in subba rao  @xcite ) .",
    "we now describe our proving strategy in detail . to start with ,",
    "we replace the spectral norm in ( [ lemmaa1-statement ] ) by the norm which is much easier to handle .",
    "as these two norms are equivalent , there exists a finite constant @xmath326 such that @xmath411 with @xmath412 .",
    "next , we split up the term @xmath413 into two parts , @xmath414 where @xmath415 with @xmath416 and a constant @xmath417 to be specified later on .",
    "lemma  [ lemmaa1 ] is a direct consequence of the following two facts :    there exists a constant @xmath27 such that @xmath418 \\le c \\gamma^n$ ] for some @xmath159 .",
    "@xmath419 \\le c \\gamma^n$ ] for some @xmath159 .",
    "we start with the proof of ( i ) .",
    "letting @xmath420 with some positive constant @xmath421 , we can write @xmath422 & = & { \\mathbb{e}}\\bigl [ i ( \\mathcal{b}_{n,1 } > \\phi_n ) \\mathcal{b}_{n,1}^{\\rho } \\bigr ] + { \\mathbb{e}}\\bigl [ i ( \\mathcal{b}_{n,1 } \\le\\phi_n ) \\mathcal{b}_{n,1}^{\\rho } \\bigr ] \\\\ & \\le&\\bigl ( { \\mathbb{e}}\\bigl [ \\mathcal{b}_{n,1}^{2\\rho } \\bigr ] { \\mathbb{p } } ( \\mathcal{b}_{n,1 } > \\phi_n ) \\bigr)^{1/2 } + \\phi_n^{\\rho}.\\end{aligned}\\ ] ] it is easy to see that @xmath423 \\le c^{\\rho n}$ ] for a sufficiently large constant @xmath326 , where @xmath424 can be made arbitrarily close to one by choosing @xmath27 small enough . to show ( i ) , it thus suffices to verify that @xmath425 for the proof of ( [ bn1a ] ) , we write @xmath426\\bigr ) > \\kappa_0 n \\biggr)\\ ] ] with @xmath427 $ ] .",
    "as the variables @xmath54 have an everywhere positive density by assumption , the expectation @xmath428 $ ] is strictly smaller than one .",
    "we can thus choose @xmath429 slightly larger than @xmath428 $ ] to get that @xmath430 .",
    "as the variables @xmath431 $ ] for @xmath432 are @xmath433-dependent , a simple blocking argument together with hoeffding s inequality shows that @xmath434\\bigr ) > \\kappa_0 n \\biggr ) \\le c \\gamma^n\\ ] ] for some @xmath159 .",
    "this yields ( [ bn1a ] ) and thus completes the proof of ( i ) .",
    "let us now turn to the proof of ( ii ) .",
    "we have that @xmath435 the random matrix @xmath436 in the above expression can only take two forms : if @xmath437 , it equals @xmath438 , and if @xmath439 , it equals @xmath440 . moreover , if @xmath441 , it holds that @xmath442 for all @xmath443 and thus @xmath444 .",
    "importantly , the term @xmath445 is unequal to zero only if @xmath446 , that is , only if @xmath447 for at least @xmath448 terms . from this",
    ", we can infer that @xmath449 \\le{\\mathbb{e}}\\biggl [ \\prod _ { k=0}^n \\bigl(1 + |\\varepsilon_{t - k}|\\bigr ) \\biggr ] \\bigl\\| b(\\delta ) \\bigr\\|_1^{\\kappa n } \\bigl\\| b(\\delta)^d \\bigr\\|_1^{{(1 - \\kappa ) n}/{d}}.\\ ] ] by direct calculations , we can verify that @xmath450 with the constant @xmath451 that only depends on the dimension @xmath5 .",
    "moreover , @xmath452 . plugging this into ( [ mean1 ] ) yields @xmath453 \\le\\bigl(1 + { \\mathbb{e}}|\\varepsilon_0|\\bigr ) \\bigl [ \\bigl(1 + { \\mathbb{e}}|\\varepsilon_0|\\bigr ) ( \\delta+ 1)^{\\kappa } ( c_d \\delta)^{{(1 - \\kappa)}/{d } } \\bigr]^n.\\ ] ] straightforward calculations show that the term in square brackets is strictly smaller than one for @xmath454^{-1}.\\ ] ] assuming that @xmath86 satisfies the above condition , we thus arrive at ( ii ) .",
    "the proof can be found in the supplement  @xcite .      to start with ,",
    "note that the process @xmath65 is @xmath5-markovian .",
    "this implies that @xmath455 with @xmath456.\\ ] ] in the following , we bound the expression @xmath457 for arbitrary sets @xmath458 .",
    "this provides us with a bound for the mixing coefficients @xmath459 of the process @xmath65 .",
    "we use the following notation : throughout the proof , we let @xmath460 , @xmath461 and @xmath462 be values of @xmath463 , @xmath464 and @xmath465 , respectively .",
    "moreover , we use the shorthand @xmath466 for @xmath467 , where we suppress the dependence on the arguments @xmath468 and @xmath469 in the notation . finally",
    ", note that by ( [ cond - dens ] ) , the above conditional density can be expressed in terms of the error density @xmath95 as @xmath470 with @xmath471 and @xmath472 defined analogously .",
    "the functions @xmath473 , @xmath474 were introduced in the preliminaries section of the appendix .    with this notation at hand , we can write @xmath475 | { \\underline{y}}_{t - k , t } \\bigr ] \\\\ & & \\qquad= \\int i(y\\in{s } ) f_{{\\underline{y}}_{t+d-1,t}|\\varepsilon _ { t-1}^{t - k+1},{\\underline{y}}_{t - k , t}}(y|e,{\\underline{y}}_{t - k , t } ) \\prod _ { l=1}^{k-1 } f_{\\varepsilon}(e_{t - l } ) \\,de \\,dy \\\\ & & \\qquad= \\int i(y\\in{s } ) \\prod_{j=0}^{d-1 } f_j(y_{t+j}|{\\underline{y}}_{t - k , t } ) \\prod _ { l=1}^{k-1 } f_{\\varepsilon}(e_{t - l } ) \\,de \\,dy\\end{aligned}\\ ] ] and likewise @xmath476 using the shorthand @xmath477",
    ", we thus arrive at @xmath478}_{= : ( * ) } \\prod_{l=1}^{k-1 } f_{\\varepsilon } ( e_{t - l } ) f_{\\underline{{y}}}(z ) \\,de \\,dz.\\end{aligned}\\ ] ] we next consider @xmath479 more closely . a telescoping argument together with fubini s theorem yields that @xmath480 \\,dy \\\\ & = & \\sum_{i=0}^{d-1 } \\int\\biggl [ \\int \\biggl [ \\int\\prod_{j = i+1}^{d-1 } f_j(y_{t+j}|z ) \\,dy_{t+d-1 } \\cdots dy_{t+i+1 } \\biggr ] \\\\ & & \\hspace*{79pt } { } \\times\\bigl| f_i(y_{t+i}|z ) - f_i(y_{t+i}| \\underline{{y } } ) \\bigr| \\,dy_{t+i } \\biggr ] \\\\ & & \\hspace*{25.5pt}{}\\times\\prod_{j=0}^{i-1 } f_j(y_{t+j}|\\underline{{y } } ) \\,dy_{t+i-1 } \\cdots dy_t \\\\ & \\le & \\sum_{i=0}^{d-1 } \\int\\underbrace { \\biggl [ \\int\\bigl| f_i(y_{t+i}|z ) - f_i(y_{t+i}| \\underline{{y } } ) \\bigr| \\,dy_{t+i } \\biggr]}_{= : ( * * ) } \\prod _ { j=0}^{i-1 } f_j(y_{t+j}| \\underline{{y } } ) \\,dy_{t+i-1 } \\cdots dy_t,\\end{aligned}\\ ] ] where the last inequality exploits the fact that @xmath481 is a conditional probability and thus almost surely bounded by one . using formula ( [ cond - dens ] )",
    "together with  [ e3 ] , it is straightforward to see that @xmath482 where @xmath483 is some constant with @xmath141 .",
    "iterating backward @xmath484 times in the same way as in theorem  [ theo - loc - stat ] , we can further show that @xmath485\\\\[-8pt ] & & \\qquad\\le c \\sum_{j=1}^{d - i } \\biggl\\| \\prod _ { m=0}^n { b}_{t - j - m } \\biggr\\| \\bigl ( 1 + \\bigl\\|e_{t - j - n-1}^{t - j - n - d}\\bigr\\| \\bigr ) , \\nonumber\\end{aligned}\\ ] ] where @xmath486 denotes the euclidean norm for vectors and the spectral norm for matrices . the matrix @xmath388 was introduced in ( [ iteration - matrix ] ) .",
    "note that @xmath388 was defined there in terms of the random vector @xmath487 . slightly abusing notation",
    ", we here use the symbol @xmath388 to denote the matrix with @xmath487 replaced by the realization @xmath488 . keeping in mind",
    "that the matrix @xmath388 only depends on the residual values @xmath488 , we can plug ( [ mixing - star3 ] ) into the bound for @xmath489 and insert this into the bound for @xmath479 to arrive at @xmath490 as a consequence , @xmath491 using the arguments from lemma  [ lemmaa1 ] , we can show that for @xmath492 sufficiently small , the expectation on the right - hand side is bounded by @xmath493 for some positive constant @xmath421 .",
    "choosing @xmath494 , for instance , we thus arrive at @xmath495 for some constant @xmath159 .",
    "this immediately implies that @xmath496 .",
    "[ [ appb ] ]    in this appendix , we prove the results of section [ section - estimation ] . before we turn to the proofs , we state two auxiliary lemmas which are repeatedly used throughout the appendix .",
    "the proofs are straightforward and thus omitted .",
    "[ lemmab1 ] suppose the kernel @xmath168 satisfies  [ c6 ] and let @xmath497 $ ] .",
    "then for @xmath498 , @xmath499    [ lemmab4 ] suppose @xmath168 satisfies  [ c6 ] and let @xmath500 \\times{\\mathbb{r}}^d \\rightarrow{\\mathbb{r}}$ ] , @xmath501 be continuously differentiable w.r.t . @xmath13 .",
    "then for any compact set @xmath502 , @xmath503      to show the result , we use a blocking argument together with an exponential inequality for mixing arrays , thus following the common proving strategy to be found , for example , in bosq  @xcite , masry  @xcite or hansen  @xcite .",
    "in particular , we go along the lines of hansen s proof of theorem  2 in  @xcite , modifying his arguments to allow for local stationarity in the data .",
    "a detailed version of the arguments can be found in the supplement  @xcite .",
    "we write @xmath504 with @xmath505 we first derive some intermediate results for the above expressions :    [ conv - a ] by theorem  [ theo - stochastic - part ] with @xmath223 , @xmath217 , x \\in s } \\bigl| \\hat{g}^v(u , x ) \\bigr| = o_p \\biggl ( \\sqrt{\\frac{\\log t}{t h^{d+1 } } } \\biggr).\\ ] ]    [ conv - b ] applying the arguments for theorem [ theo - stochastic - part ] to @xmath506 yields @xmath507 , x \\in s } \\bigl| \\hat{g}^b(u , x ) - m(u , x ) \\hat{f}(u , x)\\\\ & & \\qquad\\quad\\hspace*{12.5pt } { } - { \\mathbb{e}}\\bigl [ \\hat{g}^b(u , x ) - m(u , x ) \\hat { f}(u , x ) \\bigr ] \\bigr|\\\\ & & \\qquad = o_p \\biggl ( \\sqrt{\\frac{\\log t}{t h^{d+1 } } } \\biggr).\\end{aligned}\\ ] ]    [ conv - c ] it holds that @xmath508 \\bigr| \\\\ & & \\qquad= h^2 \\frac{\\kappa_2}{2 } \\sum_{i=0}^d \\bigl ( 2 \\,\\partial_i m(u , x ) \\,\\partial_i f(u , x ) + \\partial_{ii}^2 m(u , x ) f(u , x ) \\bigr)\\\\ & & \\qquad\\quad { } + o \\biggl ( \\frac{1}{t^r h^d } \\biggr ) + o\\bigl(h^2\\bigr)\\end{aligned}\\ ] ] with @xmath236 . the proof is postponed until the arguments for theorem  [ theo - nw ] are completed .",
    "[ conv - d ] we have that @xmath509 for the proof , we split up the term @xmath510 into a variance part @xmath511 and a bias part @xmath512 . applying theorem  [ theo - stochastic - part ] with @xmath294 yields that the variance part is @xmath513 uniformly in @xmath13 .",
    "the bias part can be analyzed by a simplified version of the arguments used to prove  [ conv - c ] .    combining the intermediate results  [ conv - a][conv - c ] , we arrive at @xmath514 & & \\qquad\\le\\bigl ( \\sup\\hat{f}(u , x)^{-1 } \\bigr ) \\bigl ( \\sup\\bigl| \\hat { g}^v(u , x ) \\bigr| + \\sup\\bigl| \\hat{g}^b(u , x ) - m(u , x ) \\hat{f}(u , x ) \\bigr| \\bigr ) \\\\[-2pt ] & & \\qquad= \\bigl(\\sup\\hat{f}(u , x)^{-1 } \\bigr ) o_p \\biggl ( \\sqrt { \\frac { \\log t}{t h^{d+1 } } } + \\frac{1}{t^r h^d } + h^2 \\biggr)\\end{aligned}\\ ] ] with @xmath236 .",
    "moreover ,  [ conv - d ] and the condition that @xmath224 , x \\in s } f(u,\\break x ) > 0 $ ] immediately imply that @xmath515 .",
    "this completes the proof .",
    "proof of  [ conv - c ] let @xmath516 be a lipschitz continuous function with support @xmath517 $ ] for some @xmath518 .",
    "assume that @xmath519 for all @xmath520 $ ] and write @xmath521 . then @xmath522 = q_1(u , x ) + \\cdots+ q_4(u , x)\\ ] ] with @xmath523 and @xmath524 , \\\\",
    "q_2(u , x ) & = & { \\mathbb{e}}\\biggl [ \\prod_{j=1}^d \\bar{k}_h\\bigl(x^j - x_{t , t}^j \\bigr ) \\prod_{j=1}^d k_h \\biggl(x^j - x_t^j\\biggl ( { \\frac{t}{t } } \\biggr ) \\biggr ) \\\\ & & \\hspace*{34.8pt } { } \\times\\biggl\\ { m \\biggl ( { \\frac { t}{t}},x_{t , t } \\biggr ) - m \\biggl ( { \\frac { t}{t}},x_t\\biggl ( { \\frac{t}{t}}\\biggr ) \\biggr ) \\biggr\\ } \\biggr ] , \\\\",
    "q_3(u , x ) & = & { \\mathbb{e}}\\biggl [ \\biggl\\ { \\prod_{j=1}^d \\bar{k}_h\\bigl(x^j - x_{t , t}^j \\bigr ) - \\prod_{j=1}^d \\bar{k}_h \\biggl(x^j - x_t^j \\biggl ( { \\frac{t}{t}}\\biggr ) \\biggr ) \\biggr\\ } \\\\[-2pt ] & & \\hspace*{11pt } { } \\times\\prod_{j=1}^d k_h \\biggl(x^j - x_t^j\\biggl ( { \\frac{t}{t } } \\biggr ) \\biggr ) \\biggl\\ { m \\biggl ( { \\frac{t}{t}},x_t\\biggl ( { \\frac{t}{t}}\\biggr ) \\biggr ) - m(u , x ) \\biggr\\}\\biggr ] , \\\\[-2pt ] q_4(u , x ) & = & { \\mathbb{e}}\\biggl [ \\prod_{j=1}^d k_h \\biggl(x^j - x_t^j\\biggl ( { \\frac{t}{t}}\\biggr ) \\biggr ) \\biggl\\ { m \\biggl ( { \\frac{t}{t}},x_t \\biggl ( { \\frac{t}{t}}\\biggr ) \\biggr ) - m(u , x ) \\biggr\\ } \\biggr].\\end{aligned}\\ ] ] we first consider @xmath525 . as the kernel @xmath168 is bounded , we can use a telescoping argument to get that @xmath526 . once again exploiting the boundedness of @xmath168",
    ", we can find a constant @xmath527 with @xmath528 for @xmath236 .",
    "hence , @xmath529\\\\[-9pt ] & & \\qquad\\le c \\sum_{k=1}^d \\biggl| k_h \\bigl(x^k - x_{t , t}^k\\bigr ) -",
    "k_h \\biggl(x^k - x_t^k\\biggl ( { \\frac { t}{t } } \\biggr ) \\biggr ) \\biggr|^r . \\nonumber\\end{aligned}\\ ] ] using ( [ kernel - product ] ) , we obtain @xmath530 & & \\qquad\\le\\frac{c}{th^{d+1 } } \\sum_{t=1}^t k_h \\biggl(u - \\frac { t}{t } \\biggr ) \\\\[-2pt ] & & \\qquad\\quad\\hspace*{47pt}{}\\times{\\mathbb{e}}\\biggl [ \\sum _ { k=1}^d \\biggl| k_h\\bigl(x^k - x_{t , t}^k\\bigr ) -",
    "k_h \\biggl(x^k - x_t^k\\biggl ( { \\frac{t}{t}}\\biggr )",
    "\\biggr ) \\biggr|^r \\\\[-2pt ] & & \\qquad\\quad\\hspace*{70.5pt } { } \\times\\prod_{j=1}^d \\bar{k}_h\\bigl(x^j - x_{t , t}^j\\bigr ) \\biggl| m \\biggl(\\frac{t}{t},x_{t , t } \\biggr ) - m(u , x ) \\biggr| \\biggr]\\end{aligned}\\ ] ] with @xmath236 .",
    "the term @xmath531 in the above expression can be bounded by @xmath532 . since @xmath168 is lipschitz , @xmath533 and the variables @xmath534",
    "have finite @xmath237th moment , we can infer that @xmath535 \\\\ & & \\qquad\\le\\frac{c}{th^d } \\sum_{t=1}^t k_h \\biggl(u - \\frac { t}{t } \\biggr ) { \\mathbb{e}}\\biggl [ \\sum _ { k=1}^d \\biggl| \\frac{1}{th } u_{t , t } \\biggl ( { \\frac{t}{t}}\\biggr ) \\biggr|^r \\biggr ] \\\\ & & \\qquad\\le{\\frac{c}{t^r h^{d-1+r}}}\\end{aligned}\\ ] ] uniformly in @xmath13 and @xmath176 . using similar arguments",
    ", we can further show that @xmath536 and @xmath537 .",
    "finally , applying lemmas  [ lemmab1 ] and  [ lemmab4 ] and exploiting the smoothness conditions on @xmath6 and  @xmath538 , we obtain that uniformly in @xmath13 and @xmath176 , @xmath539 combining the results on @xmath540 yields  [ conv - c ] .",
    "the result can be shown by using the techniques from theorem [ theo - nw ] together with a blocking argument .",
    "more details are given in the supplement  @xcite .",
    "[ [ appc ] ]    in this appendix , we prove the results concerning the smooth backfitting estimates of section  [ section - add ] . throughout the appendix ,",
    "conditions  [ add1 ] and  [ add2 ] are assumed to be satisfied .",
    "before we come to the proof of theorems  [ theorem - sbf-1 ] and [ theorem - sbf-2 ] , we provide results on uniform convergence rates for the kernel smoothers that are used as pilot estimates in the smooth backfitting procedure .",
    "we start with an auxiliary lemma which is needed to derive the various rates .",
    "[ lemmac - obs ] define @xmath541^d}}]$ ] .",
    "then uniformly for @xmath542 , @xmath543^d\\bigr ) + o\\bigl(t^{-{\\rho}/({1+\\rho } ) } \\bigr ) + o(h)\\ ] ] with @xmath32 defined in assumption  [ c1 ] and @xmath544^d}}- t_0}{t_0 } = o_p \\biggl(\\sqrt { \\frac{\\log t}{t h } } \\biggr).\\ ] ]    the proof can be found in the supplement @xcite .",
    "we now examine the convergence behavior of the pilot estimates of the backfitting procedure .",
    "we first consider the density estimates @xmath545 and @xmath290 .    [ lemma - add - kde ] define @xmath546 , @xmath547 and @xmath548 with @xmath549 .",
    "moreover , let @xmath550 .",
    "then @xmath551 } \\bigl| \\hat{p}_j\\bigl(u , x^j\\bigr ) - \\kappa_0\\bigl(x^j\\bigr ) p_j \\bigl(u , x^j\\bigr ) \\bigr| & = & o_p(v_{t,2 } ) + o(b_{t , r } ) + o(h ) , \\\\",
    "\\sup_{u , x^j , x^k \\in i_h } \\bigl| \\hat{p}_{j , k}\\bigl(u , x^j , x^k \\bigr ) - p_{j , k}\\bigl(u , x^j , x^k\\bigr ) \\bigr| & = & o_p(v_{t,3 } ) + o(b_{t , r } ) + o(h),\\\\[-35pt]\\end{aligned}\\ ] ]    @xmath552 } \\bigl| \\hat{p}_{j , k } \\bigl(u , x^j , x^k\\bigr ) - \\kappa_0 \\bigl(x^j\\bigr ) \\kappa_0\\bigl(x^k\\bigr ) p_{j , k}\\bigl(u , x^j , x^k\\bigr ) \\bigr| \\\\ & & \\hspace*{140.5pt}\\qquad\\qquad = o_p(v_{t,3 } ) + o(b_{t , r } ) + o(h).\\end{aligned}\\ ] ]    we only consider the term @xmath289 , the proof for @xmath290 being analogous . defining @xmath553^d ) k_h(u,\\frac{t}{t } ) k_h(x^j , x_{t , t}^j)$ ] with @xmath541^d}}]$ ] , we obtain that @xmath554^d}}- t_0}{t_0 } \\biggr]^{-1 } \\check{p}_j \\bigl(u , x^j\\bigr ) \\\\ & = & \\biggl [ 1 - \\frac{{t_{[0,1]^d}}- t_0}{t_0 } + o_p \\biggl(\\frac{{t_{[0,1]^d}}- t_0}{t_0 } \\biggr)^2 \\biggr ] \\check{p}_j\\bigl(u , x^j \\bigr).\\end{aligned}\\ ] ] by ( [ obs2 ] ) from lemma  [ lemmac - obs ] , this implies that @xmath555 uniformly for @xmath542 and @xmath556 $ ] . applying the proving strategy of theorem  [ theo - nw ] to @xmath557 completes the proof .",
    "we next examine the nadaraya ",
    "watson smoother @xmath291 . to this purpose",
    ", we decompose it into a variance part @xmath558 and a bias part @xmath559 .",
    "the decomposition is given by @xmath560 with @xmath561^d } } } \\sum_{t=1}^t i \\bigl(x_{t , t } \\in[0,1]^d\\bigr ) k_h \\biggl(u , \\frac{t}{t } \\biggr ) k_h\\bigl(x^j , x_{t , t}^j \\bigr ) \\varepsilon_{t , t } / \\hat{p}_j\\bigl(u , x^j \\bigr ) , \\\\ \\hat{m}_j^b\\bigl(u , x^j\\bigr ) & = & \\frac{1}{{t_{[0,1]^d } } } \\sum_{t=1}^t i \\bigl(x_{t , t } \\in[0,1]^d\\bigr ) k_h \\biggl(u , \\frac{t}{t } \\biggr ) k_h\\bigl(x^j , x_{t , t}^j \\bigr ) \\\\ & & \\hspace*{44.3pt } { } \\times\\biggl ( m_0 \\biggl(\\frac{t}{t } \\biggr ) + \\sum _ { k=1}^d m_k \\biggl ( \\frac{t}{t},x_{t , t}^k \\biggr ) \\biggr ) \\big/ \\hat{p}_j\\bigl(u , x^j\\bigr).\\end{aligned}\\ ] ] the next two lemmas characterize the asymptotic behavior of @xmath562 and @xmath559 .",
    "[ lemma - add - nw - var ] it holds that @xmath563 } \\bigl| \\hat{m}_j^a \\bigl(u , x^j\\bigr ) \\bigr| = o_p \\biggl ( \\sqrt { \\frac{\\log t}{t h^2 } } \\biggr).\\ ] ]    replacing the occurrences of @xmath308^d}}$ ] in @xmath562 by @xmath541^d}}]$ ] and then applying theorem [ theo - stochastic - part ] gives the result .",
    "[ lemma - add - nw - bias ] it holds that @xmath564 with @xmath565 \\setminus i_h$ ] and @xmath566 here , @xmath567 , \\\\",
    "\\beta(u , x ) & = & \\kappa_2 \\,\\partial_u m_0(u ) \\,\\partial_u",
    "\\log p(u , x ) \\\\ & & { } + \\sum_{k=1}^d \\biggl\\ { \\kappa_2 \\,\\partial_u m_k\\bigl(u , x^k \\bigr ) \\,\\partial_u",
    "\\log p(u , x ) + \\frac{\\kappa_2}{2 } \\,\\partial_{uu}^2 m_k\\bigl(u , x^k \\bigr ) \\\\ & & \\hspace*{31pt } { } + \\kappa_2 \\,\\partial_{x^k } m_k \\bigl(u , x^k\\bigr ) \\,\\partial_{x^k } \\log p(u , x ) + \\frac{\\kappa_2}{2 } \\,\\partial_{x^k x^k}^2 m_k \\bigl(u , x^k\\bigr ) \\biggr\\},\\end{aligned}\\ ] ] where the symbol @xmath568 denotes the partial derivative of the function @xmath569 with respect to @xmath359 and @xmath570 as well as @xmath571 for @xmath572 .",
    "as the proof is rather lengthy and involved , we only sketch its idea .",
    "a detailed version can be found in the supplement @xcite . to provide the stochastic expansion of @xmath573 in ( [ bias - expansion1 ] ) and ( [ bias - expansion2 ] )",
    ", we follow the proving strategy of theorem 4 in mammen et al .",
    "@xcite . adapting",
    "this strategy is , however , not completely straightforward .",
    "the complication mainly results from the fact that we can not work with the variables @xmath3 directly but have to replace them by the approximations @xmath235 . to cope with the resulting difficulties , we exploit ( [ obs1 ] ) and ( [ obs2 ] ) of lemma  [ lemmac - obs ] and use arguments similar to those for theorem  [ theo - nw ] .",
    "we finally state a result on the convergence behavior of the term @xmath272 .",
    "[ lemma - add - m0 ] it holds that @xmath574    the claim can be shown by replacing @xmath308^d}}$ ] with @xmath541^d}}]$ ] in the expression for @xmath272 and then using arguments from theorem  [ theo - nw ] .       using the auxiliary results from the previous subsection , it is not difficult to show that the high - level conditions ( a1)(a6 ) , ( a8 ) and ( a9 ) of mammen et al .",
    "@xcite are satisfied .",
    "we can thus apply their theorems  , which imply the statements of theorems [ theorem - sbf-1 ] and  [ theorem - sbf-2 ] . note that the high - level conditions are satisfied uniformly for @xmath542 rather than only pointwise . inspecting the proofs of theorems 13 in @xcite ,",
    "this allows us to infer that the convergence rates in ( [ rates1 ] ) hold uniformly over @xmath542 rather than only pointwise . a list of the high - level conditions together with the details of the proof can be found in the supplement  @xcite .",
    "i am grateful to enno mammen , oliver linton and suhasini subba rao for numerous helpful suggestions and comments . moreover",
    ", i would like to thank an associate editor and three anonymous referees for their constructive comments which helped a lot to improve the paper ."
  ],
  "abstract_text": [
    "<S> in this paper , we study nonparametric models allowing for locally stationary regressors and a regression function that changes smoothly over time . </S>",
    "<S> these models are a natural extension of time series models with time - varying coefficients . </S>",
    "<S> we introduce a kernel - based method to estimate the time - varying regression function and provide asymptotic theory for our estimates . </S>",
    "<S> moreover , we show that the main conditions of the theory are satisfied for a large class of nonlinear autoregressive processes with a time - varying regression function . </S>",
    "<S> finally , we examine structured models where the regression function splits up into time - varying additive components . </S>",
    "<S> as will be seen , estimation in these models does not suffer from the curse of dimensionality . </S>"
  ]
}