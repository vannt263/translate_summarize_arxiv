{
  "article_text": [
    "clustering analysis ( or hca ) is an extensively studied field of unsupervised learning . very useful in dimensionality reduction problems , we will study ways of using this clustering method with the aim of reducing ( or removing ) the need for human intervention .",
    "this problem of human intervention stems from the fact that hca is used when we do not know the correct number of clusters in our data ( otherwise we might use , say , k - means ) .",
    "while the ability to cluster data with an unknown number of clusters is a powerful one , we often need a researcher to interpret the results - or cutoff the algorithm - to recover a meaningful cluster number .",
    "while our motivation stems from dna micro - array data and gene expression problems , these methods can apply to any similarly structured scenario . specifically",
    ", we will analyze different existing automated methods for cutting off hca and propose two new ones .    in section ii we will discuss background material on hca and the existing methods and in section iii we will present some technical details on these methods and introduce our own .",
    "section 4 will contain results on simulated and actual data , and section 5 will examine data sampling procedures to improve accuracy .",
    "hierarchical clustering , briefly , seeks to pair up data points that are most similar to one another . with the agglomerative ( or bottom - up ) approach , we begin with @xmath0 data points forming singleton clusters .",
    "for each point , we measure the distance between it and its @xmath1 neighbors . the pair with the shortest distance between them is taken to form a new cluster .",
    "we then look at the distance between the @xmath2 points remaining and the newly formed cluster , and again pair off the two with shortest distance ( either adding to our 2-cluster , or forming another one ) .",
    "this process is repeated until we have a single cluster with @xmath0 points ( regardless of the absolute distance between points ) .",
    "naturally , this is a very good dimensionality reduction algorithm .",
    "unfortunately , it keeps going until we ve flattened our data to 1 dimension . in cases where in truth we have @xmath3 clusters ,",
    "this is problematic .",
    "the results of a hca are often expressed as a dendrogram , a tree - like graph that contains vital information about the distances measured in the clustering and the pairings generated .",
    "an example of a dendrogram can be seen in figure  [ fig:1 ] .",
    "briefly , horizontal lines denote pairings , and the height of those lines represent the distance that needed to be bridged in order to cluster the points together .",
    "that is , the smaller the height ( or jump ) of a pairing , the closer the points were to begin with .",
    "our goal is to find a way to say , once the full tree is made ,",
    " jumps beyond this point are not reasonable \" , and we can cutoff the algorithm , keeping only those clusters generated before that point .",
    "the problem of cutting off a dendrogram is one that researchers encounter often , but there are no reliable automated methods for doing it .",
    "often , the gap statistic is the only proposed automated method , as in  @xcite . as such",
    ", many researchers will inspect the finished dendrogram and manually select a cutoff point , based on their own judgment .",
    "apart from the obviously slow nature of this exercise , there is also the question of human error to consider - as well as bias . in cases where the cutoff is not easily determined , two different researchers may arrive at different conclusions as to the correct number of clusters - which could both be incorrect .",
    "algorithmic approaches aim to eliminate this , and range from simpler methods to more complex ones .",
    "an excellent summary of existing methods is given in  @xcite , which is in fact referenced in  @xcite .",
    "the latter , more importantly , develops the gap statistic .",
    "we will present the technical aspects in section iii , but we quickly discuss some properties here .",
    "first , the gap statistic is one of few methods that is capable of accurately estimating single clusters ( in the case where all our data belongs to one cluster ) , a situation often undefined for other methods .",
    "while it is rather precise overall , it requires the use of a  reference distribution \" , which must be chosen by the researcher .",
    "they put forward that the uniform distribution is in fact the best choice for unimodal distributions .",
    "a powerful result , it is still limited in other cases , and thus many researchers still take the manual approach",
    ". however , it generally outperforms other complex methods , as such we focus on the gap statistic .    on the other side of the complexity spectrum",
    ", we have variants of the  elbow method \" .",
    "the elbow method looks to explain the variance in the data as a function of the number of clusters we assign .",
    "the more clusters we assign , the more variance we can explain .",
    "however , the marginal gain from adding new clusters will begin to diminish - we choose this point as the number of clusters .",
    "a variant of this method , often applied to dendrograms , looks for the largest acceleration of distance growth  @xcite .",
    "while this method is very flexible , it can not handle the single - cluster case .",
    "we will look at both the elbow method variant and the gap statistic , as well as our own 2 methods we are presenting in this paper . while there are many other methods to compare them to",
    ", the gap statistic is quite representative of a successful ( if more complex ) solution - and tends to outperform the other known methods .",
    "the elbow method represents the more accepted simple approaches . in all tests in this paper",
    ", we use an agglomerative hierarchy , with average linkage and euclidean distance measure .",
    "the gap statistic is constructed from the within - cluster distances , and comparing their sum to the expected value under a null distribution .",
    "specifically , as given in  @xcite , we have for @xmath4 clusters @xmath5 @xmath6 - \\log w_k\\ ] ] where , with @xmath7 , @xmath8 that is , we are looking at the sum of the within - cluster distances @xmath9 , across all @xmath4 clusters @xmath5 .",
    "computationally , we estimate the gap statistic and find the number of clusters to be ( as per  @xcite ) @xmath10 where @xmath11 is the standard error from the estimation of @xmath12 .",
    "as mentioned ,  @xcite considers both a uniform distribution approach and a principal component construction . in many cases , the uniform distribution performs better , and this is the one we will use .",
    "this variant of the elbow method , which looks at the acceleration , is seen in  @xcite . a straightforward method",
    ", we simply look at the acceleration in jump sizes .",
    "so given the set of distances from our clustering @xmath13 , the acceleration can be written as @xmath14 we choose our number of clusters as the jump with the highest acceleration , giving us @xmath15 } \\left\\ { d_i-2d_{i-1}-d_{i-2 } \\right\\}.\\ ] ] while very simple and very fast , this method will never find the endpoints , ie , the @xmath0 singleton clusters and the single @xmath0-element cluster cases .",
    "the first method we propose is based on the empirical distribution of jump sizes .",
    "specifically , we look to the mode of the distribution @xmath16 , denoted @xmath17 , adjusted by the standard deviation ( @xmath18 ) .",
    "our motivation is that the most common jump size likely does not represent a good cutoff point , and we should look at a higher jump threshold . as such",
    ", we take the number of clusters to be @xmath19 where @xmath20 is a parameter that can be tuned .",
    "naturally , tuning @xmath20 would require human intervention or a training data set .",
    "as such , we set it to @xmath21 somewhat arbitrarily .",
    "our second method is even simpler , but is surprisingly absent from the literature .",
    "inspired by the elbow method , we look at the maximum jump difference - as opposed to acceleration . our number of clusters",
    "is then given by @xmath22}\\left\\ { d_i - d_{i-1 } \\right\\}.\\ ] ] this method shares the elbow method s drawback that it can not solve the single cluster case ( though it can handle singleton clusters ) , but we thought it prudent to examine as the literature seemed to focus on acceleration and not velocity .",
    "we present results of simulations on several numbers of true clusters , drawn from a @xmath23-dimensional normal distribution .",
    "each cluster is comprised of @xmath24 samples .",
    "we are most interested in tracking the success rate and the _ error size given an incorrect estimate_. that is , how often can we correctly estimate the number of clusters @xmath25 and when we ca nt , by how much are we off ?",
    "formally , this is given by @xmath26 and @xmath27 we chose this measure of the error to avoid under - estimating error size ( which would happen in a method that is often correct ) .",
    "the data used was drawn from a standard normal distribution , with cluster centers at @xmath28 , shown in figure  [ fig:2 ] . in the case of @xmath29 cluster , the first is taken , for @xmath23 clusters , the first two , and so on .",
    "we present the results of the methods on @xmath30 simulations below in tables i - iii , with the best results in bold .    .average cluster numbers over @xmath30 runs . [ cols=\"<,>,>,>,>\",options=\"header \" , ]     while the maximum difference method seems robust in the face of different sampling , it seems that this exercise has revealed some instability in the mode method , which has reverted back to a lackluster performance . to a much lesser extent ,",
    "the elbow method is having some more trouble as well .",
    "it seems more likely that the choice of sampling parameters could be the cause of the clustering in the biobase data in figure  [ fig:6 ] .",
    "more generally , we should look into determining optimal mixing parameters @xmath31 and @xmath32 and/or their impact on these methods .",
    "this mixing method does appear to perform better for the gene expression set than the previous choice of mixing parameters , which seems to confirm our hypothesis that there is perhaps an oversampling effect that can come into play , or something along those lines which much be explored more fully .",
    "we have developed two new empirical methods for clustering data in a hierarchical framework . while our methods are substantially faster than the existing gap statistic , we insist that they do not handle the single - cluster case .",
    "in other cases , our maximum difference method is at least comparable to the gap statistic and outperforms the elbow method .",
    "in addition , the use of the data mixing procedure presented can greatly improve performance ( especially for the mode method ) , leading to the maximum difference method outperforming the other 3 .",
    "lastly , these methods can be implemented in a few lines of code and should allow researchers to quickly utilize them at little computational cost .    in the future",
    "we hope to study the possibility of finding optimal mixing numbers @xmath31 and @xmath32 and the impact of the choice of these parameters of our results .",
    "hopefully , they are related to the instability detected in the mode method when using @xmath33 in our mixing procedure ."
  ],
  "abstract_text": [
    "<S> we propose two new methods for estimating the number of clusters in a hierarchical clustering framework in the hopes of creating a fully automated process with no human intervention . </S>",
    "<S> the methods are completely data - driven and require no input from the researcher , and as such are fully automated . </S>",
    "<S> they are quite easy to implement and not computationally intensive in the least . </S>",
    "<S> we analyze performance on several simulated data sets and the biobase gene expression set , comparing our methods to the established gap statistic and elbow methods and outperforming both in multi - cluster scenarios .    shell : bare demo of ieeetran.cls for journals    clustering , hierarchy , dendrogram , gene expression , empirical . </S>"
  ]
}