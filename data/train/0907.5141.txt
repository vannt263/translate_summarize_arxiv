{
  "article_text": [
    "distributed learning is a field that generalizes classical machine learning algorithms to a distributed framework . unlike the classical learning framework , where one has full access to all the data and has unlimited central computation capability , in the framework of distributed learning , the data are distributed among a number of agents , who have limited access to the data .",
    "these agents are capable of exchanging certain types of information , which , due to limited computational power and communication restrictions ( limited bandwidth or limited power ) , is usually restricted in terms of content and amount . research in distributed learning seeks effective learning algorithms and theoretical limits within such constraints on computation , communication , and confidentiality .    distributed learning can be categorized into many subareas . in terms of the way that the data are distributed , it can be categorized into homogeneous data ( instance / horizontally distributed data ) and heterogeneous data ( attribute / vertically distributed data ) . in terms of the structure of the entire system , it can be categorized into systems with a fusion center and systems without a fusion center .",
    "each category has its unique applications and challenges .",
    "we focus , in this paper , on the case in which data are attribute - distributed .",
    "the algorithm that we develop can be adapted to systems either with or without a fusion center .",
    "the homogeneous - data problems have been widely studied .",
    "two important types of models are established in @xcite and @xcite respectively : instance distributed learning with and without a fusion center .",
    "the relationship between the information transmitted among individual agents and the fusion center and the ensemble learning capability are discussed in these papers .",
    "classical learning algorithms are more easily adapted to the homogeneous cases because for each agent , the form of the classifier / estimator is exactly the same as that of the centralized learning algorithm .",
    "the homogeneity in the individual classifiers / estimators is a great advantage for designing distributed learning algorithms that compare and combine them .",
    "however , these advantages disappear in the heterogeneous data case , where different agents observe different attributes , and thus have many different forms of classifiers / estimators .",
    "this makes it harder to evaluate , compare and combine the estimators . nevertheless , there are some research results in this area ( e.g. , @xcite , @xcite ) .",
    "some basic ideas include voting / averaging , meta - learning , collective data mining , and residual refitting . the voting / averaging algorithm simply combines ( linearly ) the predictions of the individual agents .",
    "the training process is purely non - cooperative . in the meta - learning case ( see @xcite and @xcite ) , the fusion center seeks a more sophisticated way to integrate predictions of individual estimators by taking their predictions as a new training set ( learning of learning results ) , i.e. , the fusion center treats the output of individual estimators as the input covariates .",
    "although this hierarchical training scheme looks more delicate , it is still non - cooperative and hence fails to learn hidden rules in which covariates of different agents intertwine in a complicated way .",
    "in contrast , the collective data mining algorithms ( see @xcite , @xcite and @xcite ) are cooperative .",
    "they seek the information required to be shared among the agents so that the optimal estimator can be decomposed into an additive form without compromising the performance of the ensemble estimator ( compared to the estimator trained by the centralized algorithm ) . yet",
    "this requirement is rather strong and hence this technique relies on specific types of transformations , which require much prior knowledge of the problem , and thus is hard to generalize to other problems .",
    "the residual refitting algorithm , another cooperative training algorithm described in @xcite , has the advantage of not being dependent on individual learning algorithms . the only way that the agents communicate with each other is through their residuals .",
    "however , these algorithms are based on an additive model , and are susceptible to overtraining and pitfalls of local optima , though under some assumptions , optimality can be guaranteed .    in this paper , we develop another cooperative training scheme , using a modeling framework similar to that of the residual - refitting algorithms .",
    "however , instead of refitting the residuals directly , our new algorithm seeks to reshape the covariance matrix of the residuals generated by all the agents so that the linear combination of the estimators maintained by the agents can achieve a low ensemble test error .",
    "again , residuals are the only information that the agents communicate to each other , yet they are used more intelligently than in the case of residual - refitting . in the case",
    "when residual - refitting is guaranteed to achieve global optimality , our new algorithm , iterative covariance optimization algorithm ( icoa ) also achieves similar results - and due to its insusceptibility to overtraining , icoa usually outperforms .",
    "another important issue for a distributed learning system is the trade - off between the amount of information exchanged among the agents and the performance of the ensemble estimator / classifier . to study this relationship",
    ", the major challenge is how to quantify the information exchanged and the ensemble performance . in this paper , based on icoa with minimax protection",
    ", the relationship between the amount of information exchanged ( measured by the compression rate ) and the optimal test error of the ensemble estimator is discussed .",
    "more interestingly , an upper bound on the test error of the ensemble estimator is also derived .",
    "the rest of this paper is organized as follow . in section 2",
    ", we describe the basic model and abstract the problem of finding an optimal additive ensemble estimator into a two - stage optimization . in section 3",
    ", we analyze this optimization problem and introduce icoa , with its efficacy demonstrated by simulation . in section 4 , we discuss the problem of how to keep icoa functioning when the covariance is not accurately estimated , which leads to minimax protection , and we demonstrate the trade - off between data transmission and system performance with an upper bound on the test error with respect to the data compression rate .",
    "section 5 contains our conclusions .",
    "our discussion is based on an estimation / regression problem with attribute - distributed data .",
    "the estimation problem is specified as follow :    there are @xmath0 covariates ( or attributes ) @xmath1 and one outcome @xmath2 , so the entire data set of @xmath3 instances is comprised of @xmath4 where @xmath3 is the number of instances , @xmath5 is the @xmath6-th instance of @xmath7 , and @xmath8 is the @xmath6-th instance of @xmath2 .",
    "we assume that there exists a hidden deterministic function ( rule / hypothesis ) @xmath9 such that @xmath10 where @xmath11 is an independently drawn sample from a zero - mean random variable @xmath12 that is independent of @xmath1 and @xmath2 .",
    "suppose there are @xmath13 agents , each of which has only limited access to certain attributes .",
    "define @xmath14 to be the set of attributes accessible by agent @xmath15 , and define @xmath16 , assuming that @xmath17 .",
    "the outcome @xmath2 , with all its instances , is visible to all the agents .",
    "these assumptions specify the  attribute - distributed \" properties of our problem .    to highlight the distributed nature of the system ,",
    "we add an extra restriction : the only information that the agents can communicate with each other is their training residuals ( or information that can be locally derived from the training residuals ) .",
    "this is a reasonable assumption considering that the data observable by one agent are usually confidential or incompatible with the learning algorithm run by another agent .",
    "therefore , for these @xmath13 agents , each agent @xmath6 maintains an estimator @xmath18 of the outcome , which is a function that takes covariates @xmath19 as its input .",
    "given individual estimators @xmath18 fixed , the problem of finding an optimal ensemble estimator of additive form can be described as an optimization problem @xmath20 , \\label{optproblem0}\\ ] ] where @xmath21 are the weighting coefficients . moreover ,",
    "if we assume that each estimator has no  bias \" after training , or equivalently , if we assume that the residuals have zero mean , then it follows that @xmath22=\\mathbb{e}[y]$ ] .",
    "therefore , it is obvious that the sum of all weighting coefficients is equal to @xmath23 , i.e. @xmath24    consequently , we can rewrite the objective function as @xmath25\\right ) ^{2}\\right].\\ ] ] note that the @xmath6th term in the parentheses is the residual of the @xmath6th agent , defined as @xmath26 .",
    "therefore , the objective function can be rewritten as @xmath27.\\ ] ]    to simplify our derivation , define the covariance matrix of the residuals as @xmath28 where @xmath29_{ij } = \\mathrm{cov}(r_i , r_j)$ ] ; then the problem can be further simplified into a more concise form : @xmath30 where @xmath31{cccc}a_{1 } & a_{2 } & \\cdots & a_{d}\\end{array } \\right ]   ^{t } $ ] .",
    "this is our starting point for the distributed regression problem .",
    "the optimization problem of finding the best ensemble estimator of the form of a linear combination of individual estimators is equivalent to finding the best individual estimators that generate the most desirable residuals that cancel each other out .",
    "the problem described by ( [ p1 ] ) and ( [ p2 ] ) is readily solved if the covariance matrix @xmath28  is known and fixed .",
    "this is equivalent to the problem of finding the best linear combination of the estimators with these estimators given and fixed - a case that appears in the non - cooperative training algorithms .",
    "however , in a cooperative training algorithm , by communicating with each other , the agents have a chance to change their training residuals intelligently and repeatedly so as to reshape the covariance matrix @xmath28 and to minimize the ensemble error .",
    "it is this step that makes the problem interesting and difficult .",
    "therefore , the entire problem can be summarized as a two - stage optimization problem : @xmath32 note that @xmath28 is subject to training restrictions because the residual generated by each agent is not arbitrary . for @xmath33",
    ", it must be achievable in the form of @xmath34 , which is highly restrictive because of the space to which @xmath18 belongs .",
    "the first ( inner ) step of the optimization has a closed form solution ( solved by lagrange multipliers ) . when @xmath35 the minimum value @xmath36 is achieved : @xmath37 i.e. the minimum value is the inverse of the sum of all the elements of the inverse of @xmath28 .",
    "moreover , since @xmath28 is a covariance matrix , and thus must be positive definite , the second stage optimization problem is equivalent to @xmath38 it is necessary to bear in mind that @xmath28 is subject to training restrictions .",
    "the optimization problem described in ( [ opt2 ] ) is the key step of our algorithm .",
    "the most difficult step is to quantify the  training constraints \" of the covariance matrix of the residuals . to tackle this problem ,",
    "it is necessary to examine the inner structure of @xmath28 .",
    "as previously assumed , we have @xmath13 agents , and each agent @xmath6 maintains an estimator specified by the function @xmath39 .",
    "then , obviously , the covariance matrix @xmath28 can be expressed as @xmath40_{ij } = \\mathbb{e } \\left [ \\left ( y - f_i(x_{f_i } ) \\right ) \\left ( y - f_j(x_{f_j } ) \\right)\\right].\\ ] ] where @xmath29_{ij}$ ] stands for the element of @xmath28 in the @xmath6th row and @xmath15th column . however , for numerical purposes , we need to write the matrix in the form of a statistic by describing everything in terms of actual data .",
    "so we characterize the function @xmath18 by a vector @xmath41 , which is the prediction of the function @xmath18 on all the training data points of agent @xmath6 .",
    "similarly , we define @xmath42 as the value of the outcome @xmath2 for all data instances .",
    "then , the covariance matrix @xmath28 can be estimated by ( with the assumption of the unbiasedness of all the estimators ) @xmath40_{ij } = \\frac{1}{n}\\left [ \\left ( \\mathbf{y}-\\mathbf{f}_i\\right)^t \\left ( \\mathbf{y}-\\mathbf{f}_j\\right)\\right].\\ ] ]    with this notation , the optimization problem can now be converted into a more specific and implementable one : @xmath43 where @xmath44 denotes the space to which @xmath41 belongs , which depends on the class of functions in which @xmath18 resides .",
    "thus , the constraints on @xmath28 are implicitly included in the constraints of the vectors @xmath45 .",
    "the next step , ordinarily , is to massage the optimization problem and to prove the convexity of the objective function and the domain so that we can apply gradient descent algorithms and guarantee global optimality . yet for our problem , since the objective and constraints are both rather intricate and to some extent only implicitly specified , it is not very feasible to prove convexity without additional assumptions",
    ". therefore , we will directly develop an algorithm based on gradient descent and test the algorithm empirically before we delve deeply into the problem of global optimality .",
    "the first thing required for a gradient - based algorithm is to find an expression for the gradient of the objective @xmath46 with respect to @xmath41 . by rather lengthy and intricate computation , a closed form expression for the gradient",
    "is given by @xmath47_{ij}\\right)\\\\ & -&\\frac{2}{|\\mathbf{a}|}\\left(\\sum_{j\\ne i}\\left ( \\mathbf{f}_{k}\\mathbf{-f}_{j}\\right)[\\mathbf{b } ^{\\ast}(k)]_{ij}\\right ) , \\label{grad1}\\end{aligned}\\ ] ] where @xmath48 , @xmath49 denotes the adjoint of @xmath28 , and @xmath50 is a @xmath51 matrix given by @xmath52_{ij}=(\\mathbf{f}_k-\\mathbf{f}_{i+\\zeta_{ik}})^t(\\mathbf{f}_k-\\mathbf{f}_{j+\\zeta_{jk}})\\ ] ] where @xmath53 if @xmath54 , and @xmath55 if @xmath56 .",
    "this provides us with a feasible yet complex algorithm for estimating the gradient .",
    "it is worth noting that the gradient depends only on the residuals of the agents ( through easy conversion of @xmath41 as @xmath57 ) . in practice",
    ", we can also use numerical methods to estimate the gradient , i.e. we can perturb the components of @xmath41 , compute the change of the objective and use the ratio between the change and the perturbation as an approximation of that component of the gradient .",
    "there is another important issue before we develop the algorithm : we can search , by gradient descent , for a _ desirable _ @xmath58 to replace @xmath41 so that we can increase the value of the objective function , yet @xmath58 might not be achievable because agent @xmath6 may not be able to find a new estimator @xmath59 such that @xmath58 is realizable by @xmath60 .",
    "therefore , what is reasonable to do is to use @xmath58 as the new outcome for agent @xmath6 ( instead of @xmath42 ) to train and find a new estimator @xmath18 , i.e. , we find the best projection of @xmath58 onto the space @xmath44 .    based on the description above , the basic idea of icoa is summarized as follows .",
    "first , cooperatively , all the agents determine the present covariance matrix of their residuals @xmath28 .",
    "then , one by one , each agent finds its estimate of the gradient @xmath61 , after which the selected agent @xmath6 updates its vector @xmath41 to @xmath62 using gradient descent .",
    "after that , agent @xmath6 projects @xmath63 onto @xmath44 by training with @xmath63 as the outcome and thus obtains the new version of @xmath18 . then",
    ", after agent @xmath6 updates its residual , all the agents update their estimates of covariance matrix @xmath28 .",
    "more precisely , the algorithm is as shown below :   +   +     +      in order to compare distributed regression implemented by icoa to other multi - dimensional regression algorithms ( distributed or non - distributed ) , we use three functions used in @xcite as the hidden rule to generate our simulation training data sets .",
    "the three functions and the corresponding joint distribution of the covariates are :    * friedman-1 : @xmath64where @xmath65,\\text{~}j=1,\\ldots,5 $ ] ; * friedman-2 : @xmath66 where @xmath67,\\text{~}x_2\\sim u[40\\pi,560\\pi],\\ ] ] @xmath68,\\text{~}x_4\\sim u[1,11].\\ ] ] * friedman-3 : @xmath69 where the distributions of the covariates are the same as those of friedman-2 .",
    "all the covariates are independent of one another , and before running the algorithm , the outcomes are normalized to the range @xmath70 $ ] . also , to highlight the effects of the distributed nature of the system , the independent additive white noise @xmath71 is set to a negligible level in our simulation .",
    "furthermore , it is worth pointing out that in friedman-2 and friedman-3 , attribute @xmath72 is irrelevant , serving purely as a nuisance variable .",
    "the structure of the entire distributed system is as follows .",
    "there are 5 attributes , @xmath73 , and we assume that there are 5 agents , with agent @xmath6 observing attribute @xmath74 exclusively .",
    "each agent uses a regression tree as its individual estimator .    with the setup above",
    ", the simulation results of the two algorithms are as shown in table [ tab2 ] . as a comparison , we ran two other distributed regression algorithms : averaging and residual refitting ( or icea , see @xcite for details ) .",
    ".test errors ( mean squared ) of icoa , the residual refitting algorithm and the averaging algorithm on friedman-1 , -2 and -3 . [ cols=\"^,^,^,^\",options=\"header \" , ]     it is worth pointing out two phenomena .",
    "first , when icoa with minimax protection converges , the performance is almost independent of the compression rate @xmath75 .",
    "second , given @xmath75 , when the value of @xmath76 is above a certain level , icoa almost always converges , yet below that level , icoa does not converge .",
    "these two phenomena allow us to find an optimal @xmath76 for every given @xmath75 , so that we can optimize the performance of icoa under a given compression rate .    in table",
    "[ tab3 ] , another dramatic phenomenon is the case for @xmath77 and @xmath78 . since we have only @xmath79 training data instances , this means in each iteration , we use only @xmath80 pairs of numbers to estimate the covariance between two agents . and minimax protection with properly selected @xmath76 , enables us to achieve a decent test mean square error of @xmath81 , only about @xmath82 times of the optimal value @xmath83 .",
    "yet only @xmath84 of the data transmission is needed compared with the amount needed in the optimal case ( after taking into consideration the longer convergence time ) .",
    "thus icoa provides us with a very useful tool to trade off between performance and data transmission .      from the simulation results shown in table [ tab3 ] ,",
    "it is of interest to investigate the relationship between the compression rate @xmath75 and the optimal performance ( measured by test error ) of the system .",
    "as analyzed previously , the key is to select a proper @xmath76 so that we neither under - protect icoa ( leading to unstable convergence ) nor over - protect ( leading to worse performance ) .",
    "this requires us to investigate the statistical properties of the estimator of the correlation coefficient between two random variables . in @xcite",
    ", it is shown that the pivot statistic @xmath85 of the sample correlation coefficient has the student s t - distribution ; that is , @xmath86 where @xmath3 is the number of data instances .",
    "therefore the @xmath87 confidence interval of the correlation coefficient is given by @xmath88 $ ] , where @xmath89 .",
    "if we assume that the largest variance of all the residuals is @xmath90 , then an approximation to the optimal @xmath76 ( as a function of @xmath75 ) can be given by @xmath91 the basic idea is to find the smallest @xmath76 that covers , with high probability , the possible domain of the covariance matrix , given a crude estimate @xmath92 .    with this approximation",
    ", we are able to develop an upper bound on the test mean square error as a function of @xmath75 .",
    "define @xmath93 as the covariance matrix ( accurate ) of the residuals of all individual estimators before we run icoa . for each step ,",
    "icoa with minimax protection improves the test error ( not merely training error ) , because minimax protection guarantees , with high probability , that the true covariance matrix is in the range @xmath94 defined in ( [ defc ] ) .",
    "therefore , the solution to @xmath95 with constraint ( [ p2 ] ) provides us with an upper bound ( with high probability ) on the generalization error with respect to the compression rate @xmath75 .",
    "figure [ upperbound ] illustrates the comparison between this upper bound and the simulated optimal performance .",
    "in this paper , we have shown that icoa , as a cooperative training algorithm , demonstrates its efficacy for finding an optimal ensemble estimator of additive form , while demonstrating an insusceptibility to overtraining . moreover",
    ", minimax protection provides us with a tool to run icoa when covariances are not accurately estimated , and hence enable us to trade off between performance and data transmission .",
    "minimax protection , combined with icoa , also helps us to develop an upper bound on the test error for the ensemble estimator .",
    "99 j.  b.  predd , s.  r.  kulkarni and h.  v.  poor , `` distributed learning in wireless sensor networks , '' _ ieee signal processing magazine _ , vol .",
    "5669 , 2006 . j.  b.  predd , _ topics in distributed inference _",
    "thesis , department of electrical engineering , princeton university , princeton , nj , 2006 .",
    "s.  merugu and j.  ghosh , `` a distributed learning framework for heterogeneous data sources , '' _ proceeding of the 11th acm sigkdd international conference on knowledge discovery in data mining _ , vol .  1 , no .  1 ,",
    "pp .  208 - 217 , chicago , il , 2005 .",
    "h.  zheng , s.  r.  kulkarni and h.  v.  poor ,  dimensionally distributed learning : models and algoithm , \" _ proceedings of the 11th international conference on information fusion _ , pp .  1 - 8 , cologne , germany , 2008 .    h.  kargupta , b.  park , d.  hershberger and e.  johnson ,  collective data mining : a new perspective toward distributed data mining , \" _ advances in distributed and parallel knowledge discovery _",
    ", aaai press / the mit press , cambridge , ma , 1999 .    s.  mcconnell and d.  skillicorn ,  building predictors from vertically distributed data , \" _ proceedings of the 2004 conference of the centre for advanced studies on collaborative research _ , markham , ontario , canada , pp",
    "150 - 162 , 2004 .",
    "h.  kargupta , et al . ,",
    " collective data mining from distributed vertically partitioned feature space , \" _ proceedings of the workshop on distributed data mining _ , pp 70 - 91 , new york , ny , 1998 .",
    "d.  hershberger and h.  kargupta,distributed multivariate regression using wavelet - based collective data mining , \" _ journal of parallel and distributed computing _ ,",
    "61 , issue  3 , pp .",
    "372 - 400 , 2001 .",
    "chan and s.  stolfo ,  toward parallel and distributed learning by meta - learning , \" _ working notes of the aaai workshop , knowledge discovery in databases _ , pp .",
    "227 - 240 , washington d.c . , 1993"
  ],
  "abstract_text": [
    "<S> this paper introduces a modeling framework for distributed regression with agents / experts observing attribute - distributed data ( heterogeneous data ) . under this model , a new algorithm , the iterative covariance optimization algorithm ( icoa ) , is designed to reshape the covariance matrix of the training residuals of individual agents so that the linear combination of the individual estimators minimizes the ensemble training error . </S>",
    "<S> moreover , a scheme ( minimax protection ) is designed to provide a trade - off between the number of data instances transmitted among the agents and the performance of the ensemble estimator without undermining the convergence of the algorithm . </S>",
    "<S> this scheme also provides an upper bound ( with high probability ) on the test error of the ensemble estimator . </S>",
    "<S> the efficacy of icoa combined with minimax protection and the comparison between the upper bound and actual performance are both demonstrated by simulations .    distributed learning , heterogeneous data , cooperative training </S>"
  ]
}