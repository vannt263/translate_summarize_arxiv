{
  "article_text": [
    "in bayesian inference the information on an unknown parameter @xmath0 given an observed dataset @xmath1 is contained in the posterior distribution @xmath2 .",
    "when a posterior distribution does not belong to a well - characterized family of distributions , markov chain monte carlo ( mcmc ) is a standard computational approach to bayesian inference via sampling from the posterior distribution .",
    "typical examples of mcmc include the metropolis - hastings ( mh ) algorithm ( metropolis et al .",
    "1953 , hastings 1970 ) and the gibbs sampler ( geman and geman 1984 , gelfand and smith 1990 , tanner and wong 1987 ) .",
    "thorough reviews of recent developments on monte carlo methods and their applications in bayesian computation can be found in chen et al .",
    "( 2001 ) and liu ( 2008 ) .",
    "the posterior mean @xmath3 and other expectations are usually approximated from a monte carlo sample to summarize the posterior distribution .",
    "however , these unconditional expectations may not offer good summaries of the information for bayesian inference when a posterior distribution has multiple local modes .",
    "one can easily construct a multimodal posterior distribution of which the mean is located in a low - density region and thus using it as an estimator for @xmath0 lacks a conventional interpretation . to extract more information contained in a multimodal posterior distribution , it is desired to identify all major modes and calculate various statistics in appropriate neighborhoods of these modes .    to achieve these tasks , we propose to partition the sample space of @xmath0 into a collection of domains such that the posterior distribution restricted to each domain is unimodal .",
    "the most parsimonious partition that minimizes the number of domains is to use the domains of attraction ( to be defined rigorously later ) of the local modes .",
    "take the trimodal distribution @xmath4 in figure  [ fig : intro ] as an illustration .",
    "the space is partitioned into three domains , denoted by @xmath5 and @xmath6 : each domain contains exactly one local mode ; if we move any @xmath7 @xmath8 along the trajectory that always follows the gradient direction of @xmath4 , it will eventually reach the mode in the domain @xmath9 .",
    "we may then calculate various conditional expectations on these domains , @xmath10 $ ] ( @xmath11 ) , for different functions @xmath12 . together with the probability masses of the domains ,",
    "@xmath13 , they provide more informative summaries of the distribution @xmath4 than unconditional expectations . such a summary",
    "is called a domain - based representation ( dr ) for @xmath4 .    , title=\"fig:\",width=240 ] +    although desired , construction of drs for an arbitrary distribution is very challenging in practice .",
    "sufficient monte carlo samples from domains of all local modes are necessary for estimating drs , but efficient sampling from a multimodal distribution has always been a hard problem . in this article",
    ", we develop a computational method that is able to construct domain - based representations for an arbitrary multimodal distribution .",
    "we partition the sample space into domains of attraction and utilize an iterative weighting scheme aiming at sampling from each domain with an equal frequency .",
    "the weighting scheme was proposed by wang and landau ( 2001 ) , and further developed and generalized by liang ( 2005 ) , liang et al .",
    "( 2007 ) and atchad and liu ( 2010 ) among others",
    ". however , a direct application of these existing methods can not provide accurate estimation of drs , due to at least two reasons .",
    "first , sample space partition used in these methods is usually predetermined according to a set of selected density levels .",
    "but partitioning the space into domains of attraction , as employed in our method , can not be completed beforehand because it is a nontrivial job to detect all local modes and their domains in real applications .",
    "second , the above methods mostly rely on simple local moves and lack a coherent global move to jump between different local modes . to obtain accurate estimation of drs",
    ", we propose a dynamic scheme to partition the sample space into domains of attraction and devise a global move that utilizes estimated drs along sampling iterations to enable fast transitions between multiple domains . since the main feature of our method is to sample from multiple domains and construct drs , we call it the multi - domain ( md ) sampler",
    ".    the md sampler can be applied to a wide range of bayesian inference problems and it is particularly powerful in tackling problems with complicated posterior distributions .",
    "although there are many such applications in different fields , this article mainly concerns structural learning of causal bayesian networks from experimental data . learning network structure via",
    "monte carlo sampling is very challenging as the multimodality of the posterior distribution is extremely severe ( friedman and koller 2003 , ellis and wong 2008 , liang and zhang 2009 ) . in this problem ,",
    "a domain of attraction is defined by a set of network structures , each represented by a directed acyclic graph ( dag ) . in a sense ,",
    "the goal of the md sampler is to construct a detailed landscape of the posterior distribution , which can provide new insights into the structural learning problem .",
    "application of our method to a scientific problem is illustrated by a study on constructing protein - signaling networks from single - cell experimental data .",
    "a living cell is highly responsive to its environment due to the existence of widespread and diverse signal transduction pathways .",
    "these pathways constitute a complex signaling network as cross - talks usually exist between them .",
    "knowledge of the structure of this network is critical to the understanding of various cellular behaviors and human diseases .",
    "recent advances in biotechnology allow the biologist to measure the states of a collection of molecules on a cell - by - cell basis .",
    "such large - scale data contain rich information for statistical inference of signaling networks , but powerful computational methods are needed given the complexity in the likelihood function and the posterior distribution . with the md sampler , not only",
    "can we build a signaling network from the posterior mean graph , but also we may discover new pathway connections revealed by different domains of the posterior distribution , which are not accessible by other approaches .    the remaining part of this article is organized as follows .",
    "section  [ sec : doa ] defines the domain of attraction and domain - based representation .",
    "in section  [ sec : md ] we develop the md sampler and its estimation of drs , with convergence and ergodicity of the sampler established in appendix .",
    "the method is tested in section  [ sec : exrm ] on an example in euclidean space and implemented in section  [ sec : bn ] for bayesian inference of network structure with a simulation study .",
    "section  [ sec : signaling ] is the main application to the construction of signaling networks in human t cells .",
    "the article concludes with a discussion on related and future works .",
    "let @xmath14 , @xmath15 , be the density of the target distribution .",
    "suppose that @xmath14 is differentiable and denote by @xmath16 the gradient of @xmath17 at @xmath18 .",
    "define a differential equation @xmath19 and write a solution path of this equation as @xmath20 , where @xmath21 is a chosen initial point . under some mild regularity conditions",
    ", @xmath22 converges to a local mode of @xmath14 , which is the basic intuition behind the gradient descent algorithm to maximize @xmath14 .",
    "denote by @xmath23 all the local modes , including the global mode , of @xmath14 .",
    "for @xmath24 , let @xmath25 and define the domain partition index by @xmath26 it maps @xmath18 to the index of the local mode to which the solution path starting at @xmath18 converges .",
    "[ def : domain ] the domain of attraction of @xmath27 is @xmath28 for @xmath29 . for simplicity we may call @xmath30 an attraction domain or a domain of @xmath17 .    if the stationary points of @xmath14 have zero probability mass , then @xmath31 form a partition of the sample space @xmath32 except for a set of zero probability mass .",
    "this is the default setting for this article .",
    "let @xmath33 be a @xmath17-integrable function of @xmath18 .",
    "write the probability mass of @xmath30 and the conditional expectation of @xmath34 given @xmath35 as @xmath36=\\frac{1}{\\lambda_k } \\int_{d_k } h({\\mathbf{x } } ) p({\\mathbf{x } } ) d{\\mathbf{x } } , \\label{eq : muhk}\\end{aligned}\\ ] ] respectively , for @xmath29 .",
    "the domain - based representation of @xmath12 with respect to the distribution @xmath17 is a @xmath37 array , @xmath38 .",
    "the dr is equivalent to the probability mass function of @xmath39 $ ] that assigns probability @xmath40 to @xmath41 for @xmath29 .",
    "it provides the expectation @xmath42=\\sum_k   \\lambda_k \\mu_{h , k}$ ] and the decomposed contributions from the attraction domains of @xmath14 .",
    "such a representation gives an informative low - dimensional summary of a multimodal distribution .",
    "for a complex distribution with many local modes , however , we can not afford to estimate @xmath43 for every domain when @xmath44 is too large and are less interested in domains of negligible probability masses ( @xmath40 very close to zero ) . due to these reasons",
    "we define domain - based representations with respect to a set of local modes @xmath45 .",
    "index all the local modes as @xmath46 @xmath47 .",
    "define the domain partition index with respect to @xmath48 by @xmath49 if @xmath50 and @xmath51 otherwise .",
    "then the sample space @xmath52 can be partitioned into @xmath53 for @xmath54 , where @xmath30 is the domain of @xmath55 and @xmath56 .",
    "the dr of @xmath12 with respect to @xmath48 is defined by the array @xmath57 , where @xmath58 and @xmath59 are defined for @xmath60 similarly as in equations ( [ eq : lambdak ] ) and ( [ eq : muhk ] ) , respectively .",
    "note that one can still obtain @xmath42=\\sum_{k=0}^m   \\lambda_k \\mu_{h , k}$ ] after merging @xmath61 into @xmath60 .",
    "there is a geometric interpretation for the attraction domains of a posterior distribution .",
    "suppose that @xmath62 is a sample from an unknown distribution @xmath63 .",
    "we assume a parametric family @xmath64 , @xmath65 , as the model for @xmath66 and put a prior @xmath67 on the unknown parameter @xmath0 .",
    "the posterior distribution of @xmath0 is given by @xmath68   } \\ ] ] when the sample size @xmath69 is large , where @xmath70 = \\int \\log [ f({\\mathbf{y}}\\mid{\\bm{\\theta } } ) ] \\psi ( { \\mathbf{y } } ) d{\\mathbf{y}}$ ] .",
    "denote the kullback - leibler ( kl ) divergence between @xmath71 and @xmath72 by @xmath73 \\psi({\\mathbf{y } } ) d{\\mathbf{y}}.\\ ] ] then , @xmath74 $ ] when @xmath69 is large . in the space of density functions , we can regard @xmath75 as the  distance \" to the point @xmath71 from a point in the manifold @xmath76 .",
    "note that @xmath71 is not necessarily in @xmath77 if our model assumption on @xmath66 is incorrect",
    ". then @xmath78 may be interpreted as a boltzmann distribution on the manifold @xmath77 under a potential field @xmath79 .",
    "this potential pushes every @xmath72 , indexed by @xmath0 , towards @xmath71 , and the collection of @xmath0 which will be driven to an identical stationary point in @xmath77 forms an attraction domain of @xmath78 .",
    "to develop an algorithm that is able to construct domain - based representations with respect to the target distribution @xmath17 , it is necessary to identify the attraction domain of any @xmath24 .",
    "when @xmath14 is differentiable , this can be achieved by application of the gradient descent ( gd ) algorithm that finds local modes of @xmath14 , or @xmath80 for computational convenience .",
    "generalization to the space of network structures will be discussed later .",
    "a naive two - step approach to the construction of drs is quite obvious .",
    "we may first apply a monte carlo algorithm to simulate a sample @xmath81 from @xmath14 or from a diffuse version of @xmath14 , e.g. , @xmath82^{1/\\tau}$ ] for @xmath83 as used in parallel tempering ( geyer 1991 ) . then , for every @xmath84 we determine @xmath85 by a gd search initiated at @xmath86 to find to which domain it belongs .",
    "this approach partitions the sample into attraction domains so that we can estimate the probability masses and conditional expectations for all identified domains .",
    "although simple to implement , this two - step approach has a few drawbacks in terms of efficiency . without a careful and specific design",
    "the monte carlo algorithm , even targeting at a diffuse version of @xmath14 , may not generate enough samples from all major domains or may completely miss some modes . as a result ,",
    "estimation on some attraction domains may be inaccurate or unavailable .",
    "in addition , this approach does not utilize the information on the target distribution provided by the constructed drs . to overcome these drawbacks",
    ", we develop the md sampler that may achieve simultaneously an efficient simulation from a multimodal distribution and an accurate construction of domain - based representations , with comparable computational complexity as the naive two - step approach .",
    "we wish to sample sufficiently from the majority of attraction domains . however , the density at the boundary between two neighboring domains is often exponentially low ( e.g. , figure  [ fig : intro ] ) , which makes it difficult for an mh algorithm or a gibbs sampler to jump across multiple domains .",
    "thus , we need to allow the sampler to generate enough samples from such low - density regions that connect different domains .",
    "these considerations motivate the following double - partitioning design in the md sampler .",
    "suppose that we are given a set of local modes of @xmath14 , @xmath87 , which may include the global mode .",
    "given @xmath88 , define the density partition index @xmath89 if @xmath90 for @xmath91 .",
    "we partition the space @xmath52 into @xmath92 subregions , @xmath93 where @xmath94 is the domain partition index with respect to @xmath48 .",
    "then , the attraction domain of the local mode @xmath27 is @xmath95 .",
    "note that some @xmath96 may be empty ; if @xmath97 then all @xmath96 for @xmath98 are empty . in what follows ,",
    "we only consider nonempty subregions . for a given matrix @xmath99 ,",
    "define a working density @xmath100 where @xmath101 is an indicator function .",
    "let @xmath102 such that @xmath103 .",
    "then , the probability masses of @xmath96 are identical under @xmath104 .",
    "sampling from @xmath104 has two immediate implications .",
    "first , the sample sizes on the attraction domains , @xmath105 , will be comparable , and thus , domain - based representations can be constructed with a high accuracy .",
    "note that commonly used mcmc strategies for multimodal distributions , such as tempering , can not generate samples of comparable sizes from different domains .",
    "second , the sampler will stay in low - density regions ( e.g. , @xmath106 ) for a substantial fraction of time , which makes it practically possible to jump between domains . conversely , domain - based representations may be utilized to design efficient local and global moves for sampling from @xmath104 .",
    "we may construct online estimate of the covariance matrix on the domain of a local mode , which can be used for tuning the step size of a local move in this domain .",
    "for a multimodal distribution , tuning step size for each domain is more useful than tuning the overall step size ( harrio et al .",
    "once we have identified sufficient local modes and estimated covariances of their respective domains , we can use them to design global moves that may jump from one domain to another .",
    "as one can see , these proposals can be implemented only if we have partitioned samples into attraction domains .    for @xmath107 ,",
    "let @xmath108 be a proposal from @xmath18 to @xmath109 and @xmath110 a distribution with parameters @xmath0 and @xmath111 .",
    "we first develop the main algorithm of the md sampler , assuming that the density ladder @xmath112 is fixed and @xmath113 local modes @xmath48 have been identified .",
    "dynamic update of these parameters will be discussed in section  [ sec : burnin ] as the burn - in algorithm .",
    "let @xmath114 be a map from @xmath32 to @xmath115 for @xmath116 .",
    "[ alg : main ] initialize @xmath117 , parameters @xmath118 @xmath119 , @xmath120 , and @xmath121 . set @xmath122 . for @xmath123",
    ":    1 .   draw @xmath66 from @xmath124 with probability @xmath125 or from the mixture distribution @xmath126 with probability @xmath127 .",
    "2 .   determine the density partition index @xmath128 and perform a gd search initiated at @xmath66 to determine the domain partition index @xmath129 .",
    "3 .   accept or reject @xmath66 according to the mh ratio targeting at @xmath130 to obtain @xmath131 .",
    "4 .   for @xmath54 and @xmath91 update @xmath132 for @xmath116 update @xmath133 { \\bm{1}}(i_m({\\mathbf{x}}^{t+1})=k),\\ ] ] and determine @xmath134 .    we may regard @xmath135 as a weight for the subregion @xmath96 .",
    "after each visit to @xmath96 , the weight @xmath136 increases by @xmath137 unit ( [ eq : updatew ] ) , which decreases the probability mass of @xmath96 under the working density @xmath138 ( [ eq : weightds ] ) .",
    "such a weighting scheme aims to visit every @xmath96 uniformly .",
    "there are two typical choices of @xmath139 .",
    "the first choice follows the standard design in stochastic approximation which employs a predetermined sequence such that @xmath140 and @xmath141 for @xmath142 ( andrieu et al .",
    "2005 , andrieu and moulines 2006 , liang et al .",
    "the second design , originally proposed by wang and landau ( 2001 ) , adjusts @xmath137 in an adaptive way .",
    "however , there is difficulty in establishing its convergence ( atchad and liu 2010 ) , and thus we adopt a modified wang - landau ( mwl ) design to update @xmath139 in the md sampler .",
    "initialize @xmath143 for all @xmath144 and @xmath145 in algorithm  [ alg : main ] .",
    "the mwl update at iteration @xmath146 is given below .",
    "[ mwldesign ] if @xmath147 , set @xmath148 ; otherwise :    * set @xmath149 for all @xmath150 and calculate @xmath151 , where @xmath152 is the average of all @xmath153 . *   if @xmath154 , set @xmath155 ; otherwise , set @xmath156 and @xmath157 for all @xmath150 .    that is , if @xmath158 we decrease @xmath137 ( @xmath159 ) only when the sampler has visited every subregion @xmath96 with a roughly equal frequency since our last modification of @xmath137 .",
    "let @xmath160 . for @xmath161",
    "the update becomes deterministic with @xmath162 , where @xmath163 .",
    "the default setting for all the results in this article is given by @xmath164 , @xmath165 and @xmath166 .",
    "under some regularity conditions and the mwl update of @xmath139 , @xmath167 after being normalized to sum up to one , @xmath168 @xmath169 as @xmath170 .",
    "see theorem  [ thm : convergence ] in appendix for more details . if @xmath171 , we often choose @xmath172 so that @xmath173 is close to the covariance matrix of the conditional distribution @xmath174 $ ] , where @xmath175 .",
    "we use the mode @xmath27 instead of the mean because the mode can be obtained accurately via a gd algorithm .",
    "the use of @xmath176 in equation ensures that @xmath177 is positive definite if @xmath178 is positive definite .",
    "there are two types of proposals in step 1 of the algorithm , a local proposal @xmath124 and a mixture distribution proposal .",
    "one advantage of partitioning samples into attraction domains is embodied in the mixture distribution proposal , in which we randomly draw a domain partition index @xmath179 and then propose a sample @xmath66 from @xmath180 .",
    "equal mixture proportions ( @xmath181 ) are used because a uniform sampling across domains is preferred .",
    "the default choice of the distribution @xmath180 in @xmath182 is @xmath183 for @xmath116 , which gives a mixture normal proposal that matches the mode and the covariance on each domain of the working target @xmath130 .",
    "this proposal uses a mixture distribution to approximate the multimodal target",
    ". it can generate efficient global jumps from one domain to another if @xmath130 on the domain @xmath30 can be well approximated by @xmath184 with the identified mode @xmath27 and the estimated @xmath178 .",
    "for simplicity we call this proposal the _ mixed jump_. the typical design for @xmath124 in @xmath182 is to proposal @xmath185 , where @xmath186 is a scalar and @xmath187 is the identity matrix .",
    "however , when the covariances are very different between domains , using a single local proposal may cause high autocorrelation , since the step size might be either too big for domains with small covariances or too small for those with large covariances , or both . in this case , we may incorporate an adaptive local proposal , @xmath188 , in addition to @xmath124 , such that the learned covariance structure of a domain is utilized to guide the local proposal .",
    "this shows another advantage of the domain - partitioning design .",
    "we summarize the unique features of the main algorithm .",
    "first , domain partitioning is incorporated in the framework of the wang - landau ( wl ) algorithm .",
    "this allows a more uniform sampling from different domains , which facilitates construction of drs . at each iteration ,",
    "a gd search is employed to determine @xmath129 and thus the computational complexity of this algorithm is comparable to the naive two - step approach .",
    "second , an adaptive global move , the mixed jump , is proposed given drs constructed along the iteration , which utilizes identified modes and learned covariances to achieve between - domain moves .",
    "[ rmk : diagnostics ] verification of the regularity conditions for convergence of the algorithm ( see appendix ) is recommended before application .",
    "furthermore , we suggest a few convergence diagnostics that can be conveniently used in practice .",
    "first , @xmath189 should be small enough at the final iteration and the frequency of visiting different @xmath96 should be roughly identical .",
    "second , @xmath190 and @xmath191 should have converged with an acceptable accuracy .",
    "violations of these two criteria indicate that more iterations may be necessary .",
    "third , the adaptive parameters used in the mixed jump ( @xmath178 ) should always stay in a reasonable range .",
    "for example , if @xmath178 is a covariance matrix , one may check whether its eigenvalues are close to zero or unreasonably large , which may indicate divergence of the current run .",
    "if the last criterion is not satisfied , it is suggested to reinitialize the md sampler with a smaller @xmath192 .      in practical applications of the md sampler , the density ladder @xmath112 and the local modes",
    "@xmath193 are updated dynamically in a burn - in period before the main algorithm ( algorithm [ alg : main ] ) .",
    "the dynamic updating schemes are crucial steps for constructing domain - based representations in real applications , as one can not partition the sample space into domains of attraction beforehand .",
    "we set @xmath194 as an evenly spaced sequence so that @xmath195 is a constant for @xmath196 .",
    "let @xmath197 be the density ladder and @xmath198 be the set of @xmath199 identified modes at iteration @xmath84 .",
    "let @xmath200 be the maximum number of modes to be recorded and denote by @xmath201 the mode of the domain that @xmath18 belongs to .",
    "let @xmath202 be the zero matrix with dimension determined by the context .",
    "the following routine is used to update @xmath203 when a new sample @xmath66 is proposed .",
    "[ rt : updatemodes ] let @xmath204 .    * if @xmath205 and @xmath206 , set @xmath207 , @xmath208 , and initialize @xmath209 for all @xmath145 ; * if @xmath205 , @xmath210 and @xmath211 , set @xmath212 , @xmath213 for @xmath214 , @xmath215 , and assign @xmath216 and @xmath217 for all @xmath145 .",
    "* otherwise set @xmath218 and @xmath215 .",
    "according to this routine , we record at most the @xmath200 highest modes identified during the burn - in period .",
    "if there are more than @xmath200 modes , algorithm  [ alg : main ] will construct drs with respect to the recorded modes .",
    "the weights @xmath219 are updated when a new mode replaces an old one in @xmath220 , for which the assignment operator ` @xmath221 ' is used to distinguish from equality .",
    "the density ladder @xmath197 is adjusted such that @xmath222 , the lower bound of the highest density partition interval , is close to @xmath223 , where @xmath224 is the density of the highest mode identified so far . if @xmath225 we move upwards the density ladder by @xmath226 unit and update the weights @xmath227 accordingly , with details provided in routine  [ rt : updateladder ] .",
    "this strategy helps the sampler to explore the high - density part , which is important for statistical estimation and finding the global mode .",
    "[ rt : updateladder ] given @xmath228 , find @xmath229 .    *",
    "if @xmath230 , set @xmath231 for @xmath232 ; for @xmath233 , assign @xmath234 , @xmath235 for @xmath236 , and @xmath237 ; * otherwise set @xmath238 .",
    "[ alg : burnin ] input @xmath239 and @xmath200 . set @xmath240 .",
    "initialize @xmath241 , @xmath242 , @xmath243 , @xmath244 , and @xmath245 .",
    "set @xmath246 and @xmath247 for @xmath248 . for @xmath249",
    ":    1 .   draw @xmath250 and find @xmath251 by a gd search .",
    "2 .   given @xmath251 ,",
    "update @xmath228 and @xmath252 by routine [ rt : updatemodes ] ; if @xmath253 is a new mode in @xmath228 , initialize @xmath254 . given @xmath228 , update @xmath255 by routine [ rt : updateladder ] .",
    "3 .   given @xmath228 and @xmath255 ,",
    "accept or reject @xmath66 with the mh ratio targeting at @xmath130 to obtain @xmath131 .",
    "4 .   execute step 4 of algorithm [ alg : main ] with @xmath256 .",
    "note that @xmath257 for every iteration in the burn - in algorithm .",
    "this pushes the sampler to explore different regions in the sample space so that more local modes can be identified . in this case , the weight @xmath135 records the number of visits to @xmath96 before the @xmath84th iteration , which is the reason for our updating schemes on @xmath258 in routine  [ rt : updatemodes ] when a mode is updated in @xmath228 and in routine [ rt : updateladder ] when the density ladder changes .",
    "[ rmk : burninopt ] the burn - in algorithm can be used as an optimization method that searches for up to @xmath200 local modes of the highest densities .",
    "as demonstrated in the bayesian network applications , this algorithm is very powerful in finding global modes .",
    "the md sampler requires only a few input parameters , @xmath259 , and @xmath200 .",
    "a practical rule is to choose @xmath260 and @xmath226 such that the range of the density partition intervals , @xmath261 in log scale , is wide enough to cover important regions . in this paper",
    ", we set @xmath261 around 20 for the low - dimensional test example in section  [ sec : exrm ] and around 200 for learning bayesian networks in sections  [ sec : bn ] and [ sec : signaling ] . by default the probability of proposing a mixed jump @xmath262 .",
    "the effect of keeping only @xmath200 modes will be studied later with the examples .",
    "the domain - based representation of @xmath12 is constructed by estimating @xmath40 ( [ eq : lambdak ] ) and @xmath41 ( [ eq : muhk ] ) for @xmath54 with post burn - in samples , denoted by @xmath263 .",
    "let @xmath264 , @xmath265 , @xmath266 , and @xmath267 such that @xmath268 .",
    "the key identity for our estimation is @xmath269 which follows from as @xmath270 asymptotically .",
    "see liang ( 2009 ) and atchad and liu ( 2010 ) for similar results .",
    "however , @xmath271 may be far from @xmath272 even for post burn - in iterations .",
    "thus , it is desired to use a weighted version of so that @xmath131 will carry a higher weight if @xmath273 is closer to @xmath272 .",
    "since decrease in @xmath137 indicates convergence of the md sampler and @xmath274 for @xmath275 ( supplementary document ) , a reasonable choice is to weight @xmath131 by @xmath276 so that unnormalized @xmath227 will be used in .",
    "consequently , @xmath277 is constructed with @xmath278 for @xmath54 .",
    "then , @xmath279 $ ] is estimated by @xmath280 .",
    "please see supplementary document for more discussion on this weighted estimation .    in the next three sections we demonstrate the effectiveness of the md sampler in statistical estimation , especially estimation of drs , compared to the naive two - step approach .",
    "for all examples , we employ the wl algorithm with the mwl update ( routine  [ mwldesign ] ) as the monte carlo method in the two - step approach . to minimize hidden artifacts in a comparison due to coding differences",
    ", we implement the wl algorithm with the same burn - in and main algorithms of the md sampler . in the main algorithm ( algorithm [ alg : main ] ) we replace the updating scheme in equation ( [ eq : updatew ] ) with @xmath281 for @xmath54 and @xmath91 and",
    "modify the burn - in algorithm accordingly , so that @xmath282 for every iteration .",
    "consequently , the working density is effectively @xmath283 as used in the wl algorithm , which is a diffuse version of @xmath14 such that each density partition interval will be equally sampled after convergence .",
    "note that the same gd search is applied at each iteration to partition samples into attraction domains for estimating drs .",
    "our comparison aims to highlight the effect of domain partitioning and the mixed jump in the md sampler which are the key differences from the wl algorithm .",
    "we test the md sampler with an example in @xmath182 . for this example",
    ", domain - based representations can be obtained via one - dimensional numerical integration with a high accuracy , which provides the basis to evaluate our estimation .",
    "we choose @xmath284 , which is greater than the total number of local modes , to construct complete drs .",
    "let @xmath285 . the rastrigin function ( gordon and whitley 1993 )",
    "is defined as @xmath286,\\ ] ] where @xmath287 is a positive constant .",
    "we set @xmath288 and @xmath289 in ( [ eq : rastrigin ] ) to obtain our target distribution @xmath290 $ ] , which has @xmath291 local modes formed by all the elements of the product set @xmath292 .",
    "these local modes have five distinct log density values , 0 , @xmath293 , @xmath294 , @xmath295 , and @xmath296 , dependent on the combinations of their coordinates .",
    "they are grouped accordingly into five layers so that the number of zeros and the number of @xmath297 in the coordinates of a local mode at the @xmath144th layer are @xmath298 and @xmath299 , respectively , for @xmath300 .",
    "the attraction domains of local modes at the same layer have identical probability masses and identical conditional means up to a permutation and change of signs of the coordinates .",
    "we applied the md sampler 100 times independently , each run with @xmath301 density partition intervals , @xmath302 , @xmath303k burn - in iterations and a total of 5 million ( m ) iterations ( including the burn - in iterations ) .",
    "the local proposal was simply @xmath304 .",
    "the average acceptance rate was 0.26 for the local move and was 0.56 for the mixed jump .",
    "let @xmath305 and @xmath306 .",
    "we estimated @xmath307 , @xmath308 , @xmath309 , @xmath310 , and @xmath311 , all via domain - based representations .",
    "since the target density of this example is a product of one - dimensional marginal densities , the above expectations can be calculated accurately through one - dimensional numerical integration .",
    "we compared our estimates from md sampling with the results from numerical integration by computing mean squared errors ( mses ) .",
    "we report the average mse of the estimated log probability masses ( @xmath312 ) and the average mse of the estimated conditional means ( @xmath313 ) over all the local modes at the same layer ( @xmath300 ) , and for other functions we only report the mses of the estimated expectations to save space ( table  [ tab : rastrigin ] ) .       denote by @xmath314 the normalizing constant of",
    ". then @xmath315 , where @xmath316 .",
    "let @xmath317 be a nonempty subset of @xmath318 and @xmath319 . given a map @xmath320 ,",
    "let @xmath321 $ ] with respect to @xmath322 .",
    "define a map @xmath323 by @xmath324\\ ] ] and the mean field @xmath325 , i.e. , @xmath326.\\ ] ] consider the equation @xmath327 .",
    "let @xmath328_{1:\\kappa}$ ] and @xmath329 .",
    "as @xmath330 is invariant to translation of @xmath331 by a scalar : @xmath332 , @xmath333 , the solution set to this equation is @xmath334 .",
    "set @xmath335 and choose an arbitrary point @xmath336 to initialize @xmath337 .",
    "a doubly adaptive mcmc is employed to find a solution @xmath338 and to estimate @xmath339 for a function @xmath340 .",
    "denote the @xmath349 norm by @xmath350 and let @xmath351 , where @xmath352 are vectors and @xmath287 is a set .",
    "our goal is to establish that @xmath353 almost surely ( with respect to the probability measure of the process @xmath354 ) and that @xmath355 is ergodic .",
    "clearly , translation of @xmath356 by a scaler does not change the working density @xmath357 or affect the convergence of @xmath358 to @xmath359 .",
    "thus , the theory for algorithm  [ alg : damcmc ] can be applied to the md sampler with reinitialization ( remark  [ rmk : diagnostics ] ) .",
    "the update of @xmath356 , up to translation by a scalar , and the update of @xmath360 correspond to , respectively , the update of @xmath361 and the update of @xmath178 for any @xmath144 in the md sampler .",
    "we state four conditions for establishing the main results .    1 .",
    "the sample space @xmath32 is compact , @xmath362 for all @xmath363 , @xmath364 is bounded , and @xmath359 is nonempty . the map @xmath365 and the function @xmath12 are @xmath17-integrable and bounded .",
    "2 .   there exist @xmath366 and @xmath367 such that @xmath368 implies that @xmath369 for all @xmath370 .",
    "there exist an integer @xmath371 , @xmath372 and a probability measure @xmath373 , such that @xmath374 for @xmath375 and @xmath376 , @xmath377 and @xmath378 .",
    "4 .   for all @xmath379 and all",
    "@xmath380 , @xmath381 and @xmath382 has continuous partial derivatives with respect to all the components of @xmath383 .    to avoid mathematical complexity",
    ", we assume that @xmath32 is compact ( c1 ) . this assumption does not lose much generality in practice as we may always restrict the sample space to @xmath384 given an sufficiently small @xmath385 .",
    "due to the compactness of @xmath32 , any continuous map and function on @xmath32 will be bounded .",
    "conditions ( c2 , c3 ) are standard conditions on the fixed proposal @xmath108 to guarantee irreducibility and aperiodicity of the mh kernel @xmath386 .",
    "they are satisfied by all the local moves used in this article .",
    "a regularity condition on the adaptive proposal @xmath387 is specified in ( c4 ) . for the mixed jumps in the examples ,",
    "@xmath383 is either the covariance matrix of a multivariate normal distribution or the cell probability vector of a multinomial distribution , and ( c4 ) is satisfied .",
    "by definition , @xmath394 for every @xmath395 . for any @xmath396 and @xmath397 , @xmath398 \\\\                      & \\geq & c_1 q({\\mathbf{x}},d{\\mathbf{y } } ) \\min\\left[1 , \\frac{p({\\mathbf{y}})q({\\mathbf{y}},{\\mathbf{x}})}{p({\\mathbf{x}})q({\\mathbf{x}},{\\mathbf{y}})}\\right ] = c_1s_0({\\mathbf{x}},d{\\mathbf{y}}).\\end{aligned}\\ ] ] thus , @xmath391 for all @xmath393 .",
    "lemma  [ lemma1 ] and ( c3 ) implies that @xmath392 , @xmath401 if @xmath402 , where @xmath403 . by theorem 4.2 of atchad and liu ( 2010 ) , @xmath404",
    "where @xmath405 .",
    "this implies that the @xmath139 defined by the mwl update ( routine  [ mwldesign ] ) will decrease below any given @xmath406 after a finite number of iterations , i.e. , @xmath407 , almost surely .",
    "then , algorithm  [ alg : damcmc ] becomes a stochastic approximation algorithm with a deterministic sequence of @xmath139 .",
    "according to proposition 6.1 and theorem 5.5 in andrieu et al .",
    "( 2005 ) , we only need to verify the drift conditions ( dri1 - 3 ) and assumptions ( a1 , a4 ) given in that paper to show the convergence of @xmath408 .",
    "_ verifying the drift conditions_. let @xmath409 be any compact subset of @xmath364 , and @xmath410 and @xmath411 be the projections of @xmath409 into @xmath412 and @xmath413 , respectively . since @xmath410 is compact , there is an @xmath414 $ ] such that @xmath415 . by lemma  [ lemma1 ] and ( c3 )",
    ", there is a @xmath416 such that @xmath417 where @xmath371 and @xmath373 are defined in ( c3 ) .",
    "this gives the minorization condition in ( dri1 ) .",
    "given ( c2 ) and that @xmath418 , is bounded away from 0 and @xmath419 under ( c1 ) , @xmath420 is irreducible and aperiodic for every @xmath421 , according to theorem 2.2 of roberts and tweedie ( 1996 ) .",
    "consequently , for every @xmath422 , @xmath423 is also irreducible and aperiodic as @xmath424 .",
    "let @xmath425 for all @xmath363 .",
    "it is then easy to verify other conditions in ( dri1 )",
    ".      condition ( dri3 ) can be verified by the same argument used in liang et al .",
    "( 2007 ) once we find a constant @xmath430 such that @xmath431 for all @xmath370 , @xmath432 and all @xmath433 , where @xmath434 is the @xmath433th component of @xmath395 .",
    "denote by @xmath435 the @xmath145th component of @xmath383 .",
    "straightforward calculation leads to @xmath436 and @xmath437 \\alpha t_{{\\bm{\\phi}}}({\\mathbf{x}},{\\mathbf{y } } ) , & \\mbox { if } r_{{\\bm{\\theta}}}({\\mathbf{x}},{\\mathbf{y } } ) < 1 \\\\ \\left[{\\partial \\log t_{{\\bm{\\phi}}}({\\mathbf{x}},{\\mathbf{y}})}/{\\partial \\phi_j}\\right ] \\alpha t_{{\\bm{\\phi}}}({\\mathbf{x}},{\\mathbf{y } } ) , & \\mbox { otherwise , } \\end{array}\\right.\\ ] ] where @xmath438 .",
    "condition ( c4 ) with the compactness of @xmath411 and @xmath32 guarantees that @xmath439 as @xmath440 , holds and ( dri3 ) is verified .",
    "_ verifying assumptions _",
    "( a1 , a4 ) .",
    "it is assumed in assumption ( a1 ) the existence of a global lyapunov function for @xmath330 .",
    "let @xmath441 , where @xmath442 .",
    "using straightforward algebra one can show that @xmath443 where @xmath444 , @xmath445 , and the arguments ( @xmath446 and @xmath331 ) in @xmath447 and @xmath314 have been dropped . since @xmath364 is bounded , @xmath448 for all @xmath433 . because @xmath365 is @xmath17-integrable , @xmath449 is bounded for all @xmath433 and @xmath450 ensures that @xmath451 for any @xmath65 with equality if and only if @xmath452 .",
    "furthermore , @xmath453 is compact for some @xmath454 and the closure of @xmath455 has an empty interior .",
    "thus , all the conditions in assumption ( a1 ) are satisfied . since @xmath456 and @xmath162 for @xmath161 , verifying assumption ( a4 ) is immediate .",
    "this completes the proof of the convergence of @xmath408 .",
    "the result can be established similarly as the proof of proposition 6.2 in atchad and liu ( 2010 ) .",
    "we only give an outline here .",
    "the drift conditions imply that for any @xmath422 , there exist @xmath457 , @xmath458 , and @xmath459 $ ] such that @xmath460 and @xmath461 where @xmath462 and for @xmath463 , @xmath464 . see proposition 6.1 and assumption ( a3 ) of andrieu et al .",
    "then , following an essentially identical proof to that of lemma 6.6 in atchad and liu ( 2010 ) , we can show that @xmath465 $ ] has a finite limit almost surely .",
    "since @xmath466 as @xmath467 , kronecker s lemma applied to the above infinite sum leads to the desired result .",
    "becker , o.m . and karplus , m. ( 1997 ) ,  the topology of multidimensional potential energy surface : theory and application to peptide structure and kinetics , \" _ journal of chemical physics _ , 106 , 1495 - 1517 .",
    "cooper , g. f. , and yoo , c. ( 1999 ) ,  causal discovery from a mixture of experimental and observational data , \" in _ proceedings of the 15th conference on uncertainty in artificial intelligence _",
    ", san francisco , ca : morgan kaufmann , pp . 116 - 125 .",
    "holland , p. w. ( 1988 ) ,  causal inference , path analysis , and recursive structural equations models , \" in _ sociological methodology _ , c.c .",
    "clogg ed . , washington dc : american sociological association , pp .",
    "449 - 484 .",
    "neyman , j. ( 1990 ) ,  sur les applications de la thar des probabilities aux experience agaricales : essay des principle , \" [ english translation of excerpts by d. dabrowska and t. speed ] , _ statistical science _",
    ", 5 , 463 - 472 .",
    "robins , j. ( 1986 ) ,  a new approach to causal inference in mortality studies with sustained exposure periods : application to control of the healthy worker survivor effect , \" _ math modeling _ , 7 , 1393 - 1512 .",
    "wales , d.j . and doye , j.p.k .",
    "( 1997 ) ,  global optimization by basin - hopping and the lowest energy structures of lennard - jones clusters containing up to 110 atoms , \" _ journal of physical chemistry a _ , 101 , 5111 - 5116 ."
  ],
  "abstract_text": [
    "<S> when a posterior distribution has multiple modes , unconditional expectations , such as the posterior mean , may not offer informative summaries of the distribution . motivated by this problem </S>",
    "<S> , we propose to decompose the sample space of a multimodal distribution into domains of attraction of local modes . </S>",
    "<S> domain - based representations are defined to summarize the probability masses of and conditional expectations on domains of attraction , which are much more informative than the mean and other unconditional expectations . a computational method , the multi - domain sampler , is developed to construct domain - based representations for an arbitrary multimodal distribution . </S>",
    "<S> the multi - domain sampler is applied to structural learning of protein - signaling networks from high - throughput single - cell data , where a signaling network is modeled as a causal bayesian network . </S>",
    "<S> not only does our method provide a detailed landscape of the posterior distribution but also improves the accuracy and the predictive power of estimated networks .    </S>",
    "<S> key words : domain - based representation ; multimodal distribution ; monte carlo ; network structure ; protein - signaling network ; wang - landau algorithm . </S>"
  ]
}