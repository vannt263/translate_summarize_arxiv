{
  "article_text": [
    "consider the regression model @xmath2 where @xmath3 and @xmath4 are real - valued random variables , with @xmath3 distributed according to a non - atomic law @xmath5 on @xmath6 $ ] , @xmath7<\\infty$ ] and @xmath8=0 $ ] .",
    "we want to estimate the regression function @xmath9 , assuming it is of bounded variation . since @xmath5 is non - atomic , we will further assume , without loss of generality , that @xmath9 is right - continuous .",
    "the jordan decomposition states that @xmath9 can be written as the sum of a non - decreasing function @xmath10 and a non - increasing function @xmath11 @xmath12 the underlying idea of the estimator that we introduce in this paper consists in viewing this decomposition as an additive model involving the increasing and the decreasing parts of @xmath9 .",
    "this leads us to propose an `` iterative isotonic regression '' estimator ( abbreviated to ) that combines the isotonic regression and backfitting algorithms , two well - established algorithms for estimating monotone functions and additive models , respectively . + the jordan decomposition ( [ eq : jordan - decomposition ] ) is not unique in general . however , if one requires that both terms on the right - hand side have singular associated stieltjes measures and that @xmath13}r(x)\\mu(dx)=\\int_{[0,1]}u(x)\\mu(dx),\\ ] ] then the decomposition is unique and the model is identifiable . let us emphasize that , from a statistical point of view , our assumption on @xmath9 is mild .",
    "the classical counterexample of a function that is not of bounded variation is @xmath14 for @xmath15 $ ] , with @xmath16 . + estimating a monotone regression function is the archetypical shape restriction estimation problem .",
    "specifically , assume that the regression function @xmath9 in ( [ eq : general - model ] ) is non - decreasing , and suppose we are given a sample @xmath17 of i.i.d .",
    "@xmath18 valued random variables distributed as a generic pair @xmath19 .",
    "then denote @xmath20 the ordered sample and @xmath21 the corresponding observations . in this framework",
    ", the pool - adjacent - violators algorithm ( pava ) determines a collection of non - decreasing level sets solution to the least square minimization problem @xmath22 these estimators have raised great interest in the literature for decades since they are nonparametric , data driven and easy to implement .",
    "early work on the maximum likelihood estimators of distribution parameters subject to order restriction date back to the 50 s , starting with ayer _",
    "_ @xcite and brunk @xcite .",
    "comprehensive treatises on isotonic regression include barlow _",
    "_ @xcite and robertson _ et al .",
    "_ @xcite . for improvements and extensions of the pava approach to more general order restrictions , see best and chakravarti @xcite , dykstra @xcite , and lee @xcite , among others .",
    "+ the solution of ( [ eq : min1 ] ) can be seen as the metric projection , with respect to the euclidean norm , of the vector @xmath23 on the isotone cone @xmath24 @xmath25 that projection is not linear , which is the reason why analyzing these estimators is technically challenging .",
    "+ interestingly , one can interpret the isotonic regression estimator as the slope of a convex approximation of the primitive integral of @xmath9 .",
    "this leads to an explicit relation between @xmath26 and the vector of the adjusted values , known as the `` min - max formulas '' ( see anevski and soulier @xcite for a rigorous justification ) .",
    "this point of view plays a key role in the study of the asymptotic behavior of isotonic regression .",
    "the consistency of the estimator was established by brunk @xcite and hanson _ et al .",
    "brunk @xcite proved its cube - root convergence at a fixed point and obtained the pointwise asymptotic distribution , and durot @xcite provided a central limit theorem for the @xmath27-error .",
    "+ let us now discuss the additive aspect of the model . in a multivariate",
    "setting , the additive model was originally suggested by friedman and stuetzle @xcite and popularized by hastie and tibshirani @xcite as a way to accommodate the so - called curse of dimensionality .",
    "the underlying idea of additive models is to approximate a high dimension regression function @xmath28 by a sum of one - dimensional univariate functions , that is @xmath29 not only do additive models provide a logical extension of the standard linear regression model which facilitates the interpretation , but they also achieve optimal rates of convergence that do not depend on the dimension @xmath30 ( see stone @xcite ) .",
    "+ buja _ et al . _",
    "@xcite proposed the backfitting algorithm as a practical method for estimating additive models .",
    "it consists in iteratively fitting the partial residuals from earlier steps until convergence is achieved . specifically , if the current estimates are @xmath31 , then @xmath32 is updated by smoothing @xmath33 against @xmath34 .",
    "the backfitted estimators have mainly been studied in the case of linear smoothers .",
    "hrdle and hall @xcite showed that when all the smoothers are orthogonal projections , the whole algorithm can be replaced by a global projection operator .",
    "opsomer and ruppert @xcite , and opsomer @xcite , gave asymptotic bias and variance expressions in the context of additive models fitted by local polynomial regression .",
    "mammen , linton and nielsen @xcite improved these results by deriving a backfitting procedure that achieves the oracle efficiency ( that is , each component can be estimated as well as if the other components were known ) .",
    "this procedure was extended to several different one - dimensional smoothers including kernel , local polynomials and splines by horowitz , klemel and mammen @xcite .",
    "alternative estimation procedures for additive models have been considered by kim , linton and hengartner @xcite , and by hengartner and sperlich @xcite .",
    "+ in the present context , we propose to apply the backfitting algorithm to decompose a univariate function by alternating isotonic and antitonic regressions on the partial residuals in order to estimate the additive components @xmath10 and @xmath11 of the jordan decomposition ( [ eq : jordan - decomposition ] ) .",
    "the finite sample behavior of this estimator has been studied in a related paper by guyader _",
    "( see @xcite ) . among other results",
    ", it is stated that the sequence of estimators obtained in this way converges to an interpolant of the raw data ( see section [ sec : iir ] below for details ) .",
    "+ backfitted estimators in a non - linear case have also been studied by mammen and yu @xcite . specifically , assuming that the regression function @xmath9 in ( [ eq : additive - models ] ) is an additive function of isotonic one - dimensional functions @xmath35 , they estimate each additive component by iterating the pava in a backfitting fashion .",
    "moreover , mammen and yu show that , as in the linear case , their estimator achieves the oracle efficiency and , in each direction , they recover the limit distribution exhibited by brunk @xcite .",
    "+ the main result addressed in this paper states the consistency of our estimator . denoting @xmath36 the iterative isotonic regression estimator resulting from @xmath37 iterations of the algorithm",
    ", we prove the existence of a sequence of iterations @xmath38 , increasing with the sample size @xmath1 , such that @xmath39\\underset{n\\rightarrow \\infty}{\\longrightarrow}0\\ ] ] where @xmath40 is the quadratic norm with respect to the law @xmath5 of @xmath3 .",
    "our analysis identifies two error terms : an estimation error that comes from the isotonic regression , and an approximation error that is governed by the number of iterations @xmath37 .",
    "+ concerning the estimation error , we wish to emphasize that all asymptotic results about isotonic regression mentioned above assume monotonicity of the regression function @xmath9 . in our context , at each stage of the iterative process , we apply an isotonic regression to an arbitrary function ( of bounded variation ) . as a result",
    ", we prove in section [ sec : isotonic - consistency ] the @xmath41 consistency of isotonic regression for the metric projection of @xmath9 onto the cone of increasing functions ( see theorem [ thm : iso - consistency ] ) . + the approximation term can be controlled by increasing the number of iterations .",
    "this is made possible thanks to the interpretation of as a von neumann s algorithm , and by applying related results in convex analysis ( see proposition [ pro : approximation ] ) . putting estimation and approximation errors together finally leads to the consistency result ( [ eq : existing - sequence1 ] ) .",
    "+ let us remark that , as far as we know , rates of convergence of von neumann s algorithm have not yet been studied in the context of bounded variation functions .",
    "hence , at this time , it seems difficult to establish rates of convergence for our estimator without further restrictions on the shape of the underlying regression function .",
    "thus , the results we present here may be considered as a starting point in the study of novel methods which would consist in applying isotonic regression with no particular shape assumption on the regression function .",
    "+ the remainder of the paper is organised as follows .",
    "we first give further details and notations about the construction of in section [ sec : iir ] . the general consistency result for isotonic regression",
    "is given in section [ sec : isotonic - consistency ] .",
    "the main result of this article , the consistency of , is established in section [ sec : iir - consistency ] . most of the proofs are postponed to section [ sec : appendix ] , while related technical results are gathered in section [ tech ] .",
    "denote by @xmath23 the vector of observations corresponding to the ordered sample @xmath42 .",
    "we implicitly assume in this writing that the law @xmath5 of @xmath3 has no atoms .",
    "we denote by @xmath43 ( resp .",
    "@xmath44 ) the metric projection of @xmath26 with respect to the euclidean norm onto the isotone cone @xmath45 ( resp .",
    "@xmath46 ) defined in ( [ eq : isotone - cone ] ) : @xmath47 the backfitting algorithm consists in updating each component by smoothing the partial residuals , i.e. , the residuals resulting from the current estimate in the other direction .",
    "thus the iterative isotonic regression algorithm goes like this :    \\(1 ) initialization : @xmath48,\\ldots,\\hat{b}^{(0)}_n[n]\\right)=0 $ ] ( 2 ) cycle : for @xmath49 + @xmath50 ( 3 ) iterate ( 2 ) until a stopping condition to be specified is achieved .",
    "guyader _ et al . _",
    "@xcite prove that the terms of the decomposition @xmath51 have singular stieltjes measures .",
    "furthermore , by starting with isotonic regression , the terms @xmath52 have all the same empirical mean as the original data @xmath26 , while all the @xmath53 are centered .",
    "hence , for each @xmath37 , the decomposition @xmath51 satisfies the condition ( [ iasbc ] ) , and that decomposition is unique ( identifiable ) .",
    "+ algorithm [ algo : iir ] furnishes vectors of adjusted values . in the following",
    ", we will consider one - to - one mappings between such vectors and piecewise functions defined on the interval @xmath6 $ ] .",
    "for example , the vector @xmath54,\\ldots,\\hat{u}_n^{(k)}[n])$ ] is associated to the real - valued function @xmath52 defined on @xmath6 $ ] by @xmath55{{\\mathbbm{1}}}_{[0,x_{(2)})}(x)+\\sum_{i=2}^{n-1}\\hat{u}_n^{(k)}[i]{{\\mathbbm{1}}}_{[x_{(i)},x_{(i+1)})}(x)+\\hat{u}_n^{(k)}[n]{{\\mathbbm{1}}}_{[x_{(n)},1]}(x).\\ ] ] observe that our definition of @xmath56 makes it right - continuous .",
    "obviously , equivalent formulations hold for @xmath53 and @xmath36 as well .",
    "+ figure [ fig : iir - interpolation ] illustrates the application of on an example .",
    "the top left - hand side displays the regression function @xmath9 , and @xmath57 points @xmath58 , with @xmath59 , where the @xmath60 s are gaussian centered random variables .",
    "the three other figures show the estimations @xmath36 obtained on this sample for @xmath61 , and @xmath62 iterations . according to ( [ eq : prolongement ] ) , our method fits a piecewise constant function .",
    "moreover , increasing the number of iterations tends to increase the number of jumps .",
    "the bottom right figure illustrates that , as established in guyader _",
    "@xcite , for fixed sample size @xmath1 , the function @xmath63 converges to an interpolant of the data when the number of iterations @xmath37 tends to infinity , _",
    "i.e. _ , for all @xmath64 , @xmath65    one interpretation of the above result is that increasing the number of iterations leads to overfitting .",
    "thus , iterating the procedure until convergence is not desirable . on the other hand , as illustrated on figure [",
    "fig : iir - interpolation ] , iterations beyond the first step typically improve the fit .",
    "this suggests that we need to couple the algorithm with a stopping rule . in this respect ,",
    "two important remarks are in order .",
    "firstly , since equation ( [ eq : prolongement ] ) enables predictions at arbitrary locations @xmath66 $ ] , all the standard data - splitting techniques can be applied to stop the algorithm . + secondly ,",
    "the choice of a stopping criterion as a model selection suggests stopping rules based on akaike information criterion , bayesian information criterion or generalized cross validation .",
    "these criteria can be written in the generic form @xmath67 + here , @xmath68 denotes the residual sum of squares and @xmath69 is an increasing function .",
    "the parameter @xmath70 stands for the number ( or equivalent number ) of parameters . for isotonic regression",
    ", we refer to meyer and woodroofe @xcite to consider that the number of jumps provides the effective dimension of the model .",
    "therefore , a natural extension for is to replace @xmath70 by the number of jumps of @xmath36 in ( [ eq : stop - commonform ] ) .",
    "the comparisons of these criteria and the practical behavior of the procedure will be addressed elsewhere by the authors .",
    "in this section , we focus on the first half step of the algorithm , which consists in applying isotonic regression to the original data . to simplify the notations ,",
    "we omit in this section the exponent related to the number of iterations @xmath37 , and simply denote @xmath71 the isotonic regression on the data , that is , @xmath72 let @xmath73 denote the closest non - decreasing function to the regression function @xmath9 with respect to the @xmath41 norm .",
    "thus , @xmath73 is defined as @xmath74}(r(x)-u(x))^2\\mu(dx),\\ ] ] where @xmath75 denotes the cone of non - decreasing functions in @xmath41 . since @xmath75 is closed and convex , the metric projection @xmath73 exists and is unique in @xmath41 .",
    "+ for mathematical purpose , we also introduce @xmath76 , the result from applying isotonic regression to the sample @xmath77 , @xmath78 , that is @xmath79 finally , we note that , since @xmath9 is bounded , so are @xmath73 and @xmath76 , independently of the sample size @xmath1 ( see for example lemma 2 in anevski and soulier @xcite ) . figure [ fig : cons - iso ] displays the three terms involved .",
    "the main result of this section states that @xmath80 \\underset{n\\rightarrow \\infty}{\\longrightarrow}0,\\ ] ] where the expectation is taken with respect to the sample @xmath81 .",
    "our analysis decomposes @xmath82 into two distinct terms : @xmath83 as @xmath84 does not depend on the response variable @xmath85 , one could interpret it as a bias term , whereas @xmath86 plays the role of a variance term .",
    "+ throughout this section , our results are stated for both the empirical norm @xmath87 and the @xmath41 norm @xmath40 , as both are informative .",
    "the following proposition states the convergence of the bias term ( its proof is postponed to section [ sec : biais - iso ] ) .",
    "[ lem : biais - iso ] with the previous notations , we have @xmath88 and @xmath89    applying lebesgue s dominated convergence theorem ensures that both @xmath90=0\\qquad \\textrm{and } \\qquad\\lim_{n\\rightarrow \\infty}{\\mathbb e}\\left[\\|u_n - u_+\\|^2\\right]=0.\\ ] ] analysis of the variance term requires that we assume that the noise @xmath91 is bounded .",
    "it then follows from anevski and soulier @xcite that @xmath71 is bounded , independently of the sample size @xmath1 .",
    "the proof of the following result is given in section [ sec : variance - iso ] ) .",
    "[ lem : variance - iso ] assume that the random variable @xmath91 is bounded , then we have @xmath92=0,\\ ] ] and @xmath93=0.\\ ] ]    combining proposition [ lem : biais - iso ] and proposition [ lem : variance - iso ] yields the following theorem .",
    "[ thm : iso - consistency ] consider the model @xmath94 , where @xmath95\\rightarrow{\\mathbb r}$ ] belongs to @xmath41 , @xmath5 is a non - atomic distribution on @xmath6 $ ] , and @xmath91 is a bounded random variable satisfying @xmath8=0 $ ] .",
    "denote @xmath73 and @xmath71 the functions resulting from the isotonic regression applied on @xmath9 and on the sample @xmath81 , respectively .",
    "then we have @xmath96\\to 0\\ ] ] and @xmath80\\to 0\\ ] ] when the sample size @xmath1 tends to infinity .",
    "this result generalizes the consistency of isotonic regression when applied in a more general context than the one of monotone functions .",
    "it will be of constant use when iterating our algorithm .",
    "this is the topic of the upcoming section .",
    "we now proceed with our main result , which states that there is a sequence of iterations @xmath0 , increasing with the sample size @xmath1 , such that @xmath97\\underset{n\\rightarrow \\infty}{\\longrightarrow}0.\\ ] ] in order to control the expectation of the @xmath98 distance between the estimator @xmath36 and the regression function @xmath9 , we shall split @xmath99 as follows : let @xmath100 be the result from applying the algorithm on the regression function @xmath9 itself @xmath37 times , that is @xmath101 , where @xmath102",
    "we then upper - bound @xmath103 in this decomposition , the first term is an approximation error , while the second one corresponds to an estimation error .",
    "+ figure [ fig : demarche - consistance ] displays the function @xmath100 for two particular values of @xmath37 .",
    "one can see that , after @xmath37 steps of the algorithm , there generally remains an approximation error @xmath104 .",
    "nonetheless , one also observes that this error decreases when iterating the algorithm .",
    "the following proposition states that the approximation error can indeed be controlled by increasing the number of iterations @xmath37 .",
    "its proof relies on the interpretation of as a von neumann s algorithm ( see section [ sec : approximation ] for the proof ) .",
    "[ pro : approximation ] assume that @xmath9 is a right - continuous function of bounded variation and @xmath5 a non - atomic law on @xmath6 $ ] .",
    "then the approximation term @xmath104 tends to @xmath105 when the number of iterations grows : @xmath106 where @xmath40 denotes the quadratic norm in @xmath41 .",
    "coming back to ( [ eq : estim - approx ] ) , we further decompose the estimation error into a bias and a variance term to obtain    [ cols=\"^,^,^,^,^ \" , ]     the function @xmath107 results from @xmath37 iterations of the algorithm on the sample @xmath108 , @xmath78 , and can be seen as the equivalent of the function @xmath76 defined in ( [ eq :",
    "u - n ] ) .",
    "this decomposition allows us to make use of the consistency results of the previous section , and to control the estimation error when the sample size @xmath1 goes to infinity .",
    "we now state the main theorem of this paper .",
    "[ thm : iir - consistency ] consider the model @xmath94 , where @xmath95\\rightarrow{\\mathbb r}$ ] is a right - continuous function of bounded variation , @xmath5 a non - atomic distribution on @xmath6 $ ] , and @xmath91 a bounded random variable satisfying @xmath8=0 $ ] .",
    "then there exists an increasing sequence of iterations @xmath38 such that @xmath97\\underset{n\\rightarrow \\infty}{\\longrightarrow}0,\\ ] ] where @xmath40 denotes the quadratic norm in @xmath41 .",
    "coming back to the original notation , theorem [ thm : iso - consistency ] states that @xmath109=0\\qquad \\textrm{and } \\qquad\\lim_{n\\rightarrow \\infty}{\\mathbb e}\\left[\\|\\hat{u}_n^{(1)}-u^{(1)}\\|^2\\right]=0.\\ ] ] in the following , we show that this result still holds when applying the backfitting algorithm . before proceeding ,",
    "just remark that , since @xmath9 and @xmath91 are bounded , this will also be the case for all the quantities at stake in the remainder of the proof . in particular , this allows us to use the concentration inequalities established in section [ sec : inegalites - concentration ] .",
    "+ @xmath110 we first describe the end of the first step by showing that @xmath111\\rightarrow 0 $ ] .",
    "+ recall the definitions @xmath112 in order to mimic the previous step , let us consider the vectors @xmath113 so that @xmath114 and @xmath115 to study the term @xmath116 , one can apply _ mutatis mutandis _ the result of theorem [ thm : iso - consistency ] , replacing @xmath117 by @xmath118 , @xmath9 by @xmath119 , and isotonic regression by antitonic regression .",
    "hence , @xmath120=0\\qquad \\textrm{and } \\qquad\\lim_{n\\rightarrow \\infty}{\\mathbb e}\\left[\\|\\tilde{b}_n^{(1)}-b^{(1)}\\|^2\\right]=0.\\ ] ] as projection reduces distances , we also have @xmath121 thanks to equations ( [ eq : bvn - finalstep05 ] ) and ( [ oebv ] ) , we deduce @xmath122\\leq 2\\times \\left\\{{\\mathbb e}\\left [   \\|\\hat{b}_n^{(1)}-\\tilde{b}_n^{(1)}\\|^2_n\\right]+{\\mathbb e}\\left[\\|\\tilde{b}_n^{(1)}-b^{(1)}\\|^2_n \\right]\\right\\}\\rightarrow 0.\\ ] ] invoking the same arguments as those at the end of the proof of proposition [ lem : variance - iso ] , we also have @xmath123=0.\\ ] ] finally , at the end of the first iteration , we have @xmath124\\leq 2\\times\\left\\{{\\mathbb e}\\left[\\|\\hat{u}_n^{(1)}-u^{(1)}\\|^2\\right]+{\\mathbb e}\\left[\\|\\hat{b}_n^{(1)}-b^{(1)}\\|^2\\right]\\right\\}\\rightarrow 0.\\ ] ] @xmath110 for the beginning of the second iteration , consider this time @xmath125 let us introduce @xmath126 we apply theorem [ thm : iso - consistency ] again , replacing @xmath9 by @xmath127 , and @xmath117 by @xmath128 .",
    "this leads to @xmath129=0.\\ ] ] thanks to the reduction property of isotonic regression and using the conclusion of the first iteration , we get @xmath130\\leq { \\mathbb e}\\left[\\|y-\\hat{b}_n^{(1)}-((r - b^{(1)})+\\varepsilon)\\|^2_n\\right]= { \\mathbb e}\\left[\\|\\hat{b}_n^{(1)}-b^{(1)}\\|^2_n\\right]\\rightarrow 0.\\ ] ] therefore @xmath131\\leq 2\\times \\left\\{{\\mathbb e}\\left [ \\|\\hat{u}_n^{(2)}-\\tilde{u}_n^{(2)}\\|^2_n\\right]+{\\mathbb e}\\left[\\|\\tilde{u}_n^{(2)}-u^{(2)}\\|^2_n\\right]\\right\\}\\rightarrow 0\\ ] ] and , as before , we also have @xmath132=0.\\ ] ] the same scheme leads to @xmath133=0 $ ] , so that @xmath134\\leq 2\\times\\left\\{{\\mathbb e}\\left[\\|\\hat{u}_n^{(2)}-u^{(2)}\\|^2\\right]+{\\mathbb e}\\left[\\|\\hat{b}_n^{(2)}-b^{(2)}\\|^2\\right]\\right\\}\\rightarrow 0.\\ ] ] @xmath110 by iterating this process , it is readily seen that , for all @xmath49 , @xmath135=0,\\ ] ] which means that , at each iteration , the estimation error goes to 0 when the sample size tends to infinity . + we deduce that we can construct an increasing sequence @xmath136 such that for each @xmath49 and for all @xmath137 @xmath138\\leq \\|r^{(k)}-r\\|+\\frac{1}{k}.\\ ] ] notice that the term @xmath104 might be equal to zero ( _ e.g. _ , @xmath139 if @xmath9 is monotone ) , hence the additive term @xmath140 in the previous inequality .",
    "consequently , @xmath141\\leq 2\\|r^{(k)}-r\\|+\\frac{1}{k}.\\ ] ] then let us consider the sequence @xmath38 defined as : @xmath142 if @xmath143 , @xmath144 if @xmath145 , and so on .",
    "obviously @xmath38 tends to infinity and @xmath146\\leq2\\|r^{(k_n)}-r\\|+\\frac{1}{k_n}\\xrightarrow[n\\to\\infty]{}0.\\ ] ] this ends the proof of theorem [ thm : iir - consistency ] .",
    "for @xmath147 and @xmath148 two functions defined on @xmath6 $ ] , we denote @xmath149 the random variable @xmath150\\right\\}.\\ ] ] we first show that @xmath151 to this end , we proceed in two steps , proving in a first time that @xmath152 and in a second time that @xmath153 for the first inequality , let us denote @xmath154 by the definition of @xmath155 , note that for all @xmath1 , @xmath156 so that on @xmath157 , @xmath158 consequently @xmath159 therefore @xmath160 invoking lemma [ lem : annexe - concentration1 ] and borel - cantelli lemma , we conclude that @xmath161 , and hence @xmath162 on the set @xmath163 , we have @xmath164 which proves equation ( [ eq : limsup1 ] ) .",
    "+ conversely , we now establish equation ( [ eq : limsup2 ] ) . by definition of @xmath165 , observe that for all @xmath1 , @xmath166 consider the sets @xmath167}^+}\\vert\\delta_n(r - h)\\vert >",
    "n^{-1/3}\\right\\}\\ \\textrm{and}\\ d_n=\\left\\{\\|r - u_n\\|_n^2\\geq \\|r - u_{+}\\|^2-n^{-1/3}\\right\\}\\ ] ] so that @xmath168 , and by applying lemma [ lem : annexe - concentration2 ] , @xmath169 on the set @xmath170 , one has @xmath171 which proves ( [ eq : limsup2 ] ) . combining equations ( [ eq : limsup1 ] ) and ( [ eq : limsup2 ] ) leads to ( [ eq : biais1 ] ) .",
    "+ next , using lemma [ lem : annexe - concentration2 ] again , we get @xmath172 combined with ( [ eq : biais1 ] ) , this leads to @xmath173 it remains to prove the almost sure convergence of @xmath76 to @xmath73 .",
    "for this , it suffices to use the parallelogram law . indeed , noting @xmath174 , we have @xmath175 since both @xmath165 and @xmath76 belong to the convex set @xmath75 , so does @xmath176 .",
    "hence @xmath177 , and @xmath178 combining this with ( [ eq : res1 - 2 ] ) , we conclude that @xmath179 finally , lemma [ lem : annexe - concentration2 ] guarantees the same result for the empirical norm , that is @xmath180 and the proof is complete .",
    "let us denote @xmath181 the inner product associated to the empirical norm @xmath87 .",
    "since isotonic regression corresponds to the metric projection onto the closed convex cone @xmath24 with respect to this empirical norm , the vectors @xmath71 et @xmath76 are characterized by the following inequalities : for any vector @xmath182 , @xmath183 setting @xmath184 in ( [ eq:1 ] ) and @xmath185 in ( [ eq:2 ] ) , we get @xmath186 since @xmath187 , this leads to @xmath188    next , we have to use an approximation result , namely lemma [ lem : nick ] in section [ lknxn ] .",
    "the underlying idea is to exploit the fact that any non - decreasing bounded sequence can be approached by the element of a subspace @xmath189 at distance less than @xmath190 .",
    "specifically , if @xmath191 is an upper - bound for the absolute value of the considered non - decreasing bounded sequences , we can construct such a subspace @xmath189 with dimension @xmath192 where @xmath193 . from now on",
    ", we will take @xmath194 . before proceeding ,",
    "just notice that the boundedness assumption on the random variables @xmath60 allows us to find a common upper bound @xmath191 for the absolute values of the components of @xmath71 and @xmath76 .",
    "+ let us introduce the vectors @xmath195 and @xmath196 defined by @xmath197 so that @xmath198 from this , we get @xmath199 according to ( [ eq:3 ] ) , we deduce @xmath200 so that @xmath201 where @xmath202 stands for the metric projection of @xmath91 onto @xmath189 . put differently , we have @xmath203 and taking the expectation on both sides leads to @xmath204\\leq { \\mathbb e}\\left[\\|\\hat{u}_n - u_n\\|_n\\times\\|\\pi_{h_+}(\\varepsilon)\\|_n\\right]+2\\delta\\left\\{{\\mathbb e}\\left[\\|\\pi_{h_+}(\\varepsilon)\\|_n\\right]+{\\mathbb e}\\left[\\|\\varepsilon\\|_n\\right]\\right\\}.\\ ] ] if we denote @xmath205\\\\ \\alpha_n&=\\sqrt{{\\mathbb e}\\left[\\|\\pi_{h_+}(\\varepsilon)\\|_n^2\\right]}\\\\",
    "\\beta_n&=2\\delta\\left\\{{\\mathbb e}\\left[\\|\\pi_{h_+}(\\varepsilon)\\|_n\\right]+{\\mathbb e}\\left[\\|\\varepsilon\\|_n\\right]\\right\\ } \\end { array}\\right .\\ ] ] an application of cauchy - schwarz inequality gives @xmath206 which means that @xmath207\\leq \\left(\\frac{\\alpha_n+\\sqrt{\\alpha_n^2 + 4\\beta_n}}{2}\\right)^2.\\ ] ] since the random variables @xmath60 are i.i.d . with mean",
    "zero and common variance @xmath208 , a straightforward computation shows that @xmath209=\\frac{1}{n}{\\mathbb e}\\left[(\\pi_{h_+}\\varepsilon)'(\\pi_{h_+}\\varepsilon)\\right]=\\frac{1}{n}{\\mathbb e}\\left[\\operatorname{tr}\\left((\\pi_{h_+}\\varepsilon)'(\\pi_{h_+}\\varepsilon)\\right)\\right]=\\frac{1}{n}\\operatorname{tr}\\left({\\mathbb e}\\left[\\varepsilon\\varepsilon'\\right]\\pi_{h_+}\\right),\\ ] ] and since @xmath189 has dimension @xmath193 , this gives @xmath209=\\sigma^2\\frac{n}{n}\\ \\rightarrow\\   \\alpha_n=\\sigma \\sqrt{\\frac{n}{n}}=\\frac{2\\sqrt{2}c\\sigma}{\\delta\\sqrt{n}}.\\ ] ] set @xmath210 with @xmath211 , it then follows that @xmath212 goes to zero when @xmath1 goes to infinity .",
    "moreover , jensen s inequality implies @xmath213 as both @xmath214 and @xmath212 tend to zero when @xmath1 goes to infinity , we have shown that @xmath215=0.\\ ] ] remark that for any non negative random variable @xmath3 , @xmath216=\\int_{0}^{+\\infty}\\p\\left(x\\geq t\\right)dt\\leq n^{-1/4}+\\int_{0}^{+\\infty}\\p\\left(x\\geq t\\right){{\\mathbbm{1}}}_{\\{t\\geq n^{-1/4}\\}}\\ dt.\\ ] ] from equation ( [ ieuzoc ] ) in the proof of lemma [ lem : annexe - concentration3 ] , we know that for any @xmath217 , @xmath218 thus , setting @xmath219}(t)+\\exp\\left(2\\left\\lceil\\frac{64c^2}{t}\\right\\rceil\\log n-\\frac{t^2n}{32c^2}\\right){{\\mathbbm{1}}}_{\\{t\\geq n^{-1/4}\\}},\\ ] ] we deduce that @xmath220\\leq n^{-1/4}+\\int_{0}^{+\\infty}f_n(t)\\ dt.\\ ] ] then , it is readily seen that there exists an integer @xmath221 such that for all @xmath222 and for all @xmath223 , one has @xmath224 .",
    "since for all @xmath217 fixed , @xmath225 goes to 0 when @xmath1 tends to infinity , it remains to invoke lebesgue s dominated convergence theorem to conclude @xmath204-{\\mathbb e}\\left[\\|\\hat{u}_n - u_n\\|^2\\right]\\rightarrow 0.\\ ] ] combining the latter with equation ( [ eq : part1 ] ) , we have obtained @xmath226 ^ 2=0,\\ ] ] which ends the proof of proposition [ lem : variance - iso ] .",
    "consider the translated cone @xmath227 figure [ fig : von - neumann1 ] provides a very simple interpretation of the algorithm : namely , it illustrates that the sequences of functions @xmath228 and @xmath229 might be seen as alternate projections onto the cones @xmath75 and @xmath230 . in what follows",
    ", we justify this illuminating geometric interpretation in a rigorous way , and we explain its key role in the proof of the convergence as @xmath37 goes to infinity .",
    "+ by definition , we have @xmath231 where @xmath232 denotes the metric projection onto @xmath75 .",
    "classical properties of projections ensure that @xmath233 coming back to the definition of @xmath234 , we are led to @xmath235 in the same manner , since @xmath236 , we get @xmath237 more generally , denoting @xmath238 , this yields for all @xmath49 ( see also figure [ fig : von - neumann1 ] ) @xmath239    it remains to invoke theorem 4.8 in bauschke and borwein @xcite to conclude that @xmath240 { } 0,\\ ] ] which ends the proof of proposition [ pro : approximation ] .",
    "throughout the previous proofs , we repeatedly needed to pass from the empirical norm @xmath87 to the @xmath41 norm @xmath40 .",
    "this was made possible thanks to several exponential inequalities that we justify in this section .",
    "+ specifically , let @xmath147 and @xmath148 denote two mappings from @xmath241 $ ] to @xmath242 $ ] , and consider the random variable @xmath243\\right\\}=\\|g - h\\|_n^2-\\|g - h\\|^2.\\ ] ] in what follows , we focus on the concentration of @xmath149 around zero .",
    "the first result is a straightforward application of hoeffding s inequality .",
    "[ lem : annexe - concentration1 ] for any couple of mappings @xmath147 and @xmath148 from @xmath6 $ ] to @xmath242 $ ] , there exist positive real numbers @xmath244 , @xmath245 , @xmath246 and @xmath247 , depending only on @xmath191 , and such that @xmath248      the next lemma goes one step further , by considering , for fixed @xmath147 , the tail distribution of @xmath257}}|\\delta_n(g - h)|.\\ ] ] for obvious reasons , this type of result is sometimes called a maximal inequality .",
    "the proof shares elements with the one of theorem 3.1 of van de geer and wegkamp @xcite .",
    "[ lem : annexe - concentration2 ] let @xmath147 be a function from @xmath6 $ ] to @xmath242 $ ] and let @xmath258}$ ] denote the set of non - decreasing functions from @xmath6 $ ] to @xmath242 $ ] . there exist positive real numbers",
    "@xmath259 , @xmath260 , @xmath261 and @xmath262 depending only on @xmath191 and such that @xmath263}}|\\delta_n(g - h)|>n^{-\\alpha'}\\right)\\leq c'_1\\exp\\left(-c'_2n^{\\beta'}\\right).\\ ] ]    the first step consists in showing that the mapping @xmath264 is lipschitz . for any pair of functions @xmath148 and @xmath265 , we have @xmath266.\\end{aligned}\\ ] ] since @xmath148 and @xmath265 take values in @xmath242 $ ] , we get @xmath267\\right\\}\\ ] ] and according to jensen s inequality , @xmath268 now , since @xmath269 $ ] , if the inequality @xmath270 is satisfied , we also have @xmath271 .",
    "thus , @xmath272 and the mapping @xmath264 is lipschitz for the empirical norm @xmath273 . + next , let us consider a @xmath190-covering @xmath274 of @xmath258}$ ] for the empirical norm @xmath87 .",
    "we stress that this set @xmath275 is random since it depends on the points @xmath276 , but its cardinality @xmath277 may be chosen deterministic and upper - bounded as follows ( see lemma [ combinatoire ] ) : denoting @xmath278 , where @xmath279 stands for the ceiling function , we have @xmath280 where the last inequality is satisfied for any integer @xmath281 as soon as @xmath282 . + then , for any @xmath148 in @xmath258}$ ] , there exists @xmath283 in @xmath275 such that @xmath284 . from the previous lipschitz property ,",
    "we know that @xmath285 letting @xmath217 and @xmath286 , our objective is to upper bound @xmath287}}\\vert\\delta_n(g - h)\\vert > t\\right).\\ ] ] in this aim , for any @xmath148 in @xmath258}$ ] and any @xmath283 in @xmath275 , we start with the decomposition @xmath288 for any @xmath148 such that @xmath289 , since there exists @xmath283 in @xmath275 such that @xmath290 we necessarily have @xmath291 , and consequently @xmath292 in other words , @xmath293}}\\vert\\delta_n(g - h)\\vert > t\\right)&\\leq \\p\\left(\\max_{j=1\\cdots m}\\vert\\delta_n(g - e^*_j)\\vert > t/2\\right)\\\\ & \\leq \\p\\left(\\bigcup_{j=1}^m\\vert\\delta_n(g - e^*_j)\\vert >",
    "t/2\\right)\\\\ & \\leq \\sum_{j=1}^m\\p\\left(\\vert\\delta_n(g - e^*_j)\\vert > t/2\\right).\\end{aligned}\\ ] ] according to ( [ eq : lem1 - 0 ] ) and to the fact that @xmath294 fixing @xmath286 leads to @xmath287}}\\vert\\delta_n(g - h)\\vert >",
    "t\\right)\\leq 2m\\exp\\left(-\\frac{t^2n}{8c^2}\\right)\\leq2\\exp\\left(\\left\\lceil\\frac{32c^2}{t}\\right\\rceil\\log n-\\frac{t^2n}{32c^2}\\right).\\ ] ] finally , for any @xmath295 , there exists @xmath296 such that for any integer @xmath1 , @xmath297 hence the desired result with @xmath298 and @xmath299 .",
    "[ lem : annexe - concentration3 ] let us denote @xmath258}$ ] the set of non decreasing mappings from @xmath6 $ ] to @xmath242 $ ] .",
    "there exist positive real numbers @xmath300 , @xmath301 , @xmath302 and @xmath303 , depending only on @xmath191 , and such that @xmath304},h_2\\in { \\cal c}^+_{[0,1]}}\\vert\\delta_n(h_1-h_2)\\vert > n^{-\\alpha''}\\right)\\leq c''_1\\exp\\left(-c''_2n^{\\beta''}\\right).\\ ] ]    with the same notations as before , just note that for any mapping @xmath305}$ ] ( respectively @xmath306 ) , there exists @xmath307 ( respectively @xmath308 ) in the @xmath190-covering @xmath275 of @xmath258}$ ] , such that @xmath309 following the same line as in the proof of the previous lemma , we have , for any mapping @xmath147 with values in @xmath242 $ ] , that @xmath310 in particular @xmath311 moreover , @xmath312 set @xmath313 , then @xmath314 in the same manner , @xmath315 and @xmath316 hence , for any @xmath317 and @xmath306 in @xmath258}$ ] , @xmath318 as a consequence , the choice @xmath313 gives @xmath319},h_2\\in{\\cal c}^+_{[0,1]}}\\vert \\delta_n(h_1-h_2)\\vert >",
    "t\\right)&\\leq \\p\\left(\\max_{h^*_1,h^*_{2}\\in{\\cal e}^*}\\vert\\delta_n(h^*_j - h^*_{j'})\\vert > t/2\\right)\\\\ & \\leq \\sum_{1\\leq j_1\\neq j_2\\leq m}\\p\\left(\\vert\\delta_n(e^*_j - e^*_{j'})\\vert > t/2\\right)\\\\ & \\leq m^2\\exp\\left(-\\frac{t^2n}{32c^2}\\right).\\end{aligned}\\ ] ] according to ( [ zecnv ] ) , we are led to @xmath320},h_2\\in{\\cal c}^+_{[0,1]}}\\vert \\delta_n(h_1-h_2)\\vert >",
    "t\\right)\\leq \\exp\\left(2\\left\\lceil\\frac{64c^2}{t}\\right\\rceil\\log n-\\frac{t^2n}{32c^2}\\right).\\ ] ] for any @xmath321 , there exists a real number @xmath322 such that for any integer @xmath1 @xmath323 hence the desired result with @xmath324 and @xmath325 .",
    "[ combinatoire ] denote @xmath258}$ ] the set of non - decreasing mappings from @xmath6 $ ] to @xmath242 $ ] , and @xmath87 the empirical norm with respect to the sample @xmath326 . for any @xmath327",
    ", there exists a @xmath190-covering of @xmath328},\\|.\\|_n)$ ] with cardinality less than @xmath329 , where @xmath278 , and @xmath279 stands for the ceiling function .",
    "let us rewrite @xmath330 the reordering of the sample @xmath326 in increasing order .",
    "recall that the empiric norm is defined for any pair of functions @xmath147 and @xmath148 in @xmath258}$ ] by @xmath331 hence , if @xmath332 for all indices @xmath333 , we also have @xmath334 . + for the sake of simplicity , let us assume that @xmath335 is an integer and let us consider the following partition of the interval @xmath242 $ ] @xmath336 let us denote @xmath337}$ ] the set of non - decreasing functions defined on @xmath6 $ ] , with values in @xmath338 and piecewise constant on the intervals @xmath339 .",
    "we also suppose that they are constant on the intervals @xmath340 $ ] and @xmath341 $ ] , with respective values the ones of @xmath342 and @xmath343 .",
    "+ firstly , it is readily seen that any function @xmath147 in @xmath258}$ ] may be approximated at a distance less than or equal to @xmath190 with respect to the empirical norm @xmath87 by a function in @xmath337}$ ] . for this",
    ", it indeed suffices to pick at each point @xmath344 the nearest value of @xmath345 in the partition @xmath338 .",
    "secondly , it is well - known in discrete mathematics ( see for example lovsz _",
    "@xcite , theorem 3.4.2 ) that @xmath346}\\vert=\\binom{n+n}{n}.\\ ] ]      consider the subset @xmath347 of @xmath24 consisting in all vectors whose absolute values of the components are bounded by a real number @xmath191 .",
    "consider @xmath348 such that @xmath194 . for each @xmath349 ,",
    "let us introduce the vector @xmath350,\\cdots , h^+_j[n]\\right)'$ ] of @xmath351 as follows @xmath352 = \\left \\ { \\begin{array}{lll } 0 & \\textrm { if }   i \\leq \\lfloor\\frac{jn}{n}\\rfloor \\\\ 1 & \\textrm { otherwise } & \\end{array } \\right .\\ ] ] and define @xmath353 finally , set @xmath354 .",
    "we denote @xmath357,\\ldots , f[n])'$ ] , with @xmath358\\leq \\dots\\leq f[n]\\leq c.\\ ] ] set @xmath359 $ ] and , for @xmath360 , @xmath361=1}f[i]\\ ] ] we define also the vectors @xmath362 and @xmath363 of @xmath189 as follows @xmath364 and @xmath365 then we note that @xmath366 , so that @xmath367 with @xmath368 remark that , for all @xmath369 , @xmath370 and @xmath371 as well .",
    "thus , taking into account that the decomposition ( [ lqbjac ] ) is orthogonal , we get @xmath372 since @xmath373 and @xmath374 , we have @xmath375 considering that @xmath376 , we finally get the desired result , that is @xmath377      * acknowledgments . *",
    "we wish to thank dragi anevski and enno mammen to have made us aware of reference @xcite .",
    "arnaud guyader is greatly indebted to bernard delyon for fruitful discussions on von neumann s algorithm ."
  ],
  "abstract_text": [
    "<S> this article introduces a new nonparametric method for estimating a univariate regression function of bounded variation . </S>",
    "<S> the method exploits the jordan decomposition which states that a function of bounded variation can be decomposed as the sum of a non - decreasing function and a non - increasing function . </S>",
    "<S> this suggests combining the backfitting algorithm for estimating additive functions with isotonic regression for estimating monotone functions . </S>",
    "<S> the resulting iterative algorithm is called iterative isotonic regression ( ) . </S>",
    "<S> the main technical result in this paper is the consistency of the proposed estimator when the number of iterations @xmath0 grows appropriately with the sample size @xmath1 . </S>",
    "<S> the proof requires two auxiliary results that are of interest in and by themselves : firstly , we generalize the well - known consistency property of isotonic regression to the framework of a non - monotone regression function , and secondly , we relate the backfitting algorithm to von neumann s algorithm in convex analysis .    </S>",
    "<S> _ index terms _  nonparametric statistics , isotonic regression , additive models , metric projection onto convex cones .    </S>",
    "<S> _ 2010 mathematics subject classification _ : 52a05 , 62g08 , 62g20 .    </S>",
    "<S> iterative isotonic regression +    arnaud guyader + universit rennes 2 , inria and irmar + campus de villejean , rennes , france +    nick hengartner + los alamos national laboratory + nm 87545 , usa +    nicolas jgou + universit rennes 2 + campus de villejean , rennes , france +    eric matzner - lber + universit rennes 2 + campus de villejean , rennes , france + </S>"
  ]
}