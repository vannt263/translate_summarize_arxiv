{
  "article_text": [
    "-based resource allocation schemes are expected to become an essential element of emerging 5 g networks , as 5 g devices will have the capability to accurately self - localize and predict relevant channel quality metrics ( cqm ) @xcite based on crowd - sourced databases .",
    "the geo - tagged cqm ( including , e.g. , received signal strength , delay spread , and interference levels ) from users enables the construction of a dynamic database , which in turn allows the prediction of cqm at arbitrary locations and future times .",
    "current standards are already moving in this direction through the so - called minimization of drive test ( mdt ) feature in 3gppp release 10 @xcite . in mdt , users collect radio measurements and associated location information in order to assess network performance . in terms of applications ,",
    "prediction of spatial wireless channels ( e.g. , through radio environment maps ) and its utilization in resource allocation can reduce overheads and delays due to the ability to predict channel quality beyond traditional time scales @xcite .",
    "exploitation of location - aware cqm is relevant for interference management in two - tier cellular networks @xcite , coverage hole detection and prediction @xcite , cooperative spectrum sensing in cognitive radios @xcite , anticipatory networks for predictive resource allocation @xcite , and proactive caching @xcite .    in order to predict location - dependent radio propagation channels ,",
    "we rely on mathematical models , in which the physical environment , including the locations of transmitter and receiver , play an important role . the received signal power in a wireless channel",
    "is mainly affected by three major dynamics , which occur at different length scales : path - loss , shadowing , and small - scale fading@xcite .",
    "small - scale fading decorrelates within tens of centimeters ( depending on the carrier frequency ) , making it infeasible to predict based on location information . on the other hand ,",
    "shadowing is correlated up to tens of meters , depending on the propagation environment ( e.g. , 50100 m for outdoor @xcite and 12 m for indoor environments@xcite ) .",
    "finally , path - loss , which captures the deterministic decay of power with distance , is a deterministic function of the distance to the transmitter . in rich scattering environments ,",
    "the measurements average small - scale fading either in frequency or space provided sufficient bandwidth or number of antennas@xcite . _",
    "_ thus , provided that measurements are dominated by large - scale fading , location - dependent models for path - loss and shadowing can be developed based on the physical properties of the wireless channel . with the help of spatial regression tools , these large - scale channel components can be predicted at other locations and used for resource allocation @xcite .",
    "however , since localization is subject to various error sources ( e.g. , the global positioning system ( gps ) gives an accuracy of around 10 m @xcite in outdoor scenarios , while ultra - wide band ( uwb ) systems can give sub - meter accuracy ) , there is a fundamental need to account for location uncertainties when developing spatial regression tools .",
    "spatial regression tools generally comprise a training / learning phase , in which the underlying channel parameters are estimated based on the available training database , and a testing / prediction phase , in which predictions are made at test locations , given learned parameters and the training database . among such tools , gaussian processes ( gp ) is a powerful and commonly used regression framework , since it is generally considered to be the most flexible and provides prediction uncertainty information @xcite . two important limitations of gp are its computational complexity @xcite and its sensitivity to uncertain inputs @xcite . to alleviate the computational complexity ,",
    "various sparse gp techniques have been proposed in @xcite , while online and distributed gp were treated in @xcite and @xcite , respectively .",
    "the impact of input uncertainty was studied in @xcite , which showed that gp was adversely affected , both in training and testing , by input uncertainties .",
    "the input uncertainty in our case corresponds to location uncertainty .",
    "no framework has yet been developed to mathematically characterize and understand the spatial predictability of wireless channels with location uncertainty . in this paper",
    ", we build on and adapt the framework from @xcite to cqm prediction in wireless networks .",
    "our main contributions are as follows :    * we show that not considering location uncertainty leads to poor learning of the channel parameters and poor prediction of cqm values at other locations , especially when location uncertainties are heterogeneous ; * we relate and unify existing gp methods that account for uncertainty during both learning and prediction , by operating directly on an input set of distributions , rather than an input set of locations ; * we describe and delimit proper choices for mean functions and covariance functions in this unified framework , so as to incorporate location uncertainty in both learning and prediction ; and * we demonstrate the use of the proposed framework for simulated data and apply it to a spatial resource allocation application .",
    "the remainder of the paper is structured as follows .",
    "section [ sec : system - model ] presents the channel model and details the problem description for location - dependent channel prediction with location uncertainty . in section [ sec : cgp ] , we review channel learning and prediction in the classical gp ( cgp ) setup with no localization errors .",
    "section [ sec : ugp ] details learning and prediction procedures using the proposed gp framework that accounts for uncertainty on training and test locations , termed uncertain gp ( ugp ) .",
    "finally , numerical results are given in section [ sec : numerical - results ] in addition to a resource allocation example , followed by our conclusions in section [ sec : conclusion ] .",
    "vectors and matrices are written in bold ( e.g. , a vector @xmath0 and a matrix * @xmath1 * ) ; @xmath2 denotes transpose of * @xmath1 * ; @xmath3 denotes determinant of * @xmath1 * ; @xmath4_{ij}$ ] denotes entry @xmath5 of @xmath1 ; * @xmath6 * denotes identity matrix of appropriate size ; @xmath7 and @xmath8 are vectors of ones and zeros , respectively , of appropriate size ; @xmath9 denotes @xmath10-norm unless otherwise stated ; @xmath11 $ ] denotes the expectation operator ; @xmath12 $ ] denotes covariance operator ( i.e. , @xmath13=\\mathbb{e}[\\mathbf{y}_{1}\\mathbf{y}_{2}^{\\mathrm{t}}]-\\mathbb{e}[\\mathbf{y}_{1}]\\,\\mathbb{e}[\\mathbf{y}_{2}]^{\\mathrm{t}}$ ] ) ; @xmath14 denotes a gaussian distribution evaluated in @xmath15 with mean vector @xmath16 and covariance matrix @xmath17 and @xmath18 denotes that @xmath15 is drawn from a gaussian distribution with mean vector @xmath16 and covariance matrix @xmath17 .",
    "important symbols used in the paper are : @xmath19 is an exact , true location ; @xmath20 , @xmath21 is a vector that describes ( e.g. , in the form of moments ) the location distribution @xmath22 .",
    "for example in the case of gaussian distributed localization error , @xmath23 , then a possible choice is @xmath24^{\\mathrm{t}}$ ] , where @xmath25 $ ] stacks all the elements of @xmath26 in a vector .",
    "finally , @xmath27 is a location estimate extracted from @xmath28 through a function @xmath29 ( e.g. , the mean or mode ) .",
    "first , we give an overview of the literature on gp with uncertain inputs .",
    "one way to deal with the input noise is through linearizing the output around the mean of the input @xcite . in @xcite ,",
    "the input noise was viewed as extra output noise by linearization at each point and this is proportional to the squared gradient of the gp posterior mean .",
    "however , the proposed method works under the condition of constant - variance input noise . in @xcite ,",
    "a delta method was used for linearization under the assumption of gaussian distributed inputs and proposed a corrected covariance function that accounts for the input noise variance . for gaussian distributed test inputs and known training inputs ,",
    "the exact and approximate moments of the gp posterior was examined for various forms of covariance functions @xcite .",
    "training on gaussian distributed input points by calculating the expected covariance matrix was studied in @xcite .",
    "two approximations were evaluated in @xcite , first a joint maximization of joint posterior on uncertain inputs and hyperparameters ( leading to over - fitting ) , and second using a stochastic expectation  maximization algorithm ( at a high computational cost ) .",
    "we now review previous works on gp for channel prediction , which include spatial correlation of shadowing in cellular @xcite and ad - hoc networks @xcite , as well as tracking of transmit powers of primary users in a cognitive network @xcite . in @xcite ,",
    "gp was shown to model spatially correlated shadowing to predict shadowing and path - loss at any arbitrary location .",
    "a multi - hop network scenario was considered @xcite , and shadowing was modeled using a spatial loss field , integrated along a line between transmitter and receiver . in @xcite ,",
    "a cognitive network setting was evaluated , in which the transmit powers of the primary users were tracked with cooperation among the secondary users . for this purpose a distributed radio channel tracking framework using kriged kalman filter was developed with location information . a study on the impact of underlying channel parameters on the spatial channel prediction variance using gp",
    "was presented in @xcite .",
    "a common assumption in @xcite was the presence of perfect location information .",
    "this assumption was partially removed in @xcite , which extends @xcite to include the effect of localization errors on spatial channel prediction .",
    "it was found that channel prediction performance was degraded when location errors were present , in particular when either the shadowing standard deviation or the shadowing correlation were large .",
    "however , @xcite did not tackle combined learning and prediction under location uncertainty .",
    "the only work that explicitly accounts for location uncertainty was @xcite , in which the laplace approximation was used to obtain a closed - form analytical solution for the posterior predictive distribution .",
    "however , @xcite did not consider learning of parameters in presence of location uncertainty .",
    "consider a geographical region @xmath30 , where a source node is located at the origin and transmits a signal with power @xmath31 to a receiver located at @xmath32 through a wireless propagation channel .",
    "the received radio signal is affected mainly by distance - dependent path - loss , shadowing due to obstacles in the propagation medium , and small - scale fading due to multipath effects .",
    "the received power @xmath33 can be expressed as ( * ? ? ?",
    "2 ) @xmath34 where @xmath35 is a constant that captures antenna and other propagation gains , @xmath36 is the path - loss exponent , @xmath37 is the location - dependent shadowing and @xmath38 is the small - scale fading .",
    "we assume measurements average small - scale fading , either in time ( measurements taken over a time window ) , frequency ( measurements represent average power over a large frequency band ) , or space ( measurements taken over multiple antennas ) @xcite .",
    "therefore , the resulting received signal power from the source node to a receiver node @xmath39 can be expressed in db scale as @xmath40}=l_{0}-10\\,\\eta\\,\\log_{10}(\\vert\\mathbf{x}_{i}\\vert)+\\psi(\\mathbf{x}_{i}),\\label{eq : loc_dep_rec_pow}\\ ] ] where @xmath41}+g_{0}$ ] with @xmath42 and @xmath43 .",
    "a common choice for modeling shadowing in wireless systems is through a log - normal distribution , i.e. , @xmath44 , where @xmath45 is the shadowing variance .",
    "shadowing @xmath46 is spatially correlated , with well - established correlation models @xcite , among which the gudmundson model is widely used @xcite .",
    "let @xmath47 be the scalar observation of the received power at node @xmath39 , which is written as @xmath48 where @xmath49 is a zero mean additive white gaussian noise with variance @xmath50 .",
    "for the sake of notational simplicity , we do not consider a three - dimensional layout , the impact of non - uniform antenna gain patterns , or distance - dependent path - loss exponents .      in practice , nodes may not have access to their true location @xmath51 , but only to a distribution @xmath22 is used for @xmath52 for notational simplicity . ] .",
    "the distribution @xmath22 is obtained from the positioning algorithm in the devices , and depends on the specific positioning technology ( e.g. , for gps the distribution @xmath22 can be modeled as a gaussian ) .",
    "we will assume that all distributions @xmath22 come from a given family of distributions ( e.g. , all bivariate gaussian distributions ) .",
    "these distributions can be described by a finite set of parameters , @xmath20 , @xmath21 , e.g. , a mean and a covariance matrix for gaussian distributions .",
    "the set of descriptions of all distributions from the given family is denoted by @xmath53 . within this set ,",
    "the set of all delta dirac distributions over locations is denoted by @xmath54 .",
    "note that @xmath55 is equivalent to the set @xmath56 of possible locations .",
    "finally , we introduce a function @xmath57 that extracts a position estimate from the distribution ( in our case chosen as the mean ) , and denote @xmath58 .",
    "we will generally make no distinction between a distribution @xmath22 and its representation @xmath28 .",
    "we assume a central coordinator , which collects a set of received power measurements @xmath59^{t}$ ] with respect to a common source from @xmath60 nodes , along with their corresponding location distributions @xmath61^{\\mathrm{t}}$ ] .",
    "our goals are to perform    1 .",
    "_ learning _ : construct a spatial model ( through estimating model parameters @xmath62 , to be defined later ) of the received power based on the measurements ; 2 .",
    "_ prediction _ : determine the predictive distribution @xmath63 of the power in test locations @xmath64 and the distribution of the expected should be interpreted as the expected received power , @xmath65 , where @xmath66 is described by @xmath67 received power , @xmath68 , for test location distributions @xmath69 .",
    "we will consider two methods for learning and prediction : classical gp ( section [ sec : cgp ] ) , which ignores location uncertainty and only considers @xmath70 , and uncertain gp ( section [ sec : ugp ] ) , which is a method that explicitly accounts for location uncertainty . we introduce @xmath71^{\\mathrm{t}}$ ] and @xmath72^{\\mathrm{t}}$ ] as the collection of true and estimated locations respectively .",
    "a high level comparison of cgp and ugp is shown in fig .",
    "[ fig : cgp_ugp_highlevel ] , where cgp operates on @xmath73 and @xmath74 , while ugp operates on @xmath75 and @xmath74 .",
    "and estimates @xmath73 of the ( unobserved ) actual locations @xmath76 where those observations have been taken .",
    "@xmath73 is obtained through a positioning system .",
    "the true locations @xmath76 are marked with a triangle and are generally different from the estimated locations @xmath73 , marked with a blue and red dot . during prediction , cgp predicts received power at an estimated test location , @xmath77 .",
    "in contrast , ugp considers the distribution of the locations @xmath76 , described by @xmath75 ( and depicted by the red and blue circle ) , during learning . during prediction",
    ", ugp utilizes the distribution @xmath78 of the test location .",
    "note that the amount of uncertainty ( radius of the circle ) can change . ]",
    "we first present cgp under the assumption that all locations during learning and prediction are known exactly , based on @xcite . later in this section",
    ", we will discuss the impact of location uncertainties on cgp in learning / training and prediction / testing .",
    "we designate @xmath32 as the _ input _ variable , and @xmath33 as the _ output _ variable .",
    "we model @xmath33 as a gp with mean function @xmath79 and a positive semidefinite covariance function @xmath80 , and we write @xmath81 where @xmath82 stands for a gaussian process . the mean function",
    "is defined as @xmath83=l_{0}-10\\,\\eta\\;\\log_{10}(\\vert\\mathbf{x}_{i}\\vert)$ ] , due to ( [ eq : loc_dep_rec_pow ] ) .",
    "the covariance function is defined as @xmath84 $ ] .",
    "we will consider a class of covariance functions of the form : @xmath85 where @xmath86 for @xmath87 and zero otherwise , @xmath88 , @xmath89 is the correlation distance of the shadowing , and @xmath90 captures any noise variance term that is not due to measurement noise ( more on this later ) . setting @xmath91 in ( [ eq : generic_cov_function ] ) , gives the exponential covariance function that is commonly used to describe the covariance properties of shadowing @xcite , and @xmath92 , gives the squared exponential covariance function that will turn out to be useful in section [ sub : uncertain - gp ] .",
    "note that the mean and covariance depend on @xmath93,\\ ] ] which may not be known a priori .",
    "the objective during learning is to infer the model parameters @xmath62 from observations @xmath94 of the received power at @xmath60 _ known _ locations @xmath76 .",
    "the resulting training database is thus @xmath95 . due to the gp model ,",
    "the joint distribution of the @xmath60 training observations exhibits a gaussian distribution @xmath96 where @xmath97^{\\mathrm{t}}$ ] is the mean vector and @xmath1 is the covariance matrix of the measured received powers , with entries @xmath4_{ij}=c(\\mathbf{x}_{i},\\mathbf{x}_{j})+\\sigma_{n}^{2}\\,\\delta_{ij}$ ] .",
    "the model parameters can be learned through maximum likelihood estimation , given the training database @xmath95 , by minimizing the negative log - likelihood function with respect to @xmath62 : @xmath98 the negative log - likelihood function is usually not convex and may contain multiple local optima .",
    "additional details on the learning process are provided later . once @xmath99 is determined from @xmath95 ,",
    "the training process is complete .      after learning",
    ", we can determine the predictive distribution of @xmath100 at a new and arbitrary test location @xmath64 , given the training database @xmath95 and @xmath99 .",
    "we first form the joint distribution @xmath101\\sim\\mathcal{n}\\left(\\left[\\begin{array}{c } \\boldsymbol{\\mu}(\\mathbf{x})\\\\ \\mu(\\mathbf{x } _ { * } ) \\end{array}\\right],\\left[\\begin{array}{cc } \\mathbf{k } & \\mathbf{k}_{*}\\\\ \\mathbf{k}_{*}^{\\mathrm{t } } & k _ { * * } \\end{array}\\right]\\right),\\label{eq : gaussian_joint distribution}\\ ] ] where @xmath102 is the @xmath103 vector of cross - covariances @xmath104 between the received power at @xmath64 and at the training locations @xmath51 , and @xmath105 is the prior variance ( i.e. , the variance in the absence of measurements ) , given by @xmath106 . conditioning on the observations * @xmath94 * , we obtain the gaussian posterior distribution @xmath107 for the test location @xmath64 .",
    "the mean ( @xmath108 ) and variance ( @xmath109 ) of this distribution turn out to be @xcite @xmath110_{ij}\\,(y_{j}-\\mu(\\mathbf{x}_{j}))\\ , c(\\mathbf{x}_{*},\\mathbf{x}_{i})\\nonumber \\\\ = & \\mu(\\mathbf{x}_{*})+\\sum_{i=1}^{n}\\beta_{i}\\ , c(\\mathbf{x}_{*},\\mathbf{x}_{i}).\\nonumber \\\\ v_{\\mathrm{rx}}(\\mathbf{x}_{*})= & k_{**}-\\mathbf{k}_{*}^{\\mathrm{t}}\\,\\mathbf{k}^{-1}\\,\\mathbf{k}_{*}\\label{eq : var_pred_classicgp}\\\\ = & k_{**}-\\sum_{i , j=1}^{n}[\\mathbf{k}{}^{-1}]_{ij}\\ , c(\\mathbf{x}_{*},\\mathbf{x}_{i})\\ , c(\\mathbf{x}_{*},\\mathbf{x}_{j}),\\nonumber\\end{aligned}\\ ] ] where @xmath111_{ij}(y_{j}-\\mu(\\mathbf{x}_{j}))$ ] . in ( [ eq : mu_pred_classicgp ] )",
    ", @xmath112 corresponds to the deterministic path - loss component at @xmath64 , which is corrected by a term involving the database and the correlation between the measurements at the training locations and the test location . in ( [ eq : var_pred_classicgp ] ) , we see that the prior variance @xmath113 is reduced by a term that accounts for the correlation of nearby measurements .    impact of location uncertainty for a one - dimensional example : the red curve depicts the received signal power @xmath114 as a function of @xmath15 ( or equivalently , the distance to the base station ) , while the markers show @xmath115@xmath33 as a function of @xmath70 .",
    "training measurements are grouped into three regions : ( + ) corresponds to high uncertainty , ( @xmath116 ) corresponds to low uncertainty , and ( * ) corresponds to medium uncertainty , respectively .",
    "the location uncertainty results in output noise . ]",
    "now let us consider the case when the nodes do not have access to their true location @xmath51 , but only to a distribution @xmath22 , which is described by @xmath117 .",
    "[ fig : training - locations - with ] illustrates the impact of location uncertainties assuming gaussian location errors for a one - dimensional example .",
    "the figure shows ( in red ) the true received power @xmath114 as a function of @xmath15 as well as the measured power @xmath33 as a function of @xmath70 for a discrete number of values of @xmath118 , shown as markers . to clearly illustrate the impact of different amounts on uncertainty on the position",
    ", we have artificially created three regions : high location uncertainty close to the transmitter , medium location uncertainty far away , and low location uncertainty for intermediate distances . when there is no location uncertainty ( 70 m until 140 m from the transmitter ) , @xmath119 , so @xmath120 , and hence the black dots coincide with the red curve . for medium and high uncertainty ,",
    "@xmath121 can differ significantly from @xmath51 , so the data point with coordinates @xmath122 $ ] can lie far away from the red curve , especially for high location uncertainty ( distances below 70 m ) . from fig .",
    "[ fig : training - locations - with ] it is clear that the input uncertainty manifests itself as output noise , with a variance that grows with increasing location uncertainty around @xmath51 , since a locally flat function will lead to less output noise than a steep function , under the same location uncertainty . ] .",
    "this output noise must be accounted for in the model during learning and prediction . when these uncertainties are ignored",
    ", both learning and prediction will be of poor quality , as described below .      in this case , the training database * @xmath123 * comprises locations @xmath70 and power measurements @xmath124 at the true ( but unknown ) locations @xmath51 .",
    "the measurements will be of the form shown in fig .",
    "[ fig : training - locations - with ] . the estimated model parameters @xmath99 can take two forms : ( i ) assign very short correlation distances @xmath125 , large @xmath126 , and small @xmath127 , as some seemingly nearby events will appear uncorrelated : or ( ii ) assign larger correlation distances @xmath125 , smaller @xmath126 , and explain the measurements by assigning a higher value to @xmath127 @xcite . in the first case , correlations between measurement can not be exploited , so that during prediction , the posterior mean will be close to the prior mean and the posterior variance will be close to the prior variance . in the second case , predictions will be better , as correlations can be exploited to reduce the posterior variance .",
    "however , the model must explain different levels of input uncertainty with a single covariance function , which can make no distinctions between locations with low , medium , or high uncertainty .",
    "this will lead to poor performance when location error statistics differ from node to node .      in the case where training locations are exactly known",
    "( i.e. , @xmath128 , @xmath129 ) , we may want to predict the power at an uncertain test location @xmath69 , made available to cgp in the form @xmath130 , while the true test location @xmath64 is not known .",
    "this scenario can occur when a mobile user relies on a low - quality localization system and reports an erroneous location estimate to the base station .",
    "the wrong location has impact on the predicted posterior distribution since the predicted mean @xmath131 will differ from the correct mean @xmath112 .",
    "in addition , @xmath102 will contain erroneous entries : the @xmath132-th entry will be too small when @xmath133 and too large when @xmath134 .",
    "this will affect both the posterior mean ( [ eq : mu_pred_classicgp ] ) and variance ( [ eq : var_pred_classicgp ] ) . in the case",
    "were training locations are also unknown , i.e. , @xmath135 , and @xmath136 , these effects are further exacerbated by the improper learning of @xmath62 .",
    "in the previous section , we have argued that cgp is unable to learn and predict properly when training or test locations are not known exactly , especially when location error statistics are heterogeneous . in this section ,",
    "we explore several possibilities to explicitly incorporate location uncertainty .",
    "we recall that @xmath137 denotes the set of all distributions over the locations in the environment @xmath56 , while @xmath54 represents the delta dirac distributions over the positions and has a one - to - one mapping to @xmath56 .",
    "we will describe three approaches .",
    "first , a bayesian approach where the uncertain input ( i.e. , the uncertain location ) is marginalized , leading to a non - gaussian output ( i.e. , the received power ) distribution .",
    "second , we derive a gaussian approximation of the output distribution through moment matching and detail the corresponding learning and prediction expressions . from these expressions , the concepts of expected mean function and expected covariance function naturally appear .",
    "finally , we discuss uncertain gp , which is a gaussian process with input @xmath118 from input set @xmath137 and output @xmath138 . we will relate these three approaches in a unified view . for each approach",
    ", we detail the quality of the solution and the computational complexity .",
    "we note that other approaches exist , e.g. , through linearizing the output around the mean of the input @xcite , but they are limited to mildly non - linear scenarios .      in a bayesian context",
    ", we learn and predict by integrating the respective distributions over the uncertainty of the training and test locations . as this method will involve monte carlo integration , we will refer to it as monte carlo gp ( mcgp ) .",
    "given the training database @xmath139 , the likelihood function with uncertain training locations @xmath140 is obtained by integrating @xmath141 over the random training locations : @xmath142 where @xmath143 . as there is generally no closed - form expression for the integral ( [ eq : modified_likelihood ] ) , we resort to a monte carlo approach by drawing @xmath144 i.i.d .",
    "samples @xmath145 , @xmath146 so that @xmath147 where @xmath148_{ij}=c(\\mathbf{x}_{i}^{(m)},\\mathbf{x}_{j}^{(m)})+\\sigma_{n}^{2}\\,\\delta_{ij}$ ] and @xmath149^{\\mathrm{t}}$ ] .",
    "finally , an estimate of @xmath62 can be found by minimizing the negative log - likelihood function @xmath150 which has to be solved numerically .",
    "[ rem : this - optimization - involves]this optimization involves high computational complexity and possibly numerical instability ( due to the sum of exponentials ) . more importantly , a good estimate of @xmath62 can only be found if a sample @xmath151 is generated that is close to the true locations @xmath76 .",
    "due to the high dimensionality ( * ? ? ?",
    "* section 29.2 ) , this is unlikely , even for large @xmath144 .",
    "hence , ( [ eq : mclearning ] ) will lead to poor estimates of @xmath152 .",
    "given the training database @xmath139 and @xmath152 , we wish to determine @xmath153 for an uncertain test location with associated distribution @xmath66 , described by @xmath69 .",
    "the posterior predictive distribution @xmath153 is obtained by integrating @xmath107 with respect to @xmath76 and @xmath64 : @xmath154 this integral is again analytically intractable .",
    "the laplace approximation was utilized in @xcite to solve ( [ eq : preditive_dist_uncertain_train ] ) , while here we again resort to a monte carlo method by drawing @xmath144 i.i.d .",
    "samples @xmath145 and @xmath155 , so that @xmath156 as @xmath144 increases , the approximate distribution will tend to the true distribution .",
    "we refer to ( [ eq : mclearning ] ) and ( [ eq : mc_ul_ut ] ) as monte carlo gp ( mcgp ) . from ( [ eq : mc_ul_ut ] ) , we can compute the mean ( @xmath157 ) and the variance ( @xmath158 ) ( * ? ? ?",
    "* eq . ( 14.10 ) and eq . ( 14.11 ) ) as @xmath159    prediction is numerically straightforward , though it involves the inversion of an @xmath160 matrix @xmath1 for each of the @xmath144 samples @xmath151 . in the case training locations are known , we can utilize cgp to obtain a good estimate of @xmath62 and efficiently and accurately compute @xmath157 and @xmath158 . when both training and test locations are known , the above procedure reverts to cgp .",
    "we have seen that while mcgp can account for location uncertainty during prediction , it will fail to deliver adequate estimates of @xmath62 during learning ( see remark [ rem : this - optimization - involves ] ) . to address this",
    ", we can modify @xmath140 from ( [ eq : modified_likelihood ] ) using a gaussian approximation through moment matching .",
    "in addition , we can also form a gaussian approximation of @xmath153 for prediction .",
    "we will term this approach gaussian approximation gp ( gagp ) .",
    "the expressions that are obtained in the learning of gagp , namely the expectation of mean and covariance functions will be used later in the design of uncertain gp ( described in section [ sub : uncertain - gp ] ) .",
    "given the training database @xmath139 , the mean of @xmath140 is given by @xmath161 & = \\iint\\mathbf{y}\\ , p(\\mathbf{y}\\vert\\mathbf{x},\\boldsymbol{\\theta})\\ , p(\\mathbf{x})\\,\\mathrm{d}\\mathbf{x}\\,\\mathrm{d}\\mathbf{y } \\nonumber \\\\   & = \\iint(\\mathbf{y}\\ , p(\\mathbf{y}\\vert\\mathbf{x},\\boldsymbol{\\theta})\\,\\mathrm{d}\\mathbf{y})\\ , p(\\mathbf{x})\\,\\mathrm{d}\\mathbf{x}\\nonumber \\\\   & = \\int\\mathbf{\\mathbb{\\boldsymbol{\\mu}}}(\\mathbf{x})\\ , p(\\mathbf{x})\\,\\mathrm{d}\\mathbf{x}\\nonumber \\\\   & = \\mathbf{\\mathbb{\\boldsymbol{\\mu}}}(\\mathbf{u}),\\label{eq : exp_mean}\\end{aligned}\\ ] ] where @xmath162^{\\mathrm{t}}$ ] and @xmath163 .",
    "the covariance matrix of @xmath140 can be expressed as @xmath164\\nonumber \\\\   & = \\int\\mathbf{y}\\mathbf{y}^{\\mathrm{t}}\\ , p(\\mathbf{y}\\vert\\mathbf{x},\\boldsymbol{\\theta})\\ , p(\\mathbf{x})\\,\\mathrm{d\\mathbf{x}}\\,\\mathrm{d}\\mathbf{y}-\\mathbf{\\mathbb{\\boldsymbol{\\mu}}}(\\mathbf{u})\\mathbf{\\mathbb{\\boldsymbol{\\mu}}}(\\mathbf{u})^{\\mathrm{t}}\\nonumber \\\\   & = \\int\\bigl(\\mathbf{k}+\\mathbf{\\mathbb{\\boldsymbol{\\mu}}}(\\mathbf{x})\\mathbf{\\mathbb{\\boldsymbol{\\mu}}}(\\mathbf{x})^{\\mathrm{t}}\\bigr)\\ , p(\\mathbf{x})\\,\\mathrm{d\\mathbf{x}}-\\mathbf{\\mathbb{\\boldsymbol{\\mu}}}(\\mathbf{u})\\mathbf{\\mathbb{\\boldsymbol{\\mu}}}(\\mathbf{u})^{\\mathrm{t}}\\nonumber \\\\   & = \\mathbf{k}_{\\mathrm{u}}+\\delta,\\label{eq : exp_cov}\\end{aligned}\\ ] ] where @xmath165_{ij}=c_{\\mathrm{u}}(\\mathbf{u}_{i},\\mathbf{u}_{j})+\\sigma_{n}^{2}\\,\\delta_{ij}$ ] in which @xmath166 and @xmath167 is a diagonal matrix with entries @xmath168_{ii}=\\int\\mu^{2}(\\mathbf{x}_{i})p(\\mathbf{x}_{i})\\,\\mathrm{d}\\mathbf{x}_{i}-\\mu^{2}(\\mathbf{u}_{i}).\\label{eq : deltawithexpectedmean}\\ ] ] we will refer to @xmath169 and @xmath170 as the _ expected mean _ and _ expected covariance _ function .",
    "we can now express the likelihood function as @xmath171 so that @xmath62 can be estimated by minimizing the negative log - likelihood function @xmath172    learning in gagp involves computation of the expected mean in ( [ eq : exp_mean ] ) and ( [ eq : deltawithexpectedmean ] ) , as well as the expected covariance function in ( [ eq : expectedcovariance ] ) .",
    "these integrals are generally again intractable , but there are cases where closed - form expression exist @xcite .",
    "these will be discussed in detail in section [ sub : uncertain - gp ] .",
    "gagp avoids the numerical problems present in mcgp and will hence generally be able to provide a good estimate of @xmath62 .",
    "given the training database @xmath139 and @xmath152 , we approximate the predictive distribution @xmath153 by a gaussian with mean @xmath173 and variance @xmath174 .",
    "these are given by @xmath175\\nonumber \\\\   & = \\int\\bar{p}_{\\mathrm{rx}}(\\mathbf{x}_{*})\\ , p(\\mathbf{x})\\ , p(\\mathbf{x}_{*})\\,\\mathrm{d}\\mathbf{x}\\,\\mathrm{d}\\mathbf{x}_{*}\\nonumber \\\\   & = \\mu(\\mathbf{u}_{*})+\\sum_{i=1}^{n}\\int\\beta_{i}\\ , c(\\mathbf{x}_{*},\\mathbf{x}_{i})\\ , p(\\mathbf{x})\\ , p(\\mathbf{x}_{*})\\,\\mathrm{d}\\mathbf{x}\\,\\mathrm{d}\\mathbf{x}_{*}.\\label{eq : mean_gagp}\\end{aligned}\\ ] ] note that @xmath176 is itself a function of all * @xmath76 * s and @xmath64 .",
    "similarly @xmath174 is calculated as @xmath177-\\bar{p}_{\\mathrm{rx}}^{\\mathrm{ga}}(\\mathbf{u}_{*})^{2}\\\\   & = \\int\\bigl(v_{\\mathrm{rx}}(\\mathbf{x}_{*})+\\bar{p}_{\\mathrm{rx}}(\\mathbf{x}_{*})^{2}\\bigr)\\ , p(\\mathbf{x})\\ , p(\\mathbf{x}_{*})\\,\\mathrm{d}\\mathbf{x}\\,\\mathrm{d}\\mathbf{x}_{*}\\nonumber \\\\   & -\\bar{p}_{\\mathrm{rx}}^{\\mathrm{ga}}(\\mathbf{u}_{*})^{2}.\\label{eq : var_gagp}\\end{aligned}\\ ] ] note that both @xmath108 and @xmath109 are functions of @xmath76 ( see ( [ eq : mu_pred_classicgp])([eq : var_pred_classicgp ] ) ) .",
    "prediction in gagp requires complex integrals to be solved in ( [ eq : mean_gagp])([eq : var_gagp ] ) for which no general closed - form expressions are known .",
    "hence , a reasonable approach is to use gagp to learn @xmath152 and mcgp for prediction .    in case training locations",
    "are known , i.e. , @xmath178 , ( [ eq : mean_gagp ] ) reverts to @xmath179 and ( [ eq : var_gagp ] ) becomes @xmath180_{ij}\\int c(\\mathbf{x}_{*},\\mathbf{x}_{i})\\ , c(\\mathbf{x}_{*},\\mathbf{x}_{j})\\ , p(\\mathbf{x}_{*})\\,\\mathrm{d}\\mathbf{x}_{*}\\nonumber \\\\   & + \\int\\mu(\\mathbf{x}_{*})^{2}\\ , p(\\mathbf{x}_{*})\\,\\mathrm{d}\\mathbf{x}_{*}+2\\sum_{i=1}^{n}\\beta_{i}\\bigl(\\int\\mu(\\mathbf{x}_{*})\\ , c(\\mathbf{x}_{*},\\mathbf{x}_{i})\\,\\nonumber \\\\   & \\times p(\\mathbf{x}_{*})\\,\\mathrm{d}\\mathbf{x}_{*}\\bigr)+\\sum_{i , j=1}^{n}\\beta_{i}\\beta_{j}\\int c(\\mathbf{x}_{*},\\mathbf{x}_{i})\\ , c(\\mathbf{x}_{*},\\mathbf{x}_{j})\\ , p(\\mathbf{x}_{*})\\,\\mathrm{d}\\mathbf{x}_{*}\\nonumber \\\\   & -\\bar{p}_{\\mathrm{rx}}^{\\mathrm{ga}}(\\mathbf{u}_{*})^{2},\\label{eq : gauss_approx_cl_ut_var}\\end{aligned}\\ ] ] both of which can be computed in closed form , under some conditions , when @xmath181 is constant in @xmath15 ( * ? ? ?",
    "* section 3.4 ) .",
    "when both @xmath178 and @xmath182 , gagp reverts to cgp .",
    "while gagp avoids the learning problems inherent to mcgp , prediction is generally intractable .",
    "hence , gagp is not a fully coherent approach to deal with location uncertainty . to address this",
    ", we consider a new type of gp ( ugp ) , which _ operates directly on the location distributions , rather than on the locations_. ugp involves a mean function @xmath183 and a positive semidefinite covariance function @xmath184 , which considers as inputs @xmath185 and as outputs @xmath186 .",
    "in other words , @xmath187 the mean function is given by @xmath188 $ ] , already introduced as the expected mean function in ( [ eq : exp_mean ] ) .",
    "however , for the mean function to be useful in a gp context , it should be available in closed form .",
    "as in cgp , we have significant freedom in our choice of covariance function .",
    "apart from all technical conditions on the covariance function as described in @xcite , it is desirable to have a covariance function that ( i ) is available in closed form ; ( ii ) leads to decreasing correlation with increasing input uncertainty ( even when both inputs have same mean ) ; ( iii ) can account for varying amounts of input uncertainty ; ( iv ) reverts to a covariance function of the form ( [ eq : generic_cov_function ] ) when @xmath189 , ( v ) does not depend on the mean function @xmath181 .",
    "we will now describe the mean function @xmath190 and covariance function @xmath191 in detail .      according to law of iterated expectations ,",
    "the mean function @xmath169 is expressed as @xmath192.\\label{eq : mean_uncertain_input}\\ ] ] while there is no closed - form expression available for ( [ eq : mean_uncertain_input ] ) , we can form a polynomial approximation @xmath193 , where the coefficients @xmath194 are found by least squares minimization .",
    "for a given range of @xmath195 , this approximation can be made arbitrarily close by increasing the order @xmath196 .",
    "when @xmath197 is approximately gaussian ( which may be the case for @xmath198 ) , @xmath199 $ ] can be evaluated in closed form , since all gaussian moments are known .",
    "see appendix [ sec : expected - mean - function ] for details on the approximation .",
    "while any covariance function meeting the criteria ( i)(v ) listed above can be chosen , a natural choice is ( see section [ sub : classical - gp - description ] ) @xmath200\\nonumber \\\\   & = \\textrm{cov}[y_{i},y_{j}\\vert\\mathbf{u},\\boldsymbol{\\theta}]-\\delta_{ij}\\sigma_{n}^{2}.\\end{aligned}\\ ] ] unfortunately , as we can see from ( [ eq : exp_cov ] ) , this choice does not satisfy criterion ( v ) .",
    "an alternative choice is the expected covariance function @xmath170 from ( [ eq : expectedcovariance ] ) .",
    "this choice clearly satisfies criteria ( ii ) , ( iii ) , ( iv ) , and ( v ) . to satisfy ( i ) , we can select appropriate covariance functions , tailored to the distributions @xmath22 , or appropriate distributions @xmath22 for a given covariance function .",
    "examples include :    * polynomial covariance functions for gaussian @xmath22 @xcite . * covariance functions of the form ( [ eq : generic_cov_function ] ) with @xmath91 , @xmath201 , for laplacian @xmath22 .",
    "* covariance functions of the form ( [ eq : generic_cov_function ] ) with @xmath92 , @xmath19 , for gaussian @xmath22 ( i.e. , @xmath202 ) . the expected covariance function is then given by @xcite @xmath203 note that the factor @xmath204 ensures that inputs @xmath205 with the same mean ( i.e. , @xmath206 ) exhibit lower correlation with increasing uncertainty .",
    "the factor @xmath207 ensures that the measurements taken at locations with low uncertainty ( smaller than @xmath89 ) can be explained by a large value of @xmath89 , while for measurements taken at locations with high uncertainty , @xmath170 will be small and decreasing with increasing uncertainty .      given the training database @xmath139 and choosing @xmath208 and @xmath209 , the model parameters are found by minimizing the log - likelihood function @xmath210 note that in contrast to gagp , we have constructed ugp so that @xmath211 and @xmath212 are available in closed form , making numerical minimization tractable .",
    "learning of ugp ( [ eq : loglikelihood_ugp ] ) corresponds to the case of learning ( [ eq : likelihood_gaussian_approx ] ) in gagp for @xmath213 ( e.g. , for constant mean processes ) .",
    "let @xmath214 be the mean and @xmath215 be the variance of the posterior predictive distribution @xmath153 of ugp with uncertain training and test locations , then @xmath216 .",
    "the expressions for @xmath214 and @xmath215 are now in standard gp form : @xmath217 where @xmath218 is the @xmath103 vector of cross - covariances @xmath219 between the received power at the test distribution @xmath69 and at the training distribution @xmath28 , and @xmath220 is the a priori variance @xmath221 .    in case",
    "the training locations are known , i.e. , @xmath178 , the mean @xmath214 and the variance @xmath215 can be obtained from the expressions ( [ eq : uncertain_mean ] ) and ( [ eq : uncertain_variance ] ) , respectively , by setting @xmath222 .",
    "furthermore , the resulting mean @xmath214 is exactly the same as ( [ eq : gauss_approx_cl_ut_mean ] ) , obtained in gagp .",
    "however , due to a different choice of covariance function , the predicted variance @xmath215 is different from ( [ eq : gauss_approx_cl_ut_var ] ) .",
    "when the test location is known , i.e. , @xmath182 , the mean @xmath108 and the variance @xmath109 are obtained from ( [ eq : uncertain_mean ] ) and ( [ eq : uncertain_variance ] ) by setting @xmath223 .",
    "learning and prediction phases of cgp and ugp .",
    "the difference in learning in ugp compared to cgp is that it considers location uncertainty of the nodes .",
    "the estimated model parameters @xmath99 are derived during the learning phase and are generally different in cgp compared to ugp .",
    "the mean @xmath224 and variance @xmath225 of the posterior predictive distribution in cgp corresponds to a location @xmath226 extracted from @xmath69 , which in turn represents @xmath66 .",
    "in contrast , the mean @xmath214 and variance @xmath215 of the posterior predictive distribution in ugp pertains to the entire location distribution represented by @xmath69 . ]",
    "relation between cgp , mcgp , gagp , and ugp . all methods are equivalent when the input is limited to @xmath55 ( grey shaded area ) . ]",
    "we are now ready to recap the main differences between cgp and ugp , and to provide a unified view of the four methods ( cgp , mcgp , gagp , and ugp ) .",
    "[ fig : learning - and - testing ] describes the main processes in ugp and cgp , along with the inputs and outputs during the learning and prediction processes .",
    "the four methods are depicted in fig .",
    "[ fig : relation - between - cgp - ugp ] : all four methods revert to cgp when training and predictions occur in @xmath55 , i.e. , when there is no uncertainty about the locations .",
    "mcgp is able to consider general input distributions in @xmath137 , but leads to non - gaussian output distributions . through a gaussian approximation of these output distributions ,",
    "gagp can consider general inputs and directly determine a gaussian output distribution .",
    "both of these approaches ( mcgp and gagp ) have in common that they treat the process with input @xmath227 as a gp .",
    "in contrast , ugp treats the process with input @xmath185 as a gp .",
    "this allows for a direct mapping from inputs in @xmath137 to gaussian output distributions . in terms of tractability for learning and prediction ,",
    "the four methods are compared in table [ tab : comparision - of - benifits ] .",
    "we see that among all four methods , ugp combines tractability with good performance .",
    ".[tab : comparision - of - benifits]comparison of tractability for cgp , mcgp , gagp , and ugp in learning and prediction . [ cols=\"^,^,^\",options=\"header \" , ]      a geographical region @xmath56 is considered and a base station is placed at the origin .",
    "a one dimensional radio propagation field is generated with sampling locations at a resolution of 0.25 m using an exponential covariance function @xmath228 , corresponding to the gudmundson model .",
    "small - scale fading is assumed to have been averaged out.the simulation parameters used to obtain the numerical results are given in table [ tab : simulation - parameters-1 ] .",
    "we assume isotropic localization errors , so that @xmath229 .",
    "to capture the effect of heterogeneous location errors , we draw the location error standard deviations from an exponential distribution , i.e. , @xmath230 , where @xmath231 is the average location error standard deviation . for cgp and mcgp , in order to not provide any unfair advantage to ugp",
    ", we use a covariance function of the form ( [ eq : generic_cov_function ] ) with @xmath91 , in order to match the true covariance function @xmath232 . for ugp ,",
    "we use ( [ eq : uncertain_se_cov ] ) .",
    "since ugp exhibits a mismatch in the covariance function , we absorb this mismatch in @xmath90 , which is learned offline ( more on this in appendix [ sec : learning ] ) .",
    "we assume nodes know @xmath233 and @xmath234 , which be inferred using standard methods @xcite , so they are not included in the learning process .",
    "[ fig : impact_location_error_learning ] depicts the impact of location uncertainty on the learning of hyperparameters @xmath235 $ ] for cgp , ugp , and mcgp .",
    "the learning of the hyperparameters is detailed in appendix [ sec : learning ] .",
    "we first consider a variant of cgp , denoted as cgp - no - proc , in which @xmath90 is fixed to zero . in cgp - no - proc , when @xmath236 , the estimate @xmath125 is non - zero .",
    "however , it can be observed in fig .",
    "[ fig : impact_location_error_learning ] ( a ) , that with increase in @xmath231 , @xmath125 decreases quickly to zero .",
    "hence , cgp - no - proc will model the gp as a white process with high variance @xmath237 and thus can not handle the location uncertainty . on the other hand , in cgp where we estimate @xmath90 ,",
    "@xmath127 absorbs part of location uncertainty ( see fig .  [",
    "fig : impact_location_error_learning ] ( c ) ) .",
    "consequently , the part of the observations that must be explained through @xmath238 is reduced , leading to a reduction of @xmath126 with @xmath231 . due to this",
    ", cgp considers the measurements constitute a slowly varying process , therefore @xmath125 increases with @xmath231 .",
    "an interesting observation is that the error bars for @xmath125 also increase with @xmath231 .",
    "hence , among cgp - no - proc and cgp , only cgp can reasonably deal with location uncertainty .",
    "the behavior is similar to that of cgp , i.e. , an increase in @xmath125 , and a decrease in @xmath126 , when increasing @xmath231 .",
    "however , @xmath126 decreases more quickly with @xmath231 when compared to cgp .",
    "these effects can be attributed to two causes : first of all , the inherent problem of drawing a finite number of samples as detailed at the end of section [ sub : mcgplearning ] ; secondly , the fluctuations in the estimated path loss exponent @xmath239 with increasing @xmath231 ( see fig .  [",
    "fig : impact_location_error_learning ] ( d ) ) .",
    "the error bars of the estimates in this case are even higher than in cgp .",
    "as expected , mcgp is not suitable for learning .      as mentioned before , in ugp @xmath90 is determined offline .",
    "the ugp model has the capability to absorb the location uncertainty into the covariance function . due to this flexibility",
    ", it can handle higher values of @xmath231 and still maintain an almost constant @xmath125 and @xmath126 with increase in @xmath231 . for fair comparison with cgp",
    ", we also consider the case where @xmath90 is estimated as part of the learning , referred to as ugp - proc .",
    "it can be observed in fig .",
    "[ fig : impact_location_error_learning ] ( c ) that @xmath127 increases with increase in @xmath231 .",
    "when comparing ugp - proc to ugp , we observe a lower value of @xmath126 and higher values of @xmath125 and @xmath127 for a particular value of @xmath231 . from this , we conclude that ugp should be preferred over ugp - proc , as it can explain the observations with smaller @xmath127 and leads to simpler optimization . finally , note that the error bars of the ugp estimates are relatively small when compared to cgp .",
    "four cases can be considered , depending on whether training or testing inputs are in @xmath55 or @xmath137 .",
    "we will focus on the case where _ either _ training or test locations are uncertain , but not both . from these ,",
    "the behavior when both training and testing inputs are in @xmath137 can be easily understood : only ugp can give reasonable performance among cgp , mcgp , and ugp , as the estimates of @xmath240 in cgp and mcgp are of poor quality .",
    "in this case @xmath117 and @xmath182 .",
    "[ fig : perf_comp_ul_ct ] ( a ) depicts the prediction results in terms of the predictive mean and predictive standard deviation ( shown as shaded areas ) for a particular realization of the channel field .",
    "it can be observed that ugp is able to predict the received power comparatively better than cgp and mcgp .",
    "ugp is able to estimate the underlying channel parameters better with the expected covariance function , which takes in to account the location uncertainty of the nodes . in turn",
    ", this means that ugp can track the faster variations in the channel .",
    "cgp tries to model the true function with a slow varying process due to very high @xmath125 .",
    "furthermore , cgp has higher uncertainty in predictions due to high @xmath127 ( see fig .  [",
    "fig : impact_location_error_learning ] ( c ) ) . on the other hand , mcgp has slightly better prediction performance ( the standard deviation is not shown , but is slightly smaller than for cgp ) compared to cgp due to the averaging by drawing samples from the distribution of the uncertain training locations . averaging the prediction error over multiple channel realizations , fig .",
    "[ fig : perf_comp_ul_ct ] ( b ) shows the mean squared error ( mse ) of the received power prediction of cgp and ugp with respect to @xmath231 ( mcgp is not shown due to its similar performance to cgp ) .",
    "ugp clearly outperforms cgp ( except fo @xmath236 ) due to its better tracking of the true channel ( see fig .",
    "[ fig : perf_comp_ul_ct ] ( a ) ) despite uncertainty on the training locations .",
    "the reason for higher mse in the case of @xmath236 for ugp is due to its kernel mismatch .      in this case @xmath241 and @xmath242 ( with a constant location error standard deviation @xmath243 m ) . now",
    "the performance must be assessed with respect to the expected received power @xmath244 , where @xmath245 , in which @xmath226 is the mean of distribution described by @xmath69 .",
    "an example is shown in fig .",
    "[ fig : perf_comp_cl_ut ] ( a ) , depicting @xmath246 as a function of @xmath226 , as well as the predictions from cgp , mcgp , and ugp .",
    "it can be observed that ugp and mcgp follow well @xmath246 .",
    "specifically , mcgp tracks @xmath246 quite closely as it is near - optimal in this case .",
    "in contrast , cgp follows the actual received power at @xmath226 , rather than the averaged power .",
    "this leads to fast variations in cgp , which are not present in ugp and mcgp .",
    "[ fig : perf_comp_cl_ut ] ( b ) shows the mse of the received power prediction of cgp , mcgp , and ugp with respect to @xmath243 when averaging the prediction error over multiple channel realizations .",
    "as expected , mcgp has the lower mse than ugp and cgp .",
    "however , ugp performs better than cgp for all considered @xmath243 , except @xmath247 ( due to kernel mismatch ) .",
    "furthermore , the performance of ugp is very close to that of mcgp .        in this section ,",
    "we compare cgp and ugp for a simple proactive resource allocation scenario .",
    "we consider a user moving through a region @xmath56 and predict the cqm at each location .",
    "the supported rate , expressed in bits per channel use ( bpu ) , for a user at location @xmath64 is defined as @xmath248 where @xmath249 , is the signal - to - noise ratio at location @xmath64 , @xmath250 is the receiver thermal noise and @xmath251 is the received power , both measured in linear scale .",
    "the average rate in the region @xmath56 , denoted as @xmath252 , is defined as @xmath253 where @xmath254 denotes area of the region @xmath56 .",
    "the predicted rate for a user at a future location @xmath64 , based on the predicted cqm values @xmath255 , is defined as @xmath256 where @xmath257 is a confidence parameter , @xmath258 and @xmath259 .",
    "the user moves through the environment according to a known trajectory .",
    "the base station allocates bits to each future location , proportional to @xmath260 .",
    "when the user is at location @xmath64 , only a fraction of the bits , proportional to @xmath261 would be delivered .",
    "therefore , the effective rate @xmath262 for the user at location @xmath64 is    @xmath263    the average effective rate @xmath264 for a given confidence level @xmath265 is then computed by spatial average of @xmath262 over region @xmath56 as @xmath266.\\end{aligned}\\ ] ] when @xmath267 , a part of the allocated bits can not be delivered .",
    "the total fraction of undelivered bits over the environment is given by @xmath268 hence , @xmath264 describes the rate that the user will receive ( penalizing under - estimation of the rate ) , while @xmath269 describes the loss due to lost bits ( penalizing over - estimating of the rate ) .",
    "we predict the cqm at known test locations @xmath270 , based on training with uncertain locations ( considering @xmath271 m ) , all within a one - dimensional region @xmath56 . the average effective rate @xmath264 and the fraction of undelivered bits @xmath269 , as a function of @xmath265 , are shown in fig  [ fig : resource - allocation - example ] ( a)(b ) , respectively .",
    "as expected , increasing @xmath265 leads to a more conservative allocation , thus reducing both @xmath264 and @xmath269 .",
    "for a specific value of @xmath265 , increase in @xmath231 decreases @xmath264 .",
    "this is due to the fact that with increase in @xmath231 , the mean @xmath108 is of poor quality and the variance @xmath109 is high for cqm predictions .",
    "it is evident that when @xmath236 , ugp and cgp attain similar performance , both in terms of @xmath264 and @xmath269 .",
    "when @xmath231 is increased to 10 m , cgp suffers from a significant reduction in effective rate @xmath264 , while at the same time dropping up to 4.5 % of the bits .",
    "this is due to cgp s poor predictions , which are either too low ( leading to a reduction in @xmath264 ) or too high ( leading to an increase in @xmath269 ) .",
    "in contrast , ugp , which is able to track the channel well despite uncertain training , achieves a higher effective rate , especially for high confidence values ( e.g. , around 2 times higher rate for @xmath272 , for @xmath269 less than 0.1% ) .",
    "channel quality metrics can be predicted using spatial regression tools such as gaussian processes ( gp ) .",
    "we have studied the impact of location uncertainties on gp and have demonstrated that , when heterogeneous location uncertainties are present , the classical gp framework is unable to ( i ) learn the underlying channel parameters properly ; ( ii ) predict the expected channel quality metric . by introducing a gp that operates directly on the location distribution , we find uncertain gp ( ugp ) , which is able to both learn and predict in the presence of location uncertainties .",
    "this translates in better performance when using ugp for predictive resource allocation .",
    "possible avenues of future research include validation using real measurements , modeling correlation of shadowing in the temporal dimension , study of better approximations for learning with uncertain locations , and the extension to ad - hoc networks .",
    "let @xmath273 and recall from random variable transformation theory that @xmath274 we assume @xmath275 , so @xmath276 follows a rician distribution @xmath277 where @xmath278 is a modified bessel function of zero - th order . for @xmath279 , @xmath276",
    "can be approximated as a gaussian distribution @xmath280 the integral ( [ eq : log_integral ] ) still does not have a closed form expression with @xmath281 . now approximating the @xmath282 function with a polynomial function of the form",
    "@xmath283 then ( [ eq : log_integral ] ) can be written as @xmath284 which can be computed exactly .",
    "in this appendix , we detail the learning of @xmath285 $ ] for cgp , ugp , and mcgp .",
    "we consider nodes know @xmath233 and @xmath234 , therefore they are not estimated as part of the learning process .",
    "let the remaining set of hyperparameters be @xmath286 $ ] and @xmath36 .      based on section [ sec : system - model ] ,",
    "we can write the received measurements @xmath94 with their corresponding training locations @xmath76 in matrix form as @xmath287 where @xmath288^{\\mathrm{t}}$ ] , @xmath289^{\\mathrm{t}},$ ] and @xmath290^{\\mathrm{t}}$ ] . assuming the measurements are uncorrelated , then the least squares estimate of the path - loss exponent can be computed as @xmath291 once the path - loss exponent is estimated , the mean component of the received measurements can be subtracted as , @xmath292 . then , @xmath293 becomes a zero - mean gaussian process .",
    "now the likelihood function ( [ eq : gptraining ] ) becomes @xmath294 .",
    "the hyperparameters @xmath62 are estimated by minimizing negative logarithm of @xmath295 @xmath296 we calculate the variance of the process @xmath293 as @xmath297_{i}^{2}$ ] . the variance of the process should be captured by the hyperparameters @xmath90 , @xmath233 , and @xmath238 .",
    "we define @xmath298 , as a result @xmath295 becomes a function of only @xmath89 and @xmath238 .",
    "we solve ( [ eq : new_likelihood ] ) and find @xmath125 and @xmath126 by an exhaustive grid search .",
    "once @xmath125 and @xmath126 are found , then @xmath127 can be calculated as @xmath299 .      in this case",
    ", the path - loss exponent is estimated as @xmath300 where @xmath301^{\\mathrm{t}}$ ] . once again removing the mean from the measurements , we obtain @xmath302 the hyperparameters @xmath62",
    "are estimated by minimizing the modified negative log - likelihood function @xmath303 again , @xmath304_{i}^{2}$ ] , is the variance of the process . as a result , @xmath126 becomes @xmath305 and due to this @xmath295 is now only a function of @xmath89 .",
    "we solve ( [ eq : new_likelihood - ugp ] ) and find @xmath125 by an exhaustive grid search .",
    "the learning process can be simplified for ugp : since @xmath90 only captures kernel mismatch irrespective of the location uncertainty and path loss , the value of @xmath127 can be obtained off - line with noise - free training locations by performing learning as in the case of cgp , but with a covariance function of the form ( [ eq : generic_cov_function ] ) for @xmath92 .",
    "this approach gives an advantage to cgp and thus makes the comparison between ugp and cgp more fair for all values of @xmath306 .",
    "it is no longer feasible to estimate @xmath36 first and subtract to make the process zero mean , because of summation in the monte carlo integration ( [ eq : mclearningsummation ] ) .",
    "therefore , we optimize ( [ eq : mclearning ] ) with respect to the hyperparameters @xmath36 and @xmath62 using @xmath307 function of matlab .",
    "the authors would like to thank ido nevat , lennart svensson , ilaria malanchini , and vinay suryaprakash for their feedback on the manuscript .        r.  di  taranto , s.  muppirisetty , r.  raulefs , d.  slock , t.  svensson , and h.  wymeersch , `` location - aware communications for 5 g networks , '' _ ieee signal processing magazine _ , vol .  31 , no .  6 , pp .",
    "102112 , nov 2014 .",
    "a.  zalonis , n.  dimitriou , a.  polydoros , j.  nasreddine , and p.  mahonen , `` femtocell downlink power control based on radio environment maps , '' in _ ieee wireless communications and networking conference _ , april 2012 , pp .",
    "12241228 .",
    "a.  galindo - serrano , b.  sayrac , s.  ben  jemaa , j.  riihijarvi , and p.  mahonen , `` harvesting mdt data : radio environment maps for coverage analysis in cellular networks , '' in _ international conference on cognitive radio oriented wireless networks _",
    ", july 2013 , pp . 3742",
    ".      j.  tadrous , a.  eryilmaz , and h.  el  gamal , `` proactive resource allocation : harnessing the diversity and multicast gains , '' _ ieee transactions on information theory _ , vol .",
    "59 , no .  8 , pp . 48334854 , aug 2013 .",
    "n.  jalden , p.  zetterberg , b.  ottersten , a.  hong , and r.  thoma , `` correlation properties of large scale fading based on indoor measurements , '' in _ ieee wireless communications and networking conference _ , march 2007 , pp .",
    "18941899 .",
    "s.  sarkka , a.  solin , and j.  hartikainen , `` spatiotemporal learning via infinite - dimensional bayesian filtering and smoothing : a look at gaussian process regression through kalman filtering , '' _ ieee signal processing magazine _ , vol .",
    "30 , no .  4 , pp . 5161 , 2013 .",
    "m.  jadaliha , y.  xu , j.  choi , n.  johnson , and w.  li , `` gaussian process regression for sensor networks under localization uncertainty , '' _ ieee transactions on signal processing _ ,",
    "61 , no .  2 ,",
    "pp . 223237 , 2013 .",
    "kim , e.  dallanese , and g.  giannakis , `` cooperative spectrum sensing for cognitive radios using kriged kalman filtering , '' _ ieee journal of selected topics in signal processing _ , vol .  5 , no .  1 ,",
    "2436 , 2011 .",
    "s.  choi , m.  jadaliha , j.  choi , and s.  oh , `` distributed gaussian process regression for mobile sensor networks under localization uncertainty , '' in _",
    "52nd annual conference on decision and control _ , dec 2013 , pp . 47664771 .",
    "y.  yan and y.  mostofi , `` impact of localization errors on wireless channel prediction in mobile robotic networks , '' in _ ieee globecom , workshop on wireless networking for unmanned autonomous vehicles _ , dec .",
    "2013 .",
    "a.  goldsmith , l.  greenstein , and g.  foschini , `` error statistics of real - time power measurements in cellular channels with multipath and shadowing , '' _ ieee transactions on vehicular technology _ , vol .  43 , no .  3 , pp . 439446 ,",
    "aug 1994 .",
    "s.  s. szyszkowicz , h.  yanikomeroglu , and j.  s. thompson , `` on the feasibility of wireless shadowing correlation models , '' _ ieee transactions on vehicular technology _ , vol .",
    "59 , no .  9 , pp . 42224236 , 2010 .",
    "y.  mostofi , m.  malmirchegini , and a.  ghaffarkhah , `` estimation of communication signal strength in robotic networks , '' in _ ieee international conference on robotics and automation _ , 2010 ,",
    ". 19461951 ."
  ],
  "abstract_text": [
    "<S> spatial wireless channel prediction is important for future wireless networks , and in particular for proactive resource allocation at different layers of the protocol stack . </S>",
    "<S> various sources of uncertainty must be accounted for during modeling and to provide robust predictions . </S>",
    "<S> we investigate two channel prediction frameworks , classical gaussian processes ( cgp ) and uncertain gaussian processes ( ugp ) , and analyze the impact of location uncertainty during learning / training and prediction / testing , for scenarios where measurements uncertainty are dominated by large - scale fading . </S>",
    "<S> we observe that cgp generally fails both in terms of learning the channel parameters and in predicting the channel in the presence of location uncertainties.in contrast , ugp explicitly considers the location uncertainty . using simulated data , </S>",
    "<S> we show that ugp is able to learn and predict the wireless channel .    </S>",
    "<S> gaussian processes , uncertain inputs , location uncertainty , spatial predictability of wireless channels . </S>"
  ]
}