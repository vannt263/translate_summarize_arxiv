{
  "article_text": [
    "in the past century there occurred a revolution in terms of mathematical understanding of biological systems : their _ dynamical _ nature was appreciated at all levels of organization , from single cells , through organisms , and to the populations of organisms , meaning that their state is not static , but is continuously _ changing _ in time . in particular ,",
    "the generality and persistence of oscillations in living systems has been widely acknowledged .",
    "just a few examples include pacemaker cells and neuron firings at the cellular level , heart beats , breathing and circadian rhythms at the level of an organism , and fluctuations in population size in the communities of organisms .",
    "since then , living systems have often been modelled as dynamical systems .",
    "a concept of a dynamical disease was proposed @xcite , and nowadays new medicines require testing with mathematical models before their mass - production is approved @xcite .    a dynamical system is a mathematical construction incorporating a vector @xmath0 , that describes the system state at any time moment @xmath1 , and some rule that determines how the state evolves in time .",
    "this evolution rule can be defined , e.g. by a system of ordinary differential equations , @xmath2 here , @xmath3 is a _ phase velocity vector field _ , which can be loosely understood as a  force \" that pushes the state @xmath4 in a certain direction , and is generally different at different positions in the state space .",
    "remarkably , even if the vector field @xmath5 is permanently fixed at all points , it generally makes the state change , i.e. creates the  behavior \" .",
    "crucially , all living systems are _ dissipative _ because they permanently lose energy as they function .",
    "mathematically , they can be described by dissipative dynamical systems that have attractors : geometrical objects in the phase space to which all solutions converge from a certain vicinity @xcite .",
    "attractors are very important in the context of self - organization : a dissipative system can be launched from a randomly chosen initial condition , but with time its behavior will automatically settle down on the same stationary mode , whose geometrical image is an attractor .",
    "the most prominent feature of all living systems is their ability to _ modify _ themselves under the influence of the environment .",
    "an extreme example would be a lizard that grows a new tail after the old one is lost .",
    "some more common examples include the growth of frequently used muscles , the development of stamina in response to exercise , and increasing the flexibility of the joints in response to their stretching .",
    "importantly , the environmental influence is generally quite _ random _ , but the living system responds to it in a coherent manner . with account of this adaptation ability , it might be more appropriate to model living systems as dynamical systems , whose vector field _ modifies _ itself in time automatically in response to the external random stimulus .    * learning in the brain . *",
    "the most striking feature of a sufficiently advanced living system is its ability to _",
    "learn_. learning mechanisms in living systems are associated with the nervous system : the brain and its connections with all parts of the body . since the first discovery that the brain does not represent a homogeneous substance , but is rather a collection of intertwined discrete units called neurons @xcite , a huge volume of biological and psychological research has been carried out in order to reveal the biological mechanisms of learning .",
    "it is well established that in the course of learning the _ architecture _ of the brain changes .",
    "namely , while the internal structure of the individual neurons remains roughly the same , the _ connections _ between different neurons change in time in response both to the sensor stimuli , and to the processes inside the brain @xcite .",
    "this fact has given rise to a separate research area in the field of artificial intelligence : artificial neural networks .",
    "at the same time , it contributed to the cognitive theory , and to the philosophy of science in general , by giving birth to the connectionism paradigm @xcite , within which all knowledge ( or _ information _ ) in the brain is represented in the form of the strengths of connections between the neurons .",
    "note , that the sensory stimuli that the brain receives are typically quite _ random _ , but the brain seems to accumulate information in a consistent and orderly manner .",
    "* information .",
    "* we point out that while the term  information \" has penetrated all spheres of human activity and is used most broadly , we are still lacking an accurate and at the same time sufficiently broad definition of it .",
    "information theory , which has been introduced and developed within mathematical and physical sciences , operates with sequences of symbols and various probabilities of their occurrence .",
    "there are a few definitions of information , and the most widely used seem to be those proposed by shannon @xcite and fisher @xcite .",
    "where a message can not be reduced to a sequence of symbols , there is no suitable mathematical theory .",
    "one example illustrating the limitations of modern information theory is our perception of facial expression , e.g. a smile . while it might be easy to classify the message as a smile , the subtle _ meaning _ of it might vary considerably , from approving to ridiculing .",
    "an ideal information theory should be able to detect all the _ meanings _ in the message together with their relative quantities .",
    "another general problem of scientific and philosophical thought is the relationship between information , energy and matter @xcite .    within this paper we do not aim to contribute to the proper development of a meaning - based information theory , or to resolve the debate above",
    "however , we propose a somewhat broader definition of information , which we feel could be useful for the practical purposes of this paper , and would contribute to the  matter ",
    "information \" debate .",
    "consider a simple example : a sequence of symbols can be written on paper , on the sand , or made of concrete blocks .",
    "regardless of the material used , the message contains exactly the same amount of information .",
    "therefore , it is the _ shape _ that the material object takes , that can be called information . the shape can be certainly understood quite _ broadly _ , not only as a geometrical shape of a material object , but also as its architecture or internal structure .",
    "e.g. the shape of an envelope of high - frequency electromagnetic waves can carry the same information as the sound perceived as mechanical oscillations of an ear membrane",
    ".    * definition .",
    "* information is the _ shape of the matter_.    * learning and shaping . * if learning can be understood as acquiring information , for practical ( e.g. engineering ) purposes we define learning as _ changing the shape _ of the system in response to the external stimulus .",
    "* learning by a dynamical system .",
    "* for the rest of the paper we will stay within the framework of _ dynamical systems theory_. * definition .",
    "* _ learning _ by a dynamical system is the shaping of its _ velocity vector field _ in response to external stimuli and/or internal processes",
    ".    * goal .",
    "* we wish to construct a dynamical system ( [ ds ] ) experiencing a continuous , generally random , external force , and allow this force to systematically _ deform _ the velocity vector field according to a certain rule .",
    "the external influence should accumulate and , despite its random nature , give rise to a _",
    "smooth _ vector field , which could eventually become fully _ deterministic _ and highly organized , and thus give rise to a new behavior of the dynamical system .",
    "importantly , the resulting structure of the vector flow in the system should be determined by the _",
    "statistical _ properties of the random input .",
    "we propose to call such systems _ self - shaping dynamical systems_.    self - shaping systems would be different from the well - known random dynamical systems of the form @xmath6 , in which @xmath7 is a random input and @xmath8 with @xmath9 being the vector field from ( [ ds ] ) @xcite . in the latter systems the random input only _ perturbs _ the existing vector field , while in the self - shaping systems the vector field will be _ created _ by the random input .",
    "in this paper we concentrate on the simplest form of the self - shaping systems , the so - called gradient ( or potential ) systems , in which the vector field @xmath5 is the gradient of a certain energy function @xmath10 , @xmath11 where @xmath12 represents the location in @xmath13-dimensional space .",
    "the state point in such a system behaves just like a massless particle that is placed into a potential energy landscape @xmath14 , which moves towards the relevant local minimum . here",
    ", we assume that the energy @xmath10 is also a function of time @xmath1 , to take into account the continuous shaping process .",
    "illustration of the idea of the flexible energy landscape as a memory foam . for a one - dimensional  foam \" stretched in the @xmath12 direction ,",
    "assume that initially it is flat , i.e. its landscape is described as @xmath15@xmath16@xmath17 ( see @xmath1@xmath16@xmath17 ) .",
    "if a stone drops onto the foam at position @xmath12@xmath16@xmath18 , the landscape is deformed : a dent appears , which is the deepest exactly at @xmath12@xmath16@xmath19 , and gets shallower at larger distances from @xmath18 ( see @xmath1@xmath16@xmath20 ) . in other words ,",
    "the foam will learn about the occurrence of the stone and of its position .",
    ", scaledwidth=40.0% ]    below we derive an equation describing the shaping of the energy @xmath10 in response to the random stimulus .",
    "it is helpful to employ a loose analogy with the  memory foam \" used in orthopedic mattresses .",
    "this foam takes the shape of a body pressed against it , but slowly returns to its original shape after the pressure is removed .",
    "it helps to use the auxiliary function @xmath21 describing the foam landscape , as illustrated by fig .",
    "[ fig_foam_ill ] .",
    "also , assume that the foam is elastic with elasticity factor @xmath22 that models the capacity of the system to forget . here",
    ", we make a simplified assumption that the deeper the dent at the position @xmath12 is , the faster the foam tries to come back to @xmath23@xmath16@xmath17 .",
    "however , the forgetting term can be modelled in a variety of ways , depending on what the situation requires .",
    "now assume that we subject the foam to a continually varying external stimulus @xmath24 , as if at any new time moment @xmath1 a new stone drops at a new position @xmath12@xmath16@xmath25 ( fig .",
    "[ fig_foam_ill ] , @xmath1@xmath16@xmath26 ) .",
    "thus the  foam \" will undergo a continuous shaping process .",
    "the signal @xmath24 can be of either deterministic , or stochastic nature , and can have arbitrary statistical properties .",
    "consider how the foam landscape changes over a small , but finite time interval @xmath27 : @xmath28 where @xmath29 is some non - negative bell - shaped function , describing the shape of a single dent , e.g. a gaussian function , @xmath30    in ( [ eq1 ] ) move @xmath31 to the left - hand side , divide both parts by @xmath27 , and take the limit as @xmath32 , to obtain @xmath33 it can be shown by numerical simulation with some arbitrary @xmath24 , that the solution @xmath31 has a linear trend , i.e. it behaves as a linearly decaying function of @xmath1 with superimposed fluctuations .",
    "we wish to eliminate this trend and see if we can achieve some sort of stationary behavior of @xmath31 .",
    "perform the change of variables @xmath34 and rewrite ( [ eq2 ] ) as follows @xmath35 within this model , the energy landscape and the vector field of eq .",
    "( [ eq_particle ] ) progressively smooth out and stabilize , as illustrated in fig .",
    "2 , if @xmath25 is a stationary and ergodic process .",
    "* proof of shaping into the input density .",
    "* next , we prove that under certain conditions listed below , the energy landscape @xmath10 of ( [ eq_particle ] ) automatically shapes into the negative of the probability density distribution of the input random process .",
    "consider the evolution of @xmath36 , where the @xmath13-dimensional input vector @xmath25 is a realization of a strict - sense _ stationary _ and _ ergodic _ random process @xmath37 with some arbitrary probability density distribution ( pdd ) @xmath38 .",
    "due to _ stationarity _",
    ", @xmath39 does not change in time ; due to _ ergodicity _ , any single realization @xmath25 contains all information about @xmath39 , i.e. any statistical characteristic can be obtained from @xmath25 by averaging over time , rather than over the ensemble of realizations that would have been required for a non - ergodic process @xcite .",
    "below we will show that with time , @xmath10 takes the shape of @xmath39 .",
    "assume that @xmath40 , i.e. that the system ( [ eq3 ] ) does not forget what it learnt . multiply both parts of eq .",
    "( [ eq3 ] ) by @xmath41 and integrate .",
    "a stationary behavior of @xmath10 implies @xmath42 consider the integral of the right - hand side of eq .",
    "( [ eq3 ] ) and its limit as @xmath43 @xmath44 representing the ( negative of the ) time average @xmath45 of the expression under the integral .",
    "the term @xmath46 is a non - linear smooth function of an ergodic process @xmath47 . as proved in @xcite ,",
    " zero - memory nonlinear operations on ergodic processes are ergodic \"  therefore , @xmath46 is also an ergodic random process .",
    "thus we can replace the time average ( [ eq_time ] ) by the statistical average , @xmath48 in the above , the integral with respect to @xmath18 represents , for brevity , @xmath13 integrals with respect to the components @xmath49 of vector @xmath18 . since @xmath10 does not depend on @xmath18 explicitly , the first term in the right - hand side of ( [ eq5 ] ) is equal to @xmath10 .",
    "the second term is the convolution of @xmath50 with the function @xmath51 .",
    "if @xmath52 , where @xmath53 is dirac delta - function of several variables , this term is equal to minus @xmath54 , due to the sifting property of delta - function @xcite . from ( [ eq3 ] ) combined with ( [ eq4 ] ) it follows that the expression ( [ eq5 ] ) is equal to @xmath17 .",
    "we therefore proved that as time @xmath1 goes to infinity , @xmath55 tends to @xmath56 , provided that @xmath29 tends to the dirac delta - function .",
    "* illustration of shaping into the input density . * in fig . [ fig_uncor_cor ]",
    "the evolution of @xmath36 is illustrated , as two kinds of scalar stimuli are applied to the one - dimensional system ( [ eq3 ] ) .",
    "their pdds are of similar two - peak shape ( see solid lines at the front in ( a , c ) ) , but two consecutive values are non - correlated in ( a , b ) , and correlated in ( c , d ) . the stimulus illustrated in fig .",
    "[ fig_uncor_cor ] ( a , b ) is obtained by taking gaussian white noise and applying a non - linear transformation , that changed its pdd .",
    "thus , the pdd took the shape shown in ( a ) by solid line , but the consecutive values remained uncorrelated .",
    "the stimulus in ( c , d ) is obtained by applying gaussian white noise to a differential equation describing a particle moving in a non - symmetric double - well potential with large viscosity @xcite .",
    "the pdd of the output signal has the shape shown in ( c ) by solid line , and the consecutive values are correlated .",
    "evolution of the energy landscape @xmath36 as the random stimulus is applied by numerically simulating eq .",
    "( [ eq3 ] ) : ( a , c ) 3d view ; ( b , d ) projection of @xmath36 onto @xmath57 plane shown by color ( shade of grey ) , and the stimulus applied  by filled circles . in ( a ,",
    "c ) the probability density distribution of stimulus is given by solid line at the front .",
    "in ( a , b ) the consecutive values of the stimulus are uncorrelated , and in ( c , d )  correlated .",
    ", scaledwidth=45.0% ]    the actual signals applied are shown by filled circles in ( b , d ) , and in @xmath29 we used @xmath58@xmath16@xmath59 .",
    "one can see that eventually both energies shape into the respective pdds , but if the stimulus values are uncorrelated , the convergence is faster .",
    "if the random process @xmath37 is not stationary , the energy @xmath10 evolves into a _ time - averaged _ density of the input .",
    "* relevance to kernel density estimation . *",
    "the shaping mechanism which we employed for gradient systems is related to the kernel density estimation used in statistics @xcite .",
    "here , we incorporated this mechanism into the continuous dynamical shaping of the vector field , which is done for the first time to the best of our knowledge . also , the standard assumptions about the kernel density estimators include the statistical _ independence _ of the successive values of the input .",
    "namely , a sequence of input numbers / vectors is regarded as a collection of the values of some random ( scalar or vector ) _ variable _ with a certain pdd .",
    "the convergence to this pdd was proved under these simplifying assumptions only . here , we prove the convergence to the pdd under a more general assumption , that the successive input values are generated by a random _ process _ and can be correlated with each other .",
    "the only requirements used are those of stationarity and ergodicity of this process .",
    "the self - shaping systems are in a sense an extension of a neural network ( nn ) paradigm . in spite of the steadily growing volume of neuroscience research",
    ", it would be too premature to claim that we can confidently explain how exactly biological nns function . however , the most essential features of biological nns seem to be captured by _ artificial _ nns and their mathematical models .",
    "firstly , either biological or artificial nns are made up of a large number of units ( neurons ) , each with a fixed structure .",
    "notably , it is assumed that one can not amend the inner structure of individual units .",
    "secondly , these neurons are coupled together through the synaptic connections . unlike the individual neurons",
    ", the couplings can change in the course of time .",
    "namely , new connections can be formed , the old ones can disappear , and the strengths of all connections can change either spontaneously , like in biological nns , or by a certain pre - defined algorithm , like in artificial nns .",
    "this ability is called synaptic plasticity and is associated with the ability to learn .",
    "below we demonstrate how self - shaping dynamical systems are related to the two types of nns : hopfield and probabilistic ones .",
    "consider a collection of one - dimensional  neurons \" , whose states can be any real numbers .",
    "an example would be a hopfield continuous - time nn that can be written , e.g. as follows @xcite : @xmath60 in the above @xmath61 is the current state of @xmath62th neuron and @xmath63 is the connection strength , or _ weight _",
    ", between the neuron number @xmath62 and the neuron number @xmath64 .",
    "each neuron is essentially a threshold device with the threshold @xmath65 or , in more general terms , a non - linear device , with the non - linearity described by the  sigmoid \" function @xmath66 , e.g. @xmath67 .",
    "this is one of the possible models for artificial nns , and although it does not capture the real firing and spiking transmission processes observed in biological neurons , it provides an approximate mathematical description of the most important ability of a nn  the ability to recognize patterns , or to _",
    "the nn paradigm was a breakthrough in the field of artificial intelligence for the following reason . in conventional computing ,",
    "two objects are regarded as the same only if they are identical .",
    "therefore , to attribute a new pattern to an appropriate class ( to recognize a pattern ) , a computer needs to know _ all _ elements that form the given class .",
    "this is not consistent with our everyday experience , in which living systems can successfully recognize patterns which they have never seen previously .",
    "this fundamental limitation was overcome by nns as described below .",
    "if the function @xmath68 and the thresholds @xmath65 are fixed , the system ( [ nn ] ) can be perceived as a non - linear dissipative dynamical system , whose vector field is determined by the weights @xmath63 .",
    "if the weights are symmetrical , i.e. @xmath63@xmath16@xmath69 , one can introduce an energy function @xmath70 @xcite , such that the right - hand sides of eq .",
    "( [ nn ] ) are the coordinates of the gradient of @xmath70 .",
    "the function @xmath70 would typically have a number of local minima , each being a stable fixed point in the phase space with its own basin of attraction .    * pattern recognition by a hopfield nn with fixed weights . *",
    "each minimum of energy @xmath70 represents the most typical or average representative of a certain class , or class centre .",
    "all patterns that belong to the same class are represented by the phase points in the basin of attraction of the respective stable fixed point .",
    "since there are infinitely many points in the basin , there can be infinitely many patterns that belong to the same class , just like in reality .",
    "e.g. infinitely many projections of a certain flower , registered by a cat looking at it at different angles , are perceived as the same flower .",
    "an input pattern is represented by initial conditions in the phase space , which would fall in one of the basins of attraction available .",
    "then the phase point follows the vector field and moves towards the respective fixed point . when the fixed point is reached , the pattern is deemed recognized .",
    "* learning by a hopfield nn .",
    "* before the nn acquires the ability to classify , it needs to learn .",
    "learning is understood as the adjustment of the weights @xmath63 , and in its turn the shaping of the energy landscape @xmath70 .",
    "there exist a considerable number of algorithms to find the values of @xmath63 , see , e.g. @xcite and references therein . depending on the algorithm , learning in nns",
    "can be supervised , semi - supervised @xcite , reinforced or unsupervised @xcite . in any case , to train a nn , one presents it with a relatively large , but finite , number of example patterns . in _ supervised _ learning , the teacher also tells the nn how to classify each training pattern , i.e. manually attributes it to a certain basin of attraction .",
    "in addition , it specifies the total number of classes and the locations of the class centres , i.e. of fixed points . on",
    "the other extreme , in _ unsupervised _ learning , the nn is trying to figure out all fixed points and their basins on its own , by extracting some statistical information from the training set . _",
    "unsupervised _ learning presents the largest challenge out of all types of learning .",
    "also , typically , a nn first learns and fixes its weights , and then performs recognition .",
    "however , there has been some effort in the direction of _ on - line learning _ , in which a nn would adjust its weights in the process of learning @xcite .    * comparison with hopfield nns . *",
    "if continuous - time hopfield nns could learn in an unsupervised and on - line manner , they would work in the same way as the gradient self - shaping systems .    * advantage over hopfield nns . *",
    "the existing algorithms used for the adjustment of weights in hopfield nns are quite good at developing the attractors ( typically stable fixed points at the minima of the energy function ) and of their basins of attraction , in the right locations .",
    "however , whatever algorithm is used , it is very difficult , if not impossible , to control how the _ whole _ vector field changes in response to the training input .",
    "the largest problem is the occurrence of _ spurious _ minima , which develop by themselves as the weights are adjusted , and do not correspond to any valid classes .",
    "these minima affect pattern recognition , and this problem has still not been resolved after many years of effort .",
    "the desirable energy landscape should possess local minima at the points , where the most probable class representatives appear , and have no other minima .",
    "a function that would perfectly satisfy this condition is a pdd of all possible patterns , taken with a negative sign . and",
    "it is the pdd , that appears to be the energy in gradient self - shaping systems , albeit smoothed by the kernel with a finite width .",
    "thus , unlike hopfield nns , in the gradient self - shaping systems spurious minima do not occur .",
    "the gradient self - shaping systems also have one feature in common with another type of nns , called _",
    "probabilistic neural networks _ @xcite .",
    "the purpose of the latter is to estimate the pdd of the incoming patterns , and then use it for classification purposes .",
    "such nns were developed in the attempt to overcome the spurious minima problem of the hopfield nns .",
    "the paradigm used here is essentially the same as in all nns : there is a collection of units with rigid architecture , and there are flexible / adjustable couplings between them .",
    "however , such nns have a somewhat different architecture as compared to hopfield nns .",
    "namely , in them there is always a separate layer of neurons , such that each neuron codes a separate element of the training set .",
    "thus , in order to take into account a new training pattern , one needs to physically add a new neuron to the system , thus making the whole system larger . in practice",
    "this implies that only a finite number of training patterns can be used , which imposes a considerable restriction on the system s performance . to lift the requirement of  one pattern  one neuron \" , this technique was improved @xcite , but the general idea remained the same : the system needs to be expanded to learn better .",
    "this paradigm in fact accounts for the popular  grandmother neuron \" hypothesis @xcite , which at the early ages of neuroscience suggested that in the brain the memory about a certain object was coded by a special neuron .",
    "e.g. , the memory about one s grandmother has to be coded by the respective single neuron .",
    "this hypothesis contradicts the hopfield nns idea @xcite , that many memories can be coded by the same collection of neurons , as explained above .",
    "* comparison with probabilistic nns .",
    "* gradient self - shaping systems can do the same job as probabilistic nns , i.e. to estimate the probability density distribution of incoming patterns and thus single out separate classes and their most typical representatives  without supervision and on - line .    * advantages over probabilistic nns . * in estimating the pdd ,",
    "the gradient self - shaping systems do not rely on the physical addition of new units in the course of learning , at least within the mathematical paradigm proposed .",
    "they can make use of as many training patterns as needed without any restrictions on their number .",
    "here , we illustrate how a gradient self - shaping system automatically discovers and memorises musical notes and phrases .",
    "a children s song  mary had a little lamb \" was performed with a flute by an amateur musician six times .",
    "the song involves three musical notes ( @xmath71 , @xmath72 and @xmath73 ) , consists of 32 beats and was chosen for its simplicity to illustrate the principle .",
    "the signal was recorded as a wave - file with sampling rate @xmath74khz . in agreement with what is usually done in speech recognition @xcite",
    ", the short - time fourier transform was applied @xcite to the waveform with a sliding window of duration @xmath75@xmath16@xmath76 sec , which was roughly the duration of each note .",
    "the highest spectral peak was extracted for each window , which corresponded to the main frequency @xmath77 hz of the given note .",
    "a sequence of frequencies @xmath78 was used to stimulate the system ( [ eq3 ] ) .",
    "note , that each value of @xmath78 was slightly different from the exact frequency of the respective note , because of the natural variability introduced by a human musician , and the signal @xmath78 was in fact random , as seen from fig .",
    "[ fig_flute_1d](b ) .",
    "( color online . )",
    "musical note recognition .",
    "( a ) evolution of the energy landscape @xmath36 in response to a musical signal performed by an amateur musician .",
    "local minima that develop eventually are very close to the frequencies of the musical notes @xmath79 , @xmath80 and @xmath81 that enter the song .",
    "( b ) filled circles show the actual values of the input , and the shade of the background shows the depth of the energy function .",
    ", scaledwidth=45.0% ]    firstly , we illustrate how individual musical notes can be automatically identified . a one - dimensional system ( [ eq3 ] ) received the signal @xmath25@xmath16@xmath78 , resampled to @xmath74hz to save computation time .",
    "the function @xmath78 can be seen as a realization of a 1st - order stationary and ergodic process @xmath82 , consisting of infinitely many repetitions of the same song , which we observe during finite time .",
    "this process has a one - dimensional pdd @xmath83 , which does not change in time .",
    "a gaussian kernel @xmath29 was used with @xmath58@xmath16@xmath84 hz . as shown in fig .",
    "[ fig_flute_1d](a ) , the energy converges to some pdd ( with negative sign ) shown by the solid line .",
    "it automatically discovers the most probable frequencies as follows , figures in brackets showing the exact frequencies of the respective musical notes : 434hz ( 440hz ) for @xmath80 , 490hz ( 493.88hz ) for @xmath81 , and 388hz ( 392hz ) for @xmath79 .",
    "secondly , we show how the system ( [ eq3 ] ) can discover and memorize temporal _ patterns _  musical phrases consisting of four beats .",
    "the 4d  foam \" was used , and to each of its channels the same signal @xmath78 was applied , but with a phase shift .",
    "namely , at each time @xmath1 the system ( [ eq3 ] ) received a vector stimulus @xmath85@xmath16@xmath86 , @xmath75@xmath16@xmath76 sec .",
    "the procedure of creating a vector with the coordinates made of the delayed versions of the same signal is called delay embedding @xcite . for the purpose of this part",
    ", we can regard @xmath85 as a realization of a 4th - order stationary and ergodic vector random process @xmath87 ( which we observe during finite time ) with @xmath88-dimensional pdd @xmath89 .",
    "we used a multivariate gaussian kernel @xmath90 with @xmath58@xmath16@xmath84 hz in all of its four variables .",
    "( color online . )",
    "musical phrase recognition .",
    "description is in text.,scaledwidth=45.0% ]    one can not visualize evolution of a 4d landscape in the same way as we did in figs .",
    "[ fig_uncor_cor]-[fig_flute_1d ] , and we use an alternative representation . we take four half - axes and make their origins coincide ( fig .",
    "[ fig_flute_4d](a ) ) . for each feasible input",
    "@xmath91@xmath16@xmath92 we put 4 points with coordinates @xmath93 on each of half - axes , and connect them by lines .",
    "thus , any feasible input pattern is represented by a polygon on a plane .",
    "( this can be done for any dimension of input vector . )",
    "the value of @xmath94 at each point can be represented by the color of the respective polygon ( fig .",
    "[ fig_flute_4d](b ) ) .",
    "the polygon , whose color is the darkest , is the most probable pattern .",
    "unfortunately , when too many polygons overlap , it might be difficult to see the darkest ones .",
    "but they can be found using the paradigm of a particle in the 4d landscape , that will go to one of the local minima representing one of the most probable patterns : five such patterns are given in smaller scale in fig .",
    "[ fig_flute_4d](c ) .",
    "recognition of musical phrases is also illustrated by the supplementary audio files @xcite .",
    "we started by proposing to treat information broadly as the shape of the matter , and the process of acquiring information , i.e. learning , as shaping of the matter in general .",
    "staying within the dynamical systems framework , we introduced a mathematical concept of a self - shaping dynamical system , which exploits these definitions of information and learning .",
    "we showed how such systems perform unsupervised learning and compare this mechanism with the one in the neural networks .",
    "the self - shaping systems shape their velocity vector fields automatically under the influence of the external random stimulus .",
    "the resulting properties of the vector field , and consequently of the vector flow , are dictated by the statistical properties of the stimulus applied .",
    "we demonstrated how the simplest self - shaping systems of a gradient type develop the fixed point attractors together with their basins of attraction .",
    "we proved that for a stationary and ergodic input random process , the energy of such gradient systems converges to a smoothed probability density distribution of the input signal .",
    "the relevance of the new type of dynamical systems to the neural networks of two types is discussed .",
    "it is argued that the gradient self - shaping systems could serve the same purpose as neural networks , but would be lacking their limitations .",
    "the performance of a gradient self - shaping system is illustrated with an example in the form of a musical pattern .",
    "namely , it is shown how the system automatically discovers separate musical notes and musical phrases .",
    "self - shaping systems of a gradient type , that were considered here , present only the simplest form of such systems .",
    "we predict that it will be possible to construct self - shaping systems that develop more complex attractors , such as limit cycles and chaotic attractors . obviously , they would not be of a gradient type .",
    "finding the general mechanisms of their formation will be the subject of our future work .",
    "what we present here is a mathematical proposal for the systems of a new class .",
    "we argue that , if implemented in hardware , such systems would have considerable advantages over neural networks .",
    "however , the physical principles upon which such systems could be built are not obvious at the moment .",
    "therefore , this proposal represents an engineering challenge and calls for the development of the devices of a new kind .",
    "* self - organization and self - shaping . * a very important property of non - linear systems ,",
    "both natural and man - made , is their ability to self - organize .",
    "some famous examples are benard cells @xcite that automatically form in a heated liquid , and belousov - zhabotinsky chemical reaction @xcite , in which the liquid spontaneously changes colour . in terms of dynamical systems , self - organization has been traditionally understood as automatic shaping of the _ solutions _ that start from a range of initial conditions , given the fixed structure of the vector field and/or of its perturbations .",
    "we now wish to extend the self - organization principle to the automatic shaping of the vector field itself .",
    "it most vividly manifests itself in living systems , that continuously change themselves in response to external influence .",
    "therefore , the suggested self - shaping approach might prove a helpful paradigm when modelling adaptation and development in living systems in general .",
    "the authors are grateful to alexander  balanov for thorough reading and helpful critical comments on all drafts of this paper , to mark robbins , victoria marsh and scott dickson for their feedback on the paper , and to victoria  marsh for playing the flute .",
    "szilard l ( 1929 ) uber die entropieverminderung in einem thermodynamischen system bei eingriffen intelligenter wesen . _ zeitschrift fur physik _ 53 : 840 - 856 .",
    "english translation : szilard l. ( 1964 ) on the decrease in entropy in a thermodynamic system by the intervention of intelligent beings .",
    "_ behavioral science _",
    "9(4 ) : 301 - 310 ."
  ],
  "abstract_text": [
    "<S> we associate learning and adaptation in living systems with the shaping of the velocity vector field in the respective dynamical systems in response to external , generally random , stimuli . with this </S>",
    "<S> , a mathematical concept of self - shaping dynamical systems is proposed . </S>",
    "<S> initially there is a zero vector field and an  empty \" phase space with no attractors or other non - trivial objects . </S>",
    "<S> as the random stimulus begins , the vector field deforms and eventually becomes smooth and deterministic , despite the random nature of the applied force , while the phase space develops various geometrical objects . </S>",
    "<S> we consider gradient self - shaping systems , whose vector field is the gradient of some energy function , which under certain conditions develops into the multi - dimensional probability density distribution ( pdd ) of the input . </S>",
    "<S> self - shaping systems are relevant to neural networks ( nns ) of two types : hopfield , and probabilistic . </S>",
    "<S> firstly , we show that they can potentially perform pattern recognition tasks traditionally delegated to hopfield nns , but without supervision and on - line , and without developing spurious minima of the energy . secondly , like probabilistic nns </S>",
    "<S> , they can reconstruct the pdd of input signals , without the limitation that new training patterns have to enter as new hardware units . </S>",
    "<S> thus , self - shaping systems can be regarded as a generalization of the nn concept , achieved by abandoning the  rigid units \" -  flexible couplings \" paradigm and making the vector field fully flexible and amenable to external force . </S>",
    "<S> the new concept presents an engineering challenge requiring new principles of hardware design . </S>",
    "<S> it might also become an alternative paradigm for modeling of living and learning systems . </S>"
  ]
}