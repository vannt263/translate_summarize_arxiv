{
  "article_text": [
    "during the last period of years research on bayesian model selection has largely focused on the choice of suitable and meaningful priors for the parameters of competing models .",
    "in particular , a large body of literature has been devoted to the development of default priors for `` objective '' bayesian variable selection methods in regression problems , a justifiable tendency on the ground that typically there is a lack of subjective knowledge about the regression coefficients . in such cases",
    "one must rely on objective variable selection procedures which will circumvent : i ) the use of proper but diffuse priors which result in posterior model odds that are highly sensitive to the prior variances owing to the jeffreys - lindley - bartlett paradox ( bartlett , 1957 ; lindley , 1957 ) and ii ) the use of improper non - informative priors which yield indeterminate posterior odds due to the fact that the unknown normalizing constants do not cancel out in the calculation of bayes factors .",
    "objective priors are widely established within the framework of bayesian variable selection in normal linear regression . a key contribution within this field is the @xmath0-prior of zellner ( 1986 ) which provided the basis for a series of notable studies related to the specification of the hyper - parameter @xmath0 ( see e.g. fernndez et al . , 2001 ) and to the hyper-@xmath0 prior extensions developed by zellner and siow ( 1980 ) and more recently by liang et al .",
    "( 2008 ) , maruyama and george ( 2011 ) and bayarri et al .",
    "( 2012 ) . a comparison of hyper-@xmath0 priors focusing on econometric applications can be found in ley and steel ( 2012 ) . in parallel ,",
    "several authors developed general objective procedures , not necessarily restricted to regression problems , that lead to well defined bayes factors under improper priors .",
    "ohagan ( 1995 ) introduced the fractional bayes factor approach based on training samples , while berger and perricchi ( 1996 ) developed the intrinsic bayes factor and the intrinsic prior based on the concept of `` imaginary '' data and on the use of `` minimal '' training samples .",
    "prez and berger ( 2002 ) introduced the expected - posterior prior which is a generalization of the intrinsic prior .",
    "stochastic search algorithms for variable selection in normal linear regression using the intrinsic prior and the expected - posterior prior can be found in casella and moreno ( 2006 ) and fouskakis and ntzoufras ( 2013 ) , respectively .",
    "consistency properties for the posterior distribution under the intrinsic prior and connections to mixtures of @xmath0-priors are investigated in detail in womack et al .",
    "( 2014 ) .",
    "a key issue in the procedure of forming sensible and compatible prior distributions is the concept of underlying `` imaginary '' data .",
    "this concept is directly related to the power prior approach ( ibrahim and chen , 2000 ) initially developed for historical data .",
    "for instance , zellner s @xmath0-prior can be expressed as a power prior with a fixed set of imaginary data ( ibrahim and chen , 2000 ; zellner , 1986 ) and , similarly , that is the case for any mixture of @xmath0-prior , with the difference that additional uncertainty is introduced to the volume of information that the imaginary data contribute . in addition , the mechanism of imaginary data forms the basis of the intrinsic prior ( berger and perricchi , 1996 ) and the expected - posterior prior ( prez and berger , 2002 ) .",
    "recently , the power prior and the expected - posterior prior approaches were combined and used for applications of variable selection in normal linear regression .",
    "fouskakis et al .",
    "( 2015 ) introduced the power - expected - posterior ( pep ) prior by applying jointly the power prior and the expected - posterior prior to the regression parameters and the error variance . in fouskakis and ntzoufras ( 2015 )",
    "the power - conditional - expected - posterior ( pcep ) prior is developed by combining the two approaches for the regression parameters conditional on the error variance .",
    "extending the use of objective priors to the family of generalized linear models ( glms ) is the topic of several rigorous studies which aim to overcome analytical and computational problems that arise in non - normal glms owing to the absence of conjugate relationships .",
    "chen and ibrahim ( 2003 ) introduced a class of conjugate priors based on an initial prior prediction of the data ( similar to the concept of imaginary data ) associated with a scalar precision parameter .",
    "this approach essentially leads to a glm @xmath0/hyper-@xmath0 prior analogue where the precision parameter has the role of @xmath0 .",
    "however , except of the case of the normal linear regression model the prior of chen and ibrahim ( 2003 ) is not known in closed form . for non - normal glms ,",
    "chen et al . (",
    "2008 ) propose a markov chain monte carlo ( mcmc ) sampling - based approach as a general solution .",
    "ntzoufras et al .",
    "( 2003 ) considered a unit - information @xmath0-prior ( kass and wasserman , 1995 ) for variable selection and link determination in binomial models through reversible - jump mcmc sampling .",
    "bov and held ( 2011 ) applied the conjugate prior of chen and ibrahim ( 2003 ) to the regression coefficients conditional on the intercept term and used an integrated laplace approximation for the calculation of the marginal likelihood and a metropolis - hastings algorithm for posterior sampling .",
    "several alternative glm variations of the @xmath0-prior have been proposed , where typically model selection is based on laplace approximations and posterior inference on mcmc sampling ; see for instance hansen and yu ( 2003 ) , wang and george ( 2007 ) and marin and robert ( 2007 ) .",
    "recently , li and clyde ( 2015 ) introduced the confluent hypergeometric hyper-@xmath0 prior for glms using a laplace approximation for the evaluation of bayes factors .",
    "finally , mcmc algorithms for objective bayesian model selection in probit regression models based on the intrinsic prior can be found in leon - novelo et al .",
    "( 2012 ) .    in this paper",
    "we consider extensions of the pcep prior of fouskakis and ntzoufras ( 2015 ) for glm applications .",
    "the structure of the paper is as follows . in section [ sec2 ]",
    "we define the pcep prior within the glm framework explaining the relationship to the power prior and the expected - posterior prior , its interpretation and computational advantages , and also discuss feasible mcmc algorithms for non - normal glms . in section [ sec3 ]",
    "we elaborate more on computational issues that arise in non - normal glms and present two variations of the pcep prior which facilitate calculations .",
    "the theoretical properties of the two pcep variants are examined in the conjugate case of the normal linear regression model . in section [ sec4 ]",
    "we consider hyper prior extensions related to the pcep power - parameter @xmath1 .",
    "the proposed methods are evaluated and compared to other commonly used approaches under various simulation examples and one real data example in section [ sec5 ] .",
    "a discussion is provided in section 6 .",
    "we consider glms where we have @xmath2 observations of a dependent variable @xmath3 , which follows an exponential family distribution , and a set of potential predictor variables @xmath4 .",
    "the variable selection problem involves selecting an optimal subset from the @xmath5 available predictors .",
    "let @xmath6 index all @xmath7 subsets of predictors , serving as a model indicator , and let @xmath8 denote the size of the @xmath9th subset .",
    "then , the sampling distribution of the response vector @xmath10 under a specific model @xmath11 is given by @xmath12 the functions @xmath13 , @xmath14 and @xmath15 determine the particular distribution of the exponential family .",
    "the parameter @xmath16 is the canonical parameter which regulates the location of the distribution through the relationship @xmath17 , where @xmath18 is the link function connecting the mean of the response @xmath19 with the linear predictor @xmath20 and @xmath21 is the inverse function of @xmath22 .",
    "commonly , a canonical @xmath23 function is used , so that @xmath24 .",
    "we assume that an intercept term is included in all @xmath7 models under consideration , so @xmath25 is the @xmath26 vector of regression coefficients , where @xmath27 , and @xmath28 is the @xmath29th row of the @xmath30 design matrix @xmath31 with a vector of 1 s in the first column and the @xmath9th subset of the @xmath32 s in the remaining @xmath8 columns .",
    "the parameter @xmath33 controls the dispersion and the function @xmath34 is typically of the form @xmath35 , where the @xmath36 is a known fixed weight that may either vary or remain constant per observation .",
    "in addition , the nuisance parameter @xmath37 is commonly considered as a common parameter across models , therefore we assume throughout that @xmath38 without loss of generality .",
    "given the above formulation , we have that @xmath39 and @xmath40 .",
    "the pcep prior of fouskakis and ntzoufras ( 2015 ) , initially developed for variable selection in normal linear regression , combines ideas from the power prior approach of ibrahim and chen ( 2000 ) and the expected  posterior prior proposed in prez and berger ( 2002 ) .",
    "we start this section by introducing the concept of imaginary data , the power prior and the expected - posterior prior , and then proceed with the formulation of the pcep prior for glms .      in order to understand the pcep approach ,",
    "let us initially consider applying the power prior of ibrahim and chen ( 2000 ) to a set of imaginary data of size @xmath41 , denoted by @xmath42 . then , for any model @xmath43 with sampling distribution @xmath44 as defined in and a structured `` baseline '' non - informative prior of the form @xmath45 , we can obtain a flexible `` baseline - posterior '' prior from @xmath46 the reasoning in introducing the power - parameter @xmath1 in the likelihood function of @xmath25 and @xmath47 is to allow control of the weight that the imaginary data contribute to the `` final '' posterior distributions of @xmath25 and @xmath47 .",
    "thinking of the two extreme cases @xmath48 and @xmath49 is informative .",
    "when @xmath48 all @xmath50 data points contribute , so that the prior in is exactly equal to the posterior distribution of @xmath25 and @xmath47 after having observed the imaginary data @xmath51 .",
    "contrary , when @xmath52 the contribution of the imaginary data to the overall posterior is equal to one data point , leading to a minimally - informative prior with a unit - information interpretation ( kass and wasserman , 1995 ) .    under the power prior approach in the prior of @xmath25 conditional on parameters @xmath47 , @xmath1 and the imaginary data @xmath51 is @xmath53 if we assume a reference baseline prior for @xmath25 , i.e. @xmath54 , then , based on standard bayesian asymptotic theory ( bernando and smith , 2000 ) , for @xmath55 the distribution in converges to @xmath56 where @xmath57 is the mle of @xmath25 for data @xmath51 and design matrix @xmath58 , @xmath59 , with @xmath60^{-1 } $ ] and @xmath61 , and @xmath62 denotes the density of the @xmath63-dimensional normal distribution with mean @xmath64 and variance - covariance matrix @xmath65 .",
    "it is straightforward to see that the asymptotic distribution in has a @xmath0-prior form ; recall that the extension of zellner s @xmath0-prior for glms , according to the definition in ntzoufras et al .",
    "( 2003 ) and bov and held ( 2011 ) is of the form @xmath66 thus , the zellner s @xmath0-prior can be interpreted as a power prior on imaginary data with @xmath1 having the role of @xmath0 and @xmath67 , @xmath68 .",
    "the familiar zero - mean representation in , i.e. @xmath69 , arises when all imaginary data in are the same , i.e. @xmath70 , and @xmath71 , where @xmath72 is a vector of ones of size @xmath50 and @xmath36 is a known fixed weight ; for details see ntzoufras et al .",
    "( 2003 ) and bov and held ( 2011 ) .",
    "the second key approach in the construction of the pcep prior is the expected - posterior ( ep ) prior introduced by prez and berger ( 2002 ) .",
    "the ep prior is defined as the posterior distribution of the parameter vector of model @xmath11 , given imaginary data @xmath51 , averaged over all possible `` minimal samples '' of @xmath51 coming from the prior predictive distribution @xmath73 of a suitable `` reference '' model @xmath74 .",
    "typically , the reference model is considered to be the simplest model ; see prez and berger ( 2002 ) for more details , as well as berger and perricchi ( 1996 , 2004 ) for information about minimal training samples and the definition of the reference model under the intrinsic prior approach .",
    "focusing on the glm context , we define the conditional - ep ( cep ) prior of @xmath25 and @xmath47 as @xmath75 , where @xmath76 is the baseline prior for @xmath47 and @xmath77 the prior in corresponds to the ep prior of prez and berger ( 2002 ) for @xmath25 conditional on @xmath47 ; the components of this prior are given by @xmath78 and @xmath79 where @xmath80 and @xmath81 are the likelihood and baseline prior , respectively , of the reference model @xmath74 which in our context is the null model containing only the intercept and the dispersion parameter .",
    "applications of the ep prior for variable selection in normal regression can be found in fouskakis and ntzoufras ( 2013 ) .      before proceeding to the definition of the pcep",
    "prior it is useful to highlight the common characteristics and also the differences between the power prior and the ep prior presented in sections [ pp ] and [ ep ] , respectively . in specific ; i ) both approaches use a `` baseline posterior '' prior conditioned upon @xmath51 , with the difference that the likelihood of the power prior in is raised to the power of @xmath82 while the likelihood of the ep prior in is not , and ii ) the power prior is based on a fixed set of imaginary data @xmath83 , whereas the ep prior averages the `` baseline posterior '' over minimal training samples of random imaginary data coming from @xmath84 .",
    "the pcep prior of fouskakis and ntzoufras ( 2015 ) is a modification of the power - expected - posterior ( pep ) which was developed earlier in fouskakis et al .",
    "( 2015 ) for variable selection in normal regression models .",
    "the difference between the two priors is that the pep prior is applied jointly to the regression parameters and the variance component , while the pcep prior is applied to the regression parameters conditional on the variance .",
    "the main advantage of the pcep prior over the pep prior in gaussian linear regression models is that it leads to a fully conjugate design which simplifies considerably the calculations .",
    "however , it is should be noted that for glms where @xmath85 , like binomial and poisson models , there is no actual distinction between the pep and pcep priors .",
    "here we present the general case , i.e. conditional on @xmath47 .    the pcep prior",
    "is constructed by raising the likelihood used in the ep prior in to the power of @xmath86 ; the prior formulation is as follows @xmath87 where @xmath88 and @xmath89 it is important to note at this point that the sampling distribution in is the _ density - normalized _ power - likelihood , i.e. @xmath90 which is in contrast to the power prior of ibrahim and chen ( 2000 ) presented in , where the power - likelihood is not normalized .",
    "the normalization involved in is crucial to the overall applicability of the pcep prior to non - normal glms , however , for the sake of a clear exposition we defer with this issue until section 3 . the pcep formulation is completed by defining @xmath91 and @xmath92 which correspond to the prior predictive distribution of @xmath51 conditional on @xmath47 and @xmath1 under model @xmath93 and model @xmath74 , respectively .",
    "so far , we have not discussed about the choice of the baseline priors @xmath94 and @xmath95 .",
    "the pcep prior provides flexibility in this aspect of the model formulation in the sense that regardless whether one selects proper or improper baseline priors the pcep prior in will always be proper and also compatible across models .",
    "for instance , for normal linear regression under the pep prior fouskakis et al .",
    "( 2015 ) consider an improper jeffrey s baseline prior for ( @xmath96 ) as well as a zellner @xmath0-prior for @xmath25 combined with an inverse - gamma prior for @xmath97 , while under the pcep prior fouskakis and ntzoufras ( 2015 ) focus on the @xmath0-prior / inverse - gamma prior design .",
    "the same freedom of choice applies to glms in general , as one can adopt flat improper priors or the glm equivalents of jeffrey s prior ( laud and ibrahim , 1991 ) and zellner s @xmath0-prior ( e.g. ntzoufras et al . , 2003 ; sabans bov and held 2011 ) as suitable baseline priors .      as seen in",
    ", the pcep prior is the average of the posterior of @xmath25 given @xmath51 over the prior predictive of the reference model @xmath74 .",
    "it is therefore an objective prior which introduces an extra hierarchical level accounting for the uncertainty in the imaginary data .",
    "notice that if we further take into account the asymptotic result in , the pcep prior can be expressed as @xmath98 where @xmath99 and @xmath100 is a function of @xmath51 . from this perspective",
    "the pcep prior is interpreted as a mixture of @xmath0-priors with a hyper - prior assigned to @xmath51 rather than to the variance multiplicator @xmath0 , which is here substituted by the power - parameter @xmath1 .",
    "interestingly , the pcep prior can also be interpreted as the expected bayes factor of @xmath74 versus @xmath11 with respect to the sampling distribution of @xmath51 times the baseline prior @xmath94 .",
    "this is easy to see by rewriting the pcep prior as @xmath101 ,   \\label{interpret2}\\end{aligned}\\ ] ] where @xmath102 is the bayes of @xmath74 versus @xmath103 conditional on @xmath47 and @xmath1 .    implementing the pcep methodology ,",
    "depends on the selection of the power parameter @xmath1 , the size @xmath50 of the imaginary sample and the choice of the reference model . following the guidelines provided in fouskakis et al .",
    "( 2015 ) , we recommend :    * to set @xmath49 so that the imaginary sample contributes information equal to one data point , leading to a unit - information interpretation ( kass and wasserman , 1995 ) . * to set @xmath104 and consequently @xmath105 ; this way we dispense with the need of selecting and averaging over `` minimal '' training samples as in the intrinsic and expected - posterior prior approaches ( berger and pericchi , 1996 ; prez and berger , 2002 ) ; see fouskakis et al .",
    "( 2015 ) for a detailed discussion . * to use the constant model @xmath74 as the reference model in order to support a - priori the most parsimonious data - generating assumption ; see spiegelhalter et al .",
    "( 2004 ) for a related discussion .",
    "it worth noting that selecting @xmath104 provides significant computational advantages over the ep prior ( prez and berger , 2002 ) in normal regression models .",
    "at the same time , fouskakis et al .",
    "( 2015 ) performed extensive sensitivity analyses for a wide range of values of @xmath50 and showed that the posterior inclusion probabilities are not significantly affected by the size of the imaginary sample .      in normal regression models",
    "the pcep prior is a conjugate normal - inverse gamma distribution which leads to fast and simple computations ( fouskakis and ntzoufras , 2015 ) .",
    "for the rest of glms , however , conjugate designs are not available as the integration involved for deriving the pcep prior is intractable .",
    "nonetheless , we can work with the hierarchical model , i.e. without marginalizing in , and use an mcmc algorithm in order to sample from the joint posterior distribution of @xmath106 and @xmath51 .    specifically , from , and we have the following hierarchical form @xmath107 a further problem that arises in non - normal glms is that the prior predictive distributions @xmath108 and @xmath109 defined in and , respectively , are not known in closed form .",
    "one solution is to use the laplace approximation for both .",
    "alternatively , a more accurate solution is to augment the parameter space further and include the parameter vector @xmath110 of the reference model in the joint posterior , thus avoiding to approximate @xmath109 . under this approach the posterior in can be augmented as @xmath111 which leads to the need of just one laplace approximation for @xmath112 .",
    "sampling from the posterior distributions presented in is possible with standard metropolis - within - gibbs algorithms , i.e. sampling sequentially @xmath113 and @xmath51 from the respective full conditional distribution with metropolis - hastings steps .",
    "the algorithmic complexity is in general simplified when one uses flat baseline priors proportional to some constant .",
    "it is also worth noting that when using a flat baseline prior on @xmath25 , i.e. @xmath114 , one can also approximate the fraction appearing in by @xmath115 as defined in . finally , for commonly used glms , such as logistic or poisson regression models , @xmath85 , which further simplifies the posterior in .    irrespective of the choice of the baseline priors or whether parameter @xmath47 is constant or not , we can use the general expressions in or to sample from the posterior of a single model . for variable selection procedures",
    ", one can utilize gibbs - based stochastic search algorithms ( e.g. george and mcculloch , 1993 ; carlin and chib , 1995 , dellaportas et al .",
    "2003 ) through the inclusion of the binary indicator vector @xmath9 in the joint posterior distribution . at present",
    "we will not go into more detail over this topic .",
    "analytic information about variable selection for logistic and poisson regression under the two glm - adapted variations of the pcep prior ( which are the subject of section [ sec3 ] ) is given in appendix b.",
    "as noted in section [ pcep ] a crucial part of the pcep prior formulation is based on normalizing the power - likelihood of @xmath25 and @xmath47 . for the normal model where @xmath116",
    ", it is easy to show that the normalized power - likelihood presented in is also a normal distribution , namely @xmath117 the same analogy holds for other distributions of the exponential family as well ; for the bernoulli , the exponential and the beta distributions , the normalized power - likelihood is again a bernoulli , an exponential and a beta distribution , respectively .",
    "this property considerably simplifies calculations when using the pcep methodology .",
    "however , this is not the case for all members of the exponential family .",
    "for instance , for the popular binomial and poisson regression models the normalized power - likelihood is a discrete distribution , and although it is feasible to evaluate it per observation , the additional computational burden may render implementation of the pcep prior time - consuming and inefficient . one possible practical solution to this problem",
    "could be the use of an exchange - rate type of algorithm for doubly - intractable distributions ( see e.g. murray et al . , 2006 ) .",
    "however , this would involve an additional computational burden to mcmc algorithms which are already quite demanding .    in this paper",
    ", we pursue a more general orientation by redefining the pcep prior , in order to make it applicable for all glms . specifically , we consider two variations of the pcep prior which can be potentially used for all members of the exponential family . for the remainder of this paper we restrict the scale parameter @xmath118 to be fixed , which is the case for the commonly used glms such as the binomial , poisson and gaussian linear regression models . without loss of generality",
    "we assume @xmath85 .",
    "the variants of the pcep prior for glms are based on the following two considerations :    * to use the unnormalized power - likelihood ( with @xmath49 ) for model @xmath11 but define the prior predictive distribution of model @xmath74 based on the real likelihood ( with @xmath1=1 ) , * to use the unnormalized power - likelihood ( with @xmath49 ) both for model @xmath11 and for the definition of the prior predictive of model @xmath74 .",
    "thus , both approaches under consideration utilize the unnormalized power likelihood for model @xmath11 , leading to a baseline - posterior prior of the form @xmath119 where @xmath120 note that this approach is also used in the power prior of ibrahim and chen ( 2000 ) presented in .",
    "the difference between the two pcep variations lies in the definition of the sampling distribution of @xmath51 . in what follows",
    "we present the prior formulation under each approach and discuss some theoretical aspects for the conjugate case in normal linear regression .      in the first approach",
    "we define the prior predictive of model @xmath74 as @xmath121 assuming that @xmath1 is fixed at 1 for the reference model . because the likelihood in is concentrated ( in relation to the next approach )",
    "we call the resulting prior as _ concentrated - reference _ pcep ( cr - pcep ) .",
    "the cr - pcep prior is given by @xmath122 \\times \\nonumber \\\\ & &   \\times \\pi_0^{{\\mathrm{n}}}(\\beta_0)\\mathrm{d}\\beta_0   \\label{cr - pcep }   \\end{aligned}\\ ] ] note that under the dr - pcep prior the interpretation based on the expected value of a bayes factor , presented in , does not hold anymore owing to the fact that the likelihood functions for models @xmath74 and @xmath11 are not the same .",
    "under the second approach the prior predictive of model @xmath74 is defined as @xmath123 notice that here we use the unnormalized likelihood for the reference model and normalize the prior predictive .",
    "the likelihood in becomes more diffuse as @xmath1 gets larger , therefore we call the resulting prior _ diffuse - reference _ pcep ( dr - pcep ) .",
    "specifically , we have that @xmath124 \\times \\nonumber \\\\ & &   \\times \\pi_0^{{\\mathrm{n}}}(\\beta_0)\\mathrm{d}\\beta_0 ,   \\label{dr - pcep}\\end{aligned}\\ ] ] where the normalizing constant is the denominator in .",
    "contrary to the cr - pcep approach , the dr - pcep prior can be interpreted as the expectation of a `` diffuse - type '' of bayes factor with respect to the ( normalized ) sampling distribution of model @xmath11 .",
    "specifically , the dr - pcep can be expressed as @xmath125 , \\label{interpret3}\\ ] ] where @xmath126 is the normalizing constant of @xmath127 and the expectation of the bayes factor of model @xmath74 versus model @xmath11 is taken with respect to the normalized sampling distribution of @xmath51 under model @xmath11 .",
    "we characterize @xmath128 as a `` diffuse - type '' bayes factor because the prior predictive distributions @xmath129 and @xmath130 defined in and , respectively , both involve unnormalized power - likelihoods .",
    "specifically , @xmath131 see appendix a.1 for details .",
    "in this section we examine the properties of the cr - pcep and dr - pcep priors and compare them to the corresponding properties of the pcep prior in normal linear regression .",
    "we work within the conjugate framework considered in fouskakis and ntzoufras ( 2015 ) ; specifically , we use as baseline priors a zellner s @xmath0-prior for @xmath25 conditional on @xmath97 and a reference prior for @xmath97 , i.e. @xmath132 and @xmath133 , respectively .",
    "in addition we use the recommended values for the hyperparameters , namely @xmath134 , @xmath104 and @xmath135 .",
    "the justification for the latter option is that the contribution of the pcep prior to the posterior , when using a @xmath0-prior as baseline , will be equal to @xmath136 data points . in the following we assume throughout that @xmath104 so that @xmath137 .      before proceeding to the specific properties of the cr and dr - pcep priors",
    ", it is useful for comparison reasons to list the basic results obtained by fouskakis and ntzoufras ( 2015 ) for the pcep prior . specifically , the pcep prior under a @xmath0-baseline prior has the following attributes :    * the pcep prior for @xmath25 and @xmath97 is given by @xmath138 where @xmath139 , @xmath140{\\mathbf{x}_{\\gamma}}\\right)^{-1 } $ ] , + @xmath141 and @xmath142 . *",
    "the pcep posterior of @xmath25 and @xmath97 is @xmath143 where @xmath144 , @xmath145 , @xmath146 , @xmath147 with @xmath148 and @xmath149 denotes the density of the inverse gamma distribution with shape @xmath150 and scale @xmath151 .",
    "* under the pcep prior the marginal likelihood in log scale is given by @xmath152 where @xmath153 is a constant that does not depend on the model structure @xmath11 .",
    "for large @xmath2 can be approximated by @xmath154 thus , the limiting behavior of marginal likelihood under the pcep @xmath0-prior is the same as that of the bic , which is known to be consistent under a minor realistic assumption ( fernndez et al .",
    "2001 ; liang et al . , 2008 ) ; see fouskakis and ntzoufras ( 2015 ) for more information and for a detailed proof of . *",
    "the determinant of the pcep covariance matrix @xmath155 , defined below , is given by @xmath156^{d_{\\gamma}-d_0}g^{d_0}\\big | { \\mathbf{x}_{\\gamma}^{t}}{\\mathbf{x}_{\\gamma}}\\big |^{-1}. \\label{determinant}\\ ] ] for @xmath134 and @xmath135 the variance multiplier appearing in is equal to @xmath157^{d_{\\gamma}-d_0}g^{d_0}=n^{2d_{\\gamma } } \\big [ \\frac{2n+1}{(n+1)^2 } \\big]^{d_{\\gamma}-d_0 } , \\label{pcep_multiplier}\\ ] ] which is greater than @xmath158 , i.e. the corresponding multiplier of zellner s @xmath0-prior for @xmath159 , for any sample size @xmath160 ; see fouskakis and ntzoufras ( 2015 ) for details .",
    "this result shows that the pcep prior is more dispersed in comparison to the @xmath0-prior and , therefore , results in a more parsimonious variable selection procedure .",
    "let us consider initially the form of the baseline - posterior prior of @xmath25 conditional on @xmath51 , @xmath97 and @xmath1 and the baseline prior predictive under model @xmath11 as defined in and , respectively .",
    "it is easy to show that the prior predictive is given by @xmath161 where @xmath162 and @xmath163 , consequently .",
    "the prior predictive is not proper , however , this does not affect the baseline - posterior prior as it turns out that @xmath164 where @xmath57 is the ml estimate from @xmath51 and @xmath139 ; see appendix a.2 for a proof . thus , the conditional baseline - posterior prior for @xmath25 is exactly the same as the one under the pcep prior ; see eq .",
    "( 3 ) in fouskakis and ntzoufras ( 2015 ) .",
    "what significantly differentiates the cr - pcep prior from the initial pcep prior is the definition of the prior predictive distribution of the reference model @xmath74 in . following the reasoning of the proof in appendix 2.1 used for the derivation of it is straightforward to see that @xmath165 where in this case we have @xmath166 and @xmath167 given and it is trivial to show that the cr - pcep prior and posterior have exactly the same form as the pcep prior and posterior given in and , respectively .",
    "the difference is that the cr - pcep prior covariance matrix is equal to @xmath168{\\mathbf{x}_{\\gamma}}\\right)^{-1}\\ ] ] which is different from the pcep prior covariance matrix @xmath155 , presented below , owing to the fact that @xmath169 . as a consequence",
    ", we can not take for granted that the properties ( iii ) and ( iv ) of the pcep prior discussed in section [ sec_pcep ] also hold under the cr - pcep prior . in the following we show that the cr - pcep prior is consistent and also that it is more parsimonious than zellner s @xmath0-prior , but not to the same extent as the pcep prior",
    ". + _ consistency of the cr - pcep prior _",
    "+ similarly to , the log marginal likelihood under the cr - pcep prior is @xmath170 for the first logarithmic term in , following the proof found in the supplementary ( section d , eq .",
    "( d.1 ) and eq .",
    "( d.2 ) ) of fouskakis and ntzoufras ( 2015 ) yields @xmath171 note that the approximation is valid asymptotically as we assume that @xmath134 and @xmath135 , so that @xmath172 . given these values",
    ", we can also approximate the second logarithmic term in as @xmath173 the proof for is presented in appendix a.3 .",
    "hence , the limiting behavior of the log marginal likelihood under the cr - pcep prior can be approximated by @xmath174 for @xmath134 and large @xmath2 .",
    "thus , variable selection , based on the cr - pcep prior with a @xmath0-prior as baseline , is consistent .",
    "+ _ parsimony of the cr - pcep prior _",
    "+ similar to fouskakis and ntzoufras ( 2015 ) we can compare the volume of the cr - pcep prior covariance matrix to the volume of the @xmath0-prior covariance matrix in order to examine the dispersion of each prior distribution .",
    "the determinant of the cr - pcep prior is equal to @xmath175 a detailed proof for this result can be found in appendix a.4 . for @xmath134 , @xmath135 and @xmath176",
    "the variance multiplier appearing in becomes @xmath177^{d_{\\gamma}-d_0 } \\left[\\frac{n^2+n+2}{(n+1)^2}\\right]^{d_0 } \\nonumber \\\\ & = n^{2d_{\\gamma}}\\left[\\frac{n+2}{(n+1)^2}\\right]^{d_{\\gamma } } \\left[\\frac{n^2+n+2}{n+2}\\right]^{d_0}. \\label{crmultiplier}\\end{aligned}\\ ] ] comparing the ratio of the variance multiplier in over the variance multiplier of the @xmath0-prior , which is @xmath158 for @xmath159 , we have on log - scale @xmath178^{d_{\\gamma } } \\left[\\frac{n^2+n+2}{n+2}\\right]^{d_0}\\right ) , \\label{crratio}\\end{aligned}\\ ] ] with @xmath179 the function @xmath180 is monotonically increasing but it is not always positive .",
    "for instance , @xmath181 for @xmath182 and @xmath183 .",
    "however , @xmath2 and @xmath184 can not be treated separately , since a minimal restriction is that the sample size must be at least @xmath185 in order to be able to estimate the regression parameters and the error variance .",
    "given this constraint , there is no meaning in evaluating @xmath186 , because @xmath187 allows for only one regressor . for @xmath188 and @xmath189 ,",
    "@xmath180 is positive , and the same holds for any @xmath2 as long as @xmath190 .",
    "thus , the variance of the cr - pcep prior is larger than that of the @xmath0-prior , which means that cr - pcep prior will in general tend to favor less complex models .",
    "however , the difference between the two variances depends strongly on the relative magnitudes of @xmath2 and @xmath184 . if we rewrite the variance multiplier in as @xmath191^{d_{\\gamma } } \\left[\\frac{n^2+n+2}{n+2}\\right]^{d_0 } , \\label{crmultiplier2}\\end{aligned}\\ ] ] then for relatively large @xmath2 the first fraction in tends to one while the second fraction tends to @xmath2 . assuming that @xmath189 , the cr - pcep variance multiplier is then approximately equal to @xmath192 and the ratio in will be @xmath193 . also for increasing @xmath184",
    "the cr - pcep variance multiplier will be @xmath194 .",
    "thus , for reasonably large samples the variance of the cr - pcep prior will approach the variance of the @xmath0-prior as the number of covariates increases ; see section [ numerical ] for numerical illustrations .",
    "the comparison with respect to the initial pcep approach is more straightforward . in this case , the ratio of the cr - pcep variance multiplier over the corresponding multiplier of the pcep prior , given in , is @xmath195^{d_{\\gamma } } \\left[\\frac{n^2+n+2}{n+2}\\right]^{d_0 } \\left[\\frac{2n+1}{(n+1)^2}\\right]^{d_0-d_{\\gamma } }   \\right ) \\nonumber \\\\ & = &   \\log\\left ( \\left[n+2\\right]^{d_{\\gamma}-d_0 } \\left[2n+1\\right]^{d_0-d_{\\gamma } }   \\left[\\frac{n^2+n+2}{(n+1)^2}\\right]^{d_0 } \\right)\\nonumber \\\\ & = & \\log\\left ( \\left[\\frac{n+2}{2n+1}\\right]^{d_{\\gamma}-d_0 } \\left[\\frac{n^2+n+2}{n^2 + 2n+1}\\right]^{d_0 } \\right ) .",
    "\\label{ratiopcep}\\end{aligned}\\ ] ] both fractions appearing in are smaller than or equal to one for @xmath196 and therefore the ratio is always negative .",
    "this means that the cr - pcep prior will likely result in less parsimonious variable selection compared to the pcep prior .",
    "the baseline - posterior prior @xmath197 under the dr - pcep prior is exactly of the same form as the corresponding one under the cr - pcep prior presented in .",
    "what changes now is the definition of the prior predictive of model @xmath74 , given in .",
    "based on in appendix 2.1 and the definition of the dr - pcep prior predictive in , we have that @xmath198 with @xmath199 and @xmath200 notice that the prior predictive in has exactly the same form as the corresponding pcep prior predictive and also the same covariance matrix , i.e. @xmath201 .    from",
    "and we have that @xmath202   \\sigma^{-2 }   \\nonumber\\\\ & \\propto & \\big[\\int f_{\\mathrm{n}_{d_{\\gamma}}}({\\boldsymbol{\\beta}_\\gamma};w\\widehat{{\\boldsymbol{\\beta}_\\gamma}^*},w\\delta({\\mathbf{x}_{\\gamma}^{t}}{\\mathbf{x}_{\\gamma}})^{-1}\\sigma^{2 } ) f_{\\mathrm{n}_{n}}({\\mathbf{y}}^*;\\mathbf 0,{\\mathbf{\\lambda}_{0}^{-1}}\\sigma^{2 } ) \\mathrm{d}{\\mathbf{y}}^*\\big]\\sigma^{-2 } \\nonumber \\\\ & \\propto &   f_{\\mathrm{n}_{d_{\\gamma}}}\\big({\\boldsymbol{\\beta}_\\gamma};\\mathbf{0 } , \\delta\\left ( { \\mathbf{x}_{\\gamma}^{t}}\\left [ w^{-1}{\\mathbf{i}_n}-(\\delta{\\mathbf{\\lambda}_{0}}+w{\\mathbf{h}_{\\gamma}})^{-1 } \\right]{\\mathbf{x}_{\\gamma}}\\right)^{-1}\\sigma^{2}\\big)\\sigma^{-2}.\\nonumber \\\\ \\label{dr_normal}\\end{aligned}\\ ] ] thus , the dr - pcep prior for @xmath25 and @xmath97 conditional on and @xmath1 is exactly the same as the pcep prior in , therefore , the posterior distributions will also match .",
    "hence , the properties ( iii ) and ( iv ) of the pcep prior , discussed in section [ sec_pcep ] , automatically also hold for the dr - pcep prior .",
    "namely , the limiting behavior of the dr - pcep marginal likelihood will approximate that of the bic criterion , since @xmath203 for large @xmath2 and @xmath134 , and the variance multiplier of the dr - pcep prior will be the same as in , resulting in a ratio over the @xmath0-prior variance multiplier given in log scale by @xmath204^{d_{\\gamma}-d_0 } \\big ) , \\label{drratio}\\ ] ] which is positive for all @xmath205 ; see fouskakis and ntzoufras ( 2015 ) .",
    "this means that the dr - pcep prior has a larger variance than the @xmath0-prior , thus favoring more parsimonious models .",
    "in addition , based on the result in it also has a larger variance than the cr - pcep prior .",
    "here we provide some basic illustrations that highlight the behavior of the variance multipliers of the cr - pcep and dr - pcep priors for varying sample size and number of predictors , given the restriction that @xmath206 and assuming that @xmath189 .",
    "figure [ ratios ] presents plots , under fixed values of @xmath184 and varying @xmath2 , of the ratios of the cr - pcep and dr - pcep variance multipliers over the corresponding @xmath0-prior variance multiplier as presented in and , respectively .",
    "as seen both ratios are positive , but as expected the ratio under the dr - pcep approach is always greater than the corresponding ratio based on the cr - pcep prior . in addition , the ratio of the dr - pcep prior over the @xmath0-prior is becoming larger for increasing values of @xmath184 , while the corresponding ratio of the cr - pcep prior is not affected by @xmath184 as it becomes equal to @xmath207 for relatively large samples .",
    "-prior for @xmath208 and varying sample size ; the black circles correspond to the approximation @xmath207 . ]    in figure [ multipliers1_plot ] the variance multipliers of the cr - pcep prior , the dr - pcep prior and the @xmath0-prior are plotted on log - scale for fixed values of @xmath184 and increasing sample size @xmath2 .",
    "the plots of figure [ multipliers1_plot ] clearly illustrate that the variance of the cr - pcep prior is always in between the range defined by the variance of the @xmath0-prior and the variance of the dr - pcep prior .",
    "in addition as @xmath184 becomes larger the cr - pcep variance gets closer to the variance of the @xmath0-prior .",
    "this feature of the cr - pcep prior is also illustrated in figure [ multipliers2_plot ] , where we examine how the magnitude of the variance multipliers changes when sample size is fixed and the number of predictor variables is increased .",
    "priors for @xmath208 and varying sample size . ]     priors for an increasing number of predictors @xmath184 and fixed sample size @xmath2 . ]",
    "similarly to the pcep posterior sampling approach discussed in section [ 2.4 ] we can augment the parameter space by including the parameter vector of the reference model @xmath74 and utilize the general hierarchical model form in order to sample jointly @xmath209 and @xmath51 .",
    "the posterior distributions under the two variations of the pcep are very similar to the one presented in as the only change is essentially the definition of the likelihood function for model @xmath11 and model @xmath74 . specifically , the cr - pcep posterior is @xmath210 while the dr - pcep posterior is given by @xmath211 where the prior predictive of model @xmath11 appearing in the denominator of and of is given in .",
    "based on the laplace approximation this quantity can be evaluated as @xmath212 where @xmath213 is the posterior mode and @xmath214 is minus the inverse hessian matrix evaluated at @xmath213 , i.e. @xmath215 .    as discussed previously",
    ", one can implement stochastic search algorithms based on gibbs sampling to perform variable selection given the cr - pcep and dr - pcep posterior distributions .",
    "we specifically utilize the gibbs variable selection ( gvs ) algorithm of dellaportas et al .",
    "( 2002 ) for this task , although one can also use other stochastic search approaches .",
    "detailed gvs algorithms under cr - pcep and dr - pcep for variable selection in logistic and poisson regression models are provided in appendix b.      in this section we proposed two alternative versions of the pcep prior which can be used for variable selection in glms .",
    "we presented some of the properties of the cr - pcep and dr - pcep prior for the normal linear regression model assuming a zellner s @xmath0-prior as baseline prior for the regression parameters and a reference baseline prior for the variance parameter , based on the recommended hyper - parameter values suggested in fouskakis et al .",
    "the cr - pcep prior was found to be a consistent prior for model selection but less parsimonious than the pcep prior . on the other hand ,",
    "the dr - pcep prior matches exactly the initial pcep prior which means that it is equally parsimonious and that the marginal likelihood under dr - pcep is consistent .",
    "it should be noted , however , that the theoretical results presented here are initial exploratory results that do not necessarily hold under different baseline priors and/or for non - normal glms .",
    "as discussed in section [ 2.3 ] the pcep prior can be interpreted as a mixture of @xmath0-priors where the mixture distribution is the prior predictive of the reference model and parameter @xmath1 is equivalent to parameter @xmath0 .",
    "this means that in comparison to the hyper-@xmath0 prior design ( liang et al.,2008 ) , we assign a hyper - prior to the imaginary data @xmath51 , instead of assigning a hyper - prior to @xmath216 .",
    "given this consideration , it is easy to think of pcep extensions that include an extra hierarchical level by adding a hyper - prior for @xmath1 as well . under this setting",
    "we can define the pcep hyper-@xmath1 prior as @xmath217 where @xmath57 is the mle given the imaginary data .",
    "note that without loss of generality and for ease of exposition in we use the normal approximation given in for the baseline - posterior prior @xmath218 .",
    "sensible options for @xmath219 are the hyper-@xmath0 equivalents proposed in liang et al .",
    "specifically we can use a hyper-@xmath1 prior of the form @xmath220 which corresponds to a beta@xmath221 for the shrinkage factor @xmath222 . thinking in terms of shrinkage , liang et al .",
    "( 2008 ) propose setting @xmath223 in order to place most of the probability mass near 1 or @xmath224 which leads to a uniform prior .",
    "the option @xmath225 results in the reference prior and jeffreys prior .",
    "another alternative option would be a hyper-@xmath226 prior of the form @xmath227    incorporating the above hyper - priors to the cr - pcep and dr - pcep approaches is similar in manner , therefore , the related details will not be discussed further here .",
    "in addition , other priors from the related hyper-@xmath0 literature could also be incorporated in the pcep design ; for instance the hyper prior of zellner and siow ( 1980 ) or the more recent hyper-@xmath0 priors proposed by maruyama and george ( 2011 ) and bayarri et al .",
    "of course , it should be noted that when working outside the context of the normal regression model the integration in with respect to @xmath1 will not be tractable .",
    "this means that one must additionally sample from the posterior of @xmath1 based on the extended hierarchical model that includes @xmath1 .",
    "the cr - pcep and dr - pcep variable selection algorithms , utilized in this study , also include the option of letting @xmath1 be random ; see appendix b for analytic information .",
    "a subtle point in our case is that @xmath1 is not directly related to the real data @xmath228 , which implies that @xmath1 has no support from @xmath228 .",
    "however , a counterargument is that the posterior of @xmath1 is dependent on the parameters of models @xmath11 and @xmath74 which are in turn influenced by @xmath228 , thus the posterior of the power - parameter is supported by the real data indirectly .",
    "another arguable point is that when @xmath1 is random instead of being fixed at @xmath49 , then the pcep prior , and its variations considered here , are not anymore unit - information priors as discussed in section [ 2.3 ] . this can be considered as a downside , in terms of interpretation , yet this also the case for zellner s @xmath0-prior under various approaches that set @xmath229 ( see e.g. fernndez et al . , 2000 ) and also under the hyper-@xmath0 priors proposed in liang et al .",
    "in this section we present two simulation studies concerning logistic regression , one simulation for poisson regression and one real data example for binomial responses . in the results that follow we consider using the initial approach of fouskakis et al .",
    "( 2015 ) with @xmath230 as well as the hyper-@xmath1 and hyper-@xmath226 extensions discussed in [ sec4 ] .",
    "the models under consideration do not involve a nuisance parameter so essentially we work with the pep variations .",
    "jeffrey s prior for glms ( ibrahim and laud , 1991 ) is used as the baseline prior for @xmath25 , namely @xmath231 , where @xmath232 is the diagonal matrix containing the glm weights .",
    "in addition the columns of the design matrix @xmath233 are centered to their corresponding sample mean .",
    "comparisons are presented with respect to the @xmath0-prior and to several commonly used hyper-@xmath0 prior approaches .",
    "here we consider two simulation scenarios for logistic and poisson regression , presented in hansen and yu ( 2003 ) and chen et al .",
    "( 2008 ) , respectively .",
    "both of these scenarios are also considered in li and clyde ( 2015 ) . in the logistic simulation",
    "the number of predictors is @xmath234 , whereas in the poisson simulation we have @xmath235 predictors .",
    "each predictor is drawn from a standard normal distribution with pairwise correlation given by @xmath236 two cases are examined : ( i ) independent predictors ( @xmath237 ) and ( ii ) correlated predictors ( @xmath238 ) .",
    "in addition , four scenarios of different sparsity are considered ; the true data - generating models are summarized in table 1 . for the logistic case",
    "we assume the same sample size as in hansen and yu ( 2003 ) , namely @xmath239 , but set the regression coefficients in way that the odds ratios are much lower than the corresponding ones considered in hansen and yu ( 2003 ) , in order to reduce the signal from the generated data . given the coefficients in table [ simulation1 ] ,",
    "the odds ratios are approximately 2 , 2.5 and 3.5 for the sparse , medium and full models , respectively .",
    "for the poisson simulation we use the same regression coefficients as chen et al .",
    "( 2008 ) , but reduce the sample size to one fifth , i.e. @xmath239 .",
    "each simulation is repeated 100 times .",
    ".four simulation scenarios for logistic and poisson regression assuming independent and correlated predictors . [ cols=\"<,^,^,^,^,^,^,^,^,^,^ \" , ]     as before , we consider the various pep priors and compare them with the @xmath0-prior , the hyper-@xmath0 and hyper-@xmath240 priors ( with @xmath223 ) and the mg hyper-@xmath0 prior .",
    "we assign a beta - binomial mixture prior on model space for an appropriate multiplicity adjustment ; see .",
    "for each prior we used 41000 iterations of gvs - mcmc sampling and discarded the first 1000 as the burn - in period .",
    "table [ pima_inclusion ] shows the resulting posterior inclusion probabilities of each covariate under the various methods . in table",
    "[ pima_inclusion ] we also include the corresponding results presented in bov and held ( 2011 ) who consider the bic criterion , the hyper-@xmath0 of zellner and siow ( 1980 ) , i.e. @xmath241 , the hyper-@xmath240 of liang et al . ( 2008 ) with @xmath224 , and a non - informative hyper-@xmath0 of the form @xmath242 .",
    "henceforth , zellner and siow prior and the non - informative inverse - gamma prior are denoted as zs - ig and ni - ig , respectively .    as seen in table",
    "[ pima_inclusion ] , there are no significant differences among the prior methods and bic in the estimates of the strongly influential covariates @xmath243 and @xmath244 .",
    "contrary , the posterior marginal inclusion probabilities for the `` uncertain '' covariates @xmath245 and @xmath246 differ substantially .",
    "specifically , the inclusion probabilities for these three variables under the cr - pep and dr - pep priors with fixed @xmath1 and the @xmath0-prior with @xmath159 are very similar to the corresponding ones under bic and are much lower in comparison to the inclusion probabilities obtained through the other methods ( including most of the cr / dr - pep hyper prior extensions ) . thinking in terms of the shrinkage factors @xmath247 and @xmath222 , the results show that the shrinkage is greater under the @xmath0-prior with @xmath159 and the cr / dr - pep priors with @xmath134 , which means that covariates which are either insignificant or near the limit of being included in a model are shrunk faster towards zero .",
    "on the other hand , letting @xmath0 or @xmath1 random leads to higher posterior inclusion probabilities for variables @xmath245 and @xmath246 .",
    "it is worth noting , however , that the dr - pep hyper-@xmath226 prior , compared to all other hyper - prior approaches , results in the smallest posterior inclusion probabilities .",
    "l|ccccccc & & @xmath248 & @xmath249 & @xmath250 & @xmath251 & @xmath252 & @xmath253 & @xmath254 bic@xmath255 & 0.946 & 1.000 & 0.100 & 0.103 & 0.997 & 0.987 & 0.334zs - ig hyper-@xmath0@xmath255 & 0.961 & 1.000 & 0.252 & 0.248 & 0.998 & 0.994 & 0.528ni - ig hyper-@xmath0@xmath255 & 0.968 & 1.000 & 0.353 & 0.346 & 0.998 & 0.996 & 0.629hyper-@xmath240@xmath255 @xmath256 & 0.965 & 1.000 & 0.309 & 0.303 & 0.998 & 0.995 & 0.586@xmath0-prior ( @xmath159 ) & 0.946 & 1.000 & 0.099 & 0.099 & 0.997 & 0.986 & 0.330hyper-@xmath0 ( @xmath223 ) & 0.981 & 1.000 & 0.513 & 0.495 & 0.999 & 0.998 & 0.769 hyper-@xmath240 ( @xmath223 ) & 0.974 & 1.000 & 0.407 & 0.393 & 0.999 & 0.997 & 0.688 mg hyper-@xmath0 & 0.961 & 1.000 & 0.244 & 0.236 & 0.998 & 0.995 & 0.534 cr - pep & 0.948 & 1.000 & 0.100 & 0.104 & 0.998 & 0.987 & 0.339cr - pep hyper-@xmath1 & 0.964 & 1.000 & 0.296 & 0.291 & 0.998 & 0.995 & 0.602cr - pep hyper-@xmath226 & 0.956 & 1.000 & 0.223 & 0.225 & 0.998 & 0.992 & 0.520dr - pep & 0.948 & 1.000 & 0.102 & 0.104 & 0.997 & 0.988 & 0.324dr - pep hyper-@xmath1 & 0.954 & 1.000 & 0.174 & 0.173 & 0.997 & 0.991 & 0.442dr - pep hyper-@xmath226 & 0.951 & 1.000 & 0.125 & 0.120 & 0.998 & 0.987 & 0.346         for the cr - pep and dr - pep hyper-@xmath1 priors based on a posterior sample of 40000 draws . ]     for the cr - pep and dr - pep hyper-@xmath226 priors based on a posterior sample of 40000 draws . ]",
    "in order to have a picture of the variability of the estimates we split the posterior sample into 40 batches of size 1000 and calculated the posterior inclusion probabilities in each batch .",
    "figure [ inclusion_pima ] presents boxplots of the resulting batched estimates",
    ". as seen , assigning a hyper prior to @xmath1 mainly affects the posterior inclusion probabilities of the `` uncertain '' covariates @xmath245 and @xmath246 .",
    "based on figure [ inclusion_pima ] , we can make the following remarks .",
    "first , as expected , letting @xmath1 be random induces greater variability , thus resulting in larger mc errors .",
    "second , the shrinkage effect under the hyper-@xmath226 priors is larger in comparison to that of the hyper-@xmath1 priors , but smaller in comparison to that of the fixed-@xmath1 priors .",
    "third , compared to the cr - pep random-@xmath1 priors , the shrinkage under the dr - pep random-@xmath1 priors is stronger .",
    "the results imply that assigning a hyper prior on @xmath1 can potentially lead to a more complex map model or median probability model , especially when some covariates are near the limit of being considered influential .",
    "for instance , here we see that the median probability model for the cr - pep hyper-@xmath1 and hyper-@xmath226 includes predictor @xmath246 , which based on the @xmath0-prior and the rest of the pep variations should not be included . in figures",
    "[ shrinkage_d ] and [ shrinkage_d / n ] we present some basic convergence diagnostic plots and histograms for the shrinkage parameter @xmath257 under the hyper-@xmath1 and hyper-@xmath226 approaches , respectively .",
    "the histograms are informative in terms of the behavior of the shrinkage parameter . comparing the hyper-@xmath1 approach ( figure [ shrinkage_d ] ) to the hyper-@xmath226 approach ( figure [ shrinkage_d / n ] ) shows that the posterior distribution of the shrinkage parameter under the latter approach is more concentrated near 1 , which explains why shrinkage is greater with the hyper-@xmath226 prior . also , comparing the histograms in figures [ shrinkage_d ] and [ shrinkage_d / n ] from a cr - pep versus a dr - pep perspective shows that the posterior distributions of the shrinkage parameter ( both for the hyper-@xmath1 and hyper-@xmath226 approaches ) are more concentrated near 1 under the dr - pep prior . as a last remark we note that the shrinkage parameter for fixed @xmath1 is constant and approximately equal to 0.998 , which explains the lower posterior inclusion probabilities presented in table [ pima_inclusion ] and figure [ inclusion_pima ] for the fixed @xmath1 cr / dr - pep priors .",
    "in this paper we presented two variations of the pep / pcep priors of fouskakis et al .",
    "( 2015 ) and fouskakis and ntzoufras ( 2015 ) with the aim to extend the particular prior specification to glms .",
    "specifically , we explained the reason why the initial prior formulation is not convenient to work with for cases of non - normal glms and introduced the more flexible cr - pcep and dr - pcep prior alternatives .",
    "the properties of the two pcep variations have been investigated for the conjugate case of the normal linear regression model , leading to the conclusion that both pcep variations are consistent , but that the dr - pcep prior is more parsimonious than cr - pcep prior , in fact equally parsimonious to the initial pcep prior .",
    "furthermore , we considered hyper-@xmath1 type of prior extensions for both methods .",
    "the empirical results presented in this paper , so far suggest that both pcep variations and their corresponding random-@xmath1 extensions can potentially outperform hyper-@xmath0 prior approaches in terms of introducing larger shrinkage for non - influential or for partially influential predictors . focusing on the internal comparison between the two pcep variations and their corresponding random-@xmath1 extensions ,",
    "the empirical results of this study indicate that a fixed choice of the power - parameter @xmath1 set equal to @xmath2 results in more parsimonious model selection procedures and that the dr - pcep prior framework seems to be preferable in comparison to the cr - pcep prior framework , as it less sensitive to the choice of fixed versus random @xmath1 and seems to lead to more parsimonious variable selection in general .",
    "we intend to investigate in more detail the theoretical properties of the two pcep variations for glms .",
    "specifically , issues such as the consistency of bayes factors and the amount of prior information under both pcep variants remains to be examined in the general glm framework .",
    "furthermore , we intend to provide laplace - based marginal likelihood approximations which can be potentially used as fast and efficient alternatives to the rather computationally intense mcmc approaches which are currently in use .",
    "let @xmath258 , then the dr - pcep prior can be expressed as @xmath259 \\frac{f_{\\gamma } ( { \\mathbf{y}}^*|{\\boldsymbol{\\beta}_\\gamma})^{{1/\\delta}}}{c_{\\gamma}({\\boldsymbol{\\beta}_\\gamma},\\delta)}\\mathrm{d}{\\mathbf{y}}^*\\nonumber \\\\ & \\propto & c_{\\gamma}({\\boldsymbol{\\beta}_\\gamma},\\delta ) \\pi_{\\gamma}^{{\\mathrm{n}}}({\\boldsymbol{\\beta}_\\gamma } ) \\int \\frac{\\int f_0({\\mathbf{y}}^*|\\beta_0)^{{1/\\delta } }   \\pi_0^{{\\mathrm{n}}}(\\beta_0 ) \\mathrm{d}\\beta_0 } { \\int f_{\\gamma } ( { \\mathbf{y}}^*|{\\boldsymbol{\\beta}_\\gamma})^{{1/\\delta}}\\pi_{\\gamma}^{{\\mathrm{n}}}({\\boldsymbol{\\beta}_\\gamma})\\mathrm{d}{\\boldsymbol{\\beta}_\\gamma } } \\frac{f_{\\gamma } ( { \\mathbf{y}}^*|{\\boldsymbol{\\beta}_\\gamma})^{{1/\\delta}}}{c_{\\gamma}({\\boldsymbol{\\beta}_\\gamma},\\delta)}\\mathrm{d}{\\mathbf{y}}^*\\nonumber \\\\ & \\propto & c_{\\gamma}({\\boldsymbol{\\beta}_\\gamma},\\delta ) \\pi_{\\gamma}^{{\\mathrm{n}}}({\\boldsymbol{\\beta}_\\gamma } )   \\mathrm{e } _ { { \\mathbf{y}}^*|{\\boldsymbol{\\beta}_\\gamma},\\delta}\\big[{\\mathrm{bf}}_{0,{\\gamma}}^{{\\mathrm{n}}}({\\mathbf{y}}^*|\\delta)\\big ] , \\label{dr - pcep_interpret}\\end{aligned}\\ ] ] where the expectation is taken with respect to the normalized sampling distribution under model @xmath11 and @xmath260 is a `` diffuse - type '' of bayes factor of model @xmath74 over model @xmath11 .      the unnormalized likelihood for model @xmath11 is given by @xmath261^{{1/\\delta}}\\nonumber \\\\",
    "& = &    ( 2\\pi\\sigma^{2}\\delta)^{\\frac{n}{2 } } ( 2\\pi\\sigma^{2})^{-\\frac{n}{2\\delta } } \\times \\nonumber \\\\ &   & \\times \\big [ ( 2\\pi\\sigma^{2}\\delta)^{-\\frac{n}{2 } } \\exp\\big(-\\frac{1}{2\\sigma^{2}\\delta}({\\mathbf{y}}^*-{\\mathbf{x}_{\\gamma}}{\\boldsymbol{\\beta}_\\gamma})^t({\\mathbf{y}}^*-{\\mathbf{x}_{\\gamma}}{\\boldsymbol{\\beta}_\\gamma})\\big ) \\big]\\nonumber \\\\ & = & \\delta^{\\frac{n}{2 } } ( 2\\pi\\sigma^{2})^{\\frac{n(\\delta-1)}{2\\delta } } f_{n_{n}}({\\mathbf{y}}^*;{\\mathbf{x}_{\\gamma}}{\\boldsymbol{\\beta}_\\gamma},\\sigma^{2}\\delta{\\mathbf{i}_n } ) .",
    "\\label{a21}\\end{aligned}\\ ] ] using the function in , the prior predictive of model @xmath11 under a @xmath0 baseline - prior is @xmath262 with @xmath263 and @xmath264 the integration leading to is exactly the same as in fouskakis and ntzoufras ( supplementary material a , 2015 ) .",
    "so the conditional posterior @xmath265 is @xmath266 where @xmath267 and @xmath268 .      @xmath269 + { \\mathbf{x}_{\\gamma}^{t}}{\\mathbf{x}_{\\gamma}}\\bigg)^{-1}{\\mathbf{x}_{\\gamma}^{t}}{\\mathbf{y}}\\nonumber\\\\ & =   { { \\mathbf{y}}}^t{\\mathbf{y}}- \\delta{{\\mathbf{y}}}^t{\\mathbf{x}_{\\gamma}}\\bigg ( w^{-1}{\\mathbf{x}_{\\gamma}^{t}}{\\mathbf{x}_{\\gamma}}-   { \\mathbf{x}_{\\gamma}^{t}}\\big(\\delta { \\mathbf{\\lambda}_{0}^{({\\mathrm{cr}})}}+ w{\\mathbf{h}_{\\gamma}}\\big)^{-1}{\\mathbf{x}_{\\gamma}}+   \\delta{\\mathbf{x}_{\\gamma}^{t}}{\\mathbf{x}_{\\gamma}}\\bigg)^{-1}{\\mathbf{x}_{\\gamma}^{t}}{\\mathbf{y}}\\nonumber \\\\ & = { { \\mathbf{y}}}^t{\\mathbf{y}}- \\delta{{\\mathbf{y}}}^t{\\mathbf{x}_{\\gamma}}\\bigg ( \\frac{1+\\delta w}{w}{\\mathbf{x}_{\\gamma}^{t}}{\\mathbf{x}_{\\gamma}}-{\\mathbf{x}_{\\gamma}^{t}}\\big(\\delta { \\mathbf{\\lambda}_{0}^{({\\mathrm{cr}})}}+ w{\\mathbf{h}_{\\gamma}}\\big)^{-1}{\\mathbf{x}_{\\gamma}}\\bigg)^{-1}{\\mathbf{x}_{\\gamma}^{t}}{\\mathbf{y}}\\nonumber \\\\ & = { { \\mathbf{y}}}^t{\\mathbf{y}}-   \\delta{{\\mathbf{y}}}^t{\\mathbf{x}_{\\gamma}}\\bigg ( \\frac{1+\\delta w}{w}{\\mathbf{x}_{\\gamma}^{t}}{\\mathbf{x}_{\\gamma}}-{\\mathbf{x}_{\\gamma}^{t}}\\big(\\delta \\big({\\mathbf{i}_n}-\\frac{g}{g+1}\\mathbf{h}_0\\big ) + w{\\mathbf{h}_{\\gamma}}\\big)^{-1}{\\mathbf{x}_{\\gamma}}\\bigg)^{-1}{\\mathbf{x}_{\\gamma}^{t}}{\\mathbf{y}}\\nonumber \\\\ & = { { \\mathbf{y}}}^t{\\mathbf{y}}-   \\frac{w\\delta}{1+w\\delta}{{\\mathbf{y}}}^t{\\mathbf{x}_{\\gamma}}\\bigg ( { \\mathbf{x}_{\\gamma}^{t}}{\\mathbf{x}_{\\gamma}}-\\frac{w}{1+w\\delta}{\\mathbf{x}_{\\gamma}^{t}}\\big(\\delta \\big({\\mathbf{i}_n}-\\frac{g}{g+1}\\mathbf{h}_0\\big ) + w{\\mathbf{h}_{\\gamma}}\\big)^{-1}{\\mathbf{x}_{\\gamma}}\\bigg)^{-1}{\\mathbf{x}_{\\gamma}^{t}}{\\mathbf{y}}\\nonumber \\\\ & = { { \\mathbf{y}}}^t{\\mathbf{y}}-   \\frac{w\\delta}{1+w\\delta}{{\\mathbf{y}}}^t{\\mathbf{x}_{\\gamma}}\\bigg ( { \\mathbf{x}_{\\gamma}^{t}}{\\mathbf{x}_{\\gamma}}-\\frac{w}{(1+w\\delta)\\delta}{\\mathbf{x}_{\\gamma}^{t}}\\big({\\mathbf{i}_n}-\\frac{g}{g+1}\\mathbf{h}_0 + \\frac{w}{\\delta}{\\mathbf{h}_{\\gamma}}\\big)^{-1}{\\mathbf{x}_{\\gamma}}\\bigg)^{-1}{\\mathbf{x}_{\\gamma}^{t}}{\\mathbf{y } } , \\end{aligned}\\ ] ]    where @xmath142 and @xmath270 . for the derivation of the first expression ,",
    "see woodbury s matrix identity ( harville , 1997 ; p. 423426 ) . for large values of @xmath1 and @xmath271",
    "we have approximately @xmath272 , @xmath273 and @xmath274 , which yields the approximation @xmath275      the determinant of the cr - pcep prior covariance matrix is @xmath276 based on the matrix determinant lemma ( harville , 1997 ; p. 416 ) , which states that @xmath277 for any square invertible matrices @xmath278 and @xmath279 , we can write as @xmath280 using repeatedly the matrix determinant lemma on the last term of yields @xmath281 note that the transition from to the following equation is due to the fact that @xmath282 , since @xmath283 for any sub - matrix @xmath284 of @xmath233 . from",
    "and we have that @xmath285",
    "here we provide details about the prior specification and the posterior sampling algorithm using the gibbs variable selection ( gvs ) method of dellaportas et al .",
    "we focus on the data - augmented cr - pcep and dr - pcep posterior distributions presented in and in , respectively , and further consider the option of random @xmath1 ; see section [ sec4 ] .",
    "the gvs stochastic search algorithm is based on the vector of binary indicators @xmath286 representing which of the @xmath5 possible sets of covariates are included in a model .",
    "we partition the regression vector @xmath287 into @xmath288 , corresponding to those components of @xmath289 that are included ( @xmath290 ) and excluded ( @xmath291 ) from the model .",
    "given the likelihood specification presented in section [ likelihood ] we assume that the intercept term is included in all models under consideration , so that @xmath25 and @xmath292 are of dimensionality @xmath27 and @xmath293 , respectively , where @xmath294 .    under the gvs setting , indicator @xmath9 is random , and the joint prior of @xmath287 and @xmath9 is specified as @xmath295 where the actual baseline prior choice involves only @xmath25 , since @xmath296 is just a _ pseudo - prior _ used for balancing the dimensions between model spaces ; see dellaportas et al .",
    "( 2002 ) . specifically for @xmath297 and @xmath1",
    "we consider the following priors .",
    "+   + common choices for the baseline prior of the active part of the regression vector @xmath287 are either a flat improper prior @xmath298 or jeffreys prior for glms ( laud and ibrahim , 1991 ) which is of the form @xmath299 where @xmath232 is the diagonal matrix containing the glm weights which are functions of @xmath25 .",
    "+   + for the inactive part of the regression vector @xmath287 we use as pseudo - prior a multivariate normal distribution of dimensionality @xmath300 . specifically , @xmath301 where @xmath302 and @xmath303 are the ml estimates and the corresponding standard errors of @xmath292 , respectively , from the full regression model on @xmath228 , i.e. the model with all @xmath5 potential covariates .",
    "+   + for the intercept term of the null model the baseline prior specification must be equivalent to the baseline prior of @xmath25 , i.e. either assume that @xmath304 or use jeffrey s glm prior which for the case of the null model is given by @xmath305 .",
    "+   + for @xmath9 a common non - informative option is to use a product bernoulli distribution where each covariate has an equal probability of being included or excluded in the model ( uniform prior ) , which leads to @xmath306 an alternative option is to use the hierarchical prior @xmath307 and @xmath308 in order to account for an appropriate multiplicity adjustment ( scott and berger , 2010 ) . in this case",
    "the resulting prior model probabilities are @xmath309     + for the power - parameter @xmath1 the standard option proposed in fouskakis et al .",
    "( 2015 ) is to keep it fixed at @xmath230 . in this case",
    "the prior of @xmath1 reduces to a degenerate distribution with point mass at @xmath2 .",
    "alternatively , one can consider the prior extensions discussed in section [ sec4 ] , namely the hyper-@xmath1 prior @xmath310 or the hyper-@xmath226 prior @xmath227 following the recommendation in liang et al .",
    "( 2008 ) , parameter @xmath150 is set equal to 3 .",
    "+ given the prior in , the fully hierarchical analogues of the cr - pep posterior in and of the dr - pep posterior in are given by @xmath311 and @xmath312 respectively .",
    "all of the quantities appearing in the right - hand side of and of are known in closed form except of @xmath313 .",
    "the latter is estimated through the following laplace approximation @xmath314 where @xmath213 is the mle for data @xmath51 given the configuration of @xmath9 and @xmath214 is minus the inverse hessian matrix evaluated at @xmath213 , i.e. @xmath215 , with @xmath232 being the @xmath315 diagonal matrix containing the glm weights .",
    "note that under jeffrey s prior for @xmath25 the laplace estimate simplifies to @xmath316      given the posterior distributions in and in we propose the following gvs metropolis - within - gibbs sampling scheme :    * set starting values @xmath317 and @xmath318 . for fixed @xmath1 set",
    "@xmath134 , for random @xmath1 set starting starting value @xmath319 * for iterations @xmath320 : * * sampling of @xmath321 given the current state of @xmath9 , @xmath1 and @xmath51 . * * * generate @xmath322 from the proposal distribution @xmath323 , where @xmath324 is the ml estimate from a weighted regression on @xmath325 , using weights @xmath326 , and @xmath327 is the estimated variance - covariance matrix of @xmath324 . + * * * calculate the probability of move : @xmath328.\\\\\\ ] ] * * * set @xmath329 + * * sampling of @xmath330 given the current state of @xmath9 . * * * generate @xmath331 from the pseudo - prior @xmath332 .",
    "+ * * * set @xmath333 with probability equal to 1 .",
    "+ * * sampling of @xmath334 given the current state of @xmath1 and @xmath51 .",
    "* * * generate @xmath335 from the proposal distributions @xmath336 under the cr - pep setting and @xmath337 under the dr - pep setting , where @xmath338 is the ml estimate from the regression on @xmath51 and @xmath339 is the standard error of @xmath338 . + * * * under the cr - pep setting , calculate the probability of move as @xmath340.\\ ] ] under the dr - pep setting , calculate the probability of move as @xmath341.\\\\\\ ] ] * * * set @xmath342 + * * sampling of @xmath343 given the current state of @xmath25 , @xmath344 , @xmath9 and @xmath1 . * * * generate @xmath345 from a proposal distribution @xmath346 ; see remark 3 below for details about the proposal . +",
    "* * * calculate the mles given @xmath347 and @xmath345 and compute the laplace approximations @xmath348 and @xmath349 . + * * * under the cr - pep setting calculate the probability of move as @xmath350.\\ ] ] under the dr - pep setting calculate the probability of move as @xmath351.\\\\\\ ] ] * * * set @xmath352 + * * sampling of @xmath1 given the current state of @xmath25 , @xmath344 and @xmath9 . * * * if @xmath1 is fixed at @xmath2 go to step 6 , else implement ( b)-(e ) of step 5 .",
    "+ * * * generate @xmath353 from the proposal distribution @xmath354 with shape @xmath355 and rate @xmath356 . + * * * compute the laplace approximations @xmath357 and @xmath358 . + * * * under the cr - pep setting calculate the probability of move as @xmath359.\\ ] ] under the dr - pep setting calculate the probability of move as @xmath360.\\\\\\ ] ] * * * set @xmath361 + * * sampling of @xmath362 , for @xmath363 , given the current state of @xmath364 , @xmath1 and @xmath51 . * * * calculate the mles for @xmath290 and @xmath291 and compute the laplace approximations @xmath365 and @xmath366 . + * * * evaluate the odds @xmath367 * * * sample @xmath368 and set @xmath369 with probability equal to 1 . + * * update @xmath370 based on the current configuration of @xmath9 . + * repeat the steps in b until convergence .",
    "+    * remarks *    1 .",
    "metropolis - hasting steps are not needed for @xmath292 and @xmath9 ; the former is sampled directly from the pseudo - prior distribution , while the latter is sampled directly from the full conditional bernoulli distribution .",
    "no specific fine tuning is required for the proposal distributions of @xmath25 and @xmath344 ( normal proposals based on mles ) and also for the proposal of @xmath9 ( random - walk gamma proposal ) .",
    "3 .   for @xmath51",
    "we recommend the following proposals depending on the likelihood of the model and on the pep prior that is used : * for logistic regression a product binomial proposal distribution with probability of success equal to @xmath371 under the cr - pep prior and equal to @xmath372^{{1/\\delta}}}\\ ] ] under the dr - pep prior , with @xmath373 and @xmath374 .",
    "* for poisson regression under the cr - pep prior a product poisson proposal distribution with mean equal to @xmath375 where @xmath376 and @xmath377 .",
    "under the dr - pep prior we utilize a product poisson random - walk proposal with mean equal to the state of @xmath51 at the previous iteration .",
    "this research has been co - financed in part by the european union ( european social fund - esf ) and by greek national funds through the operational program  education and lifelong learning \" of the national strategic reference framework ( nsrf)-research funding program : aristeia ii / pep - bvs .",
    "fouskakis , d. and ntzoufras , i. ( 2015 ) .",
    "power - conditional - expected priors : using @xmath0-priors with random imaginary data for variable selection .",
    "_ journal of computational and graphical statistics _ , forthcoming .",
    "murray , i. , ghahramani , z. and mackay , d.j.c .",
    "mcmc for doubly - intractable distributions . in : _ proceedings of the 22nd annual conference on uncertainty in artificial intelligence _ , ( uai-06 ) , auai press , pp .",
    "359366 .",
    "womack , a.j .",
    ", len - novelo , l. and casella , g. ( 2014 ) .",
    "inference from intrinsic bayes procedures under model selection and uncertainty .",
    "_ journal of the american statistical association _ ,",
    "* 109 * , 10401053 .",
    "zellner , a. ( 1986 ) . on assessing prior distributions and bayesian regression analysis with g - prior distributions .",
    "bayesian inference and decision techniques : essays in honour of bruno de finetti _ , eds .",
    "p. goel and a. zellner , amsterdam : north - holland , pp .",
    "233243 .",
    "zellner , a. and siow , a. ( 1980 ) .",
    "posterior odds ratios for selected regression hypotheses . in : _",
    "proceedings of the first international meeting held in valencia : bayesian statistics _ , eds .",
    "bernardo , m.h .",
    "degroot , d.v .",
    "lindley and a.f.m .",
    "smith , valencia : university of valencia press , pp ."
  ],
  "abstract_text": [
    "<S> the power - conditional - expected - posterior ( pcep ) prior developed for variable selection in normal regression models combines ideas from the power - prior and expected - posterior prior , relying on the concept of random imaginary data , and provides a consistent variable selection method which leads to parsimonious inference . in this paper </S>",
    "<S> we discuss the computational limitations of applying the pcep prior to generalized linear models ( glms ) and present two pcep prior variations which are easily applicable to regression models belonging to the exponential family of distributions . </S>",
    "<S> we highlight the differences between the initial pcep prior and the two glm - based pcep prior adaptations and compare their properties in the conjugate case of the normal linear regression model . </S>",
    "<S> hyper prior extensions for the pcep power parameter are further considered . </S>",
    "<S> we consider several simulation scenarios and one real data example for evaluating the performance of the proposed methods compared to other commonly used methods . </S>",
    "<S> empirical results indicate that the two glm - pcep adaptations lead to parsimonious variable selection inference .    _ </S>",
    "<S> keywords : expected - posterior prior , @xmath0/hyper-@xmath0 priors , glm regression variable selection , objective bayesian model selection , power prior _ </S>"
  ]
}