{
  "article_text": [
    "ontologies and knowledge bases such as wordnet @xcite or yago @xcite are extremely useful resources for query expansion @xcite , coreference resolution @xcite , question answering ( siri ) , information retrieval ( google knowledge graph ) , or generally providing inference over structured knowledge to users .",
    "much work has focused on extending existing knowledge bases @xcite using patterns or classifiers applied to large corpora .",
    "we introduce a model that can accurately learn to add additional facts to a database using only that database .",
    "this is achieved by representing each entity ( i.e. , each object or individual ) in the database by a vector that can capture facts and their certainty about that entity .",
    "each relation is defined by the parameters of a novel neural tensor network which can explicitly relate two entity vectors and is more powerful than a standard neural network layer .",
    "furthermore , our model allows us to ask whether even entities that were not in the database are in certain relationships by simply using distributional word vectors .",
    "these vectors are learned by a neural network model @xcite using unsupervised text corpora such as wikipedia .",
    "they capture syntactic and semantic information and allow us to extend the database without any manually designed rules or additional parsing of other textual resources .",
    "the model outperforms previously introduced related models such as that of bordes et al .",
    "we evaluate on a heldout set of relationships in wordnet .",
    "the accuracy for predicting unseen relations is 75.8% .",
    "we also evaluate in terms of ranking .",
    "for wordnet , there are 38,696 different entities and we use 11 relationship types . on average for each left entity there are 100 correct entities in a specific relationship . for instance , _ dog _ has many hundreds of hyponyms such as _ puppy , barker _ or _",
    "dachshund_. in 20.9% of the relationship triplets , the model ranks the correct test entity in the top 100 out of 38,696 possible entities .",
    "there is a vast amount of work extending knowledge bases using external corpora @xcite , among many others . in contrast , little work has been done in extensions based purely on the knowledge base itself .",
    "the work closest to ours is that by bordes et al .",
    "we implement their approach and compare to it directly .",
    "our model outperforms it by a significant margin in terms of both accuracy and ranking .",
    "both models can benefit from initialization with unsupervised word vectors .",
    "another related approach is that by sutskever et al .",
    "@xcite who use tensor factorization and bayesian clustering for learning relational structures . instead of clustering the entities in a nonparametric bayesian framework we rely purely on learned entity vectors .",
    "their computation of the truth of a relation can be seen as a special case of our proposed model . instead of using mcmc for inference",
    ", we use standard backpropagation which is modified for the neural tensor network .",
    "lastly , we do not require multiple embeddings for each entity .",
    "instead , we consider the subunits ( space separated words ) of entity names .",
    "this allows more statistical strength to be shared among entities .",
    "many methods that use knowledge bases as features such as @xcite could benefit from a method that maps the provided information into vector representations .",
    "we learn to modify unsupervised word representations via grounding in world knowledge .",
    "this essentially allows us to analyze word embeddings and query them for specific relations .",
    "furthermore , the resulting vectors could be used in other tasks such as ner @xcite or relation classification in natural language @xcite .",
    "lastly , ranzato et al .",
    "@xcite introduced a factored 3-way restricted boltzmann machine which is also parameterized by a tensor .",
    "in this section we describe the full neural tensor network .",
    "we begin by describing the representation of entities and continue with the model that learns entity relationships .",
    "we compare using both randomly initialized word vectors and pre - trained @xmath0-dimensional word vectors from the unsupervised model of collobert and weston @xcite . using free wikipedia text",
    ", this model learns word vectors by predicting how likely it is for each word to occur in its context .",
    "the model uses both local context in the window around each word and global document context .",
    "similar to other local co - occurrence based vector space models , the resulting word vectors capture distributional syntactic and semantic information . for further details and evaluations of these embeddings ,",
    "see @xcite .    for cases where the entity name has multiple words ,",
    "we simply average the word vectors .",
    "the neural tensor network ( ntn ) replaces the standard linear layer with a bilinear layer that directly relates the two entity vectors .",
    "let @xmath1 be the vector representations of the two entities .",
    "we can compute a score of how plausible they are in a certain relationship @xmath2 by the following ntn - based function : @xmath3}_r e_2 + v_r \\left [ \\begin{matrix }   e_1\\\\ e_2\\\\ \\end{matrix } \\right ] + b_r \\right ) , \\label{eq : ntn}\\ ] ] where @xmath4 is a standard nonlinearity .",
    "we define @xmath5 } \\in \\mathbb{r}^{d \\times d \\times k}$ ] as a tensor and the bilinear tensor product results in a vector @xmath6 , where each entry is computed by one slice of the tensor : @xmath7}e_2 .",
    "\\label{eq : tensorhid}\\ ] ] the remaining parameters for relation @xmath2 are the standard form of a neural network : @xmath8 and @xmath9 .",
    "the main advantage of this model is that it can directly relate the two inputs instead of only implicitly through the nonlinearity .",
    "the bilinear model for truth values in @xcite becomes a special case of this model with @xmath10 .    in order to train the parameters @xmath11 , we minimize the following contrastive max - margin objective : @xmath12 where @xmath13 is the number of training triplets and we score the correct relation triplets higher than a corrupted one in which one of the entities was replaced with a random entity",
    ". for each correct triplet we sample @xmath14 random corrupted entities .",
    "the model is trained by taking gradients with respect to the five sets of parameters and using minibatched l - bfgs .",
    "in our experiments , we follow the data settings of wordnet in @xcite .",
    "there are a total of 38,696 different entities and 11 relations .",
    "we use 112,581 triplets for training , 2,609 for the development set and 10,544 for final testing .",
    "the wordnet relationships we consider are _ has instance , type of , member meronym , member holonym , part of , has part , subordinate instance of , domain region , synset domain region , similar to , domain topic_.    we compare our model with two models in bordes et al .",
    "@xcite , which have the same goal as ours .",
    "the model of @xcite has the following scoring function : @xmath15 where @xmath16 .",
    "the model of @xcite also maps each relation type to an embedding @xmath17 and scores the relationships by : @xmath18 where @xmath19 . in the comparisons below",
    ", we call these two models the _ similarity model _ and the _ hadamard model _ respectively . while our function scores correct triplets highly , these two models score correct triplets lower .",
    "all models are trained in a contrastive max - margin objective functions .",
    "our goal is to predict `` correct '' relations @xmath20 in the testing data .",
    "we can compute a score for each triplet @xmath20 .",
    "we can consider either just a classification accuracy result as to whether the relation holds , or look at a ranking of @xmath21 , for considering relative confidence in particular relations holding .",
    "we use a different evaluation set from bordes et al .",
    "@xcite because it has became apparent to us and them that there were issues of overlap between their training and testing sets which impacted the quality and interpretability of their evaluation .      for each triplet @xmath20",
    ", we compute the score @xmath22 for all other entities in the knowledge base @xmath23 .",
    "we then sort values by decreasing order and report the rank of the correct entity @xmath21 .    for wordnet the total number of entities",
    "is @xmath24 .",
    "some of the questions relating to triplets are of the form `` a is a type of ? '' or",
    "`` a has instance ? ''",
    "since these have multiple correct answers , we report the percentage of times that @xmath21 is ranked in the top @xmath0 of the list ( recall @ 100 ) .",
    "the higher this number , the more often the specific correct test entity has likely been correctly estimated .",
    "after cross - validation of the hyperparameters of both models on the development fold , our neural tensor net obtains a ranking recall score of 20.9% while the similarity model achieves 10.6% , and the hadamard model achieves only 7.4% .",
    "the best performance of the ntn with random initialization instead of the semantic vectors drops to 16.9% and the similarity model and the hadamard model only achieve 5.7% and 7.1% .      in this experiment , we ask the model whether any arbitrary triplet of entities and relations is true or not . with the help of the large vocabulary of semantic word vectors",
    ", we can query whether certain wordnet relationships hold or not even for entities that were not originally in wordnet .",
    "we use the development fold to find a threshold @xmath25 for each relation such that if @xmath26 , the relation @xmath20 holds , otherwise it is considered false . in order to create negative examples ,",
    "we randomly switch entities and relations from correct testing triplets , resulting in a total of @xmath27 triplets .",
    "the final accuracy is based on how many of of triplets are classified correctly .",
    "the neural tensor network achieves an accuracy of 75.8% with semantically initialized entity vectors and 70.0% with randomly initialized ones . in comparison ,",
    "the similarity based model only achieve 66.7% and 51.6% , the hadamard model achieve 71.9% and 68.2% with the same setup .",
    "all models improve in performance if entities are represented as an average of their word vectors but we will leave experimentation with this setup to future work .",
    "we introduced a new model based on neural tensor networks . unlike previous models for predicting relationships purely using entity representations in knowledge bases ,",
    "our model allows direct interaction of entity vectors via a tensor .",
    "this architecture allows for much better performance in terms of both ranking correct answers out of tens of thousands of possible ones and predicting unseen relationships between entities .",
    "it enables the extension of databases even without external textual resources but can also benefit from unsupervised large corpora even without manually designed extraction rules ."
  ],
  "abstract_text": [
    "<S> knowledge bases provide applications with the benefit of easily accessible , systematic relational knowledge but often suffer in practice from their incompleteness and lack of knowledge of new entities and relations . </S>",
    "<S> much work has focused on building or extending them by finding patterns in large unannotated text corpora . </S>",
    "<S> in contrast , here we mainly aim to complete a knowledge base by predicting additional true relationships between entities , based on generalizations that can be discerned in the given knowledgebase . </S>",
    "<S> we introduce a neural tensor network ( ntn ) model which predicts new relationship entries that can be added to the database . </S>",
    "<S> this model can be improved by initializing entity representations with word vectors learned in an unsupervised fashion from text , and when doing this , existing relations can even be queried for entities that were not present in the database . </S>",
    "<S> our model generalizes and outperforms existing models for this problem , and can classify unseen relationships in wordnet with an accuracy of 75.8% . </S>"
  ]
}