{
  "article_text": [
    "stochastic learning algorithms are of central interest in machine learning due to their simplicity and , as opposed to batch methods , their low memory and computational complexity requirements @xcite . for instance , stochastic learning algorithms are commonly used to train deep belief networks ( dbns )  @xcite which perform extremely well on tasks involving massive data - sets @xcite .",
    "stochastic algorithms are also broadly used to train conditional random fields ( crfs ) @xcite , solve maximum likelihood problems @xcite and perform variational inference @xcite .",
    "most stochastic optimization approaches fall into two groups : first - order methods and second - order methods .",
    "popular first - order methods include stochastic gradient descent ( sgd ) @xcite and its many extensions @xcite .",
    "these methods typically have low computational cost per iteration ( such as @xmath0 where @xmath1 is the data dimensionality ) and either sub - linear ( most stochastic gradient methods ) or linear ( as shown recently in  @xcite ) convergence rate which makes them particularly relevant for large - scale learning . despite its simplicity , sgd has many drawbacks : it has slow asymptotic convergence to the optimum @xcite , it has limited ability to handle certain regularized learning problems such as @xmath2-regularization @xcite , it requires step - size tuning and it is difficult to parallelize @xcite .",
    "many works have tried to incorporate second - order information ( i.e. hessian ) into the optimization problem to improve the performance of traditional sgd methods . a straightforward way of doing so is to simply replace the gain in sgd with the inverse of the hessian matrix which , when naively implemented , induces a computational complexity of @xmath3 .",
    "this makes the approach impractical for large problems .",
    "the trade - offs in large - scale learning for various prototypical batch and stochastic learning algorithms are conveniently summarized in  @xcite ) .",
    "therein , several new methods are developed , including variants of newton s method which use both gradient and hessian information to compute the descent direction . by carefully exploring different first- and second - order techniques , the overall computational complexity of optimization",
    "can be reduced as in the stochastic meta - descent ( smd ) algorithm @xcite .",
    "although it still uses the gradient direction to converge , smd also efficiently exploits certain hessian - vector products to adapt the gradient step - size .",
    "the algorithm is shown to converge to the same quality solution as limited - memory bfgs ( lbfgs ) an order of magnitude faster for crf training @xcite .",
    "there also exists stochastic versions of quasi - newton methods like online bfgs and online lbfgs @xcite , the latter applicable to large - scale problems , which while using convenient size mini - batches performs comparably to a well - tuned natural gradient descent @xcite on the task of training crfs , but at the same time is more scalable . in each iteration",
    "the inverse of the hessian , that is assumed to have no negative eigenvalues , is estimated .",
    "computational complexity of this online lbfgs method is @xmath4 per iteration , where @xmath5 is the size of the buffer used to estimate the inverse of the curvature .",
    "the method degrades ( large @xmath5 ) for sparse data - sets .",
    "another second - order stochastic optimization approach proposed in the literature explores diagonal approximations of the hessian matrix or gauss - newton matrix@xcite . in some cases this approach appears to be overly simplistic @xcite , but turned out successful in very particular applications , i.e. for learning with linear support vector machines @xcite .",
    "there is also a large body of work on stochastic second - order methods particularly successful in training deep belief network like hessian - free optimization @xcite .",
    "finally , there are also many hybrid methods using existing stochastic optimization tools as building blocks and merging them to obtain faster and more robust learning algorithms @xcite .",
    "this paper contributes to the family of existing second - order stochastic optimization methods with a new algorithm that is using a globally guaranteed quadratic bound with a curvature different than the hessian .",
    "therefore our approach is not merely a variant of newton s method .",
    "this is a stochastic version of a recently proposed majorization method @xcite which performed maximum ( latent ) conditional likelihood problems more efficiently than other state - of - the - art first- and second- order batch optimization methods like bfgs , lbfgs , steepest descent ( sd ) , conjugate gradient ( cg ) and newton .",
    "the corresponding stochastic bound majorization method is compared with a well - tuned sgd with either constant or adaptive gain in @xmath6-regularized logistic regression and turns out to outperform competitor methods in terms of the number of iterations , the convergence time and even the quality of the obtained solution measured by the test error and test likelihood .",
    "a staggering number of machine learning and statistics frameworks involve linear combinations or cascaded linear combinations of soft - maximum functions : @xmath7 where @xmath8 is a parameter vector , @xmath9 is any vector - valued function mapping an input @xmath10 to some arbitrary vector ( we assume @xmath11 is finite and @xmath12 is enumerable ) , @xmath13 is the size of the data - set and @xmath14 is some non - negative weight .",
    "these functions emerge in multi - class logistic regression , crfs @xcite , hidden variable problems , dbns @xcite , discriminatively trained speech recognizers @xcite and maximum entropy problems @xcite . for simplicity , we focus herein on the crf training problem in particular .",
    "crfs use the density model : @xmath15 where @xmath16 are _ iid _ input - output pairs and @xmath17 is a partition function : @xmath18 . following the maximum likelihood approach , the objective function to maximize in this setting is : @xmath19 - \\frac{\\lambda}{2}\\|\\btheta\\|^2 , \\label{eq : objective}\\ ] ] where @xmath20 is a regularization hyper - parameter . let @xmath21 , where @xmath22",
    ". for large numbers of data points @xmath13 , and potentially large dimensionality @xmath1 , summations in equation  [ eq : objective ] need not be handled in a batch form , but rather , can be processed stochastically or semi - stochastically .",
    "we next review the most commonly used stochastic algorithm , sgd , which will be a key comparator for our stochastic bound majorization algorithm .",
    "batch gradient descent updates the parameter vector @xmath23 after seeing the entire training data - set using the following formula : @xmath24 where @xmath25 , @xmath26 and @xmath27 is typically chosen via line search .",
    "in contrast , stochastic gradient descent updates the parameter vector @xmath23 after seeing each training data point ( resp . each mini - batch of data points ) as follows : @xmath28 where @xmath29 is the current parameter vector , @xmath30 is the current gain or step - size , and @xmath31 is the size of the mini - batch .",
    "we assume a data point is randomly selected and take its index to be @xmath32 .",
    "we intentionally index @xmath23 with @xmath33 and @xmath34 to emphasize that the update is done after seeing single example ( resp . mini - batch of examples ) . for the batch method ,",
    "we do not index @xmath23 since the update is done after passing through the entire data - set ( epoch ) . in the experimental section",
    "we explore two existing variants of stochastic gradient descent : one with constant gain ( sgd ) and one with adaptive gain ( asgd ) . for the latter , we consider two strategies for modifying the gain and present the results for the better one ( in section  [ sec : experiments ] the absence of a @xmath35 value indicates that the second strategy is better since it does not require a @xmath35 parameter ) :    * @xmath36",
    "* @xmath37 , where @xmath38 are tuning parameters .      [ cols= \" < , < \" , ]     the stochastic bound majorization algorithm also readily admits mini - batches . algorithm  [ alg : stochasticbound ] captures the full - rank version of the proposed algorithm ( we always use constant step size @xmath39 ) .",
    "we have also investigated many other potential variants of algorithm  [ alg : stochasticbound ] including heuristics borrowed from other stochastic algorithms in the literature @xcite .",
    "some heuristics involved using memory to store previous values of updates , gradients and second order matrix information .",
    "remarkably , all such heuristics and modifications slowed down the convergence of algorithm  [ alg : stochasticbound ] .    on caveat remains .",
    "the computational complexity of the proposed stochastic bound majorization method is @xmath40 per iteration which is less appealing than the @xmath41 complexity of sgd .",
    "this shortcoming is resolved in the next subsection .",
    "we next provide a low - rank version of algorithm  [ alg : stochasticbound ] to maintain a @xmath41 run - time per stochastic update .",
    "consider the update on @xmath42 inside the loop over @xmath10 in algorithm  [ alg : stochasticbound ] .",
    "this update can be rewritten as @xmath43 where @xmath44 and @xmath45 are the matrices that , after being updated , become @xmath46 and @xmath47 respectively : @xmath48 , @xmath49 and @xmath50 ( rank @xmath51 update ) .",
    "we can store the matrix @xmath47 using a low - rank representation @xmath52 , where @xmath53 is a rank ( @xmath54 ) , @xmath55 is orthonormal , @xmath56 is positive semi - definite and @xmath57 is non - negative diagonal .",
    "we can directly update @xmath58 , @xmath59 and @xmath60 online rather than incrementing matrix @xmath47 by a rank @xmath51 update . in the case of batch",
    "bound majorization method this still guarantees an overall upper bound @xcite .",
    "we directly apply this technique as well in our stochastic setting . due to space constraints",
    "we will not present this technique ( we refer the reader to  @xcite ) . given a low - rank version of the matrix , we use the woodbury formula to invert it in each iteration : @xmath61 .",
    "that leads to a low - rank version of algorithm  [ alg : stochasticbound ] , which requires only @xmath62 work per iteration which is pseudo - linear in dimension if @xmath53 is assumed to be a logarithmic or constant function of @xmath1 .",
    "cc +    consider figure  [ fig : toydataset ] which is an example of a binary classification problem which exposes some of difficulties with sgd .",
    "intuitively , in this example , the gradients in the sgd update rule will point in almost random directions which could lead to very slow progress .",
    "four training algorithms will be compared : sgd , asgd , stochastic bound majorization algorithm ( sbm ) and lbfgs .",
    "we have tried several parameter settings for sgd and asgd .",
    "the range of tested step size @xmath63 was as broad as @xmath64 $ ] .",
    "the sgd method however exhibits high instability until the step size is reduced to an unreasonably small value such as @xmath65 ( for comparison we also show sgd performance for @xmath66 ) .",
    "the reason for is that this is a highly symmetric and non - linearly separable data - set .",
    "therefore , the information captured in the gradients is extremely noisy which causes sgd to oscillate and fail to converge in practice . for asgd we obtained the best and stable result for @xmath67 ( for comparison",
    "we also show asgd performance for @xmath68 ) and @xmath69 ( larger values of @xmath35 weaken performance ) . for both methods we tested the mini - batch size @xmath5 from @xmath51 ( a single data point ) to @xmath70 and noticed no meaningful difference .",
    "clearly both sgd and asgd are stuck in solutions that are only slightly better than random guessing .",
    "meanwhile sbm ( with @xmath71 and a constant step size @xmath72 ) finds the same solution as a batch lbfgs method .",
    "it does so with a single pass through the data and simultaneously outperforming the competitor methods .",
    "we emphasize that all methods used as comparators to our algorithm were well - tuned , i.e. the initial gain @xmath63 is set as high as possible for each method while maintaining stability .",
    "cc +    next , we focus on the ecoli uci data - set ( ) , a simple small - scale classification problem .",
    "we compare lbfgs , batch bound majorization method ( bbm ) , sgd , asgd and sbm .",
    "results are summarized in figure  [ fig : ecoli ] .",
    "bbm , which was already shown to outperform leading batch methods @xcite , performs comparably to asgd and sgd .",
    "the only method that beats bbm is sbm . in the first row of figure",
    "[ fig : ecoli ] , we plot the objective with respect to the number of passes through the data .",
    "however , looking more closely at the objective with respect to each iteration ( where a single iteration corresponds to a single update of the parameter vector ) , we note an interesting property of sbm : it clearly remains monotonic in its convergence despite its stochastic nature .",
    "this is in contrast to sgd and asgd which ( as expected ) fluctuate much more noisily .",
    "note that , in all experiments in this section and the next , @xmath73 of the data is used for training and the rest for testing , the results are averaged over @xmath70 random initializations close to the origin and the regularization value @xmath20 is chosen through crossvalidation .",
    "all methods were implemented in c++ using the mex environment under matlab .",
    "we next evaluate the performance of the new algorithm empirically .",
    "we compare sgd , asgd and sbm for @xmath6-regularized logistic regression on the mnist@xmath74 , gisette@xmath75 , secstr@xmath76 , digitl@xmath76 and text@xmath76 data - sets , @xmath75 and @xmath76 ] .",
    "we show two variants of the sbm algorithm : full - rank ( on secstr and mnist ) and low - rank ( on the remaining data - sets ) . for the experiments with full - rank sbm , we plot the testing log - likelihood and error versus passes through the data ( epoch iterations ) . for the experiments with low - rank sbm , we plot the likelihood versus cpu time . for sgd and asgd we tested mini - batches of size from @xmath71 to @xmath78 and chose the best setting . for the mnist",
    "data - set we explored mini - batches of up to @xmath79 to achieve optimal sgd behavior . for sbm",
    "we always simply used @xmath71 . for each data - set ,",
    "figure  [ fig : largeresults ] reports the optimal step size @xmath63 for sgd and asgd and the optimal parameter @xmath35 for asgd ( if it was necessary ) . for the full - rank version of sbm we always use @xmath72 , however for its low - rank version ( where we simply assumed @xmath80 ) we tuned @xmath63 .",
    "the chosen value of @xmath63 is also reported in figure  [ fig : largeresults ] . clearly , sbm is less prone to over - fitting and achieves higher testing likelihood than sgd and asgd as well as lower testing error .",
    "simultaneously , sbm exhibits the fastest convergence in terms of the number of passes through the data till convergence .",
    "furthermore , low - rank sbm had the fastest convergence in terms of cpu time , outperforming the leading stochastic first - order methods ( sgd and asgd ) as shown in the plots of likelihood over time .",
    "we have proposed a new stochastic bound majorization method for optimizing the partition function of log - linear models that uses second - order curvature through a global bound ( rather than a local hessian ) .",
    "the method is obtained by applying sherman - morrison to the batch update rule to convert it into an iterative summation over the data which can easily be made stochastic by interleaving parameter updates .",
    "this ( full - rank ) stochastic method requires no parameter tuning .",
    "a low - rank version of this stochastic update rule makes this effectively second - order method remain linear in the dimensionality of the data .",
    "we showed experimentally that the method has significant advantage over the state - of - the - art first - order stochastic methods like sgd and asgd making majorization competitive in both stochastic and batch settings @xcite .",
    "stochastic bound majorization achieves convergence in fewer iterations , in less computation time ( when using the low - rank version ) , and with better final solutions .",
    "future work will involve providing theoretical guarantees for the method as well as application to deep architectures with cascaded linear combinations of soft - max functions ."
  ],
  "abstract_text": [
    "<S> recently a majorization method for optimizing partition functions of log - linear models was proposed alongside a novel quadratic variational upper - bound . in the batch setting , it outperformed state - of - the - art first- and second - order optimization methods on various learning tasks . </S>",
    "<S> we propose a stochastic version of this bound majorization method as well as a low - rank modification for high - dimensional data - sets . </S>",
    "<S> the resulting stochastic second - order method outperforms stochastic gradient descent ( across variations and various tunings ) both in terms of the number of iterations and computation time till convergence while finding a better quality parameter setting . </S>",
    "<S> the proposed method bridges first- and second - order stochastic optimization methods by maintaining a computational complexity that is linear in the data dimension and while exploiting second order information about the pseudo - global curvature of the objective function ( as opposed to the local curvature in the hessian ) . </S>"
  ]
}