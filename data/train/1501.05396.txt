{
  "article_text": [
    "human speech perception is not only about hearing but also about seeing : our brain integrates the waveforms representing the speech information as well as the lips poses and motions , often called visemes , which carry important visual information about what is being said .",
    "this has been demonstrated by the so called mcgurk effect @xcite , which shows that a voicing of _ ba _ and a mouthing of _ ga _ is perceived as being  _ da_. in the presence of noise and multiple speakers ( cocktail party effect ) , humans rely on lip reading in order to enhance speech recognition @xcite .",
    "the visual information is also important in a clean speech scenario as it helps in disambiguating voices with similar acoustics @xcite .",
    "+ in audio - visual automatic speech recognition ( av - asr ) , both audio recordings and videos of the person talking are available at training time .",
    "it is challenging to build models that integrates both visual and audio information , and that enhance the recognition performance of the overall system . while most previous works in av - asr focused on enhancing the performance in the noisy case @xcite , where the visual information can be crucial , we focus in this paper on showing that the visual information is indeed helpful even in the clean speech scenario .",
    "+ multimodal learning consists of fusing and relating information coming from different sources , hence av - asr is an important multimodal problem .",
    "finding correlations between different modalities , and modeling their interactions , has been addressed in various learning frameworks and has been applied to av - asr @xcite .",
    "deep neural networks ( dnn ) have shown impressive performance in both audio and visual classification tasks , which is why we restrict ourselves to the deep multimodal learning framework @xcite .",
    "+ in this paper , we propose methods in deep learning to fuse modalities , and validate them on the _ ibm av - asr large vocabulary studio dataset _ ( section [ sec : avsr ] ) .",
    "first we consider the training of two networks on the audio and the visual modality separately .",
    "then , considering the last layer of each network as a better feature space , and concatenating them , we train a classifier on that joint representation , and obtain gains in phone error rates ( per ) , with respect to an audio - only trained network .",
    "we then propose a new bilinear network that accounts for correlations between modalities and allows for joint training of the two networks , we show that a committee of such bilinear networks , fused at the level of posteriors , achieves a better per in a clean speech scenario .",
    "+ the paper is organized as follows . in section [",
    "sec : avsr ] we present the ibm av - asr large vocabulary studio dataset , our feature extraction pipeline for the audio and the visual channels .",
    "next , in section  [ bimodal ] , we present results for the fusion of networks separately trained on each modality . in section [ sec : joint ]",
    "we introduce the bilinear dnn that allows for a joint training and captures correlations between the two modalities , and derive its back - propagation algorithm in section [ sec : backprop ] .",
    "finally we present posterior combination of bimodal and bilinear bimodal dnns in section [ sec : exp ] .",
    "in this section we present the ibm av - asr large vocabulary studio dataset , and our feature extraction pipeline .",
    "the ibm av - asr large vocabulary studio dataset consists of @xmath3 hours of audio - visual recordings from @xmath4 speakers .",
    "these were carried out in clean , studio conditions .",
    "the audio is sampled at @xmath5 khz along with the video frame rate of @xmath6 frames per second at @xmath7 resolution .",
    "the vocabulary size in these recordings is @xmath8 words .",
    "this data set was divided into a test set of @xmath9 hours of audio+video from @xmath10 speakers , with the rest used for training .      for the audio channel",
    "we extract 24 mfcc coefficients at 100 frames per second .",
    "nine consecutive frames of mfcc coefficients are stacked and projected to 40 dimensions using an lda matrix .",
    "input to the audio neural network is formed by concatenating @xmath11 lda frames to the central frame of interest , resulting in an audio feature vector of dimension @xmath12 .    for the visual channel we start by detecting the face in the image using the opencv implementation of the viola - jones algorithm .",
    "we then do a mouth carving by an opencv mouth detection model .",
    "both these utilize the encara2 model as described in  @xcite . in order to get an invariant representation to small distortions and scales",
    "we then extract level 1 and level 2 scattering coefficients @xcite on the @xmath13 mouth region of interest and then reduce their dimension to 60 using lda ( linear discriminant analysis ) . in order to match the audio frame rate we replicate video frames according to audio and video time stamps .",
    "we also add @xmath11 context frames to the central frame of interest , and obtain finally a visual feature vector of dimension @xmath14 .",
    "each audio+video frame is labeled with one of @xmath15 targets that represent context dependent phonemes .",
    "@xmath16 phones in phonetic context of @xmath17 are clustered using decision trees down to @xmath15 classes .",
    "we measure classification error rate at the level of these @xmath15 classes , this is referred to as phone error rate ( per ) .",
    "in the supervised multimodal scenario , we are given a training set @xmath18 of @xmath19 labeled examples , and @xmath20 classes : @xmath21 where @xmath22 correspond to the first and the second modality feature vectors , respectively .",
    "we note @xmath23 the classification targets , where @xmath24 is the canonical basis in @xmath25 .",
    "let @xmath26 be the posterior probability of being in class @xmath27 given the two modalities @xmath28 and @xmath29 . in a classification task",
    ", we would like to find the model that maximizes the cross - entropy @xmath30 : @xmath31    the first multimodal modeling approach we study is to train two separate networks @xmath32 and @xmath33 on the audio and the visual features , respectively .",
    "the networks are optimized under the cross - entropy objective   using the stochastic gradient descent .",
    "we formed a joint audio - visual feature representation by concatenating the outputs of final hidden layers of these two networks , as shown in figure  [ fig : bidnn ] .",
    "this feature space is then kept fixed while a deep or a shallow ( softmax only ) network is trained in this fused space up to the targets . to keep the feature space dimension manageable , we configure the individual audio and video networks to have a low dimensional final hidden layer .",
    "we consider for @xmath32 and @xmath33 the following architecture  @xmath34 , where @xmath35 for @xmath32 and @xmath36 for @xmath33 .",
    "the fused feature space dimension is @xmath37 .",
    "while @xmath32 achieves a per of @xmath38 , @xmath33 alone achieves a per of @xmath39 , showing that the visual information alone carries some information but that is not enough in itself to get a low error rate . a deep network built in the fused feature space results in a per of @xmath40 while a softmax layer only in this feature space yields per of @xmath1 .",
    "this substantial per gain from joint audio - visual representation , even in clean audio conditions , demonstrates the value of visual information for the phoneme classification task .",
    "interestingly , the deep and the shallow fusion are roughly on par in terms of per .",
    "results are summarized in the following table :    [ datas ]    .empirical evaluation on the av - asr studio dataset . [ cols=\"<,<,<\",options=\"header \" , ]",
    "in this paper we have studied deep multimodal learning for the task of phonetic classification from audio and visual modalities .",
    "we demonstrate that even in clean acoustic conditions using visual channel in addition to speech results in signifiantly improved classification performance .",
    "a bilinear bimodal dnn is introduced which leverages correlation between the audio and visual modalities , and leads to further error rate reduction .",
    "patrick lucey and sridha sridharan .",
    "patch - based representation of visual speech . in _ proceedings of the hcsnet workshop on use of vision in human - computer interaction - volume 56 _ , vishci 06 , pages 7985 , darlinghurst , australia , australia , 2006 .",
    "australian computer society , inc .",
    "modesto  castrillon mcastrillon , oscar deniz , daniel hernandez , and javier lorenzo . a comparison of face and facial feature detectors based on the viola ",
    "jones general object detection framework .",
    ", 22(3):481494 , 2011 .",
    "vassilis pitsikalis , athanassios katsamanis , george papandreou , and petros maragos .",
    "adaptive multimodal fusion by uncertainty compensation . in _",
    "interspeech 2006 - icslp , ninth international conference on spoken language processing , pittsburgh , pa , usa , september 17 - 21 , 2006 _ , 2006 .",
    "george papandreou , athanassios katsamanis , vassilis pitsikalis , and petros maragos .",
    "multimodal fusion and learning with uncertain features applied to audiovisual speech recognition . in _",
    "ieee 9th workshop on multimedia signal processing , mmsp 2007 , chania , crete , greece , october 1 - 3 , 2007 _ , pages 264267 , 2007 .",
    "george papandreou , athanassios katsamanis , vassilis pitsikalis , and petros maragos .",
    "adaptive multimodal fusion by uncertainty compensation with application to audiovisual speech recognition .",
    ", 17(3):423435 , 2009 ."
  ],
  "abstract_text": [
    "<S> in this paper , we present methods in deep multimodal learning for fusing speech and visual modalities for audio - visual automatic speech recognition ( av - asr ) . </S>",
    "<S> first , we study an approach where uni - modal deep networks are trained separately and their final hidden layers fused to obtain a joint feature space in which another deep network is built . </S>",
    "<S> while the audio network alone achieves a phone error rate ( per ) of @xmath0 under clean condition on the ibm large vocabulary audio - visual studio dataset , this fusion model achieves a per of @xmath1 demonstrating the tremendous value of the visual channel in phone classification even in audio with high signal to noise ratio . </S>",
    "<S> second , we present a new deep network architecture that uses a bilinear softmax layer to account for class specific correlations between modalities . we show that combining the posteriors from the bilinear networks with those from the fused model mentioned above results in a further significant phone error rate reduction , yielding a final per of @xmath2 . </S>"
  ]
}