{
  "article_text": [
    "owing to the increasing availability of high - dimensional datasets , regression models for multivariate response and high - dimensional predictors have become important tools .    in this article , we describe two procedures where a random target variable @xmath0 depends on explanatory variables or covariates within a cluster - specific regression model . each cluster is represented by a parametric conditional distribution , the entire dataset being modeled by a mixture of these distributions .",
    "it provides a rigorous statistical framework .",
    "the model assumes that each observation @xmath1 originates from one of @xmath2 disjoint classes and that the data @xmath3 are independent and identically distributed such that if @xmath4 belongs to class @xmath5 , the target variable @xmath6 results from a conditional regression model @xmath7 with an unknown matrix of coefficients @xmath8 and independent errors @xmath9 with an unknown diagonal covariance matrix @xmath10 .",
    "we work with high - dimensional dataset , in other words the number of parameters to estimate @xmath11 is larger than the number of observed target values @xmath12 .",
    "two ways are considered in this paper , coefficients sparsity and ranks sparsity .",
    "the first approach consists in estimating the matrix @xmath8 by a matrix with few nonzero coefficients .",
    "the well - known lasso estimator , introduced by @xcite for linear models , is the solution chosen here .",
    "we refer to @xcite for an overview of the lasso estimator and to @xcite for stability selection results .    in the second approach",
    ", we consider the rank sparsity in @xmath8 .",
    "this approach dates back to the @xmath13 s and was initiated by @xcite for the linear model .",
    "@xcite introduced the term of reduced - rank regression for this class of models .",
    "for more recent works , we refer to @xcite and to @xcite . nevertheless , the linear model is appropriate for homogeneous observations , which is not always the case .",
    "we extend in this paper those methods to mixture regression models .",
    "an important example of high - dimensional dataset is functional dataset .",
    "we refer to ramsay and silverman s book @xcite for an overview .",
    "moreover , a lot of recent works have been done on regression models for functional datasets : for example , we refer to @xcite for a study with scalar response and functional regressors . in this paper , we focus on the projection of functions into a well - suited wavelet basis .",
    "indeed , they handle many types of functional data , because they represent global and local attributes of functions and can deal for example with discontinuities .",
    "moreover , a large class of functions can be well represented with few nonzero coefficients for a suitable wavelet , which leads to sparse matrix of coefficients and then to sparse regression matrix .",
    "we propose here two procedures for clustering high - dimensional or functional datasets , where the high - dimension or functional random target variable @xmath0 depends on high - dimensional or functional predictors @xmath14 with a cluster - specific regression model .",
    "remark that we estimate the number of components , model parameters and cluster proportions . in the case of a large number of regressor variables , we use variable selection tools in order to detect relevant regressors . since the structure of interest",
    "may often be contained into a subset of available variables and many attributes may be useless or even harmful to detect a reasonable clustering structure , it is important to select the relevant variables .",
    "moreover , removing irrelevant variables enables us to get an easier model and can largely enhance comprehension .",
    "our two procedures are mainly based on three recent works .",
    "firstly , we refer to the article of @xcite , which studies finite mixture regression model .",
    "even if we work on a multivariate version of it , the model considered in the article of @xcite is adopted here .",
    "the second , the article of @xcite , deals with model - based clustering in density estimation .",
    "they propose a procedure , called lasso - mle procedure , which determines the number of clusters , the set of relevant variables for the clustering and a clustering of the observations , with high - dimensional data .",
    "we extend this procedure to regression models .",
    "finally , @xcite suggests a low rank estimator for the linear model . to consider the matrix structure , we develop this last approach to mixture models .",
    "we consider a finite mixture of gaussian regression models .",
    "the two procedures we propose follow the same sequence of steps .",
    "firstly , a penalized likelihood approach is considered to determine potential sets of relevant variables . introduced by @xcite , the lasso is used to select variables .",
    "varying the regularization parameter , it constructs efficiently a data - driven model collection where each model has a reasonable complexity .",
    "the second step of the procedures consists in refitting parameters by a less biased estimator , focusing on selected variables .",
    "then , we select a model among the collection using the slope heuristic , which was developed by @xcite .",
    "the difference between the two procedures is the refitting step . in the first procedure ,",
    "later called lasso - mle procedure , the maximum likelihood estimator is used/ the second procedure , called lasso - rank procedure , deals with low rank estimation . for each model in the collection , a subcollection of models with conditional means estimated by various low rank matrices is constructed .",
    "it leads to sparsity for the coefficients and for the rank , and considers the conditional mean within its matrix structure .",
    "the article is organized as follows .",
    "section [ gaussianmixtureregression ] deals with gaussian mixture regression models . the model collection that we consider is described . in section [ twoprocedures ] ,",
    "the two procedures that we propose to solve the problem of high - dimensional regression data clustering are described .",
    "section [ illustrativeexample ] presents an illustrative example , to highlight each choice done in the two procedures .",
    "section [ functionaldatasets ] states the functional data case , with a description of the projection proposed to convert these functions into coefficients data .",
    "we end this section by studying simulated and benchmark data .",
    "finally , a conclusion section ends this article .",
    "the model used is a finite gaussian mixture regression models .",
    "@xcite describe this model when the deterministic predictors @xmath15 are multivariate and the target variable @xmath16 is scalar . in this section , it is generalized to multivariate response .",
    "let us mention that this model has already been introduced , we refer for example to @xcite . in this paper , we deal with high - dimensional data .",
    "[ clustering ] we observe @xmath17 independent couples @xmath18 , which are realizations of random variables @xmath19 . here , @xmath20 are fixed or random covariates and @xmath21 is a multivariate response variable .",
    "conditionally to @xmath22 , @xmath6 is a random variable of unknown conditional density @xmath23 .",
    "the random response variable @xmath21 depends on a set of explanatory variables , written @xmath20 , through a regression - type model .",
    "if @xmath6 , conditionally to @xmath22 , originates from an individual in class @xmath24 , we assume that    @xmath25 where @xmath26 , @xmath27 is the matrix of class - specific regression coefficients and @xmath28 is diagonal , positive definite in @xmath29 .",
    "we consider in the following the gaussian regression mixture model :    * conditionally to the @xmath30 s , @xmath6 s are independent , for all @xmath31 ; * each variable @xmath6 follows a law of density @xmath32 , with @xmath33    we have denoted by @xmath34 the proportion of the class @xmath24 . for all @xmath35 , for all @xmath36 , for @xmath37 , @xmath38_m= \\sum_{j=1}^{p } [ \\mathbf{b}_{k}]_{m , j } \\xi_{j}$ ] is the @xmath39th component of the conditional mean of the @xmath24th mixture component .",
    "we prefer to work with a reparametrized version of this model whose penalized maximum likelihood estimator is scale - invariant and easier to compute .",
    "indeed , it is not equivariant under scaling of the response .",
    "more precisely , consider the transformation @xmath40 for @xmath41 which leaves the model invariant .",
    "a reasonable estimator based on transformed data @xmath42 should lead to estimator which are related to the first ones up to the homothetic transformation .",
    "this is not the case for the maximum likelihood estimator of @xmath43 and @xmath44 .",
    "secondly , the optimization of the loglikelihood is non - convex and hence , it leads to computational issues .",
    "then , we reparametrize the model described above by generalizing the reparametrization described by stdler et al .",
    "@xcite .",
    "for all @xmath35 , define new parameters @xmath45 where @xmath46 , @xmath47 is the cholesky decomposition of the positive definite matrix @xmath48 .",
    "remark that @xmath47 is a diagonal matrix of size @xmath49 .",
    "the model with its reparametrized form then equals    * conditionally to the @xmath30 s , @xmath6 s are independent , for all @xmath31 ; * each variable @xmath6 follows a law of density @xmath50 , with @xmath51    for the sample @xmath52 , the log - likelihood of this model is equal to , @xmath53 and the maximum log - likelihood estimator ( mle ) is @xmath54    since we deal with the high - dimension case ( @xmath55 ) , this estimator has to be regularized to get stable estimates .",
    "therefore , we propose the @xmath56-norm penalized mle    @xmath57    where @xmath58 where @xmath59_{m , j}|$ ] and with @xmath60 to specify .",
    "this estimator is not the usual @xmath56-estimator , called the lasso estimator , introduced by @xcite .",
    "it penalizes the @xmath56-norm of the coefficients matrices @xmath61 and small variances simultaneously , which has some close relations to the bayesian lasso ( see @xcite ) .",
    "moreover , the reparametrization allows us to consider non - standardized data .",
    "notice that we restrict ourselves in this article to diagonal covariance matrices which are dependent of the clusters .",
    "we assume that the coordinates of the @xmath6 are independent , which is a strong assumption , but it allows us to reduce easily the dimension .",
    "we refer to @xcite for different parametrization of the covariance matrix .",
    "nevertheless , because we work with high - dimensional data ( @xmath62 and @xmath49 are high and @xmath17 is small ) we prefer a parsimonious model from the diagonal family .",
    "we allow different volume clusters because they are capable to detect many clustering structures , as explained in @xcite .",
    "suppose that there is a known number of clusters @xmath24 and assume that we get , from the observations , an estimator @xmath63 such that @xmath64 well approximates the unknown conditional density @xmath65 .",
    "we look at this problem as a missing data problem : if we denote by @xmath66 the component membership , with @xmath67_1,\\ldots , [ \\boldsymbol{z}_{i}]_k)$ ] for @xmath1 is defined by    @xmath68_k= \\left\\ { \\begin{array}{ll }                       1 & \\text { if } i \\text { originates from mixture component } k ; \\\\                       0 & \\text { otherwise ; }                      \\end{array }                      \\right.\\ ] ] the complete data are @xmath69 .",
    "thanks to the estimation @xmath63 , we use the maximum a posteriori principle ( map principle ) to cluster data . specifically , for all @xmath1 , for all @xmath70 , consider    @xmath71 the posterior probability of @xmath72 with @xmath4 coming from the component @xmath24 , where @xmath73 .",
    "then , data are partitioned by the following rule :    @xmath74_k= \\left\\ { \\begin{array}{ll }                      1 & \\text { if } \\hat{\\boldsymbol{\\tau}}_{i , k}(\\hat{\\theta } ) > \\hat{\\boldsymbol{\\tau}}_{i , l}(\\hat{\\theta } ) \\text { for all } l\\neq k \\text { ; } \\\\                      0 & \\text { otherwise . }",
    "\\end{array }                      \\right.\\ ] ]      first , we introduce a generalized em algorithm to approximate @xmath75 defined in .",
    "then , we discuss initialization and stopping rules in practice , before studying convergence of this algorithm .      from an algorithmic point of view",
    ", we use a generalized em algorithm to compute the mle and the @xmath56-norm penalized mle .",
    "the em algorithm was introduced by @xcite to approximate the maximum likelihood estimator of mixture model parameters .",
    "it is an iterative process based on the minimization of the expectation of the likelihood for the complete data conditionally to the observations and to the current estimation of the parameter @xmath76 at each iteration @xmath77 . using to the karush - kuhn - tucker conditions , we extend the second step to compute the maximum likelihood estimators , penalized or not , under rank constraint or not , as it was done in the scalar case in @xcite .",
    "all those computations are available in appendix [ em ] .",
    "we therefore obtain the next updating formulae for the lasso estimator defined by .",
    "remark that it includes maximum likelihood estimator .",
    "let denote by @xmath78 the euclidean ineer product .",
    "if we denote , for all @xmath79 , for all @xmath70 , for all @xmath80 , for all @xmath81 , @xmath82^{\\text{(ite)}}_{k , m } = \\sqrt{\\boldsymbol{\\tau}^{(\\text{ite})}_{i , k } } [ \\boldsymbol{y}_i]_m;\\\\\\nonumber & [ \\boldsymbol{\\widetilde{x}}_i]^{\\text{(ite)}}_{k , j } = \\sqrt{\\boldsymbol{\\tau}^{(\\text{ite})}_{i , k } } [ \\boldsymbol{x}_i]_j ; \\\\\\nonumber & \\delta_{k , m } = \\left ( -n_k \\langle [ \\mathbf{\\boldsymbol{\\widetilde{y}}}_.]^{\\text{(ite)}}_{k , m},[\\boldsymbol{\\phi}_k]^{\\text{(ite)}}_{m , . }",
    "[ \\mathbf{\\boldsymbol{\\widetilde{x}}}_.]^{\\text{(ite)}}_{k , . }",
    "\\rangle \\right)^2 - 4   ||[\\mathbf{\\boldsymbol{\\widetilde{y}}}_.]^{\\text{(ite)}}_{k , m}|| _ 2 ^ 2 ; \\\\ \\label{s } & [ \\boldsymbol{s}_{k}]_{j , m}^{(\\text{ite})}=-\\sum_{i=1}^{n } [ \\boldsymbol{\\widetilde{x}}_i]^{\\text{(ite)}}_{k , j } [ \\boldsymbol{p}_k]^{(\\text{ite})}_{m , m } [ \\boldsymbol{\\widetilde{y}}_i]^{\\text{(ite)}}_{k , m } + \\sum_{\\genfrac{}{}{0pt}{}{j_2=1 } { j_2\\neq j}}^{p } [ \\boldsymbol{\\widetilde{x}}_i]^{\\text{(ite)}}_{k , j } [ \\boldsymbol{\\widetilde{x}}_i]^{\\text{(ite)}}_{k , j_2 } [ \\boldsymbol{\\phi}_k]^{(\\text{ite})}_{m , j_2 } ; \\\\ \\nonumber & n_{k } = \\sum_{i=1}^{n } \\boldsymbol{\\tau}_{i , k}^{(\\text{ite } ) } ; \\\\ \\nonumber\\end{aligned}\\ ] ] and if we denote by @xmath83 $ ] the largest value in @xmath84 such that the update of @xmath85 leads to improve the expected complete penalized log likelihood , we update the parameters by @xmath86_{m_1,m_2}^{\\text{(ite+1 ) } } & = & \\left\\ { \\begin{array}{lll }       & \\frac{n_k \\langle [ \\mathbf{\\boldsymbol{\\widetilde{y}}}_.]^{\\text{(ite)}}_{k , m } , [ \\boldsymbol{\\phi}_k]_{m,.}^{(\\text{ite } ) }   [ \\mathbf{\\boldsymbol{\\widetilde{x}}}_.]^{\\text{(ite)}}_{k , . }",
    "\\rangle + \\sqrt{\\delta}_{k , m}}{2 n_k ||[\\mathbf{\\boldsymbol{\\widetilde{y}}}_.]^{\\text{(ite)}}_{k , m}||_2 ^ 2 } & \\text { if } m_1=m_2=m;\\\\    & 0 & \\text { elsewhere;}\\\\                                                        \\end{array }                                  \\right.\\\\ \\label{phi } [ \\boldsymbol{\\phi}_k]_{m , j}^{\\text{(ite+1 ) } } & = & \\left\\ { \\begin{array}{lll }              & \\frac{-[\\boldsymbol{s}_{k}]^{(\\text{ite})}_{j , m}+ n\\lambda \\pi^{(\\text{ite})}_{k } } { ||[\\mathbf{\\boldsymbol{\\widetilde{x}}}_.]^{\\text{(ite)}}_{k , j}||_{2}^2 } & \\text { if }   [ \\boldsymbol{s}_{k}]^{(\\text{ite})}_{j , m}>n \\lambda \\pi^{(\\text{ite})}_{k } ; \\\\              & -\\frac{[\\boldsymbol{s}_{k}]^{(\\text{ite})}_{j , m}+ n\\lambda \\pi^{(\\text{ite})}_{k } } { ||[\\mathbf{\\boldsymbol{\\widetilde{x}}}_.]^{\\text{(ite)}}_{k , j}||_{2}^2 } & \\text { if } [ \\boldsymbol{s}_{k}]^{(\\text{ite})}_{j , m } < -n \\lambda \\pi_{k}^{(\\text{ite } ) } ; \\\\              & 0 & \\text { else ; }                                  \\end{array }                                  \\right.\\end{aligned}\\ ] ]    in our case , the em algorithm corresponds to alternate between the e - step which corresponds to the computation of and the m - step , which corresponds to the computation of , and .",
    "remark that to approximate the mle under rank constraint , we use an easier em algorithm which is described in details in appendix . in the e - step ,",
    "we compute the a posteriori probability of each observation to belong to each cluster according to the current estimations . in the m - step ,",
    "we consider each observation within its estimated cluster , and consider the linear regression estimators under rank constraint by keeping the biggest singular values .",
    "we initialize the clustering process with the @xmath24-means algorithm on couples @xmath87 , for all @xmath1 .",
    "according to this clustering , we compute the linear regression estimators in each class . then , we run a small number of times the em - algorithm , repeat this initialization many times and keep the one which corresponds to the highest log - likelihood .",
    "to stop the algorithm , we propose to run it a minimum number of times , and to specify a maximum number of iterations to make sure that it will stop .    between these two bounds ,",
    "we stop if a relative criteria on the log - likelihood is small enough and if a relative criteria on the parameters is small enough too .",
    "those criteria are adapted from @xcite .    by those rules ,",
    "we are trying to attain a local optimum as close to a global optimum of the likelihood function as possible .",
    "we address here the convergence properties of the algorithm described in paragraph [ algogem ] .",
    "although convergence to stationary points has been get for the em algorithm ( see @xcite ) , it is not true without conditions hard to verify for generalized em algorithm .",
    "results we propose are quite similar to the one get by @xcite , because algorithms are similar .",
    "we refer the interested reader to this article for proof of the following results , when @xmath88 .",
    "same ideas are working for any values of @xmath49 .",
    "[ prop1 ] the algorithm described in paragraph [ algogem ] has the descent property : @xmath89    the proposition [ prop1 ] is clear by construction of @xmath90 , the m - step of the algorithm as a coordinate - wise minimization .",
    "[ prop2 ] assume that @xmath91 for all @xmath1 .",
    "then , for @xmath92 , the function @xmath93 is bounded from above for all values @xmath94 .",
    "this is true because we penalize the log - likelihood by @xmath95 , where @xmath96 , then small variances are penalized also .",
    "the penalized criteria therefore stays finite whenever @xmath10 , @xmath35 .",
    "this is not the case for the unpenalized , which is unbounded if the variance of a variable tends to @xmath97 .",
    "we refer for example to @xcite .",
    "[ cor ] for the algorithm described in paragraph [ algogem ] , @xmath98 decreases monotonically to some value @xmath99 .",
    "according to this corollary , we know that the algorithm converges to some finite value , depending on initial values .    [ thm ] for the algorithm described in paragraph [ algogem ] , every cluster point @xmath100 of the sequence @xmath101 , generated by the algorithm , is a stationary point of the function @xmath102 .",
    "this theorem proves that the algorithm converges to a stationary point .",
    "we refer to @xcite for the definition of a stationary point for non - differentiable functions .",
    "corollary [ cor ] is deduced from proposition [ prop1 ] and proposition [ prop2 ] .",
    "theorem [ thm ] is more difficult to prove .",
    "we deal with high - dimensional data where we observe a sample of small size @xmath17 and we have to estimate many coefficients ( @xmath103 ) .",
    "then , we have to focus on variables that are relevant for the clustering .",
    "the notion of irrelevant indices has to be defined .",
    "a couple @xmath104 is said to be _ irrelevant _ for the clustering if @xmath105_{m , j } = \\ldots = [ \\boldsymbol{\\phi}_{k}]_{m , j}=0 $ ] , which means that the variable @xmath106 does not explain the variable @xmath107 for the clustering process .",
    "we also say that the indices @xmath108 is irrelevant if the couple @xmath104 is irrelevant .",
    "a _ relevant _ couple is a couple which is not irrelevant : at least in one cluster @xmath24 , the coefficient @xmath109_{m , j}$ ] is not equal to zero .",
    "we denote by @xmath110 the indices set of relevant couples .",
    "remark that @xmath111 .",
    "we denote by @xmath112 the complement of @xmath110 in @xmath113 . for all @xmath35 ,",
    "we denote by @xmath114}$ ] the matrix of size @xmath115 with @xmath97 on the set @xmath112 .",
    "be also define by @xmath116 the model with @xmath2 components and with @xmath110 for indices set of relevant couples :    @xmath117 } \\boldsymbol{x } ) ^t(\\boldsymbol{p}_k\\boldsymbol{y}-\\boldsymbol{\\phi}_k^{[j ] } \\boldsymbol{x } ) } { 2 } \\right ) , \\right.\\\\ & \\hspace{1cm}\\left .",
    "\\theta=(\\pi_1,\\ldots , \\pi_k,\\boldsymbol{\\phi}_1^{[j]},\\ldots , \\boldsymbol{\\phi}_k^{[j ] } , \\boldsymbol{p}_1,\\ldots,\\boldsymbol{p}_k ) \\in \\theta_{(k , j ) } = \\pi_k \\times \\left ( \\mathbb{r}^{q \\times p } \\right)^k \\times \\left(\\mathbb{r}_+^{q } \\right)^k \\right\\}. \\nonumber   \\end{aligned}\\ ] ]    we construct a model collection by varying the number of components @xmath2 and the indices set of relevant couples @xmath110 .",
    "the subset @xmath110 is constructed with the lasso estimator defined in .",
    "nevertheless , to be consistent with the definition of relevant couples , the group - lasso estimator could be preferred , where coefficients @xmath118_{m , j},\\ldots,\\boldsymbol{\\phi}_{k}]_{m , j}\\}$ ] are groupped .",
    "we focus here on the lasso estimator , but the group - lasso approach is described in appendix .",
    "it gives mainly the same results , but the lasso estimator leads to consider also isolated irrelevant couples .",
    "the goal of our procedures is , given a sample @xmath119 , to discover the relation between the variable @xmath120 and the variable @xmath121 .",
    "thus , we have to estimate , according to the representation of @xmath116 , the number of clusters @xmath2 , the relevant variables set @xmath110 , and the parameter @xmath122 . to overcome this difficulty , we want to take advantage of the sparsity property of the @xmath56-penalization to perform automatic variable selection in clustering high - dimensional data .",
    "then , we compute another estimator restricted on relevant variables , which will work better because it is no longer an high - dimensional issue .",
    "thus , we avoid shrinkage problems due to the lasso estimator .",
    "the first procedure takes advantage of the maximum likelihood estimator , whereas the second one takes into account the matrix structure of @xmath123 with a low rank estimation .",
    "this procedure is decomposed into three main steps : we construct a model collection , then in each model we compute the maximum likelihood estimator and finally we select the best one among the model collection .",
    "the first step consists of constructing a model collection @xmath124 in which @xmath116 is defined by equation , and the model collection is indexed by @xmath125 .",
    "we denote by @xmath126 the possible number of components .",
    "we assume that we could bound @xmath127 without loss of generality .",
    "we also note @xmath128 .    to detect the relevant variables and construct the set @xmath129",
    ", we penalize the log - likelihood by an @xmath56-penalty on the mean parameters proportional to @xmath59_{m , j}|$ ] . in @xmath56-procedures ,",
    "the choice of the regularization parameters is often difficult : fixing the number of components @xmath130 , we propose to construct a data - driven grid @xmath131 of regularization parameters by using the updating formulae of the mixture parameters in the em algorithm",
    ". we can give a formula for @xmath132 , the regularization parameter , depending on which coefficients we want to shrink to zero , for all @xmath133 : @xmath134_{m , j } = 0 \\hspace{0.5 cm }   \\leftrightarrow & \\hspace{0.5 cm }   [ \\boldsymbol{\\lambda}_{k}]_{j , m}= \\frac{|[\\boldsymbol{s}_{k}]_{j , m}|}{n \\pi_{k } } ; \\end{aligned}\\ ] ] where @xmath135_{j , m}$ ] is defined by .",
    "then , we define the data - driven grid by @xmath136_{j , m } , k\\in \\{1,\\ldots , k\\ } , j \\in \\{1,\\ldots , p\\ } , m \\in \\{1,\\ldots , q\\}\\right\\}. \\label{grid}\\end{aligned}\\ ] ] we could compute it from maximum likelihood estimations .",
    "then , for each @xmath137 , we could compute the lasso estimator defined by @xmath138 for a fixed number of mixture components @xmath130 and a regularization parameter @xmath137 , we could use an em algorithm , recalled in appendix [ em ] , to approximate this estimator . then , for each @xmath130 and for each @xmath137 , we could construct the relevant variables set @xmath139 .",
    "we denote by @xmath140 the collection of these sets .",
    "the second step consists of approximating the mle @xmath141 using the em algorithm for each model @xmath142 .",
    "the third step is devoted to model selection . rather than selecting the regularization parameter",
    ", we select the refitted model . instead of using an asymptotic criterion , as bic or aic",
    ", we use the slope heuristic described in @xcite , which is a non - asymptotic criterion for selecting a model among a model collection .",
    "let us explain briefly how it works .",
    "firstly , models are grouping according to their dimension @xmath143 , to obtain a model collection @xmath144 .",
    "the dimension of a model is the number of parameters estimated in the model . for each dimension",
    "@xmath143 , let @xmath145 be the estimator maximizing the likelihood among the estimators associated to a model of dimension @xmath143 . also , the function @xmath146 has a linear behavior for large dimensions .",
    "we estimate the slope , denoted by @xmath147 , which will be used to calibrate the penalty .",
    "the minimizer @xmath148 of the penalized criterion @xmath149 is determined , and the model selected is @xmath150 .",
    "remark that @xmath151 .",
    "note that the model is selected after refitting , which avoids issue of regularization parameter selection . for an oracle inequality to justify the slope heuristic used here , see @xcite .",
    "whereas the previous procedure does not take into account the multivariate structure , we propose a second procedure to perform this point .",
    "for each model belonging to the collection @xmath116 , a subcollection is constructed , varying the rank of @xmath123 .",
    "let us describe this procedure .    as in the lasso - mle procedure",
    ", we first construct a collection of models , thanks to the @xmath56-approach . for @xmath152",
    ", we obtain an estimator for @xmath122 , denoted by @xmath153 , for each model belonging to the collection .",
    "we could deduce the set of relevant columns , denoted by @xmath139 and this for all @xmath130 : we deduce @xmath140 the collection of relevant variables set .",
    "the second step consists to construct a subcollection of models with rank sparsity , denoted by @xmath154 the model @xmath155 has @xmath2 components , the set @xmath110 for active variables and @xmath156 is the vector of the ranks of the matrix of regression coefficients in each group :    @xmath157    where @xmath158 } \\boldsymbol{x } ) ^t(\\boldsymbol{p}_k\\boldsymbol{y}-(\\boldsymbol{\\phi}_k^{r_k})^{[j ] } \\boldsymbol{x } ) } { 2 } \\right ) ; \\\\",
    "\\theta&=(\\pi_1,\\ldots , \\pi_k,(\\boldsymbol{\\phi}_1^{r_1})^{[j]},\\ldots , ( \\boldsymbol{\\phi}_k^{r_k}))^{[j ] } , \\boldsymbol{p}_1,\\ldots,\\boldsymbol{p}_k ) \\in \\theta_{(k , j , r)}\\\\   \\theta_{(k , j , r ) } & = \\pi_k \\times \\psi_{(k , j , r ) } \\times \\left(\\mathbb{r}_+^{q } \\right)^k ; \\\\",
    "\\psi_{(k , j , r ) } & = \\left\\{\\left .",
    "( ( \\boldsymbol{\\phi}_1^{r_1})^{[j]},\\ldots , ( \\boldsymbol{\\phi}_k^{r_k})^{[j ] } ) \\in \\left(\\mathbb{r}^{q \\times p } \\right)^k \\right| \\text{rank}(\\boldsymbol{\\phi}_k)=r_k \\text { for all } k \\in \\{1,\\ldots , k\\ } \\right\\ } ; \\end{aligned}\\ ] ]    and @xmath159 .",
    "we denote by @xmath160 the possible number of components , @xmath140 a collection of subsets of @xmath161 and @xmath162 the set of vectors of size @xmath130 with rank values for each mean matrix .",
    "we compute the mle under the constrained ranks thanks to an em algorithm .",
    "indeed , we constrain the estimation of @xmath61 , for the cluster @xmath24 , to have a rank equals to @xmath163 , by keeping only the @xmath163 largest singular values . more details are given in section [ em2 ] .",
    "it leads to an estimator of the mean with row sparsity and low rank for each model .",
    "as described in the above section , a model is selected using the slope heuristic .",
    "this step is justified theoretically in @xcite .",
    "those two procedures have both been implemented in matlab , with the help of benjamin auder and the matlab code is available .",
    "we run the em algorithm several times : once to construct the regularization grid , twice for each regularization parameter for the lasso - mle procedure ( once to approximate the lasso estimator , and once to refit it with the maximum likelihood estimator ) and more times for the lasso - rank procedure ( we vary also the ranks vector ) .",
    "if we look at every regularization parameters in the grid defined in , there are @xmath164 values and then we compute the em algorithm @xmath165 times , which could be large .",
    "even if each em algorithm is fast ( implemented with c ) , repeat it numerous times could be time - consuming .",
    "we propose to the user to select relevant regularization parameters : either a regular subcollection of @xmath131 , to get various sparsities , or focus on the large values of regularization parameters , to get sparse solutions .",
    "the model collection constructed by the lasso - mle procedure is included in the model collection constructed by the lasso - rank procedure : @xmath166 then , the second procedure will work better .",
    "nevertheless , it is time - consuming to construct all those models , with various rank vectors and maybe it is not necessary , depending on the data .",
    "it is known that the framework of low - rank arises in many applications , among which analysis of eeg data decoding @xcite , neural response modeling @xcite . however , in some data the low - rank structure could be not adapted and then not needed .",
    "we illustrate our procedures on four different simulated dataset , adapted from @xcite .",
    "firstly , we present the models used in these simulations . then , we validate numerically each step and we finally compare the results of our procedures with others . remark that we propose here some examples to illustrate our methods , but not a complete analysis .",
    "we highlight some issues which seem important .",
    "moreover , we do not illustrate the one - component case , focusing on the clustering .",
    "if , on some dataset , we are not convinced by the clustering , we could add to the model collection models with one component , more or less sparse , using the same pattern ( computing the lasso estimator to get the relevant variables for various regularization parameters and refitting parameters with the maximum likelihood estimator , under constrained ranks or not ) and then we select a model among this collection of linear and mixture models .",
    "let @xmath167 be a sample of size @xmath17 distributed according to multivariate standard gaussian .",
    "we consider a mixture of two components . besides , we fix the number of active variables to @xmath168 in each cluster .",
    "more precisely , the first four variables of @xmath120 are explained respectively by the four first variables of @xmath121 .",
    "fix @xmath169 and @xmath170 for all @xmath171 , where @xmath172 denotes the identity matrix of size @xmath49 .",
    "the difficulty of the clustering is partially controlled by the signal - to - noise ratio ( denoted snr ) . in this context",
    ", we could extend the natural idea of the snr with the following definition , where @xmath173 denotes the trace of the matrix @xmath174 .",
    "@xmath175 remark that it only controls the distance between the signal with or without the noise and not the distance between the both signals .",
    "we compute four different models , varying @xmath17 , the snr and the distance between the clusters .",
    "details are available in the table [ tablerecap ] .",
    ".description of the different models .",
    "[ cols=\"^,^,^,^,^,^\",options=\"header \" , ]     because the models are constructed on the learning sample , the mape are lower than for the test sample .",
    "nevertheless , results are similar , saying that models are well constructed .",
    "this is particularly the case for the model 1 , which is more consistent over a new sample .    to conclude this study",
    ", we could highlight the advantages of our procedure on these data .",
    "it provides a clustering of data , similar to the one done with supervised clustering in @xcite , but we could explain how this clustering is done .",
    "this work has been done with the lasso - mle procedure",
    ". however , the same kind of results have been get with the lasso - rank procedure .",
    "in this article , two procedures are proposed to cluster regression data . detecting the relevant clustering variables , they are especially designed for high - dimensional dataset .",
    "we use an @xmath56-regularization procedure to select variables and then deduce a reasonable random model collection .",
    "thus , we recast estimations of parameters of these models into a general model selection problem . these procedures are compared with usual criteria on simulated data : the bic criterion used to select a model , the maximum - likelihood estimator and the oracle when we know it . in addition , we compare our procedures to others on benchmark data .",
    "one main asset of those procedures is that it can be applied to functional dataset .",
    "we also develop this point of view .",
    "in those appendices , we develop computing for em algorithm updating formulae in section [ em ] , for lasso and maximum likelihood estimators and for low ranks estimators . in section [ grouplasso ] ,",
    "we extend our procedures with the group - lasso estimator to select relevant variables , rather than using the lasso estimator",
    ".        introduced by @xcite , the em ( expectation - maximization ) algorithm is used to compute maximum likelihood estimators , penalized or not .",
    "the expected complete negative log - likelihood is denoted by @xmath176 in which @xmath177_k \\log\\left ( \\frac{\\det(\\boldsymbol{p}_k)}{(2 \\pi)^{q/2 } } \\exp \\left(- \\frac{1}{2 } ( \\boldsymbol{p}_k \\boldsymbol{y}_i - \\boldsymbol{x}_i\\boldsymbol{\\phi}_k)^t ( \\boldsymbol{p}_k \\boldsymbol{y}_i - \\boldsymbol{x}_i\\boldsymbol{\\phi}_k ) \\right ) \\right)\\\\ & + [ \\boldsymbol{z}_i]_k \\log(\\pi_{k } ) ; \\end{aligned}\\ ] ]    with @xmath178_k$ ] are independent and identically distributed unobserved multinomial variables , showing the component - membership of the @xmath179 observation in the finite mixture regression model .",
    "the expected complete penalized negative log - likelihood is @xmath180    [ [ calculus - for - updating - formula ] ] calculus for updating formula + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    * e - step : compute @xmath181 , or , equivalently , compute for @xmath35 , @xmath1 , @xmath182_k|\\textbf{y } ) \\\\ & = \\frac{\\pi_{k}^{(\\text{ite } ) } \\det \\boldsymbol{p}_k^{(\\text{ite } ) } \\exp \\left ( -\\frac{1}{2}\\left ( \\boldsymbol{p}_k^{(\\text{ite } ) } \\boldsymbol{y}_i-\\boldsymbol{x}_i \\boldsymbol{\\phi}_k^{(\\text{ite})}\\right)^t\\left(\\boldsymbol{p}_k^{(\\text{ite } ) } \\boldsymbol{y}_i-\\boldsymbol{x}_i \\boldsymbol{\\phi}_k^{(\\text{ite})}\\right ) \\right ) } { \\sum_{r=1}^{k}\\pi_{k}^{(\\text{ite})}\\det \\boldsymbol{p}_k^{(\\text{ite } ) } \\exp \\left ( -\\frac{1}{2}\\left(\\boldsymbol{p}_k^{(\\text{ite } ) } \\boldsymbol{y}_i-\\boldsymbol{x}_i \\boldsymbol{\\phi}_k^{(\\text{ite})}\\right)^t\\left(\\boldsymbol{p}_k^{(\\text{ite } ) } \\boldsymbol{y}_i-\\boldsymbol{x}_i \\boldsymbol{\\phi}_k^{(\\text{ite})}\\right ) \\right ) }     \\end{aligned}\\ ] ] this formula updates the clustering , thanks to the map principle . *",
    "m - step : improve @xmath183 .",
    "+ for this , rewrite the karush - kuhn - tucker conditions .",
    "we have @xmath184_k \\log \\left ( \\frac{\\det(\\boldsymbol{p}_k)}{(2 \\pi)^{q/2 } } \\exp \\left ( -\\frac{1}{2 } ( \\boldsymbol{p}_k \\boldsymbol{y}_i - \\boldsymbol{x}_i \\boldsymbol{\\phi}_k)^t(\\boldsymbol{p}_k \\boldsymbol{y}_i - \\boldsymbol{x}_i \\boldsymbol{\\phi}_k ) \\right ) \\right ) \\left| \\textbf{y } \\right .",
    "\\right ) \\nonumber\\\\ & - \\frac{1}{n } \\sum_{i=1}^{n } \\sum_{k=1}^{k } e_{\\theta^{(\\text{ite } ) } } \\left ( [ \\boldsymbol{z}_i]_k \\log \\pi_{k } |\\textbf{y } \\right ) + \\lambda \\sum_{k=1}^{k } \\pi_{k } ||\\boldsymbol{\\phi}_k||_1   \\nonumber\\\\ = & - \\frac{1}{n } \\sum_{i=1}^{n } \\sum_{k=1}^{k } -\\frac{1}{2 } ( \\boldsymbol{p}_k \\boldsymbol{y}_i - \\boldsymbol{x}_i \\boldsymbol{\\phi}_k)^t ( \\boldsymbol{p}_k \\boldsymbol{y}_i - \\boldsymbol{x}_i \\boldsymbol{\\phi}_k)e_{\\theta^{(\\text{ite } ) } } \\left ( [ \\boldsymbol{z}_i]_k |\\textbf{y } \\right)\\nonumber\\\\ & - \\frac{1}{n } \\sum_{i=1}^{n } \\sum_{k=1}^{k }   \\sum_{m=1}^q \\log \\left ( \\frac{[\\boldsymbol{p}_k]_{m , m}}{\\sqrt{2 \\pi } } \\right ) e_{\\theta^{(\\text{ite } ) } } \\left [ [ \\boldsymbol{z}_i]_k |\\textbf{y } \\right ] \\nonumber\\\\ & - \\frac{1}{n } \\sum_{i=1}^{n } \\sum_{k=1}^{k } e_{\\theta^{(\\text{ite } ) } } \\left ( [ \\boldsymbol{z}_i]_k   |\\textbf{y } \\right)\\log \\pi_{k } + \\lambda \\sum_{k=1}^{k } \\pi_{k } ||\\boldsymbol{\\phi}_k||_1 .",
    "\\end{aligned}\\ ] ] firstly , we optimize this formula with respect to @xmath185 : it is equivalent to optimize @xmath186 + we obtain @xmath187 with @xmath188 $ ] being the largest value in the grid @xmath189 , with @xmath190 , such that the function is not increasing . + to optimize with respect to @xmath191 , we could rewrite the expression : it is similar to the optimization of @xmath192_{m , m } ) - \\frac{1}{2 } ( \\boldsymbol{p}_k \\boldsymbol{\\widetilde{y}}_i - \\boldsymbol{\\widetilde{x}}_i \\boldsymbol{\\phi}_k)^t(\\boldsymbol{p}_k \\boldsymbol{\\widetilde{y}}_i - \\boldsymbol{\\widetilde{x}}_i \\boldsymbol{\\phi}_k ) \\right ) + \\lambda \\pi_{k } ||\\boldsymbol{\\phi}_k||_1\\ ] ] for all @xmath35 , which is equivalent to the optimization of + @xmath193_{m , m } ) + \\frac{1}{2n } \\sum_{i=1}^n \\sum_{m=1}^q   \\left ( [ \\boldsymbol{p}_k]_{m , m } [ \\boldsymbol{\\widetilde{y}}_i]_{k , m } - [ \\boldsymbol{\\phi}_k]_{m , . }",
    "[ \\boldsymbol{\\widetilde{x}}_i]_{k , . }",
    "\\right ) ^2 + \\lambda \\pi_{k } ||\\boldsymbol{\\phi}_k||_1 ; \\ ] ] where @xmath194 .",
    "the minimum in @xmath195_{m , m}$ ] is the function which vanishes its partial derivative with respect to @xmath195_{m , m}$ ] : @xmath196_{m , m } } + \\frac{1}{2n }   \\sum_{i=1}^n 2 [ \\boldsymbol{\\widetilde{y}}_i]_{k , m } \\left ( [ \\boldsymbol{p}_k]_{m , m } [ \\boldsymbol{\\widetilde{y}}_i]_{k , m } - [ \\boldsymbol{\\phi}_k]_{m , . } [ \\boldsymbol{\\widetilde{x}}_i]_{k,.}\\right)=0\\ ] ] for all @xmath35 , for all @xmath36 , which is equivalent to + @xmath197_{m , m}^2 \\sum_{i=1}^n [ \\boldsymbol{\\widetilde{y}}_i]_{k , m}^2 - \\frac{1}{n_k } [ \\boldsymbol{p}_k]_{m , m } \\sum_{i=1}^n [ \\boldsymbol{\\widetilde{y}}_{i}]_{k , m } [ \\boldsymbol{\\phi}_k]_{m , . } [ \\boldsymbol{\\widetilde{x}}_i]_{k,.}&=0\\\\ \\leftrightarrow -1 + [ \\boldsymbol{p}_k]_{m , m}^2 \\frac{1}{n_k } || [ \\mathbf{\\boldsymbol{\\widetilde{y}}}]_{k , m } ||_2 ^ 2 - [ \\boldsymbol{p}_k]_{m , m}\\frac{1}{n_k } \\langle[\\widetilde{\\textbf{y}}]_{k , m } , [ \\boldsymbol{\\phi}_k]_{m , . }",
    "[ \\widetilde{\\textbf{x}}]_{k , . } \\rangle&=0.\\end{aligned}\\ ] ] + the discriminant is @xmath198_{k , m},[\\boldsymbol{\\phi}_k]_{m , . }",
    "[ \\widetilde{\\textbf{x}}]_{k , . }",
    "\\rangle \\right)^2 - \\frac{4}{n_k }   ||[\\mathbf{\\boldsymbol{\\widetilde{y}}}]_{k , m}||_2 ^ 2.\\ ] ] then , for all @xmath35 , for all @xmath36 , @xmath199_{m , m}= \\frac{n_k \\langle [ \\widetilde{\\textbf{y}}]_{k , m } , [ \\boldsymbol{\\phi}_k]_{m , . }",
    "[ \\widetilde{\\textbf{x}}]_{k , . } \\rangle + \\sqrt{\\delta}}{2 n_k",
    "||[\\mathbf{\\boldsymbol{\\widetilde{y}}}]_{k , m}||_2 ^ 2}.\\ ] ] + we could also look at the equation as a function of the variable @xmath123 : according to the partial derivative with respect to @xmath109_{m , j}$ ] , we obtain for all @xmath36 , for all @xmath35 , for all @xmath79 , @xmath200_{k , j } \\left([\\boldsymbol{p}_k]_{m , m } [ \\boldsymbol{\\widetilde{y}}_i]_{k ,",
    "m } - \\sum_{j_2 = 1}^{p } [ \\boldsymbol{\\widetilde{x}}_i]_{k , j_2 } [ \\boldsymbol{\\phi}_k]_{m , j_2}\\right ) -n\\lambda \\pi_{k } \\text{sgn}([\\boldsymbol{\\phi}_k]_{m , j})=0,\\ ] ] where @xmath201 is the sign function . then , for all @xmath202 , @xmath203_{m , j } = \\frac{\\sum_{i=1}^{n } [ \\boldsymbol{\\widetilde{x}}_i]_{k , j } [ \\boldsymbol{p}_k]_{m , m } [ \\boldsymbol{\\widetilde{y}}_i]_{k , m } - \\sum_{\\genfrac{}{}{0pt}{}{j_2=1}{j_2\\neq j}}^{p } [ \\boldsymbol{\\widetilde{x}}_i]_{k , j } [ \\boldsymbol{\\widetilde{x}}_i]_{k , j_2 } [ \\boldsymbol{\\phi}_k]_{m , j_2 } - n\\lambda \\pi_{k } \\text{sgn}([\\boldsymbol{\\phi}_k]_{j , m})}{||[\\mathbf{\\boldsymbol{\\widetilde{x}}}]_{k , j}||_{2}^2}.\\ ] ] + let , for all @xmath202 , @xmath204_{j , m}=-\\sum_{i=1}^{n } [ \\boldsymbol{\\widetilde{x}}_i]_{k , j } [ \\boldsymbol{p}_k]_{m , m } [ \\boldsymbol{\\widetilde{y}}_i]_{k , m } + \\sum_{\\genfrac{}{}{0pt}{}{j_2=1 } { j_2\\neq j}}^{p } [ \\boldsymbol{\\widetilde{x}}_i]_{k , j } [ \\boldsymbol{\\widetilde{x}}_i]_{k , j_2 } [ \\boldsymbol{\\phi}_k]_{m , j_2}.\\ ] ] then @xmath205_{m , j}&= \\frac{-[\\boldsymbol{s}_{k}]_{j , m}- n\\lambda \\pi_{k } \\text{sgn}([\\boldsymbol{\\phi}_k]_{m , j})}{||[\\mathbf{\\boldsymbol{\\widetilde{x}}}]_{k , j}||_{2}^2 } \\\\ & = \\left\\ { \\begin{array}{lll }              & \\frac{-[\\boldsymbol{s}_{k}]_{j , m}+ n\\lambda \\pi_{k } } { ||[\\mathbf{\\boldsymbol{\\widetilde{x}}}]_{k , j}||_{2}^2 } & \\text{if }   [ \\boldsymbol{s}_{k}]_{j , m}>n \\lambda \\pi_{k}\\\\              & -\\frac{[\\boldsymbol{s}_{k}]_{j , m}+ n\\lambda \\pi_{k}}{||[\\mathbf{\\boldsymbol{\\widetilde{x}}}]_{k , j}||_{2}^2 } & \\text{if } [ \\boldsymbol{s}_{k}]_{j , m } < -n \\lambda \\pi_{k}\\\\              & 0 & \\text{elsewhere . }",
    "\\end{array } \\right.\\end{aligned}\\ ] ]    from these equalities , we could write the updating formulae . for @xmath206 , @xmath207 ,",
    "let @xmath208_{j , m}^{(\\text{ite})}=-\\sum_{i=1}^{n } [ \\boldsymbol{\\widetilde{x}}_i]_{k , j } [ \\boldsymbol{p}_k]^{(\\text{ite})}_{m , m } [ \\boldsymbol{\\widetilde{y}}_i]_{k , m } +   \\sum_{\\genfrac{}{}{0pt}{}{j_2=1 } { j_2\\neq j}}^{p } [ \\boldsymbol{\\widetilde{x}}_i]_{k , j } [ \\boldsymbol{\\widetilde{x}}_i]_{k , j_2 } [ \\boldsymbol{\\phi}_k]^{(\\text{ite})}_{m , j_2 } ; \\\\ &",
    "n_{k } = \\sum_{i=1}^{n } \\boldsymbol{\\tau}_{i , k } ; \\\\ & ( [ \\boldsymbol{\\widetilde{y}}_{i}]_{k,.},[\\boldsymbol{\\widetilde{x}}_i]_{k , . } ) = \\sqrt{\\boldsymbol{\\tau}_{i , k } } ( \\boldsymbol{y}_i,\\boldsymbol{x}_i).\\end{aligned}\\ ] ]      to take into account the matrix structure , we want to make a dimension reduction on the rank of the mean matrix .",
    "if we know to which cluster each sample belongs , we could compute the low rank estimator for linear model in each component .",
    "indeed , an estimator of fixed rank @xmath209 is known in the linear regression case : denoting @xmath210 the moore - penrose pseudo - inverse of @xmath174 and @xmath211_r = u d_r v^t$ ] in which @xmath212 is obtained from @xmath143 by setting @xmath213 for @xmath214 , with @xmath215 the singular decomposition of @xmath174 , if @xmath216 , an estimator of @xmath217 with rank @xmath209 is @xmath218_r$ ] .",
    "we do not know the clustering of the sample , but the e - step in the em algorithm computes it .    [ em2 ] we suppose in this case that @xmath28 and @xmath34 are known , for all @xmath35 .",
    "we use this algorithm to determine @xmath61 , for all @xmath35 , with ranks fixed to @xmath219 .    *",
    "e - step : compute for @xmath35 , @xmath1 , @xmath220_k|y ) \\\\ & = \\frac{\\pi_{k}^{(\\text{ite } ) } \\det \\boldsymbol{p}_k^{(\\text{ite } ) } \\exp \\left ( -\\frac{1}{2}\\left ( \\boldsymbol{p}_k^{(\\text{ite } ) } \\boldsymbol{y}_i-\\boldsymbol{x}_i \\boldsymbol{\\phi}_k^{(\\text{ite})}\\right)^t\\left(\\boldsymbol{p}_k^{(\\text{ite } ) } \\boldsymbol{y}_i-\\boldsymbol{x}_i \\boldsymbol{\\phi}_k^{(\\text{ite})}\\right )   \\right)}{\\sum_{r=1}^{k}\\pi_{k}^{(\\text{ite})}\\det \\boldsymbol{p}_k^{(\\text{ite } ) } \\exp \\left ( -\\frac{1}{2}\\left(\\boldsymbol{p}_k^{(\\text{ite } ) } \\boldsymbol{y}_i-\\boldsymbol{x}_i \\boldsymbol{\\phi}_k^{(\\text{ite})}\\right)^t \\left(\\boldsymbol{p}_k^{(\\text{ite } ) } \\boldsymbol{y}_i-\\boldsymbol{x}_i \\boldsymbol{\\phi}_k^{(\\text{ite})}\\right ) \\right ) }     \\end{aligned}\\ ] ] * m - step : assign each observation to its estimated cluster , by the map principle applied thanks to the e - step .",
    "we say that @xmath221 comes from component number @xmath222 .",
    "then , we can define @xmath223 , in which @xmath224 and @xmath225 correspond to the observations belonging to the cluster @xmath24 .",
    "we decompose @xmath226 in singular value : @xmath227 .",
    "then , the estimator is @xmath228 .",
    "one way to perform those procedures is to consider the group - lasso estimator rather than the lasso estimator to select relevant variables .",
    "indeed , this estimator is more natural , according to the relevant variable definition .",
    "nevertheless , results are very similar , because we select grouped variables in both case , selected by the lasso or by the group - lasso estimator . in this section ,",
    "we describe our procedures with the group - lasso estimator , which could be understood as an improvement of our procedures .",
    "our both procedures take advantage of the lasso estimator to select relevant variables and to reduce the dimension in case of high - dimensional dataset .",
    "first , recall what is a relevant variable .",
    "a variable indexed by @xmath229 is _ irrelevant _ for the clustering if @xmath230_{m , j } = \\ldots = [ \\boldsymbol{\\phi}_{k}]_{m , j}=0.\\ ] ] a _ relevant _ variable is a variable which is not irrelevant .",
    "we denote by @xmath110 the set of relevant variables .",
    "the _ group - lasso estimator _ for mixture regression models with regularization parameter @xmath152 is defined by @xmath233 where @xmath234_{m , j}||_2 ; \\ ] ] where @xmath235_{m , j}||_2 ^ 2 = \\sum_{k=1}^k |[\\boldsymbol{\\phi}_k]_{m , j}|^2 $ ] and with @xmath132 to specify .      however , depending on the dataset , it could be interesting to look for which variables are equal to zero first . one way",
    "could be to extend this work with lasso - group - lasso estimator , described for the example for the linear model in @xcite .",
    "the first step consists of constructing a collection of models @xmath236 in which @xmath237 is defined by @xmath238 where @xmath239 } x ) ^t(\\boldsymbol{p}_ky-\\boldsymbol{\\phi}_k^{[\\widetilde{j } ] }   x)}{2 } \\right),\\ ] ] and @xmath240    the model collection is indexed by @xmath241 .",
    "denote by @xmath160 the possible number of components .",
    "we could bound @xmath127 without loss of estimation .",
    "denote also @xmath242 a collection of subsets of @xmath243 , constructed by the group - lasso estimator .    to detect the relevant variables and construct the set @xmath244 , we will use the group - lasso estimator defined by . in the @xmath56-procedures ,",
    "the choice of the regularization parameters is often difficult : fixing the number of components @xmath130 , we propose to construct a data - driven grid @xmath131 of regularization parameters by using the updating formulae of the mixture parameters in the em algorithm .",
    "then , for each @xmath137 , we could compute the group - lasso estimator defined by @xmath245_{m , j}||_2 \\right\\ } .\\ ] ] for a fixed number of mixture components @xmath130 and a regularization parameter @xmath132 , we could use a generalized em algorithm to approximate this estimator . then , for each @xmath130 and for each @xmath137 , we have constructed the set of relevant variables @xmath246 .",
    "we denote by @xmath242 the collection of all these sets .",
    "we propose a second procedure to take into account the matrix structure . for each model belonging to the collection @xmath237",
    ", a subcollection is constructed , varying the rank of @xmath123 .",
    "let us describe this procedure .    as in the group - lasso - mle procedure",
    ", we first construct a collection of models , thanks to the @xmath56-approach .",
    "we obtain an estimator for @xmath122 , denoted by @xmath249 , for each model belonging to the collection .",
    "we could deduce the set of relevant variables , denoted by @xmath250 and this for all @xmath130 : we deduce @xmath242 the collection of set of relevant variables .",
    "the second step consists in constructing a subcollection of models with rank sparsity , denoted by @xmath251 the model @xmath252 has @xmath2 components , the set @xmath250 for active variables and @xmath156 is the vector of the ranks of the matrix of regression coefficients in each group :      where @xmath254 } \\boldsymbol{x } ) ^t(\\boldsymbol{p}_k\\boldsymbol{y}-(\\boldsymbol{\\phi}_k^{r_k})^{[\\widetilde{j } ] } \\boldsymbol{x } ) } { 2 } \\right ) ; \\\\",
    "\\theta=&(\\pi_1,\\ldots , \\pi_k,\\boldsymbol{\\phi}_1^{r_1},\\ldots , \\boldsymbol{\\phi}_k^{r_k } , \\boldsymbol{p}_1,\\ldots,\\boldsymbol{p}_k ) \\in \\pi_k \\times \\psi_k^r \\times \\left(\\mathbb{r}_+^{q } \\right)^k ; \\\\",
    "\\psi_k^r= & \\left\\{(\\boldsymbol{\\phi}_1^{r_1},\\ldots , \\boldsymbol{\\phi}_k^{r_k } ) \\in \\left(\\mathbb{r}^{q\\times p } \\right)^k | \\text{rank}(\\boldsymbol{\\phi}_1 ) = r_1,\\ldots , \\text{rank}(\\boldsymbol{\\phi}_k)=r_k \\right\\ } ; \\end{aligned}\\ ] ]    and @xmath255 .",
    "denote by @xmath126 the possible number of components , @xmath242 a collection of subsets of @xmath243 and @xmath162 the set of vectors of size @xmath130 with rank values for each mean matrix .",
    "we could compute the mle under the rank constraint thanks to an em algorithm .",
    "indeed , we could constrain the estimation of @xmath61 , for all @xmath24 , to have a rank equal to @xmath163 , in keeping only the @xmath163 largest singular values .",
    "more details are given in section [ em2 ] .",
    "it leads to an estimator of the mean with row sparsity and low rank for each model .",
    "m. misiti , y. misiti , g. oppenheim and j - m poggi . clustering signals using wavelets .",
    "in francisco sandoval , alberto prieto , joan cabestany and manuel graa , editors , _ computational and ambient intelligence _ ,",
    "volume 4507 of _ lecture notes in computer science _ , pages 514521 .",
    "springer berlin heidelberg , 2007 ."
  ],
  "abstract_text": [
    "<S> finite mixture regression models are useful for modeling the relationship between response and predictors arising from different subpopulations . in this article , we study high - dimensional predictors and high - dimensional response and propose two procedures to cluster those observations according to the link between predictors and the response . </S>",
    "<S> to reduce the dimension , we propose to use the lasso estimator , which takes into account the sparsity and a maximum likelihood estimator penalized by the rank , to take into account the matrix structure . to choose the number of components and the sparsity level , we construct a model collection , </S>",
    "<S> varying those two parameters and select a model among this collection with a non - asymptotic criterion . </S>",
    "<S> we extend these procedures to functional data , where predictors and responses are functions . for this purpose </S>",
    "<S> , we use a wavelet - based approach . </S>",
    "<S> for each situation , we provide algorithms and apply and evaluate our methods both on simulated and real dataset , to understand how it works in practice . </S>"
  ]
}