{
  "article_text": [
    "ess @xcite , popular heuristics that excel in global optimization of continuous search landscapes , utilize a gaussian - based update ( variation ) step with an evolving covariance matrix . since the development of ess , it has been assumed that this learned covariance matrix , which defines the variation operation , approximates the inverse hessian of the search landscape .",
    "it was supported by the rationale that locating the global optimum by an es can be accommodated using mutation steps that fit the actual landscape , or in other words , that the optimal covariance distribution can offer mutation steps whose _ equidensity probability contours _ match the _ level sets _ of the landscape ( maximizing the progress rate at the same time ) @xcite .",
    "it has been additionally argued that reducing a general problem to an _ isotropic quadratic _ problem may be achieved by sampling search - points based upon a covariance matrix that is the inverse hessian ( in equivalence to replacing the euclidean distance measure with the mahalanobis metric ) .",
    "since ess operate well on such isotropically quadratic problems , a successful es run suggests that learning the covariance was accomplished .",
    "nevertheless , it has never been formally proven that ess machinery can indeed learn the inverse of the hessian .",
    "rudolph @xcite showed that ess are capable of facilitating such learning and derived practical bounds on the population size toward the end of a successful learning period .",
    "that study paved the way toward accumulation of past search information by ess by means of covariance matrices or any other forms of statistically learned algebraic structures .",
    "especially , accumulation of selected individuals is practically utilized by derandomized es variants @xcite , and it has led to the formulation of a successful family of search heuristics @xcite .    the goal of the current study is to investigate the statistical learning potential of an accumulated set of selected individuals ( the so - called _ winners _ of each generation ) * when the es operates in the vicinity of a landscape optimum*. we argue and prove that accumulation of such _ winning _ individuals carries the potential to reveal valuable search landscape information .",
    "in particular , we consider the statistically - constructed covariance matrix over _ winning _ decision vectors and prove that it _ commutes _ with the hessian matrix about the optimum ( i.e. , the two matrices share the same eigenvectors , and therefore their level sets are positioned along the same axes ) .",
    "this result indicates that in learning a covariance , an es deduces the sensitive directions for an effective optimization .",
    "this carries the potential of a great benefit to an es that learns a covariance matrix and may deduce the sensitive directions for effective optimization .",
    "furthermore , we provide an analytic approximation of the covariance matrix , which holds for a large population size and when the hessian is well - behaved ( formal details appear below ) .    the remainder of this paper is organized as follows .",
    "the problem is formally stated in section [ sec : problem ] , where the assumed model is described in detail . in section [ sec : covariance ] we formulate the covariance matrix , derive the necessary density function , and then prove that the covariance and the hessian commute .",
    "section [ sec : solving_singlewinner ] provides an analytical covariance approximation for the problem of a @xmath0-es . a simulation study encompassing various landscape scenarios for @xmath0 selection",
    "is presented in section [ sec : simulation ] , constituting a numerical corroboration for the theoretical outcomes in sections [ sec : covariance ] and [ sec : solving_singlewinner ] .",
    "finally , the results are discussed in section [ sec : discussions ] .",
    "we outline the _ research question _ that we target :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ what is the relation between the statistically - learned covariance matrix and the landscape hessian if a single winner is selected in each iteration assuming generated samples that follow an isotropic gaussian ( no adaptation ) ? _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    we focus on the _ a posteriori _ statistical construction of the covariance matrix of the decision variables upon reaching the proximity of the optimum , when subject to es operation .    in what follows",
    ", we formulate the problem , assume a model and present our notation .",
    "let @xmath2 denote the objective function subject to minimization .",
    "we assume that @xmath3 is minimized at the location @xmath4 , which is assumed for simplicity to be the origin .",
    "the objective function may be _ taylor - expanded _ about the optimum .",
    "we model the @xmath5-dimensional basin of attraction about @xmath4 by means of a quadratic approximation .",
    "we assume that this expansion is precise @xmath6 with @xmath7 being the landscape hessian about the optimum .",
    "the canonical non - elitist single - parent es search process operates in the following manner : the es generates @xmath1 search - points @xmath8 in each iteration , based upon gaussian sampling with respect to the given search - point .",
    "we are especially concerned with the canonical variation operator of es , which adds a normally distributed _ mutation _ @xmath9 .",
    "that is , @xmath8 are independent and each is @xmath10 . following the evaluation of those @xmath1 search points with respect to @xmath11 , the best ( minimal ) individual is selected and recorded as @xmath12 finally , let @xmath13 denote the _ winning _ objective function value , @xmath14 where @xmath15 .",
    "we mention the difference between the optimization phase , which aims to arrive at the optimum and is not discussed here , to the statistical learning of the basin  which lies in the focus of this study .",
    "the sampling procedure is summarized as algorithm 1 , wherein ` statcovariance ` refers to a routine for _ statistically _ constructing a covariance matrix from raw observations .",
    "@xmath16 @xmath17 @xmath18 [ algo : es_sampling ]      the length of a mutation vector , @xmath19 , obeys the so - called @xmath20-distribution with @xmath5 degrees of freedom . upon assuming a _ quadratic _ basin of attraction , @xmath21 is a random variable which obeys a generalized @xmath22-distribution .",
    "we consider two cases :    the basic , simplified case of an isotropic basin , that is , its hessian matrix constitutes the identity : @xmath23 . in this case , the distribution of @xmath24 is the standard @xmath22-distribution , possessing the following cumulative distribution function ( _ cdf _ ) accounting for the search - space dimensionality @xmath5 : @xmath25 with @xmath26 being the gamma function , defined by : @xmath27 the probability density function ( _ pdf _ ) is given by : @xmath28    the generalized case of a globally minimal quadratic basin , where the hessian matrix is positive definite with the following eigendecomposition form , @xmath29,\\ ] ] with @xmath30 being the eigenvalues .",
    "the random variable @xmath31 now obeys a generalized @xmath22-distribution , whose _ exact distribution function _ is described as follows @xcite : @xmath32 with an unknown closed form . at the same time , this cdf is known to follow an _ approximation _",
    "@xcite , @xmath33 with @xmath34 and @xmath35 accounting for matching the first two moments of @xmath36 ( and the subscript @xmath37 marks the transformed distribution ) : @xmath38 the density function of this approximation reads : @xmath39 * the accuracy of this approximation depends upon the standard deviation of the eigenvalues @xcite , which is clearly related to the so - called condition number*. we assume that standard deviation to be moderate , and we thus adopt this approximation herein . for the isotropic case",
    ", it can be easily verified that eq .",
    "[ eq : nchi_density ] reduces to eq .",
    "[ eq : pdf_chi ] .",
    "we conclude this section , by * summarizing the relevant notation * : the random vector @xmath40 is a normal gaussian mutation and @xmath21 .",
    "the random vectors @xmath8 are @xmath1 independent copies of @xmath40 , and @xmath41 .",
    "the winner is @xmath42 , and @xmath43 .",
    "the matrix @xmath7 is the hessian about the optimum @xmath4 , and @xmath44 is the covariance matrix of @xmath42 .",
    "in the current section we formulate the covariance matrix by means of its defining density functions and then prove it commutes with the landscape hessian .    by construction , the origin is set at the parent search - point , which is located at the optimum .",
    "analytically , the covariance elements are thus reduced to the following _ expectation values _ : @xmath45 where @xmath46 is an @xmath5-dimensional density function characterizing the _ winning _ decision variables about the optimum . * in essence , the current study aims at understanding this expression in eq .",
    "[ eq : cov0 ] . to this end , revealing the nature of @xmath47 is necessary for the interpretation of the covariance matrix*. importantly , the selection mechanism of the heuristic is blind to the location of the candidate solutions in the search space , and its sole criterion is the ranked function values .",
    "specifically , in the _ decision - space perspective _ , the density function of a _ winning _ vector of decision variables @xmath42 is related to the density of the _ winning _ function value @xmath13 via the following relation : @xmath48 with @xmath49 denoting the density function for generating an individual , and @xmath50 denoting the density function of the objective function values ( eqs .",
    "[ eq : pdf_chi ] and [ eq : nchi_density ] )",
    ". a brief justification follows .",
    "the density functions satisfy the conditional probability relation : @xmath51 now consider the distribution of @xmath52 $ ] on @xmath53 .",
    "the density of @xmath42 conditioned on the value of @xmath54 is that of a normal gaussian subject to this conditioning , since we may sample @xmath52 $ ] by the following construction : first sample @xmath55 according to @xmath50 independently . then sample @xmath56 conditioned on the values of @xmath57 independently .",
    "finally , @xmath3 may be set to @xmath58 that is minimal and @xmath42 set to the respective @xmath59 .",
    "in other words , following selection , a winning value of @xmath3 is chosen to be @xmath13 , and the corresponding @xmath60 becomes the winning vector @xmath42 .",
    "importantly , the winning vector @xmath42 conditioned upon the winning value @xmath13 is generated in the same manner as a normally - distributed @xmath40 conditioned upon @xmath24 . as a result ,",
    "the conditional probability for the generation of @xmath42 conditioned upon @xmath13 is the same as that for the creation of @xmath40 conditioned upon @xmath24 , i.e. , @xmath61 .",
    "this density therefore reads : @xmath62    [ thm : commuting ] the covariance matrix and the hessian are * commuting matrices * when the objective function follows the quadratic approximation .",
    "given the density function in eq .",
    "[ eq : x_pdf_2 ] , the objective function is assumed to satisfy @xmath63 , and the covariance matrix reads : @xmath64 consider the orthogonal matrix @xmath65 , which diagonalizes @xmath7 into @xmath66 and possesses a determinant of value @xmath67 : @xmath68\\\\ \\medskip \\displaystyle \\vec{\\vartheta } = \\mathcal{u}^{-1 } \\vec{x}\\\\ \\displaystyle \\textrm{d}\\vec{\\vartheta } = \\textrm{d}\\vec{x } \\end{array}.\\ ] ] we target the integral @xmath69 and apply a change of variables into @xmath70 ( after changing order of summations ) : @xmath71 @xmath72 vanishes for any @xmath73 due to symmetry considerations : the overall integrand is an _ odd _ function , because all the terms are _ even _ functions , except for @xmath74 , @xmath75 when they differ .",
    "therefore , the integration over the entire domain yields zero . hence , @xmath76 is the diagonalized form of @xmath44 , with @xmath65 holding the eigenvectors .",
    "@xmath44 is thus diagonalized by the same eigenvectors as @xmath7 , and therefore , by definition , they are commuting matrices , as claimed .",
    "in this section we provide an approximation for @xmath77 and consequently for @xmath78 in order to explicitly calculate the covariance matrix using eq .",
    "[ eq : cov0 ] .",
    "a non - elitist multi - child selection is considered here , where in each iteration a single individual is deterministically selected out of @xmath1 generated offspring . in particular , consider a random sample from an absolutely continuous population with _ density _ @xmath79 and _ distribution _ @xmath80 . in order to formulate the _ density _ of those _ winners _ , it is convenient to first characterize the distribution function of the winning event amongst @xmath1 candidates is _ even _ , the distribution of the winners for the @xmath22 distribution ( isotropic basin case ) possesses a simple form : @xmath81 .",
    "] : @xmath82 the density function is obtained upon differentiating eq .",
    "[ eq : y_cdf ] : @xmath83 upon substituting the explicit forms into @xmath84 and @xmath50 ( using either eqs .",
    "( [ eq : cdf_chi],[eq : pdf_chi ] ) for the _ standard _ @xmath22 or eqs .",
    "( [ eq : nchi],[eq : nchi_density ] ) for the _ generalized _ @xmath22 ) , the desired density function @xmath77 is obtained , however not in a closed form .",
    "+ gupta @xcite derived explicit order statistic results from the gamma distribution , to which the @xmath22 distribution belongs , including the distribution function as well as moments of the @xmath85 order statistic .",
    "such results could reveal a closed form for @xmath77 , which seems cumbersome and far too complex to address when targeting eq .",
    "[ eq : cov0 ] .",
    "next , we will seek an _ approximation _ for @xmath77 , which will enable us to realize the relation of eq .",
    "[ eq : x_pdf_2 ] when large values of @xmath1 are assumed .",
    "we treat the derived winners distribution for large sample sizes , i.e. , when the population size @xmath1 tends to infinity .",
    "we denote the cdf in eq .",
    "[ eq : y_cdf ] with a subscript @xmath1 , @xmath86 , and consider the limit when @xmath1 tends to infinity : @xmath87 according to the fisher - tippett theorem @xcite , also known as the _ extremal types theorem _ , the _ von - mises _ family of distributions for * minima * ( or the minimal generalized extreme value distributions ( @xmath88 ) ) are the only non - degenerate family of distributions satisfying this limit .",
    "they are characterized as a unified family of distributions by the following cdf : @xmath89^{1/\\kappa_3 } \\right\\}.\\ ] ] furthermore , since the minimum distribution moves toward the origin as @xmath1 increases , normalizing constants are needed to avoid degeneracy and to obtain : @xmath90 the location parameter , @xmath91 , and the scale parameter , @xmath92 , are obviously interlinked to the aforementioned normalizing constants .",
    "the shape parameter , @xmath93 , determines the identity of the characteristic cdf , namely either weibull , gumbell , or frecht .",
    "this parameter is evaluated by means of the following limit , whose existence is a necessary and sufficient condition for a continuous distribution function @xmath80 to belong to the domain of attraction for minima of @xmath94 : @xmath95 where @xmath96 refers to the inverse cdf ( the quantile function of @xmath80 ; see theorem 9.6 in @xcite [ pp .",
    "204 - 205 ] ) :    * if @xmath97 , @xmath80 belongs to the weibull minimal domain of attraction , * if @xmath98 , @xmath80 belongs to the gumbel minimal domain of attraction , and * if @xmath99 , @xmath80 belongs to the frecht minimal domain of attraction .",
    "note that rudolph had already taken a related mathematical approach , which he termed _",
    "asymptotic theory of extreme order statistics _ , to characterize convergence properties of ess on a class of convex objective functions @xcite .",
    "also , gevd is introduced to the broad perspective of stochastic global optimization in @xcite , a book which also constitutes a proper mathematical reference for this topic , yet in a slightly different light .    for the standard @xmath22 distribution ( i.e. , isotropic basin ) , the limit for eq .",
    "[ eq : kappa_def ] exists and reads @xmath100 .",
    "the limit needs to be evaluated about @xmath101 . by inserting an asymptotic expansion of the gamma function s integrand ,",
    "the overall cdf @xmath102 ( eq . [ eq : cdf_chi ] ) may be written and approximated using stirling s formula as @xmath103 taking only the zeroth - order term in the sum into consideration .",
    "the quantile ( inverse ) function has the form : @xmath104 targeting the limit in eq .",
    "[ eq : kappa_def ] yields @xmath105 which allows to conclude with : @xmath106    for the generalized @xmath22 distribution ( i.e. , non - isotropic basin ) , the limit for eq .",
    "[ eq : kappa_def ] exists and also reads @xmath100 .    in the limit @xmath101 ,",
    "the generalized distribution ( eq .  [ eq : nchi ] ) has a similar cdf , with an approximated quantile function @xmath107 and thus eq .",
    "[ eq : limit_kappa ] holds as is .    for the standard and generalized @xmath22 distributions",
    ", the normalizing constants @xmath108 ensure that the limit distribution of eq .",
    "[ eq : limitmin ] is not degenerate .    given the constants @xmath109 the limit becomes ( using @xmath110 ) @xmath111\\right\\}^{\\lambda } = \\\\",
    "\\medskip \\displaystyle \\lim_{\\lambda\\longrightarrow \\infty } 1 - \\left[1-r\\left(n\\right ) \\left(\\frac{\\psi^{n/2}}{\\lambda } \\right ) \\right]^{\\lambda } = \\\\",
    "\\displaystyle 1-\\exp\\left[-r\\left(n\\right ) \\left(\\psi \\right)^{n/2 } \\right ] \\end{array},\\ ] ] with @xmath112 .",
    "hence , the limit distribution exists and is not degenerate , as claimed .",
    "since the shape parameter @xmath93 is always positive , the extreme minima of the @xmath22-distributions belong to the weibull domain of attraction .",
    "the normalized extreme minima , @xmath113 , may be represented by a random variable @xmath114 , which then reduces eq .",
    "[ eq : minimagevd ] to the following transformed cdf ( importantly , the so - called * tail index * reads @xmath115 ) :    @xmath116    see @xcite and @xcite for an overview on the family of generalized extreme value distributions and on the limit distributions of order statistics .",
    "in particular , see table 9.1 in @xcite[p .",
    "200 ] for the relationship between the parameters of the gevd and the weibull distribution , which allows the reduction of eq .",
    "[ eq : minimagevd ] to eq .",
    "[ eq : weibull ] .",
    "also , for the exact determination of the tail index value , when assuming certain conditions on the sampling distribution , see theorem 2.3 in @xcite .    under the gevd approximation for treating large populations , @xmath117 , upon normalizing the variable to @xmath118 and using the tail index result @xmath119 ,",
    "the cdf and pdf forms for the single winning event read :    @xmath120      by setting the _ weibull _ form as the characteristic density @xmath121 , we may rewrite eq .  [",
    "eq : cov0 ] by utilizing eq .",
    "[ eq : x_pdf_2 ] as follows with the normalized @xmath122 : @xmath123 \\times \\\\ \\displaystyle   \\times \\frac{\\frac{1}{\\sqrt{\\left(2\\pi \\right)^n}}\\exp\\left(-\\frac{1}{2 } \\vec{x}^t \\vec{x } \\right)}{\\frac{\\upsilon^{\\eta}}{\\gamma\\left(\\eta\\right ) } j(\\vec{x})^{\\eta-1}\\exp\\left(-\\upsilon j(\\vec{x } ) \\right ) } \\textrm{d}x_1\\textrm{d}x_2\\cdots \\textrm{d}x_n .",
    "\\end{array}\\ ] ] @xmath3 is assumed here to satisfy @xmath63 , and must be normalized only for the @xmath121 term by means of @xmath124 alone since @xmath125 : @xmath126 \\textrm{d}x_1\\textrm{d}x_2\\cdots \\textrm{d}x_n .",
    "\\end{array } } \\ ] ] with a normalizing constant @xmath127 . for the isotropic case , @xmath128",
    ", the integration is straightforward ( @xmath129 , @xmath130 )  the attained covariance is proportional to the inverse hessian , multiplied by an explicit factor : @xmath131 wherein @xmath132    for the general case of any positive - definite hessian @xmath7 , the integral in eq .",
    "[ eq : cij ] has an unknown closed form .",
    "we were able , nevertheless , to numerically corroborate it for @xmath133 .",
    "we note that this form of the covariance also commutes with the hessian , in line with the theorem discussed above .",
    "we implemented our model into a numerical procedure in order to compare the statistical measures to the analytical calculations in practice , adhering to algorithm 1 , @xmath134 , statistically sampled as described therein .",
    "numerical validation is provided here for two aspects of our theoretical work : theorem [ thm : commuting ] and the analytic approximation for the covariance matrix .",
    "we generated a large set of random positive - definite matrices at various dimensions @xmath135 with a spectrum of condition numbers .",
    "for each trial @xmath136 , the numerical procedure generated a random symmetric matrix @xmath137 , diagonalized it into a set of orthonormal eigenvectors @xmath138 , drew @xmath139 random positive numbers in a diagonal matrix @xmath140 , and set @xmath141 .",
    "we then applied algorithm 1 by considering @xmath142 as landscape hessians .",
    "the resultant covariance matrices , @xmath143 , were diagonalized and compared to the hessian matrices and their eigendecomposition  which always matched . in practice , it was evident that the two matrices always commute ( applying the commutator operator yields a zero matrix to a practical precision considering the _ max norm _ ) , @xmath144 as claimed .",
    "however , the covariance matrices were * not * the inverse forms of the hessian matrices .      here",
    ", we corroborated the analytic approximation for the covariance matrix . to this end , we considered four quadratic basins of attraction at various search - space dimensions :    @xmath133 , @xmath145 $ ]    @xmath146 , @xmath147 $ ]    @xmath148 , @xmath149 $ ]    @xmath150 , @xmath151      figure [ table : approxchi2 ] depicts the approximated density functions of the generalized @xmath22 distribution , @xmath152 ( eq .  [ eq : nchi_density ] ) for the four hessian forms ( h-1)-(h-4 ) , which evidently constitute sound approximations .",
    "figures [ table : winners ] and [ table : h2winners ] provide validation for the winners density , which was exactly described by @xmath121 in eq .",
    "[ eq : y_pdf ] , and was later approximated by @xmath153 in eq .",
    "[ eq : cdf_y_evd ] for large @xmath1 .",
    "interestingly , @xmath121 , which is realized here by the approximated generalized @xmath22 distribution @xmath154 , exhibits decreased accuracy on @xmath155 , @xmath156 and @xmath157 .",
    "evidently , it is highly sensitive to the approximation error of @xmath154 , which is amplified by the exponent @xmath1 . at the same time , @xmath153 exhibits decreased accuracy on @xmath156 , @xmath157 and @xmath158 , due to its sensitivity to the population size @xmath1 .",
    "indeed , improvements for this approximation were evident when @xmath1 was increased ( see additional settings on figure [ table : h2winners ] ) .      finally , we compared @xmath134 to the obtained analytical approximations .",
    "for the isotropic case , the result of eq .",
    "[ eq : isoresult ] has been successfully corroborated for a range of search - space dimensions @xmath5 .",
    "for instance , @xmath134 for the @xmath159-dimensional case ( h-4 ) was constructed with @xmath160 and over @xmath161 iterations to obtain a diagonal with an expected value @xmath162 .",
    "[ eq : isoresult ] obtained a value of @xmath163 .    for the general case , we considered the @xmath164-dimensional case ( h-1 ) .",
    "@xmath134 was constructed with @xmath165 and over @xmath166 iterations , to be presented side - by - side with the numerical integration of eq .",
    "[ eq : cij ] in table [ table : h1 ] .",
    "additionally , their explicit eigenvectors are provided therein .",
    "simulation study : the distribution of @xmath167 over a single iteration +    [ cols=\"<,<\",options=\"header \" , ]     @xmath168 +    at @xmath169 , respectively with the exact forms listed in ( h-1),(h-3),(h-4 ) .",
    "the winners distributions amongst a population of @xmath1 individuals are depicted over @xmath170 iterations : [ left column ] statistical histograms of raw samples ( bars ) , versus the analytical pdf ( dashed curve ) according to @xmath121 in eq .",
    "[ eq : y_pdf ] ; [ right column ] statistical histograms of normalized samples ( bars ) , versus the analytically derived gevd approximation to the pdf ( dashed curve ) according to @xmath153 in eq .",
    "[ eq : cdf_y_evd ] .",
    "[ table : winners],title=\"fig : \" ]    at @xmath169 , respectively with the exact forms listed in ( h-1),(h-3),(h-4 ) .",
    "the winners distributions amongst a population of @xmath1 individuals are depicted over @xmath170 iterations : [ left column ] statistical histograms of raw samples ( bars ) , versus the analytical pdf ( dashed curve ) according to @xmath121 in eq .",
    "[ eq : y_pdf ] ; [ right column ] statistical histograms of normalized samples ( bars ) , versus the analytically derived gevd approximation to the pdf ( dashed curve ) according to @xmath153 in eq .",
    "[ eq : cdf_y_evd ] .",
    "[ table : winners],title=\"fig : \" ] + @xmath171 +    at @xmath169 , respectively with the exact forms listed in ( h-1),(h-3),(h-4 ) .",
    "the winners distributions amongst a population of @xmath1 individuals are depicted over @xmath170 iterations : [ left column ] statistical histograms of raw samples ( bars ) , versus the analytical pdf ( dashed curve ) according to @xmath121 in eq .",
    "[ eq : y_pdf ] ; [ right column ] statistical histograms of normalized samples ( bars ) , versus the analytically derived gevd approximation to the pdf ( dashed curve ) according to @xmath153 in eq .",
    "[ eq : cdf_y_evd ] .",
    "[ table : winners],title=\"fig : \" ]    at @xmath169 , respectively with the exact forms listed in ( h-1),(h-3),(h-4 ) .",
    "the winners distributions amongst a population of @xmath1 individuals are depicted over @xmath170 iterations : [ left column ] statistical histograms of raw samples ( bars ) , versus the analytical pdf ( dashed curve ) according to @xmath121 in eq .",
    "[ eq : y_pdf ] ; [ right column ] statistical histograms of normalized samples ( bars ) , versus the analytically derived gevd approximation to the pdf ( dashed curve ) according to @xmath153 in eq .",
    "[ eq : cdf_y_evd ] .",
    "[ table : winners],title=\"fig : \" ] + @xmath172 +    at @xmath169 , respectively with the exact forms listed in ( h-1),(h-3),(h-4 ) .",
    "the winners distributions amongst a population of @xmath1 individuals are depicted over @xmath170 iterations : [ left column ] statistical histograms of raw samples ( bars ) , versus the analytical pdf ( dashed curve ) according to @xmath121 in eq .",
    "[ eq : y_pdf ] ; [ right column ] statistical histograms of normalized samples ( bars ) , versus the analytically derived gevd approximation to the pdf ( dashed curve ) according to @xmath153 in eq .",
    "[ eq : cdf_y_evd ] .",
    "[ table : winners],title=\"fig : \" ]    at @xmath169 , respectively with the exact forms listed in ( h-1),(h-3),(h-4 ) .",
    "the winners distributions amongst a population of @xmath1 individuals are depicted over @xmath170 iterations : [ left column ] statistical histograms of raw samples ( bars ) , versus the analytical pdf ( dashed curve ) according to @xmath121 in eq .",
    "[ eq : y_pdf ] ; [ right column ] statistical histograms of normalized samples ( bars ) , versus the analytically derived gevd approximation to the pdf ( dashed curve ) according to @xmath153 in eq .  [ eq : cdf_y_evd ] .",
    "[ table : winners],title=\"fig : \" ] +    @xmath173 +   [ top ] , with two additional gevd approximations for @xmath156 featuring different settings [ bottom ] .",
    "[ table : h2winners],title=\"fig : \" ]   [ top ] , with two additional gevd approximations for @xmath156 featuring different settings [ bottom ] .",
    "[ table : h2winners],title=\"fig : \" ] + @xmath174:}~ \\left(\\lambda=10000 , ~ n_{\\texttt{iter}}=10 ^ 6\\right)~\\textrm{[right]:}~ \\left(\\lambda=20000 , ~ n_{\\texttt{iter}}=5\\cdot 10 ^ 6\\right)$ ] +   [ top ] , with two additional gevd approximations for @xmath156 featuring different settings [ bottom ] .",
    "[ table : h2winners],title=\"fig : \" ]   [ top ] , with two additional gevd approximations for @xmath156 featuring different settings [ bottom ] .",
    "[ table : h2winners],title=\"fig : \" ]",
    "our analytical work modeled passive es learning , that is , no step - size adaptation nor covariance matrix adaptation were utilized when constructing a covariance matrix from winning decision vectors .",
    "we proved that the statistically - constructed covariance _ commutes _ with the landscape hessian about the optimum when a quadratic basin is assumed .",
    "the implication of this result is the enhanced capacity of ess to identify _ sensitive optimization directions _ by extracting this information from the learned covariance matrix .",
    "we then derived an analytical approximation for the covariance matrix , based on two presumptions ",
    "( i ) the generalized @xmath22 density function was approximated , assuming moderate standard deviation of the eigenvalues , and ( ii ) the winners distribution was shown to follow the _ weibull _ distribution with a calculated _ tail index _ when the population size @xmath1 is large , adhering to the limit distributions of order statistics .",
    "our results were then numerically validated at multiple levels , where the accuracy of the approximations was discussed ."
  ],
  "abstract_text": [
    "<S> we study the theoretical capacity to statistically learn local landscape information by evolution strategies ( ess ) . </S>",
    "<S> specifically , we investigate the covariance matrix when constructed by ess operating with the selection operator alone . </S>",
    "<S> we model continuous generation of candidate solutions about quadratic basins of attraction , with deterministic selection of the decision vectors that minimize the objective function values . </S>",
    "<S> our goal is to rigorously show that accumulation of winning individuals carries the potential to reveal valuable information about the search landscape , e.g. , as already practically utilized by derandomized es variants . </S>",
    "<S> we first show that the statistically - constructed covariance matrix over such winning decision vectors shares the same eigenvectors with the hessian matrix about the optimum . </S>",
    "<S> we then provide an analytic approximation of this covariance matrix for a non - elitist multi - child @xmath0-strategy , which holds for a large population size @xmath1 . </S>",
    "<S> finally , we also numerically corroborate our results .    </S>",
    "<S> * keywords * : theory of evolution strategies , statistical learning , covariance matrix adaptation , landscape hessian , limit distributions of order statistics , extreme value distributions </S>"
  ]
}