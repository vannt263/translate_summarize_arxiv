{
  "article_text": [
    "the sparse vector recovery problems emerging in many areas of scientific research and engineering practice have attracted considerable attention in recent years ( @xcite-@xcite ) .",
    "typical applications include regression @xcite , visual coding @xcite , signal processing @xcite , compressed sensing @xcite , @xcite , machine learning @xcite , and microwave imaging @xcite .",
    "these problems can be modeled as the following @xmath5-norm regularized optimization problem @xmath6 where @xmath7 is a proper lower - semicontinuous function , @xmath8 , commonly called the @xmath5-norm , denotes the number of nonzero components of @xmath9 and @xmath10 is a regularization parameter .",
    "the @xmath5 regularized least squares problem is a special case of ( [ l0reg ] ) where @xmath11 .",
    "blumensath and davies @xcite proposed the iterative _ hard _ thresholding algorithm to solve this problem , and showed that the algorithm converges to a local minimizer .",
    "recently , lu and zhang @xcite proposed a penalty decomposition method for solving a more general class of @xmath5 regularized problems .",
    "in addition , lu @xcite proposed an iterative _ hard _ thresholding method and its variant for solving @xmath5 regularization over a conic constraint , and established its convergence as well as the iteration complexity .    besides the @xmath5 regularized optimization problem , a more general class of problems",
    "are considered a lot in both practice and theory , that is , @xmath12 where @xmath13 is a certain separable , continuous penalty with @xmath14 , and @xmath15 .",
    "one of the most important cases is the @xmath16-norm with @xmath17 .",
    "the @xmath16-norm is convex and thus , the corresponding @xmath16-norm regularized optimization problem can be efficiently solved .",
    "because of this , the @xmath16-norm becomes popular and has been accepted as a very useful tool for the modeling of the sparsity problems .",
    "nevertheless , the @xmath16-norm may not induce adequate sparsity when applied to certain applications @xcite , @xcite , @xcite , @xcite .",
    "alternatively , many non - convex penalties were proposed as relaxations of the @xmath5-norm .",
    "some typical non - convex examples are the @xmath0-norm ( @xmath1 ) @xcite , @xcite , @xcite , smoothly clipped absolute deviation ( scad ) @xcite , minimax concave penalty ( mcp ) @xcite and log - sum penalty ( lsp ) @xcite .",
    "compared with the @xmath16-norm , the non - convex penalties can usually induce better sparsity while the corresponding non - convex regularized optimization problems are generally more difficult to solve .",
    "there are mainly four classes of algorithms to solve the non - convex regularized optimization problem ( [ nonscs ] ) .",
    "the first one is the half - quadratic ( hq ) algorithm @xcite , @xcite .",
    "hq algorithms can be efficient when both subproblems are easy to solve ( particularly , when both subproblems have closed - form solutions ) .",
    "the second class is the iterative reweighted algorithm including iterative reweighted least squares ( irls ) minimization @xcite , @xcite , @xcite and iterative reweighted @xmath16-minimization ( irl1 ) @xcite algorithms .",
    "recently , lu @xcite extended some existing iterative reweighted methods and then proposed new variants for the general @xmath0 ( @xmath1 ) regularized unconstrained minimization problems",
    ". nevertheless , the iterative reweighted algorithms can be only efficient when the corresponding non - convex penalty can be well approximated via the quadratic function or the weighted @xmath16-norm function .",
    "the third class is the difference of convex functions algorithm ( dc programming ) @xcite , which is also called multi - stage ( ms ) convex relaxation @xcite .",
    "the dc programming considers a proper decomposition of the objective function .",
    "hence , it can be only applied to those non - convex penalties that can be decomposed as a difference of convex functions .",
    "the last class is the iterative thresholding algorithm , which fits the framework of the forward - backward splitting algorithm @xcite and the generalized gradient projection algorithm @xcite when applied to a separable non - convex penalty .",
    "intuitively , the iterative thresholding algorithm can be viewed as a procedure of landweber iteration projected by a certain thresholding operator .",
    "thus , the thresholding operator plays a key role in the iterative thresholding algorithm . for some special non - convex penalties such as scad , mcp , lsp and @xmath0-norms with @xmath3",
    ", the associated thresholding operators can be expressed analytically @xcite , @xcite , @xcite .",
    "compared to the other types of non - convex algorithms such as the hq , irls , irl1 and dc programming algorithms , the iterative thresholding algorithm is easy to implement and has almost the least computational complexity for large scale problems @xcite , @xcite .",
    "consequently , the iterative thresholding algorithm becomes popular .",
    "one of the significant differences between the convex and non - convex algorithms is that the convergence analysis of a non - convex algorithm is in general tricky .",
    "although the effectiveness of the iterative thresholding algorithms for the non - convex regularized optimization problems has been verified in many applications , except for the iterative _ hard _ @xcite and _ half _ @xcite thresholding algorithms , the convergence of most of these algorithms has not been thoroughly investigated .",
    "more specifically , there are still three mainly open questions .    1 .",
    "when does the algorithm converge ?",
    "under what conditions , the iterative thresholding algorithm converges strongly in the sense that the whole sequence generated , regardless of the initial point , is convergent .",
    "2 .   where does the algorithm converge ?",
    "does the algorithm converge to a global minimizer or more practically , a local minimizer due to the non - convexity of the optimization problem ?",
    "3 .   what is the convergence rate of the algorithm ?      in this paper",
    ", we give the convergence analysis for the iterative jumping thresholding algorithm ( called ijt algorithm henceforth ) for solving a certain class of non - convex regularized optimization problems .",
    "one of the most significant features of such non - convex problems is that the corresponding thresholding functions are discontinuous with jump discontinuities ( see fig .",
    "[ fig_thresfun ] ) . moreover , the corresponding thresholding functions are not nonexpansive in general . among these non - convex penalties ,",
    "the well - known @xmath0-norm with @xmath1 is one of the most typical cases .",
    "the main contribution can be summarized as follows .    1 .",
    "we prove that the supports and signs of any sequence generated by ijt algorithm can converge within finite iterations .",
    "such property brings a possible way to construct a new sequence in a special subspace such that the new sequence has the same convergence behavior of the original sequence generated by ijt algorithm .",
    "2 .   under a further assumption that the objective function satisfies the so - called restricted kurdyka - ojasiewicz ( rkl ) property ( see definition [ def_rklprop ] ) at some limit point , the strong convergence of ijt algorithm can be assuredly guaranteed ( see theorem [ thm_strongconv ] ) .",
    "the introduced rkl property is generally weaker than the well - known kurdyka - ojasiewicz property that is widely used to study the convergence of nonconvex algorithms .",
    "3 .   under certain second - order conditions",
    ", we demonstrate that ijt algorithm converges to a local minimizer at an asymptotically linear rate ( see theorems [ thm_localconv]-[thm_convrate2 ] ) .",
    "such asymptotically linear convergence speed means that when the iterative vector is sufficiently close to the convergent point , the rate of convergence of ijt algorithm is linear .",
    "this implies that given a good initial guess , ijt algorithm can converge very fast .",
    "4 .   as a typical case , we apply the developed convergence results to the @xmath0 regularization ( @xmath1 ) .",
    "when applied to the @xmath0 regularization , ijt algorithm can converge to a local minimizer at an asymptotically linear rate as long as the matrix satisfies a certain concentration property ( see theorem [ thm_convratelq_mu ] ) .",
    "we also provide simulations to support the correctness of theoretical assertions and compare the convergence speed of ijt algorithm for the @xmath2 regularization problems ( @xmath3 ) with other known typical algorithms like the iterative reweighted least squares ( irls ) algorithm and the iterative reweighted @xmath4 minimization ( irl1 ) algorithm .      we denote @xmath18 and @xmath19 as the real number and natural number sets , respectively . for any vector @xmath20",
    ", @xmath21 is its @xmath22-th component , and for a given index set @xmath23 , @xmath24 represents its subvector containing all the components restricted to @xmath25 .",
    "@xmath26 represents the complementary set of @xmath25 , i.e. , @xmath27 @xmath28 represents the euclidean norm of a vector @xmath9 .",
    "@xmath29 is the support set of @xmath9 , i.e. , @xmath30 .",
    "for any matrix @xmath31 , @xmath32 and @xmath33 ( @xmath34 and @xmath35 ) denote as the @xmath22-th and minimal singular values ( eigenvalues ) of @xmath36 , respectively .",
    "similar to the vector case , for a given index set @xmath25 , @xmath37 represents the submatrix of @xmath36 containing all the columns restricted to @xmath25 . for any @xmath38",
    ", @xmath39 denotes its sign function , i.e. , @xmath40{ll}1 , & \\mbox{for } \\",
    "z>0\\\\ 0 , & \\mbox{for } \\",
    "z=0\\\\ -1 , & \\mbox{for } \\",
    "z<0\\\\ \\end{array } \\right .. \\ ] ]    the remainder of this paper is organized as follows . in section",
    "ii , we give the problem settings and then introduce ijt algorithm with some basic properties . in section iii ,",
    "we give the convergence analysis of ijt algorithm . in section iv , we apply the established theoretical analysis to the @xmath0 ( @xmath1 ) regularization . in section",
    "v , we discuss some related work . in section",
    "vi , we conduct the simulations to substantiate the theoretical results .",
    "we conclude this paper in section vii .",
    "in this section , we first present the basic settings of the considered non - convex regularized optimization problems , then introduce ijt algorithm for these problems . in the end of this section ,",
    "we briefly review some basic properties of ijt algorithm obtained in @xcite .",
    "we consider the following composite optimization problem @xmath41 where @xmath13 is assumed to be separable with @xmath14 .",
    "moreover , we make several assumptions on the problem ( [ nonopt ] ) .",
    "[ aonf ] @xmath7 is weakly lower - semicontinuous and differentiable with lipschitz continuous gradient , i.e. , it holds that @xmath42 where @xmath43 is the lipschitz constant .",
    "it should be noted that assumption [ aonf ] is a general assumption for @xmath44 .",
    "many formulations in machine learning satisfy assumption [ aonf ] . for example , the following least squares and logistic loss functions are two commonly used functions which satisfy assumption [ aonf ] : @xmath45 where @xmath46 for @xmath47 , @xmath48^t \\in \\mathbf{r}^{m\\times n}$ ] is a data matrix and @xmath49 is a target vector .",
    "moreover , in both signal and image processing , @xmath44 is commonly taken as the least squares of the observation model , that is , @xmath50 where @xmath51 is an observation vector and @xmath52 is an observation matrix .",
    "it can be easily verified that such @xmath44 also satisfies assumption [ aonf ] .    in the following ,",
    "we give some basic assumptions on @xmath53 , most of which were considered in @xcite .",
    "[ aonphi ] @xmath54 is continuous and satisfies the following assumptions :    1 .",
    "@xmath53 is non - decreasing with @xmath55 and @xmath56 when @xmath57 .",
    "2 .   for each @xmath58",
    ", there exists an @xmath59 such that @xmath60 for @xmath61 $ ] .",
    "@xmath53 is differentiable on @xmath62 and the derivative @xmath63 is strictly convex with @xmath64 for @xmath65 and @xmath66 for @xmath57 .",
    "4 .   @xmath53 has a continuous second derivative @xmath67 on @xmath62 .    in assumption",
    "[ aonphi ] , ( a ) and ( b ) are taken from assumption 3.1 in @xcite , while ( c ) and ( d ) are adapted from assumption 3.2 in @xcite .",
    "it can be observed that assumption [ aonphi](a ) ensures the coercivity of @xmath53 , and thus the existence of the minimizer of the optimization problem ( [ nonopt ] ) .",
    "assumption [ aonphi](b ) guarantees the weakly sequential lower semi - continuity of @xmath53 in @xmath68 , and assumption [ aonphi](c ) induces the sparsity of the penalty @xmath69 . in practice , there are many non - convex functions satisfying assumption [ aonphi ] .",
    "two of the most typical subclasses are @xmath70 and @xmath71 with @xmath72 as shown in fig .",
    "[ fig_thresfun ] .     satisfying assumption [ aonphi ] and the corresponding thresholding functions .",
    "more specifically , we plot the figures of the penalty functions @xmath73 , and their corresponding thresholding functions . for comparison",
    ", we also plot the figures of two well - known cases , i.e. , @xmath5-norm with @xmath74 as the indicator function of @xmath75 , @xmath16-norm with @xmath76 , and their corresponding thresholding functions .",
    "( a ) typical penalty functions .",
    "( b ) thresholding functions . ]",
    "\\(a ) typical penalty functions     satisfying assumption [ aonphi ] and the corresponding thresholding functions .",
    "more specifically , we plot the figures of the penalty functions @xmath73 , and their corresponding thresholding functions . for comparison",
    ", we also plot the figures of two well - known cases , i.e. , @xmath5-norm with @xmath74 as the indicator function of @xmath75 , @xmath16-norm with @xmath76 , and their corresponding thresholding functions .",
    "( a ) typical penalty functions .",
    "( b ) thresholding functions . ]",
    "\\(b ) thresholding functions      in order to describe ijt algorithm , we need to generalize the proximity operator from the convex case to a non - convex penalty @xmath69 , that is , @xmath77 where @xmath78 is a parameter .",
    "since @xmath69 is separable , computing @xmath79 is reduced to solve a one - dimensional minimization problem , that is , @xmath80 therefore , @xmath81    as shown by ( [ svproxoper ] ) , the proximity operator is defined through an optimization problem , which is commonly hard for computing and analysis . in order to present a simpler form of the proximity operator for analysis",
    ", we show a preparatory lemma in the following .",
    "[ lemm_pre ] * ( lemma 3.10 in @xcite ) * assume that @xmath53 satisfies assumption [ aonphi ] , then    1 .",
    "for each @xmath78 , the function @xmath82 is well defined on @xmath83 and , moreover , it is strictly convex and attains a minimum at @xmath84 ; 2 .",
    "the function @xmath85 is strictly decreasing and one - to - one on @xmath86 ; 3 .",
    "for any @xmath87 , it holds that @xmath88 ; 4 .   for any @xmath87 ,",
    "@xmath89 is negative and monotonically increasing .    with lemma [ lemm_pre ] , @xmath90 can be expressed as follows .",
    "[ lemm_jumpthresfun ] * ( lemma 3.12 in @xcite ) * assume that @xmath53 satisfies assumption 2 , then @xmath91 is well defined and can be specified as @xmath92{ll}sign(z)\\rho_{\\mu}^{-1}(|z| ) , & \\mbox{for } \\",
    "|z|\\geq \\tau_{\\mu}\\\\ 0 , & \\mbox{for } \\ |z|\\leq \\tau_{\\mu } \\end{array } \\right . ,",
    "\\label{proxmapexp}\\ ] ] for any @xmath93 with @xmath94 and @xmath95 moreover , the range of @xmath90 is @xmath96 .",
    "it can be observed that the proximity operator is discontinuous with a jump discontinuity , which is one of the most significant features of such a class of non - convex penalties studied in this paper .",
    "moreover , it can be easily checked that the proximity operator is not nonexpansive in general .",
    "due to these , the convergence analysis of the corresponding non - convex algorithm gets challenging .",
    "( some specific proximity operators are shown in fig . [ fig_thresfun](b ) . )    with the definition of the proximity operator , ijt algorithm can be proposed to solve the non - convex regularized optimization problem ( [ nonopt ] ) .",
    "formally , the iterative form of ijt algorithm can be expressed as follows @xmath97 where @xmath78 is a step size parameter . for simplicity",
    ", we define @xmath98 for any @xmath20 . henceforth , we call @xmath90 the jumping thresholding function .",
    "[ rthresq ] for some specific @xmath0-norm ( say , @xmath3 ) , the proximity operator can be expressed analytically @xcite , @xcite ( as shown in fig .",
    "1(b ) ) .",
    "[ rhard ] although the @xmath5-norm does not satisfy assumption 2 , the _ hard _ thresholding function is also discontinuous with jump discontinuities . due to such discontinuity of the _ hard _ thresholding function , we will discuss that the convergence of the _ hard _ algorithm can be easily developed according to a similar analysis of ijt algorithm in section iii .      in this subsection",
    ", we briefly review some basic properties of ijt algorithm , which serve as the basis of the further analysis in the next sections .",
    "some of these properties can be found in @xcite .",
    "[ prop_suffdecrease ] * ( proposition 2.1 and corollary 2.2 in @xcite ) * let @xmath99 be a sequence generated by ijt algorithm with a bounded initialization .",
    "assume that @xmath100 , then it holds    1 .",
    "@xmath101 , and there exists a positive constant @xmath102 such that @xmath103 as @xmath104 ; 2 .   @xmath105 as @xmath104 .    property [ prop_suffdecrease](a ) is commonly called the sufficient decrease property , which is a basic property desired for a descent method . with property [ prop_suffdecrease ] ,",
    "the subsequential convergence of ijt algorithm can be easily claimed as the following property .",
    "[ prop_subseqconv ] * ( proposition 2.3 in @xcite ) .",
    "* let @xmath99 be a sequence generated by ijt algorithm with a bounded initialization .",
    "suppose that @xmath100 , then    1 .",
    "each minimizer of @xmath106 is a fixed point of @xmath107 ; 2 .",
    "there exists a convergent subsequence of @xmath99 and the limit point is a fixed point of @xmath107 .",
    "besides properties [ prop_suffdecrease ] and [ prop_subseqconv ] , we can derive the following property directly from the definition of the proximity operator .",
    "[ prop_optcond ] let @xmath108 be a fixed point of @xmath107 and @xmath99 be a sequence generated by ijt algorithm , then it holds    1 .",
    "@xmath109 and @xmath110_i + \\lambda sign(x_i^*)\\phi'(|x_i^*|)=0 $ ] for any @xmath111 , and @xmath112_i|\\leq \\tau_{\\mu}/\\mu$ ] for any @xmath113 ; 2 .",
    "@xmath114_i$ ] for any @xmath115 and @xmath116_i| \\leq \\tau_{\\mu}$ ] for any @xmath117 , @xmath118 ,    where @xmath110_i$ ] and @xmath119_i$ ] represent the @xmath22-th component of @xmath120 and @xmath121 respectively .",
    "actually , property [ prop_optcond](a ) is a certain type of optimality conditions of the non - convex regularized optimization problem ( [ nonopt ] ) .",
    "moreover , we call @xmath108 a _ stationary point _ of ( [ nonopt ] ) if @xmath108 satisfies property [ prop_optcond](a ) , and we denote @xmath122 the stationary point for a given @xmath123 .",
    "in the last section , it can be only claimed that any sequence @xmath99 generated by ijt algorithm subsequentially converges to a stationary point . in this section , we will answer the open questions concerning ijt algorithm presented in the introduction , i.e. , when , where and how fast does the algorithm converge ? more specifically , we first prove that ijt algorithm converges to a stationary point under the so - called restricted kurdyka - ojasiewicz ( rkl ) property ( see definition [ def_rklprop ] ) , and then show that the stationary point is also a local minimizer of the optimization problem with some additional assumptions , and further demonstrate that the convergence rate of ijt algorithm is asymptotically linear .",
    "kurdyka - ojasiewicz ( kl ) property has been widely used to prove the convergence of the nonconvex algorithms ( see , @xcite for an instance ) .",
    "specifically , the kl property is the following .",
    "[ def_klprop ] * ( @xcite ) * the function @xmath124 is said to have the kurdyka - ojasiewicz property at @xmath125 dom @xmath126 if there exist @xmath127 $ ] , a neighborhood @xmath128 of @xmath108 and a continuous concave function @xmath129 such that :    1 .   @xmath130 ; 2 .",
    "@xmath131 is @xmath132 on @xmath133 ; 3 .   for all",
    "@xmath134 , @xmath135 ; 4 .   for all @xmath9 in @xmath136 ,",
    "the kurdyka - ojasiewicz inequality holds @xmath137    proper lower semi - continuous functions which satisfy the kurdyka - ojasiewicz inequality at each point of dom @xmath126 are called kl functions .    roughly speaking",
    ", kl inequality means that the function considered is sharp up to a reparametrization at a neighborhood of some point . from definition [ def_klprop ] , we can observe that kl inequality is actually certain type of first - order condition , which implies that the gradient ( subgradient or subdifferential ) of the transformed function via a concave function @xmath131 is sharp and far away from zero .",
    "functions satisfying the kl inequality include real analytic functions , semialgebraic functions and locally strongly convex functions ( more information can be referred to sec .",
    "2.2 in @xcite and references therein )",
    ".    if further the objective function @xmath106 in ( [ nonopt ] ) is a kl function and the so - called relative error condition holds for the sequence @xmath99 generated by ijt algorithm , then according to theorem 5.1 in @xcite , the strong convergence of ijt algorithm can naturally hold . however , on one hand , the relative error condition may be violated for @xmath99 .",
    "actually , as justified in the consequent lemma [ lemm_newseq3cond ] , such relative error condition only holds for the support sequence of @xmath99 . on the other hand ,",
    "as listed in appendix a , we can construct a one - dimensional function that satisfies assumptions [ aonf ] and [ aonphi ] , but is not a kl function .",
    "this motivates us to introduce the following so - called restricted kurdyka - ojasiewicz ( rkl ) property to derive the convergence of ijt algorithm . to describe the definition of rkl property",
    "conveniently , we define a projection mapping associated with an index set @xmath138 , that is , @xmath139 we also denote @xmath140 as the transpose of @xmath141 , i.e. , @xmath142 where @xmath143 is the cardinality of @xmath25 and @xmath144 .",
    "[ def_rklprop ] a function @xmath145 is said to have the @xmath25-restricted kurdyka - ojasiewicz property at @xmath125 dom @xmath126 with @xmath25 being a given subset of @xmath146 , if the function @xmath147 satisfies the kl inequality at @xmath148    obviously , the introduced rkl property is weaker than the kl property . if @xmath149",
    ", then rkl property is exactly equivalent to the kl property . from definition [ def_rklprop ]",
    ", rkl property only requires the subdifferential of the function with respect to a part of variables can get sharp after certain a concave transform , while kl property requires such well property for all the variables around some point . it can be observed that rkl property is a natural extension of kl property .",
    "assume that @xmath150 is a kl function , and @xmath151 is an arbitrary function .",
    "let @xmath152 , where @xmath153 and @xmath154 .",
    "then obviously , @xmath155 is a @xmath156-rkl function , but not a kl function . in the following , we will give a sufficient condition of the rkl property .",
    "[ lemm_suffcond_rkl ] given an index set @xmath138 , consider the function @xmath157 .",
    "assume that @xmath158 is a stationary point of @xmath159 , and @xmath159 is twice continuously differentiable at a neighborhood of @xmath158 , i.e. , @xmath160 .",
    "moreover , if @xmath161 is nonsingular , then @xmath155 satisfies @xmath25-rkl property at the point @xmath162 . actually , it holds @xmath163 for some @xmath164 and a positive constant @xmath165 .",
    "the proof of this lemma is shown in appendix b. from lemma [ lemm_suffcond_rkl ] , @xmath159 actually satisfies the kl inequality at @xmath158 with a desingularizing function of the form @xmath166 where @xmath167 is a constant .",
    "distinguished with the well - known kl inequality condition , the sufficient condition listed in the above lemma is some type of second - order condition , i.e. , the hessian of @xmath159 is nonsingular at some stationary point @xmath158 .",
    "the similar condition is also used to guarantee the convergence of the steepest descent method in @xcite ( theorem 2 , pp .",
    "obviously , if a stationary point @xmath158 is a strictly local minimizer ( or maximizer ) , or a strict saddle point of @xmath159 , then the nonsingularity of @xmath161 holds naturally .      as analyzed in the section",
    "ii , we have known that the sequence @xmath99 converges weakly .",
    "let @xmath168 be the limit point set of @xmath99 , @xmath169 .",
    "in the following , we first show that both the support and sign of the sequence will converge within finite iterations , and also any limit point @xmath170 has the same support and sign .",
    "these results are stated as the following lemma .",
    "[ lemm_suppconv ] let @xmath99 be a sequence generated by ijt algorithm .",
    "assume that @xmath100 , then there exist a sufficiently large positive integer @xmath171 , an index set @xmath25 and a sign vector @xmath172 such that when @xmath173 , it holds    1 .",
    "@xmath174 ; 2 .",
    "@xmath175 ; 3 .",
    "@xmath176 ; 4 .",
    "@xmath177 .",
    "the proof of this lemma is presented in appendix c. this lemma gives a possible way to construct a new sequence on a special subspace that has the same convergence behavior of @xmath99 .",
    "thus , if we can prove the convergence of the new sequence , then the strong convergence of @xmath99 can naturally be claimed .",
    "specifically , such new sequence can be constructed as follows . by lemma [ lemm_suppconv ]",
    ", there exists a sufficiently large integer @xmath178 such that when @xmath179 , @xmath180 therefore , we can claim that @xmath99 converges to @xmath108 if the new sequence @xmath181 converges to @xmath108 , which is also equivalent to the convergence of the sequence @xmath182 , i.e. , @xmath183 with @xmath184 and @xmath185 .",
    "let @xmath186 , then @xmath187 has the same convergence behavior of @xmath99 .    for any @xmath188 , we define a one - dimensional real space @xmath189 particularly , let @xmath190 .",
    "denote @xmath191 .",
    "we define two new functions @xmath192 and @xmath193 with @xmath194 for any @xmath195 , respectively . for any @xmath196",
    ", it can be observed that @xmath197 by lemma [ lemm_jumpthresfun ] , and @xmath158 is indeed a critical point of @xmath198 from property [ prop_optcond](a ) .",
    "moreover , we define a series of mappings @xmath199 and @xmath200 as follows @xmath201 @xmath202 @xmath203 , where @xmath204 represents the diagonal matrix generated by @xmath205 . for brevity",
    ", we will denote @xmath206 and @xmath207 as @xmath208 and @xmath209 respectively when @xmath210 is fixed and there is no confusion .    by properties",
    "[ prop_suffdecrease]-[prop_optcond ] , we can easily justify that @xmath187 satisfies the following so - called sufficient decrease , relative error and continuity conditions .",
    "[ lemm_newseq3cond ] @xmath187 satisfies the following conditions :    1 .",
    "( sufficient decrease condition ) . for each @xmath211 ,",
    "@xmath212 2 .",
    "( relative error condition ) . for each @xmath211 ,",
    "@xmath213 3 .",
    "( continuity condition ) .",
    "there exists a subsequence @xmath214 and @xmath158 such that @xmath215    from this lemma , if @xmath198 further has the kl property at the limit point @xmath158 , then according to theorem 2.9 in @xcite , @xmath187 definitely converges to @xmath158 .",
    "lemma [ lemm_newseq3cond](a ) and ( c ) are obvious by properties [ prop_suffdecrease]-[prop_subseqconv ] , the specific form of @xmath198 and the construction of @xmath187 .",
    "lemma [ lemm_newseq3cond](b ) holds mainly due to property [ prop_optcond](b ) and assumptions [ aonf]-[aonphi ] .",
    "specifically , by property [ prop_optcond](b ) , it can be easily checked that @xmath216 which implies @xmath217 thus , @xmath218 by assumption 1 , @xmath219 is lipschitz continuous with the lipschitz constant @xmath220 , then @xmath221_i - [ \\nabla f(p_i^t\\hat{z}^{n})]_i\\|_2 \\nonumber\\\\ & \\leq \\|\\nabla f(p_i^t\\hat{z}^{n+1 } ) -",
    "\\nabla f(p_i^t\\hat{z}^{n})\\|_2 \\\\\\nonumber & \\leq l\\|p_i^t\\hat{z}^{n+1 } - p_i^t\\hat{z}^{n}\\|_2 = l \\|\\hat{z}^{n+1 } - \\hat{z}^{n}\\|_2.\\end{aligned}\\ ] ] therefore , @xmath222    by lemma [ lemm_newseq3cond ] and the construction form of @xmath187 , we can obtain the following convergence result of ijt algorithm .",
    "[ thm_strongconv ] assume that @xmath44 and @xmath53 satisfy assumptions 1 and 2 , respectively .",
    "consider any sequence @xmath99 generated by ijt algorithm with a bounded initialization .",
    "suppose that @xmath100 , then @xmath99 converges subsequentially to a set @xmath168 .",
    "if further @xmath106 satisfies the @xmath25-rkl property at some limit point @xmath223 with @xmath224 , then the whole sequence @xmath99 indeed converges to @xmath108 .",
    "the first part of this theorem states that the sequence @xmath99 converges subsequentially to a limit point set @xmath168 as long as the step size parameter @xmath123 is sufficiently small .",
    "the second part shows that the objective function further satisfies the introduced rkl property at some limit point @xmath108 , then the sequence @xmath99 converges to @xmath108 .",
    "furthermore , combining lemma [ lemm_suffcond_rkl ] and theorem [ thm_strongconv ] , we can obtain the following corollary .",
    "[ coro_strongconv ] assume that @xmath44 and @xmath53 satisfy assumptions 1 and 2 , respectively .",
    "consider any sequence @xmath99 generated by ijt algorithm with a bounded initialization .",
    "suppose that @xmath100 , and if further there exists a limit point @xmath108 such that @xmath44 is twice continuously differentiable at @xmath108 and @xmath225 is nonsingular , then the whole sequence @xmath99 indeed converges to @xmath108 .",
    "as shown in corollary [ coro_strongconv ] , if @xmath225 is nonsingular at some limit point @xmath108 , then the sequence generated by ijt algorithm converges to @xmath108 , which is also a stationary point . in this subsection",
    ", we will justify that @xmath108 is also a local minimizer of the optimization problem if @xmath225 is positive definite .",
    "[ thm_localconv ] suppose that @xmath44 and @xmath53 satisfy assumptions 1 and 2 , respectively .",
    "assume that @xmath100 , and the sequence @xmath99 generated by ijt algorithm converges to @xmath108 .",
    "then @xmath108 is a local minimizer of @xmath106 provided that @xmath44 is twice continuously differentiable at @xmath108 and @xmath225 is positive definite .",
    "the proof of this theorem is rather intuitive . in the following ,",
    "we will present some simple derivations . by property [ prop_optcond](a )",
    "we have @xmath226_i + \\lambda \\phi_1(x_i^ * ) = 0 .",
    "\\label{optcond}\\ ] ] this together with the condition of the theorem @xmath227 imply that the second - order optimality conditions hold at @xmath228 , where @xmath229 for sufficiently small vector @xmath230 , we denote @xmath231 .",
    "it then follows @xmath232 furthermore , by assumption [ aonphi](c ) , it obviously holds that @xmath233_{i^c}\\|_{\\infty}+2)t/\\lambda,\\ ] ] for sufficiently small @xmath234 . by this fact and the differentiability of @xmath44",
    ", one can observe that for sufficiently small @xmath230 , there hold @xmath235_{i^c } + \\lambda \\sum_{i\\in i^c } \\phi(|h_i|)+ o(h_{i^c } ) \\nonumber\\\\ & \\geq\\sum_{i\\in i^c } ( \\|[\\nabla f(x^*)]_{i^c}\\|_{\\infty } - [ \\nabla   f(x^*)]_i+1)|h_i| \\geq 0.\\end{aligned}\\ ] ] summing up the above two inequalities ( [ supppart])-([zeropart ] ) , one has that for all sufficiently small @xmath230 , @xmath236 and hence @xmath108 is a local minimizer .    actually , we can observe that when @xmath237 , then at least one of these two inequalities ( [ supppart ] ) and ( [ zeropart ] ) will hold strictly , which implies that @xmath108 is a strictly local minimizer .      in order to derive the rate of convergence of ijt algorithm , we first show some observations on @xmath219 and @xmath63 in the neighborhood of @xmath108 . for any @xmath238",
    ", we define a neighborhood of @xmath108 as follows @xmath239 if @xmath44 is twice continuously differentiable at @xmath108 and also @xmath240 , then for any @xmath241 , there exist two sufficiently small positive constants @xmath242 and @xmath243 ( both @xmath242 and @xmath243 depending on @xmath244 with @xmath245 and @xmath246 as @xmath247 ) such that @xmath248_i - [ \\nabla f(x^*)]_i , x_i - x_i^ * \\rangle \\\\\\nonumber & \\geq ( \\lambda_{\\min}(\\nabla_{ii}^2 f(x^ * ) ) - c_f ) \\|x_i - x_i^*\\|_2 ^ 2,\\end{aligned}\\ ] ] and @xmath249 where ( [ condphi ] ) holds for @xmath63 being strictly convex on @xmath62 , and thus @xmath67 being nondecreasing on @xmath62 , consequently , @xmath250 . with the observations ( [ condf ] ) and ( [ condphi ] ) , we obtain the following theorem .",
    "[ thm_convrate1 ] suppose that @xmath44 and @xmath53 satisfy assumptions 1 and 2 , respectively .",
    "assume that the sequence @xmath99 generated by ijt algorithm converges to @xmath108 .",
    "let @xmath251 .",
    "moreover , if @xmath44 is twice continuously differentiable at @xmath108 and the following conditions hold    1 .",
    "@xmath240 ; 2 .",
    "@xmath252 3 .",
    "@xmath253 ,    then there exists a sufficiently large positive integer @xmath254 and a constant @xmath255 such that when @xmath256 @xmath257 and @xmath258    the proof of theorem [ thm_convrate1 ] is presented in appendix d. this theorem states that ijt algorithm has asymptotically linear convergence rate under certain conditions",
    ". let @xmath185 .",
    "conditions ( a ) and ( b ) in this theorem imply that the hessian of @xmath198 at @xmath158 , @xmath259 is strongly positive definite , since @xmath260 thus , @xmath198 is locally strongly convex at @xmath158 .",
    "theorem [ thm_convrate1 ] actually implies that the auxiliary sequence @xmath187 converges linearly if @xmath198 is strongly convex at @xmath158 and the step size parameter @xmath123 is sufficiently small . as shown by this theorem ,",
    "if we can fortunately obtain a sufficiently good initialization , then ijt algorithm may converge fast with a linear rate . on the other hand ,",
    "theorem [ thm_convrate1 ] also provides a posteriori computable error estimation of the algorithm , which can be used to design an efficient terminal rule of ijt algorithm .",
    "it can be observed that the conditions of theorem [ thm_convrate1 ] are slightly stricter than those of corollary [ coro_strongconv ] , and thus , @xmath108 is also a local minimizer under the conditions of theorem [ thm_convrate1 ] . in the following , we will show that the condition on @xmath123 in theorem [ thm_convrate1 ] can be extended to @xmath261 if we add some additional assumptions on the higher order differentiability of @xmath53 in the neighborhood of the local minimizer @xmath108 .",
    "we state this as the following theorem .",
    "[ thm_convrate2 ] assume that @xmath262 .",
    "let @xmath99 be a sequence generated by ijt algorithm and converge to @xmath108 .",
    ". moreover , if @xmath44 is twice continuously differentiable at @xmath108 and the following conditions hold    1 .",
    "@xmath240 , 2 .",
    "@xmath252 3 .   for any sufficiently small @xmath238 ,",
    "the derivative of @xmath67 , @xmath263 is well - defined , bounded and nonzero on the set @xmath264 , where @xmath265 ,    then there exists a sufficiently large positive integer @xmath266 and a constant @xmath267 such that when @xmath268 @xmath269 and @xmath270    the proof of this theorem is given in appendix e. note that the condition ( c ) can be easily satisfied if the penalty @xmath53 has the continuous third - order derivative on @xmath62 . in the next section , we will show that the @xmath0-norm ( @xmath1 ) is one of the most typical subclass of these non - convex penalties that satisfy the condition ( c ) in theorem [ thm_convrate2 ] .",
    "in this section , we apply the established theoretical results to a typical case , @xmath0 regularization with @xmath1 .",
    "mathematically , @xmath271 @xmath272 regularization can be formulated as follows @xmath273 where @xmath52 ( commonly , @xmath274 ) is usually called the sensing matrix , @xmath51 is called the measurement vector , @xmath9 is commonly assumed to be sparse , i.e. , @xmath275 , and @xmath276 .",
    "thus , in such special case , @xmath11 and @xmath277 with @xmath278 defined on @xmath62 . in @xcite , bredies and",
    "lorenz demonstrated that the one - dimensional proximity operator @xmath279 of @xmath0-norm can be expressed as @xmath280{ll}(\\cdot + \\lambda \\mu q sign(\\cdot ) |\\cdot|^{q-1})^{-1}(z ) , & |z|\\geq \\tau_{\\mu , q}\\\\ 0 , &   |z|\\leq \\tau_{\\mu , q } \\end{array } \\right .",
    "\\label{proxmapexplq}\\ ] ] for any @xmath93 with @xmath281 @xmath282 and the range of @xmath279 is @xmath283 .",
    "furthermore , for some special @xmath284 ( say , @xmath3 ) , the corresponding proximity operators can be expressed analytically @xcite , @xcite .    according to @xcite ( see example 5.4 , page 122 )",
    ", the function @xmath285 is a kl function and obviously satisfies the rkl propety at any limit point . by applying theorem [ thm_strongconv ] to the @xmath0 regularization",
    ", we can obtain the following corollary directly .",
    "[ coro_strongconvlq ] let @xmath99 be a sequence generated by ijt algorithm for @xmath0 regularization with @xmath72 .",
    "assume that @xmath286 , then @xmath99 converges to a stationary point of @xmath0 regularization .    in @xcite , attouch et al .",
    "showed the convergence of the inexact forward - backward splitting algorithm for @xmath0 regularization ( see theorem 5.1 , page 118 ) under exactly the same condition of corollary [ coro_strongconvlq ] .",
    "furthermore , it is easy to check that @xmath11 and @xmath70 satisfy assumptions 1 and 2 , respectively .",
    "in addition , @xmath70 also satisfies the condition ( c ) in theorem [ thm_convrate2 ] naturally .",
    "therefore , as a direct corollary of theorem [ thm_convrate2 ] , we show the asymptotically linear convergence rate of ijt algorithm for @xmath0 regularization as follows .",
    "[ coro_convratelq ] assume that @xmath287 .",
    "let @xmath99 be a sequence generated by ijt algorithm for @xmath0 ( @xmath1 ) regularization and converge to @xmath108 .",
    "let @xmath288 and @xmath251 .",
    "moreover , if the following conditions hold :    1 .",
    "@xmath289 , 2 .",
    "@xmath290    then there exists a sufficiently large positive integer @xmath291 and a constant @xmath267 such that when @xmath268 @xmath269 and @xmath292 in addition , @xmath108 is also a local minimizer of @xmath0 regularization .",
    "the condition ( b ) in corollary [ coro_convratelq ] means that the regularization parameter should be sufficiently small to guarantee that the limit point is a local minimizer . instead of adding the assumption on the regularization parameter @xmath293",
    ", we give another sufficient condition characterized by the matrix @xmath36 .",
    "such condition is mainly derived via taking advantage of the specific form of the threshold value ( [ threshvalueylq ] ) .",
    "more specifically , by ( [ threshvalueylq ] ) , it holds @xmath294 then if @xmath295 and @xmath296 the conditions in corollary [ coro_convratelq ] hold naturally .",
    "therefore , we can obtain the following theorem on the asymptotically linear convergence rate of ijt algorithm applied to @xmath0 regularization .",
    "[ thm_convratelq_mu ] assume that @xmath287 .",
    "let @xmath99 be a sequence generated by ijt algorithm for @xmath0 ( @xmath1 ) regularization and converge to @xmath108 .",
    ". moreover , if the following conditions hold :    1 .",
    "@xmath295 , 2 .",
    "@xmath297    then there exists a sufficiently large positive integer @xmath291 and a constant @xmath267 such that when @xmath268 @xmath269 and @xmath292 in addition , @xmath108 is also a local minimizer of @xmath0 regularization .    from theorem [ thm_convratelq_mu ] , it means that if the matrix @xmath36 satisfies a certain concentration property and the step size @xmath123 is chosen appropriately , then ijt algorithm can converge to a local minimizer at an asymptotically linear rate .",
    "note that the condition ( a ) in theorem [ thm_convratelq_mu ] implies @xmath298 naturally .",
    "thus , the condition ( b ) of theorem [ thm_convratelq_mu ] is a natural and reachable condition and , furthermore , whenever this condition is satisfied , the sequence @xmath99 is indeed convergent by corollary [ coro_strongconvlq ] .",
    "this shows that only the condition ( a ) is essential in theorem [ thm_convratelq_mu ] .",
    "we notice that the condition ( a ) is a concentration condition on eigenvalues of the submatrix @xmath299 , and , in particular , it implies @xmath300 or equivalently @xmath301 where @xmath302 is the condition number of @xmath299 .",
    "( [ condnum ] ) thus shows that the submatrix @xmath299 is well - conditioned with the condition number lower than @xmath303 .    in recent years",
    ", a property called the restricted isometry property ( rip ) of a matrix @xmath36 was introduced to characterize the concentration degree of the eigenvalues of its submatrix with @xmath304 columns @xcite .",
    "a matrix @xmath36 is said to be of the @xmath304-order rip ( denoted then by @xmath305-rip ) if there exists a @xmath306 such that @xmath307 in other words , the rip ensures that all submatrices of @xmath36 with @xmath304 columns are close to an isometry , and therefore distance - preserving .",
    "let @xmath308 .",
    "it can be seen from ( [ rip ] ) that if @xmath36 possesses @xmath309-rip with @xmath310 , then @xmath311 thus , we can claim that when @xmath36 satisfies a certain rip , the condition ( a ) in theorem [ thm_convratelq_mu ] can be satisfied . in particular , we have the following proposition .    [ proposition_rip ]",
    "assume that @xmath312 and @xmath36 satisfies @xmath309-rip with @xmath313 or @xmath314-rip with @xmath315 , then the condition ( a ) in theorem [ thm_convratelq_mu ] holds .",
    "this can be directly checked by the facts that @xmath316 , @xmath317 , @xmath318 , @xmath319 and @xmath320 ( c.f .",
    "proposition 1 in @xcite ) .    from proposition [ proposition_rip ] , we can see , for instance , when @xmath321 and @xmath36 satisfies @xmath309-rip with @xmath322 or @xmath314-rip with @xmath323 , the condition ( a ) in theorem [ thm_convratelq_mu ] is satisfied , and therefore , by theorem [ thm_convratelq_mu ] , ijt algorithm converges to a local minimizer of the @xmath0 regularization at an asymptotically linear rate .",
    "it is noted that in the condition of proposition [ proposition_rip ] , we always have @xmath324 and @xmath325    [ rhalf ] in a recent paper @xcite , zeng et al .",
    "have justified the convergence of a specific iterative thresholding algorithm called the iterative _ half _ thresholding algorithm for @xmath326 regularization .",
    "it can be observed that the convergence results of the iterative _ half _ thresholding algorithm obtained in @xcite is just a special case of the results presented in this section .",
    "[ rhard1 ] recently , lu @xcite proposed an iterative _ hard _ thresholding method and its variant for solving @xmath5 regularization over a conic constraint , and established its convergence as well as the iteration complexity .",
    "although the @xmath5-norm does not satisfies assumption [ aonphi ] , it can be observed that the finite support and sign convergence property ( i.e. , lemma [ lemm_suppconv ] ) holds naturally for _ hard _ algorithm due to the _ hard _ thresholding function possesses the similar discontinuity of the jumping thresholding function .",
    "furthermore , once the support of the sequence converges , the iterative form of _ hard _ algorithm is equal to the simple landweber iteration , and thus the convergence and asymptotically linear convergence rate of _ hard _ algorithm can be directly claimed .",
    "recently , attouch et al .",
    "@xcite have justified the convergence of a family of descent methods by assuming the objective function has the kl property @xcite , @xcite , and also the generated sequence satisfies the sufficient decrease property , relative error condition and continuity condition ( sec .",
    "2.3 in @xcite ) . instead of the well - known kl inequality condition",
    ", we introduce a weaker condition called the rkl property to check the convergence of ijt algorithm .",
    "besides the strong convergence , we also justify the asymptotically linear convergence rate of ijt algorithm under certain second - order conditions .",
    "compared with the other algorithms including hq @xcite , focuss @xcite , irl1 @xcite and dc programming @xcite algorithms , we derive a sufficient condition instead of the direct assumption that the accumulation points are isolated , for the convergence of ijt algorithm .",
    "furthermore , the convergence speed of ijt algorihtm is also demonstrated in this paper .",
    "besides the aforementioned non - convex algorithms , there are some other related algorithms . in the following",
    ", we will compare the obtained theoretical results of ijt algorithm with those of these algorithms .",
    "the first class of closely related algorithms are the iterative shrinkage and thresholding ( ist ) algorithms , which mainly refer to two generic algorithms and some specific algorithms .",
    "the first generic algorithm related to ijt algorithm is the generalized gradient projection ( called ggp for short ) algorithm @xcite , @xcite . in @xcite ,",
    "the ggp algorithm was proposed for the @xmath16 regularization problem .",
    "in such a convex setting , the finite support convergence and eventually linear convergence rate was given in @xcite .",
    "in @xcite , bredies and lorenz extended the ggp algorithm to solve the following general non - convex optimization model in the infinite - dimensional hilbert space @xmath327 where @xmath328 is an infinite - dimensional hilbert space , @xmath329 is assumed to be a proper lower - semicontinuous function with lipschitz continuous gradient @xmath330 , and @xmath331 is weakly lower - semicontinuous ( possibly non - smooth and non - convex ) .",
    "furthermore , the iterative form of the ggp algorithm is specified as @xmath332 where @xmath79 represents the proximity operator of @xmath69 as defined in ( [ proxoper ] ) .",
    "it can be observed that ijt algorithm is a special case of ggp algorithm when applied to a separable @xmath69 in the finite - dimensional real space .",
    "nevertheless , it was only justified that ggp algorithm can converge subsequentially to a stationary point @xcite ( that is , there is a subsequence that converges to a stationary point ) . however , as a specific case of ggp algorithm , we have justified that ijt algorithm can assuredly converge to a local minimizer at an asymptotically linear convergence rate under certain conditions .",
    "another closely related generic algorithm is the general iterative shrinkage and thresholding ( gist ) algorithm suggested in @xcite .",
    "the gist algorithm is proposed for the following general non - convex regularized optimization problem @xmath333 where @xmath44 is assumed to be continuously differentiable with lipschitz continuous derivative , and @xmath334 is a continuous function and can be rewritten as the difference of two different convex functions . as compared with assumption [ aonphi ] , we can find that the optimization model considered in this paper is distinguished from the model ( [ gistaopt ] ) studied in @xcite .",
    "moreover , only the subsequential convergence of the gist algorithm can be justified in @xcite , while the convergence of the whole sequence and further the asymptotically linear convergence rate of ijt algorithm are demonstrated in this paper .    besides these two generic algorithms , there are some other specific iterative thresholding algorithms related to ijt algorithm . among them , the _ hard _ algorithm and the _ soft _ algorithm are two representatives , which respectively solves the @xmath335 regularization and @xmath4 regularization @xcite , @xcite .",
    "it was demonstrated in @xcite , @xcite that when @xmath336 both @xmath337 and @xmath338 algorithms can converge to a stationary point whenever @xmath339 .",
    "these classical convergence results can be generalized when a step size parameter @xmath123 is incorporated with the ist procedures , and in this case , the convergence condition becomes @xmath340 it can be seen from corollary [ coro_strongconvlq ] that ( [ 4.1 ] ) is the exact condition of the convergence of ijt algorithm when applied to the @xmath0 regularization with @xmath1 , which then supports that the classical convergence results of ist has been extended to the non - convex @xmath2 ( @xmath1 ) regularization case .",
    "furthermore , it was shown in @xcite that when the measurement matrix @xmath36 satisfies the so - called finite basis injective ( fbi ) property and the stationary point possesses a strict sparsity pattern , the _ soft _ algorithm can converge to a global minimizer of @xmath4 regularization with a linear convergence rate .",
    "such result is not surprising because of the convexity of @xmath4 regularization . as for convergence speed of the _ hard _ algorithm",
    ", it was demonstrated in @xcite that under the condition @xmath336 and @xmath339 , _ hard _ algorithm will converge to a local minimizer with an asymptotically linear convergence rate .",
    "however , as algorithms for solving non - convex models , corollary [ coro_convratelq ] and theorem [ thm_convratelq_mu ] reveal that ijt algorithm shares the same asymptotic convergence speed with _",
    "hard _ algorithm .",
    "we conduct a set of numerical experiments in this section to substantiate the validity of the theoretical analysis on the convergence of ijt algorithm .",
    "while the effectiveness of ijt algorithm applied to large - scale applications such as the synthetic aperture radar ( sar ) imaging and image processing can be referred to @xcite and @xcite .",
    "( the corresponding matlab code of ijt algorithm can be referred to https://github.com/jinshanzeng/ijt_alg . )",
    "we start with an experiment to confirm the linear rate of asymptotic convergence . for this purpose ,",
    "given a sparse signal @xmath9 with dimension @xmath341 and sparsity @xmath342 shown as in fig .",
    "[ fig_asympconvrate](b ) , we considered the signal recovery problem through observation @xmath343 where the measurement matrix @xmath36 is of dimension @xmath344 with gaussian @xmath345 i.i.d . entries . such measurement matrix is known to satisfy ( with high probability ) the rip with optimal bounds @xcite , @xcite .",
    "we then applied ijt algorithm to the problem with two different non - convex penalties , that is , @xmath346 . in both cases",
    ", the jumping thresholding operators can be analytically expressed as shown in @xcite and @xcite , respectively , and thus the corresponding ijt algorithms can be efficiently implemented . in both cases",
    ", we took @xmath347 and @xmath348 .",
    "moreover , we considered two different initial guesses including 0 and the solution of the @xmath16-minimization problem to justify the effect on the convergence speed .",
    "the experiment results are reported in fig .",
    "[ fig_asympconvrate ] .    it can be seen from fig .",
    "[ fig_asympconvrate](a ) how the iteration error ( @xmath349 varies .",
    "more specifically , when 0 was taken as the initial guess , after approximately @xmath350 and @xmath351 iterations , ijt algorithm converges to a stationary point with a linear decay rate for both penalties @xmath352 and @xmath353 , as shown by the blue and black lines in fig .",
    "[ fig_asympconvrate](a ) , respectively . while from the red and green lines in fig .",
    "[ fig_asympconvrate](a ) , if we took the solution of the @xmath16-minimization problem as the initialization , the ijt algorithm converges to a stationary point with a linear convergence rate starting from almost the first iteration for both penalties .",
    "this indicates that the solution of the @xmath16-minimization problem is a good initialization , which is sufficiently close to the stationary point . moreover , fig .",
    "[ fig_asympconvrate](b ) shows that the original sparse signal has been recovered by ijt algorithm with very high accuracy .",
    "this experiment clearly justifies the convergence properties of ijt algorithm we have verified , particularly the expected asymptotically linear convergence rate of ijt algorithm is substantiated .    .",
    "( b ) recovery signal .",
    "the labels `` @xmath326 ( init : @xmath16-min ) '' and `` @xmath354 ( init : @xmath16-min ) '' represent the cases of @xmath355 and @xmath353 with the solution of the @xmath16-minimization problem as the initial guess , respectively . the labels `` @xmath326 ( init : 0 ) '' and `` @xmath354 ( init : 0 ) '' represent the cases of @xmath355 and @xmath356 with 0 as the initial guess , respectively .",
    "the recovery mses of the four cases , that is , @xmath326 ( init : @xmath16-min ) , @xmath354 ( init : @xmath16-min ) , @xmath326 ( init : 0 ) and @xmath354 ( init : 0 ) are @xmath357 , @xmath358 , @xmath359 and @xmath360 , respectively . ]",
    "\\(a ) iteration error    .",
    "( b ) recovery signal .",
    "the labels `` @xmath326 ( init : @xmath16-min ) '' and `` @xmath354 ( init : @xmath16-min ) '' represent the cases of @xmath355 and @xmath353 with the solution of the @xmath16-minimization problem as the initial guess , respectively . the labels `` @xmath326 ( init : 0 ) '' and `` @xmath354 ( init : 0 ) '' represent the cases of @xmath355 and @xmath356 with 0 as the initial guess , respectively .",
    "the recovery mses of the four cases , that is , @xmath326 ( init : @xmath16-min ) , @xmath354 ( init : @xmath16-min ) , @xmath326 ( init : 0 ) and @xmath354 ( init : 0 ) are @xmath357 , @xmath358 , @xmath359 and @xmath360 , respectively . ]",
    "\\(b ) recovery signal      as shown by the iterative form ( [ ggpa ] ) of ijt algorithm , the step size parameter @xmath123 is a crucial parameter of ijt algorithm . in this subsection",
    ", we conducted a series of experiments to verify the effect of @xmath123 on both the recovery precision and convergence speed .",
    "the measurement matrix and the true sparse signal were set the same as in subsection 6.1 .",
    "we applied ijt algorithm for both latexmath:[$\\phi(|z| ) =    @xmath123 to recover the sparse signal from the given measurements .",
    "we varied @xmath123 uniformly in the interval @xmath362 for 100 times .",
    "the experimental results are shown in fig .",
    "[ fig_mu ] .    .",
    "( a ) the trend of the recovery error .",
    "( b ) the trend of the required iteration numbers to achieve the setting accuracy .",
    "( c ) the detail trend of the required iteration numbers .",
    "the regularization parameter @xmath293 was taken as @xmath363 , the initialization was taken as the solution of the @xmath16-minimization problem and the terminal rule of ijt algorithm was set as @xmath364 for both penalties . ]",
    "\\(a ) recovery error    .",
    "( a ) the trend of the recovery error .",
    "( b ) the trend of the required iteration numbers to achieve the setting accuracy .",
    "( c ) the detail trend of the required iteration numbers .",
    "the regularization parameter @xmath293 was taken as @xmath363 , the initialization was taken as the solution of the @xmath16-minimization problem and the terminal rule of ijt algorithm was set as @xmath364 for both penalties . ]",
    "\\(b ) iteration number    .",
    "( a ) the trend of the recovery error .",
    "( b ) the trend of the required iteration numbers to achieve the setting accuracy .",
    "( c ) the detail trend of the required iteration numbers .",
    "the regularization parameter @xmath293 was taken as @xmath363 , the initialization was taken as the solution of the @xmath16-minimization problem and the terminal rule of ijt algorithm was set as @xmath364 for both penalties . ]",
    "\\(c ) detail    from fig .",
    "[ fig_mu](a ) , we can observe that @xmath123 has almost no effect on the recovery quality of ijt algorithm for both penalties .",
    "while the number of iterations required to attain the same terminal rule decreases monotonically as @xmath123 increasing as demonstrated by fig .",
    "[ fig_mu](b ) and ( c ) .",
    "this phenomenon coincides with the common sense .",
    "it demonstrates that when @xmath123 is larger , the algorithm converges faster , and thus fewer iterations are required to attain a given precision .",
    "more specifically , as shown by fig .",
    "[ fig_mu](b ) , the number of iterations decreases much sharper when @xmath365 .",
    "accordingly , we recommend that in practical application of ijt algorithm , a larger step size @xmath123 should be taken .",
    "in addition , we found that the performance of ijt algorithm for @xmath326 regularization is slightly better than the performance for @xmath354 regularization in the perspectives of both recovery quality and iteration number , as shown in fig .",
    "[ fig_mu ] .",
    "the additional advantage of ijt algorithm for @xmath326 regularization in the perspective of cpu time was also demonstrated in the next subsection over ijt algorithm for @xmath354 regularization .",
    "this set of experiments were conducted to compare the time costs of ijt algorithm , irls algorithm @xcite and irl1 algorithm @xcite for solving the same signal recovery problem with different settings @xmath366 where , as in subsection 8.2 in @xcite , we took @xmath367 , @xmath368 and @xmath369 .",
    "we applied ijt algorithm for two different penalties , i.e. , @xmath352 and @xmath353 .",
    "we implemented all algorithms using matlab without any specific optimization .",
    "in particular , we used the cvx matlab package by michael grant and stephen boyd ( http://www.stanford.edu/ @xmath370boyd / cvx/ ) to perform the weighted @xmath4-minimization at each iteration step of irl1 algorithm .",
    "again , the measurement matrix @xmath36 was taken to be the @xmath371 dimensional matrices with i.i.d .",
    "gaussian @xmath372 entries .",
    "the experiment results are shown in fig .",
    "[ fig_compreweighted ] . as shown in fig .",
    "[ fig_compreweighted](a ) , when @xmath373 is lower than @xmath374 , irls algorithm is slightly faster than ijt algorithm with @xmath375 .",
    "this is due to that in the low - dimensional cases , the computational burden of solving a low - dimensional least squares problem in irls is relatively low .",
    "nevertheless , when @xmath376 it can be observed that ijt algorithm with @xmath375 outperforms both irls and irl1 algorithms in the perspective of cpu time .",
    "furthermore , we can observe from fig .",
    "[ fig_compreweighted](b ) that as @xmath373 increases , the cpu times cost by irl1 and irls algorithms increase much faster than ijt algorithm , that is to say , the outperformance of ijt algorithm in time cost can get more significant as dimension increases .    ) . ]",
    "\\(a ) cpu time    ) . ]",
    "\\(b ) ratio of cpu time",
    "we have conducted a study of the convergence of ijt algorithm for a class of non - convex regularized optimization problems .",
    "one of the most significant features of such class of iterative thresholding algorithms is that the associated thresholding functions are discontinuous with jump discontinuities .",
    "moreover , the corresponding thresholding functions are in general not nonexpansive due to the nonconvexity of the penalties . among such class of non - convex optimization problems ,",
    "the @xmath0 ( @xmath1 ) regularization problem is one of the most typical subclass .",
    "the main contribution of this paper is the establishment of the convergence and rate - of - convergence results of ijt algorithm for a certain class of non - convex optimization problems .",
    "we first prove the finite support and sign convergence of ijt algorithm as long as @xmath377 where @xmath220 is the lipschitz constant of @xmath378 then we show the strong convergence of ijt algorithm under certain a rkl property .",
    "furthermore , we demonstrate that ijt algorithm converges to a local minimizer at an asymptotically linear rate under certain second - order conditions .",
    "when applied to the @xmath0 regularization , ijt algorithm can converge to a local minimizer at an asymptotically linear rate as long as the matrix satisfies a certain concentration property .",
    "the obtained convergence results to a local minimizer generalize those known for the _ soft _ and _ hard _ algorithms .",
    "we have also provided a set of simulations to support the correctness of the established theoretical assertions .",
    "the efficiency of ijt algorithm is further compared through simulations with the known reweighted techniques , another type of typical non - convex regularization algorithms .",
    "in the following , we give a specific one - dimensional function that satisfies assumptions 1 and 2 , but not a kl function . given any function @xmath53 satisfying assumption 2 ,",
    "let @xmath379 with @xmath155 being defined as follows @xmath380{ll}a_1(z - b_1)^2+c_1 , & \\mbox{for } \\",
    "z\\leq 1/2\\\\ \\exp\\left(-\\frac{1}{(z-1)^2}\\right)-\\phi(z)+c , & \\mbox{for } \\ 1/2<z<1\\\\ c-\\phi(1 ) , & \\mbox{for } \\",
    "z=1\\\\ \\exp\\left(-\\frac{1}{(z-1)^2}\\right)-\\phi(z)+c , & \\mbox{for } \\",
    "1<z<3/2\\\\ a_2(z - b_2)^2+c_1 , & \\mbox{for } \\   z\\geq 3/2 \\end{array } \\right . ,",
    "\\label{specificcasef}\\ ] ] where @xmath381 , @xmath382 @xmath383 @xmath384 , @xmath385 and @xmath386 thus , @xmath387{ll}a_1(z - b_1)^2+c_1 + \\phi(|z| ) , & \\mbox{for } \\",
    "z\\leq 1/2\\\\ \\exp\\left(-\\frac{1}{(z-1)^2}\\right)+c , & \\mbox{for } \\ 1/2<z<1\\\\ c , & \\mbox{for } \\",
    "z=1\\\\ \\exp\\left(-\\frac{1}{(z-1)^2}\\right)+c , & \\mbox{for } \\",
    "1<z<3/2\\\\ a_2(z - b_2)^2+c_1 + \\phi(z ) , & \\mbox{for } \\   z\\geq 3/2 \\end{array } \\right .. \\label{specificcaseg}\\ ] ] when @xmath388 , we define a function @xmath389 as @xmath390{ll}\\exp\\left(-\\frac{1}{(z-1)^2}\\right ) , & \\mbox{for } \\",
    "1/2<z<1\\\\ 0 , & \\mbox{for } \\",
    "z=1\\\\ \\exp\\left(-\\frac{1}{(z-1)^2}\\right ) , & \\mbox{for } \\ 1<z<3/2\\\\ \\end{array } \\right ..",
    "\\label{specificcaseh}\\ ] ] it can be easily checked that @xmath155 satisfies assumption 1 due to the function @xmath230 is @xmath391 and @xmath53 is @xmath392 in the interval @xmath393 . however , according to @xcite ( sec . 1 , page 1 )",
    ", it shows that @xmath230 fails to satisfy the kl inequality ( [ klineq ] ) at @xmath394 .",
    "therefore , @xmath159 must be not a kl function .",
    "the figures of @xmath155 and @xmath159 are shown in fig .",
    "[ fig_nonklfun ] with @xmath352 .",
    "note that @xmath158 is a stationary point of @xmath159 , i.e. , @xmath395 , then @xmath396 since @xmath159 is twice continuously differentiable at @xmath160 , then it obviously exists constants @xmath397 such that @xmath398 for any @xmath399 and @xmath400 .",
    "thus , it follows @xmath401    on the other hand , for any @xmath399 , there exists a @xmath402 such that @xmath403 since @xmath161 is nonsingular and by the continuity of @xmath404 at @xmath160 , then there exists @xmath164 such that for any @xmath405 @xmath406 denote @xmath407 then ( [ dt1 ] ) becomes @xmath408 let @xmath409 combining ( [ difffun1 ] ) and ( [ dt2 ] ) , it implies @xmath410 thus , we complete the proof of the lemma .      *",
    "( i ) * by property [ prop_suffdecrease](b ) , there exists a sufficiently large positive integer @xmath291 such that @xmath411 when @xmath412 .",
    "we first show that @xmath413 by contradiction .",
    "assume this is not the case , that is , @xmath414 for some @xmath415 .",
    "then it is easy to derive a contradiction through distinguishing the following two possible cases :      _ case 2 : _",
    "@xmath416 and @xmath422 under this circumstance , it is obvious that @xmath423 .",
    "thus , there exists an @xmath424 such that @xmath425 .",
    "it then follows from lemma [ lemm_jumpthresfun ] that @xmath426 and it contradicts to @xmath427 .",
    "thus , ( [ lemma3a ] ) holds true .",
    "it also means that the support set sequence @xmath428 converges .",
    "we denote @xmath25 the limit of @xmath429 . then for any @xmath268 @xmath430 .    *",
    "( ii ) * for any limit point @xmath170 , there exits a subsequence @xmath431 converging to @xmath108 , i.e. , @xmath432 thus , there exists a sufficiently large positive integer @xmath433 such that @xmath434 and @xmath435 when @xmath436 .",
    "similar to the proof procedure ( i ) , it can be also claimed that @xmath437 for any @xmath436 . on the other hand , by ( [ lemma3a ] ) , @xmath438 .",
    "thus , for any limit point @xmath108 , @xmath439 .      * ( iii ) * as @xmath441 for any @xmath179 and @xmath442 , it suffices to show that @xmath443 and @xmath444 for any @xmath445 , @xmath436 , @xmath179 .",
    "similar to the first two parts of the proof , we will first check that @xmath443 , and then @xmath444 for any @xmath445 by contradiction .",
    "we now prove @xmath443 for any @xmath445 and @xmath179 .",
    "assume this is not the case .",
    "then there exists an @xmath446 such that @xmath447 , and hence , @xmath448 from lemma [ lemm_jumpthresfun ] , it is easy to check @xmath449 which contradicts again to @xmath450 .",
    "this contradiction shows @xmath451 when @xmath179 .",
    "it follows that the sign sequence @xmath452 is convergent .",
    "let @xmath172 be the limit of the sign sequence @xmath452 .",
    "similarly , we can also show that @xmath453 whenever @xmath436 .",
    "therefore , @xmath454 when @xmath179 and for any @xmath442 .",
    "this finishes the proof of lemma [ lemm_suppconv ] .",
    "let @xmath455 and @xmath456 by the assumptions of theorem [ thm_convrate1 ] , it is easy to check that @xmath457 since both @xmath458 and @xmath243 approach to zero as @xmath244 approaches zero , then we can take a sufficiently small @xmath238 such that @xmath459 and @xmath460 furthermore , let @xmath461 then under assumptions of theorem [ thm_convrate1 ] , there hold @xmath462 and @xmath463 and further @xmath464 @xmath465 @xmath466    since @xmath99 converges to @xmath108 , then for any @xmath467 , there exists a sufficiently large integer @xmath468 ( where @xmath171 is specified as in lemma [ lemm_suppconv ] ) such that @xmath469 when @xmath412 .",
    "let @xmath169 .",
    "by lemma [ lemm_suppconv ] , it holds @xmath430 and @xmath470 when @xmath412",
    ". furthermore , by property [ prop_optcond ] , for any @xmath445 , @xmath471_i,\\ ] ] and @xmath472_i,\\ ] ] when @xmath412 . consequently , @xmath473_i - [ \\nabla f(x^*)]_i ) , \\ ] ] and",
    "then @xmath474_i - [ \\nabla f(x^*)]_i ) \\rangle .",
    "\\label{lemma4.1}\\end{aligned}\\ ] ] by ( [ condphi ] ) , the left side of ( [ lemma4.1 ] ) satisfies @xmath475 and the right side of ( [ lemma4.1 ] ) satisfies @xmath476_i - [ \\nabla f(x^*)]_i ) \\rangle \\leq\\nonumber\\\\ & \\|x_i^{n+1}-x_i^*\\|_2   \\|(x_i^{n } - x_i^ * ) - \\mu ( [ \\nabla f(x^n)]_i - [ \\nabla f(x^*)]_i)\\|_2.\\end{aligned}\\ ] ] without loss of generality , we assume that @xmath477 , otherwise , it demonstrates that ijt algorithm converges to @xmath108 in finite iterations .",
    "thus , it becomes @xmath478_i - [ \\nabla f(x^*)]_i)\\|_2.\\end{aligned}\\ ] ] furthermore , by ( [ condf ] ) , it follows @xmath479_i - [ \\nabla f(x^*)]_i)\\|_2 ^ 2 \\nonumber\\\\ & = \\|x_i^{n } - x_i^*\\|_2 ^ 2 + \\mu^2 \\|[\\nabla f(x^n)]_i",
    "- [ \\nabla f(x^*)]_i\\|_2 ^ 2 \\nonumber\\\\ & - 2\\mu \\langle x_i^{n } - x_i^ * , [ \\nabla f(x^n)]_i - [ \\nabla",
    "f(x^*)]_i \\rangle \\nonumber\\\\ & \\leq ( 1 - 2\\mu \\alpha_{f,\\varepsilon } + \\mu^2 l^2 ) \\|x_i^n - x_i^*\\|_2 ^ 2 .",
    "\\label{lemma4.3}\\end{aligned}\\ ] ] combing ( [ lemma4.2 ] ) and ( [ lemma4.3 ] ) , it implies @xmath480 let @xmath481 by ( [ rho*1])-([rho*2 ] ) , it is easy to check that @xmath482 thus , when @xmath412 @xmath483 consequently , the asymptotic convergence rate of ijt algorithm is linear .",
    "let @xmath485 by the assumptions of theorem [ thm_convrate2 ] , it holds @xmath486 . for any @xmath487 ,",
    "let @xmath488 and @xmath489 for some @xmath490 since @xmath491 is non - decreasing with respective to @xmath492 , and thus @xmath493 is non - increasing with respect to @xmath492 .",
    "therefore , there exists a positive constant @xmath494 such that @xmath495    since @xmath99 converges to @xmath108 , then there exists an @xmath496 ( where @xmath171 is specified as in lemma [ lemm_suppconv ] ) , when @xmath497 , it holds @xmath498 by lemma [ lemm_suppconv ] , when @xmath497 , it holds @xmath430 and @xmath470 , and thus @xmath499 . by property [ prop_optcond ] , for any @xmath445 , @xmath500_i - [ \\nabla f(x^*)]_i ) \\nonumber\\\\ & = ( x_{i}^{n+1 } - x_{i}^ { * } ) + sign(x_{i}^ { * } ) \\lambda \\mu ( \\phi'(|x_{i}^{n+1}|)- \\phi'(|x_{i}^{*}| ) ) . \\ ] ] by taylor expansion , for any @xmath445 , there exists an @xmath501 , such that @xmath502 where @xmath503 .",
    "let @xmath504 , then by the above two inequalities , it follows @xmath505_{i } - [ \\nabla f(x^*)]_{i } ) , \\label{th6eq2}\\ ] ] where @xmath506 denotes the hadamard product or elementwise product , @xmath507 and @xmath508 are two different diagonal matrices with @xmath509 moreover , by the twice differentiability of @xmath44 at @xmath108 , we have @xmath510_{i } - [ \\nabla f(x^*)]_{i } = \\nabla_{ii}^2 f(x^*)h_{i}^n + o(\\|h_{i}^n\\|_2 ) . \\label{taylorf2}\\ ] ] plugging ( [ taylorf2 ] ) into ( [ th6eq2 ] ) , it becomes @xmath511 where @xmath512 denotes as the identity matrix with the size @xmath513 with @xmath143 being the cardinality of the set @xmath25 . by the assumptions of theorem [ thm_convrate2 ] , for any @xmath445",
    ", @xmath514 thus , @xmath507 is invertible .",
    "then it follows @xmath515 by the definition of @xmath516 , there exists a constant @xmath517 ( depending on @xmath518 ) such that @xmath519 when @xmath520 .",
    "thus , we can take @xmath521 and @xmath522 such that when @xmath412 , @xmath523 then ( [ th6eq4 ] ) implies that @xmath524 where the second inequality holds for the definition of @xmath525 as specified in ( [ const_g ] ) and @xmath526 , the third inequality holds for @xmath527 and @xmath528 , the last inequality holds for @xmath529 and the definition of @xmath530 as specified in ( [ const_c1 ] ) .",
    "furthermore , by ( [ c_epsilon ] ) and ( [ const_c * ] ) , it holds @xmath531 therefore , it implies that @xmath532 and then @xmath533 let @xmath534 then @xmath535 thus , the asymptotic convergence rate of ijt algorithm is linear .                                                            , _ convergence of descent methods for semi - algebraic and tame problems : proximal algorithms , forward - backward splitting , and regularized gauss - seidel methods _ , math .",
    "a , 137 : 91 - 129 , 2013 .        , _ a general iteartive shrinkage and thresholding algorithm for non - convex regularized optimization problems _ , in proceedings of the 30th international conference on machine learning ( icml ) , atlanta , georgia , usa , 2013 .",
    ", a block coordinate descent method for regularized multiconvex optimization with applications to nonnegative tensor factorization and completion , siam journal on imaging sciences , 6(3 ) : 1758 - 1789 , 2013 ."
  ],
  "abstract_text": [
    "<S> in recent studies on sparse modeling , non - convex penalties have received considerable attentions due to their superiorities on sparsity - inducing over the convex counterparts . compared with the convex optimization approaches </S>",
    "<S> , however , the non - convex approaches have more challenging convergence analysis . in this paper , we study the convergence of a non - convex iterative thresholding algorithm for solving sparse recovery problems with a certain class of non - convex penalties , whose corresponding thresholding functions are discontinuous with jump discontinuities . </S>",
    "<S> therefore , we call the algorithm the iterative jumping thresholding ( ijt ) algorithm . the finite support and sign convergence of ijt algorithm is firstly verified via taking advantage of such jump discontinuity . together with the assumption of the introduced restricted kurdyka - ojasiewicz ( rkl ) property , then the strong convergence of ijt algorithm can be proved . </S>",
    "<S> furthermore , we can show that ijt algorithm converges to a local minimizer at an asymptotically linear rate under some additional conditions . </S>",
    "<S> moreover , we derive a posteriori computable error estimate , which can be used to design practical terminal rules for the algorithm . it should be pointed out that the @xmath0 quasi - norm ( @xmath1 ) is an important subclass of the class of non - convex penalties studied in this paper . in particular , when applied to the @xmath0 regularization , ijt algorithm can converge to a local minimizer with an asymptotically linear rate under certain concentration conditions . </S>",
    "<S> we provide also a set of simulations to support the correctness of theoretical assertions and compare the time efficiency of ijt algorithm for the @xmath2 regularization ( @xmath3 ) with other known typical algorithms like the iterative reweighted least squares ( irls ) algorithm and the iterative reweighted @xmath4 minimization ( irl1 ) algorithm .    </S>",
    "<S> sparse regularization , non - convex optimization , iterative thresholding algorithm , @xmath0 regularization ( @xmath1 ) , kurdyka - ojasiewicz inequality </S>"
  ]
}