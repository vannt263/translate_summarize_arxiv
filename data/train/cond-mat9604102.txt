{
  "article_text": [
    "the understanding of the learning process of neural networks is of great importance from both theoretical and applications points of view @xcite .",
    "while the properties of the simplest neural network , the perceptron , are now well explained , the picture we have for the learning phase of the far more relevant case of multilayer neural networks remains unsatisfactory .",
    "due to the internal degrees of freedom present in multilayer networks ( the state variables of the hidden units ) , the structure of the weight space inherited from the learning procedure is highly non trivial @xcite .",
    "gardner s framework of statistical mechanics@xcite has been proven to be useful in understanding the learning process by providing some bounds on the optimal performances of neural networks .",
    "in particular , it has allowed to derive the storage capacity and the generalization abilities of neural networks inferring a rule by example . however , the drawback of such an approach is that it does not give any microscopic information concerning the internal structure of the coupling space , in particular about internal representations .",
    "recently , an extension of gardner s approach has been proposed @xcite which leads to a deeper insight on the structure of the weight space by looking at the components of the latter corresponding to different states of the internal layers of the network .",
    "such an approach has been successful in explaining some known features of multilayer neural networks and has permitted to find some new results concerning their learning ",
    "generalization performances as well as to make a rigorous connection with information theory@xcite .",
    "in this paper we focus on multilayer neural networks with binary weights @xcite .",
    "this allows us to compare the analytical study with extensive numerical simulations and thus to provide a concrete check of the liability of the theory .",
    "indeed , both the structure of internal representations and the ( symmetry  breaking ) learning phase transition predicted by our theory turn out to be in remarkable agreement with the numerical findings .",
    "the paper is organized as follows . in section 2 ,",
    "we present our method from a general point of view and apply it to the parity machine with binary weights in section 3 .",
    "section 4 is devoted to numerical simulations .",
    "our results are summed up in the conclusion .",
    "as discussed in ref.@xcite , the method we adopt consists in a rather natural generalization of the well known gardner approach based on the study of the fractional weight space volume not ruled out by the optimal , yet unknown , learning process @xcite .",
    "we analyze the detailed decomposition of such volume in elementary volumes each one associated to a possible internal representations of the learned examples .",
    "the dynamical variables entering the statistical mechanics formalism are the ( binary valued ) interaction couplings and the spin  like states of the hidden units .",
    "in what follows , we focus on non  overlapping multilayer networks composed of @xmath0 perceptrons with weights @xmath1 and connected to @xmath0 sets of independent inputs @xmath2",
    "( @xmath3 , @xmath4 ) .",
    "the learning process may be thought of as a two step geometrical process taking place in the weight space from the input to the hidden layer .",
    "first the @xmath5dimensional subspace belonging to the @xmath6th perceptron ( or hidden unit ) is divided in a number of volumes ( @xmath7 ) , each of which being labeled by a @xmath8components vector @xmath9 @xmath10 is the spin variable representing the state of the @xmath6th hidden unit when pattern number @xmath11 is presented at the input .",
    "next , the solution space is defined as the direct product of the volumes belonging to all hidden nodes and satisfying the condition imposed by the decoder function @xmath12 where @xmath13 is the output classifying the input pattern .",
    "the overall space of solution is thus composed by a set of internal volumes @xmath14 identified by the @xmath15 matrix @xmath16 called _ internal representation _ of the learning examples .",
    "the computation of the whole distribution of volumes @xmath14 , both their typical size and their typical number , yields a deeper understanding on the storage problem by the comparison of the number @xmath17 of volumes giving the dominant contribution to gardner s volume with the upper bound given by total number @xmath18 of non  empty volumes ( i.e. the total number of implementable internal representations ) .",
    "moreover , the physics of the learning transition ( the freezing phenomena and the replica symmetry breaking transition ) acquires a detailed geometrical interpretation .    here",
    "we consider the case of parity machines which are characterized by a decoder function defined as the product of the internal representation , @xmath19 .",
    "as mentioned , given a set of @xmath20 binary input ",
    "output random relations , the learning process can be described as a geometrical selection process aimed to finding a suitable set of internal representations @xmath21 characterized by a non zero elementary volume @xmath22 defined by @xmath23 where @xmath24 is the heaviside function .",
    "the overall volume of the weight space available for learning ( the gardner volume @xmath25 ) can be written as @xmath26    for the learning problem , the distribution of volumes can be derived through the free",
    " energy @xmath27 by calculating the entropy @xmath28 $ ] of the volumes @xmath29 whose inverse sizes are equal to @xmath30 , given by the legendre relations @xmath31=-{\\frac{\\partial g(r)}{\\partial ( 1/r ) } } \\ ; \\ ; \\;. \\label{legendre}\\ ] ]    when @xmath32 , @xmath33 is dominated by volumes of size @xmath34 whose corresponding entropy ( i.e. the logarithm of their number divided by @xmath35 ) is @xmath36 $ ] and , at the same time , the most numerous ones are those of smaller size @xmath37 ( since in the limit @xmath38 all the @xmath39 are counted irrespective of their relative volumes ) whose entropy @xmath40 $ ] is the ( normalized ) logarithm of the total number of implementable internal representations .",
    "both @xmath41 and @xmath42 allow to built a rigorous link between statistical mechanics and information theory .",
    "the former ( @xmath41 ) coincides with the quantity of information @xmath43 contained in the internal representation distribution @xmath39 and concerning the weights whereas the latter ( @xmath42 ) is the information capacity of the system , i.e. the maximal quantity information one can extract from the knowledge of the internal representations @xcite .",
    "in the following , we shall apply the above method to derive the weight space structure of the non  overlapping parity machine with binary couplings .",
    "the analysis of binary models @xcite is indeed more complicated than that of their continuous counterpart due to replica symmetry breaking ( rsb ) effects .",
    "however , in the binary case extensive numerical simulations on finite systems become available allowing for a very detailed check of the theory .    in the computation of @xmath44 , @xmath28 $ ] and @xmath45 one assumes that , due to their extensive character , the self  averaging property holds .",
    "we proceed in the computation of the @xmath44 following the scheme presented in @xcite and discussed above .",
    "the basic technical difference with the standard gardner approach resides in the double analytic continuation inherited from the presence of two sets of replica indices in the weight vectors .",
    "the first coming from the integer power @xmath46 of the internal volumes appearing in the partition function , the second from the replica trick .",
    "the replicated partition function reads @xmath47 \\ ; \\ ; \\ ; , \\label{zvol}\\ ] ] with @xmath48 and @xmath49 and which in turn implies the introduction of four sets of order parameters . in the above formula , with no loss of generality ,",
    "we have posed @xmath50 .    at variance with gardner s approach",
    ", the partition function ( [ zvol ] ) requires a double configuration trace , over the internal state variables and the binary couplings .",
    "we find @xmath51 where @xmath52 reads @xmath53 + \\nonumber\\\\   & + &   \\alpha \\ln\\left[tr_{\\{\\tau_\\ell^\\alpha\\}}\\theta\\,\\left(\\prod_\\ell \\tau_\\ell^{\\alpha}\\right ) \\int   \\prod_\\ell \\frac{d\\vec{x}_\\ell d\\vec{\\hat{x}}_\\ell}{2\\pi}\\,\\prod_{\\alpha,\\nu,\\ell }   \\theta(x_\\ell^{\\alpha \\nu}\\tau_\\ell^\\alpha)\\ ; e^{-\\frac{1}{2}(\\sum_\\ell   \\vec{\\hat{x}}_\\ell q_\\ell \\vec{\\hat{x}}_\\ell + \\sum_\\ell \\vec{\\hat{x}}_\\ell \\vec{\\hat{x}}_\\ell )   + i \\sum_\\ell \\vec{x}_\\ell\\vec{\\hat{x}}_\\ell}\\right ] \\ ; , \\label{f}\\end{aligned}\\ ] ] with @xmath54 , @xmath55 , @xmath56 @xmath57dimensional vectors .",
    "the elements of the @xmath58 matrices @xmath59 e @xmath60 are the overlaps @xmath61 between two coupling vectors belonging to the same hidden unit @xmath6 and their conjugate variables .",
    "the simplest non trivial ansatz ( which can be physically understood within the cavity approach @xcite ) on the structure of the above matrices , the replica symmetric ( rs ) ansatz of our approach , must distinguish elements with @xmath62 or @xmath63 , whereas ignores difference between replica blocks and between hidden units .",
    "the matrices @xmath64 become independent of @xmath6 and with elements @xmath65 we then find @xmath66 \\label{grs } \\end{aligned}\\ ] ] where we have posed @xmath67 , @xmath68 and @xmath69 .",
    "one may notice that the above expression evaluated for @xmath70 reduces to the rs gardner s like result on the parity machine@xcite independent on the parameters @xmath71 and @xmath72 @xmath73 where @xmath25 is the gardner volume .",
    "the geometrical organization of the domains is thus hidden in the gardner volume and shows up only when @xmath74 or if derivatives with respect to @xmath46 are considered , leading to an explicit dependence on the order parameters @xmath75 @xmath76 in particular , the functions @xmath77 $ ] and @xmath34 , being derivatives of @xmath44 , will depend on @xmath71 and @xmath72 .",
    "the rs saddle point equations read :    \\1 ) @xmath78 : @xmath79 ^ 2 } { \\left[\\int\\delta y\\ ; \\cosh^r(\\sqrt{\\hat{q}_0}\\,x+ \\sqrt{\\hat{q}^*-\\hat{q}_0}\\,y)\\right]^2 } \\;,\\ ] ]    \\2 ) @xmath80 : @xmath81    \\3 ) @xmath82 : @xmath83 ^ 2 } { \\left[\\,tr_{\\{\\tau_\\ell\\}}\\prod_\\ell\\int \\delta x_\\ell \\,h^r(a_\\ell)\\,\\right]^2 } \\;,\\ ] ] in which @xmath84    \\4 ) @xmath85 : @xmath86 } { \\,tr_{\\{\\tau_\\ell\\}}\\prod_\\ell\\int \\delta x_\\ell \\,h^r(a_\\ell ) } \\ , \\int \\delta x_1 \\;h^{r-2}(a_1)\\ ; e^{-a_1 ^ 2}\\;.\\ ] ]    the case of the parity machine is relatively simple in that a consistent solution for the first two equations leads @xmath87 and @xmath88 ( as it happens in the computation of @xmath25@xcite ) , which means that the domains remain uncorrelated during the learning process .",
    "the latter two equations simplify to @xmath89    @xmath90    with a free energy given by @xmath91    for the parameters @xmath71,@xmath72 there are two kinds of solution : a first one @xmath92 , @xmath93 , which leads @xmath94 and @xmath95=(1-a)\\ln2 $ ] independently on @xmath46 .",
    "the second kind must be computed numerically form ( [ eq1 ] ) and ( [ eq2 ] ) .    in the replica theory , the choice of the right saddle solution , i.e. the maximization or the minimization of the free energy , is not completely straightforward due to the unusual @xmath96 analytic continuation @xcite .",
    "here we must deal with a double analytic continuation and the overall criterion that must be followed is given by @xmath97 where @xmath98 or @xmath99 indicates whether one must chose the solution which maximizes or minimizes the free energy @xmath44 respectively .",
    "like the zero entropy criterion for the binary perceptron , the behaviour of @xmath100 $ ] and @xmath45 ( the cases @xmath101 and @xmath70 being of particular interest ) tells us when the rs ansatz breaks down . notice that in the binary case also the volume size @xmath45 assumes the role of an entropy in that it coincides with ( minus ) the logarithm of the normalized number of binary weight vectors belonging to a domain .",
    "the legendre transforms ( [ legendre ] ) of @xmath44 lead to the formulas @xmath102 and @xmath103 & = &   \\frac{r^2}{2}q^*\\hat{q}^ * + \\ln \\left [ \\int\\delta y\\;2^r\\ ; \\cosh^r(\\sqrt{\\hat{q}^*}\\,y ) \\right]- \\nonumber \\\\ & & r \\frac{\\int\\delta y\\ ; \\cosh^r(\\sqrt{\\hat{q}^*}\\,y )    \\;\\ln(2\\cosh(\\sqrt{\\hat{q}^*}\\,y ) ) }    { \\int\\delta y\\;\\cosh^r(\\sqrt{\\hat{q}^*}\\,y ) }   \\ ; +    \\alpha ( k-1)\\ln2 + \\nonumber \\\\   & & \\alpha k \\ln \\left[\\int \\delta x\\ ; h^r(\\sqrt{\\frac{q^ * } { 1-q^*}}x)\\right ]   - \\alpha k r\\;\\frac{\\int \\delta x\\ ; h^r(\\sqrt{\\frac{q^ * }   { 1-q^*}}x )   \\ln h(\\sqrt{\\frac{q^ * }    { 1-q^*}}x ) }   { \\int \\delta x\\ ; h^r(\\sqrt{\\frac{q^ * }   { 1-q^*}}x ) } \\ ; \\ ; \\;. \\end{aligned}\\ ] ]    the number @xmath41 of domains composing @xmath25 is given by @xmath104=-g(1)+w(1)$ ] : @xmath105 & = & \\frac{\\hat{q}^*}{2}(q^*+1 ) -    \\frac{\\int\\delta y\\ ; \\cosh(\\sqrt{\\hat{q}^*}\\,y )    \\;\\ln(2\\cosh(\\sqrt{\\hat{q}^*}\\,y ) ) }    { \\int\\delta y\\;\\cosh(\\sqrt{\\hat{q}^*}\\,y)}\\ ; + \\nonumber\\\\   & + & ( 1-\\alpha)\\ln2 -    2\\alpha k \\;\\int \\delta x\\ ; h(\\sqrt{\\frac{q^ * }    { 1-q^*}}x )    \\;\\ln h(\\sqrt{\\frac{q^ * }     { 1-q^*}}x ) \\ ; \\ ; \\ ; .",
    "\\label{cr1 }     \\end{aligned}\\ ] ] the number @xmath42 of the most numerous domains , i.e. the total number of implementable internal representations , is given by the limit @xmath101 .",
    "we find @xmath106 and @xmath107= \\alpha ( k-1)\\ln2 + \\alpha k \\-ln \\left[\\frac{1}{2 } + \\lim_{r \\to 0 } \\int_{0}^{\\infty } \\frac{dx}{\\sqrt{2\\pi } } e^{-x^2\\frac{(1-q^*+rq^*)}{2(1-q^*)}}\\right ] \\ ; \\ ; \\;. \\label{c0}\\ ] ] the second term of the r.h.s .",
    "of above expression is different from zero only if @xmath108 , as it happens in the continuous case @xcite . in both the continuous and binary cases , beyond a certain value @xmath109 of @xmath110 ,",
    "the number of internal representations which can be realized becomes smaller than @xmath111 as the domains progressively disappear .",
    "however , in the binary case the parameters @xmath71 does not vanish continuously and a first order rsb transition to a theory described by two order parameters @xmath112 is required .    at the point where the @xmath45 vanishes the rs ansatz must be changed .",
    "following the same rsb scheme as in @xcite , the one step rsb expression is obtained by breaking the symmetry within each elementary volume and introducing the corresponding order parameters @xmath113 in place of @xmath114 .",
    "the free energy reads @xmath115^{\\frac { r}{m } } - \\nonumber\\\\ & & \\frac{\\alpha}{r}(k-1)\\ln2 - \\frac{\\alpha k}{r } \\ln\\int\\delta y \\left[\\int \\delta z h^m(\\frac{\\sqrt{q_1^*-q_0^*}\\,z + \\sqrt{q_0^*}\\,y ) } { \\sqrt{1-q_1^ * } } ) \\right]^{\\frac{r}{m } } \\ ; \\ ; \\;.\\end{aligned}\\ ] ] as for the binary perceptron , posing @xmath116 leads @xmath117 and @xmath118 therefore , we may also write @xmath119 the saddle point equation with respect to @xmath120 reads @xmath121 such equation is nothing but the condition @xmath122 that , in order to be satisfied , requires @xmath123 where the parameters values @xmath124 , @xmath125 and @xmath126 are computed at the @xmath127 transition point . from the relations",
    "@xmath128 it follows @xmath129= { \\cal n}^{rs}\\left[w^{rs}(r_c)\\right ] \\ ; \\ ; \\;.\\ ] ] in fig.1 we show the behaviour of @xmath130 versus @xmath46 for @xmath131 .",
    "the part of the curve with positive slope can not exists and hence beyond the @xmath126 value the function remains constant and equal to @xmath132 $ ] .    just like in the binary perceptron @xcite or in the random energy model @xcite ( for which the one step rsb solution is exact ) , below @xmath126 and for fixed @xmath110 , the system is completely frozen .",
    "the function @xmath130 behaves like the free energy of the above mentioned systems though in such cases the freezing takes place with respect to the temperature and beyond the critical temperature the free energy is equal to the constant value of the internal energy .",
    "the detailed phase diagram in the @xmath133 plane is reported in fig.2 .",
    "the behaviour of @xmath100 $ ] versus @xmath45 for @xmath134 and four different values of @xmath110 are shown in fig.3 .",
    "one may observe four different phases :    1 .   for @xmath135",
    ", the curve does not touch the @xmath127 abscissa and the domains have volumes between the two values @xmath136 for which the ordinate vanishes . for @xmath137=0\\right)$ ] or @xmath138=0\\right)$ ] the rs solution leads to a number of domains less then one and must be rejected .",
    "the freezing process takes place at the level of domains in that there are no domains with @xmath139 values greater then @xmath140 and lower then @xmath141 .",
    "the rsb ansatz substitutes the @xmath142 order parameter with @xmath143 .",
    "2 .   for @xmath144 ,",
    "the curve starts at @xmath127 with slope @xmath145 ; hence @xmath146={\\cal n}[w(r_c(\\alpha ) ) ] \\ ; , \\;\\forall\\ , r < r_c(\\alpha)$ ] .",
    "3 .   at @xmath147",
    "we have @xmath148 .",
    "the value @xmath149 , where the zero temperature entropy vanishes , is simply the critical capacity of a binary perceptron with @xmath150 input units ( the size of most numerous domains corresponds to the solution volume of a subperceptron ) . beyond this @xmath110 value ,",
    "the curve will be enclosed in the region of positive slope ( @xmath151 ) and the number of internal representations @xmath152 it is no longer @xmath153",
    "( i.e. the maximal one ) but is given by the value of @xmath100 $ ] at the starting point of the curve : @xmath154 \\ ; \\ ; \\;.\\ ] ] 4 .   at @xmath155",
    "the starting slope is @xmath156 and @xmath157=(1-\\alpha)\\ln2 $ ] ( consistent with the condition @xmath158 ) .",
    "5 .   for @xmath159 ,",
    "the point @xmath146=(1-\\alpha)\\ln2 $ ] is off the curve and @xmath160 is the point at which the two solutions of the saddle point equations lead to the same free energy value , i.e. such that @xmath161}{r_s(\\alpha)}+w(r_s(\\alpha ) ) = -\\frac{1}{r _ s(\\alpha)}(1-\\alpha)\\ln2 \\ ; \\ ; \\;.\\ ] ] the starting point of the curve @xmath162 grows with @xmath110 . for @xmath163 ,",
    "the correct saddle point solution is the one giving @xmath164=(1-\\alpha)\\ln2 $ ] independently on @xmath46 , i.e. the isolated point marked in fig.3 .",
    "the switch between the two solutions can be understood by noticing that it correspond to the only possible way of obtaining @xmath165 for @xmath166 . moreover",
    ", its physical meaning is that for @xmath167 it is not necessary to distinguish among different domains in that @xmath25 is dominated by the domains of zero entropy independently on the freezing process .",
    "6 .   for @xmath168",
    "only one point remains .",
    "7 .   at @xmath169",
    "also the point disappears .    in the following section we will compare the behaviour of @xmath42 and",
    "@xmath41 computed for @xmath134 with the results of numerical simulations on finite systems .",
    "very schematically we have @xmath170   & 0.277<\\,\\alpha<\\,0.41 \\\\                      ( 1-\\alpha)\\ln2   & \\alpha>0.41               \\end{array }      \\right.\\ ] ] and @xmath171 & \\alpha \\leq 0.41 \\\\",
    "( 1-\\alpha)\\ln2 & \\alpha \\geq 0.41 \\end { array}\\right . \\ ; \\ ; \\;.\\ ] ]    the overall scenario arising from the analytical computation may be summarized briefly as follows .",
    "we find a freezing transition at @xmath172 within the domains . for values of @xmath173 the domains , though still distributed over the whole space of solution ( @xmath174 ) , are composed by configurations with overlap @xmath92 .",
    "the point @xmath175 is the symmetry breaking point also corresponding to the critical capacity of the model @xmath176 @xcite .",
    "we have checked the above scenario by performing two distinct sets of extended numerical simulations on the weight space structure of a parity machine with binary weights and three hidden units .    in the first simulation we have measured both the dimension @xmath45 and the number @xmath100 $ ] of domains depending on the loading parameter @xmath110 .",
    "in particular we have considered the cases @xmath70 and @xmath101 giving respectively the measure of the number @xmath41 of domains contributing to the total gardner volume @xmath25 and the overall number @xmath42 of implementable internal representation . in the second set of simulations",
    "we have reconstructed the plot of @xmath177 and @xmath100 $ ] as function of @xmath46 and for fixed @xmath110 .",
    "the numerical method adopted is the exact enumeration of the configurations @xmath178 on finite systems .",
    "very schematically the procedure is the following .    1",
    ".   choose @xmath8 random patterns ; 2 .",
    "divide , for every subperceptron , the set of @xmath179 configurations in subsets labeled by the vectors @xmath180 ( @xmath181 ) @xmath182 3 .",
    "try all the subsets combinations between the three subperceptron and identify the domains of solutions as those which satisfy @xmath183 .",
    "the above scheme yields a parallel enumeration and classification of the @xmath184 weights configurations in the three subperceptron . to avoid ambiguities in the signs of the hidden fields",
    "the number of inputs connected to each hidden unit must be odd .",
    "the sizes of the systems taken under consideration are @xmath185 for the first type of simulation and @xmath186 for the second .",
    "more in detail , the three steps of the numerical procedure are the following .    1 .",
    "we use gaussian patterns in order to reduce finite sizes effects ( as has been done for the binary perceptron @xcite ) . from the replica method one",
    "expects that the results are equivalent to those of binary weights in that they depend only on the first two moments of the quenched variables .",
    "the classification of the @xmath184 weights configurations is as follows : we start with @xmath187 .",
    "next we compute for every @xmath6 and @xmath11 the field @xmath188 together with its sign @xmath189 so that the vector @xmath180 labels the first subset .",
    "the subsequent @xmath190 configurations are generated by means of the gray code which flips just one of the @xmath191 components at each time step and allows to update the field values with a single operation @xmath192 ( this reduces the number of operations by a factor n ) .",
    "then , depending on whether the vector @xmath180 is different from the previous one or not , we use @xmath180 as new label of the second subset or increment the number of vectors contained in the first one .",
    "we thus proceed in this way to scan the @xmath193 configurations .",
    "if @xmath8 varies from 1 to @xmath194 , every @xmath193 configuration is classified @xmath195 times on each subperceptron . at the end",
    "we obtain @xmath196 ( @xmath8 fixed ) or @xmath194 ( p varying from 1 to 3n ) tables whose columns ( in number @xmath197 ) are the @xmath180 vectors labeling the subsets and to which are associated the numbers of @xmath198 belonging to each subset .",
    "finally , in the case of a given @xmath8 , we take a column in each of the three tables and verify whether the product between the two chosen columns from the first two tables is equal to the column of the third one .",
    "if so , the internal representation given by the three columns matrix is implementable and the volume of the corresponding domain is the product of the numbers of @xmath198 belonging to the subset .",
    "once the domains volumes @xmath199 have been measured , we compute : @xmath200 @xmath201 ( which is the domain size computed on the saddle point of the partition function ) and @xmath202=-r g(r)+r w(r ) \\ ; \\ ; \\;.\\ ] ]    for the first set of simulations , the above functions are computed just for @xmath203 and the averages are taken over 10000 ( n=15 ) , 1000 ( n=21 ) or 50 ( n=27 ) samples . in the case of the second set of simulations , in order to allow for a comparison between all the finite sizes considered , @xmath110 is settled at @xmath131 .",
    "@xmath46 runs from -1.5 to 3 and the average is done over 10000 ( n=15,n=21 ) , 5000 ( n=27 ) or 200 ( n=33 ) samples .",
    "the statistical errors bars are within @xmath204 .",
    "as shown in fig.4 , both theoretical and experimental results give @xmath205 which coincides with the annealed approximation ( so that the total volume is reduced simply to a half for every added pattern and @xmath176 @xcite ) . at the value @xmath149 ( fig .",
    "5 ) , the total number of internal state vectors belonging to the most numerous volumes ( i.e. volumes characterized by @xmath101 ) becomes non  extensive ( @xmath206 ) .",
    "beyond such a value and in perfect agreement with simulations , the correct solution is given by one step of rsb which , in fact , predicts @xmath207 .",
    "as shown in fig.6 , beyond @xmath208 the domains begin to disappear and the number of internal representations ceases to be constant ( equal to @xmath153 ) and starts to decrease with @xmath110 . for @xmath70 the freezing transition takes place at @xmath209 , see fig.7 and fig.8 .",
    "as shown in fig.1 , for @xmath131 the theoretical value for the freezing transition is @xmath210 ; for @xmath211 the slope of the curve @xmath177 is zero ( it can not become positive ) and @xmath212=-0.43 $ ] . finally , the plot of @xmath100 $ ] versus @xmath45 , for @xmath131 , is given in fig.9 .",
    "in this paper we have applied the internal representation volumes approach to the case of binary multilayer networks , in particular to the non  overlapping parity machine .",
    "the chief result of our study consists in a detailed comparison between the analytical prediction and the numerical simulations , allowing for a definitive confirmation of the method .",
    "the detailed geometrical structure of the weights space predicted by the theory , both @xmath213@xmath42 as well as the rsb transitions within the volumes , turn out to be in remarkable agreement with the numerical simulations performed on finite systems .    as a general remark ,",
    "let us emphasize that multilayer neural networks with binary weights behave differently from their continuous counterpart .",
    "while the breaking of symmetry in the former occurs inside the representations volumes , we have already shown that in the case of real valued couplings the transition takes place between different volumes @xcite .",
    "therefore , the richness of the distribution of internal representations found in the continuous case , i.e. the presence of a `` finite '' number of macroscopic regions in the weight space containing a very large number of different internal representations , is partially lost when one deals with discrete weights .    the method can be easily extended @xcite to address the rule inference capability problem .",
    "thus , another very interesting and important issue related to the present approach would be the study of the distribution of metastable states arising from a gradient learning process .",
    "work is in progress along these lines .",
    "email : cocco@roma1.infn.it email : monasson@physique.ens.fr ; lptens is a unit propre du cnrs , associe  lecole normale suprieure et  luniversit de paris  sud .",
    "email : zecchina@to.infn.it ; work supported by an elsag ",
    "isi grant .",
    "j. hertz , a. krogh , r. g. palmer , _ introduction to the theory of neural computation _",
    ", addison ",
    "wesley pub .",
    "( 1991 ) r. monasson , r. zecchina , _ phys .",
    "lett _ * 75 * , 2432 ( 1995 ) e. gardner , _ j. phys .",
    "_ * a 21 * , 257 ( 1988 ) .4 cm e. gardner , b. derrida , _ j. phys . _ * a 21 * , 271 ( 1988 ) e. barkai , d. hansel , i. kanter , _ phys .",
    "lett . _ * 65 * , 2312 ( 1990 ) .4 cm e. barkai , d. hansel , h. sompolinsky , _ phys .",
    "a _ * 45 * , 4146 ( 1992 ) .4 cm a. engel , h.m .",
    "kohler , f. tschepke , h. vollmayr , a. zippelius , _ phys .",
    "rev . a _ * 45 * , 7590 ( 1992 ) e. barkai , i. kanter , _ europhys",
    "* 14 * , 107 ( 1991 ) m. opper , _ phys .",
    "* e 51 * , 3613 ( 1995 ) , preprint cond  mat/9604070 ( 1996 ) g.j .",
    "mitchison , r.m .",
    "durbin , _ bio .",
    "cybern . _ * 60 * , 345 ( 1989 ) m. mezard , g. parisi , m.a .",
    "virasoro , _ spin glass theory and beyond _ , world scientific , singapore , ( 1987 ) r. monasson , d. okane , _ europhys .",
    "* 27 * , 85 ( 1994 ) r. monasson , r. zecchina , _ learning and generalization theories of large committee  machines _ , to be published in _ int .",
    "* b * w. krauth , m. mzard _ j. physique _",
    ", * 50 * , 3057 ( 1989 ) b. derrida , _ phys . rev .",
    "_ b24 , 2613 ( 1981 ) w. krauth , m. opper , _ j. phys .",
    "_ * a 22 * , l519 ( 1989 ) h. gutfreund , y. stein , _",
    "_ * a 23 * , 2613 ( 1990 )"
  ],
  "abstract_text": [
    "<S> we study the weight space structure of the parity machine with binary weights by deriving the distribution of volumes associated to the internal representations of the learning examples . </S>",
    "<S> the learning behaviour and the symmetry breaking transition are analyzed and the results are found to be in very good agreement with extended numerical simulations . </S>"
  ]
}