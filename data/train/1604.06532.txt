{
  "article_text": [
    "tev astrophysics has become the window of extragalactic science by about three decades of development .",
    "topics to which ground - based observations of tev cosmic rays can make very important contributions are the following : supermassive black hole , acceleration mechanism of cosmic ray , dark matter and so on .",
    "extended air shower ( eas from now on ) arrays @xcite and imaging atmospheric cherenkov telescopes ( iacts from now on ) @xcite are two important techniques for ground - based observations of tev cosmic rays . compared to an iact",
    ", a large eas array can provide the large field of view(about 90 deg ; 1.8 sr ) and nearly 100@xmath2 duty cycle .",
    "so these characteristics make their observatories particularly suited to conduct all - sky surveys and detect emission from extended astrophysical sources ( larger than about 1 deg , e.g. plane of the galaxy ) @xcite .",
    "the primary energy of a shower can be evaluated by the lateral density of observed shower particles in an eas observatory . and the particle density at a certain distance from the cascade core is used as an energy estimator @xcite . whereas the energy resolution evaluated by this method can reach only about 18@xmath2 at 1 pev @xcite .    the bayesian neural networks ( bnns from now on ) @xcite is an algorithm of the neural networks trained by the bayesian statistics .",
    "it is not only a non - linear function , but also controls model complexity .",
    "so its flexibility makes it possible to discover more general relationships in data than the conventional statistical methods and its preferring simple models make it possible to solve the over - fitting problem better than the general neural networks @xcite .",
    "compared to the energy estimator mentioned above , the bnns is more suitable for the energy reconstruction in eas array experiments . in the present paper",
    ", an eas due to the interaction between a proton and the atmosphere is simulated using the aires2.8.2a with sibyll package @xcite .",
    "the bnns is applied to reconstruct the primary energy of a shower .",
    "it is discussed the comparison between the results using the bnns and energy estimator in the following sections .",
    "the idea of the bnns is to regard the process of training a neural network as a bayesian inference .",
    "bayes theorem is used to assign a posterior density to each point , @xmath3 , in the parameter space of the neural networks .",
    "each point @xmath3 denotes a neural network . in the method of bnns ,",
    "one performs a weighted average over all points in the parameter space of the neural network , that is , all neural networks .",
    "the methods make use of training data \\{(@xmath4,@xmath5 ) , ( @xmath6,@xmath7), ... ,(@xmath8,@xmath9 ) } , where @xmath10 is the known target value associated with data @xmath11 , which has @xmath12 components if there are @xmath12 input values in the regression .",
    "that is the set of data @xmath13(@xmath4,@xmath6, ...",
    ",@xmath8 ) which corresponds to the set of target @xmath14(@xmath5,@xmath7, ...",
    ",@xmath9 ) .",
    "the posterior density assigned to the point @xmath3 , that is , to a neural network , is given by bayes theorem    @xmath15    where data @xmath16 does not depend on @xmath3 , so @xmath17 .",
    "we need the likelihood @xmath18 and the prior density @xmath19 , in order to assign the posterior density @xmath20to a neural network defined by the point @xmath3 .",
    "@xmath21 is called evidence and plays the role of a normalizing constant , so we ignore the evidence .",
    "we consider a class of neural networks defined by the function    @xmath22    the neural networks have @xmath12 inputs , a single hidden layer of @xmath23 hidden nodes and one output .",
    "because of quicker convergence and better accuracy , a tanh function is used as an activation function for the hidden layer of the bnns in our work ( see eq.(2 ) ) . in the particular bnns",
    "described here , each neural network has the same structure .",
    "the parameter @xmath24 and @xmath25 are called the weights and @xmath26 and @xmath27 are called the biases . both sets of parameters",
    "are generally referred to collectively as the weights of the bnns , @xmath3 .",
    "@xmath28 is the predicted target value .",
    "we assume that the noise on target values can be modeled by the gaussian distribution .",
    "so the likelihood of @xmath29 training events is    @xmath30=\\exp[-\\sum_{i=1}^{n}(t_{i}-y\\left(x_{i},\\bar{\\theta}\\right))^2/2\\sigma^{2})]\\ ] ]    where @xmath10 is the target value , and @xmath31 is the standard deviation of the noise .",
    "it has been assumed that the events are independent with each other .",
    "then , the likelihood of the predicted target value is computed by eq . ( 3 ) .    we get the likelihood , meanwhile we need the prior to compute the posterior density . but the choice of prior is not obvious .",
    "experience suggests a reasonable class is the priors of gaussian class centered at zero , which prefers smaller rather than larger weights , because smaller weights yield smoother fits to data . in the paper , a gaussian prior",
    "is specified for each weight using the bayesian neural networks package of radford neal .",
    "the variance for weights belonging to a given group(either input - to - hidden weights(@xmath24 ) , hidden -biases(@xmath26 ) , hidden - to - output weights(@xmath25 ) or output - biases(@xmath27 ) ) is chosen to be the same : @xmath32 , @xmath33 , @xmath34 , @xmath35 , respectively .",
    "since we do nt know , a priori , what these variances should be , their values are allowed to vary over a large range , while favoring small variances .",
    "this is done by assigning each variance a gamma prior    @xmath36    where @xmath37 , and with the mean @xmath38 and shape parameter @xmath39 set to some fixed plausible values .",
    "the gamma prior is referred to as a hyperprior and the parameter of the hyperprior is called a hyperparameter .",
    "then , the posterior density , @xmath20 , is gotten according to eqs . ( 3 ) and the prior of gaussian distribution . given an event with data @xmath40 , an estimate of the target value is given by the weighted average    @xmath41    currently , the only way to perform the high dimensional integral in eq .",
    "( 5 ) is to sample the density @xmath20 with the markov chain monte carlo ( mcmc ) method@xcite . in the mcmc method ,",
    "one steps through the @xmath3 parameter space in such a way that points are visited with a probability proportional to the posterior density , @xmath20 .",
    "points where @xmath20 is large will be visited more often than points where @xmath20 is small .",
    "( 5 ) approximates the integral using the average    @xmath42    where @xmath43 is the number of points @xmath3 sampled from @xmath20 .",
    "each point @xmath3 corresponds to a different neural network with the same structure .",
    "so the average is an average over neural networks , and is closer to the real value of @xmath44 , when @xmath43 is sufficiently large .",
    "in our work , a toy detector array is designed to detect tev cosmic rays . and it is located on the ground with the altitude of 4300 m , like yangbajing , tibet , china .",
    "there are two components in the array : one is an electron detector ( ed from now on ) array covering one km@xmath1 square ; the other is a muon detector ( md from now on ) array included in the ed array .",
    "these detectors ( eds and mds ) consist of scintillator coun .",
    "but each ed is a 1@xmath451 m@xmath1 scintillator counter and covered by one 0.5 cm thick pb plate used as a gamma converter .",
    "3969 of them are located in a square grid with a side length of 16 meters .",
    "each md is a 6@xmath456 m@xmath1 scintillator counter and covered by overburden of 3 meters thick earth to remove electro - magnetic components in an air shower with a threshold energy of 2 gev .",
    "676 of them are located in a square grid with a side length of 39 meters",
    ".    a shower due to the interaction between a proton and the atmosphere is simulated by the aires2.8.4a with sibyll package and the response of its secondary particles deposited in the toy detector array is simulated with the geant4 package @xcite .",
    "the primary energy of an eas is sampled from a spectrum between 10 tev to 10@xmath46 tev and its arrival directions is sampled from a isotropic distribution in a range of the zenith angle from 0 to @xmath47/4 and azimuth angle from 0 to 2@xmath47 .",
    "the lateral distribution of charged particles from an eas has a direct relationship with its primary energy .",
    "the lateral distributions of electrons and muons are simulated with the aires2.8.4a with sibyll package . to estimate the primary energy of an eas , one has to observe lateral densities of the charged particles with ground detectors . in our work ,",
    "@xmath48 , @xmath49 and @xmath50 are regarded as the lateral densities of the charged particles . @xmath48 and @xmath49 are the sums of electrons and muons in eds and mds in @xmath51 divided by @xmath52 , respectively .",
    "@xmath53 is the distance from hit to the shower core and @xmath54 is the zenith angle of the shower .",
    "@xmath50 is the same parameter in ref .",
    "@xcite :    @xmath55    in the present paper , the primary energy of an eas is estimated with the linear fitting method(lfm from now on ) , like the lhaaso experiment @xcite , and the bnns , respectively .",
    "the parameters mentioned above are used as the inputs of the lfm and bnns in our energy estimation .",
    "there is an approximative linear relationship between the logarithms of the primary energy and density of charged particles for an eas .",
    "this relationship is used to estimate the primary energy of a shower in the lhaaso experiment @xcite . in our work , the lfm by taken as a standard method is used to estimate the primary energy , too .",
    "the primary energy @xmath56 of a shower is sampled from a power law spectrum between 10 tev to @xmath57 tev whose index is set to -2.7 in our work .",
    "the @xmath58 events are generated by the aires2.8.4a with sibyll package and used as the training sample for the lfm .",
    "that is , the approximative linear relationship between @xmath59 and @xmath60 is obtained through fitting the training sample .",
    "1 shows that this relationship :    @xmath61    the values of a and b are obtained after fitting the equation ( 8) to the training sample , respectively(see fig .",
    "1 ) . they are 0.977 and 12.763 , respectively .",
    "we get the following equation :    @xmath62    3000 showers are generated at 100 tev with the aires2.8.4a with sibyll package .",
    "the 8 different data files are prepared at 1000 tev interval in the energy range from 1000 tev to 8000 tev in the same way . besides , the 9 different data files are prepared at 5@xmath63 interval in the zenith angle range from 3@xmath63 to 45@xmath63 at 3000 tev in the same way .",
    "these data files are taken as the test samples for the lfm , and their energies are reconstructed with the equation ( 9 ) .",
    "they are used as the test samples for the bnn , too .    in fig .",
    "2 , the black triangles denote the energy resolutions reconstructed with the lfm at nine different energies . in fig .",
    "3 , the black triangles denote the energy resolutions reconstructed with the lfm at nine different zenith angles at 3000 tev .      the primary energy of an eas is sampled from a spectrum between 10 tev to 10@xmath46 tev . this spectrum is made up of three ranges with uniform distributions .",
    "5000 , 5@xmath45@xmath57 and 5@xmath45@xmath58 showers are produced in the three ranges between 10 tev and 100 tev , 100 tev and 1000 tev , and 1000 tev and 10@xmath46 tev , respectively .",
    "these showers are taken as the training samples for the bnns .",
    "the test samples for the bnns are the same ones as the lfm . in our work ,",
    "@xmath64 and @xmath60 are used as inputs to the bnns , which have the input layer of 2 inputs , the single hidden layer of 4 nodes and the output layer of a output .",
    "the logarithms of primary energies ( @xmath59 ) for the test samples are predicted by the bnns",
    ". a markov chain of neural networks is generated using the bayesian neural networks package of radford m. neal @xcite , with the training sample , in the process of the energy reconstruction .",
    "seven hundred iterations , of twenty markov chain monte - carlo steps each , are used in our work .",
    "the neural network parameters are stored after each iteration , since the correlation between adjacent steps is very high .",
    "that is , the points in neural network parameter space are saved to lessen the correlation after twenty steps .",
    "it is also necessary to discard the initial part of the markov chain because the correlation between the initial point of the chain and the ones of the part is very high .",
    "the initial three hundred iterations are discarded in our work .    in fig .",
    "2 , the black squares denote the energy resolutions reconstructed with the bnns at nine different energies . in fig .",
    "3 , the black squares denote the energy resolutions reconstructed with the bnns at nine different zenith angles at 3000 tev .",
    "fig . 2 and fig .",
    "3 illustrate the results of the energy reconstruction with the lfm and bnns . fig .",
    "2 shows that the resolutions of the different primary energies reconstructed with the bnns are obviously different from the lfm .",
    "compared to the lfm , the resolutions with the bnns decrease by 28.2% and 43.0% at 100 tev and @xmath65 tev , respectively .",
    "this improvement is more significant with increasing the primary energy .",
    "3 shows that the energy resolutions at different zenith angles reconstructed with the bnns are obviously different from the lfm at 3000tev .",
    "compared to the lfm , the resolutions with the bnns decrease by 48.5% and 18.0% at zenith angles of @xmath66 and @xmath67 , respectively .",
    "this improvement is more significant with decreasing the zenith angle .",
    "the energy resolutions are about 40% and 18% at 100 tev and 1 pev in the as-@xmath68 and lhaaso experiments @xcite , respectively .",
    "the resolutions of 23.6% and 13.4% with the bnns are obviously less than the ones in the as-@xmath68 and lhaaso experiments ( see fig .",
    "the difference between the results of the lfm and bnns is just because the bnns is a method of neural networks trained by bayesian statistics : first , the prior information explicitly , which distinguishes the bnns from the lfm , provides the necessary link between the training sample and not yet predicted future sample ; second , the bnns is a non - linear function like the neural networks , and can extract more information from data than the lfm .",
    "therefore , the bnns can be well applied to the energy reconstruction in the eas experiments for ground - based tev astrophysics , and the better energy resolution can be obtained by the bnns .",
    "although the discussion in the present paper are only for the eas experiments , it is expected that the algorithm of the bnns can also be applied to the event reconstruction of other experiments and will find wide application in the experiments of tev astrophysics .",
    "this work was supported in part by the national natural science foundation of china ( nsfc ) under the contract no .",
    "11235006 , the science fund of fujian university of technology under the contract no .",
    "gy - z14061 , the natural science foundation of fujian province in china under the contract no .",
    "2015j01577 and the science and technology projects of the education department of fujian province in china under the contract no .",
    "jb14071 .",
    "xinhua ma et al . , the lhasso collaboration , in _ proceedings of the 31st icrc , lodz 2009 _ a.u .",
    "abeysekara et al .",
    ", hawc collaboration , astrophys . j. 796 , 108 ( 2014 ) m. amenomori et al . ,",
    "as@xmath68 collaboration , phys .",
    "lett . , 69 , 2468 - 2471 ( 1992 ) b. bartoli , lhaaso collaboration , chinese physics c , 38 , 045001 ( 2014 ) aharonian f. et al . , hess collaboration , nature , 432 , 75 ( 2004 ) albert j. et al . ,",
    "magic collaboration , science , 312 , 1771 ( 2006 ) acciari v. a. et al . , veritas collaboration 2008 , astrophys . j. , 684 , l73 ( 2008 ) k. byrum et al . ,",
    "technology section of the white paper on the status and future of ground - based tev gamma - ray astronomy , arxiv : 0810.4367 r. m. neal , _",
    "bayesian learning of neural networks _ , 1st edn .",
    "( springer - verlag , new york , 1996 ) r. beale and t. jackson , _ neural computing : an introduction _",
    ", 1st edn .",
    "( adam hilger , new york , 1991 ) s.j .",
    "sciutto , aires : a minimum document , auger collaboration technical note gap-97 - 029 p. c. bhat and h. b. prosper , in _ proceedings of statistical problems in particle physics , astrophysics and cosmology , oxford , uk 12 - 15 , september 2005 _ , edited by l. lyons and m. k. unel , p. 151",
    "p. b. mackenzie , physics letters b * 226 * , 369 ( 1989 ) s. agostinelli , et al .",
    "a506 , 250 ( 2003 ) http://www.cs.toronto.edu/  radford / fbm.software.html xu chen , et al . ,",
    "proceeding of science ( icrc2015 ) , 432 ( 2015 )"
  ],
  "abstract_text": [
    "<S> a toy detector array is designed to detect a shower generated by the interaction between a tev cosmic ray and the atmosphere . in the present paper , </S>",
    "<S> the primary energies of showers detected by the detector array are reconstructed with the algorithm of bayesian neural networks ( bnns ) and a standard method like the lhaaso experiment @xcite , respectively . </S>",
    "<S> compared to the standard method , the energy resolutions are significantly improved using the bnns . and </S>",
    "<S> the improvement is more obvious for the high energy showers than the low energy ones .    </S>",
    "<S> @xmath0school of mathematics and physics , fujian university of technology , fuzhou 350118 , china    @xmath1school of information science and engineering , fujian university of technology , fuzhou 350118 , china    bayesian neural networks , energy reconstruction , tev cosmic ray astrophysics    pacs numbers : 07.05.mh , 29.85.fj , 95.55.vj </S>"
  ]
}