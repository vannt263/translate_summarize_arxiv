{
  "article_text": [
    "our understanding of non linear dynamical systems and networks has made tremendous progress during the past decades . in most cases the autonomous dynamics",
    "is studied .",
    "the situation where the network is strongly driven by an external signal has so far been less investigated even though it arises in many different contexts in the natural and artificial world .",
    "examples include networks of interacting chemicals ( proteins , rna ) in a cell driven by unpredictable external chemical signals ; networks of neurons driven by an external sensory input ; artificial neural networks and their applications in machine learning ; the response of population dynamics and ecological networks to changes in external conditions such as the weather ; the responses of stock prices to economically significant news such as a company earnings , or unemployment numbers . in all these cases taking into account the external input",
    "is essential if one wants to understand correctly the dynamics , both because the external input is often large ( it can not be treated as a small perturbation ) , and because in some cases the systems itself has been selected according to its response to the fluctuating and unpredictable external variables .",
    "the aim of the present work is to show , through the study of a specific but important example , how mean field techniques can provide a detailed understanding of dynamical networks strongly driven by an external signal . in the mean field approach the average feedback of the variables on themselves is taken into account through a self consistent equation , while the correlations between individual variables are neglected .",
    "the apparently extremely complicated dynamics of the network is thus reduced to much simpler evolution equations for a few collective variables .",
    "previous applications of the mean field approach to dynamical systems ( but without including an external input ) , and in particular neural networks , include e.g. @xcite . for previous studies of dynamical systems in the presence of external signals",
    "( with however a quite different emphasis than in the present work ) see e.g. @xcite .",
    "mean field analysis of multi - population neural networks in the presence of stochastic noise have been recently presented in @xcite .",
    "the specific system we will consider is taken from the field of artificial neural networks .",
    "it consists of a network of randomly connected idealized neurons evolving in discrete time , and driven by an external time dependent signal , known in the machine learning community as an `` echo state network '' @xcite , see also the continuous time analog with no input studied in @xcite .",
    "such systems , when supplemented by a single linear output layer , fall within the class of `` reservoir computers '' @xcite and currently hold records for several highly non trivial machine learning tasks such as time series prediction or some speech recognition benchmarks , see e.g. @xcite for a review . because of their importance in the machine learning community , it is highly desirable to better understand the dynamics of these systems .",
    "in addition they can serve as toy models for investigating the dynamics of neural networks , and more generally any recurrent dynamical systems , driven by external inputs .    here",
    "we show that the dynamics of echo state networks can be concisely described by a single collective variable , namely the variance @xmath0 of the variables describing the echo state network . in the limit when the number of internal variables is large ( which is the case in reservoir computing applications )",
    ", the variance obeys a closed evolution equation , similar to the logistic map , but with a source term ( due to the source term that drives the echo state network ) .",
    "we further show how to derive the onset of chaos in echo state networks , and we derive the lyapunov exponent . in the case of a sigmoid non linearity",
    "it can be shown that the external input stabilizes the system , as is well known in the community working on echo state networks ( see e.g. @xcite ) .",
    "we note that lyapunov exponents for dynamical systems driven by stochastic noise were studied in e.g. @xcite where it was shown that the noise can dramatically change the stability of the system .",
    "stabilization of chaotic systems by controlled inputs was described in @xcite . throughout our work",
    "we compare in the figures the predictions of the mean field theory with the exact integration of the equations of motion . in all cases",
    "we find excellent agreement .",
    "an echo state network consists of a large number @xmath1 of artificial neurons evolving in discrete time @xmath2 .",
    "we denote by @xmath3 the `` activation potential '' of neuron @xmath4 at time @xmath5 . at time @xmath6",
    ", neuron @xmath4 sends a signal to the other neurons with strength @xmath7 given by @xmath8 where the function @xmath9 is taken to be a sigmoidal functions , i.e. @xmath9 is odd , monotonously increasing , has finite limit for large @xmath10 , and its first derivative @xmath11 decreases monotonously for positive @xmath10 . by rescaling @xmath12 and @xmath10",
    "we can redefine @xmath13 .",
    "we choose the scales such that @xmath14 and @xmath15 . in the illustrative figures , we choose for @xmath9 the hyperbolic tangent @xmath16 ,",
    "as this is the form most often used in echo state networks .",
    "the update rule for the activation potentials is @xmath17 where @xmath18 is a time independent coupling matrix which gives the strength of the coupling of neuron @xmath19 to neuron @xmath4 , @xmath20 is the time dependent external input , and @xmath21 is a time independent vector which determines the strength with which the input is coupled to neuron @xmath4 .    in echo state networks ,",
    "the @xmath18 and @xmath21 are chosen independently at random , except for global scaling factors @xmath22 , @xmath23 . by adjusting these scaling factors and by using an optimized linear readout it is possible to obtain excellent performance on a variety of machine learning tasks .",
    "the general heuristic is that the factor @xmath24 should be adjusted for the system to be at the threshold of chaos , whereupon the response of the neural network to the input is highly complex , but deterministic .",
    "previous studies of the dynamics of echo state networks have aimed to understand how different dynamical regimes are related to changes in their information processing capability @xcite .",
    "most closely connected to the present work is @xcite where , based on earlier work on feed forward networks @xcite , the information processing capability of echo state networks could be studied in the limit where the number @xmath1 of neurons tends to infinity .",
    "the echo state network equations ( [ eq : xrc ] ) and ( [ eq : arc ] ) can be modified in a number of ways , all of which are also amenable to treatment using the mean field approximation .",
    "comparison between these different dynamical systems helps understand the generality but also limitations of the mean field approximation .",
    "we list here the most important such generalizations . in all cases eq .",
    "( [ eq : xrc ] ) is unchanged .",
    "it is the update rule for the activation potential eq .",
    "( [ eq : arc ] ) that is modified",
    ".      in some applications of echo state networks , such as e.g. preprocessed speech signals or image processing , there are multiple inputs @xmath25 that drive the system .",
    "this is also important in e.g. spatiotemporal processing by the cortex@xcite .",
    "we model the multiple inputs @xmath26 as independent random variables .    in this case",
    "the update rule for the activation potential becomes : @xmath27 where @xmath18 and @xmath28 are time independent coefficients .",
    "when @xmath29 then the source terms in eq .",
    "( [ eq : arc_multin ] ) become independent random variables ( provided the @xmath28 are rescaled to keep the variance of the source term finite ) , and the update rule for the activation potential becomes : @xmath30 where @xmath18 are time independent coefficients , and the @xmath31 are independent random variables .      in the annealing approximation ( see e.g. @xcite in the case of discrete variables ) , the update rule for the activation potential is : @xmath32 where @xmath33 and @xmath34 are time dependent coefficients that , at each time @xmath5 , are drawn independently at random from the same distribution . in the annealing approximation",
    "any structure arising from the fixed values of the coefficients @xmath18 and @xmath35 is erased , since these coefficients change at each time @xmath5 .",
    "the annealing approximation can be equally applied to the case of multiple inputs eq .",
    "( [ eq : arc_multin ] ) and independent inputs eq.([eq : arc_indepin ] ) .    the mean field approach ( discussed in the remainder of this article ) can be equally applied in the case of the annealing approximation .",
    "the resulting equations are identical to those obtained from equations ( [ eq : xrc ] , [ eq : arc ] , [ eq : arc_multin ] , [ eq : arc_indepin ] ) . that is the mean field approach can not reveal structure that arises from the fixed values of the coefficients @xmath18 and @xmath28 .",
    "the key insight behind the present work is to make the assumption that , at each time @xmath5 , the @xmath36 behave as independent identically distributed random variables which are also independent of the @xmath18 and the @xmath21",
    ". then the term @xmath37 in eq .",
    "( [ eq : arc ] ) is a sum of many identically distributed independent variables , and the law of large numbers tells us that this sum is distributed as a gaussian , see fig . [ fig : hist_no_source ] .    .",
    "a reservoir with size @xmath38 and normalized gain of @xmath39 and no source was run for 200 time steps , then the histogram of the activation potential was plotted in green . for comparison a gaussian with the variance predicted by the mean field theory",
    "was plotted in blue.[fig : hist_no_source ] ]    with this assumption we can compute the distribution of @xmath3 , and then using eq .",
    "( [ eq : xrc ] ) the distribution of @xmath7 .",
    "this analysis will yield a very simple one dimensional recurrence , similar to the logistic map , which captures the essence of the dynamics of the echo state network .    in more detail ,",
    "we reason as follows .",
    "because the function @xmath9 is odd , the distribution from which are drawn the @xmath36 has mean zero @xmath40 , where @xmath41 denotes ensemble average , i.e. average over the index @xmath4 at fixed time @xmath5 .",
    "we denote the variance of the @xmath36 by @xmath42 . assuming that the @xmath18 are drawn independently at random from a distribution with mean zero @xmath43=0 $ ] and variance @xmath44=w^{2}$ ] , and introducing the rescaled gain @xmath45 as @xmath46 we find that the term @xmath47 has gaussian distribution .",
    "we now assume for simplicity that the @xmath21 are drawn independently at random from a gaussian distribution with zero mean and variance @xmath48 : @xmath49 then the activation potential @xmath50 also has a gaussian distribution .",
    "( if the @xmath21 are drawn from a distribution other than gaussian , then the distribution of the @xmath3 can in principle be calculated , but the expressions will be more complicated ) . for future use",
    "we denote the variance of the @xmath3 as @xmath51 finally , the distribution of @xmath52 at time @xmath6 is given by @xmath53 we thus obtain a closed one - dimensional recurrence for the variances of @xmath36 and @xmath3 : @xmath54 where @xmath55}{\\sqrt{2\\pi\\sigma^{2}}}\\nonumber \\\\   & = & \\int dy\\ f^{2}(\\sigma y)\\ \\frac{\\exp\\left[-\\frac{y^{2}}{2}\\right]}{\\sqrt{2\\pi}}\\ .\\label{eq : f(sigma)}\\end{aligned}\\ ] ] from the properties of the sigmoid funtion @xmath9 ( given below eq .",
    "( [ eq : xrc ] ) ) , it follows that @xmath56 satisfies : @xmath57 , @xmath58 , @xmath59 , @xmath60 .    in the illustrative figures discussed below",
    "we take @xmath61 as this is the case most used in applications of echo state networks . the integral eq .",
    "( [ eq : f(sigma ) ] ) yielding the stationary solution @xmath62 must then be carried out numerically ( this can be done very efficiently ) .",
    "it is however interesting to note that the function @xmath63 can be computed analytically in two cases .",
    "the first does not correspond to a sigmoid function @xmath9 , but it is of interest as it has been used in a recent experimental implementation of reservoir computing @xcite : if @xmath64 , then @xmath65 .",
    "the second case ( obtained by first evaluating @xmath66 , see @xcite ) corresponds to a sigmoid function @xmath9 : if @xmath67 , then @xmath68 .",
    "results for these cases are similar to those when @xmath61 and also show very good agreement between the mean field approximation and the integration of the exact equations ( [ eq : xrc ] , [ eq : arc ] ) .",
    "( figures for these cases are not shown ) .",
    "we now consider the solutions of the mean field equations , i.e. the coupled one dimensional recurrence eq .",
    "( [ eq : recurrence ] ) .",
    "we first consider the case when there is no source @xmath69 .",
    "when @xmath70 , there is a single stationary solution to eq .",
    "( [ eq : recurrence ] ) : @xmath71 , corresponding to a quiescent system .",
    "@xmath72 corresponds to a branching point .",
    "when @xmath73 , the stable stationary solution of eq .",
    "( [ eq : recurrence ] ) is different from zero : @xmath74 , @xmath75 .",
    "we will see below that when @xmath73 , not only is @xmath75 but the lyapunov exponent of the system eqs .",
    "( [ eq : xrc ] , [ eq : arc ] ) is greater than @xmath76 .",
    "this therefore corresponds to a chaotic regime . in the limit of infinite gain @xmath77",
    ", we have @xmath78 , @xmath79 .    when the source @xmath20 is non - zero , integrating the recurrence eq .",
    "( [ eq : recurrence ] ) yields a distribution of values for @xmath80 and @xmath81 . in the figures , we take for illustrative purposes the @xmath20 to be independently drawn at each time",
    "@xmath5 from the same probability distribution , that is we assume there are no temporal correlations between successive values of the source . for definiteness",
    "we take the source @xmath82 to be distributed according to a gaussian with zero mean and variance @xmath83 .",
    "we denote @xmath84    comparison of the mean field theory and the exact distribution for @xmath0 obtained by integrating the equations of motion is given in fig .",
    "[ fig : histogram - source ] . and in fig .",
    "[ fig : std_var_vs_sourcevol ] we find excellent agreement between the mean field theory predictions and the exact solutions for the mean ( over time ) of @xmath85 for input strength @xmath86 as a function of @xmath87 .     of the activation potential in the presence of source .",
    "a reservoir of size @xmath88 with normalized gain @xmath39 and a single i.i.d normal source with variance @xmath89 was run for 200 time steps to remove the influence of the initial state .",
    "then the reservoir was run for 30,000 time steps , the variance of the activation potential @xmath85 was collected , and its histogram plotted in red .",
    "the same procedure was done following the mean field theory prediction of eq .",
    "( [ eq : arc ] ) and plotted in green .",
    "[ fig : histogram - source ] ]     as a function of normalized gain @xmath45 . for each activation potential ,",
    "a reservoir of size @xmath88 with a single i.i.d .",
    "gaussian source with variance @xmath90 was run according to eqs .",
    "( [ eq : xrc ] , [ eq : arc ] ) for 200 time steps to eliminate transitory effects , then the variance @xmath85 of the activation potential was recorded for 2000 time steps . for each gain , the mean and standard deviation ( over time ) of @xmath85 was computed .",
    "the mean is plotted in red , and mean @xmath91 one standard deviation are plotted in doted red .",
    "the green dashed curve ( superimposed on the red curve in the figure ) corresponds to the predictions of the mean field theory computed according to eq .",
    "( [ eq : recurrence ] ) .",
    "the asymptotic behaviors discussed in section [ mfmi ] , @xmath92 and @xmath93 , are clearly visible on the graph .",
    "[ fig : variance - vs - gain ] ]      we now consider the mean field approximation in the case where there are multiple inputs eq .",
    "( [ eq : arc_multin ] ) and independent inputs eq .",
    "( [ eq : arc_indepin ] ) .",
    "the new features that arise in this case can be understood qualitatively as follows .",
    "when there is a single input @xmath20 , then if at some time @xmath5 , @xmath20 is large ( small ) , then the @xmath94 will have larger ( smaller ) absolute value than its average , and @xmath95 will be larger ( smaller ) .",
    "thus @xmath95 fluctuates in time . when there are @xmath96 independent inputs , the same phenomenon occurs , but is less pronounced since the fluctuations of the different inputs @xmath97 tend to counterbalance each other . in the limit when there are infinitely many inputs , or equivalently when all the inputs are independent , the time dependent fluctuations of the individual inputs completely average out when we compute @xmath95 , which becomes time independent .    to analyze the case of multiple inputs , we take the coefficients @xmath28 to be independent random variables drawn from a normal distribution",
    "@xmath98 we take the source terms @xmath97 to be independent random variables , with zero mean and variance @xmath99\\ .\\ ] ] we denote by @xmath100 the variance of the source term @xmath101 .",
    "the reasoning leading to the mean field equations can then be followed exactly as in section [ mfe ] and one finds the same equations : @xmath102 the dependence on the number @xmath96 of inputs only appears in the source term @xmath103 which is a sum of @xmath96 squares of gaussians .",
    "it is therefore distributed as a chi - squared with @xmath96 degrees of freedom @xmath104 with expectation and variance @xmath105=\\xi^2   \\quad , \\quad   \\mbox{var}\\left[v^2_k(t)\\right]=2 k u^4 s^4 = \\frac{2 \\xi^4 } { k}\\ ] ] ( where the expectations are time averages ) .    thus if we keep the strength of the source term constant , that is if we keep @xmath106 constant , but increase the number of source terms , then the variance of the time dependent source term @xmath107 in the mean field eqs . ( [ eq : recurrencemult ] ) decreases . in the limit @xmath29 ,",
    "the source term becomes time independent .",
    "this is the form of the mean field equation that obtains in the case of independent inputs eq .",
    "( [ eq : arc_indepin ] ) .",
    "we now discuss in a little more detail the later case of independent inputs eq .",
    "( [ eq : arc_indepin ] ) .",
    "this corresponds to a time independent source term in eq .",
    "( [ eq : recurrencemult ] ) : @xmath108 . the recurrence eq .",
    "( [ eq : recurrencemult ] ) then admits a stationary solution .",
    "when @xmath109 , the stationary solution of eqs .",
    "( [ eq : recurrencemult ] ) is always different from zero : @xmath74 , @xmath75 . for very small gain @xmath110",
    "we have @xmath111 , @xmath112 . for large gain @xmath113",
    "the stationary solution tends to the solution in the absence of source @xmath78 , @xmath79 .    in fig .",
    "[ fig : variance - vs - gain ] we compute the variance @xmath114 of an echo state network driven by a single source when the source term is weak @xmath86 .",
    "because the source term is weak , the cases of a single input and of multiple inputs are similar : the time average of the variance is identical to the stationary solution , and the standard deviation of @xmath114 is small .",
    "the asymptotic behaviors @xmath92 and @xmath93 are clearly visible on the graph .    in fig .",
    "[ fig : std_var_vs_sourcevol ] we compute the standard deviation of @xmath95 of an echo state network driven by one source , by multiple source , and in the case of independent sources , as a function of the source volatility @xmath106 .",
    "we compare the predictions of the mean field theory eq .",
    "( [ eq : recurrencemult])and of the exact equations ( [ eq : xrc],[eq : arc],[eq : arc_multin],[eq : arc_indepin ] ) .     as a function of source volatility @xmath115 .",
    "the normalized gain was chosen to be @xmath116 .",
    "reservoirs of size @xmath88 with @xmath117 i.i.d .",
    "gaussian source with total variance @xmath118 were simulated according to eqs .",
    "( [ eq : xrc ] , [ eq : arc_multin ] ) .",
    "the reservoirs were run for 200 time steps to eliminate transitory effects , then the variance @xmath85 of the activation potential was recorded for 2000 time steps . the standard deviation ( over time ) of @xmath85",
    "was computed and is plotted in the figure using dots .",
    "this is compared the predictions of the mean field theory eq .",
    "( [ eq : recurrencemult ] ) , plotted using triangles .",
    "note that the mean field predictions go to zero as @xmath119 , but we can not have more than 500 sources for a reservoir of this size . [ fig : std_var_vs_sourcevol ] ]",
    "the mean field theory also allows a derivation of the lyapunov exponents of the echo state network . for simplicity",
    "we discuss only the case of a single input .",
    "suppose that an echo state network is run until it reaches a typical state . at some time ,",
    "say @xmath120 the solution is slightly perturbed , and then let to evolve .",
    "we therefore have two neighbouring solutions @xmath94 and @xmath121 .",
    "denote by @xmath122 .",
    "the largest lyapunov exponent of the system is given by @xmath123{\\delta^{2}(t)/\\delta^{2}(0)}$ ] .",
    "the mean field theory allows one to evaluate the largest lyapunov exponent @xmath124 .",
    "we suppose that the two neighboring solutions @xmath3 and @xmath125 have joint gaussian distribution : @xmath126 , @xmath127 .",
    "we take @xmath128 small and wish to compute how it evolves with time .",
    "we have @xmath129 where @xmath130 is the derivative of @xmath9 with respect to its argument . from this equation",
    "we derive that @xmath131}{\\sqrt{2\\pi\\sigma^{2}}}\\ .\\label{eq : stability - delta2}\\end{aligned}\\ ] ] in fig .",
    "[ lyapunov_n_steps ] we compare the lyapunov exponents computed from the exact equations of motion and from the mean field theory .    from expression ( [ eq : stability - delta2 ] )",
    "we deduce two interesting properties .",
    "first , in the absence of source the quiescent stationary solution @xmath132 has largest lyapunov exponents smaller than @xmath76 for @xmath70 , and largest lyapunov exponents larger than @xmath76 for @xmath73 .",
    "( this follows from the fact that the integral in eq .",
    "( [ eq : stability - delta2 ] ) equals @xmath76 in the limit @xmath133 , since @xmath14 ) .",
    "thus , in the absence of source , @xmath72 is the threshold for chaos in the dynamical system .",
    "second , for sigmoidal functions @xmath9 , we find the property ( well known to those who use echo state networks for machine learning tasks , see e.g. @xcite ) that increasing the strength of source term @xmath118 stabilizes the system . indeed",
    "when @xmath118 increase , @xmath81 also increases . for sigmoidal functions",
    "@xmath11 is a decreasing function of @xmath134 , and hence the integral in eq .",
    "( [ eq : stability - delta2 ] ) decreases when @xmath81 increases .",
    "[ lyapunov_vs_gain_source_vol ] for illustrations of this prediction .    , normalized gain @xmath39 , and source variance @xmath90",
    "was run for 200 steps .",
    "then the states were perturbed with i.i.d .",
    "gaussians with standard deviation @xmath135 and the size of the perturbation was recorded for 100 time steps .",
    "the lyapunov exponent @xmath124 was computed for the different lags : @xmath136{\\delta^{2}(t)/\\delta^{2}(0)}$ ] .",
    "the green line is the result of a simulation done according to eqs .",
    "( [ eq : xrc ] , [ eq : arc ] ) , the doted red line is the result of a simulation done in the annealed approximation , the straight blue line is the mean field theory prediction computed from ( [ eq : stability - delta2 ] ) . ]    , @xmath39 reservoir was run for 200 time steps , than perturbed by i.i.d .",
    "gaussians with standard deviation @xmath137 and run for 20 time steps .",
    "the lyapunov exponent was computed based on how much the perturbation changed , and plotted in red .",
    "the mean field theory prediction is in blue . in panel",
    "a the source variance is @xmath86 ; in panel b the gain is @xmath39 .",
    ", title=\"fig : \" ] , @xmath39 reservoir was run for 200 time steps , than perturbed by i.i.d .",
    "gaussians with standard deviation @xmath137 and run for 20 time steps .",
    "the lyapunov exponent was computed based on how much the perturbation changed , and plotted in red .",
    "the mean field theory prediction is in blue . in panel",
    "a the source variance is @xmath86 ; in panel b the gain is @xmath39 .",
    ", title=\"fig : \" ]",
    "in the present work we have shown that mean field theory can be applied to large random networks driven by external signals .",
    "the agreement , at least in the specific case of echo state networks , with the exact integration of the equations of motion is remarkably good , as illustrated by the figures .",
    "the mean field theory itself is closely related to the annealed approximation wherein the @xmath33 and the @xmath138 are redrawn independently at random at each time @xmath5 from the same distribution , i.e. the coupling coefficients become time dependent , see eq .",
    "( [ eq : arc_anneal ] ) .",
    "we expect the mean field theory to be exact in the large @xmath1 limit of the annealed approximation .",
    "compared with the case when there is no external signal , we recover a number of features which are well known empirically to people working with echo state networks , but which have not been derived analytically before .",
    "in particular we find that if in the absence of external signal the system has a trivial stable state ( corresponding in our analysis to the case @xmath70 ) , then in the presence of external signal the dynamics becomes non trivial .",
    "we also find that the presence of the external signal tends to stabilize the system ( i.e. lyapunov exponents which decrease when the external signal increases ) .",
    "we also compare the cases where the dynamical system is driven by independent input signals , and by a single or a small number of input signals .",
    "we find a qualitative difference .",
    "namely in the first case the mean field theory predicts that the collective variables take on stationary values , whereas in the second case the mean field theory predicts their statistical distribution ."
  ],
  "abstract_text": [
    "<S> dynamical systems driven by strong external signals are ubiquitous in nature and engineering . here </S>",
    "<S> we study echo state networks , networks of a large number of randomly connected nodes , which represent a simple model of a neural network , and have important applications in machine learning . </S>",
    "<S> we develop a mean field theory of echo state networks . </S>",
    "<S> the dynamics of the network is captured by the evolution law , similar to a logistic map , for a single collective variable . when the network is driven by many independent external signals , </S>",
    "<S> this collective variable reaches a steady state . </S>",
    "<S> but when the network is driven by a single external signal , the collective variable is non stationary but can be characterized by its time averaged distribution . </S>",
    "<S> the predictions of the mean field theory , including the value of the largest lyapunov exponent , are compared with the numerical integration of the equations of motion . </S>"
  ]
}