{
  "article_text": [
    "in the literature and in software packages there is confusion in regard to what is termed the ward hierarchical clustering method .",
    "this relates to any and possibly all of the following : ( i ) input dissimilarities , whether squared or not ; ( ii ) output dendrogram heights and whether or not their square root is used ; and ( iii ) there is a subtle but important difference that we have found in the loop structure of the stepwise dissimilarity - based agglomerative algorithm .",
    "our main objective in this work is to warn users of hierarchical clustering about this , to raise awareness about these distinctions or differences , and to urge users to check what their favorite software package is doing .    in r , the function hclust of stats with the method=``ward '' option produces results that correspond to a ward method ( ward , 1963 ) described in terms of a lance - williams updating formula using a sum of dissimilarities , which produces updated dissimilarities .",
    "this is the implementation used by , for example , wishart ( 1969 ) , murtagh ( 1985 ) on whose code the hclust implementation is based , jain and dubes ( 1988 ) , jambu ( 1989 ) , in xplore ( 2007 ) , in clustan ( www.clustan.com ) , and elsewhere .",
    "an important issue though is the form of input that is necessary to give ward s method . for an input data matrix , x , in r s hclust function the following command",
    "is required : ` hclust(dist(x)^2,method=\"ward \" ) ` . in later sections ( in particular ,",
    "section [ sect31 ] ) of this article we explain just why the squaring of the distances is a requirement for the ward method . in section [ sect4 ]",
    "( experiment 4 ) it is discussed why we may wish to take the square roots of the agglomeration , or dendrogram node height , values .    in r ,",
    "the agnes function of cluster with the method=``ward '' option is also presented as the ward method in kaufman and rousseeuw ( 1990 ) , legendre and legendre ( 2012 ) , among others .",
    "a formally similar algorithm is used , based on the lance and williams ( 1967 ) recurrence .",
    "lance and williams ( 1967 ) did not themselves consider the ward method , which instead was first investigated by wishart ( 1969 ) .",
    "what is at issue for us here starts with how hclust and agnes give different outputs when applied to the same dissimilarity matrix as input .",
    "what therefore explains the formal similarity in terms of criterion and algorithms , yet at the same time yields outputs that are different ?",
    "we recall that a distance is a positive , definite , symmetric mapping of a pair of observation vectors onto the positive reals which in addition satisfies the triangular inequality . for observations",
    "@xmath0 , @xmath1 , @xmath2 we have : @xmath3 . for observation set , @xmath4 ,",
    "with @xmath5 we can write the distance as a mapping from the cartesian product of the observation set into the positive reals : @xmath6 .",
    "a dissimilarity is usually taken as a distance but without the triangular inequality ( @xmath7 ) .",
    "lance and williams , 1967 , use the term `` an @xmath8-measure '' for a dissimilarity .",
    "an ultrametric , or tree distance , which defines a hierarchical clustering ( and also an ultrametric topology , which goes beyond a metric geometry , or a p - adic number system ) differs from a distance in that the strong triangular inequality is instead satisfied .",
    "this inequality , also commonly called the ultrametric inequality , is : @xmath9 .    for observations @xmath0 in a cluster @xmath10 , and a distance @xmath11 ( which can potentially be relaxed to a dissimilarity ) we have the following definitions .",
    "we may want to consider a mass or weight associated with observation @xmath0 , @xmath12 .",
    "typically we take @xmath13 when @xmath14 , i.e.  1 over cluster cardinality of the relevant cluster .    with the context being clear ,",
    "let @xmath10 denote the cluster ( a set ) as well as the cluster s center .",
    "we have this center defined as @xmath15 .",
    "furthermore , and again the context makes this clear , we have @xmath0 used for the observation label , or index , among all observations , and the observation vector .    some further definitions follow .",
    ": :    error sum of squares : @xmath16 .",
    ": :    variance ( or centered sum of squares ) :    @xmath17 .",
    ": :    inertia : @xmath18 which becomes    variance if @xmath13 , and becomes error sum of    squares if @xmath19 .",
    ": :    euclidean distance squared using norm @xmath20 : if    @xmath21 , i.e.  these observations    have values on attributes @xmath22 ,    @xmath23 is the attribute set , @xmath24 denotes    cardinality , then    @xmath25 .",
    "consider now a set of masses , or weights , @xmath26 for observations @xmath0 .",
    "following benzcri ( 1976 , p.  185 ) , the centered moment of order 2 , @xmath27 of the cloud ( or set ) of observations @xmath28 , is written : @xmath29 where the center of gravity of the system is @xmath30 .",
    "the variance , @xmath31 , is @xmath32 , where @xmath33 is the total mass of the cloud . due to huyghen s theorem",
    "the following can be shown ( benzcri , 1976 , p.  186 ) for clusters @xmath10 whose union make up the partition , @xmath34 :    @xmath35 @xmath36 @xmath37 @xmath38    the @xmath39 and @xmath40 definitions here are discussed in jambu ( 1978 , pp .",
    "154155 ) .",
    "the last of the above can be seen to decompose ( additively ) total variance of the cloud @xmath4 into ( first term on the right hand side ) variance of the cloud of cluster centers ( @xmath41 ) , and summed variances of the clusters .",
    "we can consider this last of the above relations as : @xmath42 , where @xmath43 is the between clusters variance , and @xmath44 is the summed within clusters variance . a range of variants of the agglomerative clustering criterion and algorithm are discussed by jambu ( 1978 ) .",
    "these include : minimum of the centered order 2 moment of the union of two clusters ( p.  156 ) ; minimum variance of the union of two clusters ( p.  156 ) ; maximum of the centered order 2 moment of a partition ( p.157 ) ; and maximum of the centered order 2 moment of a partition ( p.158 ) .",
    "jambu notes that these criteria for maximization of the centered order 2 moment , or variance , of a partition , were developed and used by numerous authors , with some of these authors introducing modifications ( such as the use of particular metrics ) . among authors referred to are ward ( 1963 ) , orlocci , wishart ( 1969 ) , and various others .      in the previous section ,",
    "the variance was written as @xmath17 .",
    "this is the so - called population variance .",
    "when viewed in statistical terms , where an unbiased estimator of the variance is needed , we require the sample variance : @xmath45 .",
    "the population quantity is used in murtagh ( 1985 ) .",
    "the sample statistic is used in le roux and rouanet ( 2004 ) , and by legendre and legendre ( 2012 ) .",
    "the sum of squares , @xmath16 , can be written in terms of all pairwise distances :    @xmath46 .",
    "this is proved as follows ( see , e.g. , legendre and fortin , 2010 ) .",
    "write    @xmath47    @xmath48 where , as already noted in section [ defns ] , @xmath49 .",
    "@xmath50    @xmath51    @xmath52    by writing out the right hand term , we see that it equals 0 .",
    "hence our result .    as noted in legendre and legendre ( 2012 )",
    "there are many other alternative expressions for calculating @xmath16 , such as using the trace of a particular distance matrix , and the sum of eigenvalues of a principal coordinate analysis of the distance matrix .",
    "the latter is invoking what is known as the parseval relation , i.e.the equivalence of the norms of vectors in inner product spaces that can be orthonormally transformed , one space to the other .",
    "lance and williams ( 1967 ) established a succinct form for the update of dissimilarities following an agglomeration .",
    "the parameters used in the update formula are dependent on the cluster criterion value .",
    "consider clusters ( including possibly singletons ) @xmath0 and @xmath1 being agglomerated to form cluster @xmath53 , and then consider the re - defining of dissimilarity relative to an external cluster ( including again possibly a singleton ) , @xmath2 .",
    "we have :    @xmath54 where @xmath11 is the dissimilarity used  which does not have to be a euclidean distance to start with , insofar as the lance and williams formula can be used as a repeatedly executed recurrence , without reference to any other or separate criterion ; coefficients @xmath55 are defined with reference to the clustering criterion used ( see tables of these coefficients in murtagh , 1985 , p.  68 ; jambu , 1989 , p.  366 ) ; and @xmath24 denotes absolute value .",
    "the lance - williams recurrence formula considers dissimilarities and not dissimilarities squared .",
    "the original lance and williams ( 1967 ) paper does not consider the ward criterion .",
    "it does however note that it allows one to `` generate an infinite set of new strategies '' for agglomerative hierarchical clustering .",
    "wishart ( 1969 ) brought the ward criterion into the lance - williams algorithmic framework .    even starting the agglomerative process with a euclidean distance",
    "will not avoid the fact that the inter - cluster ( non - singleton , i.e.  with 2 or more members ) dissimilarity does not respect the triangular inequality , and hence it does not respect this euclidean metric property .      the lance and williams recurrence formula has been generalized in various ways .",
    "see e.g.  batagelj ( 1988 ) who discusses what he terms `` generalized ward clustering '' which includes agglomerative criteria based on variance , inertia and weighted increase in variance .",
    "jambu ( 1989 , pp.356 et seq . ) considers the following cluster criteria and associated lance - williams update formula in the generalized ward framework : centered order 2 moment of a partition ; variance of a partition ; centered order 2 moment of the union of two classes ; and variance of the union of two classes .",
    "if using a euclidean distance , the murtagh ( 1985 ) and the jambu ( 1989 ) lance - williams update formulas for variance and related criteria ( as discussed by jambu , 1989 ) are associated with an alternative agglomerative hierarchical clustering algorithm which defines cluster centers following each agglomeration , and thus does not require use of the lance - williams update formula . the same is true for hierarchical agglomerative clustering based on median and centroid criteria .",
    "as noted , the lance - williams update formula uses a dissimilarity , @xmath11 .",
    "szkely and rizzo ( 2005 ) consider higher order powers of this , in the ward context : `` our proposed method extends ward s minimum variance method .",
    "ward s method minimizes the increase in total within - cluster sum of squared error .",
    "this increase is proportional to the squared euclidean distance between cluster centers .",
    "in contrast to ward s method , our cluster distance is based on euclidean distance , rather than squared euclidean distance .",
    "more generally , we define ... an objective function and cluster distance in terms of any power @xmath56 of euclidean distance in the interval ( 0,2 ] ... ward s mimimum variance method is obtained as the special case when @xmath57 . ''",
    "then it is indicated what beneficial properties the case of @xmath58 has , including : lance - williams form , ultrametricity and reducibility , space - dilation , and computational tractability . in szkely and rizzo ( 2005 ,",
    "it is stated that `` we have shown that '' the @xmath58 case , rather than @xmath57 , gives `` a method that applies to a more general class of clustering problems '' , and this finding is further emphasized in their conclusion . notwithstanding",
    "this finding of szkely and rizzo ( 2005 ) , viz.that the @xmath58 case is best , in this work our interest remains with the @xmath57 ward method .",
    "our objective in this section has been to discuss some of the ways that the ward method has been generalized .",
    "we now , in the next section , come to our central theme in this article .",
    "we now come to the central part of our work , distinguishing in subsections [ sect31 ] and [ sect32 ] how we can arrive at subtle but important differences in relation to how the ward method , or what is said to be the ward method , is understood in practice , and put into software code .",
    "we consider : data inputs , the main loop structure of the agglomerative , dissimilarity - based algorithms , and the output dendrogram node heights .",
    "the subtle but important differences that we uncover are further explored and exemplified in section [ sect4 ] .",
    "consider hierarchical clustering in the following form . on an observation set , @xmath4 ,",
    "define a dissimilarity measure .",
    "set each of the observations , @xmath59 etc .",
    "@xmath60 to be a singleton cluster .",
    "agglomerate the closest ( i.e. least dissimilarity ) pair of clusters , deleting the agglomerands , or agglomerated clusters .",
    "redefine the inter - cluster dissimilarities with respect to the newly created cluster .",
    "if @xmath61 is the cardinality of observation set @xmath4 then this agglomerative hierarchical clustering algorithm completes in @xmath62 agglomerative steps .    through use of the lance - williams update formula",
    ", we will focus on the updated dissimilarities relative to a newly created cluster . unexpectedly in this work",
    ", we found a need to focus also on the form of input dissimilarities .",
    "the function to be minimized , or minimand , in the ward2 case ( see subsection [ sect32 ] ) , as stated by kaufman and rousseeuw ( 1990 , p.  230",
    "relation ( 22 ) ) is :    @xmath63    whereas for the ward1 case , as discussed in subsection [ sect31 ] , we have :    @xmath64    it is clear therefore that the same criterion is being optimized .",
    "both implementations minimize the change in variance , or the error sum of squares .",
    "since the error sum of squares , or minimum variance , or other related , criteria are not optimized precisely in practice , due to being np - complete optimization problems ( implying therefore that only exponential search in the solution space will guarantee an optimal solution ) , we are content with good heuristics in practice , i.e.  sub - optimal solutions .",
    "such a heuristic is the sequence of two - way agglomerations carried out by a hierarchical clustering algorithm .    in either form",
    "the criterion ( [ wardcrit2 ] ) , ( [ wardcrit1 ] ) is characterized in le roux and rouanet ( 2004 , p.  109 ) as the variance index ; the inertia index ; the centered moment of order 2 ; the ward index ( citing ward , 1963 ) ; and the following  given two classes @xmath65 and @xmath66 , the variance index is the contribution of the dipole of the class centers , denoted as in ( [ wardcrit1 ] ) .",
    "the resulting clustering is termed euclidean classification by le roux and rouanet ( 2004 ) .",
    "we start with ( let us term it ) the ward1 algorithm as given in murtagh ( 1985 ) .",
    "it was initially wishart ( 1969 ) who wrote the ward algorithm in terms of the lance - williams update formula .",
    "in wishart ( 1969 ) the lance - williams formula is written in terms of squared dissimilarities , in a way that is formally identical to the following .",
    "cluster update formula :    @xmath68    @xmath69    @xmath70    for the optimand of section [ optim ] , the input dissimilarities need to be as follows : @xmath71 . note the presence of the _ squared euclidean distance _ in this initial dissimilarity specification .",
    "this is the ward algorithm of murtagh ( 1983 , 1985 and 2000 ) , and the way that ` hclust ` in r needs to be used .",
    "when , however , the ward1 algorithm is used with euclidean distances as the initial dissimilarities , then the clustering topology can be very different , as will be seen in section [ sect4 ] .",
    "the weight @xmath72 is the cluster cardinality , and thus for a singleton , @xmath73 .",
    "an immediate generalization is to consider probabilities given by @xmath74 .",
    "generalization to arbitrary weights can also be considered .",
    "ward implementations that take observation weights into account are available in murtagh ( 2000 ) .",
    "@xmath75 , i.e.0.5 times euclidean distances squared , is the sample variance ( cf .  section [ defns ] ) of the new cluster , @xmath76 . to see this , note that the variance of the new cluster @xmath77 formed by merging @xmath65 and @xmath66 is @xmath78 where @xmath79 is both cardinality and the mass of cluster @xmath65 , and @xmath20 is the euclidean norm .",
    "the new cluster s center of gravity , or mean , is @xmath80 . by using this expression for",
    "the new cluster s center of gravity ( or mean ) in the expression given for the variance , we see that we can write the variance of the new cluster @xmath81 combining @xmath65 and @xmath66 to be @xmath82 .",
    "so when @xmath83 we have the stated result , i.e.  the variance of the new cluster equaling 0.5 times euclidean distances squared .",
    "the criterion that is optimized arises from the foregoing discussion ( previous paragraph ) , i.e.  the variance of the dipole formed by the agglomerands .",
    "this is the variance of new cluster @xmath81 minus the variances of ( now agglomerated ) clusters @xmath65 and @xmath66 , which we can write as var@xmath84var@xmath85var@xmath86 .",
    "the variance of the partition containing @xmath81 necessarily decreases , so we need to minimize this decrease when carrying out an agglomeration .",
    "murtagh ( 1985 ) also shows how this optimization criterion is viewed as achieving the ( next possible ) partition with maximum between - cluster variance .",
    "maximizing between - cluster variance is the same as minimizing within - cluster variance , arising out of huyghen s variance ( or inertia ) decomposition theorem . with reference to section [ defns ]",
    "we are minimizing the change in @xmath43 , hence maximizing @xmath43 , and hence minimizing @xmath44 .",
    "jambu ( 1978 , p.  157 ) calls the ward1 algorithm the maximum centered order 2 moment of a partition ( cf .",
    "section [ defns ] above ) .",
    "the criterion is denoted by him as @xmath87 .",
    "we now look at the ward2 algorithm discussed in kaufman and rousseeuw ( 1990 ) , and legendre and legendre ( 2012 ) . at each agglomerative step , the extra sum of squares caused by agglomerating clusters is minimized , exactly as we have seen for the ward1 algorithm above .",
    "we have the following .",
    "cluster update formula :    @xmath68    @xmath88    @xmath89    exactly as for ward1 , we have input dissimilarities given by the _ squared euclidean distance_. note though the required form for this , in the case of equation [ ward2 ] : @xmath90 .",
    "it is such squared euclidean distances that interest us , since our motivation arises from the error sum of squares criterion .",
    "note , very importantly , that the @xmath91 function is not the same in equation [ ward1 ] and in equation [ ward2 ] ; this @xmath91 function is , respectively , a squared distance and a distance .    a second point to note",
    "is that equation [ ward2 ] relates to , on the right hand side , _ the square root of a weighted sum of squared distances_. consider how in equation [ ward1 ] the cluster update formula was in terms of _ a weighted sum of distances_.    a final point about equation [ ward2 ] is that in the cluster update formula it is the set of @xmath91 values that we seek .",
    "now let us look further at the relationship between equations [ ward2 ] and [ ward1 ] , and show their relationship .",
    "rewriting the cluster update formula establishes that we have :    @xmath92    @xmath93    let us use the notation @xmath94 because then , with    @xmath95    @xmath96    we see exactly the form of the lance - williams cluster update formula ( section [ sect22 ] ) .",
    "although the agglomerative clustering algorithm is not fully specified as such in cailliez and pags ( 1976 ) , it appears that the ward2 algorithm is the one attributed to ward ( 1963 ) .",
    "see their criterion @xmath97 ( cailliez and pags , 1976 , pp .",
    "531 , 571 ) .    with the appropriate choice of @xmath91 , different for ward1 and for ward2",
    ", what we have here is the identity of the algorithms ward1 and ward2 , although they are implemented to a small extent differently .",
    "we show this as follows .",
    "take the ward2 algorithm one step further than above , and write the input dissimilarities and cluster update formula using @xmath98 .",
    "we have the following then .",
    "input dissimilarities : @xmath99 .",
    "cluster update formula :    @xmath95    @xmath96    @xmath100    in this form , equation [ ward2b ] , implementation ward2 ( equation [ ward2 ] ) is _ identical _ to implementation ward1 ( equation [ ward1 ] ) . we conclude that we _ can _ have ward1 and ward2 implementations such that the outputs are identical .",
    "the hierarchical clustering programs used in this set of case studies are :    * hclust in package stats , `` the r stats package '' , in r. based on code by f.  murtagh ( murtagh , 1985 ) , included in r by ross ihaka . * agnes in package cluster , `` cluster analysis extended rousseeuw et al . '' , in r , by l. kaufman and p.j .",
    "rousseeuw . * hclust.pl , an extended version of hclust in r , by p.   legendre . in this function , the ward1 algorithm is implemented by method=``ward.d '' and the ward2 algorithm by method=``ward.d2 '' .",
    "we ensure reproducible results by providing all code used and , to begin with , by generating an input data set as follows .",
    ".... # fix the seed of the random number generator in order # to have reproducible results .",
    "set.seed(19037561 ) # create the input matrix to be used .",
    "y < - matrix(runif(20 * 4),nrow=20,ncol=4 ) # look at overall mean and column standard deviations .",
    "mean(y ) ; sd(y ) 0.4920503    # mean 0.2778538 0.3091678 0.2452009 0.2918480   # std . devs .",
    "....      the r code used is shown in the following , with output produced . in all of these experiments",
    ", we used the dendrogram node heights , associated with the agglomeration criterion values , in order to quickly show numerical equivalences .",
    "this is then followed up with displays of the dendrograms .    .... # experiment 1 ---------------------------------- x.hclustpl.wardd2 = hclust.pl(dist(y),method=\"ward.d2 \" ) x.agnes.wardd2 = agnes(dist(y),method=\"ward \" )",
    "sort(x.hclustpl.wardd2$height ) 0.1573864 0.2422061 0.2664122 0.2901741 0.3030634   0.3083869 0.3589344 0.3830281 0.3832023 0.5753823   0.6840459 0.7258152 0.7469914 0.7647439 0.8042245   0.8751259 1.2043397",
    "1.5665054 1.8584163    sort(x.agnes.wardd2$height ) 0.1573864 0.2422061 0.2664122 0.2901741 0.3030634   0.3083869 0.3589344 0.3830281 0.3832023 0.5753823   0.6840459 0.7258152 0.7469914 0.7647439 0.8042245   0.8751259 1.2043397 1.5665054 1.8584163 ....",
    "this points to : hclust.pl with the method=``ward.d2 '' option being identical to : agnes with the method=``ward '' option .",
    "figure [ expt1 ] displays the outcome , and we see the same visual result in both cases .",
    "that is , the two dendrograms are identical except for inconsequential swiveling of nodes . in group theory terminology",
    "we say that the trees are wreath product invariant .        to fully complete our reproducibility of research agenda ,",
    "this is the code used to produce figure [ expt1 ] :    .... par(mfrow = c(1,2 ) ) plot(x.hclustpl.wardd2,main=\"x.hclustpl.wardd2\",sub=\"\",xlab= \" \" ) plot(x.agnes.wardd2,which.plots=2,main=\"x.agnes.wardd2\",sub=\"\",xlab= \" \" ) ....      code used is as follows , with output shown .    ....    # experiment 2 ---------------------------------- x.hclust = hclust(dist(y)^2 , method=\"ward \" ) x.hclustpl.sq.wardd = hclust.pl(dist(y)^2 , method=\"ward.d \" )    sort(x.hclust$height ) 0.02477046 0.05866380 0.07097546 0.08420102 0.09184743   0.09510249 0.12883390 0.14671052 0.14684403 0.33106478   0.46791879 0.52680768 0.55799612 0.58483318 0.64677705   0.76584542 1.45043423",
    "2.45393902 3.45371103    sort(x.hclustpl.sq.wardd$height ) 0.02477046 0.05866380 0.07097546 0.08420102 0.09184743   0.09510249 0.12883390 0.14671052 0.14684403 0.33106478   0.46791879 0.52680768 0.55799612 0.58483318 0.64677705   0.76584542 1.45043423 2.45393902 3.45371103 ....",
    "this points to : hclust , with `` ward '' option , on squared input being identical to : hclust.pl with method=``ward.d '' option , on squared input .",
    "the clustering levels shown here in experiment 2 are the squares of the clustering levels produced by experiment 1 .",
    "figure [ expt2 ] displays the outcome , and we see the same visual result in both cases .",
    "this is the code used to produce figure [ expt2 ] :        .... par(mfrow = c(1,2 ) ) plot(x.hclust , main=\"x.hclust\",sub=\"\",xlab= \" \" ) plot(x.hclustpl.sq.wardd , main=\"x.hclustpl.sq.wardd\",sub=\"\",xlab= \" \" ) ....      in this experiment , with different ( non - squared valued ) input , we achieve a well - defined hierarchical clustering , but one that differs from ward .",
    "code used is as follows , with output shown .    ....    # experiment 3 ---------------------------------- x.hclustpl.wardd = hclust.pl(dist(y),method=\"ward.d \" ) x.hclust.nosq = hclust(dist(y),method=\"ward \" )    sort(x.hclustpl.wardd$height ) 0.1573864 0.2422061 0.2664122 0.2901741 0.3030634   0.3083869 0.3589344 0.3832023 0.4018957 0.5988721   0.7443850 0.7915592 0.7985444 0.8016877 0.8414950   0.9273739 1.4676446",
    "2.2073106 2.5687307    sort(x.hclust.nosq$height ) 0.1573864 0.2422061 0.2664122 0.2901741 0.3030634   0.3083869 0.3589344 0.3832023 0.4018957 0.5988721   0.7443850 0.7915592 0.7985444 0.8016877 0.8414950   0.9273739 1.4676446 2.2073106 2.5687307 ....",
    "this points to : hclustpl.wardd with method=``wardd '' option being the same as : hclust with method=``ward '' option .",
    "note : there is no squaring of inputs in the latter , nor in the former either .",
    "the clustering levels produced in this experiment using non - squared distances as input differ from , and are not monotonic relative to , those produced in experiments 1 and 2 .",
    "figure [ expt3 ] displays the outcome , and we see the same visual result in both cases .",
    "this is the code used to produce figure [ expt3 ] :        .... par(mfrow = c(1,2 ) ) plot(x.hclustpl.wardd , main=\"x.hclustpl.wardd\",sub=\"\",xlab= \" \" ) plot(x.hclust.nosq , main=\"x.hclust.nosq\",sub=\"\",xlab= \" \" ) ....      in this experiment , given the formal equivalences of the ward1 and ward2 implementations in sections [ sect31 ] and [ sect32 ] , we show how to bring about identical output .",
    "we do this by squaring or not squaring input dissimilarities , and by playing on the options used .    ....",
    "> # experiment 4 ---------------------------------- x.hclust = hclust(dist(y)^2 , method=\"ward \" ) x.hierclustpl.sq.wardd = hclust.pl(dist(y)^2 , method=\"ward.d \" ) x.hclustpl.wardd2 = hclust.pl(dist(y ) , method=\"ward.d2 \" ) x.agnes.wardd2 = agnes(dist(y),method=\"ward \" ) ....    we will ensure that the node heights in the tree are in `` distance '' terms , i.e.  in terms of the initial , unsquared euclidean distances as used in this article . of course , the agglomerations redefine such distances to be dissimilarities .",
    "thus it is with unsquared dissimilarities that we are concerned .",
    "while these dissimilarities are inter - cluster measures , defined in any given partition , on the other hand the inter - node measures that are defined on the tree are ultrametric .    ....",
    "sort(sqrt(x.hclust$height ) ) 0.1573864 0.2422061 0.2664122 0.2901741 0.3030634",
    "0.3083869 0.3589344 0.3830281 0.3832023 0.5753823   0.6840459 0.7258152 0.7469914 0.7647439 0.8042245   0.8751259 1.2043397 1.5665054 1.8584163    sort(sqrt(x.hierclustpl.sq.wardd$height ) ) 0.1573864 0.2422061 0.2664122 0.2901741 0.3030634   0.3083869 0.3589344 0.3830281 0.3832023 0.5753823 0.6840459 0.7258152 0.7469914 0.7647439 0.8042245   0.8751259 1.2043397",
    "1.5665054 1.8584163    sort(x.hclustpl.wardd2$height ) 0.1573864 0.2422061 0.2664122 0.2901741 0.3030634   0.3083869 0.3589344 0.3830281 0.3832023 0.5753823 0.6840459 0.7258152 0.7469914 0.7647439 0.8042245   0.8751259 1.2043397",
    "1.5665054 1.8584163    sort(x.agnes.wardd2$height ) 0.1573864 0.2422061 0.2664122 0.2901741 0.3030634   0.3083869 0.3589344 0.3830281 0.3832023 0.5753823 0.6840459 0.7258152 0.7469914 0.7647439",
    "0.8042245   0.8751259 1.2043397 1.5665054 1.8584163    ....    there is no difference of course between sorting the squared agglomeration or height levels , versus sorting them and then squaring them .",
    "consider the following examples , the first repeated from the foregoing ( experiment 4 ) batch of results .    ....",
    "sqrt(sort(x.hierclustpl.sq.wardd$height ) ) 0.1573864 0.2422061 0.2664122 0.2901741 0.3030634   0.3083869 0.3589344 0.3830281 0.3832023 0.5753823 0.6840459 0.7258152 0.7469914 0.7647439 0.8042245   0.8751259 1.2043397",
    "1.5665054 1.8584163    sqrt(sort(x.hierclustpl.sq.wardd$height ) ) 0.1573864 0.2422061 0.2664122 0.2901741 0.3030634   0.3083869 0.3589344 0.3830281 0.3832023 0.5753823 0.6840459 0.7258152 0.7469914 0.7647439 0.8042245   0.8751259 1.2043397 1.5665054 1.8584163    ....    our experiment 4 points to :     : :    output of hclustpl.wardd2 , with the method=``ward.d2 '' option   : :    or   : :    output of agnes , with the method=``ward '' option   : :    being the same as both of the following with node heights square    rooted :   : :    hclust , with the `` ward '' option on squared input ,   : :    hclust.pl , with the method=``ward.d '' option on squared input .",
    "figure [ expt4 ] displays two of these outcomes , and we see the same visual result in both cases , in line with the numerical node ( or agglomeration ) `` height '' values .",
    "this is the code used to produce figure [ expt4 ] :        .... par(mfrow = c(1,2 ) ) temp < - x.hclust temp$height",
    "< - sqrt(x.hclust$height ) plot(temp , main=\"x.hclust -- sqrt(height ) \" , sub= \" \" , xlab= \" \" ) plot(x.hclustpl.wardd2 , main=\"x.hclustpl.wardd2 \" , sub= \" \" , xlab= \" \" ) ....",
    "a short discussion follows on the implications of this work . in experiments 1 and 2",
    ", we see the crucial importance of inputs ( squared or not ) and options used .",
    "we set out , with experiment 1 , to implement ward2 . with experiment 2",
    ", we set out to implement ward1 .",
    "experiment 3 shows how easy it is to modify either implementation , ward1 or ward2 , to get another ( well defined ) non - ward hierarchy .",
    "finally experiment 4 shows the underlying equivalence of the experiment 1 and experiment 2 results , i.e.  respectively the ward2 and ward1 implementations .    on looking closely at the experiment 1 and experiment 2 figures , figures [ expt1 ] and [ expt2 ] , we can see that the morphology of the dendrograms is the same",
    "however the cluster criterion values ",
    "the node heights  are not the same .    from section [ sect32 ] , ward2 implementation ,",
    "the cluster criterion value is most naturally the square root of the same cluster criterion value as used in section [ sect31 ] , ward1 implementation . from a dendrogram morphology viewpoint , this is not important because one morphology is the same as the other ( as we have observed above ) . from an optimization viewpoint ( section [ optim ] )",
    ", it plays no role either since one optimant is monotonically related to the other .",
    "those were reasons as to why it makes no difference to choose the ward1 implementation versus the ward2 implementation .",
    "next , we will look at some practical differences .",
    "looking closer at forms of the criterion in ( [ wardcrit2 ] ) and ( [ wardcrit1 ] ) in section [ optim ]  and contrasting these forms of the criterion with the input dissimilarities in sections [ sect31 ] ( ward1 ) and [ sect32 ] ( ward2 ) leads us to the following observation .",
    "the ward2 criterion values are `` on a scale of distances '' whereas the ward1 criterion values are `` on a scale of distances squared '' .",
    "hence to make direct comparisons between the ultrametric distances read off a dendrogram , and compare them to the input distances , it is preferable to use the ward2 form of the criterion . thus , the use of cophenetic correlations can be more directly related to the dendrogram produced .",
    "alternatively , with the ward1 form of the criterion , we can just take the square root of the dendrogram node heights .",
    "this we have seen in the generation of figure [ expt4 ] .",
    "the above algorithm can be used for `` stored dissimilarity '' and `` stored data '' implementations , a distinction first made in anderberg ( 1973 ) .",
    "the latter is where the dissimilarity matrix is not used , but instead the dissimilarities are created on the fly .",
    "murtagh ( 2000 ) has implementations of the latter , `` stored data '' , implementations ( programs hc.f , hcl.java , see murtagh , 2000 ) as well as the former , `` stored data '' ( hcon2.f ) . for both ,",
    "murtagh ( 1985 ) lists the formulas .",
    "the nearest neighbor and reciprocal nearest neighbor algorithms can be applied to bypass the need for a strict sequencing of the agglomerations .",
    "see murtagh ( 1983 , 1985 ) .",
    "these algorithms provide for provably worst case @xmath101 implementations , as first introduced in de rham and juan , and published in , respectively , 1980 and 1982 .",
    "cluster criteria such as ward s method must respect bruynooghe s reducibility property if they are to be inversion - free ( or with monotonic variation in cluster criterion value through the sequence of agglomerations ) . apart from computational reasons ,",
    "the other major advantage of such algorithms ( nearest neighbor chain , reciprocal nearest neighbor ) is use in distributed computing ( including virtual memory ) environments .",
    "having different very close implementations that differ by just a few lines of code ( in any high level language ) , yet claiming to implement a given method , is confusing for the learner , for the practitioner and even for the specialist . in this work",
    ", we have first of all reviewed all relevant background .",
    "then we have laid out in very clear terms the two , differing implementations . additionally , with differing inputs , and with somewhat different processing driven by options set by the user , in fact our two different implementations had the appearance of being quite different methods .",
    "two algorithms , ward1 and ward2 , are found in the literature and software , both announcing that they implement the ward ( 1963 ) clustering method . when applied to the same distance matrix d",
    ", they produce different results .",
    "this article has shown that when they are applied to the same dissimilarity matrix d , only ward2 minimizes the ward clustering criterion and produces the ward method .",
    "the ward1 and ward2 algorithms can be made to optimize the same criterion and produce the same clustering topology by using ward1 with d - squared and ward2 with d. furthermore , taking the square root of the clustering levels produced by ward1 used with d - squared produces the same clustering levels as ward2 used with d. the constrained clustering package of legendre ( 2011 ) , ` const.clust ` , derived from ` hclust ` in r , offers both the ward1 and ward2 options .",
    "we have shown in this article how close these two implementations are , in fact .",
    "furthermore we discussed in detail what the implications are for the few , differing lines of code .",
    "software developers who only offer the ward1 algorithm are encouraged to explain clearly how the ward2 output is to be obtained , as described in the previous paragraph .",
    "1 .   anderberg , m.r . ,",
    "1973 . cluster analysis for applications , academic .",
    "2 .   batagelj , v. , 1988 .",
    "generalized ward and related clustering problems , in h.h .",
    "bock , ed . , classification and related methods of data analysis , north - holland , pp .",
    "3 .   benzcri , j.p . , 1976 .",
    "lanalyse des donnes , tome 1 , la taxinomie , dunod , ( 2nd edn . ; 1973 , 1st edn . ) .",
    "cailliez , f. and pags , j .- p . , 1976 .",
    "introduction  lanalyse des donnes , smash ( socit de mathmatiques appliques et sciences humaines ) .",
    "fisher , r.a . , 1936 .",
    "the use of multiple measurements in taxonomic problems .",
    "annals of eugenics , 7 , 179188 . 6 .",
    "jain , a.k . and dubes , r.c . , 1988 .",
    "algorithms for clustering data , prentice - hall",
    "jambu , m. , 1978 .",
    "classification automatique pour lanalyse des donnes .",
    "i. mthodes et algorithmes , dunod .",
    "jambu , m. , 1989 .",
    "exploration informatique et statistique des donnes , dunod .",
    "kaufman , l. and rousseeuw , p.j . , 1990 . finding groups in data : an introduction to cluster analysis , wiley .",
    "lance , g.n . and williams , w.t . , 1967 . a general theory of classificatory sorting strategies",
    "hierarchical systems , the computer journal , 9 , 4 , 373380 .",
    "legendre , p. and fortin , m .- j . ,",
    "comparison of the mantel test and alternative approaches for detecting complex relationships in the spatial analysis of genetic data .",
    "molecular ecology resources , 10 , 831844 . 12 .",
    "legendre , p. , 2011",
    "` const.clust ` , r package to compute space - constrained or time - constrained agglomerative clustering .",
    "+ http://www.bio.umontreal.ca/legendre/indexen.html 13 .",
    "legendre , p. and legendre , l. , 2012 . numerical ecology , 3rd ed . ,",
    "le roux , b. and rouanet , h. , 2004 .",
    "geometric data analysis : from correspondence analysis to structured data analysis , kluwer . 15 .",
    "murtagh , f. , 1983 .",
    "a survey of recent advances in hierarchical clustering algorithms , the computer journal , 26 , 354359 . 16 .",
    "murtagh , f. , 1985 .",
    "multidimensional clustering algorithms , physica - verlag .",
    "murtagh , f. , 2000 .",
    "multivariate data analysis software and resources , + http://www.classification-society.org/csna/mda-sw 18 .",
    "szkely , g.j . and rizzo , m.l . , 2005 .",
    "hierarchical clustering via joint between - within distances : extending ward s minimum variance method , journal of classification , 22 ( 2 ) , 151183 . 19 .",
    "ward , j.h . , 1963 .",
    "hierarchical grouping to optimize an objective function , journal of the american statistical association , 58 , 236244 .",
    "wishart , d. , 1969 .",
    "an algorithm for hierachical classifications , biometrics 25 , 165170 .",
    "xplore , 2007 ."
  ],
  "abstract_text": [
    "<S> the ward error sum of squares hierarchical clustering method has been very widely used since its first description by ward in a 1963 publication . </S>",
    "<S> it has also been generalized in various ways . </S>",
    "<S> however there are different interpretations in the literature and there are different implementations of the ward agglomerative algorithm in commonly used software systems , including differing expressions of the agglomerative criterion . </S>",
    "<S> our survey work and case studies will be useful for all those involved in developing software for data analysis using ward s hierarchical clustering method .    </S>",
    "<S> * keywords : * hierarchical clustering , ward , lance - williams , minimum variance . </S>"
  ]
}