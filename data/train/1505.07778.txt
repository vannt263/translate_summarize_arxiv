{
  "article_text": [
    "enabling indexing and browsing over large handwritten databases is an elusive goal in document analysis .",
    "state of the art ocr technologies available for printed documents are not directly applicable to these type of documents due to challenges like diversity of the handwriting style or the presence of noise and distortion in historical manuscripts .",
    "this problem becomes even more challenging in multi - writer datasets .",
    "word spotting has been proposed as an alternative to ocr , as a form of content - based retrieval procedure , which results in a ranked list of word images that are similar to the query word .",
    "the query can be either an example image ( query - by - example ( qbe ) ) or an string containing the word to be searched ( query - by - string ( qbs ) ) . methods following qbe paradigm presents a huge disadvantage in practical applications as in order to spot a word the user needs to first locate / input an instance of such word .",
    "on the other hand qbs methods allow the user to type the keyword to search in a much more natural way .",
    "initial approaches on word spotting followed a similar pipeline as ocr technologies , starting with binarization followed by structural / layout analysis and segmentation at word and/or character level .",
    "example of this type framework are the works of @xcite . the main drawbacks of these methods come from the dependence on the segmentation step , which can be very sensible to handwriting distortions .",
    "other initial attempts on qbs based methods relied on the extraction of letter or glyph templates , either manually @xcite or by means of some clustering scheme @xcite . then these character templates are put together in order to synthetically generate an example of the sought word .",
    "although such methods proved to be effective and user- friendly , their applicability is limited to scenarios where individual characters can be easily segmented .",
    "more generic solutions have been proposed in @xcite , where they learned models for individual characters and the relationship among them using either an an hmm @xcite or a nn @xcite .",
    "these models are trained on the whole word or even on complete text lines without needing an explicit character segmentation .",
    "they are used to generate a word model from the query string that has to be compared with the whole database at query time .",
    "therefore , computational time can quickly increase with the size of the dataset . in this context",
    "it can be mentioned that example based methods are in a clear advantage as they can represent handwritten word images(queries ) holistically by compact numeric feature vectors .",
    "one difficulty using a compact representation in qbs methods is that word strings and word images are not directly comparable . in a recent work @xcite almazn proposed a phoc based attribute representation which can be used to represent both word images and strings .",
    "the attribute representation encodes the spatial position of characters in the word image through a pyramidal histogram of characters ( phoc ) and is learned using the powerful fisher vector representation of the images .",
    "once word images are represented in this attribute space comparing images and strings is reduced to a nearest neighbour problem .",
    "though this framework has achieved high accuracy in case of segmented words it can not be applied directly in a segmentation - free approach using sliding window protocol over entire document as it involves computation of costly fisher vector representation , unfeasible at query time .    in this work",
    "we propose to extend this approach to a segmentation - free scenario . to overcome the computational cost we propose to index different image regions based on the presence of character bi - grams ( henceforth referred as only bi - grams in this article ) to reduce the search space before using a sliding window protocol .",
    "document images are segmented in different regions using a very basic segmentation method and then , for each bi - gram a ranked list of these regions is created .",
    "n - gram based language models have been widely used to improve ocr accuracy , but also for out - of - vocabulary word spotting in @xcite and for recognising complex degraded documents in @xcite . however , our approach to using character bi - grams is more similar to inverted index files , a common practice in most successful text retrieval systems . at query time , given a query string , the indexing structure is accessed to retrieve all image regions containing any of the bi - grams in the string .",
    "then , these regions are searched using a sliding window framework over the phoc attribute representation . to achieve more computational efficiency , we propose to pre - compute an integral image of the attribute representation .",
    "for that , some simplifications have to be done which makes the final attribute representation a bit less discriminative . to overcome this we propose an additional re - ranking step of the top candidate windows which uses the same attribute representation as of@xcite .",
    "our main contributions can be summarized as : i ) we propose an indexing scheme of document images based on character bi - grams .",
    "ii ) we propose an efficient computation of the attribute word representation over a whole document using an integral image .",
    "iii ) we combine an initial ranking based on the bi - gram indexing and integral image based word representation with a re - ranking step on the top candidate windows using a more powerful attribute representation .",
    "iv ) we show results in a full segmentation - free scenario where , up to our knowledge , no previous results have been reported .",
    "the rest of the paper is organized as follows : in section [ sec : methoddesc ] the proposed methodology is discussed in detail including index generation using bi - grams , computation of word attributes , retrieval and re - ranking . in section [ sec : results ] experimental validation of the proposed method is discussed .",
    "finally , we devote a section to conclude the paper with future directions of research .",
    "the proposed approach is illustrated in figure [ figure : schematicqbs ] .",
    "the main idea is to index the regions over the document according to the presence of particular bi - grams in that region . at query time",
    "bi - grams present in the query word are identified and then the regions corresponding to the bi - grams are searched for presence of the query word . to identify the regions to be indexed according to bi - grams , a very naive segmentation based on connected component analysis is performed . to generate the index and to match candidate word images with text strings an attribute based representations similar to that of almazn@xcite is used . in the following subsections we describe in detail each of the components of our approach : the attribute representation proposed in @xcite , efficient computation using an integral image , the indexing scheme , retrieval and re - ranking .      in almazn@xcite a common low dimensional representation",
    "is learned for word images and text strings , that permits to address retrieval as a simple nearest neighbor problem .",
    "though this representation can be utilized to accomplish both qbe and qbs , here our focus is on qbs word spotting .",
    "first , text strings are represented by a @xmath0 dimensional binary embedding .",
    "this embedding  called pyramidal histogram of characters ( phoc ) ",
    "encodes if a particular character appears in a particular spatial region of the string using a pyramidal decomposition that makes it more discriminative .",
    "the first level is just a basic histogram of characters encoding the presence or absence of a particular character in the string .",
    "then , new levels are added where at each level of the pyramid the word is further split and a new histogram of characters is added for each new division to account for characters at different parts of the word . in particullar",
    "almazn@xcite used histograms at levels 2 , 3 , 4 and 5 .",
    "in addition they also used a histogram of the 50 most popular bi - grams at level 2 thus resulting in a @xmath1 dimensional word representations .",
    "then , this embedding is used as a source for learning character attributes from word images .",
    "each word image is projected into a @xmath0dimensional space ( same dimension as the phoc representation ) where each dimension is a character attribute . in a way similar to the phoc string representation",
    "each characte attribute encodes the probability of appearance of a given character in a particular division of the image , using the same pyramidal decomposition as in the phoc representation .",
    "each attribute is independently learned using an svm classifier on a fisher vector description of the word image , enriched with the @xmath2 and @xmath3 coordinates and the scale of the sift descriptor .    more formally , given a training image @xmath4 and its associated text transcription , we can compute its fisher vector representation @xcite @xmath5 , where @xmath5 is a function of the form @xmath6 being @xmath7 the dimension of the fisher vector representation . now to project fisher vector representations into the phoc attribute space , we learn an embedding function @xmath8 of the form @xmath9 such that + @xmath10    where @xmath11 is a matrix with an svm - based classifier for each attribute , that are learned using the phoc labels obtained from the text transcription of all the training words .    at query time , text queries are encoded using the phoc representation and word images are described with this attribute representation .",
    "retrieval simply translates into finding the word candidates whose attribute representation is close to that of the query image .",
    "almazan @xcite proposed an additional step consisting of learning a common subspace between strings and images as direct comparison between phocs and attribute representations is not well defined since phocs are binary , while the attribute scores are not .",
    "thus , a final calibration step is added , using canonical correlation analysis , that aims at maximizing the correlation among both representations .",
    "this final calibration and dimensionality reduction step can be represented with an additional embedding function @xmath12 represented as : @xmath13 and can be given as : @xmath14    being @xmath15 the transformation matrix obtained with canonical correlation analysis    in this work , we have used this representation in the final retrieval step . for indexing , and relying on the same framework",
    ", we have defined an alternative representation based on bi - grams .",
    "our starting hypothesis is that bi - grams can be discriminative word features that can be used as the basis for localizing areas of the document where the word is likely to appear .",
    "thus , the goal of this new representation is to identify the presence of a particular bi - gram in a word image . to select the bi - grams that are used to generate the index over the handwritten documents , we refer to the study done by jones in @xcite where they studied a large corpus of 183 million words .",
    "we consider 150 most popular bigrams from this study which covers 99.21% of the total corpus . to include numeric fields the digits from 0 - 9",
    "are also included in this representation .",
    "then , this particular representation is obtained using 150 bi - grams at level 2 of decomposition and using 10 digits at levels 2 and 3 , thus resulting in a 350 dimensional representation .",
    "this representation is henceforth referred as phob ( pyramidal histogram of bi - grams ) in this article . using the strategy described above",
    ", a similar embedding function is learned to project the fisher vector representation of word images into the phob attribute space .      in a scenario where the retrieval procedure has to rely on deciding among many ( probably overlapping ) candidate windows , the main bottleneck of using word attributes as basic representation",
    "is that it involves the redundant and costly computation of sift descriptors and fisher vector representation at run time ",
    "it takes around @xmath16ms for a single candidate window . to alleviate these problems we propose to pre - compute off - line the attribute representation for every pixel of the image and store it in an efficient integral image @xcite that can be used to compute very fast the representation of any candidate window at query time .    to describe the computation of the integral image of the attribute representation , let us denote the document images of the dataset as @xmath17 where n is the total number of images . for a given image @xmath18 , we first compute the set of dense sift descriptors @xmath19 at every location @xmath20",
    ". then , we can define the embedding function into the attribute space @xmath8 for every pixel location as : @xmath21 where @xmath22 is the fisher vector representation for the @xmath20 pixel of image @xmath18 and @xmath11 is a matrix encoding the attribute classifiers as in previous section .",
    "finally this attribute representation for every pixel is projected to the lower dimensional subspace obtained through canonical correlation analyisis using the same transformation matrix @xmath15 introduced in previous section : @xmath23    once we have the final attribute representation for every pixel , it can be easily aggregated into an integral image @xmath24 : @xmath25    the time and memory requirements for computing the attribute representation representation can be further reduced if we arrange the image into @xmath26 dimensional blocks and instead of computing fisher vector representation for every pixel , we only compute one fisher vector for each block .",
    "although we end up obtaining an integral image encoding an attribute - based representation very similar to that of @xcite , in the process we have to apply some simplifications : i ) as we are computing fisher vector on a per pixel basis , we can not have , at the time of computing the integral image , the relative position of the key - points inside a given candidate box .",
    "therefore , sift descriptors can not be enriched using the relative positional information @xmath2 , @xmath3 coordinates , as explained in section [ sec : attributes - original ] .",
    "ii ) also , as we can not know the size of the underlying window can not apply the window size normalization performed in the original approach .",
    "these simplifications make the final representation a bit less discriminative . in section [ sec : reranking ] we will introduce a re - ranking step to compensate this loss of disciminability .      the goal of the indexing scheme is to create an ordered list of regions per bi - gram over the entire document database , so that at query time only regions relevant to bi - grams in the query word have to be searched .",
    "this index file is similar to inverted index files used frequently in text retrieval .    to generate a list of regions for each bi - gram",
    ", the document is first segmented .",
    "please note that the goal of segmentation is only to identify regions of plausible occurrence of bi - grams in documents not to find an exact and accurate word segmentation .      for segmentation , the document image is first binarized by setting the threshold as 75% of the mean intensity of the image .",
    "then , each document in the database is represented as a set of connected components .",
    "connected components which are overlapping with each other are merged into a single region .",
    "however in english handwriting some descendant of a line can sometimes overlap with ascendants of the line below . thus merging connected component",
    "sometimes can merge components from two different lines . to avoid this",
    "a minimal bounding box for each connected component is found(by greedily finding the smallest region that contains 90% of the density of the binarized image ) . to find overlapping connected components , instead of actual bounding box this minimal bounding box",
    "connected components which are very close along width of the document page are also merged together to form a single region .",
    "however this notion of closeness can vary from one document to another . to overcome this we use different thresholds thus yielding one set of regions per threshold after merging . at the end",
    "we merge all of these regions to obtain the list of regions which are used for indexing .      to generate index over the document database represented by the segmented regions ,",
    "bi - grams are considered as strings and represented by the phob representation introduced in section [ sec : attributes - original ] .",
    "each segmented region is also represented by the corresponding phob based attribute representation using the fisher vector of the region .",
    "both representations are then converted to a low dimensional common subspace using cca calibration .",
    "similarity between bi - grams and segmented regions can be computed as a dot product between their corresponding representations in this space .",
    "for each bi - gram , segmented regions are sorted in order of decreasing similarity and stored as inverted index files .      at retrieval time , given a text string the aim is to extract all the occurrences of the string in the entire dataset . for that , first all distinct bi - grams of the text string are found .",
    "then , for each bi - gram the inverted index is searched and top @xmath27 regions for each one are further considered for retrieval .",
    "thus , for a query having @xmath28 distinct bi - grams , @xmath29 regions are searched .",
    "however many regions can be indexed by more then one bi - gram thus making the number of distinct regions to search much less for most cases .",
    "once potential regions are identified , we employ a sliding window search using the integral image representation described in section [ sec : integral - image ] .",
    "we apply the sliding window over the region as returned by the index file if the region is big enough to accommodate the query word as calculated by the mean width of all the same training words .",
    "if the region is small then we merge it with the regions on the left and on the right to make a bigger region where the sliding window can be employed . using the integral image , the attribute - based representation of each candidate window explored by the sliding window can be easily obtained and compared through the cosine similarity with the phoc representation of the query string .",
    "finally , the candidate window list is ranked in order of decreasing similarity and non maximal suppression is performed to obtain the final relevance list .      as we have explained in section [ sec : integral - image ] computing offline the integral image of attributes before query time requires certain simplifications with respect to the original attribute representation .",
    "though these simplifications make the overall procedure efficient and faster to execute , they also make the representation less discriminative leading to a significant loss in accuracy . in order to alleviate this effect , and",
    "similar to other applications in image retrieval @xcite a re - ranking step is introduced .",
    "basically it consists of applying more discriminative and costly features to the best retrieved windows by the first ranking step in order to obtain the final ranking list . in word",
    "spotting , @xcite applied re - ranking by using fisher vector as a second ranking step after selecting windows using a hog based representation .    in this work we use the same strategy :",
    "the top @xmath30 candidates from the ranked list given by the initial ranking obtained with the sliding window search are re - ranked using the more discriminative original attribute representation described in section [ sec : attributes - original ] .",
    "l|cccc & segmentation&queries&gw & iam +    ' '' ''    proposed ( 1 ) & -&all queries & 64.32&39.47 + proposed ( 2 ) with rr ( 60% ) & - & all queries & 69.87&43.78 + proposed ( 3 ) with rr ( 80% ) & -&all queries & 73.7&48.57 +    ' '' ''    liang et al.@xcite & word - level & 38 queries&67% at rank 10 + fischer et al.@xcite & line - level&in - vocabulary words & 62 . 08&47.75 + frinken et al .",
    "@xcite & line - level & in - vocabulary words & 71&76 +",
    "the proposed method have been evaluated two publicly available databases widely used in state - of - the art word spotting systems .",
    "one of them is a single writer handwritten database popularly known as * the george washington ( gw ) dataset*,comprising 5000 words annotated at word level from 20 handwritten letters written by george washington to his associates in 18th century .    to evaluate our method in a multi - writer scenario * *",
    "the iam offline dataset**@xcite is used .",
    "it is a large dataset comprised of 1539 pages of modern handwritten english text written by 657 different writers .",
    "the document images are annotated at word and line level and contain the transcriptions of more than 13000 lines and 115000 words .",
    "we follow the official partition for writer independent text line recognition task .    to evaluate the proposed method every unique ground truth text in the gw dataset is considered as a query and after ranking a candidate window is considered as a true positive if it overlaps by more then 50% with any of the ground truth annotated boxes of the same word .",
    "we measure accuracy in terms of mean average precision .",
    "this protocol is in line with most of the segmentation - free word spotting works such as @xcite .    in the case of the iam dataset",
    "however , we follow a different strategy , performing line spotting instead of word spotting , the whole lines are retrieved if they contain the query word .",
    "each query word is searched inside all annotated text lines .",
    "the distance between the query and a given text line is defined as the distance between the query and the closest candidate word of that line .",
    "a similar strategy has been followed by almazn in @xcite .",
    "table [ table : resmap ] summarizes the results of our method .",
    "we provide results for three settings of our method : the first one without re - ranking , and then re - ranking with two different choices of @xmath31 .",
    "table [ table : resmap ] also reports the performance of other similar works reported in @xcite .",
    "however direct comparison with these methods is not straightforward , as they use different protocols to evaluate their methods : both @xcite compute the average precision at line level and only use in - vocabulary words in the evaluation . besides they do not perform full segmentation - free word spotting as they require a precise line segmentation . in @xcite liang et al .",
    "used a selected set of queries to obtain the map just considering the first 10 retrieved elements . from the results it can be observed that results of our baseline system without any re - ranking is slightly poorer than liang @xcite . with re - ranking",
    "we obtain the best results among all in the gw dataset . in the iam",
    "dataset our system also gives better result than that of fisher @xcite , but performs poorer than that of frinken @xcite .",
    "however , fair comparison with this method is difficult due to differences in the method and the evaluation protocol .",
    "average computational time to evaluate a query is an important criteria for any word spotting system in real life applications .",
    "unfortunately , none of the methods used for comparison reported computational time .",
    "thus comparison is not possible but it is worth mentioning that it takes about 1.45s per query o n average to evaluate a query using our method without re - ranking for the gw dataset .",
    "however this computational time increases to 8.63s per query when top 80% candidates are re - ranked .",
    "it can be observed that the proposed method without the re - ranking step is quite fast to be used in a real time environment .",
    "the re - ranking step significantly increases the computational time as it must compute sift and fisher vector for each candidate window .",
    "this paper proposes a segmentation - free approach to word spotting using qbs in document images . to reduce computational cost of comparing a large number of candidates an inverted index per character bi - grams",
    "is generated over the entire document offline . at query time",
    "a query word is searched only in these indexed regions .",
    "both query strings and candidate words are represented using phoc based compact numeric feature vector . an efficient way to compute phoc based word attributes in query time by using pre - computed integral image representation",
    "is also shown .",
    "the results of our method shows significant improvements over the current state - of - the - art .",
    "computational time could be further improved integrating our approach with a compression technique such as product quantization as done in @xcite .",
    "the proposed method is based on a simplification of the original attribute word representation .",
    "context information around a pixel can be exploited in the future in order to compensate for the poorer fisher vector representation that we use in our method .",
    "t. konidaris , b. gatos , k. ntzios , i. pratikakis , s. theodoridis , and s. perantonis , keyword - guided word spotting in historical printed doc- uments using synthetic data and user feedback , international journal on document analysis and recognition , vol . 9 , no .",
    "167177,april 2007 .",
    "v. frinken , a. fischer , r. manmatha , and h. bunke , a novel word spotting method based on recurrent neural networks , ieee transac- tions on pattern analysis and machine intelligence , vol .",
    "2 , pp . 211224 , february 2012 udit roy , naveen sankaran , k. pramod sankar and c. v. jawahar , character n - gram spotting on handwritten documents using weakly - supervised segmentation , proceedings of the 2013 12th international conference on document analysis and recognition pages 577 - 581    shrey dutta , naveen sankaran , k pramod sankar and cv jawahar , robust recognition of degraded documents using character n - grams , proccesdings of 10th iapr international workshop on document analysis systems ( das),pages 130 - 134    michael n. jones and d. j. k. mewor , case - sensitive letter and bigram frequency counts from large - scale english corpora , behavior research methods , instruments & computers august 2004 , volume 36 , issue 3 , pp 388 - 396"
  ],
  "abstract_text": [
    "<S> in this paper we propose a segmentation - free query by string word spotting method . </S>",
    "<S> both the documents and query strings are encoded using a recently proposed word representation that projects images and strings into a common atribute space based on a pyramidal histogram of characters(phoc ) . </S>",
    "<S> these attribute models are learned using linear svms over the fisher vector representation of the images along with the phoc labels of the corresponding strings . in order to search through the whole page , </S>",
    "<S> document regions are indexed per character bi - gram using a similar attribute representation . on top of that </S>",
    "<S> , we propose an integral image representation of the document using a simplified version of the attribute model for efficient computation . </S>",
    "<S> finally we introduce a re - ranking step in order to boost retrieval performance . </S>",
    "<S> we show state - of - the - art results for segmentation - free query by string word spotting in single - writer and multi - writer standard datasets . </S>"
  ]
}