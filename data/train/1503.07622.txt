{
  "article_text": [
    "it is often said that the language of science is mathematics .",
    "it could well be said that the language of experimental science is statistics . it is through statistical concepts that we quantify the correspondence between theoretical predictions and experimental observations .",
    "while the statistical analysis of the data is often treated as a final subsidiary step to an experimental physics result , a more direct approach would be quite the opposite .",
    "in fact , thinking through the requirements for a robust statistical statement is an excellent way to organize an analysis strategy . in these lecture notes",
    "i will devote significant attention to the strategies used in high - energy physics for developing a statistical model of the data .",
    "this modeling stage is where you inject your understanding of the physics .",
    "i like to think of the modeling stage in terms of a conversation .",
    "when your colleague asks you over lunch to explain your analysis , you tell a story .",
    "it is a story about the signal and the backgrounds  are they estimated using monte carlo simulations , a side - band , or some data - driven technique ? is the analysis based on counting events or do you use some discriminating variable , like an invariant mass or perhaps the output of a multivariate discriminant ?",
    "what are the dominant uncertainties in the rate of signal and background events and how do you estimate them ?",
    "what are the dominant uncertainties in the shape of the distributions and how do you estimate them ?",
    "the answer to these questions forms a _ scientific narrative _ ; the more convincing this narrative is the more convincing your analysis strategy is .",
    "the statistical model is the mathematical representation of this narrative and you should strive for it to be as faithful a representation as possible .    once you have constructed a statistical model of the data , the actual statistical procedures should be relatively straight forward .",
    "in particular , the statistical tests can be written for a generic statistical model without knowledge of the physics behind the model .",
    "the goal of the ` roostats ` project was precisely to provide statistical tools based on an arbitrary statistical model implemented with the ` roofit ` modeling language . while the formalism for the statistical procedures can be somewhat involved , the logical justification for the procedures is based on a number of abstract properties for the statistical procedures .",
    "one can follow the logical argument without worrying about the detailed mathematical proofs that the procedures have the required properties . within the last five years",
    "there has been a significant advance in the field s understanding of certain statistical procedures , which has led to to some commonalities in the statistical recommendations by the major lhc experiments .",
    "i will review some of the most common statistical procedures and their logical justification .",
    "this section specifies my notations and conventions , which i have chosen with some care .",
    "is manifestly lorentz invariant and @xmath0 is manifestly wrong . ]",
    "our statistical claims will be based on the outcome of an experiment .",
    "when discussing frequentist probabilities , one must consider ensembles of experiments , which may either be real , based on computer simulations , or mathematical abstraction .",
    "figure  [ fig : hierarchy ] establishes a hierarchy that is fairly general for the context of high - energy physics .",
    "imagine the search for the higgs boson , in which the search is composed of several `` channels '' indexed by @xmath1 . here",
    "a channel is defined by its associated event selection criteria , not an underlying physical process .",
    "in addition to the number of selected events , @xmath2 , each channel may make use of some other measured quantity , @xmath3 , such as the invariant mass of the candidate higgs boson .",
    "the quantities will be called `` observables '' and will be written in roman letters e.g. @xmath3 .",
    "the notation is chosen to make manifest that the observable @xmath4 is frequentist in nature .",
    "replication of the experiment many times will result in different values of @xmath4 and this ensemble gives rise to a _ probability density function _ ( pdf ) of @xmath4 , written @xmath5 , which has the important property that it is normalized to unity @xmath6 in the case of discrete quantities , such as the number of events satisfying some event selection , the integral is replaced by a sum .",
    "often one considers a parametric family of pdfs @xmath7 read `` @xmath8 of @xmath4 given @xmath9 '' and , henceforth , referred to as a _ probability model _ or just _",
    "model_. the parameters of the model typically represent parameters of a physical theory or an unknown property of the detector s response .",
    "the parameters are not frequentist in nature , thus any probability statement associated with @xmath9 is bayesian . when the joint distribution @xmath10 is defined in a frequentist sense . ] in order to make their lack of frequentist interpretation manifest , model parameters will be written in greek letters , e.g. : @xmath11 . and",
    "@xmath12 for the number of expected signal and background , these are parameters _ not _ observables , so i will write @xmath13 and @xmath14 .",
    "this is one of few notational differences to ref .",
    "@xcite . ] from the full set of parameters , one is typically only interested in a few : the _ parameters of interest_. the remaining parameters are referred to as _ nuisance parameters _ , as we must account for them even though we are not interested in them directly .",
    "while @xmath5 describes the probability density for the observable @xmath4 for a single event , we also need to describe the probability density for a dataset with many events , @xmath15 .",
    "if we consider the events as independently drawn from the same underlying distribution , then clearly the probability density is just a product of densities for each event .",
    "however , if we have a prediction that the total number of events expected , call it @xmath16 , then we should also include the overall poisson probability for observing @xmath17 events given @xmath16 expected .",
    "thus , we arrive at what statisticians call a marked poisson model , @xmath18 where i use a bold @xmath19 to distinguish it from the individual event probability density @xmath5 . in practice",
    ", the expectation is often parametrized as well and some parameters simultaneously modify the expected rate and shape , thus we can write @xmath20 . in ` roofit ` both @xmath8 and @xmath19 are implemented with a ` rooabspdf ` ; where ` rooabspdf::getval(x ) ` always provides the value of @xmath5 and depending on ` rooabspdf::extendmode ( ) ` the value of @xmath16 is accessed via ` rooabspdf::expectedevents ( ) ` .    the _ likelihood function _",
    "@xmath21 is numerically equivalent to @xmath22 with @xmath4 fixed  or @xmath23 with @xmath24  fixed .",
    "the likelihood function should not be interpreted as a probability density for @xmath9 . in particular",
    ", the likelihood function does not have the property that it normalizes to unity @xmath25 it is common to work with the log - likelihood ( or negative log - likelihood ) function . in the case of a marked poisson , we have what is commonly referred to as an extended likelihood  @xcite @xmath26 to reiterate the terminology , _ probability density function",
    "_ refers to the value of @xmath8 as a function of @xmath4 given a fixed value of @xmath9 ; _ likelihood function",
    "_ refers to the value of @xmath8 as a function of @xmath9 given a fixed value of @xmath4 ; and _ model _ refers to the full structure of @xmath22 .",
    "probability models can be constructed to simultaneously describe several channels , that is several disjoint regions of the data defined by the associated selection criteria .",
    "i will use @xmath27 as the index over events and @xmath1 as the index over channels .",
    "thus , the number of events in the @xmath28 channel is @xmath2 and the value of the @xmath29 event in the @xmath28 channel is @xmath30 . in this context",
    ", the data is a collection of smaller datasets : . in ` roofit ` the index @xmath1 is referred to as a ` roocategory ` and it is used to inside the dataset to differentiate events associated to different channels or categories .",
    "the class ` roosimultaneous ` associates the dataset @xmath31 with the corresponding marked poisson model .",
    "the key point here is that there are now multiple poisson terms .",
    "thus we can write the combined ( or simultaneous ) model @xmath32 \\ ; , \\ ] ] remembering that the symbol product over channels has implications for the structure of the dataset .",
    "auxiliary measurements or control regions can be used to estimate or reduce the effect of systematic uncertainties .",
    "the signal region and control region are not fundamentally different . in the language that we are using here",
    ", they are just two different channels .",
    "a common example is a simple counting experiment with an uncertain background . in the frequentist way of thinking , the true , unknown background in the signal region",
    "is a nuisance parameter , which i will denote @xmath14 . with @xmath33 ,",
    "thus it reduces to just the poisson term . ]",
    "if we call the true , unknown signal rate @xmath13 and the number of events in the signal region @xmath34 then we can write the model @xmath35 . as long as @xmath14 is a free parameter",
    ", there is no ability to make any useful inference about @xmath13 .",
    "often we have some estimate for the background , which may have come from some control sample with @xmath36 events .",
    "if the control sample has no signal contamination and is populated by the same background processes as the signal region , then we can write @xmath37 , where @xmath36 is the number of events in the control region and @xmath38 is a factor used to extrapolate the background from the signal region to the control region .",
    "thus the total probability model can be written @xmath39 .",
    "this is a special case of eq .",
    "[ eq : simultaneous ] and is often referred to as the  on / off problem  @xcite .    based on the control region alone , one would estimate ( or ` measure ' ) @xmath40 .",
    "intuitively the estimate comes with an ` uncertainty ' of @xmath41 .",
    "we will make these points more precise in sec .",
    "[ s : estimation ] , but the important lesson here is that we can use auxiliary measurements ( ie .",
    "@xmath36 ) to describe our uncertainty on the nuisance parameter @xmath14 statistically .",
    "furthermore , we have formed a statistical model that can be treated in a frequentist formalism  meaning that if we repeat the experiment many times @xmath36 will vary and so will the estimate of @xmath14 .",
    "it is common to say that auxiliary measurements ` constrain ' the nuisance parameters . in principle",
    "the auxiliary measurements can be every bit as complex as the main signal region , and there is no formal distinction between the various channels .",
    "the use of auxiliary measurements is not restricted to estimating rates as in the case of the on / off problem above .",
    "one can also use auxiliary measurements to constrain other parameters of the model . to do so",
    ", one must relate the effect of some common parameter @xmath42 in multiple channels ( ie .",
    "the signal region and a control regions ) .",
    "this is implicit in eq .",
    "[ eq : simultaneous ] .",
    "the intuitive interpretation of measurement of @xmath14 to be @xmath43 is that the parameter @xmath14 has a distribution centered around @xmath44 with a width of @xmath41 . with some practice",
    "you will be able to immediately identify this type of reasoning as bayesian .",
    "it is manifestly bayesian because we are referring to the probability distribution of a parameter .",
    "the frequentist notion of probability of an event is defined as the limit of its relative frequency in a large number of trials .",
    "the large number of trials is referred to as an ensemble . in particle physics the ensemble is formed conceptually by repeating the experiment many times .",
    "the true values of the parameters , on the other hand , are states of nature , not the outcome of an experiment .",
    "the true mass of the @xmath45 boson has no frequentist probability distribution .",
    "the existence or non - existence of the higgs boson has no frequentist probability associated with it .",
    "there is a sense in which one can talk about the probability of parameters , which follows from bayes s theorem : @xmath46 bayes s theorem is a theorem , so there s no debating it .",
    "it is not the case that frequentists dispute whether bayes s theorem is true .",
    "the debate is whether the necessary probabilities exist in the first place .",
    "if one can define the joint probability @xmath47 in a frequentist way , then a frequentist is perfectly happy using bayes theorem .",
    "thus , the debate starts at the very definition of probability .",
    "the bayesian definition of probability clearly ca nt be based on relative frequency .",
    "instead , it is based on a degree of belief .",
    "formally , the probability needs to satisfy kolmogorov s axioms for probability , which both the frequentist and bayesian definitions of probability do .",
    "one can quantify degree of belief through betting odds , thus bayesian probabilities can be assigned to hypotheses on states of nature . in practice",
    "human s bets are not generally not ` coherent ' ( see ` dutch book ' ) , thus this way of quantifying probabilities may not satisfy the kolmogorov axioms .    moving past the philosophy and accepting the bayesian procedure at face value ,",
    "the practical consequence is that one must supply prior probabilities for various parameter values and/or hypotheses . in particular , to interpret our example measurement of @xmath36 as implying a probability distribution for @xmath14 we would write @xmath48 where @xmath49 is called the _",
    "posterior _ probability density , @xmath50 is the likelihood function , and @xmath51 is the _ prior _ probability . here",
    "i have suppressed the somewhat curious term @xmath52 , which can be thought of as a normalization constant and is also referred to as the _",
    "evidence_. the main point here is that one can only invert ` the probability of @xmath36 given @xmath14 ' to be ` the probability of @xmath14 given @xmath36 ' if one supplies a prior .",
    "humans are very susceptible to performing this logical inversion accidentally , typically with a uniform prior on @xmath14 .",
    "furthermore , the prior degree of belief can not be derived in an objective way .",
    "there are several formal rules for providing a prior based on formal rules ( see jefferey s prior and reference priors ) , though these are not accurately described as representing a degree of belief .",
    "thus , that style of bayesian analysis is often referred to as objective bayesian analysis .",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ `` using bayes s theorem does nt make you a bayesian , * always * using bayes s theorem makes you a bayesian . '' _",
    "unknown _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ `` bayesians address the questions everyone is interested in by using assumptions that no one believes .",
    "frequentist use impeccable logic to deal with an issue that is of no interest to anyone.''_- louis lyons _",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _      often a detailed probability model for an auxiliary measurement are not included directly into the model . if the model for the auxiliary measurement were available , it could and should be included as an additional channel as described in sec .",
    "[ s : auxmeas ] . the more common situation for background and systematic uncertainties",
    "only has an estimate , `` central value '' , or best guess for a parameter @xmath42 and some notion of uncertainty on this estimate . in this case one",
    "typically resorts to including idealized terms into the likelihood function , here referred to as `` constraint terms '' , as surrogates for a more detailed model of the auxiliary measurement .",
    "i will denote this estimate for the parameters as @xmath53 , to make it manifestly frequentist in nature . in this case",
    "there is a single measurement of @xmath53 per experiment , thus it is referred to as a `` global observable '' in .",
    "the treatment of constraint terms is somewhat _ ad hoc _ and discussed in more detail in section  [ s : constraintexamples ] .",
    "i make it a point to write constraint terms in a manifestly frequentist form @xmath54",
    ".    probabilities on parameters are legitimate constructs in a bayesian setting , though they will always rely on a prior . in order to distinguish bayesian pdfs from frequentist ones , greek letters will be used for their distributions .",
    "for instance , a generic bayesian pdf might be written @xmath55 . in the context of a main measurement , one might have a prior for @xmath42 based on some estimate @xmath53 . in this case , the prior @xmath56 is really a posterior from some previous measurement .",
    "it is desirable to write with the help of bayes theorem @xmath57 where @xmath58 is some more fundamental prior . by taking the time to undo the bayesian reasoning into an objective pdf or likelihood and a prior we are able to write a model that can be used in a frequentist context . within",
    ", the care is taken to separately track the frequentist component and the prior ; this is achieved with the ` modelconfig ` class .",
    "if one can identify what auxiliary measurements were performed to provide the estimate of @xmath42 and its uncertainty , then it is not a logical fallacy to approximate it with a constraint term , it is simply a convenience .",
    "however , not all uncertainties that we deal result from auxiliary measurements . in particular , some theoretical uncertainties are not statistical in nature . for example , uncertainty associated with the choice of renormalization and factorization scales and missing higher - order corrections in a theoretical calculation are not statistical .",
    "uncertainties from parton density functions are a bit of a hybrid as they are derived from data but require theoretical inputs and make various modeling assumptions . in a bayesian setting",
    "there is no problem with including a prior on the parameters associated to theoretical uncertainties .",
    "in contrast , in a formal frequentist setting , one should not include constraint terms on theoretical uncertainties that lack a frequentist interpretation .",
    "that leads to a very cumbersome presentation of results , since formally the results should be shown as a function of the uncertain parameter . in practice , the groups often read eq .",
    "[ eq : urprior ] to arrive at an effective frequentist constraint term .",
    "i will denote the set of parameters with constraint terms as @xmath59 and the global observables @xmath60 with @xmath61 . by including the constraint terms explicitly ( instead of implicitly as an additional channel )",
    "we arrive at the total probability model , which we will not need to generalize any further : @xmath62 \\cdot \\prod_{p \\in \\mathbb{s } } f_p(a_p | \\alpha_p)\\ ; .\\ ] ]",
    "one of the most common tasks of the working physicist is to estimate some model parameter .",
    "we do it so often , that we often do nt realize it . for instance , the sample mean @xmath63 is an estimate for the mean , @xmath64 , of a gaussian probability density @xmath65 .",
    "more generally , an _ estimator _ @xmath66 is some function of the data and its value is used to estimate the true value of some parameter @xmath9 .",
    "there are various abstract properties such as variance , bias , consistency , efficiency , robustness , etc  @xcite .",
    "the bias of an estimator is defined as @xmath67-\\alpha$ ] , where @xmath68 means the expectation value of or the probability - weighted average .",
    "clearly one would like an unbiased estimator .",
    "the variance of an estimator is defined as @xmath69 = e [ ( \\alpha - e[\\hat{\\alpha } ] ) ^2 ] $ ] ; and clearly one would like an estimator with the minimum variance .",
    "unfortunately , there is a tradeoff between bias and variance .",
    "physicists tend to be allergic to biased estimators , and within the class of unbiased estimators , there is a well defined minimum variance bound referred to as the cramr - rao bound ( that is the inverse of the fisher information , which we will refer to again later ) .",
    "the most widely used estimator in physics is the maximum likelihood estimator ( mle ) .",
    "it is defined as the value of @xmath9 which maximizes the likelihood function @xmath21 .",
    "equivalently this value , @xmath70 , maximizes @xmath71 and minimizes @xmath72 . the most common tool for finding the maximum likelihood estimator is ` minuit ` , which conventionally minimizes @xmath72 ( or any other function )  @xcite .",
    "the jargon is that one ` fits ' the function and the maximum likelihood estimate is the ` best fit value ' .    when one has a multi - parameter likelihood function @xmath73 , then the situation is slightly more complicated .",
    "the maximum likelihood estimate for the full parameter list , @xmath74 , is clearly defined .",
    "the various components @xmath75 are referred to as the _ unconditional maximum likelihood estimates_. in the physics jargon , one says all the parameters are ` floating ' .",
    "one can also ask about maximum likelihood estimate of @xmath42 is with some other parameters @xmath76 fixed ; this is called the _",
    "conditional maximum likelihood estimate _ and is denoted @xmath77 .",
    "these are important quantities for defining the profile likelihood ratio , which we will discuss in more detail later .",
    "the concept of variance of the estimates is also generalized to the covariance matrix @xmath78 = e[(\\hat\\alpha_p - \\alpha_p)(\\hat\\alpha_{p'}- \\alpha_{p'})]$ ] and is often denoted @xmath79 .",
    "note , the diagonal elements of the covariance matrix are the same as the variance for the individual parameters , ie .",
    "@xmath80 = var[\\alpha_p]$ ] .    in the case of a poisson model",
    "@xmath81 the maximum likelihood estimate of @xmath16 is simply .",
    "thus , it follows that the variance of the estimator is @xmath82=var[n]=\\nu$ ] .",
    "thus if the true rate is @xmath16 one expects to find estimates @xmath83 with a characteristic spread around @xmath16 ; it is in this sense that the measurement has a estimate has some uncertainty or ` error ' of @xmath84 .",
    "we will make this statement of uncertainty more precise when we discuss frequentist confidence intervals .",
    "when the number of events is large , the distribution of maximum likelihood estimates approaches a gaussian or normal distribution .",
    "this does not depend on the pdf @xmath5 having a gaussian form . for small samples",
    "this is nt the case , but this limiting distribution is often referred to as an _",
    "asymptotic distribution_. furthermore , under most circumstances in particle physics , the maximum likelihood estimate approaches the minimum variance or cramr - rao bound .",
    "in particular , the inverse of the covariance matrix for the estimates is asymptotically given by @xmath85   \\;,\\ ] ] where i have written explicitly that the expectation , and thus the covariance matrix itself , depend on the true value @xmath86 .",
    "the right side of eq .",
    "[ eq : expfisher ] is called the ( expected ) fisher information matrix .",
    "remember that the expectation involves an integral over the observables .",
    "since that integral is difficult to perform in general , one often uses the observed fisher information matrix to approximate the variance of the estimator by simply taking the matrix of second derivatives based on the observed data @xmath87 this is what ` minuit ` s ` hesse ` algorithm calculates to estimate the covariance matrix of the parameters .",
    "let us examine the statistical statement associated to the claim of discovery for new physics .",
    "typically , new physics searches are looking for a signal that is additive on top of the background , though in some cases there are interference effects that need to be taken into account and one can not really talk about signal and background in any meaningful way .",
    "discovery is formulated in terms of a hypothesis test where the background - only hypothesis plays the role of the null hypothesis and the signal - plus - background hypothesis plays the roll of the alternative .",
    "roughly speaking , the claim of discovery is a statement that the data are incompatible with the background - only hypothesis .",
    "consider the simplest scenario where one is counting events in the signal region , @xmath34 and expects @xmath14 events from background and @xmath13 events from the putative signal .",
    "then we have the following hypotheses :    [ cols=\"<,<,<,<\",options=\"header \" , ]     finally , it is worth mentioning that the uncertainty on some parameters is not the result of an auxiliary measurement  so the constraint term idealization , it is not just a convenience , but a real conceptual leap .",
    "this is particularly true for theoretical uncertainties from higher - order corrections or renormalizaiton and factorization scale dependence . in these cases",
    "a formal frequentist analysis would not include a constraint term for these parameters , and the result would simply depend on their assumed values . as this is not the norm , we can think of reading table  [ tab : constraints ] from right - to - left with a subjective bayesian prior @xmath55 being interpreted as coming from a fictional auxiliary measurement .",
    "the gaussian constraint for @xmath42 corresponds to the familiar situation .",
    "it is a good approximation of the auxiliary measurement when the likelihood function for @xmath42 from that auxiliary measurement has a gaussian shape .",
    "more formally , it is valid when the maximum likelihood estimate of @xmath42 ( eg . the best fit value of @xmath42 ) has a gaussian distribution .",
    "here we can identify the maximum likelihood estimate of @xmath42 with the global observable @xmath53 , remembering that it is a number that is extracted from the data and thus its distribution has a frequentist interpretation .",
    "@xmath88\\ ] ] with @xmath89 by default .",
    "note that the pdf of @xmath53 and the likelihood for @xmath42 are positive for all values .    when the auxiliary measurement is actually based on counting events in a control region ( eg .",
    "a poisson process ) , a more accurate to describe the auxiliary measurement with a poisson distribution .",
    "it has been shown that the truncated gaussian constraint can lead to undercoverage ( overly optimistic ) results , which makes this issue practically relevant  @xcite .",
    "table  [ tab : constraints ] shows that a poisson pdf together with a uniform prior leads to a gamma posterior , thus this type of constraint is often called a `` gamma '' constraint .",
    "this is a bit unfortunate since the gamma distribution is manifestly bayesian and with a different choice of prior , one might not arrive at a gamma posterior .",
    "when dealing with the poisson constraint , it is no longer convenient to work with our conventional scaling for @xmath42 which can be negative .",
    "instead , it is more natural to think of the number of events measured in the auxiliary measurement @xmath90 and the mean of the poisson parameter .",
    "this information is not usually available , instead one usually has some notion of the relative uncertainty in the parameter @xmath91 ( eg . a the jet energy scale is known to 10% ) . in order to give some uniformity to the different uncertainties of this type and",
    "think of relative uncertainty , the nominal rate is factored out into a constant @xmath92 and the mean of the poisson is given by @xmath93 .",
    "@xmath94 here we can use the fact that var@xmath95=\\sqrt{\\tau_p\\alpha_p}$ ] and reverse engineer the nominal auxiliary measurement @xmath96 where the superscript @xmath97 is to remind us that @xmath90 will fluctuate in repeated experiments but @xmath98 is the value of our measured estimate of the parameter .",
    "one important thing to keep in mind is that there is only one constraint term per nuisance parameter , so there must be only one @xmath99 per nuisance parameter .",
    "this @xmath99 is related to the fundamental uncertainty in the source and we can not infer this from the various response terms @xmath100 or @xmath101 .",
    "another technical difficulty is that the poisson distribution is discrete .",
    "so if one were to say the relative uncertainty was 30% , then we would find @xmath102 , which is not an integer .",
    "rounding @xmath90 to the nearest integer while maintaining @xmath103 will bias the maximum likelihood estimate of @xmath42 away from 1 . to avoid this , one can use the gamma distribution , which generalizes more continuously with @xmath104 this approach works fine for likelihood fits , bayesian calculations , and frequentist techniques based on asymptotic approximations , but it does not offer a consistent treatment of the pdf for the global observable @xmath90 that is needed for techniques based on monte carlo sampling .    from eadie et al . , `` the log - normal distribution represents a random variable whose logarithm follows a normal distribution .",
    "it provides a model for the error of a process involving many small multiplicative errors ( from the central limit theorem ) .",
    "it is also appropriate when the value of an observed variable is a random proportion of the previous observation . ''",
    "this logic of multiplicative errors applies to the the measured value , not the parameter .",
    "thus , it is natural to say that there is some auxiliary measurement ( global observable ) with a log - normal distribution . as in the gamma / poisson case",
    "above , let us again say that the global observable is @xmath90 with a nominal value @xmath96 then the conventional choice for the corresponding log - normal distribution is @xmath105\\ ] ] while the likelihood function is ( blue curve in fig .  [",
    "fig : lognormal](a ) ) .",
    "@xmath106;.\\ ] ] to get to the posterior for @xmath42 given @xmath90 we need an ur - prior @xmath107 ) @xmath108\\ ] ] if @xmath58 is uniform , then the posterior looks like the red curve in fig .",
    "[ fig : lognormal](b ) .",
    "however , when paired with an `` ur - prior '' @xmath109 ( green curve in fig .  [ fig : lognormal](b ) ) , this results in a posterior distribution that is also of a log - normal form for @xmath42 ( blue curve in fig .  [ fig : lognormal](b ) ) .     and ( right ) the likelihood function , the posterior based on a flat prior on @xmath42 , and the posterior based on a @xmath110 prior . ]",
    "the histogram based approach described above are based monte carlo simulations of full detector simulation .",
    "these simulations are very computationally intensive and often the histograms are sparsely populated . in this case",
    "the histograms are not good descriptions of the underlying distribution , but are estimates of that distribution with some statistical uncertainty .",
    "barlow and beeston outlined a treatment of this situation in which each bin of each sample is given a nuisance parameter for the true rate , which is then fit using both the data measurement and the monte carlo estimate  @xcite .",
    "this approach would lead to several hundred nuisance parameters in the current analysis .",
    "instead , the ` histfactory ` employs a lighter weight version in which there is only one nuisance parameter per bin associated with the total monte carlo estimate and the total statistical uncertainty in that bin . if we focus on an individual bin with index @xmath12 the contribution to the full statistical model is the factor @xmath111 where @xmath112 is the number of events observed in the bin , @xmath113 is the number of events expected in the bin where monte carlo statistical uncertainties need not be included ( either because the estimate is data driven or because the monte carlo sample is sufficiently large ) , @xmath114 is the number of events estimated using monte carlo techniques where the statistical uncertainty needs to be taken into account .",
    "both expectations include the dependence on the parameters @xmath86 .",
    "the factor @xmath115 is the nuisance parameter reflecting that the true rate may differ from the monte carlo estimate @xmath116 by some amount .",
    "if the total statistical uncertainty is @xmath117 , then the relative statistical uncertainty is given by @xmath118 .",
    "this corresponds to a total monte carlo sample in that bin of size @xmath119 . treating the monte carlo estimate as an auxiliary measurement , we arrive at a poisson constraint term @xmath120 , where @xmath121 would fluctuate about @xmath122 if we generated a new monte carlo sample .",
    "since we have scaled @xmath123 to be a factor about 1 , then we also have @xmath124 ; however , @xmath125 is treated as a fixed constant and does not fluctuate when generating ensembles of pseudo - experiments .",
    "it is worth noting that the conditional maximum likelihood estimate @xmath126 can be solved analytically with a simple quadratic expression .",
    "@xmath127 with @xmath128 @xmath129 @xmath130    in a bayesian technique with a flat prior on @xmath115 , the posterior distribution is a gamma distribution . similarly , the distribution of @xmath131 will take on a skew distribution with an envelope similar to the gamma distribution , but with features reflecting the discrete values of @xmath121 . because the maximum likelihood estimate of @xmath115 will also depend on @xmath112 and @xmath132 , the features from the discrete values of @xmath121 will be smeared",
    ". this effect will be more noticeable for large statistical uncertainties where @xmath125 is small and the distribution of @xmath131 will have several small peaks . for smaller statistical uncertainties where @xmath125 is large",
    "the distribution of @xmath131 will be approximately gaussian .",
    "the strength of the simulation narrative lies in its direct logical link from the underlying theory to the modeling of the experimental observations .",
    "the weakness of the simulation narrative derives from the weaknesses in the simulation itself .",
    "data - driven approaches are more motivated when they address specific deficiencies in the simulation . before moving to a more abstract or general discussion of the data - driven narrative ,",
    "let us first consider a few examples .",
    "the first example we have already considered in sec .  [ s : auxmeas ] in the context of the `` on / off '' problem .",
    "there we introduced an auxiliary measurement that counted @xmath133 events in a control region to estimate the background @xmath14 in the signal region . in order to do this we needed to understand the ratio of the number of events from the background process in the control and signal regions , @xmath38 .",
    "this ratio @xmath38 either comes from some reasonable assumption or simulation . for example , if one wanted to estimate the background due to jets faking muons @xmath134 for a search selecting @xmath135 , then one might use a sample of @xmath136 events as a control region . here",
    "the motivation for using a data - driven approach is that modeling the processes that lead to @xmath134 rely heavily on the tails of fragmentation functions and detector response , which one might reasonably have some skepticism . if one assumes that control region is expected to have negligible signal in it , that backgrounds that produce @xmath135 other than the jets faking muons , and that the rate for @xmath137 is the same and not @xmath138 , there is clearly a reason to worry if this assumption is valid . ] as the rate for @xmath139 , then one can assume @xmath140 .",
    "thus , this background estimate is as trustworthy as the assumptions that went into it . in practice ,",
    "several of these assumptions may be violated .",
    "another approach is to use simulation of these background processes to estimate the ratio @xmath38 ; a hybrid of the data - driven and simulation narratives .",
    "let us now consider the search for @xmath141 shown in fig  [ fig : h2photons ]  @xcite .",
    "the right plot of fig  [ fig : h2photons ] shows the composition of the backgrounds in this search , including the continuum production of @xmath142 , the @xmath123+jets process with a jet faking a photon @xmath143 , and the multi jet process with two jets faking photons .",
    "the continuum production of @xmath144 has a theoretical uncertainty that is much larger than the statistical fluctuations one would expect in the data .",
    "similarly , the rate of jets faking photons is sensitive to fragmentation and the detector simulation .",
    "these uncertainties are large compared to the statistical fluctuations in the data itself .",
    "thus we can use the distribution in fig  [ fig : h2photons ] to measure the total background rate .",
    "of course , the signal would also be in this distribution , so one either needs to apply a mass window around the signal and consider the region outside of the window as a sideband control sample or model the signal and background contributions to the distribution . in the case of the @xmath141 shown in fig  [ fig : h2photons ]  @xcite the modeling of the distribution signal and background distributions is not based on histograms from simulation , but instead a continuous function is used as an effective model .",
    "i will discuss this effective modeling narrative below , but point out that here this is another example of a hybrid narrative .",
    "the left plot shows a fit of a an effective model to the data and the right plot shows an estimate of the @xmath144 , @xmath123+jet , and diet contributions.,title=\"fig : \" ]    search .",
    "the left plot shows a fit of a an effective model to the data and the right plot shows an estimate of the @xmath144 , @xmath123+jet , and diet contributions.,title=\"fig : \" ]    the final example to consider is an extension of the ` on / off ' model , often referred to as the ` abcd ' method .",
    "let us start with the ` on / off ' model : . as mentioned above , this requires that one estimate @xmath38 either from simulation or through some assumptions .",
    "the abcd method aims to estimate introduce two new control regions that can be used to measure @xmath38 . to see this ,",
    "let us imagine that the signal and control regions correspond to requiring some continuous variable @xmath4 being greater than or less than some threshold value @xmath3 .",
    "if we could introduce a second discriminating variable @xmath145 such that the distribution for background factorizes @xmath146 , then we have a handle to measure the factor @xmath38 .",
    "typically , one introduces a threshold @xmath147 so that the signal contribution is small below this threshold . ] .",
    "figure  [ fig : abcd ] shows an example where @xmath148 . with this",
    "we these two thresholds we have four regions that we can schematically refer to as a , b , c , and d. in the case of simply counting events in these regions we can write the total expectation as @xmath149 where @xmath64 is the signal rate in region a , @xmath150 is the ratio of the signal in the regions b , c , d with respect to the signal in region a , @xmath151 is the rate of background in each of the regions being estimated from simulation , @xmath152 is the rate of the background being estimated with the data driven technique in the signal region , and @xmath153 are the ratios of the background rates in the regions b , c , and d with respect to the background in region a. the key is that we have used the factorization @xmath146 to write @xmath154 . the right panel of fig .",
    "[ fig : abcd ] shows a more complicated extension of the abcd method from a recent atlas susy analysis  @xcite .",
    "d    an alternative parametrization , which can be more numerically stable is + @xmath155     plane of two observables @xmath4 and @xmath145 ( left ) . a more complex example with several regions in the @xmath156 plane  @xcite.,title=\"fig : \" ] plane of two observables @xmath4 and @xmath145 ( left ) .",
    "a more complex example with several regions in the @xmath156 plane  @xcite.,title=\"fig : \" ]      in the simulation narrative the model of discriminating variable distributions @xmath157 is derived from discrete samples of simulated events @xmath158 .",
    "we discussed above how one can use histograms or kernel estimation to approximate the underlying distribution and interpolation strategies to incorporate systematic effects .",
    "another approach is to assume some parametric form for the distribution to serve as an effective model .",
    "for example , in the @xmath141 analysis shown in fig .",
    "[ fig : h2photons ] a simple exponential distribution was used to model the background .",
    "the state - of - the - art theoretical predictions for the continuum @xmath144 background process do not predict exactly an exponentially falling distribution , and the analysis must ( and does ) incorporate the systematic associated to the effective model . similarly , it is common to use a polynomial in some limited sideband region to estimate backgrounds under a peak .",
    "these effective models can range from very ad hoc   described in @xcite ( see eq . 2 of the corresponding section ) ] to more motivated .",
    "for instance , one might use knowledge of kinematics and phase space and/or detector resolution to construct an effective model that captures the relevant physics .",
    "the advantage of a well motivated effective model is that few nuisance parameters may describe well the relevant family of probability densities , which is the challenge for generic ( and relatively unsophisticated ) interpolation strategies usually employed in the simulation narrative .",
    "ideally , one would not use a single discriminating variable to distinguish the process of interest from the other background processes , but instead would use as much discriminating power as possible .",
    "this implies forming a probability model over a multi - dimensional discriminating variable ( ie . a multivariate analysis technique ) .",
    "in principle , both the histogram - based and kernel - based approach generalize to distributions of multi - dimensional discriminating variables ; however , in practice , they are limited to only a few dimensions . in the case of histograms",
    "this is particularly severe unless one employs clever binning choices , while in the kernel - based approach one can model up to about 5-dimensional distributions with reasonable monte carlo sample sizes . in practice",
    ", one often uses multivariate algorithms like neural networks or boosted decision trees to map the multiple variables into a single discriminating variable .",
    "often these multivariate techniques are seen as somewhat of a black - box . if we restrict ourselves to discriminating variables associated with the kinematics of final state particles ( as opposed to the more detailed signature of particles in the detector ) , then we can often approximate he detailed simulation of the detector with a parametrized detector response .",
    "if we denote the kinematic configuration of all the final state particles in the lorentz invariant phase space as @xmath159 , the initial state as @xmath160 , the matrix element ( potentially averaged over unmeasured spin configurations ) as @xmath161 , and the probability due to parton density functions for the initial state @xmath160 going into the hard scattering as @xmath162 , then we can write that the distribution of the , possibly multi - dimensional , discriminating variable @xmath4 as @xmath163 where @xmath164 is referred to as the transfer function of @xmath4 given the final state configuration @xmath159 .",
    "it is natural to think of @xmath164 as a conditional distribution , but here i let @xmath165 encode the efficiency and acceptance so that we have @xmath166 otherwise , the equation above looks like another application one bayes s theorem where @xmath164 plays the role of the pdf / likelihood function and @xmath161 plays the role of the prior over the @xmath159 .",
    "it is worth pointing out that this is a frequentist use of bayes s theorem since @xmath167 is the lorentz invariant phase space which explicitly has a measure associated with it .      in some cases one would like to provide a distribution for the discriminating variable @xmath4 based conditional on some other observable in the event @xmath145 : @xmath168 .",
    "for instance , one might want to say that the energy resolution for electrons depends on the energy itself through a well - known calorimeter resolution parametrization like @xmath169 .",
    "these types of conditional distributions can be built in .",
    "a subtle point studied by punzi is that if @xmath170 depends on @xmath86 the inference on @xmath86 can be biased  @xcite . in particular ,",
    "if one is trying to estimate the amount of signal in a sample and the distribution of @xmath145 for the signal is different than for the background , the estimate of the signal fraction will be biased .",
    "this can be remedied by including terms related to @xmath170 , colloquially called ` punzi factors ' .",
    "importantly , this means one can not build conditional models like this without knowing or assuming something about @xmath170 .",
    "here i summarize the procedure used by the lhc higgs combination group for computing frequentist @xmath171-values uses for quantifying the agreement with the background - only hypothesis and for determining exclusion limits .",
    "the procedures are based on the profile likelihood ratio test statistic .",
    "the parameter of interest is the overall signal strength factor @xmath64 , which acts as a scaling to the total rate of signal events .",
    "we often write @xmath172 , where @xmath173 is the standard model production cross - section ; however , it should be clarified that the same @xmath64 factor is used for all production modes and could also be seen as a scaling on the branching ratios .",
    "the signal strength is called so that @xmath174 corresponds to the background - only model and @xmath175 is the standard model signal .",
    "it is convenient to separate the full list of parameters @xmath86 into the parameter of interest @xmath64 and the nuisance parameters @xmath176 : @xmath177 .    for a given data set @xmath178 and values for the global observables",
    "@xmath179 there is an associated likelihood function over @xmath64 and @xmath180 derived from combined model over all the channels including all the constraint terms in eq .",
    "[ eq : ftot ] @xmath181 the notation @xmath182 leaves the dependence on the data implicit , which can lead to confusion .",
    "thus , we will explicitly write the dependence on the data when the identity of the dataset is important and only suppress @xmath183 when the statements about the likelihood are generic .",
    "we begin with the definition of the procedure in the abstract and then describe three implementations of the method based on asymptotic distributions , ensemble tests ( toy monte carlo ) , and importance sampling .",
    "this definitions in this section are all relative to a given dataset @xmath178 and value of the global observables @xmath179 , thus we will suppress their appearance .",
    "the nomenclature follows from ref .",
    "@xcite .",
    "the maximum likelihood estimates ( mles ) @xmath184 and @xmath185 and the values of the parameters that maximize the likelihood function @xmath182 or , equivalently , minimize @xmath186 .",
    "the dependence of the likelihood function on the data propagates to the values of the mles , so when needed the mles will be given subscripts to indicate the data set used .",
    "for instance , @xmath187 is the mle of @xmath176 derived from the observed data and global observables .",
    "the conditional maximum likelihood estimate ( cmles ) @xmath188 is the value of @xmath176 that maximizes the likelihood function with @xmath64 fixed ; it can be seen as a multidimensional function of the single variable @xmath64 .",
    "again , the dependence on @xmath178 and @xmath179 is implicit .",
    "this procedure for choosing specific values of the nuisance parameters for a given value of @xmath64 , @xmath178 , and @xmath179 is often referred to as `` profiling '' .",
    "similarly , @xmath188 is often called `` the profiled value of @xmath176 '' .",
    "given these definitions , we can construct the profile likelihood ratio @xmath189 which depends explicitly on the parameter of interest @xmath64 , implicitly on the data @xmath178 and global observables @xmath179 , and is independent of the nuisance parameters @xmath176 ( which have been eliminated via `` profiling '' ) .",
    "+    in any physical theory the rate of signal events is non - negative , thus @xmath190 .",
    "however , it is often convenient to allow @xmath191 ( as long as the pdf @xmath192 everywhere ) .",
    "in particular , @xmath193 indicates a deficit of events signal - like with respect to the background only and the boundary at @xmath174 complicates the asymptotic distributions . ref .",
    "@xcite uses a trick that is equivalent to requiring @xmath190 while avoiding the formal complications of a boundary , which is to allow @xmath194 and impose the constraint in the test statistic itself .",
    "in particular , one defines @xmath195",
    "@xmath196 \\frac { l(\\mu , \\hat{\\hat{\\vec{\\theta}}}(\\mu ) ) }   { l(0 , \\hat{\\hat{\\vec{\\theta}}}(0 ) ) } & \\hat{\\mu } < 0                 \\end{array } \\right.\\ ] ] this is not necessary when ensembles of pseudo - experiments are generated with `` toy '' monte carlo techniques , but since they are equivalent we will write @xmath197 to emphasize the boundary at @xmath174 .    for discovery",
    "the test statistic @xmath198 is used to differentiate the background - only hypothesis @xmath174 from the alternative hypothesis @xmath199 : @xmath200 0 & \\hat{\\mu } \\le 0                 \\end{array } \\right.\\ ] ] note that @xmath198 is test statistic for a one - sided alternative .",
    "note also that if we consider the parameter of interest @xmath190 , then it is equivalent to the two - sided test ( because there are no values of @xmath64 less than @xmath174 .    for limit setting the test statistic @xmath201",
    "is used to differentiate the hypothesis of signal being produced at a rate @xmath64 from the alternative hypothesis of signal events being produced at a lesser rate @xmath202 : @xmath203 0 & \\hat{\\mu } >",
    "\\mu                 \\end{array } \\right .   \\quad = \\quad \\ : \\left\\ { \\ ! \\ ! \\begin{array}{lll } - 2 \\ln \\frac{l(\\mu , \\hat{\\hat{\\vec{\\theta}}}(\\mu ) ) } { l(0 , \\hat{\\hat{\\theta}}(0 ) ) }    & \\hat{\\mu } < 0 \\ ; , \\\\*[0.2 cm ] -2",
    "\\ln \\frac{l(\\mu , \\hat{\\hat{\\vec{\\theta}}}(\\mu ) ) } { l(\\hat{\\mu } , \\hat{\\vec{\\theta } } ) } & 0 \\le \\hat{\\mu } \\le \\mu   \\ ; , \\\\*[0.2 cm ] 0 & \\hat{\\mu } > \\mu \\;.",
    "\\end{array } \\right.\\ ] ] note that @xmath201 is a test statistic for a one - sided alternative ; it is a test statistic for a one - sided upper limit .",
    "the test statistic @xmath204 is used to differentiate signal being produced at a rate @xmath64 from the alternative hypothesis of signal events being produced at a lesser or greater rate @xmath205 .",
    "@xmath206 note that @xmath204 is a test statistic for a two - sided alternative ( as in the case of the feldman - cousins technique , though this is more general as it incorporates nuisance parameters ) .",
    "note that if we consider the parameter of interest @xmath190 and we the test at @xmath174 then there is no `` other side '' and we have @xmath207 .",
    "finally , if one relaxes the constraint @xmath208 then the two - sided test statistic is written @xmath209 or , simply , @xmath210 .",
    "the test statistic should be interpreted as a single real - valued number that represents the outcome of the experiment .",
    "more formally , it is a mapping of the data to a single real - valued number : . for the observed data the test statistic has a given value , eg .",
    "@xmath211 . if one were to repeat the experiment many times the test statistic would take on different values , thus , conceptually , the test statistic has a distribution . similarly",
    ", we can use our model to generate pseudo - experiments using monte carlo techniques or more abstractly consider the distribution . since the number of expected events @xmath212 and the distributions of the discriminating variables @xmath213 explicitly depend on @xmath176 the distribution of the test statistic will also depend on @xmath176 .",
    "let us denote this distribution @xmath214 and we have analogous expressions for each of the test statistics described above .",
    "the @xmath171-value for a given observation under a particular hypothesis ( @xmath215 ) is the probability for an equally or more ` extreme ' outcome than observed assuming that hypothesis @xmath216 the logic is that small @xmath171-values are evidence against the corresponding hypothesis . in toy monte",
    "carlo approaches , the integral above is really carried out in the space of the data @xmath217 .",
    "the immediate difficulty is that we are interested in @xmath64 but the @xmath171-values depend on both @xmath64 and @xmath176 . in the frequentist approach the hypothesis",
    "@xmath218 would not be rejected unless the @xmath171-value is sufficiently small _ for all _ values of @xmath176 .",
    "equivalently , one can use the supremum @xmath171-value for over all @xmath176 to base the decision to accept or reject the hypothesis at @xmath218 .",
    "the key conceptual reason for choosing the test statistics based on the profile likelihood ratio is that asymptotically ( ie .",
    "when there are many events ) the distribution of the profile likelihood ratio is independent of the values of the nuisance parameters .",
    "this follows from wilks s theorem . in that limit @xmath220 for all",
    "@xmath176 .",
    "the asymptotic distributions and are known and described in sec .",
    "[ sec : asymptotic ] . for results based on generating ensembles of pseudo - experiements using toy monte carlo techniques does not assume the form of the distribution @xmath221 , but knowing that it is approximately independent of @xmath176 means that one does not need to calculate @xmath171-values for all @xmath176 ( which is not computationally feasible ) .",
    "since there may still be some residual dependence of the @xmath171-values on the choice of @xmath176 we would like to know the specific value of @xmath222 that produces the supremum @xmath171-value over @xmath176 . since larger @xmath171-values indicate better agreement of the data with the model , it is not surprising that choosing @xmath223 is a good estimate of @xmath222 .",
    "this has been studied in detail by statisticians , and is called the hybrid resampling method and is referred to in physics as the ` profile construction '  @xcite .    based on the discussion above , the following @xmath171-value is used to quantify consistency with the hypothesis of a signal strength of @xmath64 : @xmath224 a standard 95% confidence - level , one - sided frequentist confidence interval ( upper limit ) is obtained by solving for @xmath225 . for downward fluctuations the upper limit of the confidence interval can be arbitrarily small , though it will always include @xmath174 .",
    "this feature is considered undesirable since a physicist would not claim sensitivity to an arbitrarily small signal rate .",
    "the feature was the motivation for the modified frequentist method called @xmath226  @xcite . and the alternative approach called power - constrained limits  @xcite .    to calculate the @xmath226 upper limit , we define @xmath227 as a ratio of p - values , @xmath228 where @xmath229 is the @xmath171-value derived from the same test statistic under the background - only hypothesis @xmath230 the @xmath226 upper - limit on @xmath64 is denoted @xmath231 and obtained by solving for @xmath225 .",
    "it is worth noting that while confidence intervals produced with the `` cls '' method over cover , a value of @xmath64 is regarded as excluded at the 95% confidence level if @xmath232 .",
    "the amount of over coverage is not immediately obvious ; however , for small values of @xmath64 the coverage approaches 100% and for large values of @xmath64 the coverage is near the nominal 95% ( due to @xmath233 ) .    for the purposes discovery one is interested in compatibility of the data with the background - only hypothesis .",
    "statistically , a discovery corresponds to rejecting the background - only hypothesis .",
    "this compatibility is based on the following @xmath171-value @xmath234 this @xmath171-value is also based on the background - only hypothesis , but the test statistic @xmath235 is suited for testing the background - only while the test statistic @xmath236 in eq .  [",
    "eq : pb ] is suited for testing a hypothesis with signal .",
    "it is customary to convert the background - only @xmath171-value into the quantile ( or `` sigma '' ) of a unit gaussian .",
    "this conversion is purely conventional and makes no assumption that the test statistic @xmath237 is gaussian distributed .",
    "the conversion is defined as : @xmath238 where @xmath239 is the inverse of the cumulative distribution for a unit gaussian .",
    "one says the significance of the result is @xmath240 and the standard discovery convention is @xmath241 , corresponding to @xmath242 .",
    "the expected sensitivity for limits and discovery are useful quantities , though subject to some degree of ambiguity . intuitively , the expected upper limit is the upper limit one would expect to obtain if the background - only hypothesis is true .",
    "similarly , the expected significance is the significance of the observation assuming the standard model signal rate ( at some @xmath243 ) . to find the expected limit one needs a distribution @xmath244 . to find the expected significance one needs the distribution @xmath245 or , equivalently , @xmath246 .",
    "we use the median instead of the mean , as it is invariant to the choice of @xmath45 or @xmath247 .",
    "more importantly , is that the expected limit and significance depend on the value of the nuisance parameters @xmath176 , for which we do not know the true values .",
    "thus , the expected limit and significance will depend on some convention for choosing @xmath176 .",
    "while many nuisance parameters have a nominal estimate ( i.e. the global observables in the constraint terms ) , others do not ( eg . the exponent in the @xmath141 background model ) .",
    "thus , we choose a convention that treats all of the nuisance parameters consistently , which is the profiled value based on the observed data .",
    "thus for the expected limit we use @xmath248 and for the expected significance we use @xmath249 .",
    "an unintuitive and possibly undesirable feature of this choice is that the expected limit and significance depend on the observed data through the conventional choice for @xmath176 .",
    "with these distributions we can also define bands around the median upper limit .",
    "our standard limit plot shows a dark green band corresponding to @xmath250 defined by @xmath251 and a light yellow band corresponding to @xmath252 defined by @xmath253      the @xmath171-values in the procedure described above require performing several integrals . in the case of the asymptotic approach ,",
    "the distributions for @xmath254 and @xmath235 are known and the integral is performed directly . when the distributions are not assumed to take on their asymptotic form , then they must be constructed using monte carlo methods . in the `` toy monte carlo ''",
    "approach one generates pseudo - experiments in which the number of events in each channel @xmath2 , the values of the discriminating variables @xmath255 for each of those events , and the auxiliary measurements ( global observables ) @xmath53 are all randomized according to @xmath256 .",
    "we denote the resulting data @xmath257 and global observables @xmath258 . by doing this several times one can build an ensemble of pseudo - experiments and evaluate the necessary integrals .",
    "recall that monte carlo techniques can be viewed as a form of numerical integration .",
    "the fact that the auxiliary measurements @xmath53 are randomized is unfamiliar in particle physics . the more familiar approach for toy monte carlo is that the nuisance parameters are randomized .",
    "this requires a distribution for the nuisance parameters , and thus corresponds to a bayesian treatment of the nuisance parameters .",
    "the resulting @xmath171-values are a hybrid bayesian - frequentist quantity with no consistent definition of probability . to maintain a strictly frequentist procedure ,",
    "the corresponding operation is to randomize the auxiliary measurements .    while formally this procedure is well motivated , as physicists we also know that our models can have deficiencies and we should check that the distribution of the auxiliary measurements does not deviate too far from our expectations . in section  [ sec : crosschecks ]",
    "we show the distribution of the auxiliary measurements and the corresponding @xmath185 from the toy monte carlo technique .",
    "technically , the pseudo - experiments are generated with the ` roostats ` ` toymcsampler ` , which is used by the higher - level tool ` frequentistcalculator ` , which is in turn used by ` hypotestinverter ` .",
    "the following has been extracted from ref .",
    "@xcite and has been reproduced here for convenience .",
    "the primary message of ref .",
    "@xcite is that for a sufficiently large data sample the distributions of the likelihood ratio based test statistics above converge to a specific form . in particular ,",
    "wilks s theorem  @xcite can be used to obtain the distribution @xmath259 , that is the distribution of the test statistic @xmath260 when @xmath64 is true .",
    "note that the asymptotic distribution is independent of the value of the nuisance parameters .",
    "theorem  @xcite provides the generalization to @xmath261 , that is when the true value is not the same as the tested value .",
    "the various formulae listed below are corollaries of wilks s and wald s theorems for the likelihood ratio test statistics described above .",
    "the asimov data described immediately below was a novel result of ref .",
    "@xcite .",
    "the asymptotic formulae below require knowing the variance of the maximum likelihood estimate of @xmath64 @xmath263\\;.\\ ] ] one result of ref .",
    "@xcite is that @xmath264 can be estimated with an artificial dataset referred to as the _ asimov _ dataset .",
    "the asimov dataset is defined as a binned dataset , where the number of events in bin @xmath12 is exactly the number of events expected in bin @xmath12 .",
    "note , this means that the dataset generally has non - integer number of events in each bin . for our general model one can write @xmath265 where the subscript @xmath266 denotes that this is the asimov data .",
    "note , that the dataset depends on the value of @xmath86 implicitly . for an model of unbinned data",
    ", one can simply take the limit of narrow bin widths for the asimov data .",
    "we denote the likelihood evaluated with the asimov data as @xmath267 .",
    "the important result is that one can calculate the expected fisher information of eq .",
    "[ eq : expfisher ] by computing the observed fisher information on the likelihood function based on this special asimov dataset .",
    "a related and convenient way to calculate the variance of @xmath184 is @xmath268 where @xmath269 is the to use the @xmath254 test statistic based on a background - only asimov data ( ie .",
    "the one with@xmath174 in eq .",
    "[ eq : asimovdata ] ) .",
    "it is worth noting that higher - order corrections to the formulae below are being developed to address the case when the variance of @xmath184 depends strongly on @xmath64 .      for a sufficiently large data sample",
    ", the pdf @xmath271 is found to approach @xmath272   \\;.\\ ] ] for the special case of @xmath273 , this reduces to @xmath274 that is , one finds a mixture of a delta function at zero and a chi - square distribution for one degree of freedom , with each term having a weight of @xmath275 . in the following",
    "we will refer to this mixture as a half chi - square distribution or @xmath276 .    from eq .",
    "( [ eq : fq0muprimewald ] ) the corresponding cumulative distribution is found to be @xmath277    the important special case @xmath273 is therefore simply @xmath278 the @xmath171-value of the @xmath174 hypothesis is @xmath279 and therefore for the significance gives the simple formula @xmath280      for a sufficiently large data sample , the pdf @xmath282 is found to approach @xmath283   & + &   \\ : \\left\\ { \\ ! \\ ! \\begin{array}{lll } \\frac{1}{2 } \\frac{1}{\\sqrt{2 \\pi } } \\frac{1}{\\sqrt{\\tilde{q}_{\\mu } } } \\exp \\left [ -\\frac{1}{2 } \\left ( \\sqrt{\\tilde{q}_{\\mu } } - \\frac{\\mu - \\mu^{\\prime}}{\\sigma } \\right)^2 \\right ]                   & 0",
    "< \\tilde{q}_{\\mu } \\le \\mu^2/\\sigma^{2 }   \\\\*[0.5 cm ] \\frac{1}{\\sqrt{2 \\pi } \\sigma } \\exp \\left [ -\\frac{1}{2 } \\frac { ( \\tilde{q}_{\\mu } -   ( \\mu^2 - 2 \\mu \\mu^{\\prime})/\\sigma^{2 } ) ^2 } { ( 2 \\mu/\\sigma)^2 } \\right ]                    &   \\quad \\tilde{q}_{\\mu } > \\mu^2/\\sigma^{2 }                 \\end{array }         \\right . \\;.\\end{aligned}\\ ] ] the special case @xmath284 is therefore @xmath285 \\frac{1}{\\sqrt{2 \\pi } \\sigma } \\exp \\left [ -\\frac{1}{2 } \\frac { ( \\tilde{q}_{\\mu } + \\mu^2/\\sigma^2 ) ^2 } { ( 2 \\mu/\\sigma)^2 } \\right ]                    &   \\quad \\tilde{q}_{\\mu } > \\mu^2/\\sigma^2 \\;.                \\end{array }         \\right.\\ ] ] the corresponding cumulative distribution is @xmath286 \\phi \\left ( \\frac { \\tilde{q}_{\\mu } -   ( \\mu^2 - 2 \\mu \\mu^{\\prime})/\\sigma^{2 } } { 2\\mu/\\sigma } \\right )                   &   \\quad \\tilde{q}_{\\mu } > \\mu^2/\\sigma^{2 } \\;.",
    "\\end{array }         \\right.\\ ] ]    the special case @xmath284 is @xmath287 \\phi \\left ( \\frac { \\tilde{q}_{\\mu } + \\mu^2/\\sigma^2 } { 2\\mu/\\sigma } \\right )                   &   \\quad \\tilde{q}_{\\mu } > \\mu^2/\\sigma^2 \\;.",
    "\\end{array }         \\right.\\ ] ]    the @xmath171-value of the hypothesized @xmath64 is as before given by one minus the cumulative distribution ,    @xmath288    as when using @xmath289 , the upper limit on @xmath64 at confidence level @xmath290 is found by setting @xmath291 and solving for @xmath64 , which reduces to the same result as found when using @xmath289 , namely ,    @xmath292    note that because @xmath264 depends in general on @xmath64 , eq .",
    "( [ eq : muuptilde ] ) must be solved numerically .      for the @xmath226 method",
    "we need distributions for @xmath236 for the hypothesis at @xmath64 and @xmath174 .",
    "we find @xmath294 the median and expected error bands will therefore be @xmath295 with @xmath296 @xmath297 , @xmath64 can be taken as @xmath298 in the calculation of @xmath264 .",
    "note that for @xmath299 we find the median limit @xmath300    the fact that @xmath264 ( the variance of @xmath301 ) defined in eq .",
    "[ eq : sigmaofmu ] in general depends on @xmath64 complicates situations and can lead to some discrepancies between the correct value of the bands and those obtained with the equation above .",
    "the bands tend to be too narrow .",
    "a modified treatment of the bands taking into account the @xmath64 dependence of @xmath264 is under development .",
    "[ the following section has been adapted from text written primarily by sven kreiss , alex read , and myself for the atlas higgs combination .",
    "it is reproduced here for convenience . ]",
    "to claim a discovery , it is necessary to populate a small tail of a test statistic distribution .",
    "toy monte - carlo techniques use the model @xmath256 to generate toy data @xmath302 . for every pseudo - experiment ( toy ) ,",
    "the test statistic is calculated and added to the test statistic distribution .",
    "building this distribution from toys is independent of the assumptions that go into the asymptotic calculation that describes this distribution with an analytic expression .",
    "recently progress has been made using importance sampling to populate the extreme tails of the test statistic distribution , which is much more computationally intensive with standard methods .",
    "the presented algorithms are implemented in  ` toymcsampler ` .",
    "an ensemble of `` standard toys '' is generated from a model representing the null hypothesis with @xmath174 and the nuisance parameters @xmath176 fixed at their profiled values to the observed data @xmath303 , written + . with importance sampling",
    "however , the underlying idea is to generate toys from a different model , called the importance density .",
    "a valid importance density is for example the same model with a non - zero value of @xmath64 .",
    "the simple likelihood ratio is calculated for each toy and used as a weight .",
    "@xmath304    the weighted distribution is equal to a distribution of unweighted toys generated from the null .",
    "the choice of the importance density is a delicate issue .",
    "michael woodroofe presented a prescription for creating a well behaved importance density  @xcite .",
    "unfortunately , this method is impractical for models as large as the combined higgs models .",
    "an alternative approach is shown below .",
    "the first improvement from naive importance sampling is the idea of taking toys from both , the null density and the importance density .",
    "there are various ways to do that .",
    "simply stitching two test statistic distributions together at an arbitrary point has the disadvantage that the normalizations of both distributions have to be known .    instead , it is possible to select toys according to their weights .",
    "first , toys are generated from the null and the simple likelihood ratio is calculated .",
    "if it is larger than one , the toy is kept and otherwise rejected .",
    "next , toys from the importance density are generated . here again , the simple likelihood ratio is calculated but this time the toy is rejected when the likelihood ratio is larger than one and kept when it is smaller than one . if kept , the toy s weight is the simple likelihood ratio which is smaller than one by this prescription .    in the following section , this idea",
    "is restated such that it generalizes to multiple importance densities .",
    "the above procedure for selecting and reweighting toys that were generated from both densities can be phrased in the following way :    * a toy is generated from a density with @xmath305 and the likelihoods @xmath306 and @xmath307 are calculated . *",
    "the toy is veto - ed when the likelihood with @xmath305 is not the largest .",
    "otherwise , the toy is used with a weight that is the ratio of the likelihoods .",
    "this can be generalized to any number of densities with @xmath308 .",
    "for the toys generated from model @xmath160 : @xmath309    the number of importance densities has to be known when applying the vetos .",
    "it should not be too small to cover the parameter space appropriately and it should not be too large , because too many importance densities lead to too many vetoed toys which decreases overall efficiency .",
    "the value and error of @xmath301 from a fit to data can be used to estimate the required number of importance densities for a given target overlap of the distributions .",
    "the sampling efficiency in the tail can be further improved by generating a larger number of toys for densities with larger values of @xmath64 .",
    "for example , for @xmath17 densities , one can generate @xmath310 of the overall toys per density @xmath311 with @xmath312 .",
    "the toys have to be re - weighted for example by @xmath313 resulting in a minimum re - weight factor of one .",
    "the current implementation of the error calculation for the p - value is independent of an overall scale in the weights .",
    "the method using multiple importance densities is similar to michael woodroofe s @xcite prescription of creating a suitable importance density with an integral over @xmath64 . in the method presented here ,",
    "the integral is approximated by a sum over discrete values of @xmath64 .",
    "instead of taking the sum , a mechanism that allows for multiple importance densities is introduced .",
    "future versions of this document will discuss the so - called look - elsewhere effect in more detail .",
    "here we point to the primary development recently : @xcite .",
    "particle physicists regularly set upper - limits on cross sections and other parameters that are bounded to be non - negative .",
    "standard frequentist confidence intervals should nominally cover at the stated value .",
    "the implication that a 95% confidence level upper - limit covers the true value 95% of the time is that it does nt cover the true value 5% of the time .",
    "this is true no matter how small the cross section is .",
    "that means that if there is no signal present , 5% of the time we would be excluding any positive value of the cross - section .",
    "experimentalists do not like this since we would not consider ourselves sensitive to arbitrarily small signals .",
    "two main approaches have been proposed to protect from excluding signals to which we do not consider ourselves sensitive .",
    "the first is the cls procedure introduced by read and described above  @xcite .",
    "the cls procedure produce intervals that over - cover  meaning that the intervals cover the true value more than the desired level .",
    "the coverage for small values of the cross - section approaches 100% , while for large values of the cross section , where the experiment does have sensitivity , the coverage converges to the nominal level ( see fig .  [",
    "fig : clscoverage ] ) .",
    "unfortunately , the coverage for intermediate values is not immediately accessible without more detailed studies .",
    "interestingly , the modified frequentist cls procedure reproduces the one - sided upper limit from a bayesian procedure with a uniform prior on the cross section for simple models like number counting analyses . even in very complicated models we see very good numerical agreement between cls and the bayesian approach , even though the interpretation of the numbers is different .",
    "an alternate approach called power - constrained limits ( pcl ) is to leave the standard frequentist procedure unchanged while adding an additional requirement for a parameter point to be considered ` excluded ' .",
    "the additional requirement is directly a measure of the sensitivity of to that parameter point based on the notion of power ( or type ii error ) .",
    "this approach makes the coverage of the procedure manifest  @xcite .",
    "surprisingly , one - sided upper limits on a bounded parameter are a subtle topic that has led to debates among the experts of statistics in the collaborations and a string of interesting articles from statisticians .",
    "the discussion is beyond the scope of the current version of these notes , but the interested reader is invited and encouraged to read  @xcite and the responses from notable statisticians on the topic .",
    "more recently cousins tried to formalize the sensitivity problem in terms of a concept called negatively biased relevant subsets ( nbrs )  @xcite .",
    "while the power - constrained limits do not formally emit nbrs , it is an interesting insight .",
    "even more recently , vitells has found interesting connections with cls and the work of birnbaum  @xcite .",
    "this connection is significant since statisticians have primarily seen cls as an ad hoc procedure mixing the notion of size and power with no satisfying properties .",
    "[ this section is far from complete . some key practical issues and references to other literature are given . ]    unsurprisingly , bayesian procedures are based on bayes s theorem as in eq .",
    "[ eq : bayes ] and eq .",
    "[ eq : urprior ] .",
    "the bayesian approach requires one to provide a prior over the parameters , which can be seen either as an advantage or a disadvantage  @xcite . in practical terms ,",
    "one typically wants to build the posterior distribution for the parameter of interest .",
    "this typically requires integrating , or _ marginalizing _ , over all the nuisance parameters as in eq .",
    "[ eq : credible ] .",
    "these integrals can be over very high dimensional posteriors with complicated structure .",
    "one of the most powerful algorithms for this integration is markov chain monte carlo , described below . in terms of the prior one",
    "can either embrace the subjective bayesian approach  @xcite or take a more objective approach in which the prior is derived from formal rules .",
    "for instance , jeffreys s prior  @xcite or their generalization in terms of reference priors  @xcite .",
    "given the logical importance of the choice of prior , it is generally recommended to try a few options to see how the result numerically depends on the choice of priors ( i.e .. sensitivity analysis ) .",
    "this leads me to a few great quotes from prominent statisticians :    `` sensitivity analysis is at the heart of scientific bayesianism ''",
    "michael goldstein    `` perhaps the most important general lesson is that the facile use of what appear to be uninformative priors is a dangerous practice in high dimensions '' -brad efron    `` meaningful prior specification of beliefs in probabilistic form over very large possibility spaces is very difficult and may lead to a lot of arbitrariness in the specification ''  michael goldstein    `` objective bayesian analysis is the best frequentist tool around '' jim berger      it is worth mentioning that in particle physics there has been widespread use of a hybrid bayesian - frequentist approach in which one marginalizes nuisance parameters .",
    "perhaps the most well known example is due to a paper by cousins and highland  @xcite . in some instances",
    "one obtains a bayesian - averaged model that depends only on the parameters of interest @xmath314 and then proceeds with the typical frequentist methodology for calculating p - values and constructing confidence intervals .",
    "note , in this approach the constraint terms that are appended to @xmath315 of eq .",
    "[ eq : simultaneous ] to obtain @xmath256 of eq .",
    "[ eq : ftot ] are interpreted as in eq .",
    "[ eq : urprior ] and @xmath316 is usually a uniform prior .",
    "furthermore , the global observables or auxiliary measurements @xmath53 are typically left fixed to their nominal or observed values and not randomized .",
    "in other variants the full model without constraints @xmath317 is used to define the test statistic but the distribution of the test statistic is obtained by marginalizing ( or randomizing ) the nuisance parameters as in eq .",
    "[ eq : urprior ] .",
    "see the following references for more details @xcite .",
    "the shortcomings of this approach are that the coverage is not guaranteed and the method uses an inconsistent notion of probability .",
    "thus it is hard to define exactly what the p - values and intervals mean in a formal sense .",
    "the metropolis - hastings algorithm is used to construct a markov chain @xmath318 , where the samples @xmath319 are proportional to the target posterior density or likelihood function .",
    "the algorithm requires a proposal function @xmath320 that gives the probability density to propose the point @xmath86 given that the last point in the chain is @xmath321 .",
    "note , the density only depends on the last step in the chain , thus it is considered a markov process . at each step in the algorithm ,",
    "a new point in parameter space is proposed and possibly appended to the chain based on its likelihood relative to the current point in the chain . even when the proposal density function is not symmetric , metropolis hastings maintains ` detailed balance ' when constructing the markov chain by counterbalancing the relative likelihood between the two points with the relative proposal density .",
    "that is , given the current point @xmath86 , proposed point @xmath321 , likelihood function @xmath322 , and proposal density function @xmath323 , we visit @xmath321 if and only if @xmath324\\ ] ] note , if the proposal density is symmetric , @xmath325 , then the ratio of the proposal densities can be neglected ( which can be computationally expensive ) .",
    "above we have written the algorithm to sample the likelihood function @xmath326 , but typically one would use the posterior @xmath327 . within  the metropolis - hastings algorithm is implemented with the ` metropolishastings ` class , which returns a ` markovchain ` .",
    "another powerful tool is the bayesian analysis toolkit ( bat )  @xcite .",
    "note , one can use a  /  model in the bat environment .",
    "note , an alternative to markov chain monte carlo is the nested sampling approach of skilling  @xcite and the ` multinest ` implementation  @xcite .",
    "lastly , we mention that sampling algorithms associated to bayesian belief networks and graphical models may offer enormous advantages to both mcmc and nested sampling due to the fact that they can take advantage of the conditional dependencies in the model .",
    "one of the great advances in bayesian methodology was the introduction of jeffreys s rule for selecting a prior based on a formal rule  @xcite .",
    "the rule selects a prior that is invariant under reparametrization of the observables and covariant with reparametrization of the parameters .",
    "the rule is based on information theoretic arguments and the prior is given by the square root of the determinant of the fisher information matrix , which we first encountered in eq .",
    "[ eq : expfisher ] .",
    "@xmath328}\\ ] ] while the right - most form of the prior looks daunting with complex integrals over partial derivatives , the asimov data described in sec .",
    "[ s : asimov ] and ref .",
    "@xcite provide a convenient way to calculate the fisher information .",
    "[ fig : jeffreyspriorgaussian ] and [ fig : jeffreyspriorpoisson ] show examples of  numerical algorithm for calculating jeffreys s prior compared to analytic results on a simple gaussian and a poisson model .",
    "and @xmath264 calculated numerically in ` roostats ` and compared to the analytic result . ]",
    "calculated numerically in ` roostats ` and compared to the analytic result . ]    unfortunately , jeffreys s prior does not behave well in multidimensional problems .",
    "based on a similar information theoretic approach , bernardo and berger have developed the reference priors  @xcite and the associated reference analysis . while attractive in many ways , the approach is fairly difficult to implement .",
    "recently , there has been some progress within the particle physics context in deriving the reference prior for problems relevant to particle physics  @xcite .      for those interested in the deeper and more philosophical aspects of statistical inference",
    ", the likelihood principle is incredibly interesting .",
    "this section will be expanded in the future , but for now i simply suggest searching on the internet , the wikipedia article , and ref .  @xcite . in short",
    "the principle says that all inference should be based on the likelihood function of the observed data .",
    "frequentist procedures violate the likelihood principle since p - values are tail probabilities associated to hypothetical outcomes ( not the observed data ) .",
    "generally , bayesian procedures and those based on the asymptotic properties of likelihood tests do obey the likelihood principle .",
    "somewhat ironically , the objective bayesian procedures such as reference priors and jeffreys s prior can violate the likelihood principle since the prior is based on expectations over hypothetical outcomes .",
    "another topic for the future .",
    "the basic aim of unfolding is to try to correct distributions back to the true underlying distribution before detector smearing. for now , see @xcite .",
    "it was a pleasure to lecture at the 2011 eshep school in cheile gradistei and the 2013 clashep school in peru .",
    "quite a bit of progress has been made in the last few years in terms of statistical methodology , in particular the formalization of a fully frequentist approach to incorporating systematics , a deeper understanding of the look - elsewhere effect , the development of asymptotic approximations of the distributions important for particle physics , and in roads to bayesian reference analysis .",
    "furthermore , most of these developments are general purpose and can be applied across diverse models .",
    "while those developments are interesting , the most important area for most physicists to devote their attention in terms of statistics is to improve the modeling of the data for his or her individual analysis ."
  ],
  "abstract_text": [
    "<S> this document is a pedagogical introduction to statistics for particle physics . </S>",
    "<S> emphasis is placed on the terminology , concepts , and methods being used at the large hadron collider . </S>",
    "<S> the document addresses both the statistical tests applied to a model of the data and the modeling itself . </S>",
    "<S> i expect to release updated versions of this document in the future . </S>"
  ]
}