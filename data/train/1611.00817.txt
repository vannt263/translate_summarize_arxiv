{
  "article_text": [
    "survival analysis is a branch of statistics focused on the study of time - to - event data , usually called survival times .",
    "this type of data appears in a wide range of applications such as failure times in mechanical systems , death times of patients in a clinical trial or duration of unemployment in a population .",
    "one of the main objectives of survival analysis is the estimation of the so - called survival function and the hazard function .",
    "if a random variable has density function @xmath0 and cumulative distribution function @xmath1 , then its survival function @xmath2 is @xmath3 , and its hazard @xmath4 is @xmath5 . while the survival function @xmath6 gives us the probability a patient survives up to time @xmath7 , the hazard function @xmath8 is the instant probability of death given that she has survived until @xmath7 .    due to the nature of the studies in survival analysis ,",
    "the data contains several aspects that make inference and prediction hard .",
    "one important characteristic of survival data is the presence of many covariates .",
    "another distinctive flavour of survival data is the presence of censoring .",
    "a survival time is censored when it is not fully observable but we have an upper or lower bound of it . for instance , this happens in clinical trials when a patient drops out the study .",
    "there are many methods for modelling this type of data .",
    "arguably , the most popular is the kaplan - meier estimator @xcite .",
    "the kaplan - meier estimator is a very simple , nonparametric estimator of the survival function .",
    "it is very flexible and easy to compute , it handles censored times and requires no - prior knowledge of the nature of the data .",
    "nevertheless , it can not handle covariates naturally and no prior knowledge can be incorporated . a well - known method that incorporates covariates",
    "is the cox proportional hazard model @xcite .",
    "although this method is very popular and useful in applications , a drawback of it , is that it imposes the strong assumption that the hazard curves are proportional and non - crossing , which is very unlikely for some data sets .",
    "there is a vast literature of bayesian nonparametric methods for survival analysis @xcite .",
    "some examples include the so - called neutral - to - the - right priors @xcite , which models survival curves as @xmath9)}$ ] , where @xmath10 is a completely random measure on @xmath11 .",
    "two common choices for @xmath10 are the dirichlet process @xcite and the beta - stacy process @xcite , the latter , being a bit more tractable due its conjugacy .",
    "other alternatives place a prior on the hazard function , one example of this , is the extended gamma process @xcite .",
    "the weakness of the above methods is that there is no natural nor direct way to incorporate covariates and thus , they have not been extensively used by practitioners of survival analysis .",
    "more recently , @xcite developed a new model called anova - ddp which mixes ideas from anova and dirichlet processes .",
    "this method successfully incorporates covariates without imposing strong constraints , though it is not clear how to incorporate expert knowledge . within the context of gaussian process",
    ", a few models has been considered , for instance @xcite and @xcite .",
    "nevertheless these models fail to overcome the proportional hazard assumption , which corresponds to one of the aims of this work .",
    "recently , we became aware of the work of @xcite , which uses a so - called accelerated failure times model . here ,",
    "the dependence of the failure times on covariates is modelled by rescaling time , with the rescaling factor modelled as a function of covariates with a gaussian process prior .",
    "this model is different from our proposal , and is more complex to study and to work with .",
    "lastly , another well - known method is random survival forest @xcite .",
    "this can be seen as a generalisation of kaplan meier estimator to several covariates .",
    "it is fast and flexible , nevertheless it can not incorporate expert knowledge and lacks interpretation which is fundamental for survival analysis .    in this paper",
    "we introduce a new semiparametric bayesian model for survival analysis .",
    "our model is able to handle censoring and covariates .",
    "our approach models the hazard function as the multiplication of a parametric baseline hazard and a nonparametric part .",
    "the parametric part of our model allows the inclusion of expert knowledge and provides interpretability , while the nonparametric part allow us to handle covariates and to amend incorrect or incomplete prior knowledge .",
    "the nonparametric part is given by a non - negative function of a gaussian process on @xmath11 .",
    "giving the hazard function @xmath4 of a random variable @xmath12 , we sample from it by simulating the first jump of a poisson process with intensity @xmath4 . in our case , the intensity of the poisson process is a function of a gaussian process , obtaining what is called a gaussian cox process .",
    "one of the main difficulties of working with gaussian cox processes is the problem of learning the ` true ' intensity given the data because , in general , it is impossible to sample the whole path of a gaussian process .",
    "nevertheless , exact inference was proved to be tractable by @xcite .",
    "indeed , the authors developed an algorithm by exploiting a nice trick which allows them to make inference without sampling the whole gaussian process but just a finite number of points .    in this paper , we study basic properties of our prior .",
    "we also provide an inference algorithm based in a sampler proposed by @xcite which is a refined version of the algorithm presented in @xcite . to make the algorithm scale we introduce a random fourier features to approximate the gaussian process and we supply the respective inference algorithm .",
    "we demonstrate the performance of our method experimentally by using synthetic and real data .",
    "consider a continuous random variable @xmath12 on @xmath13 , with density function @xmath0 and cumulative distribution function @xmath1 .",
    "associated with @xmath12 , we have the survival function @xmath14 and the hazard function @xmath15 .",
    "the survival function @xmath6 gives us the probability a patient survives up to time @xmath7 , while the hazard function @xmath8 gives us the instant risk of patient at time @xmath7 .",
    "we define a gaussian process prior over the hazard function @xmath4 .",
    "in particular , we choose @xmath16 , where @xmath17 is a baseline hazard function , @xmath18 is a centred stationary gaussian process with covariance function @xmath19 , and @xmath20 is a positive link function . for our implementation ,",
    "we choose @xmath20 as the sigmoidal function @xmath21 , which is a quite standard choice in applications . in this way",
    ", we generate @xmath12 as the first jump of the poisson process with intensity @xmath4 , i.e. @xmath12 has distribution @xmath22 . our model for a data set of i.i.d .",
    "@xmath23 , without covariates , is    [ eqn : model1 ]",
    "l ( ) ~(0 , ) , & & ( t)|l,_0(t ) = _ 0(t)(l(t ) ) , & & t_i| ( t)e^-_0^t_i ( s)ds ,    which can be interpreted as a baseline hazard with a multiplicative nonparametric noise .",
    "this is an attractive feature as an expert may choose a particular hazard function and then the nonparametric noise amends an incomplete or incorrect prior knowledge .",
    "the incorporation of covariates is discussed later in this section , while censoring is discussed in section  [ sec : inference ] .",
    "notice that @xmath24 for a zero - mean gaussian random variable .",
    "then , as we are working with a centred gaussian process , it holds that @xmath25 .",
    "hence , we can imagine our model as a random hazard centred in @xmath26 with a multiplicative noise . in the simplest scenario",
    ", we may take a constant baseline hazard @xmath27 with @xmath28 .",
    "in such case , we obtain a random hazard centred in @xmath29 , which is simply the hazard function of a exponential random variable with mean @xmath30 .",
    "another choice might be @xmath31 , which determines a random hazard function centred in @xmath32 , which corresponds to the hazard function of the weibull distribution , a popular default distribution in survival analysis .",
    "in addition to the hierarchical model in  , we include hyperparameters to the kernel @xmath19 and to the baseline hazard @xmath17 .",
    "in particular for the kernel , it is common to include a length scale parameter and an overall variance .",
    "finally , we need to ensure the model we proposed defines a well - defined survival function , i.e. @xmath33 as @xmath7 tends to infinity .",
    "this is not trivial as our random survival function is generated by a gaussian process .",
    "the next proposition , proved in the appendix , states that under suitable regularity conditions , the prior defines proper survival functions .",
    "[ propo : propersurvival ] let @xmath34 be a stationary continuous gaussian process .",
    "suppose that @xmath35 . moreover , assume it exists @xmath36 and @xmath37 such that @xmath38 for all @xmath39 .",
    "let @xmath6 be the random survival function associated with @xmath40 , then @xmath41 with probability 1 .",
    "note the above proposition is satisfied by the hazard functions of the exponential and weibull distributions .",
    "also , the condition @xmath35 is satisfied by all @xmath42 decreasing to @xmath43 .",
    "we model the relation between time and covariates by the kernel of the gaussian process prior .",
    "a simple way to generate kernels in time and covariates is to construct kernels for each covariate and time , and then perform basic operation of them , e.g. addition or multiplication .",
    "let @xmath44 denotes a time @xmath7 and with covariates @xmath45 .",
    "then for pairs @xmath44 and @xmath46 we can construct kernels like    k((t , x),(s , y ) ) = k_0(t , s)+_j=1^d k_j(x_j , y_j ) ,    or , the following kernel , which is the one we use in our experiments ,    k((t , x),(s , y ) ) = k_0(t , s)+_j=1^d x_jy_j k_j(t , s ) .    observe that the first kernel establishes an additive relation between time and covariates while the second creates an interaction between the value of the covariates and time .",
    "more complicated structures that include more interaction between covariates can be considered .",
    "we refer to the work of @xcite for details about the construction and interpretation of the operations between kernels .",
    "observe the new kernel produces a gaussian process from the space of time and covariates to the real line , i.e it has to be evaluated in a pair of time and covariates .",
    "the new model to generate @xmath23 , assuming we are given the covariates @xmath47 , is @xmath48    in our construction of the kernel @xmath49 , we choose all kernels @xmath50 as stationary kernels ( e.g. squared exponential ) , so that @xmath49 is stationary with respect to time , so proposition  [ propo : propersurvival ] is valid for each fixed covariate @xmath51 , i.e. giving a fix covariate @xmath51 , we have @xmath52 as @xmath53 .",
    "notice that the likelihood of the model in equation   has to deal with terms of the form @xmath54 as these expressions come from the density of the first jump of a non - homogeneous poisson process with intensity @xmath55 .",
    "in general the integral is not analytically tractable since @xmath55 is defined by a gaussian process . a numerical scheme can be used , but it is approximate and computationally expensive . following @xciteand  @xcite",
    ", we develop a data augmentation scheme based on thinning a poisson process that allows us to efficiently avoid a numerical method .",
    "if we want to sample a time @xmath12 with covariate @xmath51 , as given in equation  , we can use the following generative process . simulate a sequence of points @xmath56 of points distributed according a poisson process with intensity @xmath17 .",
    "we assume the user is using a well - known parametric form and then , sampling the points @xmath56 is tractable ( in the weibull case this can be easily done ) .",
    "starting from @xmath57 we accept the point @xmath58 with probability @xmath59 .",
    "if it is accepted we set @xmath60 , otherwise we try the point @xmath61 and repeat .",
    "we denote by @xmath62 the set of rejected point , i.e. if we accepted @xmath58 , then @xmath63 .",
    "note the above sampling procedure needs to evaluate the gaussian process in the points @xmath64 instead the whole space .    following the above scheme to sample @xmath12",
    ", the following proposition can be shown .",
    "[ propo : augmentation ] let @xmath65 , then @xmath66    consider a poisson process on @xmath67 with intensity @xmath17 .",
    "then , the first term is the density of putting points exactly in @xmath68 .",
    "the second term is the probability of putting no points in @xmath69 \\setminus ( g \\cup \\{t\\})$ ] , i.e. @xmath70 . the second term is independent of the first one .",
    "the last term comes from the acceptance / rejection part of the process .",
    "the points @xmath71 are rejected with probability @xmath72 , while the point @xmath12 is accepted with probability @xmath73 .",
    "since the acceptance / rejection of points is independent of the poisson process we get the result .    using the above proposition , the model of equation",
    "can be reformulated as the following tractable generative model : @xmath74 our model states a joint distribution for the pair @xmath75 where @xmath62 is the set of rejected jump point of the thinned poisson process and @xmath12 is the first accepted one .    to perform inference",
    "we need data @xmath76 , whereas we only receive points @xmath77 .",
    "thus , we need to sample the missing data @xmath78 given @xmath77 .",
    "the next proposition gives us a way to do this .",
    "[ prop : gdadot]@xcite let @xmath12 be a data point with covariate @xmath51 and let @xmath62 be its set of rejected points",
    ". then the distribution of @xmath62 given @xmath79 is distributed as a non - homogeneous poisson process with intensity @xmath80 on the interval @xmath69 $ ] .",
    "the above data augmentation scheme suggests the following inference algorithm . for each data point",
    "@xmath77 sample @xmath81 , then sample @xmath82 , where @xmath83 is the number of data points .",
    "observe that the sampling of @xmath84 given @xmath85 can be seen as a gaussian process binary classification problem , where the points @xmath78 and @xmath23 represent two different classes .",
    "a variety of mcmc techniques can be used to sample @xmath84 , see @xcite for details .    for our algorithm",
    "we use the following notation .",
    "we denote the dataset as @xmath86 .",
    "the set @xmath78 refers to the set of rejected points of @xmath23 .",
    "we denote @xmath87 and @xmath88 for the whole set of rejected and accepted points , respectively . for a point @xmath89",
    "we denote @xmath18 instead of @xmath90 , but remember that each point has an associated covariate . for a set of points",
    "@xmath91 we denote @xmath92 .",
    "also @xmath93 refers to @xmath94 and @xmath95 denotes its inverse function ( it exists since @xmath93 is increasing ) .",
    "finally , @xmath96 denotes the number of iterations we are going to run our algorithm .",
    "the pseudo code of our algorithm is given in algorithm  1 .",
    "lines  [ line : sampleg ] to @xmath97 sample the set of rejected points @xmath78 for each survival time @xmath23 . particularly lines  [ line : samplingn_0 ] to  [ line : endsamplingn_0 ] used the mapping theorem , which tells us how to map a homogeneous poisson process into a non - homogeneous with the appropriate intensity . observe it makes uses of the function @xmath98 and its inverse function , which shall be provided or be easily computable",
    "the following lines classify the points drawn from the poisson process with intensity @xmath99 in the set @xmath78 as in proposition  [ prop : gdadot ] .",
    "line  [ line : sample la ] is used to sample the gaussian process in the set of points @xmath100 given the values in the current set @xmath101 .",
    "observe initially @xmath102 .      usually , in survival analysis",
    ", we encounter three types of censoring : right , left and interval censoring .",
    "we assume each data point @xmath23 is associated with an ( observable ) indicator @xmath103 , denoting the type of censoring or if the time is not censored .",
    "we describe how the algorithm described before can easily handle any type of censorship .",
    "* right censorship : * in presence of right censoring , the likelihood for a survival time @xmath23 is @xmath104 . the related event in terms of the rejected points correspond to do not accept any location @xmath105 .",
    "hence , we can treat right censorship in the same way as the uncensored case , by just sampling from the distribution of the rejected jump times prior @xmath23 . in this case",
    ", @xmath23 is not an accepted location , i.e. @xmath23 is not considered in the set @xmath106 of line  [ line : sample la ] nor  [ line : sample lgt ] .",
    "* left censorship : * in this set - up , we know the survival time is at most @xmath23 , then the likelihood of such time is @xmath107 .",
    "treating this type of censorship is slightly more difficult than the previous case because the event is more complex .",
    "we ask for accepting at least one jump time prior @xmath23 , which might leads us to have a larger set of latent variables . in order to avoid this ,",
    "we proceed by imputing the ` true ' survival time @xmath108 by using its truncated distribution on @xmath109 $ ] .",
    "then we proceed using @xmath108 ( uncensored ) instead of @xmath23 .",
    "we can sample @xmath108 as following : we sample the first point of a poisson process with the current intensity @xmath4 , if such point is after @xmath23 we reject the point and repeat the process until we get one .",
    "the imputation step has to be repeated at the beginning of each iteration .",
    "* interval censorship : * if we know that survival time lies in the interval @xmath110 $ ] we can deal with interval censoring in the same way as left censoring but imputing the survival time @xmath108 in @xmath111 .",
    "as shown is algorithm 1 , in line  [ line : sample la ] we need to sample the gaussian process @xmath40 in the set of points @xmath100 from its conditional distribution , while in line  [ line : sample lgt ] , we have to update @xmath40 in the set @xmath101 .",
    "both lines require matrix inversion which scales badly for massive datasets or for data @xmath112 that generates a large set @xmath113 . in order to help the inference we use a random feature approximation of the kernel  @xcite .",
    "we exemplify the idea on the kernel we use in our experiment , which is given by @xmath114 where each @xmath50 is a square exponential kernel , wuth overall variance @xmath115 and length scale parameter @xmath116 hence , for @xmath117 , the approximation of our gaussian process is given by    [ eqn : approxscheme ] g^m(t , x ) = g^m_0(t)+_j=1^d x_j g^m_j(t )    where each @xmath118 , and each @xmath119 and @xmath120 are independent samples of @xmath121 where @xmath115 is the overall variance of the kernel @xmath50 .",
    "moreover , @xmath122 are independent samples of @xmath123 where @xmath116 is the length scale parameter of the kernel @xmath50 . notice that @xmath124 is a gaussian process since each @xmath125 is the sum of independent normally distributed random variables .",
    "it is know that as @xmath126 goes to infinity , the kernel of @xmath127 approximates the kernel @xmath50 .",
    "the above approximation can be done for any stationary kernel and we refer the reader to @xcite for details .",
    "the inference algorithm for this scheme is practically the same , except for two small changes .",
    "the values @xmath128 in line  [ line : sample la ] are easier to evaluate because we just need to know the values of the @xmath119 and @xmath120 , and no matrix inversion is needed . in line  [",
    "line : sample lgt ] we just need to update all values @xmath129 and @xmath130 . since they are independent variables there is no need for matrix inversion .",
    "all the experiments are performed using our approximation scheme of equation   with a value of @xmath131 . recall that for each gaussian process , we used a squared exponential kernel with overall variance @xmath132 and length scale parameter @xmath116 .",
    "hence for a set of @xmath133 covariates we have a set of @xmath134 hyper - parameters associated to the gaussian processes . in particular , we follow a bayesian approach and place a log - normal prior for the length scale parameter @xmath116 , and a gamma prior ( inverse gamma is also useful since it is conjugate ) for the variance @xmath115 .",
    "we use elliptical slice sampler @xcite for jointly updating the set of coefficients @xmath135 and length - scale parameters .    with respect the baseline hazard we consider two models .",
    "for the first option , we choose the baseline hazard @xmath136 of a weibull random variable . following a bayesian approach ,",
    "we choose a gamma prior on @xmath137 and a uniform @xmath138 on @xmath139 .",
    "notice the posterior distribution for @xmath137 is conjugate and thus we can easily sample from it . for @xmath139 , use a metropolis step to sample from its posterior .",
    "additionally , observe that for the prior distribution of @xmath139 , we constrain the support to @xmath140 .",
    "the reason for this is because the expected size of the set @xmath141 increases with respect to @xmath139 and thus slow down computations .    as second alternative is to choose the baseline hazard as @xmath142 , with gamma prior over the parameter @xmath29 .",
    "the posterior distribution of @xmath29 is also gamma .",
    "we refer to both models as the weibull model ( w - sgp ) and the exponential model ( e - sgp ) respectively .",
    "the implementation for both models is exactly the same as in algorithm 1 and uses the same hyper - parameters described before .",
    "as the tuning of initial parameters can be hard , we use the maximum likelihood estimator as initial parameters of the model .      in this section we present experiments made with synthetic data .",
    "here we perform the experiment proposed in @xcite for crossing data .",
    "we simulate @xmath143 and @xmath144 points from each of the following densities , @xmath145 and @xmath146 , restricted to @xmath11 .",
    "the data contain the sample points and a covariate indicating if such points were sampled from the p.d.f @xmath147 or @xmath148 .",
    "additionally , to each data point , we add 3 noisy covariates taking random values in the interval @xmath149 $ ] .",
    "we report the estimations of the survival functions for the exponential and weibull model in figures  [ fig : crossingexponentialh ] and  [ fig : weicross ] , respectively .    [ cols=\"^,^,^,^ \" , ]",
    "we introduced a bayesian semiparametric model for survival analysis .",
    "our model is able to deal with censoring and covariates . in",
    "can incorporate a parametric part , in which an expert can incorporate his knowledge via the baseline hazard but , at the same time , the nonparametric part allows the model to be flexible .",
    "future work consist in create a method to choose initial parameter to avoid sensitivity problems at the beginning .",
    "construction of kernels that can be interpreted by an expert is something desirable as well .",
    "finally , even though the random features approximation is a good approach and helped us to run our algorithm in large datasets , it is still not sufficient for datasets with a massive number of covariates , specially if we consider a large number of interactions between covariates .",
    "ywt s research leading to these results has received funding from the european research council under the european union s seventh framework programme ( fp7/2007 - 2013 ) erc grant agreement no .",
    "tamara fernndez and nicols rivera were supported by funding from becas chile .",
    "99 ryan  prescott adams , iain murray , and david  jc mackay .",
    "tractable nonparametric bayesian inference in poisson processes with gaussian process intensities . in _ proceedings of the 26th annual international conference on machine learning _",
    ", pages 916 .",
    "acm , 2009 .",
    "james  e barrett and anthony  cc coolen .",
    "gaussian process regression for survival data with competing risks . ,",
    "regression models and life - tables . , 34(2):187220 , 1972 .",
    "maria de  iorio , wesley  o johnson , peter mller , and gary  l rosner .",
    "bayesian nonparametric nonproportional hazards survival modeling .",
    ", 65(3):762771 , 2009 .",
    "kjell doksum .",
    "tailfree and neutral random probabilities and their posterior distributions . ,",
    "pages 183201 , 1974 .",
    "david  k duvenaud , hannes nickisch , and carl  e rasmussen .",
    "additive gaussian processes . in _ advances in neural information processing systems _ , pages 226234 , 2011 .",
    "rl  dykstra and purushottam laud .",
    "a bayesian nonparametric approach to reliability .",
    ", pages 356367 , 1981 .",
    "thomas  s ferguson . a bayesian analysis of some nonparametric problems .",
    ", pages 209230 , 1973 .",
    "nils  lid hjort , chris holmes , peter mller , and stephen  g walker . ,",
    "volume  28 .",
    "cambridge university press , 2010 .",
    "hemant ishwaran , udaya  b kogalur , eugene  h blackstone , and michael  s lauer",
    ". random survival forests . , pages 841860 , 2008 .",
    "heikki joensuu , aki vehtari , jaakko riihimki , toshirou nishida , sonja  e steigen , peter brabec , lukas plank , bengt nilsson , claudia cirilli , chiara braconi , et  al .",
    "risk of recurrence of gastrointestinal stromal tumour after surgery : an analysis of pooled population - based cohorts .",
    ", 13(3):265274 , 2012 .",
    "edward  l kaplan and paul meier .",
    "nonparametric estimation from incomplete observations .",
    ", 53(282):457481 , 1958 .",
    "sara martino , rupali akerkar , and hvard rue .",
    "approximate bayesian inference for survival models .",
    ", 38(3):514528 , 2011 .",
    "iain murray and ryan  p adams .",
    "slice sampling covariance hyperparameters of latent gaussian models . in _ advances in neural information processing systems _ ,",
    "pages 17321740 , 2010 .",
    "iain murray , ryan  prescott adams , and david  jc mackay .",
    "elliptical slice sampling . in _ aistats _ , volume  13 , pages 541548 , 2010 .",
    "ali rahimi and benjamin recht .",
    "random features for large - scale kernel machines . in _ advances in neural information processing systems _ , pages 11771184 , 2007 .",
    "vinayak rao and yee  w. teh .",
    "gaussian process modulated renewal processes . in _ advances in neural information processing systems _ ,",
    "pages 24742482 , 2011 .",
    "terry  m therneau and thomas lumley .",
    "r package ` survival ' , 2015 .",
    "stephen walker and pietro muliere .",
    "beta - stacy processes and a generalization of the plya - urn scheme . , pages 17621780 , 1997 .",
    "denote with @xmath150 the probability associated with the gaussian process @xmath34 and by @xmath151 the corresponding expected values .",
    "we just need to prove that the latter term tends to 0 .",
    "consider the stochastic process @xmath156 given by @xmath157 .",
    "we compute the expected value and variance of @xmath158 . by tonelli",
    "s theorem we have that              the integral in @xmath172 can be computed using the following bound , valid for any @xmath173 , @xmath174 the proof of the above inequality is given in lemma  [ lemma : lem1 ] .",
    "therefore , by denoting by @xmath175 a large enough constant we have @xmath176    using the change of variables @xmath177 and @xmath178 we get from equation  [ eqn : cint ] that @xmath179 adding the integral on @xmath91 with the one over @xmath172 , we get that it exists a large constant @xmath171 , depending on @xmath139 such that for large enough @xmath7 it holds @xmath180 then , by chebyshev s inequality we get @xmath181    let @xmath182 be the event @xmath183 .",
    "let @xmath184 be an increasing succession of times , such that @xmath185 and @xmath186 as @xmath83 tends to @xmath187 .",
    "observe it is always possible to find such @xmath188 because equation  .",
    "observe @xmath189 , then by using the borel - cantelli lemma it holds that all but finite event @xmath190 holds .",
    "then it exists @xmath96 such that for all @xmath191 we have @xmath192 implying that @xmath193 from there , for @xmath191 we have @xmath194 for a small constant @xmath195 , independent of @xmath188 .",
    "then since @xmath6 is decreasing it holds @xmath196      assume @xmath198 .",
    "using that @xmath199 we have @xmath200 note the last integral is the expected value of the sigmoid function of a normal random variable with mean 0 and variance @xmath201 . by the symmetry of the sigmoid function ,",
    "this expected value is the same as if we have variance @xmath202 . finally , by deleting @xmath203 in both sides of the above equation",
    ", we get the desired result ."
  ],
  "abstract_text": [
    "<S> we introduce a semi - parametric bayesian model for survival analysis . </S>",
    "<S> the model is centred on a parametric baseline hazard , and uses a gaussian process to model variations away from it nonparametrically , as well as dependence on covariates . </S>",
    "<S> as opposed to many other methods in survival analysis , our framework does not impose unnecessary constraints in the hazard rate or in the survival function . </S>",
    "<S> furthermore , our model handles left , right and interval censoring mechanisms common in survival analysis . </S>",
    "<S> we propose a mcmc algorithm to perform inference and an approximation scheme based on random fourier features to make computations faster . </S>",
    "<S> we report experimental results on synthetic and real data , showing that our model performs better than competing models such as cox proportional hazards , anova - ddp and random survival forests .    </S>",
    "<S> = 1 </S>"
  ]
}