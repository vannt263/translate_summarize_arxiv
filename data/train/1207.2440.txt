{
  "article_text": [
    "recently there has been a surge of interest in finding low - rank decompositions of matrix - valued data subject to some problem - specific constraints @xcite .",
    "while the methodology proposed herein is applicable to a wide range of low - rank applications , we will focus our attention on the robust principal component analysis problem ( rpca ) described in @xcite .",
    "we begin with the observation model @xmath0 where @xmath1 is an observed data matrix , @xmath2 is an unknown low - rank component , @xmath3 is a sparse corruption matrix , and @xmath4 is diffuse noise , with iid elements distributed as @xmath5 . without loss of generality",
    ", we will assume throughout that @xmath6 .",
    "to estimate @xmath2 and @xmath3 given @xmath7 , one possibility is to solve @xmath8 + \\|s\\|_0,\\ ] ] where @xmath9 denotes the matrix @xmath10 norm of @xmath3 , or a count of the nonzero elements in @xmath3 .",
    "the reason for the factor of @xmath11 is to ensure that the rank and sparsity terms are properly balanced , meaning that both terms range from @xmath12 to @xmath13 , which reflects our balanced uncertainty regarding their relative contributions to @xmath7 .",
    "unfortunately , solving ( [ eq : rpca_prob ] ) is problematic because the objective function is both discontinuous and non - convex . in general , the only way to guarantee that the global minimum is found is to conduct an exhaustive combinatorial search , which is intractable in all but the simplest cases .",
    "the most common alternative , sometimes called principle component pursuit ( pcp ) @xcite , is to replace ( [ eq : rpca_prob ] ) with a convex surrogate such as @xmath14 where @xmath15 denotes the nuclear norm of @xmath2 ( or the sum of its singular values ) .",
    "note that the scale factor from ( [ eq : rpca_prob ] ) has been changed from @xmath11 to @xmath16 ; this is an artifact of the relaxation mechanism balancing the nuclear and @xmath17 norms .",
    "a variety of recent theoretical results stipulate when solutions of ( [ eq : pcp_prob ] ) , particularly in the limiting case where @xmath18 ( reflecting the assumption that @xmath19 ) , will produce reliable estimates of @xmath2 and @xmath3 @xcite .",
    "however , in practice these results have marginal value since they are based upon strong , typically unverifiable assumptions on the support of @xmath3 and the structure of @xmath2 . in general , the allowable support of @xmath3 may be prohibitively small and unstructured ( possibly random ) ; related assumptions are required for the rank and structure of @xmath2 . thus there",
    "potentially remains a sizable gap between what can be achieved by minimizing the ` ideal ' cost function ( [ eq : rpca_prob ] ) and the convex relaxation ( [ eq : pcp_prob ] ) .    in section [ sec : map_estimation ] , as a motivational tool we discuss a simple non - convex scheme based on a variational majorization - minimization approach for locally minimizing ( [ eq : rpca_prob ] ) .",
    "then in section [ sec : empirical_bayes ] we reinterpret this method as _ maximum a posteriori _ ( map ) estimation and use this perspective to design an alternative empirical bayesian algorithm that avoids the major shortcomings of map estimation . section [ sec : analysis ] investigates analytical properties of this empirical bayesian alternative with respect to globally and locally minimizing solutions . later in section [ sec : empirical ] we compare a state - of - the - art pcp algorithm with the proposed approach on simulated data as well as a photometric stereo problem .",
    "one possible alternative to ( [ eq : pcp_prob ] ) is to replace ( [ eq : rpca_prob ] ) with a non - convex yet smooth approximation that can at least be locally minimized . in the sparse estimation literature , one common substitution for the @xmath10 norm",
    "is the gaussian entropy measure @xmath20 , which may also sometimes include a small regularizer to avoid taking the log of zero .",
    "this can be justified ( in part ) by the fact that @xmath21 an analogous approximation suggests replacing the rank penalty with @xmath22 as has been suggested for related rank minimization problems @xcite , since @xmath23 , where @xmath24 are the singular values of @xmath25 .",
    "this leads to the alternative cost function @xmath26 optimization of ( [ eq : map_prob ] ) can be accomplished by a straightforward majorization - minimization approach based upon variational bounds on the non - convex penalty terms @xcite .",
    "for example , because @xmath27 is a concave function of @xmath28 , it can be expressed using duality theory @xcite as the minimum of a particular set of upper - bounding lines : @xmath29 here @xmath30 is a non - negative variational parameter controlling the slope .",
    "therefore , for any fixed @xmath30 we have a strict , convex upper - bound on the concave log function .",
    "likewise , for the rank term we can use the analogous representation @xcite @xmath31 + n\\log| \\psi | + c,\\ ] ] where @xmath32 is an irrelevant constant and @xmath33 is a positive semi - definite matrix of variational parameters .",
    "is full rank , then @xmath33 must be positive definite . ] combining these bounds we obtain an equivalent optimization problem @xmath34 @xmath35 + n\\log| \\psi |,\\ ] ] where @xmath36 is a matrix of non - negative elements composed of the variational parameters corresponding to each @xmath37 . with",
    "@xmath36 and @xmath33 fixed , ( [ eq : map_prob_expand ] ) is quadratic in @xmath2 and @xmath3 and can be minimized in closed form via @xmath38 where @xmath39 , @xmath40 , and @xmath41 represent the @xmath42-th columns of @xmath7 , @xmath2 , and @xmath3 respectively and @xmath43 is a diagonal matrix formed from the @xmath42-th column of @xmath36 .",
    "likewise , with @xmath2 and @xmath3 fixed , @xmath36 and @xmath33 can also be obtained in closed form using the updates @xmath44 while local minimization of ( [ eq : map_prob ] ) is clear cut , finding global solutions can still be highly problematic just as before .",
    "whenever any coefficient of @xmath3 goes to zero , or whenever the rank of @xmath2 is reduced , we are necessarily at a local minimum with respect to this quantity such that we can never increase the rank or a zero - valued coefficient magnitude in search of the global optimum .",
    "( this point will be examined in further detail in section [ sec : analysis ] . )",
    "thus the algorithm may quickly converge to one of a combinatorial number of local solutions .",
    "from a bayesian perspective we can formulate ( [ eq : map_prob ] ) as a map estimation problem based on the distributions @xmath45 \\nonumber \\\\ p(x ) & \\propto & \\frac{1}{\\left| x x^t \\right|^{n/2 } } \\nonumber \\\\",
    "p(s ) & \\propto & \\prod_{i , j } \\frac{1}{\\left|s_{ij}\\right|}.\\end{aligned}\\ ] ] it is then transparent that solving @xmath46 is equivalent to solving ( [ eq : map_prob ] ) after an inconsequential @xmath47 transformation . but",
    "as implied above , this strategy is problematic because the effective posterior is characterized by numerous spurious peaks rendering map estimation intractable .",
    "a more desirable approach would ignore most of these peaks and focus only on regions with significant posterior mass , regions that hopefully also include the posterior mode .",
    "one way to accomplish this involves using the bounds from ( [ eq : bound1 ] ) and ( [ eq : bound2 ] ) to construct a simple approximate posterior that reflects the mass of the original @xmath48 sans spurious peaks .",
    "we approach this task as follows .    from ( [ eq : bound1 ] ) and ( [ eq : bound2 ] ) ) we can infer that @xmath49 where @xmath50 \\\\",
    "\\hat{p}(x;\\psi ) & \\triangleq & \\exp\\left [ -\\frac{1}{2 } \\mbox{trace}\\left[x x^t \\psi^{-1 } \\right ] - \\frac{n}{2 } \\log| \\psi |    \\right ] , \\nonumber\\end{aligned}\\ ] ] which can be viewed as unnormalized approximate priors offering strict lower bounds on @xmath51 and @xmath52 .",
    "we also then obtain a tractable posterior approximation given by @xmath53 here @xmath54 is a gaussian distribution with closed - form first and second moments , e.g. , the means of @xmath3 and @xmath2 are actually given by the righthand sides of ( [ eq : means ] ) .",
    "the question remains how to choose @xmath36 and @xmath33 . with the goal of reflecting the mass of the true distribution @xmath55",
    ", we adopt the approach from @xcite and attempt to solve @xmath56 @xmath57    the basic idea here is that we only care that the approximate priors match the true ones in regions where the likelihood function @xmath58 is significant ; in other regions the mismatch is more or less irrelevant .",
    "moreover , by virtue of the strict lower variational bound , ( [ eq : min_error_mass2 ] ) reduces to @xmath59 where @xmath60\\ ] ] with @xmath61 this @xmath62 can be viewed as the covariance of the @xmath42-th column of @xmath7 given fixed values of @xmath33 and @xmath36 . to recap then , we need now minimize @xmath63 with respect to @xmath33 and @xmath36 , and then plug these estimates into ( [ eq : approx_posterior ] ) giving the approximate posterior .",
    "the mean of this distribution ( see below ) can then be used as a point estimate for @xmath2 and @xmath3 .",
    "this process is sometimes referred to as _ empirical bayes _ because we are using the data to guide our search for an optimal prior distribution @xcite .",
    "it turns out that minimization of ( [ eq : emp_bayes_cost ] ) can be accomplished concurrently with computation of the posterior mean leading to simple , efficient update rules . while ( [ eq : emp_bayes_cost ] ) is non - convex , we can use a majorization - minimization approach analogous to that used for map estimation . for this purpose",
    ", we utilize simplifying upper bounds on both terms of the cost function as has been done for related sparse estimation problems @xcite .",
    "first , the data - dependent term is concave with respect to @xmath64 and @xmath65 and hence can be expressed as a minimization over @xmath66-dependent hyperplanes . with some linear algebra , it can be shown that @xmath67 for all @xmath42 . with a slight abuse of notation ,",
    "we adopt @xmath68 $ ] and @xmath69 $ ] as the variational parameters in ( [ eq : data_decomp ] ) because they end up playing the same role as the unknown low - rank and sparse coefficients and provide a direct link to the map estimates .",
    "additionally , the @xmath40 and @xmath41 which minimize ( [ eq : data_decomp ] ) turn out to be equivalent to the posterior means of ( [ eq : approx_posterior ] ) given @xmath33 and @xmath36 and will serve as our point estimates .    secondly ,",
    "for the log - det term , we first use the determinant identity @xmath70 where @xmath71   + \\left [ \\begin{array}{cc } \\psi^{-1 } & { \\bf 0 }   \\\\ { \\bf 0 } & \\bar{\\gamma}_j^{-1 } \\end{array } \\right]\\ ] ] and @xmath32 is an irrelevant constant .",
    "the term @xmath72 is jointly concave in both @xmath64 and @xmath73 and thus can be bounded in a similar fashion as ( [ eq : data_decomp ] ) , although a closed - form solution is no longer available .",
    "( other decompositions lead to different bounds and different candidate update rules . ) here we use @xmath74 @xmath75 - h^*(u_j , v_j ) \\nonumber\\ ] ] where @xmath76 is the concave conjugate function of @xmath72 with respect to @xmath64 and @xmath77 .",
    "note that while @xmath76 has no closed - form solution , the minimizing values of @xmath78 and @xmath79 can be computed in closed - form via @xmath80    when we drop the minimizations over the variational parameters @xmath40 , @xmath41 , @xmath78 , and @xmath79 for all @xmath42 , we arrive at a convenient family of upper bounds on the cost function @xmath63 . given some estimate of @xmath33 and @xmath36",
    ", we can evaluate all variational parameters in closed form ( see below ) .",
    "likewise , given all of the variational parameters we can solve directly for @xmath33 and @xmath36 because now @xmath63 has been conveniently decoupled and we need only compute @xmath81 \\right ) + n\\log\\left|\\psi \\right|\\ ] ] and @xmath82_{ii}}{\\gamma_{ij } } + \\log \\gamma_{ij } , \\hspace*{0.3 cm } \\forall i , j.\\ ] ]    we summarize the overall procedure next .      1 .",
    "compute @xmath83 2 .",
    "initialize @xmath84 , and @xmath85 for all @xmath42 .",
    "3 .   for the @xmath86-th iteration , compute the optimal @xmath40 and @xmath41 via @xmath87",
    "@xmath88 4 .",
    "likewise , compute the optimal @xmath78 and @xmath79 via @xmath89 @xmath90 5 .",
    "update @xmath33 and @xmath36 using the new variational parameters via + @xmath91 \\nonumber\\ ] ] + @xmath92_{ii } , \\forall i , j\\ ] ] 6 .   repeat steps 3 through 5 until convergence .",
    "( recall that @xmath43 is a diagonal matrix formed from the @xmath42-th column of @xmath36 . )",
    "this process is guaranteed to reduce or leave unchanged the cost function at each iteration .",
    "note that if we set @xmath93 for all @xmath42 , then the algorithm above is guaranteed to ( at least locally ) minimize the map cost function from ( [ eq : map_prob ] ) .",
    "additionally , for matrix completion problems @xcite , where the support of the sparse errors is known a priori , we need only set each @xmath94 corresponding to a corrupted entry to @xmath95 .",
    "this limiting case can easily be handled with efficient reduced rank updates .",
    "one positive aspect of this algorithm is that it is largely parameter free .",
    "we must of course choose some stopping criteria , such as a maximum number of iterations or a convergence tolerance .",
    "( for all experiments in section [ sec : empirical ] we simply set the maximum number of iterations at 100 . )",
    "we must also choose some value for @xmath96 , which balances allowable contributions from a diffuse error matrix @xmath4 , although frequently methods have some version of this parameter , including the pcp algorithm .",
    "for all of our experiments we simply choose @xmath97 since we did not include an @xmath4 component consistent with the original rpca formulation from @xcite .    from a complexity standpoint ,",
    "each iteration of the above algorithm can be computed in @xmath98 , where @xmath99 , so it is linear in the larger dimension of @xmath7 and cubic in the smaller dimension . for many computer vision applications ( see section [ sec : empirical ] for one example ) ,",
    "images are vectorized and then stacked , so @xmath7 may be @xmath100number - of - images by @xmath101 number - of - pixels .",
    "this is relatively efficient , since the number of images may be on the order of 100 or fewer ( see @xcite ) .",
    "however , when @xmath7 is a large square matrix , the updates are more expensive to compute . in the future we plan to investigate various approximation techniques to handle this scenario .    as a final implementation - related point , when given access to a priori knowledge regarding the rank of @xmath2 and/or sparsity of @xmath3 , it is possible to bias the algorithm s initialization ( from step 1 above ) and improve the estimation accuracy . however , we emphasize that for all of the experiments reported in section [ sec : empirical ] we assumed no such knowledge .",
    "two other bayesian - inspired methods have recently been proposed for solving the rpca problem .",
    "the first from @xcite is a hierarchical model with conjugate prior densities on model parameters at each level such that inference can be performed using a gibbs sampler .",
    "this method is useful in that the @xmath96 parameter balancing the contribution from diffuse errors @xmath4 is estimated directly from the data .",
    "moreover , the authors report significant improvement over pcp on example problems .",
    "a potential downside of this model is that theoretical analysis is difficult because of the underlying complexity .",
    "additionally , a large number of mcmc steps are required to obtain good estimates leading to a significant computational cost even when @xmath7 is small .",
    "it also uses an estimate of @xmath102 $ ] which can effect the convergence rate of the gibbs sampler .",
    "a second method from @xcite similarly employs a hierarchial bayesian model but uses a factorized mean - field variational approximation for inference @xcite .",
    "note that this is an entirely different type of variational method than ours , relying on a posterior distribution that factorizes over @xmath2 and @xmath3 , meaning @xmath103 , where @xmath104 and @xmath105 are approximating distributions learned by minimizing a free energy - based cost function .",
    "unlike our model , this factorization implicitly decouples @xmath2 and @xmath3 in a manner akin to map estimation , and may potentially produce more locally minimizing solutions ( see analysis below ) .",
    "moreover , while this approach also has a mechanism for estimating @xmath96 , there is no comprehensive evidence given that it can robustly expand upon the range of corruptions and rank that can already be handled by pcp .    to summarize both of these methods then",
    ", we would argue that while they offer a compelling avenue for computing @xmath96 automatically , the underlying cost functions are substantially more complex than pcp or our method rendering more formal analyses somewhat difficult . as we shall see in sections",
    "[ sec : analysis ] and [ sec : empirical ] , the empirical bayesian cost function we propose is analytically principled and advantageous , and empirically outperforms pcp by a wide margin .",
    "in this section we will examine global and local minima properties of the proposed method and highlight potential advantages over map , of which pcp can also be interpreted as a special case . for analysis purposes and comparisons with map estimation",
    ", it is helpful to convert the empirical bayes cost function ( [ eq : emp_bayes_cost ] ) into @xmath106-space by first optimizing over @xmath78 , @xmath79 , @xmath33 and @xmath36 , leaving only the unknown coefficient matrices @xmath2 and @xmath3 . using this process",
    ", it is easily shown that the estimates of @xmath2 and @xmath3 obtained by globally ( or locally ) minimizing ( [ eq : emp_bayes_cost ] ) will also globally ( or locally ) minimize @xmath107 where the penalty function is given by @xmath108 @xmath109 note that the implicit map penalty from ( [ eq : map_prob ] ) is nearly identical : @xmath110 @xmath111 the primary distinction is that in the map case the variational parameters separate whereas in empirical bayesian case they do not .",
    "( note that , as discussed below , we can apply a small regularizer analogous to @xmath96 to the log terms in the map case as well . )",
    "this implies that @xmath112 can be expressed as some @xmath113 whereas @xmath114 can not .",
    "a related form of non - separability has been shown to be advantageous in the context of sparse estimation from overcomplete dictionaries @xcite .",
    "we now examine how this crucial distinction can be beneficial in producing maximally sparse , low - rank solutions that optimize ( [ eq : rpca_prob ] ) .",
    "we first demonstrate how ( [ eq : x_space_type_ii ] ) mimics the global minima profile of ( [ eq : rpca_prob ] ) .",
    "later we show how the smoothing mechanism of the empirical bayesian marginalization can mitigate spurious locally minimizing solutions .",
    "the original rpca development from @xcite assumes that @xmath19 , which is somewhat easier to analyze .",
    "we consider this scenario first .",
    "[ thm : global_min_match ] assume that there exists at least one solution to @xmath115 such that @xmath102 + \\max_j \\| { { \\bm s } } _ j\\|_0 < m$ ] .",
    "then in the limit as @xmath18 , any solution that globally minimizes ( [ eq : x_space_type_ii ] ) will globally minimize ( [ eq : rpca_prob ] ) .",
    "proofs will be deferred to a subsequent journal publication .",
    "note that the requirement @xmath102 + \\max_j \\| { { \\bm s } } _",
    "j\\|_0 < m$ ] is a relatively benign assumption , because without it the matrices @xmath2 and @xmath3 are formally unidentifiable even if we are able to globally solve ( [ eq : rpca_prob ] ) . for @xmath116 , we may still draw direct comparisons between ( [ eq : x_space_type_ii ] ) and ( [ eq : rpca_prob ] ) when we deviate slightly from the bayesian development and",
    "treat @xmath114 as an abstract , stand - alone penalty function . in this context",
    "we may consider @xmath117 , with @xmath118 as a more general candidate for estimating rpca solutions .",
    "[ cor : global_min_match2 ] assume that @xmath119 and @xmath120 are a unique , optimal solution to ( [ eq : rpca_prob ] ) and that @xmath121 + \\max_j \\|\\left [ { { \\bm s } } _ { ( \\lambda)}\\right]_j\\|_0 < m$ ] . then there will always exist some @xmath122 and @xmath123 such that the global minimum of @xmath124 denoted @xmath125 and @xmath126 , satisfies the conditions @xmath127 and @xmath128 , where @xmath129 can be arbitrarily small .    of course map estimation can satisfy a similar property as theorem [ thm : global_min_match ] and corollary [ cor : global_min_match2 ] after a minor modification .",
    "specifically , we may define @xmath130 @xmath131 and then achieve a comparable result to the above using @xmath132 .",
    "the advantage of empirical bayes then is not with respect to global minima , but rather with respect to local minima .",
    "the separable , additive low - rank plus sparsity penalties that emerge from map estimation will always suffer from the following limitation :    [ thm : global_min_match2 ] let @xmath133 denote any matrix @xmath3 with @xmath134 .",
    "now consider any optimization problem of the form @xmath135 where @xmath136 is an arbitrary function of the singular values of @xmath2 and @xmath137 is an arbitrary function of the magnitudes of the elements in @xmath3 . then to ensure that a global minimum of ( [ eq : general_rpca_map ] ) is a global minimum of ( [ eq : rpca_prob ] ) for all possible @xmath7 , we require that @xmath138 - g_2\\left[s_{ij}^{(0)}\\right]}{\\epsilon } = \\infty\\ ] ] for all @xmath139 and @xmath42 and @xmath3 .",
    "an analogous condition holds for the function @xmath136 .",
    "this result implies that whenever an element of @xmath3 approaches zero , it will require increasing the associated penalty @xmath140 against an arbitrarily large gradient to escape in cases where this coefficient was incorrectly pruned . likewise ,",
    "if the rank of @xmath2 is prematurely reduced in the wrong subspace , there may be no chance to ever recover since this could require increasing @xmath141 against an arbitrarily large gradient factor .",
    "in general , theorem [ thm : global_min_match2 ] stipulates that if we would like to retain the same global minimum as ( [ eq : rpca_prob ] ) using a map estimation - based cost function , then we will necessarily enter an inescapable basin of attraction whenever _ either _",
    "@xmath102 < m$ ] _ or _",
    "@xmath142 for some @xmath42 .",
    "this is indeed a heavy price to pay .",
    "crucially , because of the coupling of low - rank and sparsity regularizers , the penalty function @xmath114 does not have this limitation .",
    "in fact , we only encounter insurmountable gradient barriers when @xmath102 + \\| { { \\bm s } } _ j\\|_0 < m$ ] for some @xmath42 , in which case the covariance @xmath62 from ( [ eq : covariance ] ) becomes degenerate ( with @xmath96 small ) , a much weaker condition . to summarize ( emphasize ) this point then , map can be viewed as heavily dependent on degeneracy of the matrices @xmath33 and @xmath36 in isolation , whereas empirical bayes is only sensitive to degeneracy of their summation",
    ".    this distinction can also be observed in how the effective penalties on @xmath2 and @xmath3 , as imbedded in @xmath114 , vary given fixed values of @xmath36 or @xmath33 respectively .",
    "for example , when @xmath33 is close to being full rank and orthogonal ( such as when the algorithm is initialized ) , then the implicit penalty on @xmath3 is minimally non - convex ( only slightly concave ) .",
    "in fact , as @xmath33 becomes large and orthogonal , the penalty converges to a scaled version of the @xmath17 norm .",
    "in contrast , as @xmath33 becomes smaller and low - rank , the penalty approaches a scaled version of the @xmath10 norm , implying that maximally sparse corruptions will be favored .",
    "thus , we do not aggressively favor maximally sparse @xmath3 until the rank has already been reduced and we are in the basin of attraction of a good solution .",
    "of course no heuristic annealing strategy is necessary , the transition is handled automatically by the algorithm .",
    "additionally , whenever @xmath33 is fixed , the resulting cost function formally decouples into @xmath11 separate , canonical sparse estimation problems on each @xmath41 in isolation . with @xmath143 ,",
    "it not difficult to show that each of these subproblems is equivalent to solving @xmath144 where @xmath145 is a concave sparsity penalty on @xmath41 and @xmath146 is any matrix such that @xmath147 . is full rank . ]",
    "when @xmath146 is nearly orthogonal , this problem has no local minima and a global solution that approximates the hard thresholding of the @xmath10 norm ; however , direct minimization of the @xmath10 norm will have @xmath148 local minima @xcite .",
    "in contrast , when @xmath146 is poorly conditioned ( with approximately low - rank structure , it has been argued in @xcite that penalties such as @xmath149 are particularly appropriate for avoiding local minima .",
    "something similar occurs when @xmath36 is now fixed and we evaluate the penalty on @xmath2 .",
    "this penalty approaches something like a scaled version of the nuclear norm ( less concave ) when elements of @xmath36 are set to a large constant and it behaves more like the rank function when @xmath36 is small . at initialization ,",
    "when @xmath36 is all ones , we are relatively free to move between solutions of various rank without incurring a heavy penalty . later as @xmath36 becomes sparse , solutions satisfying @xmath102 + \\| { { \\bm s } } _ j\\|_0 < m$ ] for some @xmath42 become heavily favored .    as a final point , the proposed empirical bayesian approach can be implemented with alternative variational bounds and possibly optimized with something akin to simultaneous reweighted nuclear and @xmath17 norm minimization , a perspective that naturally suggests further performance analyses such as those applied to sparse estimation in @xcite .",
    "this section provides some empirical evidence for the efficacy of our rpca method .",
    "first , we present comparisons with pcp recovering random subspaces from corrupted measurements .",
    "later we discuss a photometric stereo application . in all cases",
    "we used the the augmented lagrangian method ( alm ) from @xcite to implement pcp .",
    "this algorithm has efficient , guaranteed convergence and in previous empirical tests alm has outperformed a variety of other methods in computing minimum nuclear norm plus @xmath17 norm solutions .",
    "here we demonstrate that the empirical bayesian algorithm from section [ eq : algorithm ] , which we will refer to as eb , can recovery unknown subspaces from corrupted measurements in a much broader range of operating conditions compared to the convex pcp .",
    "in particular , for a given value of @xmath102 $ ] , our method can handle a substantially larger fraction of corruptions as measured by @xmath150 .",
    "likewise , for a given value of @xmath151 , we can accurately estimate an @xmath2 with much higher rank .",
    "consistent with @xcite , we consider the case where @xmath19 , such that all the error is modeled by @xmath3 .",
    "this allows us to use the stable , convergent alm code available online .",
    "the first experiment proceeds as follows .",
    "we generate a low - rank matrix @xmath2 with dimensions reflective of many computer vision problems : _ number - of - images _",
    "@xmath152 _ number - of - pixels_. here we choose @xmath153 and @xmath154 , the later dimension equivalent to a @xmath155 pixel image . for each trial , we compute an @xmath156 matrix with iid @xmath157 entries .",
    "we then compute the svd of this matrix and set all but the @xmath158 largest singular values to zero to produce a low - rank @xmath2 .",
    "@xmath3 is generated with nonzero entries selected uniformly with probability @xmath159 .",
    "nonzero values are sampled from an iid uniform[-10,10 ] distribution .",
    "we then compute @xmath115 and try to estimate @xmath2 and @xmath3 using the eb and pcp algorithms .",
    "estimation results averaged over multiple trials as @xmath158 is varied from @xmath160 to @xmath161 are depicted in figure [ fig : empirical_results_1 ] .",
    "we plot normalized mean - squared error ( mse ) as computed via @xmath162 as well as the average angular error between the estimated and true subspaces . in both cases",
    "the average is across 10 trials .    from figure",
    "[ fig : empirical_results_1 ] we observe that eb can accurately estimate @xmath2 for substantially higher values of the rank .",
    "interestingly , we are also still able to estimate the correct subspace spanned by columns of @xmath2 perfectly even when the mse of estimating @xmath2 starts to rise ( compare figure [ fig : empirical_results_1](_top _ ) with figure [ fig : empirical_results_1](_bottom _ ) ) .",
    "basically , this occurs because , even if we have estimated the subspace perfectly , reducing the mse to zero implicitly requires solving a challenging sparse estimation problem for every observation column @xmath39 . for each column",
    ", this problem requires learning @xmath163",
    "+ \\| { { \\bm s } } _ j\\|_0 $ ] nonzero entries given only @xmath153 observations . for our experiment",
    ", we can have @xmath164 with high probability for some columns when the rank is high , and thus we may expect some errors in @xmath165 ( not shown ) .",
    "however , the encouraging evidence here is that eb is able to keep these corrupting errors at a minimum and estimate the subspace accurately long after pcp has failed . moreover , if an accurate estimate of @xmath2 is needed , as opposed to just the correct spanning subspace , then a postprocessing error correction step can potentially be applied to each column individually to improve performance .",
    "the second experiment is similar to the first only now we hold @xmath102 $ ] fixed at 4 , meaning @xmath102/m = 0.2 $ ] , and vary the fraction of corrupted entries in @xmath3 from @xmath166 to 0.8 . figure [ fig : empirical_results_2 ] shows that eb is again able to drastically expand the range whereby successful estimates are obtained .",
    "notably it is able to recover the correct subspace even with 70% corrupted entries .    as a final comparison , we tested pcp and eb on a @xmath167 observation matrix @xmath7 generated as above with a @xmath102/m = 0.1 $ ] and @xmath168 .",
    "the estimation results are reported in table [ tab:400_by_400_result ] .",
    "pcp performs poorly since the normalized mse is above one , meaning we would have been better off simply choosing @xmath169 in this regard . additionally , the angular error is very near 90 degrees , consistent with the error from a randomly chosen subspace in high dimensions .",
    "in contrast , eb provides a reasonably good estimate considering the difficulty of the problem .",
    "[ sample - table ]    rrrr & & +   + mse ( norm . ) & 1.235 & 0.066 + angular error & 88.50 & 5.01 +      photometric stereo is a method for estimating surface normals of an object or scene by capturing multiple images from a fixed viewpoint under different lighting conditions @xcite . at a basic level",
    ", this methodology assumes a lambertian object surface , point light sources at infinity , an orthographic camera view , and a linear sensor response function . under these conditions",
    ", it has been shown that the intensities of a vectorized stack of images @xmath7 can be expressed as @xmath170 where @xmath171 is a @xmath172 matrix of @xmath173 normalized lighting directions , @xmath174 is a @xmath175 matrix of surface normals at @xmath11 pixel locations , and @xmath176 is a diagonal matrix of diffuse albedo values @xcite .",
    "thus , if we were to capture at least 3 images with known , linearly independent lighting directions we can solve for @xmath174 using least squares .",
    "of course in reality many common non - lambertian effects can disrupt this process , such as specularities , cast or attached shadows , and image noise , etc .",
    "in many cases , these effects can be modeled as an additive sparse error term @xmath3 applied to ( [ eq : lambertian ] ) .    as proposed in @xcite",
    ", we can estimate the subspace containing @xmath174 by solving ( [ eq : rpca_prob ] ) assuming @xmath177 and @xmath178 .",
    "the resulting @xmath179 , combined with possibly other _ a priori _ information regarding the lighting directions @xmath171 can lead to an estimate of @xmath174 . @xcite",
    "propose using a modified version of pcp for this task , where a shadow mask is included to simplify the sparse error correction problem .",
    "however , in practical situations it may not always be possible to accurately locate all shadow regions in this manner so it is desirable to treat them as unknown sparse corruptions .    for this experiment",
    "we consider the synthetic caesar image from the inria 3d meshes research database with known surface normals .",
    "multiple 2d images with different known lighting conditions can easily be generated using the cook - torrance reflectance model @xcite .",
    "these images are then stacked to produce @xmath7 . because shadows are extremely difficult to handle in general , as a preprocessing step we remove rows of @xmath7 corresponding to pixel locations with more than 10% shadow coverage .",
    "specular corruptions were left unfiltered .",
    "we tested our algorithm as the number of images , drawn randomly from a batch of 40 total , was varied from 10 to 40 .",
    "results averaged across 5 trials are presented in figure [ fig : empirical_results_photo ] .",
    "the error metrics have been redefined to accommodate the photometric stereo problem .",
    "we now define the normalized mse as @xmath180 , which measures how much improvement we obtain beyond just using the observation matrix @xmath7 directly .",
    "similarly we normalized the angular error by dividing by the angle between @xmath7 and the true @xmath2 for each trial .    from figure",
    "[ fig : empirical_results_photo ] it is clear that eb outperforms pcp in both mse and angular error , especially when there are fewer images present .",
    "it is not entirely clear however why the mse and angular error are relatively flat for eb as opposed to dropping lower as @xmath173 increases .",
    "of course these are errors relative to using @xmath7 directly to predict @xmath2 , which could play a role in this counterintuitive effect .",
    "in this paper we have analyzed a new empirical bayesian approach for matrix rank minimization in the context of rpca , where the goal is to decompose a given data matrix into low - rank and sparse components . using a variational approximation and subsequent marginalization , we ultimately arrive at a novel regularization term that couples low - rank and sparsity penalties in such a way that locally minimizing solutions are effectively smoothed while the global optimum matches that of the ideal rpca cost function .",
    "wipf , d.p . and nagarajan , s. iterative reweighted @xmath17 and @xmath181 methods for finding sparse solutions .",
    "_ journal of selected topics in signal processing ( special issue on compressive sensing ) _ , 40 ( 2 ) , april 2010 ."
  ],
  "abstract_text": [
    "<S> in many applications that require matrix solutions of minimal rank , the underlying cost function is non - convex leading to an intractable , np - hard optimization problem . </S>",
    "<S> consequently , the convex nuclear norm is frequently used as a surrogate penalty term for matrix rank . </S>",
    "<S> the problem is that in many practical scenarios there is no longer any guarantee that we can correctly estimate generative low - rank matrices of interest , theoretical special cases notwithstanding . </S>",
    "<S> consequently , this paper proposes an alternative empirical bayesian procedure build upon a variational approximation that , unlike the nuclear norm , retains the same globally minimizing point estimate as the rank function under many useful constraints . </S>",
    "<S> however , locally minimizing solutions are largely smoothed away via marginalization , allowing the algorithm to succeed when standard convex relaxations completely fail . while the proposed methodology is generally applicable to a wide range of low - rank applications , we focus our attention on the robust principal component analysis problem ( rpca ) , which involves estimating an unknown low - rank matrix with unknown sparse corruptions . </S>",
    "<S> theoretical and empirical evidence are presented to show that our method is potentially superior to related map - based approaches , for which the convex principle component pursuit ( pcp ) algorithm @xcite can be viewed as a special case . </S>"
  ]
}