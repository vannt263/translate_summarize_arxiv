{
  "article_text": [
    "this paper is a contribution to the methodology of bayesian variable selection for linear gaussian regression models , an important problem which has been much discussed both from a theoretical and a practical perspective ( see chipman _ et al .",
    "_ , 2001 and clyde and george , 2004 for literature reviews ) .",
    "recent advances have been made in two directions , unravelling the theoretical properties of different choices of prior structure for the regression coefficients ( fernndez _ et al .",
    "_ , 2001 ; liang _ et al .",
    "_ , 2008 ) and proposing algorithms that can explore the huge model space consisting of all the possible subsets when there are a large number of covariates , using either mcmc or other search algorithms ( kohn _ et al . _ , 2001 ; dellaportas _ et al .",
    "_ , 2002 ; hans _ et al .",
    "_ , 2007 ) .    in this paper",
    ", we propose a new sampling algorithm for implementing the variable selection model , based on tailoring ideas from evolutionary monte carlo ( liang and wong , 2000 ; jasra _ et al . _ , 2007 ; wilson _ et al .",
    "_ , 2009 ) in order to overcome the known difficulties that mcmc samplers face in a high dimension multimodal model space : enumerating the model space becomes rapidly unfeasible even for a moderate number of covariates . for a bayesian approach to be operational",
    ", it needs to be accompanied by an algorithm that samples the indicators of the selected subsets of covariates , together with any other parameters that have not been integrated out .",
    "our new algorithm for searching through the model space has many generic features that are of interest _ per se _ and can be easily coupled with any prior formulation for the variance - covariance of the regression coefficients .",
    "we illustrate this by implementing @xmath3-priors for the regression coefficients as well as independent priors : in both cases the formulation we adopt is general and allows the specification of a further level of hierarchy on the priors for the regression coefficients , if so desired .",
    "the paper is structured as follows . in section [ background ] , we present the background of bayesian variable selection , reviewing briefly alternative prior specifications for the regression coefficients , namely @xmath3-priors and independent priors .",
    "section [ mcmc sampler ] is devoted to the description of our mcmc sampler which uses a wide portfolio of moves , including two proposed new ones .",
    "section [ performance_ess ] demonstrates the good performance of our new mcmc algorithm in a variety of real and simulated examples with different structures on the predictors . in section [ simulation study ]",
    "we complement the results of the simulation study by comparing our algorithm with the recent shotgun stochastic search algorithm of hans _ et al . _",
    "finally section [ discussion ] contains some concluding remarks .",
    "let @xmath4 be a sequence of @xmath1 observed responses and @xmath5 a vector of predictors for @xmath6 , @xmath7 , of dimension @xmath8 .",
    "moreover let @xmath9 be the @xmath10 design matrix with @xmath11th row @xmath12 .",
    "a gaussian linear model can be described by the equation @xmath13 where @xmath14 is an unknown constant , @xmath15 is a column vector of ones , @xmath16 is a @xmath8 vector of unknown parameters and @xmath17 .",
    "suppose one wants to model the relationship between @xmath18 and a subset of @xmath19 , but there is uncertainty about which subset to use .",
    "following the usual convention of only considering models that have the intercept @xmath14 , this problem , known as variable selection or subset selection , is particularly interesting when @xmath0 is large and parsimonious models containing only a few predictors are sought to gain interpretability . from a bayesian perspective",
    "the problem is tackled by placing a constant prior density on @xmath14 and a prior on @xmath20 which depends on a latent binary vector @xmath21 , where @xmath22 if @xmath23 and @xmath24 if @xmath25 , @xmath26 .",
    "the overall number of possible models defined through @xmath27 grows exponentially with @xmath0 and selecting the best model that predicts @xmath18 is equivalent to find one over the @xmath28 subsets that form the model space .",
    "given the latent variable @xmath29 , a gaussian linear model can therefore be written as @xmath30 where @xmath31 is the non - zero vector of coefficients extracted from @xmath20 , @xmath32 is the design matrix of dimension @xmath33 , @xmath34 , with columns corresponding to @xmath22 .",
    "we will assume that , apart from the intercept @xmath14 , @xmath35 contains no variables that would be included in every possible model and that the columns of the design matrix have all been centred with mean @xmath36 .",
    "it is recommended to treat the intercept separately and assign it a constant prior : @xmath37 , fernndez _ et al . _",
    "when coupled with the latent variable @xmath38 , the conjugate prior structure of @xmath39 follows a normal - inverse - gamma distribution @xmath40 @xmath41 with @xmath42 .",
    "some guidelines on how to fix the value of the hyperparameters @xmath43 and @xmath44 are provided in kohn _",
    "( 2001 ) , while the case @xmath45 corresponds to the jeffreys prior for the error variance , @xmath46 .",
    "taking into account ( [ t1 ] ) , ( [ t2 ] ) , ( [ t3 ] ) and the prior specification for @xmath14 , the joint distribution of all the variables ( based on further conditional independence conditions ) can be written as @xmath47    the main advantage of the conjugate structure ( [ t2 ] ) and ( [ t3 ] ) is the analytical tractability of the marginal likelihood whatever the specification of the prior covariance matrix @xmath48 : @xmath49 where @xmath50 , with @xmath51 , @xmath52 and @xmath53 ( brown _ et al .",
    "_ , 1998 ) .",
    "while the mean of the prior ( [ t2 ] ) is usually set equal to zero , @xmath54 , a neutral choice ( chipman _ et al .",
    "_ , 2001 ; clyde and george , 2004 ) , the specification of the prior covariance @xmath55 matrix leads to at least two different classes of priors :    * when @xmath56 , where @xmath3 is a scalar and @xmath57 , it replicates the covariance structure of the likelihood giving rise to so called @xmath3-priors first proposed by zellner ( 1986 ) . * when @xmath58 , but @xmath59 the components of @xmath31 are conditionally independent and the posterior covariance matrix is driven towards the independence case .",
    "we will adopt the notation @xmath60 as we want to cover both prior specification in a unified manner .",
    "thus in the @xmath3-prior case , @xmath61 while in the independent case , @xmath62",
    ". we will refer to @xmath63 as the _ variable selection coefficient _ for reasons that will become clear in the next section .    to complete the prior specification in ( [ t4 ] ) ,",
    "@xmath64 must be defined .",
    "a complete discussion about alternative priors on the model space can be found in chipman ( 1996 ) and chipman _ et al . _",
    "( 2001 ) . here",
    "we adopt the beta - binomial prior illustrated in kohn _",
    "( 2001 ) @xmath65 with @xmath66 , where the choice @xmath67 implicitly induces a binomial prior distribution over the model size and @xmath68 .",
    "the hypercoefficients @xmath69 and @xmath70 can be chosen once @xmath71 and @xmath72 have been elicited . in the large @xmath0 , small @xmath1  framework , to ensure sparse regression models where @xmath73 , it is recommended to centre the prior for the model size away from the number of observations .        it is a known fact that @xmath3-priors have two attractive properties .",
    "firstly they possess an automatic scaling feature ( chipman _ et al . _ , 2001 ; kohn _ et al .",
    "_ , 2001 ) .",
    "in contrast , for independent priors , the effect of @xmath74 on the posterior distribution depends on the relative scale of @xmath9 and standardisation of the design matrix to units of standard deviation is recommended .",
    "however , this is not always the best procedure when @xmath9 is possibly skewed , or when the columns of @xmath9 are not defined on a common scale of measurement .",
    "the second feature that makes @xmath3-priors particularly appealing is the rather simple structure of the marginal likelihood ( [ t5 ] ) with respect to the constant @xmath63 which becomes @xmath75 where , if @xmath54 , @xmath76 . for computational reasons explained in the next section",
    ", we assume that ( [ t6 ] ) is always defined : since we calculate @xmath77 using the qr - decomposition of the regression @xmath78 ( brown _ et al .",
    "_ , 1998 ) , when @xmath79 , @xmath80 .",
    "despite the simplicity of ( [ t6 ] ) , the choice of the constant @xmath63 for @xmath3-priors is complex , see fernndez _",
    "( 2001 ) , cui and george ( 2008 ) and liang _ et al . _",
    "( 2008 ) .",
    "historically the first attempt to build a comprehensive bayesian analysis placing a prior distribution on @xmath63 dates back to zellner and siow ( 1980 ) , where the data adaptivity of the degree of shrinkage adapts to different scenarios better than assuming standard fixed values .",
    "zellner - siow priors , z - s hereafter , can be thought as a mixture of @xmath3-priors and an inverse - gamma prior on @xmath63 , @xmath81 , leading to @xmath82 liang _ et al . _",
    "( 2008 ) analyse in details z - s priors pointing out a variety of theoretical properties . from a computational point of view , with z - s priors , the marginal likelihood @xmath83 is no more available in closed form , something which is advantageous in order to quickly perform a stochastic search ( chipman _ et al . _ , 2001 ) .",
    "even though z - s priors need no calibration and the laplace approximation can be derived ( tierney and kadane , 1986 ) , see appendix [ laplace_approx ] , never became as popular as @xmath3-priors with a suitable constant value for @xmath63 .",
    "for alternative priors , see also cui and george ( 2008 ) and liang _ et al . _",
    "( 2008 ) .",
    "when all the variables are defined on the same scale , independent priors represent an attractive alternative to @xmath3-priors .",
    "the likelihood marginalised over @xmath84 , @xmath31 and @xmath85 becomes @xmath86 where , if @xmath54 , @xmath87 .",
    "note that ( [ t13 ] ) is computationally more demanding than ( [ t6 ] ) due to the extra determinant operator .",
    "geweke ( 1996 ) suggests to fix a different value of @xmath88 , @xmath26 , based on the idea of substantially significant determinant  of @xmath89 with respect to @xmath90 .",
    "however it is common practice to standardise the predictor variables , taking @xmath91 in order to place appropriate prior mass on reasonable values of the regression coefficients ( hans _ et al .",
    "_ , 2007 ) .",
    "another approach , illustrated in bae and mallick ( 2004 ) , places a prior distribution on @xmath88 without standardising the predictors .",
    "regardless of the prior specification for @xmath63 , using the qr - decomposition on a suitable transformation of @xmath92 and @xmath93 , the marginal likelihood ( [ t13 ] ) is always defined .",
    "in this section we propose a new sampling algorithm that overcomes the known difficulties faced by mcmc schemes when attempting to sample a high dimension multimodal space .",
    "we discuss in a unified manner the general case where a hyperprior on the variable selection coefficient @xmath63 is specified .",
    "this encompasses the @xmath3-prior and independent prior structure as well as the case of fixed @xmath63 if a point mass prior is used .",
    "the multimodality of the model space is a known issue in variable selection and several ways to tackle this problem have been proposed in the past few years .",
    "liang and wong ( 2000 ) suggest an extension of parallel tempering called evolutionary monte carlo , emc hereafter , nott and green , n&g hereafter , ( 2004 ) introduce a sampling scheme inspired by the swendsen - wang algorithm while jasra _",
    "( 2007 ) extend emc methods to varying dimension algorithms .",
    "finally hans _ et al . _",
    "( 2007 ) propose when @xmath94 a new stochastic search algorithm , sss , to explore models that are in the same neighbourhood in order to quickly find the best combination of predictors .",
    "we propose to solve the issue related to the multimodality of model space ( and the dependence between @xmath29 and @xmath63 ) along the lines of emc , applying some suitable parallel tempering strategies directly on @xmath95 .",
    "the basic idea of parallel tempering , pt hereafter , is to weaken the dependence of a function from its parameters by adding an extra one called temperature .",
    "multiple markov chains , called population  of chains , are run in parallel , where a different temperature is attached to each chain , their state is tentatively swap at every sweep by a probabilistic mechanism and the latent binary vector @xmath29 of the non - heated chain is recorded .",
    "the different temperatures have the effect of flatting the likelihood .",
    "this ensures that the posterior distribution is not trapped in any local mode and that the algorithm mixes efficiently , since every chain constantly tries to transmit information about its state to the others .",
    "emc extents this idea , encompassing the positive features of pt and genetic algorithms inside a mcmc scheme .",
    "since @xmath20 and @xmath85 are integrated out , only two parameters need to be sampled , namely the latent binary vector and the variable selection coefficient . in this set - up",
    "the full conditionals to be considered are @xmath96 ^{1/t_{l}}\\propto \\left [ p\\left ( y\\left\\vert \\gamma _ { l},\\tau \\right .",
    "\\right ) \\right ] ^{1/t_{l}}\\left [ p\\left ( \\gamma _ { l}\\right ) \\right ] ^{1/t_{l } } \\label{t23bis}\\ ] ] @xmath97 ^{1/t_{l}}p\\left ( \\tau \\right ) , \\label{t24bis}\\ ] ] where @xmath98 is the number of chains in the the population and @xmath99 , @xmath100 , is the temperature attached to the @xmath101th chain while the population @xmath102 corresponds to a set of chains that are retained simultaneously .",
    "conditions for convergence of emc algorithms are well understood and illustrated for instance in jasra _",
    "( 2007 ) .    at each sweep of our algorithm ,",
    "first the population @xmath102 in ( [ t23bis ] ) is updated using a variety of moves inspired by genetic algorithms : local moves , the ordinary metropolis - hastings or gibbs update on every chain ; and global moves  that include : i ) selection of the chains to swap , based on some probabilistic measures of distance between them ; ii ) crossover operator , i.e. partial swap of the current state between different chains ; iii ) exchange operator , full state swap between chains .",
    "both local and global moves are important although global moves are crucial because they allow the algorithm to jump from one local mode to another . at the end of the update of @xmath102 , @xmath63",
    "is then sampled using ( [ t24bis ] ) .",
    "the implementation of emc that we propose in this paper includes several novel aspects : the use of a wide range of moves including two new ones , a local move , based on the fast scan metropolis - hastings sampler , particularly suitable when @xmath0 is large and a bold global move that exploits the pattern of correlation of the predictors .",
    "moreover , we developed an efficient scheme for tuning the temperature placement that capitalises the effective interchange between the chains .",
    "another new feature is to use a metropolis - within - gibbs with adaptive proposal for updating @xmath63 , as the full conditional ( [ t24bis ] ) is not available in closed form .      in what follows",
    ", we will only sketch the rationale behind all the moves that we found useful to implement and discuss further the benefits of the new specific moves in section [ real data examples ] .",
    "for the large @xmath0 , small @xmath1paradigm and complex predictor spaces , we believe that using a wide portfolio of moves is needed and offers better guarantee of mixing .    from a notational point of view , we will use the double indexing @xmath103 , @xmath104 and @xmath26 to denote the @xmath105th latent binary indicator in the @xmath101th chain .",
    "moreover we indicate by @xmath106 the vector of binary indicators that characterise the state of the @xmath101th chain of the population @xmath107 .      given @xmath63 , we first tried the simple mc@xmath108 idea of madigan and york ( 1995 ) , also used by brown _",
    "( 1998 ) where add / delete and swap moves are used to update the latent binary vector @xmath109 . for an",
    "add / delete move , one of the @xmath0 variables is selected at random and if the latent binary value is @xmath36 the proposed new value is @xmath110 or _",
    "vice versa_. however , when @xmath111 , where @xmath112 is the size of the current model for the @xmath101th chain , the number of sweeps required to select by chance a binary indicator with a value of @xmath110 follows a geometric distribution with probability @xmath113 which is much smaller than @xmath114 to select a binary indicator with a value of @xmath36 .",
    "hence , the algorithm spends most of the time trying to add rather than delete a variable . note that this problem also affects rj - type algorithms ( dellaportas _ et al .",
    "_ , 2002 ) .",
    "on the other hand , gibbs sampling ( george and mcculloch , g&mcc hereafter , 1993 ) is not affected by this issue since the state of the @xmath101th chain is updated by sampling from @xmath115 ^{1/t_{l}}\\propto \\exp \\left\\ { \\left ( \\log p\\left ( y\\left\\vert \\gamma _ { l , j}^{\\left ( 1\\right ) } , \\tau \\right .",
    "\\right ) + \\log p\\left ( \\gamma _ { l , j}=1\\left\\vert \\gamma _ { l , j^{-}}\\right .",
    "\\right ) \\right ) /t_{l}\\right\\ } ,   \\label{t34}\\ ] ] where @xmath116 indicates for the @xmath101th chain all the variables , but the @xmath105th , @xmath26 and + @xmath117 .",
    "the main problem related to gibbs sampling is the large number of models it evaluates if a full gibbs cycle or any permutation of the indices is implemented at each sweep .",
    "each model requires the direct evaluation , or at least the update , of the time consuming quantity @xmath118 , equation ( [ t6 ] ) or ( [ t13 ] ) , making practically impossible to rely solely on the gibbs sampler when @xmath0 is very large .",
    "however , as sharply noticed by kohn _",
    "( 2001 ) , it is wasteful to evaluate all the @xmath0 updates in a cycle because if @xmath112 is much smaller than @xmath0 and given @xmath119 , it is likely that the sampled value of @xmath103 is again @xmath36 .",
    "when @xmath0 is large , we thus consider instead of the standard mc@xmath108 add / delete , swap moves , a novel fast scan metropolis - hastings scheme , fsmh hereafter , specialised for emc / pt .",
    "it is computationally less demanding than a full gibbs sampling on all @xmath120 and do not suffer from the problem highlighted before for mc@xmath108 and rj - type algorithms when @xmath111 .",
    "the idea behind the fsmh move is to use an additional acceptance / rejection step ( which is very fast to evaluate ) to choose the number of indices where to perform the gibbs - like step : the novelty of our fsmh sampler is that the additional probability used in the acceptance / rejection step is based not only on the current chain model size @xmath121 , but also on the temperature @xmath122 attached to the @xmath101th chain .",
    "therefore the aim is to save computational time in the large @xmath0 set - up when multiple chains are simulated in parallel and finding an alternative scheme to a full gibbs sampler . to save computational time",
    "our strategy is to evaluate the time consuming marginal likelihood ( [ t5 ] ) in no more than approximately @xmath123 times per cycle in the @xmath101th chain ( assuming convergence is reached ) , where @xmath124 is the probability to select a variable to be added in the acceptance / rejection step which depends on the current model size @xmath121 and the temperature @xmath122 and similarly for @xmath125 ( @xmath126 indicates the integer part ) . since for chains attached to lower temperatures @xmath127 , the algorithm proposes to update _ almost all _ binary indicators with value @xmath110 , while it selects at random a group of approximately @xmath128 binary indicators with value 0 to be updated . at higher temperatures since @xmath129 and",
    "@xmath130 become more similar , the number of models evaluated in a cycle increases because much more binary indicators with value @xmath36 are updated .",
    "full details of the fsmh scheme is given in the appendix [ fsmh scheme ] , while evaluation of them and comparison with mc@xmath108 embedded in emc are presented in sections [ real data examples ] and [ simulation study ]      the first step of this move consists of selecting the pair of chains @xmath131 to be operated on .",
    "we firstly compute a probability equal to the weight of the boltzmann probability , @xmath132 , where @xmath133 is the log transformation of the full conditional ( [ t23bis ] ) assuming @xmath134 @xmath135 , @xmath104 , and @xmath136 for some specific temperature @xmath137 , and then rank all the chains according to this .",
    "we use normalised boltzmann weights to increase the chance that the two selected chains will give rise , after the crossover , to a new configuration of the population with higher posterior probability .",
    "we refer to this first step as selection operator .",
    "suppose that two new latent binary vectors are then generated from the selected chains according to some crossover operator described below .",
    "the new proposed population of chains + @xmath138 is accepted with probability @xmath139 where @xmath140 is the proposal probability , see liang and wong ( 2000 ) .    in the following",
    "we will assume that four different crossover operators are selected at random at every emc sweep : @xmath110-point crossover , uniform crossover , adaptive crossover ( liang and wong , 2000 ) and a novel block crossover .",
    "of these four moves , the uniform crossover which shuffles  the binary indicators along all the selected chains is expected to have a low acceptance , but to be able to genuinely traverse regions of low posterior probability .",
    "the block crossover essentially tries to swap a group of variables that are highly correlated and can be seen as a multi - points crossover whose crossover points are not random but defined from the correlation structure of the covariates . in practice",
    "the block crossover is defined as follows : one variable is selected at random with probability @xmath141 , then the pairwise correlation @xmath142 between the @xmath105th selected predictor and each of the remaining covariates , @xmath143 , @xmath144 , is calculated .",
    "we then retain for the block crossover all the covariates with positive ( negative ) pairwise correlation with @xmath145 such that @xmath146 .",
    "the threshold @xmath147 is chosen with consideration to the specific problem , but we fixed it at @xmath148 .",
    "evaluation of block crossover and comparisons with other crossover operators are presented on a real data example in section [ real data examples ] .",
    "the exchange operator can be seen as an extreme case of crossover operator , where the first proposed chain receives the whole second chain state @xmath149 , and _",
    "vice versa_. in order to achieve a good acceptance rate , the exchange operator is usually applied on adjacent chains in the temperature ladder , which limits its capacity for mixing . to obtain better mixing",
    ", we implemented two different approaches : the first one is based on jasra _ et al . _",
    "( 2007 ) and the related idea of delayed rejection ( green and mira , 2001 ) ; the second , a bolder all - exchange  move , is based on a precalculation of all the @xmath150 exchange acceptance rates between all chains pairs ( calvo , 2005 ) .",
    "full relevant details are presented in appendix [ exchange ] .",
    "both of these bold moves perform well in the real data applications , see section [ real data examples ] , and simulated examples , see section [ simulation study ] , thus contributing to the efficiency of the algorithm .",
    "as noted by goswami and liu ( 2007 ) , the placement of the temperature ladder is the most important ingredient in population based mcmc methods .",
    "we propose a procedure for the temperature placement which has the advantage of simplicity while preserving good accuracy .",
    "first of all , we fix the size @xmath98 of the population . in doing this ,",
    "we are guided by several considerations : the complexity of the problem , i.e. @xmath151 , the size of the data and computational limits .",
    "we have experimented and we recommend to fix @xmath152 . even though some of the simulated examples had @xmath153 ( section [ simulation study ] ) , we found that @xmath154 was sufficient to obtain good results . in our real data examples ( section [ real data examples ] ) , we used @xmath155 guided by some prior knowledge on @xmath151 .",
    "secondly , we fix at an initial stage , a temperature ladder according to a geometric scale such that @xmath156 , @xmath157 , @xmath104 with @xmath158 relatively large , for instance @xmath159 .",
    "to subsequently tune the temperature ladder , we then adopt a strategy based on monitoring only the acceptance rate of the delayed rejection exchange operator towards a target of @xmath160 .",
    "details of the implementation are left in appendix [ temperature ]      various strategies can be used to avoid having to sample from the posterior distribution of the variable selection coefficient @xmath63 .",
    "the easiest way is to integrate it out through a laplace approximation ( tierney and kadane , 1986 ) or using a numerical integration such as quadrature on an infinite interval .",
    "we do not pursue these strategies and the reasons can be summarised as follows . integrating out @xmath63 in the population",
    "implicitly assumes that every chain has its own value of the variable selection coefficient @xmath161 ( and of the latent binary vector @xmath162 ) . in this set - up",
    ", two unpleasant situations can arise : firstly , if a laplace approximation is applied , _ equilibrium _ in the product space is difficult to reach because the posterior distribution of @xmath163 depends , through the marginal likelihood obtained using the laplace approximation , on the _ chain specific value _ of the posterior mode for @xmath161 , @xmath164 ( details in appendix [ laplace_approx ] ) .",
    "since the strength of @xmath165 to predict the response is weakened for chains attached to high temperatures , it turns out that for these chains , @xmath164 is likely to be close to zero .",
    "when the variable selection coefficient is very small , the marginal likelihood dependence on @xmath165 decreases even further , see for instance ( [ t6 ] ) , and chains attached to high temperatures will experience a very unstable behaviour , making the convergence in the product space hard to reach .",
    "in addition , if an automatic tuning of temperature ladder is applied , chains will increasingly be placed at a closer distance in the temperature ladder to balance the low acceptance rate of the global moves , negating the purpose of emc .    in this paper the convergence",
    "is reached instead in the product space @xmath166 ^{1/t_{l}}p\\left ( \\tau \\right ) $ ] , i.e. the whole population is conditioned on a value of @xmath167 _ common to all chains_. this strategy will alleviate the problems highlighted before allowing for faster convergence and better mixing among the chains .",
    "the procedure just described comes with an extra cost , i.e. sampling the value of @xmath63 .",
    "however , this step is inexpensive in relation to the cost required to sample @xmath162 , @xmath104 .",
    "there are several strategies that can be used to sample @xmath63 from ( [ t24bis ] ) .",
    "we found useful to apply the idea of adaptive metropolis - within - gibbs described in roberts and rosenthal ( 2008 ) .",
    "conditions for the asymptotic convergence and ergodicity are guaranteed as we enforce the _ diminishing adaptive condition _ , i.e. the transition kernel stabilises as the number of sweeps goes to infinity and the _ bounded convergence condition _ , i.e. the convergence time of the kernel is bounded in probability . in our",
    "set - up using an adaptive proposal to sample @xmath63 has several benefits ; amongst others it avoids the known problems faced by the gibbs sampler when the prior is proper , but relatively flat ( natarajan and mcculloch , 1998 ) as can happen for z - s priors when @xmath1 is large or for the independent case considered by bae and mallick ( 2004 ) .",
    "moreover , given an upper limit on the number of sweeps , the adaptation guarantees a better exploration of the tails of @xmath168 than with a fixed proposal . for details of the implementation and discussion of conditions for convergence , see appendix [ adaptive_cond ] .      in the following ,",
    "we refer to our proposed algorithm , evolutionary stochastic search as ess .",
    "if @xmath3-priors are chosen the algorithm is denoted as ess@xmath3 , while we use ess@xmath11 if independent priors are selected ( the same notation is used when @xmath63 is fixed or given a prior distribution ) . without loss of generality , we assume that the response vector and the design matrix have both been centred and , in the case of independent priors , that the design matrix is also rescaled . based on the two full conditionals ( [ t23bis ] ) and ( [ t24bis ] ) and the local and global moves introduced earlier , our ess algorithm can be summarised as follows .",
    "* given @xmath63 , sample the population s states @xmath102 from the two steps : 1 .",
    "with probability @xmath160 perform local move and with probability @xmath160 apply at random one of the four crossover operators : @xmath110-point , uniform , block and adaptive crossover .",
    "if local move is selected , use fsmh sampling scheme independently for each chain ( see appendix [ fsmh scheme ] ) .",
    "moreover every @xmath169 sweeps apply on the first chain a complete scan by a gibbs sampler .",
    "2 .   perform the delayed rejection exchange operator or the all - exchange operator with equal probability . during the burn - in , only select the delayed rejection exchange operator .",
    "* when @xmath63 is not fixed but has a prior @xmath170 , given the latent binary configuration @xmath171 , sample @xmath63 from an adaptive metropolis - within - gibbs sampling ( section [ amhwg sampler ] ) .    from a computational point of view",
    ", we used the same fast form for updating @xmath118 as brown _ et al . _",
    "( 1998 ) , based on the qr - decomposition . besides its numerical benefits , qr- decomposition can deal with the case @xmath172 .",
    "this avoids having to restrict the search to models with @xmath173 , and helps mixing during the burn - in phase .",
    "the first real data example is an application of linear regression to investigate genetic regulation . to discover the genetic causes of variation in the expression ( i.e. transcription ) of genes ,",
    "gene expression data are treated as a quantitative phenotype while genotype data ( snps ) are used as predictors , a type of analysis known as expression quantitative trait loci ( eqtl ) .",
    "here we focus on the ability of ess to find a parsimonious set of predictors in an animal data set ( hubner _ et al .",
    "_ , 2005 ) , where the number of observations , @xmath174 , is small with respect to the number of covariates @xmath175 .",
    "this situation , where @xmath176 , is quite common in animal experiments since environmental sources of variation are controlled as well as the biological diversity of the sample . for illustration , we report the analysis of one gene expression response , where we apply ess@xmath3 with and without the hyperprior on @xmath63 , see table [ table_t1] eqtl . in the former case , thanks to the adaptive proposal , the markov chain for @xmath63 mixes very well reaching an overall acceptance rate which is close to the target value @xmath177 .",
    "convergence issue is not a problem since the trace of the proposal s standard deviation stabilises quickly and well inside the bounded conditions , see figure [ fig_s1 ] .    in both cases a good mixing among the @xmath155 chains",
    "is obtained ( figure [ fig_t1 ] , top panels , ess@xmath3 with @xmath178 ) .",
    "although in the case depicted in figure [ fig_t1 ] with fixed @xmath63 , the convergence is reached in the product space @xmath179 ^{1/t_{l}}$ ] , by visual inspection we see that each chain _ marginally _ reaches its _ equilibrium _ with respect to the others ; moreover , thanks to the automatic tuning of the temperature placement during the burn - in , the distributions of the chains log posterior probabilities overlap nicely , allowing effective exchange of information between the chains .",
    "table [ table_t1]eqtl , confirms that the automatic temperature selection works well ( with and without the hyperprior on @xmath63 ) reaching an acceptance rate for the monitored exchange ( delayed rejection ) operator close to the selected target of @xmath180 . the all - exchange operator shows a higher acceptance rate , while , in contrast to jasra _",
    "( 2007 ) , the overall crossover acceptance rate is reasonable high : in our experience the good performance of the crossover operator is both related to the selection operator ( section [ emc sampler ] ) and the new block crossover which shows an acceptance rate far higher than the others .",
    "finally the computational time on the same desktop computer ( see details in appendix [ performance_comparison ] ) is rather similar with or without the hyperprior @xmath63 , @xmath181 and @xmath182 minutes respectively for @xmath183 sweeps with @xmath184 as burn - in .",
    "the main difference among the two implementations of ess@xmath3 is related to the posterior model size : when @xmath63 is fixed at @xmath185 ( unit information prior , fernndez _ et al .",
    "_ , 2001 ) , there is more uncertainty and support for larger models , see figure [ fig_t2 ] ( a ) . in both cases we fix @xmath186 and @xmath187 , following prior biological knowledge on the genetic regulation .",
    "the posterior mean of the variable selection coefficient is a little smaller than the unit information prior , with ess@xmath3 coupled with the z - s prior favouring smaller models than when @xmath63 is set equal to @xmath188 .",
    "the best model visited ( and the corresponding @xmath189 ) is the same for both version of ess@xmath3 , while , when a hyperprior on @xmath63 is implemented , the stability index  which indicates how much the algorithm persists on the first chain top @xmath190 ( not unique ) visited models ranked by the posterior probability ( appendix [ performance_comparison ] ) , shows a higher stability , see table [ table_t1] eqtl . in this case , having a data - driven level of shrinkage helps the search algorithm to better discriminate among competing models .",
    "our second example is related to the application of model ( [ t1 ] ) in another genomics example : @xmath2 snps , selected genome - wide from a candidate gene study , are used to predict the variation of mass spectography metabolomics data in a small human population , an example of a so - called mqtl experiment",
    ". a suitable dimension reduction of the data is performed to divide the spectra in regions or bins and @xmath191-transformation is applied in order to normalise the signal .",
    "we present the key findings related to a particular metabolite bin , but the same comments can be extended to the analysis of the whole data set , where we regressed every metabolites bin _ versus _ the genotype data ( @xmath192 and @xmath193 ) . in this very challenging case , we still found an efficient mixing of the chains ( see table [ table_t1]mqtl ) .",
    "note that in this case the posterior mean of @xmath63 , @xmath194 , is a little larger than the unit information prior , @xmath195 , although the influence of the hyperprior is less important than in the previous real data example , see figure [ fig_t2 ] ( b ) .",
    "in both examples , the posterior model size favours clearly polygenic control with significant support for up to four genetic control points ( figure [ fig_t2 ] ) highlighting the advantage of performing multivariate analysis in genomics rather than the traditional univariate analysis .    as expected in view of the very large number of predictors ,",
    "in the mqtl example the computational time is quite large , around @xmath196 hours for @xmath197 sweeps after a burn - in of @xmath184 , but as shown in table [ table_t1 ] by the stability index  ( @xmath198 ) , we believe that the number of iterations chosen exceeds what is required in order to visit faithfully the model space . for such large data analysis tasks , parallelisation of the code could provide big gains of computer time and would be ideally suited to our multiple chains approach .    [ table [ table_t1 ] about here  figure [ fig_t1 ] about here  figure [ fig_t2 ] about here  figure [ fig_s1 ] about here ]    we also evaluate the superiority of our ess algorithm , and in particular the fsmh scheme and the block crossover , with respect to more traditional emc implementations illustrated for instance in liang and wong ( 2000 ) . albeit we believe that using a wide portfolio of different moves enables any searching algorithm to better explore complicated model spaces , we reanalysed the first real data example , eqtl analysis , comparing : ( i ) ess@xmath3 with only fsmh as local move _ vs _ ess with only mc@xmath108 as local move ; ( ii ) ess@xmath3 with only block crossover _ vs _ ess@xmath3 with only 1-point , only uniform and only adaptive crossover respectively . to avoid dependency of the results on the initialisation of the algorithm",
    ", we replicated the analysis @xmath199 times .",
    "moreover , to make the comparison fair , in experiment ( i ) we run the two versions of ess@xmath3 for a different number of sweeps ( @xmath183 and @xmath200 with @xmath184 and @xmath201 as burn - in respectively ) , but matching the number of models evaluated .",
    "results are presented in table [ table_s1 ] .",
    "we report here the main findings :    1 .   over the @xmath199 runs",
    ", ess@xmath3 with fsmh reaches the same top visited model @xmath202% ( 17/25 ) while ess@xmath3 with mc@xmath108 the same top model only @xmath181% , with a fixed @xmath63 , and @xmath203% and @xmath204% respectively with z - s prior .",
    "this ability is extended to the top models ranked by the posterior probability , data not shown , providing indirect evidence that the proposed new move helps the algorithm to increase its predictive power .",
    "the great superiority when fsmh scheme are implemented can be explained by comparing subplot ( a ) and ( c ) in figure [ fig_t1 ] : the exchange of information between chains for ess@xmath3 with mc@xmath108 as local move when @xmath94 ( and @xmath205 ) is rather poor , negating the purpose of emc .",
    "ess@xmath3 with mc@xmath108 has more difficulties to reach convergence in the product space and , in contrast to ess@xmath3 with fsmh , the retained chain does not easily escape from local modes .",
    "this later point can be seen looking at figure [ fig_t1 ] ( d ) which magnifies the right hand tail of the kernel density of @xmath206 for the recorded chain , pulling together the @xmath199 runs : interestingly ess@xmath3 with fsmh is less bumpy , showing a better ability to escape from local modes and to explore more efficiently the right tail .",
    "2 .   regarding the second comparison when",
    "@xmath63 is fixed , ess@xmath3 with only block crossover beats constantly the other crossover operators , with @xmath207% _ vs _ about @xmath208% , in terms of best model visited ( table [ table_s1 ] ) and models with higher posterior probability ( data not shown ) , has higher acceptance rate ( table [ table_s2 ] ) , showing also a great capacity to accumulate posterior mass as illustrated in figure [ fig_s2 ] .",
    "the specific benefit of the block crossover is less pronounced when a prior on @xmath63 is specified , but we have already noticed that in this case having a hyperprior on @xmath63 greatly improves the efficiency of the search .",
    "[ table [ table_s1 ] about here  table [ table_s2 ] about here  figure [ fig_s2 ] about here ]      we briefly report on a comprehensive study of the performance of ess in a variety of simulated examples as well as a comparison with sss . to make comparison with sss fair , we use ess@xmath209 , the version of our algorithm which assumes independent priors , @xmath210,with @xmath63 fixed at @xmath110 .",
    "details of the simulated examples ( 6 set - ups ) and how we conducted the simulation experiment ( 25 replication of each set - up ) are given in appendix [ performance_appendix ] .",
    "the rationale behind the construction of the examples was to benchmark our algorithm against both @xmath211 and @xmath212 cases , to use as building blocks intricate correlation structures that had been used in previous comparisons by g&mcc ( 1993 , 1997 ) and n&g ( 2004 ) , as well as a realistic correlation structure derived from genetic data , and to include elements of model uncertainty in some of the examples by using a range of values of regression coefficients .",
    "in our example we observe an effective exchange of information between the chains ( reported in table [ table_s3 ] ) which shows good overall acceptance rates for the collection of moves that we have implemented .",
    "the dimension of the problem does not seem to affect the acceptance rates in table [ table_s3 ] , remarkably since values of @xmath0 range from @xmath208 to @xmath190 between the examples .",
    "we also studied specifically the performance of the global moves ( table [ table_s4 ] ) to scrutinise our temperature tuning and confirmed the good performance of ess@xmath11 with good frequencies of swapping ( not far from the case where adjacent chains are selected to swap at random with equal probability ) and good measures of overlap between chains .",
    "all the examples were run in parallel with ess@xmath209 and sss 2.0 ( hans _ et al .",
    "_ , 2007 ) for the same number of sweeps ( 22,000 ) and matching hyperparameters on the model size .",
    "comparison were made with respect to the marginal probability of inclusion as well as the ability to reach models with high posterior probability and to persist in this region . for a detailed discussion of all comparison , see appendix [ performance_comparison ] .",
    "overall the covariates with non - zero effects have high marginal posterior probability of inclusion for ess@xmath11 in all the examples , see figure [ fig_s4 ] .",
    "there is good agreement between the two algorithms in general , with additional evidence on some examples ( figure [ fig_s4 ] ( c ) and ( d ) ) that ess@xmath11 is able to explore more fully the model space and in particular to find small effects , leading to a posterior model size that is close to the true one",
    ". measures of goodness of fit and stability , table [ table_s5 ] , are in good agreement between ess@xmath11 and sss .",
    "the comparison highlight that a key feature of sss , its ability to move quickly towards the right model and to persist on it , is accompanied by a drawback in having difficulty to explore far apart models with competing explanatory power , in contrast to ess@xmath11 ( contaminated example set - up ) .",
    "altogether ess@xmath11 shows a small improvement of @xmath213 , related to its ability to pick up some of the small effects that are missed by sss .",
    "finally ess@xmath11 shows a remarkable superiority in terms of computational time , especially when the simulated ( and estimated ) @xmath214 is large . altogether",
    "our comparisons show that we have designed a fully bayesian mcmc - emc sampler which is competitive with the effective search provided by sss@xmath11 .    in the same spirit of the real data example analysis",
    ", we also evaluate the superiority of the fsmh scheme with respect to more traditional emc implementations , i.e when a mc@xmath108 local move is selected . while both versions of the search algorithm visit almost the same top models ranked by the posterior probability , ess persists more on the top models .    [ table [ table_s3 ] about here  table [ table_s4 ] about here  table [ table_s5 ] about here + figure [ fig_s3 ] about here  figure [ fig_s4 ] about here ]",
    "the key idea in constructing an effective mcmc sampler for @xmath27 and @xmath63 is to add an extra parameter , the temperature , that weakens the likelihood contribution and enables escaping from local modes .",
    "running parallel chains at different temperature is , on the other hand , expensive and the added computational cost has to be balanced against the gains arising from the various exchanges  between the chains .",
    "this is why we focussed on developing a good strategy for selecting the pairs of chains , using both marginal and joint information between the chains , attempting bold and more conservative exchanges .",
    "combining this with an automatic choice of the temperature ladder during burn - in is one of the key element of our ess algorithm . using pt in this way has the potential to be effective in a wide range of situations where the posterior space is multimodal .",
    "to tackle the case where @xmath0 is large with respect to @xmath215 , the second important element in our algorithm is the use of a metropolised gibbs sampling - like step performed on a subset of indices in the local updating of the latent binary vector , rather than an mc@xmath108 or rj - like updating move .",
    "the new fast scan metropolis hastings sampler that we propose to perform these local moves achieves an effective compromise between full gibbs sampling that is not feasible at every sweep when @xmath0 is large and vanilla add / delete moves .",
    "comparison of fsmh _ vs _ mc@xmath108 scheme on a real data example and simulation study shows the superiority of our new local move .    when a model with a prior on the variable selection coefficient @xmath63 is preferred , the updating of @xmath63 itself present no particular difficulties and is computationally inexpensive .",
    "moreover , using an adaptive sampler makes the algorithm self contained without any time consuming tuning of the proposal variance .",
    "this latter strategy works perfectly well both in the @xmath3-prior and independent prior case as illustrated in sections [ real data examples ] and [ simulation study ] .",
    "our current implementation does not make use of the output of the heated chains for posterior inference .",
    "whether gains in variance reduction could be achieved in the spirit of gramacy _ et al . _",
    "( 2007 ) is an area for further exploration , which is beyond the scope of the present work .",
    "our approach has been applied so far to linear regression with univariate response @xmath18 .",
    "an interesting generalisation is that of a multidimensional @xmath216 response @xmath217 and the identification of regressors that jointly predict the @xmath217 ( brown _ et al . _ , 1998 ) .",
    "much of our set - up and algorithm carries through without difficulties and we have already implemented our algorithm in this framework in a challenging case study in genomics with multidimensional outcomes .",
    "the authors are thankful to norbert hubner and timothy aitman for providing the data of the eqtl example , gareth roberts and jeffrey rosenthal for helpful discussions about adaptation and michail papathomas for his detailed comments .",
    "sylvia richardson acknowledges support from the mrc grant go.600609 .",
    "in this section we will describe some technical details omitted from the paper and related to the sampling schemes we used for the population of binary latent vectors @xmath218 and the selection coefficient @xmath63 .",
    "let @xmath103 , @xmath104 and @xmath26 to denote the @xmath105th latent binary indicator in the @xmath101th chain .",
    "as in kohn _",
    "( 2001 ) , let @xmath219 and + @xmath220 .",
    "furthermore let @xmath221 and @xmath222 and finally @xmath223 and @xmath224 . from ( [ t7 ] )",
    "it is easy to prove that @xmath225 where @xmath121 is the current model size for the @xmath101th chain . using the above equation , for @xmath226 the normalised version of ( [ t34 ] )",
    "can be written as @xmath115 ^{1/t_{l}}=\\frac{\\left . \\theta",
    "_ { l , j}^{\\left ( 1\\right ) } \\right . ^{1/t_{l}}\\left .",
    "l_{l , j}^{\\left ( 1\\right ) } \\right .",
    "^{1/t_{l}}}{s\\left ( 1/t_{l}\\right ) } , \\label{al2}\\ ] ] where @xmath227 with @xmath228 ^{1/t_{l}}$ ] defined similarly . hence if @xmath229 is very small , then @xmath230 ^{1/t_{l}}$ ] is small as well .",
    "therefore for the gibbs sampler with a beta - binomial prior on the model space , the posterior probability of @xmath226 depends crucially on @xmath231 .    in the following we derive a fast scan metropolis - hastings scheme specialised for evolutionary monte carlo or parallel tempering .",
    "we define @xmath232 as the proposal probability to go from @xmath110 to @xmath36 and @xmath233 the proposal probability to go from @xmath36 to @xmath110 for the @xmath105th variable and @xmath101th chain . moreover using the notation introduced before",
    ", the metropolis - within - gibbs version of ( [ t34 ] ) to go from @xmath36 to @xmath110 in the emc local move is @xmath234 with a similar expression for @xmath235 .",
    "the proof of the propositions are omitted since they are easy to check .",
    "we first introduce the following proposition which is useful for the calculation of the acceptance probability in the fsmh scheme .",
    "[ prop1 ] the following three conditions are equivalent : a ) @xmath236 ; + b ) @xmath237 ; c)@xmath238 , where @xmath239 is the convex combination of the marginal likelihood @xmath240 and @xmath241 with weights @xmath242 and @xmath243 .",
    "the fsmh scheme can be seen as a random scan metropolis - within - gibbs algorithm where the number of evaluations is linked to the prior / current model size and the temperature attached to the chain .",
    "the computation requirement for the additional acceptance / rejection step is very modest since the normalised tempered version of ( [ al1 ] ) is used .",
    "[ prop3 ] let @xmath104 , @xmath26 ( or any permutation of them ) , @xmath244 and @xmath245 with @xmath243 .",
    "the acceptance probabilities are @xmath246 @xmath247    the above sampling scheme works as follows .",
    "given the @xmath101th chain , if @xmath248 ( and similarly for @xmath249 ) , it proposes the new value from a bernoulli distribution with probability @xmath250 : if the proposed value is different from the current one , it evaluates ( [ al6 ] ) ( and similarly [ al7])otherwise it selects a new covariate .",
    "finally it can be proved that the gibbs sampler is more efficient than the fsmh scheme , i.e. for a fixed number of iterations , gibbs sampling mcmc standard error is lower than for fsmh sampler .",
    "however the gibbs sampler is computationally more expensive so that , if @xmath0 is very large , as described in kohn _ et al . _ ( 2001 ) , fsmh scheme becomes more efficient per floating point operation .",
    "the exchange operator can be seen as an extreme case of crossover operator , where the first proposed chain receives the whole second chain state @xmath149 , and the second proposed chain receives the whole first state chain @xmath251 , respectively .    in order to achieve a good acceptance rate ,",
    "the exchange operator is usually applied on adjacent chains in the temperature ladder , which limits its capacity for mixing . to obtain better mixing",
    ", we implemented two different approaches : the first one is based on jasra _ et al . _",
    "( 2007 ) and the related idea of delayed rejection ( green and mira , 2001 ) ; the second one on gibbs distribution over all possible chains pairs ( calvo , 2005 ) .    1",
    ".   the delayed rejection exchange operator tries first to swap the state of the chains that are usually far apart in the temperature ladder , but , once the proposed move has been rejected , it performs a more traditional ( uniform ) adjacent pair selection , increasing the overall mixing between chains on one hand without drastically reducing the acceptance rate on the other .",
    "however its flexibility comes at some extra computational costs and in particular the additional evaluation of the pseudo move necessary to maintain detailed balance ( green and mira , 2001 ) .",
    "details are reported below .",
    "+ suppose two chains are selected at random , @xmath101 and @xmath252 with @xmath253 , in order to swap their binary latent vector .",
    "then , given that @xmath149 , @xmath254 and @xmath255 , ( 13 ) reduces to @xmath256 since the two chains are selected at random , the above acceptance probability decreases exponentially with the difference @xmath257 and therefore most of the proposed moves are rejected .",
    "if rejected , a delayed rejection - type move is applied between two random adjacent chains , with @xmath101 the first one and @xmath258 , @xmath259 , the second one , giving rise to the new acceptance probability @xmath260 where the pseudo move @xmath261 is necessary in order to maintain the detailed balance condition ( green and mira , 2001 ) .",
    "2 .   alternatively",
    ", we attempt a bolder all - exchange  operator . swapping the state of two chains that are far apart in the temperature ladder speeds up the convergence of the simulation since it replaces several adjacent swaps with a single move .",
    "however , this move can be seen as a rare event whose acceptance probability is low and unknown .",
    "since the full set of possible exchange moves is finite and discrete , it is easy and computationally inexpensive to calculate all the @xmath262 exchange acceptance rates between all chains pairs , inclusive the rare ones , @xmath263 . to maintain detailed balance condition",
    ", the possibility not to perform any exchange ( rejection ) must be added with unnormalised probability one .",
    "finally the chains whose states are swopped are selected at random with probability equal to @xmath264 where in ( [ t32 ] ) each pair @xmath265 is denoted by a single number @xmath266 , @xmath267 , including the rejection move , @xmath268 .",
    "first we select the number @xmath98 of chains close to the complexity of the problem , i.e. @xmath151 , although the size of the data and computational limits need to be taken into account .",
    "secondly , we fix a first stage temperature ladder according to a geometric scale such that @xmath156 , @xmath157 , @xmath104 with @xmath158 relatively large , for instance @xmath159 .",
    "finally , we adopt a strategy similar to the one described in roberts and rosenthal ( 2008 ) , but _ restricted to the burn - in stage _ , monitoring only the acceptance rate of the delayed rejection exchange operator . after the @xmath269th batch  of emc sweeps , to be chosen but usually set equal to @xmath169 , we update @xmath270 , the value of the constant @xmath158 up to the @xmath269th batch , by adding or subtracting an amount @xmath271 such that the acceptance rate of the delayed rejection exchange operator is as close as possible to @xmath180 ( liu , 2001 ; jasra _ et al .",
    "_ , 2007 ) , @xmath272 .",
    "specifically the value of @xmath273 is chosen such that at the end of the burn - in period the value of @xmath158 can be 1 . to be precise",
    ", we fix the value of @xmath271 as @xmath274 , where @xmath275 is the first value assigned to the geometric ratio and @xmath276 is the total number of batches in the burn - in period .        under model ( 1 ) and",
    "prior specification for @xmath14 , ( 2 ) and ( 3 ) , we provide the laplace approximation of @xmath95 for the @xmath3-prior case , while the approximation for the independent case can be derived following the same line of reasoning . for easy of notation",
    "we drop the chain subscript index and we assume that the observed responses @xmath18 have been centred with mean @xmath36 , i.e. @xmath277 . in the following",
    "we will distinguish the cases in which the posterior mode @xmath278 is a solution of a cubic or quadratic equation .",
    "conditions on the existence of the solutions are provided as well as those that guarantee the positive semidefiniteness of the variance approximation . recall that @xmath279 where @xmath280 is the posterior mode after the transformation @xmath281 , which is necessary to avoid problems on the boundary , @xmath282 is the approximate squared root of the variance calculated in @xmath280 and @xmath283 is the jacobian of the transformation .",
    "details about laplace approximation can be found in tierney and kadane ( 1986 ) .",
    "similar derivations when @xmath284 are presented in liang _",
    "finally throughout the presentation we will assume that @xmath285 and that @xmath286 and @xmath287 are fixed small as in kohn _",
    "( 2001 ) .",
    "+ if @xmath288 the posterior @xmath280 mode is the only positive root of the integrand function @xmath289 \\right\\ } ^{-\\left ( 2a_{\\sigma } + n-1\\right)/2 } \\frac{e^{-b_{\\tau}/e^{\\lambda } } } { \\left ( e^{\\lambda } \\right ) ^{a_{\\tau}+1}}e^{\\lambda } , \\ ] ] where the last factor in the above equation @xmath290 is the jacobian of the transformation . after the calculus of the first derivative of the log transformation and some algebra manipulations",
    ", it can be shown that @xmath291 is the solution of the cubic equation @xmath292 and that @xmath293 _ { \\lambda = \\hat{\\lambda}}^{-1 } , \\label{al2_bis}\\end{aligned}\\ ] ] where @xmath294 , @xmath295 , @xmath296 and @xmath297 . following liang _",
    "( 2008 ) , since @xmath298 , because @xmath299 , and @xmath300 , because @xmath301 , at least one real positive solution exists .",
    "moreover since @xmath302 , the remaining two real solutions should have the same sign ( abramowitz and stegun , 1970 ) .",
    "a necessary condition for the existence of just one real positive solution is that the summation of all the pairs - products of the coefficients is negative @xmath303 and this happens if @xmath304 .",
    "when @xmath305 and thus @xmath306 , the above condition corresponds to @xmath307 and when @xmath308 , as @xmath309 especially when @xmath310 is large , which might be expected when @xmath1 becomes large , the condition is equivalent to @xmath311 .",
    "therefore it turns out that a sufficient condition for the existence of just one real positive solution in ( [ al1 ] ) is @xmath311 .",
    "the positive semidefiniteness of the approximate variance can be proved as follows .",
    "first of all it is worth noticing that all the terms in ( [ al2_bis ] ) are of the same order @xmath312",
    ". then , when @xmath305 , the positive semidefiniteness is always guaranteed , while when @xmath308 , provided that @xmath310 is large , the middle term in ( [ al2_bis ] ) tends to zero and the condition is fulfilled if @xmath313 .     + if @xmath314 , with @xmath315 , @xmath291 is only the positive root of the integrand function @xmath316 \\right\\ } ^{-\\left ( 2a_{\\sigma } + n-1\\right ) /2}e^{\\lambda } \\ ] ] or , after the first derivative of the log transformation , the solution of the quadratic equation @xmath317 with @xmath318 /2 $ ] and @xmath319 , @xmath320 and @xmath321 defined as above .",
    "the discriminant of the quadratic equation is @xmath322 which is always greater than zero and therefore two real roots exist .",
    "since one of them is positive in order to prove that ( [ al3_bis ] ) admits just one positive solution , it is necessary to show that @xmath323 which is true provided that @xmath324 .",
    "moreover the approximate variance can be written as @xmath325 _ { \\lambda = \\hat{\\lambda}}^{-1 } \\label{al4_bis}\\ ] ] which is positive semidefinite when @xmath305 if @xmath326 , which is always verified , while , if @xmath308 and @xmath310 is large , equation ( [ al4_bis ] ) is not positive unless @xmath327 .",
    "the explicit solution of the posterior mode is also available @xmath328 /\\left [",
    "2a_{\\sigma } + n-1-\\left ( p_{\\gamma } + 2c_{\\tau}\\right ) \\right ] } -1,0\\right\\ } \\label{al13}\\end{aligned}\\ ] ] which corresponds to mle if @xmath329 .",
    "since @xmath63 is defined on the real positive axis we propose the new value of @xmath63 on the logarithm scale .",
    "in particular we use as proposal the normal distribution centred at the current value of @xmath330 in the @xmath3-prior and independent prior case .",
    "the variance of the proposal distribution is controlled as illustrated in roberts and rosenthal ( 2008 ) : every @xmath169 emc sweeps , the same value of sweeps used in the temperature placement , we monitor the acceptance rate of the metropolis - within - gibbs algorithm : if it is lower ( higher ) than the optimal acceptance rate , i.e. 0.44 , a constant @xmath331 is added ( subtracted ) to @xmath332 , the log standard deviation of the proposal distribution in the @xmath269th batch of emc sweeps .",
    "the value of the constant to be added or subtracted is rather arbitrary , but we found useful to fix it as @xmath333 , where @xmath276 is the total number of batches in the burn - in period : during the burn - in the log standard deviation should be able to reach any values at a distance @xmath334 in log scale from the initial value of @xmath335 usually set equal to zero .",
    "the _ diminishing adaptive condition _ is obtained imposing @xmath336 , where @xmath269 is the current number of batches , including the burn - in . to ensure the _ bounded convergence condition _",
    "we follow roberts and rosenthal ( 2008 ) , restricting each @xmath332 to be inside @xmath337 $ ] and we fix them equal to @xmath338 and @xmath339 respectively . in practise these bounds do not create any restriction since the sequence of the standard deviations of the proposal distribution stabilises almost immediately , indicating that the transition kernel converges in a bounded number of batches , see figure [ fig_t2 ] .",
    "in this section we report in details on the performance of ess in a variety of simulated examples .",
    "main conclusions are summarised in the section [ simulation study ] .",
    "firstly we analyse the simulated examples with ess@xmath209 the version of our algorithm which assumes independent priors , @xmath210 , so as to enable comparisons with sss which also implements an independent prior .",
    "moreover , in order to make to comparison with sss fair , in the simulation study only the first step of the algorithm described in section 3.3 is performed , with @xmath63 fixed at @xmath110 . as in sss , standardisation of the covariates",
    "is done before running ess@xmath209 .",
    "we run ess@xmath209 and sss 2.0 ( hans _ et al .",
    "_ , 2007 ) for the same number of sweeps ( 22,000 ) and with matching hyperparameters on the model size .",
    "secondly , to discuss the mixing properties of ess when a prior @xmath170 is defined on @xmath63 , we implement both the @xmath3-prior and independent prior set - up for a particular simulated experiment . to be precise in the former case we will use the zellner - siow priors ( [ t9 ] ) , and for the latter we will specify a proper but diffuse exponential distribution as suggested by bae and mallick ( 2004 ) .",
    "we apply ess with independent priors to an extensive and challenging range of simulated examples with @xmath63 fixed at @xmath110 : the first three examples ( ex1-ex3 ) consider the case @xmath211 while the remaining three ( ex4-ex6 ) have @xmath212 .",
    "moreover in all examples , except the last one , we simulate the design matrix , creating more and more intricated correlation structures between the covariates in order to test the proposed algorithm in different and increasingly more realistic scenarios . in the last example",
    ", we use , as design matrix , a genetic region spanning @xmath340-kb from the hapmap project ( altshuler _ et al . _ , 2005 ) .",
    "simulated experiments ex1-ex5 share in common the way we build @xmath9 . in order to create moderate to strong correlation",
    ", we found useful referring to two simulated examples in george and mcculloch , g&mcc hereafter , ( 1993 ) and in g&mcc ( 1997 ) : throughout we call @xmath341 ( @xmath342 ) and @xmath343 @xmath344 the design matrix obtained from these two examples .",
    "in particular the @xmath105th column of @xmath341 , indicated as @xmath345 , is simulated as @xmath346 , where @xmath347 iid @xmath348 independently form @xmath349 , inducing a pairwise correlation of @xmath160 .",
    "@xmath343 is generated as follows : firstly we simulated @xmath350 iid @xmath348 and we set @xmath351 for @xmath352 only . to induce strong multicollinearity , we then set @xmath353 , @xmath354 , @xmath355 , @xmath356 and @xmath357 .",
    "a pairwise correlation of about 0.998 between @xmath358 and @xmath359 for @xmath360 is introduced and similarly strong linear relationship is present within the sets @xmath361 and @xmath362 .    then , as in nott and green , n&g hereafter , ( 2004 ) example 2 , more complex structures are created by placing side by side combinations of @xmath341 and/or @xmath343 , with different sample size .",
    "we will vary the number of samples @xmath1 in @xmath341 and @xmath343 as we construct our examples .",
    "the levels of @xmath20 are taken from the simulation study of fernndez _ et al . _",
    "( 2001 ) , while the number of true effects , @xmath363 , with the exception of ex3 , varies from @xmath196 to @xmath364 .",
    "finally the simulated error variance ranges from @xmath365 to @xmath366 in order to vary the level of difficulty for the search algorithm . throughout",
    "we only list the non - zero @xmath367 and assume that @xmath368 .",
    "the six examples can be summarised as follows :    * @xmath369 is a matrix of dimension @xmath370 , where the responses are simulated from ( 1 ) using @xmath371 , @xmath372 , @xmath373 , and @xmath374 . in the following",
    "we will not refer to the intercept @xmath14 any more since , as described in section 3.3 in the paper , we consider @xmath18 centred and hence there is no difference in the results if the intercept is simulated or not .",
    "this is the simplest of our example , although , as reported in g&mcc ( 1993 ) the average pairwise correlation is about @xmath160 , making it already hard to analyse by standard stepwise methods .",
    "* this example is taken directly from n&g ( 2004 ) , example 2 , who first introduce the idea of combining simpler building blocks  to create a new matrix @xmath9 : in their example @xmath375 $ ] is a @xmath376 matrix , where @xmath377 and @xmath378 are of dimension @xmath379 and have each the same structure as @xmath380 .",
    "moreover @xmath381 , @xmath382 and @xmath383 .",
    "we chose this example for two reasons : firstly , since the correlation structure in @xmath343 is very involved , we test the proposed algorithm under strong and complicated correlations between the covariates ; secondly , since @xmath18 is not simulated from the second block , we are interested to see if the proposed algorithm does _ not _ select any variable that belongs to the second group . * as in g&mcc ( 1993 ) , example 2 , @xmath369 , is a @xmath370 matrix , @xmath384 , @xmath385 , @xmath386 , @xmath387 , @xmath388 and @xmath389 .",
    "the motivation behind this example is to test the strength of the proposed algorithm to select a subset of variables which is large with respect to @xmath0 while preserving the ability _ not _ to choose any of the first @xmath390 variables . *",
    "the design matrix @xmath9 , @xmath391 , is constructed as follows :",
    "firstly we create a new @xmath392 building block , @xmath393 , combining @xmath343 and a smaller version of @xmath341 , @xmath394 , a @xmath395 matrix simulated as @xmath341 , such that @xmath396 $ ] ( dimension @xmath370 ) .",
    "secondly we place side by side five copies of @xmath393 , @xmath397 $ ] : the new design matrix alternates blocks of covariates of high and complicated correlation , as in g&mcc ( 1997 ) , with regions where the correlation is moderate as in g&mcc ( 1993 ) .",
    "we simulate the response selecting @xmath364 variables from @xmath9 , + @xmath398 such that every pair belongs alternatively to @xmath343 or @xmath341 .",
    "we simulate @xmath18 using + @xmath399 with @xmath400 .",
    "this example is challenging in view of the correlation structure , the number of covariates @xmath401 and the different levels of the effects . *",
    "this is the most challenging example that we simulated and it is based on the idea of contaminated models .",
    "the matrix @xmath9 , @xmath402 , is @xmath403    $ ] , with @xmath404 , a @xmath405 larger version of @xmath341 .",
    "we partitioned the responses such that @xmath406^{t}$ ] : @xmath407 is simulated from model 1  ( @xmath408 and @xmath409 ) while @xmath410 is simulated from model 2  ( @xmath411 and @xmath412 ) .",
    "finally , fixing @xmath413 and the sample size in the two models such that @xmath414 and @xmath415 are vectors of dimension @xmath416 and @xmath417 respectively , @xmath18 is retained if , given the sampling variability , we find @xmath418 and @xmath419 : in this way we know that model 1 accounts for most of the variability of @xmath18 , but without a negligible effect for model 2 . in this example , we measure the ability of the proposed algorithm to recognise the most promising model and therefore being robust to contaminations .",
    "however since ess can easily jump between local modes we are also interested to see if model 2  is selected .",
    "* the last simulated example is based on phased genotype data from hapmap project ( altshuler _ et al .",
    "_ , 2005 ) , region enm014 , yoruba population : the data set originally contained 1,218 snps ( single nucleotide polymorphism ) for 120 chromosomes , but after eliminating redundant variables , the design matrix reduced to @xmath420 .",
    "while in the previous examples a block structure  of correlated variables is artificially constructed , in this example blocks of linkage disequilibrium ( ld ) derive naturally from genetic forces , with a slow decay of the level of pairwise correlation between snps .",
    "finally we chose @xmath421 such that the effects are visually inside blocks of ld , with their size simulated from @xmath422 with @xmath423 . since the simulated effects can range",
    "roughly between @xmath424 , this will allow us to test also the ability of ess@xmath11 to select small effects .",
    "we conclude this section by reporting how we conducted the simulation experiment : every example from ex1 to ex6 has been replicated @xmath199 times and the results presented for example ex1 to ex5 are averaged over the @xmath199 replicates . for ex6 the effects size change so average across replicated is only done for the mixing properties .",
    "ess@xmath11 with @xmath63 = 1 was applied to each example / sample , recording the visited sequence of @xmath425 for @xmath197 sweeps after a burn - in of @xmath426 required for the automatic tuning of the temperature placement , section [ emc sampler ] with the exception of ex2 and ex3 , where we used an indifferent prior , @xmath427 , we analysed the remaining examples setting @xmath428 with @xmath429 which corresponds to a binomial prior over @xmath214 . in order to establish the sensitivity of the proposed algorithm to the choice of @xmath151 we also analysed ex1 fixing @xmath430 and @xmath431 .",
    "moreover in all the examples we chose @xmath154 with the starting value of @xmath102 chosen at random .",
    "the remaining two hyperparameters to be fixed , namely @xmath43 and @xmath44 , are set equal to @xmath432 and @xmath433 as in kohn _",
    "( 2001 ) which corresponds to a relative uninformative prior .",
    "in this section we report some stylised facts about the performance of the ess@xmath209 with @xmath63 fixed at @xmath110 .",
    "figure [ fig_s3 ] , top panels , shows for one of the replicates of ex1 , the overall mixing properties of ess@xmath11 .",
    "as expected , the chains attached to higher temperatures shows more variability . albeit the convergence is reached in the product space @xmath434 ^{1/t_{l}}$ ] , by visual inspection each chain _ marginally _ reaches its _ equilibrium _ with respect to the others ; moreover , thanks to the automatic tuning of the temperature placement during the burn - in , the distributions of their log posterior probabilities overlap nicely , allowing effective exchange of information between the chains . figure [ fig_s3 ] , bottom panels , shows the trace plot of the log posterior and the model size for a replicate of ex4 .",
    "we can see that also in the case @xmath94 , the chains mix and overlap well with no gaps between them , the automatic tuning of the temperature ladder being able to improve drastically the performance of the algorithm .",
    "this effective exchange of information is demonstrated in table [ table_s3 ] which shows good overall acceptance rates for the collection of moves that we have implemented .",
    "the dimension of the problem does not seem to affect the acceptance rate of the ( delayed rejection ) exchange operator which stays very stable and close to the target : for instance in ex4 ( @xmath435 ) and ex6 ( @xmath436 ) the mean and standard deviation of the acceptance rate are @xmath437 ( @xmath438 ) and @xmath439 ( @xmath440 ) while in ex5 ( @xmath441 ) we have @xmath442 ( @xmath443 ) : the higher variability in ex4 being related to the model size @xmath215 .    with regards to the crossover operators ,",
    "again we observe stability across all the examples .",
    "moreover , in contrast to jasra _",
    "( 2007 ) , when @xmath94 , the crossover average acceptance rate across the five chains is quite stable between @xmath444 , ex4 , and @xmath445 , ex6 ( with the lower value in ex4 here again due to @xmath215 ) : within our limited experiments , we believe that the good performance of crossover operator is related to the selection operator and the new block crossover , see section [ emc sampler ] .    some finer tuning of the temperature ladder could still be performed as there seems to be an indication that fewer global moves are accepted with the higher temperature chain , see table [ table_s4 ] , where swapping probabilities for each chain are indicated .",
    "note that the observed frequency of successful swaps is not far from the case where adjacent chains are selected to swap at random with equal probability .",
    "other measures of overlapping between chains ( liang and wong , 2000 ; iba 2001 ) , based on a suitable index of variation of @xmath446 across sweeps , confirm the good performance of ess@xmath11 .",
    "again some instability is present in the high temperature chains , see in table [ table_s4 ] the overlapping index between chains @xmath447 and @xmath448 in example 3 to 6 .",
    "in ex1 , we also investigate the influence of different values of the prior mean of the model size .",
    "we found that the average ( standard deviation in brackets ) acceptance rate across replicates for the delayed rejection exchange operator ranges from @xmath449 ( @xmath450 ) to 0.500 ( 0.040 ) for different values of the prior mean on the model size , while the acceptance rate for the crossover operator ranges from @xmath451 ( @xmath452 ) to @xmath453 ( @xmath454 ) .",
    "this strong stability is not surprising because the automatic tuning modifies the temperature ladder in order to compensate for @xmath455 .",
    "finally we notice that the acceptance rates for the local move , when @xmath211 , increases with higher values of the prior mean model size , showing that locally the algorithm moves more freely with @xmath456 than with @xmath457 .",
    "we conclude this section by discussing in details the overall performance of ess@xmath11 with respect to the selection of the true simulated effects . as a first measure of performance ,",
    "we report for all the simulated examples the marginal posterior probability of inclusion as described in g&mcc ( 1997 ) and hans _ et al . _",
    "( 2007 ) . in the following , for ease of notation",
    ", we drop the chain subscript index and we exclusively refer to the first chain .",
    "to be precise , we evaluate the marginal posterior probability of inclusion as : @xmath458 with @xmath459 and @xmath460 the number of sweeps after the burn - in .",
    "the posterior model size is similarly defined , @xmath461 , with @xmath462 as before .",
    "besides plotting the marginal posterior inclusion probability ( [ l35 ] ) averaged across sweeps and replicates for our simulated examples , we will also compute the interquartile range of ( [ l35 ] ) across replicates as a measure of variability .    in order to thoroughly compare the proposed ess algorithm to sss ( hans _ et al .",
    "_ , 2007 ) , we present also some other measures of performance based on @xmath463 and @xmath464 : first we rank @xmath465 in decreasing order and record the indicator @xmath29 that corresponds to the maximum and @xmath190 largest @xmath463 ( after burn - in ) . given the above set of latent binary vectors , we then compute the corresponding @xmath464 leading to @xmath464 : @xmath466  as well as the mean @xmath464 over the @xmath190 largest @xmath467 , @xmath468 : @xmath190 largest @xmath463 , both quantities averaged across replicates .",
    "moreover the actual ability of the algorithm to reach regions of high posterior probability and persist on them is monitored : given the sequence of the @xmath190 best @xmath27s ( based on @xmath465 ) , the standard deviation of the corresponding @xmath464s shows how stable is the searching strategy at least for the top ranked ( not unique ) posterior probabilities : averaging over the replicates , it provides an heuristic measures of stability  of the algorithm .",
    "finally we report the average computational time ( in minutes ) across replicates of ess@xmath11 written in matlab code and run on a 2mhz cpu with 1.5 gb ram desktop computer and of sss version 2.0 on the same computer .",
    "figure [ fig_s4 ] presents the marginal posterior probability of inclusion for ess@xmath11 with @xmath91 averaged across replicates and , as a measure of variability , the interquartile range , blue left triangles and vertical blue solid line respectively . in general",
    "the covariates with non - zero effects have high marginal posterior probability of inclusion in all the examples : for example in ex3 , figure [ fig_s4 ] ( a ) , the proposed ess@xmath11 algorithm , blue left triangle , is able to perfectly select the last @xmath469 covariates , while the first @xmath390 , which do not contribute to @xmath18 , receive small marginal posterior probability .",
    "it is interesting to note that this group of covariates , @xmath470 , although correctly recognised having no influence on @xmath18 , show some variability across replicates , vertical blue solid line : however , this is not surprising since independent priors are less suitable in situations where all the covariates are mildly - strongly correlated as in this simulated example . on the other hand the second set of covariates with small effects , @xmath471 ,",
    "are univocally detected .",
    "the ability of ess@xmath11 to select variables with small effects is also evident in ex6 , figure [ fig_s4 ] ( d ) , where the two smallest coefficients , @xmath472 and @xmath473 ( the second and last respectively from left to right ) , receive from high to very high marginal posterior probability ( and similarly for the other replicates , data not shown ) . in some cases however , some covariates attached with small effects are missed ( e.g. ex4 , figure [ fig_s4 ] ( b ) , the last simulated effect which is also the smallest , @xmath474 , is not detected ) . in this situation",
    "however the vertical blue solid line indicates that for some replicates , ess@xmath11 is able to assign small values of the marginal posterior probability giving evidence that ess@xmath11 fully explore the whole space of models .",
    "superimposed on all pictures of figure [ table_s4 ] are the median and interquartile range across replicates of @xmath475 , @xmath26 , for sss , red right triangles and vertical red dashed line respectively .",
    "we see that there is good agreement between the two algorithms in general , with in addition evidence that ess@xmath11 is able to explore more fully the model space and in particular to find small effects , leading to a posterior model size that is close to the true one .",
    "for instance in ex3 , figure [ fig_s4 ] ( a ) , where the last @xmath182 covariates accounts for most of @xmath464 , sss has difficulty to detect @xmath476 , while in ex6 , it misses @xmath472 , the smallest effect , and surprisingly also @xmath477 assigning a very small marginal posterior probability ( and in general for the small effects in most replicates , data not shown ) .",
    "however the most marked difference between ess@xmath11 and sss is present in ex5 : as for ess@xmath11 , sss misses three effects of model 1 but in addition @xmath478 , @xmath479 and @xmath480 receive also very low marginal posterior probability , red right triangle , with high variability across replicates , vertical red dashed line .",
    "moreover on the extreme left , as noted before , ess@xmath11 is able to capture the biggest coefficient of model 2 while sss misses completely all contaminated effects .",
    "no noticeable differences between ess@xmath11 and sss are present in ex1 and ex2 for the marginal posterior probability , while in ex4 , sss shows more variability in @xmath481 ( red dashed vertical lines compared to blue solid vertical lines ) for some covariates that do receive the highest marginal posterior probability .",
    "in contrast to the differences in the marginal posterior probability of inclusion , there is general agreement between the two algorithms with respect to some measures of goodness of fit and stability , see table [ table_s5 ] .",
    "again , not surprisingly , the main difference is seen in ex5 where ess@xmath11 with @xmath91 reaches a better @xmath464 both for the maximum and the @xmath190 largest @xmath463 .",
    "sss shows more stability in all examples , but the last : this was somehow expected since one key features of sss in its ability to move quickly towards the right model and to persist on it ( hans _ et al .",
    "_ , 2007 ) , but a drawback of this is its difficulty to explore far apart models with competing @xmath464 as in ex5 . note that ess@xmath11 shows a small improvement of @xmath464 in all the simulated examples .",
    "this is related to the ability of ess@xmath11 to pick up some of the small effects that are missed by sss , see figure [ fig_s4 ] .",
    "finally ess@xmath11 shows a remarkable superiority in terms of computational time especially when the simulated ( and estimated ) @xmath214 is large ( in other simulated examples , data not shown , we found this is always true when @xmath482 ) : the explanation lies in the number of different models sss and ess@xmath11 evaluate at each sweep .",
    "indeed , sss evaluates @xmath483 , where @xmath214 is the size of the current model , while ess@xmath11 theoretically analyses an equally large number of models , @xmath484 , but , when @xmath94 , the actual number of models evaluated is drastically reduced thanks to our fsmh sampler . in only one",
    "case sss beats ess@xmath11 in term of computational time ( ex5 ) , but in this instance sss clearly underestimates the simulated model and hence performs less evaluations than would be necessary to explore faithfully the model space . in conclusion , we see that the rich porfolio of moves and the use of parallel chains makes ess robust for tackling complex covariate space as well as competitive against a state of the art search algorithm .",
    "= 1.5em = 1 abramowitz , m. and stegun , i. ( 1970 ) .",
    "_ handbook of mathematical functions_. new york : dover publications , inc .              = 1em = 1 chipman , h. , george , e.i . and mcculloch , r.e .",
    "( 2001 ) . the practical implementation of bayesian model selection ( with discussion ) . in _",
    "model selection _",
    "( p. lahiri , ed ) , 66 - 134 .",
    "ims : beachwood , oh .                = 1em = 1 geweke , j. ( 1996 ) .",
    "variable selection and model comparison in regression . in _",
    "bayesian statistics 5 , proc .",
    "5th int . meeting _",
    "bernardo , j.o .",
    "berger , a.p .",
    "dawid and a.f.m .",
    "smith , eds ) , 609 - 20 .",
    "claredon press : oxford , uk .",
    "= 1em = 1 wilson , m.a . ,",
    "iversen , e.s . ,",
    "clyde , m.a . ,",
    "schmidler , s.c . and shildkraut , j.m .",
    "bayesian model search and multilevel inference for snp association studies .",
    "available at : + ` http://arxiv.org/abs/0908.1144 `    = 1em = 1 zellner , a. ( 1986 ) . on assessing prior distributions and bayesian regression analysis with @xmath3-prior distributions . in _",
    "bayesian inference and decision techniques - essays in honour of bruno de finetti _",
    "goel and a. zellner , eds ) , 233 - 243 .",
    "amsterdam : north - holland .",
    "= 1em = 1 zellner , a. and siow , a. ( 1980 ) .",
    "posterior odds ratios for selected regression hypotheses . in _",
    "bayesian statistics , proc .",
    "bernardo , m.h .",
    "de groot , d.v .",
    "lindley and a.f.m .",
    "smith , eds ) , 585 - 603 .",
    "valencia : university press .",
    "replicates of the analysis of the first real data example and normalised by the total mass found by ess@xmath3 , @xmath486 , with only block crossover move ( @xmath487 ) .",
    "1-point and uniform crossover accumulate around @xmath488% of the total mass accumulated by ess@xmath3 with only block crossover , while adaptive crossover only @xmath489%.,title=\"fig : \" ]"
  ],
  "abstract_text": [
    "<S> implementing bayesian variable selection for linear gaussian regression models for analysing high dimensional data sets is of current interest in many fields . in order to make such analysis operational , </S>",
    "<S> we propose a new sampling algorithm based upon evolutionary monte carlo and designed to work under the large @xmath0 , small @xmath1  paradigm , thus making fully bayesian multivariate analysis feasible , for example , in genetics / genomics experiments . </S>",
    "<S> two real data examples in genomics are presented , demonstrating the performance of the algorithm in a space of up to @xmath2 covariates . </S>",
    "<S> finally the methodology is compared with a recently proposed search algorithms in an extensive simulation study .    </S>",
    "<S> _ keywords _ : evolutionary monte carlo ; fast scan metropolis - hastings schemes ; linear gaussian regression models ; variable selection . </S>"
  ]
}