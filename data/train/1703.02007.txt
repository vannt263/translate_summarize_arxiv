{
  "article_text": [
    "in this paper , we investigate a probabilistic numerical method to approximate the solution of the following non - local pde @xmath0 } }    + f\\bigl(x,{\\mathcal{u}}(t , x,\\mu),\\partial_x { \\mathcal{u}}(t , x,\\mu ) \\sigma(x,\\mu),\\nu\\bigr )   \\\\ & \\hspace{5pt}+ \\int_{{\\mathbb{r}}^d}\\partial_\\mu { \\mathcal{u}}(t , x,\\mu)(\\upsilon )   \\cdot b(\\upsilon,{\\mathcal{u}}(t,\\upsilon,\\nu),\\nu){\\mathrm{d}}\\mu(\\upsilon )     + \\int_{{\\mathbb{r}}^d } \\frac12 { \\ensuremath{{\\rm tr}[\\partial_{x}\\partial_\\mu { \\mathcal{u}}(t , x,\\mu)(\\upsilon ) a(\\upsilon,\\mu ) ] } } { \\mathrm{d}}\\mu(\\upsilon )   = 0 \\ ; , \\end{split}\\ ] ] for @xmath1 with the terminal condition @xmath2 , where @xmath3 is a notation for the image of the probability measure @xmath4 by the mapping @xmath5 . above , @xmath6(x,\\mu)$ ] .",
    "the set @xmath7 is the set of probability measures with a finite second - order moment , endowed with the wasserstein distance i.e. @xmath8 for @xmath9 , the infimum being taken over the probability distributions @xmath10 on @xmath11 whose marginals on @xmath12 are respectively @xmath4 and @xmath13 .    whilst the first two lines in form a classical non - linear parabolic equations , the last two terms are non - standard .",
    "not only are they non - local , in the sense that the solution or its derivatives are computed at points @xmath14 different from @xmath15 , but also they involve derivatives in the argument @xmath4 , which lives in a space of probability measures . in this regard ,",
    "the notation @xmath16 denotes the so - called _ wasserstein derivative _ of the function @xmath17 in the direction of the measure , computed at point @xmath18 and taken at the _ continuous coordinate _ @xmath14 .",
    "we provide below a short reminder of the construction of this derivative , as introduced by lions , see @xcite or ( * ? ? ?",
    "these pdes arise in the study of large population stochastic control problems , either of mean field game type , see for instance @xcite or ( * ? ? ?",
    "12 ) and the references therein , or of mean field control type , see for instance @xcite . in both cases , @xmath17 plays the role of a value function or , when the above equation is replaced by a system of equations of the same form , the gradient of the value function . generally speaking ,",
    "these types of equations are known as `` master equations '' .",
    "we refer to the aforementioned papers and monographes for a complete overview of the subject , in which existence and uniqueness of classical or viscosity solutions have been studied . in particular , in our previous paper @xcite",
    ", we tackled classical solutions by connecting @xmath17 with a system of fully coupled forward - backward stochastic differential equations of the mckean - vlasov type ( mkv fbsde ) , for which @xmath17 plays the role of a decoupling field .",
    "we also refer to ( * ? ? ?",
    "12 ) for a similar approach .    in the current paper",
    ", we build on this link to design our numerical method .",
    "the connection between @xmath17 and fbsdes may be stated as follows .",
    "basically , @xmath17 may be written as @xmath19 for all @xmath20\\times { \\mathbb{r}}^d \\times { \\mathcal{p}}_2({\\mathbb{r}}^d)$ ] , where @xmath21 together with @xmath22 solves the following standard fbsde : @xmath23 } } ) { \\mathrm{d}}r + \\int_t^s \\sigma(x^{t , x,\\mu}_r,{\\ensuremath { [ x^{t , x,\\mu}_r ] } } ) { \\mathrm{d}}w_r \\label{eq xtxmu }   \\\\",
    "y^{t , x,\\mu}_s & = g(x^{t , x,\\mu}_{t},{\\ensuremath { [ x_t^{t,\\xi } ] } } ) + \\int_s^t f(x^{t , x,\\mu}_{r},y^{t , x,\\mu}_{r},z^{t , x,\\mu}_r,{\\ensuremath { [ x^{t,\\xi}_r , y^{t,\\xi}_r ] } } )   { \\mathrm{d}}r - \\int_s^t z^{t , x,\\mu}_r \\cdot { \\mathrm{d}}w_r \\ , , \\label{eq ytxmu }   \\end{aligned}\\ ] ] which is parametrized by the law of the following mkv fbsde : @xmath24 } } ) { \\mathrm{d}}r + \\int_t^s \\sigma(x^{t,\\xi}_r,{\\ensuremath { [ x^{t,\\xi}_{r } ] } } ) { \\mathrm{d}}w_r   \\label{eq xtxi }   \\\\",
    "y^{t,\\xi}_s & = g(x^{t,\\xi}_{t},{\\ensuremath { [ x_t^{t,\\xi } ] } } ) + \\int_s^t f(x^{t,\\xi}_{r},y^{t,\\xi}_{r},z^{t,\\xi}_r,{\\ensuremath { [ x^{t,\\xi}_r , y^{t,\\xi}_r ] } } )   { \\mathrm{d}}r - \\int_s^t z^{t,\\xi}_r \\cdot { \\mathrm{d}}w_r \\ , ,   \\label{eq ytxi }   \\end{aligned}\\ ] ] where @xmath25 is a brownian motion and",
    "@xmath26 has @xmath4 as distribution . in the previous equations and in the sequel , we use the notation @xmath27 } } $ ] for the law of a random variable @xmath28 . in particular",
    ", in the above , we have that @xmath29 } } = \\mu$ ] .",
    "so , to obtain an approximation of @xmath30 given by the initial value of , our strategy is to approximate the system - as its solution appears in the coefficients of - . in this",
    "regard , our approach is probabilistic .",
    "actually , our paper is not the first one to address the numerical approximation of equations of the type by means of a probabilistic approach . in its phd dissertation",
    ", alanko @xcite develops a numerical method for mean field games based upon a picard iteration : given the proxy for the equilibrium distribution of the population ( which is represented by the mean field component in the above fbsde ) , one solves for the value function by approximating the solution of the ( standard ) bsde associated with the control problem ; given the solution of the bsde , we then get a new proxy for the equilibrium distribution and so on ... up to a girsanov transformation , the bsde associated with the control problem coincides with the backward equation in the above fbsdes . in @xcite ,",
    "the girsanov transformation is indeed used to decouple the forward and backward equations and it is the keystone of the paper to address the numerical impact of the change of measure onto the mean field component . under our setting , this method would more or less consist in solving for the backward equation given a proxy for the forward equation and then in iterating , which is what we call the _ picard method _ for the fbsde system . unfortunately , convergence of the picard iterations is a difficult issue , as the convergence is known in small time only , see the numerical examples in section [ se numerics ] below .",
    "it is indeed well - known that picard theorem only applies in small time for fully coupled problems . in this regard",
    ", it must be stressed that our system - is somehow doubly coupled , once in the variable @xmath15 and once in the variable @xmath4 , which explains why a change measure does not permit to decouple it entirely .",
    "the goal of our paper is precisely to go further and to propose an algorithm whose convergence is known on any interval of a given length ( observe that the convergence is not studied in @xcite ) . in the classical case",
    ", this question has been addressed by several authors , among which @xcite and @xcite , but all these methods rely on the markov structure of the problem . here",
    ", the markov property is true but at the price of regarding the entire @xmath31 as state space : the fact that the second component is infinite dimensional makes intractable the complexity of these approaches . to avoid any similar problem , we use a pathwise approach for the forward component ; it consists in iterating successively the _ picard method _ on small intervals , all the picard iterations being implemented with a tree approximation of the brownian motion .",
    "this strategy is inspired from the method of continuation , the parameter in the continuation argument being the time length @xmath32 itself .",
    "the advantage for working on a tree is twofold : as we said , we completely bypass any markov argument ; also , we get , not only , an approximation of the system - but also , for free , an approximation of the system - , which `` lives '' on a subtree obtained by conditioning on the initial root . we prove that the method is convergent and provide a rate of convergence for it .",
    "numerical examples are given in section [ se numerics ] .",
    "of course , the complexity remains pretty high in comparison with the methods developed in the classical non mckean - vlasov case .",
    "this should not come as a surprise since , as we already emphasized , the problem is somehow infinite dimensional .",
    "we refer the interested reader to the following papers for various numerical methods , based upon finite differences or variational approaches , for mean field games : @xcite and @xcite .",
    "recently , a markov chain approximation method was also suggested in @xcite .",
    "the paper is organized as follows .",
    "the method for the system - is exposed in section [ se algo mkvfbsde 1 ] .",
    "the convergence is addressed in section [ se algo mkvfbsde analysis ] . in section [ se numerics ] , we explain how to compute in practice @xmath30 ( and thus approximate - ) from the approximation of the sole - and we present some numerical results validating empirically the convergence results obtained in section [ se algo mkvfbsde analysis ] .",
    "we collect in the appendix some key results for the convergence analysis .",
    "as announced right above , we will focus on the approximation of the following type of mckean - vlasov forward - backward stochastic differential equation : @xmath33 } } \\bigr ) dt + \\sigma\\bigl(x_t,{\\ensuremath { [ x_{t } ] } } \\bigr)dw_{t } , \\\\ & dy_{t } = -f\\bigl(x_{t},y_{t},z_t,{\\ensuremath { [ x_t , y_t ] } } \\bigr){\\mathrm{d}}t + z_{t } \\cdot dw_{t } , \\quad t \\in [ 0,t ] \\ ,   \\\\ & y_{t } = g\\bigl(x_{t},{\\ensuremath { [ x_t ] } } \\bigr)\\,\\;\\text { and } \\ ; x_0 = \\xi \\ , , \\end{split}\\ ] ] for some time horizon @xmath34 . throughout the analysis ,",
    "the equation is regarded on a complete filtered probability space @xmath35 , equipped with a @xmath36-dimensional @xmath37-brownian motion @xmath38 . to simplify , we assume that the state process @xmath39 is of the same dimension .",
    "the process @xmath40 is @xmath41-dimensional . as a result",
    ", @xmath42 is @xmath36-dimensional .    in",
    ", the three processes @xmath43 , @xmath40 and @xmath42 are required to be @xmath37-progressively measurable .",
    "both @xmath43 and @xmath40 have continuous trajectories . generally speaking",
    ", the initial condition @xmath44 is assumed to be square - integrable , but at some point , we will assume that @xmath44 belongs to @xmath45 , for some @xmath46 .",
    "accordingly , @xmath43 , @xmath40 and @xmath47 must satisfy : @xmath48 } : =   \\ee \\biggl [ \\sup_{0 \\le t \\le t } \\bigl ( \\vert x_{t }",
    "\\vert^2 + \\vert y_{t } \\vert^2 \\bigr ) + \\int_{0}^t \\vert z_{t } \\vert^2 dt \\biggr]^{1/2 } < \\infty.\\ ] ]    the domains and codomains of the coefficients are defined accordingly .",
    "the assumption that @xmath49 is assumed to be independent of the variable @xmath50 is consistent with the global solvability results that exist in the literature for equations like .",
    "for instance , it covers cases coming from optimization theory for large mean field interacting particle systems .",
    "we refer to our previous paper @xcite for a complete overview on the subject , together with the references @xcite . in light of the examples tackled in @xcite ,",
    "the fact that @xmath51 is independent of @xmath52 may actually seem more restrictive , as it excludes cases when the forward - backward system of the mckean - vlasov type is used to represent the value function of the underlying optimization problem .",
    "it is indeed a well - known fact that , with or without mckean - vlasov interaction , the value function of a standard optimization problem may be represented as the backward component of a standard fbsde with a drift term depending upon the @xmath52 variable .",
    "this says that , in order to tackle the aforementioned optimization problems of the mean field type by means of the numerical method investigated in this paper , one must apply the algorithm exposed below to the pontryagin system .",
    "the latter one is indeed of the form , provided that @xmath53 is allowed to be multi - dimensional .",
    "( below , we just focus on the one - dimensional case , but the adaptation is straightforward . )    in fact , our choice for assuming @xmath51 to be independent of @xmath52 should not come as a surprise .",
    "the same assumption appears in the papers @xcite dedicated to the numerical analysis of standard fbsdes , which will serve us as a benchmark throughout the text .",
    "see however remark [ re z meme pas peur ] .",
    "finally , the fact that the coefficients are time - homogeneous is for convenience only .    as a key ingredient in our analysis , we use the following representation result given in e.g. proposition 2.2 in @xcite , @xmath54 } } ) \\;,\\end{aligned}\\ ] ] where @xmath55 \\times \\rr^d \\times { \\mathcal{p}}_{2}(\\rr^d ) \\rightarrow \\rr$ ] is assumed to be the classical solution , in the sense of ( * ? ? ?",
    "* definition 2.6 ) , to . in this regard ,",
    "the derivative with respect to the measure argument is defined according to lions approach to the wasserstein derivative . in short , the _ lifting _ @xmath56 of @xmath17 to @xmath57 , which we define by @xmath58 ) , \\qquad t \\in [ 0,t ] , \\ x \\in \\rr^d , \\",
    "\\xi \\in l^2(\\omega,{\\mathcal{f}}_{0},\\pp;\\rr^d),\\ ] ] is assumed to be frchet differentiable .",
    "of course , this makes sense as long as the space @xmath59 is rich enough so that , for any @xmath60 , there exists a random variable @xmath61 such that @xmath62 .",
    "so , in the sequel , @xmath59 is assumed to be atomless , which makes it rich enough .",
    "a crucial point with lions approach to wasserstein differential calculus is that the frchet derivative of @xmath56 in the third variable , which can be identified with a square - integrable random variable , may be represented at point @xmath63 as @xmath64)(\\xi)$ ] for a mapping @xmath65 .",
    "this latter function plays the role of wasserstein derivative of @xmath17 in the measure argument . to define a classical solution ,",
    "it is then required that @xmath66 is differentiable , both @xmath67 and @xmath68 being required to be continuous at any point @xmath69 such that @xmath70 is in the support of @xmath4 .    * assumptions . *",
    "our analysis requires some minimal regularity assumptions on the coefficients @xmath51 , @xmath49 , @xmath71 and the function @xmath17 . as for the coefficients functions ,",
    "we assume that there exists a constant @xmath72 such that :    - @xmath73 : the functions @xmath51 , @xmath49 , @xmath71 and @xmath74 are @xmath75-lipschitz continuous in all the variables , the space @xmath76 being equipped with the wasserstein distance @xmath77 . moreover , the function @xmath49 is bounded by @xmath75 .",
    "we now state the main assumptions on @xmath17 , see remark [ re main ass ] for comments .",
    "- @xmath78 : for any @xmath79 $ ] and @xmath80 , the mckean - vlasov forward - backward system set on @xmath81 $ ] instead of @xmath82 $ ] with @xmath83 as initial condition at time @xmath84 has a unique solution @xmath85 ; in parallel , @xmath17 is the classical solution , in the sense of ( * ? ? ?",
    "* definition 2.6 ) , to ; and @xmath17 and its derivatives satisfy @xmath86 } } ) ( \\xi ) } \\right \\|_2 } \\le \\lambda \\ ; , \\label{eq bound first deriv } \\\\ &   |\\partial^2_{xx } { \\mathcal{u}}(t , x,{\\mu } ) |+{\\left \\|{\\partial_\\upsilon \\partial_\\mu { \\mathcal{u}}(t , x,{\\ensuremath { [ \\xi ] } } ) ( \\xi ) } \\right \\|_2 } \\le \\lambda \\ ; , \\label{eq bound second deriv } \\\\   & { \\text{and } |\\partial^2_{xx } { \\mathcal{u}}(t , x,{\\mu } ) - \\partial^2_{xx } { \\mathcal{u}}(t , x',{\\mu})| \\le \\lambda |x - x'|\\ ; , \\label{eq lip second deriv } } \\end{aligned}\\ ] ] for @xmath87 \\times \\rr^{d } \\times \\rr^{d } \\times l^2(\\omega,{\\mathcal{f}}_{0},\\pp;\\rr^d)$ ] and @xmath88 . also , we require that @xmath89 } } ) - { \\mathcal{u}}(t , x,{\\ensuremath { [ \\xi ] } } ) | +    \\end{split } \\label{eq cont grad t}\\ ] ] and for all @xmath90 , @xmath91\\times{\\mathbb{r}}^d$ ] , @xmath61 and @xmath92 , @xmath93 } } ) ( \\upsilon )   & -\\partial_\\upsilon \\partial_\\mu { \\mathcal{u}}(t , x,{\\ensuremath { [ \\xi ] } } ) ( \\upsilon')|   \\le \\lambda { \\ensuremath{\\ { 1 + |\\upsilon|^{2\\alpha } + |\\upsilon'|^{2\\alpha } + { \\left \\|{\\xi } \\right \\|_2}^{2\\alpha } \\}}}^\\frac12|\\upsilon-\\upsilon'|\\;,\\end{aligned}\\ ] ] for some @xmath94 .",
    "[ re main ass ] in @xcite , it is shown that , under some conditions on the coefficients @xmath51 , @xmath71 and @xmath49 , the pde has indeed a unique classical solution which satisfies the assumption @xmath78 .    1 .",
    "estimate is obtained by combining definition 2.6 and proposition 3.9 in @xcite .",
    "a major difficulty in the analysis provided below is the fact that @xmath95 may be larger than 1 , in which case the lipschitz bound for the second order derivative is super - linear .",
    "this problem is proper to the mckean - vlasov structure of the equation and does not manifest in the classical setting , compare for instance with @xcite .",
    "below , we tackle two cases : the case when @xmath96 , which has been investigated in @xcite and ( * ? ? ?",
    "12 ) under stronger conditions on the coefficients , and the case when @xmath97 but @xmath17 is bounded .",
    "estimates - are required to control the convergence error when the coefficients ( @xmath51 or @xmath71 ) depend on @xmath98 . 1 .",
    "the estimate can be retrieved from the computations made in @xcite .",
    "see the comments at the bottom of page 60 , near equation @xmath99 .",
    "the estimate comes from the theory of fbsdes ( without mckean - vlasov interaction ) .",
    "indeed , using the lipschitz property of @xmath17 and @xmath100 in the variable @xmath4 , it suffices to prove @xmath101 } } ) - { \\mathcal{u}}(t , x,{\\ensuremath { [ x^{t,\\xi}_{t } ] } } ) | +    \\\\ & \\le \\lambda h^\\frac12\\bigl ( 1 + \\vert x \\vert + \\| \\xi \\|_{2 } \\bigr ) \\ , .",
    "\\end{split}\\ ] ] as stated in proposition 2.2 in @xcite , for @xmath62 , @xmath102 } } ) = u_{t,\\mu}(s , x)$ ] where @xmath103 is solution to a quasi - linear pde .",
    "then the estimate follows from standard results on non - linear pdes , see e.g. theorem 2.1 in @xcite .    in comparison with the assumption used in @xcite",
    ", the condition @xmath78 is more demanding .",
    "in @xcite , there is no need for assuming the second - order derivative to be lipschitz in space .",
    "this follows from the fact that , here , we approximate the brownian increments by random variables taking a small number of values , whilst in @xcite , the brownian increments are approximated by a quantization grid with a larger number of points . in this regard , our approach is closer to the strategy implemented in @xcite .      the goal of the numerical method exposed in the paper is to approximate @xmath17 .",
    "the starting point is the formula and , quite naturally , the strategy is to approximate the process @xmath104 .    generally speaking",
    ", this approach raises a major difficulty , as it requires to handle the strongly coupled forward - backward structure of .",
    "indeed , theoretical solutions to may be constructed by means of basic picard iterations but in small time only , which comes in contrast with similar results for decoupled forward or backward equations for which picard iterations converge on any finite time horizon . in the papers",
    "@xcite which deal with the non mckean - vlasov case , this difficulty is bypassed by approximating _ the decoupling field _",
    "@xmath17 at the nodes of a time - space grid .",
    "obviously , this strategy is hopeless in the mckean - vlasov setting as the state variable is infinite dimensional ; discretizing it on a grid would be of a non - tractable complexity .",
    "this observation is the main rationale for the approach exposed below .",
    "our method is a variation of the so - called _ method of continuation_. in full generality , it consists in increasing step by step the coupling parameter between the forward and backward equations .",
    "of course , the intuition is that , for a given time length @xmath32 , the picard scheme should converge for very small values of the coupling parameter .",
    "the goal is then to insert the approximation computed for a small coupling parameter into the scheme used to compute a numerical solution for a higher value of the coupling parameter .",
    "below , we adapt this idea , but we directly regard @xmath32 itself as a coupling parameter .",
    "so we increase @xmath32 step and by step and , on each step , we make use of a picard iteration based on the approximations obtained at the previous steps .",
    "this naturally motivates the introduction of an equidistant grid @xmath105 of the time interval @xmath82 $ ] , with @xmath106 and @xmath107 for @xmath108 . in the following",
    "we shall consider that @xmath109 is `` small enough '' and state more precisely what it means in the main results , see theorem [ th error picard ] and theorem [ th propagation error ii ] .    for @xmath110 , we consider intervals @xmath111 $ ] and on each interval , the following fbsde , for @xmath112 ( which is a shorter notation for @xmath113 ) : @xmath114 } } \\bigr){\\mathrm{d}}s + \\int_{r_k}^t \\sigma(x_s,{\\ensuremath { [ x_{s } ] } } ) { \\mathrm{d}}w_s\\;,\\;\\\\   y_t & = g\\bigl(x_t,{\\ensuremath { [ x_t ] } }",
    "\\bigr ) + \\int_{t}^t f\\bigl(x_{s},y_{s},z_s,{\\ensuremath { [ x_s , y_s ] } } \\bigr){\\mathrm{d}}s -\\int_t^tz_s \\cdot { \\mathrm{d}}w_s .",
    "\\label{eq main step k y}\\end{aligned}\\ ] ]    [ [ picard - iterations . ] ] picard iterations .",
    "+ + + + + + + + + + + + + + + + + +    we need to compute backwards the value of @xmath115 } } ) $ ] for some @xmath112 , @xmath116 .",
    "we are then going to solve the fbsde - on the interval @xmath117 . as explained above",
    ", the difficulty is the arbitrariness of @xmath32 : when @xmath118 is large , @xmath117 is of a small length , but this becomes false as @xmath118 decreases .",
    "fortunately , we can rewrite the forward - backward system on a smaller interval at the price of changing the terminal boundary condition . indeed , from @xmath119 , we know that @xmath120 solves : @xmath121 } } \\bigr){\\mathrm{d}}s + \\int_{r_k}^t \\sigma\\bigl(x_s,{\\ensuremath { [ x_{s } ] } } \\bigr){\\mathrm{d}}w_s \\ ; , \\;\\\\",
    "y_t & = & { \\mathcal{u}}\\bigl(r_{k+1 } , x_{r_{k+1}},{\\ensuremath { [ x_{r_{k+1 } } ] } } \\bigr )    + \\int_{t}^{r_{k+1 } } f\\bigl(x_{s},y_{s},z_s,{\\ensuremath { [ x_s , y_s ] } } \\bigr){\\mathrm{d}}s - \\int_t^{r_{k+1 } } z_s \\cdot { \\mathrm{d}}w_s \\ ; ,     \\end{array }    \\right .",
    "\\end{aligned}\\ ] ] for @xmath122 $ ] .",
    "if @xmath109 is small enough , a natural approach is to introduce a picard iteration scheme to approximate the solution of the above equation .",
    "to do so , one can implement the following recursion ( with respect to the index @xmath123 ) : @xmath124 } } \\bigr){\\mathrm{d}}s +   \\int_{r_k}^t \\sigma\\bigl(x^j_s , { \\ensuremath { [ x^j_s ] } }    \\bigr){\\mathrm{d}}w_s\\;,\\;\\\\   y^{j}_t & = & { \\mathcal{u}}\\bigl(r_{k+1 } , x^{j-1}_{r_{k+1}},{\\ensuremath { [ x^{j-1}_{r_{k+1 } } ] } } \\bigr )     + \\int_{t}^{r_{k+1 } } f\\bigl(x_{s}^{j-1},y_{s}^j , z^j_s,{\\ensuremath { [ x^{j-1}_s , y^j_s ] } } \\bigr){\\mathrm{d}}s -\\int_{t}^{r_{k+1}}z^j_s \\cdot { \\mathrm{d}}w_s     \\end{array }    \\right.\\ , .",
    "\\end{aligned}\\ ] ] with @xmath125 } } \\bigr){\\mathrm{d}}s +   \\int_{r_k}^t \\sigma\\bigl(x^0_s , { \\ensuremath { [ x^0_s ] } }    \\bigr){\\mathrm{d}}w_s ) _ { r_{k } \\leq s \\leq r_{k+1}}$ ] and @xmath126 .",
    "it is known that , for @xmath109 small enough , @xmath127 , in the sense that @xmath128 } \\rightarrow_{j \\rightarrow \\infty } 0 $ ] .",
    "but in practice we will encounter three main difficulties .    1 .",
    "the procedure has to be stopped after a given number of iterations @xmath129 .",
    "2 .   the above picard iteration assumes the perfect knowledge of the map @xmath17 at time @xmath130 , but @xmath17 is exactly what we want to compute ... 3 .",
    "the solution has to be discretized in time and space .",
    "* ideal recursion . *",
    "we first discuss 1 ) and 2 ) above .",
    "the main idea is to use a recursive algorithm ( with a new recursion , but on the time parameter ) .",
    "+ namely , for @xmath131 , we assume that we are given a solver which computes @xmath132(}}\\xi\\text{\\texttt { ) } } } } =   { \\mathcal{u}}(r_{k+1},\\xi,{\\ensuremath { [ \\xi ] } } ) + \\epsilon^{k+1}(\\xi)\\,,\\end{aligned}\\ ] ] where @xmath133 is an error made , for any @xmath134 .",
    "we shall sometimes refer to @xmath135(}}\\cdot\\text{\\texttt { ) } } } } $ ] as `` the solver at level @xmath136 '' .",
    "taking these observations into account , we first define an ideal solver , which assumes that each picard iteration in the approximation of the solution of the forward - backward system can be perfectly computed . we denote it by @xmath137(}}\\text{\\texttt { ) } } } } $ ] .",
    "accordingly , we identify ( for the time being ) @xmath135(}}\\text{\\texttt { ) } } } } $ ] with @xmath138(}}\\text{\\texttt { ) } } } } $ ] .",
    "given @xmath138(}}\\text{\\texttt { ) } } } } $ ] , @xmath139(}}\\text{\\texttt { ) } } } } $ ] is defined as follows .",
    "@xmath140 } } \\bigr){\\mathrm{d}}s   + \\int_{r_k}^t    \\sigma\\bigl({\\tilde{x}^{k , j}}_s ,   { \\ensuremath { [ { \\tilde{x}^{k , j}}_s ] } }   \\bigr ) { \\mathrm{d}}w_s \\;,\\;\\\\   { \\tilde{y}^{k , j}}_t & = & { \\ensuremath {   \\text{\\texttt{picard[}}k+1\\text{\\texttt{](}}{\\tilde{x}^{k , j-1}}_{r_{k+1}}\\text{\\texttt { ) } } } }    - \\int_t^{r_{k+1 } } { \\tilde{z}^{k , j}}_s \\cdot { \\mathrm{d}}w_s \\\\   & & \\hspace{15pt}+ \\int_{t}^{r_{k+1 } } f\\bigl({\\tilde{x}^{k , j-1}}_{s},{\\tilde{y}^{k , j}}_{s},{\\tilde{z}^{k , j}}_s,{\\ensuremath { [ { \\tilde{x}^{k , j-1}}_s,{\\tilde{y}^{k , j}}_s ] } } \\bigr){\\mathrm{d}}s \\ , ,     \\end{array }    \\right .     \\end{aligned}\\ ] ] for @xmath141 and with    @xmath142 } } ) { \\mathrm{d}}s +   \\int_{r_k}^t \\sigma(x^{k,0}_s , { \\ensuremath { [ x^{k,0}_s ] } } ) { \\mathrm{d}}w_s \\bigr)_{r_{k } \\le s \\le r_{k+1}}$ ] ,    and @xmath143 .",
    "we then define @xmath144(}}\\xi\\text{\\texttt { ) } } } } : = y^{k , j}_{r_{k } } \\text { and } \\epsilon^{k}(\\xi ) : =   y^{k , j}_{r_{k } } - { \\mathcal{u}}(r_{k},\\xi,{\\ensuremath { [ \\xi ] } } ) \\;,\\end{aligned}\\ ] ] where @xmath145 is the number of picard iterations . at level @xmath146 , which is the last level for our recursive algorithm , the picard iteration scheme is given by @xmath147 } } \\bigr){\\mathrm{d}}s    \\\\    & & \\hspace{15pt }   + \\int^t_{r_{n-1}}\\sigma\\bigl({\\tilde{x}^{n-1,j}}_s ,   { \\ensuremath { [ { \\tilde{x}^{n-1,j}}_s ] } }   \\bigr ) { \\mathrm{d}}w_s\\;,\\;\\\\   { \\tilde{y}^{n-1,j}}_t & = & g({\\tilde{x}^{n-1,j-1}}_t,{\\ensuremath { [ { \\tilde{x}^{n-1,j-1}}_t ] } } )    - \\int_t^{t } { \\tilde{z}^{n-1,j}}_s \\cdot { \\mathrm{d}}w_s\\ ,   \\\\    & & \\hspace{15pt}+ \\int_t^tf\\bigl({\\tilde{x}^{n-1,j-1}}_{s},{\\tilde{y}^{n-1,j}}_{s},{\\tilde{z}^{n-1,j}}_s,{\\ensuremath { [ { \\tilde{x}^{n-1,j-1}}_{s},{\\tilde{y}^{n-1,j}}_{s } ] } } \\bigr){\\mathrm{d}}s\\ , .",
    "\\end{array }    \\right .",
    "\\end{aligned}\\ ] ] here , the terminal condition @xmath74 is known and the error comes from the fact that the picard iteration is stopped .",
    "it is then natural to set , for @xmath148,@xmath149(}}\\xi\\text{\\texttt { ) } } } } & = g(\\xi,{\\ensuremath { [ \\xi ] } } ) \\text { and } \\epsilon^n(\\xi ) = 0\\ , .",
    "\\label{eq error solver very last level}\\end{aligned}\\ ] ]    [ [ practical - implemention . ] ] practical implemention .",
    "+ + + + + + + + + + + + + + + + + + + + + + +    as already noticed in 3 ) above , it is not possible to solve the backward and forward equations in perfectly , even though the system is decoupled .",
    "hence , we need to introduce an approximation that can be implemented in practice . given a continuous adapted input process @xmath150 such that @xmath151",
    "< \\infty$ ] and @xmath152 , we thus would like to solve @xmath153 } } \\bigr ) { \\mathrm{d}}s + \\int_{r_k}^t \\sigma\\bigl(\\tilde{x}_s,{\\ensuremath { [ \\tilde{x}_s ] } }     \\bigr){\\mathrm{d}}w_s    \\\\",
    "\\tilde{y}_t & = & \\eta + \\int_{t}^{r_{k+1 } } f\\bigl(\\mathfrak{x}_{s},\\tilde{y}_{s},\\tilde{z}_s,{\\ensuremath { [ \\mathfrak{x}_{s},\\tilde{y}_s ] } } \\bigr){\\mathrm{d}}s -\\int_t^{r_{k+1 } } \\tilde{z}_s \\cdot { \\mathrm{d}}w_s\\ ; ,    \\end{array }    \\right .",
    "\\end{aligned}\\ ] ] for @xmath122 $ ] .",
    "let @xmath10 be a discrete time grid of @xmath82 $ ] such that @xmath154 , @xmath155 for @xmath110 , we note @xmath156 and for later use , we define the indices @xmath157 as follows @xmath158 for all @xmath159 .",
    "so , instead of a perfect solver for an iteration of the picard scheme , we assume that we are given a numerical solver , denoted by @xmath160(}}\\bar{\\mathfrak{x}}\\text{\\texttt{,}}{\\eta}\\text{\\texttt{,}}f\\text{\\texttt { ) } } } } $ ] , which computes an approximation of the process @xmath161 on @xmath162 for a discretization @xmath163 of the time continuous process @xmath164 .",
    "the _ output _ is denoted by @xmath165 . in parallel , we call _ input _ the triplet formed by the random variable @xmath166 , the discrete - time process @xmath167 and the driver @xmath71 of the backward equation . in short ,",
    "the output is what the numerical solver returns after one iteration in the picard scheme when the discrete input is @xmath168 .",
    "pay attention that , in contrast with @xmath51 and @xmath49 , we shall allow @xmath71 to vary ; this is the rationale for regarding it as an input .",
    "however , when the value of @xmath71 is clear , we shall just regard the input as the pair @xmath169 .",
    "the full convergence analysis , including the discretization error , will be discussed in the next section in the following two cases : first for a generic ( or abstract ) solver @xmath170 and second for an explicit solver , as given in the example below .",
    "[ ex binomial tree ] this example is the prototype of the solver .",
    "we consider an approximation of the brownian motion obtained by quantization of the brownian increments . at every time @xmath171 ,",
    "we denote by @xmath172 the value at time @xmath84 of the discretized brownian motion .",
    "it may expressed as @xmath173 where @xmath174 @xmath175 mapping @xmath12 onto a finite grid of @xmath12 .",
    "importantly , @xmath175 is assumed to be bounded by @xmath75 and each @xmath176 is assumed to be centered and to have the identity matrix as covariance matrix .",
    "of course , this is true if @xmath175 is of the form @xmath177 where @xmath178 is a bounded odd function from @xmath179 onto a finite subset of @xmath179 with a normalized second order moment under the standard gaussian measure . in practice ,",
    "@xmath175 is intended to take a small number of values .",
    "of course , the typical example is the so - called _ binomial approximation _ , in which case @xmath178 is the sign function .    on each interval",
    "@xmath180 $ ] , given a discrete - time input process @xmath181 and a terminal condition @xmath166 , we thus implement the following scheme ( below , @xmath182 is the conditional expectation given @xmath183 ) :    1 .   for the backward component : 1 .   set as terminal condition , @xmath184 .",
    "2 .   for @xmath185 ,",
    "compute recursively @xmath186 } } \\bigr ) \\right ] } } \\ , ,   \\quad & { \\bar{z}}{}_{{t_{i } } } = { \\ensuremath {      \\mathbb{e}_{{t_{i } } } \\ , \\!\\!\\left[\\frac{\\delta \\bar{w}_i } { { t_{i + 1}}-{t_{i } } } { \\bar{y}}{}_{{t_{i + 1}}}\\right ] } } \\,.\\ ] ] 2 .   for the forward component : 1 .   set as initial condition , @xmath187 .",
    "2 .   for @xmath188 ,",
    "compute recursively + @xmath189 } } \\bigr)({t_{i + 1}}-{t_{i } } ) + \\sigma\\bigl({\\bar{x}}_{{t_{i } } } , { \\ensuremath { [ { \\bar{x}}_{{t_{i } } } ] } } \\bigr)\\delta \\bar{w}_i\\;.\\end{aligned}\\ ] ]    [ [ full - algorithm - for - ensuremath - texttextttsolvertexttexttttexttexttt . ] ] full algorithm for @xmath190(}}\\text{\\texttt { ) } } } } $ ] . + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    using @xmath170 , for each level , we can now give a completely implementable algorithm for @xmath190(}}\\text{\\texttt { ) } } } } $ ] .",
    "its description is as follows .    the value @xmath191(}}\\xi\\text{\\texttt { ) } } } } $ ] , i.e. the value of the solver at level @xmath118 with initial condition @xmath192 , is obtained through :    1 .",
    "initialize the backward component at @xmath193 for @xmath194 and regard @xmath195 as the forward component of @xmath160(}}\\xi\\text{\\texttt{,}}0\\text{\\texttt{,}}0\\text{\\texttt { ) } } } } $ ] 2 .   for @xmath196 1 .",
    "compute @xmath197(}}\\bar{x}^{k , j-1}_{r_{k+1}}\\text{\\texttt { ) } } } } $ ] .",
    "2 .   compute @xmath198(}}\\bar{x}^{k , j-1}\\text{\\texttt{,}}\\bar{y}^{k , j}_{r_{k+1}}\\text{\\texttt{,}}f\\text{\\texttt { ) } } } } $ ] 3 .",
    "return @xmath199 .",
    "following , we let @xmath200(}}\\xi\\text{\\texttt { ) } } } } & = g(\\xi,{\\ensuremath { [ \\xi ] } } ) \\ , .",
    "\\label{eq error solver very last level solver}\\end{aligned}\\ ] ]    we first explain the initialization step .",
    "the basic idea is to set the backward component to @xmath201 and then to solve the forward component as an approximation of the autonomous mckean - vlasov diffusion process in which the backward entry is null .",
    "of course , this may be solved by means of any standard method , but to make the notation shorten , we felt better to regard the underlying solver as a specific case of a forward - backward solver with null coefficients in the backward equation .",
    "we specify in the analysis below the conditions that this initial solver @xmath202(}}\\text{\\texttt{,}}0\\text{\\texttt{,}}0\\text{\\texttt { ) } } } } $ ] must satisfy .",
    "it is also worth noting that each picard iteration used to define the solver at level @xmath118 calls the solver at level @xmath136 .",
    "this is a typical feature of the way the continuation method manifests from the algorithmic point of view . in particular , the total complexity is of order @xmath203 , where @xmath204 is the complexity of the solver @xmath170 . in this regard",
    ", it must be stressed that , for a given length @xmath32 , @xmath205 is fixed , regardless of the time step @xmath206 . also , @xmath129 is intended to be rather small as the picard iterations are expected to converge geometrically fast , see the numerical examples in section [ se numerics ] in which we choose @xmath207 .",
    "however , it must be noticed that the complexity increases exponentially fast when @xmath32 tends to @xmath208 , which is obviously the main drawback of this method .",
    "again , we refer to section [ se numerics ] for numerical illustrations .",
    "[ [ useful - notations . ] ] useful notations .",
    "+ + + + + + + + + + + + + + + + +    throughout the paper , @xmath209 denotes the @xmath210 norm on @xmath211 .",
    "also , @xmath212 stands for a copy of @xmath213 .",
    "it is especially useful to represent the lions derivative of a function of a probability measure and to distinguish the ( somewhat artificial ) space used for representing these derivatives from the ( physical ) space carrying the wiener process . for a random variable @xmath214 defined on @xmath211",
    ", we shall denote by @xmath215 its copy on @xmath212 .",
    "we shall use the notations @xmath216 for constants only depending on @xmath75 ( and possibly on the dimension as well ) .",
    "they are allowed to increase from line to line .",
    "we shall use the notation @xmath217 for constants not depending upon the discretization parameters .",
    "again , they are allowed to increase from line to line . in most of the proofs ,",
    "we shall just write @xmath217 for @xmath218 , even if we use the more precise notation @xmath218 in the corresponding statement .      to conclude this section , we want to understand how the error propagates through the solvers used at different levels in the ideal case where the picard iteration in can be perfectly computed or equivalently when the solver is given by @xmath191(}}\\text{\\texttt { ) } } } } { } = { \\ensuremath {   \\text{\\texttt{picard[}}k\\text{\\texttt{](}}\\text{\\texttt { ) } } } } $ ] . for @xmath141 , we then denote by @xmath219 , the solution on @xmath180 $ ] of .",
    "the main result of the section , see theorem [ th error picard ] , is an upper bound for the error when we use @xmath220(}}\\cdot\\text{\\texttt { ) } } } } $ ] to approximate @xmath17 .",
    "the proof of this theorem requires the following proposition , which gives a local error estimate for each level .",
    "[ pr loc error picard ] let us define , for @xmath221 , @xmath222 , @xmath223}\\bigl({\\tilde{y}^{k , j}}_t - { \\mathcal{u}}(t,{\\tilde{x}^{k , j}}_t,{\\ensuremath { [ { \\tilde{x}^{k , j}}_t ] } } ) \\bigr ) \\bigr\\|_2\\ ] ] then , there exist constants @xmath216 such that , for @xmath224 , @xmath225    we recall that @xmath226 stands for the error term : @xmath227(}}\\xi\\text{\\texttt { ) } } } } - { \\mathcal{u}}(r_{k},\\xi,{\\ensuremath { [ \\xi ] } } ) \\ ; ,   \\quad \\textrm{with } \\",
    "\\epsilon^{n}(\\xi ) = 0 \\ ; .\\end{aligned}\\ ] ]    [ re z meme pas peur ] a careful inspection of the proof shows that , whenever @xmath49 depends on @xmath53 or @xmath51 depends on @xmath98 , the same result holds true but with a constant @xmath218 depending on @xmath205 .",
    "as @xmath205 is fixed in practice , this might still suffice to complete the analysis of the discretization scheme in that more general setting .",
    "we suppose that the full algorithm is initialized at some level @xmath228 , with an initial condition @xmath192 . as the value of the index @xmath118",
    "is fixed throughout the proof , we will drop it in the notations @xmath229 and @xmath230 .",
    "+ applying ito s formula for functions of a measure argument , see @xcite , we have @xmath231 } } )    & = \\bigg (   b({\\tilde{x}^{j}}_{t},{\\tilde{y}^{j}}_t,{\\ensuremath { [ { \\tilde{x}^{j}}_t,{\\tilde{y}^{j}}_t ] } } ) \\cdot \\partial_x { \\mathcal{u}}(t,{\\tilde{x}^{j}}_t,{\\ensuremath { [ { \\tilde{x}^{j}}_t ] } } )   \\\\ & \\hspace{15pt }     + \\frac12 { \\rm tr }   \\bigl [ a\\bigl({\\tilde{x}^{j}}_t,{\\ensuremath { [ { \\tilde{x}^{j}}_t ] } } \\bigr ) \\partial^2_{xx } { \\mathcal{u}}(t,{\\tilde{x}^{j}}_t,{\\ensuremath { [ { \\tilde{x}^{j}}_t ] } } ) \\bigr ]   \\\\   & \\hspace{15pt } + \\hat{\\ee } \\bigl [ b({\\ensuremath { \\langle { \\tilde{x}^{j}}_t \\rangle } } , { \\ensuremath { \\langle { \\tilde{y}^{j}}_t \\rangle } } , { \\ensuremath { [ { \\tilde{x}^{j}}_t,{\\tilde{y}^{j}}_t ] } } )    \\cdot \\partial_\\mu { \\mathcal{u}}(t,{\\tilde{x}^{j}}_t,{\\ensuremath { [ { \\tilde{x}^{j}}_t ] } } ) \\bigr ]   \\\\ & \\hspace{15pt }    + \\hat{\\ee } \\bigl [ \\frac12 { \\rm tr } \\bigl [   a\\bigl({\\ensuremath { \\langle { \\tilde{x}^{j}}_t \\rangle } } , { \\ensuremath { [ { \\tilde{x}^{j}}_t ] } } \\bigr ) \\partial_\\upsilon \\partial_\\mu { \\mathcal{u}}(t,{\\tilde{x}^{j}}_t,{\\ensuremath { [ { \\tilde{x}^{j}}_t ] } } ) \\bigr ]    \\bigr ] + \\partial_t { \\mathcal{u}}(t,{\\tilde{x}^{j}}_t,{\\ensuremath { [ { \\tilde{x}^{j}}_t ] } } ) \\biggr){\\mathrm{d}}t   \\\\ & \\hspace{5pt}+ \\partial_x { \\mathcal{u}}(t,{\\tilde{x}^{j}}_t,{\\ensuremath { [ { \\tilde{x}^{j}}_t ] } } ) \\cdot \\bigl ( \\sigma\\bigl({\\tilde{x}^{j}}_t,{\\ensuremath { [ { \\tilde{x}^{j}}_t ] } } \\bigr ) { \\mathrm{d}}w_t \\bigr ) \\ , .   \\end{aligned}\\ ] ] expressing the integral in as expectations on @xmath212 and combining with and , we obtain @xmath232    & = \\big (   \\bigl\\{b\\bigl({\\tilde{x}^{j}}_t,{\\tilde{y}^{j}}_t,{\\ensuremath { [ { \\tilde{x}^{j}}_t,{\\tilde{y}^{j}}_t ] } } \\bigr ) - b\\bigl({\\tilde{x}^{j}}_t,\\check{y}^j_t,{\\ensuremath { [ { \\tilde{x}^{j}}_t,\\check{y}^j_t ] } } \\bigr)\\bigr\\ }   \\cdot    \\partial_x { \\mathcal{u}}(t,{\\tilde{x}^{j}}_t,{\\ensuremath { [ { \\tilde{x}^{j}}_t ] } } )    \\\\ & \\hspace{5pt } + { \\ensuremath {      \\widehat{\\mathbb{e}}\\!\\left [ \\bigl\\{b\\bigl({\\ensuremath { \\langle { \\tilde{x}^{j}}_t \\rangle } } , { \\ensuremath { \\langle { \\tilde{y}^{j}}_t \\rangle } } , { \\ensuremath { [ { \\tilde{x}^{j}}_t,{\\tilde{y}^{j}}_t ] } } \\bigr ) - b\\bigl({\\ensuremath { \\langle { \\tilde{x}^{j}}_t \\rangle } } , { \\ensuremath { \\langle \\check{y}^j_t \\rangle } } , { \\ensuremath { [ { \\tilde{x}^{j}}_t,\\check{y}^j_t ] } } \\bigr)\\bigr\\ } \\cdot   \\partial_\\mu { \\mathcal{u}}(t,{\\tilde{x}^{j}}_t,{\\ensuremath { [ { \\tilde{x}^{j}}_t ] } } )   \\right ] } }   \\\\   & \\hspace{5pt } + f\\bigl({\\tilde{x}^{j}}_t,{\\tilde{y}^{j}}_t,{\\tilde{z}^{j}}_t,{\\ensuremath { [ { \\tilde{x}^{j}}_t,{\\tilde{y}^{j}}_t ] } } \\bigr ) - f\\bigl({\\tilde{x}^{j}}_t,\\check{y}^j_{t},\\check{z}^j_t,{\\ensuremath { [ { \\tilde{x}^{j}}_t,\\check{y}^j_t ] } } \\bigr ) \\big ) { \\mathrm{d}}t + [ \\check{z}^j_t - { \\tilde{z}^{j}}_t ] \\cdot { \\mathrm{d}}w_t\\ , , \\ ] ] where @xmath233 } } ) $ ] and @xmath234 } } ) \\sigma({\\tilde{x}^{j}}_t,{\\ensuremath { [ { \\tilde{x}^{j}}_t ] } } ) $ ] . observe that this argument is reminiscent of the four - step scheme , see @xcite . + using standard arguments from bsde theory and @xmath73@xmath78 , we then compute @xmath235 } } ) - { \\tilde{y}^{j}}_{r_{k+1 } } \\bigr\\|_{2 }   \\\\   & \\le e^{c \\delta } \\bigl ( \\bigl\\| \\epsilon^{k+1}({\\tilde{x}^{j-1}}_{r_{k+1 } } )    \\bigr\\|_{2 } + \\bigl\\| { \\mathcal{u}}(r_{k+1},{\\tilde{x}^{j}}_{r_{k+1}},{\\ensuremath { [ { \\tilde{x}^{j}}_{r_{k+1 } } ] } } ) - { \\mathcal{u}}(r_{k+1},{\\tilde{x}^{j-1}}_{r_{k+1}},{\\ensuremath { [ { \\tilde{x}^{j-1}}_{r_{k+1 } } ] } } ) \\bigr\\|_{2 } \\bigr),\\end{aligned}\\ ] ] recalling @xmath236(}}{\\tilde{x}^{j-1}}_{r_{k+1}}\\text{\\texttt { ) } } } } $ ] and .",
    "since @xmath17 is lipschitz , we have @xmath237 we also have that @xmath238 } } \\bigr ) & - b\\bigl({\\tilde{x}^{j-1}}_s,{\\tilde{y}^{j-1}}_s,{\\ensuremath { [ { \\tilde{x}^{j-1}}_t,{\\tilde{y}^{j-1}}_t ] } } \\bigr)\\bigr\\ } { \\mathrm{d}}s   \\\\   & \\hspace{-40pt } + \\int_{r_k}^t \\bigl\\ { \\sigma\\bigl({\\tilde{x}^{j}}_s , { \\ensuremath { [ { \\tilde{x}^{j}}_s ] } }   \\bigr ) - \\sigma\\bigl({\\tilde{x}^{j-1}}_s , { \\ensuremath { [ { \\tilde{x}^{j-1}}_s ] } } \\bigr)\\bigr\\ } { \\mathrm{d}}w_s \\;.\\end{aligned}\\ ] ] using usual arguments ( squaring , taking the sup , using brkholder - davis - gundy inequality ) , we get , since @xmath51 and @xmath49 are lipschitz continuous , @xmath239 } \\vert { \\tilde{x}^{j}}_t - { \\tilde{x}^{j-1}}_t \\vert    \\bigr\\|_{2 }   \\le c \\bigl ( \\delta    \\bigl\\|    \\sup_{t\\in [ r_k , r_{k+1 } ] } \\vert { \\tilde{y}^{j}}_t - { \\tilde{y}^{j-1}}_t \\vert   \\bigr\\|_{2 }   + \\delta^{\\frac12 }    \\bigl\\|    \\sup_{t\\in [ r_k , r_{k+1 } ] } \\vert { \\tilde{x}^{j}}_t - { \\tilde{x}^{j-1}}_t \\vert   \\bigl\\|_{2 }   \\bigr ) \\ , .\\end{aligned}\\ ] ] observing that @xmath240 } } ) | + |{\\tilde{y}^{j-1}}_s - { \\mathcal{u}}(s,{\\tilde{x}^{j-1}}_s,{\\ensuremath { [ { \\tilde{x}^{j-1}}_s ] } } ) |     \\\\    & \\hspace{15pt}+ \\lambda(|{\\tilde{x}^{j-1}}_s-{\\tilde{x}^{j}}_s|+ \\|{\\tilde{x}^{j-1}}_s-{\\tilde{x}^{j}}_s \\|_2 ) \\ , , \\end{aligned}\\ ] ] we obtain , for @xmath109 small enough , @xmath241 } \\vert { \\tilde{x}^{j}}_t - { \\tilde{x}^{j-1}}_t \\vert \\bigr\\|_{2 } \\le c \\delta ( \\delta^j + \\delta^{j-1})\\ , .\\end{aligned}\\ ] ] combining the previous inequality with , we obtain , for @xmath109 small enough , @xmath242 which by induction leads to @xmath243 and concludes the proof .",
    "we now state the main result of this section , which explains how the local error induced by the fact that the picard iteration is stopped at rank @xmath129 propagates through the various levels @xmath244 .    [",
    "th error picard ] we can find two constants @xmath245 and a continuous non - decreasing function @xmath246 matching @xmath201 in @xmath201 , only depending on @xmath75 , such that , for @xmath247 and @xmath248 satisfying @xmath249 where @xmath129 is the number of picard iterations in a period , it holds , for any period @xmath250 and @xmath192 , @xmath251(}}\\xi\\text{\\texttt { ) } } } } - { \\mathcal{u}}(r_k,\\xi,{\\ensuremath { [ \\xi ] } } ) } \\right \\|_2 } \\le \\lambda    \\frac{e^{\\beta c_{\\lambda } t}}{\\beta }       { \\bar{\\delta}}^{j-1}\\bigl(1+{\\left \\|{p_{r_{k},t}^\\star(\\xi ) } \\right \\|_2}\\bigr ) \\ ; ,   \\end{aligned}\\ ] ] where @xmath252 is the solution at time @xmath84 of the stochastic differential equation @xmath253 } } \\bigr ) { \\mathrm{d}}s +   \\sigma \\bigl ( x_{s}^0,{\\ensuremath { [ x_{s}^0 ] } } \\bigr ) { \\mathrm{d}}w_{s } \\",
    ", , \\ ] ] with @xmath254 as initial condition , and @xmath255 } \\vert p_{r_{k},s}(\\xi ) \\vert$ ] .    of course , it is absolutely straightforward to bound @xmath256 by @xmath257 in .",
    "theorem [ th error picard ] may be restated accordingly , but the form used in the statement is more faithful to the spirit of the proof .",
    "we prove the claim by an induction argument .",
    "we show below that for all @xmath258 , @xmath259(}}\\xi\\text{\\texttt { ) } } } } - { \\mathcal{u}}(r_k,\\xi,{\\ensuremath { [ \\xi ] } } ) } \\right \\|_2 } \\le \\theta_{k}\\left(1+{\\left \\|{p_{r_{k},t}^\\star(\\xi ) } \\right \\|_2}\\right ) \\ , , \\end{aligned}\\ ] ] where @xmath260 is defined by the following backward induction : @xmath261 , recall , and for @xmath228 , @xmath262 where @xmath263 is such that @xmath264 with this definition , we have , for all @xmath265 , @xmath266 which gives the expected result .",
    "+ we now prove .",
    "observe that it is obviously true for the last step @xmath205 .",
    "assume now that it holds true at step @xmath136 , for @xmath159 , and that holds true for @xmath267 .",
    "then , using , we have @xmath268 from proposition [ pr loc error picard ] , we have @xmath269 using the induction hypothesis , we compute @xmath270 we study the last sum . observe that for @xmath271 , @xmath272 we observe that @xmath273 , for @xmath274 $ ] . hence , @xmath275 .",
    "also , it is well - checked that there exists a constant @xmath218 such that each @xmath276 is @xmath218-lipschitz continuous from @xmath277 into @xmath278 .",
    "then , @xmath279 using in the proof of proposition [ pr loc error picard ] and changing the definition of @xmath280 , we obtain @xmath281 observing that , for all @xmath282 , @xmath283 we get @xmath284 where @xmath285 . inserting the previous estimate into and changing @xmath280 into @xmath286 , we obtain @xmath287 we note that @xmath288 . recalling @xmath289 in , equation leads to @xmath290 where we set @xmath291 we have @xmath292 and then @xmath293 we compute @xmath294 which combined with the properties and leads to , for all @xmath295 , @xmath296 where we recall that @xmath297 .",
    "we insert the previous inequality into for @xmath298 and get @xmath299 using , this rewrites @xmath300 and validates and thus .",
    "we then obviously have that holds true .",
    "we now study the convergence of a generic implementable solver @xmath190(}}\\text{\\texttt { ) } } } } $ ] , based upon the local solver @xmath170 as described above , as long as the output of the local solver @xmath301 satisfies some conditions , which are shown to be true for example [ ex binomial tree ] .    in order to define the required assumption",
    ", we use the same letters @xmath75 and @xmath95 as in @xmath73 and @xmath78 , except that , without any loss of generality , we assume that @xmath95 is greater than 1 . for the same coefficients as in the equation , and in particular for the same driver @xmath71 , we then ask @xmath160(}}\\text{\\texttt{,}}\\text{\\texttt{,}}\\text{\\texttt { ) } } } } $ ] to satisfy the following three conditions .",
    "@xmath302 } } ) -{\\bar{y}}_t } \\bigr \\|_{2\\alpha } } \\le e^{\\lambda \\delta } { \\bigl \\|{{\\mathcal{u}}(r_{k+1},{\\bar{x}}_{r_{k+1}},{\\ensuremath { [ { \\bar{x}}_{r_{k+1 } } ] } } ) -{\\bar{y}}_{r_{k+1 } } } \\bigr \\|_{2\\alpha } }   \\\\ & \\quad\\quad \\quad\\quad\\quad\\quad\\quad\\quad \\quad\\quad\\quad   + \\lambda \\max_{j_k \\le i < j_{k+1}}{\\bigl \\|{{\\bar{x}}_{t_i } - \\bar{\\mathfrak{x}}_{t_i } } \\bigr \\|_{2\\alpha } } + \\mathcal{d}^1(|\\pi| ) + \\mathcal{d}^2(|\\pi|)\\bigl(1 +   { \\|{\\xi }   \\|_{2\\alpha}}^{\\alpha}\\bigr)\\ , , \\\\ ( a2 ) & \\quad    \\sup_{t \\in \\pi^k } { \\bigl \\|{{\\bar{x}}_t-{\\bar{x}}'_t } \\bigr \\|_{2\\alpha } } \\le \\lambda \\delta \\sup_{t \\in \\pi^k}{\\bigl \\|{{\\bar{y}}_t-{\\bar{y}}'_t } \\bigr \\|_{2\\alpha}}\\ , , \\\\ ( a3 ) & \\quad   { \\bigl \\|{{\\mathcal{u}}(r_{k+1},{\\bar{x}}_{r_{k+1}},{\\ensuremath { [ { \\bar{x}}_{r_{k+1 } } ] } } ) -{\\bar{y}}_{r_{k+1 } } } \\bigr \\|_{2\\alpha}}^{\\alpha }   \\le \\lambda   { \\bigl \\|{{\\mathcal{u}}(r_{k+1},{\\bar{x}}_{r_{k+1}},{\\ensuremath { [ { \\bar{x}}_{r_{k+1 } } ] } } ) -{\\bar{y}}_{r_{k+1 } } } \\bigr \\|_{2\\alpha}}\\ , , \\end{aligned}\\ ] ] where @xmath303(}}\\bar{\\mathfrak{x}}\\text{\\texttt{,}}\\eta\\text{\\texttt{,}}f\\text{\\texttt { ) } } } } $ ] , for @xmath71 as before , and @xmath304(}}\\bar{\\mathfrak{x}}'\\text{\\texttt{,}}\\eta'\\text{\\texttt{,}}f'\\text{\\texttt { ) } } } } $ ] , for another @xmath305 either equal to @xmath71 or @xmath201 , are two output values of @xmath170 associated to two input processes @xmath181 , @xmath306 , with the same initial condition @xmath307 , and to two different terminal conditions @xmath308 and @xmath309 . for @xmath310 ,",
    "the function @xmath311 is a discretization error associated to the use of the grid @xmath10 , which satisfies @xmath312 .",
    "importantly , both @xmath313 and @xmath314 are independent of @xmath181 , @xmath315 , @xmath129 and @xmath205 .    in full analogy with the discussion right below theorem [ th error picard ] , we shall also need some conditions on the solver @xmath160(}}\\text{\\texttt{,}}0\\text{\\texttt{,}}0\\text{\\texttt { ) } } } } $ ] used to initialize the algorithm at each step .",
    "following the definition of @xmath316 introduced in the statement of theorem [ th error picard ] , we let by induction , for a given @xmath228 : @xmath317(}}\\xi\\text{\\texttt{,}}0\\text{\\texttt{,}}0\\text{\\texttt { ) } } } } \\bigr)^1_{t } , \\quad t \\in \\pi^{k } \\",
    ", , \\quad \\xi \\in l^2({\\mathcal{f}}_{r_{k } } ) \\ , , \\end{split}\\ ] ] where we recall that @xmath318(}}\\xi\\text{\\texttt{,}}0\\text{\\texttt{,}}0\\text{\\texttt { ) } } } } \\bigr)^1 $ ] is the forward component of the algorithm s output , and , for @xmath319 , @xmath320 and then @xmath321 } \\vert { \\tt p}_{r_k , s}(\\xi)\\vert$ ] , for @xmath112 . it then makes sense to assume @xmath322 where @xmath323 and @xmath324 .",
    "the main challenging assumption ( and maybe the most surprising one ) is @xmath325 .",
    "it is obviously satisfied when @xmath326 as long as @xmath75 is assumed to be greater than 1 .",
    "we refer to @xcite and ( * ? ? ?",
    "12 ) for sets of conditions under which this is indeed true .",
    "when @xmath97 , assumption @xmath325 is checked provided we have an _ a priori bound _ on @xmath327 } } ) -{\\bar{y}}_{r_{k+1}}\\|_{2\\alpha}$ ] , see lemma [ le : a3 ] .",
    "this permits to invoke the result proven in our previous paper @xcite , which holds true in a weaker setting than the solvability results obtained in @xcite and ( * ? ? ?",
    "[ th propagation error ii ] we can find two constants @xmath245 and a continuous non - decreasing function @xmath246 matching @xmath201 in @xmath201 , only depending on @xmath75 , such that , for @xmath247 and @xmath248 satisfying @xmath328 where @xmath129 is the number of picard iterations in a period , it holds , for any period @xmath250 and @xmath192 , @xmath329(}}\\xi\\text{\\texttt { ) } } } } - { \\mathcal{u}}(r_k,\\xi,{\\ensuremath { [ \\xi ] } } ) } \\bigr \\|_{2\\alpha } } & \\le c     \\left({\\bar{\\delta}}^{j-1}+ ( n - k){\\mathcal{d}}^2(|\\pi|)\\right)\\bigl ( 1 + { \\|{\\xi }   \\|_{2\\alpha}}^\\alpha    \\bigr ) + c(n - k){\\mathcal{d}}^1(|\\pi| ) \\ ; ,   \\end{aligned}\\ ] ] for a constant @xmath217 independent of the discretization parameters .",
    "the proof will follow closely the proof of theorem [ th error picard ] but we now have to take into account the discretization error . we will first show that for all @xmath330 , @xmath331 where @xmath332(}}\\xi\\text{\\texttt { ) } } } } - { \\mathcal{u}}(r_k,\\xi,{\\ensuremath { [ \\xi ] } } ) \\ , , \\ ] ] and @xmath333 is defined by the following backward induction : @xmath334 , recall , and for @xmath228 , @xmath335 @xmath263 being defined as in equation . + assume for a while that thids holds true . then",
    ", we have , for all @xmath336 , @xmath337 recalling that @xmath338 , we get the announced inequality .    we now prove .",
    "obviously , it holds true for the last step @xmath205 .",
    "assume now that it is true at step @xmath136 , for @xmath159 and that holds for @xmath267 and @xmath339 .",
    "+ in particular , using , we observe that @xmath340 _ first step . _ for @xmath341 ,",
    "let @xmath342 } } ) -{\\bar{y}}^{k , j}_t } \\bigr \\|_{2\\alpha } } \\ , .\\ ] ] under @xmath343 , we will prove in this first step an upper bound for @xmath344 , for @xmath345 , similar to the one obtained in proposition [ pr loc error picard ] .    using @xmath346 and @xmath78 and the fact that @xmath347 } } \\bigr ) + \\epsilon^{k+1}({\\bar{x}}^{k , j-1}_{r_{k+1}})\\,,\\ ] ]",
    "we observe that @xmath348 } } \\bigr ) -{\\mathcal{u}}\\bigl(r_{k+1},{\\bar{x}}^{k , j-1}_{r_{k+1}},{\\ensuremath { [ { \\bar{x}}^{k , j-1}_{r_{k+1 } } ] } } \\bigr ) } \\bigr \\|_{2\\alpha } }   \\right .   \\nonumber \\\\ & \\hspace{15pt } \\left .",
    "+ { \\bigl \\| {   \\epsilon^{k+1}({\\bar{x}}^{k , j-1}_{r_{k+1 } } ) } \\bigr \\|_{2\\alpha } } \\right ] + \\lambda \\max_{j_k \\le i < j_{k+1}}{\\bigl \\|{{\\bar{x}}^{k , j}_{t_i } - { \\bar{x}}^{k , j-1}_{t_i } } \\bigr \\|_{2\\alpha } }    + \\mathcal{d}^1(|\\pi| ) +   \\mathcal{d}^2(|\\pi|)\\bigl(1 +    { \\bigl \\|{\\xi } \\bigr \\|_{2\\alpha}}^\\alpha   \\bigr ) \\nonumber \\\\ & \\le c_\\lambda \\max_{t \\in \\pi^k}{\\bigl \\| { { \\bar{x}}^{k , j}_{t } - { \\bar{x}}^{k , j-1}_{t } } \\bigr \\|_{2\\alpha } } +              e^{\\lambda \\delta }             { \\bigl \\| {   \\epsilon^{k+1}({\\bar{x}}^{k , j-1}_{r_{k+1 } } ) } \\bigr \\|_{2\\alpha } }       \\label{eq temp analysis ii } \\\\ & \\hspace{15pt }   + \\mathcal{d}^1(|\\pi| ) + \\mathcal{d}^2(|\\pi|)\\bigl(1 +    { \\bigl \\|{{\\tt p}_{r_{k},t}^\\star(\\xi ) } \\bigr \\|_{2\\alpha}}^\\alpha \\bigr)\\ , .           \\nonumber\\end{aligned}\\ ] ] using @xmath349 , we also have @xmath350 } } ) } \\bigr \\|_{2\\alpha } } + \\lambda { \\bigl \\|{{\\bar{x}}^{k , j}_t - { \\bar{x}}^{k , j-1}_t } \\bigr \\|_{2\\alpha } } \\right .",
    "\\nonumber \\\\    &      \\left .          \\quad\\quad\\quad\\quad\\quad\\quad\\quad",
    "+ { \\bigl \\| { { \\mathcal{u}}(t,{\\bar{x}}^{k , j-1}_t,{\\ensuremath { [ { \\bar{x}}^{k , j-1}_t ] } } ) - { \\bar{y}}^{k , j-1}_t } \\bigr \\|_{2\\alpha } } \\right ]   \\nonumber \\\\ & \\le c_\\lambda \\delta \\left ( \\bar{\\delta}^j_k + \\bar{\\delta}^{j-1}_k \\right ) \\ , , \\nonumber\\end{aligned}\\ ] ] for @xmath109 small enough . inserting the previous inequality in",
    ", we get @xmath351 with @xmath352 .",
    "we note that compared to , there is a new term , namely @xmath353 , which is due to the discretization .",
    "+ _ second step .",
    "_ using at the previous step @xmath136 and noting that @xmath354 , we claim that @xmath355 where @xmath356 .",
    "this corresponds to equation adapted to our context .",
    "by @xmath349 , we have , for @xmath357 , @xmath358 using @xmath359 , we then compute , recalling that @xmath360 , @xmath361 where for the last inequality we used the fact that @xmath109 is small enough . observing that @xmath362 and combining the previous inequality with , we obtain @xmath363 so that , by using the fact that @xmath364 together with a convexity argument , @xmath365 appealing to @xmath325 and redefining @xmath280 , we get @xmath366 which may be rewritten as @xmath367 recalling the notation @xmath368 and letting @xmath369 , we obtain a new version of , namely @xmath370 where we changed the constant @xmath371 in into @xmath372 as we changed the value of @xmath280 , and where we put @xmath373 we straightforwardly deduce that @xmath374 which yields @xmath375 where we used @xmath376 .",
    "thanks to , we get @xmath377 recalling that @xmath378 , we deduce that , for @xmath280 small enough , @xmath379 provided that @xmath263 satisfies @xmath380 this validates and concludes the proof .",
    "we now analyse the global error of our method when the numerical algorithm is given by our benchmark example [ ex binomial tree ] , see section [ subse approx u(t , x , mu ) ] .",
    "[ le stab ] ( scheme stability ) condition @xmath349 holds true for the scheme given in example [ ex binomial tree ] .    for @xmath381 , we consider @xmath382(}}\\bar{\\mathfrak{x}}\\text{\\texttt{,}}\\eta\\text{\\texttt{,}}f\\text{\\texttt { ) } } } } $ ] and @xmath383(}}\\bar{\\mathfrak{x}}'\\text{\\texttt{,}}\\eta'\\text{\\texttt{,}}f'\\text{\\texttt { ) } } } } $ ] with @xmath307 .",
    "letting @xmath384 and @xmath385 , we observe @xmath386 for @xmath387 , where @xmath388 } } ) -b({\\bar{x}}'_{t_\\ell},{\\bar{y}}'_{t_\\ell},{\\ensuremath { [ { \\bar{x}}'_{t_\\ell},{\\bar{y}}'_{t_\\ell } ] } } ) $ ] and , similarly , @xmath389 } } ) -\\sigma({\\bar{x}}'_{t_\\ell},{\\ensuremath { [ { \\bar{x}}'_{t_\\ell } ] } } ) $ ] . + invoking cauchy - schwartz inequality for the first term and the brkholder - davis - gundy inequality for discrete martingales for the second term and appealing to the lipschitz property of @xmath51 and @xmath49 , we get @xmath390 where we used the identity @xmath391 .",
    "for @xmath109 small enough ( taking the sup in the sum ) , we then obtain @xmath392 which concludes the proof .",
    "we now turn to the study of the approximation error .",
    "[ le approx ] assume that @xmath73-@xmath78 are in force .",
    "then , condition @xmath346 holds true for the scheme given in example [ ex binomial tree ] with @xmath393    _ first step .",
    "_ given the scheme defined in example [ ex binomial tree ] , we introduce its piecewise continuous version , which we denote by @xmath394 . for @xmath395 , @xmath396 , @xmath397 with @xmath398 } } ) , \\sigma({\\bar{x}}_{t_i},{\\ensuremath { [ { \\bar{x}}_{t_i } ] } } ) ) $ ] . in preparation for the proof , we also introduce a piecewise cd - lg version , denoted by @xmath399 , where @xmath400 is a parameter in @xmath401 . for @xmath395 , @xmath396 , @xmath402 for the reader s convenience",
    ", we also set @xmath403 } } \\bigr ) \\,,\\ ; \\\\ { \\bar{v}}_s^x & : = \\partial_x { \\mathcal{u}}\\bigl(s , { \\bar{x}}_s,{\\ensuremath { [ { \\bar{x}}_s ] } } \\bigr ) \\,,\\ ;   { \\bar{v}}^{\\mu}_s : = \\partial_\\mu { \\mathcal{u}}\\bigl(s , { \\bar{x}}_s,{\\ensuremath { [ { \\bar{x}}_s ] } } \\bigr)({\\ensuremath { \\langle { \\bar{x}}_s \\rangle } } ) \\ ; , \\\\ { \\bar{v}}^{x,0}_s & : = \\partial_x { \\mathcal{u}}\\bigl(s , { \\bar{x}}^{(0)}_s,{\\ensuremath { [ { \\bar{x}}_s ] } } \\bigr ) \\;.\\end{aligned}\\ ] ] applying the discrete it formula given in proposition [ pr disc ito formula ] , and using the pde solved by @xmath17 , recall , we compute @xmath404 } } \\bigr ) - b\\bigl({\\bar{x}}_{t_{i}},{\\bar{u}}_{t_i } , { \\ensuremath { [ { \\bar{x}}_{t_i},{\\bar{u}}_{t_i } ] } } \\bigr )   \\right\\ } { \\mathrm{d}}s   \\\\ & \\hspace{15pt } + \\int_{t_i}^{t_{i+1 } } { \\ensuremath{\\hat{\\mathbb{e } } \\ ; \\!\\ ! \\left [ { \\bar{v}}_s^\\mu \\cdot \\bigl\\ { { \\ensuremath { \\langle b\\bigl({\\bar{x}}_{t_i},{\\bar{y}}_{t_i},{\\ensuremath { [ { \\bar{x}}_{t_i},{\\bar{y}}_{t_i } ] } } \\bigr ) - b\\bigl({\\bar{x}}_{t_i},{\\bar{u}}_{t_i } , { \\ensuremath { [ { \\bar{x}}_{t_i},{\\bar{u}}_{t_i } ] } } \\bigr ) \\bigr\\ \\rangle } } } \\right ] } } { \\mathrm{d}}s \\\\ & \\hspace{15pt } -(t_{i+1 } - t_i ) f\\bigl(\\bar{\\mathfrak x}_{t_{i}},{\\bar{u}}_{t_{i}},\\sigma^\\dagger \\bigl({\\bar{x}}_{t_i},{\\ensuremath { [ { \\bar{x}}_{t_{i } } ] } } \\bigr ) { \\bar{v}}^x_{t_i},{\\ensuremath { [ \\bar{\\mathfrak{x}}_{t_i},{\\bar{u}}_{t_i } ] } } \\bigr ) \\\\ & \\hspace{15pt}+ { \\bar{v}}_{t_i}^x \\cdot \\bigl ( \\sqrt{t_{i+1}-t_i } \\sigma\\bigl({\\bar{x}}_{t_i},{\\ensuremath { [ { \\bar{x}}_{t_i } ] } } \\bigr ) \\varpi_i \\bigr ) \\\\ & \\hspace{15pt } + { \\mathcal{r}}^{w}_i + { \\mathcal{r}}^f_i   + { \\mathcal{r}}^{bx}_i+{\\mathcal{r}}^{b\\mu}_i + { \\mathcal{r}}^{\\sigma x}_i + { \\mathcal{r}}^{\\sigma \\mu}_i + \\delta { \\mathcal m}(t_{i},t_{i+1})+   \\delta { \\mathcal t}(t_{i},t_{i+1 } ) \\ , , \\end{aligned}\\ ] ] with @xmath405 } } ) \\varpi_i}{2 \\sqrt{s - t_i } } { \\mathrm{d}}s\\ , , \\ ;   \\\\   { \\mathcal{r}}^f_i : = & \\int_{t_i}^{t_{i+1 } } \\left \\ { f\\bigl({\\bar{x}}_{s},{\\bar{u}}_{s } ,    \\sigma^\\dagger\\bigl({\\bar{x}}_s,{\\ensuremath { [ { \\bar{x}}_{s } ] } } \\bigr ) { \\bar{v}}_s^x ,    { \\ensuremath { [ { \\bar{x}}_s,{\\bar{u}}_s ] } } \\bigr ) \\right .    \\\\ & \\hspace{100pt } \\left . - f\\bigl(\\bar{\\mathfrak{x}}_{t_{i}},{\\bar{u}}_{t_{i } } ,   \\sigma^\\dagger\\bigl({\\bar{x}}_{t_i},{\\ensuremath { [ { \\bar{x}}_{t_{i } } ] } } \\bigr ) { \\bar{v}}_{t_i}^x , { \\ensuremath { [ \\bar{\\mathfrak{x}}_{t_i},{\\bar{u}}_{t_i } ] } } \\bigr ) \\right \\ } { \\mathrm{d}}s\\ , ,    \\\\   { \\mathcal{r}}^{bx}_i : = & \\int_{t_i}^{t_{i+1}}{\\bar{v}}_s^x   \\cdot   \\left \\ {   b\\bigl({\\bar{x}}_{t_{i}},{\\bar{u}}_{t_i},{\\ensuremath { [ { \\bar{x}}_{t_i},{\\bar{u}}_{t_i } ] } } \\bigr ) -    b\\bigl({\\bar{x}}_{s},{\\bar{u}}_s,{\\ensuremath { [ { \\bar{x}}_s,{\\bar{u}}_s ] } } \\bigr )   \\right \\ }   { \\mathrm{d}}s \\ , ,   \\\\    { \\mathcal{r}}^{b \\mu}_i   : = & \\int_{t_i}^{t_{i+1 } } { \\ensuremath{\\hat{\\mathbb{e } } \\ ; \\!\\ ! \\left [ { \\bar{v}}_s^\\mu\\cdot \\bigl\\ { { \\ensuremath { \\langle b\\bigl({\\bar{x}}_{t_{i}},{\\bar{u}}_{t_i } ,   { \\ensuremath { [ { \\bar{x}}_{t_i},{\\bar{u}}_{t_i } ] } } \\bigr ) -b\\bigl({\\bar{x}}_{s},{\\bar{u}}_{s } , { \\ensuremath { [ { \\bar{x}}_{s},{\\bar{u}}_{s } ] } } \\bigr )   \\rangle } } \\bigr\\ } \\right ] } } { \\mathrm{d}}s \\ , , \\end{aligned}\\ ] ] and @xmath406 where @xmath407 } } \\bigr )   a({\\bar{x}}_{t_i},{\\ensuremath { [ { \\bar{x}}_{t_i } ] } } )   - \\partial_{xx}^2 { \\mathcal{u}}\\bigl(s,{\\bar{x}}_s,{\\ensuremath { [ { \\bar{x}}_s ] } } \\bigr)a({\\bar{x}}_{s},{\\ensuremath { [ { \\bar{x}}_{s } ] } } ) \\bigr\\ } \\nonumber \\\\",
    "\\delta^\\mu(s,\\lambda ) & : = \\!\\hat{\\ee } \\bigl",
    "[ \\textrm{\\rm tr } \\bigl\\ {   \\partial_v \\partial_\\mu { \\mathcal{u}}\\bigl(s,{\\bar{x}}_s,{\\ensuremath { [ { \\bar{x}}_s ] } } \\bigr)({\\ensuremath { \\langle { \\bar{x}}^{(\\lambda)}_s \\rangle } } ) { \\ensuremath { \\langle a({\\bar{x}}_{t_i},{\\ensuremath { [ { \\bar{x}}_{t_i } ] } } ) \\rangle } }   \\nonumber \\\\ & \\hspace{100pt}- \\partial_v \\partial_\\mu { \\mathcal{u}}\\bigl(s,{\\bar{x}}_s,{\\ensuremath { [ { \\bar{x}}_s ] } } \\bigr)({\\ensuremath { \\langle { \\bar{x}}_s \\rangle } } ) { \\ensuremath { \\langle a({\\bar{x}}_{s},{\\ensuremath { [ { \\bar{x}}_{s } ] } } ) \\rangle } } \\bigr\\ }   \\bigr ] \\nonumber \\;.    \\end{aligned}\\ ] ] also , @xmath408 is a martingale increment satisfying @xmath409 } } ^{1/(2\\alpha ) }   \\leq c h_{i}$ ] and @xmath410 , recall proposition [ pr disc ito formula ] .    _ second step .",
    "_ denoting @xmath411 and @xmath412 } } \\bigr ) - b\\bigl({\\bar{x}}_{t_{i}},{\\bar{u}}_{t_i } , { \\ensuremath { [ { \\bar{x}}_{t_i},{\\bar{u}}_{t_i } ] } } \\bigr )   \\right\\ } { \\mathrm{d}}s   \\\\ & \\hspace{15pt } + \\frac1{h_i } \\int_{t_i}^{t_{i+1 } } { \\ensuremath{\\hat{\\mathbb{e } } \\ ; \\!\\ ! \\left [ { \\bar{v}}_s^\\mu \\cdot \\bigl\\ { { \\ensuremath { \\langle b\\bigl({\\bar{x}}_{t_{i}},{\\bar{y}}_{t_i},{\\ensuremath { [ { \\bar{x}}_{t_i},{\\bar{y}}_{t_i } ] } } \\bigr ) - b\\bigl({\\bar{x}}_{t_{i}},{\\bar{u}}_{t_i } , { \\ensuremath { [ { \\bar{x}}_{t_i},{\\bar{u}}_{t_i } ] } } \\bigr ) \\rangle } } \\bigr\\ } \\right ] } } { \\mathrm{d}}s \\ , , \\end{aligned}\\ ] ] the previous equation reads @xmath413 } } \\bigr ) { \\bar{v}}_{t_i}^x,{\\ensuremath { [ \\bar{\\mathfrak{x}}_{t_i},{\\bar{u}}_{t_i } ] } } \\bigr )   + { \\bar{v}}_{t_i}^x \\cdot \\bigl ( \\sigma({\\bar{x}}_{t_i},{\\ensuremath { [ { \\bar{x}}_{t_i } ] } } ) \\urpi_i    \\bigr )     \\bigr ] \\;,\\end{split}\\ ] ] where @xmath414 on the other hand , the scheme can be rewritten as @xmath415 } } \\bigr )   - h_i { \\bar{z}}_{t_i } \\cdot \\urpi_i   - \\delta m_i   \\ ; , \\end{aligned}\\ ] ] where @xmath416 satisfies @xmath417 } } = 0\\ , , \\ ; { \\ensuremath {      \\mathbb{e}_{{t_{i } } } \\ , \\!\\!\\left[\\urpi_i \\cdot \\delta m_i\\right ] } } = 0 \\text { and } { \\ensuremath{\\mathbb{e } \\ , \\!\\ ! \\left[|\\delta m_i|^2\\right ] } } < \\infty \\,.\\end{aligned}\\ ] ] denoting @xmath418 , @xmath419 } } ) { \\bar{v}}_{t_i}^x$ ] , and adding and , we get @xmath420 where @xmath421 } } \\bigr)-    f\\bigl(\\bar{\\mathfrak{x}}_{t_i},{\\bar{u}}_{t_{i } } , \\sigma^\\dagger\\bigl({\\bar{x}}_{t_i},{\\ensuremath { [ { \\bar{x}}_{t_i } ] } } \\bigr ) { \\bar{v}}^x_{t_i},{\\ensuremath { [ \\bar{\\mathfrak{x}}_{t_i},{\\bar{u}}_{t_i } ] } } \\bigr ) \\;. \\end{split}\\ ] ] for later use , we observe that @xmath422 to @xmath423 , we obtain @xmath424 squaring both sides and taking expectation , we compute , using for the left side and young s and conditional cauchy - schwarz inequality for the right side , @xmath425 } } + \\sum_{\\ell = i}^{j_{k+1}-1}\\!\\!\\ ! h_{\\ell } { \\ensuremath {      \\mathbb{e}_{t_q{}}\\ , \\!\\!\\left[|\\delta { \\bar{z}}_{\\ell}|^2 \\right ] } } \\\\ & \\hspace{15pt } \\le { \\ensuremath {      \\mathbb{e}_{t_q{}}\\ , \\!\\!\\left[(1+c\\delta)|\\delta { \\bar{y}}_{j_{k+1}}|^2 +   c\\sum_{\\ell = i}^{j_{k+1}-1 } h_{\\ell}|\\delta b_{\\ell } + \\delta     f_{\\ell } |^2 + \\frac{c}\\delta \\biggl(\\sum_{\\ell = i}^{j_{k+1}-1 } \\zeta_\\ell\\biggr)^2 \\right ] } } \\ , , \\end{split}\\ ] ] for @xmath426 . combining and young s inequality , this leads to @xmath425 } } + \\frac12\\sum_{\\ell = i}^{j_{k+1}-1}\\!\\!\\",
    "! h_{\\ell } { \\ensuremath {      \\mathbb{e}_{t_q{}}\\ , \\!\\!\\left[|{\\bar{z}}_{\\ell}|^2\\right ] } } \\le { \\ensuremath {      \\mathbb{e}_{t_q{}}\\ , \\!\\!\\left[e^{c\\delta}|\\delta { \\bar{y}}_{j_{k+1}}|^2 +   c\\sum_{\\ell = i}^{j_{k+1}-1 } h_{\\ell}|\\delta { \\bar{y}}_{\\ell}|^2 + \\frac{c}{\\delta } \\biggl(\\sum_{\\ell = i}^{j_{k+1}-1 } \\zeta_\\ell \\biggr)^2   \\right ] } } \\ , .",
    "\\end{split}\\ ] ] using the discrete version of gronwall s lemma and recalling that @xmath427 , we obtain , for @xmath428 , @xmath429 } } \\ ; , \\end{split}\\ ] ] and then , @xmath430    _ third step .",
    "_ to conclude , we need an upper bound for the error @xmath431 where @xmath432 is defined in .",
    "to do so , we study each term in separately .",
    "we also define @xmath433 and we recall that @xmath434 .",
    "_ third step a. _ we first study the contribution of @xmath435 to the global error term and note that @xmath436 we will upper bound this last term .",
    "+ let us first observe , that , for @xmath437 , @xmath438 } } , { \\ensuremath { [ { \\bar{x}}_{t_i } ] } } )   + h_i^\\frac12 \\bigl ( 1 + \\vert { \\bar{x}}_{t_{i } } \\vert    +    \\| { \\bar{x}}_{t_{i } } \\|_{2 } \\bigr )   \\right ) \\ , , \\ ] ] where we used the lipschitz property of @xmath439 given in @xmath78 , together with and . hence , @xmath440 from the boundedness of @xmath49 and the lipschitz property of @xmath51 and @xmath17 , we compute @xmath441 using lemma [ le basic estimates for x&y ] from the appendix below",
    ", we obtain @xmath442 from the boundedness of @xmath439 , @xmath49 and the lipschitz property of @xmath49 , we obtain @xmath443 } } \\bigr )   { \\bar{v}}_s^x     - \\sigma^\\dagger \\bigl({\\bar{x}}_{t_i},{\\ensuremath { [ { \\bar{x}}_{t_{i } } ] } } \\bigr ) { \\bar{v}}_{t_i}^x } \\bigr \\|_{2\\alpha}}^2 \\le c\\left ( h_i    \\bigl ( 1 +    { \\|{\\xi }   \\|_{2\\alpha}}^2 \\bigr )    + h_i^2 \\delta_y^2   \\right ) \\ , , \\ ] ] where we used the same argument as above to handle the difference between the two @xmath49 terms . combining the previous inequality with the lipschitz property of @xmath71 and replicating the analysis to handle the difference between the @xmath444 terms",
    ", we deduce @xmath445    _ third step b. _ combining the lipschitz property of @xmath51 , the fact that @xmath446 \\le c$ ] and cauchy - schwarz inequality , we get @xmath447 arguing as in the previous step , we easily get @xmath448 _ third step c. _ we now study the contribution of the terms @xmath449 to the global error . from the independance property of @xmath450 , we may regard each @xmath451 as a martingale increment . by burkholder - davies - gundy inequalities for discrete martingales , we first compute , using the fact that each @xmath452 is uniformly bounded , @xmath453 } } )   \\frac{\\bar{v}^{0,x}_s -\\bar{v}^{0,x}_{t_i}}{\\sqrt{s - t_i}}{\\mathrm{d}}s\\right|^2 \\right\\|_{\\alpha }   \\\\   & \\le c\\sum_{\\ell = j_k}^{j_{k+1}-1 } h_i    \\biggl ( h_{i } \\bigl ( 1 + { \\|{\\xi }   \\|_{2\\alpha}}^2 \\bigr ) +    { \\bigl \\|{\\sup_{s \\in [ t_i , t_{i+1 } ] } |{\\bar{x}}_{s}^{(0 ) } - { \\bar{x}}_{t_i}|^2 } \\bigr \\|_{\\alpha}}\\biggr ) \\ ; .\\end{aligned}\\ ] ] since @xmath454 } } ) |$ ] , for @xmath455 $ ] , so that @xmath456 , the previous inequality , together with lemma [ le basic estimates for x&y ] , leads to @xmath457 similarly , @xmath458 hence , @xmath459 _ third step d. _ ( i )",
    "we study the contribution of @xmath460 .",
    "we observe that @xmath461 } } \\bigr )    -    \\partial^2_{xx}{\\mathcal{u}}\\bigl(s,{\\bar{x}}_{s},{\\ensuremath { [ { \\bar{x}}_s ] } } \\bigr )   \\bigr| \\cdot \\vert a({\\bar{x}}_{t_i},{\\ensuremath { [ { \\bar{x}}_{t_i } ] } } ) \\vert     \\\\   & + \\bigl|   \\partial^2_{xx}{\\mathcal{u}}(s,{\\bar{x}}_s,{\\ensuremath { [ { \\bar{x}}_s ] } } )   \\bigr\\vert \\cdot \\bigl\\vert   a({\\bar{x}}_{t_i},{\\ensuremath { [ { \\bar{x}}_{t_i } ] } } )     -   a({\\bar{x}}_{s},{\\ensuremath { [ { \\bar{x}}_{s } ] } } ) \\bigr|     \\ , , \\end{aligned}\\ ] ] for @xmath462 $ ] .",
    "using the boundedness and lipschitz continuity of @xmath463 and @xmath49 , we get , from the previous expression , @xmath464 observing that @xmath465 , we obtain using , for @xmath466 @xmath467 which leads , using lemma [ le basic estimates for x&y ] again , to @xmath468    \\(ii ) to study @xmath469 , we first observe that @xmath470 } } ) ( { \\ensuremath { \\langle { \\bar{x}}_s^{(\\lambda ) } \\rangle } } )    -    \\partial_\\upsilon \\partial_\\mu { \\mathcal{u}}(s,{\\bar{x}}_s,{\\ensuremath { [ { \\bar{x}}_s ] } } ) ( { \\ensuremath { \\langle { \\bar{x}}_s \\rangle } } ) \\bigr|      \\right ] } }       \\label{eq decomp delta mu }   \\\\   & \\hspace{15pt } +    { \\ensuremath{\\hat{\\mathbb{e } } \\ ; \\!\\ ! \\left [    \\bigl|\\partial_\\upsilon \\partial_\\mu { \\mathcal{u}}(s,{\\bar{x}}_s,{\\ensuremath { [ { \\bar{x}}_s ] } } ) ( { \\ensuremath { \\langle { \\bar{x}}_s \\rangle } } )   \\bigr\\vert \\cdot    \\bigl\\vert   { \\ensuremath { \\langle a({\\bar{x}}_{t_i},{\\ensuremath { [ { \\bar{x}}_{t_i } ] } } )     - a({\\bar{x}}_{s},{\\ensuremath { [ { \\bar{x}}_{s } ] } } )      \\rangle } } \\bigr\\vert\\right ] } } \\ , .    \\nonumber\\end{aligned}\\ ] ] for the last term , we combine cauchy - schwarz inequality and boundedness and lipschitz continuity of @xmath49 to get @xmath471 } } ) ( { \\ensuremath { \\langle { \\bar{x}}_s \\rangle } } ) \\bigr\\vert \\cdot   \\bigl\\vert    { \\ensuremath { \\langle a({\\bar{x}}_{t_i},{\\ensuremath { [ { \\bar{x}}_{t_i } ] } } )    -    a({\\bar{x}}_{s},{\\ensuremath { [ { \\bar{x}}_{s } ] } } )    \\rangle } } \\bigr|\\right ] } }    \\\\   & \\hspace{15pt }   \\le c    { \\left \\|{{\\bar{x}}_{t_i}-{\\bar{x}}_{s } } \\right \\|_2 }   \\le    c    { \\bigl \\|{{\\bar{x}}_{t_i}-{\\bar{x}}_{s } } \\bigr \\|_{2\\alpha}}.\\end{aligned}\\ ] ] recalling from that @xmath472 , we obtain , using lemma [ le basic estimates for x&y ] , that @xmath471 } } ) ( { \\ensuremath { \\langle { \\bar{x}}_s \\rangle } } ) \\bigr\\vert   \\cdot   \\bigl\\vert    { \\ensuremath { \\langle a({\\bar{x}}_{t_i},{\\ensuremath { [ { \\bar{x}}_{t_i } ] } } )   -    a({\\bar{x}}_{s},{\\ensuremath { [ { \\bar{x}}_{s } ] } } )     \\rangle } } \\bigr| \\right ] } } \\nonumber    \\\\   & \\hspace{15pt }   \\le    c_{\\lambda } h_i^\\frac12\\left(1 + h_i^\\frac12{\\ensuremath{\\ {   \\delta_y + { \\|{\\xi }   \\|_{2\\alpha } } \\ } } } \\right ) \\ , .   \\label{eq control delta mu 1 }   \\end{aligned}\\ ] ] for the first term in , we use @xmath119 equation to get @xmath473 } } ) ( { \\ensuremath { \\langle { \\bar{x}}_s^{(\\lambda ) } \\rangle } } )    -    \\partial_\\upsilon \\partial_\\mu { \\mathcal{u}}(s,{\\bar{x}}_s,{\\ensuremath { [ { \\bar{x}}_s ] } } ) ( { \\ensuremath { \\langle { \\bar{x}}_s \\rangle } } ) |    \\\\    & \\le   c\\left\\ { 1 + |{\\ensuremath { \\langle { \\bar{x}}_s^{(\\lambda ) } \\rangle } } |^{2\\alpha }             + |{\\ensuremath { \\langle { \\bar{x}}_s \\rangle } } |^{2 \\alpha }            + { \\left \\|{{\\bar{x}}_s } \\right \\|_2}^{2 \\alpha }            \\right\\}^{\\frac12 } |{\\ensuremath { \\langle { \\bar{x}}_s^{(\\lambda ) } \\rangle } } -{\\ensuremath { \\langle { \\bar{x}}_s \\rangle } } |\\,.\\end{aligned}\\ ] ] by cauchy schwarz inequality , we obtain @xmath474 } } ) ( { \\ensuremath { \\langle { \\bar{x}}_s^{(\\lambda ) } \\rangle } } )    -    \\partial_\\upsilon \\partial_\\mu { \\mathcal{u}}(s,{\\bar{x}}_s,{\\ensuremath { [ { \\bar{x}}_s ] } } ) ( { \\ensuremath { \\langle { \\bar{x}}_{s } \\rangle } } ) \\nonumber     \\ } } } \\nonumber    |\\right ] } }     \\\\   & \\le   \\label{eq first part temp }   c \\sqrt{h_{i } } \\left(1+\\| { \\bar{x}}_s^{(\\lambda ) } \\|_{2 \\alpha}^{\\alpha } + \\| { \\bar{x}}_s \\|_{2 \\alpha}^{\\alpha } \\right ) \\ , .   \\end{aligned}\\ ] ] we then observe that @xmath475 where we used lemma [ le basic estimates for x&y ] for the last inequality .",
    "combining the last inequality with and using also , we compute @xmath476 and then @xmath477    \\4 . collecting the estimates , and , we compute @xmath478 observing that @xmath479 and combining the previous inequality with , and , we obtain @xmath480 which concludes the proof for @xmath109 small enough .    [ le : a3 ] assume that @xmath74 and @xmath481 } } ) $ ] are bounded",
    ". then @xmath325 is satisfied whatever the value of @xmath95 .",
    "it suffices to prove that @xmath17 is bounded on the whole space and that @xmath482 is bounded independently of the discretization parameters .",
    "we refer to @xcite for the proof of the boundedness of @xmath17 .",
    "the bound for @xmath482 may obtained by squaring and then by taking the conditional expectation exactly as done in the second step of the proof of lemma [ le approx ] .",
    "assumptions @xmath359 and @xmath483 are easily checked .",
    "it suffices to observe that @xmath484 coincides with the solution of the discrete euler scheme : @xmath485 } } \\bigr ) + \\sqrt{t_{i+1}-t_{i } } \\sigma\\bigl(x_{t_{i}}^0,{\\ensuremath { [ x_{t_{i}}^0 ] } } \\bigr ) \\varpi_{i},\\ ] ] with @xmath486 as initial condition .",
    "[ le approx ] , lemma [ le stab ] and lemma [ le : a3 ] with theorem [ th propagation error ii ] , we have the following result .",
    "[ cor : implemented ] under @xmath78-@xmath73 , assuming , the following holds @xmath487(}}\\xi\\text{\\texttt { ) } } } } - { \\mathcal{u}}(r_k,\\xi,{\\ensuremath { [ \\xi ] } } ) } \\bigr \\|_{2\\alpha } }   \\le c     \\left ( ( c \\delta)^{j-1}+ |\\pi|^{\\frac12}\\delta^{-1 } ( 1+{\\|{\\xi }   \\|_{2\\alpha } } )     \\right ) \\ ; ,   \\end{aligned}\\ ] ] for @xmath280 small enough",
    ".    the first term in the right hand side is connected with the local picard iterations on a step of length @xmath109 . as expected , it decreases geometrically fast with the number of iterations .",
    "the second term is due to the propagation of the error along the mesh .",
    "the leading term @xmath488 is consistent with that observed for classical forward - backward systems , see for instance @xcite .",
    "the normalization by @xmath109 is due to the propagation of the error through the successive local solvers .",
    "in practice , we would like to approximate the value of @xmath489 at some point @xmath490 . in the first section below ,",
    "we explain how to retrieve such approximation using the approximation of @xmath491 } } ) $ ] given by the algorithm @xmath492(}}\\text{\\texttt { ) } } } } $ ] , for some @xmath62 . in a second part",
    ", we discuss the numerical results obtained by implementing @xmath492(}}\\text{\\texttt { ) } } } } $ ] with two levels , i.e. @xmath493 .",
    "in particular , we show that it is more efficient than an algorithm based simply on picard iterations .",
    "the goal of this section is to show how to obtain an approximation of @xmath495 } } ) $ ] with @xmath62 and @xmath496 .",
    "we will assume that we thus have at hand a discrete valued random variable @xmath497 such that @xmath498 is a good approximation of @xmath4 for the wasserstein distance .",
    "for instance , such an approximation can be constructd by using quantization techniques .",
    "then , we can use @xmath492(}}\\xi^{|\\pi|}\\text{\\texttt { ) } } } } $ ] to obtain an approximation of @xmath499 } } ) $ ] .",
    "note that @xmath492(}}\\xi^{|\\pi|}\\text{\\texttt { ) } } } } $ ] is a discrete random variable as the algorithm is initialised by a discrete random variable as well . in practice",
    ", this means that each point @xmath500 will be the root of a tree and will be associated to an output value @xmath501 } } ) $ ] and then @xmath492(}}\\xi^{|\\pi|}\\text{\\texttt { ) } } } } \\sim \\sum_{\\ell=1}^m p_\\ell \\delta_{y^\\ell } $ ] .",
    "it is important to remark that the computations on the trees are connected via the mckean - vlasov interaction .    using the lipschitz continuity of @xmath17 , one easily obtains @xmath502 where @xmath503 is a point in the support of @xmath504 realising the minimum in the first line .    in many cases",
    ", it will be easy to have @xmath505 and thus reduce the above error to the term @xmath506 .",
    "this is obviously the case if @xmath26 is deterministic .",
    "as mentioned above , the approximation of @xmath507 is obtained by running @xmath492(}}\\xi^{|\\pi|}\\text{\\texttt { ) } } } } $ ] and by taking its value on the tree initiated at @xmath503 , precisely we have @xmath508 .",
    "the corresponding pointwise error is given by @xmath509 } } ) - { \\ensuremath {   \\text{\\texttt{solver[}}0\\text{\\texttt{](}}\\xi^{|\\pi|}\\text{\\texttt { ) } } } } } \\right \\|_2 } \\ ; , \\ ] ] but this is very poor when the initial distribution @xmath4 is diffuse and accordingly when @xmath510 has a large support , in which case @xmath511 is expected to be small .    to bypass this difficulty ,",
    "we must regard @xmath512 as a conditional error . somehow , it is the error of the numerical scheme conditional on the initial root of the tree .",
    "it requires a new analysis , but it should not be so challenging : now that we have investigated the error for the mckean - vlasov component , we can easily revisit the proof of theorem [ th propagation error ii ] in order to derive a bound for this conditional error .    instead of revisiting the whole proof , we can argue by doubling the variables . for @xmath26 and @xmath15 as above , we can regard the four equations , , and as a single forward - backward system of the mckean - vlasov type .",
    "the forward component of such a doubled system is @xmath513 and the backward components are @xmath514 and @xmath515 . except for the fact that the dimension of @xmath516 is no longer equal to the dimension of the noise , which we assumed to be true for convenience only , and",
    "for the fact that @xmath517 takes values in @xmath518 , the setting is exactly the same as before , namely @xmath519 can be regarded as the solution of a mckean - vlasov forward - backward sde in which the mean field component reduces to the marginal law of @xmath520 .",
    "we observe in particular that @xmath521 } } ) ,   \\quad    y^{0,\\xi}_{t } = { \\mathcal{u}}(t , x_{t}^{0,\\xi},{\\ensuremath { [ x_{t}^{0,\\xi } ] } } ) , \\quad t \\in [ 0,t],\\ ] ] with similar relationships for @xmath522 and @xmath523 .",
    "hence , @xmath524 ( and @xmath525 ) can be represented as a function of @xmath516 , which was the key assumption in our analysis . for sure , the fact that @xmath517 takes values in dimension 2 is not a limitation for duplicating the arguments used to prove theorem [ th propagation error ii ] .",
    "numerically speaking , the tree initiated at root @xmath503 under the initial distribution @xmath498 provides an approximation of @xmath526 } } ) $ ] , which is equal to @xmath527 } } } $ ] .",
    "so our numerical ( implemented ) scheme is in fact a numerical for the whole process @xmath519 .",
    "this leads us to the following result .",
    "let @xmath528 be the approximation of @xmath494 obtained by calling @xmath492(}}\\xi^{\\vert \\pi \\vert}\\text{\\texttt { ) } } } } $ ] , where @xmath529 is defined in .",
    "then , the following holds @xmath530 where @xmath512 can be estimated by corollary [ cor : implemented ] , with @xmath531 replaced by @xmath532 .      in this section",
    ", we will prove empirically the convergence of the approximation obtained by the solver @xmath190(}}\\text{\\texttt { ) } } } } $ ] .",
    "in particular , we will compare the output of our algorithm @xmath190(}}\\text{\\texttt { ) } } } } $ ] , when implemented with two levels , i.e. @xmath493 ( we simply call it _ two - level algorithm _ ) , with the output of a basic algorithm based only on picard iterations , which can be seen as a solver @xmath190(}}\\text{\\texttt { ) } } } } $ ] , but with only one level , i.e. @xmath533 ( we simply call it _ one - level algorithm _ ) . in both cases ,",
    "we use example [ ex binomial tree ] as discretization scheme , with a standard bernoulli quantization of the normal distribution , @xmath36 being equal to @xmath41 . in the numerical studies below ,",
    "we show that the _ two - level algorithm _ converges in case when the _ one - level algorithm _ fails .      in this part",
    ", we compare the output of both algorithms for the following linear model where a closed - form solution is available : @xmath534 } } _ t { \\mathrm{d}}t + \\sigma { \\mathrm{d}}w_t\\ ; , \\quad \\ ; x_0 = x\\ , ,    \\\\    { \\mathrm{d}}y_t & = -a y_t { \\mathrm{d}}t   + z_t { \\mathrm{d}}w_t \\ , , \\quad \\;\\text { and } y_t = x_t\\ , ,   \\end{aligned}\\ ] ] for @xmath535 , and the true solution for @xmath536 } } = m_0 $ ] is given by @xmath537 the errors for various time steps and for both algorithms are shown on the log - log error plot of figure @xmath41 .",
    "the parameters are fixed as follows : @xmath538 , @xmath539 , @xmath540 , @xmath541 and @xmath542 . moreover ,",
    "the _ two - level algorithm _ uses @xmath543 picard iterations per level , and the _ one - level algorithm _ computes @xmath544 picard iterations .",
    "[ fig safety check ]       in this section , we compare the _ two - level algorithm _ and the _ one - level algorithm _ on two models , for which existence and uniqueness to the master equation ( or the fbsde system ) hold true for any arbitrary terminal time @xmath32 and lipschitz constant @xmath545 of the coefficients function . nevertheless , as stated in the theorems above , the convergence of the algorithms is guaranted only for a periods of time which are controlled by @xmath545 and @xmath32 . here",
    ", we fix the terminal date @xmath32 and allow @xmath545 to vary with the use of a coupling parameter @xmath546 , see equations ( for a case without mckean - vlasov interaction ) and ( for a case with mckean - vlasov interaction ) .",
    "we will see below that , as expected , the _ two - level algorithm _ converges for a larger range of coupling parameter than the _",
    "one - level algorithm_.    [ [ an - example - with - no - mckean - vlasov - interaction ] ] an example with no mckean - vlasov interaction + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +     + here , the model is the following @xmath547 } } \\ , .",
    "\\end{split}\\ ] ]    on figure @xmath548 , we plot the output of the two - level and one - level algorithm along with a proxy of the true solution computed by usual bsde approximation method ( after a girsanov transform ) and with a very high - level of precision . on the graph , the value @xmath549 stands for the approximation of @xmath550 : there is no dependence upon the initial measure as there is no mkv interaction in this example .",
    "the parameters are fixed as follows : @xmath540 , @xmath541 and @xmath551 . moreover ,",
    "the _ two - level algorithm _",
    "uses @xmath543 picard iterations per level , and the _ one - level algorithm _ computes @xmath544 picard iterations .    ,",
    "the discrepancy for large coupling parameter coming most probably from the discrete - time error .",
    "interestingly , the _ one - level algorithm _ shows bifurcations . , scaledwidth=80.0% ]    [ [ an - example - from - large - population - stochastic - control ] ] an example from large population stochastic control + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +     + for this part , the model is given by @xmath552 } } ) { \\mathrm{d}}t + z_t { \\mathrm{d}}w_t \\text { and } y_t = g'(x_t):=\\mathrm{atan}(x_t ) .",
    "\\end{split}\\ ] ] coming from pontryagin principle applied to mfg @xmath553 } } ) \\right ) { \\mathrm{d}}t   \\right ] } }   \\end{aligned}\\ ] ] with @xmath554 , see e.g. @xcite .",
    "we do not know the exact solution for this model and it is not possible to obtain easily an approximation as in the previous example .",
    "we plot on figure @xmath555 , the output value of the _ one - level algorithm _ and _ two - level algorithm_. on the graph , the value @xmath549 stands for the approximation of @xmath556 .",
    "the parameters are fixed as follows : @xmath540 , @xmath541 and @xmath557 .",
    "moreover , the _ two - level algorithm _ uses @xmath543 picard iterations per level , and the _ one - level algorithm _ computes @xmath544 picard iterations .",
    "we consider the following euler scheme on the discrete time grid @xmath10 of the interval @xmath82 $ ] , recall , @xmath558 where @xmath559 are i.i.d .",
    "centered @xmath12-valued random variables such that the covariance matrix @xmath560 $ ] is the identity matrix and @xmath561 , and @xmath562 , for all @xmath563 .",
    "we also introduce a piecewise continuous version of the previous scheme , for @xmath395 , @xmath564 and @xmath565 $ ] , the process @xmath566 , @xmath567 and @xmath568 . following the notation used in the proof of lemma [ le approx ] ,",
    "we just write @xmath569 for @xmath570 , which defines a continuous version of the euler scheme given in .",
    "[ pr disc ito formula ] for any @xmath571 , the following holds true : @xmath572 } } )   = { \\mathcal{u}}(t_i,{\\bar{x}}_{t_i},{\\ensuremath { [ { \\bar{x}}_{t_i } ] } } )   + \\int_{t_i } ^{t_{i+1 } } \\partial_t { \\mathcal{u}}(s,{\\bar{x}}_s,{\\ensuremath { [ { \\bar{x}}_s ] } } ) { \\mathrm{d}}s   \\\\   & + \\int_{t_i}^{t_{i+1 } } \\left (    \\partial_x { \\mathcal{u}}(s,{\\bar{x}}_s,{\\ensuremath { [ { \\bar{x}}_s ] } } ) \\cdot b_i      +   \\frac12\\int_0 ^ 1 \\textrm{\\rm tr } \\bigl [ \\partial^2_{xx } { \\mathcal{u}}(s,{\\bar{x}}^{(\\lambda)}_s,{\\ensuremath { [ { \\bar{x}}_s ] } } ) a_i \\bigr ] { \\mathrm{d}}\\lambda     \\right )     { \\mathrm{d}}s \\\\ & +    \\int_{t_i}^{t_{i+1 } }    { \\ensuremath{\\hat{\\mathbb{e } } \\ ; \\!\\ ! \\left[\\partial_\\mu { \\mathcal{u}}(s,{\\bar{x}}_s,{\\ensuremath { [ { \\bar{x}}_s ] } } ) ( { \\ensuremath { \\langle { \\bar{x}}_s \\rangle } } ) \\cdot { \\ensuremath { \\langle b_i \\rangle } } \\right ] } } ds \\\\ &    + \\frac12\\int_0 ^ 1   { \\ensuremath{\\hat{\\mathbb{e } } \\ ; \\!\\ ! \\left [   \\textrm{\\rm tr } \\bigl [    \\partial_\\upsilon \\partial_\\mu { \\mathcal{u}}(s,{\\bar{x}}_s,{\\ensuremath { [ { \\bar{x}}_s ] } } ) ( { \\ensuremath { \\langle { \\bar{x}}^{(\\lambda)}_s \\rangle } } )   { \\ensuremath { \\langle a_i \\rangle } } \\bigr ] { \\mathrm{d}}\\lambda    \\right ] } }    { \\mathrm{d}}s \\\\ & + \\int_{t_i}^{t_{i+1}}\\partial_x { \\mathcal{u}}(s , { \\bar{x}}^{(0)}_s , { \\ensuremath { [ { \\bar{x}}_s ] } } )    \\frac{\\sigma_i \\varpi_i}{2 \\sqrt{s - t_i } } { \\mathrm{d}}s      + \\delta { \\mathcal m}(t_{i},t_{i+1 } ) + \\delta { \\mathcal t}(t_{i},t_{i+1 } )    \\ ; ,   \\end{aligned}\\ ] ] where @xmath573 is here equal to @xmath574 , and @xmath408 is a martingale increment satisfying @xmath575 and @xmath410 .    by writing @xmath576 and by using the standard chain rule for continuously differentiable functions on a hilbert space ,",
    "we get @xmath577 } } )   & = { \\mathcal{u}}(t_{i},{\\bar{x}}_{t_{i}},{\\ensuremath { [ { \\bar{x}}_{t_{i } } ] } } )   + \\int_{t_i}^{t_{i+1 } } \\partial_t { \\mathcal{u}}(s,{\\bar{x}}_s,{\\ensuremath { [ { \\bar{x}}_s ] } } ) { \\mathrm{d}}s   \\\\   & \\hspace{15pt}+ \\int_{t_i}^{t_{i+1 } }   \\left ( \\partial_x { \\mathcal{u}}(s,{\\bar{x}}_s,{\\ensuremath { [ { \\bar{x}}_s ] } } ) \\cdot \\bigl(b_i + \\frac{\\sigma_i \\varpi_i}{2\\sqrt{s - t_i}}\\bigr ) { \\mathrm{d}}s \\right . \\\\ & \\hspace{30pt } + \\left .",
    "{ \\ensuremath{\\hat{\\mathbb{e } } \\ ; \\!\\ ! \\left[\\partial_\\mu { \\mathcal{u}}(s,{\\bar{x}}_s,{\\ensuremath { [ { \\bar{x}}_s ] } } ) ( { \\ensuremath { \\langle { \\bar{x}}_s \\rangle } } ) \\cdot                { \\ensuremath { \\langle b_i + \\frac{\\sigma_i \\varpi_i}{2\\sqrt{s - t_i } } \\rangle } } \\right ] } } \\right )    { \\mathrm{d}}s\\;.   \\end{aligned}\\ ] ] now we observe that , @xmath578 } } ) & = \\partial_x { \\mathcal{u}}(s,{\\bar{x}}^{(0)}_s,{\\ensuremath { [ { \\bar{x}}_s ] } } ) + \\sqrt{s - t_i } \\int_0 ^ 1 \\partial^2_{xx } { \\mathcal{u}}(s,{\\bar{x}}^{(\\lambda)}_s,{\\ensuremath { [ { \\bar{x}}_s ] } } ) \\sigma_i\\varpi_i { \\mathrm{d}}\\lambda \\\\ & = \\partial_x { \\mathcal{u}}(s,{\\bar{x}}^{(0)}_s,{\\ensuremath { [ { \\bar{x}}_s ] } } ) + \\sqrt{s - t_i}\\ , \\partial^2_{xx } { \\mathcal{u}}(s,{\\bar{x}}^{(0)}_s,{\\ensuremath { [ { \\bar{x}}_s ] } } ) \\sigma_i\\varpi_i    + \\sqrt{s - t_{i}}{\\mathcal t}_{1}(s ) \\ , ,   \\end{aligned}\\ ] ] where @xmath579 is a random variable defined on @xmath211 such that @xmath580 , and @xmath581 } } ) ( { \\ensuremath { \\langle { \\bar{x}}_s \\rangle } } ) & =   \\partial_\\mu { \\mathcal{u}}(s,{\\bar{x}}_s,{\\ensuremath { [ { \\bar{x}}_s ] } } ) ( { \\ensuremath { \\langle { \\bar{x}}^{(0)}_s \\rangle } } )   \\\\   & \\hspace{15pt}+\\sqrt{s - t_i } \\int_0 ^ 1 \\partial_\\upsilon \\partial_\\mu { \\mathcal{u}}(s,{\\bar{x}}_s,{\\ensuremath { [ { \\bar{x}}_s ] } } ) ( { \\ensuremath { \\langle { \\bar{x}}^{(\\lambda)}_s \\rangle } } )    { \\ensuremath { \\langle \\sigma_i   \\varpi_i \\rangle } } { \\mathrm{d}}\\lambda   \\\\   & = \\partial_{\\mu } { \\mathcal{u}}(s,{\\bar{x}}_s,{\\ensuremath { [ { \\bar{x}}_s ] } } ) ( { \\ensuremath { \\langle { \\bar{x}}^{(0)}_s \\rangle } } )   \\\\   & \\hspace{15pt } + \\sqrt{s - t_i } \\ , \\partial_{v } \\partial_{\\mu } { \\mathcal{u}}(s,{\\bar{x}}_s,{\\ensuremath { [ { \\bar{x}}_s ] } } ) ( { \\ensuremath { \\langle { \\bar{x}}_{s } ^{(0 ) } \\rangle } } )   { \\ensuremath { \\langle \\sigma_i\\varpi_i \\rangle } }   + \\sqrt{s - t_{i } } { \\mathcal t}_{2}(s ) \\ , , \\end{aligned}\\ ] ] where @xmath582 is a random variable on the enlarged space @xmath583 such that @xmath584 } } ^{1/(2\\alpha ) } \\leq   c h_{i}^{\\frac12}$ ] .",
    "we insert these expansions back into the identity we obtained for the term @xmath585 } } ) $ ] .",
    "we let @xmath586 } } )   \\sigma_{i } \\varpi_{i } \\cdot   \\bigl (   \\sigma_{i } \\varpi_{i } \\bigr ) \\\\ & \\hspace{50pt } - { \\ensuremath {      \\mathbb{e}_{t_i}\\ , \\!\\!\\left[\\partial^2_{xx } { \\mathcal{u}}(s,{\\bar{x}}^{(0)}_{s},{\\ensuremath { [ { \\bar{x}}_{s } ] } } )   \\sigma_{i } \\varpi_{i } \\cdot   \\bigl (   \\sigma_{i } \\varpi_{i } \\bigr)\\right ] } } \\bigr ]   { \\mathrm{d}}s \\ , , \\\\ \\delta { \\mathcal t}(t_{i},t_{i+1 } ) & =   \\frac12 \\int_{t_{i}}^{t_{i+1 } }   \\bigl ( { \\mathcal t}_{1}(s ) +   { \\mathcal t}_{2}(s ) \\bigr ) \\cdot \\sigma_{i } \\varpi_{i }   { \\mathrm{d}}s \\ , .   \\end{split}\\ ] ] it defines a martingale increment satisfying @xmath587 } } ^{1/(2\\alpha ) }   \\leq c h_{i}$ ] . observing that for @xmath466 , @xmath588 } } ) ( { \\ensuremath { \\langle { \\bar{x}}_s^{(0 ) } \\rangle } } ) \\cdot { \\ensuremath { \\langle \\sigma_{i}\\varpi_i \\rangle } } \\right ] } } = 0 \\ , ,   \\\\   & { \\ensuremath {      \\mathbb{e}_{t_i}\\ , \\!\\!\\left[\\partial^2_{xx } { \\mathcal{u}}(s,{\\bar{x}}^{(0)}_s,{\\ensuremath { [ { \\bar{x}}_s ] } } )   \\sigma_{i } \\varpi_{i }   \\cdot \\bigl ( \\sigma_{i } \\varpi_{i}\\bigr ) \\right ] } } = { \\ensuremath {      \\mathbb{e}_{t_i}\\ , \\!\\!\\left[\\textrm{\\rm tr } \\bigl(\\partial^2_{xx } { \\mathcal{u}}(s,{\\bar{x}}^{(0)}_s,{\\ensuremath { [ { \\bar{x}}_s ] } } ) a_{i } \\bigr ) \\right ] } } \\ , ,   \\\\   & { \\ensuremath {      \\mathbb{e}_{t_i}\\ , \\!\\!\\left[\\partial^2_{xx } { \\mathcal{u}}(s,{\\bar{x}}^{(0)}_s,{\\ensuremath { [ { \\bar{x}}_s ] } } )   \\sigma_{i } \\varpi_{i }   \\cdot \\bigl ( \\sigma_{i } \\varpi_{i}\\bigr ) \\right ] } } = { \\ensuremath {      \\mathbb{e}_{t_i}\\ , \\!\\!\\left[\\textrm{\\rm tr } \\bigl(\\partial^2_{xx } { \\mathcal{u}}(s,{\\bar{x}}^{(0)}_s,{\\ensuremath { [ { \\bar{x}}_s ] } } ) a_{i } \\bigr ) \\right ] } } \\ , , \\\\ & { \\ensuremath{\\hat{\\mathbb{e } } \\ ; \\!\\ ! \\left [ \\partial_{v } \\partial_{\\mu } { \\mathcal{u}}(s,{\\bar{x}}_s,{\\ensuremath { [ { \\bar{x}}_s ] } } ) ( { \\ensuremath { \\langle { \\bar{x}}_s^{(0 ) } \\rangle } } ) \\sqrt{s - t_i }   { \\ensuremath { \\langle \\sigma_i\\varpi_i \\rangle } } \\cdot { \\ensuremath { \\langle \\sigma_i\\varpi_i \\rangle } } \\right ] } }   = { \\ensuremath{\\hat{\\mathbb{e } } \\ ; \\!\\ ! \\left[\\textrm{\\rm tr } \\bigl(\\partial_{v }   \\partial_{\\mu } { \\mathcal{u}}(s,{\\bar{x}}_s,{\\ensuremath { [ { \\bar{x}}_s ] } } ) ( { \\ensuremath { \\langle { \\bar{x}}_s^{(0 ) } \\rangle } } ) { \\ensuremath { \\langle a_{i } \\rangle } } \\bigr ) \\right ] } } \\ , , \\end{aligned}\\ ] ] we complete the proof .      [ le basic estimates for x&y ] under @xmath73-@xmath78 , the following holds for the forward component of the scheme given in example [ ex binomial tree ] and its continuous version , @xmath589 } } ) - y_{t } } \\bigr \\|_{2\\alpha}}\\right)\\ , , \\label{eq control xd } \\ ] ]    we introduce @xmath590 } } )   - { \\bar{y}}_{t_i}|$ ] and observe from the lipschitz property of @xmath51 and @xmath17 that @xmath591 } } \\bigr)\\bigr| \\le c_\\lambda   \\bigl(1 + |{\\bar{x}}_{t_i}| +   { \\bigl \\|{{\\bar{x}}_{t_i } } \\bigr \\|_{2\\alpha } } + \\mathfrak{d}_i + { \\bigl \\|{\\mathfrak{d}_i } \\bigr \\|_{2\\alpha } }   \\bigr)\\;.\\end{aligned}\\ ] ] recall that the scheme for the forward component reads @xmath592 } } \\bigr)(t_{\\ell+1}-t_\\ell ) +   \\sum_{\\ell = j_k}^{i } \\sigma\\bigl({\\bar{x}}_{t_\\ell } , { \\ensuremath { [ { \\bar{x}}_{t_\\ell } ] } } \\bigr)\\delta \\bar{w}_\\ell\\;.\\end{aligned}\\ ] ] squaring the previous inequality , using cauchy - schwarz inequality for the first sum and the martingale property for the second sum , we obtain @xmath593 } } ) } \\bigr \\|_{2\\alpha}}^2   +   \\|\\sigma({\\bar{x}}_{t_\\ell } ,    { \\ensuremath { [ { \\bar{x}}_{t_\\ell } ] } }     ) \\|^2_{2 \\alpha } \\right ) \\ ; \\ , \\end{aligned}\\ ] ] where we used again brkholder - davis - gundy inequality for discrete martingales .",
    "\\(2014 ) estimes nouvelles pour les quations quasilinaires _ seminar in applied mathematics at the collge de france _"
  ],
  "abstract_text": [
    "<S> this paper is dedicated to the presentation and the analysis of a numerical scheme for forward - backward sdes of the mckean - vlasov type , or equivalently for solutions to pdes on the wasserstein space . because of the mean field structure of the equation , earlier methods for classical forward - backward systems fail . </S>",
    "<S> the scheme is based on a variation of the method of continuation . </S>",
    "<S> the principle is to implement recursively local picard iterations on small time intervals .    </S>",
    "<S> we establish a bound for the rate of convergence under the assumption that the decoupling field of the forward - bakward sde ( or equivalently the solution of the pde ) satisfies mild regularity conditions . </S>",
    "<S> we also provide numerical illustrations . </S>"
  ]
}