{
  "article_text": [
    "tomographic image reconstruction from noisy incomplete projection data at few views or in a limited - angle setting is a common challenge in computed tomography , yet classical methods such as filtered back - projection ( fbp ) are well known to fail ( see , e.g. , @xcite and @xcite ) . to compute a robust solution",
    "it is necessary to incorporate adequate prior information into the mathematical reconstruction formulation .",
    "total variation ( tv ) regularization can be suited for edge - preserving imaging problems in low dose and/or few - view data sets @xcite ; but in the presence of noise tv tends to oversmooth textures in natural images @xcite .",
    "`` training images '' in the form of a carefully chosen set of images can be used to represent samples of the prior  such as texture  for the desired solution .",
    "clearly , such images must be reliable and application - specific . a suitable way to incorporate the prior information from training images and to extract",
    "the prototype features of such images is to form a dictionary from the images such that they are reproducible from a limited ( aka sparse ) linear combination of those images .",
    "the dictionary is then used for representation of other images with similar features .",
    "such `` sparse coding '' of natural images was introduced by olshausen and field @xcite .",
    "recent development in the theory and computational tools for sparse representation of signals and images ( see , e.g. , @xcite and @xcite ) has enabled us to analyze massive training data .",
    "dictionary learning methods are now widely used to compute basis elements and learn sparse representations of training signals and images ( see , e.g. , @xcite and @xcite ) . likewise , sparse representation in terms of such dictionaries has attracted increased interest in solving imaging problems such as denoising @xcite , deblurring @xcite , and restoration @xcite , in addition to solving tomographic image reconstruction problems @xcite .",
    "one common feature in the aforementioned papers on dictionary learning and sparse representation in terms of these dictionaries is the reliance on the ( invertible ) mapping of 2d images to vectors and subsequent use of a linear algebraic framework : matrices are used for the dictionary representation ( the columns represent vectorized forms of image feature ) and the use of a linear combination of the columns of the dictionary gives the expression of the image , in its vectorized form .",
    "however , the training data itself can be more naturally represented as a multidimensional array , called a tensor .",
    "for example , a collection of @xmath0 gray - scale images of size @xmath1 could be arranged in an @xmath2 array , also known as a third - order tensor .",
    "recent work in imaging applications such as facial recognition @xcite and video completion @xcite has shown that using the right kind of factorizations of particular tensor - based representations of the data can have a distinct advantage over matrix - based counterparts .",
    "for this reason , in this paper we will develop a fundamentally new approach for both the dictionary learning and image reconstruction tasks that is based on a particular type of tensor decomposition based around the t - product introduced in @xcite .",
    "there are several different tensor factorizations and decompositions such as candecomp / parafac ( cp ) @xcite and tucker decomposition @xcite .",
    "the use of different decompositions is driven by applications as well as the properties of the decompositions .",
    "for an extensive list of tensor decompositions , their applications , and further references , see @xcite .",
    "some recent works provide algorithms and analysis for tensor sparse coding and dictionary learning based on different factorization strategies .",
    "caiafa and cichocki @xcite discuss multidimensional compressed sensing algorithms using the tucker decomposition .",
    "zubair and wang @xcite propose a tensor learning algorithm based on the tucker model with a sparsity constraint on the core tensor .",
    "tensor - based extensions of the method of optimal directions ( mod ) @xcite and the ksvd algorithm @xcite have been studied in @xcite for separable multidimensional dictionaries .",
    "an algorithm for tensor dictionary learning based on the cp decomposition , called k - cpd , is presented in @xcite .    recent work by kilmer et al .",
    "@xcite sets up a new theoretical framework which facilitates a straightforward extension of matrix factorizations to third - order tensors based on a new tensor multiplication definition , called the _",
    "t - product_. the motivation for our work is to use the t - product as a natural extension for the dictionary learning problem and image reconstruction in a third - order tensor formulation with the factorization based on the framework in @xcite and @xcite .",
    "the goal of this paper is to re - visit the dictionary learning approach in @xcite for x - ray ct reconstruction , now using a tensor formulation of the problem .",
    "as we will show , the new formulation is not a trivial reformulation of the matrix - based method , and we demonstrate that in the tensor formulation we obtain a much sparser representation of both the dictionary and the reconstruction .    this paper is organized as follows .",
    "we first establish basic notation and definitions in section [ sec : not ] . in section",
    "[ sec : back ] we briefly discuss the dictionary learning problem and the reconstruction problem using dictionaries . in section [ sec : tdl ] we describe the tensor dictionary learning problem and the corresponding algorithm based on the alternating direction method of multipliers .",
    "then , in section [ sec : rec ] we utilize the learned tensor dictionary to formulate the reconstruction problem as a regularized inverse problem . numerical results are presented in section [ sec : exp ] demonstrating the performance and parameter choice of our algorithm .",
    "in this section we present the definitions and notations that will be used throughout this paper .",
    "we exclusively consider the tensor definitions and the tensor product notation introduced in @xcite and @xcite . throughout the paper ,",
    "a capital italics letter such as @xmath3 denotes a matrix and a capital calligraphy letter such as @xmath4 denotes a tensor .    a _ tensor _ is a multidimensional array of numbers .",
    "the _ order _ of a tensor refers to its dimensionality .",
    "thus , if @xmath5",
    "then we say @xmath4 is a third - order tensor .",
    "a @xmath6 tensor is called a _",
    "tube fiber_. a graphical illustration of a third - order tensor decomposed into its tube fibers is given in the upper right image of fig .",
    "[ fig : fibersslices ] .",
    "thus , one way to view a third - order tensor is as a matrix of tube fibers .",
    "in particular , an @xmath7 tensor is a vector of tube fibers . to make this clear",
    ", we use the notation @xmath8 to denote the @xmath9th `` column '' or _ lateral slice _ of the third - order tensor ( see the middle figure of the bottom row of fig .",
    "[ fig : fibersslices ] ) .",
    "the @xmath10th _ frontal slice _ , which is an @xmath11 matrix , is denoted by @xmath12 .",
    "frontal slices and other decompositions of a third - order tensor are all shown in fig .",
    "[ fig : fibersslices ] .",
    "+    we can consider an @xmath13 tensor is a matrix oriented into the third dimension .",
    "it will therefore be useful to use notation from @xcite that allows us to easily move between @xmath14 matrices and their @xmath13 counterparts . specifically , the @xmath15 operation on @xmath16 is identical to the @xmath15 function in matlab : @xmath17    the @xmath18 function unwraps the tensor @xmath4 into a vector of length @xmath19 by column stacking of frontal slices , i.e. , in matlab notation :  @xmath20 . for the tensor @xmath4",
    "we define the ` unfold ` and ` fold ` functions in terms of frontal slices : @xmath21 the block circulant matrix of size @xmath22 that is generated via @xmath23 is given as @xmath24    [ def : tprod ] let @xmath25 and @xmath26 . then the * t - product * from @xcite",
    "is defined by @xmath27 from which it follows that @xmath4 is an @xmath28 tensor .",
    "the t - product can be considered as a natural extension of the matrix multiplication  @xcite . in general",
    "the t - product is not commutative between two arbitrary tensors , but it is commutative between tube fibers .    given @xmath29 tube fibers @xmath30 , @xmath31 a * t - linear combination * @xcite of the lateral slices @xmath32 , @xmath31 , is defined as @xmath33 where @xmath34 the multiplication @xmath35 is not defined unless @xmath36 .",
    "the * identity tensor * @xmath37 is the tensor whose first frontal slice is the @xmath38 identity matrix , and whose other frontal slices are all zeros .",
    "an @xmath39 tensor @xmath4 has an * inverse * @xmath40 , provided that @xmath41    following @xcite , if @xmath4 is @xmath42 , then the * transposed tensor * @xmath43 is the @xmath44 tensor obtained by transposing each of the frontal slices and then reversing the order of transposed frontal slices @xmath45 through @xmath46 .",
    "let @xmath47 be the @xmath48 element of @xmath4 .",
    "then the * frobenius norm * of the tensor @xmath4 is @xmath49    we also use the following notation : @xmath50 if @xmath3 is a matrix then @xmath51 . let @xmath52 , @xmath53 denote the singular values of @xmath3 .",
    "the _ nuclear norm _",
    "( also known as the trace norm ) is defined as @xmath54 where @xmath55 is the the transpose of @xmath3 .",
    "in classical dictionary learning , the problem is to find `` basis elements '' and sparse representations of the training signals / images .",
    "that is , we want to write the input vectors as a weighted linear combination of a small number of ( unknown ) basis vectors .    in practice , to find localized features of a training image ( and to reduce the computational work ) , training patches of smaller size are extracted .",
    "let the patches be of size @xmath56 and let the matrix @xmath57 \\in   \\mathbb{r}^{pr \\times t}$ ] consist of @xmath58 training patches arranged as vectors / columns of length @xmath59 .",
    "the resulting dictionary learning problem , based on this matrix formulation , is then defined as follows : for the given data matrix @xmath60 and a given number of elements @xmath61 , find two matrices @xmath62 and @xmath63 , whose product approximates @xmath60 as well as possible :  @xmath64 .",
    "the columns of the _ dictionary matrix _",
    "@xmath65 ( often with @xmath66 ) represents a `` basis '' of @xmath61 image patches , and the matrix @xmath67 contains the representation of each training image patch approximated by the dictionary elements .",
    "it is often assumed that , as @xmath60 is non - negative , @xmath65 and @xmath67 should be constrained to be non - negative as well .",
    "even with a non - negativity constraint , the decomposition is not unique  @xcite .",
    "other constraints , such as sparseness , statistical independence , and low complexity are often exploited in forming the basis and the representation .",
    "different prior considerations lead to different learning methods such as non - negative matrix factorization ( nmf ) @xcite , mod @xcite , k - svd @xcite , the online dictionary learning method @xcite and many more ( see , e.g. , @xcite and @xcite ) .",
    "a framework for tomographic image reconstruction using matrix dictionary priors was developed and described in @xcite . in tomography , a signal @xmath68 is measured from rays or signals passing through an object of interest .",
    "the discretized tomographic model is represented by an @xmath69 matrix @xmath3 .",
    "considering an unknown vector @xmath70 as vector of pixels / voxels for the reconstructed image ; this yields a linear system @xmath71 with an ill - conditioned matrix  @xmath3 .",
    "generally the image @xmath70 is not sparse but the situation changes when we know that @xmath70 has a sparse representation in terms of a known basis .    using a vectorized formulation in @xcite , a global matrix dictionary @xmath72",
    "is formed from the learned patch dictionary @xmath65 , and the linear tomographic problem is solved such that the vectorized image @xmath70 is a conic combination of a small number of dictionary elements , i.e. , @xmath73 and @xmath74 is sparse , meaning that the solution lies in the cone spanned by dictionary images but that many of the weights are zero .",
    "the simplicity of this approach is that once the basis elements have been determined , the solution is linear in these new variables .",
    "images are naturally two - dimensional objects and we find that it is fundamentally sound to work with them in their natural form .",
    "for example we are looking for correlations image - to - image ( not just pixel - to - pixel ) that let us reduce the overall redundancy in the data .",
    "therefore , we will consider a collection of training patches as a third - order tensor , with each 2d image making up a slice of the data tensor .",
    "it is natural to use higher - order tensor decomposition approaches , which are nowadays frequently used in image analysis and signal processing @xcite .",
    "we describe this approach in more detail in the next section .",
    "in recent years there has also been an increasing interest in obtaining a non - negative _ tensor _ factorization ( ntf ) ( often based on cp and tucker decompositions ) as a natural generalization of the nmf for a nonnegative data .",
    "similar to nmf , the sparsity of the representation has been empirically observed in ntf based on cp and tucker decompositions . for ntf based on a subset of tensor decomposition methods ,",
    "we refer to @xcite .",
    "unlike the work in @xcite , we express the dictionary learning problem in a third - order tensor framework based on the t - product .",
    "this will be described in detail below , but the key is a t - product - based ntf reminiscent of the nmf .",
    "the ntf based on the t - product was proposed in @xcite , where preliminary work with mri data showed the possibility that sparsity is encouraged when non - negativity is enforced . here",
    ", we extend the work by incorporating sparsity constraints and we provide the corresponding optimization algorithm . given the patch tensor dictionary @xmath75 , we compute reconstructed images that have a sparse representation in the space defined by the t - product and @xmath75 . thus , both the dictionary and the sparsity of the representation serve to regularize the ill - posed problem .",
    "let the third - order _ data tensor _",
    "@xmath76 consist of @xmath58 training image patches of size @xmath56 , arranged as the lateral slices of @xmath77 , i.e. , @xmath78 our non - negative tensor decomposition problem , based on the t - product , is the problem of writing the non - negative data tensor as a product @xmath79 of two tensors @xmath80 and @xmath81 .",
    "the tensor @xmath75 consists of @xmath61 dictionary 2d image patches of size @xmath56 arranged as the lateral slices of @xmath75 , while @xmath82 is the tensor of coefficients .",
    "the main difference between ntf and nmf is that the @xmath83 tensor @xmath82 has @xmath84 times more degrees of freedom in the representation than the @xmath85 matrix @xmath67 .",
    "the t - product from definition [ def : tprod ] involves unfolding and forming a block circulant matrix of the given tensors .",
    "using the fact that a block circulant matrix can be block - diagonalized by the discrete fourier transform ( dft ) @xcite , the t - product is computable in the fourier domain @xcite .",
    "specifically , we can compute @xmath79 by applying the dft along tube fibers of @xmath75 and @xmath82 : @xmath86 where @xmath87 denotes dft ; in matlab notation we apply the dft across the third dimension:@xmath88,3)}$ ] . working in the fourier domain",
    "conveniently reduces the number of arithmetic operations @xcite , and since the operation is separable in the third dimension it allows for parallelism .",
    "although the representation of the training patches in the third - order tensor resembles the matrix formulation , it is not a re - formulation of the matrix problem packaged as tensors .",
    "in fact , the tensor formulation gives a richer approach of formulating the problem , as we now describe .",
    "recall that the @xmath9th patch @xmath89 is the @xmath9th lateral slice of @xmath90 , i.e. , @xmath91 .",
    "hence , as shown in @xcite , @xmath92 in other words , the @xmath9th patch is a sum over all the lateral slices of @xmath75 , each one `` weighted '' by multiplication with a circulant matrix derived from a tube fiber of  @xmath82 .",
    "we use a small example to show why this is significant . consider the @xmath93 downshift matrix and the ( column ) circulant matrix generated by the vector @xmath94 : @xmath95 = \\mathtt{circ}(v ) = \\begin{pmatrix } v_1 & v_3 & v_2 \\\\ v_2 & v_1 & v_3",
    "\\\\        v_3 & v_2 & v_1 \\end{pmatrix } .\\ ] ] noting that @xmath96 = \\sum_{k=1}^3 v_k z^{k-1 } = v_1 i +      v_2 \\begin{pmatrix } 0 \\ & 0 \\ & 1 \\\\ 1 \\ & 0 \\ & 0 \\\\ 0 \\ & 1 \\ & 0 \\end{pmatrix } +      v_3 \\begin{pmatrix } 0 \\ & 1 \\ & 0 \\\\ 0 \\ & 0 \\ & 1 \\\\ 1 \\ & 0 \\ & 0 \\end{pmatrix } .\\ ] ] it follows that @xmath97 = \\sum_{k=1}^3 v_k d z^{k-1}.\\ ] ] extrapolating to ( [ eq : rep ] ) , we obtain the following result .",
    "let @xmath98 denote the @xmath99 downshift matrix . with @xmath100 and @xmath101 ,",
    "the @xmath9th image patch is given by @xmath102      = \\sum_{i=1}^s \\left ( h^{(ij)}_1 d_i + \\sum_{k=2}^n h^{(ij)}_k d_i z^{k-1 } \\right ) .\\ ] ]    to show the relevance of this result we note that the product @xmath103 is @xmath104 with its columns cyclically shifted left by @xmath105 columns .",
    "assuming that @xmath104 represents a `` prototype '' element / feature in the image , we now have a way of also including shifts of that prototype in our dictionary _ without _ explicitly storing those shifted bases in the dictionary .",
    "note that if @xmath106 , @xmath107 then @xmath89 is a ( standard ) linear combination of matrices @xmath104 ; this shows that our new approach effectively subsumes the matrix - based approach from @xcite , while making the basis richer with the storage of only a few entries of a circulant matrix rather than storing extra basis image patches !",
    "one is usually not interested in a perfect factorization of the data because over - fitting can occur , meaning that the learned parameters do fit well the training data , but have a bad generalization performance .",
    "this issue is solved by making a priori assumptions on the dictionary and coefficients .",
    "based on the approximate decomposition @xmath108 , we consider the generic tensor - based dictionary learning problem ( similar to the matrix formulation from @xcite ) : @xmath109 the misfit of the factorization approximation is measured by the loss function @xmath110 , ( e.g. , the frobenius norm ) .",
    "different priors on the dictionary @xmath75 and the representation tensor @xmath82 are controlled by the regularization functions @xmath111 and @xmath112 .",
    "ntf itself results in a sparse representation .",
    "imposing sparsity - inducing norm constraints on the representation allows us to further enforce sparsity of the representation of the training image , i.e. , the training patches being represented as a combination of a small number of dictionary elements . at the same time this alleviates the non - uniqueness drawback of the ntf .",
    "therefore , we pose the tensor dictionary learning problem as a non - negative sparse coding problem @xcite : @xmath113 here @xmath114 is a closed set defined below , @xmath115 denotes the indicator function of a set @xmath98 , and @xmath116 is a regularization parameter that controls the sparsity - inducing penalty @xmath117 .",
    "if we do not impose bound constraints on the dictionary elements , then the dictionary and coefficient tensors @xmath75 and @xmath82 can be arbitrarily scaled , because for any @xmath118 we have @xmath119 .",
    "we define the compact and convex set @xmath114 such that @xmath120 prevents this inconvenience : @xmath121 when @xmath122 then collapses to the standard non - negative sparse coding problem .",
    "the optimization problem is non - convex , while it is convex with respect to each variable @xmath75 or @xmath82 when the other is fixed .",
    "computing a local minimizer can be done using the alternating direction method of multipliers ( admm )  @xcite , which is a splitting method from the augmented lagrangian family .",
    "we therefore consider an equivalent form of : @xmath123 where @xmath124 and @xmath125 .",
    "the augmented lagrangian for is @xmath126 where @xmath127 and @xmath128 are lagrange multiplier tensors , @xmath129 is the quadratic penalty parameter , and @xmath130 denotes the hadamard ( entrywise ) product .    the objective function becomes separable by introducing the auxiliary variables @xmath131 and @xmath132 . the alternate direction method is obtained by minimizing @xmath133 with respect to @xmath75 , @xmath82 , @xmath131 , @xmath132 one at a time while fixing the other variables at their most recent values and updating the lagrangian multipliers @xmath134 and @xmath135",
    ". if @xmath136 is the metric projection on @xmath114 ( which is computed using dykstra s alternating projection algorithm @xcite ) , then the admm updates are given by :    [ update ] @xmath137    here @xmath138 and @xmath139 denotes soft thresholding .",
    "the updates for @xmath140 and @xmath141 are computed in the fourier domain .",
    "the kkt - conditions for can be expressed as @xmath142 @xmath143 @xmath144 where @xmath145 denotes the sub - differential of @xmath146 at @xmath147 .",
    "the kkt conditions are used to formulate stopping criteria for the admm algorithm , and we use the following conditions :    [ stcrit ] @xmath148    where @xmath149 is a given tolerance .",
    "algorithm [ alg1 ] summarizes the algorithm to solve .",
    "note that satisfaction of the kkt conditions produces a local minimum ; this is not a guarantee of convergence to the global optimum .    :",
    "tensor of training image patches @xmath150 , number of dictionary images @xmath61 , tolerances @xmath151 .",
    ": tensor dictionary @xmath152 , tensor representation @xmath153 .",
    ": let the lateral slices of @xmath131 be randomly selected training patches , let @xmath132 be the identity tensor , let @xmath154 , and let @xmath155 be zero tensors of appropriate sizes .    under rather mild conditions",
    "the admm method can be shown to converge for all values of the algorithm parameter @xmath156 in the lagrange function @xmath133 , cf .",
    "small values of @xmath156 lead to slow convergence ; larger values give faster convergence but puts less emphasis on minimizing the residual for the ntf .",
    "for the convergene properties of admm and the impact of the parameter @xmath156 see @xcite and the references therein .",
    "recall that a linear tomographic problem is often written @xmath157 with @xmath158 , where the vector @xmath70 represents the unknown @xmath1 image , the vector @xmath68 is the inaccurate / noisy data , and the matrix @xmath3 represents the forward tomography model .",
    "since we assume that the vector @xmath70 represents an image of absorption coefficients we impose a nonnegativity constraint on the solution .    without loss of generality",
    "we assume that the size of the image is a multiple of the patch sizes in the dictionary .",
    "we partition the image into @xmath159 non - overlapping patches of size @xmath160 , i.e. , @xmath161 for @xmath162 .    in the matrix - based formulation of the reconstruction problem @xcite ,",
    "once the patch dictionary is formed we write the image patches we want to recover ( subvectors of the reconstructed image @xmath70 ) as conic combinations of the patch dictionary columns .",
    "the inverse problem then becomes one of recovering the expansion coefficients subject to non - negativity constraints ( which produces a nonnegative @xmath70 because the dictionary elements are nonnegative ) .",
    "here we define a similar reconstruction problem in our tensor - based formulation .",
    "we arrange all the patches @xmath163 of the reconstructed image as lateral slices of a @xmath164 tensor @xmath147 , i.e. , @xmath165 moreover , we assume that there exists a @xmath166 coefficient tensor @xmath167 such that the image patches can be written as t - linear combinations of the patch dictionary elements , i.e. , @xmath168 where the tube fibers of @xmath169 can be considered as the expansion coefficients . in other words , we restrict our solution so that it is a t - linear combination of the dictionary images .",
    "then , similar to , each patch @xmath163 in the reconstruction can be built from the matrices @xmath170 , @xmath171 : @xmath172 since the circulant matrices are not scalar multiples of the identity matrix , @xmath163 is not a simple linear combination of the matrices @xmath170 .",
    "thus , we want to find a tensor @xmath167 such that @xmath173 solves the reconstruction problem , and to ensure a nonnegative reconstruction , we enforce non - negativity constraints on @xmath167 .",
    "then we write the vectorized image as @xmath174 , where the permutation matrix @xmath175 ensures the correct shuffling of the pixels from the patches .",
    "then our generic reconstruction problem takes the form @xmath176 the data fidelity is measured by the loss function @xmath177 , and regularization is imposed via @xmath178 which enforces a arsity prior on @xmath179 , and @xmath180 which enforces an age prior on the reconstruction . by choosing these three functions to be convex , we can solve by means of convex optimization methods .",
    "our patches are non - overlapping because overlapping patches tend to produce blurring in the overlap regions of the reconstruction .",
    "non - overlapping patches may give rise to block artifacts in the reconstruction , because the objective in the reconstruction problem does not penalize jumps across the values at the boundary of neighboring patches . to mitigate this type of jumps ,",
    "we add an image penalty term @xmath181 that discourages such artifacts , where @xmath182 is a regularization parameter , and the function @xmath183 is defined by @xmath184 the matrix @xmath185 is a defined such that @xmath186 is a vector with finite - difference approximations of the vertical / horizontal derivatives across the patch boundaries .",
    "the denominator is the total number of pixels along the boundaries of the patches in the image .",
    "we consider two different ways to impose a sparsity prior on @xmath167 in the form @xmath187 , @xmath188 , where @xmath189 is a regularization parameter and @xmath190 in which the @xmath191 matrix @xmath192 is defined as @xmath193 the first prior @xmath194 corresponds to a standard sparsity prior in reconstruction problems .",
    "the second prior @xmath195 , which tends to produce a sparse and low - rank @xmath192 , is inspired by a similar use in compressed sensing  @xcite .",
    "to summarize , we consider a reconstruction problem of the form @xmath196",
    "\\mbox{subject to } \\        & { \\mathcal{c } } \\geq 0 ,      \\end{array }   \\end{aligned}\\ ] ] where @xmath189 and @xmath182 are regularization parameters .",
    "we note that is a convex but non - differentiable optimization problem .",
    "it is solved using the software package tfocs ( templates for first - order conic solvers ) @xcite .",
    "the implementation details are included in the appendix .",
    "we note that imposing the non - negativity constraint on the solution implies that each image patch @xmath163 belongs to a closed set defined by @xmath197 the set @xmath198 is a cone , since for any @xmath199 and any nonnegative tube fiber @xmath200 the product @xmath201 belongs to @xmath198 . clearly , if the dictionary @xmath75 contains the standard basis that spans @xmath202 then @xmath198 is equivalent to the entire nonnegative orthant @xmath202 , and any image patch @xmath163 can be reconstructed by a t - linear combination of dictionary basis images . however",
    ", in the typical case where @xmath198 is a proper subset of @xmath202 then not all nonnegative images have an exact representation in @xmath198 , leading to an approximation error .",
    "we conclude with computational tests to examine the tensor formulation .",
    "all experiments are run in matlab ( r2014a ) on a 64-bit linux system .",
    "the reconstruction problems are solved using the software package tfocs version 1.3.1 @xcite and compared with results from the matrix - based approach  @xcite .",
    "we use a @xmath203 high - resolution photo of peppers ; from this image we extract the @xmath204 training image patches , as well as the @xmath205 ground - truth or exact image @xmath206 , see fig .",
    "[ fig : images ] . the exact image is not contained in the training set , so that we avoid committing an inverse crime .",
    "all the images are gray - level and scaled in the interval @xmath207 $ ] .",
    "patch sizes should be sufficiently large to capture the desired structure in the training images , but the computational cost of the dictionary learning increases with the patch size .",
    "a study of the patch size @xmath56 and number @xmath61 of elements in @xcite shows that a reasonably large patch size gives a good trade - off between the computational work and the approximation error by the dictionary , and that the over - representation factor @xmath208 can be smaller for larger patches .",
    "for these reasons , we have chosen @xmath209 and ( unless otherwise noted ) @xmath210 for both the dictionary learning and tomographic reconstruction studies .",
    "we extract 52,934 patches from the high - resolution image and apply algorithm  [ alg1 ] to learn the dictionary .",
    "the tensor dictionary @xmath75 and the coefficient tensor @xmath82 are @xmath211 and @xmath212 , respectively .",
    "convergence plots for @xmath213 , 1 , and 10 are shown in fig .",
    "[ fig : conv ] . for @xmath214",
    "we put emphasis on minimizing the sparsity penalty , and after about 200 iterations we have reached convergence where the residual term dominates the objective function . for @xmath215 we put more emphasis on minimizing the residual term , and we need about 500 iterations to converge ; now the objective function is dominated by the sparsity penalty .     for @xmath213 , 1 , and 10 .",
    "we plot @xmath216 versus the number of iterations .",
    "note the different scalings of the axes . ]     and @xmath67 , respectively , as functions of  @xmath217 ( small @xmath217 give a larger number of nonzeros ) . ]    next we consider the approximation errors mentioned in the previous section .",
    "following @xcite , a way to bound these errors is to consider how well we can approximate the exact image @xmath206 with patches in the cone @xmath198 defined by the dictionary .",
    "consider the @xmath218 approximation problems for all blocks @xmath219 , @xmath220 , of the exact ] if @xmath221 denotes the solution to the @xmath9th problem , then @xmath222 is the best approximation in @xmath198 of the @xmath9th block @xmath219 .",
    "we define the _ mean approximation error _ as @xmath223 the mae is defined analogously for the matrix formulation in @xcite .",
    "figure [ fig : dicplots ] shows how these maes vary with the number of nonzeros of @xmath82 and @xmath67 , as a function of @xmath217 , for both @xmath224 and @xmath210 .",
    "this plot shows that for a given number of nonzeros in @xmath82 or @xmath67 we obtain approximately the same mean approximation error . in other words",
    "despite the fact that the @xmath225 tensor @xmath82 has @xmath84 times more degrees of freedom in the representation than the @xmath226 matrix @xmath67 , we do not need more nonzero values to represent our training images .",
    "hence the number of nonzeros is not a decisive factor .",
    "that minimizes @xmath227 . ]    in fig .",
    "[ fig : dicplots ] we note that for large enough @xmath217 both @xmath82 and @xmath67 consist entirely of zeros , in which case the dictionaries @xmath75 and @xmath65 are solely determined by the constraints . hence , as @xmath217 increases the mae settles at a value that is almost independent on  @xmath217 .    to determine a suitable value of the regularization parameter @xmath217 in we plot the residual norm @xmath228 versus @xmath117 for various @xmath229 $ ] in fig .",
    "[ fig : x ] .",
    "we define the optimal parameter to be the one that minimizes @xmath230 , which is obtained for @xmath231 , and we use this value throughout the rest of our experiments .     patches and @xmath231 and @xmath210.,title=\"fig : \" ]   patches and @xmath231 and @xmath210.,title=\"fig : \" ]    figure [ fig : dicimages ] shows examples of tensor and matrix dictionary elements / images , where lateral slices of the tensor dictionary and columns of the matrix dictionary are represented as images .",
    "the dictionary images are sorted according to increasing variance .",
    "the tensor and matrix dictionary images are different but they are visually similar .",
    ", for both the tensor and matrix formulations .",
    "left :  the density of @xmath82 and @xmath67 .",
    "right :  the mae associated with the dictionaries.,title=\"fig : \" ] , for both the tensor and matrix formulations . left :  the density of @xmath82 and @xmath67 .",
    "right :  the mae associated with the dictionaries.,title=\"fig : \" ]    we conclude these experiments with a study of how the number @xmath61 of dictionary elements influences the dictionary , for the fixed @xmath232 . specifically , fig .",
    "[ fig : densitymaes ] shows how the density and the mae varies with @xmath61 in the range from 100 to 500 .",
    "as we have already seen for @xmath210 the density of @xmath82 is consistently much lower than that of @xmath67 , and it is also less dependent on @xmath61 in the tensor formulation .",
    "we also see that the mae for the tensor formulation is consistently lower for the tensor formulation :  even with @xmath233 dictionary elements in the matrix formulation we can not achieve the tensor formulation s low mae for @xmath234 .",
    "these results confirm our intuition that the tensor formulation is better suited for sparsely representing the training image , because due to the ability of capturing repeating fectures we can use a much smaller dictionary .      in this section we present numerical experiments for 2d tomographic reconstruction in few - projection and noisy settings .",
    "we perform two different experiments to analyze our algorithm : first we examine the role of different regularization terms and then we study the reconstruction quality in different tomography scenarios .",
    "we also present results using a more realistic test problem .",
    "recall from section [ sec : rec ] that our reconstruction model has the form @xmath235 with @xmath236 .",
    "we consider parallel - beam geometry and the test problem is generated by means of the function ` paralleltomo ` from air tools @xcite .",
    "the exact data is generated by the forward model @xmath237 , to which we add white gaussian noise .",
    "the accuracy of the reconstruction is measured by the relative 2-norm error @xmath238 we also report the structural similarity index measure ( ssim ) @xcite ( recall that a larger ssim means a better reconstruction ) .",
    "we remind that the error is due to the combination of the approximation error , the error from the data , and the regularization error .    the parameters @xmath182 and @xmath189 in the reconstruction problem both play a role in terms of regularization ; to simplify we set @xmath239 .",
    "as described in @xcite , a nonnegative constraint in the reconstruction problem plays an extra role of regularization and therefore the reconstruction is not very sensitive to the regularization parameters @xmath182 and @xmath240 , hence they are chosen from a few numerical experiments such that a solution with the smallest error is obtained .",
    "we compare our method with filtered back projection ( fbp ) , tikhonov regularization , and total variation ( tv ) .",
    "the fbp solution is computed using matlab s @xmath241 function with the `` shepp - logan '' filter .",
    "the tikhonov solution solves the problem @xmath242 where @xmath243 is the tikhonov regularization parameter .",
    "the tv regularization problem has the form @xmath244 where @xmath245 computes a finite - difference approximation of the gradient at each pixel , and @xmath246 is the tv regularization parameter .",
    "we solve this problem with the software tvreg @xcite .",
    "the tikhonov and tv regularization parameters are chosen to yield the smallest reconstruction error .",
    "the computational bottleneck of the objective function evaluation in solving is calculating @xmath247 , where @xmath248 and @xmath249 . recall that the computation is done in the fourier domain , and since @xmath250 the computational complexity of the t - product is @xmath251 @xcite . in the matrix formulation @xcite",
    "the computational bottleneck is the matrix multiplication @xmath252 where @xmath62 and @xmath253 , also with complexity  @xmath254 .",
    "this gives the tensor formulation an advantage , since we can use a much smaller @xmath61 here , say , 23 times smaller than in the matrix formulation .",
    "since computation times vary between different computers , and since we did not pay specific attention to efficiency , we report the number of objective function evaluations returned by tfocs .",
    "we stop the iterations when the relative change in the iteration vector is less than @xmath255 . for the comparison to be fair , the starting point in all the computations",
    "is the zero vector / matrix of appropriate size .",
    "we solve the reconstruction problem using the exact image shown in fig .",
    "[ fig : images ] .",
    "moreover , we use @xmath256 patches , @xmath257 , and @xmath232 . for the problems in this section we use @xmath258 projections , @xmath259 rays per projection , and 1% noise .",
    "we compare two different regularization terms in the reconstruction problem .",
    "the 1-norm ( sparsity ) regularization @xmath260 is similar to the 1-norm regularization in the dictionary learning problem  . the regularization term @xmath261 results in coefficient tensors that are simultaneously low rank and sparse .",
    "+    .comparison of the best solutions computed by different reconstruction methods .",
    "the bold numbers indicate the lowest iteration number , density of @xmath167 and compression percentages , and highest ssim measure . [",
    "cols=\"<,<,<,<,<,<\",options=\"header \" , ]     figure [ fig : imres3 ] shows the reconstructed images with the matrix and tensor formulations .",
    "all regularization parameters are chosen empirically to give the smallest reconstruction errors .",
    "all three reconstructions are similar , since the reconstruction errors are dominated by the error coming from the regularization of the noisy data .",
    "more data are given in table [ tab:3 ] .",
    "imposing the sparsity prior @xmath262 in the tensor formulation produces the sparsest representation .",
    "the solution is computed in fewer iterations with the @xmath263 regularization term while the reconstruction has a negligible improvement in terms of re and ssim .",
    "we conclude that our tensor algorithm is also well suited for more realistic tomographic problems .",
    "we presented the problem of dictionary learning in a tensor formulation and focused on solving the tomographic image reconstruction in the context of a t - product tensor - tensor factorization .",
    "we posed the tensor dictionary learning problem as a non - negative sparse tensor factorization problem , and we computed a regularized nonnegative reconstruction in the tensor - space defined by the t - product .",
    "we also gave an algorithm based on the alternating direction method of multipliers ( admm ) for solving the tensor dictionary learning problem and , using the tensor dictionary , we formulated a convex optimization problem to recover the solution s coefficients in the expansion under the t - product .",
    "we presented numerical experiments on the properties of the representation in the learned tensor dictionary in the context of tomographic reconstruction .",
    "the dictionary - based reconstruction quality is superior to well know classical regularization schemes , e.g. , filtered back projection and total variation , and the solution representation in terms of the tensor dictionary is more sparse compared to a similar matrix dictionary representations @xcite . in future work",
    "the authors would like to further study the tensor dictionary representation property using other products from the family of tensor - tensor products introduced , e.g. , in @xcite .",
    "the authors acknowledge collaboration with martin s. andersen from dtu compute .",
    "we would like to thank professor samuli siltanen from university of helsinki for providing the high - resolution image of the peppers and dr .",
    "hamidreza abdolvand from university of oxford for providing the zirconium image .",
    "the reconstruction problem is a convex , but @xmath260 and @xmath264 are not differentiable which rules out conventional smooth optimization techniques .",
    "the tfocs software @xcite provides a general framework for solving convex optimization problems , and the core of the method computes the solution to a standard problem of the form @xmath265 where the functions @xmath266 and @xmath267 are convex , @xmath3 is a linear operator , and @xmath68 is a vector ; moreover @xmath266 is smooth and @xmath267 is non - smooth .",
    "to solve problem by tfocs , it is reformulated as a constrained linear least squares problem : @xmath268 \\nicefrac{\\delta}{c } \\ , l \\end{pmatrix }      \\pi \\mathtt{vec}(\\mathcal{d}*\\mathcal{c } )      - \\begin{pmatrix } b \\\\ 0 \\end{pmatrix } \\right\\|_2 ^ 2 +      \\mu \\ , \\varphi_{\\nu}(\\mathcal{c } )      \\qquad \\text{s.t . }",
    "\\qquad   \\mathcal{c } \\geq 0,\\ ] ] where @xmath269 .",
    "referring to , @xmath270 is the squared 2-norm residual and @xmath271 .",
    "the methods used in tfocs require computation of the proximity operators of the non - smooth function @xmath267 .",
    "the proximity operator of a convex function is a natural extension of the notion of a projection operator onto a convex set @xcite .",
    "let @xmath272 and @xmath273 be defined on the set of real - valued matrices and note that @xmath274 . for @xmath275",
    "consider the minimization problem @xmath276 whose unique solution is @xmath277 . while the prox operators for @xmath278 and @xmath264 are easily computed",
    ", the prox operator of the sum of two functions is intractable .",
    "although the tfocs library includes implementations of a variety of prox operators  including norms and indicator functions of many common convex sets  implementation of prox operators of the form @xmath279 is left out .",
    "hence we compute the prox operator for @xmath280 iteratively using a dykstra - like proximal algorithm @xcite , where prox operators of @xmath281 and @xmath282 are consecutively computed in an iterative scheme .",
    "let @xmath283 .",
    "for @xmath284 and @xmath285 , @xmath286 is the one - sided elementwise shrinkage operator @xmath287 the proximity operator of @xmath288 has an analytical expression via the singular value shrinkage ( soft threshold ) operator @xmath289 where @xmath290 is the singular value decomposition of @xmath291  @xcite .",
    "the computation of @xmath292 can be done very efficiently since @xmath192 is @xmath191 with @xmath293 .",
    "the iterative algorithm which computes an approximate solution to @xmath294 is given in algorithm [ alg2 ] .",
    "every sequence @xmath295 generated by algorithm [ alg2 ] converges to the unique solution @xmath294 of problem @xcite .",
    "bian , j. , siewerdsen , j.h .",
    ", han , x. , sidky , e.y . , prince , j.l . ,",
    "pelizzari , c.a . , and pan , x. : evaluation of sparse - view reconstruction from flat - panel - detector cone - beam ct . phys",
    "55 * , 65756599 ( 2010 ) .",
    "boyd , s. , parikh , n. , chu , e. , peleato , b. , and eckstein , j. : distributed optimization and statistical learning via the alternating direction method of multipliers . found .",
    "trends mach .",
    "* 3 * , 1122 ( 2011 )            cichocki , a. , zdunek , r. , phan , a.h . , and amari , s. : nonnegative matrix and tensor factorizations : applications to exploratory multi - way data analysis and blind source separation .",
    "john wiley & sons , ltd ( 2009 )    combettes , p.l . and pesquet , j. : proximal splitting methods in signal processing . in : bauschke , h.h .",
    "et al . ( eds . )",
    "fixed - point algorithms for inverse problems in science and engineering , pp .",
    "185212 , springer , new york ( 2011 )    donoho , d. , and stodden , v. : when does non - negative matrix factorization give a correct decomposition into parts ?",
    ", in thrun , s. , saul , l. , and schlkopf , b. , ( eds . ) , adv .",
    "neural inf . process .",
    "* 16 * , mit press , cambridge , ma . ( 2004 )    duan , g. , wang , h. , liu , z. , deng , j. , and , chen y .- w .",
    ": k - cpd : learning of overcomplete dictionaries for tensor sparse coding . in : ieee",
    "21st int . conf . on pattern recognition ( icpr ) ,",
    "pp . 493496 ( 2012 )",
    "ghadimi , e. , teixeira , a. , shames , i. , and johansson , m. : optimal parameter selection for the alternating direction method of multipliers ( admm ) :  quadratic problems .",
    "ieee trans .",
    "* 60 * , 644658 ( 2015 )    golbabaee , m. and vandergheynst , p. : hyperspectral image compressed sensing via low - rank and joint - sparse matrix recovery . in : ieee int .",
    "conf . on acoustics , speech and signal processing , pp .",
    "27412744 ( 2012 )                        kilmer , m.e . ,",
    "braman , k. , hao , n. , and hoover , r.c . : third - order tensors as operators on matrices : a theoretical and computational framework with applications in imaging .",
    "siam j. matrix anal .",
    "appl . * 34 * , 148172 ( 2013 )          liu , q. , liang , d. , song , y. , luo , j. , zhu , y. , and li , w. : augmented lagrangian - based sparse representation method with dictionary updating for image deblurring .",
    "siam j. imaging sci .",
    "* 6 * , 16891718 ( 2013 )          roemer , f. , del galdo , g. , and haardt , m. : tensor - based algorithms for learning multidimensional separable dictionaries . in : ieee int",
    ". conf . on acoustics , speech and signal processing , pp . 39633967 ( 2014 )              zhang z. , ely , g. , aeron , s. , hao n. , kilmer , m. : novel methods for multilinear data completion and de - noising based on tensor - svd . in : ieee conf . on computer vision and pattern recognition ( cvpr ) ,",
    "pp . 38423849 ( 2014 )"
  ],
  "abstract_text": [
    "<S> we consider tomographic reconstruction using priors in the form of a dictionary learned from training images . </S>",
    "<S> the reconstruction has two stages : first we construct a tensor dictionary prior from our training data , and then we pose the reconstruction problem in terms of recovering the expansion coefficients in that dictionary . </S>",
    "<S> our approach differs from past approaches in that a ) we use a third - order tensor representation for our images and b ) we recast the reconstruction problem using the tensor formulation . </S>",
    "<S> the dictionary learning problem is presented as a non - negative tensor factorization problem with sparsity constraints . </S>",
    "<S> the reconstruction problem is formulated in a convex optimization framework by looking for a solution with a sparse representation in the tensor dictionary . </S>",
    "<S> numerical results show that our tensor formulation leads to very sparse representations of both the training images and the reconstructions due to the ability of representing repeated features compactly in the dictionary . </S>"
  ]
}