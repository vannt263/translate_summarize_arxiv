{
  "article_text": [
    "in this paper , we use the following gaussian process @xmath3 with stationary increments as the driving noise process : @xmath4 where @xmath5 and @xmath6 are real constants such that @xmath7 and @xmath8 is a one - dimensional brownian motion satisfying @xmath9 . the parameters @xmath5 and @xmath6 describe the memory of @xmath10 .",
    "in the simplest case @xmath11 , @xmath10 is reduced to the brownian motion , i.e. , @xmath12 .",
    "it should be noticed that ( [ eq:1.1 ] ) is not a semimartingale representation of @xmath13 with respect to the natural filtration @xmath14 of @xmath13 since @xmath15 is not @xmath14-adapted . using the innovation theory as described in liptser and shiryayev @xcite and a result in anh et al .",
    "@xcite , we show ( theorem [ thm:2.1 ] ) that there exists a one - dimensional brownian motion @xmath16 , called the _ innovation process _ , satisfying @xmath17 and @xmath18 where @xmath19 is a volterra kernel given by @xmath20 with respect to the natural filtration @xmath21 of @xmath22 , which is equal to @xmath14 , ( [ eq:1.3 ] ) gives the semimartingale representation of @xmath10 .",
    "thus the process @xmath10 has the virtue that it possesses the property of a stationary increment process with memory and a simple semimartingale representation simultaneously .",
    "we know no other process with this kind of properties .",
    "the two properties of @xmath10 become a great advantage , for example , in its parameter estimation .    in @xcite and anh et al .",
    "@xcite , the process @xmath10 is used as the driving process for a financial market model with memory .",
    "the empirical study for s&p 500 data in anh et al .",
    "@xcite shows that the model captures very well the memory effect when the market is stable .",
    "the work in these references suggests that the process @xmath10 can serve as an alternative to brownian motion when the random disturbance exhibits dependence between different observations .    in this paper",
    ", we are concerned with the filtering problem of the two - dimensional partially observable process @xmath23 governed by the following linear system of equations : @xmath24 here @xmath25 and @xmath26 represent the state and the observation respectively . for @xmath27 , the noise process @xmath28",
    "is described by ( [ eq:1.1 ] ) with @xmath29 and @xmath30 .",
    "we assume that the brownian motions @xmath31 and @xmath32 , whence @xmath33 and @xmath34 , are independent .",
    "the coefficients @xmath35 with @xmath36 are known constants , and @xmath37 is a centered gaussian random variable independent of @xmath38 .",
    "the basic filtering problem for the linear model ( [ eq:1.5 ] ) with memory is to calculate the conditional expectation @xmath39 $ ] , called the ( optimal ) _ filter _ , where @xmath40 is the natural filtration of the observation process @xmath41 . in the classical kalman - bucy theory ( see kalman @xcite , kalman and bucy @xcite , bucy and joseph @xcite , davis @xcite and @xcite ) , brownian motion is used as the driving noise .",
    "attempts have been made to resolve the filtering problem of dynamical systems with memory by replacing brownian motion by other processes . in kleptsyna",
    "@xcite and others , fractional brownian motion was used .",
    "notice that fractional brownian motion is not a semimartingale . in the discrete - time setting , autoregressive processes are used as driving noise ( see , e.g. , @xcite , @xcite and jazwinski @xcite ) .",
    "our model may be regarded as a continuous - time analogue of the latter since it is shown in @xcite that @xmath10 is governed by a continuous - time ar@xmath42-type equation .",
    "the kalman - bucy filter is a computationally tractable scheme for the optimal filter of a markovian system .",
    "we aim to derive a similar effective filtering algorithm for the system ( [ eq:1.5 ] ) which has memory .",
    "however , rather than considering ( [ eq:1.5 ] ) itself , we start with a general continuous gaussian process @xmath43 as the state process and @xmath44 defined by @xmath45 as the observation process , where @xmath46 is a deterministic function and @xmath10 is a process which is independent of @xmath47 and given by ( [ eq:1.1 ] ) .",
    "using ( [ eq:1.3 ] ) and ( [ eq:1.4 ] ) , we derive explicit volterra integral equations for the optimal filter ( theorem [ thm:3.1 ] ) . in the special case ( [ eq:1.5 ] ) ,",
    "the integral equations are reduced to differential equations , which gives an extension to kalman - bucy filtering equations ( theorem [ thm:3.4 ] ) . due to the non - markovianness of the formulation ( [ eq:1.5 ] )",
    ", it is expected that the resulting filtering equations would appear in the integral equation form ( cf .",
    "kleptsyna et al .",
    "the fact that good kalman - bucy - type differential equations can be obtained here is due the special properties of ( [ eq:1.5 ] ) .",
    "this interesting result does not seem to hold for any other formulation where memory is inherent .",
    "we apply the results to an optimal portfolio problem in a partially observable financial market model .",
    "more precisely , we consider a stock price model that is driven by the solution @xmath10 to ( [ eq:1.1 ] ) .",
    "assuming that the investor can observe the stock price but not the drift process , we discuss the portfolio optimization problem of maximizing the expected logarithmic utility from terminal wealth . to solve this problem",
    ", we make use of our results on filtering to reduce the problem to the case where the drift process is adapted to the observation process .",
    "we then use the martingale methods ( cf .",
    "karatzas and shreve @xcite ) to work out the explicit formula for the optimal portfolio ( theorem [ thm:4.1 ] ) .",
    "this paper is organized as follows . in section [ sec:2 ] , we prove the semimartingale representation ( [ eq:1.3 ] ) with ( [ eq:1.4 ] ) for @xmath10 .",
    "section [ sec:3 ] is the main body of this paper .",
    "we derive closed form equations for the optimal filter . in section [ sec:4 ] , we apply the results to finance . in section [ sec:5 ] , we illustrate the advantage of @xmath10 in parameter estimation .",
    "some numerical results on monte carlo simulation are presented .",
    "finally , in section [ sec:6 ] , we numerically compare the performance of our filter with the kalman - bucy filter in the presence of memory effect .",
    "let @xmath48 , and let @xmath49 be a complete probability space . for a process @xmath50 ,",
    "we denote by @xmath51 the @xmath52-augmentation of the filtration @xmath53 , @xmath54 .",
    "let @xmath55 be the process given by ( [ eq:1.1 ] ) .",
    "the process @xmath56 is a continuous gaussian process with stationary increments .",
    "the aim of this section is to prove ( [ eq:1.3 ] ) with ( [ eq:1.4 ] ) .    by ( * ?",
    "* theorem 7.16 ) , there exists a one - dimensional brownian motion @xmath57 , called the innovation process , such that @xmath58 , @xmath54 and that @xmath59 \\qquad ( 0\\le t\\le t ) .",
    "\\notag\\end{gathered}\\ ] ] thus , @xmath60 is a @xmath61-semimartingale with representation ( [ eq:2.1 ] ) .    to find a good representation of @xmath62",
    ", we recall the following result from ( * ? ? ?",
    "* example 5.3 ) : @xmath63 with @xmath64 from the theory of volterra integral equations , there exists a function @xmath65 ^ 2 $ ] , called the resolvent of @xmath66 , such that , for almost every @xmath67 , @xmath68 ( see ( * ? ? ?",
    "* chapter 4 , section 3 ) and ( * ? ? ?",
    "* chapter 9 ) ) . using @xmath69",
    ", we have the following representation of @xmath70 in terms of the innovation process @xmath71 : @xmath72 we shall solve ( [ eq:2.2 ] ) explicitly to obtain @xmath19 .    [ thm:2.1 ] the expression @xmath73 holds .",
    "we have @xmath74 for @xmath75 , where , for @xmath76 $ ] , @xmath77 fix @xmath78 $ ] and define @xmath79 and @xmath80 by @xmath81 then , from ( [ eq:2.2 ] ) we obtain @xmath82 the solution @xmath83 is given by @xmath84 whence @xmath19 is obtained as @xmath85    we have @xmath86 by the change of variable @xmath87 , we obtain @xmath88 so that @xmath89 thus @xmath90 or ( [ eq:1.4 ] ) , as desired .",
    "let @xmath91 be a two - dimensional centered continuous gaussian process .",
    "the process @xmath25 represents the state process , while @xmath92 is another process which is related to the dynamics of @xmath25 .",
    "let @xmath16 be a one - dimensional brownian motion that is independent of @xmath93 .",
    "we define the processes @xmath10 and @xmath94 by ( [ eq:2.1 ] ) and ( [ eq:2.3 ] ) , respectively . in this subsection",
    ", we assume that @xmath19 in ( [ eq:2.3 ] ) is an arbitrary volterra - type bounded measurable function ( i.e. , @xmath95 for @xmath96 ) rather than the special form ( [ eq:1.4 ] ) . of course , the function @xmath19 in ( [ eq:1.4 ] ) satisfies this assumption .",
    "we consider the observation process @xmath97 satisfying @xmath98 where @xmath46 is a bounded measurable deterministic function on @xmath99 $ ] such that @xmath100 for @xmath101 .    as in section [ sec:2 ] , let @xmath102 be the augmented filtration generated by @xmath26 . for @xmath103-dimensional column vector processes @xmath50 and @xmath104 , we write @xmath105 \\qquad ( 0\\le t\\le t ) , \\\\",
    "\\gamma_{ac}(t , s)=\\mathbb{e}[a(t)c^{*}(s ) ] \\qquad ( 0\\le s\\le t\\le t),\\end{gathered}\\ ] ] where @xmath106 denotes the transposition .",
    "notice that @xmath107 .",
    "we put @xmath108 and define the error matrix @xmath109 by @xmath110\\qquad ( 0\\leq s\\leq t\\leq t).\\ ] ]    the next theorem gives an answer to the filtering problem for the partially observable process @xmath111 .",
    "it turns out that this will be useful in the filtering problem for ( [ eq:1.5 ] ) for example .",
    "[ thm:3.1 ] the filter @xmath112 satisfies the stochastic integral equation @xmath113 and the error matrix @xmath114 is the unique solution to the following matrix riccati - type integral equation such that @xmath115 is symmetric and nonnegative definite for @xmath54 : @xmath116 where @xmath117    since @xmath93 is independent of @xmath71 , @xmath118 forms a gaussian system .",
    "we have @xmath119 thus we can define the innovation process @xmath120 by @xmath121 which is a brownian motion satisfying @xmath122 ( cf .",
    "* theorem 7.16 ) ) .",
    "notice that @xmath120 can be written as @xmath123 by ( * ? ? ?",
    "* corollary of theorem 7.16 ) , there exists an @xmath124-valued volterra - type function @xmath125 on @xmath99 ^ 2 $ ] such that @xmath126 where @xmath127 denotes the euclidean norm    now let @xmath128 be an arbitrary bounded measurable row - vector function on @xmath99 $ ] .",
    "then , for @xmath76 $ ] , @xmath129=0.\\ ] ] from this , ( [ eq:3.3 ] ) , ( [ eq:3.4 ] ) and the fact that @xmath93 and @xmath71 are independent , we have @xmath130 \\\\   & = \\mathbb{e}\\left[\\int_0^t g(s)\\{(z(s)-\\widehat{z}(s))^{*}a(s)ds    + db(s)\\}\\cdot z(t)\\right]\\\\   & = \\int_0^t g(s)\\mathbb{e}[z(t)(z(s)-\\widehat{z}(s))^{*}]a(s)ds      + \\int_0^t g_3(s)l(t , s)ds \\\\   & = \\int_0^t g(s)p(t , s)a(s)ds      + \\int_0^t g(s)d(t , s)a(s)ds . \\end{split}\\ ] ] since @xmath131 is arbitrary , we deduce that @xmath132 or @xmath133    the sde ( [ eq:3.1 ] ) follows from ( [ eq:3.5 ] ) and the representation @xmath134 the equation ( [ eq:3.2 ] ) follows from ( [ eq:3.5 ] ) and the equality @xmath135-     \\mathbb{e}[\\widehat{z}(t)\\widehat{z}^{*}(s)].\\ ] ] the matrix @xmath115 is clearly symmetric and nonnegative definite .",
    "finally , the uniqueness of the solution to ( [ eq:3.2 ] ) follows from proposition [ prop:3.2 ] below .",
    "[ prop:3.2 ] the solution @xmath114 to the matrix integral equation @xmath136 such that @xmath115 is symmetric and nonnegative definite for @xmath54 is unique .",
    "by coninuity , there exists a positive constant @xmath137 such that @xmath138 where @xmath139 for @xmath140 .",
    "let @xmath141 be a solution to ( [ eq:3.2 ] ) such that @xmath115 is symmetric and nonnegative definite for @xmath76 $ ] .",
    "we put @xmath142 .",
    "then ( [ eq:3.2 ] ) with @xmath143 is @xmath144 from this , we have @xmath145 therefore , @xmath146 is at most @xmath147    let @xmath148 and @xmath149 be two solutions of ( [ eq:3.2 ] ) .",
    "we define @xmath150 for @xmath151 .",
    "we put @xmath152 for @xmath96 and @xmath151 .",
    "since @xmath153 and @xmath154 are bounded , it follows from the above estimate that there exists a positive constant @xmath155 satisfying @xmath156 it follows that @xmath157 from this and ( [ eq:3.2 ] ) , we obtain @xmath158 therefore , gronwall s lemma gives @xmath159 thus the uniqueness follows .",
    "we consider the case in which @xmath160 and the state process @xmath47 is the ornstein - uhlenbeck process satisfying @xmath161 where @xmath162 and @xmath163 is a one - dimensional brownian motion that is independent of @xmath164 .",
    "we also assume that @xmath165 a constant . then @xmath166 and @xmath167 by theorem [ thm:3.1 ] , we have @xmath168 where @xmath169",
    "$ ] for @xmath170 .",
    "let @xmath171 , @xmath101 , be the @xmath52-augmentation of the filtration generated by @xmath172 .",
    "then @xmath173 is @xmath174 &   = \\mathbb{e}[\\mathbb{e}[x(t)|\\mathcal{f}(s)](x(s)-\\widehat{x}(s ) ) ] \\\\ & = \\mathbb{e}[e^{\\theta ( t - s)}x(s)(x(s)-\\widehat{x}(s))]=e^{\\theta ( t - s)}\\gamma ( s ) \\end{split}\\ ] ] with @xmath175 $ ] .",
    "thus ( [ eq:3.6 ] ) is reduced to @xmath176 differentiating ( [ eq:3.7 ] ) in @xmath143 , we get @xmath177 the equations ( [ eq:3.8 ] ) and ( [ eq:3.9 ] ) are the well - known kalman - bucy filtering equations ( see @xcite , @xcite , @xcite , @xcite and @xcite ) .",
    "let @xmath23 be the two - dimensional partially observable process satisfying ( [ eq:1.5 ] ) . for @xmath27 , the noise process",
    "@xmath178 there is described by ( [ eq:1.1 ] ) with @xmath30 and @xmath29 satisfying ( [ eq:1.2 ] ) . the brownian motions @xmath31 and",
    "@xmath32 , whence @xmath33 and @xmath34 , are independent . in ( [ eq:1.5 ] ) , the coefficients @xmath179 with @xmath36 are known constants and the initial value @xmath37 is a gaussian random variable that is independent of @xmath38 .",
    "the processes @xmath25 and @xmath26 represent the state and the observation , respectively .",
    "by theorem [ thm:2.1 ] , we have @xmath180 with @xmath181 for @xmath27 and @xmath182 , where @xmath183 and @xmath184 are two independent brownian motions , and , for @xmath67 , @xmath185 we put @xmath186 for @xmath27 , that is , @xmath187 it holds that @xmath188 , where @xmath189 .",
    "we denote by @xmath171 , @xmath54 , the @xmath52-augmentation of the filtration @xmath190 , @xmath182 .",
    "[ lem:3.3 ] for @xmath67 , we have @xmath191 = e^{\\theta(t - s)}x(s)-\\sigma b(t - s)\\alpha_1(s),\\ ] ] where @xmath192    for @xmath76 $ ] , @xmath193 is @xmath194 since @xmath195 or @xmath196 @xmath197 $ ] with @xmath198 is equal to @xmath199 however , by elementary calculation , we have @xmath200 thus the lemma follows .",
    "we put , for @xmath54 , @xmath201 we also put @xmath202 recall that @xmath203 $ ] and that @xmath204 \\qquad ( 0\\le t\\le t).\\ ] ] we define the error matrix @xmath205 by @xmath206\\qquad ( 0\\le t\\le t).\\ ] ]    here is the solution to the optimal filtering problem for ( [ eq:1.5 ] ) .",
    "[ thm:3.4 ] the filter @xmath207 satisfies the stochastic differential equation @xmath208 with @xmath209,0,0)^{\\ast } $ ] , and @xmath210 follows the matrix riccati equation @xmath211 with @xmath212 $ ] for @xmath213 .    for @xmath67",
    ", we put @xmath214.\\ ] ] then we have @xmath215 .",
    "we also put , for @xmath67 , @xmath216    by @xmath217(z(s)-\\widehat{z}(s))^{*}]\\ ] ] and lemma [ lem:3.3 ] , we have @xmath218 , where @xmath219 with @xmath220 as in lemma [ lem:3.3 ] .",
    "we also see that @xmath221 . combining , @xmath222 . however , @xmath223 since @xmath224 and @xmath225 is the unit matrix .",
    "thus we obtain @xmath226    from ( [ eq:3.13 ] ) and theorem [ thm:3.1 ] with @xmath227 and @xmath228 , it follows that @xmath229 the sde ( [ eq:3.11 ] ) follows easily from ( [ eq:3.14 ] ) .",
    "differentiating both sides of ( [ eq:3.15 ] ) yields @xmath230 since @xmath231 with @xmath232 we see by integration by parts that @xmath233 is equal to @xmath234.\\ ] ] it follows that @xmath235 is equal to @xmath236 + \\mathbb{e}\\left[\\int_{0}^{t}dz(s)z(s)^{*}\\right]+\\mathbb{e}[r(t)r^{*}(t)]\\\\   & \\qquad=-\\int_0^t\\gamma_{zz}(s)f^{*}ds-\\int_0^t f\\gamma_{zz}(s)ds   + \\mathbb{e}[r(t)r^{*}(t ) ] .",
    "\\end{split}\\ ] ] thus @xmath237    = \\int_0^t\\left(g(s)+d(s)aa^{*}d^{*}(s)\\right)ds .",
    "\\end{split}\\ ] ] this and ( [ eq:3.16 ] ) yield ( [ eq:3.12 ] )",
    ".    we can easily extend theorem [ thm:3.4 ] to a more general setting where the functions @xmath238 take the form @xmath239 .",
    "[ cor:3.5 ] we assume that @xmath240 , i.e. , @xmath241 . let @xmath242 and @xmath243\\in\\mathbf{r}^{2\\times 2}$ ] . then the filter @xmath112 and the error matrix @xmath244 satisfy , respectively , @xmath245 , 0)^ { * } , \\\\",
    "\\frac{dp(t)}{dt}=g(t)-fp(t)-p(t)f^{*}-p(t)aa^{*}p(t ) , \\quad p_{ij}(0)=\\delta_{i1}\\delta_{j1}\\mathbb{e}[(x_0)^2],\\end{gathered}\\ ] ] where @xmath246    [ cor:3.6 ] we assume that @xmath247 , i.e. , @xmath248 . let @xmath249 and @xmath243 \\in\\mathbf{r}^{2\\times 2}$ ] . then the filter @xmath112 and the error matrix @xmath244 satisfy , respectively , @xmath250 with @xmath251 , 0)^{*}$ ] and @xmath252 $ ] , where @xmath253    we consider the estimation problem of the value of a signal @xmath254 from the observation process @xmath255 given by @xmath256 where @xmath10 and @xmath257 are as in section [ sec:2 ] .",
    "we assume that @xmath254 is a random variable having the normal distribution with mean zero and variance @xmath258 .",
    "this is the special case @xmath259 of the setting of corollary [ cor:3.6 ] .",
    "let @xmath260 and @xmath261 be as above .",
    "let @xmath262 and @xmath263 be as in corollary [ cor:3.6 ] with @xmath264 and @xmath265 .",
    "we define @xmath266 by @xmath267 $ ] with @xmath268",
    ". then , by corollary [ cor:3.6 ] , the filter @xmath269 satisfies @xmath270dt \\\\ &",
    "\\qquad \\qquad \\qquad \\qquad \\qquad + \\{p_{21}(t)-p_{22}(t)+l(t)\\}dy(t)\\end{aligned}\\ ] ] with @xmath271,0)$ ] , and the error matrix @xmath210 follows @xmath272.\\ ] ] by the linearization method as described in ( * ? ? ?",
    "* chapter 5 ) , we can solve the equation for @xmath210 to obtain @xmath273 where @xmath274 the analytical forms of @xmath275 , @xmath276 , @xmath277 and @xmath278 can be derived .",
    "we omit the details .",
    "in this section , we apply the results in the previous section to the problem of expected utility maximization for an investor with partial observations .",
    "let @xmath279 , @xmath280 , @xmath27 , be as in section [ sec:3 ] .",
    "in particular , @xmath33 and @xmath34 are independent .",
    "we consider the financial market consisting of a share of the money market with price @xmath281 at time @xmath76 $ ] and a stock with price @xmath282 at time @xmath76 $ ] .",
    "the stock price @xmath283 is governed by the stochastic differential equation @xmath284 where @xmath285 and @xmath286 are positive constants and @xmath92 is a gaussian process following @xmath287 the parameters @xmath288 , @xmath289 and @xmath290 are constants , and @xmath291 is a gaussian random variable that is independent of @xmath38 . for simplicity ,",
    "we assume that @xmath292    let @xmath171 , @xmath54 , be the @xmath52-augmentation of the filtration generated by @xmath293 and @xmath291 . then @xmath92 is @xmath294-adapted but not @xmath295-adapted ; recall from section [ sec:2 ] that @xmath295 is the augmented filtration generated by @xmath283 .",
    "suppose that we can observe neither the disturbance process @xmath34 nor the drift process @xmath92 but only the price process @xmath283 .",
    "thus only @xmath295-adapted processes are observable .    in many references such as @xcite , @xcite , and @xcite , the partially observable model described by ( [ eq:4.1 ] ) and ( [ eq:4.2 ] ) with @xmath2 s replaced by brownian motions , i.e. , @xmath296 ,",
    "is studied .",
    "we consider the following expected logarithmic utility maximization from terminal wealth : for given initial capital @xmath297 , @xmath298 \\quad \\mbox{over all   } \\pi\\in\\mathcal{a}(x),\\ ] ] where @xmath299 and @xmath300 the value @xmath301 is the dollar amount invested in the stock at time @xmath302 , whence @xmath303 is the number of units of stock held at time @xmath302 .",
    "the process @xmath304 is the wealth process associated with the self - financing portfolio determined uniquely by @xmath305 .",
    "an analogue of the problem ( [ eq:4.3 ] ) for full observations is solved in @xcite . for related work , see @xcite , @xcite , @xcite and the references therein .",
    "we solve the problem ( [ eq:4.3 ] ) by combining the results above on filtering and the martingale method as described in @xcite .",
    "consider the process @xmath306 defined by @xmath307 which we regard as the observation process .",
    "let @xmath308 be the innovation process associated with @xmath26 that is given by @xmath309 where @xmath310 $ ] as in the previous sections .",
    "the innovation process @xmath120 is a @xmath102-brownian motion satisfying @xmath311 .",
    "let @xmath312 be the exponential @xmath294-martingale given by @xmath313 by ( * ? ? ?",
    "* chapter 7 ) , we find that , for @xmath76 $ ] , @xmath314 the process @xmath315 is a @xmath102-martingale .    for @xmath297 and @xmath316 , we see from it formula that the process @xmath317 is a local @xmath102-martingale , whence a @xmath102-supermartingale since @xmath304 is nonnegative .",
    "it follows that , for @xmath297 , @xmath316 , and @xmath318 , @xmath319&\\le   \\mathbb{e}[\\log(x^{x,\\pi}(t))-y\\widehat{l}(t)x^{x,\\pi}(t)]+ yx \\\\ & \\le \\mathbb{e}[\\log\\{1/(y\\widehat{l}(t))\\}-1]+ yx ,   \\notag\\end{aligned}\\ ] ] where , in the second inequality , we have used @xmath320 the equalities in ( [ eq:4.5 ] ) hold if and only if @xmath321 thus the portfolio process @xmath305 satisfying ( [ eq:4.6 ] ) is optimal .    put @xmath322 since @xmath323 and @xmath324 we see from ( [ eq:4.4 ] ) that the process @xmath325 satisfies ( [ eq:4.6 ] ) , whence it is the desired optimal optimal portfolio process .",
    "it also satisfies @xmath326    we put @xmath327).\\ ] ] we define the error matrix @xmath205 by @xmath328 $ ] . combining the results above with theorem [ thm:3.4 ] which describes the dynamics of @xmath329 and @xmath330 , we obtain the next theorem .",
    "[ thm:4.1 ] the optimal portfolio process @xmath331 for the problem @xmath332 is given by @xmath333 and satisfies @xmath334 the filter @xmath112 follows @xmath335 with @xmath336 , 0 , 0)^{*}$ ] , and the error matrix @xmath244 satisfies the matrix riccati equation @xmath337 with @xmath338\\ ( i , j=1,2,3)$ ] , where @xmath339 , @xmath340 , @xmath341 and @xmath262 are as in theorem @xmath342 with @xmath343 .",
    "let @xmath10 be the process given by ( [ eq:1.1 ] ) .",
    "we can estimate the values of the parameters @xmath5 and @xmath6 there from given data of @xmath10 by a least - squares approach ( @xcite ) .",
    "in fact , since @xmath10 is a stationary increment process , the variance of @xmath344 is a function in @xmath345 . to be precise , @xmath346 where the function @xmath347 is given by @xmath348 suppose that for @xmath349 , @xmath350 , the value of @xmath351 is @xmath352 . for simplicity , we assume that @xmath353",
    ". the unbiased estimation @xmath354 that corresponds to @xmath355 is given by @xmath356 where @xmath357 is the mean of @xmath358 s :",
    "@xmath359 fitting @xmath360 to @xmath361 by least squares , we obtain the desired estimated values of @xmath5 and @xmath6 .    for example , we produce sample values @xmath362 for @xmath10 with @xmath363 by a monte carlo simulation .",
    "we use this data to numerically calculate the values @xmath364 and @xmath365 of @xmath5 and @xmath6 , respectively , that minimize @xmath366 it turns out that @xmath367 and @xmath368 . in figure",
    "[ fig:5.1 ] , we plot @xmath360 ( theory ) , @xmath361 ( sample ) and @xmath369 ( fitted ) .",
    "it is seen that the fitted curve follows the theoretical curve reasonably well .    , the sample data and the fitted function @xmath370 . ]",
    "we extend the approach above to that for the estimation of the parameters @xmath5 , @xmath6 , @xmath288 and @xmath290 in @xmath371 where @xmath372 , the process @xmath55 is given by ( [ eq:1.1 ] ) as above , and the initial value @xmath37 is independent of @xmath60 and satisfies @xmath373<\\infty$ ] . the solution @xmath25 is the following ornstein - uhlenbeck - type process with memory : @xmath374 ) .",
    "\\label{eq:5.2}\\ ] ] put @xmath375    [ prop:5.1 ] we have @xmath376 = h(t - s)\\quad ( 0\\le s",
    "< t\\le t ) ,   \\label{eq:5.3}\\ ] ] where , for @xmath377 , the function @xmath378 is given by @xmath379    by ( [ eq:5.2 ] ) , the left - hand side of ( [ eq:5.3 ] ) is equal to @xmath380.\\ ] ] we put @xmath381 for @xmath382 . by proposition 3.2 in @xcite , @xmath383 is given by @xmath384 thus @xmath385 $ ] is equal to @xmath386 by integration by parts and the equalities @xmath387 we obtain the desired result .",
    "suppose that for @xmath349 , @xmath350 , the value of @xmath388 is @xmath389 .",
    "we assume @xmath353 for simplicity .",
    "the estimation @xmath390 that corresponds to @xmath391 is given by @xmath392 where @xmath393 is the mean of @xmath394 , @xmath395 : @xmath396 fitting @xmath397 to @xmath398 by least squares , we obtain the desired estimated values of @xmath5 , @xmath6 , @xmath399 and @xmath400 .    for example , we produce sample values @xmath401 for @xmath47 with @xmath402 by a monte carlo simulation .",
    "we use this data to numerically calculate the values @xmath364 , @xmath365 , @xmath403 and @xmath404 of @xmath5 , @xmath6 , @xmath405 and @xmath400 , repectively , that minimize @xmath406 it turns out that @xmath407 in figure [ fig:5.2 ] , we plot @xmath408 ( theory ) , @xmath409 ( sample with estimated  @xmath405 ) and @xmath410 ( fitted ) .",
    "it is seen that the fitted curve follows closely the theoretical curve .    , the sample data @xmath411 with estimated @xmath412 and the fitted function @xmath413 . ]",
    "as we have seen , the process @xmath10 described by ( [ eq:1.1 ] ) has both stationary increments and a simple semimartingale representation as brownian motion does , and it reduces to brownian motion when @xmath11 . in this sense , we may see @xmath10 as a generalized brownian motion . since @xmath10 is non - markovian unless @xmath11",
    ", we have now a wide choice for designing models driven by either white or colored noise .    in this section ,",
    "we compare the performance of the optimal filter with the kalman - bucy filter in the presence of colored noise .",
    "we consider the partially observable process @xmath111 governed by ( [ eq:1.5 ] ) with @xmath414 .",
    "suppose that an engineer uses the conventional markovian model @xmath415 to describe the non - markovian system process @xmath47 .",
    "then he will be led to use the kalman - bucy filter @xmath416 governed by @xmath417 where @xmath418 is the solution to @xmath419 ( see ( [ eq:3.8 ] ) and ( [ eq:3.9 ] ) ) , instead of the right optimal filter @xmath420 as described by theorem [ thm:3.4 ] .",
    "we adopt the following parameters : @xmath421 let @xmath422 . for the @xmath423-th run of monte carlo simulation",
    ", we sample @xmath424 for @xmath47 .",
    "let @xmath425 and @xmath426 , @xmath427 , be the kalman - bucy filter and the optimal filter respectively . for @xmath428 or @xmath429 , we consider the _ average error norm _ @xmath430 and the _ average error _",
    "@xmath431    in table [ table:6.1 ] , we show the resulting @xmath432 s of @xmath433 and @xmath434 for the following five sets of @xmath435 : @xmath436 we see that there are clear differences between the two filters in the cases @xmath437 and @xmath438 .",
    "we notice that , in these two cases , @xmath439 is large than the parameters @xmath440 and @xmath441 . in figure",
    "[ fig:6.1 ] , we compare the graphs of @xmath442 for the two filters in the case @xmath443 .",
    "it is seen that the error of the optimal filter is consistently smaller than that of the kalman - bucy filter ."
  ],
  "abstract_text": [
    "<S> we study the linear filtering problem for systems driven by continuous gaussian processes @xmath0 and @xmath1 with memory described by two parameters . </S>",
    "<S> the processes @xmath2 have the virtue that they possess stationary increments and simple semimartingale representations simultaneously . </S>",
    "<S> it allows for straightforward parameter estimations . after giving the semimartingale representations of @xmath2 by innovation theory </S>",
    "<S> , we derive kalman - bucy - type filtering equations for the systems . </S>",
    "<S> we apply the result to the optimal portfolio problem for an investor with partial observations . </S>",
    "<S> we illustrate the tractability of the filtering algorithm by numerical implementations . </S>"
  ]
}