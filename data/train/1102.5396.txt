{
  "article_text": [
    "recent studies have suggested that minimization of the helmholtz free energy in statistical physics [ 1 ] plays a central role in understanding action , perception , and learning ( see [ 2 ] and the references therein ) .",
    "in fact , it has been suggested that the principle of free energy minimization is even more fundamental than the redundancy reduction principle ( also known as the principle of efficient coding ) articulated by barlow [ 3 ] and later formalized by linsker as the infomax principle [ 4 ] .",
    "specifically , the principle of efficient coding states that the brain should optimize the mutual information between its sensory signals and some parsimonious neuronal representations .",
    "this is identical to optimizing the parameters of a generative model to maximize the accuracy of predictions , under complexity constraints .",
    "both are mandated by the free - energy principle , which can be regarded as a probabilistic generalization of the infomax principle .",
    "the infomax principle has been central to the development of independent component analysis ( ica ) and the allied problem of blind source separation ( bss ) [ 5 ] . within the ica / bss context , very few models based on minimization of the free energy exist ,",
    "the most prominent of them originated by szu and co - workers ( eg . see refs .",
    "[ 6,7 ] ) to achieve source separation in remote sensing ( i.e. hyperspectral imaging ( hsi ) ) using the maximum entropy principle .",
    "the ica / bss problem may be summarized in terms of the relation @xmath1 where @xmath2 is the _ unknown _ source vector to be extracted , @xmath3 is the _ unknown _ mixing matrix ( also known as reflectance matrix or material abundance matrix in hsi ) , and @xmath4 is the _ known _ vector of observed data .",
    "the helmholtz free energy is described within the framework of boltzmann - gibbs - shannon ( b - g - s ) statistics as @xmath5 where @xmath6 is the thermodynamic temperature ( or haemostatic temperature in the parlance of cybernetics ) , @xmath7 the boltzmann constant , @xmath8 the internal energy , and @xmath9 shannon s entropy . a more principled and systematic manner in which to study free energy minimization within the context of the maximum entropy principle ( maxent )",
    "is by substituting the minimization of the helmholtz free energy principle with the maximizing of the massieu potential [ 8 ] @xmath10 where @xmath11 is the inverse thermodynamic temperature .",
    "the massieu potential is the legendre transform of the helmholtz free energy , i.e. : @xmath12 .",
    "the generalized ( also , interchangeably , nonadditive , deformed , or nonextensive ) statistics of tsallis has recently been the focus of much attention in statistical physics , complex systems , and allied disciplines [ 9 ] .",
    "nonadditive statistics suitably generalizes the extensive , orthodox b - g - s one .",
    "the scope of tsallis statistics has lately been extended to studies in lossy data compression in communication theory [ 10 ] and machine learning [ 11,12 ] .",
    "it is important to note that power law distributions like the q - gaussian distribution can not be accurately modeled within the b - g - s framework [ 9 ] .",
    "one of the most commonly encountered source of q - gaussian distributions occurs in the process of normalization of measurement data using _ studentization _ techniques [ 13 ] .",
    "q - gaussian behavior is also exhibited by elliptically invariant data , which generalize spherically symmetric distributions .",
    "q - gaussian s are also an excellent approximation to correlated gaussian data , and other important and fundamental physical and biological processes ( for example , see [ 14 ] and the references therein ) .",
    "this paper intends to accomplish the following objectives :    * @xmath13 to formulate and solve a variational principle for source separation using the maximum dual tsallis entropy with constraints defined by normal averages expectations , * @xmath14 to amalgamate the variational principle with hopfield - like learning rules [ 15 ] to acquire information regarding unknown parameters via an unsupervised learning paradigm , * @xmath15 to formulate a numerical framework for the generalized statistics unsupervised learning model and demonstrate , with the aid of numerical examples for separation of independent sources ( _ endmembers _ ) , the superiority of the generalized statistics source separation model _ vis -  - vis _ an equivalent b - g - s model for a single pixel .",
    "it is important to note that by amalgamating the information - theoretic model with the hopfield model , @xmath16 $ ] acquires the role of the associative memory ( am ) matrix .",
    "_ further , employing a hopfield - like learning rule renders the model presented in this paper readily amenable to hardware implementation using field programmable gate arrays ( fpgas)_.    the additive duality is a fundamental property in generalized statistics [ 9 ] .",
    "one implication of the additive duality is that it permits a deformed logarithm defined by a given nonadditivity parameter ( say , @xmath0 ) to be inferred from its _ dual deformed _",
    "logarithm parameterized by : @xmath17 .",
    "this paper derives a variational principle for source separation using the dual tsallis entropy using normal averages constraints .",
    "this approach has been previously utilized ( for eg .",
    "[ 16 ] ) , and possess the property of seamlessly yielding a @xmath18-deformed exponential form on variational extremization .    an important issue to address concerns the manner in which expectation values are computed . of the various forms in which expectations may be defined in nonextensive statistics",
    "has , only the linear constraints originally employed by tsallis [ 9 ] ( also known as _ normal averages _ ) of the form : @xmath19 , has been found to be physically satisfactory and consistent with both the generalized h - theorem and the generalized _ stosszahlansatz _ ( molecular chaos hypothesis ) [ 17 , 18 ] . a re - formulation of the variational perturbation approximations in nonextensive statistical physics followed [ 18 ] , via an application of @xmath0-deformed calculus [ 19 ] .",
    "results from the study in ref . [",
    "19 ] have been successfully utilized in section iv of this paper .",
    "this introductory section is concluded by briefly describing the suitability of employing a generalized statistics model to study the source separation problem .",
    "first , in the case of remote sensing applications , and even more so in the case of hsi , the observed data are highly correlated , even in the case of a single pixel .",
    "next , the observed data are required to be normalized ( scaled ) .",
    "the _ studentization _",
    "process is one of the most prominent methods utilized to normalize the observed data [ 20,21 ] .",
    "both these features lead to an excursion from the gaussian framework ( b - g - s statistics ) and result in q - gaussian pdf s characterized by the @xmath0-deformed exponential : @xmath20^{\\frac{1}{{1 - q}}}$ ] , which maximizes the tsallis entropy .",
    "the section introduces the essential concepts around which this communication revolves .",
    "the tsallis entropy is defined as [ 9 ] @xmath21 the _ q - deformed _ logarithm and the _ q - deformed _ exponential are defined as [ 9 , 19 ] @xmath22^{\\frac{1}{{1 - q } } } ; 1 + \\left ( { 1 - q } \\right)x \\ge 0 , \\\\   0;otherwise \\\\   \\end{array } \\right .",
    "\\end{array}\\ ] ] note that as @xmath23 , ( 4 ) acquires the form of the equivalent b - g - s entropies . likewise in ( 5 ) , @xmath24 and @xmath25 .",
    "the operations of _ q - deformed _ relations are governed by _ q - deformed _ algebra and _ q - deformed _ calculus [ 19 ] .",
    "apart from providing an analogy to equivalent expressions derived from b - g - s statistics , _ q - deformed _ algebra and _ q - deformed _ calculus endow generalized statistics with a unique information geometric structure .",
    "the _ q - deformed _ addition @xmath26 and the _ q - deformed _ subtraction @xmath27 are defined as [ 19 ] @xmath28    the _ q - deformed _ derivative , is defined as [ 19 ] @xmath29\\frac{{df\\left ( x \\right)}}{{dx } } \\ ] ] as @xmath30 , @xmath31 , the newtonian derivative . the leibnitz rule for _ deformed _ derivatives [ 19 ]",
    "is @xmath32 = b\\left ( x \\right)d_{q}^x a\\left ( x \\right ) + a\\left ( x \\right)d_{q}^x b\\left ( x \\right).\\ ] ]    re - parameterizing ( 5 ) via the _ additive duality _ [ 10 ] : @xmath33 , yields the _ dual deformed _ logarithm and exponential @xmath34    the dual tsallis entropy is defined by [ 10 , 16 ] @xmath35    here , @xmath36 . _",
    "the dual tsallis entropies acquire a form similar to the b - g - s entropies , with @xmath37 replacing @xmath38_.",
    "consider the lagrangian @xmath39 =   - \\sum\\limits_j { s_j \\ln _ { q^ *   } s_j }   - \\sum\\limits_{i = 1}^n { \\sum\\limits_{j = 1}^n { \\lambda _ i } \\left ( { a_{ij } s_j   - x_i } \\right ) }   \\\\    +   { \\lambda _ 0}\\left ( { \\sum\\limits_{j = 1}^n { s_j }   - 1 } \\right ) , \\\\   \\end{array}\\ ] ] subject to the component - wise constraints @xmath40 clearly , the rhs of the lagrangian ( 11 ) is the @xmath18-deformed massieu potential : @xmath41 $ ] , subject to the normalization constraint on @xmath42 .",
    "the variational extremization of ( 11 ) , performed using the ferri - martinez - plastino methodology [ 22 ] , leads to @xmath43^{\\frac{1}{{1 - q^ *   } } }   \\\\ \\end{array}\\ ] ]    multiplying the second relation in ( 13 ) by @xmath42 and summing over all @xmath44 , yields after application of the normalization condition in ( 12 ) @xmath45 where : @xmath46 , and substituting ( 14 ) into the third relation in ( 13 ) yields @xmath47^{\\frac{1}{{1 - q^ *   } } } ; \\\\   \\tilde \\lambda _",
    "i   = \\frac{{\\lambda _ i } } { { \\left ( { 2 - q^ *   } \\right)}}. \\\\   \\end{array}\\ ] ] eq .",
    "( 15 ) yields after some algebra @xmath48 where @xmath49 here @xmath50 is the canonical partition function , where : @xmath51 .",
    "the dual tsallis entropy takes the form @xmath52 = \\frac{{\\aleph _ { q^ *   }   - 1}}{{\\left ( { q^ *    - 1 } \\right ) } } ; \\sum\\limits_{j = 1}^n { s_j }   = 1 \\\\    \\rightarrow \\aleph _ { q^ *   }   = 1 + \\left ( { q^ *    - 1 } \\right)s_{q^ *   } [ { s } ] \\\\   \\end{array}\\ ] ] substituting now ( 18 ) into the expression for : @xmath50 in ( 17 ) results in @xmath53 - \\sum\\limits_{j = 1}^n { \\sum\\limits_{i = 1}^n { \\tilde \\lambda _",
    "i a_{ij } s_j } }   = \\phi_{q^ * } \\left [ { \\tilde \\lambda } \\right ] .",
    "\\\\   \\end{array}\\ ] ] clearly , @xmath54 $ ] in ( 19 ) is a @xmath18-deformed massieu potential . by substituting ( 18 ) into ( 14 )",
    "we arrive at @xmath55 - \\sum\\limits_{j = 1}^n { \\sum\\limits_{i = 1}^n { \\tilde \\lambda _",
    "i a_{ij } s_j } }   = -\\tilde \\lambda _ 0 + \\frac{1}{{\\left ( { 1 - q^ *   } \\right ) } } = \\hat \\lambda _ 0 ; \\\\   \\tilde \\lambda _ 0   = \\frac{\\lambda_0 } { { \\left ( { 2 - q^ *   } \\right)}}. \\\\   \\end{array}\\ ] ] again , @xmath56 in ( 20 ) is a @xmath18-deformed massieu potential : @xmath57 $ ] .",
    "we wish to relate @xmath58 and @xmath50 . to this end ,",
    "comparison of ( 19 ) and ( 20 ) yields @xmath59^{\\frac{1}{{q^ *    - 1 } } } ;",
    "\\tilde \\lambda _ 0   = \\frac{{\\lambda _ 0 } } { { \\left ( { 2 - q^ *   } \\right ) } } , \\\\   \\end{array}\\ ] ] so that , by substituting ( 18 ) into ( 15 ) and then invoking ( 20 ) we get @xmath60^{\\frac{1}{{1 - q^ *   } } } ; \\\\   \\hat \\lambda _ 0   =   - \\tilde \\lambda _ 0   + \\frac{1}{{\\left ( { 1 - q^ *   } \\right)}}. \\\\   \\end{array}\\ ] ] here , ( 22 ) is re - defined with the aid of ( 20 ) as @xmath61^{\\frac{1}{{1 - q^ *   } } } } } { { \\left [ { 1 - \\left ( { 1 - q^ *   } \\right)\\hat \\lambda _ 0 } \\right]^{\\frac{1}{{q^ *    - 1 } } } } }   = \\frac{{\\left [ { 1 - \\left ( { 1 - q^ *   } \\right)\\sum\\limits_{i = 1}^n { \\tilde \\lambda _ i^ *   a_{ij } } } \\right]^{\\frac{1}{{1 - q^ *   } } } } } { { \\tilde z_{q^ *   } } } ; \\\\   { \\rm where } \\\\",
    "\\tilde \\lambda _ i^ *    = \\frac{{\\tilde \\lambda _ i } } { { 1 - \\left ( { 1 - q^ *   } \\right)\\hat \\lambda _ 0 } } , \\tilde z_{q^ * } = \\sum\\limits_{j = 1}^n { \\left [ { 1 - \\left ( { 1 - q^ *   } \\right ) \\sum\\limits_{i = 1}^n { \\tilde \\lambda _ i^ *   a_{ij } } } \\right]^{\\frac{1}{{1 - q^ *   } } } } .",
    "\\\\   \\end{array}\\ ] ] with the aid of ( 21 ) , ( 22 ) is re - cast in the form @xmath62^{\\frac{1}{{q^ *    - 1 } } } } } ; \\\\",
    "\\,{\\rm where},\\,\\ , \\tilde \\lambda _ i   = \\frac{{\\lambda _ i } } { { \\left ( { 2 - q^ *   } \\right)}},\\tilde \\lambda _ 0   = \\frac{{\\lambda _ 0 } } { { \\left ( { 2 - q^ *   } \\right ) } } , \\tilde \\lambda _ i^ *    = \\frac{{\\tilde \\lambda_i } } { { \\left [ { \\left ( { 1 - q^ *   } \\right)\\tilde \\lambda _ 0 } \\right]}}. \\\\ \\end{array}\\ ] ] finally , invoking the normalization of @xmath42 , ( 24 ) yields @xmath63^{\\frac{1}{{q^ *    - 1 } } }   = \\sum\\limits_{j = 1}^n { \\left [ { 1 - \\left ( { 1 - q^ *   } \\right)\\sum\\limits_{i = 1}^n { \\tilde \\lambda _ i^ *   a_{ij } } } \\right]^{\\frac{1}{{1 - q^ *   } } } .}\\ ] ] note the _ self - referential _ nature of ( 23 ) in the sense that : @xmath64 ( defined in ( 20 ) and ( 23 ) is a function of @xmath65 .",
    "the lagrange multiplier @xmath64 is henceforth defined in this paper as the _ dual normalized lagrange force multiplier_.",
    "the process of unsupervised learning is amalgamated to the above information theoretic structure via a hopfield - like learning rule to update the am matrix @xmath16 $ ] in the case of a perturbation @xmath66 of the observed data @xmath67}}{{\\partial s_j } } \\\\",
    "= -\\frac{{1 - \\left ( { 1 - q^ *   } \\right)\\tilde \\lambda } } { { \\left ( { 1 - q^ *   } \\right)\\tilde \\lambda _ 0 } } - \\frac{{\\ln _ { q^ *   } s_j } } { { \\tilde \\lambda _ 0 } } - \\left ( { 1 - q^ *   } \\right)\\sum\\limits_{i = 1}^n { \\tilde \\lambda _ i^ *   } a_{ij } \\\\    \\rightarrow \\delta x_j   \\\\    = -\\left [ { \\frac{{1 - \\left ( { 1 - q^ *   } \\right)\\tilde \\lambda _ 0 } } { { \\left ( { 1 - q^ *   } \\right)\\tilde \\lambda _ 0 } } + \\frac{{\\ln _ { q^ *   } s_j } } { { \\tilde \\lambda _ 0 } } + \\left ( { 1 - q^ *   } \\right)\\sum\\limits_{i = 1}^n { \\tilde \\lambda _ i^ { * } } a_{ij } } \\right]\\delta t ; \\\\ where , \\tilde \\phi _ { q^ *   } ^ *   \\left [ { s_j } \\right ] = \\frac{{\\phi _ { q * } \\left [ { s_j } \\right]}}{{\\left ( { 2 - q^ *   } \\right)\\tilde \\lambda _ 0 } } , \\end{array}\\ ] ] which is obtained from the first relation in ( 13 ) and ( 24 ) . gradient ascent along with ( 24 ) originates the second learning rule @xmath68}}{{\\partial a_{ij } } } = -\\tilde \\lambda _ i^ *   s_j   \\rightarrow \\delta x_j   = -\\left ( { \\tilde \\lambda _ i^ *   s_j } \\right)\\delta t ; \\\\ where , \\phi _ { q^ *   } ^ *   \\left [ { s_j } \\right ] = \\frac{{\\phi _ { q^ *   } \\left [ { s_j } \\right ] } } { { \\left ( { 1 - q^ *   } \\right)\\tilde \\lambda _ 0}}. \\\\   \\end{array}\\ ] ] in ( 26 ) and ( 27 ) , @xmath69 $ ] is the lhs of the lagrangian ( 11 ) .",
    "critical _ update rule is that for the change in the _ dual normalized lagrange force multipliers _",
    "@xmath70 resulting from a perturbation @xmath66 in the observed data . usually ( as stated within the context of the b - g - s framework ) , such an update would entail a taylor - expansion yielding up to the first order : @xmath71 .",
    "such an analysis is valid only for distributions characterized by the regular exponential @xmath72 . for probability distributions characterized by @xmath0-deformed exponentials ,",
    "i.e. , the ones we face here , such a perturbation treatment would lead to un - physical results [ 18 ] .",
    "thus , following the prescription given in ref . [ 18 ] , for a function : @xmath73 the chain rule yields : @xmath74 .",
    "thus , replacing the newtonian derivative : @xmath75 by the _",
    "@xmath18-deformed _ one defined by ( 7 ) ( see ref . [",
    "19 ] ) : @xmath76\\frac{{df\\left ( \\tau \\right)}}{{d\\tau } } $ ] and defining : @xmath77 as well , facilitates the desired transformation : @xmath78 .",
    "consequently , the update rule for @xmath79 is re - formulated via @xmath0-deformed calculus in the fashion @xmath80 } \\delta \\tilde \\lambda _ k^ *    = \\sum\\limits_{k = 1}^n { \\left [ { \\sum\\limits_{i = 1}^n { d_{q^ *   } ^\\tau   a_{ji } s_i } } \\right ] } \\delta \\tilde \\lambda _ k^ * .\\ ] ] additionally , setting : @xmath81 in ( 23 ) leads to @xmath82^{\\frac{1}{{1 - q^ *   } } } } } { { \\tilde z_{q^ *   } } } .\\ ] ] employing at this stage the leibnitz rule for @xmath18-deformed derivatives ( and replacing @xmath0 by @xmath18 in ( 8) ) , the term within square parenthesis rhs in ( 28 ) yields @xmath83^{\\frac{1}{{1 - q^ *   } } } } \\right . }",
    "\\\\   \\left . { + a_{ji }   \\left [ { 1 + \\left ( { 1 - q^ *   } \\right)\\tau } \\right]^{\\frac{1}{{1 - q^ *   } } } d_{q^ *   } ^\\tau   \\left ( { \\frac{1}{{\\tilde z_{q^ *   } } } } \\right ) } \\right\\ } , \\\\   \\end{array}\\ ] ] a relation that , after expansion turns into @xmath84\\frac{{\\partial \\tau } } { { \\partial \\tilde \\lambda _ k^ *   } } \\frac{\\partial } { { \\partial \\tau } } \\left [ { 1 + \\left ( { 1 - q^ *   } \\right)\\tau } \\right]^{\\frac{1}{{1 - q^ *   } } } } \\right . }",
    "\\\\   \\left . { + a_{ji } \\left [ { 1 + \\left ( { 1 - q^ *   } \\right)\\tau } \\right]^{\\frac{1}{{1 - q^ *   } } } d_{q^ *   } ^\\tau   \\left ( { \\frac{1}{{\\tilde z_{q^ *   } } } } \\right ) } \\right\\ } \\\\    = \\sum\\limits_{i = 1}^n { \\left\\ { { - \\frac{{a_{ji } } } { { \\tilde z_{q^ *   } } } \\left [ { 1 + \\left ( { 1 - q^ *   } \\right)\\tau } \\right]^{\\frac{1}{{1 - q^ *   } } } a_{ik } } \\right . }",
    "\\\\   \\left .",
    "{ - a_{ji } \\left [ { 1 + \\left ( { 1 - q^ *   } \\right)\\tau } \\right]^{\\frac{1}{{1 - q^ *   } } } \\left [ { 1 + \\left ( { 1 - q^ *   } \\right)\\tau } \\right]\\frac{{\\partial \\tau } } { { \\partial \\tilde \\lambda _ k^ *   } } \\tilde z_{q^ *   } ^ { - 2 } \\frac{{\\partial \\tilde z_{q^ *   } } } { { \\partial \\tau } } } \\right\\ } \\\\    =   - \\sum\\limits_{i = 1}^n { a_{ji } s_i a_{ik } }   \\\\    + \\sum\\limits_{i = 1}^n { a_{ji } \\frac{{\\left [ { 1 + \\left ( { 1 - q^ *   } \\right)\\tau } \\right]^{\\frac{1}{{1 - q^ *   } } } } } { { \\tilde z_{q^ *   } } } } \\sum\\limits_{k = 1}^n { \\frac{{a_{ik } } } { { \\tilde z_{q^ *   } } } } \\left [ { 1 + \\left ( { 1 - q^ *   } \\right)\\tau } \\right]^{\\frac{1}{{1 - q^ *   } } }   \\\\    =   - \\sum\\limits_{i = 1}^n { a_{ji } s_i a_{ik } }   + x_j x_k . \\\\   \\end{array}\\ ] ] finally , the update rule for @xmath79 with respect to @xmath66 adopts the appearance @xmath85",
    "the procedure for our double recursion problem is summarized in the pseudo - code below    @xmath86  * input * : @xmath13 . observed data : @xmath4 ,  @xmath14 .",
    "trial values of dual normalized lagrange force multipliers : @xmath87 ,  @xmath15 .",
    "dual nonadditive parameter : @xmath18 .",
    "@xmath88  * initialization * : + obtain @xmath89 from:@xmath90 + 50 @xmath91 random noise to break any rank-1 singularity .",
    "the @xmath18-deformed sigmoid logistic function is : @xmath92 .",
    "@xmath93  * first recursion * + @xmath13 compute : @xmath94 from ( 23 ) , + @xmath14 compute:@xmath95 from ( 21 ) , + @xmath15 compute : @xmath96 , @xmath97 , and @xmath98 from ( 23)/(24 ) , + @xmath99 compute : @xmath100 from ( 5 ) , thus : @xmath101 , + @xmath102 compute @xmath103 by inverting ( 32 ) , + @xmath104 compute next estimate : @xmath105 .",
    "+ @xmath106 * second recursion * + @xmath107 compute improved estimate of : @xmath108 from ( 26 ) by setting @xmath109 and solving : @xmath110 $ ] .",
    "+ @xmath111 go to @xmath93    following the procedure outlined in the above pseudo - code , values of @xmath112 $ ] and @xmath113 $ ] are provided .",
    "these values are the same as those in ref .",
    "[ 7 ] and constitute experimentally obtained landsat data for a single pixel .",
    "the difference between the generalized statistics model presented in this paper and the b - g - s model of [ 6,7 ] lies in the fact that the former has initial inputs of @xmath70 s , whereas the latter merely has initial inputs of @xmath114 s ( a far simpler case ) . _ the self - rerentiality in ( 23 )",
    "mandates use of @xmath70 s as the primary operational lagrange multiplier_. note that the correlation coefficient of @xmath115 is unity , a signature of highly correlated data .",
    "a value of @xmath116 is chosen .",
    "figure 1 and figure 2 depict , vs. the number of iterations , the source separation for the generalized statistics model and for the b - g - s model , respectively .",
    "values of @xmath4 are denoted by `` o '' s .",
    "it is readily appreciated that the generalized statistics exhibits a more pronounced source separation than the b - g - s model . owing to the highly correlated nature of the observed data ,",
    "such results are to be expected .",
    "a generalized statistics model for source separation that employs an unsupervised learning paradigm has been presented in this communication .",
    "this model is shown to exhibit superior separation performance as compared to an equivalent model derived within the b - g - s framework .",
    "our encouraging results should inspire future work studies on the implications of first - order and second - order phase transitions of the massieu potential .",
    "one would wish for a self - consistent scheme enabling one to obtain self - consistent values of lagrange multipliers based on the principle of phase transitions and symmetry breaking .",
    "rcv gratefully acknowledges support from _ rand - msr _ contract _ csm - di @xmath117 s - qit-101155 - 03 - 2009_.                            a. f. t. martins , n. a. smith , e. p. xing , p. m. q. aguiar , and m. a. t. figueiredo , , * 10 * , 935 , 2009 .",
    "c. vignat and a. plastino , `` why is the detection of _ q - gaussian _ behavior such a common occurrence ? '' , _ physica a _ , * 388 * , 601 , 2009 .",
    "r. hanel , s. thurner , and c. tsallis , `` limit distributions of scale - invariant probabilistic models of correlated random variables with the q - gaussian as an explicit example '' , _ eur .",
    "* 72 * , 2 , 263 , 2009 ."
  ],
  "abstract_text": [
    "<S> a generalized - statistics variational principle for source separation is formulated by recourse to tsallis entropy subjected to the additive duality and employing constraints described by normal averages . </S>",
    "<S> the variational principle is amalgamated with hopfield - like learning rules resulting in an unsupervised learning model . </S>",
    "<S> the update rules are formulated with the aid of @xmath0-deformed calculus . </S>",
    "<S> numerical examples exemplify the efficacy of this model . </S>"
  ]
}