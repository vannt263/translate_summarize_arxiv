{
  "article_text": [
    "the newton method is a classical numerical scheme for solving systems of nonlinear equations and smooth optimization @xcite .",
    "however , there are at least two reasons that prevent the use of newton methods from solving large - scale problems . first ,",
    "while these methods often offer a fast local convergence rate , which can be up to a quadratic rate , their global convergence has not been well - understood @xcite . in practice , one can use a damped - step scheme utilizing the lipschitz constant of the objective derivatives to compute a suitable step - size as often done in gradient - type methods , or incorporate the algorithm with a globalization strategy such as line - search , trust - region , or filter to guarantee a descent property @xcite .",
    "both strategies allow us to prove a global convergence of the underlying newton - type method in some sense .",
    "unfortunately , in practice , there exist several problems whose objective does not have lipschitz gradient or hessian such as logarithmic or reciprocal functions",
    ". this class of problems does not provide us some uniform bounds to obtain a constant step - size in optimization algorithms . on the other hand",
    ", using a globalization strategy for determining step - size often requires centralized computation such as function evaluations , which prevent us from using distributed computation , and stochastic variants .",
    "second , newton algorithms are second - order methods , which often require a high per - iteration complexity due to the operations on the hessian of the objective function or its approximations .",
    "in addition , these methods require the underlying functions to be smooth up to a given smoothness levels , which often does not hold in many practical models .",
    "[ [ motivation ] ] * motivation : * + + + + + + + + + + + + +    in recent years , there has been a huge interest in newton - type methods for solving convex optimization problems and monotone equations due to the development of new techniques and mathematical tools in optimization , machine learning , and randomized algorithms @xcite .",
    "several combinations of newton - type methods and other techniques such as proximal operators @xcite , cubic regularization @xcite , gradient regularization @xcite , randomized algorithms such as sketching @xcite , subsampling @xcite , and fast eigen - decomposition @xcite create a new research direction and have attracted a great attention in solving nonsmooth and large - scale problems . hitherto ,",
    "research in this direction remains focusing on specific classes of problems where standard assumptions such as nonsingularity and lipschitz hessian conditions are preserved .",
    "however , such assumptions do not hold for many other examples as shown in @xcite .",
    "moreover , if they are satisfied , we often get a lower bound of possible step - sizes for our algorithm , which may leads to a poor performance .    in the seminar work @xcite , nesterov and",
    "nemirovskii showed that the class of log - barrier does not satisfy the standard assumptions of the newton method if the solution is closed to the boundary .",
    "they introduced a powerful concept called `` self - concordance '' to overcome this drawback and developed new newton schemes to achieve global and local convergence without requiring any additional assumption , or a globalization strategy .",
    "while the self - concordance notion was initially invented to study interior - point methods , it is less well - known in other communities .",
    "recent works @xcite have popularized this concept to solve other problems arising from machine learning , statistics , image processing , and variational inequalities .",
    "[ [ our - goals ] ] * our goals : * + + + + + + + + + + + +    in this paper , motivated by @xcite , we aim at generalizing the self - concordance concept in @xcite to a broader class of smooth and convex functions .",
    "to illustrate our idea , we consider a univariate smooth and convex function @xmath0 . if @xmath1 satisfies the inequality @xmath2 for all @xmath3 in the domain of @xmath1 and for a given constant @xmath4 , then we say that @xmath1 is self - concordant ( in the nesterov and nemirovskii sense @xcite ) .",
    "we instead , generalize this inequality to @xmath5 for all @xmath3 in the domain of @xmath1 and for a given constant @xmath6 .",
    "we emphasize that generalizing from univariate to multivariate functions in the standard self - concordant case ( e.g. , @xmath7 ) @xcite preserves several important properties , while , unfortunately , they do not hold for the case @xmath8 .",
    "we therefore modify the definition in @xcite to overcome this drawback .",
    "we note that a similar idea has been also studied in @xcite for a class of logistic - type functions .",
    "unfortunately , the definition using in these works is still limited and creates certain difficulty for developing further theory .",
    "our second goal is to develop a unified mechanism to analyze the convergence ( including global and local convergence ) of the following newton - type scheme : @xmath9 where @xmath10 is the jacobian of @xmath11 , and @xmath12 $ ] is a given step - size , and @xmath11 can be presented as the right - hand - side of a monotone equation @xmath13 or the optimality condition of a convex optimization or a convex - concave saddle - point problem . despite the newton scheme",
    "is invariant to a change of variables @xcite , its convergence property relies on the growth of the hessian along the newton iterative process . in classical settings ,",
    "the lipschitz continuity of the hessian and the non - degenerate of the hessian in a neighborhood of the solution set are key assumptions to achieve local quadratic convergence rate @xcite .",
    "these assumptions have been considered to be standard , but they are often very difficult to check in practice , especially the second one . a natural idea is to classify the functionals of the underlying problem into a known class of functions to choose a suitable method for solving it .",
    "while first - order methods for convex optimization essentially rely on the lipschitz gradient function assumption , newton methods usually use the lipschitz continuity of the hessian and its non - degenerate to obtain a well - defined newton direction as we have mentioned . for self - concordant functions ,",
    "the second condition automatically holds , while the first assumption is failed to hold .",
    "however , both full - step and damped - step newton methods still work in this case by appropriately choosing a suitable metric .",
    "this situation has been observed and standard assumptions have been modified in different directions to still guarantee the convergence of newton methods , see @xcite for an intensive study of generic newton methods , and @xcite for the self - concordant function class .    [",
    "[ our - approach ] ] * our approach : * + + + + + + + + + + + + + + +    we first attempt to develop some background theory for a broad class of smooth and convex functions under the structure . by adopting the local norm defined via the hessian of such a convex function from @xcite",
    ", we can prove some lower and upper bound estimates for the local norm distance between two points in the domain as well as for the growth of the hessian of the function . together with this background theory",
    ", we also identify a class of functions using in generalized linear models @xcite as well as in empirical risk minimization @xcite that falls into our generalized self - concordance class for many well - known link functions as listed in table [ tbl : examples ] .    applying our generalized self - concordant theory",
    ", we then develop a class of newton methods to solve the following composite convex minimization problem : @xmath14 where @xmath15 is a generalized self - concordant function in our context , and @xmath16 is a proper , closed and convex function that can be referred to as a regularization term .",
    "we consider two cases .",
    "the first case is a non - composite convex minimization in which @xmath16 is vanished ( i.e. , @xmath17 ) . in the second case , we assume that @xmath16 is equipped with a `` tractably '' proximal operator ( see for the definition ) .    [",
    "[ our - contribution ] ] * our contribution : * + + + + + + + + + + + + + + + + + + +    to this end , our main contribution can be summarized as follows .",
    "* we generalize the self - concordant notion in @xcite to a more broader class of smooth convex functions , which we call generalized self - concordance .",
    "we identify several link functions that can be cast into our generalized self - concordant class .",
    "we also prove several fundamental properties and show that the sum of _ generalized self - concordant _  functions are _ generalized self - concordant _  for a given range of @xmath18 or under suitable assumptions .",
    "* we develop lower and upper bounds on the hessian , the gradient and the function values for generalized self - concordant functions .",
    "these estimates are key to analyze several numerical optimization methods including newton - type methods .",
    "* we propose a class of newton methods including full - step and damped - step schemes to minimize a generalized self - concordant function .",
    "we show explicitly how to choose a suitable step - size to guarantee a descent direction in the damped - step scheme , and prove a local quadratic convergence for both the damped - step and the full - step schemes using a suitable metric .",
    "* we also extend our newton schemes to handle the composite setting .",
    "we develop both full - step and damped - step proximal newton methods to solve this problem and provide a rigorous theoretical convergence guarantee in both local and global sense .",
    "* we also study a quasi - newton variant of our newton scheme to minimize a generalized self - concordant function . under a modification of the well - known dennis - mor condition @xcite or a bfgs update ,",
    "we show that our quasi - newton method locally converges at a superlinear rate to the solution of the underlying problem .",
    "let us emphasize the following of our contribution .",
    "first , we observe that the self - concordance notion is a powerful concept and has been widely used in interior - point methods as well as in other optimization schemes @xcite , generalizing it to a broader class of smooth convex functions can substantially cover a number of new applications or can develop new methods for solving old problems including logistic and multimonomial logistic regression , optimization involving exponential objectives , and distance - weighted discrimination problems in classification ( see table [ tbl : examples ] below ) .",
    "second , verifying theoretical assumptions for convergence guarantees of a newton method is not trivial , our theory allows one to classify the underlying functions into different subclasses by using different parameters @xmath18 and @xmath19 and to choose suitable algorithms to solve the corresponding optimization problem .",
    "third , the theory developed in this paper can potentially apply to other optimization methods such as gradient - type , sketching and sub - sampling newton , and frank - wolfe algorithms as done in the literature @xcite .",
    "finally , our generalization also shows that it is possible to impose additional structure such as in self - concordant barrier to develop path - following scheme for solving a subclass of the composite convex minimization problems of the form .",
    "we believe that our theory is not limited to convex optimization , but can be extended to solve convex - concave saddle - point problems , and monotone equations / inclusions involving generalized self - concordant functions .    [ [ related - work ] ] * related work : * + + + + + + + + + + + + + + +    since the self - concordance concept was introduced in 1990s @xcite , its first extension is perhaps proposed by @xcite for a class of logistic regression . in @xcite , the authors extended @xcite to study proximal newton method for logistic , multinomial logistic , and exponential loss functions . by augmenting a strongly convex regularizer ,",
    "zhang and lin in @xcite showed that the regularized logistic regression is indeed standard self - concordant . in @xcite bach continued exploiting his results @xcite to show that the averaging stochastic gradient method can achieve the same best known convergence rate as in strongly convex case without adding a regularizer . in @xcite ,",
    "the authors exploited standard self - concordance theory in @xcite to develop several classes of optimization including proximal newton , proximal - quasi newton and proximal gradient methods to solve composite convex minimization problems .",
    "lu in @xcite extended @xcite to study randomized block coordinate descent methods . in a recent paper @xcite ,",
    "gao and goldfarb studied quasi - newton methods for self - concordant problems .",
    "the theory developed in this paper , on the one hand , is a generalization of the well - known self - concordance notion developed in @xcite . on the other hand",
    ", it also covers the work in @xcite as specific examples .",
    "several specific applications and extensions of self - concordance notion can also be found in the literature including @xcite .",
    "[ [ paper - organization ] ] * paper organization : * + + + + + + + + + + + + + + + + + + + + +    the rest of this paper is organized as follows .",
    "section [ sec : gsc_background ] develops the foundation theory for our generalized self - concordant functions including definitions , examples , basic properties and key bounds .",
    "section [ sec : gsc_min ] is devoted to study a full - step and damped - step newton schemes to minimize a generalized self - concordant function including their convergence guarantee .",
    "section [ sec : gsc_composite_min ] extends to the composite setting and studies proximal newton - type methods , and investigates their convergence guarantees .",
    "section  [ sec : quasi_newton ] deals with a quasi - newton scheme for solving the noncomposite problem of .",
    "numerical examples are provided in section [ sec : num_experiments ] to illustrate the advantages of our theory .",
    "finally , several technical results and proofs are moved to the appendix .",
    "we generalize the class of self - concordant functions introduced by nesterov and nemirovskii in @xcite to a broader class of smooth and convex functions .",
    "we identify several examples of such functions .",
    "then , we study several properties of this function class by utilizing our new definition .",
    "[ [ notation ] ] * notation : * + + + + + + + + + + +    given a proper , closed and convex function @xmath20 , we denote by @xmath21 the domain of @xmath15 , and @xmath22 the subdifferential of @xmath15 at @xmath23 .",
    "we use @xmath24 to denote a class of three times continuously differentiable functions on its open domain @xmath25 . for a twice continuously differentiable convex function @xmath15 ,",
    "its hessian @xmath26 is symmetric positive semidefinite , and can be written as @xmath27 .",
    "if it is positive definite , we write @xmath28 .",
    "we use @xmath29 and @xmath30 to denote the set of symmetric positive semidefinite and symmetric positive definite matrices of size @xmath31 , respectively . any matrix @xmath32 ( respectively , @xmath33 ) satisfies @xmath34 ( respectively , @xmath35 ) .",
    "[ [ generalized - self - concordance ] ] * generalized self - concordance : * + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    let @xmath0 be a three times continuously differentiable function on the open domain @xmath36 .",
    "then , we write @xmath37 . in this case , @xmath1 is convex if @xmath38 for all @xmath39 .",
    "we introduce the following definition .",
    "[ de : gsc_def0 ] _ let @xmath0 be a @xmath40 and univariate function with open domain @xmath36 , @xmath6 , and @xmath41 be two constants .",
    "we say that @xmath1 is @xmath42-generalized self - concordant if @xmath43 _    from definition  [ de : gsc_def0 ] , it also indicates that @xmath38 for all @xmath44 .",
    "hence , @xmath1 is convex .",
    "clearly , if @xmath45 for any constants @xmath46 and @xmath47 , we have @xmath48 and @xmath49 .",
    "the inequality is automatically satisfied for any @xmath6 and @xmath41 .",
    "the smallest value of @xmath19 is zero .",
    "hence , any convex quadratic function is @xmath50-_generalized self - concordant _  for any @xmath6 .",
    "while holds for any other constant @xmath51 , we often require that @xmath19 is the smallest constant satisfying .",
    "[ [ examples ] ] * examples : * + + + + + + + + + + +    let us now provide some common examples satisfying definition [ de : gsc_def0 ] .    *",
    "_ standard self - concordant functions : _ if we choose @xmath7 , then becomes @xmath2 , which is the standard self - concordant functions introduced in @xcite . *",
    "_ logistic functions : _ in @xcite , bach modified the standard self - concordant inequality in @xcite to obtain @xmath52 , and showed that the well - known logistic loss @xmath53 satisfies this definition .",
    "in @xcite the authors also exploited this definition , and develop a class of first - order and second - order methods to solve composite convex minimization problems .",
    "hence , @xmath53 is a generalized self - concordant function with @xmath54 and @xmath55 . * _ exponential functions : _ the exponential function @xmath56 also belongs to with @xmath54 and @xmath55 .",
    "this function is often used , e.g. , in ada - boost @xcite . * _ distance - weighted discrimination _ ( dwd ) : we consider a more general function @xmath57 on @xmath58 and @xmath59 studied in @xcite for dwd using in classification .",
    "as shown in table [ tbl : examples ] , this function satisfies definition [ de : gsc_def0 ] with @xmath60{q(q+1)}}$ ] and @xmath61 . * _ entropy function : _ we consider the well - known entropy function @xmath62 for @xmath63 .",
    "we can easily show that @xmath64 .",
    "hence , it is generalized self - concordant with @xmath65 and @xmath54 in the sense of definition [ de : gsc_def0 ] . *",
    "_ arcsine distribution : _ we consider the function @xmath66 for @xmath67 .",
    "this function is convex and smooth .",
    "moreover , we verify that this function satisfies definition [ de : gsc_def0 ] with @xmath68 and @xmath69 .",
    "we can generalize this function to @xmath70 for @xmath71 , where @xmath72 and @xmath73 .",
    "then , we can find @xmath74 . *",
    "_ robust regression : _ consider a monomial function @xmath75 for @xmath76 studied in @xcite for robust regression using in statistics .",
    "then , @xmath77{q(q-1)}}$ ] and @xmath78 .    as concrete examples , the following table , table [ tbl : examples ] ,",
    "provides a non - exhaustive list of generalized self - concordant functions used in the literature .",
    ".examples of univariate generalized self - concordant functions ( @xmath79 means that @xmath1 has lipschitz gradient ) . [ cols=\"<,<,^,^,<,<,<,<\",options=\"header \" , ]     as shown in table  [ tbl : logis_ncp2 ] , using the step - size @xmath80 as a lower bound for backtracking linesearch also reduces the number of function evaluations in these three problems .",
    "we note that the number of function evaluations depends on the starting point @xmath81 as well as the factor @xmath82 in .",
    "if we set @xmath82 too small , then the decrease on @xmath15 can be small .",
    "otherwise , if we set @xmath82 too high , then our decrement @xmath83 may never achieve , and the linesearch is failed .",
    "if we change the starting point @xmath81 , the number of function evaluations can significantly increase .      in this example",
    ", we test the performance of algorithm  [ alg : newton_alg ] on the distance - weighted discrimination ( dwd ) problem introduced in @xcite . in order to directly use algorithm  [ alg : newton_alg ] , we slightly modify the setting in @xcite to obtain the following form : @xmath85^{\\top}\\in{\\mathbb{r}}^p}\\big\\{f({\\mathbf{x } } ) : = \\frac{1}{n}\\sum_{i=1}^n \\frac{1}{({\\mathbf{a}}_i^{\\top}{\\mathbf{w}}+ \\mu y_i + \\xi_i)^q } + { \\mathbf{c}}^{\\top}\\xi + \\frac{1}{2}\\left(\\lambda_1 \\vert { \\mathbf{w}}\\vert_2 ^ 2 + \\lambda_2\\mu^2 + \\lambda_3\\vert\\xi\\vert_2 ^ 2\\right ) \\big\\},\\ ] ] where @xmath73 , @xmath86 ( @xmath87 and @xmath88 are given , and @xmath89 ( @xmath90 ) are three regularization parameters for @xmath91 , @xmath92 and @xmath93 , respectively . here , the variable @xmath94 consists of the support vector @xmath91 , the intercept @xmath92 , and the slack variable @xmath93 as used in @xcite . here , we penalize these variables by using least squares terms instead of the @xmath95-penalty term as in @xcite .",
    "we note that the setting is not just limited to the dwd application above , but can also be used to formulate other practical models such as path - tracking in robotics @xcite if we choose an appropriate parameter @xmath96 .",
    "since @xmath57 is @xmath97-generalized self - concordant with @xmath98{q(q+1)}}n^{\\frac{1}{q+2}}$ ] and @xmath99 , using proposition  [ pro : addrule ] , we can show that @xmath15 is @xmath100-generalized self - concordant with @xmath101{q(q+1)}}n^{\\frac{1}{q+2}}\\max{\\left\\{{\\left\\vert({\\mathbf{a}}_i^{\\top } , y_i , { \\mathbf{e}}_i^{\\top})^{\\top}\\right\\vert}_2^{q/(q+2)},1\\leq i\\leq n\\right\\}}$ ] ( here , @xmath102 is the @xmath103-th unit vector ) .",
    "problem can be transformed into a second - order cone program @xcite , and can be solved by interior - point methods .",
    "for instance , if we choose @xmath104 , then , by introducing intermediate variables @xmath105 and @xmath106 , we can transform into a second - order cone program using the fact that @xmath107 is equivalent to @xmath108 .",
    "we implement algorithm  [ alg : newton_alg ] to solve and compare it with the interior - point methods implemented in an open - source software : sdpt3 @xcite , and a commercial software : mosek . as in the previous example",
    ", we also integrate algorithm  [ alg : newton_alg ] with a backtracking linesearch using our step - size @xmath109 ( ls with @xmath109 ) as a lower bound .",
    "we note that since @xmath15 does not have lipschitz gradient , we can not apply gradient - type methods to solve   due to the lack of a theoretical guarantee .",
    "since we can not run sdpt3 on big data sets , we rather test our algorithms and these two interior - point solvers on the @xmath110 small and medium size problems using data from @xcite available at https://www.csie.ntu.edu.tw/~cjlin / libsvm/. we choose the regularization parameters as @xmath111 and @xmath112 .",
    "we note that if the data set has the size of @xmath113 , then number of variables in becomes @xmath114 . hence",
    ", we use a built - in matlab conjugate gradient solver to compute the newton direction @xmath115 .",
    "the initial point @xmath81 is chosen as @xmath116 , @xmath117 and @xmath118 . in our algorithms",
    ", we use @xmath119 as a stopping criterion .",
    "lrr | rrr | rrr | rr | rr & & & & + & & & iter & time[s ] & @xmath120 & iter & time[s ] & @xmath120 & time[s ] & @xmath120 & time[s ] & @xmath120 +   + a1a & 1605 & 119 & 170 & 1.35 & 9.038e-12 & 13 & 0.12 & 4.196e-13 & 25.46 & 9.086e-09 & 0.49 & 1.806e-08 + a2a & 2265 & 119 & 192 & 2.71 & 1.661e-13 & 12 & 0.15 & 8.549e-09 & 32.91 & 3.223e-09 & 0.50 & 2.858e-08 + a4a & 4781 & 122 & 247 & 5.60 & 1.180e-13 & 12 & 0.27 & 5.380e-10 & 76.32 & 2.130e-09 & 0.94 & 1.740e-08 + leu & 38 & 7129 & 54 & 2.71 & 2.214e-10 & 15 & 0.58 & 3.995e-13 & 40.93 & 4.930e-10 & 0.72 & 2.828e-07 + w1a & 2270 & 300 & 169 & 2.88 & 9.752e-09 & 13 & 0.17 & 4.968e-09 & 39.68 & 2.184e-10 & 0.50 & 1.561e-08 + w2a & 3184 & 300 & 193 & 3.32 & 4.532e-13 & 13 & 0.27 & 1.428e-09 & 62.15 & 1.288e-10 & 0.61 & 1.793e-08 +   + a1a & 1605 & 119 & 166 & 2.28 & 6.345e-12 & 14 & 0.15 & 5.185e-13 & 30.47 & 1.434e-10 & 0.48 & 1.617e-09 + a2a & 2265 & 119 & 186 & 2.63 & 3.028e-12 & 13 & 0.22 & 5.015e-09 & 45.48 & 2.667e-09 & 0.56 & 3.070e-09 + a4a & 4781 & 122 & 235 & 5.03 & 8.676e-13 & 13 & 0.31 & 4.347e-10 & 129.42 & 2.041e-09 & 1.25 & 4.039e-09 + leu & 38 & 7129 & 57 & 3.08 & 1.631e-10 & 16 & 0.63 & 2.754e-12 & 38.92 & 2.634e-11 & 0.73 & 6.436e-08 + w1a & 2270 & 300 & 146 & 2.15 & 1.311e-12 & 14 & 0.22 & 4.057e-09 & 53.54 & 1.043e-09 & 0.59 & 1.295e-09 + w2a & 3184 & 300 & 165 & 3.43 & 3.397e-09 & 14 & 0.29 & 1.187e-09 & 88.73 & 3.144e-10 & 0.71 & 1.653e-09 +    the results and performance of these algorithms are reported in table  [ tbl : dwd_ncp1 ] for two cases : @xmath121 and @xmath122 .",
    "we can see that algorithm  [ alg : newton_alg ] with our step - size outperforms the well - established sdpt3 solver .",
    "if we combine it with a backtracking linesearch , then it also outperforms mosek , a commercial software package .",
    "all the algorithms achieve a very high accuracy in terms of the relative norm of the gradient @xmath120 , which is up to @xmath123 .",
    "we emphasize that our methods are highly parallelizable and their performance can be improved by exploiting this structure @xcite .      in this example",
    ", we aim at verifying algorithm  [ alg : prox_nt_alg ] for solving the composite generalized self - concordant minimization problem   with @xmath7 .",
    "we illustrate this algorithm on the following portfolio optimization problem with logarithmic utility functions @xcite ( scaling by a factor @xmath124 ) : @xmath125 where @xmath126 for @xmath127 are given vectors presenting the returns at the @xmath103-th period of the assets considered in the portfolio data .",
    "more precisely , as indicated in @xcite , @xmath128 measures the return as the ratio @xmath129 between the closing prices @xmath130 and @xmath131 of the stocks on the current day @xmath103 and on the previous day @xmath132 , respectively ; @xmath133 is the vector of all ones .",
    "the aim is to find an optimal strategy to assign the proportion of the assets in order to maximize the expected return among all portfolios .",
    "let @xmath134 be the standard simplex , and @xmath135 be the indicator function of @xmath136 .",
    "then , we can formulate into .",
    "the function @xmath137 defined in is @xmath138-generalized self - concordant with @xmath7 and @xmath139 .",
    "we implement algorithm  [ alg : prox_nt_alg ] using an accelerated projected gradient method @xcite to compute the proximal newton direction .",
    "we also implement the frank - wolfe algorithm and its linesearch variant in @xcite , and a projected gradient method using barzilai and borwein s step - size .",
    "we name these algorithms by ` fw ` , ` fw - ls ` and ` pg - bb ` , respectively .",
    "we emphasize that both ` pg - bb ` and ` fw`-ls do not have a theoretical guarantee when solving . ` fw",
    "` has a theoretical guarantee as recently proved in @xcite , but the complexity bound is very large .",
    "we terminate all the algorithms using @xmath140 , where @xmath141 in algorithm  [ alg : prox_nt_alg ] , @xmath142 in ` pg - bb ` , and @xmath143 in ` fw ` and ` fw - ls ` .",
    "we choose different accuracies for these methods due to the limitation of first - order methods in the last three algorithms .",
    "we test these algorithms on two categories of dataset : synthetic and real stock data . for the synthetic data , we generate matrix @xmath144 with given the price ratios as described above .",
    "more precisely , we generate @xmath145 , which allows the closing prices to vary about @xmath146 between two consecutive periods .",
    "we test with three instances , where @xmath147 , @xmath148 , and @xmath149 , respectively .",
    "we name these three datasets by , and , respectively . for the real data ,",
    "we download a us stock dataset using an excel tool http://www.excelclout.com / historical - stock - prices - in - excel/. this tool gives us the closing prices of us stock markets in a given period of time .",
    "we generate three datasets with different sizes using different numbers of stocks from 2005 to 2016 as described in @xcite .",
    "we pre - processed the data by moving stocks that are empty or lacking of information in the time period we specified .",
    "we name these three datasets by , and , respectively .",
    "the results and the performance of the four algorithms are given in table  [ tbl : portf_cp1 ] . here , ` iter ` gives the number of iterations , ` error ` measures the relative difference between the approximate solution @xmath150 given by the algorithms and the interior - point solution provided by cvx @xcite with the high precision configuration ( up to @xmath151 : @xmath152 .",
    "lrr | rrr | rrr | rrr | rrr & & & & + & & & iter & time[s ] & error & iter & time[s ] & error & iter & time[s ] & error & iter & time[s ] & error +   + portfsyn1 & 1000 & 800 & 6 & 5.68 & 2.4e-04 & 645 & 3.98 & 2.3e-04 & 15530 & 96.47 & 2.3e-04 & 6509 & 47.88 & 2.3e-04 + portfsyn2 & 1000 & 1000 & 6 & 6.96 & 6.8e-05 & 1207 & 11.54 & 7.5e-05 & 17201 & 166.89 & 1.7e-04 & 6664 & 70.15 & 1.4e-04 + portfsyn3 & 1000 & 1200 & 7 & 12.91 & 3.2e-04 & 959 & 9.55 & 3.0e-04 & 16391 & 159.28 & 3.3e-04 & 5750 & 64.36 & 3.2e-04 +   + stocks1 & 473 & 500 & 8 & 1.22 & 7.1e-06 & 736 & 1.22 & 1.9e-06 & 16274 & 24.93 & 7.0e-05 & 2721 & 5.28 & 4.1e-04 + stocks2 & 625 & 723 & 8 & 3.71 & 2.7e-05 & 1544 & 4.37 & 8.0e-06 & 11956 & 34.35 & 3.1e-04 & 2347 & 9.33 & 5.2e-04 + stocks3 & 625 & 889 & 10 & 6.83 & 5.6e-05 & 1074 & 6.54 & 5.4e-06 & 13027 & 52.89 & 1.7e-04 & 2096 & 8.46 & 7.4e-04 +    from table  [ tbl : portf_cp1 ] we can see that algorithm [ alg : prox_nt_alg ] has a comparable performance to the first - order methods : ` fw - ls ` and ` pg - bb ` .",
    "while our method has a rigorous convergence guarantee , these first - order methods remains lacking of a theoretical guarantee .",
    "we note that algorithm [ alg : prox_nt_alg ] and ` pg - bb ` are faster than the ` fw ` method and its linesearch variant although the optimal solution @xmath153 of this problem is very sparse .",
    "we also note that ` pg - bb ` gives a smaller error to the cvx solution , but this cvx solution is not the ground - truth @xmath153 but gives a high approximation to @xmath153 .",
    "in fact , the cvx solution is dense .",
    "hence , it is not clear if ` pg - bb ` produces a better solution than other methods .",
    "we apply our proximal newton and proximal quasi - newton methods to solve the following sparse multinomial logistic problem studied in various papers including @xcite : @xmath155}_{f({\\mathbf{x } } ) } + \\underbrace{\\lambda\\vert{\\mathrm{vec}({\\mathbf{x}})}\\vert_1}_{g({\\mathbf{x } } ) } \\big\\},{\\!\\!\\ ! } \\vspace{-0.5ex}\\ ] ] where @xmath94 can be considered as a matrix variable of size @xmath156 formed from @xmath157 , @xmath158 is the vectorization operator , and @xmath159 is a regularization parameter . both @xmath160 and @xmath161",
    "are given as input data for @xmath162 and @xmath163 .",
    "the function @xmath15 defined in has a closed form hessian .",
    "however , forming the full hessian matrix @xmath164 requires an intensive computation in large - scale problems when @xmath165 .",
    "hence , we apply our proximal - quasi - newton methods in this case . as shown in (",
    "* lemma 4 ) , the function @xmath15 is @xmath166-generalized self - concordant with @xmath55 and @xmath167 .",
    "we implement our proximal quasi - newton methods to solve and compare them with the accelerated first - order methods implemented in a well - established software package called tfocs @xcite .",
    "we use three different variants of tfocs : tfocs with n07 ( using nesterov s 2007 method with two proximal operations per iteration ) , tfocs with n83 ( using nesterov s 1983 method with one proximal operation per iteration ) , and tfocs with at ( using auslender and teboulle s accelerated method ) .",
    "we test on a collection of @xmath168 multi - class datasets downloaded from https://www.csie.ntu.edu.tw/~cjlin / libsvm/. we set the parameter @xmath169 in at @xmath170 after performing a fine tuning .",
    "we terminate all the algorithms if @xmath171 .",
    "we first plot the convergence behavior in terms of iterations of three proximal newton algorithms we proposed in this paper in figure  [ fig : mnlogistic_profiles ] ( left ) for the ` dna ` problem with @xmath172 classes , @xmath173 data points and @xmath174 features .     of @xmath175 methods ( _ right_),scaledwidth=100.0% ]",
    "as we can see from this figure , the proximal newton method takes less iterations than the two other methods .",
    "however , each iteration of this method is more expensive than the proximal - quasi - newton methods due to the evaluation of the hessian matrix . in our experiment ,",
    "the quasi - newton method with l - bfgs outperforms the one with bfgs .",
    "next , we build a performance profile in time [ second ] to compare five different algorithms : two proximal quasi - newton methods proposed in this paper ( bfgs and l - bfgs ) , and three variants of the accelerated first - order methods implemented in tfocs .",
    "the performance profile was studied in @xcite , which can be considered as a standard way to compare different optimization algorithms .",
    "a performance profile is built based on a set @xmath176 of @xmath177 algorithms ( solvers ) and a collection @xmath178 of @xmath179 problems .",
    "we build a profile based on computational time .",
    "we denote by @xmath180 .",
    "we compare the performance of algorithm @xmath181 on problem @xmath182 with the best performance of any algorithm on this problem ; that is we compute the performance ratio @xmath183 .",
    "now , let @xmath184 for @xmath185 .",
    "the function @xmath186 $ ] is the probability for solver @xmath181 that a performance ratio is within a factor @xmath187 of the best possible ratio .",
    "we use the term `` performance profile '' for the distribution function @xmath188 of a performance metric . in the following numerical examples",
    ", we plotted the performance profiles in @xmath189-scale , i.e. @xmath190 .    figure  [ fig : mnlogistic_profiles ] ( right ) shows the performance profile of the five algorithms on a collection of @xmath168 problems indicated above . the proximal quasi - newton method with l - bfgs achieves @xmath191 ( @xmath192 ) with the best performance , while the bfgs obtains @xmath193 ( @xmath194 ) with the best performance . in terms of computational time ,",
    "both proximal quasi - newton methods outperform the optimal proximal gradient methods in this experiment .",
    "it is also clearly our quasi - newton - type methods achieve a higher accuracy approximation solution in this experiment than the first order methods .",
    "we have generalized the self - concordance notion in @xcite to a more general class of smooth and convex functions .",
    "such a function class covers several well - known examples , including logistic , exponential , reciprocal and standard self - concordant functions .",
    "we developed a unified theory to reveal the smoothness structure of this functional class .",
    "we provided several key bounds on the hessian , gradient and function value of this function class .",
    "then , we illustrated our theory by applying it to solve a class of smooth convex minimization problems and its composite setting .",
    "we believe that our theory provides an appropriate approach to exploit the curvature of these problems and allows us to compute an explicit step - size in newton - type methods that have a global convergence guarantee even for non - lipschitz gradient / hessian functions .",
    "while our theory is still valid for the case @xmath195 , we have not found yet a representative application in a high - dimensional space .",
    "we therefore limit our consideration to newton and proximal newton methods for @xmath196 $ ] , but our key bounds in subsection  [ subsec : key_bounds ] remain valid for @xmath195 .    our future research is to focus on several aspects .",
    "first , we can exploit this theory to develop more practical inexact and quasi - newton methods that can easily capture practical applications .",
    "second , we will combine our approach and stochastic , randomized and coordinate descent methods to develop new variants of algorithms that can scale better .",
    "third , by exploiting both generalized self - concordant , lipschitz gradient , and strong convexity , one can also develop first - order methods to solve convex optimization problems .",
    "finally , we plan to generalize our theory to primal - dual setting and monotone operators to apply to other classes of convex problems such as convex - concave saddle points , constrained convex optimization , and monotone equations and inclusions .",
    "* acknowledgments : * this work is partially supported by the nsf - grant no .",
    "dms-1619884 , usa .",
    "this appendix provides the full proofs of technical results in the main text .",
    "first , we prove some technical results used in the paper .",
    "then , we provide the full convergence analysis of the results in the main text .        * for a fixed @xmath197 , we have @xmath198 for all @xmath199 $ ] .",
    "@xmath200 t .",
    "\\end{array}\\ ] ] * for a fixed @xmath201 or @xmath202 , we have @xmath203 for all @xmath199 $ ] .",
    "consequently , we have @xmath204t,\\vspace{1.25ex}\\\\ & \\frac{1}{rt}\\left(1 - ( 1-t)^r\\right ) - 1 \\leq   \\left[\\frac{(1-r)}{2 }   + \\frac{(r-1)(r-2)}{6}t\\right]t . \\end{array}\\ ] ] * for @xmath205 , we have @xmath206 , and for @xmath207 , @xmath208 .",
    "* for @xmath63 , we also have @xmath209 . * for @xmath210 , we have @xmath211 .",
    ". then @xmath213 . by",
    ", we have @xmath214 .",
    "hence , we have @xmath215 where @xmath216 and @xmath217 are the two integrals in the above inequality . computing these integrals explicitly",
    ", we can show that          [ le : h_norm ] given @xmath227 , the matrix @xmath228 defined by @xmath229\\nabla^2f({\\mathbf{x}})^{-1/2},\\ ] ] satisfies @xmath230 where @xmath231 is defined as follows for @xmath210 : @xmath232 & \\text{if $ 2",
    "< \\nu < \\frac{8}{3}$}\\\\ \\frac{(3 - 2t)}{2(1-t)^2 } & \\text{if $ \\nu = \\frac{8}{3}$}\\\\ \\frac{1}{2(1-t)^{\\frac{4-\\nu}{\\nu-2}}}\\left[\\frac{1}{\\nu-2 } - \\frac{(4-\\nu)(3-\\nu)}{(\\nu-2)^2}t\\right ] & \\text{if $ \\frac{8}{3 } < \\nu < 3$}\\\\ \\frac{1}{1-t } & \\text{if $ \\nu = 3$}. \\end{cases}\\ ] ]        for @xmath242 and @xmath220 , we have @xmath243 indeed , we show that @xmath244 .",
    "let @xmath245 and @xmath246 .",
    "the last inequality is equivalent to @xmath247 , which can be reformulated as @xmath248 .",
    "consider @xmath249 .",
    "it is clear that @xmath250 for all @xmath199 $ ] .",
    "we obtain @xmath251 .",
    "finally , our statement is proved .",
    "let us define @xmath252 . then , @xmath253 . if @xmath202 , then @xmath254 , and if @xmath255 , then @xmath256 . if @xmath257 , then @xmath258 . if we define @xmath259 , then @xmath260 .",
    "hence , if @xmath261 , then @xmath262 .",
    "( a )  if @xmath254 , i.e. , @xmath263 or @xmath264 , i.e. @xmath265 , then we have @xmath266t\\\\ & = \\frac{1}{2(1-t)^{\\frac{4-\\nu}{\\nu-2}}}\\left[\\frac{2}{\\nu-2 } - \\frac{2(3\\nu-4)}{6(\\nu-2)^2}t + \\frac{(4-\\nu)(6 - 2\\nu)(8 - 3\\nu)}{3(\\nu-2)^3}t^2\\right]t .",
    "\\end{array}\\ ] ]      ( c )  if @xmath256 , then @xmath257 , and we have @xmath269 t \\\\ & = \\frac{1}{2(1-t)^{\\frac{4-\\nu}{\\nu-2}}}\\left[\\frac{2}{\\nu-2 } - \\frac{(4-\\nu)(6 - 2\\nu)}{(\\nu-2)^2}t\\right]t .",
    "\\end{array}\\ ] ] ( d )  if @xmath7 , i.e. , @xmath270 , and @xmath271 .",
    "consider the level set @xmath273 .",
    "for any @xmath274 and @xmath275 , by and the convexity of @xmath16 , we have @xmath276 by the cauchy - schwarz inequality , we have @xmath277 now , using the assumption @xmath278 for some @xmath279 , we have @xmath280 .    1 .   if @xmath55 , then @xmath281 .",
    "this estimate together with imply @xmath282 2 .   if @xmath283 , then @xmath284 this inequality together with imply @xmath285 3 .   if @xmath7 , then @xmath286 . combining this estimate and we get @xmath287    now , let us consider the function @xmath288 .",
    "this function is increasing in @xmath289 with its value in @xmath289 in each case above .",
    "therefore , it is invertible , and its inverse is also increasing . because the level set @xmath290 is bounded , and thus there exists a solution @xmath291 to problem .    by , and the non - singularity of @xmath292 for some @xmath23 , the function @xmath15 is strictly convex . moreover , since @xmath16 is convex , @xmath293 is strictly convex due to the fact that @xmath15 is strictly convex , the uniqueness of @xmath291 follows .",
    "@xmath226    [ [ apdx : th : damped_step_nt ] ] the proof of theorem  [ th : damped_step_nt ] : the convergence of the damped - step newton method ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~        from proposition [ pro : fx_bound1 ] , for any @xmath294 , if @xmath295 , then we have @xmath296 now , using , we have @xmath297 . on the other hand , we have @xmath298 using the definition of @xmath299 in , the last equalities , and , we can easily show that @xmath300 . substituting these relations into the first estimate",
    ", we obtain @xmath301 we consider the following cases :    if @xmath55 , by , we have @xmath302 with @xmath303 .",
    "this function attains the maximum at @xmath304 with @xmath305 = \\left(\\frac{\\lambda_k}{\\beta_k}\\right)^2\\left [ ( 1 \\!+\\ ! \\beta_k)\\ln(1 \\!+\\ !",
    "\\beta_k ) \\!-\\ !",
    "\\beta_k\\right].\\ ] ] it is easy to check from the rightmost term of the last expression that @xmath306 for @xmath307 .    if @xmath7 , by , we have @xmath308 with @xmath309 .",
    "we can also show that @xmath310 achieves the maximum at @xmath311 with @xmath312.\\ ] ] we can also easily check that the last term @xmath313 of this expression is positive for @xmath314 .    if @xmath283 , then we have @xmath315 .",
    "then , by , we have @xmath316 our aim is to find @xmath317 $ ] by solving @xmath318}\\eta_k(\\tau)$ ] .",
    "this problem always has a global solution .",
    "first , we compute the first and the second order derivatives of @xmath319 as follows : @xmath320\\textrm { and } \\eta_k''(\\tau)=-\\lambda_k^2(1-\\tau d_k)^{\\frac{-2}{\\nu-2}}.\\ ] ]    let us set @xmath321",
    ". then , we get @xmath322 \\in ( 0,1)~~~~\\textrm{(by the bernoulli inequality)},\\ ] ] with @xmath323+\\left(\\frac{\\lambda_k}{d_k}\\right)^2 \\frac{\\nu-2}{2(3-\\nu)}\\left[1-\\left(1+\\frac{4-\\nu}{\\nu-2}d_k\\right)^{2-\\nu}\\right].\\ ] ] in addition , we can check that @xmath324 .",
    "hence , the value of @xmath109 above achieves the maximum of @xmath325 .",
    "then , we have @xmath326 .",
    "let @xmath327 be the optimal solution of .",
    "we have @xmath328 hence , we can write @xmath329 \\vert_{{\\mathbf{x}}^k}.\\ ] ] let us define @xmath330\\big\\vert_{{\\mathbf{x}}^k}$ ] and consider three cases as follows :    @xmath331   for @xmath55 , using corollary  [ co : hessian_bound2 ] , we have @xmath332 , where @xmath333 . using the above inequality",
    ", we can show that @xmath334 let @xmath335 .",
    "we first derive @xmath336 where @xmath337 . using corollary  [ co : hessian_bound2 ] and noting that @xmath333",
    ", we can estimate @xmath338 . using the two last estimates , and the definition of @xmath339",
    ", we can derive @xmath340 provided that @xmath341 . since , the step - size @xmath342 , we have @xmath343 . on the other hand , @xmath344 for all @xmath345 .",
    "substituting @xmath346 into and using these relations , we have @xmath347 provided that @xmath341 . on the other hand , by proposition  [ pro : hessian_bounds ]",
    ", we have @xmath348 and @xmath349 . in addition , @xmath350 combining the above inequalities , we finally get @xmath351 under the fact that @xmath352 , and @xmath353 , this estimate shows that @xmath354 quadratically converges to zero . since @xmath355 , we can also conclude that @xmath356 quadratically converges to zero .",
    "@xmath357   for @xmath7 , we can follow @xcite . however ,",
    "for completeness , we give a short proof here . using corollary  [ co : hessian_bound2 ]",
    ", we have @xmath358 , where @xmath359 . using the above inequality",
    ", we can show that @xmath360 substituting @xmath346 into and using @xmath361 , we have @xmath362 next , we need to upper bound @xmath363 . since @xmath364 . using corollary  [ co : hessian_bound2 ]",
    ", we can bound @xmath363 as @xmath365 provided that @xmath366 . overestimating the above inequality using this bound",
    ", we get @xmath367r_k^2}{1-r_k } \\leq ( 2 + m_f)r_k^2,\\ ] ] provided that @xmath368 . on the other hand , we can also estimate @xmath369 . combining the last two inequalities ,",
    "we get @xmath370 the right - hand side function @xmath371 on @xmath372 $ ] for a given positive constant @xmath373 . hence , if @xmath374 , then @xmath375 .",
    "this shows that if @xmath376 is chosen such that @xmath377 , then @xmath378 quadratically converges to zero .",
    "@xmath379   for @xmath84 , with the same argument as in the proof of theorem  [ th : full_step_nt_scheme_converg ] , we can show that @xmath380 where @xmath272 is defined by and @xmath381 . using again the argument as in the proof of theorem  [",
    "th : full_step_nt_scheme_converg ] , we have @xmath382 here , @xmath383 is a given function driving from @xmath272 . under the condition that @xmath384 and @xmath385 are sufficiently small , we can show that @xmath386 .",
    "hence , the last inequality shows that @xmath387 quadratically converges to zero . since @xmath388 , where @xmath389 , we have @xmath390 .",
    "hence , we can conclude that @xmath391 also locally converges to zero at a quadratic rate .",
    "@xmath226    [ [ apdx :",
    "th : full_step_nt_scheme_converg ] ] the proof of theorem  [ th : full_step_nt_scheme_converg ] : the convergence of the full - step newton method ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~      [ [ the - quadratic - convergence - of - bigfraclambda_kunderlinesigma_kfrac3-nu2big ] ] * the quadratic convergence of @xmath392 * : + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    since the full - step newton scheme updates @xmath394 , if we denote by @xmath395 , then the last expression leads to @xmath396 . in addition , @xmath397 . using the definition of @xmath398 in",
    ", we denote @xmath399 .",
    "first , by @xmath396 and the mean - value theorem , we can show that @xmath400{\\mathbf{d}}^kdt.\\ ] ] let us define @xmath401dt$ ] and @xmath402 .",
    "then , the above estimate implies @xmath403 .",
    "hence , we can show that @xmath404 ^ 2 & = { \\langle \\nabla^2{f}({\\mathbf{x}}^k)^{-1}{\\mathbf{g}}_k{\\mathbf{d}}^k , { \\mathbf{g}}_k{\\mathbf{d}}^k\\rangle }   = { \\langle { \\mathbf{h}}_k\\nabla^2{f}({\\mathbf{x}}^k)^{1/2}{\\mathbf{d}}^k , { \\mathbf{h}}_k\\nabla^2{f}({\\mathbf{x}}^k)^{1/2}{\\mathbf{d}}^k\\rangle}\\nonumber\\\\ & \\leq \\vert { \\mathbf{h}}_k\\vert^2\\vert { \\mathbf{d}}^k\\vert_{{\\mathbf{x}}^k}^2 = \\vert { \\mathbf{h}}_k\\vert^2\\lambda_k^2.\\end{aligned}\\ ] ] by lemma [ le : h_norm ] , we can estimate @xmath405 where @xmath272 is defined by . combining the two last inequalities and using proposition [ pro : hessian_bounds ] , we consider the following cases :    ( a )  if @xmath55 , then we have @xmath406 ^ 2 $ ] , which implies @xmath407 .",
    "note that @xmath408 and @xmath409 .",
    "based on the above inequality , we have @xmath410 by a numerical calculation , we can easily check that if @xmath411 , then @xmath412 consequently , if @xmath413 , then we can prove @xmath414 by induction . under the condition @xmath415 , the above inequality shows that the ratio @xmath416 converges to zero at a quadratic rate .",
    "now , if @xmath242 , we consider different cases .",
    "we note that @xmath417 ^ 2,\\ ] ] which follows that @xmath418 note that @xmath419 , @xmath420 and @xmath421 .",
    "based on these relations and we can argue as follows :    @xmath357  if @xmath283 , then @xmath422 , which follows that @xmath423 .",
    "hence , @xmath424 if @xmath425 , where @xmath426 is the unique solution to the equation @xmath427 then @xmath428 .",
    "hence , if we choose @xmath376 such that @xmath429 , then we can prove the following two inequalities together by induction : @xmath430 in addition , the above inequality also shows that @xmath431 quadratically converges to zero .",
    "@xmath379  if @xmath7 , then @xmath432 , and @xmath433 directly checking the right - hand side of the above estimate , one can show that if @xmath434 , then @xmath435 .",
    "hence , if @xmath436 , then we can prove the following two inequalities together by induction : @xmath437 moreover , the first inequality above also shows that @xmath438 converges to zero at a quadratic rate .",
    "[ [ the - quadratic - convergence - of - bigvertmathbfxk --- mathbfxstar_fvert_mathbfh_kbig ] ] * the quadratic convergence of @xmath393 : * + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    first , using proposition  [ pro : gradient_bound1 ] with @xmath439 and @xmath440 , and noting that @xmath364 , we have @xmath441 where the last inequality follows from the cauchy - schwarz inequality .",
    "hence , we obtain @xmath442 we consider three cases :    ( 1 )  when @xmath55 , we have @xmath443 .",
    "hence , @xmath444 whenever @xmath445 . using this inequality in ,",
    "we have @xmath446 provided that @xmath445 .",
    "one the other hand , by the definition of @xmath447 , we have @xmath448 . combining the two last inequalities , we obtain @xmath449 provided that @xmath445 .",
    "since @xmath416 locally converges to zero at a quadratic rate , the last relation also shows that @xmath450 also locally converges to zero at a quadratic rate .",
    "( 2 )  for @xmath7 , we have @xmath451 and @xmath452 .",
    "hence , from , we obtain @xmath453 .",
    "this implies @xmath454 as long as @xmath455 .",
    "clearly , since @xmath363 locally converges to zero at a quadratic rate , @xmath385 also locally converges to zero at a quadratic rate .",
    "( 3 )  for @xmath283 , we have @xmath456 provided that @xmath457 .",
    "similar to the case @xmath55 , we have @xmath458 , where @xmath389 . hence , @xmath459 .",
    "since @xmath460 locally converges to zero at a quadratic rate , @xmath461 also locally converges to zero at a quadratic rate .",
    "@xmath226    [ [ apdx :",
    "th : comp_decr ] ] the proof of theorem  [ th : comp_decr ] : the convergence of damped - step proximal newton method ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~    given @xmath33 and a proper , closed and convex function @xmath462 , we define @xmath463 if @xmath464 is the hessian of a strictly convex function @xmath15 , we can also write @xmath465 shortly as @xmath466 for our notational convenience .",
    "the following lemma will be used in the sequel , whose proof can be found in  @xcite .",
    "[ le : nonexpansiveness ] let @xmath462 be a proper , closed and convex function , and @xmath33 . then , the mapping @xmath467 defined above is nonexpansive with respect to the weighted norm defined by @xmath468 , i.e. , for any @xmath469 , we have @xmath470        since @xmath474 satisfies the optimality condition  , we have @xmath475 using proposition [ pro : fx_bound1 ] we obtain @xmath476 since @xmath477 , using this relation and the convexity of @xmath16 , we have @xmath478 summing up the last two inequalities , we obtain the following estimate @xmath479 with the same argument as in the proof of theorem [ th : damped_step_nt ] , we obtain the conclusion of theorem  [ th : comp_decr ] .",
    "we consider the distance between @xmath480 and @xmath291 measured by @xmath481 . by the definition of @xmath480",
    ", we have @xmath482 using the new notations in , it follows from the optimality condition and that @xmath483 and @xmath484 . by lemma [ le : nonexpansiveness ] and the triangle inequality",
    ", we can show that @xmath485 by following the same argument as in @xcite , if we apply lemma  [ le : h_norm ] , then we can derive @xmath486 where @xmath487 is defined by .",
    "next , using the same argument as the proof of in theorem  [ th : comp_full_pnt_scheme ] below , we can bound the second term @xmath488 of as @xmath489 \\vert   { \\mathbf{z}}^k - { \\mathbf{x}}^k \\vert _ { { \\mathbf{x}}^{\\star } } , ~~~&\\text{if $ \\nu > 2 $ } \\vspace{1ex}\\\\ \\big(e^{d_{\\nu}({\\mathbf{x}}^{\\star},{\\mathbf{x}}^k ) } - 1 \\big ) \\vert { \\mathbf{z}}^k - { \\mathbf{x}}^k \\vert _ { { \\mathbf{x}}^{\\star } } & \\text{if $ \\nu = 2$}. \\end{cases}\\ ] ] combining this inequality , , and , we obtain @xmath490 where @xmath491 is defined as @xmath492 after a few simple calculations , one can show that there exists a constant @xmath493 such that if @xmath494 , then @xmath495 . using this bound , , , and the fact that @xmath496 , we can bound @xmath497\\vert { \\mathbf{x}}^k-{\\mathbf{x}}^{\\star}\\vert_{{\\mathbf{x}}^{\\star}}.\\ ] ] let @xmath498 be the smallest eigenvalue of @xmath499 .",
    "we consider the following cases :    ( a )  if @xmath500 , then , for @xmath501 , we can bound @xmath502 as @xmath503 on the other hand , we have @xmath504 .",
    "using these estimates into , we get @xmath505 let @xmath506 .",
    "the last estimate shows that if @xmath507 , then @xmath508 quadratically converges to zero .",
    "( b )  if @xmath7 , we can bound @xmath502 as @xmath509 since @xmath510 , this estimate leads to @xmath511 , provided that @xmath512 . substituting this estimate into we get @xmath513 similarly ,",
    "if we define @xmath514 , then if @xmath515 , the last estimate shows that @xmath508 quadratically converges to zero .",
    "( c )  if @xmath516 , then we first show that @xmath517 hence , if @xmath518 , where @xmath519 , then @xmath520 .",
    "next , using the definition of @xmath521 in , we can bound it as @xmath522^{\\nu-2}\\frac{\\vert { \\mathbf{z}}^k - { \\mathbf{x}}^k\\vert_{{\\mathbf{x}}^{\\star}}^{3-\\nu}}{(\\underline{\\sigma}^{\\star})^{\\frac{3-\\nu}{2 } } } \\vspace{1ex}\\\\ \\nonumber & \\leq   \\frac{m_f}{(1-\\bar{d}_{\\nu})(\\underline{\\sigma}^{\\star})^{\\frac{3-\\nu}{2}}}\\left(\\frac{\\nu}{2}-1\\right)\\vert { \\mathbf{z}}^k - { \\mathbf{x}}^k\\vert_{{\\mathbf{x}}^{\\star } }   \\overset{\\tiny\\eqref{eq : bound_on_zk}}{\\leq }   \\frac{m_f(\\nu-2)}{2(1-\\bar{d}_{\\nu})(\\underline{\\sigma}^{\\star})^{\\frac{3-\\nu}{2}}}c_{\\nu}\\bar{d}_{\\nu}\\vert { \\mathbf{x}}^k - { \\mathbf{x}}^{\\star } \\vert_{{\\mathbf{x}}^{\\star}}. \\end{array}\\ ] ] using this estimate , we can bound @xmath502 as follows : @xmath523 where @xmath524 . substituting this estimate into and noting that @xmath525",
    ", we get @xmath526 hence , if @xmath527 , then the last estimate shows that the sequence @xmath508 quadratically converges to zero .      [ [ apdx : th : comp_full_pnt_scheme ] ] the proof of theorem  [ th : comp_full_pnt_scheme ] : local quadratic convergence of the proximal newton method ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~    since @xmath474 is the optimal solution to , which satisfies , we have @xmath531 . using this optimality condition and @xmath532 , we get @xmath533 let us define @xmath534 .",
    "then , by lemma and the triangular inequality , we have @xmath535 let us first bound the term @xmath536 as follows : @xmath537 where @xmath231 is defined as .",
    "indeed , from the mean - value theorem , we have @xmath538{\\mathbf{p}}^k{\\mathrm{d}}t\\right\\vert}_{{\\mathbf{x}}^k } \\leq { \\left\\vert{\\mathbf{h}}({\\mathbf{x}}^k,{\\mathbf{x}}^{k+1})\\right\\vert}\\lambda_k,\\ ] ] where @xmath468 is defined as . combining the above inequality and in lemma [ le : h_norm ]",
    ", we get .",
    "next we bound the term @xmath539 as follows : @xmath540\\tilde{\\lambda}_{k+1 } , ~~~&\\text{if $ \\nu > 2$}\\\\ ( e^{d_{\\nu}^k}-1)\\tilde{\\lambda}_{k+1 } & \\text{if $ \\nu = 2$}. \\end{cases}\\ ] ] we note that @xmath541({\\mathbf{z}}^{k+1}-{\\mathbf{x}}^{k+1})\\right\\vert}_{{\\mathbf{x}}^k}^ { * } \\leq \\vert\\widetilde{{\\mathbf{h}}}({\\mathbf{x}}^k,{\\mathbf{x}}^{k+1})\\vert\\tilde{\\lambda}_{k+1},\\ ] ] where @xmath542 by proposition [ pro : hessian_bounds ] , we have @xmath543 this inequality can be simplified as @xmath544 hence , the inequality holds .",
    "now , we combine , and , if @xmath55 , and assuming that @xmath545 , then we get @xmath546 by proposition [ pro : hessian_bounds ] , we have @xmath547 . combining this estimate and the last inequality ,",
    "we get @xmath548 we note that @xmath549 and @xmath409 . based on we have @xmath550 by a numerical calculation",
    ", we can check that if @xmath551 , then @xmath412 hence , if we choose @xmath552 such that @xmath553 , then we can prove the following two inequalities together by induction : @xmath554 these inequalities show the nonincreasing monotonicity of @xmath555 and @xmath438 .",
    "the above inequality also shows the local quadratic convergence of the sequence @xmath416 .",
    "now , if @xmath242 and assume that @xmath556 , then @xmath557 by proposition [ pro : hessian_bounds ] , we have @xmath558 . hence , combining these inequalities , we get @xmath559 we note that @xmath560 , @xmath420 and @xmath421 . based on these relations and , we consider two cases :    if @xmath7 , then @xmath561 , and @xmath562 by a simple numerical calculation , we can show that if @xmath563 , then @xmath435 .",
    "hence , if @xmath564 , then we can prove the following two inequalities together by induction @xmath565 these inequalities show the non - increasing monotonicity of @xmath555 and @xmath438 . the above inequality also shows the quadratic convergence of the sequence @xmath438 .    if @xmath283 , then @xmath566 , which implies that @xmath423 .",
    "hence , we have @xmath567 if @xmath425 , then @xmath428 , where @xmath426 is the unique solution to the equation @xmath568 therefore ,",
    "if @xmath569 , then we can prove the following two inequalities together by induction : @xmath570 these inequalities show the non - increasing monotonicity of @xmath555 and @xmath438 .",
    "the above inequality also shows the quadratic convergence of the sequence @xmath392 .",
    "finally , to prove the local quadratic convergence of @xmath571 to @xmath153 , we use the same argument as in the proof of theorem  [ th : full_step_nt_scheme_converg ] and theorem  [ th : comp_decr ] , where we omit the detail here .",
    "@xmath226      the full - step quasi - newton method for solving can be written as @xmath572 .",
    "this is equivalent to @xmath573 . using this relation and @xmath364 ,",
    "we can write @xmath574.\\ ] ] we first consider @xmath575\\vert_{{\\mathbf{x}^{\\star}}_f}$ ] .",
    "similar to the proof of theorem  [ th : full_step_nt_scheme_converg ] , we can show that @xmath576({\\mathbf{x}}^k \\!-\\ !",
    "{ \\mathbf{x}^{\\star}}_f)\\big\\vert_{{\\mathbf{x}^{\\star}}_f } \\!\\leq\\ ! r_{\\nu } ( d_{\\nu}^k ) d_{\\nu}^k\\vert{\\mathbf{x}}^k \\!-\\ ! { \\mathbf{x}^{\\star}}_f\\vert_{{\\mathbf{x}^{\\star}}_f}\\ ] ] where @xmath272 is defined by and @xmath577 .",
    "moreover , we note that @xmath578 combining this estimate , , and , we can derive @xmath579 first , we prove statement ( a ) . indeed , from the dennis - mor condition , we have @xmath580 where @xmath581 . substituting this estimate into , and noting that @xmath582 , where @xmath583 , we can show that @xmath584 provided that @xmath585 and @xmath586 . here",
    ", @xmath587 is a given value such that @xmath588 is finite .",
    "the estimate shows that if @xmath589 is sufficiently small , @xmath590 super - linearly converges to zero .",
    "finally , the statement ( b ) is proved similarly by combining statement ( a ) and ( * ? ? ?",
    "* theorem 9 ) . @xmath226                                              m.  grant , s.  boyd , and y.  ye . disciplined convex programming . in l.",
    "liberti and n.  maculan , editors , _ global optimization : from theory to implementation _ , nonconvex optimization and its applications , pages 155210 .",
    "springer , 2006 .",
    "a.  kyrillidis , r.  karimi , q.  tran - dinh , and v.  cevher .",
    "scalable sparse covariance estimation via self - concordance . in _ proc .",
    "of the 28th aaai conference on artificial intelligence _ , pages 19461952 , 2014 .",
    "y.  nesterov .",
    "cubic regularization of newton s method for convex problems with constraints .",
    "core discussion paper 2006/39 , catholic university of louvain ( ucl ) - center for operations research and econometrics ( core ) , 2006 .",
    "g.  odor , y .- h .",
    "li , a.  yurtsever , y .- p .",
    "hsieh , q.  tran - dinh , m.  el - halabi , and v.  cevher .",
    "rank - wolfe works for non - lipschitz continuous gradient objectives : scalable poisson phase retrieval . in _ 2016 ieee international conference on acoustics , speech and signal processing ( icassp ) _ , pages 62306234 .",
    "ieee , 2016 .",
    "q.  tran - dinh , y .- h .",
    "li , and v.  cevher .",
    "composite convex minimization involving self - concordant - like cost functions . in t.",
    "pham  dinh h.  a. le - thi and n.  t. nguyen , editors , _ modelling , computation and optimization in information systems and management sciences _ , pages 155168 .",
    "springer - verlag , 2015 .",
    "q.  tran - dinh , i.  necoara , and m.  diehl .",
    "a dual decomposition algorithm for separable nonconvex optimization using the penalty function framework . in _ proceedings of the conference on decision and control ( cdc ) _ , pages 23722377 , florence , italy , december 2013 ."
  ],
  "abstract_text": [
    "<S> we study the smooth structure of convex functions by generalizing a powerful concept so - called _ self - concordance </S>",
    "<S> _ introduced by nesterov and nemirovskii in the early 1990s to a broader class of convex functions , which we call _ generalized self - concordant functions_. this notion allows us to develop a unified framework for designing newton - type methods to solve convex optimization problems . </S>",
    "<S> the proposed theory provides a mathematical tool to analyze both local and global convergence of newton - type methods without imposing unverifiable assumptions as long as the underlying functionals fall into our generalized self - concordant function class . </S>",
    "<S> first , we introduce the class of generalized self - concordant functions , which covers standard self - concordant functions as a special case . </S>",
    "<S> next , we establish several properties and key estimates of this function class , which can be used to design numerical methods . </S>",
    "<S> then , we apply this theory to develop several newton - type methods for solving a class of smooth convex optimization problems involving the generalized self - concordant functions . </S>",
    "<S> we provide an explicit step - size for the damped - step newton - type scheme which can guarantee a global convergence without performing any globalization strategy . </S>",
    "<S> we also prove a local quadratic convergence of this method and its full - step variant without requiring the lipschitz continuity of the objective hessian . </S>",
    "<S> then , we extend our result to develop proximal newton - type methods for a class of composite convex minimization problems involving generalized self - concordant functions . </S>",
    "<S> we also achieve both global and local convergence without additional assumption . </S>",
    "<S> finally , we verify our theoretical results via several numerical examples , and compare them with existing methods . </S>"
  ]
}