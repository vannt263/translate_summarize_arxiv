{
  "article_text": [
    "clustering is the art of partitioning data into distinct and scientifically relevant classes by assigning labels to observations .",
    "clustering is typically performed in an unsupervised setting , where none of the observed data initially have labels . in semi - supervised learning ,",
    "some of the data have labels , but others do not ; the goal is to classify the unlabeled data by assigning them to the same classes as the labeled data . in the gap between these two settings ,",
    "there is semi - supervised clustering , in which some of the data have labels , and there may not be labeled data from every class .",
    "furthermore , the total number of classes may be unknown .",
    "just as in semi - supervised learning , leveraging the known labels allows for a more informed clustering .",
    "there are many strategies for solving clustering problems .",
    "some clustering procedures are based on heuristics that lack a principled justification , and many of the basic questions of clustering ( e.g. , how many classes there are ) are often left to the intuition of the practitioner .",
    "fraley and raftery @xcite review model - based clustering , which recasts the task of clustering as a model selection problem , which is well - studied in the field of statistics .",
    "the main assumption in model - based clustering is that the data are drawn from one of @xmath0 distributions , where each distribution represents a cluster .",
    "model selection can be accomplished by computing approximate bayes factors for competing models with different numbers and/or types of components .",
    "much of the recent work in model - based clustering has centered on variable selection .",
    "raftery and dean @xcite proposed a greedy algorithm for model - based clustering in the unsupervised setting ; it used the bic to choose the number of clusters and the features to consider while clustering by proposing linear relationships between variables that influence clustering and those independent of clustering .",
    "@xcite adapted raftery and dean s algorithm to semi - supervised learning .",
    "_ @xcite consider many more scenarios of dependency structures between the clustering variables and the remaining variables than in @xcite . taking a different approach , witten _ et al . _",
    "@xcite offer a framework for variable selection in clustering using sparse k - means or sparse hierarchical algorithms by modifying the corresponding objective function to include penalty constraints . in their simulations",
    ", they outperform the methods of @xcite considerably .",
    "the bic is generally denoted as @xmath1 where @xmath2 is the maximum of the likelihood , @xmath3 is the number of parameters estimated , and @xmath4 is the number of data used to estimate those parameters .",
    "alternative information criteria generally differ by having different values for the penalty term .",
    "for example , the sample size adjusted bic @xcite is defined as @xmath5    because @xmath6 one may wonder of the efficacy of such a deviation from the bic .",
    "this question is particularly salient when considering that the standard derivation of the bic as the approximation to the integrated loglikelihood is only @xmath7 accurate asymptotically ( so that o(1 ) terms are negligible ) . in this paper",
    ", we will present such a modified bic that is asymptotically equivalent to the standard bic and thoroughly explore an analytical example of how such a modification could be useful .    in this paper , we quickly review model based clustering in section [ review ] .",
    "we state our derivation of a modified bic to the semi - supervised case in section [ methods ] that is asymptotically equivalent to the standard bic .",
    "we follow this with a detailed discussion of @xmath8 equivalent information criteria , analytical calculations for a toy example , and simulations for a semi - supervised clustering example .",
    "next , we apply our methods to the fly larvae behavioral dataset in section [ fly ] . finally , we conclude the paper with a discussion of the results and extensions to our method .",
    "assume that the data @xmath9 are distributed i.i.d . according to a mixture model",
    ", @xmath10 where @xmath11 are the parameters of the @xmath12 component of the mixture and @xmath0 is the total number of components .",
    "it can be shown that this is equivalent to the following generative process : for each @xmath13    1 .",
    "draw @xmath14 from @xmath15 2 .",
    "draw @xmath16 from @xmath17 .    if we consider each of the @xmath0 components as representing a single cluster , then the problem of clustering the data is that of unveiling each latent @xmath14 from the generative process .",
    "the expectation - maximization ( em ) algorithm can be used to find the maximum likelihood estimates for the parameters of the model and the posterior probabilities of cluster membership for each @xmath16 ( cf .",
    "@xcite ) .    with this formulation",
    ", we have a fully probabilistic clustering scheme .",
    "we can set the cluster of the @xmath18 observation to the maximum a posteriori estimate : @xmath19 note that the posterior probability can give us a measure of confidence about the @xmath18 classification .    here ,",
    "we have used @xmath20 as a general density function , but it should be noted that the multivariate gaussian distribution is the most common choice of distribution .",
    "we will eventually use an information criterion , such as the bayesian information criterion ( bic ) , to assess the relative quality of different clusterings .",
    "then , the number of clusters can be chosen using the bic , which rewards model fit and penalizes model complexity .    by considering different constraints to the covariance matrices in the gaussian components",
    ", we can reduce the number of parameters estimated , thus lowering the influence of the penalty term in the bic .",
    "this allows for simpler clusters to be chosen over complex clusters .",
    "celeux and govaert @xcite consider a spectral decomposition of the @xmath12 component s covariance matrix , @xmath21 where @xmath22 represents the largest eigenvalue , @xmath23 is a diagonal matrix whose largest entry is @xmath24 , and @xmath25 is a matrix of the corresponding eigenvectors .",
    "the interpretation of each term as it relates to cluster @xmath26 is as follows : @xmath22 represents the volume , @xmath25 the orientation , and @xmath23 the shape . by forcing one or more of these terms to be the same across all clusters",
    ", we can reduce the number of parameters to be estimated from @xmath27 in the unconstrained case ( @xmath28 ) to @xmath29 in the most constrained case ( @xmath30 ) .",
    "we can also force additional constraints , such @xmath31 and/or @xmath32 , leading to the simplest model : @xmath33 with only @xmath34 parameters to estimate .",
    "[ sec : ssmodel ] the main theoretical contribution of this paper is to derive an adjustment to the bic for the semi - supervised clustering setting followed by an exploration about @xmath8 equivalent information criteria .",
    "we will set forth a formalization of a more general setting , derive the bic in this broader setting , and then apply our result to the special case of semi - supervised clustering .",
    "the modified bic then represents a principled measure by which to choose the number of clusters and the variables for clustering when performing semi - supervised clustering .",
    "consider @xmath35 independent random variables @xmath36 where @xmath37 and @xmath38 for @xmath39 and @xmath40 collect all of the @xmath41 into set @xmath42 .",
    "model @xmath43 , while more general than strictly necessary , encompasses the semi - supervised case .",
    "consider the first group of @xmath44 random variables as those for which we do not have labels and each of the other @xmath45 groups as those whose labels we know .",
    "then , for a proposed total number of clusters @xmath46 , we assume @xmath47 where @xmath48 is a multivariate normal pdf with mean @xmath49 and covariance matrix @xmath50 , and @xmath51 . also , for each @xmath52 , @xmath53 where the double subscript @xmath54 is to account for possible relabeling .",
    "it follows that @xmath55 where @xmath56\\ } \\times \\mathbb{r}^{d \\times g } \\times \\{(\\sigma_1,\\sigma_2 , \\dots , \\sigma_g ) | \\sigma_i \\succeq 0 , \\sigma_i \\in \\mathbb{r}^{d \\times d}\\}.\\end{aligned}\\ ] ]    then , for each @xmath52 , @xmath57 is the same as @xmath58 except that the mixing coefficients @xmath59 are constrained such that @xmath60 and all other @xmath61 .    with this model , we can derive the expectation step ( e - step ) and maximization ( m - step ) of the em algorithm . note that semi - supervised em is not new ; indeed , similar calculations can be found in @xcite section 2.19 and @xcite .    here , we generalize the bic to the semi - supervised case with the aim of not overly penalizing for supervised data .",
    "the modified bic approximation to the integrated likelihood of model @xmath43 is @xmath62 where @xmath63 is the maximum likelihood estimate ( mle ) .",
    "we provide the derivation in the appendix .",
    "compare the above result to the classical bic approximation : @xmath64 the derivation of the bic ( adjusted and normal ) involves an o(1 ) term , which is dropped at the end . due to this , the difference between the two bics is a smaller penalty in the semi - supervised case ( i.e. @xmath65 vs. @xmath66 , which is a @xmath8 difference ) .",
    "this can be interpreted as not penalizing more complicated cluster structures for the supervised data .",
    "we will discuss the merit of such a deviation in section [ sec : o1equiv ] .",
    "it is known that finite mixture models do not meet the regularity conditions for the bic approximation used to approximate the integrated likelihood",
    ". however , @xcite comment that it is still an appropriate and effective approximation in the paradigm of model - based clustering .",
    "thus , we will not be too remiss in considering theorem [ bic ] as applicable when performing semi - supervised clustering . because the model @xmath43 is appropriate for the semi - supervised case",
    ", we can use the modified bic as a way to calculate bayes factors for different number of total clusters @xmath0 , subsets of variables to be included in the clustering , and parameterizations of @xmath67 .",
    "the largest bic value will therefore correspond to our chosen model for clustering .",
    "we will call the semi - supervised clustering algorithm using the bic from theorem [ bic ] to make the aforementioned selections _ ssclust _ henceforth .",
    "here , we comment on the use of different penalty terms",
    ". suppose that we define an adjusted bic , say @xmath68 as a function of @xmath69 as @xmath70    when does using such an adjustment matter ?",
    "consider two models , say @xmath71 and @xmath72 , in competition for selection .",
    "fix an @xmath73 .",
    "suppose that    1 .",
    "@xmath74 2 .",
    "@xmath75 and 3 .",
    "@xmath76    in this case , we see that under the original definition of the bic , we would choose @xmath72 over @xmath71 , and with the modified definition , we would do the opposite .    in order to analyze this under some assumptions ,",
    "define two statistical tests : @xmath77 and @xmath78    when do @xmath79 and @xmath80 differ ?    1 .",
    "@xmath81 and @xmath82 for @xmath83 this is impossible .",
    "2 .   @xmath84 and @xmath85 we have two subcases . 1 .",
    "suppose @xmath72 is the true model",
    ". we would be correct in choosing @xmath86 over @xmath79 .",
    "2 .   suppose @xmath71 is the true model .",
    "note that in this case , we would make a mistake by choosing @xmath86 over @xmath79 .",
    "we would like to analyze the probabilities in case 2 .",
    "preferably , ( 2.a ) is much more probable than ( 2.b ) .",
    "unfortunately , under ( 2.a ) the distribution of @xmath87 is only known for certain cases .",
    "for example , with local alternatives of the form @xmath88 @xmath89 is known to have noncentral chi square distribution with @xmath90 and the appropriate non - centrality parameter .",
    "since this result requires rather quixotic assumptions to hold , we will not theoretically bound the probabilities for more general cases ; instead , we will defer our analysis to a specific example .",
    "we can say more about ( 2.b ) with some assumptions .",
    "suppose the models are nested ( so that @xmath71 is a submodel of @xmath72 ) .",
    "define @xmath91 under @xmath71 , @xmath84 and @xmath92 with probability @xmath93 where @xmath94 is the distribution function of an @xmath95 distribution with @xmath96    we can do some algebra on @xmath97 and @xmath98 to obtain equivalent condition that @xmath99    if the models are nested ( so that @xmath71 is a submodel of @xmath72 ) , then the numerator of @xmath87 converges in law to a @xmath100 distribution with @xmath101 by wilk s theorem . in this case",
    ", we note that by slutsky s theorem , @xmath87 asymptotically has the distribution of an f - statistic with @xmath96    hence , under the nested models assumption , the probability that @xmath97 and @xmath98 hold when they should * not * ( i.e. choose @xmath72 when @xmath71 is the truth ) is given by equation ( [ eqn : badprob ] ) .",
    "we now present an analysis for specific null and alternative models where we can explicitly calculate the probabilities of cases ( 2.a ) and ( 2.b ) .",
    "the purpose is to demonstrate that for finite @xmath4 , the choice of penalty term is nontrivial .",
    "let @xmath102 with @xmath103 .",
    "consider the nested models :    * @xmath104 * @xmath105    where @xmath106 is the @xmath107dimensional identity matrix .",
    "let @xmath108^t \\\\",
    "\\mu^*_{0 , d_0 } = \\left [ \\begin{matrix }   1 & \\frac{1}{\\sqrt{2 } } & \\frac{1}{\\sqrt{3 } } & \\dots & \\frac{1}{\\sqrt{d_0 } } & 0 & 0 & \\dots & 0\\end{matrix}\\right]^t.\\end{gathered}\\ ] ]    then , @xmath109 and @xmath110 suppose that @xmath111 according to whether or not @xmath72 is the true model .",
    "fix a realization of the data @xmath112 let @xmath113 and @xmath114 be @xmath115 with the coordinates after @xmath116 set to 0 .",
    "twice the maximized loglikelihood of the data under @xmath72 is ( up to constants ) @xmath117 under @xmath118 it is @xmath119 since @xmath120 .",
    "thus we find that @xmath121    1 .   under @xmath122 , @xmath123 where @xmath124 noncentral @xmath125 hence , for any @xmath126 @xmath127 therefore , @xmath128 2 .   under @xmath129 @xmath130 where @xmath131",
    "thus , @xmath132 hence , @xmath133    clearly , we can vary @xmath134 and @xmath116 to influence these probabilities",
    ". we will now do so to show how ( a ) some penalty is better than none ; ( b ) the aic penalty can result in mistakes ; and ( c ) the optimal penalty depends on @xmath134 and @xmath116 .",
    "figure [ fig : probsnvary ] depicts the probability of ( 2.a ) and ( 2.b ) when @xmath4 and @xmath73 vary for fixed @xmath135 and @xmath136 under the alternative ( a ) and null ( b ) hypotheses .",
    "lower penalties than the bic would use generally result in better decisions under @xmath122 .",
    "however , there is approximately a @xmath137 chance of error under @xmath138 when the bic would be correct with the penalty of @xmath139 that of the aic .",
    "and @xmath140 note that @xmath141 the aic penalty . ]",
    "figure [ fig : probsd1vary ] depicts the probability of ( 2.a ) and ( 2.b ) when @xmath3 and @xmath73 vary for fixed @xmath142 and @xmath143 under the alternative ( a ) and null ( b ) hypotheses .",
    "lower penalties than the bic would use generally result in better decisions under @xmath122 without sacrificing making too many new mistakes under @xmath138 .",
    "and @xmath144    figure [ fig : probsgapvary ] depicts the probability of ( 2.a ) and ( 2.b ) when @xmath116 and @xmath73 vary for fixed @xmath142 and @xmath145 under the alternative ( a ) and null ( b ) hypotheses .",
    "lower penalties than the bic would use generally result in better decisions under @xmath122 and not too much worse decisions under @xmath138 .",
    "however , there is approximately a larger chance of error under @xmath138 when the bic would be correct with the aic penalty for smaller @xmath116 .     and",
    "@xmath146    this example may seem overly simplistic .",
    "indeed , it does not involve semi - supervised clustering at all .",
    "however , it does demonstrate the complexities of penalization in model selection with finite samples .",
    "we would like to demonstrate the impact of the previously discussed model selection complexities from the illustrative example on the inference task of semi - supervised clustering .",
    "to that end , consider the following procedure for constructing a dataset @xmath147 , semi - supervising it , and clustering it using model - based clustering with different penalties .",
    "we will then compare the resulting ari scores .",
    "let @xmath148 also , let @xmath149 and @xmath150 consider the following procedure for generating one monte carlo replicate .    1 .",
    "define the pdf of each datum as being from a mixture model @xmath151 where @xmath152 with @xmath153 generated using the onion method of @xcite 2 .",
    "draw @xmath154 supervised from the first two components .",
    "draw @xmath155 unsupervised from the full mixture model .",
    "4 .   cluster using @xmath156 gaussians with various constraints on the proposed covariance matrices . 5 .",
    "choose the models for clustering based on different values of @xmath73 in the penalty term @xmath157 where @xmath158.$ ] 6 .",
    "calculate resulting aris using the chosen models ..    figures [ fig : bicpenalty ] and [ fig : bicfinalpenalty ] shows how different penalties can affect the overall clustering performance on the unlabeled data .",
    "generally , we see that with more unsupervised data , there are less differences between the different choices of penalty terms in the interval @xmath159,$ ] which makes sense given the asymptotic results . in this particular example , less penalization allowed us to detect the third component sooner , improving the ari values in the resulting clustering .",
    "thus , we have shown that in an example closer to our problem of interest , the restricted penalization derived in the previous section can be efficacious as compared to the standard penalty . additionally , as @xmath4 grows , the differences between the information criteria that are dependent on @xmath4 to the same order becomes negligible even for relatively small values of @xmath4 .     for largest value of the total dataset simulated .",
    "higher is better .",
    "red circles represent significant paired wilcoxon tests for larger ari values than vs the standard bic ( at the .05 level ) .",
    "the area to the right of the green line represents the interval @xmath160.$ ] ]",
    "in one of the motivating applications for this work , classes of neurons in _ drosopholia _",
    "larvae are controlled using optogenetics ( cf .",
    "@xcite regarding optogenetics ) . in @xcite",
    ", they observe the reactions of the affected larvae to stimuli in high - throughput behavioral assays . the goal is to determine which classes of neurons cause similar changes in behavior when deactivated .",
    "we initially collected data on @xmath161 larvae grouped into @xmath162 dishes . by changing the optogenetic procedure ,",
    "@xmath163 known lines are created , pbd1 , 38a1 , 61d0 , ppk1 , 11f0 , pbd2 , 38a2 , pbd3 , ppk2 , iav1 , and 20c0 .",
    "of these lines , we discarded the larvae in pbd3 and ppk2 because they had less than 40 larvae each .",
    "further , we discarded all larvae with an unknown line .",
    "after curating the data , we now have @xmath164 larvae . each larva is observed while responding to various stimuli .",
    "_ citevogelstein2014discovery expand on the methods of @xcite and @xcite and describe how the observations are embedded into @xmath165 , where here @xmath166 .",
    "we use the method presented in @xcite to select the elbow of the scree plot to further reduce the data to @xmath167 dimensions .",
    "we did not perform any additional feature selection .    for each monte carlo replicate",
    ", we use a small subset of the data where the line was known ( 101 randomly chosen animals from each of the 9 remaining lines ) along with @xmath168 pre - labeled data randomly chosen from the 101 animals in each line .",
    "we wrote an r package entitled ssclust to perform semi - supervised gmm with similar options to the popular mclust software , which is an r package for gmm @xcite .",
    "we cluster the points using both ssclust and mclust .",
    "then , we compute the ari against the line type for both methods .    we observe that the initialization strategy of using ss - k - means++ instead of hierarchical clustering results in a significant improvement to the ari even with no supervision .",
    "we find that a single animal per line significantly improves the clustering results , as expected ( cf .",
    "figure [ fig : martaexper ] ) .",
    "further , we can see that there are diminishing returns on additional supervision starting at @xmath169 supervised examples per line .",
    "vogelstein _ et al .",
    "_ @xcite posed and answered questions of the form , `` is line x different than line y ( in terms of behaviotypes ) ? '' as an illustrative example , we will perform a similar analysis comparing the ppk1 and pbd1 lines but will additionally incorporate some supervision . here",
    ", our interpretation of the clusters will shift from lines to behaviotypes .",
    "our proposed procedure for distinguishing between lines is as follows :    1 .",
    "sample @xmath170 animals from each line 2 .",
    "label @xmath169 animals from each line according to a labeling strategy ( see below ) 3 .",
    "cluster the animals using ssclust and mclust into between 2 and 12 clusters using the parameterizations eee , vvv , vii , and eii .",
    "4 .   collect the results and construct empirical probability of cluster membership for each line 5 .",
    "compute the hellinger distance between the two lines to be compared and store this as statistic @xmath171 6 .",
    "simulate the distribution of @xmath171 under the null hypothesis that the lines are the same by permuting the labels and computing the hellinger distance @xmath172 for @xmath173 for some large integer @xmath174 .",
    "return an empirical p - value based on steps ( 5 ) and ( 6 ) .    in item",
    "( 2 ) we did not specify a labeling strategy in detail .",
    "we propose a strategy that is reasonably realistic to execute for our particular dataset .",
    "@xcite used a hierarchical clustering scheme in which the first few layers were visually identifiable by watching the worms .",
    "thus , by using their labels from an early layer ( layer 2 , with 4 clusters total ) , we have a plausible level of supervision for a human to have performed .",
    "specifically , we sample at random a label from the true labels among a line with weights proportional to the counts of each label in that line . using that label , we sample 3 worms of that label in that line to be the supervised examples .",
    "next , for the other line , we sample a different label , and 3 examples with that true label .",
    "we see that based on all three labeling strategies that both ssclust and mclust are able to corroborate the results from @xcite even with small amounts of data : ppk1 and pbd1 are statistically different ( p - value @xmath175 for both for 500 mc replicates ) .",
    "we now show that ssclust can answer these questions  sooner \" than mclust .",
    "that is , the p - value ( @xmath176 ) will be below the significance level with fewer unsupervised examples . to quantify this concept , we introduce the  answering time \" for algorithm a :    @xmath177    where here we use the notation @xmath178 to be the labeled and unlabeled data .",
    "the p - value constraint bears some explanation ; it says that we require a significant p - value for all datasets at least as large as with @xmath4 unsupervised examples per line .",
    "note that here we assume our datasets are nested and that they all use the same supervised examples . since the answering time will be dependent on the datasets used , we perform a monte carlo simulation with 500 random sequences of datasets and report the answering times for both ssclust and mclust ( cf .",
    "figure [ fig : answeringtime ] ) .",
    "the median answering time for ssclust is significantly lower than for mclust ( p - value = 4.7e-12 for paired wilcoxon signed - rank test ) .",
    "previously explored approaches to semi - supervised clustering include a modified k - means , which can be seen as our algorithm constrained to spherical and identical covariance matrices without the adjusted bic for model selection ( cf .",
    "others have used latent variables representing clusters and cluster - dependent distributions to give a probabilistic approach to a slightly different problem , where instead of labels being known , only pairwise constraints in the form of must - link and cannot - link are given ( cf .",
    "@xcite applied an appropriately modified k - means to this problem .",
    "finally , @xcite and @xcite offer a different approach to semi - supervised clustering involves training a measure to account for constraints or labels .",
    "the main contribution of this paper over previous works is presenting a probabilistic framework for semi - supervised clustering and deriving an information criterion consistent with the framework to allow selection of number of clusters and/or clustering variables . with the corrected bic",
    ", we found that in the simulated examples and fly larvae dataset our method outperformed the most comparable method , mclust . in the fly dataset ,",
    "ssclust was able to recover lines better and yielded a lower answering time more often for the question of whether two particular lines were different , behaviorally .",
    "this indicates that incorporating even a small amount of information can help guide the clustering process .",
    "future areas of research will involve handling the more nuanced must - link and cannot - link constraints , which are flexible enough to encompass the labels - known problem we have explored in this paper .",
    "* a derivation of the bic for the semi - supervised model * + assume the data are distributed according to a member of model @xmath43 described in section [ sec : ssmodel ] .",
    "consider the integrated likelihood : @xmath179 where @xmath180 is a probability , pmf , or pdf where appropriate .",
    "let @xmath181 the log of the posterior likelihood .",
    "suppose that the posterior mode exists , say @xmath182 .",
    "a second order taylor expansion about @xmath182 gives @xmath183 by the first order optimality necessary conditions , we know @xmath184 note @xmath185 faster than @xmath186 as @xmath187 . by ignoring the last term , we will approximate @xmath188 with the truncated taylor expansion : @xmath189    recalling ( [ eqn : intll ] ) , we may approximate @xmath190 using a saddle point approximation : @xmath191 recognize the integral as proportional to the density of a multivariate guassian with mean @xmath182 and covariance @xmath192 .",
    "let @xmath193 then , we have @xmath194 where @xmath3 is number of free parameters in @xmath195 .",
    "if the conditions in proposition 3.4.3 in @xcite hold , we have for @xmath4 sufficiently large , @xmath196 where @xmath197 is the fisher information matrix .",
    "now , we must calculate @xmath200 .",
    "define @xmath201 observe @xmath202\\\\ & = & \\text{var}\\left[\\sum_{i=1}^n \\frac{\\partial}{\\partial \\theta } \\log(p(x_i,\\theta))\\right]\\\\ & = & \\sum_{i=1}^c n_i i(\\theta_i ) \\text { by independence.}\\end{aligned}\\ ] ] henceforth , we will use @xmath203 to denote @xmath204 .",
    "we will assume that @xmath205 is positive definite , so that it can be written as @xmath206 for some non - singular matrix @xmath207 .",
    "let @xmath208 denote the @xmath3-dimensional identity matrix . for notational purposes ,",
    "let @xmath209    observe @xmath210 where @xmath211 .",
    "it should be noted that @xmath174 is a positive semi - definite matrix , as it is a @xmath212transform of a sum of positive semi - definite matrices , so that sylvester s theorem states that @xmath174 has the same inertia as a positive semi - definite matrix .",
    "next , we would like to describe the growth of @xmath213 .",
    "for any eigenvalue of @xmath214 , say @xmath215 , we have by weyl s theorem @xmath216 observe @xmath217 which is independent of @xmath4 .",
    "let @xmath218 then , we have @xmath219    the only term growing with @xmath4 above is @xmath220 the ratio of the supervised data over the unsupervised data . in general , it is usually much more expensive to obtain additional supervised data than unsupervised ; thus , we find it reasonable to posit that @xmath221 in @xmath4 ( i.e. is @xmath8 ) . in this case",
    ", @xmath222 is @xmath8 by continuity and our bounds .",
    "when the posterior mode is nearly or is equal to the mle , as is the case when the prior on @xmath195 is uniform and @xmath224 is finite ( cf .",
    "bickel and doksum pp .",
    "114 ) , substitute @xmath63 , the mle , for @xmath182 , the posterior mode .",
    "then , we have @xmath225 note that the terms of order less than @xmath7 get washed out in the limit as @xmath226 . if we drop them , we have our derivation of the adjusted bic .          sugato basu , mikhail bilenko , and raymond  j mooney . a probabilistic framework for semi - supervised clustering . in _ proceedings of the tenth acm sigkdd international conference on knowledge discovery and data mining _ , pages 5968 .",
    "acm , 2004 .",
    "arthur  p dempster , nan  m laird , and donald  b rubin .",
    "maximum likelihood from incomplete data via the em algorithm .",
    "_ journal of the royal statistical society .",
    "series b ( methodological ) _ , pages 138 , 1977 .",
    "chris fraley , adrian  e. raftery , t.  brendan murphy , and luca scrucca .",
    "mclust version 4 for r : normal mixture modeling for model - based clustering , classification , and density estimation .",
    "technical report 597 , university of washington , department of statistics , june 2012 .",
    "cathy maugis , gilles celeux , and m - l martin - magniette .",
    "variable selection in model - based clustering : a general variable role modeling .",
    "_ computational statistics & data analysis _ , 530 ( 11):0 38723882 , 2009 .",
    "thomas  brendan murphy , nema dean , and adrian  e raftery .",
    "variable selection and updating in model - based discriminant analysis for high dimensional data with food authenticity applications . _ the annals of applied statistics _ , 40 ( 1):0 396 , 2010 .",
    "noam shental , aharon bar - hillel , tomer hertz , and daphna weinshall . computing gaussian mixture models with em using side - information . in _ proc . of workshop on the continuum from labeled to unlabeled data in machine learning and data mining _ , 2003 .",
    "joshua  t vogelstein , youngser park , tomoko ohyama , rex  a kerr , james  w truman , carey  e priebe , and marta zlatic . discovery of brainwide neural - behavioral maps via multiscale unsupervised structure learning .",
    "_ science _ ,",
    "3440 ( 6182):0 386392 , 2014 .",
    "kiri wagstaff , claire cardie , seth rogers , stefan schrdl , et  al .",
    "constrained k - means clustering with background knowledge . in _ proceedings of the eighteenth international conference on machine learning _ , volume  1 , pages 577584 , 2001 .",
    "eric  p xing , michael  i jordan , stuart russell , and andrew  y ng .",
    "distance metric learning with application to clustering with side - information . in _ advances in neural information processing systems _ , pages 505512 , 2002 ."
  ],
  "abstract_text": [
    "<S> we consider an extension of model - based clustering to the semi - supervised case , where some of the data are pre - labeled . </S>",
    "<S> we provide a derivation of the bayesian information criterion ( bic ) approximation to the bayes factor in this setting . </S>",
    "<S> we then use the bic to the select number of clusters and the variables useful for clustering . </S>",
    "<S> we demonstrate the efficacy of this adaptation of the model - based clustering paradigm through two simulation examples and a fly larvae behavioral dataset in which lines of neurons are clustered into behavioral groups .    </S>",
    "<S> # 1    0    0    1    0    * a model - based semi - supervised clustering methodology *    _ keywords : _ bic , machine learning , behavioral data , model selection , gmm , mclust </S>"
  ]
}