{
  "article_text": [
    "for concreteness in this introduction , consider a classic linear regression analysis , based on a data matrix @xmath0 of @xmath1 rows and @xmath2 columns , with the first @xmath3 columns containing the values of the predictor variables and the last column consisting of values of the response variable .. some of the elements of the matrix may be missing , a condition that is in the r language denoted by na .",
    "a wide variety of methods have been developed to deal with the missing values .",
    "the most popular fall into one of two categories , again described in our regression analysis context for convenience :    * * complete cases ( cc ) : * here one deletes any row in the data matrix that has at least one na value . *",
    "* multiple imputation ( mi ) : * these methods involve estimating the conditional distribution of a variable from the others , and then sampling from that distribution via simulation . multiple alternate versions of the data matrix are generated , with the na values replaced by values that might have been the missing one .",
    "here we are interested in a third approach :    * * available cases ( ac ) : * if the statistical method involves computation involving , say , various pairs of varaibles , include in such a calculation any observation for which this pair is intact , regardless of whether the other variables are intact .",
    "the same holds for triples of variables and so on .    for example , as will be detailed below , linear regression analysis only involves computation of certain pairwise - intact values of the form    @xmath4    thus we may compute ( [ dirs ] ) for all rows @xmath5 for which both @xmath6 and @xmath7 are intact  even if some other @xmath8 are missing . in ( [ dirs ] )",
    "the factor @xmath9 would be changed to @xmath10 , where @xmath11 is the number of rows with intact @xmath12 pairs in the matrix , as in for example ( cohen and cohen , 1983 ) . in other words , ( [ dirs ] ) becomes    @xmath13    where @xmath14 is 1 or 0 , depending on whether @xmath15 and @xmath16 are intact .",
    "( as noted , there are important assumptions underlying these methods , but we defer discussion on this to section [ assume ] . )    though ac was considered in the early literature on missing data , over the years , mi methods became more and more sophisticated , and they enjoy high popularity today . in r , for instance , there are packages * amelia * ( honaker _ et al _ , 2011 ) , * mi * ( su _ et al _ , 2011 ) and * mice * ( van buuren , 2011 ) that apply mi techniques .",
    "see ( little _ et al _ , 2002 ) for very detailed coverage , or _ http://sites.stat.psu.edu/",
    "jls / mifaq.html _ for an overview .",
    "concurrently , interest in ac waned , not only due to its stringent assumptions but also out of a concern that the cross products matrix whose elements are given by ( [ dirs ] ) may not be positive definite .",
    "we believe that ac can be a very useful tool . as marsh notes ( marsh , 1998 ) ,",
    "ac `` follows naturally from a desire to use as much of the data as possible .",
    "'' we will show here that ac can indeed yield significant improvements in statistical accuracy over cc , while avoiding the very slow computational speed of mi .",
    "in addition to investigating the standard ac application of linear regression nodeling , we also investigate principal components analysis ( pca ) and analysis of contingency tables .",
    "we make software available to implement these methods .",
    "we chose * amelia * as our representative mi method , arbitrarily using the criterion that it has the most citations on google scholar . under the assumption that the population distribution of the rows of @xmath17 is multivariate normal ,",
    "an outline of its approach is as follows .",
    "* starting the with original data @xmath18 , @xmath19 perturbations of this data @xmath20 are created , @xmath21 , through bootstrap sampling .",
    "note that these new data sets do contain na values . * for @xmath21 ,",
    "do : * * replace the nas by 0s .",
    "* * use the em algorithm and the assumption of multivariate normality to estimate the population mean vector and covariance matrix from this data . * * replace each na value by an imputed one , consisting of a value drawn at random from the estimated conditional distribution of this variable , given the intact values of the other variables . * combine the @xmath19 data sets , say by averaging the @xmath19 values of a quantity of interest , such as a regression coefficient .    in our initial empirical investigation",
    ", we quickly found that * amelia * was not performing well :    * its statistical accuracy was no better than those of cc and ac .",
    "* it was slow .",
    "for instance , in a pca simulation with @xmath22 and @xmath23 , cc and ac took 0.011 and 1.967 seconds , respectively , while mi took 92.928 seconds .",
    "for this reason , we will present empirical results here only for the cc and ac methods .",
    "it is crucial to keep in mind , though , that cc and ac require more stringent assumptions than mi . thus later in this paper",
    "we will return to mi in general , and * amelia * in particular .",
    "as noted , in the literature , ac has mostly been considered in the context of linear regression .",
    "thus we will begin there .      consider the case of random - x regression .",
    "define the matrix @xmath24 and the vector @xmath25 to be @xmath17 minus the last column , and the last column of @xmath17 , respectively .",
    "then the classic formula for the vector of estimated regression coefficients , assuming intact data , is    @xmath26    which as @xmath27 converges to    @xmath28^{-1 } e(xy)\\ ] ]    where the random column vector @xmath29 and and the random scalar variable @xmath30 have the population distribution from which the rows of @xmath24 and elements of @xmath25 are sampled .",
    "the point is that this convergence still holds if in ( [ uprimeuetc ] ) , we replace the @xmath12 element of @xmath31 in ( [ uprimeuetc ] ) by ( [ newdirs ] ) , @xmath32 and replace element @xmath33 in @xmath34 by ( [ newdirs ] ) with @xmath35 .",
    "r code for use of ac as a replacement for * lm ( ) * is available in two implementations ( not just two locations ) , a function * lmmv ( ) * at _ https://github.com/maxguxiao/available-cases_ , and a function * lmac ( ) * in the * regtools * package at _",
    "https://github.com / matloff / regtools_.    the latter takes advantage of the fact that r s * cov ( )",
    "* function offers an argument option * use = pairwise.complete.obs * , which applies ac to finding covariance matrices , which in turn can be used to estimate regression coefficients : :    .... # arguments :    # x : predictor values ( no 1s column ) # y : response variable values    lmac < - function(x , y ) {     p < - ncol(x )     tmp < - cov(cbind(x , y),use='pairwise.complete.obs ' )     upu < - tmp[1:p,1:p ]     upv < - tmp[1:p , p+1 ]     bhat < - solve(upu , upv )     bhat0 < -         mean(y , na.rm = true ) - colmeans(x , na.rm = true ) % * % bhat     c(bhat0,bhat ) } ....    this works because for centered data , ( [ classformula ] ) is equal to    @xmath36    since the * use = pairwise.complete.obs * option in r s * cov ( ) * uses the ac method , this gives us ac estimation for linear regression .",
    "the above code , * lmac ( ) * , is much faster than * lmmv ( ) * , since r s * cov ( ) * function operates at c - level , as opposed to the use of r * for ( ) * loops in * lmmv()*.      our code computes standard errors for the estimated regression coefficients in two different ways .",
    "* lmmv ( ) : *    the * lmmv ( ) * function uses the delta method , together with numerical calculation of derivatives using the * numderiv * package .",
    "any component of ( [ uprimeuetc ] ) is a function of the @xmath37 in ( [ newdirs ] ) for @xmath38 .",
    "the function * gend ( ) * in * numderiv * is then used to compute the numerical gradient @xmath39 of this function .",
    "the standard error is then    @xmath40    where @xmath41 is the estimated covariance matrix for the @xmath37 ( conditional on the @xmath11 ) .",
    "we have    @xmath42    there are various ways to evaluate this , such as calling * cov ( ) * with the * pairwise.complete.obs * option . * lmac ( ) : *    in * lmac ( ) * , we simply use the bootstrap to generate standard errors .",
    "though this may seem more time - consuming than using the delta method , this consideration is countered by the fact that * gend ( ) * is written in r rather than c , and thus involves slow loop computation .",
    "we present here simulations that run on real or simulated data .",
    "the idea is that , starting with a given data set , in each repetition of the simulation , random na values are inserted , and the value of @xmath43 is recorded .",
    "the variance of such values for * lmac ( ) * is compared to that for * lm ( ) * ; the latter represents cc , as its method of handling nas is cc .",
    "we tried it for several real data sets .",
    "one is the pima study at the uci machine learning repository , _ https://archive.ics.uci.edu / ml / datasets / pima+indians+diabetes_. here @xmath44 and @xmath45 .",
    "we took blood pressure as our response variable , and all the other variables as predictors .",
    "results for inserting 1% , 5% and 10% nas were as shown in table [ pima ] .    0.5 in    .pima data , linear regression [ cols=\">,>,>\",options=\"header \" , ]",
    "as mentioned , there has been concern about ac in two senses : positive definiteness of covariance matrices , and stringency of assumptions . here",
    "we revisit both of these issues .",
    "one of the concerns that have arisen for the ac method in the past was possible lack of positive definiteness of @xmath31 in ( [ uprimeuetc ] ) . yet",
    "marsh found that this is rarely a problem ( marsh , 1998 ) .",
    "furthermore , the problem can occur in * amelia * as well .",
    "this is because the procedure replaces na values in the bootstrapped versions of the original data by 0s .",
    "indeed , the * amelia * code does include checks for this , halting the procedure upon detection of a problem .",
    "the issue of assumptions is more delicate , especially since , as is well recognized , the assumptions involved with cc , ac and mi are difficult to check using the data .",
    "let @xmath30 denote a variable of interest , and let @xmath46 be 1 or 0 , depending on whether @xmath30 is missing . also , let @xmath17 denote the vector of the other varialbes , which for simplicity we assume are never missing .",
    "for the same reason , we also assume the variables are discrete - valued rather than continuous .",
    "cc and ac assume a missing completely at random ( mcar ) setting , which is usually defined as something like    @xmath47    where @xmath48 is in general vector - valued .",
    "thus @xmath46 is independent of @xmath49 . turning this around , we have    @xmath50    for @xmath51 . in other words ,",
    "the distribution of @xmath49 is the same , whether @xmath30 is missing or not , and thus inference made from the cases in which @xmath30 is observed generalize properly to the full distribution of @xmath49 .",
    "mi assumes somewhat less , a condition known as missing at random ( mar ) . in our context here , this is defined as    @xmath52    a typical example of the idea behind mar is given in ( cohen and cohen , 1983 ) , concerning a study of student motivation in a classroom survey .",
    "we might surmise that students who have low levels of motivation are less likely to answer the survey question , @xmath30 , concerning their level of motivation .",
    "but other factors @xmath17 , such as socioeconomic status may explain @xmath30 so well that ( [ mar ] ) holds .",
    "the problem of course is that the predictive ability of @xmath17 may not be strong enough to justify ( [ mar ] ) .",
    "moreover , in practice some of the values in the vector @xmath17 will also be missing , further weakening the mar assumption .",
    "also , in the case of * amelia * in particular , recall that in its em computations , it replaces na values by 0s , possibly producing further bias .",
    "the literature on missing data often includes casual comments to the effect that use of cc in settings in which mcar fails , but in which mar holds , results in bias .",
    "actually , this is not necessarily the case , as will be discussed in the next two sections . though some careful treatments exist for the regression case , such as ( glynn and laird , 1986 ) , the analysis here will go into greater generality , i.e.  will not be limited to expected values , and in any case is simple enough to include here .",
    "let s see what happens under mar in the case of regression analyses and other types of association analysis .    rewrite ( [ mar ] ) as    @xmath53    where the next - to - last equality comes from ( [ mar ] )",
    ".    in other words , if we are interested in the relation between @xmath30 and @xmath17 , say by performing regression analysis of @xmath30 on @xmath17  i.e.modeling the conditional distribution of @xmath30 given @xmath17  our being deprived of the missing values of @xmath30 will not bias our regression analysis .",
    "in fact , ( [ sameconddist ] ) has the rather ironic implication :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ the mar assumption is meant to apply to situations in which cc and ac ostensibly can not be used . yet",
    ", if our goal is regression analysis or other types of measures of association , cc and ac can indeed be used in mar settings after all .",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _      on the other hand ,    @xmath54    in other words , our estimate of @xmath55 , an unconditional quantity , may be biased upward or downward .",
    "take the student motivation example , for instance . for values",
    "of @xmath56 coding high motivation , we surmise in ( [ diffunconddist ] ) ,    @xmath57    thus causing an upward bias in the intact data .",
    "this work has found the following :    * studies on various real data sets were presented here that showed that ( under the mcar assumption ) , ac can greatly outperform cc . *",
    "although mi is thought of as a method to use when ac s mcar assumption does not hold , under mi s mar assumption , ac still produces statistically correct results for regression analyses and other models of association .",
    "* situations in which mar holds but mcar does not may be rather rare . *",
    "mi computation is extremely slow , and does not seem to be any better statistically than ac . *",
    "thus , for regression / association analysis , ac may actually be a competitive alternative to mi .",
    "marsh , h. ( 1998 ) .",
    "`` pairwise deletion for missing data in structural equation models : nonpositive definite matrices , parameter estimates , goodness of fit , and adjusted sample sizes , '' _ _ structural equation modeling : a multidisciplinary journal ) , 5 , 1 , 22 - 36 ."
  ],
  "abstract_text": [
    "<S> there is a long history of devleopment of methodology dealing with missing data in statistical analysis . </S>",
    "<S> today , the most popular methods fall into two classes , complete cases ( cc ) and multiple imputation ( mi ) . </S>",
    "<S> another approach , available cases ( ac ) , has occasionally been mentioned in the research literature , in the context of linear regression analysis , but has generally been ignored . in this paper , we revisit the ac method , showing that it can perform better than cc and mi , and we extend its breadth of application .    missing values , complete cases , multiple imputation , available cases , linear regression , principle components , log - linear model </S>"
  ]
}