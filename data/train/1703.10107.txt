{
  "article_text": [
    "we consider the following regression model ; @xmath4 where @xmath5 is the @xmath6-dimensional parameter vector , while @xmath7 and @xmath8 is a @xmath9-dimensional explanatory random vector .",
    "@xmath10 is the error term .",
    "we assume that the distributions of @xmath10 is known , but the distribution of @xmath11 is unknown .",
    "the unknown parameters to be estimated are @xmath12 and @xmath13 . without loss of generality",
    ", we can assume that @xmath14 is standardized , i.e. @xmath15=0,\\quad i=1,\\ldots , p,\\qquad e[x_i x_j]= \\begin{cases } 1 , \\text { if $ 1\\leq i = j \\leq p,$ } \\\\   0 , \\text { if $ 1\\leq i\\ne j \\leq p.$ } \\end{cases}\\ ] ]    let @xmath16 and @xmath17 respectively be the p.d.f.s of @xmath10 and @xmath11 , then the p.d.f . of @xmath18 is given by @xmath19 where @xmath20 we assume that @xmath21 is positive and differentiable three times over the real line .    let s consider the maximum likelihood estimators ( say @xmath22 ) of @xmath23 .",
    "one way to evaluate the performance of m.l.e . is the closeness of the predictive distribution designated by the p.d.f @xmath24 to the true distribution given by .",
    "we adopt divergences as the measure of closeness between two given distributions .",
    "a divergence is a premetric .",
    "namely a divergence function @xmath25 $ ] evaluated at two distributions @xmath26 and @xmath27 on a same sigma field @xmath28 satisfies @xmath29 \\geq 0 \\text { for any distributions",
    "$ d_1 $ and $ d_2$}\\ ] ] with equality iff @xmath30 , but it is asymmetric , and the `` triangular inequality '' does not always hold .    among possible divergences , _ f - divergence _ is natural in dealing with probability distributions .",
    "( see amari and nagaoka , vajda @xcite . )",
    "first @xmath31-divergence is parameter - free . if we change the way of parametrization of a parametric model , @xmath31-divergence is invariant in the following sense .",
    "suppose a distribution @xmath32 on @xmath28 can be designated by a parameter @xmath33 in a parametric model @xmath34 , while it is expressed in another parametrization as @xmath35 in @xmath36 .",
    "if @xmath37 and @xmath38 is the same distribution for @xmath39 , @xmath40 = d [ ( d|\\eta_1 ) :   ( d|\\eta_2 ) ] .\\ ] ]    second it is invariant with respect to the transformation between the random variables that retains information . let @xmath41 be a sufficient statistic for the parametric model of a random object @xmath42 , then @xmath31-divergence satisfies @xmath43 = d [ ( y|\\theta_1 ) : ( y|\\theta_2 ) ] , \\ ] ] where @xmath44 is the distribution of @xmath42 given by a parameter @xmath45 @xmath46 .    in order to proceed a practical investigation of regression models ,",
    "we need a more specific form of @xmath31-divergence . in this paper",
    "we focus on an @xmath0-divergence .",
    "it is an important subclass of @xmath31-divergence .",
    "generally a divergence gives a geometrical structure on the manifold of a parametric distribution model , @xmath34 .",
    "( see eguchi @xcite , amari and nagoka . )",
    "the possible geometrical structures given by @xmath31-divergence can be realized by @xmath0-divergences .",
    "furthermore it is a basic divergence from the perspective of information geometry since it gives rise to a `` dual '' structure between @xmath0 and @xmath47 for the manifold of the given parametric model ( see eguchi @xcite , amari @xcite , and amari and cichocki ) .",
    "specifically @xmath0-divergence @xmath48 between the two distributions , each of which is given respectively by the p.d.f .",
    "@xmath49 and @xmath50 , is defined as @xmath51= \\begin{cases } \\frac{4}{1-\\alpha^2}\\bigl\\ { 1- \\int_{\\mathfrak x}f^{(1-\\alpha)/2}(x ; \\theta_1 )   f^{(1+\\alpha)/2}(x ; \\theta_2 ) d\\mu \\bigr\\ } , & \\text { if $ \\alpha \\ne \\pm 1,$}\\\\ \\int_{\\mathfrak x}f(x ; \\theta_2 ) \\log \\bigl(f(x ; \\theta_2)/f(x ; \\theta_1 ) \\bigr)d\\mu , & \\text { if   $ \\alpha=1$,}\\\\ \\int_{\\mathfrak x}f(x ; \\theta_1 ) \\log \\bigl(f(x ; \\theta_1)/f(x ; \\theta_2 ) \\bigr ) d\\mu , & \\text { if $ \\alpha=-1$. } \\end{cases}\\ ] ] @xmath0-divergence is a broad class of divergences .",
    "actually it includes kullback  leibler divergence ( @xmath52 ) , the hellinger distance ( @xmath53 ) and @xmath1 divergence ( @xmath54 ) .",
    "we will measure the performance of m.l.e .",
    "@xmath55 by the expected @xmath0-divergence between two distributions and ; @xmath56\\bigr],\\ ] ] where @xmath57 are @xmath2 independent random samples from the true distribution .",
    "in other words , we evaluate the performance of m.l.e . using the risk of m.l.e . with respect to an @xmath0 divergence",
    ". however , this risk of m.l.e . can not be gained explicitly in many ( most in a practical sense ) cases , hence if its asymptotic expansion with @xmath2 is useful since it gives a good approximation under a large size of samples .",
    "sheena @xcite gave the asymptotic expansion of @xmath58 up to the @xmath3 order for a general parametric model .",
    "( henceforth , we will call the truncated @xmath58 up to the @xmath3 order by the name of `` the approximated @xmath58 '' . ) in this paper , we focused ourselves into the regression model , and gained the specific form of each component in the general expression of sheena @xcite ( section [ general_result ] ) .",
    "the approximated @xmath58 for the general regression model is too lengthy to be out of use for a practical purpose .",
    "instead we gave a mathematica program ( appendix [ prog_mathematica ] ) that enables us to gain the approximated @xmath58 as the function of @xmath59 and the joint moments of @xmath11 once a specific error term distribution is given .",
    "in section [ homo_x ] , [ real_data ] , we considered three error term distributions in order to observe an explicit form of the expansion of @xmath58 : a normal distribution ( section [ normal distribution error ] ) , a @xmath60-distribution ( section [ t - distribution error ] ) , a skew - normal distribution ( section [ skew normal distribution error ] ) . in section [ homo_x ]",
    ", we considered the case where the explanatory variable @xmath11 has a homogeneous distribution ( i.e. invariant w.r.t .",
    "the permutations of the @xmath61 ) . we combined the above error term distributions with various types of joint moments of @xmath11 to gain a concrete form of the approximated @xmath58 as the function of @xmath62 we observed how @xmath63 affect @xmath58 . in section [ real_data ]",
    ", we treated the two real datasets , one of which has a small dimension of @xmath11 ( @xmath64 ) and the other has a relatively large dimension ( @xmath65 ) .",
    "we showed how the general theory can be applied to real datasets .",
    "as one of the possible applications of @xmath58 , we considered the sample size problem.when a parametric distribution model is given , the difficulty of estimation ( specification ) of the parameter for that model could be measured in various ways .",
    "sheena @xcite proposed to measure it by the approximated @xmath58 . in the paper ,",
    "the author tried to use the approximated @xmath58 of a binomial distribution model @xmath66 as a benchmark since it gives us an intuitive interpretation .",
    "for example , if a parametric distribution model has a similar value of @xmath67 ( at a given @xmath33 ) to @xmath68 , we can understand that the task of the estimation is hard , since the value @xmath69 is too small to be estimated from as little as 10 samples . on the contrary",
    ", @xmath67 of the model is close to that of @xmath70 , it is a relatively easy task to estimate the parameter . as the indicator of the difficulty of the estimation for a regression model , we will use the value of @xmath71 of the binomial model @xmath72 which attains the same value of the approximated @xmath58 as that of the given regression model .",
    "( we call it `` _ _ i.d.e . _ _ '' . )",
    "comparison of @xmath67 s between the regression model and the binomial model can be reversely used for the determination of the samples size .",
    "we will use the value of @xmath2 of the regression model that attains the same level of @xmath73 as the required sample size ( we abbreviate it as `` _ _ r.s.s .",
    "if we collect a sample of that size , it guarantees relatively easy estimation of the parameters in the regression model . in section [ homo_x ] and [ real_data ] , we calculated both _ i.d.e . _ and _ r.s.s .",
    "_ under the given error distributions and the moments of @xmath11 .",
    "first we introduce a general result of sheena @xcite on the asymptotic risk of m.l.e . with respect to @xmath0-divergence . in order to improve readability , we use einstein s summation convention , that is , the summation is carried out as every pair of upper and lower index moves from 1 to @xmath9 .",
    "let @xmath74 be a parametric family of probability distributions on a space @xmath75 , which is given by a family of positive - valued densities @xmath76 on @xmath75 with respect to a measure @xmath77 : @xmath78 where @xmath79 is an open set in @xmath80 .",
    "consider the maximum likelihood estimator @xmath81 of @xmath33 based on @xmath2 samples @xmath82 independently chosen from the distribution @xmath76 .",
    "closeness @xmath83 and the true parameter @xmath33 is measured by , namely @xmath84 $ ] .",
    "the risk is defined as the expectation of this random variable ; @xmath85\\bigr].\\ ] ]    the asymptotic expansion of @xmath58 w.r.t .",
    "@xmath2 is given by @xmath86\\nonumber\\\\ & + o(n^{-2}),\\label{expan_ed_final}\\end{aligned}\\ ] ] where @xmath87 the main term equals @xmath88 .",
    "@xmath89 is the ratio of the number of the parameters to the sample size .",
    "( we will call this quantity `` @xmath90 ratio '' hereafter . ) the coefficient of @xmath3 , i.e. terms inside the bracket have a geometrical meaning if we view @xmath91 as a riemannian manifold .",
    "we omit the geometrical explanation ( see sheena @xcite ) , and just describe their formal definitions .",
    "define the following notations ; for @xmath92 , @xmath93,\\quad",
    "l_{ij}\\triangleq e_\\theta[l_i l_j ] , \\\\ & l_{(ij)k}\\triangleq e_\\theta[l_{ij } l_k ] , \\quad l_{ijk}\\triangleq",
    "e_\\theta[l_i l_j l_k ] \\\\ & l_{(ij)(kl)}\\triangleq e_\\theta[l_{ij } l_{kl } ] , \\quad l_{(ijk)l}\\triangleq e_\\theta[l_{ijk}l_l ] , \\quad   l_{(ij)kl}\\triangleq e_\\theta[l_{ij}l_k l_l],\\quad l_{ijkl}\\triangleq e_\\theta[l_i l_j l_k l_l ] ,   \\end{split}\\ ] ] @xmath94 where @xmath95 is the inverse matrix of @xmath96 given by @xmath97 and @xmath98 \\triangleq \\int_{\\mathfrak x } h(x ; \\theta ) f(x ; \\theta ) d\\mu.\\end{aligned}\\ ] ]    then each term of is defined as follows .",
    "@xmath99    now we apply to the case where @xmath91 is given by @xmath100 where @xmath101 is given by .",
    "accordingly we define the following notations ; for @xmath102 @xmath103,\\quad l_{ij}\\triangleq e_\\theta[l_i l_j ] , \\\\ & l_{(ij)k}\\triangleq e_\\theta[l_{ij } l_k ] , \\quad l_{ijk}\\triangleq e_\\theta[l_i l_j l_k ] \\\\ & l_{(ij)(kl)}\\triangleq e_\\theta[l_{ij } l_{kl } ] , \\quad l_{(ijk)l}\\triangleq e_\\theta[l_{ijk}l_l ] , \\quad   l_{(ij)kl}\\triangleq e_\\theta[l_{ij}l_k l_l],\\quad l_{ijkl}\\triangleq e_\\theta[l_i l_j l_k l_l],\\end{aligned}\\ ] ] where @xmath104 with @xmath105 defined by @xmath106 we also define other notations . + for @xmath107 , @xmath108 for @xmath109 , @xmath110\\triangleq \\int_{-\\infty}^\\infty \\bigl(\\frac{d^3 \\log f(y)}{d y^3}\\bigr)^i   \\bigl(\\frac{d^2 \\log f(y)}{d y^2}\\bigr)^j \\bigl(\\frac{d \\log f(y)}{d y}\\bigr)^k y^l f(y ) dy.\\ ] ] for @xmath111 @xmath112 } m[i , j , k]=e[\\dot{x}_i \\dot{x}_j \\dot{x}_k],\\qquad m[i , j , k , l]=e[\\dot{x}_i \\dot{x}_j \\dot{x}_k \\dot{x}_l],\\ ] ] where @xmath113    straightforward calculation leads to the following results ( see appendix [ detailed_cal ] for the detailed calculation ) .",
    "@xmath114=-\\delta_{ij}\\sigma^{-2}\\eta[0,1,0,0 ] , \\quad 0 \\leq i , j \\leq p.\\label{g_ij}\\\\ g_{i\\sigma}&= \\begin{cases } \\sigma^{-2}\\eta[0,0,2,1]=-\\sigma^{-2 } \\eta[0,1,0,1 ] & \\text { if $ i=0 $ , } \\\\ 0 & \\text { if $ 1 \\leq i \\leq p.$ } \\end{cases } \\label{g_is } \\\\ g_{\\sigma\\sigma}&=\\sigma^{-2}(1 + 2\\eta[0,0,1,1]+\\eta[0,0,2,2]).\\nonumber\\\\ & = -\\sigma^{-2}(1+\\eta[0,1,0,2]+2\\eta[0,0,1,1])\\label{g_ss}\\\\ g^{ij}&=\\delta_{ij}\\sigma^{2}\\eta^{-1}[0,0,2,0],\\quad 1 \\leq i , j \\leq p.\\label{g^ij}\\\\",
    "g^{0i}&=g^{\\sigma i}=0 , \\quad 1\\leq i \\leq p.\\label{g^0i}\\\\ g^{00}&=\\sigma^{2}\\delta^{-1}(1 + 2\\eta[0,0,1,1]+\\eta[0,0,2,2]).\\label{g^00}\\\\ g^{0\\sigma}&=\\sigma^{2}\\delta^{-1}\\eta[0,1,0,1].\\label{g^0s}\\\\ g^{\\sigma\\sigma}&=\\sigma^{2}\\delta^{-1}\\eta[0,0,2,0].\\label{g^ss}\\\\ & ( \\delta=\\eta[0,0,2,0](1 + 2\\eta[0,0,1,1]+\\eta[0,0,2,2])-\\eta^2[0,1,0,1 ] ) \\nonumber\\end{aligned}\\ ] ] for @xmath115 , @xmath116\\eta_{(ij)k}\\label{l_(ij)k}\\\\ l_{ijk}&=\\sigma^{-3}m[i , j , k]\\eta_{ijk}\\label{l_ijk}\\\\ l_{(ij)(kl)}&=\\sigma^{-4 } m[i , j , k , l]\\eta_{(ij)(kl)}\\label{l_(ij)(kl)}\\\\ l_{(ijk)l}&=\\sigma^{-4}m[i , j , k , l]\\eta_{(ijk)l}\\label{l_(ijk)l}\\\\ l_{(ij)kl}&=\\sigma^{-4}m[i , j , k , l]\\eta_{(ij)kl}\\label{l_(ij)kl}\\\\ l_{ijkl}&=\\sigma^{-4}m[i , j , k , l]\\eta_{ijkl}\\label{l_ijkl},\\end{aligned}\\ ] ] where for @xmath117 , @xmath118\\label{eta_(ij)k}\\\\ \\eta_{(i\\sigma)k}&=-(\\eta[0,1,1,1]+\\eta[0,0,2,0])\\label{eta_(is)k}\\\\ \\eta_{(ij)\\sigma}&=-(\\eta[0,1,0,0]+\\eta[0,1,1,1])\\label{eta_(ij)s}\\\\ \\eta_{(i\\sigma)\\sigma}&=-(\\eta[0,1,0,1]+\\eta[0,1,1,2]+\\eta[0,0,2,1])\\label{eta_(is)s}\\\\ \\eta_{(\\sigma\\sigma)i}&=-(\\eta[0,1,1,2]+2\\eta[0,0,2,1])\\label{eta_(ss)i}\\\\ \\eta_{(\\sigma\\sigma)\\sigma}&=-(1 + 3\\eta[0,0,1,1]+\\eta[0,1,0,2]+2\\eta[0,0,2,2]+\\eta[0,1,1,3])\\label{eta_(ss)s}\\\\ \\eta_{ijk}&=-\\eta[0,0,3,0]\\label{eta_ijk}\\\\ \\eta_{ij\\sigma}&=-(\\eta[0,0,2,0]+\\eta[0,0,3,1])\\label{eta_ijs}\\\\ \\eta_{i\\sigma\\sigma}&=- ( 2\\eta[0,0,2,1]+\\eta[0,0,3,2])\\label{eta_iss}\\\\ \\eta_{\\sigma\\sigma\\sigma}&=-(1 + 3\\eta[0,0,1,1]+3\\eta[0,0,2,2]+\\eta[0,0,3,3])\\label{eta_sss}\\\\ \\eta_{(ij)(kl)}&=\\eta[0,2,0,0]\\label{eta_(ij)(kl)}\\\\ \\eta_{(i\\sigma)(kl)}&=\\eta[0,2,0,1]+\\eta[0,1,1,0]\\label{eta_(is)(kl)}\\\\ \\eta_{(i\\sigma)(j\\sigma)}&=\\eta[0,2,0,2]+2\\eta[0,1,1,1]+\\eta[0,0,2,0]\\label{eta_(is)(js)}\\\\ \\eta_{(ij)(\\sigma\\sigma)}&=\\eta[0,1,0,0]+\\eta[0,2,0,2]+2\\eta[0,1,1,1]\\label{eta_(ij)(ss)}\\\\ \\eta_{(i\\sigma)(\\sigma\\sigma)}&=\\eta[0,1,0,1]+\\eta[0,2,0,3]+3\\eta[0,1,1,2]+2\\eta[0,0,2,1]\\label{eta_(is)(ss)}\\\\ \\eta_{(\\sigma\\sigma)(\\sigma\\sigma ) } & = 1+\\eta[0,2,0,4]+4\\eta[0,0,2,2]+2\\eta[0,1,0,2]\\nonumber\\\\ & \\quad+4\\eta[0,0,1,1]+4\\eta[0,1,1,3]\\label{eta_(ss)(ss)}\\\\ \\eta_{(ijk)l}&=\\eta[1,0,1,0]\\label{eta_(ijk)l}\\\\ \\eta_{(ijk)\\sigma}&=\\eta[1,0,0,0]+\\eta[1,0,1,1]\\label{eta_(ijk)s}\\\\ \\eta_{(ij\\sigma)k}&=2\\eta[0,1,1,0]+\\eta[1,0,1,1]\\label{eta_(ijs)k}\\\\ \\eta_{(i\\sigma\\sigma)j}&=4\\eta[0,1,1,1]+2\\eta[0,0,2,0]+\\eta[1,0,1,2]\\label{eta_(iss)j}\\\\ \\eta_{(ij\\sigma)\\sigma}&=2\\eta[0,1,0,0]+\\eta[1,0,0,1]+2\\eta[0,1,1,1]+\\eta[1,0,1,2]\\label{eta_(ijs)s}\\\\ \\eta_{(i\\sigma\\sigma)\\sigma}&=4\\eta[0,1,0,1]+\\eta[1,0,0,2]+4\\eta[0,1,1,2]+2\\eta[0,0,2,1]+\\eta[1,0,1,3]\\label{eta_(iss)s}\\\\ \\eta_{(\\sigma\\sigma\\sigma)i}&=6\\eta[0,1,1,2]+6\\eta[0,0,2,1]+\\eta[1,0,1,3]\\label{eta_(sss)i}\\\\ \\eta_{(\\sigma\\sigma\\sigma)\\sigma}&=2 + 6\\eta[0,1,0,2]+6\\eta[0,0,1,1]+\\eta[1,0,0,3]\\nonumber\\\\ & \\quad+2\\eta[0,0,1,1]+6\\eta[0,1,1,3]+6\\eta[0,0,2,2]+\\eta[1,0,1,4]\\label{eta_(sss)s}\\\\ \\eta_{(ij)kl}&=\\eta[0,1,2,0]\\label{eta_(ij)kl}\\\\ \\eta_{(ij)k\\sigma}&=\\eta[0,1,1,0]+\\eta[0,1,2,1]\\label{eta_(ij)ks}\\\\ \\eta_{(i\\sigma)jk}&=\\eta[0,1,2,1]+\\eta[0,0,3,0]\\label{eta_(is)jk}\\\\ \\eta_{(ij)\\sigma\\sigma}&=\\eta[0,1,0,0]+2\\eta[0,1,1,1]+\\eta[0,1,2,2]\\label{eta_(ij)ss}\\\\ \\eta_{(i\\sigma)j\\sigma}&=\\eta[0,1,1,1]+\\eta[0,0,2,0]+\\eta[0,1,2,2]+\\eta[0,0,3,1]\\label{eta_(is)js}\\\\ \\eta_{(\\sigma\\sigma)ij}&=\\eta[0,0,2,0]+2\\eta[0,0,3,1]+\\eta[0,1,2,2]\\label{eta_(ss)ij}\\\\ \\eta_{(i\\sigma)\\sigma\\sigma}&=\\eta[0,1,0,1]+2\\eta[0,1,1,2]+2\\eta[0,0,2,1 ] + \\eta[0,1,2,3]+\\eta[0,0,3,2]\\label{eta_(is)ss}\\\\ \\eta_{(\\sigma\\sigma)i\\sigma}&=2\\eta[0,0,2,1]+\\eta[0,1,1,2]+\\eta[0,0,2,1]+2\\eta[0,0,3,2]+\\eta[0,1,2,3]\\label{eta_(ss)is}\\\\ \\eta_{(\\sigma\\sigma)\\sigma\\sigma}&=1 + 4\\eta[0,0,1,1]+\\eta[0,1,0,2]+5\\eta[0,0,2,2]\\nonumber\\\\ & \\quad+2\\eta[0,1,1,3]+2\\eta[0,0,3,3]+\\eta[0,1,2,4]\\label{eta_(ss)ss}\\\\ \\eta_{ijkl}&=\\eta[0,0,4,0]\\label{eta_ijkl}\\\\ \\eta_{ijk\\sigma}&=\\eta[0,0,3,0]+\\eta[0,0,4,1]\\label{eta_ijks}\\\\ \\eta_{ij\\sigma\\sigma}&=\\eta[0,0,2,0]+2\\eta[0,0,3,1]+\\eta[0,0,4,2]\\label{eta_ijss}\\\\ \\eta_{i\\sigma\\sigma\\sigma}&=3\\eta[0,0,2,1]+3\\eta[0,0,3,2]+\\eta[0,0,4,3]\\label{eta_isss}\\\\ \\eta_{\\sigma\\sigma\\sigma\\sigma}&=1 + 4\\eta[0,0,1,1]+6\\eta[0,0,2,2]+4\\eta[0,0,3,3]+\\eta[0,0,4,4].\\label{eta_ssss}\\end{aligned}\\ ] ]    if we insert these results , ... , into , we can calculate the values of to .",
    "note that the summation ( by einstein s convention ) in to is carried over the range @xmath119 for each index .",
    "the calculation process is so lengthy that we used mathematica @xcite .",
    "the general result expressed with abstract notations @xmath120 $ ] ( see ) and @xmath121 , m[i , j , k , l]$ ] ( see ) could be given , but it is too complicated to be out of use .",
    "instead we put the mathematica program in appendix [ prog_mathematica ] so that we can easily calculate the approximated @xmath58 once the error term distribution and the moments of the explanatory variables are given , which determine @xmath120 $ ] and @xmath121 , m[i , j , k , l]$ ] respectively .",
    "generally @xmath58 for the parametric model depends on @xmath33 .",
    "however @xmath58 for the regression model is independent of @xmath23 .",
    "this is obvious from the fact that , ... , include only @xmath122 , but it vanishes at .",
    "we report that if the support of @xmath16 is not the whole real line ( e.g. @xmath123 for negative values of @xmath10 ) , @xmath124 $ ] , hence @xmath58 could be dependent on @xmath125 .    in the next two sections ,",
    "we give the explicit result when an error distribution and the moments of @xmath11 are specified .",
    "we consider three specific cases where the error term distribution is respectively a normal distribution , a t - distribution and a skew - normal distribution .",
    "the different sets of the moments of @xmath11 are combined with these error distributions .",
    "now we mention one of the possible applications of the approximated @xmath58 .",
    "for a parametric distribution model , we naturally raise the following questions ;    1 .   at which point @xmath33 , is the parameter most difficult to be estimated ? 2 .",
    "compared with another model , this model is easier or harder to be estimated ?",
    "we propose to use the approximated @xmath58 to give an answer to these questions .",
    "maximum likelihood is the most common estimation method and intrinsic to the model , hence it is natural to measure  the difficulty of estimating the model  by its performance such as the risk w.r.t a certain loss function . as we mentioned in introduction , the risk w.r.t .",
    "@xmath0-divergence has favorable properties to answer to the above questions . in this paper",
    "we will use the approximated @xmath58 as a measure of the estimation difficulty .    in the case of the regression model ,",
    "the answer to the first question is obvious . since @xmath58 is constant ( independent of @xmath126 ) , the difficulty of estimation is same all over the parameter space . concerning the second question , we take the binomial distribution model @xmath127 ( @xmath2 : the sample size , @xmath71 : the probability of an event ) as the benchmark for comparison .    the asymptotic expansion of @xmath58 for the binomial distribution @xmath127 is given by @xmath128+o(n^{-2 } ) , \\label{expan_binom}\\end{aligned}\\ ] ] where @xmath129 and @xmath130 .",
    "( see the subsection 3.2 of sheena @xcite . ) for kullack - leibler divergence , put @xmath52 , then we have @xmath131 the graph of the approximated @xmath132 for @xmath72 is given in figure [ fig : binomial ] .",
    "we notice that the approximated @xmath132 is stable around the area @xmath133 , however it rapidly increases outside this area .     of @xmath134,width=415",
    "]    let @xmath135 denote the approximated @xmath58 for @xmath127 and @xmath136 denote that for a specific regression model where all the elements of the regression model ( @xmath9 , the error term distribution , the moments of @xmath11 ) are specified , hence @xmath137 is considered as the function of the sample size @xmath2 .",
    "here we propose an indicator of the difficulty of estimation .",
    "+ _indicator of the difficulty of estimation ( i.d.e.)_    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ use a 10 times binomial experiment @xmath72 as a benchmark . solve the equation on @xmath71 @xmath138 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    we easily notice the equation is independent of @xmath139 . taking the sample size for the regression model as @xmath140",
    ", we get the same @xmath90 ratio @xmath141 between the two models .",
    "hence it makes sense to compare the @xmath3 order terms .",
    "the solution @xmath71 tells us intuitively how difficult the parameter estimation is for the regression model .",
    "for example if @xmath142 , then we easily understand the estimation is difficult since it is difficult to estimate @xmath71 as small as 0.001 based on just 10 samples . on the contrary , if we have @xmath143 , then the estimation from 10 samples seems not so hard unless we require high precision .",
    "the above equation might have no real roots , that is , the left - hand side of the equation is larger than the right - hand side for any @xmath71 . in this case , we can conclude that the regression model could be estimated more easily than the binomial model with the same @xmath90 ratio .    in a reverse way , we can use the approximated @xmath58 of the regression model for giving an answer to the sample size problem , that is , how large sample size is required to estimate the parameters of the regression model . + _required sample size ( r.s.s.)_    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ use a 10 times normal coin toss @xmath70 as a benchmark . solve the equation @xmath144 _ _",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    the solution @xmath2 indicates the sample size large enough to guarantee as easy estimation as 10 times normal coin toss .    the equation could have no real roots",
    "this means that the left - hand side of the equation is larger than the right - hand side for any @xmath2 .",
    "since the equation is based on the `` approximated '' @xmath58 , we must notice that this does not necessarily mean just a small sample ( e.g. @xmath145 samples ) is enough for the estimation of the regression model .",
    "for the approximation to work well , the appropriate sample size is needed .",
    "if we want a concrete solution on the sample size problem , it could be gained by choosing an appropriately large @xmath139 of @xmath146 instead of 10 on the left - hand side of .",
    "in this section , we consider three concrete forms of error distribution ; a normal distribution , a t - distribution and a skew - normal distribution .",
    "a normal distribution is a theoretically basic error distribution .",
    "we are interested in how the fat tail property of a @xmath60-distribution or the skewness of a skew - normal distribution affects the ( approximated ) @xmath58 . for contrasting these properties , we choose 3 for the d.f . of the @xmath60 distribution and 3 for the shape parameter of the skew - normal distribution .",
    "figure [ fig : p.d.f_error ] is the graph of the p.d.f.s of the three error distributions ; the standard normal distribution ( @xmath147 ) , the t - distribution with the d.f . of 3 ( @xmath148 ) , the skew - normal distribution with the shape parameter of 3 ( @xmath149 ) .",
    "@xmath58 also depends on the moments of @xmath11 . as we can see from the definition  , the maximum order of the joint moments of @xmath11 is four that appear in the expansion of @xmath58 up to the @xmath3 order . in this section",
    ", we consider the homogeneous case where the distribution of @xmath150 is invariant w.r.t .",
    "any permutation of the elements .",
    "this is not practical but this case helps us observe the effect of the dimension @xmath9 , so called `` the curse of dimension '' . in section [ real_data ] ,",
    "we consider two real datasets , where we naturally treat @xmath58 under non - homogeneous explanatory variables .    here",
    "we define the notations of the homogeneous moments of @xmath11 as follows . for all distinguished @xmath151 , @xmath152,\\   m_{31}\\triangleq e[x_i^3 x_j],\\   m_{22}\\triangleq e[x_i^2 x_j^2],\\nonumber\\\\   & m_{211}\\triangleq e[x_i^2 x_j   x_k],\\   m_{1111}\\triangleq e[x_i   x_j   x_k x_l],\\\\ & m_{3}\\triangleq e[x_i^3],\\   m_{21}\\triangleq e[x_i^2 x_j],\\   m_{111}\\triangleq e[x_i x_j x_k],\\\\ & m_{2}\\triangleq e[x_i^2]=1,\\   m_{11}\\triangleq e[x_i x_j]=0,\\\\ & m_{1}\\triangleq e[x_i]=0,\\\\ & m_{0}\\triangleq e[x_0]=1 .",
    "\\label{def_m[i , j , k , l]}\\end{aligned}\\ ] ] under this homogeneity , we can give explicit forms of as a function of @xmath9 ( see appendix [ detailed_cal ] ) , which enables us to state the approximated @xmath58 explicitly for each error distribution as a function of @xmath63 and these moments .",
    "the result is given in the following subsections .",
    "when we want to analyze the approximated @xmath58 in a more concrete form , we designate the moments of @xmath11 as those from the following four distributions of @xmath11 :    1 .   the standard @xmath9-dimensional normal distributions , @xmath153 @xmath154 2 .   the standard @xmath9-dimensional @xmath60-distribution , @xmath155 , that is , the @xmath9 dimensional multivariate @xmath60-distribution with zero mean vector , the unit matrix as the scale matrix and the degree of freedom @xmath156 .",
    "its p.d.f .",
    "is given by @xmath157 note that @xmath158=0,\\ i=1,\\ldots , p$ ] and @xmath159= \\begin{cases } \\nu/(\\nu-2 ) & \\text { if   $ i = j$ , } \\\\ 0 & \\text { if $ i \\ne j$. } \\end{cases}\\ ] ] for @xmath160 .",
    "therefore after the normalization , we have @xmath161= \\begin{cases } 3(\\nu-2)/(\\nu-4 ) & \\text { if $ i = j$,}\\\\ ( \\nu-2)/(\\nu-4 ) & \\text { if $ i \\ne j$ , } \\end{cases}\\ ] ] under the condition @xmath162 notice that the effect of the fourth moment is enhanced by @xmath163 compared to the case @xmath164 .",
    "we want to check the effect of the fat tail property of a @xmath60-distribution .",
    "here we put @xmath156 as 4.2 , then we have @xmath165 3 .   a completely controlled distribution , where each @xmath61 is independently and identically distributed as @xmath166 . @xmath167 4 .",
    "pareto distributions , where each @xmath61 is independently and identically distributed as @xmath168 , pareto distributions with pareto index @xmath169 .",
    "its p.d.f .",
    "is given by @xmath170 after the normalization , we have @xmath171 we are interested in the effect of the strong skewness and heavy tail of pareto distribution .",
    "here we put @xmath169 as 4.2 .",
    "consequently @xmath172",
    "suppose that the distribution of @xmath10 is the standard normal distribution , that is , @xmath173 since @xmath174 we have for @xmath109 , @xmath175&=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty   0^i ( -1)^{(j+k ) } y^{k+l } \\exp\\bigl(-\\frac{y^2}{2}\\bigr)dy\\nonumber\\\\ & = \\begin{cases } 0 & \\text { if $ i \\geq 1$,}\\\\ 0 & \\text { if $ i=0 $ and $ k+l$ is odd,}\\\\ ( -1)^{(j+k)}(k+l-1 ) ! !",
    "& \\text { if $ i=0 $ and $ k+l$ is even } , \\label{eta_normal } \\end{cases}\\end{aligned}\\ ] ] where @xmath176    skipping long calculation ( see appendix [ detailed_cal ] for the calculation procedure using mathematica @xcite . ) , we give the final result in three different expressions for @xmath3 order term ( each expression focuses on respectively @xmath0 , the moments of @xmath11 , @xmath9 ) . @xmath177",
    "the @xmath3 order term has the following properties ;    1 .",
    "the dimension of @xmath9 is two , hence @xmath58 is asymptotically determined by the @xmath90 ratio .",
    "2 .   other moments than @xmath178 and @xmath179",
    "do not appear .",
    "the coefficients of @xmath178 and @xmath179 are non - positive when @xmath180 , that is , @xmath181 for @xmath0 within this interval , the larger @xmath179 or @xmath178 gets , the less @xmath58 becomes .",
    "the divergences often used in statistical literature are all included in this interval ; k - l divergence ( @xmath52 ) , k - l dual divergence ( @xmath182 ) , hellinger divergence ( @xmath53 ) , @xmath1 divergence ( @xmath54 ) .",
    "( see `` summary and discussion '' section . )",
    "most commonly used @xmath0-divergence is kullback - leibler divergence given by @xmath52 ; @xmath183 its `` dual '' divergence given by @xmath182 satisfies the relationship @xmath184=\\overset{\\scalebox{0.6}{\\!\\!$-1$}}{d}[\\theta_2 : \\theta_1 ] .\\ ] ] the risk of m.l.e .",
    "with respect to this divergence is given by ; @xmath185 when @xmath53 , the @xmath0-divergence becomes a distance , which is called hellinger distance ; @xmath186 when @xmath54 , it becomes @xmath1 divergence ; @xmath187    for each distribution of @xmath11 introduced in the beginning of section [ homo_x ] , we have the following results .",
    "1 .   @xmath188 @xmath189 2 .",
    "@xmath190 @xmath191 3 .",
    "@xmath11 is controlled .",
    "@xmath192 4 .",
    "@xmath193 is i.i.d . as @xmath194 @xmath195 @xmath196",
    "we made a numerical comparison to see the effect of the joint moments of @xmath11 .",
    "we set @xmath197 and @xmath198 , which means @xmath90 ratio equals @xmath141 since the number of the parameters of the regression model equals 12 when @xmath197 .",
    "figure [ fig : normal_x_-1_1 ] is the graph of the approximated @xmath132 s corresponding to each distribution of @xmath11 as @xmath139 varies from 5 to 100 .",
    "( the graph for the controlled distribution is always quite similar to that for the normal distribution , hence for the clarity of the figures we will omit it in every figure hereafter . )",
    "figure [ fig : normal_x_-1_2 ] magnifies the part @xmath199 we put as the benchmark the approximated @xmath132 of the binomial model @xmath146 , that is , the @xmath139-times normal coin toss model .",
    "when @xmath200,width=415 ]     when @xmath200,width=415 ]     when @xmath200,width=415 ]    .i.d.e . & r.s.s .",
    "for @xmath147 error distribution [ cols=\"^,^,^\",options=\"header \" , ]     we evaluate the sample size 2215 in a similar way to . if the error distribution is @xmath201 , then solving the equation @xmath202 gives us an evaluation of the actual sample size .",
    "when @xmath52 , the rounded solution is 22 or 23 for the three error distributions . though this number is much smaller than 376(377 ) in example 1 , the estimation is still not a hard task since 22-times normal coin toss gives us plenty of information .",
    "* the main term ( @xmath203 term ) of the asymptotic expansion of @xmath58 is @xmath204 , that is , @xmath90 ratio .",
    "* for the second term ( @xmath3 term ) of the expansion , we observe the following points . 1 .",
    "the maximum dimension of @xmath9 depends on the error term distribution .",
    "it can be more than two as in the case @xmath205 , where it is not enough to increase the sample size proportionally to @xmath9 for reliable estimation ( so called `` the curse of dimension '' ) .",
    "the joint moments that appear in the term is maximally of the forth order .",
    "what moments appear is different among the error term distributions . if it is a quadratic distribution ( e.g. @xmath147 , @xmath206 ) , then the moments @xmath178 and @xmath179 only appear . 3 .",
    "the effect of @xmath178 and @xmath179 depends on @xmath207 when @xmath208 , the larger @xmath178 and @xmath179 decreases the difficulty of the estimation . in a geometrical view",
    ", there is no preference among @xmath0 s .",
    "each @xmath0 gives its own geometrical structure to riemannian manifold formed by the parametric distribution model ( see e.g. amari and nagaoka ) .",
    "however there might be values for @xmath0 that is `` natural '' in a statistical sense or `` appropriate '' for a purpose of the statistical analysis .",
    "the effect of the error term distributions also depends on @xmath0 .",
    "for example , the order of the estimation difficulty among the three error distributions is quite different between @xmath52 and @xmath209 .",
    "the difference between the three error term distributions we investigated is relatively small if we use kullback - leibler divergence.this might be due to the assumption that we know the error term distributions , hence are able to use m.l.e . in most applications ,",
    "the actual error term distribution is unknown , and m.l.e .",
    "is not applicable .",
    "it is of much interest what would happen to the predictive distribution , if we use another estimator , such as the least squares estimator .",
    "* we proposed measuring the ( asymptotic ) difficulty of estimation by the approximated @xmath58 and tried to give a suggestion on the sample size .",
    "it is a method comparing the approximated @xmath58 of the regression model to that of a binomial model @xmath127 .",
    "i.d.e . tells the difficulty of estimation by the value of @xmath71 of @xmath210 , which has the same @xmath90 ratio as the regression model of the sample size @xmath140 .",
    "r.s.s . gives the sample size @xmath2 for the regression model which leads to the same difficulty of estimation as @xmath70 ( if it is needed , a more large value than 10 will be used for the binomial model ) .",
    "though there exist small difference between the error term distributions and the moments of @xmath11 , in most cases we investigated , the regression model is easier to be estimated than the normal coin toss @xmath211 under the same @xmath90 ratio @xmath141 .",
    "the sample size @xmath212 guarantees the good performance of the estimation at the same level as the 10-times normal coin toss irrespective of the error term distributions and the moments of @xmath11 which we investigated in this paper .",
    "first we give the detailed calculation process of the results from to . since @xmath213 and @xmath214 we have the following results ; @xmath215    now we state the proof of each result from to .",
    "the next formula will be often used in those proofs .",
    "suppose that an integrable function @xmath216 on @xmath217 allows the following exchangeability between the integral and the differentiation , @xmath218 then @xmath219 in the following proofs , all the functions derived from @xmath220 are supposed to satisfy this exchangeability .",
    "we also use the notation @xmath221\\triangleq \\int_{-\\infty}^\\infty h(y ) f(y ) dy\\ ] ]  proof of ",
    "@xmath222\\\\ & = \\sigma^{-2}m[i , j]\\eta[0,0,2,0]\\end{aligned}\\ ] ] from the fact @xmath223 and , @xmath224=\\delta_{ij}$ ] .",
    "the following equation also holds ; @xmath225+\\eta[0,1,0,0]&=\\int_r \\bigl(\\bigl(\\log ' f(y)\\bigr)^2+\\log''f(y ) \\bigr ) f(y ) dy \\\\ & = \\int _ r f''(y ) dy\\\\ & = 0 .\\end{aligned}\\ ] ]  proof of ",
    "@xmath226\\eta[0,0,2,1]\\\\ & = \\begin{cases } \\sigma^{-2}\\eta[0,0,2,1 ] & \\text { if $ i=0$,}\\\\ 0 & \\text { if $ 1\\leq i \\leq p$. } \\end{cases}\\end{aligned}\\ ] ] the fourth equation is due to the following relation ; @xmath227 we also have the equation , @xmath228+\\eta[0,1,0,1]&=\\int_r \\bigl(\\bigl(\\log ' f(y)\\bigr)^2+\\log''f(y ) \\bigr ) y f(y ) dy \\\\ & = \\int _ r yf''(y ) dy\\\\ & = \\int_r yf''(y)+f'(y ) dy\\\\ & = \\int_r ( y f'(y))'dy\\\\ & = 0.\\end{aligned}\\ ] ] +  proof of  @xmath229+\\eta[0,0,2,2]).\\end{aligned}\\ ] ] the following equations hold ; @xmath230 + 2\\eta[0,0,1,1]+\\eta[0,0,2,2]\\\\ & = \\int_r",
    "\\bigl(y^2 \\log''f(y ) + 2 y\\log'f(y)+\\bigl(\\log ' f(y)\\bigr)^2y^2\\bigr ) f(y ) dy & = \\int_r \\big(\\log'f(y ) y^2 f(y)\\bigr ) ' dy & = 0,\\\\ & 1+\\eta[0,0,1,1]\\\\ & = \\int_r \\bigl ( y f(y ) \\bigr ) ' dy\\\\ & = 0.\\end{aligned}\\ ] ] therefore @xmath231+\\eta[0,0,2,2]=-(1 + 2\\eta[0,0,1,1]+\\eta[0,1,0,2]).\\ ] ] +  proof of , , , and ",
    "+ from , and , we notice that @xmath232 & -\\eta[0,1,0,1 ] \\\\",
    "-\\eta[0,1,0,1 ] & 1 + 2\\eta[0,0,1,1]+\\eta[0,0,2,2 ] \\end{pmatrix}^{-1}\\\\ & = \\delta^{-1}\\sigma^2 \\begin{pmatrix }   1 + 2\\eta[0,0,1,1]+\\eta[0,0,2,2 ] & \\eta[0,1,0,1 ] \\\\",
    "\\eta[0,1,0,1 ] & \\eta[0,0,2,0 ] \\end{pmatrix}\\end{aligned}\\ ] ] and are obvious . +   + ",
    "proof of , , , , and ",
    "+ the equations from to are straightforward from the results to .",
    "is gained as follows ; @xmath233 + 2\\eta[0,0,1,1]+\\eta[0,0,1,1]+\\eta[0,1,1,3]+2\\eta[0,0,2,2]\\bigr)\\\\ & = -\\sigma^{-3}\\bigl(1 + 3\\eta[0,0,1,1]+\\eta[0,1,0,2]+2\\eta[0,0,2,2]+\\eta[0,1,1,3]\\bigr)\\end{aligned}\\ ] ]  proof of , , and",
    " + these equations are almost obvious from and .",
    "+   +  proof of , , , , and  + , , , are obvious from to . and",
    "are gained as follows ; @xmath234 \\bigl(\\eta[0,1,0,1]+\\eta[0,2,0,3]+2\\eta[0,1,1,2]+\\eta[0,1,1,2]+2\\eta[0,0,2,1]\\bigr)\\\\ & = \\sigma^{-4}m[i ] \\bigl(\\eta[0,1,0,1]+\\eta[0,2,0,3]+3\\eta[0,1,1,2]+2\\eta[0,0,2,1]\\bigr),\\\\ & l_{(\\sigma\\sigma)(\\sigma\\sigma)}\\\\ & = \\sigma^{-4}\\int_r \\bigl(1 + 2\\log'f(y )",
    "y+\\log''f(y ) y^2\\bigr)^2 f(y ) dy\\\\ & = \\sigma^{-4}\\bigl(1+\\eta[0,2,0,4]+4\\eta[0,0,2,2]+2\\eta[0,1,0,2]+4\\eta[0,0,1,1]+4\\eta[0,1,1,3]\\bigr).\\end{aligned}\\ ] ] +   +  proof of to  +",
    "we only describe the proof of .",
    "other equations are instantly gained from , ,  .",
    "@xmath235\\bigl ( 4\\eta[0,1,0,1]+\\eta[1,0,0,2]+4\\eta[0,1,1,2]+2\\eta[0,0,2,1]+\\eta[1,0,1,3]\\bigr).\\end{aligned}\\ ] ] +   +  proof of to ",
    "+ all the equations are instantly gained from  .",
    "+   +  proof of to  + all the equations are instantly gained from and .",
    "+   +  calculation of @xmath236  + we state here the calculation process of @xmath236 , which is actually used in mathematica programming in appendix [ prog_mathematica ] .",
    "let @xmath237 and @xmath238 \\\\",
    "m_{2b}&\\triangleq \\sum_{i , j , k \\in",
    "\\mathcal{i}}m[i , i , k ] m[j , j , k]\\\\ m_1&\\triangleq \\sum_{i , k \\in",
    "\\mathcal{i}}m[i , i , k , k]\\end{aligned}\\ ] ] note that if @xmath11 is homogeneous , @xmath239 are respectively given by @xmath240 + 3\\sum_{i , j \\in \\mathcal{i } , i \\ne j}m^2[i , i , j]+\\sum_{i , j , k \\in \\mathcal{i } , i \\ne j , i\\ne k , j\\ne k}m^2[i , j , k]\\nonumber\\\\ & = pm_3 ^ 2 + 3p(p-1)m_{21}^2+p(p-1)(p-2)m_{111}^2\\\\ m_{2b}&=\\sum _ { i \\in \\mathcal{i}}m^2[i , i , i]+\\sum_{i , j \\in \\mathcal{i } , i \\ne j}m^2[i , i , j]+\\sum_{i , j \\in \\mathcal{i } , i \\ne j}m[i , i , i]m[j , j , i]\\nonumber\\\\ & \\hspace{35mm}+\\sum_{i , j \\in \\mathcal{i } , i \\ne j}m[i , i , j]m[j , j , j]+\\sum_{i , j , k \\in \\mathcal{i } , i \\ne j , i\\ne k ,",
    "j\\ne k}m[i , i , k ] m[j , j , k]\\nonumber\\\\ & = pm_3 ^ 2+p(p-1)m_{21}^2 + 2p(p-1)m_3m_{21}+p(p-1)(p-2)m_{21}^2\\nonumber\\\\ & = pm_3 ^ 2+p(p-1)^2m_{21}^2 + 2p(p-1)m_3m_{21}\\\\ m_1&=\\sum_{i\\in \\mathcal{i } } m[i , i , i , i]+\\sum_{i , k \\in \\mathcal{i } , i \\ne k } m[i , i , k , k]\\nonumber\\\\ & = pm_4+p(p-1)m_{22}\\end{aligned}\\ ] ] preliminarily we have the following results . @xmath241\\nonumber\\\\ & = \\eta^{-3}[0,0,2,0]m_{2a}\\nonumber\\\\ & \\sum_{i , j \\in \\mathcal{i}}\\tilde{g}^{ii}\\tilde{g}^{jj}m^2[i , j]\\nonumber\\\\ & = \\eta^{-2}[0,0,2,0]\\sum_{i , j \\in \\mathcal{i}}m^2[i , j]\\nonumber\\\\ & = \\eta^{-2}[0,0,2,0]\\bigl(pm_2 ^ 2+p(p-1)m_{11}^2\\bigr)\\nonumber\\\\ & = \\eta^{-2}[0,0,2,0 ] p , \\\\ & \\sum_{i \\in \\mathcal{i}}\\tilde{g}^{ii}m^2[i]=0.\\end{aligned}\\ ] ] @xmath242m[j , j , k]\\nonumber\\\\ & = \\eta^{-3}[0,0,2,0]m_{2b}\\\\ & \\sum_{i , j \\in \\mathcal{i}}\\tilde{g}^{ii}\\tilde{g}^{jj}m[i , i]m[j , j]\\nonumber\\\\ & = \\eta^{-2}[0,0,2,0]\\bigl(\\sum_{i \\in \\mathcal{i}}m^2[i , i]+\\sum_{i , j \\in \\mathcal{i},i \\ne j}m[i , i]m[j , j]\\bigr)\\nonumber\\\\ & = \\eta^{-2}[0,0,2,0]\\bigl(pm_2 ^ 2+p(p-1)m_{2}^2\\bigr)\\nonumber\\\\ & = \\eta^{-2}[0,0,2,0 ] p^2 , \\\\ & \\sum_{i , j \\in",
    "\\mathcal{i},i \\ne j}\\tilde{g}^{ii}\\tilde{g}^{jj}m[i , i , j]m[j]=0\\\\ & \\sum_{i\\in \\mathcal{i}}\\tilde{g}^{ii}m^2[i]=0\\\\ & \\sum_{i\\in \\mathcal{i}}\\tilde{g}^{ii}m[i , i]=\\eta^{-1}[0,0,2,0]p\\\\ & \\sum_{i , j , k , l\\in \\mathcal{i}}\\tilde{g}^{ij}\\tilde{g}^{kl}m[i , j , k , l]\\nonumber\\\\ & = \\eta^{-2}[0,0,2,0]m_1\\\\ & \\sum_{i , j \\in \\mathcal{i } } \\tilde{g}^{ij } m[i , j]=\\eta^{-1}[0,0,2,0 ] p\\end{aligned}\\ ] ]    let s consider @xmath243 first .",
    "@xmath244m[j , l , u]\\eta_{(ik)s}\\eta_{jlu}\\\\ & \\quad+\\sum_{i , j , k , l \\in \\mathcal{i } , s , u\\in \\mathcal{s } } \\tilde{g}^{ij}\\tilde{g}^{kl}\\tilde{g}^{su } m[i , k , s]m[j , l , u]\\eta_{(ik)s}\\eta_{jlu}\\\\ & \\quad+\\sum_{i , j , s , u \\in \\mathcal{i } , k , l\\in \\mathcal{s } } \\tilde{g}^{ij}\\tilde{g}^{kl}\\tilde{g}^{su } m[i , k , s]m[j , l , u]\\eta_{(ik)s}\\eta_{jlu}\\\\ & \\quad+\\sum_{k , l , s , u   \\in \\mathcal{i } , i , j\\in \\mathcal{s } } \\tilde{g}^{ij}\\tilde{g}^{kl}\\tilde{g}^{su } m[i , k , s]m[j , l , u]\\eta_{(ik)s}\\eta_{jlu}\\\\ & \\quad+\\sum_{i , j \\in \\mathcal{i } , k , l , s , u\\in \\mathcal{s } } \\tilde{g}^{ij}\\tilde{g}^{kl}\\tilde{g}^{su } m[i , k , s]m[j , l , u]\\eta_{(ik)s}\\eta_{jlu}\\\\ & \\quad+\\sum_{k , l   \\in \\mathcal{i } , i , j , s , u\\in \\mathcal{s } } \\tilde{g}^{ij}\\tilde{g}^{kl}\\tilde{g}^{su } m[i , k , s]m[j , l , u]\\eta_{(ik)s}\\eta_{jlu}\\\\ & \\quad+\\sum_{s , u \\in \\mathcal{i } , i , j , k , l\\in \\mathcal{s } } \\tilde{g}^{ij}\\tilde{g}^{kl}\\tilde{g}^{su } m[i , k , s]m[j , l , u]\\eta_{(ik)s}\\eta_{jlu}\\\\ & \\quad+\\sum_{i , j , k , l , s , u \\in \\mathcal{s } } \\tilde{g}^{ij}\\tilde{g}^{kl}\\tilde{g}^{su } m[i , k , s]m[j , l , u]\\eta_{(ik)s}\\eta_{jlu}\\\\ & = \\eta_{(00)0}\\eta_{000}\\sum_{i , j , k \\in \\mathcal{i } } \\tilde{g}^{ii}\\tilde{g}^{jj}\\tilde{g}^{kk}m^2[i , j , k ] \\\\ & \\quad+\\sum_{i , j \\in \\mathcal{i}}\\tilde{g}^{ii}\\tilde{g}^{jj}m^2[i , j]\\sum_{s , u\\in \\mathcal{s}}\\tilde{g}^{su}\\eta_{(00)s}\\eta_{00u}\\\\ & \\quad+\\sum_{i , j\\in \\mathcal{i } } \\tilde{g}^{ii}\\tilde{g}^{jj}m^2[i , j ]   \\sum_{k , l\\in \\mathcal{s } } \\tilde{g}^{kl}\\eta_{(0k)0}\\eta_{0l0}\\\\ & \\quad+\\sum_{k , l \\in \\mathcal{i}}\\tilde{g}^{kk}\\tilde{g}^{ll}m^2[k , l ] \\sum_{i , j\\in \\mathcal{s}}g^{ij}\\eta_{(i0)0}\\eta_{j00}\\\\ & \\quad+\\sum_{i\\in \\mathcal{i}}\\tilde{g}^{ii }   m^2[i]\\sum_{k , l , s , u\\in \\mathcal{s}}\\tilde{g}^{kl}\\tilde{g}^{su}\\eta_{(0k)s}\\eta_{0lu}\\\\ & \\quad+\\sum_{k\\in \\mathcal{i}}\\tilde{g}^{kk}m^2[k]\\sum_{i , j , s , u\\in \\mathcal{s } } \\tilde{g}^{ij}\\tilde{g}^{su } \\eta_{(i0)s}\\eta_{j0u}\\\\ & \\quad+\\sum_{s \\in \\mathcal{i}}\\tilde{g}^{ss } m^2[s]\\sum_{i , j , k , l\\in \\mathcal{s } } \\tilde{g}^{ij}\\tilde{g}^{kl } \\eta_{(ik)0}\\eta_{jl0}\\\\ & \\quad+\\sum_{i , j , k , l , s , u \\in \\mathcal{s } } \\tilde{g}^{ij}\\tilde{g}^{kl}\\tilde{g}^{su } m[i , k , s]m[j , l , u]\\eta_{(ik)s}\\eta_{jlu}\\\\ & = \\eta^{-3}[0,0,2,0]m_{2a } \\eta_{(00)0}\\eta_{000}\\\\ & \\quad+\\eta^{-2}[0,0,2,0]p\\sum_{s , u\\in \\mathcal{s}}\\tilde{g}^{su}\\eta_{(00)s}\\eta_{00u}\\\\ & \\quad+\\eta^{-2}[0,0,2,0]p \\sum_{k , l\\in \\mathcal{s } } \\tilde{g}^{kl}\\eta_{(0k)0}\\eta_{0l0}\\\\ & \\quad+\\eta^{-2}[0,0,2,0]p\\sum_{i , j\\in \\mathcal{s}}\\tilde{g}^{ij}\\eta_{(i0)0}\\eta_{j00}\\\\ & \\quad+\\sum_{i , j , k , l , s , u \\in \\mathcal{s } } \\tilde{g}^{ij}\\tilde{g}^{kl}\\tilde{g}^{su}\\eta_{(ik)s}\\eta_{jlu}.\\end{aligned}\\ ] ] similarly we have @xmath245m_{2b } \\eta_{(00)0}\\eta_{000}\\\\ & \\quad+\\eta^{-2}[0,0,2,0]p^2 \\sum_{k , l\\in \\mathcal{s } }",
    "\\tilde{g}^{kl}\\eta_{(00)k}\\eta_{l00}\\\\ & \\quad+\\eta^{-1}[0,0,2,0]p \\sum_{k , l , s , u\\in \\mathcal{s } } \\tilde{g}^{kl}\\tilde{g}^{su}\\eta_{(00)k}\\eta_{lsu}\\\\ & \\quad+\\eta^{-1}[0,0,2,0]p \\sum_{i , j , k , l\\in \\mathcal{s } } \\tilde{g}^{ij}\\tilde{g}^{kl}\\eta_{(ij)k}\\eta_{l00}\\\\ & \\quad+\\sum_{i , j , k , l , s , u \\in \\mathcal{s } } \\tilde{g}^{ij}\\tilde{g}^{kl}\\tilde{g}^{su}\\eta_{(ij)k}\\eta_{lsu}.\\\\ l23&=\\eta^{-3}[0,0,2,0]m_{2a } \\eta_{000}\\eta_{000}\\\\ & \\quad+\\eta^{-2}[0,0,2,0]p \\sum_{s , u\\in \\mathcal{s } } \\tilde{g}^{su}\\eta_{00s}\\eta_{00u}\\\\ & \\quad+\\eta^{-2}[0,0,2,0]p \\sum_{k , l\\in \\mathcal{s } } \\tilde{g}^{kl}\\eta_{0k0}\\eta_{0l0}\\\\ & \\quad+\\eta^{-2}[0,0,2,0]p \\sum_{i , j\\in \\mathcal{s } } \\tilde{g}^{ij}\\eta_{i00}\\eta_{j00}\\\\ & \\quad+\\sum_{i , j , k , l , s , u \\in \\mathcal{s } } \\tilde{g}^{ij}\\tilde{g}^{kl}\\tilde{g}^{su}\\eta_{iks}\\eta_{jlu}.\\\\ l24&=\\eta^{-3}[0,0,2,0]m_{2b } \\eta_{000}\\eta_{000}\\\\ & \\quad+\\eta^{-2}[0,0,2,0]p^2 \\sum_{k , l\\in \\mathcal{s } } \\tilde{g}^{kl}\\eta_{00k}\\eta_{l00}\\\\ & \\quad+\\eta^{-1}[0,0,2,0]p \\sum_{k , l , s , u\\in \\mathcal{s } } \\tilde{g}^{kl}\\tilde{g}^{su}\\eta_{00k}\\eta_{lsu}\\\\ & \\quad+\\eta^{-1}[0,0,2,0]p \\sum_{i , j , k , l\\in \\mathcal{s } } \\tilde{g}^{ij}\\tilde{g}^{kl}\\eta_{ijk}\\eta_{l00}\\\\ & \\quad+\\sum_{i , j , k , l , s , u \\in \\mathcal{s } } \\tilde{g}^{ij}\\tilde{g}^{kl}\\tilde{g}^{su}\\eta_{ijk}\\eta_{lsu}.\\\\ l25&=\\eta^{-3}[0,0,2,0]m_{2a } \\eta_{(00)0}\\eta_{(00)0}\\\\ & \\quad+\\eta^{-2}[0,0,2,0]p \\sum_{s , u\\in \\mathcal{s } } \\tilde{g}^{su}\\eta_{(00)s}\\eta_{(00)u}\\\\ & \\quad+\\eta^{-2}[0,0,2,0]p \\sum_{k , l\\in \\mathcal{s } } \\tilde{g}^{kl}\\eta_{(0k)0}\\eta_{(0l)0}\\\\ & \\quad+\\eta^{-2}[0,0,2,0]p \\sum_{i , j\\in \\mathcal{s } } \\tilde{g}^{ij}\\eta_{(i0)0}\\eta_{(j0)0}\\\\ & \\quad+\\sum_{i , j , k , l , s , u \\in \\mathcal{s } } \\tilde{g}^{ij}\\tilde{g}^{kl}\\tilde{g}^{su}\\eta_{(ik)s}\\eta_{(jl)u}.\\\\ l26&=\\eta^{-3}[0,0,2,0]m_{2b } \\eta_{(00)0}\\eta_{(00)0}\\\\ & \\quad+\\eta^{-2}[0,0,2,0]p^2 \\sum_{k , l\\in \\mathcal{s } } \\tilde{g}^{kl}\\eta_{(00)k}\\eta_{(00)l}\\\\ & \\quad+\\eta^{-1}[0,0,2,0]p \\sum_{k , l , s , u\\in \\mathcal{s } } \\tilde{g}^{kl}\\tilde{g}^{su}\\eta_{(00)k}\\eta_{(su)l}\\\\ & \\quad+\\eta^{-1}[0,0,2,0]p \\sum_{i , j , k , l\\in \\mathcal{s } } \\tilde{g}^{ij}\\tilde{g}^{kl}\\eta_{(ij)k}\\eta_{(00)l}\\\\ & \\quad+\\sum_{i , j , k , l , s , u \\in \\mathcal{s } } \\tilde{g}^{ij}\\tilde{g}^{kl}\\tilde{g}^{su}\\eta_{(ij)k}\\eta_{(su)l}.\\\\ l11&=\\sum_{i , j , k , l \\in \\mathcal{i } } \\tilde{g}^{ij}\\tilde{g}^{kl}m[i , j , k , l]\\eta_{(il)jk}\\\\ & \\quad+\\sum_{i , j\\in \\mathcal{i } , k , l\\in \\mathcal{s } } \\tilde{g}^{ij}\\tilde{g}^{kl}m[i , j , k , l]\\eta_{(il)jk}\\\\ & \\quad+\\sum_{i , j\\in \\mathcal{s } , k , l\\in \\mathcal{i } } \\tilde{g}^{ij}\\tilde{g}^{kl}m[i , j , k , l]\\eta_{(il)jk}\\\\ & \\quad+\\sum_{i , j , k , l\\in \\mathcal{s } } \\tilde{g}^{ij}\\tilde{g}^{kl}m[i , j , k , l]\\eta_{(il)jk}\\\\ & = \\eta^{-2}[0,0,2,0]m_1\\eta_{(00)00}\\\\ & \\quad+\\eta^{-1}[0,0,2,0]p\\sum_{k , l \\in \\mathcal{s}}\\tilde{g}^{kl}\\eta_{(0l)0k}\\\\ & \\quad+\\eta^{-1}[0,0,2,0]p\\sum_{i , j \\in \\mathcal{s}}\\tilde{g}^{ij}\\eta_{(i0)j0}\\\\ & \\quad+\\sum_{i , j , k , l\\in \\mathcal{s } } \\tilde{g}^{ij}\\tilde{g}^{kl}\\eta_{(il)jk}.\\\\ l12&=\\eta^{-2}[0,0,2,0]m_1\\eta_{(00)00}\\\\ & \\quad+\\eta^{-1}[0,0,2,0]p\\sum_{k , l \\in \\mathcal{s}}\\tilde{g}^{kl}\\eta_{(00)kl}\\\\ & \\quad+\\eta^{-1}[0,0,2,0]p\\sum_{i , j \\in \\mathcal{s}}\\tilde{g}^{ij}\\eta_{(ij)00}\\\\ & \\quad+\\sum_{i , j , k , l\\in \\mathcal{s } } \\tilde{g}^{ij}\\tilde{g}^{kl}\\eta_{(ij)kl}.\\\\ l13&=\\eta^{-2}[0,0,2,0]m_1\\eta_{0000}\\\\ & \\quad+\\eta^{-1}[0,0,2,0]p\\sum_{k , l \\in \\mathcal{s}}\\tilde{g}^{kl}\\eta_{00kl}\\\\ & \\quad+\\eta^{-1}[0,0,2,0]p\\sum_{i , j \\in \\mathcal{s}}\\tilde{g}^{ij}\\eta_{ij00}\\\\ & \\quad+\\sum_{i , j , k , l\\in \\mathcal{s } } \\tilde{g}^{ij}\\tilde{g}^{kl}\\eta_{ijkl}.\\\\ l14&=\\eta^{-2}[0,0,2,0]m_1\\eta_{(00)(00)}\\\\ & \\quad+\\eta^{-1}[0,0,2,0]p\\sum_{k , l \\in \\mathcal{s}}\\tilde{g}^{kl}\\eta_{(0k)(0l)}\\\\ & \\quad+\\eta^{-1}[0,0,2,0]p\\sum_{i , j \\in \\mathcal{s}}\\tilde{g}^{ij}\\eta_{(i0)(j0)}\\\\ & \\quad+\\sum_{i , j , k , l\\in \\mathcal{s } } \\tilde{g}^{ij}\\tilde{g}^{kl}\\eta_{(ik)(jl)}.\\\\ l15&=\\eta^{-2}[0,0,2,0]m_1\\eta_{(00)(00)}\\\\ & \\quad+\\eta^{-1}[0,0,2,0]p\\sum_{k , l \\in \\mathcal{s}}\\tilde{g}^{kl}\\eta_{(00)(kl)}\\\\ & \\quad+\\eta^{-1}[0,0,2,0]p\\sum_{i , j \\in \\mathcal{s}}\\tilde{g}^{ij}\\eta_{(ij)(00)}\\\\ & \\quad+\\sum_{i , j , k , l\\in \\mathcal{s } } \\tilde{g}^{ij}\\tilde{g}^{kl}\\eta_{(ij)(kl)}.\\\\\\end{aligned}\\ ] ]",
    "for the error term distributions , @xmath147 , @xmath206 , we theoretically derived the function @xmath124 $ ] respectively as in , .",
    "when the error term distribution is a skew - normal , we derived its values numerically by monte carlo simulation",
    ". we will call this process of the mathematica programming `` eta part '' . in the next step ( called `` main part '' in the programming ) , we first calculated @xmath246 and to , and then with another input @xmath247 , m[i , j , k , l]$ ] ( instead @xmath248 when @xmath11 is homogeneous ) , the actual process for the calculation of is stated in the last part of appendix [ detailed_cal ] . here",
    "we put the `` main part '' of the program of mathematica .    ....",
    "main part   inputs :   1 .",
    "eta[i , j , k , l ] ( which is calculated in eta part for each error distribution ) 2-a .",
    "m4 , m22 , m3 , m21 , m111 ( m_{4 } , m_{22 } , m_{3 } , m{21 } , m{111 } in the text ) for homogeneous x 2-b .",
    "m[i , j , k ] , m[i , j , k , l ] for nonhomogeneous x            eta_{(i , j),k } denoted by etau[{i_,j_},k _ ] etau[{i_,j _ } , k_]:=-eta[0,1,1,0]/;i==0&&j==0 & & k==0   etau[{i_,j _ } , k_]:=-(eta[0,1,1,1]+eta[0,0,2,0])/;i==0 & & j==sigma & & k==0   etau[{i_,j _ } , k_]:=-(eta[0,1,1,1]+eta[0,0,2,0])/;j==0 & & i==sigma & & k==0   etau[{i_,j _ } , k_]:=-(eta[0,1,0,0]+eta[0,1,1,1])/;i==0&&j==0 & & k==sigma   etau[{i_,j _ } , k_]:=-(eta[0,1,0,1]+eta[0,1,1,2]+eta[0,0,2,1])/;i==0 & & j==sigma & & k==sigma   etau[{i_,j _ } , k_]:=-(eta[0,1,0,1]+eta[0,1,1,2]+eta[0,0,2,1])/;j==0 & & i==sigma & & k==sigma   etau[{i_,j _ } , k_]:=-(eta[0,1,1,2]+2*eta[0,0,2,1])/;i==sigma & & j==sigma & & k==0   etau[{i_,j _ } , k_]:=-(1 + 3*eta[0,0,1,1]+eta[0,1,0,2]+2*eta[0,0,2,2]+eta[0,1,1,3 ] )",
    "/;i==sigma & & j==sigma & & k==sigma    eta_{i , j , k } denoted by etau[i_,j_,k _ ] etau[i_,j_,k_]:=-eta[0,0,3,0]/;i==0&&j==0 & & k==0   etau[i_,j_,k_]:=-(eta[0,0,2,0]+eta[0,0,3,1])/;i==0&&j==0 & & k==sigma   etau[i_,j_,k_]:=-(eta[0,0,2,0]+eta[0,0,3,1])/;i==0&&j==sigma & & k==0 etau[i_,j_,k_]:=-(eta[0,0,2,0]+eta[0,0,3,1])/;i==sigma&&j==0 & & k==0 etau[i_,j_,k_]:=-(eta[0,0,1,0]+2*eta[0,0,2,1]+eta[0,0,3,2])/;i==0&&j==sigma & & k==sigma   etau[i_,j_,k_]:=-(eta[0,0,1,0]+2*eta[0,0,2,1]+eta[0,0,3,2])/;i==sigma&&j==0 & & k==sigma etau[i_,j_,k_]:=-(eta[0,0,1,0]+2*eta[0,0,2,1]+eta[0,0,3,2])/;i==sigma&&j==sigma & & k==0 etau[i_,j_,k_]:=-(1 + 3*eta[0,0,1,1]+3*eta[0,0,2,2]+eta[0,0,3,3 ] ) /;i==sigma&&j==sigma   & & k==sigma     eta_{(i , j)(k , l ) } denoted by etau[{i_,j_},{k_,l _ } ] etau[{i_,j_},{k_,l_}]:=eta[0,2,0,0]/;i==0&&j==0 & & k==0 & & l==0 etau[{i_,j_},{k_,l_}]:=eta[0,2,0,1]+eta[0,1,1,0]/;i==sigma&&j==0 & & k==0 & & l==0 etau[{i_,j_},{k_,l_}]:=eta[0,2,0,1]+eta[0,1,1,0]/;i==0&&j==sigma & & k==0 & & l==0 etau[{i_,j_},{k_,l_}]:=eta[0,2,0,1]+eta[0,1,1,0]/;i==0&&j==0 & & k==sigma & & l==0 etau[{i_,j_},{k_,l_}]:=eta[0,2,0,1]+eta[0,1,1,0]/;i==0&&j==0&&k==0 & & l==sigma etau[{i_,j_},{k_,l_}]:=eta[0,2,0,2]+2*eta[0,1,1,1]+eta[0,0,2,0 ] /;i==0&&j==sigma&&k==0 & & l==sigma etau[{i_,j_},{k_,l_}]:=eta[0,2,0,2]+2*eta[0,1,1,1]+eta[0,0,2,0 ] /;i==0&&j==sigma&&k==sigma & & l==0 etau[{i_,j_},{k_,l_}]:=eta[0,2,0,2]+2*eta[0,1,1,1]+eta[0,0,2,0 ] /;i==sigma&&j==0&&k==0 & & l==sigma etau[{i_,j_},{k_,l_}]:=eta[0,2,0,2]+2*eta[0,1,1,1]+eta[0,0,2,0 ] /;i==sigma&&j==0&&k==sigma & & l==0 etau[{i_,j_},{k_,l_}]:=eta[0,1,0,0]+eta[0,2,0,2]+2*eta[0,1,1,1 ] /;i==0&&j==0&&k==sigma & & l==sigma etau[{i_,j_},{k_,l_}]:=eta[0,1,0,0]+eta[0,2,0,2]+2*eta[0,1,1,1 ] /;i==sigma&&j==sigma&&k==0&&l==0",
    "etau[{i_,j_},{k_,l_}]:=eta[0,1,0,1]+eta[0,2,0,3]+3*eta[0,1,1,2]+2*eta[0,0,2,1 ] /;i==0&&j==sigma&&k==sigma & & l==sigma etau[{i_,j_},{k_,l_}]:=eta[0,1,0,1]+eta[0,2,0,3]+3*eta[0,1,1,2]+2*eta[0,0,2,1 ] /;i==sigma&&j==0&&k==sigma & & l==sigma etau[{i_,j_},{k_,l_}]:=eta[0,1,0,1]+eta[0,2,0,3]+3*eta[0,1,1,2]+2*eta[0,0,2,1 ] /;i==sigma&&j==sigma&&k==0 & & l==sigma etau[{i_,j_},{k_,l_}]:=eta[0,1,0,1]+eta[0,2,0,3]+3*eta[0,1,1,2]+2*eta[0,0,2,1 ] /;i==sigma&&j==sigma&&k==sigma & & l==0 etau[{i_,j_},{k_,l_}]:=1+eta[0,2,0,4]+4*eta[0,0,2,2]+2*eta[0,1,0,2 ] + 4*eta[0,0,1,1]+4*eta[0,1,1,3 ] /;i==sigma&&j==sigma&&k==sigma & & l==sigma    eta_{(i , j , k),l } denoted by etau[{i_,j_,k_},l _ ]",
    "etau[{i_,j_,k_},l_]:=eta[1,0,1,0]/;i==0&&j==0 & & k==0 & & l==0 etau[{i_,j_,k_},l_]:=eta[1,0,0,0]+eta[1,0,1,1]/;i==0&&j==0 & & k==0 & & l==sigma etau[{i_,j_,k_},l_]:=2*eta[0,1,1,0]+eta[1,0,1,1]/;i==0&&j==0 & & k==sigma & & l==0 etau[{i_,j_,k_},l_]:=2*eta[0,1,1,0]+eta[1,0,1,1]/;i==0&&j==sigma & & k==0 & & l==0 etau[{i_,j_,k_},l_]:=2*eta[0,1,1,0]+eta[1,0,1,1]/;i==sigma&&j==0 & & k==0 & & l==0 etau[{i_,j_,k_},l_]:=4*eta[0,1,1,1]+2*eta[0,0,2,0]+eta[1,0,1,2 ] /;i==0&&j==sigma & & k==sigma & & l==0 etau[{i_,j_,k_},l_]:=4*eta[0,1,1,1]+2*eta[0,0,2,0]+eta[1,0,1,2 ] /;i==sigma&&j==0&&k==sigma & & l==0 etau[{i_,j_,k_},l_]:=4*eta[0,1,1,1]+2*eta[0,0,2,0]+eta[1,0,1,2 ] /;i==sigma&&j==sigma & & k==0&&l==0 etau[{i_,j_,k_},l_]:=2*eta[0,1,0,0]+eta[1,0,0,1]+2*eta[0,1,1,1]+eta[1,0,1,2 ] /;i==0&&j==0 & & k==sigma & & l==sigma etau[{i_,j_,k_},l_]:=2*eta[0,1,0,0]+eta[1,0,0,1]+2*eta[0,1,1,1]+eta[1,0,1,2 ] /;i==0&&j==sigma & & k==0 & & l==sigma etau[{i_,j_,k_},l_]:=2*eta[0,1,0,0]+eta[1,0,0,1]+2*eta[0,1,1,1]+eta[1,0,1,2 ] /;i==sigma&&j==0 & & k==0 & & l==sigma etau[{i_,j_,k_},l_]:=4*eta[0,1,0,1]+eta[1,0,0,2]+4*eta[0,1,1,2 ] + 2*eta[0,0,2,1]+eta[1,0,1,3]/;i==0&&j==sigma & & k==sigma & & l==sigma etau[{i_,j_,k_},l_]:=4*eta[0,1,0,1]+eta[1,0,0,2]+4*eta[0,1,1,2 ] + 2*eta[0,0,2,1]+eta[1,0,1,3]/;i==sigma&&j==0 & & k==sigma & & l==sigma etau[{i_,j_,k_},l_]:=4*eta[0,1,0,1]+eta[1,0,0,2]+4*eta[0,1,1,2 ] + 2*eta[0,0,2,1]+eta[1,0,1,3]/;i==sigma&&j==sigma & & k==0 & & l==sigma etau[{i_,j_,k_},l_]:=6*eta[0,1,1,2]+6*eta[0,0,2,1]+eta[1,0,1,3 ] /;i==sigma&&j==sigma & & k==sigma & & l==0 etau[{i_,j_,k_},l_]:=2 + 6*eta[0,1,0,2]+6*eta[0,0,1,1]+eta[1,0,0,3 ] + 2*eta[0,0,1,1]+6*eta[0,1,1,3]+6*eta[0,0,2,2]+eta[1,0,1,4 ] /;i==sigma&&j==sigma & & k==sigma & & l==sigma    eta_{(i , j),k , l } denoted by etau[{i_,j_},k_,l _ ]",
    "etau[{i_,j_},k_,l_]:=eta[0,1,2,0]/;i==0&&j==0 & & k==0 & & l==0 etau[{i_,j_},k_,l_]:=eta[0,1,1,0]+eta[0,1,2,1]/;i==0&&j==0 & & k==0 & & l==sigma etau[{i_,j_},k_,l_]:=eta[0,1,1,0]+eta[0,1,2,1]/;i==0&&j==0 & & k==sigma & & l==0 etau[{i_,j_},k_,l_]:=eta[0,1,2,1]+eta[0,0,3,0]/;i==0&&j==sigma & & k==0 & & l==0 etau[{i_,j_},k_,l_]:=eta[0,1,2,1]+eta[0,0,3,0]/;i==sigma&&j==0 & & k==0 & & l==0 etau[{i_,j_},k_,l_]:=eta[0,1,0,0]+2*eta[0,1,1,1]+eta[0,1,2,2 ] /;i==0&&j==0 & & k==sigma & & l==sigma etau[{i_,j_},k_,l_]:=eta[0,1,1,1]+eta[0,0,2,0]+eta[0,1,2,2]+eta[0,0,3,1 ] /;i==0&&j==sigma & & k==0 & & l==sigma etau[{i_,j_},k_,l_]:=eta[0,1,1,1]+eta[0,0,2,0]+eta[0,1,2,2]+eta[0,0,3,1 ] /;i==0&&j==sigma & & k==sigma & & l==0 etau[{i_,j_},k_,l_]:=eta[0,1,1,1]+eta[0,0,2,0]+eta[0,1,2,2]+eta[0,0,3,1 ] /;i==sigma&&j==0 & & k==0 & & l==sigma etau[{i_,j_},k_,l_]:=eta[0,1,1,1]+eta[0,0,2,0]+eta[0,1,2,2]+eta[0,0,3,1 ] /;i==sigma&&j==0 & & k==sigma & & l==0 etau[{i_,j_},k_,l_]:=eta[0,0,2,0]+2*eta[0,0,3,1]+eta[0,1,2,2 ] /;i==sigma&&j==sigma & & k==0 & & l==0 etau[{i_,j_},k_,l_]:=eta[0,1,0,1]+2*eta[0,1,1,2]+2*eta[0,0,2,1 ] + eta[0,1,2,3]+eta[0,0,3,2]/;i==0&&j==sigma & & k==sigma & & l==sigma etau[{i_,j_},k_,l_]:=eta[0,1,0,1]+2*eta[0,1,1,2]+2*eta[0,0,2,1 ] + eta[0,1,2,3]+eta[0,0,3,2]/;i==sigma&&j==0 & & k==sigma & & l==sigma etau[{i_,j_},k_,l_]:=2*eta[0,0,2,1]+eta[0,1,1,2]+eta[0,0,2,1 ] + 2*eta[0,0,3,2]+eta[0,1,2,3]/;i==sigma&&j==sigma & & k==0 & & l==sigma etau[{i_,j_},k_,l_]:=2*eta[0,0,2,1 ] + eta[0,1,1,2]+eta[0,0,2,1]+2*eta[0,0,3,2]+eta[0,1,2,3 ] /;i==sigma&&j==sigma & & k==sigma & & l==0 etau[{i_,j_},k_,l_]:=1 + 4*eta[0,0,1,1]+eta[0,1,0,2]+5*eta[0,0,2,2 ] + 2*eta[0,1,1,3]+2*eta[0,0,3,3]+eta[0,1,2,4 ] /;i==sigma&&j==sigma & & k==sigma & & l==sigma    eta_{i , j , k , l } denoted by etau[i_,j_,k_,l _ ]",
    "etau[i_,j_,k_,l_]:=eta[0,0,4,0]/;i==0&&j==0 & & k==0 & & l==0 etau[i_,j_,k_,l_]:=eta[0,0,3,0]+eta[0,0,4,1]/;i==0&&j==0 & & k==0 & & l==sigma etau[i_,j_,k_,l_]:=eta[0,0,3,0]+eta[0,0,4,1]/;i==0&&j==0 & & k==sigma & & l==0 etau[i_,j_,k_,l_]:=eta[0,0,3,0]+eta[0,0,4,1]/;i==0&&j==sigma & & k==0 & & l==0 etau[i_,j_,k_,l_]:=eta[0,0,3,0]+eta[0,0,4,1]/;i==sigma&&j==0 & & k==0 & & l==0 etau[i_,j_,k_,l_]:=eta[0,0,2,0]+2*eta[0,0,3,1]+eta[0,0,4,2 ] /;i==sigma&&j==sigma & & k==0 & & l==0 etau[i_,j_,k_,l_]:=eta[0,0,2,0]+2*eta[0,0,3,1]+eta[0,0,4,2 ] /;i==sigma&&j==0 & & k==sigma & & l==0 etau[i_,j_,k_,l_]:=eta[0,0,2,0]+2*eta[0,0,3,1]+eta[0,0,4,2 ] /;i==sigma&&j==0 & & k==0 & & l==sigma etau[i_,j_,k_,l_]:=eta[0,0,2,0]+2*eta[0,0,3,1]+eta[0,0,4,2 ] /;i==0&&j==sigma & & k==sigma & & l==0 etau[i_,j_,k_,l_]:=eta[0,0,2,0]+2*eta[0,0,3,1]+eta[0,0,4,2 ] /;i==0&&j==sigma & & k==0&&l==sigma etau[i_,j_,k_,l_]:=eta[0,0,2,0]+2*eta[0,0,3,1]+eta[0,0,4,2 ] /;i==0&&j==0 & & k==sigma & & l==sigma etau[i_,j_,k_,l_]:=3*eta[0,0,2,1]+3*eta[0,0,3,2]+eta[0,0,4,3 ] /;i==0&&j==sigma & & k==sigma & & l==sigma etau[i_,j_,k_,l_]:=3*eta[0,0,2,1]+3*eta[0,0,3,2]+eta[0,0,4,3 ] /;i==sigma&&j==0 & & k==sigma & & l==sigma etau[i_,j_,k_,l_]:=3*eta[0,0,2,1]+3*eta[0,0,3,2]+eta[0,0,4,3 ] /;i==sigma&&j==sigma & & k==0 & & l==sigma etau[i_,j_,k_,l_]:=3*eta[0,0,2,1]+3*eta[0,0,3,2]+eta[0,0,4,3 ] /;i==sigma&&j==sigma & & k==sigma & & l==0 etau[i_,j_,k_,l_]:=1 + 4*eta[0,0,1,1]+6*eta[0,0,2,2]+4*eta[0,0,3,3]+eta[0,0,4,4 ] /;i==sigma&&j==sigma & & k==sigma & & l==sigma    m_{2a}(p ) , m_{2b}(p ) , m_1(p ) denoted respectively by mmu2a[p _ ] , mmu2b[p_],mmu1[p _ ] ( * mmu2a[p_]:=sum[m[i , j , k]^2 , { i , 1,p},{j,1,p},{k,1,p } ] * ) ( * mmu2b[p_]:=sum[m[i , i , k]*m[j , j , k ] , { i , 1,p},{j,1,p},{k,1,p } ] * ) ( * mmu1[p_]:=sum[m[i , i , j , j],{i , 1,p},{j,1,p } ] * ) m_{2a}(p ) , m_{2b}(p ) , m_1(p ) for the special case when $ x$ is homogeneous mmu2a[p_]:=p*m3 ^ 2 + 3*p*(p-1)*m21 ^ 2+p*(p-1)*(p-2)*m111 ^ 2 mmu2b[p_]:=p*m3 ^ 2+p*(p-1)^2*m21 ^ 2 + 2*p*(p-1)*m3*m21 mmu1[p_]:=p*m4+p*(p-1)*m22    l21 , ... , l26 , l11, ... ,l15 denoted by ll21, ...",
    ",ll15 ll21[p_]:=eta[0,0,2,0]^{-3}mmu2a[p]etau[{0,0},0]etau[0,0,0 ] + eta[0,0,2,0]^{-2}p*sum[tgin[s , u]etau[{0,0},s]etau[0,0,u],{s,{0,sigma}},{u,{0,sigma } } ] + eta[0,0,2,0]^{-2}p*sum[tgin[k , l]etau[{0,k},0]etau[0,l,0],{k,{0,sigma}},{l,{0,sigma}}]+ eta[0,0,2,0]^{-2}p*sum[tgin[i , j]etau[{i,0},0]etau[j,0,0],{i,{0,sigma}},{j,{0,sigma}}]+ sum[tgin[i , j]tgin[k , l]tgin[s , u]etau[{i , k},s]etau[j , l , u],{i,{0,sigma}},{j,{0,sigma } } , { k,{0,sigma}},{l,{0,sigma}},{s,{0,sigma}},{u,{0,sigma } } ]    ll22[p_]:=eta[0,0,2,0]^{-3}mmu2b[p]etau[{0,0},0]etau[0,0,0 ] + eta[0,0,2,0]^{-2}p^2*sum[tgin[k , l]etau[{0,0},k]etau[l,0,0],{k,{0,sigma}},{l,{0,sigma } } ] + eta[0,0,2,0]^{-1}p*sum[tgin[k , l]tgin[s , u]etau[{0,0},k]etau[l , s , u],{k,{0,sigma } } , { l,{0,sigma}},{s,{0,sigma}},{u,{0,sigma}}]+eta[0,0,2,0]^{-1}p*sum[tgin[i , j]tgin[k , l]etau[{i , j},k ] etau[l,0,0],{i,{0,sigma}},{j,{0,sigma}},{k,{0,sigma}},{l,{0,sigma}}]+sum[tgin[i , j]tgin[k , l]tgin[s , u ] etau[{i , j},k]etau[l , s , u],{i,{0,sigma}},{j,{0,sigma}},{k,{0,sigma}},{l,{0,sigma}},{s,{0,sigma}},{u,{0,sigma } } ]    ll23[p_]:=eta[0,0,2,0]^{-3}mmu2a[p]etau[0,0,0]etau[0,0,0 ] + eta[0,0,2,0]^{-2}p*sum[tgin[s , u]etau[0,0,s]etau[0,0,u],{s,{0,sigma}},{u,{0,sigma } } ] + eta[0,0,2,0]^{-2}p*sum[tgin[k , l]etau[0,k,0]etau[0,l,0],{k,{0,sigma}},{l,{0,sigma}}]+ eta[0,0,2,0]^{-2}p*sum[tgin[i , j]etau[i,0,0]etau[j,0,0],{i,{0,sigma}},{j,{0,sigma}}]+ sum[tgin[i , j]tgin[k , l]tgin[s , u]etau[i , k , s]etau[j , l , u],{i,{0,sigma}},{j,{0,sigma}},{k,{0,sigma } } , { l,{0,sigma}},{s,{0,sigma}},{u,{0,sigma } } ]    ll24[p_]:=eta[0,0,2,0]^{-3}mmu2b[p]etau[0,0,0]etau[0,0,0 ] + eta[0,0,2,0]^{-2}p^2*sum[tgin[k , l]etau[0,0,k]etau[l,0,0],{k,{0,sigma}},{l,{0,sigma } } ] + eta[0,0,2,0]^{-1}p*sum[tgin[k , l]tgin[s , u]etau[0,0,k]etau[l , s , u],{k,{0,sigma}},{l,{0,sigma } } , { s,{0,sigma}},{u,{0,sigma}}]+eta[0,0,2,0]^{-1}p*sum[tgin[i , j]tgin[k , l]etau[i , j , k]etau[l,0,0 ] , { i,{0,sigma}},{j,{0,sigma}},{k,{0,sigma}},{l,{0,sigma } } ] + sum[tgin[i , j]tgin[k , l]tgin[s , u]etau[i , j , k]etau[l , s , u],{i,{0,sigma}},{j,{0,sigma } } , { k,{0,sigma}},{l,{0,sigma}},{s,{0,sigma}},{u,{0,sigma } } ]    ll25[p_]:=eta[0,0,2,0]^{-3}mmu2a[p]etau[{0,0},0]etau[{0,0},0 ] + eta[0,0,2,0]^{-2}p*sum[tgin[s , u]etau[{0,0},s]etau[{0,0},u],{s,{0,sigma}},{u,{0,sigma } } ] + eta[0,0,2,0]^{-2}p*sum[tgin[k , l]etau[{0,k},0]etau[{0,l},0],{k,{0,sigma}},{l,{0,sigma } } ] + eta[0,0,2,0]^{-2}p*sum[tgin[i , j]etau[{i,0},0]etau[{j,0},0],{i,{0,sigma}},{j,{0,sigma } } ] + sum[tgin[i , j]tgin[k , l]tgin[s , u]etau[{i , k},s]etau[{j , l},u],{i,{0,sigma}},{j,{0,sigma}},{k,{0,sigma } } , { l,{0,sigma}},{s,{0,sigma}},{u,{0,sigma } } ]    ll26[p_]:=eta[0,0,2,0]^{-3}mmu2b[p]etau[{0,0},0]etau[{0,0},0 ] + eta[0,0,2,0]^{-2}p^2*sum[tgin[k , l]etau[{0,0},k]etau[{0,0},l],{k,{0,sigma}},{l,{0,sigma } } ] + eta[0,0,2,0]^{-1}p*sum[tgin[k , l]tgin[s , u]etau[{0,0},k]etau[{s , u},l],{k,{0,sigma } } , { l,{0,sigma}},{s,{0,sigma}},{u,{0,sigma}}]+eta[0,0,2,0]^{-1}p*sum[tgin[i , j]tgin[k , l ] etau[{i , j},k]etau[{0,0},l],{i,{0,sigma}},{j,{0,sigma}},{k,{0,sigma}},{l,{0,sigma } } ] + sum[tgin[i , j]tgin[k , l]tgin[s , u]etau[{i , j},k]etau[{s , u},l],{i,{0,sigma}},{j,{0,sigma } } , { k,{0,sigma}},{l,{0,sigma}},{s,{0,sigma}},{u,{0,sigma } } ]    ll11[p_]:=eta[0,0,2,0]^{-2}mmu1[p]etau[{0,0},0,0 ] + eta[0,0,2,0]^{-1}p*sum[tgin[k , l]etau[{0,l},0,k],{k,{0,sigma}},{l,{0,sigma } } ] + eta[0,0,2,0]^{-1}p*sum[tgin[i , j]etau[{i,0},j,0],{i,{0,sigma}},{j,{0,sigma}}]+sum[tgin[i , j]tgin[k , l]etau[{i , l},j , k ] , { i,{0,sigma}},{j,{0,sigma}},{k,{0,sigma}},{l,{0,sigma } } ]    ll12[p_]:=eta[0,0,2,0]^{-2}mmu1[p]etau[{0,0},{0,0 } ] + eta[0,0,2,0]^{-1}p*sum[tgin[k , l]etau[{0,0},k , l],{k,{0,sigma}},{l,{0,sigma } } ] + eta[0,0,2,0]^{-1}p*sum[tgin[i , j]etau[{i , j},0,0],{i,{0,sigma}},{j,{0,sigma } } ] + sum[tgin[i , j]tgin[k , l]etau[{i , j},k , l],{i,{0,sigma}},{j,{0,sigma}},{k,{0,sigma}},{l,{0,sigma } } ]    ll13[p_]:=eta[0,0,2,0]^{-2}mmu1[p]etau[0,0,0,0 ] + eta[0,0,2,0]^{-1}p*sum[tgin[k , l]etau[0,0,k , l],{k,{0,sigma}},{l,{0,sigma } } ] + eta[0,0,2,0]^{-1}p*sum[tgin[i , j]etau[i , j,0,0],{i,{0,sigma}},{j,{0,sigma } } ] + sum[tgin[i , j]tgin[k , l]etau[i , j , k , l],{i,{0,sigma}},{j,{0,sigma}},{k,{0,sigma}},{l,{0,sigma } } ]    ll14[p_]:=eta[0,0,2,0]^{-2}mmu1[p]etau[{0,0},{0,0 } ] + eta[0,0,2,0]^{-1}p*sum[tgin[k , l]etau[{0,l},{0,k}],{k,{0,sigma}},{l,{0,sigma } } ] + eta[0,0,2,0]^{-1}p*sum[tgin[i , j]etau[{i,0},{j,0}],{i,{0,sigma}},{j,{0,sigma } } ] + sum[tgin[i , j]tgin[k , l]etau[{i , k},{j , l}],{i,{0,sigma}},{j,{0,sigma}},{k,{0,sigma}},{l,{0,sigma } } ]    ll15[p_]:=eta[0,0,2,0]^{-2}mmu1[p]etau[{0,0},{0,0 } ] + eta[0,0,2,0]^{-1}p*sum[tgin[k , l]etau[{0,0},{k , l}],{k,{0,sigma}},{l,{0,sigma } } ] + eta[0,0,2,0]^{-1}p*sum[tgin[i , j]etau[{i , j},{0,0}],{i,{0,sigma}},{j,{0,sigma } } ] + sum[tgin[i , j]tgin[k , l]etau[{i , j},{k , l}],{i,{0,sigma}},{j,{0,sigma}},{k,{0,sigma}},{l,{0,sigma } } ]                    \\overset{\\alpha}{e\\!d } denoted by eedn[a_,n_,p _ ] eedn[a_,n_,p_]:=(p+2)/(2n)+1/(24n^2)*(((1-a)/2)^2 * ( 3ffe[p]+3tt1[p]-6aaem1[p]+6aaee1[p]-3aaem2[p]+3aaee2[p]+3p^2 + 6p ) + ( ( 1-a)/2)*(3ffe[p]-5tt1[p]-6tt2[p]+6aaem1[p]-6aaee1[p]+3aaem2[p]-3aaee2[p]-3p^2 - 6p ) + 12aaee1[p]-2aaem1[p]-aaem2[p]+tt1[p]+9tt2[p]+8rre[p]-9ffe[p ] ) simplify[eedn[-1,n , p ] ] simplify[eedn[1,n , p ] ] simplify[eedn[0,n , p ] ] simplify[eedn[2,n , p ] ] ....",
    "y. sheena .",
    "_ asymptotic expansion of the risk of maximum likelihood estimator with respect to @xmath0-divergence as a measure of the difficulty of specifying a parametric model _ , arxiv:1510.08226 , 2016 ."
  ],
  "abstract_text": [
    "<S> for a regression model , we consider the risk of the maximum likelihood estimator with respect to @xmath0-divergence , which includes the special cases of kullback - leibler divergence , hellinger distance and @xmath1 divergence . </S>",
    "<S> the asymptotic expansion of the risk with respect to the sample size @xmath2 is given up to the order @xmath3 . </S>",
    "<S> we are interested in how the risk convergence speed ( to zero ) is affected by the error term distributions of the regression model and the magnitude of the joint moments of the standardized explanatory variables . </S>",
    "<S> besides the general result ( which is given by mathematica program ) , we consider three concrete error term distributions ; a normal distribution , a t - distribution and a skew - normal distribution . </S>",
    "<S> we use the ( approximated ) risk of m.l.e . as a measure of the difficulty of estimation for the regression model . especially comparing the value of the ( approximated ) risk with that of a binomial distribution </S>",
    "<S> , we can give a certain standard for the sample size required to estimate the regression model .    </S>",
    "<S> msc(2010 ) _ subject classification _ : primary 60f99 ; secondary 62f12 + _ key words and phrases : _ alpha divergence , asymptotic expansion , regression model , asymptotic risk </S>"
  ]
}