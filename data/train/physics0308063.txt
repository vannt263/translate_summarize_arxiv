{
  "article_text": [
    "the question of the sensitivity of a search for new phenomena is a very common one . the need may arise either by the wish to predict the outcome of an experiment and compare several possible experiments or different configurations of the same experiment .",
    "several different ways have been used to quantify the sensitivity of a search , which makes it sometimes difficult to compare them . in particular ,",
    "two different sensitivity figures are often quoted , one that is relative to the potential for actually making a discovery , and another to characterize how strong a constraint is imposed on the unknown phenomena if no evidence is found for a deviation from the standard theory .",
    "this situation makes it difficult to optimize the design of an experiment , because it is not clear what should be maximized .",
    "i describe here a definition of sensitivity which is unique and well - defined for any experiment .",
    "this is based on purely frequentist ideas , which avoids the issue of the choice of an a - priori distribution for a new and unknown phenomena .",
    "the problem of searches for new phenomena can be stated formally in classical statistics as one of  hypothesis testing \" .",
    "we have a  default hypothesis \" @xmath0 , that is our current best theory , and as a result of the experiment we wish to either confirm or disprove the theory @xmath0 , in favor of an alternative theory @xmath1 , where @xmath2 indicates the free parameters of the new theory ( mass or set of masses of new particles , coupling constants , production cross sections , etc . ) .",
    "the experiment consists of measuring the value of a set of observables @xmath3 ( possibly a large number ) whose distribution depends on the true state of nature being @xmath0 or @xmath1 . in a simple counting experiment ,",
    "the observable @xmath3 is the number of observed counts , and hypothesis @xmath0 is defined as the distribution of @xmath3 being a poisson with the mean equal to the number of expected background events @xmath4 .",
    "hypothesis @xmath1 is that the distribution is instead a poisson with a larger mean @xmath5 , where @xmath6 is the expected contribution of the  new signal \" , which is a function of the unknown free parameters of the new theory , @xmath2 .",
    "a test of @xmath0 is specified by defining the set of values of @xmath3 that will make us decide that @xmath0 must be rejected (  critical region \" ) ; the _ significance level _ of the test , indicated by @xmath7 , is the probability of rejecting @xmath0 when it is indeed true ; that is to say , @xmath7 is the probability for @xmath3 to fall within the critical region , calculated under the assumption that @xmath0 is true .",
    "there are many possible choices of the critical region , therefore many possible different tests at the given significance level @xmath7 , and we will not be concerned here with the way the choice is made ; all of the present discussion is independent of the way the test was chosen .    what about the value of @xmath7 ? this is a  small number \" , common practice for really new physics discovery being to require @xmath7 to correspond to the @xmath8 single tail of a gaussian distribution .",
    "the other element to be considered in a test is the probability that a discovery is made .",
    "the classical way to express this is by the _ power function _",
    "@xmath9 , that is , the probability that @xmath3 will fall in the critical region ( = the probability that a discovery will be claimed ) assuming @xmath1 is true , as a function of the parameters @xmath2 .",
    "it is clearly desirable to have the greatest possible power .",
    "however , it is well known that only in very few special problems it is possible to maximize the power simultaneously for every @xmath2 . for this reason ,",
    "trying to optimize the power is subject to a judgement about what values of the parameters are more important ; in the next section we will show how to solve the issue by attacking the problem from a different angle .    after a measurement",
    "is performed , if no discovery is made the experimenter will usually produce an additional piece of information : a confidence region for the unknown parameters @xmath2 .",
    "this part is in principle completely independent from the  testing \" part , and interesting issues arise when one tries to make sure the two kinds of information are coherent .",
    "for instance , limits are often desired at a confidence level lower than the level of significance required for claiming a discovery ; this can lead easily to situations where no discovery is claimed , and yet limits are quoted that do not include the @xmath0 hypothesis . for the purpose of the present discussion we do nt need to deal with such difficult issues and we will make only minimal assumptions about the relationship between the test and the algorithm adopted for setting limits .",
    "we will just assume that the confidence band for @xmath2 be built in such a way to exclude , whenever possible , all values of @xmath3 falling within the acceptance region for @xmath0 ; ( this can be done for every @xmath2 such that @xmath10 , where cl is the desired confidence level ) .",
    "this is quite natural , and usually happens spontaneously , because it makes for tighter confidence regions when no discovery is made , at no expense .",
    "if a discovery is indeed made , the most interesting piece of information in the result will be the discovery itself , and maybe an estimate of the parameters @xmath2 , so we will not be concerned with limit setting in case of discovery , only with the probability that it happens .",
    "many definitions of sensitivity for a search have to do with either the `` average limit '' produced if @xmath0 is true ( defined in various ways ) , or with the significance of an observed signal , assuming the observation is exactly equal to the expected value in presence of a signal at @xmath2 .",
    "we suggest to characterize the sensitivity of an experiment in the following way .",
    "correct statistical practice requires to decide before the experiment the values of @xmath7 and cl , so we assume their values are given",
    ". then one can proceed by quoting the region of the parameters @xmath2 for which _ the power of the chosen test is greater or equal to the confidence level chosen for the limits in case there is no discovery _ :",
    "@xmath11    this region of @xmath2 can be thought of as a region of parameters to which the experiment is  sufficiently sensitive \" . while it is always possible to provide additional information by plotting contours of constant power in the @xmath2 space for values different from the cl , the specific region defined by eq .",
    "( [ eq : sens ] ) is particularly informative because it has a very simple and clear - cut interpretation .",
    "in fact , it is easy to verify that the following two statements hold simultaneously :    * if the true value of @xmath2 satisfies  ( [ eq : sens ] ) , then there is a probability at least @xmath12 that performing the experiment will lead to discovery ( with the chosen significance @xmath7 ) . *",
    "if performing the experiment does not lead to discovery , the resulting limits will exclude ( at least ) the entire region defined by  ( [ eq : sens ] ) , at the chosen cl .",
    "this relies on the minimal assumption of a  reasonable algorithm \" for setting limits made in previous section , and holds independently of the true value of @xmath2 . )    in short , eq .",
    "( [ eq : sens ] ) defines the region in the parameter space for which the experiment will _ certainly _ give an answer : that region will be excluded , or a discovery will be claimed , with no possible in - between . this double discovery / exclusion interpretation suggests that it deserves to be named _",
    "sensitivity region _ for the experiment and to be quoted as the single most useful information to characterize its potential and optimize it .",
    "note explicitly that there is no possibility for an experimental fluctuation to jeopardize the result ; it is possible for a fluctuation to increase the region of exclusion , but not to diminish it . in particular ,",
    "if the parameter region covers the whole range of physically interesting values for @xmath2 , the experiment can very well been said to be conclusive .",
    "this _ sensitivity region _ appears to be a more useful information than others commonly quoted , that have a more vague meaning , like :    * the  average \" excluded region , _ if _",
    "@xmath0 is true ( tells you nothing certain about the actual limits that will be quoted ; tells you nothing about what will happen if the signal exists but it is small ) * an `` average number of sigmas '' , for given values of @xmath2 , or the number of sigmas you would get in case exactly the expected number of signal events is observed ( tells you nothing about the limits in case there is no observation ; tells you little about how likely it is that a signal will actually be observed , due to the effect of statistical fluctuations )    comparison between two experiments , or experimental settings , should be made on the basis of whether one sensitivity region includes the other .",
    "it is still possible for two experiments to be non - comparable , by having none of the two region completely include the other ; in that case , the issue of which is preferable can not be resolved on a statistical basis , but it is a question of strategy .",
    "if the sensitivity regions are very different , the actual conclusion is that the two experiments are somehow ` complementary ' , probing different regions of the parameters space .",
    "there are a few other arguments in favor of quoting this quantity to characterize the sensitivity of an experiment :    * the definition is independent of the choice of metric ( in both observable and parameter space ) .",
    "* it does not require a choice of priors * it is straightforward ( and meaningful ) to apply even in complex situations .",
    "for instance : * * 1-d problems with a  non monotonic \" structure .",
    "example : search for a cp violation effect , where one measures the sine of an angle , with the range @xmath13 $ ] . in this case",
    "@xmath0 is in the middle , and it makes no sense to quote  average upper limit \" . * * multidimensional parameter problems .",
    "examples of this kind are neutrino oscillation searches , where the space is 2-d .",
    "even more complex examples are found in cp - violation measurements in neutral b mesons oscillations , where both a direct and a mixed component are possible ; in this case the allowed region for the parameters is circle of unit radius , @xmath0 being at the center , and it is impossible to use concepts like  average upper limit \" , or even  median of the limit \" . * it is independent of the expectations for a signal to be present , thus allowing an unbiased optimization . *",
    "it allows you to optimize what you really want for a search , without being distracted by other elements .",
    "for instance , if one had to concentrate on getting the maximum possible power ( e.g. by looking at its average it over a chosen region ) , one can easily be fooled into preferring an experiment that has a very high power in a region where the power is pretty high anywyay , over one that has a more even distribution of power , that is actually much more likely to provide useful information , since in a discovery measurement the power counts the most where it is  intermediate \" .",
    "considering the region rather than power in itself takes this into account .",
    "we will now apply the ideas discussed in the previous section to the very common problem of a counting experiment in presence of background . in this case",
    ", we have the discrete observable @xmath14 , the number of events observed , which is poisson - distributed with a mean determined by @xmath4 , the expected number of background events ( supposed known ) , and the possible contribution of signal events @xmath6 : @xmath15 for this problem , the only sensible definition of a critical region for the presence of non - zero signal @xmath6 takes the form of a condition like @xmath16    therefore , the test is completely defined once the desired significance level @xmath7 is chosen .",
    "figure  [ fig : cut ] shows the value of @xmath17 as a function of @xmath4 , for given values of @xmath7 , obtained by numerical calculation of sums of poisson probabilities .",
    "having completely defined the test , we can now evaluate its power as a function of @xmath2 , and determine the set of values for @xmath2 such that eq .",
    "( [ eq : sens ] ) holds . since the power of a test of the form @xmath19 grows monotonically with @xmath6 , it is easy to see that eq .",
    "( [ eq : sens ] ) leads to simple inequalities of the form : @xmath20    therefore , all is needed to completely characterize the solution of our problem is the value of @xmath21 , that is in general a function of @xmath22 , and @xmath4 .",
    "plots of @xmath21 from numerical calculation are shown in fig .",
    "[ fig : smin ] .",
    "tables of this kind of data can in principle be used to compare different experimental settings , by determining for each of them the set of values of @xmath2 such that @xmath23 , and choosing the one with the largest set .",
    "however , it is much easier to perform such optimizations tasks with the help of an analytic parametrization . for the purpose of optimization ,",
    "an approximation of the exact result is usually sufficient ; in particular , there is no need to account for the discretization effects .",
    "a simple parametrization of our result can be obtained by means of gaussian approximation of the poisson .",
    "it is easy to see that in this approximation , condition  ( [ eq : sens ] ) translates into the following equation for @xmath21 : @xmath24          this expression holds for one specific set of data selection criteria .",
    "now consider the common situation where one has to decide on the set of cuts to be used in the analysis .",
    "this means that both the background @xmath4 and the number of expected signal events @xmath6 will depend on the cuts ( let s indicate the whole set of cuts with the symbol @xmath29 ) . in a completely general case , in order to decide which set of cuts @xmath29 is best , one needs to determine for every @xmath29 the set of values @xmath30 to which the experiment is sensitive , by solving for @xmath30 the inequality : @xmath31    and then choose the cuts @xmath29 yielding the most extended region .",
    "the situation is much simpler when the efficiency @xmath32 of the chosen cuts on the signal is indipendent of @xmath2 , that is when one can write : @xmath33          reaches its maximum .",
    "note explicitly that , in the given assumption of the efficiency being independent of @xmath2 , the optimal choice of cuts _ does not depend _ on the assumed cross section for the new process @xmath35 .",
    "this is a very useful feature , since this parameter is often unknown , and it is a direct consequence of the chosen approach , that focuses on maximizing the power where it is really necessary , that is at the threshold of visibility",
    ". expression  ( [ eq : completemax ] ) becomes even simpler when the choice @xmath39 is made : @xmath40        note that expression b ) can not be maximized without knowing explicitly the cross section for the searched signal .",
    "also , it does not quite represent what one wants to maximize for a search , being more directly related to the relative uncertainty in the measurement of the yield of a new process , if found , than to significance . expression",
    "a ) , being linear in @xmath43 , shares with expression ( [ eq : simplemax ] ) the good property of being independent of the cross section of the new process , but it has the important problem of breaking down at small values of @xmath4 . imposing maximization of a ) may push the experiment efficiency down to very small values . in order to see the failure of expression a ) , it is sufficient to consider , for instance , that it will prefer an expectation of @xmath44 signal events with a background of @xmath45 over a situation with @xmath46 signal events expected and a background of @xmath47 event",
    ".    it should be apparent that expression  ( [ eq : simplemax ] ) ( or its slightly more sophisticated form  ( [ eq : completemax ] ) ) , compared with ",
    "significances \" a ) and b ) , is not only better motivated , but also unambiguously preferable from a practical viewpoint .",
    "the features of the discussed formulas are more easily seen by plotting the factor @xmath48 from the exact calculation ( that is proportional to the quantity that needs to be maximized , as in eq .",
    "( [ eq : completemax ] ) ) together with the two significance  like expressions discussed above : they all behave as @xmath49 at large @xmath4 , and it is therefore possible to normalize them to converge as @xmath50 . expression b ) is not simply proportional to @xmath43 , so we had to make a choice and we put @xmath51 , in agreement with the spirit of our current approach of focusing on the point where significance is at the threshold , and solved for @xmath52 to obtain a function of @xmath4 only .",
    "comparison of @xmath48 with the corresponding sensitivity factor given by @xmath53 ( dotted ) and @xmath54 ( dashed ) , for a search experiment with ( significance , cl ) respectively of ( 95%,95% ) , ( @xmath18,95% ) , ( @xmath8,90% ) ]    the comparison is shown in fig .",
    "[ fig : signif ] , where it appears that our suggested solution lies between a ) and b ) , where a ) largely overestimates the  sensitivity \" at low backgrounds , as expected , and conversely b ) underestimates it , expecially for high significance settings .         gaussian approximation of @xmath48 in the @xmath56 approximation ( eq .  ( [ eq : completemax ] ) ) , for a search experiment with ( significance , cl ) respectively of ( 95%,95% ) , ( @xmath18,95% ) , ( @xmath8,90% ) .",
    "curves are normalized to the asymptotic limit . ]",
    "it can be seen that the approximate formulas work well at moderate values of @xmath25 and @xmath26 , but become less accurate when high significance / cl are desired , due to the larger deviations from gaussian behavior that occur in the poisson far tails .",
    "however , the gaussian approximation can easily be improved , without losing the good features of the solutions .",
    "for instance , it is possible to obtain a more accurate expression by accounting for differences between gaussian and poisson tail integrals at the next order in @xmath25 and @xmath26 , simply by performing an empirical fit .",
    "this results in the following improved expression for @xmath21 : @xmath57    fig .  [",
    "fig : sens2 ] shows this slightly modified expression to be considerably accurate even at high significances , which makes it suitable also for searches of  really new \" effects , where a significance level of @xmath8 is a customary requirements .     improved gaussian approximation of the  sensitivity factor \" @xmath48 ( eq .  ( [ eq : improvedgauss ] ) for a search experiment with ( significance , cl ) respectively of ( 95%,95% ) , ( @xmath18,95% ) , ( @xmath8,90% ) ]"
  ],
  "abstract_text": [
    "<S> a frequentist definition of sensitivity of a search for new phenomena is discussed , that has several useful properties . </S>",
    "<S> it is based on completely standard concepts , is generally applicable , and has a very clear interpretation . </S>",
    "<S> it is particularly suitable for optimization , being independent of a - priori expectations about the presence of a signal , thus allowing the determination of a single set of cuts that is optimal both for setting limits and for making a discovery . </S>",
    "<S> simple approximate formulas are given for the common problem of poisson counts with background . </S>"
  ]
}