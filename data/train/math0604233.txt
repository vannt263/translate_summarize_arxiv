{
  "article_text": [
    "semi - supervised classification has been of growing interest over the past few years and many methods have been proposed .",
    "the methods try to give an answer to the question : `` how to improve classification accuracy using unlabeled data together with the labeled data ? '' .",
    "unlabeled data can be used in different ways depending on the assumptions on the model .",
    "there are two types of assumptions .",
    "the first one consists in assuming that we have a set of potential classifiers and we want to aggregate them . in that case",
    ", unlabeled data is used to measure the _ compatibility _ between the classifiers and reduces the complexity of the resulting classifier ( see , e.g. , @xcite , @xcite ) .",
    "the second approach is the one that we use here .",
    "it assumes that the data contains clusters that have homogeneous labels and the unlabeled observations are used to identify these clusters .",
    "this is the so - called _",
    "cluster assumption_. this idea can be put in practice in several ways giving rise to various methods .",
    "the simplest is the one presented here : estimate the clusters , then label each cluster uniformly .",
    "most of these methods use hartigan s @xcite definition of clusters , namely the connected components of the density level sets .",
    "however , they use a parametric ( usually mixture ) model to estimate the underlying density which can be far from reality . moreover , no generalization error bounds are available for such methods . in the same spirit , @xcite and @xcite propose methods that learn a distance using unlabeled data in order to have intra - cluster distances smaller than inter - clusters distances .",
    "the whole family of graph - based methods aims also at using unlabeled data to learn the distances between points .",
    "the edges of the graphs reflect the proximity between points . for a detailed survey on graph methods",
    "we refer to @xcite .",
    "finally , we mention kernel methods , where unlabeled data are used to build the kernel . recalling that the kernel measures proximity between points , such methods can also be viewed as learning a distance using unlabeled data ( see @xcite , @xcite , @xcite ) .",
    "the cluster assumption can be interpreted in another way , i.e. , as the requirement that the decision boundary has to lie in low density regions .",
    "this interpretation has been widely used in learning since it can be used in the design of standard algorithms such as boosting @xcite , @xcite or svm @xcite , @xcite , which are closely related to kernel methods mentioned above . in these algorithms ,",
    "a greater penalization is given to decision boundaries that cross a cluster .",
    "for more details , see , e.g. , @xcite , @xcite , @xcite .",
    "although most methods make , sometimes implicitly , the cluster assumption , no formulation in probabilistic terms has been provided so far . the formulation that we propose in this paper remains very close to its original text formulation and allows to derive generalization error bounds .",
    "we also discuss what can and can not be done using unlabeled data .",
    "one of the conclusions is that considering the whole excess - risk is too ambitious and we need to concentrate on a smaller part of it to observe the improvement of semi - supervised classification over standard classification .",
    "_ outline of the paper .",
    "_ after describing the model , we formulate the cluster assumption and discuss why and how it can improve classification performance in the next section . in section  [ secpop ] , we study the population case when the marginal density @xmath0 is known , to get an idea of our target .",
    "indeed , such a population case corresponds in some way to the case when the amount of unlabeled data is infinite .",
    "section  [ main ] contains the main result : we propose an algorithm for which we derive rates of convergence for the @xmath1-thresholded excess - risk as a measure of performance .",
    "an exemple of consistent density level set estimators is given in section  [ secdlse ] .",
    "section  [ secdisc ] is devoted to discussion on the choice of @xmath1 and possible improvements .",
    "proofs of the results are gathered in section  [ proofs ] .",
    "_ notation .",
    "_ throughout the paper , we denote by @xmath2 positive constants .",
    "we write @xmath3 for the complement of the set @xmath4 . for two sequences @xmath5 and @xmath6 ( in that paper , @xmath0 will be @xmath7 or @xmath8 ) , we write @xmath9 if there exists a constant @xmath10 such that @xmath11 and we write @xmath12 if @xmath13 for some constants @xmath14 . thus ,",
    "if @xmath12 , we have @xmath15 , for any @xmath16 .",
    "let @xmath17 be a random couple with joint distribution @xmath18 , where @xmath19 is a vector of @xmath20 features and @xmath21 is a label indicating the class to which @xmath22 belongs .",
    "the distribution @xmath18 of the random couple @xmath23 is completely determined by the pair @xmath24 where @xmath25 is the marginal distribution of @xmath22 and @xmath26 is the regression function of @xmath27 on @xmath22 , i.e. , @xmath28 .",
    "the goal of classification is to predict the label @xmath27 given the value of @xmath22 , i.e. , to construct a measurable function @xmath29 called a _ classifier_.",
    "the performance of @xmath30 is measured by the average classification error @xmath31 a minimizer of the risk @xmath32 over all classifiers is given by the _",
    "bayes classifier _ @xmath33 , where @xmath34 denotes the indicator function .",
    "assume that we have a sample of @xmath8 observations @xmath35 that are independent copies of @xmath17 .",
    "an empirical classifier is a random function @xmath36 constructed on the basis of the sample @xmath35 .",
    "since @xmath37 is the best possible classifier , we measure the performance of an empirical classifier @xmath38 by its _ excess - risk _ @xmath39 where @xmath40 denotes the expectation with respect to the joint distribution of @xmath41 .",
    "we denote hereafter by @xmath42 the corresponding probability .    in many applications ,",
    "a large amount of unlabeled data is available as well as a small set of labeled data @xmath35 and the goal of semi - supervised classification is to use of the unlabeled data to improve the performance of classifiers .",
    "thus , we observe two independent samples @xmath43 and @xmath44 , where @xmath8 is rather small and typically @xmath45 .",
    "it is well known that in order to make use of the additional unlabeled observations , we have to make an assumption on the dependence between the marginal distribution of @xmath22 and the joint distribution of @xmath17 .",
    "seeger @xcite formulated the rather intuitive _",
    "cluster assumption _ as follows    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ two points @xmath46 should have the same label @xmath47 if there is a path between them which passes only through regions of relatively high @xmath25 . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    this assumption , in its raw formulation can not be exploited in the probabilistic model since @xmath48 the labels are random variables @xmath49 so that the expression `` should have the same label '' is meaningless unless @xmath26 takes values in @xmath50 and @xmath51 it is not clear what `` regions of relatively high @xmath25 '' are . to match the probabilistic framework",
    ", we propose the following modifications    * @xmath52 \\ge p [ y \\neq y ' | x , x ' { \\rm      connected } ] $ ] , where `` connected '' means that there is the path between @xmath22 and @xmath53 which passes only through regions of relatively high @xmath25 . *",
    "define `` regions of relatively high @xmath25 '' in terms of _ density level sets_.    we now need to precise the term _ relatively high density_. assume that @xmath25 admits a density @xmath0 with respect to the lebesgue measure on @xmath54 denoted hereafter by @xmath55 . for a fixed @xmath56 , the @xmath1-level set of the density @xmath0",
    "is defined by @xmath57    we are now in position to give a precise definition of the cluster assumption .    cluster assumption ca(@xmath1 ) : : :    fix @xmath56 and assume that the density level set    @xmath58 has a countable number of    connected components @xmath59 .",
    "then the function    @xmath60 takes a constant value on each of the    @xmath61 .",
    "note that density level sets have the monotonicity property : @xmath62 , implies @xmath63 . in terms of the cluster assumption , it means that when @xmath1 decreases to @xmath64 , the assumption ca(@xmath1 ) becomes more restrictive . as a result , the parameter @xmath1 can be considered as a level of confidence characterizing to which extent the cluster assumption is valid for the distribution @xmath18 and its choice is left to the user . for more details on the choice of @xmath1 ,",
    "see section  [ secdisc ] .",
    "a question remains : what happens outside of the set @xmath65 ?",
    "assume that we are in the problematic case , @xmath66 such that the question makes sense .",
    "since the cluster assumption says nothing about what happens outside of the set @xmath4 , we can only perform supervised classification on @xmath3 .",
    "consider now a classifier @xmath67 built from labeled and unlabeled samples @xmath68 pooled together .",
    "the excess - risk of @xmath67 can be written ( see @xcite ) @xmath69 where @xmath70 denotes the expectation with respect to the pooled sample @xmath68 .",
    "we denote hereafter by @xmath71 the corresponding probability .",
    "since , the unlabeled sample is of no help to classify points in @xmath3 , any reasonable classifier should be based on the sample @xmath72 so that @xmath73 , and we have @xmath74 since we assumed @xmath66 , the rhs of is bounded from below by the optimal rates of convergence that appear in supervised classification .",
    "these rates are typically of the order @xmath75 ( see e.g. @xcite , @xcite , @xcite and @xcite for a comprehensive survey ) .",
    "thus , unlabeled data do not improve the rate of convergence of this part of the excess - risk . to observe the effect of unlabeled data on the rates of convergence",
    ", we have to consider the @xmath1-_thresholded excess - risk _ of a classifier @xmath67 defined by @xmath76 we will therefore focus on this measure of performance .",
    "note that for such a measure , we only need to consider classifiers @xmath67 that are defined on @xmath4 .",
    "we now propose a method to obtain good upper bounds on this quantity , taking advantage of the cluster assumption .",
    "the idea is to estimate the regions where the sign of @xmath77 is constant and make a majority vote on each region .",
    "consider the ideal situation where the density @xmath0 is known and we observe only the labeled sample @xmath78 . fix @xmath56 and assume that @xmath79 has a countable number of connected components : @xmath80 where the @xmath81 are non empty disjoint connected sets . under the cluster assumption ca(@xmath1 ) , the function @xmath82 has constant sign on each @xmath83 .",
    "thus a simple and intuitive method for classification is to perform a majority vote on each @xmath83 .    for any @xmath84 ,",
    "define @xmath85 , @xmath86 by @xmath87 the following assumption characterizes how far is @xmath26 from @xmath88 on every connected component @xmath83 .",
    "global margin assumption gma(@xmath1 ) : : :    there exists @xmath89 such that , for any    @xmath84 , either @xmath90 or    @xmath91 .    since @xmath92",
    ", a direct consequence of the gma is that only a finite number of @xmath93 are positive .",
    "the gma assumption imposes that , on average over @xmath83 , the regression function @xmath26 is away from @xmath88 for any @xmath84 such that @xmath94 .",
    "it describes the global behavior of @xmath26 on each connected component @xmath83 as opposed to the standard margin assumption formulated in @xcite and @xcite which we will call here _ local margin assumption _ ( lma ) .",
    "assumption lma characterizes the local behavior of @xmath26 in a neighborhood of @xmath88 . in @xcite , it is stated as follows    local margin assumption lma : : :    there exist constants @xmath95 and    @xmath96 such that    @xmath97    it is straightforward that when there is only a finite number of connected components @xmath98 with non - zero lebesgue measure , gma is a consequence of lma . however we will see in our analysis that the rates of convergence depend crucially on the value of @xmath89 , @xmath99 , while deriving gma from lma yields a @xmath100 depending on @xmath101 . for this reason ,",
    "it is natural to introduce gma instead of using the well known but less flexible lma .",
    "we now define our classifier based on the sample @xmath72 . for any @xmath84 , define the random variable @xmath102 and denote by @xmath103 the function @xmath104 for all @xmath105 . consider the classifier defined on @xmath4 by @xmath106 the following theorem gives exponential rates of convergence for the classifier @xmath38 under ca(@xmath1 ) .",
    "[ thpop ] fix @xmath56 and assume that ca(@xmath1 ) holds .",
    "then , the classifier @xmath38 satisfies @xmath107 moreover , if gma(@xmath1 ) holds , inequality reduces to @xmath108    a rapid overview of the proof shows that the rate of convergence @xmath109 can not be improved without further assumption",
    ". it will be our target in semi - supervised classification . however , we need estimators of the connected components @xmath110 . in the next section we provide the main result on semi - supervised learning , that is when the density @xmath0 is unknown but we can estimate it using the unlabeled sample @xmath111 .",
    "we now deal with a more realistic case where the density @xmath0 is unknown and so are the density level sets which have to be estimated using the unlabeled sample @xmath112 .",
    "fix @xmath56 and assume that @xmath79 has a countable number of connected components : @xmath80 where the @xmath81 are non empty disjoint connected sets .",
    "assume that the density @xmath0 is uniformly bounded by a constant @xmath113 and that @xmath114 , where @xmath55 denotes the lebesgue measure on @xmath54 .",
    "denote by @xmath115 and @xmath116 respectively the probability and the expectation w.r.t the sample @xmath111 of size @xmath7 .",
    "assume that for any @xmath56 , we use the sample @xmath111 to construct an estimator @xmath117 of @xmath58 satisfying @xmath118 \\to 0 , \\quad   m \\to + \\infty.\\ ] ] we call such estimators _ consistent _ estimators of @xmath4 .",
    "however , the connected components of a consistent estimator of @xmath4 are not in general consistent estimators of the connected components of @xmath4 .",
    "to ensure componentwise consistency , we have to make assumptions on the connected component of @xmath4 and those of @xmath119 .",
    "let @xmath120 be the @xmath20-dimensional closed ball of center @xmath121 and radius @xmath122 , defined by @xmath123 where @xmath124 denotes the euclidean norm in @xmath54 .",
    "fix @xmath125 and @xmath126 .",
    "we say that a set @xmath127 is @xmath128-_connected _ if for any @xmath129 , there exists a continuous map @xmath130 \\to c$ ] such that @xmath131and for any @xmath132 $ ] and any @xmath133 , we have @xmath134 a @xmath64-connected set is simply called connected or pathwise connected .",
    "this definition ensures that @xmath4 has no flat parts which allows to exclude pathological cases such as the one presented on the left of figure  [ figsep ] .",
    "now , define the distance @xmath135 , between two closed connected sets @xmath136 and @xmath137 by @xmath138 we say that a collection of connected sets @xmath139 is @xmath140-_separated _ if @xmath141 for some @xmath142 .",
    "if the connected components of @xmath4 are not @xmath140-separated for some @xmath143 , cases such as the one presented on figure  [ figsep ] ( right ) could arise .",
    "in that case , two connected components and therefore two clusters are identified which is obviously not desirable . therefore , the cluster assumption should not hold for that particular level @xmath1 but it might hold for some @xmath144 .    note that the performance of a density level estimator @xmath145 is measured by the quantity @xmath146={{\\rm i}\\kern-0.18em{\\rm e}}_m\\big[{\\mathrm{leb}_d}(\\hat g_m^c \\cap \\gamma ) \\big]+{{\\rm i}\\kern-0.18em{\\rm e}}_m\\big[{\\mathrm{leb}_d}(\\hat g_m \\cap \\gamma^c ) \\big]\\,.\\ ] ] for some estimators , such as the penalized plug - in density level sets estimators presented in section  [ secdlse ] , we can prove that the dominant term in the rhs of is @xmath147 $ ] .",
    "this ensures that with high probability the estimator @xmath145 is included in @xmath4 .",
    "we now give a precise definition of such estimators .",
    "let @xmath145 be an estimator of @xmath4 and fix @xmath148 .",
    "we say that the estimator @xmath145 is _ consistent from inside _ at rate @xmath149 if it satisfies @xmath150=\\widetilde o ( m^{-\\alpha})\\ ] ] and @xmath151=\\widetilde o ( m^{-2\\alpha})\\ ] ]    for fixed @xmath152 , let @xmath153 be a consistent from inside estimator of @xmath4 at rate @xmath149 .",
    "we begin by clipping @xmath145 in the following manner .",
    "define the set @xmath154 note that @xmath114 yields @xmath155 and therefore the clipped set @xmath156 is also consistent from inside at rate  @xmath149 .",
    "we now use only @xmath157 .",
    "it is straightforward that @xmath157 can be decomposed into a finit number @xmath158 of connected components .",
    "we write for simplicity @xmath159 where @xmath160 depends on @xmath7 and @xmath1 .",
    "denote by @xmath161 , the family of sets such that @xmath162 and @xmath163 , @xmath164 .",
    "it is not hard to see that the sets @xmath165 are uniquely defined from @xmath166 .",
    "let @xmath167 be a subset of @xmath168 .",
    "define @xmath169 and let @xmath170 be the event on which the sets @xmath171 are reduced to singletons @xmath172 which are disjoint , i.e. , @xmath173 in other words , on the event @xmath170 , there is a one - to - one correspondence between the collection @xmath174 and the collection @xmath175 .",
    "componentwise convergence of @xmath157 to @xmath4 , is ensured when @xmath176 has asymptotically overwhelming probability .",
    "the following proposition gives an upper bound on the probability of the complementary of @xmath170 under certain conditions including the finiteness of @xmath167 .",
    "[ propcc ] fix @xmath177 and let @xmath167 be a subset of @xmath178 .",
    "assume that @xmath179 is a @xmath140 separated collection of @xmath128-connected sets .",
    "then , if @xmath180 is an estimator of @xmath4 that is consistent from the inside at rate @xmath149 , we have @xmath181    the @xmath128-connectedness of all @xmath182 and @xmath114 entails that @xmath167 is necessarily finite .",
    "nevertheless , the number of connected components of @xmath4 can be infinite as long as there is only a finite number of them for which @xmath183 .",
    "-connected for any @xmath184 ( left ) and non - separated connected components ( right ) . ]",
    "-connected for any @xmath184 ( left ) and non - separated connected components ( right ) . ]    to estimate the homogeneous regions , we will simply estimate the connected components of @xmath4 .",
    "in addition , when two connected components @xmath83 and @xmath185 are close with respect to the distance @xmath135 , we merge them into the same homogeneous region .    it yields the following pseudo - algorithm .",
    "[ r ]    this method translates into two distinct error terms , one term in @xmath7 and another term in @xmath8 .",
    "we apply our three - step procedure to build a classifier @xmath186 based on the pooled sample @xmath68 .",
    "fix @xmath187 and let @xmath145 be an estimator of the density level set @xmath188 , that is consistent from inside with rate @xmath149 .",
    "for any @xmath189 , define the random variable @xmath190 where @xmath165 is defined in .",
    "denote by @xmath191 the function @xmath192 for all @xmath193 and consider the classifier defined on @xmath194 by @xmath195 note that the classifier @xmath186 assigns the label @xmath64 to any @xmath196 outside of @xmath157 .",
    "this is a notational convention and we can assign any value to @xmath196 on this set since we are only interested in the @xmath1-thresholded excess - risk .",
    "nevertheless , it is more appropriate to assign a label referring to a rejection , e.g. , the values `` 2''or `` r '' ( or any other value different from @xmath50 ) .",
    "the rejection meaning that this point should be classified using labeled data only .",
    "however , when the amount of labeled data is too small , it might be more reasonnable not to classify this point at all .",
    "this modification is of particular interest in the context of classification with a rejection option when the cost of rejection is smaller than the cost of misclassification ( see , e.g. , @xcite ) .",
    "[ mainth ] fix @xmath197 and assume that ca(@xmath1 ) holds . consider an estimator @xmath145 based on @xmath111 that is consistent from inside with rate @xmath149 .",
    "then if the connected components of @xmath65 are @xmath128-connected and @xmath140-separated , the classifier @xmath198 defined in satisfies @xmath199 for any @xmath200 . moreover , if gma(@xmath1 ) holds , inequality reduces to @xmath201    note that , since we often have @xmath202 , the first term in the rhs of and can be considered negligible so that we achieve an exponential rate of convergence in @xmath8 which is almost the same ( up to the constant @xmath203 in the exponent ) as in the case where the density @xmath0 is known .",
    "the constant @xmath203 seems to be natural since it balances the two terms .",
    "fix @xmath56 and recall that our goal is to estimate the connected components @xmath81 , @xmath99 , of @xmath204 , using the unlabeled sample @xmath111 of size @xmath7 .",
    "a simple and intuitive way to achieve this goal is to use _ plug - in estimators _ of @xmath4 defined by @xmath205 where @xmath206 is some estimator of @xmath0 . a straightforward generalization are the _",
    "penalized plug - in estimators _ of @xmath65 , defined by @xmath207 where @xmath208 is a penalization .",
    "clearly @xmath209 .",
    "therefore the connected components of @xmath210 are farther from each other than those of @xmath211 . keeping in mind that we want estimators that are consistent from inside we are going to consider sufficiently large penalization @xmath212 .",
    "plug - in rules have a practical advantage over direct methods such as empirical excess mass maximization ( see , e.g. , @xcite , @xcite , @xcite ) .",
    "once we have an estimator @xmath206 , we can compute the whole collection @xmath213 , which might be of interest for the user who wants to try several values of @xmath1 .",
    "note also that a wide range of density estimators is available in usual software .",
    "a density estimator can be parametric , typically based on a mixture model , or nonparametric such as histograms or kernel density estimators .    for any @xmath214",
    ", a function @xmath215 is said to have @xmath216-exponent at level @xmath1 if there exists a constant @xmath126 such that , for all @xmath217 , @xmath218    it is an analog of the local margin assumption but for arbitrary level @xmath1 in place of @xmath88 .",
    "when @xmath219 it ensures that the function @xmath220 has no flat part at level @xmath1 .",
    "the next theorem gives fast rates of convergence for penalized plug - in rules when @xmath206 satisfies an exponential inequality and @xmath0 has @xmath216-exponent at level @xmath1 .",
    "moreover , it ensures that when the penalization @xmath221 is suitably chosen , the plug - in estimator is consistent from inside .",
    "[ fastrates ] fix @xmath222 and @xmath223 .",
    "let @xmath224 be an estimator of the density @xmath0 such that @xmath225 , @xmath115-almost surely for some positive constant @xmath226 and let @xmath227 be a class of densities on @xmath194 .",
    "assume that there exist positive constants @xmath228 and @xmath229 , such that for @xmath25-almost all @xmath230 , we have @xmath231 assume further that @xmath0 has @xmath216-exponent at level @xmath1 and that the penalty @xmath221 is chosen as @xmath232 then the plug - in estimator @xmath233 is consistent from inside at rate @xmath234 .",
    "consider a kernel density estimator @xmath235 based on the sample @xmath111 defined by @xmath236 where @xmath237 is the bandwidth parameter and @xmath238 is a kernel .",
    "if @xmath0 is assumed to have hlder smoothness parameter @xmath16 and if @xmath239 and @xmath240 are suitably chosen , it is a standard exercise to prove inequality of type with @xmath241 . in that case",
    ", it can be shown that the rate @xmath234 is optimal in a minimax sense .",
    "we proposed a formulation of the cluster assumption in probabilistic terms .",
    "this formulation relies on hartigan s @xcite definition of clusters but it can be modified to match other definitions of clusters in the following way .    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ consider a collection of @xmath128-connected and @xmath140-separated sets ( clusters ) @xmath242",
    ". then the function @xmath243 has constant sign on each @xmath83 .",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    we also proved that there is no hope to improve the classification performance outside of these clusters .",
    "based on these remarks , we defined the @xmath1-thresholded excess - risk which can be easily generalized to the setup of general clusters defined above .",
    "finally we proved that when we have consistent estimators of the clusters , it is possible to achieve exponential rates of convergence for the @xmath1-thresholded excess - risk . the theory developed here can be extended to any definition of clusters as long as they can be consistently estimated .",
    "note that our definition of clusters is parametrized by @xmath1 which is left to the user , depending on his trust in the cluster assumption .",
    "the choice of @xmath1 can be made by fixing @xmath244 , the probability of the rejection region .",
    "we refer to @xcite for more details .",
    "note that data - driven choices of @xmath1 could be easily derived if we impose a condition on the purity of the clusters , i.e. if we are given the @xmath100 in the global margin assumption .",
    "such a choice could be made by decreasing @xmath1 until the level of purity is attained .",
    "however , any data - driven choice of @xmath1 has to be made using the labeled data .",
    "it would therefore yield much worse bounds .",
    "general open problems are : applying the cluster assumption to other definitions of clusters and study the whole excess - risk in the framework of semi - supervised classification with a rejection option .",
    "using the decomposition of @xmath4 into its connected components , we can decompose @xmath245 into @xmath246 fix @xmath247 and assume w.l.o.g . that @xmath248 on @xmath83 .",
    "it yields @xmath249 , and since @xmath38 is also constant on @xmath83 , we get @xmath250 taking expectation @xmath40 on both sides of we get @xmath251\\\\ & \\le 2 \\delta_j { \\textrm{e}}^{-n    \\delta_j^2/2}\\ , , \\end{split}\\ ] ] where we used hoeffding s inequality to get the last inequality .",
    "summing now over @xmath252 yields the theorem .",
    "define @xmath253 .",
    "since the connected components @xmath83 are @xmath128-connected , there is only a finite number @xmath254 of them .",
    "we simply denote @xmath170 by @xmath255 . for any @xmath256 , the @xmath128 connectedness of @xmath83 yields on the one hand , @xmath257=0\\ } & \\subset & \\big\\{{\\mathrm{leb}_d}\\big[\\tilde g_m {   \\mathbin{\\text{\\footnotesize$\\bigtriangleup$}}}\\gamma\\big ] > \\lambda c ( \\log   m)^{-d } \\big\\}\\,,\\\\ a_2(j ) \\triangleq\\{{\\rm card } [ \\kappa(j)]\\ge 2\\ } & \\subset & \\big\\ { { \\mathrm{leb}_d}\\big[\\tilde g_m {   \\mathbin{\\text{\\footnotesize$\\bigtriangleup$}}}\\gamma\\big ] > \\lambda c ( \\log   m)^{-d } \\big\\}\\,.\\end{aligned}\\ ] ]",
    "the previous inclusions are illustrated in figure  [ kappa1 ] .",
    "on the other hand , @xmath265 for some @xmath266 when either @xmath48 @xmath267 s.t .",
    "@xmath268 or @xmath51 @xmath269 s.t . @xmath270 and @xmath271 .",
    "both cases yield the existence of @xmath272 such that @xmath273 for @xmath261",
    ". therefore @xmath274 by construction of @xmath275 , we have @xmath276 .",
    "hence @xmath277 both cases are illustrated in figure  [ kappa2 ] .",
    "now , since @xmath278 we get @xmath279 > \\lambda c ( \\log   m)^{-d } \\big\\}+{{{\\rm i}\\kern-0.18em{\\rm p}}_{\\kern-0.25em m}}\\big\\{{\\mathrm{leb}_d}(\\tilde g_m \\cap \\gamma^c)\\ge   m^{-\\alpha}(\\log m)^{-d } \\big\\}\\,.\\ ] ] using the markov inequality for both terms we obtain @xmath280 >   \\lambda c ( \\log   m)^{-d } \\big\\}= \\widetilde o \\left(m^{-\\alpha } \\right)\\,.\\ ] ] and @xmath281 where we used the fact that @xmath157 is consistent from inside with rate @xmath149 .",
    "it yields the statement of the proposition .",
    "the @xmath1-thresholded excess - risk @xmath282 can be decomposed w.r.t the event @xmath255 and its complement .",
    "it yields @xmath283 + { { { \\rm i}\\kern-0.18em{\\rm p}}_{\\kern-0.25em m}}\\left(d^c \\right ) \\end{split}\\ ] ] we now treat the first term of the rhs of the above inequality , i.e. , on the event @xmath255 .",
    "fix @xmath247 and assume w.l.o.g . that @xmath248 on @xmath83 .",
    "simply write @xmath284 for @xmath285 . by definition of @xmath255",
    ", there is a one - to - one correspondence between the collection @xmath286 and the collection @xmath287 .",
    "we denote by @xmath288 the unique element of @xmath287 such that @xmath289 . on @xmath255 , for any @xmath290 , we have , @xmath291 on the event @xmath255 , for any @xmath200 , it holds @xmath292   \\ge ( 1-\\theta)\\delta_j \\right\\}}\\ , .",
    "\\end{split}\\ ] ] using hoeffding s inequality to control the first term , we get @xmath293   \\ge ( 1-\\theta)\\delta_j \\right\\}}\\,.\\ ] ] taking expectations , and summing over @xmath252 , the @xmath1-thresholded excess - risk is upper bounded by @xmath294 + 2\\sum_{j\\ge 1 } \\delta_j { \\textrm{e}}^{-n ( \\theta \\delta _ j)^2/2 }   + { { { \\rm i}\\kern-0.18em{\\rm p}}_{\\kern-0.25em m}}\\left(d^c \\right)\\ , , \\end{split}\\ ] ] where we used the fact that on @xmath255 , @xmath295 \\le { \\mathrm{leb}_d}\\big[\\gamma {   \\mathbin{\\text{\\footnotesize$\\bigtriangleup$}}}\\tilde g_m \\big ] \\,.\\ ] ] from proposition  [ propcc ] , we have @xmath296 and @xmath297=\\widetilde o \\left(m^{-\\alpha } \\right)$ ] and the theorem is proved .",
    "recall that @xmath298 we begin by the first term .",
    "we have @xmath299 the fubini theorem yields @xmath300 \\le { \\mathrm{leb}_d}({\\mathcal{x}})\\sup_{x \\in { \\mathcal{x } } } { { { \\rm i}\\kern-0.18em{\\rm p}}_{\\kern-0.25em m}}\\left [    |\\hat p_m(x ) - p(x)| \\ge \\ell   \\right ] \\le c_{3 } { \\textrm{e}}^{-c_{2}m^a\\ell^2}\\,,\\ ] ] where the last inequality is obtained using and @xmath301 .",
    "taking @xmath221 as in yields for @xmath302 , @xmath303",
    "\\le c_3m^{-\\gamma a}.\\ ] ] we now prove that @xmath304 = \\widetilde o \\big(m^{-\\frac{\\gamma      a}{2}}\\big)$ ] .",
    "consider the following decomposition where we drop the dependence in @xmath196 for notational convenience , @xmath305 where @xmath306 and @xmath307 using and in the same fashion as above we get @xmath308=\\widetilde o \\big(m^{-\\frac{\\gamma      a}{2}}\\big)$ ] .",
    "the term corresponding to @xmath309 is controlled using the @xmath216-exponent of density @xmath0 at level @xmath1 .",
    "indeed , we have @xmath310 the previous upper bounds for @xmath311 and @xmath309 together with yield the consistency from inside ."
  ],
  "abstract_text": [
    "<S> we consider semi - supervised classification when part of the available data is unlabeled . </S>",
    "<S> these unlabeled data can be useful for the classification problem when we make an assumption relating the behavior of the regression function to that of the marginal distribution . </S>",
    "<S> seeger @xcite proposed the well - known _ </S>",
    "<S> cluster assumption _ as a reasonable one . </S>",
    "<S> we propose a mathematical formulation of this assumption and a method based on density level sets estimation that takes advantage of it to achieve fast rates of convergence both in the number of unlabeled examples and the number of labeled examples .    </S>",
    "<S> * key words : * semi - supervised learning , statistical learning theory , classification , cluster assumption , generalization bounds . </S>"
  ]
}