{
  "article_text": [
    "how did the universe begin ? how do we understand the shape of the present - day cosmic web ?",
    "within standard cosmology , we have an observationally well - supported model for the initial conditions ( ics )  a gaussian random field  and the evolution and growth of cosmic structures is well - understood in principle .",
    "it is therefore natural to analyze large - scale structure ( lss ) surveys in terms of the simultaneous constraints they place on the statistical properties of the initial conditions of the universe and on the shape of the cosmic web . due to the computational challenge and to the lack of detailed physical understanding of the non - gaussian and non - linear processes that link galaxy formation to the large - scale dark matter distribution , the current state of the art of statistical analyses of lss surveys is far from this ideal and these problems are addressed in isolation . here , we describe progress towards the full reconstruction of four - dimensional states and illustrate the use of these results for cosmic web classification .",
    "cosmological observations are subject to a variety of intrinsic and experimental uncertainties ( incomplete observations   survey geometry and selection effects   , cosmic variance , noise , biases , systematic effects ) , which make the inference of signals a fundamentally ill - posed problem . for this reason ,",
    "no unique recovery of the initial conditions and of the shape of the present - day cosmic web is possible ; it is more relevant to quantify a probability distribution for such signals , given the observations . adopting this point of view for large - scale structure surveys , bayesian forward modeling ( gravitational structure formation is the generative model for the complex final state , starting from a simple initial state ",
    "gaussian or nearly - gaussian ics ) offers a conceptual basis for dealing with the problem of inference in presence of uncertainty ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    ".      statistical analysis of lss surveys requires to go from the few parameters describing the homogeneous universe to a point - by - point characterization of the inhomogeneous universe .",
    "the latter description typically involves tens of millions of parameters : the density in each voxel of the survey volume .",
    "no obvious reduction of the problem size exists . ",
    "curse of dimensionality \" phenomena @xcite are therefore the significant obstacle in this high - dimensional data analysis problem .",
    "they refer to the problems caused by the exponential increase in volume associated with adding extra dimensions to a mathematical space , and therefore in sparsity given a fixed amount of sampling points .",
    "numerical representations of high - dimensional probability distribution functions ( pdfs ) will tend to have very peaked features and narrow support , which means that traditional sampling methods will fail .",
    "however , gradients of these functions carry capital information , as they indicate the direction to high - density regions , permitting fast travel through a very large volume in parameter space .",
    "the hamiltonian monte carlo algorithm @xcite is an algorithm for exploring parameter spaces with particles ( samples ) .",
    "the general idea is to use classical mechanics to solve statistical problems .",
    "the algorithm interprets the negative logarithm of the pdf to sample from , @xmath0 , as a potential , @xmath1 , and integrates hamilton s equation in parameter space .",
    "due to the conservation of energy in classical mechanics , the theoretical acceptance rate is always unity .",
    "therefore , hmc beats the  curse of dimensionality \" by exploiting gradients ( @xmath2 in hamilton s equations ) and using conserved quantities .",
    "the full - scale bayesian inference code borg ( bayesian origin reconstruction from galaxies , @xcite ) uses hmc for four - dimensional inference of density fields in the linear and mildly non - linear regime .",
    "the ( approximate ) physical model for gravitational dynamics included in the likelihood is second - order lagrangian perturbation theory ( 2lpt ) , linking initial density fields ( at a scale factor @xmath3 ) to the presently observed large - scale structure ( at @xmath4 ) .",
    "the galaxy distribution is modeled as a poisson sample from these evolved density fields .",
    "the algorithm also accounts for luminosity dependent galaxy biases @xcite . in @xcite , we apply the borg code to 463,230 galaxies from the ` sample dr72 ` of the new york university value added catalogue ( nyu - vagc , @xcite ) , based ot the final data release ( dr7 ) of the sloan digital sky survey ( sdss , @xcite ) .",
    "each inferred sample ( fig .",
    "[ fig : borg ] , left ) is a  possible version of the truth \" in the form of a full physical realization of dark matter particles .",
    "the variation between samples ( fig .",
    "[ fig : borg ] , right ) quantifies joint and correlated uncertainties inherent to any cosmological observation and accounts for all non - linearities and non - gaussianities involved .          building upon these results , it is possible to post - process the samples using fully non - linear dynamics as an additional filtering step @xcite .",
    "we generate a set of data - constrained realizations of the present large - scale structure : some samples of inferred initial conditions are evolved with 2lpt to @xmath5 , then with a fully non - linear cosmological simulation ( using gadget-2 ) from @xmath5 to @xmath6 .",
    "this filtering step yields a much more precise view of the deeply non - linear regime of cosmic structure formation , sharpening overdense , virialized structures and resolving more finely the substructure of voids ( fig .",
    "[ fig : filter ] ) .",
    "-body filtering of a borg sample ( left ) , to produce a non - linear data - constrained realization of the redshift - zero large - scale structure ( right).,scaledwidth=60.0% ]",
    "the results presented in @xmath7 [ sec : borg_sdss ] form the basis of the analysis of @xcite , where we classify the cosmic large scale structure into four distinct web - types ( voids , sheets , filaments and clusters ) and quantify corresponding uncertainties .",
    "we follow the dynamic cosmic web classification procedure proposed by @xcite , based on the eigenvalues @xmath8 of the tidal tensor @xmath9 , hessian of the rescaled gravitational potential : @xmath10 , where @xmath11 follows the poisson equation ( @xmath12 ) .",
    "a voxel is in a cluster ( resp . in a filament , in a sheet , in a void )",
    "if three ( resp . two , one , zero ) of the @xmath13s are positive .    by applying this classification procedure to all density samples ,",
    "we are able to estimate the posterior of the four different web - types , conditional on the observations .",
    "the means of these pdfs are represented in fig .",
    "[ fig : ts ] .          in @xcite",
    ", we apply computational geometry tools ( vide : the void identification and examination pipeline , @xcite ) to the constrained parts of the non - linear realizations described in @xmath7 [ sec : filter ] .",
    "we find physical cosmic voids in the field traced by the dark matter particles , probing a level deeper in the mass distribution hierarchy than galaxies . due to the high density of tracers , we find about an order of magnitude more voids at all scales than the voids directly traced by the sdss galaxies . in this fashion",
    ", we circumvent the issues due to the conjugate and intricate effects of sparsity and biasing on galaxy void catalogs @xcite and drastically reduce the statistical uncertainty . for usual void statistics ( number count , radial density profiles , ellipticities ) , all the results we obtain are consistent with @xmath14-body simulations prepared with the same setup .",
    "we thank jacopo chevallard , jens jasche , nico hamaus , guilhem lavaux , emilio romano - daz , paul sutter and alice pisani for a fruitful collaboration on the projects presented here .",
    "fl acknowledges funding from an amx grant ( cole polytechnique ) and bw from a senior excellence chair by the agence nationale de la recherche ( anr-10-cexc-004 - 01 ) .",
    "this work made in the ilp labex ( anr-10-labx-63 ) was supported by french state funds managed by the anr within the investissements davenir programme ( anr-11-idex-0004 - 02 ) ."
  ],
  "abstract_text": [
    "<S> we describe an innovative statistical approach for the _ ab initio _ simultaneous analysis of the formation history and morphology of the large - scale structure of the inhomogeneous universe . </S>",
    "<S> our algorithm explores the joint posterior distribution of the many millions of parameters involved via efficient hamiltonian markov chain monte carlo sampling . </S>",
    "<S> we describe its application to the sloan digital sky survey data release 7 and an additional non - linear filtering step . </S>",
    "<S> we illustrate the use of our findings for cosmic web analysis : identification of structures via tidal shear analysis and inference of dark matter voids . </S>"
  ]
}