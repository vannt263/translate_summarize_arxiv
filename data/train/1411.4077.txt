{
  "article_text": [
    "synaptic plasticity is believed to be the fundamental building block of learning and memory in the brain .",
    "its study is of crucial importance to understanding the activity and function of neural circuits . with innovations in neural recording technology providing access to the simultaneous activity of increasingly large populations of neurons , statistical models are promising tools for formulating and testing hypotheses about the dynamics of synaptic connectivity .",
    "advances in optical techniques @xcite , for example , have made it possible to simultaneously record from and stimulate large populations of synaptically connected neurons .",
    "armed with statistical tools capable of inferring time - varying synaptic connectivity , neuroscientists could test competing models of synaptic plasticity , discover new learning rules at the monosynaptic and network level , investigate the effects of disease on synaptic plasticity , and potentially design stimuli to modify neural networks .",
    "despite the popularity of glms for spike data , relatively little work has attempted to model the time - varying nature of neural interactions . here",
    "we model interaction weights as a dynamical system governed by parametric synaptic plasticity rules . to perform inference in this model ,",
    "we use particle markov chain monte carlo ( pmcmc ) @xcite , a recently developed inference technique for complex time series .",
    "we use this new modeling framework to examine the problem of using recorded data to distinguish between proposed variants of spike - timing - dependent plasticity ( stdp ) learning rules .",
    "the glm is a probabilistic model that considers spike trains to be realizations from a point process with conditional rate  @xmath0 @xcite . from a biophysical perspective",
    ", we interpret this rate as a nonlinear function of the cell s membrane potential .",
    "when the membrane potential exceeds the spiking threshold potential of the cell ,  @xmath0 rises to reflect the rate of the cell s spiking , and when the membrane potential decreases below the spiking threshold ,  @xmath0 decays to zero .",
    "the membrane potential is modeled as the sum of three terms : a linear function of the stimulus ,  @xmath1 , for example a low - pass filtered input current , the sum of excitatory and inhibitory psps induced by presynaptic neurons , and a constant background rate . in a network of @xmath2 neurons , let  @xmath3}$ ] be the set of observed spike times for neuron  @xmath4 , where  @xmath5 is the duration of the recording and  @xmath6 is the number of spikes . the conditional firing rate of a neuron  @xmath4 can be written , @xmath7 \\right),\\end{aligned}\\ ] ] where  @xmath8 is the background rate , the second term is a convolution of the ( potentially vector - valued ) stimulus with a linear stimulus filter ,  @xmath9 , and the third is a linear summation of impulse responses ,  @xmath10 , which preceding spikes on neuron  @xmath11 induce on the membrane potential of neuron  @xmath4 .",
    "finally , the rectifying nonlinearity  @xmath12 converts this linear function of stimulus and spike history into a nonnegative rate .",
    "while the spiking threshold potential is not explicitly modeled in this framework , it is implicitly inferred in the amplitude of the impulse responses .    5.25 in ,  @xmath13 , and  @xmath14 .",
    "the corresponding weights of these synapses are strengthening over time ( darker entries in  @xmath15 ) , leading to larger impulse responses in the firing rates and a greater number of induced post - synaptic spikes ( black dots ) , as shown below.,title=\"fig : \" ]    from this semi - biophysical perspective it is clear that one shortcoming of the standard glm is that it does not account for time - varying connectivity , despite decades of research showing that changes in synaptic weight occur over a variety of time scales and are the basis of many fundamental cognitive processes .",
    "this absence is due , in part , to the fact that this direct biophysical interpretation is not warranted in most traditional experimental regimes , e.g. , in multi - electrode array ( mea ) recordings where electrodes are relatively far apart .",
    "however , as high resolution optical recordings grow in popularity , this assumption must be revisited ; this is a central motivation for the present model .",
    "there have been a few efforts to incorporate dynamics into the glm .",
    "@xcite extended the glm to take inter - spike intervals as a covariates and formulated a generalized bilinear model for weights .",
    "@xcite modeled the time - varying parameters of a glm using a dynamic bayesian network ( dbn ) .",
    "however , neither of these approaches accommodate the breadth of synaptic plasticity rules present in the literature .",
    "for example , parametric stdp models with hard bounds on the synaptic weight are not congruent with the convex optimization techniques used by @xcite , nor are they naturally expressed in a dbn .",
    "here we model time - varying synaptic weights as a potentially nonlinear dynamical system and perform inference using particle mcmc .",
    "nonstationary , or time - varying , models of synaptic weights have also been studied outside the context of glms .",
    "for example , @xcite applied hidden switching linear dynamical systems models to neural recordings .",
    "this approach has many merits , especially in traditional mea recordings where synaptic connections are less likely and nonlinear dynamics are not necessarily warranted .",
    "outside the realm of computational neuroscience and spike train analysis , there exist a number of dynamic statistical models , such as @xcite , which explored dynamic generalized linear models .",
    "however , the types of models we are interested in for studying synaptic plasticity are characterized by domain - specific transition models and sparsity structure , and until recently , the tools for effectively performing inference in these models have been limited .",
    "in order to capture the time - varying nature of synaptic weights , we extend the standard glm by first factoring the impulse responses in the firing rate of equation  [ eqn : glm_rate ] into a product of three terms : @xmath16here ,  @xmath17 is a binary random variable indicating the presence of a direct synapse from neuron  @xmath11 to neuron  @xmath4 ,  @xmath18\\to \\reals}$ ] is a non stationary synaptic `` weight '' trajectory associated with the synapse , and  @xmath19 is a nonnegative , normalized impulse response , i.e.  @xmath20 .",
    "requiring  @xmath21 to be normalized gives meaning to the synaptic weights : otherwise  @xmath22 would only be defined up to a scaling factor . for simplicity ,",
    "we assume  @xmath23 does not change over time , that is , only the amplitude and not the duration of the psps are time - varying .",
    "this restriction could be adapted in future work .",
    "as is often done in glms , we model the normalized impulse responses as a linear combination of basis functions . in order to enforce the normalization of  @xmath24",
    ", however , we use a _ convex _ combination of normalized , nonnegative basis functions .",
    "that is , @xmath25 where  @xmath26 and @xmath27 .",
    "the same approach is used to model the stimulus filters ,  @xmath28 , but without the normalization and non - negativity constraints .",
    "the binary random variables  @xmath29 , which can be collected into an  @xmath30 binary matrix  @xmath31 , model the connectivity of the synaptic network .",
    "similarly , the collection of weight trajectories @xmath32 , which we will collectively refer to as  @xmath33 , model the time - varying synaptic weights .",
    "this factorization is often called a _",
    "spike - and - slab _",
    "prior @xcite , and it allows us to separate our prior beliefs about the structure of the synaptic network from those about the evolution of synaptic weights .",
    "for example , in the most general case we might leverage a variety of random network models @xcite as prior distributions for  @xmath31 , but here we limit ourselves to the simplest network model , the erds - renyi model . under this model , each  @xmath29 is an independent identically distributed bernoulli random variable with sparsity parameter  @xmath34 .",
    "figure  [ fig : model_illustration ] illustrates how the adjacency matrix and the time - varying weights are integrated into the glm . here",
    ", a four - neuron network is connected via a chain of excitatory synapses , and the synapses strengthen over time due to an stdp rule .",
    "this is evidenced by the increasing amplitude of the impulse responses in the firing rates . with larger synaptic weights",
    "comes an increased probability of postsynaptic spikes , shown as black dots in the figure . in order to model the dynamics of the time - varying synaptic weights , we turn to a rich literature on synaptic plasticity and learning rules .",
    "decades of research on synapses and learning rules have yielded a plethora of models for the evolution of synaptic weights @xcite . in most cases ,",
    "this evolution can be written as a dynamical system , @xmath35 where  @xmath36 is a potentially nonlinear _ learning rule _ that determines how synaptic weights change as a function of previous spiking .",
    "this framework encompasses rate - based rules such as the oja rule @xcite and timing - based rules such as stdp and its variants .",
    "the additive noise ,  @xmath37 , need not be gaussian , and many models require truncated noise distributions .    following biological intuition ,",
    "many common learning rules factor into a product of simpler functions .",
    "for example , stdp ( defined below ) updates each synapse independently such that @xmath38 only depends on  @xmath39 and the presynaptic spike history  @xmath40 . biologically speaking",
    ", this means that plasticity is local to the synapse .",
    "more sophisticated rules allow dependencies among the columns of  @xmath15 .",
    "for example , the incoming weights to neuron  @xmath4 may depend upon one another through normalization , as in the oja rule @xcite , which scales synapse strength according to the total strength of incoming synapses . extensive research in the last fifteen years has identified the relative spike timing between the pre- and postsynaptic neurons as a key component of synaptic plasticity , among other factors such as mean firing rate and dendritic depolarization @xcite .",
    "stdp is therefore one of the most prominent learning rules in the literature today , with a number of proposed variants based on cell type and biological plausibility . in the experiments to follow",
    ", we will make use of two of these proposed variants .",
    "first , consider the canonical stdp rule with a ",
    "double - exponential \" function parameterized by @xmath41 , @xmath42 , @xmath43 , and @xmath44 @xcite , in which the effect of a given pair of pre - synaptic and post - synaptic spikes on a weight may be written : @xmath45\\ , \\ell_+(\\mathcal{s}_{n ' } ; a_+ , \\tau_+ )    \\;-\\;\\bbi[t \\in \\mathcal{s}_{n'}]\\ , \\ell_-(\\mathcal{s}_{n } ; a_- , \\tau_-),\\end{aligned}\\ ] ] @xmath46 this rule states that weight changes only occur at the time of pre- or post - synaptic spikes , and that the magnitude of the change is a nonlinear function of interspike intervals . a slightly more complicated model known as",
    "the multiplicative stdp rule extends this by bounding the weights above and below by  @xmath47 and  @xmath48 , respectively @xcite .",
    "then , the magnitude of the weight update is scaled by the distance from the threshold : @xmath49\\ ;   \\tilde\\ell_+(\\mathcal{s}_{n ' } ; a_+ , \\tau_+ ) \\;(w_{\\mathsf{max}}-w_{n'\\to n}(t ) ) , \\nonumber \\\\    \\;-\\ ; & \\bbi[t \\in \\mathcal{s}_{n'}]\\ ;   \\tilde\\ell_-(\\mathcal{s}_{n } ; a_- , \\tau_-)\\;(w_{n'\\to n}(t)-w_{\\mathsf{min}}).\\end{aligned}\\ ] ] here , by setting @xmath50 , we enforce that the synaptic weights always fall within  @xmath51}$ ] . with this rule , it often makes sense to set @xmath48 to zero .",
    "similarly , we can construct an additive , bounded model which is identical to the standard additive stdp model except that weights are thresholded at a minimum and maximum value . in this model ,",
    "the weight never exceeds its set lower and upper bounds , but unlike the multiplicative stdp rule , the proposed weight update is independent of the current weight except at the boundaries . likewise , whereas with the canonical stdp model it is sensible to use gaussian noise for  @xmath52 in the bounded multiplicative model we use truncated gaussian noise to respect the hard upper and lower bounds on the weights . note that this noise is dependent upon the current weight ,  @xmath53 .",
    "the nonlinear nature of this rule , which arises from the multiplicative interactions among the parameters ,  @xmath54 , combined with the potentially non - gaussian noise models , pose substantial challenges for inference",
    ". however , the computational cost of these detailed models is counterbalanced by dramatic expansions in the flexibility of the model and the incorporation of _ a priori _ knowledge of synaptic plasticity .",
    "these learning models can be interpreted as strong regularizers of models that would otherwise be highly underdetermined , as there are  @xmath55 weight trajectories and only  @xmath2 spike trains . in the next section",
    "we will leverage powerful new techniques for bayesian inference in order to capitalize on these expressive models of synaptic plasticity .",
    "the traditional approach to inference in the standard glm is penalized maximum likelihood estimation .",
    "the log likelihood of a single conditional poisson process is well known to be , @xmath56 and the log likelihood of a population of non - interacting spike trains is simply the sum of each of the log likelihoods for each neuron .",
    "the likelihood depends upon the parameters  @xmath57 through the definition of the rate function given in equation  [ eqn : glm_rate ] . for some link functions",
    "@xmath58 , the log likelihood is a concave function of  @xmath59 , and the mle can be found using efficient optimization techniques .",
    "certain dynamical models , namely linear gaussian latent state space models , also support efficient inference via point process filtering techniques @xcite .    due to the potentially nonlinear and non - gaussian nature of stdp , these existing techniques are not applicable here .",
    "instead we use particle mcmc  @xcite , a powerful technique for inference in time series .",
    "particle mcmc samples the posterior distribution over weight trajectories ,  @xmath33 , the adjacency matrix  @xmath31 , and the model parameters  @xmath59 and  @xmath60 , given the observed spike trains , by combining particle filtering with mcmc .",
    "we represent the conditional distribution over weight trajectories with a set of discrete particles .",
    "let the instantaneous weights at ( discretized ) time  @xmath61 be represented by a set of  @xmath62 particles ,  @xmath63 .",
    "the particles live in  @xmath64 and are assigned normalized _ _ particle weights _ _ ,  @xmath65 , which approximate the true distribution via  @xmath66 .",
    "particle filtering is a method of inferring a distribution over weight trajectories by iteratively propagating forward in time and reweighting according to how well the new samples explain the data . for each particle  @xmath67 at time  @xmath61",
    ", we propagate forward one time step using the learning rule to obtain a particle  @xmath68 .",
    "then , using equation  [ eqn : glm_lkhd ] , we evaluate the log likelihood of the spikes that occurred in the window  @xmath69 and update the weights .",
    "since some of these particles may have very low weights , after each step we resample the particles . after the  @xmath5-th time step we are left with a set of weight trajectories  @xmath70 ,",
    "each associated with a particle weight  @xmath65 .",
    "particle filtering only yields a distribution over weight trajectories , and implicitly assumes that the other parameters have been specified .",
    "particle mcmc provides a broader inference algorithm for both weights and other parameters .",
    "the idea is to interleave _ conditional _ particle filtering steps that sample the weight trajectory given the current model parameters and the previously sampled weights , with traditional gibbs updates to sample the model parameters given the current weight trajectory .",
    "this combination leaves the stationary distribution of the markov chain invariant and allows joint inference over weights and parameters .",
    "gibbs updates for the remaining model parameters , including those of the learning rule , are described in the supplementary material .    2.4 in   [ fig : fig3_static_trajectory ]    1.45 in   [ fig",
    ": fig3_static_stdp_rule ]    1.45 in   [ fig : fig3_static_pred_ll ]     +    2.4 in   [ fig : fig3_add_nothr_trajectory ]    1.45 in   [ fig : fig3_add_nothr_stdp_rule ]    1.45 in   [ fig : fig3_add_nothr_pred_ll ]     +    2.4 in   [ fig : fig3_mult_trajectory ]    1.45 in   [ fig : fig3_mult_stdp_rule ]    1.45 in   [ fig : fig3_mult_pred_ll ]     +        [ [ collapsed - sampling - of - ba - and - bwt ] ] collapsed sampling of @xmath31 and @xmath33 + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    in addition to sampling of weight trajectories and model parameters , particle mcmc approximates the marginal likelihood of entries in the adjacency matrix ,  @xmath31 , integrating out the corresponding weight trajectory .",
    "we have , up to a constant , @xmath71 \\pr(a_{n'\\to n}),\\end{aligned}\\ ] ] where  @xmath72 indicates all entries except for  @xmath73 , and the particle weights are obtained by running a particle filter for each assignment of  @xmath29 .",
    "this allows us to jointly sample  @xmath74 and  @xmath75 by first sampling  @xmath76 and then  @xmath75 given  @xmath76 . by marginalizing out the weight trajectory , our algorithm is able to explore the space of adjacency matrices more efficiently .",
    "we capitalize on a number of other opportunities for computational efficiency as well .",
    "for example , if the learning rule factors into independent updates for each  @xmath77 , then we can update each synapse s weight trajectory separately and reduce the particles to one - dimensional objects . in our implementation , we also make use of a pmcmc variant with ancestor sampling  @xcite that significantly improves convergence .",
    "any distribution may be used to propagate the particles forward ; using the learning rule is simply the easiest to implement and understand .",
    "we have omitted a number of details in this description ; for a thorough overview of particle mcmc , the reader should consult @xcite .",
    "we evaluated our technique with two types of synthetic data .",
    "first , we generated data from our model , with known ground - truth .",
    "second , we used the well - known simulator neuron to simulate driven , interconnected populations of neurons undergoing synaptic plasticity . for comparison ,",
    "we show how the sparse , time - varying glm compares to a standard glm with a group lasso prior on the impulse response coefficients for which we can perform efficient map estimation .      as a proof of concept ,",
    "we study a single synapse undergoing a variety of synaptic plasticity rules and generating spikes according to a glm . the neurons also have inhibitory self - connections to mimic refractory effects .",
    "we tested three synaptic plasticity mechanisms : a static synapse ( i.e. , no plasticity ) , the unbounded , additive stdp rule given by equation  [ eqn : addstdp ] , and the bounded , multiplicative stdp rule given by equation  [ eqn : multstdp ] . for each learning rule",
    ", we simulated 60 seconds of spiking activity at 1khz temporal resolution , updating the synaptic weights every 1s .",
    "the baseline firing rates were normally distributed with mean  @xmath78hz and standard deviation of  @xmath79hz .",
    "correlations in the spike timing led to changes in the synaptic weight trajectories that we could detect with our inference algorithm .    2 in   [ fig : fig5_roc_network_1 ]    3 in   [ fig :",
    "fig5_fp_analysis ]     +        figure  [ fig : glm_pairs_inference ] shows the true and inferred weight trajectories , the inferred learning rules , and the predictive log likelihood on ten seconds of held out data for each of the three ground truth learning rules . when the underlying weights are static ( top row ) , map estimation and static learning rules do an excellent job of detecting the true weight whereas the two time - varying models must compensate by either setting the learning rule as close to zero as possible , as the additive stdp does , or setting the threshold such that the weight trajectory is nearly constant , as the multiplicative model does .",
    "note that the scales of the additive and multiplicative learning rules are not directly comparable since the weight updates in the multiplicative case are modulated by how close the weight is to the threshold . when the underlying weights vary ( middle and bottom rows )",
    ", the static models must compromise with an intermediate weight .",
    "though the stdp models are both able to capture the qualitative trends , the correct model yields a better fit and better predictive power in both cases .    in terms of computational cost ,",
    "our approach is clearly more expensive than alternative approaches based on map estimation or mle .",
    "we developed a parallel implementation of our algorithm to capitalize on conditional independencies across neurons , i.e. for the additive and multiplicative stdp rules we can sample the weights  @xmath80 independently of the weights  @xmath81 . on the two neuron examples we achieve upward of 2 iterations per second ( sampling all variables in the model ) , and",
    "we run our model for 1000 iterations .",
    "convergence of the markov chain is assessed by analyzing the log posterior of the samples , and typically stabilizes after a few hundred iterations .",
    "as we scale to networks of ten neurons , our running time quickly increases to roughly 20 seconds per iteration , which is mostly dominated by slice sampling the learning rule parameters . in order to evaluate the conditional probability of a learning rule parameter , we need to sample the weight trajectories for each synapse .",
    "though these running times are nontrivial , they are not prohibitive for networks that are realistically obtainable for optical study of synaptic plasticity .      2.4 in , a sparse , time - varying glm can capture the weight trajectories and learning rules from spike trains simulated by neuron . here",
    "an excitatory synapse undergoes additive stdp with a hard upper bound on the excitatory postsynaptic current .",
    "the weight trajectory inferred by our model with the same parametric form of the learning rule matches almost exactly , whereas the static models must compromise in order to capture early and late stages of the data , and the multiplicative weight exhibits qualitatively different trajectories .",
    "nevertheless , in terms of predictive log likelihood , we do not have enough information to correctly determine the underlying learning rule .",
    "potential solutions are discussed in the main text.,title=\"fig : \" ] [ fig : fig4_add_nothr_trajectory ]    1.45 in , a sparse , time - varying glm can capture the weight trajectories and learning rules from spike trains simulated by neuron .",
    "here an excitatory synapse undergoes additive stdp with a hard upper bound on the excitatory postsynaptic current .",
    "the weight trajectory inferred by our model with the same parametric form of the learning rule matches almost exactly , whereas the static models must compromise in order to capture early and late stages of the data , and the multiplicative weight exhibits qualitatively different trajectories .",
    "nevertheless , in terms of predictive log likelihood , we do not have enough information to correctly determine the underlying learning rule .",
    "potential solutions are discussed in the main text.,title=\"fig : \" ] [ fig : fig4_add_nothr_stdp_rule ]    1.45 in , a sparse , time - varying glm can capture the weight trajectories and learning rules from spike trains simulated by neuron . here",
    "an excitatory synapse undergoes additive stdp with a hard upper bound on the excitatory postsynaptic current .",
    "the weight trajectory inferred by our model with the same parametric form of the learning rule matches almost exactly , whereas the static models must compromise in order to capture early and late stages of the data , and the multiplicative weight exhibits qualitatively different trajectories .",
    "nevertheless , in terms of predictive log likelihood , we do not have enough information to correctly determine the underlying learning rule .",
    "potential solutions are discussed in the main text.,title=\"fig : \" ] [ fig : fig4_add_nothr_pred_ll ]     +    , a sparse , time - varying glm can capture the weight trajectories and learning rules from spike trains simulated by neuron . here",
    "an excitatory synapse undergoes additive stdp with a hard upper bound on the excitatory postsynaptic current .",
    "the weight trajectory inferred by our model with the same parametric form of the learning rule matches almost exactly , whereas the static models must compromise in order to capture early and late stages of the data , and the multiplicative weight exhibits qualitatively different trajectories . nevertheless , in terms of predictive log likelihood",
    ", we do not have enough information to correctly determine the underlying learning rule .",
    "potential solutions are discussed in the main text . ]    using the biophysical simulator neuron , we performed two experiments .",
    "first , we considered a network of 10 sparsely interconnected neurons ( 28 excitatory synapses ) undergoing synaptic plasticity according to an additive stdp rule .",
    "each neuron was driven independently by a hidden population of 13 excitatory neurons and 5 inhibitory neurons connected to the visible neuron with probability 0.8 and fixed synaptic weights averaging 3.0 mv .",
    "the visible synapses were initialized close to 6.0 mv and allowed to vary between 0.0 and 10.5 mv .",
    "the synaptic delay was fixed at 1.0 ms for all synapses .",
    "this yielded a mean firing rate of 10 hz among visible neurons .",
    "synaptic weights were recorded every 1.0 ms .",
    "these parameters were chosen to demonstrate interesting variations in synaptic strength , and as we transition to biological applications it will be necessary to evaluate the sensitivity of the model to these parameters and the appropriate regimes for the circuits under study .",
    "we began by investigating whether the model is able to accurately identify synapses from spikes , or whether it is confounded by spurious correlations .",
    "figure  [ fig : pynn_roc ] shows that our approach identifies the 28 excitatory synapses in our network , as measured by roc curve ( add .",
    "stdp auc=@xmath82 ) , and outperforms static models and cross - correlation . in the sparse , time - varying glm ,",
    "the probability of an edge is measured by the mean of  @xmath31 under the posterior , whereas in the standard glm with map estimation , the likelihood of an edge is measured by area under the impulse response .",
    "looking into the synapses that are detected by the time - varying model and missed by the static model , we find an interesting pattern .",
    "the improved performance comes from synapses that decay in strength over the recording period .",
    "three examples of these synaptic weight trajectories are shown in the right panel of figure  [ fig : pynn_roc ] .",
    "the time - varying model assigns over 90% probability to each of the three synapses , whereas the static model infers less than a 40% probability for each synapse .    finally , we investigated our model s ability to distinguish various learning rules by looking at a single synapse , analogous to the experiment performed on data from the glm .",
    "figure  [ fig : pynn_pairs_inference ] shows the results of a weight trajectory for a synapse under additive stdp with a strict threshold on the excitatory postsynaptic current .",
    "the time - varying glm with an additive model captures the same trajectory , as shown in the left panel .",
    "the glm weights have been linearly rescaled to align with the true weights , which are measured in millivolts .",
    "furthermore , the inferred additive stdp learning rule , in particular the time constants and relative amplitudes , perfectly match the true learning rule .",
    "these results demonstrate that a sparse , time - varying glm is capable of discovering synaptic weight trajectories , but in terms of predictive likelihood , we still have insufficient evidence to distinguish additive and multiplicative stdp rules . by the end of the training period ,",
    "the weights have saturated at a level that almost surely induces postsynaptic spikes . at this point , we can not distinguish two learning rules which have both reached saturation .",
    "this motivates further studies that leverage this probabilistic model in an optimal experimental design framework , similar to recent work by @xcite , in order to conclusively test hypotheses about synaptic plasticity .",
    "motivated by the advent of optical tools for interrogating networks of synaptically connected neurons , which make it possible to study synaptic plasticity in novel ways , we have extended the glm to model a sparse , time - varying synaptic network , and introduced a fully - bayesian inference algorithm built upon particle mcmc .",
    "our initial results suggest that it is possible to infer weight trajectories for a variety of biologically plausible learning rules .",
    "a number of interesting questions remain as we look to apply these methods to biological recordings .",
    "we have assumed access to precise spike times , though extracting spike times from optical recordings poses inferential challenges of its own .",
    "solutions like those of @xcite could be incorporated into our probabilistic model .",
    "computationally , particle mcmc could be replaced with stochastic em to achieve improved performance @xcite , and optimal experimental design could aid in the exploration of stimuli to distinguish between learning rules . beyond these direct extensions ,",
    "this work opens up potential to infer latent state spaces with potentially nonlinear dynamics and non - gaussian noise , and to infer learning rules at the synaptic or even the network level .",
    "this work was partially funded by darpa yfa n66001 - 12 - 1 - 4219 and nsf iis-1421780 .",
    "s.w.l . was supported by an ndseg fellowship and by the nsf center for brains , minds , and machines .",
    "the main text describes the core of the inference algorithm for sampling the weights ,  @xmath33 , and the adjacency matrix ,  @xmath31 .",
    "there are a number of other parameters that we infer as well , as described here .",
    "recall that the impulse responses are modeled as , @xmath84 where  @xmath26 and  @xmath85 is the parameter of a symmetric dirichlet distribution .",
    "we sample the impulse response coefficients ,  @xmath86 , using hamiltonian monte carlo . to avoid boundary constraints",
    ", we use the `` expanded - mean '' parameterization described in  @xcite .",
    "specifically , we let , @xmath87 and  @xmath88 .",
    "our impulse response basis vectors ,  @xmath89 consist of  @xmath90 rectified cosine tuning curves , as described in  @xcite .",
    "the learning rules themselves also possess parameters , e.g. , the amplitude of the stdp update ,  @xmath91 .",
    "one of the benefits of particle mcmc is that each iteration yields samples of the weight trajectories . given these trajectories , it is generally straightforward to employ gibbs sampling on the parameters of the learning rule .",
    "the conditional probability of  @xmath60 is a function of how much the current weight trajectory differs from that predicted by a learning rule with parameters  @xmath60 .",
    "we place gamma priors on the nonnegative parameters ,  @xmath92 , @xmath43 , @xmath42 , and  @xmath41 .",
    "we use shape parameters  @xmath93 and rate parameters of 50 , 150 , 100 , and 100 , respectively ( time constants are measured in seconds ) .",
    "we restrict the weight boundaries such that  @xmath94 and  @xmath95 , and place gamma priors on these as well .",
    "for the neuron data , which consists of purely excitatory connections , we set  @xmath96 and  @xmath97 .",
    "we sample the conditional distributions using slice sampling . in theory ,",
    "particle marginal metropolis hastings updates @xcite may yield improved convergence , for example when there are strong dependencies between the current weight trajectory and the weight bounds , but in practice we find that slice sampling is sufficient for our purposes .",
    "though weights between neurons may change as a result of activity , it is less clear that self weights in the glm , which effectively implement refractoriness , should change . in our simulations",
    ", we set a self - inhibitory prior on the self weights ,  @xmath99 . for most typical choices of nonlinearities ,  @xmath100 , specifically those which are both convex and log concave",
    ", the conditional distribution of  @xmath101 will be log concave if its prior is .",
    "this condition is met by a gaussian prior , and renders the conditional distribution amenable to adaptive rejection sampling ( ars ) . furthermore , if we wish to sample the presence or absence of a self connection  @xmath102 , then under a gaussian prior we may use a joint approach as we do with the time varying weights . here",
    ", the marginal probability of an edge may be approximated using gauss - hermite quadrature .",
    "then , the weights may be sampled using ars , where the abscissae of the quadrature may seed the hull of the conditional distribution .      under typical choices of nonlinearity ,  @xmath58 , and under a log concave prior",
    ", the conditional distribution of  @xmath8 is log concave and amenable to adaptive rejection sampling . in practice",
    ", however , we opt for hamiltonian monte carlo , as with the parameters of the impulse responses .",
    "our inference algorithm was implemented in python and built upon the theano framework for automatic differentiation and compilation to c or gpu kernels .",
    "the code may be found at  https://github.com/slinderman/pyglm .",
    "though we have opted for a fully - bayesian approach , a particle saem approach could be used instead and may offer substantial improvements in runtime while yielding similar results @xcite ."
  ],
  "abstract_text": [
    "<S> learning and memory in the brain are implemented by complex , time - varying changes in neural circuitry . </S>",
    "<S> the computational rules according to which synaptic weights change over time are the subject of much research , and are not precisely understood . until recently , </S>",
    "<S> limitations in experimental methods have made it challenging to test hypotheses about synaptic plasticity on a large scale . however , as such data become available and these barriers are lifted , it becomes necessary to develop analysis techniques to validate plasticity models . </S>",
    "<S> here , we present a highly extensible framework for modeling arbitrary synaptic plasticity rules on spike train data in populations of interconnected neurons . </S>",
    "<S> we treat synaptic weights as a ( potentially nonlinear ) dynamical system embedded in a fully - bayesian generalized linear model ( glm ) . </S>",
    "<S> in addition , we provide an algorithm for inferring synaptic weight trajectories alongside the parameters of the glm and of the learning rules . using this method </S>",
    "<S> , we perform model comparison of two proposed variants of the well - known spike - timing - dependent plasticity ( stdp ) rule , where nonlinear effects play a substantial role . on synthetic data generated from the biophysical simulator neuron </S>",
    "<S> , we show that we can recover the weight trajectories , the pattern of connectivity , and the underlying learning rules .    ,    , </S>"
  ]
}