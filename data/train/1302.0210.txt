{
  "article_text": [
    "over the past few years , deadline - aware services such as web search , online social network , advertising / recommendation system , data warehouse , and online retail systems have been rapidly emerging .",
    "datacenters , as critical computing platforms for ever - growing , high - revenue , must fulfill the requirements of deadline - aware services .",
    "those requirements include : ( 1 ) most of deadline - aware services require responsiveness in the sub - second time scale at high request rates , because the shorter and in - time application latency is the key for satisfying user requirement .",
    "this is why soft real - time constraints are used for deadline - aware services .",
    "( 2 ) many of those services employ the partition - aggregate operations to parallelly process the massive data sets , which are distributed on thousands of servers , in a divide - and - conquer manner@xcite . those operations might generate all - to - all and all - to - one communication patterns ( e.g. , mapreduce@xcite ) at the same time . hence , the datacenter network must be robust enough to cope with those rapid traffic patterns .",
    "recent works on datacenter networks@xcite has shown that the key issues of current datacenters are the tree - based infrastructure and the traditional tcp - like transport protocol .",
    "the first issue is that the tree - based infrastructure can not meet the requirements of high - performance large - scale datacenters , which may consist of thousands to millions of servers@xcite ; and these requirements include : ( 1 ) high interconnection bandwidth , i.e. , provide multiple disjoin path between servers , ( 2 ) low - cost interconnection structure , i.e. , only use commodity switches without changes , and ( 3 ) low cabling complexity .",
    "we note , recently , some server - centric topology designs , e.g. , bcube@xcite , and mdcube@xcite , have been proposed to meet those requirements .    as for the second issue , the tcp shares the network resources in a laissez - faire manner , which leads to many problems such as tcp incast@xcite , low bandwidth utilization when multiple paths exist@xcite , and missing deadline@xcite problems .",
    "many literatures have been studied for coping with the problems of traditional tcp .",
    "for example , dctcp@xcite and ictcp@xcite are proposed to mitigate tcp incast problems .",
    "mptcp@xcite is proposed to well utilize multipath infrastructure .",
    "hull@xcite trades a little bandwidth to maintain low network latency for soft real - time services .",
    "d@xmath0@xcite directly take flow deadline into account .",
    "d@xmath0 proactively suspend some flows to guarantee the remaining flows can meet deadline so as to provide better goodput , i.e. , application - level throughput where the flows successfully meet their deadline . with those newly proposed infrastructures and protocol designs ,",
    "the aforementioned two requirements of deadline - aware services are investigated and fulfilled the robustness of the data center network and the soft real - time constraints .    however , we notice that the robustness of the network infrastructure and the soft real - time constraints of responses can not totally fulfill the requirements of deadline - aware services . according to recently studies@xcite ,",
    "the users most concern about is the first few results of deadline - aware services .",
    "that is , for the deadline - aware services , the quality of responses , i.e. the precision of top - k results , may significantly affect the degree of user satisfaction on usage experience , and hence this might have great impact on the revenue of the business .",
    "this also means that the high goodput , which can be provided by existing deadline - aware protocols@xcite , is not able to guarantee to get high - quality results for users .",
    "for example , in figure [ fig : ex3p ] , four flows are traveling through a bottleneck switch at the same time , and the number in the flows indicates the rank of response units within the flow .",
    "figure [ fig : ex3p](a ) shows that the bottleneck bandwidth can not let all flows pass through and meet their deadlines .",
    "figure [ fig : ex3p](b ) illustrates the behavior of d@xmath0 , which allows requests to be passed through the data center in a first - come first - serve ( fcfs ) manner .",
    "since the bottleneck bandwidth could support only three flows , one of the flows will be dropped to allow other flows to meet deadline . if the flow with responses of rank 1 , 3 , and 4 is dropped , the results will has poor quality for a user .",
    "figure [ fig : ex3p](c ) demonstrates an example of our heuristic protocol , only several low - rank response units , i.e. , rank 14 , 15 , and 16 , are dropped and all high - rank response units are delivered before the deadline .",
    "that is , although the network bottleneck exists , the deadline - aware services can still provide high quality results to users .",
    "hence , we believe how to provide high - rank results for users is an important issue toward high - quality deadline - aware services ; and , our ideas for design of an importance - aware datacenter network are as follows :    1 .   application - level importance is associated with each response unit , not flows , while deadline is associated with flows .",
    "the flows ( and the response units within it ) are valid only when they arrive before the deadline .",
    "2 .   the flow size may vary from less than 10 kb to more than 500 kb because of the different applications .",
    "that is , the protocol must be aware of the size of the flows as well .",
    "3 .   some of the flows are short and so is their deadline .",
    "in addition , the rtt is small in a datacenter .",
    "so , the reaction time of the delivery control protocol must be short enough to cope with those short and near - deadline flows .",
    "that is , centralized or heavy - weight mechanisms are impractical .",
    "the application - level importance of all applications in a datacenter must be normalized to provide a fair - sharing network .",
    "however , this can be measured and normalized manually or automatically by datacenter administrators .",
    "in this work , we exploit the application - level content information , i.e. , data importance , to meet the most important requirement , i.e. , the precision of top - k results , which affects user experience on deadline - aware services , to figure out the optimal solution and performance upper bound , we first model the datacenter network as a flow network for analyzing the application importance maximization problem with the constraints of network link capacity , flow conservation , and flow deadline .",
    "however , due to the complexity of the optimal solution , we do not consider it can be put into practice .",
    "therefore , we further propose a novel distributed rate - based importance - aware delivery control protocol .",
    "the basic idea of our protocol is to greedily choose a set of the flows in the networks that can contribute the most importance before deadline under the constraint of the network capacity .",
    "the information of flows travelling on a switch , i.e. , the deadline , remain size , averaged data importance in the flow , sending rate of each flow , is known by all servers connected to the switch at the initiation stage of the flows . hence , each server can estimate if its upcoming flow will contribute more data importance than existing flows by a new proposed metric jointly considering remain size , remain transmission time , and averaged data importance of a flow , such that we can suspend less - important flows and replace them with new upcoming flow . to utilize the multiple disjoin paths while maximize application data importance , we choose the combination of paths and rates that affects less data importance in all available disjoin paths to the destination .",
    "notice that the deadline of flows and traffic amount in a datacenter vary over time , and this means some of the suspended flows may be recovery later for them , which can still meet their deadlines .",
    "hence , once a flow has completed , the source of the flow will inform the owners of the suspended flows to see if any of them can recover flows before their deadline . to further improve the delivery ratio of highly important response units , we use statistical clustering algorithm , i.e. , k - mean algorithm , to cluster responses of a flow into two ( or more ) small flows based on the importance of responses .",
    "that is , most of highly important response units of any requests will be delivery in deadline while less - important responses will be ignored for improving user perceived service quality . in summary , the advantages of the proposed protocol include :    1 .",
    "we design the proposed protocol in a fully distributed manner , i.e. , no centralized controller is required .",
    "the proposed protocol adaptively adopts the most beneficial flows on the fly and recovers less - important flows when network resources are available .",
    "we split the original flow into two ( or more ) small flows by statistical clustering algorithm , i.e. the k - means algorithm , for further improving the successful delivery ratio of important response units .",
    "the proposed protocol is able to make use of multiple disjoin links and balances the load between links for enhancing the throughput .",
    "no expensive or customized hardware is required , i.e. , only commodity switches without changes are used .",
    "the results based on real data show that the proposed algorithm can provide better application - level user perceived performance than mptcp@xcite and d@xmath0@xcite , i.e. the precision at k. that is , we believe our protocol can better fulfill the requirements of deadline - aware services for providing better user experience , and hence producing more revenue for the providers .",
    "in this section , we formulate the application importance maximization problem for the optimal solution , which also means to find the maximal sum of the importance of flows in the network which meet deadline .",
    "we first model the importance - aware datacenter networks and formulate the problem .",
    "the complexity of the optimal solution in server - centric networks is then addressed . in summary",
    ", we found that the problem can not be solved in polynomial time .",
    "therefore , we present a distributed heuristic protocol based on similar ideas of the optimal solution .      we consider a data center network is represented by g=(v , e ) , where @xmath1 represents a set of nodes , i.e. , servers or switches , and @xmath2 represents the links between nodes . the bandwidth capacity of each link @xmath3",
    "is @xmath4 .",
    "there are @xmath5 flows , i.e. , @xmath6 , in the data center network , and each flow has its source @xmath7 , destination @xmath8 , begin time @xmath9 , deadline @xmath10 , and a set of response units @xmath11 .",
    "for each response @xmath12 in @xmath11 of flow @xmath13 , the importance of @xmath12 , i.e. , the similarity to the query of @xmath11 , is denoted as @xmath14 , and the size of each response @xmath12 is denoted as @xmath15 .",
    "therefore , flow size @xmath16 is equal to the sum of the response unit size of @xmath11 .",
    "we then denote the averaged importance of @xmath13 as @xmath17=@xmath18 .",
    "consequently , we regard the flow in the importance - aware network as a five - tuple vector , i.e. , @xmath13=(@xmath7 , @xmath8 , @xmath10 , @xmath16 , @xmath17 ) . the transmission rate of @xmath13 on link @xmath19 at time @xmath20",
    "is represented by @xmath21 .      in a data center network ,",
    "once a flow can not meet its deadline , all response units of that flow are regarded as lost .",
    "this property makes how to maximize application importance becomes difficult .",
    "hence , we split each flow into several tiny flows , and each tiny flow contains a single response unit .",
    "that is , the new flow set @xmath22 consists of @xmath23 flows .",
    "since each new flow only contains one smallest unit that contribute importance , we do not allow them to be split anymore to guarantee they can be completely delivered .",
    "the new flows can also be defined in the form of five tuples as : @xmath13=(@xmath7 , @xmath8 , @xmath10 , @xmath24 , @xmath14 ) .",
    "assume the end time of the flow @xmath13 is @xmath25 , the first begin time of the flows is @xmath26 , the last end time of the flows is @xmath27 , and the transmission and queueing delay of flow @xmath13 is @xmath28 , we can define the application importance maximization problem as : @xmath29 @xmath30 subject to :    1 .",
    "link capacity constraint : for each link @xmath19 at time @xmath20 , the total flow rate on the link must be equal to or smaller than the link capacity .",
    "@xmath31 2 .",
    "flow conservation constraint : the amount of in - flow and out - flow must be the same on all nodes unless the source or the destination of the flow is the node .",
    "we notice that the application importance maximization problem can be reduced to an unsplittable flow problem . in the unsplittable flow problem ,",
    "a @xmath33-vertex graph @xmath34 with edge capacity @xmath35 and the set of @xmath36 vertex pairs @xmath37 are given .",
    "each pair ( @xmath38 ) in @xmath39 has a demand @xmath40 and a weight @xmath41 .",
    "the goal attempts to find the subset of pairs from @xmath39 with the maximum weight so that entire demand for each of such pair can be routed on its path under the link capacity constraint .",
    "now we suppose all flows in the network has the same begin time and deadline , which represents a simplified case of the maximization problem .",
    "we can map the set of @xmath22 flows to the set of @xmath36 vertex pairs , the minimal averaged transmission rate @xmath42 that meets its deadline to the demand @xmath40 , and the importance of the flow @xmath14 to the weight @xmath41 , so as to convert this application importance maximization problem to the unsplittable flow problem .",
    "that is , the result set of the unsplittable flow problem denotes the flows that maximize the application importance .",
    "this also represents that for the flows whose demand can not be satisfied in the unsplittable flow problem , they should be dropped in advance because they will degrade the sum of the demand , i.e. , the sum of application importance .",
    "however , the unsplittable flow problem is regarded as np - complete@xcite , and solving the unsplittable flow problem needs global information , i.e. , a centralized system is required .",
    "those limitations make this solution become impractical for datacenter networks .",
    "one of the critical problem in the unsplittable flow problem is to find the path for flow @xmath13 that meets its demand , i.e. , the minimal averaged transmission rate .",
    "it is difficult to solve because in a datacenter network , many intersecting path may cross with each other on nodes in a network . finding those link - joint path",
    "makes the allocation of capacity to each flow become complicated .",
    "we note that the server - centric network architectures not only have many advantages over traditional network architectures ; and , one of their properties is to provide fixed number of link - disjoin paths between servers .",
    "this property can help us on reducing the complexity of the application importance maximization problem , and we will explain the details as follows .",
    "our basic idea is to allow each server to determine whether it can start a new flow by traversing through the disjoin paths to the destination .",
    "if ( a ) the smallest remain capacity of the paths is insufficient to meet deadline , and ( b ) the importance of the new flow is less than any other flows on the paths , the flow will be suspended till there are sufficient available capacity to meet its deadline .",
    "if the new flow can contribute more importance than existing flows , it will notify the less - important flows to be suspended and starts transmission .",
    "that is , each server can decides if a flow should be transmitted according to the partial information of the network .",
    "after the disjoin paths are given , we notice that the applications importance maximization problem can be reduced to a multiple knapsack problem .",
    "we first map the available link capacity @xmath4 of link @xmath19 to the capacity of knapsack @xmath43 .",
    "the flow @xmath13 is mapped to the item @xmath44 that are going to be place in the knapsack .",
    "the importance of the flow @xmath13 , @xmath14 , is mapped to the value of the item as @xmath45 .",
    "the minimal averaged transmission rate @xmath42 that meets its deadline is mapped to the weight of the item @xmath41 .",
    "we then define the set of disjoin paths between @xmath7 and @xmath8 that flow @xmath13 will pass through as @xmath46 .",
    "this set of links @xmath46 can be treated as a set of knapsack sets @xmath47 of flow @xmath13 .",
    "a knapsack set of @xmath47 consists of knapsack @xmath48 , which represents the available capacity of a disjoin hop - by - hop path from @xmath7 to @xmath8 and all edges on this path are denoted as @xmath49 for latter use .",
    "we use @xmath50 to denote this knapsack set and use smallest available capacity of the knapsack in the set as the capacity @xmath51 .",
    "once a item @xmath44 is placed in a knapsack set @xmath50 , we denote it as @xmath52 , and at the same time the available capacity of all knapsacks in the set will be reduced by @xmath41 .",
    "therefore , assume there are @xmath5 flows sent from the source node , we can reduce the local application importance maximization problem as follows :    @xmath53    subject to @xmath54 @xmath55 @xmath56    the multiple knapsack problem , however , is known as a np - complete problem@xcite so it requires very long computational time while the flows are used to be short and massive .",
    "even if the information of all upcoming flows are known , the problem can be solved in pseudo - polynomial time ; but , this is still impractical in datacenter networks .",
    "in this section , we describe our heuristic importance - aware delivery protocol for server - centric datacenter networks .",
    "the design goals of our protocol include :    1 .",
    "maximize application importance , i.e. , providing high quality results for deadline - aware services by finishing most important flows before deadline .",
    "light - weight and fully distributed , i.e. , fast enough to cope with datacenter networks and no need of customized or expensive hardware .",
    "high network utilization , i.e. , exploiting all available static multi - disjoin paths of server - centric networks .",
    "the key insight guiding our protocol design is : given ( a ) the topology information , ( b ) the size and importance of each response unit in a flow , as well as ( c ) the deadline of a flow , each server in the server - centric networks can determine ( 1 ) which part of the flow is important , ( 2 ) the rate to meet the deadline constraint .",
    "the servers , then , can determine if the rate should be allocated according to the importance of the flow and the network status of the paths to the destination .      before introducing our protocol",
    ", we must address a fundamental issue : how to determine whether a flow is important ? if all flows begin at the same time and they have the same deadline and size , the answer is quite simple .",
    "we use @xmath17 of flow @xmath57to decide if it can contribute more importance than other flows . however ,",
    "in the datacenter network , the flows come or leave all the time , and their deadline , size , and importance are different .",
    "thus , we need a new metric to determine the importance of a flow .",
    "our idea is to measure the carried importance per data unit per time unit .",
    "this is because after some transmission time , the remaining size of a flow will become small .",
    "however , only when the remaining part of the flow is fully delivered , the importance of the flow can be count in .",
    "that is , the metric must take the remaining size and remaining time of the flow into account in by considering temporal and spatial diversity together .",
    "hence , we imposes the averaged flow importance @xmath17 , remaining size @xmath58 , and remaining transmission time @xmath59 to compute the flow importance contribution ( fic ) of flow @xmath13 .",
    "the fic of flow @xmath13 is defined as :    @xmath60      we first give a overview of the protocol .",
    "our protocol is a distributed event - driven rate - based delivery control protocol , which means we assume the datacenter network is able to assign each flow by a fixed transmission rate on multiple paths till ( 1 ) a server tries to initial a new flow , ( 2 ) a flow ends , hence some capacity becomes available and each individual server will determine if : ( a ) some of its suspended flows can be awaken , for they are more important and the available capacity are sufficient to meet their deadline , and ( b ) some of its flows can increase their transmission rate to shorten their transmission time    to improve the delivered application importance , we further split a flow into two ( or more ) small flows by the statistical clustering algorithm , e.g. the k - means algorithm , for improving the successful delivery ratio of more important response units .",
    "the details of the protocol are in the following .      in the server - centric datacenter networks ,",
    "several servers are connected by a commodity network component , i.e. , a switch , as a set of servers .",
    "those servers may connected to other servers via another network component to form a larger network .",
    "hence , we define the neighbors of a server as the other servers connecting with the server by the same switch . for example , figure[fig : bcube ] shows a simple bcube@xcite network , in which server @xmath61 are connected by the same switch @xmath62 , and server @xmath63 are connected by @xmath64 .",
    "therefore , the neighbors of @xmath65 include @xmath66 and @xmath67 . with our protocol",
    ", each server maintains the remaining link capacity of all links to its neighbors via connected network components .",
    "it also maintains the information of the flows that traverse through its neighbors or itself , which includes the deadline of flow , the transmission rate of flow , and the averaged importance of the flow . with those information",
    ", the servers connected to the same switch can carefully allocate the rates of the links of the switch , so as to prevents oversubscribing the capacity of each link , which could result in building up packet queue on the switch , packet loss , and making flows missing deadline .",
    "+      as the deadline - aware applications expose flow size , deadline , and importance information when initializing a flow @xmath13 , the server can calculate the flow s importance contribution @xmath68 and minimal transmission rate @xmath69 .",
    "then , the server looks up the shortest equal - cost disjoin paths ( and the second shortest paths if only one shortest path exists ) to the destination for the next hops .",
    "the rate request is estimated at the server based on the flow information of its neighbors .",
    "if the network is under - loaded , the rates of existing flows with lower @xmath70 than @xmath68 will be allocated till the rates can satisfy the minimal transmission rate of the flow .",
    "if the network is over - loaded , the existing flows will be asked to release the extra rates they occupied other than their minimal rates .",
    "if the allocated rate is still insufficient to satisfy the flow , the network will be treated as over - loaded and remaining rate will be allocated from existing flows with lower @xmath70 than @xmath68 . however ,",
    "sometimes the network is light - loaded . in this case , after all flow successfully allocates its minimal transmission rate , the remaining capacity will be assigned to each flow proportion to its @xmath70 .",
    "once the initial rate request is succussed at the server , the request will be updated and forwarded to the next hops , i.e. , the relay nodes , to further allocate the rate from the next hop to the next two hops , and so on .",
    "the demanded rate and available @xmath68 of a path , which is recorded in the rate request , is proportion to the estimated available capacity of the paths based on the source s local information to balance the load between paths . to maximize the application importance by trading less - important flows with most - important flows",
    ", we only allow the new flow that its fic is less than the sum of fics of the affected flows on each node to be initialized .",
    "that is , each server along the path will allocate the most rate given available @xmath70 .",
    "the rate request will be processed and forwarded to the next hop till reaching the node on the same switch of the destination .",
    "notice that on each node , the allocation should be done twice , i.e. , to allocate the rate from the node to the network component , and the rate from the network component to the next hop .",
    "the detailed rate allocation algorithm can be found in algorithm [ alg : rate - allocation ] .",
    "after the server receives the responses of the rate request , it can determine the available capacity of a path for flow @xmath13 by the smallest available rate reported by the nodes on the path .",
    "therefore , if the sum of the smallest available rate of all paths is less than the minimal available rate , the flow should not be transmitted .",
    "otherwise , the server will allocate the flow rate proportion to the available capacity of each path , and broadcast the rate allocation notification to all neighbors .",
    "the notification will be re - broadcasted by the relay nodes until reaching the destination .",
    "besides , for some existing flows might be suspended , the on - path node on the same switch of the flows will take the responsibility to inform the sources of to - be - suspended flows to stop the transmission .",
    "the information of the suspended flows is maintained by this node for further flow recovery .",
    "note that the rtts in the datacenter network is very small ( @xmath71s ) , thus the protocol should take very short time to allocate the rate and suspend existing flows .",
    "minimal transmission rate @xmath72 , the link set @xmath73 to the nexthop , available @xmath74 of the flow of the path .",
    "allocated rate for the flow @xmath75 .",
    "* recorded info:*remaining capacity of the links in @xmath73 , flow information in @xmath73 .",
    "@xmath76 get - next - link(@xmath73 ) @xmath77 assign spare capacity in proportion to @xmath78 @xmath79 @xmath80 @xmath81 existing flows traversing through @xmath82 @xmath83 desired rate @xmath72 @xmath84 available fic of the path @xmath74 ; @xmath85 extract - min - fic(f ) @xmath86 transmission rate of the flow @xmath87 @xmath88 fic of the flow @xmath87 @xmath89 * break * ; @xmath90 ; @xmath91 @xmath92 min(@xmath93 ) @xmath75 ;    for example , we let the @xmath94 as the flow source and @xmath95 as the flow destination .",
    "the minimal transmission rate of the new @xmath13 is 120kbps .",
    "the shortest disjoin paths between @xmath94 and @xmath95 are @xmath96 , @xmath97 .",
    "next , @xmath94 will estimate the available link capacity between ( @xmath94 , @xmath98 ) @xmath99 180kbps , and ( @xmath94 , @xmath100 ) @xmath99 220kbps .",
    "since the sum of the available capacity is greater than the minimal transmission rate , the flow can be transmitted to the next hop .",
    "@xmath94 then sends the rate requests to the relay nodes @xmath98 and @xmath100 to allocate @xmath101kbps on path @xmath102 and hence 66kbps on path @xmath103 .",
    "because nodes @xmath98 and @xmath100 are on the same switches of the destination @xmath95 , the rate request no longer needs to be forwarded . now suppose the smallest available rate of path @xmath102 is 60kbps and of the path @xmath103 is 90kbps .",
    "@xmath94 will allocate @xmath104kbps on path @xmath102 , and hence 72kbps on path @xmath103 .",
    "when a flow finishes its transmission , a notification will be broadcasted and forwarded , which is similar to the final procedure of the flow initialization .",
    "the notification is to tell the nodes along the path and all of their neighbors that some network resources are released .",
    "hence , those nodes can determine if ( 1 ) some suspended flows can be awaken , and ( 2 ) the transmission rate of existing flows can be increased .",
    "this also triggers the nodes along the path that suspended flows for the ending flow to inform the sources of the suspended flows for recovering .",
    "the flow recovery and rate adjustment procedure is exactly the same as the aforementioned flow initialization procedure .",
    "the main reason we allow the recovery of suspended flows is , in the datacenter network , the deadline of flows is different .",
    "hence , some suspended flows may be recovered because they have longer deadline .",
    "that is , flow recovery can further help to maximize application importance .",
    "+      in section ii , we split each flow into tiny flows that each of them contains only one single response unit for maximize the application importance .",
    "however , it is impractical to enforce the same approach in real networks because it might generate massive amount of flows .",
    "however , we notice that in the datacenter , the datasets of deadline - aware services are mostly uniform distributed over a lot of servers . therefore , for a general query , the averaged importance of the response flows could be similar .",
    "this makes it hard to distinguish which flow is more important than others .",
    "we note that for a general query , a server will generate a few results with high similarity , and others with low similarity .",
    "figure[fig : imstat ] shows the importance distribution of the response units within the flows .",
    "the response unit with deep color represents high correlation between this response unit and the corresponding query .",
    "this result is generated based on the real - world dataset from nii test collection for ir systems ( ntcir ) project@xcite and the dataset are distributed to the hosts based on the data placement policy of gfs@xcite .",
    "therefore , we believe the phenomenon can also be found in many deadline - aware services .",
    "based on this observation , we employ statistical clustering algorithm , i.e. , k - mean algorithm , to divide the flow into several small flows .",
    "how many parts a flow should be split is ought to be determined by the application administrators and it is beyond the scope of this work .",
    "here , we simply split a flow into two smaller flows , one of them consists of most important response units , and the other consists of remaining response units .",
    "indeed , the most important flow has higher @xmath70 than the other .",
    "thus , those most important small flows will be transmitted prior to other flows .",
    "this further increases the successful delivery ratio of most important response units , and hence help to maximize application importance .",
    "we implement a three - level bcube network architecture@xcite with parameter @xmath105 as the server - centric datacenter infrastructure on the event - driven network simulator ns-2@xcite .",
    "the proposed protocol , @xmath106@xcite , and mptcp@xcite are implemented as the delivery protocols on bcube topology as well .",
    "the mptcp implementation is based on the open - source project multipath - tcp on ns-2@xcite . as mentioned in @xcite , a host within @xmath33-level bcube",
    "connects to @xmath107 switches and at least @xmath107 disjoin path between any host pair is guaranteed .",
    "hence , the bcube network consists of 125 hosts , and each host connects to 3 switches . to simulate the common commodity switches used in datacenter networks",
    ", we set the packet buffer to 4 mb at the switch , and the packet size is 1 kb .",
    "each switch can buffer at most 4000 packets before dropping any packet .",
    "however , because the proposed protocol is based on the rate control mechanism , the size of the queue are small ( @xmath108200 ) all the time .",
    "the link capacity is 1gbps .",
    "the round trip propagation delay varies from 35@xmath109s to 100@xmath109s depending on the hop counts between the pair of nodes .",
    "we follow the deadline setting as in @xcite , where the mean value of deadline of flows is set to 20 milliseconds , 30 milliseconds , and 40 milliseconds , which represent the emergency of response from tight , moderate to loose . as mentioned in section iii ,",
    "each flow is divided into two smaller parts : an important flow and a regular flow .",
    "the traffic pattern is similar to the traffic generated by partition - aggregate operations .",
    "an aggregating host is first randomly selected from the network , and the rest of 124 hosts will then generate flows to the aggregating host at the same time .",
    "we use two datasets to conduct the evaluation .",
    "the first one is a synthetic dataset . in this scenario ,",
    "the distribution of flow size follows the uniform distribution as in @xcite .",
    "the flows in the light - load network is uniformly distributed across [ 2 kb , 50 kb ] .",
    "flow size across [ 50 kb , 100 kb ] and across [ 100 kb , 150 kb ] represent medium - load and heavy - load networks respectively . for the distribution of flow importance , we assume half of the response units of a flow have high importance , which is set to 10 , and the rest of response units have low importance , which is set to 1 .",
    "the other dataset consists of a set of real - world articles , which comes from nii test collection for ir systems ( ntcir ) project@xcite .",
    "we distribute the data to the hosts according to the data placement policy of gfs@xcite .",
    "the primary goal of our evaluation is to determine the value of employing flow importance information to allocate network capacity .",
    "we would like to verify if the proposed protocol is able to exploit path diversity .",
    "thus , the following metrics are used to conduct the evaluation on the synthesis dataset .    1",
    ".   application - level throughput : the sum of meeting - deadline flow size .",
    "only the flow delivered before its deadline can be count in .",
    "application - level aggregated importance : total amount of flow importance delivered before flow deadline",
    ". flows with high importance contribute important responses to users .",
    "so , the aggregated application importance represents the quality level users experienced",
    "ratio of meeting - deadline flows : the ratio of flows delivered before their deadline .",
    "this results can verify if highly important flows of the proposed protocol does have better delivery ratio .",
    "for the evaluation on the real dataset , we use the precision at @xmath110 , which is the fraction of received response units that are relevant to the query , to determine if the proposed protocol can provide more highly important responses to the users .",
    "the formal definition of precision at @xmath110 is :    @xmath111      we compare the proposed protocol with @xmath106@xcite and mptcp@xcite .",
    "mptcp exploits the path diversity in the datacenter networks to achieve high network utilization .",
    "it adopts tcp - like transport protocol and does not consider flow deadline and importance .",
    "@xmath106 leverages explicit rate control mechanism in a semi - distributed fashion to enforce the deadline of the flows .",
    "although it takes flow deadline into account , @xmath106 does not be aware of the path diversity in the datacenter .",
    "therefore , we modify @xmath106 to randomly select one of the disjoin paths for a flow to improve the network utilization and balance the load of paths .    for a fair comparison , we improve @xmath106 and mptcp by injecting the flow splitting mechanism as mentioned in section iii.b.4 .",
    "the flow splitting mechanism helps @xmath106 and mptcp to enhance the delivery probability of highly important response units .    since the performance of original @xmath106 and mptcp are worse than the modified versions , we only present the results of modified @xmath106 and mptcp here , due to the lack of space .",
    "figure [ fig : gp ] gives the application - level throughput of the protocols .",
    "figure [ fig : gp](a ) shows the goodput in the light - load network .",
    "both the proposed protocol and d@xmath0 can mostly cope with all flows and both of them outperform mptcp .",
    "mptcp aggressively increases transmission rate till packet loss occurs .",
    "once a packet is lost , mptcp needs to wait for a interval of tcp retransmission timeout ( rto ) before retransmission . because the default value of rto is usually set to 100ms or 200ms , once a packet of a near - deadline flow gets lost",
    ", the flow will mostly miss its deadline .",
    "this is the main reason why the performance of mptcp is poor in all cases .",
    "d@xmath0 has similar goodput to the proposed protocol at low ( medium ) load with loose deadlines .",
    "however , as shown in figure [ fig : gp](b)(c ) , when the network is under heavy - loaded or when the deadline is tight , the proposed protocol outperforms d@xmath0 .",
    "this is because d@xmath0 does not well exploit path diversity .",
    "as mentioned in @xcite , at a partition - aggregate operation , massive flows will be generated , and this leads to huge amount of traffic at the links to the aggregating host .",
    "for example , when the network is light - loaded with moderate deadlines , the total network capacity to the aggregating host must be equal to or larger than @xmath112gbps for a partition - aggregate operation .",
    "since the proposed protocol provides 1.9 times , 3.67 times , and 1.7 times of goodput over d@xmath0 with tight , moderate , and loose deadlines .",
    "we believe the proposed protocol is robust and be able to deal with rapid traffic .",
    "+     +    figure [ fig : im ] illustrates the sum of importance contributed by meeting deadline flows . when the network load is heavy , the proposed protocol provide 3.0 times , 4.64 times , and 1.9 times data importance contributions to d@xmath0 with tight , moderate , and loose deadlines . note that the difference in total importance is larger than in goodput .",
    "that is , when the network capacity becomes the critical bottleneck , the proposed protocol can well utilize available bandwidth to deliver highly important flows .",
    "hence , the total data importance will be greater than those produced by conventional deadline - aware protocols .    to further verify if the proposed protocol always produces more data importance , we analyze the delivered flows at the aggregating host .",
    "figure [ fig : ratio ] shows the ratio of flows that successfully meet their deadline in the heavy - load network .",
    "the delivery ratio of important flows of the proposed protocol significantly outperforms others .",
    "this proves that our importance - aware protocol can improve the delivery ratio of most important flows .      in this scenario ,",
    "we first use a common retrieval model , i.e. the vector space model , to calculate the similarity of each data unit to each query .",
    "then , we can obtain the top-@xmath110 relevant rank lists of each query . in each rank list of a query , @xmath110 data units , which are most similar to the query , are recorded as the ground truth .",
    "hence , we use the data units received by the aggregating host before deadline and the rank list to calculate the precision at @xmath110 of the query .",
    "figure [ fig : topk_t ] gives the results of the precision at @xmath110 when the deadlines are tight .",
    "not surprisingly , the proposed protocol significantly outperforms mptcp and d@xmath0 .",
    "mptcp is unable to satisfy the requirements of deadline - aware services because of the rto problem .",
    "d@xmath0 can not achieve high precision because it does not consider flow importance .",
    "the proposed protocol , as mentioned before , always delivers most important flows first , and hence provides outstanding precision at @xmath110 all the time .",
    "in this work , we investigate the requirements to provide high quality deadline - aware online services in datacenters to users . based on the observations from the literatures@xcite and the observation of flow contents , we found that the fulfillment of deadline constraint and high bandwidth utilization do not guarantee high service quality .",
    "thus , we exploit the application - level content information , i.e. , data importance , to better fulfill the requirements of deadline - aware services for providing better user experience , and hence producing more revenue for the service providers .    to figure out the optimal solution , we first model the datacenter network as a flow network for analyzing the application importance maximization problem with the constraints of network link capacity , flow conservation , and flow deadline .",
    "although the property of static multi - disjoin routing paths , which exists in low - cost server - centric datacenter infrastructure designs i.e. , bcube@xcite and mdcube@xcite , significantly reduces the complexity of the problem , the optimal solution is still impractical for realistic datacenter networks .",
    "therefore , based on the ideas of the optimal solution , we further propose a novel distributed rate - based importance - aware delivery control protocol .",
    "the results shows that our proposed protocol provides significant benefits over even optimized versions of existing protocols in terms of both applicaion - level throughput and importance .",
    "the results of real - world dataset also demonstrate that the proposed protocol can provide better precision at @xmath110 all the time . since the growing demand for deadline - aware online services , we believe exploiting data importance to provide high - rank results for users is important toward high - quality deadline - aware services beyond the state - of - art solutions .",
    "g.  buscher , s.  dumais , and e.  cutrell , `` the good , the bad , and the random : an eye - tracking study of ad quality in web search , '' in _ proceeding of the 33rd international acm sigir conference on research and development in information retrieval_.1em plus 0.5em minus 0.4emacm , 2010 , pp .",
    "e.  cutrell and z.  guan , `` what are you looking for ? : an eye - tracking study of information usage in web search , '' in _ proceedings of the sigchi conference on human factors in computing systems_.1em plus 0.5em minus 0.4emacm , 2007 .",
    "l.  lorigo , m.  haridasan , h.  brynjarsdttir , l.  xia , t.  joachims , g.  gay , l.  granka , f.  pellacini , and b.  pan , `` eye tracking and online search : lessons learned and challenges ahead , '' _ journal of the american society for information science and technology _ , 2008 .        c.  wilson , h.  ballani , t.  karagiannis , and a.  rowtron , `` better never than late : meeting deadlines in datacenter networks , '' in _ proceedings of the acm sigcomm 2011 conference _ , ser .",
    "sigcomm 11.1em plus 0.5em minus 0.4emnew york , ny , usa : acm , 2011 , pp .",
    "5061 .      c.  guo , g.  lu , d.  li , h.  wu , x.  zhang , y.  shi , c.  tian , y.  zhang , and s.  lu , `` bcube : a high performance , server - centric network architecture for modular data centers , '' _ sigcomm comput . commun . rev . _ , 2009 .",
    "h.  wu , g.  lu , d.  li , c.  guo , and y.  zhang , `` mdcube : a high performance network structure for modular data center interconnection , '' in _ proceedings of the 5th international conference on emerging networking experiments and technologies_.1em plus 0.5em minus 0.4emacm , 2009 , pp .",
    "2536 .    c.  raiciu , c.  pluntke , s.  barre , a.  greenhalgh , d.  wischik , and m.  handley , `` data center networking with multipath tcp , '' in _ proceedings of the 9th acm sigcomm workshop on hot topics in networks _",
    "hotnets - ix .",
    "1em plus 0.5em minus 0.4emnew york , ny , usa : acm , 2010 , pp . 10:110:6 .",
    "a.  chakrabarti , c.  chekuri , a.  gupta , and a.  kumar , `` approximation algorithms for the unsplittable flow problem , '' in _ proceedings of the 5th international workshop on approximation algorithms for combinatorial optimization _ ,",
    "approx 02.1em plus 0.5em minus 0.4em london , uk , uk : springer - verlag , 2002 , pp .",
    "5166 .",
    "y.  chen , r.  griffith , j.  liu , r.  h. katz , and a.  d. joseph , `` understanding tcp incast throughput collapse in datacenter networks , '' in _ proceedings of the 1st acm workshop on research on enterprise networking _ , ser .",
    "wren 09.1em plus 0.5em minus 0.4emnew york , ny , usa : acm , 2009 .",
    "m.  alizadeh , a.  greenberg , d.  a. maltz , j.  padhye , p.  patel , b.  prabhakar , s.  sengupta , and m.  sridharan , `` data center tcp ( dctcp ) , '' in _ proceedings of the acm sigcomm 2010 conference _ , ser .",
    "sigcomm 10.1em plus 0.5em minus 0.4emnew york , ny , usa : acm , 2010 , pp . 6374 .",
    "h.  wu , z.  feng , c.  guo , and y.  zhang , `` ictcp : incast congestion control for tcp in data center networks , '' in _ proceedings of the 6th international conference _",
    "co - next 10.1em plus 0.5em minus 0.4emnew york , ny , usa : acm , 2010 .",
    "m.  alizadeh , a.  kabbani , t.  edsall , b.  prabhakar , a.  vahdat , and m.  yasuda , `` less is more : trading a little bandwidth for ultra - low latency in the data center , '' in _ proceedings of the 9th usenix conference on networked systems design and implementation _ , ser .",
    "nsdi12.1em plus 0.5em minus 0.4emberkeley , ca , usa : usenix association , 2012 , pp . 1919 ."
  ],
  "abstract_text": [
    "<S> today s datacenters face important challenges for providing low - latency high - quality interactive services to meet user s expectation . for improving the application throughput , recent research works </S>",
    "<S> have embedded application deadline information into design of network flow schedule to meet the latency requirement . here , arises a critical question : does application - level throughput mean providing better quality service ? </S>",
    "<S> we note that there are usually a set of semantic related responses ( or flows ) for answering a query ; and , some responses are highly correlative with the query while others do not . thus , this observation motivates us to associate the importance of the contents with the application flows ( or responses ) in order to enhance the service quality .    </S>",
    "<S> we first model the application importance maximization problem in a generic network and in a server - centric network . </S>",
    "<S> since both of them are too complicated to be deployed in the real world , we propose the importance - aware delivery protocol , which is a distributed event - driven rate - based delivery control protocol , for server - centric datacenter networks . </S>",
    "<S> the proposed protocol is able to make use of the multiple disjoin paths of server - centric network , and jointly consider flow importance , flow size , and deadline to maximize the goodput of most - related semantic data of a query . through real - data - based or synthetic simulations , </S>",
    "<S> the results show that our proposed protocol significantly outperforms d@xmath0 and mptcp in terms of the precision at k and the sum of application - level importance . </S>"
  ]
}