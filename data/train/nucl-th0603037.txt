{
  "article_text": [
    "consider a class of quantum many - body problems such that each member of the class is characterized by @xmath4 particles of type 1 and @xmath5 particles of a different type 2 , and by the interactions , both pairwise and multiparticle , that operate among any subset of particles . with the interactions in place , all of the states and",
    "all of the observables of any exemplar of the class are determined by the integers @xmath4 and @xmath5 .",
    "specifically , for any observable @xmath2 defined for the class , there exists a physical mapping from @xmath6 to @xmath2 .",
    "this notion is trivially extended to more than two type of constituents .",
    "quite obviously , the nuclear many - body problem defines a class of this kind , with @xmath4 and @xmath5 taken as the numbers @xmath0 and @xmath1 of protons and neutrons in a nuclide .",
    "other examples coming easily to mind : @xmath7he-@xmath8he clusters , binary and ternary alloys , etc .",
    "approaches to calculation  or prediction  of the properties of individual systems belonging to such a class span a broad spectrum from pure _ ab initio _ microscopic treatments to phenomenological models having few or many adjustable parameters , with hybrid macroscopic / microscopic and density - functional methods in between .",
    "these approaches are `` theory - thick '' to varying degrees , with the _ ab initio _ ones based in principle on exact theory and the phenomenological ones invoking physical intuition , semi - classical pictures , and free parameters .",
    "thinking in the spirit of edwin jaynes , inventor of the maxent method and charismatic proponent of bayesian probability,@xcite it becomes of special interest to go all the way in the `` theory - thin '' direction and ask the question :    * _ to what extent does the existing data on property @xmath2 across the members of a system class , _ and only the data _ , determine the mapping @xmath9 ? _    in general , this mapping takes one of two forms , depending on whether @xmath2 is a continuous variable ( e.g. , the nuclear mass excess or quadrupole moment ) or a discrete variable ( e.g. , the nuclear spin and parity ) .",
    "the former case defines a problem of function approximation , while the latter defines a classification problem . during the past three decades",
    ", powerful new methods have been developed for attacking such problems .",
    "chief among these are advanced techniques of statistical learning theory , or `` machine learning , '' with artificial neural networks as a subclass . considering the concrete example of the mapping @xmath10 that determines the nuclear ( i.e. , atomic ) mass , a learning machine consists of ( i ) an input interface where @xmath0 and @xmath1 are fed to the device in coded form , ( ii ) a system of intermediate processing elements , and ( iii ) an output interface where an estimate of the mass appears for decoding . given a body of training data to be used as exemplars of the desired mapping ( consisting of input `` patterns , '' also called vectors , and their associated outputs ) , a suitable learning algorithm is used to adjust the parameters of the machine , e.g. , the weights of the connections between the processing elements in the case of a neural network .",
    "these parameters are adjusted in such a way that the learning machine ( a ) generates responses at the output interface that reproduce , or closely fit , the masses of the training examples , and ( b ) serves as a reliable predictor of the masses of test nuclei absent from the training set .",
    "this second requirement is a strong one  the system should not merely serve as a lookup table for masses of known nuclei ; it should also perform well in the much more difficult task of prediction or _",
    "generalization_.    the most widely applied learning machine is the multilayer perceptron ( mlp ) , consisting of a feedforward neural network with at least one layer of `` hidden neurons '' between input and output interfaces.@xcite mlps are usually trained by the backpropagation algorithm,@xcite essentially a gradient - descent procedure for adjusting weight parameters incrementally to improve performance on a set of training examples . a significant measure of success has been achieved in constructing global models of nuclear properties based on such networks , with applications to atomic masses , neutron and proton separation energies , spins and parities of nuclear ground states , stability vs.  instability , branching ratios for different decay modes , and beta - decay lifetimes .",
    "( reviews and original references may be found in ref .  5 .",
    ")    the support vector machine ( svm),@xcite a versatile and powerful approach to problems in classification and nonlinear regression , entered the picture in the 1990s .",
    "rooted in the strategy of structural - risk minimization,@xcite it has become a standard tool in statistical modeling .",
    "although multilayer perceptrons as well as support vector machines are in principle universal approximators , svms eliminate much of the guesswork of mlps , since they incorporate an automatic process for determining the architecture of the learning machine . not surprisingly , they have become the method of choice for a wide variety of problems .",
    "our selection of global nuclear systematics as a concrete example for the application of advanced machine - learning algorithms is neither accidental nor academic .",
    "there exists a large and growing body of excellent data on nuclear properties for thousands of nuclides , providing the raw material for the construction of robust and accurate statistical models .",
    "moreover , interest in this classic problem in nuclear physics has never been greater .",
    "the advent of radioactive ion - beam facilities , and the promise of the coming generation epitomized by the rare isotope accelerator ( ria ) , have given new impetus to the quest for a unified , global understanding of the structure and behavior of nuclei across a greatly expanded nuclear chart .",
    "the creation of hundreds of new nuclei far from stability opens exciting prospects for discovery of exotic phenomena , while presenting difficult challenges for nuclear theory . following the pattern indicated above , traditional methods for theoretical postdiction or prediction of the properties of known or unknown nuclides include _",
    "ab initio _ many - body calculations employing the most realistic nuclear hamiltonians@xcite and , more commonly , density functional approaches and semi - phenomenological models . since computational barriers limit _ ab initio _ treatment to light nuclei ,",
    "viable global models inevitably contain parameters that are adjusted to fit experimental data on certain reference nuclei .",
    "global models currently representing the state of the art are hybrids of microscopic theory and phenomenology ; most notably , they include the macroscopic / microscopic droplet models of mller et al.@xcite and the density functional theories employing skyrme , gogny , or relativistic mean - field lagrangian parametrizations of self - consistent mean - field theory.@xcite from the standpoint of data analysis , these approaches are inherently theory - thick , since their formulation rests on a deep knowledge of the problem domain .",
    "it is evident that data - driven , `` theory - thin , '' statistical models built with machine - learning algorithms can never compete with traditional global models in providing new physical insights .",
    "nevertheless , in several respects they can be of considerable value in complementing the traditional methods , especially in the present climate of accelerated experimental and theoretical exploration of the nuclear landscape .    *",
    "a number of studies@xcite suggest that the quality and quantity of the data has already reached a point at which the statistical models can approach and possibly surpass the theory - thick models in sheer predictive performance . in this contribution , we shall present strong evidence from machine learning experiments with support vector machines that this is indeed the case . * in spite of their `` black - box '' origin , the machine - learning predictions can be directly useful to nuclear experimentalists working at radioactive ion - beam facilities , as well as astrophysicists developing refined models of nucleosynthesis . *",
    "although not straightforward , it will in fact be possible to gain some insights into the inner workings of nuclear physics through statistical learning experiments , by applying techniques analogous to gene knock - out studies in molecular biology .",
    "* it is fundamental interest to answer , for the field of nuclear physics , the jaynesian question that was posed above .",
    "in technical jargon , the support vector machine is a _ kernel method_,@xcite which , in terms of a classification problem , means that it implicitly performs a nonlinear mapping of the input data into a higher - dimensional _ feature space _ in which the problem becomes separable ( at least approximately ) .",
    "architecturally , the support vector machine and multilayer perceptron are close relatives ; in fact the svm includes the mlp with one hidden layer as a special case .",
    "however , svms in general offer important advantages over mlps , including avoidance of the curse of dimensionality through extraction of the feature space from the training set , once the kernel and error function have been specified .",
    "support vector machines may be developed for function approximation ( i.e. , nonlinear regression ) as well as classification . in either case , the output of the machine ( approximation to the function or location of the decision hyperplane , respectively ) is expressed in terms of a representative subset of the examples of the mapping @xmath11 contained in the training set .",
    "these special examples are the _ support vectors_.    the basic ideas underlying svms are most readily grasped by first considering the case of a classification problem involving linearly separable patterns .",
    "suppose some of the patterns are green and the others are red , depending on some input variables defining the @xmath12-dimensional input space . to find a decision surface that separates red from green patterns ,",
    "one _ seeks the hyperplane that provides the maximum margin between the red and green examples_. the training examples that define the margin are just the support vectors . in this simple case , an exact solution is possible , but in general errors are unavoidable . when faced with a problem involving nonseparable patterns , the objective then is to locate a decision hyperplane such that the misclassification error , averaged over the training set , is minimized .",
    "guided by the principle of structural - risk minimization,@xcite the svm approach determines an optimal hyperplane by minimizing a cost function that includes a term to reduce the vc dimension@xcite ( thereby enhancing generalization capability ) and a term that governs the tradeoff between machine complexity and the number of nonseparable patterns .    in practice ,",
    "the svm strategy actually involves two steps .",
    "the first is to implement a nonlinear mapping @xmath13 : @xmath14 , from the space of input vectors @xmath15 into a higher - dimensional feature space , which is `` hidden '' from input and output ( and corresponds to the hidden layer in mlps ) .",
    "this is done in terms of an inner - product kernel @xmath16@xmath13@xmath17@xmath13@xmath18",
    "satisfying certain mathematical conditions , notably mercer s theorem.@xcite the second step is to find a hyperplane that separates ( approximately , in general ) the features identified in the first step .",
    "this is accomplished by the optimization procedure sketched above .",
    "a self - contained introduction to the svm technique is beyond the scope of the present contribution .",
    "excellent treatments are available in the original work of vapnik as expounded in refs .",
    "6,7 and in haykin s text@xcite ( see also ref .",
    "19 ) .    to provide some essential background ,",
    "let us consider a regression problem corresponding to a map @xmath19 , where @xmath15 is an input vector with @xmath12 components @xmath20 , and suppose that @xmath21 training examples indexed by @xmath22 are made available .",
    "then the optimal approximating function takes the form @xmath23 solution of the optimization problem stated above determines the parameters @xmath24 and @xmath25 , and the support vectors of the machine are defined by those training patterns for which @xmath26 .",
    "different choices of the inner - product kernel appearing in eq .",
    "( [ est ] ) yield different versions of the support vector machine .",
    "common choices include @xmath27 corresponding to the _ polynomial learning machine _ with user - selected power @xmath28 ; a gaussian form @xmath29 containing a user - selected width parameter @xmath30 , which generates a radial - basis - function ( rbf ) network ; and @xmath31 which realizes a two - layer ( one - hidden - layer ) perceptron , only one of the parameters @xmath32 , @xmath33 being independently adjustable .",
    "we also draw attention to a generalization of the rbf kernel ( [ rbf ] ) introduced recently as a simplified version of what is called anova decomposition,@xcite having the form @xmath34 \\right)^d \\ , , \\label{anova}\\ ] ]    the support vector machine may be considered as a feedforward neural network in which the inner - product kernel , through an appropriate set of @xmath35 elements @xmath36 , defines a layer of hidden units that embody the mapping from the @xmath12-dimensional input space to the @xmath35-dimensional feature space .",
    "these hidden units process the input patterns nonlinearly and provide outputs that are weighted linearly and summed by an output unit . as already pointed out , the familiar structures of radial - basis - function networks and two - layer perceptrons can be recaptured as special cases by particular choices of kernel .",
    "however , the svm methodology transcends these limiting cases in a very important way : it automatically determines the number of hidden units suitable for the problem at hand , whatever the choice of kernel , by finding an optimally representative set of support vectors and therewith the dimension of the feature space .",
    "in essence , the support vector machine offers a generic and principled way to control model complexity .",
    "by contrast , approaches to supervised learning based on mlps trained by backpropagation or conjugate - gradient algorithms depend heavily on rules of thumb , heuristics , and trial and error in arriving at a network architecture that achieves a good compromise between complexity ( ability to fit ) and flexibility ( ability to generalize ) .",
    "in this section we summarize the findings of recent explorations of the potential of support vector machines for global statistical modeling of nuclear properties .",
    "the discussion will focus on the predictive reliability of svm models relative to that of traditional `` theory - thick '' models .",
    "the properties that are directly modeled in these initial studies , all referring to nuclear ground states , are ( i ) the nuclear mass excess @xmath37 , where @xmath38 is the atomic mass , measured in amu , ( ii ) @xmath39-decay lifetimes of nuclides that decay 100% via the @xmath40 mode , and ( iii ) nuclear spins and parities .    the requisite experimental data are taken from the on - line repository of the brookhaven national nuclear data center ( nndc ) at http://www.nndc.bnl.gov/. the experimental mass values are those of the ame03 compilation of audi et al.@xcite    extensive preliminary studies have been performed to identify inner - product kernels well suited to global nuclear modeling .",
    "earlier work converged on the anova kernel ( 5 ) as a favorable choice , and corresponding results have been published in ref .  19 .",
    "more recently , we have introduced a new kernel that yields superior results , formed by the sum of polynomial and anova kernels and named the pa kernel .",
    "( satisfaction of mercer s theorem is conserved under summation . )",
    "the new kernel contains three parameters ( @xmath28 , @xmath30 , and @xmath41 ) that may be adjusted by the user . aside from parameters contained in the inner - product kernel ,",
    "the svm procedure involves a constant @xmath42 giving the user control over the tradeoff between complexity and flexibility , plus an additional control constant @xmath43 in the regression case , measuring the tolerance permitted in the reproduction of training data .",
    "thus , svm models developed with the pa kernel contain four or five adjustable parameters ( five in all applications reported here ) .    to allow for a meaningful evaluation of predictive performance ( whether interpolation or extrapolation ) , the existing database for the property being modeled is divided into three subsets , namely the _ training set _",
    ", _ validation set _ , and _",
    "test set_. these sets are created by random sampling , consistently with approximate realization of chosen numerical proportions among them , e.g. ( 1002@xmath44):@xmath44:@xmath44 for training , validation , and test sets , respectively , with @xmath45 . the training set is used to find the support vectors and construct the machine for given values of the adjustable parameters @xmath28 , @xmath30 , @xmath41 , @xmath42 , and @xmath43 .",
    "the validation set is used to guide the optimal determination of these parameters , seeking good performance on both the training and validation examples .",
    "the test set remains untouched during this process of model development ; accordingly , the overall error ( or error rate ) of the final model on the members of the test set may be taken as a valid measure of predictive performance .",
    "when one considers how svm models might be applied in nuclear data analysis during the ongoing exploration of the nuclear landscape , it seems reasonable that consistent predictive performance for 80:10:10 or 90:5:5 partitions into training , validation , and test sets would be sufficient for the svm approach to be useful in practice .",
    "the svm approach has been applied to generate a variety of global models of nuclear mass excess , beta - decay lifetimes , and spin / parity , corresponding to different kernels , databases , partitions into training / validation / test sets , and tradeoffs between the relative performance on these three sets . here",
    "we will focus on those models considered to be the best achieved to date .",
    "moreover , due to limited space , we will restrict the discussion to the most salient features of those models and to an assessment of their quality relative to favored traditional global models and to the best available mlp models .",
    "further , more detailed information may be found at the web site http://abacus.wustl.edu/clark/svmpp.php , which generates svm estimates of the listed nuclear properties for @xmath46 pairs entered by visitors .",
    "this web site will be periodically updated as improved svm models are developed .",
    "development and testing of the svm mass models to be highlighted here are based on the ame03 data for all nuclides with @xmath47 and experimental masses having error bars below 4% .",
    "this set of nuclides is divided into the four classes : even-@xmath0-even-@xmath1 ( ee ) even-@xmath0-odd-@xmath1 ( eo ) , odd-@xmath0-even-@xmath1 ( oe ) , and odd-@xmath0-odd-@xmath1 ( oo ) .",
    "separate svm regression models were constructed for each such `` even - oddness '' class .",
    "this does introduce some minimal knowledge about the problem domain into the modeling process ; one might therefore say that the models developed are not absolutely theory - free .",
    "however , the data itself gives strong evidence for the existence of different mass surfaces depending on whether @xmath0 and @xmath1 are even or odd .",
    "knowledge of the integral character of @xmath0 and @xmath1 may , quite properly , bias the svm toward inclusion of associated quantum effects.@xcite    table 1 displays performance measures for models based on an 80:10:10 target partitioning of the full data set among training , validation , and test sets , respectively .",
    "inspection of the actual distributions of these sets in the @xmath48 plane shows that substantial fractions of the validation and test sets lie on the outer fringes of the known nuclei , significantly distant from the line of stable nuclides .",
    "accordingly , performance on the test set measures the capability of the models in extrapolation as well as interpolation .",
    "performance on a given data set is quantified by the corresponding root - mean - square ( rms ) error @xmath49 of model results relative to experiment ( as is standard in global mass modeling ) . the `` optimized '' model parameters",
    "are included in table 1 .",
    "they show enough differences from one even - oddness class to another to justify development of separate models for the four classes .",
    "the results in table 1 attest to a quality of performance , in both fitting and prediction , that is on a par with the best available from traditional modeling@xcite and from mlp models trained by an enhanced backpropagation algorithm.@xcite to emphasize this point qualitatively , we display in table 2 some representative rms error figures that have been achieved in recent work with all three approaches .",
    "( we must note , however , that the data sets used for the different entries in the table may not be directly comparable , and the division into training , validation , and test sets does not necessarily have the strict meaning assigned here . )",
    "the second svm model listed in the table was developed for a partitioning of the data into training , validation , and test sets of approximately 90:5:5 , obtained by random transfer of nuclides from the validation and test sets of the 80:10:10 model to the training set .",
    "the quality of representation that can be realized through the svm methodology may be highlighted in another way .",
    "employing the nuclear mass excess values generated by the svm models of table 1 , we have calculated the @xmath50 values for eight alpha - decay chains of the superheavy elements 110 , 111 , 112 , 114 , 115 , 116 , and 118 .",
    "( the alpha - decay @xmath51-value is defined as @xmath52 , where be stands for the binding energy of the indicated nuclide . )",
    "results are presented in graphical and tabular form on the web site http://haochen.wustl.edu/svm/svmpp.php .",
    "for the models of table 1 ( based on an 80:10:10 partition of the assumed ame03 data set ) , the average rms error of the 38 estimates of @xmath50 is 0.82 mev , while the average absolute error is 0.64 mev . we emphasize that these estimates are predictions ( rather than fits ) , since none of the nuclei involved belongs to the validation or test set .",
    "moreover , due to the situation of these superheavy nuclides in the @xmath48 plane , prediction of the associated @xmath50 values provides a strong test of extrapolation .",
    "the performance of svm mass models documented in tables 1 and 2 and in the alpha - chain predictions gives assurance that this approach to global modeling will be useful in guiding further exploration of the nuclear landscape .",
    "however , it is important to gain some sense of when and how it begins to fail .",
    "the performance figures for the two sets of svm models involved in table 2 are consistent with the natural expectation that if one depletes the validation and test sets of the 80:10:10 partition in favor of an enlarged test set , the predictive ability of the model is enhanced .",
    "conversely , one should be able to `` break '' the svm modeling approach by random depletion of the training set of the 80:10:10 model in favor of larger validation and test sets .",
    "eventually the training set will become too small for the method to work at all .",
    "the results of a quantitative study of this process are shown in figure 1 .",
    "writing the generic partition as ( 1002@xmath44):@xmath44:@xmath44 , the error measure increases roughly linearly with @xmath44 for @xmath44 greater than 10 .",
    "in addition to direct statistical modeling using either svms or mlps , a promising hybrid approach is being explored .",
    "recently , the _ differences _ @xmath53 between experimentally measured masses and the corresponding theoretical masses given by the finite - range droplet model ( frdm ) of mller , nix , and collaborators@xcite have been modeled with a feedforward neural network of architecture 46661 trained by a modified backpropagation learning algorithm.@xcite ( the integers denote the numbers of neurons in successive layers , from input to output . )",
    "the rms errors on training ( 1276 ) , validation ( 344 ) , and test ( 529 ) sets are respectively 0.40 , 0.49 , and 0.41 mev , where the numbers of nuclides in each of these sets is given in parentheses . in a similar experiment , we have constructed svm models for ee , eo , oe , and ee classes using pa kernels",
    ". overall rms errors of 0.19 , 0.26 , and 0.34 were achieved on the training ( 1712 ) , validation ( 213 ) , and test ( 213 ) sets , respectively , with little variation over even - oddness classes .",
    "error figures over comparable subsets for the frdm model in question run around 0.7 mev , again with relatively little variation from subset to subset .",
    "these results suggest that mlps and svms are capable of capturing some 1/2 to 2/3 of the physical regularities missed by the frdm . it remains to be seen whether the residual error has a systematic component or instead reflects a large number of small effects that will continue to elude global description .",
    "another important problem in global modeling involves the prediction of beta - decay halflives of nuclei . as in the case of atomic masses , this is a problem in nonlinear regression .",
    "here we restrict attention to nuclear ground states and to nuclides that decay 100% through the @xmath40 mode .",
    "for this presentation , we make the further restriction to nuclides with halflives @xmath54 below @xmath55 s ( although we have also included the longer - lived examples in another set of modeling experiments ) .",
    "the brookhaven nndc provides 838 examples fitting these criteria .",
    "since the examples still span 9 orders of magnitude in @xmath54 , it is natural to work with @xmath56 and seek an approximation to the mapping @xmath57 in the form of svms .",
    "again , we construct separate svms for the ee , eo , oe , and oo classes , and again a kernel of type pa is adopted .",
    "the full data set is divided by random distribution into training , validation , and test sets in approximately the proportions 80:10:10 .",
    "the performance of the favored models is quantified in table 3 .",
    "here we use two measures to assess the accuracy of the svm results for training , validation , and test nuclides .",
    "these are the rms error , again denoted by @xmath49 , and the mean absolute error @xmath58 of the model estimates of @xmath59 , relative to experiment .",
    "detailed studies of beta - decay systematics within the established framework of nuclear theory and phenomenology include those of staudt et al.@xcite ( 1990 ) , hirsch et al.@xcite ( 1993 ) , homma et al.@xcite ( 1996 ) , and mller et al.@xcite ( 1997 ) .",
    "however , comparison of the performance of the svm models with that of the models resulting from these studies is obscured by the differences in the data sets involved .",
    "most significantly , the data set employed here is considerably larger than those used previously , including as it does many new nuclides far from stability .",
    "analysis of svm performance on subsets of the data set , now in progress , will yield useful information on the efficacy of svm models relative to the more traditional ones , as we continue to develop improved global models of beta - decay systematics .    on the other hand , mlp models for the beta - halflife problem",
    "have been generated for the same data set as used in our svm study , allowing a meaningful comparison to be made .",
    "the best mlp models created to date@xcite show values for the rms error @xmath49 over all even - oddness classes of 0.55 in training , 0.61 in validation , and 0.64 in prediction .",
    "these values are somewhat larger than those seen in table 3 .",
    "however , it must be pointed out that the mlp results were obtained with a smaller training set , so the efficacy of the two statistical methods appears to be about equal at this stage of development . that being the case , it is relevant to note that the recent mlp models represent a distinct advance earlier versions,@xcite and that those earlier statistical models already showed better performance over short - lived data sets than the conventional models of homma et al.@xcite and mller et al.@xcite      the applications to prediction of atomic masses and beta - decay lifetimes demonstrate the predictive power of svms in two important problems of global nuclear modeling that involve function estimation .",
    "the final two applications will probe the performance of svms in global modeling of the discrete nuclear properties of parity and spin .",
    "in essence , these are problems of classification : `` which of a finite number of exclusive possibilities is associated with or implied by a given input pattern ? ''",
    "support vector machines were first developed to solve classification problems , and good svm classifier software is available on the web.@xcite however , for convenience and uniformity we prefer to treat the parity and spin problems with the same svm regression technique as in the other examples , also using the pa choice of inner - product kernel . in the parity problem",
    ", the decision of the regression svm is interpreted to be positive parity [ negative parity ] if the machine s output is positive [ negative ] . in the spin problem ,",
    "the spin assigned by the machine is taken to be correct if and only if the numerical output ( after rescaling ) is within @xmath60 of the correct value ( in @xmath61 units ) .",
    "as before , all data are taken from the brookhaven site .",
    "for parity and spin , it is especially natural to create separate svm models for the different even - oddness classes .",
    "however , as is well known , all ee nuclei have spin / parity @xmath62 . modeling",
    "this property is trivial for svms , so the ee class may be removed from further consideration .",
    "the data in and of itself permits us to do so .",
    "moreover , in the case of spin , the data itself establishes , with a high degree of certainty , that the spin of eo or oe nuclides takes half - odd integral values ( in units of @xmath61 ) , while the spin of oo nuclides is integral .",
    "although this formulation of the parity and spin problems introduces significant domain knowledge into the model - building process , the data alone provides adequate motivation .",
    "nuclei with spin values larger than 23/2 were not considered .",
    "the predictive performance that may be achieved with svm models of parity and spin is illustrated in tables 4 and 5 .",
    "performance is measured by the percentage of correct assignments .",
    "construction of both parity and spin models is based on an 80:10:10 partition of the data into training , validation , and test sets .",
    "( as usual , the target distribution is realized only approximately . )",
    "averaged over even - oddness classes , the overall performance of the parity svms is 97% correct on the training set and 95% on the validation set , with a predictive performance on the test set of 94% .",
    "obviously , assigning parity to nuclear ground states is an extremely easy task for support vector machines .",
    "one might expect quite a different situation for the spin problem : since there are 12 legitimate spin assignments for the eo or oe nuclides considered ( i.e. , obeying the rules for addition of angular momenta ) and also 12 for the oo class , the chance probability of a correct guess is low .",
    "it is then most remarkable that the svm spin models we have developed perform with very high accuracy in prediction as well as fitting and validation .",
    "while some success has been achieved previously in mlp modeling of parity and spin,@xcite consistent predictive quality within the 8090 percentile range has been elusive . within main - stream nuclear theory and phenomenology ,",
    "the problem of global modeling of ground - state spins has received little attention , and the few attempts have not been very successful . as a baseline , global nuclear structure calculations within the macroscopic / microscopic approach@xcite",
    "reproduce the ground - state spins of odd-@xmath63 nuclei with an accuracy of 60% ( agreement being found in 428 examples out of 713 ) .",
    "it should be mentioned that in the preliminary investigations described in ref .",
    "19 , the tasks of global modeling of parity and spin with svms were in fact treated as classification rather than function - estimation problems .",
    "corresponding svm classifiers were created using established procedures.@xcite based on an rbf kernel , results were obtained that surpass the available mlp models in quality , but are inferior to those reported here in tables 4 and 5 .",
    "global statistical models of atomic masses , beta - decay lifetimes , and nuclear spins and parities have been constructed using the methodology of support vector machines .",
    "the predictive power of these `` theory - thin '' models , which in essence are derived from the data and only the data , is shown to be competitive with , or superior to , that of conventional `` theory - thick '' models based on nuclear theory and phenomenology .",
    "conservative many - body theorists may be troubled by the `` black - box '' nature of the svm predictors , i.e. , the impenetrability of their computational machinery . however ,",
    "this alternative , highly pragmatic approach may represent a wave of the future in many fields of science ",
    "already visible in the proliferation of density - functional computational packages for materials physics and eventually molecular biology , which , for the user , are effectively black boxes . while it is true that the statistical models produced by advances in machine learning do not as yet yield the physical insights of traditional modeling approaches , their prospects for revealing new regularities of nature are by no means sterile .",
    "this research has received support from the u.s . national science foundation under grant no .",
    "we acknowledge helpful discussions and communications with s. athanassopoulos , m. binder , e. mavrommatis , t. papenbrock , s. c. pieper , and r. b. wiringa . in the regression studies",
    "reported herein , we have found the mysvm software and instruction manual created by s. rping@xcite ( dortmund ) to be very useful .      e. t. jaynes , _ probability theory : the logic of science _ ( cambridge university press , cambridge , 2003 ) .",
    "s.  haykin , _ neural networks : a comprehensive foundation _ , second edition ( mcmillan , new york , 1999 ) .",
    "d.  e.  rumelhart , g.  e.  hinton , and r.  j.  williams , in _ parallel distributed processing : explorations in the microstructure of cognition _ ,",
    "vol .  1 , edited by d.  e.  rumelhart _ et al . _",
    "( mit press , cambridge , ma , 1986 ) . j.  hertz , a.  krogh , and r.  g.  palmer , _ introduction to the theory of neural computation _ ( addison - wesley , redwood city , ca , 1991 ) .",
    "j. w. clark , t. lindenau , and m. l. ristig , _ scientific applications of neural nets _",
    "( springer - verlag , berlin , 1999 ) .",
    "v.  n.  vapnik , _ the nature of statistical learning theory _ ( springer - verlag , new york , 1995 ) .",
    "v.  n.  vapnik , _ statistical learning theory _ ( wiley , new york , 1998 ) .",
    "s. c. pieper and r. b. wiringa , _ annu .",
    "_  * 51 * , 53 ( 2001 ) .",
    "p.  mller and j.  r.  nix , _",
    "j.  phys .",
    "g _ * 20 * , 1681 ( 1994 ) .",
    "p.  mller , j.  r.  nix , w.  d.  myers , and w.  j.  swiatecki , _ at .",
    "data nucl .",
    "data tables _ * 59 * , 185 ( 1995 ) .",
    "m. samyn , s. goriely , p .- h .",
    "heenen , j. m. pearson , and f. tondeur , _ nucl .",
    "phys . _ * a700 * , 142 ( 2002 ) ; s. goriely , m. samyn , p .- h .",
    "heenen , j. m. pearson , and f. tondeur , _ phys .",
    "c _ * 66 * , 024326 ( 2002 ) .",
    "m. bender , p .- h .",
    "heenen , and p .-",
    "reinhard , _ rev .",
    "* 75 * , 121 ( 2003 ) ; m. bender , g. f. bertsch , and p .- h .",
    "heenen , _ phys .",
    "_ * 94 * , 102503 ( 2005 ) .",
    "g. f. bertsch , b. sabbey , and m. uusnkki , _ phys .",
    "_ c * 71 * , 054311 ( 2005 ) .",
    "k. a. gernoth and j. w. clark , _ neural networks _ * 8 * , 291 ( 1995 ) .",
    "j. w. clark , e. mavrommatis , s. athanassopoulos , a. dakos , and k. a. gernoth , _ fission dynamics of atomic clusters and nuclei _ , edited by d.  m.  brink , f.  f.  karpechine , f.  b.  malik , and j.  da providencia ( world scientific , singapore , 2001 ) , p.  76 .",
    "[ nucl - th/0109081 ] s. athanassopoulos , e. mavrommatis , k. a. gernoth , and j. w. clark , _ nucl .",
    "_ * a743 * , 222 ( 2004 ) , and references therein .",
    "s. athanassopoulos , e. mavrommatis , k. a. gernoth , and j. w. clark , in _ advances in nuclear physics , proceedings of the hellenic symposium on nuclear physics _",
    ", in press ( 2005 ) .",
    "[ nucl - th/0509075 ] s. athanassopoulos , e. mavrommatis , k. a. gernoth , and j. w. clark , in _ advances in nuclear physics _ , proceedings of the hellenic symposium on nuclear physics , in press ( 2006 ) .",
    "[ nucl - th/0511088 ] h. li , j. w. clark , e. mavrommatis , s. athanassopoulos , and k. a. gernoth , in _ condensed matter theories _ , vol .",
    "20 , edited by j. w. clark , r. m. panoff , and h. li ( nova science publishers , hauppauge , ny , 2006 ) , p.  505 .",
    "[ nucl - th/0506080 ] j.  mercer , _ transactions of the london philosophical society ( a ) _ * 209 * , 415 ( 1909 )",
    ". m.  o.  stitson , a.  gammerman , v.  vapnik , v.  vovk , c.  watkins , and j.  weston , in _ advances in kernel methods  support vector learning _ , edited by b. schkopf , c.  burges , and a.  j.  smola ( mit press , cambridge , ma , 1999 ) , p.  285 .",
    "g.  audi , a.  h.  wapstra , c.  thibault , j.  blachot , and o.  bersillon _ nucl .",
    "phys . _ * a729 * ( 2003 ) .",
    "s. gazula , j. w. clark , and h. bohr , _ nucl .",
    "phys . _ * a540 * , 1 ( 1992 ) . s.  athanassopoulos , e.  mavrommatis , k.  a.  gernoth , and j.  w.  clark , to be published .",
    "a.  staudt , e.  bender , k.  muto , and h.  v.  klapdor - kleingrothaus , _ at .",
    "data nucl .",
    "data tables _ * 44 * , 132 ( 1990 ) .",
    "m.  hirsch , a.  staudt , k.  muto , and h.  v.  klapdor - kleingrothaus , _ at .",
    "data nucl .",
    "data tables _ * 53 * , 165 ( 1993 ) .",
    "h.  homma , e.  bender , m.  hirsch , k.  muto , and h.  v.  klapdor - kleingrothaus , _ phys .",
    "c _ * 54 * , 2972 ( 1996 ) .",
    "p.  mller , j.  r.  nix , and k.  l.  kratz , _ at .",
    "data nucl .",
    "data tables _ * 66 * , 131 ( 1997 ) .",
    "n.  costiris , a.  dakos , e.  mavrommatis , k.  a.  gernoth , and j.  w.  clark , to be published .",
    "t.  joachims  ( 2004 ) ,  multi - class  support  vector  machine , http://www.cs.cornell.edu/people/tj/svm_light/svm_multiclass.html ( 2004 ) .",
    "j.  w.  clark , s.  gazula , k.  a.  gernoth , j.  hasenbein , j.  s.  prater , and h.  bohr , in _ recent progress in many - body theories _ , vol .  3 , edited by t.  l.  ainsworth , c.  e.  campbell , b.  e.  clements , and e.  krotscheck ( plenum , new york , 1992 ) , p.  371 .",
    "k.  a.  gernoth , j.  w.  clark , j.  s.  prater , and h.  bohr , _ phys .",
    "_ * b300 * , 1 ( 1993 ) .",
    "p.  mller and j.  r.  nix , _ nucl .",
    "phys .  a _",
    "* 520 * , 369c ( 1990 ) .",
    "s.  rping , mysvm , http://www-ai.cs.uni-dortmund.de/software/mysvm/ ( 2004 ) ."
  ],
  "abstract_text": [
    "<S> advances in statistical learning theory present the opportunity to develop statistical models of quantum many - body systems exhibiting remarkable predictive power . </S>",
    "<S> the potential of such `` theory - thin '' approaches is illustrated with the application of support vector machines ( svms ) to global prediction of nuclear properties as functions of proton and neutron numbers @xmath0 and @xmath1 across the nuclidic chart . based on the principle of structural - risk minimization , </S>",
    "<S> svms learn from examples in the existing database of a given property @xmath2 , automatically and optimally identify a set of `` support vectors '' corresponding to representative nuclei in the training set , and approximate the mapping @xmath3 in terms of these nuclei . </S>",
    "<S> results are reported for nuclear masses , beta - decay lifetimes , and spins / parities of nuclear ground states . </S>",
    "<S> these results indicate that svm models can match or even surpass the predictive performance of the best conventional `` theory - thick '' global models based on nuclear phenomenology . </S>"
  ]
}