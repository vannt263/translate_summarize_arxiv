{
  "article_text": [
    "sometimes , especially in applications dealing with signal processing and biology , theory provides us with some additional information allowing us to restrict the parameter space to a finite number of points ; in these cases , we speak of _ discrete parameter models_. statistical inference when the parameter space is reduced to a lattice was first considered by hammersley @xcite in a seminal paper . however , since the author was motivated by the measurement of the mean weight of insulin , he focused mainly on the case of a gaussian distribution with known variance and unknown integer mean ( see @xcite , page  192 ) ; this case was further developed by khan @xcite .",
    "the poisson case also met some attention in the literature and was dealt with by hammersley ( @xcite , page  199 ) and others @xcite .",
    "previous works have shown that the rate of convergence of @xmath0-estimators is often exponential [ @xcite , @xcite ] .",
    "general treatments of admissibility and related topics are in @xcite ( see also the book @xcite ) ; special cases have been dealt with in  @xcite ( page  424 , for the case of a translation integral parameter and of integral data under the quadratic loss ) , @xcite ( for the case of the gaussian distribution ) and @xcite ( for the case of the discrete uniform distribution ) . other papers dealing with optimality in discrete parameter spaces are @xcite .",
    "optimality of estimation under a discrete parameter space was also considered by vajda @xcite in a nonorthodox setting inspired by rnyi s theory of random search .",
    "other aspects that have been studied are bayesian encompassing @xcite , construction of confidence intervals ( @xcite , pages 224225 ) , comparison of statistical experiments ( @xcite , @xcite , section 2.2 ) , sufficiency and minimal sufficiency @xcite and best prediction @xcite . moreover , in the estimation of complex statistical models ( see @xcite , @xcite , chapter  4 ) and in the calculation of efficiency rates ( see @xcite ) , approximating a general parameter space by a sequence of finite sets has proved to be a valuable tool .",
    "a  few papers showed the practical importance of discrete parameter models in signal processing , automatic control and information theory and derived some bounds on the performance of the estimators ( see @xcite ) .",
    "more recently , the topic has received new interest in the information theory literature ( see @xcite , and the review paper @xcite ) , in stochastic integer programming ( see @xcite ) , and in geodesy ( see , e.g. ,  @xcite , section 5 ) .",
    "however , no general formula for the convergence rate has ever been obtained , no optimality proof under generic conditions has been provided and no general discussion of efficiency and superefficiency in discrete parameter models has appeared in the literature . in the present paper , we provide a full answer to these problems in the case of discrete parameter models for samples of i.i.d .",
    "( independent and identically distributed ) random variables . therefore , after introducing some examples of discrete parameter models in section  [ sect - examples ] , in section  [ sect - m - estimators ] we investigate the properties of a class of @xmath0-estimators . in particular , in section  [ sect - consistencyofmleandbe ] , we derive some conditions for strong consistency ; then , in section  [ sect - asymptoticdistributionofthemle ] , we calculate an asymptotic approximation of the distribution of the estimator and we establish its convergence rate .",
    "these results are specialized to the case of the maximum likelihood estimator ( ) and extended to bayes estimators in section  [ sect - mleandbayesestimators ] . in section  [ sect - optimalityandefficiencyofthemle ]",
    ", we derive upper bounds for the convergence rate in the standard and in the minimax contexts , and we discuss the relations between information inequalities , efficiency and superefficiency . in particular , we prove that estimators of discrete parameters have uncommon efficiency properties . indeed , under the zero  one loss function , no estimator is efficient in the class of consistent estimators for any value of @xmath1 ( @xmath2 being here the true value of the parameter ) and no estimator attains the information inequality we derive .",
    "but the still has some appealing properties since it is minimax efficient and attains the minimax information inequality bound .",
    "the following examples are intended to show the relevance of discrete parameter spaces in applied and theoretical statistics . in particular , they show that the results in the following sections solve some long - standing problems in statistics , optimization , information theory and signal processing .",
    "we recall that a _ statistical model _ is a collection of probability measures @xmath3 where @xmath4 is the _",
    "parameter space_. @xmath4 is a subset of a euclidean or of a more abstract space .",
    "we consider tumor transplantability in mice . for a certain type of mating , the probability of a tumor `` taking '' when transplanted from the grandparents to the offspring is equal to @xmath5 where @xmath6 is an integer equal to the number of genes determining transplantability . for another type of mating , the probability is @xmath7 .",
    "we aim at estimating @xmath6 knowing that @xmath8 transplants take out of @xmath9 the likelihood is given by @xmath10 in this case the parameter space is discrete and the maximum likelihood estimator can be shown to be @xmath11 $ ] where @xmath12 $ ] is the integer nearest to  @xmath13 ( see @xcite , page 236 ) .",
    "consider a  random variable @xmath14 distributed according to an exponential family where the natural parameter @xmath6 is restricted to a lattice @xmath15 , for fixed @xmath2 and @xmath16 ( see @xcite , page 759 ) .",
    "the case of a gaussian distribution has been considered in @xcite ( page  192 ) and @xcite , the poisson case in @xcite ( page 199 ) , @xcite . in particular",
    ", @xcite uses the gaussian model to estimate the molecular weight of insulin , assumed to be an integer ( however , see the remarks of tweedie in the discussion of the same paper ) .",
    "we consider the optimization problem of the form @xmath17 , where @xmath18 is an integral functional , @xmath19 is the mean under probability @xmath20,@xmath21 is a real - valued function of two variables @xmath13 and @xmath22 , @xmath23 is a random variable having probability distribution @xmath20 and @xmath24 is a finite set .",
    "we approximate this problem through the sample average function @xmath25 and the associated problem @xmath26 .",
    "see @xcite for some theoretical results and a discussion of the stochastic knapsack problem and @xcite for an up - to - date bibliography .    in many applied cases , the requirement that the true model generating the data corresponds to a point belonging to the parameter space appears to be too strong and unlikely . moreover , the objective is often to recover a model reproducing some stylized facts from the original data . in these cases ,",
    "approximation of a continuous parameter space with a finite number of points allows for obtaining such a model under weaker assumptions .",
    "this situation arises , for example , in signal processing and automatic control applications @xcite and is reminiscent of some related statistical techniques , such as the _ discretization _ device of le cam ( @xcite , section 6.3 ) , or the _ sieve estimation _ of grenander ( @xcite ; see also @xcite , remark  5 ) .    in information theory ,",
    "discrete parameter models are quite common , and their estimation is a generalization of binary hypothesis testing that goes under the names of @xmath27-_ary _ _ hypotheses _ ( or _ multihypothesis _ ) _ testing _ , _ classification _ or _ detection _ ( see the examples in @xcite ) .",
    "consider a received waveform @xmath28 described by the equation @xmath29 for @xmath30 , where @xmath31 is a deterministic signal , @xmath32 is an additive gaussian white noise and  @xmath33 is the noise intensity .",
    "the set of possible signals is restricted to a finite number of alternatives , say @xmath34 : the chosen signal is usually the one that maximizes the log - likelihood of the sample , or an alternative criterion function .",
    "for example , if the log - likelihood of the process based on the observation window @xmath35 $ ] is used , we have @xmath36.\\end{aligned}\\ ] ] much more complex cases can be dealt with ; see @xcite for an introduction .",
    "in this section , we consider an estimator obtained by maximizing an objective function of the form@xmath37 in what follows , we allow for misspecification . note that the expression @xmath0-estimator stands for _ maximum likelihood type estimator _ , in the spirit of huber  @xcite , and not for _ maximum _ ( or _ extremum _ ) _ estimator _ ( see , e.g. , @xcite , page 2114 ) .      in the case of a discrete parameter space , uniform convergence reduces to pointwise convergence .",
    "therefore , @xmath0-estimators are strongly consistent under less stringent conditions than in the standard case ; in particular , no condition is needed on the continuity or differentiability of the objective function .",
    "the following assumption is used in order to prove consistency in the case of i.i.d .",
    "replications :    1 .",
    "the data @xmath38 are realizations of i.i.d .",
    "@xmath39-valued random variables having probability measure @xmath40 .",
    "+ the estimator @xmath41 is obtained by maximizing over the set @xmath42 , of finite cardinality , the objective function @xmath43 the function @xmath44 is @xmath45-measurable for each @xmath46 and satisfies the @xmath47-domination condition@xmath48 for every @xmath46 , where @xmath49 denotes the expectation taken under the true probability measure @xmath40 .",
    "+ moreover , @xmath2 is the point of @xmath4 maximizing @xmath50 and @xmath2 is globally identified ( see  @xcite , section 2.2 ) .",
    "\\(i ) the assumption of a finite parameter space seems restrictive with respect to the more general assumption of @xmath4 being countable ( see , e.g. , @xcite ) .",
    "however , a1 is compatible with the convex hull of @xmath4 being compact , as in standard asymptotic theory .",
    "indeed , the cases analyzed in @xcite have convex likelihood functions and this is a well - known substitute for compactness of @xmath4 ( see @xcite , page 2133 ; see  @xcite , for consistency with neither convexity nor compactness ) .",
    "moreover , the restriction to finite parameter spaces seems to be necessary to derive the asymptotic approximation to the distribution of @xmath0-estimators .",
    "the relative position of the points of @xmath4 is unimportant and the choice of @xmath2 as the maximizer is arbitrary and is made only for practical purposes .",
    "note that @xmath2 has no link with @xmath40 apart from being the pseudo - true value of @xmath51 with respect to @xmath40 on the parameter space @xmath4 ( see , e.g. , @xcite , volume 1 , page 14 ) .",
    "[ pr - consistencyofthemle ] under assumption , the @xmath0-estimator @xmath41 is a @xmath40-strongly consistent estimator of  @xmath2 and is @xmath52-measurable .",
    "a similar result of consistency for discrete parameter spaces has been provided by @xcite ( page  446 ) , by @xcite ( pages 325333 ) , by @xcite(pages  12931294 ) as an application of the shannon  mcmillan ",
    "breiman theorem of information theory , by @xcite ( section 2.1 ) as a preliminary result of his work on partial likelihood , and by @xcite ( page 96 , section 7.1.6 ) .      for a discrete parameter space ,",
    "the finite sample distribution of the @xmath0-estimator @xmath41 is a discrete distribution converging to a dirac mass concentrated at @xmath2 . since the determination of an asymptotic approximation to this distribution is an interesting and open problem , we derive in this section upper and lower bounds and asymptotic estimates for probabilities of the form @xmath53 .    to simplify the following discussion",
    ", we introduce the processes : @xmath54_{j=0,\\ldots , j , j\\neq i},\\vspace*{2pt}\\cr \\mathbf{x}_{k}\\triangleq\\mathbf{x}_{k}^{(0)}\\vspace*{2pt}\\cr \\phantom{\\mathbf{x}_{k}}=[\\ln q(y_{k};\\theta _ { 0})-\\ln q(y_{k};\\theta_{j})]_{j=1,\\ldots , j } , } \\\\",
    "\\eqntext{i=1,\\ldots , j,}\\end{aligned}\\ ] ]    the probability of the estimator @xmath41 taking on the value @xmath55 can be written as @xmath56 \\\\[-8pt ] \\nonumber & = & \\mathbb{p}_{0}\\biggl(\\sum _ { k=1}^{n}\\mathbf{x}_{k}^{(i)}\\in\\operatorname{int } \\mathbb{r}_{+}^{j}\\biggr).\\end{aligned}\\ ] ] the only approaches that have been successful in  our experience are large deviations ( in logarithmic and exact form ) and saddlepoint approximations",
    ". note that we could have defined the probability in  ( [ eq - probabilityforthetaiinldform ] )  as @xmath57 or through any other combination of equality and inequality signs ; this introduces some arbitrariness in the distribution of @xmath41 .",
    "however , we will give some conditions ( see proposition  [ pr - convergencerateofmle2 ] ) under which this difference is asymptotically irrelevant .",
    "section  [ sect - apreliminaryresult ] introduces definitions and assumptions and discusses a preliminary result . in sec - tion  [ sect - largedeviationsasymptotics ]",
    "we derive some results on the asymptotic behavior of @xmath58 using large deviations principles ( ldp ) .",
    "then , we provide some refinements of the previous expressions using the theory of exact asymptotics for large deviations , with special reference to the case @xmath59 . at last , section  [ sect - saddlepointapproximation ] derives saddlepoint approximations for probabilities of the form ( [ eq - probabilityforthetaiinldform ] ) .      as concerns the distribution of the @xmath0-estimator  @xmath41",
    ", we shall need some concepts and functions derived from large deviations theory ( see @xcite ) ; we recall that the processes @xmath60 , @xmath61 and @xmath62 have been introduced in ( [ eqlikelihoodratioprocesses ] ) .",
    "then , for @xmath63 , we define the moment generating functions @xmath64}\\bigr]\\\\ & = & \\mathbb{e}_{0}\\bigl[e^{\\bolds{\\lambda}^{\\mathsf{t}}\\mathbf{x}^{(i)}}\\bigr],\\end{aligned}\\ ] ] the logarithmic moment generating functions @xmath65}\\bigr]\\\\ & = & \\ln\\mathbb{e}_{0}\\bigl[e^{\\bolds{\\lambda}^{\\mathsf{t}}\\mathbf{x}^{(i)}}\\bigr],\\end{aligned}\\ ] ] and the cramr transforms @xmath66,\\ ] ] where @xmath67 is the scalar product . note that",
    ", in what follows , @xmath68 , @xmath69 and @xmath70 are respectively shortcuts for @xmath71 , @xmath72 and @xmath73 .",
    "moreover , for a function @xmath74 , we will need the definition of the _ effective domain _ of @xmath75 , @xmath76 .",
    "the following assumptions will be used to approximate the distribution of @xmath41 .    1 .",
    "there exists a @xmath77 such that , for any @xmath78 , we have @xmath79^{\\eta}<+\\infty\\quad \\forall j , k=0,\\ldots , j.\\ ] ]    @xmath80 in what follows , this assumption could be replaced by a condition as in @xcite ( assumptions  h1 and h2 ) .    1 .",
    "@xmath81 is _ steep _ , that is , @xmath82 whenever @xmath83 is a sequence in @xmath84 converging to a boundary point of @xmath85 .    under assumptions , and",
    ", @xmath86 is _ essentially smooth _",
    "( see , e.g. , @xcite , page 44 ) . a sufficient condition for and essential smoothness is openness of @xmath87",
    "( see @xcite , page 905 , and  @xcite , pages 505506 ) .    1 .",
    "@xmath88 , where @xmath89 is the closure of the convex hull of the support of the law of @xmath90 .",
    "we will also need the following lemma showing the equivalence between assumption and the so - called _ cramr condition _",
    "@xmath91 , for any @xmath92 .",
    "[ lm - cramerconditionandeta - int]under assumption , the following conditions are equivalent :    assumption holds ;    @xmath91 , for any @xmath92 .    as concerns the saddlepoint approximation of section  [ sect - saddlepointapproximation ] , we need the following assumption :    1 .   the inequality @xmath93\\biggr|\\\\ & & \\quad<(1-\\delta)\\cdot \\biggl|\\mathbb{e}_{0}\\biggl[\\prod_{j=0,\\ldots , j , j\\neq i}\\biggl(\\frac{q(y;\\theta _ { i})}{q(y;\\theta_{j})}\\biggr)^{u_{j}}\\biggr]\\biggr|\\\\ & & \\quad<\\infty\\end{aligned}\\ ] ] holds for @xmath94 , @xmath77 and @xmath95 ( @xmath96 denotes the imaginary unit ) .      in this section",
    "we consider large deviations asymptotics .",
    "wenote that , in what follows , @xmath97 stands for@xmath98^{c}\\ } $ ] .    [ pr - convergencerateofmle2 ] for @xmath99 , under assumption , the following result holds : @xmath100 where @xmath101 is a function such that@xmath102 .    under assumptions and : @xmath103 where @xmath104 is a function such that@xmath105 .    under assumptions , , and : @xmath106    [ pr - convergencerateofmle]under assumption , the following inequality holds : @xmath107 where @xmath108 is the finite cardinality of the set@xmath109 and @xmath101 is a function such that @xmath102 .    under assumptions and : @xmath110 where @xmath104 is a function such that@xmath105 .",
    "the proposition allows us to obtain an upper bound on the bias of the @xmath0-estimator , @xmath111 .",
    "a better description of the asymptotic behavior of the probability @xmath53 could be obtained , under some additional conditions , from the study of the neighborhood of the contact point between the set @xmath112 and the level sets of the cramr transform @xmath113 .",
    "we leave the topic for future work . here",
    "we just remark the following brackets on the convergence rate .",
    "[ pr - neyconvergencerateofmle ] under assumptions , , and , for sufficiently large @xmath114 , the following result holds : @xmath115 for @xmath99 and for some @xmath116 .",
    "when @xmath59 , a more precise convergence rate can be obtained under the following assumption :    1 .   when @xmath117 , there is a positive value @xmath118 such that @xmath119 . moreover , the law of @xmath120 is nonlattice ( see @xcite , page  110 ) .",
    "[ pr - exactasymptoticsinr ] @xmath80under assumptions , ,  , and , with @xmath121 and @xmath59 , we have @xmath122    a refinement of the previous asymptotic rates can be obtained using results in @xcite .      in this section",
    "we consider a different kind of approximation of the probabilities @xmath53 .",
    "[ th - saddlepointapproximation ] @xmath123under assumptions , and  , for @xmath124 , it is possible to choose @xmath125 such that , for every @xmath126 $ ] , @xmath127 and @xmath128\\biggr)\\\\ & & { } \\cdot\\bigl[e_{s-3}\\bigl(\\mathbf{u},\\operatorname{int } \\mathbb{r}_{+}^{j}\\ominus \\mathbb{e}_{0}\\mathbf{x}^{(i)}\\bigr)\\\\ & & \\hspace*{9pt}\\phantom{\\bigl[}{}+\\delta\\bigl(\\mathbf{u},\\operatorname{int } \\mathbb { r}_{+}^{j}\\ominus\\mathbb{e}_{0}\\mathbf{x}^{(i)}\\bigr)\\bigr],\\end{aligned}\\ ] ] where @xmath129\\,\\mathrm{d}\\mathbf{y},\\\\ & & q_{\\ell\\mathbf{u}}(\\mathbf{x } ) \\\\ & & \\quad =   \\sum_{m=1}^{\\ell}\\frac{1}{m!}{\\sum}^{\\ast}{\\sum}^{\\ast\\ast}\\biggl(\\frac{\\kappa_{\\nu_{1}n}\\cdots\\kappa_{\\nu _ { m}n}}{\\nu_{1}!\\cdots\\nu_{m}!}\\biggr)\\\\ & & \\qquad{}\\cdot h_{i_{1}}(x_{1})\\cdots h_{i_{d}}(x_{d}),\\\\ & & \\bigl|\\delta\\bigl(\\mathbf{u},\\operatorname{int }",
    "\\mathbb{r}_{+}^{j}\\ominus\\mathbb { e}_{0}\\mathbf{x}^{(i)}\\bigr)\\bigr| \\\\ & & \\quad\\leq c\\cdot n^{-{(s-2)}/{2}}\\end{aligned}\\ ] ] and @xmath130 , @xmath131 , @xmath132 , @xmath133 , @xmath134 is the usual hermite ",
    "chebyshev polynomial of degree @xmath0 , @xmath135 denotes the sum over all @xmath0-tuples of positive integers",
    "@xmath136 satisfying @xmath137 , @xmath138 denotes the sum over all @xmath0-tuples @xmath139 with @xmath140 , satisfying @xmath141 , and @xmath142 . note that @xmath143 depends on @xmath125 through the cumulants calculated at  @xmath125 .",
    "the main question that this theorem leaves open is the choice of the point @xmath125 .",
    "usually this point is chosen as a solution @xmath144 of @xmath145 ; this corresponds to a saddlepoint in @xmath146 .",
    "@xcite ( section 6 ) and @xcite ( page 480 ) give some conditions for @xmath59 ; @xcite ( page 23 ) and @xcite ( page 153 ) give conditions for general @xmath147 .",
    "@xcite suggests that the most common solution is to choose @xmath148 and @xmath144 ( @xmath148 belonging to the boundary of @xmath149 $ ] and @xmath144 solving @xmath145 ) , such that for every @xmath150 $ ] , @xmath151 .",
    "this is the same as a dominating point in @xcite ; therefore , , and , for sufficiently large @xmath114 , imply the existence of this point for any @xmath152.=1      in this section , we show how the previous results can be applied to the and bayes estimators  under the zero",
    " one loss function .",
    "the is defined  by @xmath153.\\end{aligned}\\ ] ] this corresponds to the _ minimum - error - probability estimate _",
    "of @xcite and to the _ bayesian estimator _ of @xcite . on the other hand , using the prior densities given by @xmath154 for @xmath46 ,",
    "the posterior densities of the bayesian estimator are given by @xmath155 the bayes estimator relative to zero ",
    "one loss @xmath156 ( see section  [ sect - riskfunctions ] for a definition ) is the mode of the posterior distribution and is given by @xmath157 \\\\[-8pt ] \\nonumber & = & \\arg\\max_{\\theta\\in\\theta}\\biggl[\\frac{1}{n}\\sum_{i=1}^{n}\\ln f_{y_{i}}(y_{i};\\theta)+\\frac{\\ln\\pi(\\theta)}{n}\\biggr].\\\\end{aligned}\\ ] ] note that the coincides with the bayes estimator corresponding to the uniform distribution @xmath158 for any @xmath46 .",
    "assumption can be replaced by the ones ( where assumptions and entail that  the likelihood function is asymptotically maximized at  @xmath2 only):=-1    1 .   the parametric statistical model @xmath159 is formed by a set of probability measures on a measurable space @xmath160 indexed by a parameter @xmath6 ranging over a parameter space @xmath161 , of finite cardinality .",
    "let @xmath39 be a measurable space and @xmath162 a positive @xmath33-finite measure defined on @xmath39 such that , for every @xmath46 , @xmath163 is equivalent to @xmath162 ; the densities @xmath164 are @xmath45-measurable for each @xmath46 .",
    "+ the data @xmath38 are i.i.d .",
    "realizations from the probability measure @xmath40 .",
    "2 .   the log density satisfies the @xmath47-dominationcondition @xmath165 , for @xmath166,where  @xmath49 denotes the expectation taken under the true probability measure @xmath40 .",
    "@xmath2 is the point of @xmath4 maximizing @xmath167 and is globally identified .    in order to obtain the consistency of bayes estimators , we need the following assumption on the behavior of the prior distribution :    1 .   the prior distribution verifies @xmath168 for any @xmath46 .",
    "proposition  [ pr - consistencyofthemle ] holds for the under assumptions  , and , while for bayes estimators   is required , too .",
    "note that , under correct specification ( i.e. , when the true parameter value belongs to  @xmath4 ) , a  standard wald s argument ( see , e.g. , lemma  2.2 in  @xcite , page 2124 ) shows that @xmath169 is maximized for @xmath170 .    as concerns the distribution of the",
    ", we have to consider the case in which @xmath171 is given by @xmath172 , @xmath173 by the log - likelihood function @xmath174 , and @xmath61 and @xmath62 by the log - likelihood processes : @xmath175_{j=0,\\ldots , j , j\\neq i},\\vspace*{2pt}\\cr \\mathbf{x}_{k}\\triangleq[\\ln f_{y_{k}}(y_{k};\\theta_{0})-\\ln f_{y_{k}}(y_{k};\\theta_{j})]_{j=1,\\ldots , j}.}\\ ] ] also @xmath68 and @xmath176 are consequently defined .",
    "propositions  [ pr - convergencerateofmle2 ] and  [ pr - convergencerateofmle ] hold when assumption is replaced by assumptions  , and .    when the model is correctly specified , it is interesting to stress an interpretation of the moment generating function in discrete parameter models .",
    "we note that the moment generating functions can be written as follows : @xmath177}\\bigr]\\nonumber\\\\ & = & \\int f_{y}(y;\\theta_{i})^{\\sum_{j=0,\\ldots , j , j\\neq i}\\lambda_{j } } \\nonumber \\\\[-8pt ] \\\\[-8pt ] \\nonumber & & \\phantom{\\int}{}\\cdot \\prod_{j=1,\\ldots , j , j\\neq i}f_{y}(y;\\theta_{j})^{-\\lambda_{j}}\\\\ & & \\hspace*{44pt}\\qquad{}\\cdot f_{y}(y;\\theta_{0})^{1-\\lambda_{0}}\\mu(\\mathrm{d}y).\\nonumber\\end{aligned}\\ ] ] therefore , in this case , the moment generating function @xmath176 reduces to the so - called hellinger transform @xmath178 ( see @xcite , page 43 ) for a certain linear transformation of @xmath179 in @xmath180 : @xmath181^{\\gamma_{j}}\\\\ & & \\quad=\\int \\biggl[\\prod_{j=0}^{j}f_{y}(y;\\theta_{j})^{\\gamma_{j}}\\biggr]\\mu(\\mathrm{d}y),\\quad \\sum _ { j=0}^{j}\\gamma_{j}=1.\\end{aligned}\\ ] ] moreover , due to its convexity , @xmath182 is surely finite for @xmath180 belonging to the closed simplex in @xmath183 .",
    "proposition  [ pr - neyconvergencerateofmle ] holds if assumption is replaced by assumptions , and , and if and   hold true .",
    "however , assumption is unnecessary ; indeed , the fact that @xmath88 can be proved showing that @xmath184 .",
    "this is equivalent to the existence , for @xmath185 , of two sets @xmath186 and  @xmath187 of positive @xmath162-measure and included in the support of  @xmath188 such that , for @xmath189 and @xmath190 , @xmath191 and @xmath192 .",
    "this follows easily noting that these densities have to integrate to  @xmath193 , are almost surely ( a.s . ) different according to assumption and have the same support according to assumption .    in order to derive the distribution of bayes estimators , we consider equation ( [ eq - definitionofthebayesestimators ] ) and we let @xmath194_{j=0,\\ldots , j , j\\neq i}$ ]",
    "then , we can write @xmath195 and we can use the previous large deviations or saddlepoint formulas , simply changing the set over which the @xmath196 is taken .",
    "however , care is needed since both formulas hold under the assumption @xmath197 in the case @xmath59 , the similarity of these formulas with the corresponding ones for a neyman  pearson test is striking ; this revives the interpretation of a neyman  pearson test as a bayesian estimation problem .",
    "therefore , our analysis can be seen as a  ( minor ) extension of the theory of hypothesis testing to a larger number of alternatives .",
    "in this section , we are interested in the problem of efficiency , with special reference to maximum likelihood and bayes estimators . in",
    "what follows , we will suppose that the true parameter value belongs to @xmath4 ; this will be reflected in the probabilities that will be written as @xmath198 .",
    "indeed , efficiency statements for misspecified models are quite difficult to interpret .    in the statistics literature , efficiency ( or superefficiency )",
    "can be defined comparing the behavior of the estimator with respect to a lower bound or , alternatively , to a class of estimators . in the continuous case , the two concepts almost coincide ( despite superefficiency ) . however , in the discrete case , the two concepts diverge dramatically and we need more care in the derivation of the information inequalities and in the statement of the efficiency properties .",
    "an interesting problem concerns the choice ofa  measure of efficiency for the in discrete parameter models : in his seminal paper , hammersley  @xcite derives a generalization of cramr  rao inequality for the variance that is also valid when the parameter space is countable .",
    "the same inequality has been derived , in slightly more generality , in @xcite . however",
    ", this choice is well - suited only in cases in which the is a good measure of risk , for example , if the limiting distribution of the normalized estimator is normal . following the discussion by lindley in  @xcite",
    ", we consider a different cost function @xmath199 , whose risk function is given by the probability of missclassification : @xmath200 we also define the _ bayes risk _",
    "( under the zero  one loss function ) associated with a prior distribution @xmath201 on the parameter space @xmath4 . in particular , we consider the bayes risk under the risk function @xmath202 as @xmath203 if @xmath204",
    "we define @xmath205 as the _ average probability of error_. note that this is indeed the measure of error used by @xcite .    using the risk function @xmath206 , in section  [ sect - informationinequalities ] we derive some information inequalities and we prove in section  [ sect - optimalityandefficiency ] some optimality and efficiency results for bayes and estimators . in section  [ sect - riskfunctions ]",
    "we briefly deal with alternative risk functions .",
    "this section contains lower bounds for the previously introduced risk function @xmath206 . in the specific case of discrete parameters ,",
    "these generalize and unify the lower bounds proposed in @xcite . in the following ,",
    "first of all , a lower bound is proved and then a minimax version of the same result is obtained .",
    "when needed , we will refer to the former as _ chapman  robbins lower bound _ ( and to the related efficiency concept as _ chapman  robbins _ _ efficiency _ ) since it recalls the lower bound proposed by these two authors in their 1951 paper , and to the latter as _ minimax chapman ",
    "robbins lower bound_. then , from these results , we derive a lower bound for the bayes risk .",
    "the proposition of this section is intended to play the role of cramr  rao and chapman  robbins lower bounds for the variance .",
    "it corresponds essentially to stein s lemma in hypothesis testing .",
    "moreover , a  version of the same bound for estimators respecting ( [ eq - partiallyconsistentestimator ] ) is provided ; this corresponds to a similar result proposed in  @xcite    [ pr - bahadurlowerbound]under assumptions and , for a strongly consistent estimator @xmath208 : @xmath209 \\\\[-8pt ] \\nonumber & & \\quad \\ge \\sup_{\\theta_{1}\\in\\theta\\setminus\\ { \\theta _ { 0}\\ } } \\mathbb{e}_{\\mathbb{\\theta}_{1}}\\ln\\biggl(\\frac{f_{y}(y;\\theta _ { 0})}{f_{y}(y;\\theta_{1})}\\biggr).\\end{aligned}\\ ] ] on the other hand , if @xmath210 then @xmath211    [ rm - iroptimalityandcroptimality ] note that this inequality only holds for estimators that are consistent or respect condition  ( [ eq - partiallyconsistentestimator ] ) , while the one of proposition  [ pr - chapman - robbinslowerbound ] holds for any estimator .",
    "proposition  [ pr - bahadurlowerbound ] provides an upper bound for the _ inaccuracy rate _ of @xcite : @xmath212 for any @xmath16 small enough ( @xmath213 ) .",
    "the following result is a minimax lower bound on the probability of misclassification .",
    "it is based on the neyman ",
    "pearson lemma and chernoff s bound .",
    "[ pr - chapman - robbinslowerbound]under assumptions and , for any estimator @xmath214 : @xmath215.\\nonumber\\end{aligned}\\ ] ]    [ rm - hallbound ] the previous proposition provides an expression for the _ minimax bahadur risk _ ( also called _ ( minimax ) rate of inaccuracy _ ; see ) analogous to chernoff s bound , thus providing a  minimax version of remark  [ rm - iroptimalityandcroptimality ] .    other methods to derive similar minimax inequalities are fano s inequality and assouad s lemma ( see @xcite , page 220 ) ; however , in the present case they do not allow us to obtain tight bounds , since the usual application of these methods relies on the approximation of the parameter space with a finite set of points @xmath4 whose cardinality increases with  @xmath114 . clearly , this can not be done in the present case .",
    "using lemma 5.2 in @xcite , it is possible to show that the minimax bound is larger than the classical one .    under assumption",
    ", the bayes risk @xmath216 under the risk function @xmath206 and the prior @xmath201 respects the equality @xmath217 then , proposition  [ pr - chapman - robbinslowerbound ] holds also for the bayes risk : clearly this bound is independent of the prior distribution @xmath201 ( provided it is strictly positive , i.e. , holds ) and also holds for the probability of error @xmath218 .",
    "this inequality can be seen as an asymptotic version of the van trees inequality for a different risk function .      in this section ,",
    "we establish some optimality results for the in discrete parameter models .",
    "the situation is much more intricate than in regular statistical models under the quadratic loss function , in which efficiency coincides with the attainment of the cramr ",
    "rao lower bound ( despite superefficiency ) .",
    "therefore , we propose the following definition .",
    "we denote by @xmath219 the risk function of the estimator @xmath220 evaluated at @xmath2 , and by @xmath221 a class of estimators .",
    "the estimator @xmath220 is _ efficient with respect to _ ( w.r.t . ) @xmath221 _ and _ w.r.t . @xmath222",
    "_ at _ @xmath2 if @xmath223 the estimator @xmath220 is _ minimax efficient _ w.r.t .",
    "@xmath221 _ and _ w.r.t . @xmath222",
    "if @xmath224 the estimator @xmath220 is _ superefficient _ w.r.t .",
    "@xmath221 _ _ and__w.r.t .",
    "@xmath222 if for every @xmath225 : @xmath226 for every @xmath1 and there exists at least a value @xmath227 such that the inequality is replaced by a strict inequality for @xmath228 .",
    "the estimator @xmath220 is _ asymptotically _ @xmath229-_efficient _ w.r.t .",
    "@xmath222 _ at _ @xmath2 if it attains the chapman  robbins lower bound of proposition  [ pr - bahadurlowerbound ] at @xmath2 [ say @xmath230 in the asymptotic form : @xmath231 the estimator @xmath220 is _ asymptotically minimax _ @xmath232-_efficient",
    "@xmath222 if it attains the minimax chapman  robbins lower bound of proposition  [ pr - chapman - robbinslowerbound ] ( say @xmath233 ) in the asymptotic form : @xmath234 the estimator @xmath220 is _ asymptotically _ @xmath229-_superefficient _ w.r.t . @xmath222 if @xmath235 for every @xmath1 and there exists at least a value @xmath227 such that the inequality is replaced by a strict inequality for @xmath228 .    as in remark",
    "[ rm - iroptimalityandcroptimality ] , it is easy to  see that @xmath236-optimality and @xmath232-efficiency w.r.t .",
    "@xmath206 coincide .    the efficiency landscape offered by discrete parameter models",
    "will be illustrated by example  [ exam - integermeanofagaussiansample ] .",
    "this shows that , even in the simplest case , that is , the estimation of the integer mean of a gaussian random variable with known variance , the does not attain the lower bound on the missclassification probability but it attains the minimax lower bound .",
    "moreover , simple estimators are built that outperform the for certain values of the true parameter value @xmath2 .",
    "[ exam - integermeanofagaussiansample]let us consider the estimation of the mean of a gaussian distribution whose variance  @xmath237 is known : we suppose that the true mean is  @xmath238 , while the parameter space is @xmath239 , where  @xmath238 is known .",
    "the maximum likelihood estimator @xmath41 takes the value @xmath240 if the sample mean takes on its value in @xmath241 and @xmath238 if it falls in @xmath242 ( the position of @xmath243 is a convention ) .",
    "therefore : @xmath244 & = & \\int_{-\\infty}^{0}\\frac { e^{-{(\\bar{y}-\\alpha)^{2}}/{(2\\sigma^{2}/n)}}}{\\sqrt{2\\pi\\sigma ^{2}/n}}\\,\\mathrm{d}\\bar{y}\\\\ & = & \\int_{-\\infty}^{-{\\sqrt{n}\\alpha}/{\\sigma } } \\frac{e^{-{t^{2}}/{2}}}{\\sqrt{2\\pi}}\\,\\mathrm{d}t\\\\ & = & \\phi\\biggl(-\\frac{\\sqrt{n}\\alpha}{\\sigma}\\biggr)\\\\ & = & \\frac{e^{-{n\\alpha ^{2}}/{(2\\sigma^{2})}}}{\\sqrt{2\\pi n}}\\frac{\\sigma}{\\alpha}\\cdot\\biggl(1+o\\biggl(\\frac{1}{n}\\biggr)\\biggr),\\end{aligned}\\ ] ] where we have used problem 1 on page 193 in @xcite .",
    "proposition  [ pr - exactasymptoticsinr ] allows also for recovering the right convergence rate .",
    "indeed , we have @xmath245 on the other hand , the lower bound of proposition  [ pr - bahadurlowerbound ] yields @xmath246 and the lower bound of proposition  [ pr - chapman - robbinslowerbound ] yields @xmath247 therefore , the asymptotically attains the minimax lower bound but not the classical one .    in the following",
    ", we will show that estimators can be pointwise more efficient than the ; consider the estimator defined by @xmath248 when @xmath249 , @xmath250 coincides with the @xmath41 .",
    "then , the behavior of the estimator is characterized by the probabilities : @xmath251 we have ( weak ) consistency if @xmath252 the risk @xmath253 under @xmath2 is then @xmath254;\\ ] ] this can be made smaller than the probability of error of the simply taking @xmath255 , thus implying that the is not pointwise efficient .",
    "now , we show that this estimator can not converge faster than the chapman  robbins lower bound without losing its consistency .",
    "indeed , @xmath256 is smaller than the chapman  robbins lower bound if @xmath257 and this is never true under ( [ eq - consistencyconditionsintheexample ] ) .",
    "if this estimator is pointwise more efficient than the under @xmath2 , then its risk under @xmath258 is given by @xmath259,\\ ] ] and this is greater than for the .",
    "this shows that a faster convergence rate can be obtained in some points , the price to pay being a worse convergence rate elsewhere in @xmath4 .      in the following section ,",
    "we show some optimality properties of bayes and estimators .",
    "we start with an important and well - known fact .",
    "[ pr - minimizationofthebayesrisk]under , , and , the bayes risk @xmath260 ( under the zero ",
    "one loss function ) associated with a prior distribution @xmath201 is strictly minimized by the posterior mode corresponding to the prior @xmath201 , for any finite @xmath114 .",
    "the following proposition shows that the is admissible and minimax efficient under the zero  one loss and minimizes the average probability of error .",
    "it implies that estimators that are more efficient than the at a certain point @xmath261 are less efficient in at least another point @xmath262 . as a  result , estimators can be more efficient than minimax efficient ones only on portions of the parameter space , but are then strictly less efficient elsewhere .",
    "[ pr - admissibilityandminimaxefficiencyofmle]under assumptions , and  , the is admissible and minimax efficient w.r.t .",
    "the class of all estimators and w.r.t .",
    "@xmath206 and minimizes the average probability of error @xmath218 .      in this subsection , we will show that the does not attain the chapman  robbins lower bound in the form of proposition  [ pr - bahadurlowerbound ] but that it attains the minimax form of proposition  [ pr - chapman - robbinslowerbound ] and that efficiency and minimax efficiency are generally incompatible .",
    "therefore , the situation described in example  [ exam - integermeanofagaussiansample ] is general , for it is possible to show that the is generally inefficient with respect to the lower bounds exposed in proposition [ pr - bahadurlowerbound ] .    [ pr - proofofnon - ir - optimality]under assumptions , and :    the is not asymptotically @xmath232-efficient w.r.t",
    ". @xmath206 at @xmath2 ;    the is asymptotically minimax @xmath232-efficient w.r.t .",
    "@xmath206 ;    an estimator that is asymptotically @xmath232-efficient w.r.t .",
    "@xmath206 at @xmath2 is not asymptotically minimax @xmath232-efficient w.r.t . @xmath206 .",
    "the assumption of homogeneity of the probability measures , necessary to derive , can be removed in the proof of along the lines of  @xcite .      ever since it was discovered by hodges ,",
    "the problem of superefficiency has been dealt with extensively in regular statistical problems ( see , e.g. , @xcite ) . however , these proofs do not transpose to discrete parameter estimation problems , since they are mostly based on the equivalence of prior probability measures with the lebesgue measure and on properties of bayes estimators that do not hold in this case .",
    "moreover , the discussion of the previous sections has shown that , in discrete parameter problems , @xmath229-efficiency and efficiency with respect to a class of estimators do not coincide .",
    "the following proposition yields a  solution to the superefficiency problem .",
    "[ pr - superefficiency]under assumptions , and :    no estimator @xmath214 is asymptotically @xmath232-superefficient w.r.t .",
    "@xmath206 at @xmath261 ;    no estimator @xmath214 is superefficient w.r.t .",
    "the and @xmath206 .",
    "now we consider in what measure the previous results transpose when changing the risk function .",
    "following @xcite , we first consider the quadratic cost function and the corresponding risk function : @xmath263 the cost function @xmath264 has the drawback of weighting in the same way points of the parameter space that lie at different distances with respect to the true value @xmath2 . in many cases , a more general loss function can be considered , as suggested in @xcite ( volume 1 , page 51 ) for multiple tests : @xmath265 where @xmath266 for @xmath267 can be tuned in order to give more or less weight to different points of the parameter space .",
    "the risk function is therefore given by the weighted probability of misclassification @xmath268 .",
    "it is trivial to remark that @xmath269 and the lower bounds of propositions  [ pr - bahadurlowerbound ] and  [ pr - chapman - robbinslowerbound ] hold also in this case .",
    "the same equalities hold also for  @xmath270 . as a result ,",
    "proposition  [ pr - proofofnon - ir - optimality ] and proposition  [ pr - superefficiency ] apply also to these risk functions .    on the other hand ,",
    "as concerns proposition  [ pr - admissibilityandminimaxefficiencyofmle ] and proposition  [ pr - superefficiency ] , it is simple to show that with respect to the risk functions @xmath271 and @xmath272 , the results hold only asymptotically ( see @xcite , for asymptotic minimax efficiency of the estimator of the integral mean of a gaussian sample with known variance ) .",
    "proof of proposition [ pr - consistencyofthemle ] under , kolmogorov s implies that @xmath40-a.s .",
    "@xmath273 , and for @xmath40-a.s .",
    "any sequence of realizations , @xmath41 converges to @xmath2 .",
    "measurability follows from the fact that the following set belongs to @xmath274 : @xmath275    proof of lemma [ lm - cramerconditionandeta - int ] clearly implies for a certain @xmath276 . on the other hand ,",
    "suppose that holds ; then , applying recursively hlder inequality : @xmath277\\\\ & \\leq&\\sum_{j=0,\\ldots , j , j\\neq i}\\frac{1}{j}\\cdot\\ln\\mathbb { e}_{0}\\biggl[\\biggl(\\frac{q(y;\\theta_{i})}{q(y;\\theta_{j})}\\biggr)^{j\\cdot\\lambda_{j}}\\biggr]\\end{aligned}\\ ] ] and choosing the @xmath278 s adequately , we get .",
    "proof of proposition [ pr - convergencerateofmle2 ] the first two results are straightforward applications of cramr s theorem in @xmath279 ( see , e.g. , @xcite , corollary 6.1.6 , page 253 ) .",
    "indeed , it is known that the lower bound holds without any supplementary assumption , while the upper bound requires a cramr condition @xmath280 ; indeed , from lemma  [ lm - cramerconditionandeta - int ] , this is equivalent to assumption  .",
    "then , a full holds : @xmath281 & & \\quad \\geq-\\inf_{\\mathbf{y}\\in\\operatorname{int } \\mathbb { r}_{+}^{j}}\\sup_{\\bolds{\\lambda}\\in\\mathbb{r}^{j}}\\bigl\\ { \\langle\\mathbf { y},\\bolds{\\lambda}\\rangle-\\lambda^{(i)}(\\bolds{\\lambda})\\bigr\\ } , \\\\[-2pt ] & & \\limsup_{n\\rightarrow\\infty}\\frac{1}{n}\\ln\\mathbb{p}_{0}(\\hat{\\theta } ^{n}=\\theta_{i } )   \\\\[-2pt ] & & \\quad\\leq-\\inf_{\\mathbf{y}\\in\\mathbb{r}_{+}^{j}}\\sup _ { \\bolds{\\lambda}\\in\\mathbb{r}^{j}}\\bigl\\ { \\langle\\mathbf{y},\\bolds { \\lambda}\\rangle-\\lambda^{(i)}(\\bolds{\\lambda})\\bigr\\ } .\\end{aligned}\\ ] ]    in order to prove the final result , we have to show that @xmath282 is a @xmath283-_continuity set _ , that is,@xmath284 .",
    "it is enough to apply part ( ii ) in lemma on page 903 of @xcite .",
    "proof of proposition [ pr - convergencerateofmle ] first of all , we note that @xmath285",
    ". therefore , we can apply large deviations principles , with the candidate rate function @xmath70 ; this is a strictly convex function on @xmath286 globally minimized at @xmath287_{j=1,\\ldots , j}.\\ ] ] by assumption , @xmath288 is finite and belongs to @xmath289 . from the strict convexity of the level sets of @xmath70 ,",
    "the set @xmath290 has at most finite cardinality @xmath108 .",
    "moreover , since large deviations theory allows us to ignore the part of @xmath97 where @xmath291 , we can replace @xmath292 with a collection of @xmath108 disjoint sets , say @xmath293 , @xmath294 , each of them containing in its interior one and only one of the points of @xmath295 ( see @xcite , page 508 ) : @xmath296   & & \\quad=\\bigl(1+o(1)\\bigr)\\cdot\\mathbb{p}_{0}\\biggl(\\sum_{k=1}^{n}\\mathbf { x}_{k}\\in\\operatorname{int } \\bigcup_{h=1}^{h}\\gamma_{h}\\biggr)\\\\[-2pt ] & & \\quad = \\bigl(1+o(1)\\bigr)\\cdot\\sum_{h=1}^{h}\\mathbb{p}_{0}\\biggl(\\sum_{k=1}^{n}\\mathbf { x}_{k}\\in\\operatorname{int } \\gamma_{h}\\biggr).\\nonumber\\end{aligned}\\ ] ] as before , the bounds derive from cramr s theorem in @xmath279 .",
    "noting that the contribution of any @xmath293 is the same and recalling ( [ eq - setrepresentationasgammah ] ) , we get the results .",
    "proof of proposition [ pr - neyconvergencerateofmle ] the assumptions of the theorem on page 904 of @xcite are easily verified .",
    "this shows that a unique dominating point  @xmath297 exists and implies , through proposition on page  161 of  @xcite ( according to the `` remarks on the hypotheses '' in @xcite , page 905 , the `` lattice '' conditions are not necessary ) , that the stated bracketing of @xmath53 holds .",
    "proof of proposition [ pr - exactasymptoticsinr ] @xmath298under assumptions  , , and , according to proposition  [ pr - convergencerateofmle2 ] we have @xmath299 and we can study the behavior of @xmath300 assumption implies that the conditions of theorem 3.7.4 in @xcite ( page 110 ) are verified , in particular the existence of a positive @xmath301 solution to the equation @xmath302 . from lemma 2.2.5(c ) in  @xcite , this implies @xmath303 , and the result follows .",
    "proof of theorem [ th - saddlepointapproximation ] we note that the function @xmath304 in @xcite ( page 1117 ) is given by @xmath305\\\\[1pt ] & = & \\ln\\mathbb{e}_{0}\\exp\\bigl[\\mathbf{u}\\cdot\\mathbf{x}^{(i)}\\bigr]-\\mathbf { u}\\cdot\\mathbb{e}_{0}\\mathbf{x}^{(i)}\\\\ & = & \\lambda^{(i)}(\\mathbf{u})-\\mathbf{u}\\cdot\\mathbb{e}_{0}\\mathbf{x}^{(i)}.\\vspace*{1pt}\\end{aligned}\\ ] ] therefore , we write the mean @xmath306 and covariance matrix @xmath307 as @xmath308 \\mathbf{v}(\\mathbf{u } ) & = & \\kappa^{\\prime\\prime}(\\mathbf{u})=\\frac { \\partial^{2}\\kappa(\\mathbf{u})}{\\partial\\mathbf{u}^{2}}=\\frac{\\partial ^{2}\\lambda^{(i)}(\\mathbf{u})}{\\partial\\mathbf{u}^{2}}.\\vspace*{1pt}\\end{aligned}\\ ] ] from ( [ eq - probabilityforthetaiinldform ] ) , we have @xmath309 & & \\hspace*{-3pt}\\quad = \\mathbb{p}_{0}\\biggl(\\sum _ { k=1}^{n}\\mathbf{x}_{k}^{(i)}\\in\\operatorname{int } ( \\mathbb{r}_{+}^{j})\\biggr)\\\\[1pt ] & & \\hspace*{-3pt}\\quad = \\mathbb{p}_{0}\\biggl\\ { \\frac{1}{n}\\cdot\\sum_{k=1}^{n}\\bigl(\\mathbf { x}_{k}^{(i)}-\\mathbb{e}_{0}\\mathbf{x}^{(i)}\\bigr)\\in\\operatorname{int } ( \\mathbb { r}_{+}^{j})\\ominus\\mathbb{e}_{0}\\mathbf{x}^{(i)}\\biggr\\ } .\\end{aligned}\\ ] ]    now we verify assumptions ( s.1)(s.4 ) of @xcite .",
    "assumption ( s.1 ) is implied by .",
    "assumptions ( s.2 ) and ( s.3 ) hold since the random vectors are i.i.d . and nontrivial . at last , ( s.4 )",
    "is implied by ( see , e.g. ,  @xcite , page 735 ) . since @xmath310 is strictly negative by , @xmath311 does not contain @xmath312 and , according to theorem 1 in @xcite ( page 1118 ) , the result of the theorem follows .",
    "proof of proposition [ pr - bahadurlowerbound ] first of all , weprove  ( [ eq - bahadurbound - kldivergence ] ) .",
    "we suppose that @xmath313 otherwise the inequality is trivial .",
    "then , for any @xmath314 , we apply lemma 3.4.7 in @xcite ( page  94 ) with @xmath315 and @xmath316;since  @xmath214 is strongly consistent , @xmath317 is ultimately less than any @xmath318 and the bound holds .",
    "the second part can be proved as follows .",
    "define the sets @xmath319 b_{n}(j ) & = & \\biggl\\ { \\omega\\dvtx \\frac{1}{n}\\ln\\biggl(\\frac{f_{y}(y;\\theta _ { j})}{f_{y}(y;\\theta_{0})}\\biggr)\\\\[-2pt ] & & \\hspace*{6pt}\\le\\mathbb{e}_{\\mathbb{\\theta}_{j}}\\ln\\biggl(\\frac { f_{y}(y;\\theta_{j})}{f_{y}(y;\\theta_{0})}\\biggr)+\\varepsilon\\biggr\\}.\\end{aligned}\\ ] ] therefore , we have @xmath320 & & \\quad=\\mathbb{e}_{\\theta_{0}}\\mathsf{1}\\ { \\tilde{\\theta}^{n}\\neq\\theta_{0}\\ } \\\\[-1pt ] & & \\quad=\\mathbb{e}_{\\theta_{j}}\\frac{f_{y}(y;\\theta_{0})}{f_{y}(y;\\theta _ { j})}\\mathsf{1}\\ { \\tilde{\\theta}^{n}\\neq\\theta_{0}\\ } \\\\[-1pt ] & & \\quad \\ge \\mathbb{e}_{\\theta_{j}}\\frac{f_{y}(y;\\theta _ { 0})}{f_{y}(y;\\theta_{j})}\\mathsf{1}\\ { a_{n}(j)\\ } \\\\[-1pt ] & & \\quad \\ge \\mathbb{e}_{\\theta_{j}}\\mathsf{1}\\ { a_{n}(j)\\ } \\mathsf{1}\\ { b_{n}(j)\\}\\\\[-1pt ] & & \\qquad { } \\cdot\\exp\\biggl\\ { -n\\cdot\\biggl[\\mathbb{e}_{\\mathbb{\\theta}_{j}}\\ln\\biggl(\\frac { f_{y}(y;\\theta_{j})}{f_{y}(y;\\theta_{0})}\\biggr)+\\varepsilon\\biggr]\\biggr\\ } \\\\[-1pt ] & & \\quad \\ge [ 1-\\mathbb{p}_{\\theta_{j}}\\ { a_{n}^{c}(j)\\ } -\\mathbb{p}_{\\theta _ { j}}\\ { b_{n}^{c}(j)\\ } ] \\\\[-1pt ] & & \\qquad{}\\cdot\\exp\\biggl\\ { -n\\cdot\\biggl[\\mathbb{e}_{\\mathbb{\\theta } _ { j}}\\ln\\biggl(\\frac{f_{y}(y;\\theta_{j})}{f_{y}(y;\\theta_{0})}\\biggr)+\\varepsilon \\biggr]\\biggr\\ } \\\\[-1pt ] & & \\quad \\ge [ 1-\\mathbb{p}_{\\theta_{j}}\\ { \\tilde{\\theta}^{n}\\neq\\theta _ { j}\\ } -\\mathbb{p}_{\\theta_{j}}\\ { b_{n}^{c}(j)\\ } ] \\\\[-1pt ] & & \\qquad{}\\cdot\\exp\\biggl\\ { -n\\cdot \\biggl[\\mathbb{e}_{\\mathbb{\\theta}_{j}}\\ln\\biggl(\\frac{f_{y}(y;\\theta _ { j})}{f_{y}(y;\\theta_{0})}\\biggr)+\\varepsilon\\biggr]\\biggr\\ } .\\end{aligned}\\ ] ] this implies : @xmath321 & & \\quad \\ge -\\mathbb{e}_{\\mathbb{\\theta } _ { j}}\\ln\\biggl(\\frac{f_{y}(y;\\theta_{j})}{f_{y}(y;\\theta_{0})}\\biggr)-\\varepsilon\\\\[-2pt ] & & \\qquad { } + \\liminf_{n\\rightarrow\\infty}\\frac{1}{n}\\ln[1-\\mathbb{p}_{\\theta _ { j}}\\ { \\tilde{\\theta}^{n}\\neq\\theta_{j}\\ } -\\mathbb{p}_{\\theta_{j}}\\ { b_{n}^{c}(j)\\ } ] .\\vadjust{\\goodbreak}\\end{aligned}\\ ] ] now , since @xmath322 and@xmath323 , the third term in the right - hand side goes to zero ; since @xmath16 is arbitrary , the result follows .    proof of proposition [ pr - chapman - robbinslowerbound ] from the neyman ",
    "pearson lemma , we have @xmath324 for an arbitrary couple of different alternatives @xmath2 and @xmath258 in @xmath4",
    ". then we can use chernoff s bound ( @xcite , page 93 ) ; the final expression derives from the equality @xmath325 .",
    "proof of proposition [ pr - admissibilityandminimaxefficiencyofmle ] in order to prove that the is admissible and minimax we use the bayesian method . using the prior densities given by @xmath326 ,",
    "the bayes estimator relative to zero ",
    "one loss @xmath156 coincides with the @xmath41 .",
    "therefore , respectively from lemma 2.10 and proposition  6.3 in @xcite , @xmath327 is minimax and admissible . the fact that the minimizes the average probability of error derives from proposition  [ pr - minimizationofthebayesrisk ] .",
    "proof of proposition [ pr - proofofnon - ir - optimality ] in order to prove the first statement , we apply lemma 2.4 in @xcite ( page  653 ) .",
    "clearly @xmath159 is closed in total variation , since it is finite , and is not exponentially convex ; indeed , under assumption , there exist @xmath328 and @xmath329 $ ] , such that the probability measure  @xmath330 defined as @xmath331 does not belong to @xmath159 . therefore , from lemma 2.4 in @xcite , there exist @xmath332 such that equation  ( 2.12 ) in @xcite holds and , as a consequence of lemma  2.4 in @xcite , the fails to be an inaccuracy rate optimal estimator at least at one of the points @xmath333 .",
    "this means that , say for @xmath334 : @xmath335 and this implies that the chapman ",
    "robbins bound is not attained at @xmath334 .",
    "the second statement follows easily from the results of @xcite ( theorem 2 ) on @xmath336 , using equation ( [ eq - asymptoticequivalenceofbayesriskandminimaxproberror ] ) .",
    "indeed , the attains the lower bound ( [ eq - chernoffslowerbound ] ) and is therefore asymptotically minimax efficient .    if the estimator is asymptotically @xmath232-efficient w.r.t .",
    "@xmath206 at @xmath2 , this means that at @xmath2 it is more efficient than the and therefore it has to be less efficient elsewhere ( since from proposition  [ pr - admissibilityandminimaxefficiencyofmle ] the minimizes the probability of error ) .",
    "therefore , it can not be minimax @xmath232-efficient .",
    "proof of proposition [ pr - superefficiency ] for it is enough to follow the proof of proposition  [ pr - bahadurlowerbound ] and to reason by contradiction , while is simply another way of stating proposition [ pr - admissibilityandminimaxefficiencyofmle ] .",
    "the authors would like to thank lucien birg , mehmet caner , jean - pierre florens , christian gouriroux , christian hess , marc hoffmann , pierre jacob , sren johansen , rasul a. khan , oliver b. linton , christian p. robert , keunkwan ryu , igor vajda and the participants to seminars at universit paris 9 dauphine , crest and institut henri poincar , to esem 2001 in lausanne , to xxxivmes journes de statistique 2002 in bruxelles , to bs / imsc 2004 in barcelona , and to esem 2004 in madrid .",
    "all the remaining errors are our responsibility ."
  ],
  "abstract_text": [
    "<S> in some estimation problems , especially in applications dealing with information theory , signal processing and biology , theory provides us with additional information allowing us to restrict the parameter space to a finite number of points . in this case </S>",
    "<S> , we speak of discrete parameter models . </S>",
    "<S> even though the problem is quite old and has interesting connections with testing and model selection , asymptotic theory for these models has hardly ever been studied . </S>",
    "<S> therefore , we discuss consistency , asymptotic distribution theory , information inequalities and their relations with efficiency and superefficiency for a  general class of @xmath0-estimators .    . </S>"
  ]
}