{
  "article_text": [
    "forecasting changes in volatility is essential for risk management , asset pricing and scenario analysis .",
    "indeed , models for describing and forecasting the evolution of volatility and covariance among financial assets are widely applied in industry @xcite . among the most popular approaches",
    "are worth mentioning the multivariate extensions of garch @xcite , the stochastic covariance models @xcite and realized covariance @xcite .",
    "however most of these econometrics tools are not able to cope with more than few assets , due to the curse of dimensionality and the increase in the number of parameters @xcite , limiting their insight into the volatility evolution to baskets of few assets only .",
    "this is unfortunate , since gathering insights into systemic risk and the unfolding of financial crises require modelling the evolution of entire markets which are composed by large numbers of assets @xcite .",
    "we suggest to use network filtering @xcite as a valuable tool to overcome this limitation .",
    "correlation - based filtering networks are tools which have been widely applied to filter and reduce the complexity of covariance matrices made of large numbers of assets ( of the order of hundreds ) , representative of entire markets .",
    "this strand of research represents an important part of the econophysics literature and has given important insights for risk management , portfolio optimization and systemic risk regulation @xcite .",
    "the volatility of a portfolio depends on the covariance matrix of the corresponding assets @xcite .",
    "therefore , the latter can provide insights into the former . in this work we elaborate on this connection : we show that correlation matrices can be used to predict variations of volatility , once they are analysed through the lens of network filtering .",
    "this is quite an innovative use of correlation - based networks , which have been used mostly for descriptive analyses , with the connections with risk forecasting being mostly overlooked .",
    "some works have shown that is possible to use dimensionality reduction techniques , such as spectral methods @xcite , as early - warning signals for systemic risk @xcite : however these approaches , although promising , do not provide proper forecasting tools , as they are affected by high false positive ratios and are not designed to predict a specific quantity .    the approach we propose exploits network filtering to explicitly predict future volatility of markets made of hundreds of stocks . to this end , we introduce a new dynamical measure that quantifies the rate of change in the structure of the market correlation matrix : the `` correlation structure persistence '' @xmath0 . this quantity is derived from the structure of network filtering from past correlations . then we show how such measure exhibits significant predicting power on the market volatility , providing a tool to forecast it .",
    "we assess the reliability of this forecasting through out - of - sample tests on two different equity datasets .",
    "the rest of this paper is structured as follows : we first describe the two datasets we have analysed and we introduce the correlation structure persistence ; then we show how our analyses point out a strong interdependence between correlation structure persistence and future changes in the market volatility ; moreover , we describe how this result can be exploited to provide a forecasting tool useful for risk management , by presenting out - of - sample tests and false positive analysis ; then we investigate how the forecasting performance changes in time ; finally we discuss our findings and their theoretical implications .",
    "we have analysed two different datasets of equity data . the first set ( nyse dataset )",
    "is composed by daily closing prices of @xmath1 us stocks traded in new york stock exchange , covering 15 years from 02/01/1997 to 31/12/2012 .",
    "the second set ( lse dataset ) is composed by daily closing prices of @xmath2 uk stocks traded in the london stock exchange , covering 13 years from 05/01/2000 to 21/08/2013 .",
    "all stocks have been continuously traded throughout these periods of time .",
    "these two sets of stocks have been chosen in order to provide a significant sample of the different industrial sectors in the respective markets .",
    "for each asset @xmath3 ( @xmath4 ) we have calculated the corresponding daily log - return @xmath5 , where @xmath6 is the asset @xmath3 price at day @xmath7 .",
    "the market return @xmath8 is defined as the average of all stocks returns : @xmath9 . in order to calculate the correlation between different assets",
    "we have then analysed the observations by using @xmath10 moving time windows , @xmath11 with @xmath12 .",
    "each time window contains @xmath13 observations of log - returns for each asset , totaling to @xmath14 observations .",
    "the shift between adjacent time windows is fixed to @xmath15 trading days .",
    "we have calculated the correlation matrix within each time window , @xmath16 , by using an exponential smoothing method @xcite that allows to assign more weight on recent observations .",
    "the smoothing factor of this scheme has been chosen equal to @xmath17 according to previously established criteria @xcite .    from each correlation matrix @xmath16",
    "we have then computed the corresponding planar maximally filtered graph ( pmfg ) @xcite .",
    "the pmfg is a sparse network representation of the correlation matrix that retains only a subset of most significant entries , selected through the topological criterion of being maximally planar @xcite .",
    "such networks serve as filtering method and have been shown to provide a deep insight into the dependence structure of financial assets @xcite .",
    "once the @xmath10 pmfgs , @xmath18 with @xmath12 , have been computed we have calculated two measures , a backward - looking and a forward - looking one .",
    "the first is a measure that monitors the correlation structure persistence , based on a measure of pmfg similarity .",
    "this backward - looking measure , that we call @xmath19 , relies on past data only and indicates how slowly the correlation structure measured at time window @xmath11 is differentiating from structures associated to previous time windows .",
    "the forward - looking measure is the volatility ratio @xmath20 @xcite , that at each time window quantifies how good the market volatility measured at @xmath11 is as a proxy for the next time - window volatility . unlike @xmath19 ,",
    "the value of @xmath20 is not known at the end of @xmath11 .",
    "[ fig : time_windows ] shows a graphical representation of the time window set - up . in the following",
    "we define the two measures :    [ sec : methods ]    * * correlation structure persistence @xmath19 * : we define the correlation structure persistence at time @xmath11 as : + @xmath21 + where @xmath22 is an exponential smoothing factor , @xmath23 is a parameter and @xmath24 is the fraction of edges in common between the two pmfgs @xmath18 and @xmath25 , called `` edge survival ratio '' @xcite . in formula , @xmath24 reads : + @xmath26 + where @xmath27 is the number of edges ( links ) in the two pmfgs ( constant and equal to @xmath28 for a pmfg @xcite ) , and @xmath29 ( @xmath30 ) represents the edge - sets of pmfg at @xmath31 ( @xmath32 ) .",
    "the correlation structure persistence @xmath19 is therefore a weighted average of the similarity ( as measured by the edge survival ratio ) between @xmath18 and the first @xmath23 previous pmfgs , with an exponential smoothing scheme that gives more weight to those pmfgs that are closer to @xmath11 .",
    "the parameter @xmath33 in eq .",
    "[ eq : es ] can be calculated by imposing @xmath34 .",
    "intuitively , @xmath19 measures how slowly the change of correlation structure is occurring in the near past of @xmath11 . *",
    "* volatility ratio @xmath20 * @xcite : in order to quantify the agreement between the estimated and the realized risk we here make use of the volatility ratio , a measure which has been used @xcite for this purpose and defined as follows : + @xmath35 + where @xmath36 is the realized volatility of the average market return @xmath8 computed on the time window @xmath37 ; @xmath38 is the estimated volatility of @xmath8 computed on time window @xmath11 , by using the same exponential smoothing scheme @xcite described for the correlation @xmath16 .",
    "specifically , @xmath37 is the time window of length @xmath39 that follows immediately @xmath11 : if @xmath40 is the last observation in @xmath11 , @xmath37 covers observations from @xmath41 to @xmath42 ( fig .",
    "[ fig : time_windows ] ) .",
    "therefore the ratio in eq .",
    "[ eq : q ] estimates the agreement between the market volatility estimated with observations in @xmath11 and the actual market volatility observed over an investment in the @xmath43 assets over @xmath37 .",
    "if @xmath44 , then the historical data gathered at @xmath11 has underestimated the ( future ) realized volatilty , whereas @xmath45 indicates overestimation .",
    "+ let us stress that @xmath20 provides an information on the reliability of the covariance estimation too , given the relation between market return volatility and covariance @xcite : + @xmath46 + @xmath47 + where @xmath48 and @xmath49 are respectively the estimated and realized covariances .      to investigate the relation between @xmath19 and @xmath20 we have calculated the two quantities with different values of @xmath13 and @xmath23 in eqs .",
    "[ eq : es ] and [ eq : q ] , to assess the robustness against these parameters .",
    "specifically , we have used @xmath50 trading days , that correspond to time windows of length 1 , 2 , 3 and 4 years respectively ; @xmath51 , that correspond ( given @xmath15 trading days ) to an average in eq .",
    "[ eq : es ] reaching back to @xmath52 , @xmath53 , @xmath54 and @xmath55 trading days respectively .",
    "@xmath39 has been chosen equal to @xmath54 trading days ( one year ) for all the analysis .    in fig .",
    "[ fig : es_matrices ] we show the @xmath56 matrices ( eq . [ eq : es_ab ] ) for the nyse and lse dataset , for @xmath57 .",
    "we can observe a block structure with periods of high structural persistence and other periods whose correlation structure is changing faster .",
    "in particular two main blocks of high persistence can be found before and after the 2007 - 2008 financial crisis ; a similar result was found in a previous work @xcite with a different measure of similarity .",
    "these results are confirmed for all values of @xmath13 considered . in fig .",
    "[ fig : plot_342 ] we show @xmath19 and @xmath20 as a function of time , for @xmath57 and @xmath58 .",
    "as expected , main peaks of @xmath20 occur during the months before the most turbulent periods in the stock market , namely the 2002 market downturn and the 2007 - 08 credit crisis .",
    "interestingly , the corresponding @xmath19 seems to follow a specular trend .",
    "this is confirmed by explicit calculation of pearson correlation between the two signals , reported in tabs .",
    "[ tab : corr_4 ] - [ tab : corr_8 ] : as one can see , for all combinations of parameters the correlation is negative .",
    "* @xmath24 matrices for @xmath59 , for nyse ( left ) and lse dataset ( right)*. a block - like structure can be observed in both datasets , with periods of high structural persistence and other periods whose correlation structure is changing faster .",
    "the 2007 - 2008 financial crisis marks a transition between two main blocks of high structural persistence.,title=\"fig : \" ]    * @xmath24 matrices for @xmath59 , for nyse ( left ) and lse dataset ( right)*. a block - like structure can be observed in both datasets , with periods of high structural persistence and other periods whose correlation structure is changing faster .",
    "the 2007 - 2008 financial crisis marks a transition between two main blocks of high structural persistence.,title=\"fig : \" ]     * @xmath19 and @xmath20 signals represented for @xmath57 and @xmath58 * , for both nyse ( left graph ) and lse ( right graph ) datasets .",
    "it is evident the anticorrelation between the two signals .",
    "the financial crisis triggers a major drop in the structural persistence and a corresponding peak in @xmath20 .",
    ", title=\"fig : \" ]    * @xmath19 and @xmath20 signals represented for @xmath57 and @xmath58 * , for both nyse ( left graph ) and lse ( right graph ) datasets .",
    "it is evident the anticorrelation between the two signals .",
    "the financial crisis triggers a major drop in the structural persistence and a corresponding peak in @xmath20 .",
    ", title=\"fig : \" ]     * @xmath60 matrices for @xmath59 , for nyse ( left ) and lse dataset ( right)*. a block - like structure can be observed in both datasets , with periods of high structural persistence and other periods whose correlation structure is changing faster .",
    "the blocks of high similarity show higher compactness than in fig .",
    "[ fig : es_matrices].,title=\"fig : \" ]    * @xmath60 matrices for @xmath59 , for nyse ( left ) and lse dataset ( right)*. a block - like structure can be observed in both datasets , with periods of high structural persistence and other periods whose correlation structure is changing faster .",
    "the blocks of high similarity show higher compactness than in fig .",
    "[ fig : es_matrices].,title=\"fig : \" ]    in order to check the significance of this anticorrelation we can not rely on standard tests on pearson coefficient , such as fisher transform @xcite , as they assume i.i.d .",
    "series @xcite .",
    "our time series are instead strongly autocorrelated , due to the overlapping between adjacent time windows . therefore we have calculated confidence intervals by performing a block bootstrapping test @xcite .",
    "this is a variation of the bootstrapping test @xcite , conceived to take into account the autocorrelation structure of the bootstrapped series .",
    "the only free parameter in this method is the block length , that we have chosen applying the optimal selection criterion proposed in literature @xcite : such criterion is adaptive on the autocorrelation strength of the series as measured by the correlogram .",
    "we have found , depending on the parameters @xmath13 and @xmath23 , optimal block lengths ranging from 29 to 37 , with a mean of 34 ( corresponding to 170 trading days ) . by performing block bootstrapping tests we have therefore estimated confidence intervals for the true correlation between @xmath19 and @xmath20 ; in tabs .",
    "[ tab : corr_4 ] - [ tab : corr_8 ] correlations whose @xmath61 and @xmath62 confidence intervals ( ci ) do not include zero are marked with one and two stars respectively .",
    "as we can see , 14 out of 16 correlation coefficients are significantly different from zero within @xmath61 ci in the nyse dataset , and 12 out of 16 in the lse dataset . for what concerns the @xmath62 ci",
    ", we observe 13 out of 16 for the nyse and 9 out of 16 for the lse dataset .",
    "non - significant correlations appear only for @xmath63 , suggesting that this length is too small to provide a reliable measure of structural persistence .",
    "very similar results are obtained by using minimum spanning tree ( mst ) @xcite instead of pmfg as network filtering .",
    "given the interpretation of @xmath19 and @xmath20 given above , anticorrelation implies that an increase in the `` speed '' of correlation structure evolution ( low @xmath19 ) is likely to correspond to underestimation of future market volatility from historical data ( high @xmath64 ) ) , whereas when the structure evolution `` slows down '' ( high @xmath19 ) there is indication that historical data is likely to provide an overestimation of future volatility .",
    "this means that we can use @xmath19 as a valuable predictor of current historical data reliability .",
    "this result is to some extent surprising as @xmath19 is derived from pmfgs topology , which in turns depends only on the ranking of correlations and not on their actual value : yet , this information provides meaningful information about the future market volatility and therefore about the future covariance .    in principle other measures of correlation ranking structure , more straightforward than the correlation persistence @xmath19 , might capture the same interplay with @xmath20 .",
    "we have therefore considered also the metacorrelation @xmath65 , that is the pearson correlation computed between the coefficients of correlation matrices at @xmath11 and @xmath66 ( see methods for more details ) .",
    "such measure does not make use of pmfg .",
    "[ fig : meta_matrices ] displays the similarity matrices obtained with this measure for nyse and lse datasets : we can observe again block - like structures , that however carry different information from the @xmath56 in fig .",
    "[ fig : es_matrices ] ; in particular , blocks show higher intra - similarity and less structure . similarly to eq .",
    "[ eq : es ] , we have then defined @xmath67 as the weighted average over @xmath23 past time windows ( see methods ) . in tabs . [",
    "tab : corr_meta_nyse ] and [ tab : corr_meta_lse ] we show the correlation between @xmath67 and @xmath20 . as we can see , although an anticorrelation is present for each combination of parameters @xmath13 and @xmath23 , correlation coefficients are systematically closer to zero than in tabs .",
    "[ tab : corr_4 ] - [ tab : corr_8 ] , where @xmath19 was used .",
    "moreover the number of significant pearson coefficients , according to the block bootstrapping , decreases to 12 out of 16 in nyse and to 10 out of 16 in lse dataset .",
    "since @xmath68 does not make use of pmfg , this result suggests that the filtering procedure associated to correlation - based networks is a necessary step for capturing at best the correlation ranking evolution and its interplay with the volatility ratio .",
    "cc | c c c c | & & + & & * 10 * & * 25 * & * 50 * & * 100 * + & & -0.2129 & -0.2224 & @xmath69 & @xmath70 + & & @xmath71 & @xmath72 & @xmath73 & @xmath74 + & & @xmath75 & @xmath76 & @xmath77 & @xmath78 + & & @xmath79 & @xmath80 & @xmath81 & @xmath82 +     cc | c c c c | & & + & & * 10 * & * 25 * & * 50 * & * 100 * + & & @xmath83 & @xmath84 & -0.1872 & @xmath85 + & & @xmath86 & @xmath87 & @xmath88 & @xmath89 + & & @xmath90 & @xmath91 & @xmath92 & @xmath93 + & & @xmath94 & @xmath95 & @xmath96 & @xmath97 +     cc | c c c c | & & + & & * 10 * & * 25 * & * 50 * & * 100 * + & & -0.0992 & -0.0754 & -0.1055 & -0.1157 + & & -0.2146 & -0.2232 & -0.2309 & -0.2753 + & & -0.2997 & @xmath98 & @xmath99 & @xmath100 + & & @xmath101 & @xmath102 & @xmath103 & @xmath104 +     cc | c c c c | & & + & & * 10 * & * 25 * & * 50 * & * 100 * + & & -0.1470 & -0.1095 & -0.1326 & -0.1720 + & & @xmath105 & -0.2113 & @xmath106 & @xmath107 + & & @xmath108 & @xmath109 & @xmath110 & @xmath111 + & & @xmath112 & -0.2954 & -0.3163 & @xmath113 +       * partition of data into training ( left graphs ) and test ( right graphs ) set*. training sets are used to regress @xmath114 against @xmath19 , in order to estimate the coefficents in the logistic regression and therefore identify the regression threshold , shown as a vertical continuous line . the test sets are used to test the forecasting performance of such regression on a subset of data that has not been used for regression ; the model predicts @xmath115 ( @xmath44 ) if @xmath19 is greater than the regression threshold , and @xmath116 ( @xmath45 ) otherwise.,title=\"fig : \" ]   * partition of data into training ( left graphs ) and test ( right graphs ) set*. training sets are used to regress @xmath114 against @xmath19 , in order to estimate the coefficents in the logistic regression and therefore identify the regression threshold , shown as a vertical continuous line . the test sets are used to test the forecasting performance of such regression on a subset of data that has not been used for regression ; the model predicts @xmath115 ( @xmath44 ) if @xmath19 is greater than the regression threshold , and @xmath116 ( @xmath45 ) otherwise.,title=\"fig : \" ]     * partition of data into training ( left graphs ) and test ( right graphs ) set*. training sets are used to regress @xmath114 against @xmath19 , in order to estimate the coefficents in the logistic regression and therefore identify the regression threshold , shown as a vertical continuous line . the test sets are used to test the forecasting performance of such regression on a subset of data that has not been used for regression ; the model predicts @xmath115 ( @xmath44 ) if @xmath19 is greater than the regression threshold , and @xmath116 ( @xmath45 ) otherwise.,title=\"fig : \" ]   * partition of data into training ( left graphs ) and test ( right graphs ) set*. training sets are used to regress @xmath114 against @xmath19 , in order to estimate the coefficents in the logistic regression and therefore identify the regression threshold , shown as a vertical continuous line .",
    "the test sets are used to test the forecasting performance of such regression on a subset of data that has not been used for regression ; the model predicts @xmath115 ( @xmath44 ) if @xmath19 is greater than the regression threshold , and @xmath116 ( @xmath45 ) otherwise.,title=\"fig : \" ]    in this section we evaluate how well the correlation structure persistence @xmath19 can forecast the future through its relation with the forward - looking volatility ratio @xmath20 .",
    "in particular we focus on estimating whether @xmath20 is greater or less than @xmath117 : this information , although less complete than a precise estimation of @xmath20 , gives us an important insight into possible overestimation ( @xmath45 ) or underestimation ( @xmath44 ) of future volatility .",
    "we have proceeded as follows .",
    "given a choice of parameters @xmath13 and @xmath23 , we have calculated the corresponding set of pairs @xmath118 , with @xmath12 .",
    "then we have defined @xmath114 as the categorical variable that is @xmath119 if @xmath45 and @xmath117 if @xmath44 .",
    "finally we have performed a logistic regression of @xmath114 against @xmath19 : namely , we assume that @xcite :    @xmath120    where @xmath121 is the sigmoid function @xmath122 @xcite ; we estimate parameters @xmath123 and @xmath124 from the observations @xmath125 through maximum likelihood @xcite .    once the model has been calibrated , given a new observation @xmath126 we have predicted @xmath127 if @xmath128 , and @xmath129 otherwise . this classification criterion , in a case with only one predictor , corresponds to classify @xmath130 according to whether @xmath131 is greater or less than a threshold @xmath132 which depends on @xmath123 and @xmath124 , as shown in fig .",
    "[ fig : training_test ] ( right graphs ) for a particular choice of parameters .",
    "therefore the problem of predicting whether market volatility will increase or decrease boils down to a classification problem @xcite with @xmath19 as predictor and @xmath114 as target variable .",
    "we have made use of a logistic regression because it is more suitable than a polynomial model for dealing with classification problems @xcite .",
    "other classification algorithms are available ; we have chosen the logistic regression due to its simplicity .",
    "we have also implemented the knn algorithm @xcite and we have found that it provides similar outcomes but worse results in terms of the forecasting performance metrics that we discuss in this section .",
    "we have then evaluated the goodness of the logistic regression at estimating @xmath130 given a new observation @xmath131 . to this end , we have computed three standard metrics for assessing the performance of a classification method : the probability of successful forecasting @xmath133 , the true positive rate @xmath134 and the false positive rate @xmath135 . @xmath133 represents the expected fraction of correct predictions , @xmath134 is the method goodness at identifying true positives ( in this case , actual increases in volatility ) and @xmath135 quantifies the method tendency to false positives ( predictions of volatility increase when the volatility will actually decrease ) : see methods for more details .",
    "overall these metrics provide a complete summary of the model goodness at predicting changes in the market volatility @xcite .    in order to avoid overfitting",
    "we have estimated the metrics above by means of an out - of - sample procedure @xcite .",
    "we have divided our dataset into two periods , a training set and a test set . in the training",
    "set we have calibrated the logistic equation in eq .",
    "[ eq : logistic_regression ] , estimating the parameters @xmath123 and @xmath124 ; in the test set we have used the calibrated model to measure the goodness of the model predictions by computing the measures of performance in eq .",
    "[ eq : prob_forecast]-[eq : fpr ] . in fig .",
    "[ fig : training_test ] this division is shown for a particular choice of @xmath13 and @xmath23 , for both nyse and lse dataset . in this example",
    "the percentage of data included in the test set ( let us call it @xmath136 ) is @xmath137 .",
    "probabilities of successful forecasting @xmath133 are reported in tabs . [",
    "tab : nyse_p_outs ] and [ tab : lse_p_outs ] , for @xmath138 .",
    "as we can see @xmath133 is higher than @xmath139 for all combinations of parameters in nyse dataset , and in almost all combinations for lse dataset .",
    "stars mark those values of @xmath133 that are significantly higher than the same probability obtained by using the most recent value of @xmath140 instead of @xmath19 as a predictor for @xmath20 in the logistic regression ( let us call @xmath141 such probability ) .",
    "specifically , we have defined a null model where variations from such probability @xmath141 are due to random fluctuations only ; given @xmath10 observations , such fluctuations follow a binomial distribution @xmath142 , with mean @xmath143 and variance @xmath144 . then p - values have been calculated by using this null distribution for each combination of parameters .",
    "this null hypothesis accounts for the predictability of @xmath20 that is due to the autocorrelation of @xmath20 only ; therefore @xmath133 significantly higher than the value expected under this hypothesis implies a forecasting power of @xmath19 that is not explained by the autocorrelation of @xmath20 . from the table",
    "we can see that @xmath133 is significant in 12 out of 16 combinations of parameters for nyse dataset , and in 13 out of 16 for lse dataset .",
    "this means that correlation persistence is a valuable predictor for future average correlation , able to outperform forecasting method based on past average correlation trends .",
    "these results are robust against changes of @xmath136 , as long as the training set is large enough to allow an accurate calibration of the logistic regression .",
    "we have found this condition is satisfied for @xmath145 .",
    "however @xmath133 does not give any information on the method ability to distinguish between true and false positives . to investigate this aspect we need @xmath134 and @xmath135 .",
    "a traditional way of representing both measures from a binary classifier is the so - called `` receiver operating characteristic '' ( roc ) curve @xcite . in a roc plot , @xmath134",
    "is plotted against @xmath135 as the discriminant threshold is varied .",
    "the discriminant threshold @xmath146 is the value of the probability in eq .",
    "[ eq : logistic_regression ] over which we classify @xmath115 : the higher @xmath146 is , the less likely the method is to classify @xmath115 ( in the analysis on @xmath133 we chose @xmath147 ) .",
    "ideally , a perfect classifier would yield @xmath148 for all @xmath149 , whereas a random classifier is expected to lie on the line @xmath150 .",
    "therefore a roc curve which lies above the line @xmath150 indicates a classifier that is better than chance at distinguishing true from false positives @xcite .    as one can see from fig .",
    "[ fig : roc_curve ] , the roc curve s position depends on the choice of parameters @xmath13 and @xmath23 . in this respect",
    "our classifier performs better for low values of @xmath23 and @xmath13 .",
    "this can be quantified by measuring the area under the roc curve ; such measure , often denoted by auc @xcite , is shown in tabs .",
    "[ tab : auc_nyse_outs]-[tab : auc_lse_outs ] . for both datasets",
    "the optimal choice of parameters is @xmath151 and @xmath152 .",
    "cc | c c c c | & & + & & * 10 * & * 25 * & * 50 * & * 100 * + & & 0.546 & 0.560 * & 0.599 * * & 0.539 * * + & & 0.704 * * & 0.695 * * & 0.658 * * & 0.605 * * + & & 0.634 * & 0.585 & 0.539 & 0.708 * + & & 0.704 * & 0.7638 * * & 0.839 * * & 0.860 +     cc | c c c c | & & + & & * 10 * & * 25 * & * 50 * & * 100 * + & & 0.616 * * & 0.645 * * & 0.612 * * & 0.568 * * + & & 0.652 * * & 0.635 * * & 0.598 * * & 0.393 + & & 0.651 * * & 0.560 * * & 0.453 * * & 0.412 + & & 0.544 * * & 0.573 * * & 0.706 * * & 0.689 +     * receiver operating characteristic ( roc ) curve*. upper graph : true positive rate ( tpr ) against false positive rate ( fpr ) as the discriminant threshold @xmath146 of the classifier is varied , for each combination of parameters @xmath13 and @xmath23 in the nyse dataset",
    ". the closer the curve is to the upper left corner of each graph , the better is the classifier compared to chance .",
    "bottom graph : true positive rate ( tpr ) against false positive rate ( fpr ) as the discriminant threshold @xmath146 of the classifier is varied , for each combination of parameters @xmath13 and @xmath23 in the lse dataset.,title=\"fig : \" ]   * receiver operating characteristic ( roc ) curve*. upper graph : true positive rate ( tpr ) against false positive rate ( fpr ) as the discriminant threshold @xmath146 of the classifier is varied , for each combination of parameters @xmath13 and @xmath23 in the nyse dataset .",
    "the closer the curve is to the upper left corner of each graph , the better is the classifier compared to chance .",
    "bottom graph : true positive rate ( tpr ) against false positive rate ( fpr ) as the discriminant threshold @xmath146 of the classifier is varied , for each combination of parameters @xmath13 and @xmath23 in the lse dataset.,title=\"fig : \" ]    .[tab : auc_nyse_outs ] nyse dataset : * area under the curve ( auc ) * , measured from the roc curve in fig .",
    "[ fig : roc_curve ] .",
    "values greater than 0.5 indicate that the classifier performs better than chance . [ cols=\"^,^,^,^,^,^ \" , ]       * fraction of successful predictions as a function of time*. nyse ( left graph ) and lse dataset ( right graph ) .",
    "forecasting is based on logistic regression with predictor @xmath153 ( top graphs ) and most recent value of @xmath20 ( bottom graphs ) .",
    "horizontal lines represent the average over the entire period.,title=\"fig : \" ]    * fraction of successful predictions as a function of time*. nyse ( left graph ) and lse dataset ( right graph ) .",
    "forecasting is based on logistic regression with predictor @xmath153 ( top graphs ) and most recent value of @xmath20 ( bottom graphs ) .",
    "horizontal lines represent the average over the entire period.,title=\"fig : \" ]    in this section we look at how the forecasting performance changes at different time periods . in order to explore this aspect we have counted at each time window @xmath11 the number @xmath154 of @xmath114 predictions ( out of the 16 predictions corresponding to as many combinations of @xmath13 and @xmath23 ) that have turned out to be correct",
    "; we have then calculated the fraction of successful predictions @xmath155 as @xmath156 . in this way",
    "@xmath155 is a proxy for the goodness of our method at each time window .",
    "logistic regression parameters @xmath123 and @xmath124 have been calibrated by using the entire time period as training set , therefore this amounts to an in - sample analysis .    in fig .",
    "[ fig : forecast_time ] we show the fraction of successful predictions for both nyse and lse datasets ( upper graphs , blue circles ) . for comparison",
    "we also show the same measure obtained by using the most recent value of @xmath20 as predictor ( bottom graphs ) ; as in the previous section , it represents a null model that makes prediction by using only the past evolution of @xmath20 . as we can see , both predictions based on @xmath19 and on past values of @xmath20 display performances changing in time . in particular",
    "@xmath155 drops just ahead of the main financial crises ( the market downturn in march 2002 , 2007 - 2008 financial crisis , euro zone crisis in 2011 ) ; this is probably due to the abrupt increase in volatility that occurred during these events and that the models took time to detect .",
    "after these drops though performances based on @xmath19 recover much more rapidly than those based on past value of @xmath20 .",
    "for instance in the first months of 2007 our method shows quite high @xmath155 ( more than @xmath157 of successful predictions ) , being able to predict the sharp increase in volatility to come in 2008 while predictions based on @xmath20 fail systematically until 2009 .",
    "overall , predictions based on correlation structure persistence appear to be more reliable ( as shown by the average @xmath155 over all time windows , the horizontal lines in the plot ) and faster at detecting changes in market volatility .",
    "in this paper we have proposed a new tool for forecasting market volatility based on correlation - based information filtering networks and logistic regression , useful for risk and portfolio management .",
    "the advantage of our approach over traditional econometrics tools , such as multivariate garch and stochastic covariance models , is the `` top - down '' methodology that treats correlation matrices as the fundamental objects , allowing to deal with many assets simultaneously ; in this way the curse of dimensionality , that prevents e.g. multivariate garch to deal with more than few assets , is overcome .",
    "we have proven the forecasting power of this tool by means of out - of - sample analyses on two different stock markets ; the forecasting performance has been proven to be statistically significant against a null model , outperforming predictions based on past market correlation trends . moreover we have measured the roc curve and identified an optimal region of the parameters in terms of true positive and false positive trade - off .",
    "the temporal analysis indicates that our method is able to adapt to abrupt changes in the market , such as financial crises , more rapidly than methods based on past volatility .",
    "this forecasting tool relies on an empirical fact that we have reported in this paper for the first time .",
    "specifically , we have shown that there is a deep interplay between market volatility and the rate of change of the correlation structure .",
    "the statistical significance of this relation has been assessed by means of a block - bootstrapping technique .",
    "an analysis based on metacorrelation has revealed that this interplay is better highlighted when filtering based on planar maximally filtered graphs is used to estimate the correlation structure persistence .",
    "this finding sheds new light into the dynamic of correlation .",
    "the topology of planar maximally filtered graphs depends on the ranking of the @xmath158 pairs of cross - correlations ; therefore an increase in the rate of change in pmfgs topology points out a faster change of this ranking .",
    "our result indicates that such increase is typically followed by a rise in the market volatility , whereas decreases are followed by drops .",
    "a possible interpretation of this is related to the dynamics of risk factors in the market .",
    "indeed higher volatility in the market is associated to the emergence of a ( possibly new ) risk factor that makes the whole system unstable ; such transition could be anticipated by a quicker change of the correlation ranking , triggered by the still emerging factor and revealed by the correlation structure persistence .",
    "such persistence can therefore be a powerful tool for monitoring the emergence of new risks , valuable for a wide range of applications , from portfolio management to systemic risk regulation .",
    "moreover this interpretation would open interesting connections with those approaches to systemic risk that make use of principal component analysis , monitoring the emergence of new risk factors by means of spectral methods @xcite .",
    "we plan to investigate all these aspects in a future work .",
    "given two correlation matrices @xmath159 and @xmath160 at two different time windows @xmath11 and @xmath66 , their metacorrelation @xmath65 is defined as follows :    @xmath161[\\langle \\rho^2_{ij}(t_b ) \\rangle_{ij } - \\langle \\rho_{ij}(t_b ) \\rangle_{ij}^2 ] } } ,   \\label{eq : z}\\ ] ]    where @xmath162 is the average over all couples of stocks @xmath163 .",
    "similarly to eq .",
    "[ eq : es ] we have then defined @xmath67 as the weighted average over @xmath23 past time windows :    @xmath164      with reference to figs .",
    "[ fig : training_test ] b ) and d ) , let us define the number of observations in each quadrant @xmath165 ( @xmath166 ) as @xmath167 . in the terminology of classification techniques @xcite , @xmath168 is the number of true positive ( observations for which the model correctly predicted @xmath169 ) , @xmath170 is the number of true negative ( observations for which the model correctly predicted @xmath171 ) , @xmath172 the number of false negative ( observations for which the model incorrectly predicted @xmath171 ) and @xmath173 the number of false positive ( observations for which the model incorrectly predicted @xmath115 ) .",
    "we have then computed the following measures of quality of classification , that are the standard metrics for assessing the performances of a classification method @xcite :    * * probability of successful forecasting ( @xmath133 ) * @xcite : represents the method probability of a correct prediction , expressed as fraction of observed @xmath19 values through which the method has successfully identified the correspondent value of @xmath174 . in classification problems ,",
    "sometimes , the error rate @xmath175 is used @xcite , which is simply @xmath176 .",
    "@xmath133 is computed as follows : + @xmath177 * * true positive rate ( @xmath134 ) * @xcite : it is the probability of predicting @xmath115 , conditional to the fact that the real @xmath114 is indeed @xmath117 ( that is , to predict an increase in volatility when the volatility will indeed increase ) ; it represents the method sensitivity to increase in volatility .",
    "it is also called `` recall '' @xcite . in formula : + @xmath178 * * false positive rate ( @xmath135 ) * @xcite : it is the probability of predicting @xmath115 , conditional to the fact that the real @xmath114 is instead @xmath119 ( that is , to predict an increase in volatility when the volatility will actually decrease ) .",
    "it is also called `` 1-specificity '' @xcite . in formula : + @xmath179    10 url # 1`#1`urlprefix[2]#2 [ 2][]#2    _ _ ( , ) .    & . _ _ ( ) .    & _ _ ( , ) .    , , , & . _ _ * * , ( ) .    , & . _ _ * * , ( ) .    .",
    "_ _ * * , ( ) .    , , & .",
    "_ _ * * , ( ) .    .",
    "_ _ * * , ( ) .    , , & .",
    "_ _ * * , ( ) .    & .",
    "_ _ * * , ( ) .    , , , & .",
    "_ _ * * , ( ) .    , & .",
    "_ _ * * , ( ) .    , & .",
    "_ _ * * , ( ) .",
    "_ _ ( , ) .",
    "_ _ * * , ( ) .",
    "_ _ * * , ( ) .    , , & .",
    "_ _ * * , ( ) .    , & .",
    "_ _ * * , ( ) .    , & .",
    "_ _ * * , ( ) .    , & .",
    "_ _ * * , ( ) .    .",
    "_ _ * * , ( ) .",
    "_ et  al . _ . _",
    "_ * * , ( ) .",
    "_ _ * * , ( ) .    , , & . _ _ ( ) .    , &",
    "_ _ * * , ( ) .    , &",
    "_ _ * * , ( ) .    , & .",
    "_ _ * * , ( ) .    & .",
    "_ _ * * , ( ) .    & .",
    "_ _ * * , ( ) .    , & .",
    "_ _ ( ) .",
    ". _ _ * * , ( ) .    & _ _ ( , ) .    .",
    "_ _ * * , ( ) .    .",
    "_ _ * * , ( ) .",
    "_ _ * * , ( ) .",
    "_ _ * * , ( ) .",
    ", , & _ _ ( , ) .",
    "_ _ ( , ) .",
    "_ _ ( , ) .    .",
    "the authors wish to thank bloomberg for providing the data .",
    "tdm wishes to thank the cost action td1210 for partially supporting this work .",
    "ta acknowledges support of the uk economic and social research council ( esrc ) in funding the systemic risk centre ( es / k002309/1 ) .",
    "* competing financial interests . *",
    "the authors declare no competing financial interest ."
  ],
  "abstract_text": [
    "<S> we discovered that past changes in the market correlation structure are significantly related with future changes in the market volatility . by using correlation - based information filtering networks we device a new tool for forecasting the market volatility changes . </S>",
    "<S> in particular , we introduce a new measure , the `` correlation structure persistence '' , that quantifies the rate of change of the market dependence structure . </S>",
    "<S> this measure shows a deep interplay with changes in volatility and we demonstrate it can anticipate market risk variations . </S>",
    "<S> notably , our method overcomes the curse of dimensionality that limits the applicability of traditional econometric tools to portfolios made of a large number of assets . </S>",
    "<S> we report on forecasting performances and statistical significance of this tool for two different equity datasets . </S>",
    "<S> we also identify an optimal region of parameters in terms of true positive and false positive trade - off , through a roc curve analysis . </S>",
    "<S> we find that our forecasting method is robust and it outperforms predictors based on past volatility only . </S>",
    "<S> moreover the temporal analysis indicates that our method is able to adapt to abrupt changes in the market , such as financial crises , more rapidly than methods based on past volatility . </S>"
  ]
}