{
  "article_text": [
    "common problem in machine learning is the task of having to group a set of @xmath0 data points or objects into @xmath1 clusters .",
    "this is termed _",
    "these objects are collected together into a set denoted as @xmath2 .",
    "clustering can occur in varied settings .",
    "as an example , consider the case of classifying @xmath0 organisms into @xmath1 different kingdoms based on their features .",
    "this can be construed as a clustering problem where the number of features being considered is @xmath3 . in the more general sense",
    ", @xmath3 denotes the dimensionality of the set @xmath2 .",
    "furthermore , the collection of feature vectors of all the organisms forms the set @xmath2 , while the clusters denoted as @xmath4 are represented by the different kingdoms .    in machine learning",
    ", clustering falls under the domain of unsupervised learning since there are no class labels to the objects in @xmath2 .",
    "nonetheless , it can also be performed as a precursor to some supervised learning techniques .",
    "an example of this latter application is in the implementation of the radial basis function ( rbf ) with @xmath1 centers @xcite .",
    "the clusters , denoted as @xmath4 @xmath5 , are to be determined such that objects in any one cluster are similar to each other , but different from objects in all other clusters .",
    "it is assumed that the objects in @xmath2 lend themselves to some natural grouping @xcite . otherwise , any partitioning of the data can be considered valid , which would make the problem undefined . however , in the @xmath1-center rbf , such an assumption is not binding since the objective is to use the @xmath1 centers as representative points in the dataset for the construction of basis functions .",
    "clustering is , however , an ill - posed problem @xcite for the following reasons .",
    "first , the question of how to tell if any two objects are similar has no definitive answer . to illustrate this , in fig .",
    "1 ( a ) , the similarity among objects in either of the natural clusters indicated by + or o is based on the distance of a point from the center . on the other hand , in fig . 1 ( b ) , the closeness of the points to one another provides the measure of similarity among the two natural clusters indicated by + and o. thus , there is no general similarity measure by which objects are clustered .",
    "the second reason why the problem of clustering is ill - defined is that the number of clusters @xmath1 to which the objects must be classified is not known _",
    "a priori_. a rough estimate of @xmath1 is usually assumed to be available from domain expertise or from the distribution of the data .",
    "if such an estimate is not available , the common practice is that existing algorithms are run for different @xmath1 .",
    "the value of @xmath1 which minimizes some predefined criterion like the akaike information criterion ( aic ) or the bayes information criterion @xcite is then chosen .",
    "clustering algorithms may yield poor results if the @xmath1 chosen is inappropriate @xcite .",
    "the most widely used algorithm for clustering in the context of machine learning is lloyds algorithm , more commonly referred to as k - means algorithm .",
    "it is so called because it essentially computes the @xmath1 means or centroids of the different clusters .",
    "the ease of implementation of the algorithm as well as its fast runtime has accounted for its ubiquity in use .",
    "nevertheless , it has the major drawback of yielding solutions that are only locally optimal , and which may not necessarily be the global optimal solution .",
    "for this reason , several other methods have been applied to solving the clustering problem @xcite-@xcite .",
    "notable among these is the approach of al - sultan @xcite which is based on the tabu search ( ts ) algorithm developed by glover @xcite .",
    "we henceforth refer to this approach , i.e. @xcite ( our reference work ) as the tabu search clustering ( tsc ) algorithm .",
    "the performance reported was shown to be superior to that of the k - means algorithm .",
    "the ts algorithm is a metaheuristic procedure that accepts an initial solution as input , and performs a local search using neighborhood and memory structures until some stopping criterion is met .",
    "it is able to escape local minima by allowing for solutions that do not improve the objective function .",
    "ts has been applied in solving varied problems including the traveling salesman problem ( tsp ) @xcite and signal detection in multiple input multiple output ( mimo ) antenna systems @xcite .",
    "however , with regards to the clustering problem , the high computational complexity and difficulty in parameter selection required in the ts approach does not make it an attractive alternative to the k - means algorithm .",
    "our main contributions in this paper are as follows :",
    "1 .   we introduce a quantized means ts scheme for solving the clustering problem .",
    "we target the optimization of the @xmath1 centers by evolving them through a series of neighboring solutions in such a manner that leads to an efficient exploration of the search space .",
    "this procedure is well described in section iv .",
    "the scheme requires only two parameters to be set , and is of a relatively low complexity .",
    "2 .   we present experimental results obtained from the proposed approach on some test datasets ( section v ) .",
    "for the purpose of this paper , we assume that the number of clusters @xmath1 is given .",
    "we refer the reader to the works by hamerly et al . @xcite and pan et al .",
    "@xcite for a detailed treatment on how to choose @xmath1 .",
    "the dataset @xmath2 is assumed to come from a mixture distribution where the mixture component label ( which is the cluster index ) for any object in the dataset is hidden . in the most general sense , an object can belong to more than one cluster .",
    "thus , for such an interpretation , the problem of clustering is simply that of finding the clusters to which an object belongs with a high probability .",
    "mathematically , this can be stated concisely as maximizing the following probability for different models @xmath6 for a given @xmath7 : @xmath8 where @xmath6 comprises the cluster memberships for all the objects in the dataset , as well as the mixture weights @xmath9 .",
    "maximizing ( 1 ) requires knowledge of the cluster memberships and the mixture weights , as well as knowledge of the mixture distribution . in general ,",
    "none of these is known , and so the following set of simplifying assumptions @xcite are made in practice .    1 .",
    "each object in @xmath2 belongs to a single cluster ; 2 .",
    "each cluster is distributed as a multivariate gaussian ; 3 .",
    "the mixture components have equal weights @xmath9 .    0.5     0.5     the consequence of the above assumptions is that the similarity measure is now based on the euclidean norm so that points closest to each other in euclidean space are grouped under one and only one cluster .",
    "it is conceivable that a dataset may have some similarity measure other than the euclidean distance .",
    "indeed , @xcite-@xcite explore the use of other distance measures for clustering . yet , for some datasets , an appropriate representation of the points can make the euclidean distance measure valid .",
    "as an example , transformation of the points in fig .",
    "1 ( a ) into polar co - ordinates yields the representation in fig . 1 ( b ) which has the euclidean distance as a valid similarity measure .",
    "the clustering problem then yields itself to a treatment as a mathematical optimization whose aim is to minimize a parameter @xmath10 known as the intra cluster sum of squares ( icss ) or the distortion @xcite .",
    "this may be stated as :    @xmath11    where @xmath12 are all data points in cluster @xmath13 and @xmath14 is the mean or center of that @xmath13th cluster .",
    "this is the problem termed as k - means clustering .",
    "it must be mentioned that neither the cluster memberships nor the means are known .",
    "thus , this problem is computationally difficult , and is np - hard @xcite .",
    "the k - means algorithm provides an efficient way of solving ( 2 ) .",
    "it is based on the observation that the optimal placement of the @xmath1 centers is at the centroids of the respective clusters .",
    "the algorithm is typically initialized with some random means , usually chosen from objects in the dataset @xmath2 . since the k - means algorithm is a special case of the expectation - maximization ( em ) algorithm @xcite , it proceeds in two stages namely , the expectation and the maximization stages .    1 .",
    "expectation : compute the centroid of each cluster : @xmath15 where @xmath16 is the number of objects in the @xmath13th cluster .",
    "maximization : compute the cluster memberships : @xmath17    the expectation - maximization steps are carried out iteratively until there is no cluster change , at which point the algorithm is terminated .",
    "the major drawback of the k - means algorithm is as follows .",
    "first , the objective function of ( 2 ) is non - convex and may thus have several local minima .",
    "therefore , being only a local search method , the k - means algorithm is not guaranteed to find a global minimum ; it often yields solutions that are only locally optimal .",
    "this is due in part to the nature of the stopping criterion .",
    "the algorithm terminates when there is no change in cluster memberships ; this period corresponds to a local minimum .",
    "it makes no provision to consider other local minima which may be present in other areas of the search space . again , as with all local search methods , the performance of the k - means algorithm is directly tied to the quality of the initial solution .",
    "if this solution is poor , i.e. , if it is too far away from the global optimum , the algorithm may likely converge to a local minimum .",
    "our approach is motivated by the above limitation , and is based on the ts algorithm in @xcite .",
    "tabu search is a metaheuristic technique used for combinatorial optimization",
    ". it does not require the optimization problem to be convex .",
    "the algorithm makes use of neighborhood structures to explore the search space .",
    "it also utilizes a short term memory structure called a _ tabu _ , which is essentially a list of forbidden moves or solutions .",
    "tabus prevent the back and forth movements between solutions that have already been considered in the search , a phenomenon called _",
    "cycling_. moreover , ts allows for moves to solutions that do not yield any improvement in the objective function .",
    "it does so with the view that the poor solution may lead to a better one at a later time in the search .",
    "thus , it is able to escape from local minima .",
    "ts keeps in memory the best solution found at any point in the search , and returns that solution when the algorithm is terminated . in its most basic form",
    ", it follows the procedure outlined below :    1 .",
    "select an initial solution @xmath18 .",
    "this solution can be randomly generated or obtained by more formal means .",
    "set @xmath19 and @xmath20 to @xmath18 .",
    "@xmath19 and @xmath20 are the current and best solutions respectively .",
    "2 .   evaluate the objective function @xmath10 for the current solution @xmath19 .",
    "3 .   find neighboring solutions of @xmath19 .",
    "let @xmath21 denote this set .",
    "the neighbors of @xmath19 are all those solutions that are similar to , but differ in a minor aspect from @xmath19 .",
    "4 .   find the set of solutions in @xmath21 that are not in the tabu list @xmath22 .",
    "let this set be denoted by @xmath23 .",
    "the tabu is a list of solutions or moves that have already been considered in the search .",
    "tabus , as algorithmic structures , force the algorithm to other areas of the search space , thus enhancing the diversification of the search .",
    "5 .   evaluate the objective function for all the solutions in @xmath23 .",
    "find the best solution among this set .",
    "let this be @xmath24 .",
    "if @xmath25 , let @xmath26 .",
    "@xmath27 and @xmath28 are the objective function evaluations of @xmath24 and @xmath20 respectively . 7 .",
    "put the solution @xmath19 into the tabu list , and let @xmath24 be the new current solution @xmath19 .",
    "if the maximum number of iterations ( which is chosen beforehand ) has elapsed , terminate .",
    "else , go to step 3 .",
    "in this section , we discuss the proposed algorithm . as with any ts implementation ,",
    "the quantized means ts clustering follows the skeleton of the description of the ts algorithm in section iii with the following modifications and specificities .      in this formulation , a vector @xmath18 defined as @xmath29^t$ ] is considered as the initial solution , where @xmath30 are @xmath1 randomly chosen observations from the dataset @xmath2 .",
    "@xmath18 is then assigned to @xmath19 .    to navigate the search space then",
    ", neighbors of @xmath19 have to be found .",
    "neighboring solutions are typically drawn from a finite set that includes the current solution itself .",
    "alternatively , they can be obtained via a simple transformation of the current solution .",
    "it is worth mentioning that in the context of ts , neighboring solutions are not necessarily those that are closest to the current solution .    to obtain neighbors of @xmath19 , we change its individual components i.e. @xmath14 @xmath5 , by replacing them with some new means or centers",
    "however , the means are real - valued in general , and do not constitute any finite set .",
    "therefore , the set of all possible neighbors obtained in this manner is necessarily an infinite set .",
    "this set is the feasible search space .",
    "the fact of the search space being infinite makes ts ill - suited to optimizing @xmath19 , since ts is used for combinatorial optimization .",
    "a finite subset of the search space is thus necessary .",
    "for this reason , the proposed scheme makes the assumption that the @xmath1 means take on values exclusively from objects in the dataset @xmath2 .",
    "we refer to this as _",
    "quantized means_. our proposed algorithm is divided into two stages , namely , exploration and refinement ; we make the aforementioned assumption on the means only in the initial exploration stage .",
    "thus , in the exploration stage , for @xmath31 , @xmath32 is replaced with some other point @xmath33 taken from the dataset .",
    "this procedure yields the neighboring solution denoted as @xmath24 .",
    "more specifically , for every @xmath34 , we constrain the point @xmath33 to the @xmath13th cluster @xmath4 ( which is a subset of the dataset ) .",
    "this quantization of the means makes the problem formulation combinatorial . nevertheless , the resulting set of all possible combinations of @xmath19 ( i.e. the search space denoted as @xmath35 ) , although finite , is still large .",
    "= [ diamond , draw , fill = blue!20 , text width=5em , text badly centered , node distance=3 cm , inner sep=0pt ] = [ rectangle , draw , fill = blue!20 , text width=8em , text centered , rounded corners , minimum height=4em ] = [ draw , -latex ] = [ draw , ellipse , fill = red!20 , node distance=3 cm , minimum height=2em ]      due to the large size of @xmath35 , one has to choose only @xmath36 ( @xmath37 ) points from the set @xmath35 via a simple transformation of the solution @xmath19 and consider those as the neighbors of @xmath19 in any one iteration of the ts algorithm .",
    "the difficulty , however , is in the choice of which @xmath36 neighbors .",
    "if we randomly select neighbours , the search is unguided and thus likely to be slow to converge on the optimal solution .",
    "a simple guiding mechanism might be to choose nearest neighbours",
    ". however , this seems a poor choice intuitively because for many cases it will cause no change to the clustering and where it does , it might not be a change in the right direction .",
    "we therefore use the gradient information of the objective function to guide the neighbor selection . in any ts iteration , we choose @xmath36 points in @xmath35 that result in the steepest descent along the trajectory of the objective function .",
    "we consider the selection of a single neighbor , i.e @xmath38 in this paper .",
    "the following approach is then taken to find one high - quality neighbor of @xmath19 .",
    "since the objective function of ( 2 ) is non - convex , the aim is to find a neighbor @xmath39 that corresponds to a local minimum of @xmath10 .",
    "a necessary and sufficient condition for this is to have the gradient of @xmath10 to be zero at the local minimum , i.e.    @xmath40    where the notation @xmath41 represents the gradient . by definition , @xmath42 where @xmath43 @xmath5 is a unit vector in the @xmath14 direction . by ( 6 ) , it has implicitly been assumed that each @xmath14 is independent of the other .",
    "this assumption is not generally true of the k - means algorithm , as a change in some @xmath14 may change the cluster memberships and hence change the location of the other means .",
    "however , in the exploration stage of our proposed algorithm , the means are not defined as the cluster centroids as in ( 3 ) , but are chosen independently of each other in the procedure below .",
    "this permits the evaluation of the @xmath1 partial derivatives independently as : @xmath44 however , since @xmath14 has been constrained to the dataset @xmath2 , the change in the means @xmath45 being considered is not necessarily infinitesimal . for this reason ,",
    "we approximate the partial derivative in ( 6 ) as a partial difference quotient as : @xmath46 which can be evaluated from first principles as follows : @xmath47 a change in the mean @xmath48 would then cause a change @xmath49 in the objective function , i.e. , @xmath50 @xmath51 @xmath52    for the purpose of evaluating ( 8) , @xmath53 for @xmath54 due to the assumption of the independence of the means , hence @xmath55 @xmath56 where @xmath57 .    in order to evaluate ( 14 ) , we find @xmath58 that minimizes ( 13 ) , having constrained the means to the dataset @xmath2 .",
    "this minimizing parameter is denoted as @xmath59 .",
    "the neighboring solution @xmath24 is then the aggregation of all @xmath59 @xmath5 , i.e. @xmath60^t$ ] .",
    "it must be noted that if the means were to be unconstrained to @xmath2 , ( 7 ) could be evaluated directly instead of solving ( 14 ) , and the solution of ( 7 ) would be the centroids of the clusters , which is essentially what the k - means algorithm evaluates .",
    "however , the initial assumption on the means would be violated , and there would still be the risk of getting trapped at local minima .",
    "rather , this procedure allows for the consideration of solutions that worsen the objective function @xmath10 since ( 13 ) would not always yield negative values , thereby escaping from local minima .",
    "the intuitive alternative of finding the cluster centroids , and then quantizing them to the dataset @xmath2 introduces a quantization loss and yields an inferior performance to the procedure described .",
    "( init ) generate an initial solution @xmath18 .",
    "let @xmath61 . ;",
    "( jb ) evaluate @xmath28 corresponding to @xmath62 ; ( identify ) for @xmath31 , find @xmath63 ; ( evaluate ) form neighboring solution @xmath60^t$ ] ; ( jn ) evaluate @xmath27 corresponding to @xmath24 ; ( decide1 ) is @xmath64 ? ; ( expert1 ) let @xmath65 and @xmath66 ; ( expert2 ) @xmath67 ; ( current ) put @xmath19 into tabu list .",
    "let @xmath68 ; ( decide3 ) are termination criteria satisfied ? ; ( refine ) refine @xmath20 ; ( stop ) stop ; ( init ) ",
    "( jb ) ; ( jb ) ",
    "( identify ) ; ( identify )  ( evaluate ) ; ( evaluate )  ( jn ) ; ( jn ) ",
    "( decide1 ) ; ( decide1 ) -| node [ near start ] yes ( expert1 ) ; ( decide1 ) -| node [ near start ] no ( expert2 ) ; ( expert1 ) |- ( current ) ; ( expert2 ) |- ( current ) ; ( current )  ( decide3 ) ; ( decide3 )  node yes ( refine ) ; ( decide3 )  node [ near start ] no + + ( -4cm,0 ) |- ( identify ) ; ( refine )  ( stop ) ;",
    "once the means have been computed , the cluster memberships can then be determined from : @xmath69 from which the objective function in ( 1 ) can be evaluated .",
    "the tabu structure used in this formulation is a list of all the means @xmath59 that have been considered in the search .",
    "since there are @xmath1 means , the tabu considered is an array with @xmath1 rows whose column length increases as the ts algorithm proceeds .",
    "if for some @xmath13 , @xmath59 obtained from minimizing ( 13 ) is in the tabu list , it is discarded and the next point @xmath58 in increasing order of their @xmath49 evaluation is chosen .",
    "if all points in the @xmath13th cluster are in the tabu list for any @xmath13 , the last entry in the @xmath13th row of the tabu list is deleted in order to allow for at least one solution to be valid .",
    "the termination criterion employed in the proposed algorithm is two - fold .",
    "first , after the maximum number of ts iterations @xmath70 has been reached , the algorithm is terminated .",
    "secondly , there is an early termination criterion whereby the algorithm is cut off after a predefined number of iterations ( called the cut - out parameter @xmath71 ) within which there is no improvement in the best found solution @xmath20 .",
    "this is an indicator of the convergence of the algorithm .",
    "the early termination is done on the assumption that the global minimum may have already been achieved . in order for this assumption to be mostly valid",
    ", the neighboring solutions generated in any iteration must not be random .",
    "otherwise , there is a good chance the global optimal solution would be found in any ts iteration .",
    "therefore , the process of generating @xmath36 random neighbors of @xmath19 from @xmath35 would not yield good results with regards to the early termination .",
    "the early termination cuts down the computational complexity as the algorithm does not need to be run for all @xmath70 iterations .      the essence of the initial restriction on the means @xmath14 to belong to the finite set @xmath2 is to make the optimization problem combinatorial , and enable the efficient exploration of the search space .",
    "once that has been achieved at the end of the ts algorithm , the means can then be unconstrained . as a result , the components of the best found solution @xmath20 are recomputed as the centroids of the clusters obtained at the end of the ts algorithm .",
    "alternatively , one may use @xmath20 as an initial solution to the k - means algorithm to obtain a refined solution .",
    "the proposed ts scheme ( i.e. using the analytic neighborhoods ) is illustrated in the flow chart of fig .",
    "for our simulations , we use four real - world datasets namely : the bavaria postal code dataset @xcite ( for two different values of @xmath1 ) , the fisher s _ iris _ dataset , the glass identification dataset , and the normalized cloud dataset @xcite ( also for two different values of @xmath1 ) .",
    "these datasets are chosen to cut across a wide range of @xmath3 , @xmath1 , and @xmath0 values .",
    "we simulate our proposed scheme in matlab on an intel core i5 - 2400 processor using the following parameters : @xmath72 and @xmath73 .",
    "we compare the performance of this scheme to the tsc , the k - means++ @xcite , and the k - means algorithms in terms of the objective function of ( 2 ) and the time taken for completion .",
    "we use the following parameter settings for the tsc algorithm : @xmath74 as suggested by al - sultan @xcite . for each dataset",
    ", we run each algorithm @xmath75 times , and provide the worst , average and best objective function values , as well as the average time for completion .",
    "the results of our simulations are summarized in tables i - vi .    .iris flower dataset + @xmath76 [ cols=\"^,^,^,^,^\",options=\"header \" , ]     [ cloud2 ]    from the tables , it can be seen that the proposed scheme achieves the best average objective function in five of the six tests performed .",
    "the best average objective function for all the tests have been highlighted in boldface .",
    "the proposed scheme consistently outperforms the tsc algorithm in terms of the computational time , average , best and worst objective function values .",
    "specifically , on the cloud dataset for @xmath77 , our algorithm achieves as much as @xmath78 improvement on the average @xmath10 , while doing so @xmath78 faster .",
    "it must be noted that both the proposed scheme and the tsc algorithm can actually be used to obtain lower values of @xmath10 than the ones reported , by increasing the value of @xmath70 ( and @xmath79 in the case of the tsc ) .",
    "however , that would be at the expense of greater computational time .",
    "compared to the k - means and k - means++ , our algorithm also performs favorably .",
    "in particular , it outperforms the k - means algorithm by as much as @xmath80 in terms of the average @xmath10 on the bavaria postal code dataset for @xmath81 .",
    "compared to the k - means++ algorithm , our approach achieves a marginal performance improvement in the average @xmath10 , reaching to @xmath82 on the bavaria postal code dataset for @xmath83 .",
    "the proposed scheme also achieves the lowest worst objective function as compared to the k - means and k - means++ algorithms on all datasets .",
    "however , in terms of the rate of convergence , the k - means++ algorithm is shown to be the best .",
    "the ts algorithm has been applied to the k - means clustering problem with a different formulation by al - sultan @xcite where a candidate solution in the form of an array of length @xmath0 is used .",
    "this array denoted as @xmath84 is made up of the cluster indices of all the @xmath0 objects in @xmath2 . in order to obtain neighboring solutions ( also known as trial solutions ) ,",
    "the cluster indices in @xmath84 are changed according to some criterion .",
    "this method can lead to bad cluster memberships .",
    "this is because while two close objects in the dataset may show a tendency of belonging to one cluster , this scheme may assign different cluster indices to them .",
    "the algorithm also involves setting the following parameters : the number of trial solutions @xmath79 , the maximum tabu list size @xmath85 , the maximum number of ts iterations @xmath70 , and a probability threshold @xmath86 .",
    "extensive parametric study has to be carried out for a particular dataset in order to obtain the optimal values .",
    "the ts clustering algorithm in @xcite discusses essentially the same procedure as the tsc algorithm with two additional neighborhood structures presented .",
    "the process of generating neighboring solutions in both of these algorithms is largely random , causing the algorithm to behave to some degree like a random search with memory .",
    "the implication of this randomness is that the global optimal solution has an equal chance of being generated in the first ts iteration as it has in the @xmath70th iteration .",
    "consequently , the probability that a global solution may have been found after @xmath71 iterations of non - improving solutions is rather low .",
    "thus , the early termination described in section iv - b can not be applied to these algorithms without a significant performance loss .",
    "the ts algorithm has also been applied to the fuzzy c - means clustering problem @xcite , where an object in the dataset @xmath2 can belong to more than one cluster to varying degrees .",
    "the ts procedure taken in that formulation aims at optimizing the cluster means , which is similar to the approach taken in our proposed scheme .",
    "however , that is as far as the similarity goes .",
    "while our scheme constrains the means to objects in the dataset and use gradient information to generate a new neighbor , this approach generates neighboring means by perturbing the current mean along a random direction .",
    "other ts approaches for clustering includes the _ packing - releasing _",
    "algorithm @xcite which is also based on @xcite , but with the following fundamental difference : a pair of objects in the dataset that are close to each other are packed together and treated as one object .",
    "these packed objects are later released .",
    "this procedure reduces the size of the search space and guides the search to a local minimum more quickly .    while we have assumed in this work that the number of clusters @xmath1 is known beforehand , the evolution - based tabu search algorithm @xcite uses ts for the determination of the number of clusters in the dataset , by considering @xmath1 as another variable to be optimized in the ts procedure .",
    "in this paper , we have presented an efficient tabu search procedure for solving the k - means clustering problem .",
    "this involves constraining the @xmath1 means to objects in the dataset , and optimizing these means via a series of neighbors that are obtained using gradient information of the objective .",
    "we have compared the proposed scheme to an existing ts algorithm as well as the k - means and k - means++ algorithms .",
    "we have shown that this approach performs favorably with these well - known algorithms , as well as not requiring too many parameter settings .",
    "this is a promising result for a lot of machine learning applications that use k - means clustering .",
    "we note , however , that the nature of the tabu structure used in our ts implementation might require a large memory , especially for big datasets where the maximum number of ts iterations is correspondingly large .",
    "for this reason , ongoing work is focused on identifying a more compact representation of the entries in the tabu structure and consequently reducing the runtime of the algorithm .",
    "b.  schlkopf et al . , `` comparing support vector machines with gaussian kernels to radial basis function classifiers , '' _ ieee transactions on signal processing _ vol .",
    "2758 - 2765 , nov .",
    "1997 .",
    "bradley and u.m .",
    "fayyad , `` refining initial points for k - means clustering , '' _ icml98 proceedings of the fifteenth international conference on machine learning _ vol .",
    "91 - 99 , jan . 1998 .",
    "d.  arthur and s.  vassilvitskii , `` k - means++ : the advantages of careful seeding , '' _ proceedings of the eighteenth annual acm - siam symposium on discrete algorithms .",
    "society for industrial and applied mathematics 2007 _ , pp .",
    "1027 - 1035 ."
  ],
  "abstract_text": [
    "<S> the tabu search ( ts ) metaheuristic has been proposed for k - means clustering as an alternative to lloyd s algorithm , which for all its ease of implementation and fast runtime , has the major drawback of being trapped at local optima . while the ts approach can yield superior performance </S>",
    "<S> , it involves a high computational complexity . </S>",
    "<S> moreover , the difficulty in parameter selection in the existing ts approach does not make it any more attractive . </S>",
    "<S> this paper presents an alternative , low - complexity formulation of the ts optimization procedure for k - means clustering . </S>",
    "<S> this approach does not require many parameter settings . </S>",
    "<S> we initially constrain the centers to points in the dataset . </S>",
    "<S> we then aim at evolving these centers using a unique neighborhood structure that makes use of gradient information of the objective function . </S>",
    "<S> this results in an efficient exploration of the search space , after which the means are refined . </S>",
    "<S> the proposed scheme is implemented in matlab and tested on four real - world datasets , and it achieves a significant improvement over the existing ts approach in terms of the intra cluster sum of squares and computational time .    </S>",
    "<S> unsupervised learning , clustering , k - means , tabu search . </S>"
  ]
}