{
  "article_text": [
    "we are concerned with the computation of an approximate solution of linear least - squares problems of the form @xmath0 with a large matrix @xmath1 with many singular values of different orders of magnitude close to the origin . in particular , @xmath1 is severely ill - conditioned and may be singular .",
    "linear least - squares problems with a matrix of this kind often are referred to as linear discrete ill - posed problems .",
    "they arise , for instance , from the discretization of linear ill - posed problems , such as fredholm integral equations of the first kind with a smooth kernel .",
    "the vector @xmath2 of linear discrete ill - posed problems that arise in applications typically represents measured data that is contaminated by an unknown error @xmath3 .",
    "let @xmath4 denote the unknown error - free vector associated with @xmath2 , i.e. , @xmath5 and let @xmath6 be the solution of the unavailable linear system of equations @xmath7 which we assume to be consistent . if @xmath1 is singular , then @xmath6 denotes the solution of minimal euclidean norm .",
    "let @xmath8 denote the moore ",
    "penrose pseudoinverse of @xmath1 .",
    "the solution of minimal euclidean norm of , given by @xmath9 typically is not a useful approximation of @xmath6 due to severe propagation of the error @xmath10 .",
    "this depends on the large norm of @xmath8 .",
    "therefore , one generally replaces the least - squares problem by a nearby problem , whose solution is less sensitive to the error @xmath10 .",
    "this replacement is known as regularization .",
    "one of the most popular regularization methods is due to tikhonov .",
    "this method replaces by a penalized least - squares problem of the form @xmath11 where @xmath12 is referred to as a regularization matrix and the scalar @xmath13 as a regularization parameter ; see , e.g. , @xcite . throughout this paper @xmath14",
    "denotes the euclidean vector norm or the spectral matrix norm .",
    "we assume that the matrices @xmath1 and @xmath15 satisfy @xmath16 where @xmath17 denotes the null space of the matrix @xmath18 . then the minimization problem has the unique solution @xmath19 for any @xmath20 .",
    "the superscript @xmath21 denotes transposition .",
    "when @xmath15 is the identity matrix , the tikhonov minimization problem ( [ eq : tik ] ) is said to be in _ standard form _ , otherwise it is in _",
    "general form_. we are interested in minimization problems ( [ eq : tik ] ) in general form .",
    "the value of @xmath20 in ( [ eq : tik ] ) determines how sensitive @xmath22 is to the error @xmath10 , how close @xmath22 is to the desired solution @xmath23 , and how small the residual error @xmath24 is .",
    "a suitable value of @xmath25 generally is not explicitly known and has to be determined during the solution process .",
    "minimization problems in general form with matrices @xmath1 and @xmath15 of small to moderate size can be conveniently solved with the aid of the generalized singular value decomposition ( gsvd ) of the matrix pair @xmath26 ; see , e.g. , @xcite .",
    "we are interested in developing solution methods for large - scale minimization problems .",
    "these problems have to be solved by an iterative method .",
    "however , the regularization matrices @xmath15 derived also may be useful for problems of small size .",
    "common choices of regularization matrices @xmath15 in when the least - squares problem is obtained by discretizing a fredholm integral equation of the first kind in one space - dimension are the @xmath27 identity matrix @xmath28 , and scaled finite difference approximations of the first derivative operator , @xmath29\\in{{{\\mathbb r}}}^{(n-1)\\times n},\\ ] ] as well as of the second derivative operator , @xmath30\\in{{{\\mathbb r}}}^{(n-2)\\times n}.\\ ] ] the null spaces of these matrices are @xmath31^t\\}\\ ] ] and @xmath32^t,[1,2,\\ldots , n]^t\\}.\\ ] ] the regularization matrices @xmath33 and @xmath34 damp fast oscillatory components of the solution @xmath22 of more than slowly oscillatory components .",
    "this can be seen by comparing fourier coefficients of the vectors @xmath35 , @xmath36 , and @xmath37 ; see , e.g. , @xcite .",
    "these matrices therefore are referred to as _ smoothing regularization matrices_. here we think of the vector @xmath22 as a discretization of a continuous real - valued function .",
    "the use of a smoothing regularization matrix can be beneficial when the desired solution @xmath23 is a discretization of a smooth function .",
    "the regularization matrix @xmath15 in ( [ eq : tik ] ) should be chosen so that known important features of the desired solution @xmath6 of can be represented by vectors in @xmath38 , because these vectors are not damped by @xmath15 .",
    "for instance , if the solution is known to be the discretization at equidistant points of a smooth monotonically increasing function , then it may be appropriate to use the regularization matrix , because its null space contains the discretization of linear functions .",
    "several approaches to construct regularization matrices with desirable properties are described in the literature ; see , e.g. , @xcite .",
    "many of these approaches are designed to yield square modifications of the matrices and that can be applied in conjunction with iterative solution methods based on the arnoldi process .",
    "we will discuss the arnoldi process more below .",
    "the present paper describes a new approach to the construction of square regularization matrices .",
    "it is based on determining the closest matrix with a prescribed null space to a given square nonsingular matrix .",
    "for instance , the given matrix may be defined by appending a suitable row to the finite difference matrix to make the matrix nonsingular , and then prescribing a null space , say , or .",
    "the distance between matrices is measured with the frobenius norm , @xmath39 where the inner product between matrices is defined by @xmath40 our reason for using the frobenius norm is that the solution of the matrix nearness problem considered in this paper can be determined with fairly little computations in this setting .",
    "we remark that commonly used regularization matrices in the literature , such as and , are rectangular .",
    "our interest in square regularization matrices stems from the fact that they allow the solution of by iterative methods that are based on the arnoldi process .",
    "application of the arnoldi process to the solution of tikhonov minimization problems was first described in @xcite ; a recent survey is provided by gazzola et al . @xcite .",
    "we are interested in being able to use iterative solution methods that are based on the arnoldi process because they only require the computation of matrix - vector products with the matrix @xmath41 and , therefore , typically require fewer matrix - vector product evaluations than methods that demand the computation of matrix - vector products with both @xmath41 and @xmath42 , such as methods based on golub  kahan bidiagonalization ; see , e.g. , @xcite for an example .",
    "this paper is organized as follows .",
    "section [ sec2 ] discusses matrix nearness problems of interest in the construction of the regularization matrices .",
    "the application of regularization matrices obtained by solving these nearness problems is discussed in section [ sec3 ] .",
    "krylov subspace methods for the computation of an approximate solution of , and therefore of , are reviewed in section [ sec4 ] , and a few computed examples are presented in section [ sec5 ] .",
    "concluding remarks can be found in section [ sec6 ] .",
    "this section investigates the distance of a matrix to the closest matrix with a prescribed null space .",
    "for instance , we are interested in the distance of the invertible square bidiagonal matrix @xmath43\\in{{{\\mathbb r}}}^{n\\times n}\\ ] ] with @xmath44 to the closest matrix with the same null space as the rectangular matrix ( [ l1 ] ) .",
    "regularization matrices of the form ( [ l1delta ] ) with @xmath44 small have been considered in @xcite ; see also @xcite for a discussion .",
    "square regularization matrices have the advantage over rectangular ones that they can be used together with iterative methods based on the arnoldi process for tikhonov regularization @xcite as well as in gmres - type methods @xcite .",
    "these applications have spurred the development of a variety of square regularization matrices .",
    "for instance , it has been proposed in @xcite that a zero row be appended to the matrix ( [ l1 ] ) to obtain the square regularization matrix @xmath45\\in{{{\\mathbb r}}}^{n\\times n}\\ ] ] with the same null space . among the questions that we are interested in",
    "is whether there is a square regularization matrix that is closer to the matrix ( [ l1delta ] ) than @xmath46 and has the same null space as the latter matrix . throughout this paper @xmath47 denotes the range of the matrix @xmath41 .",
    "[ prop1 ] let the matrix @xmath48 have @xmath49 orthonormal columns and define the subspace @xmath50 .",
    "let @xmath51 denote the subspace of matrices @xmath52 whose null space contains @xmath53 .",
    "then @xmath54 and the matrix @xmath55 satisfies the following properties :    1 .",
    "@xmath56 ; 2 .   if @xmath57 , then @xmath58 ; 3 .   if @xmath59 , then @xmath60 .",
    "we have @xmath61 , which shows the first property .",
    "the second property implies that @xmath62 , from which it follows that @xmath63 finally , for any @xmath64 , we have @xmath65 where the last equality follows from the cyclic property of the trace .    the following result is a consequence of proposition [ prop1 ] .",
    "[ cor1 ] the matrix ( [ k1 ] ) is the closest matrix to @xmath41 in @xmath51 in the frobenius norm .",
    "the distance between the matrices @xmath41 and ( [ k1 ] ) is @xmath66 .",
    "the matrix closest to a given matrix with a prescribed null space also can be characterized in a different manner that does not require an orthonormal basis of the null space .",
    "it is sometimes convenient to use this characterization .",
    "[ prop3new ] let @xmath51 be the subspace of matrices @xmath52 whose null space contains @xmath67 , where @xmath48 is a rank-@xmath68 matrix .",
    "then the closest matrix to @xmath41 in @xmath51 in the frobenius norm is @xmath69 , where @xmath70 with @xmath71 .",
    "since the columns of @xmath72 are linearly independent , the matrix @xmath73 is positive definite and , hence , invertible .",
    "it follows that @xmath74 is an orthogonal projector with null space @xmath67 .",
    "the desired result now follows from proposition [ prop1 ] .",
    "it follows from proposition [ prop1 ] and corollary [ cor1 ] with @xmath75^t$ ] , or from proposition [ prop3new ] , that the closest matrix to @xmath76 with null space @xmath77 is @xmath78 , where @xmath79\\in{{\\mathbb r}}^{n\\times n}$ ] is the orthogonal projector given by @xmath80 hence , @xmath81\\in{{{\\mathbb r}}}^{n\\times n}.\\ ] ] thus , @xmath82 is smaller than @xmath83 .",
    "we turn to square tridiagonal regularization matrices .",
    "the matrix @xmath84\\in{{{\\mathbb r}}}^{n\\times n}\\ ] ] with the same null space as ( [ l2 ] ) is considered in @xcite .",
    "we can apply propositions [ prop1 ] or [ prop3new ] to determine whether this matrix is the closest matrix to @xmath85\\in{{{\\mathbb r}}}^{n\\times n}\\ ] ] with the null space ( [ nulll2 ] )",
    ". we also may apply proposition [ prop4 ] below , which is analogous to proposition [ prop3new ] in that that no orthonormal basis for the null space is required .",
    "the result can be shown by direct computations .",
    "[ prop4 ] given @xmath86 , the closest matrix to @xmath41 in the frobenius norm with a null space containing the linearly independent vectors @xmath87 is given by @xmath88 , where @xmath89(v^{(1)},v^{(2)})+\\|v^{(2)}\\|^2 v_i^{(1)}v_j^{(1 ) } }   { \\|v^{(1)}\\|^2\\|v^{(2)}\\|^2-(v^{(1)},v^{(2)})^2}. \\label{gc}\\ ] ]    it follows easily from proposition [ prop4 ] that the closest matrix to @xmath90 with null space @xmath91 is @xmath92 , where @xmath79\\in{{\\mathbb r}}^{n\\times n}$ ] is an orthogonal projector defined by @xmath93    the regularization matrices constructed above are generally nonsymmetric .",
    "we are also interested in determining the distance between a given nonsingular symmetric matrix , such as @xmath90 , and the closest symmetric matrix with a prescribed null space , such as ( [ nulll2 ] ) .",
    "the following results shed light on this .",
    "[ prop2 ] let the matrix @xmath94 be symmetric , let @xmath48 have @xmath49 orthonormal columns and define the subspace @xmath50 .",
    "let @xmath95 denote the subspace of symmetric matrices @xmath96 whose null space contains @xmath53 .",
    "then @xmath54 and the matrix @xmath97 satisfies the following properties :    1 .",
    "@xmath98 ; 2 .   if @xmath99 , then @xmath58 ; 3 .   if @xmath100 , then @xmath101 .",
    "we have @xmath102 and @xmath61 , which shows the first property .",
    "the second property implies that @xmath103 , from which it follows that @xmath104 finally , for any @xmath105 , it follows from the cyclic property of the trace that @xmath106    [ cor2 ] the matrix ( [ k2 ] ) is the closest matrix to @xmath41 in @xmath107 in the frobenius norm .",
    "the distance between the matrices @xmath41 and ( [ k2 ] ) is given by @xmath108 .",
    "proposition [ prop2 ] characterizes the closest matrix in @xmath95 to a given symmetric matrix @xmath94 .",
    "the following proposition provides another characterization that does not explicitly use an orthonormal basis for the prescribed null space .",
    "the result follows from proposition [ prop2 ] and corollary [ cor2 ] in a straightforward manner .",
    "[ cor4new ] let @xmath95 be the subspace of symmetric matrices @xmath96 whose null space contains @xmath67 , where @xmath48 is a rank-@xmath68 matrix .",
    "then the closest matrix to the symmetric matrix @xmath41 in @xmath95 in the frobenius norm is @xmath109 , where @xmath110 is defined by ( [ projector ] ) .",
    "we are interested in determining the closest symmetric matrix to @xmath90 with null space in ( [ nulll2 ] ) .",
    "it is given by @xmath111 , with @xmath74 defined in ( [ p2 ] ) .",
    "the inequalities @xmath112 are easy to show .",
    "figure [ fig2.1 ] displays the three distances for increasing matrix dimensions .",
    "in this section we discuss the use of regularization matrices of the form @xmath113 and @xmath114 in the tikhonov minimization problem ( [ eq : tik ] ) , where @xmath74 is an orthogonal projector and @xmath115 is nonsingular .",
    "we solve the problem ( [ eq : tik ] ) by transforming it to standard form in two steps .",
    "first , we let @xmath116 and then set @xmath117 . following eldn @xcite or morigi et al .",
    "@xcite , we express the tikhonov minimization problem @xmath118 in the form @xmath119 where @xmath120 and @xmath121    let the columns of @xmath122 form an orthonormal basis for the desired null space of @xmath15",
    ". then @xmath123 .",
    "determine the qr factorization @xmath124 where @xmath125 has orthonormal columns and @xmath126 is upper triangular .",
    "it follows from ( [ nullcond ] ) that @xmath127 is nonsingular , and we obtain @xmath128 these formulas are convenient to use in iterative methods for the solution of ; see @xcite for details .",
    "let @xmath129 solve .",
    "then the solution of ( [ eq : tik1 ] ) is given by @xmath130 .",
    "we turn to the solution of ( [ eq : tik2 ] ) .",
    "this minimization problem can be expressed in standard form @xmath131 where @xmath132 .",
    "let @xmath133 solve ( [ eq : tik3 ] ) .",
    "then the solution of ( [ eq : tik2 ] ) is given by @xmath134 .",
    "in actual computations , we evaluate @xmath135 by solving a linear system of equations with @xmath115 .",
    "we can similarly solve the problem ( [ eq : tik ] ) with @xmath114 by transforming it to standard form in three steps , where the first two steps are the same as above and the last step is similar to the first step of the case with @xmath113 .",
    "it is desirable that the matrix @xmath115 not be very ill - conditioned to avoid severe error propagation when solving linear systems of equations with this matrix .",
    "for instance , the condition number of the regularization matrix @xmath76 , defined by , depends on the parameter @xmath44 .",
    "clearly , the condition number of @xmath76 , defined as the ratio of the largest and smallest singular value of the matrix , is large for @xmath44 `` tiny '' and of moderate size for @xmath136 . in the computations reported in section [ sec5 ] , we use the latter value .",
    "a variety of krylov subspace iterative methods are available for the solution of the tikhonov minimization problem ; see , e.g. , @xcite for discussions and references . the discrepancy principle is a popular approach to determining the regularization parameter @xmath25 when a bound @xmath137 for the norm of the error @xmath10 in @xmath2 is known , i.e. , @xmath138 .",
    "it can be shown that the error in @xmath139 satisfies the same bound .",
    "the discrepancy principle prescribes that @xmath20 be chosen so that the solution @xmath140 of satisfies @xmath141 where @xmath142 is a constant independent of @xmath137 .",
    "this is a nonlinear equation of @xmath25 .",
    "we can determine an approximation of @xmath140 by applying an iterative method to the linear system of equations @xmath143 and terminating the iterations sufficiently early .",
    "this is simpler than solving , because it circumvents the need to solve the nonlinear equation for @xmath25 .",
    "we therefore use this approach in the computed examples of section [ sec5 ] .",
    "specifically , we apply the range restricted gmres ( rrgmres ) iterative method described in @xcite . at the @xmath144th step",
    ", this method computes an approximate solution @xmath145 of as the solution of the minimization problem @xmath146 where @xmath147 is a krylov subspace .",
    "the discrepancy principle prescribes that the iterations with rrgmres be terminated as soon as an iterate @xmath145 that satisfies @xmath148 has been computed .",
    "the number of iterations required to satisfy this stopping criterion generally increases as @xmath137 is decreased . using the transformation from @xmath140 to @xmath22 described in section [ sec3 ] , we transform @xmath145 to an approximate solution @xmath149 of .",
    "further details can be found in @xcite .",
    "here we only note that @xmath150 can be computed without explicitly evaluating the matrix - vector product @xmath151 .",
    "we illustrate the performance of regularization matrices of the form @xmath113 and @xmath114 .",
    "the error vector @xmath152 has in all examples normally distributed pseudorandom entries with mean zero , and is normalized to correspond to a chosen noise level @xmath153 where @xmath154 denotes the error - free right - hand side vector in ( [ consistent ] ) .",
    "we let @xmath155 in ( [ discrp1 ] ) in all examples . throughout this section @xmath156 and",
    "@xmath157 denote orthogonal projectors with null spaces and , respectively .",
    "all computations are carried out on a computer with an intel core i5 - 3230 m @ 2.60ghz processor and 8 gb ram using matlab r2012a .",
    "the computations are done with about @xmath158 significant decimal digits .",
    ".example 5.1 : number of iterations , number of matrix - vector product evaluations with the matrix @xmath1 , and relative error in approximate solutions @xmath159 determined by truncated iteration with rrgmres using the discrepancy principle and different regularization matrices for several noise levels . [ cols=\"^,^,^,^\",options=\"header \" , ]     example 5.2 .",
    "regard the fredholm integral equation of the first kind , @xmath160 where @xmath161 we discretize by a galerkin method with orthonormal box functions as test and trial functions using the matlab program deriv2 from @xcite .",
    "this program yields a symmetric indefinite matrix @xmath162 and a scaled discrete approximation of the solution @xmath163 of .",
    "adding @xmath164^{t}$ ] yields the vector @xmath165 with which we compute the error - free right - hand side @xmath166 .",
    "error vectors @xmath167 are constructed similarly as in example 5.1 , and the data vector @xmath168 in ( [ eq : sys ] ) is obtained from ( [ noisefree ] ) .",
    "table [ tab5.2 ] shows results obtained with rrgmres for different regularization matrices .",
    "the performance for three noise levels is displayed .",
    "the iterations are terminated with the aid of the discrepancy principle ( [ discrp1 ] ) .",
    "when @xmath169 , @xmath170 or @xmath171 , and the noise level is @xmath172 , as well as when @xmath171 , and the noise level is @xmath173 or @xmath174 , the initial residual @xmath175 satisfies the discrepancy principle and no iterations are carried out .",
    "figure [ fig5.2 ] shows computed approximate solutions obtained for the noise level @xmath174 with the regularization matrix @xmath170 and without regularization matrix .",
    "table [ tab5.2 ] and figure [ fig5.2 ] show the regularization matrix @xmath170 to give the most accurate approximations of the desired solution @xmath6 .",
    "we remark that addition of the vector @xmath176 to to the solution vector determined by the program deriv2 enhances the benefit of using a regularization matrix different from the identity",
    ". the benefit would be even larger , if a larger multiple of the vector @xmath176 were added to the solution .",
    "@xmath177    the above examples illustrate the performance of regularization matrices suggested by the theory developed in section [ sec2 ] .",
    "other combinations of nonsingular regularization matrices and orthogonal projectors also can be applied .",
    "for instance , the regularization matrix @xmath178 performs as well as @xmath179 when applied to the solution of the problem of example 5.1 .",
    "this paper presents a novel method to determine regularization matrices via the solution of a matrix nearness problem .",
    "numerical examples illustrate the effectiveness of the regularization matrices so obtained .",
    "while all examples used the discrepancy principle to determine a suitable regularized approximate solution of ( [ eq : sys ] ) , other parameter choice rules also can be applied ; see , e.g. , @xcite for discussions and references .",
    "sn is grateful to paolo butt for valuable discussions and comments on part of the present work .",
    "the authors would like to thank a referee for comments .",
    "s. kindermann , _ discretization independent convergence rates for noise level - free parameter choice rules for the regularization of ill - conditioned problems _ , electron .",
    ", 40 ( 2013 ) , pp ."
  ],
  "abstract_text": [
    "<S> this paper is concerned with the solution of large - scale linear discrete ill - posed problems with error - contaminated data . </S>",
    "<S> tikhonov regularization is a popular approach to determine meaningful approximate solutions of such problems . </S>",
    "<S> the choice of regularization matrix in tikhonov regularization may significantly affect the quality of the computed approximate solution . </S>",
    "<S> this matrix should be chosen to promote the recovery of known important features of the desired solution , such as smoothness and monotonicity . </S>",
    "<S> we describe a novel approach to determine regularization matrices with desired properties by solving a matrix nearness problem . </S>",
    "<S> the constructed regularization matrix is the closest matrix in the frobenius norm with a prescribed null space to a given matrix . </S>",
    "<S> numerical examples illustrate the performance of the regularization matrices so obtained .    </S>",
    "<S> tikhonov regularization , regularization matrix , matrix nearness problem .    65r30 , 65f22 , 65f10 . </S>"
  ]
}