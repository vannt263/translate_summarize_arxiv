{
  "article_text": [
    "functional data are often collected in sequential form .",
    "the common situation is a continuous - time record that can be separated into natural consecutive time intervals , such as days , for which a reasonably similar behavior is expected .",
    "typical examples include the daily price and return curves of financial transactions data and the daily patterns of geophysical , meteorological and environmental data .",
    "the resulting functions may be described by a time series @xmath0 , each term in the sequence being a ( random ) function @xmath1 defined for @xmath2 taking values in some interval @xmath3 $ ] . here",
    ", @xmath4 denotes the set of integers .",
    "the object @xmath0 will be referred to as a functional time series ( see @xcite for a recent survey on time series aspects , and @xcite and @xcite for general introductions to functional data analysis ) .",
    "interest for this paper is in the functional modeling of concentration of particulate matter with an aerodynamic diameter of less than @xmath5 in ambient air , measured half - hourly in graz , austria .",
    "it is widely accepted that exposure to high concentrations can cause respiratory and related health problems .",
    "local policy makers therefore monitor these pollutants closely .",
    "the prediction of concentration levels is then a particularly important tool for judging whether measures , such as partial traffic regulation , have to be implemented in order to meet standards set by the european union .",
    "providing reliable predictions for future realizations is in fact one of the most important goals of any time series analysis . in the univariate and multivariate framework , this is often achieved by setting up general prediction equations that can be solved recursively by methods such as the durbin - levinson and innovations algorithms ( see , for example , @xcite ) .",
    "prediction equations may be derived explicitly also for general stationary functional time series ( see section 1.6 of the monograph @xcite ) but they seem difficult to solve and implement . as a consequence , much of the research in the area has focused on the first - order functional autoregressive model , shortly far(1 ) .",
    "@xcite has derived one - step ahead predictors that are based on a functional form of the yule - walker equations .",
    "@xcite have proposed nonparametric kernel predictors and illustrated their methodology by forecasting climatological cycles caused by the el nio phenomenon .",
    "while this paper , and also @xcite , have adapted classical spline smoothing techniques , @xcite , see also @xcite , have studied far(1 ) curve prediction based on linear wavelet methods .",
    "@xcite have introduced the predictive factor method , which seeks to replace functional principal components with directions most relevant for predictions .",
    "@xcite have evaluated several competing prediction models in a comparative simulation study , finding bosq s ( 2000 ) method to have the best overall performance .",
    "other contributions to the area are @xcite , and @xcite .    in spite of its statistical relevance and its mathematical appeal",
    ", functional time series modeling has still some unpleasant limitations for the practitioner .",
    "first , to date there are not many `` ready to use '' statistical software packages that can be utilized directly for estimation and prediction purposes .",
    "the only available packages that the authors are aware of are the far package of @xcite and the ftsa package of @xcite , both implemented for the statistical software r. the lack of tailor - made procedures often requires manual implementation . this may be challenging and therefore restrict use of the methodology to an academic audience .",
    "second , the methodology developed for the far(1 ) case is difficult to generalize .",
    "if an far(1 ) approach is infeasible , one can use the multiple testing procedure of @xcite to determine an appropriate order @xmath6 for a more general far(@xmath6 ) process .",
    "in addition , exogenous predictors can be incorporated using the work of @xcite .",
    "these authors include exogenous covariates of far(1 ) type into a first - order autoregressive framework for functional ozone predictions . for more general cases",
    "functional theory and estimation have not yet been developed .",
    "the goal of this paper is then to fill in this gap by promoting a simple alternative prediction algorithm which consists of three basic steps , all of which are easy to implement by means of existing software .",
    "first , use functional principal components analysis , fpca , to transform the functional time series observations @xmath7 into a vector time series of fpca scores @xmath8 of dimension @xmath9 , where @xmath9 is small compared to @xmath10 .",
    "second , fit a vector time series to the fpca scores and obtain the predictor @xmath11 for @xmath12 .",
    "third , utilize the karhunen - love expansion to re - transform @xmath11 into a curve predictor @xmath13 .",
    "the first and the third step are simple and can be performed , for example , with the fda package in r. the second step may be tackled with standard multivariate time series methodology .",
    "details are developed in section  [ s : pr ] . while the proposed approach is conceptually quite easy ,",
    "several non - trivial questions need to be raised :    1 .   how does the resulting method differ from existing ones ? 2 .",
    "why is this method justified from a theoretical standpoint ? 3 .",
    "in order to minimize the prediction error , how can the number of principal components in the dimension reduction for step  1 be determined and how should model selection be performed in step  2 ?",
    "preferably , both choices should be made simultaneously .",
    "these issues will be addressed in section  [ s : ffar ] . in particular , a comparison to bosq s  ( 2000 ) classical benchmark far(@xmath6 ) prediction is made .",
    "a theoretical bound for the prediction error of the proposed methodology is established , which will imply asymptotic consistency . in section  [ ss : fpe ] a novel functional final prediction error criterion is developed that jointly selects the order @xmath6 and the dimensionality @xmath9 of the fpc score vectors , thereby allowing for an automatic prediction process .",
    "functional principal components have been employed in other approaches to functional prediction , for example in bosq s  ( 2000 ) far(1 ) prediction method and in @xcite .",
    "roughly speaking , these and many other existing approaches have in common that @xmath14 is regressed onto the lagged observation @xmath15 by minimizing @xmath16 ^ 2dt]$ ] with respect to a linear operator @xmath17 .",
    "the solution of this problem involves an infinite series representation of @xmath17 along fpcs .",
    "( more details will be given in section  [ ss : ffar ] . ) in contrast , the proposed approach first uses dimension reduction via fpca and then fits a model to the reduced data .",
    "no a priori knowledge of the functional model is needed and instead of a single estimator , a variety of existing tools for vector processes can be entertained .",
    "further lags or exogenous covariates are also easily included into the prediction algorithm ( see section  [ s : covariates ] ) .",
    "@xcite and @xcite have suggested a curve prediction approach based on modeling fpc scores by scalar time series .",
    "they argue that scores are uncorrelated and that hence individual time series can be fit .",
    "depending on the structure of the data , this can be quick and efficient in some cases but less accurate in other cases .",
    "the fact that fpc score vectors have no instantaneous correlation , does not imply that autocovariances at lags greater than zero remain diagonal .",
    "hence univariate modeling may invoke a loss of valuable information hidden in the dependence of the data .",
    "this will be demonstrated in section  [ s : sim ] as part of a simulation study .",
    "this issue can be avoided if one makes use of so - called _ dynamic functional principal components _ recently introduced in @xcite and @xcite .",
    "these authors propose a methodology which produces score vectors with diagonal autocovariances via time invariant functional linear filters .",
    "since the involved filters are two - sided ( they require past and future observations ) , it is not clear how this methodology could be used for prediction .",
    "it should be noted that , in this article , the data @xmath14 are assumed to be given in functional form , since the focus is on working out functional prediction methodology without getting into aspects of data preprocessing , which appears to be rather specific to the particular data at hand and therefore not conducive to a unified treatment . in practice , however , one observes only vectors @xmath18 , with spacings , @xmath19 , and number of intraday sampling points , @xmath20 , potentially varying from day to day .",
    "the problem of transforming the vector observations into ( smooth ) functions has been treated in many articles and will not be detailed here . as an excellent starting point for reading in this direction the reader",
    "is referred to chapters  37 of @xcite .",
    "it is expected that the comparative results established in this paper as part of simulations and the application will hold also if the functions are not sampled equidistantly , with the rate of improvement of the proposed method over its competitors being of similar magnitude .",
    "the remainder of the paper contains some possible extensions of the new prediction methodology in section [ s : ex ] , a supporting simulation study in section [ s : sim ] and an application to the prediction of intraday patterns of particulate matter concentrations in section  [ s : appl ] .",
    "section [ s : con ] concludes and technical proofs are given in appendix [ s : comp ] .",
    "in what follows , let @xmath0 be an arbitrary stationary functional time series .",
    "it is assumed that the observations @xmath14 are elements of the hilbert space @xmath21)$ ] equipped with the inner product @xmath22 .",
    "each @xmath14 is therefore a square integrable function satisfying @xmath23 .",
    "all random functions are defined on some common probability space @xmath24 .",
    "the notation @xmath25 is used to indicate that , for some @xmath26 , @xmath27<\\infty$ ] .",
    "any @xmath28 possesses then a mean curve @xmath29\\colon t\\in[0,1])$ ] , and any @xmath30 a covariance operator @xmath31 , defined by @xmath32 $ ] .",
    "the operator @xmath31 is a kernel operator given by @xmath33 as in the multivariate case , @xmath31 admits the spectral decomposition @xmath34 where @xmath35 are the eigenvalues ( in strictly descending order ) and @xmath36 the corresponding normalized eigenfunctions , so that @xmath37 and @xmath38 . here , @xmath39 is the set of positive integers .",
    "the @xmath36 form an orthonormal basis of @xmath40)$ ] .",
    "hence @xmath41 allows for the karhunen - love representation @xmath42 . the coefficients",
    "@xmath43 in this expansion are called the fpc scores of @xmath41 .",
    "suppose now that we have observed @xmath7 .",
    "in practice @xmath44 as well as @xmath31 and its spectral decomposition will be unknown and need to be estimated from the sample .",
    "we estimate @xmath44 by @xmath45,\\ ] ] and the covariance operator by @xmath46 under rather general weak dependence assumptions these estimators are @xmath47-consistent .",
    "one may , for example , use the concept of @xmath48-@xmath49-approximability introduced in @xcite to prove that @xmath50=o(n^{-1})$ ] and @xmath51=o(n^{-1})$ ] , where the operator norm @xmath52 is , for any operator @xmath53 , defined by @xmath54 it is shown in lemma  [ le : a1 ] of the appendix that the general results ( see theorems  5 and 6 of @xcite ) apply to the functional autoregressive processes studied in this paper . from @xmath55 , estimated eigenvalues @xmath56 and estimated eigenfunctions @xmath57 can be computed for an arbitrary fixed , but typically small , @xmath58 .",
    "these estimators inherit @xmath47-consistency from @xmath55 .",
    "see theorem  7 in @xcite . for notational convenience , @xmath59 and @xmath60 will be used in place of @xmath61 and @xmath62 .",
    "functional linear prediction equations for general stationary processes have been derived in section 1.6 of the monograph @xcite .",
    "they appear to be impractical for actual data analysis as there do not seem to be either articles discussing applications to real life examples or contributions concerned with further foundational elaboration .",
    "as pointed out in the introduction , the notable exception is the far(1 ) process defined by the stochastic recursion @xmath63 where @xmath64 are centered , independent and identically distributed innovations in @xmath65 and @xmath66 a bounded linear operator satisfying @xmath67 for some @xmath68 .",
    "the latter condition ensures that the recurrence equations have a strictly stationary and causal solution in @xmath65 .",
    "@xcite has in the far(1 ) case used the prediction equations to devise what is now often referred to as the common predictor .",
    "this one - step ahead prediction is based on an estimator @xmath69 of @xmath17 and then given by @xmath70 .",
    "details of this method are given in section [ s : ffar ] , where it will be used as a benchmark to compare with the novel methodology to be introduced in the following .",
    "the new prediction technique avoids estimating operators directly and instead utilizes existing multivariate prediction methods .    the proposed prediction algorithm proceeds in three steps .",
    "first , select @xmath9 , the number of principal components to be included in the analysis , for example by ensuring that a certain fraction of the data variation is explained . with the sample eigenfunctions , empirical fpc scores @xmath71 can now be computed for each combination of observations @xmath14 , @xmath72 , and sample eigenfunctions @xmath73 , @xmath74 .",
    "the superscript @xmath75 emphasizes that empirical versions are considered .",
    "create from the fpc scores the vectors @xmath76 where @xmath77 signifies transposition . by nature of fpca ,",
    "the vector @xmath78 contains most of the information on the curve @xmath14 . in the second step ,",
    "fix the prediction lag @xmath79 .",
    "then , use multivariate prediction techniques to produce the @xmath79-step ahead prediction @xmath80 given the vectors @xmath81 .",
    "standard methods such as the durbin - levinson and innovations algorithm can be readily applied , but other options such as exponential smoothing and nonparametric prediction algorithms are available as well . in the third and last step ,",
    "the multivariate predictions are re - transformed to functional objects .",
    "this conversion is achieved by defining the truncated karhunen - love representation @xmath82 based on the predicted fpc scores @xmath83 and the estimated eigenfunctions @xmath73 .",
    "the resulting @xmath84 is then used as the @xmath79-step ahead functional prediction of @xmath85 .",
    "the three prediction steps are summarized in algorithm  [ alg:1 ] .    1 .",
    "fix @xmath9 . for @xmath72 ,",
    "use the data @xmath7 to compute the vectors @xmath76 containing the first @xmath9 empirical fpc scores @xmath86 .",
    "2 .   fix @xmath79 .",
    "use @xmath81 to determine the @xmath79-step ahead prediction @xmath87 for @xmath88 with an appropriate multivariate algorithm .",
    "3 .   use the functional object @xmath89 as @xmath79-step ahead prediction for @xmath85 .",
    "several remarks are in order .",
    "the proposed algorithm is conceptually simple and allows for a number of immediate extensions and improvements as it is not bound by an assumed far structure or , in fact , any other particular functional time series specification .",
    "this is important because there is no well developed theory for functional versions of the the well - known linear arma time series models ubiquitous in univariate and multivariate settings .",
    "moreover , if an far(@xmath6 ) structure is indeed imposed on @xmath0 , then it appears plausible that @xmath81 should approximately follow a var(@xmath6 ) model .",
    "this statement will be made precise in appendix [ s : comp ] .",
    "the far(1 ) model should in practice be employed only if it provides a reasonable approximation to the unknown underlying dynamics . to allow for more flexible predictions ,",
    "higher - order far processes could be studied .",
    "the proposed methodology offers an automatic way to select the appropriate order @xmath6 along with the dimensionality @xmath9 ( see section  [ ss : fpe ] ) .",
    "it can , in fact , be applied to any stationary functional time series .",
    "for example , by utilizing the multivariate innovations algorithm ( see section 11.4 in @xcite ) in the second step of algorithm [ alg:1 ] .",
    "how this is done in the present prediction setting is briefly outlined in section [ s : ex ] below .    it should be emphasized that the numerical implementation of the new prediction methodology is convenient in r. for the first step , fpc score matrix @xmath90 and corresponding empirical eigenfunctions can be readily obtained with the fda package . for the second step , forecasting for the fpc scores",
    "can be done in another routine step using the vars package in case var models are employed .",
    "the obtained quantities can be easily combined for obtaining .",
    "the far(1 ) model is the most often applied functional time series model .",
    "it will be used here as a benchmark to compare the proposed methodology to . without loss of generality",
    "it is assumed that @xmath91=0 $ ] .",
    "more generally , the higher - order far(@xmath6 ) model @xmath92 is considered , assuming throughout that ( i ) @xmath64 is an i.i.d .",
    "sequence in @xmath65 with @xmath93=0 $ ] , and ( ii ) the operators @xmath94 are such that equation   possesses a unique stationary and causal solution .",
    "all the above conditions are summarized as assumption  far .      in order to obtain bosq s ( 2000 ) predictor , estimation of the autoregressive operator",
    "@xmath17 is briefly discussed .",
    "the approach is based on a functional version of the yule - walker equations .",
    "let then @xmath0 be the solution of .",
    "applying @xmath95 $ ] to for any @xmath96 , leads to @xmath97&=e[\\langle \\psi(y_{k-1}),x\\rangle y_{k-1}]+ e[\\langle \\varepsilon_k , x\\rangle y_{k-1}]=e[\\langle \\psi(y_{k-1}),x\\rangle y_{k-1}].\\end{aligned}\\ ] ] let again @xmath98 $ ] be the covariance operator of @xmath99 and also let @xmath100 $ ] be the cross - covariance operator of @xmath101 and @xmath99 . if @xmath102 denotes the adjoint operator of @xmath17 , given by the requirement @xmath103 , the operator equation @xmath104 is obtained .",
    "this formally gives @xmath105 where @xmath106 $ ] .",
    "the operator @xmath107 can be estimated by @xmath108 .",
    "a more complicated object is the unbounded operator @xmath109 . using the spectral decomposition of @xmath55 ,",
    "it can be estimated by @xmath110 for an appropriately chosen @xmath9 .",
    "combining these results with an additional smoothing step , using the approximation @xmath111 , gives the estimator @xmath112 for @xmath113 .",
    "this is the estimator of @xcite .",
    "it gives rise to the functional predictor @xmath114 for @xmath115 .",
    "theorem  8.7 of @xcite provides the strong consistency of @xmath116 under certain technical assumptions .",
    "a recent result of @xcite ( see their corollary  2.1 ) shows that consistent predictions ( meaning that @xmath117 ) can be obtained in the present setting if the innovations @xmath64 are elements of @xmath118 . for these results to hold ,",
    "it is naturally required that @xmath119 .",
    "the choice of @xmath120 crucially depends on the decay rate of the eigenvalues of @xmath31 as well as on the spectral gaps ( distances between eigenvalues ) .",
    "as these parameters are unknown , a practical guideline for the dimension reduction is needed .",
    "an approach to this problem in the context of this paper will be provided in section  [ ss : fpe ] .",
    "the goal of this section is to show that the one - step predictors @xmath13 in , based on fitting var(1 ) models in step 2 of algorithm [ alg:1 ] , and @xmath121 in are asymptotically equivalent for far(1 ) processes .",
    "this statement is justified in the next theorem .",
    "[ th : equiv_var_far ] suppose model and let assumption  far hold .",
    "assume that a var(1 ) model is fit to @xmath81 by means of ordinary least squares .",
    "the resulting predictor is asymptotically equivalent to .",
    "more specifically , if for both estimators the same dimension @xmath9 is chosen , then @xmath122    the proof of theorem  [ th : equiv_var_far ] is given in section  [ ss : compv_f - b ] , where the exact difference between the two predictors is detailed .",
    "these computations are based on a more detailed analysis given in section  [ ss : compv_f ] which reveals that the fpc score vectors @xmath81 follow indeed a var(1 ) model , albeit the non - standard one @xmath123 where the matrix @xmath124 is random and the errors @xmath125 depend on the lag @xmath126 ( with precise definitions being given in section  [ ss : compv_f ] ) . given this structure",
    ", one might suspect that the use of generalized least squares , gls , could be advantageous .",
    "this is , however , not the case .",
    "simulations not reported in this paper indicate that the gains in efficiency for gls are negligible in the settings considered .",
    "this is arguably due to the fact that possible improvements may be significant only for small sample sizes for which , in turn , estimation errors more than make up the presumed advantage .    turning to the case of far(@xmath6 ) processes ,",
    "notice first that theorem  [ th : equiv_var_far ] can be established for the more general autoregessive hilbertian model ( arh(1 ) ) . in this case ,",
    "the space @xmath40)$ ] is replaced by a general separable hilbert space .",
    "the proof remains literally unchanged .",
    "using this fact , a version of theorem  [ th : equiv_var_far ] for higher - order functional autoregressions can be derived by a change of hilbert space . following the approach in section  5.1 of @xcite ,",
    "write the far(@xmath6 ) process   in state space form @xmath127 the left - hand side of is a @xmath6-vector of functions .",
    "it takes values in the space @xmath128)^p$ ] .",
    "the matrix on the right - hand side of is a matrix of operators which will be denoted by @xmath129 .",
    "the components @xmath130 and 0 stand for the identity and the zero operator on @xmath131 , respectively . equipped with the inner product @xmath132",
    "the space @xmath133 defines a hilbert space . setting @xmath134 and @xmath135 , equation",
    "can be written as @xmath136 , with @xmath137 .",
    "now , in analogy to and , one can derive the vector - functional predictors @xmath138 and @xmath139 and obtain that @xmath140 , where @xmath141 .",
    "then , the following corollary is immediate .",
    "[ cor : equiv_var_far ] consider the far(@xmath6 ) model and let assumption  far hold .",
    "further suppose that @xmath142 for some @xmath68 .",
    "then setting @xmath143 and @xmath144 one obtains @xmath145 as @xmath146 .",
    "assume the underlying functional time series to be the causal far(@xmath6 ) process . in the population",
    "setting , meaning the model is fully known , the best linear one - step ahead prediction ( in the sense of mean - squared loss ) is @xmath147 , provided @xmath148 . in this case ,",
    "the smallest attainable mean - squared prediction error is @xmath149 $ ] .",
    "both estimation methods described in sections  [ ss : ffar ] and [ ss : comp ] , however , give predictions that live on a @xmath9-dimensional subspace of the original function space .",
    "this dimension reduction step clearly introduces a bias , whose magnitude is bounded in this section .",
    "it turns out that the bias becomes negligible as @xmath150 , thereby providing a theoretical justification for the proposed methodology described in the next subsection .",
    "unlike in the previous section , it will be avoided to build the proposed procedure on the state space representation  .",
    "rather a var(@xmath6 ) model is directly fit by means of ordinary least squares to the @xmath9-dimensional score sequence .",
    "continuing to work on the population level , the theoretical predictor @xmath151 is analyzed , where @xmath152 and @xmath153 its one - step ahead linear prediction .",
    "recall that a bounded linear operator @xmath53 is called hilbert - schmidt if , for some orthonormal basis @xmath154 , @xmath155 .",
    "note that @xmath156 defines a norm on the space of compact operators which can be shown to be independent of the choice of basis @xmath154 .",
    "[ th : prederror ] consider the far(@xmath6 ) model and suppose that assumption  far holds .",
    "suppose further that @xmath157 are hilbert - schmidt operators .",
    "then @xmath158\\leq\\sigma^2+\\gamma_d,\\ ] ] where @xmath159 ^ 2\\bigg)\\sum_{\\ell = d+1}^\\infty\\lambda_\\ell \\qquad\\text{and}\\qquad \\psi_{j;d}^2=\\sum_{\\ell = d+1}^\\infty\\|\\psi_j(v_\\ell)\\|^2.\\ ] ]    the proof of theorem [ th : prederror ] is given in appendix  [ proof : th3.2 ] .",
    "the constant @xmath160 bounds the additional prediction error due to dimension reduction .",
    "it decomposes into two terms .",
    "the first is given by the fraction of variance explained by the principal components @xmath161 .",
    "the second term gives the contribution these principal components make to the hilbert - schmidt norm of the @xmath94 .",
    "note that @xmath162 and that @xmath163 . as a simple consequence",
    ", the error in tends indeed to @xmath164 for @xmath150 .    this useful result ,",
    "however , does not provide a practical guideline for choosing @xmath9 in the proposed algorithm because the bound in becomes smaller with increasing @xmath9 .",
    "rather @xmath160 has to be viewed as the asymptotic error due to dimension reduction , when @xmath9 is fixed and @xmath146 . in practice one",
    "does not have full information on the model for the observations @xmath7 and consequently several quantities , such as the autocovariance structure of the score vectors , have to be estimated . then",
    ", with larger @xmath9 , the variance of these estimators increases . in the next section , a novel criterion is provided that allows to simultaneously choose the dimension @xmath9 and the order @xmath6 in dependence of the sample size @xmath10 .",
    "this is achieved with the objective of minimizing the mean - squared prediction error mse .",
    "given that the objective of this paper is prediction , it makes sense to choose the model to be fitted to the data as well as the dimension @xmath9 of the proposed approach such that the mse is minimized .",
    "population principal components are still considered ( recalling that estimators are @xmath47-consistent ) , but in contrast to the previous section estimated processes are studied .",
    "the resulting additional estimation error will now be taken into account .",
    "let @xmath165 be a centered functional time series in @xmath65 .",
    "motivated by corollary  [ cor : equiv_var_far ] var(@xmath6 ) models are fitted to the score vectors . the target is to propose a fully automatic criterion for choosing @xmath9 and @xmath6 . by orthogonality of the eigenfunctions @xmath36 and the fact that the fpc scores @xmath166 are uncorrelated , the mse can be decomposed as @xmath167 & = e\\bigg[\\bigg\\|\\sum_{\\ell=1}^\\infty y_{n+1,\\ell}v_\\ell-\\sum_{\\ell=1}^d\\hat y_{n+1,\\ell}v_\\ell\\bigg\\|^2\\bigg ] = e\\big[\\|\\boldsymbol{y}_{n+1}-\\hat{\\boldsymbol{y}}_{n+1}\\|^2\\big]+\\sum_{\\ell = d+1}^\\infty\\lambda_\\ell,\\end{aligned}\\ ] ] where @xmath168 is also used to denote the euclidean norm of vectors .",
    "the process @xmath169 is again stationary . assuming that it follows a @xmath9-variate var(@xmath6 ) model , that is , @xmath170 with some appropriate white noise @xmath171 , it can be shown ( see , for example , @xcite ) that @xmath172 where @xmath173^\\prime)$ ] and @xmath174^\\prime)$ ] is its least squares estimator , and where @xmath175)$ ] and @xmath176 $ ] .",
    "suppose now that the estimator @xmath177 has been obtained from some independent training sample @xmath178 .",
    "such an assumption is common in the literature .",
    "see , for example , the discussion on page 95 of @xcite .",
    "it follows then that @xmath179 & = e \\big [ \\|\\boldsymbol{y}_{n+1}-(\\hat\\phi_1\\boldsymbol{y}_n+\\cdots+\\hat\\phi_p\\boldsymbol{y}_{n - p+1})\\|^2 \\big ] \\\\ & = e\\big[\\|\\boldsymbol{z}_{n+1}\\|^2\\big]+e\\big [   \\|(\\phi_1-\\hat\\phi_1)\\boldsymbol{y}_n+\\cdots+(\\phi_p-\\hat\\phi_p)\\boldsymbol{y}_{n - p+1})\\|^2\\big ] \\\\ & = \\mathrm{tr}(\\sigma_{\\boldsymbol{z}})+e\\big[\\|[i_p\\otimes(\\boldsymbol{y}_n',\\ldots,\\boldsymbol{y}_{n - p+1}')](\\boldsymbol{\\beta}-\\boldsymbol{\\hat\\beta})\\|^2\\big].\\end{aligned}\\ ] ] the independence of @xmath177 and @xmath180 yields that @xmath181(\\boldsymbol{\\beta}-\\boldsymbol{\\hat\\beta})\\|^2\\big ] & = e\\big[\\mathrm{tr}\\big\\{(\\boldsymbol{\\beta}-\\boldsymbol{\\hat\\beta})'[i_p\\otimes\\gamma_p](\\boldsymbol{\\beta}-\\boldsymbol{\\hat\\beta})\\big\\}\\big]\\\\ & = \\mathrm{tr}\\big\\{[i_p\\otimes\\gamma_p]e\\big[(\\boldsymbol{\\beta}-\\boldsymbol{\\hat\\beta})(\\boldsymbol{\\beta}-\\boldsymbol{\\hat\\beta})'\\big]\\big\\}.\\end{aligned}\\ ] ] using , it follows that the last term is @xmath182+o(1)\\right)\\sim\\frac{pd}{n}\\,\\mathrm{tr}(\\sigma_{\\boldsymbol{z}}).\\ ] ] ( here @xmath183 means @xmath184 . )",
    "combining the previous estimates and replacing @xmath185 by @xmath186 , leads to @xmath187\\approx \\frac{n+pd}{n - pd}\\,\\mathrm{tr}(\\hat\\sigma_{\\boldsymbol { z}})+\\sum_{\\ell > d}\\lambda_\\ell.\\ ] ] it is therefore proposed to jointly select the order @xmath6 and the dimension @xmath9 as the minimizers of the functional final prediction error - type criterion @xmath188 with the use of the functional fpe criterion , the proposed prediction methodology becomes fully data driven and does not need the additional subjective specification of tuning parameters .",
    "it is in particular noteworthy that the selection of @xmath9 is now made in dependence of the sample size @xmath10 .",
    "the excellent practical performance of this method is demonstrated in sections  [ s : sim ] and  [ s : appl ] .",
    "it should finally be noted that in a multivariate context @xcite originally suggested the use of the log - determinant in place of the trace in so as to make his fpe criterion equivalent to the aic criterion ( see @xcite ) . here ,",
    "however , the use of the trace is recommended , since this puts the two terms in on the same scale .",
    "in many practical problems , such as in the particulate matter example presented in section [ s : appl ] , predictions could not only contain lagged values of the functional time series of interest , but also other exogenous covariates .",
    "these covariates might be scalar , vector - valued and functional .",
    "formally the goal is then to obtain a predictor @xmath84 given observations of the curves @xmath7 and a number of covariates @xmath189 .",
    "the exogenous variables need not be defined on the same space .",
    "for example , @xmath190 could be scalar , @xmath191 a function and @xmath192 could contain lagged values of @xmath191 .",
    "the following adaptation of the methodology given in algorithm [ alg:1 ] is derived under the assumption that @xmath0 as well as the covariates @xmath193 are stationary processes in their respective spaces .",
    "the modified procedure is summarized in algorithm [ alg:3 ] .    1",
    ".   \\(a ) fix @xmath9 . for @xmath72 ,",
    "use the data @xmath7 to compute the vectors @xmath76 containing the first @xmath9 empirical fpc scores @xmath86 .",
    "+ \\(b ) for a functional covariate , fix @xmath194 . for @xmath72 ,",
    "use the data @xmath195 to compute the vectors @xmath196 containing the first @xmath194 empirical fpc scores @xmath197 . repeat this step for each functional covariate .",
    "+ \\(c ) combine all covariate vectors into one vector @xmath198 .",
    "2 .   fix @xmath79 .",
    "use @xmath81 and @xmath199 to determine the @xmath79-step ahead prediction @xmath87 for @xmath88 with an appropriate multivariate algorithm .",
    "3 .   use the functional object @xmath200 as @xmath79-step ahead prediction for @xmath85 .",
    "the first step of algorithm [ alg:3 ] is expanded compared to algorithm [ alg:1 ]",
    ". step 1(a ) performs fpca on the response time series curves @xmath7 . in step 1(b ) , all functional covariates are first transformed via fpca into empirical fpc score vectors . for each functional covariate , a different number of principal components can be selected . vector - valued and scalar covariates can be used directly .",
    "all exogenous covariates are finally combined into one vector @xmath199 in step 1(c ) .",
    "details for step  2 and the one - step ahead prediction case @xmath201 could be as follows .",
    "since stationarity is assumed for all involved processes , the resulting fpc scores form stationary time series .",
    "define hence @xmath202 and notice that these matrices are independent of @xmath203 .",
    "fix @xmath204 .",
    "the best linear predictor @xmath205 of @xmath206 given the vector variables @xmath207 can be obtained by projecting each component @xmath208 of @xmath206 onto @xmath209 then there exist @xmath210 matrices @xmath211 and a @xmath212 matrix @xmath213 , such that @xmath214 using the projection theorem , it can be easily shown that the matrices @xmath215 and @xmath213 are characterized by the equations @xmath216 let @xmath217 assuming that @xmath218 has full rank , it follows that @xmath219 the matrices @xmath220 , @xmath221 and @xmath222 have to be replaced in practice by the corresponding sample versions .",
    "this explains why predictions should not be made conditional on all data @xmath8 .",
    "it would involve the matrices @xmath223 which can not be reasonably estimated from the sample . in the application of section  [ s : appl ] a varx(@xmath6 ) model of dimension  @xmath9",
    "is fitted .",
    "the dimension  @xmath9 and the order  @xmath6 are selected by the adjusted ffpe criterion",
    "the proposed methodology has been developed with a focus on functional autoregressive processes .",
    "for this case a fully automatic prediction procedure has been constructed in section [ ss : fpe ] .",
    "it should be noted , however , that other options are generally available to the practitioner as well if one seeks to go beyond the far framework .",
    "one way to do this would be to view the fitted far process as a best approximation to the underlying stationary functional time series in the sense of the functional fpe - type criterion in [ ss : fpe ] .    in certain cases a more parsimonious modeling",
    "could be achieved if one instead used the innovations algorithm in step 2 of algorithm [ alg:1 ] .",
    "the advantage of the innovations algorithm is that it can be updated quickly when new observations arrive",
    ". it should be particularly useful if one has to predict functional moving average processes that have an infinite functional autoregressive representation with coefficient operators whose norms only slowly decay with the lag .",
    "the application of algorithm  [ alg:2 ] requires the estimation of covariances @xmath224 for increasing lag @xmath203 .",
    "such estimates are less reliable the smaller @xmath10 and the larger @xmath203 . therefore including too many lag values has a negative effect on the estimation accuracy .",
    "if estimated eigenfunctions and the covariance matrices @xmath225 are replaced by population analogues , then this algorithm gives the best linear prediction ( in mean square sense ) of the population fpc scores based on the last @xmath49 observations .",
    "fix @xmath226 .",
    "the last @xmath49 observations will be used to compute the predictor .",
    "2 .   for @xmath227 , compute @xmath228 where @xmath229 .",
    "3 .   set @xmath230 where @xmath231 the recursion is solved in the order @xmath232    1 .   compute the @xmath9-variate score vectors @xmath233 and the sample fpcs @xmath234 .",
    "2 .   for @xmath235 fix @xmath236 and compute @xmath237 where @xmath238 , are the components of the one - step ahead prediction obtained from @xmath239 by means of a multivariate algorithm .",
    "3 .   let @xmath240 .",
    "for @xmath241 , define the residuals @xmath242 .",
    "4 .   for @xmath243 $ ] ,",
    "define @xmath244 . 5 .",
    "determine @xmath245 such that @xmath246 of the residuals satisfy @xmath247$}.\\ ] ]      to assess the forecast accuracy , a method for computing uniform prediction bands is provided in this section .",
    "the target is to find parameters @xmath248 , such that , for a given @xmath249 and @xmath250\\to [ 0,\\infty)$ ] , @xmath251$}\\big)=\\alpha.\\ ] ] there is no a priori restriction on the function @xmath252 , but clearly it should account for the structure and variation of the data .",
    "although this problem is very interesting from a theoretical standpoint , only a practical approach for the determination of @xmath245 and @xmath252 is proposed here .",
    "it is outlined in algorithm  [ alg : predbands ] .",
    "the purpose of the parameter @xmath20 is to ensure a reasonable sample size for the predictions in step  2 of algorithm  [ alg : predbands ] .",
    "the residuals @xmath253 are then expected to be approximately stationary and , by a law of large numbers effect , to satisfy @xmath254\\big)\\\\ & \\quad\\approx   p\\big(-\\underline{\\xi}_\\alpha\\gamma(t)\\leq y_{n+1}(t)-\\hat y_{n+1}(t)\\leq   \\overline{\\xi}_\\alpha\\gamma(t)\\quad\\text{for all } t\\in[0,1]\\big).\\end{aligned}\\ ] ] note that , in step  1 , the principal components @xmath234 have been obtained from the entire sample @xmath7 and not just from the first @xmath203 observations .",
    "the choice of @xmath252 in step  4 clearly accounts for the variation of the data . for an intraday time",
    "exhibiting a higher volatility there should also be a broader prediction interval .",
    "typically the constants @xmath255 and @xmath256 are chosen equal , but there may be situations when this is not desired .",
    "one advantage of this method is that it does not require particular model assumptions .",
    "if two competing prediction methods exist , then the one which is performing better on the sample will lead to narrower prediction bands .",
    "simulation results not reported in this paper indicate that algorithm  [ alg : predbands ] performs well in finite samples even for moderate sample sizes .",
    "to analyze the finite sample properties of the new prediction method , a comparative simulation study was conducted .",
    "the proposed method was tested on a number of functional time series , namely first- and second - order far processes , first - order fma processes and farma processes of order ( 1,2 ) . in each simulation run , @xmath257 ( or @xmath258 ) observations were generated of which the first @xmath259 ( or @xmath260 ) were used for parameter estimation as well as order and dimension selection with the ffpe(@xmath261 ) criterion . on the remaining @xmath262 ( or @xmath263 ) observations one - step ahead predictions and the corresponding squared prediction errors were computed . from these mean ( mse ) ,",
    "median ( medse ) and standard deviation ( sd ) were calculated .",
    "if not otherwise mentioned , this procedure was repeated @xmath264 times .",
    "more details and a summary of the results are given in sections  [ se : scalar][se : farma ] .    since in simulations",
    "one can only work in finite dimensions , the setting consisted of @xmath265 fourier basis functions @xmath266 on the unit interval @xmath267 $ ] , which together determine the ( finite - dimensional ) space @xmath268 .",
    "note that an arbitrary element @xmath96 has the representation @xmath269 with coefficients @xmath270 . if @xmath271 is a linear operator , then @xmath272 where @xmath273 is the matrix whose @xmath274-th column and @xmath275-th row is @xmath276 , and @xmath277 is the vector of basis functions .",
    "the linear operators needed to simulate the functional time series of interest can thus be represented by a @xmath278 matrix that acts on the coefficients in the basis function representation of the curves .",
    "the corresponding innovations were generated according to @xmath279 where @xmath280 are i.i.d .",
    "normal random variables with mean zero and standard deviations @xmath281 that will be specified below .      as mentioned in the introduction , a special case of the proposed method was considered by @xcite and @xcite .",
    "motivated by the fact that pca score vectors have uncorrelated components , these authors have proposed to predict the scores individually as univariate time series .",
    "this will be referred to as the _ scalar method _",
    ", in contrast to the _ vector method _ promoted in this paper .",
    "the scalar method is fast and works well as long as the cross - spectra related to the score vectors are close to zero .",
    "however , in general the score vectors have non - diagonal autocorrelations .",
    "then , scalar models are not theoretically justified . to explore the effect of neglecting cross - sectional dependence ,",
    "far(1 ) time series of length @xmath257 were generated as described above . for the purpose of demonstration @xmath282 and @xmath283",
    "were chosen .",
    "two autocovariance operators @xmath284 and @xmath285 with corresponding matrices @xmath286 were tested .",
    "both matrices are orthogonal with norm @xmath287 . in these simple settings",
    "it is easy to compute the population autocorrelation function ( acf ) of the 3-dimensional fpca score vectors .",
    "the acf related to the score sequences of the process generated by @xmath284 is displayed in figure  [ fig : acf ] .",
    "it shows that two scores are uncorrelated at lag zero and that there is almost no temporal correlation in the individual score sequences .",
    "however , at lags greater than @xmath288 there is considerable dependence in the cross - correlations between the first and the third score sequence .",
    ".,width=377 ]    the analogous plot for @xmath285 would reveal a contrary behavior : while the autocorrelations of the individual score sequences decay slowly , cross - correlations are zero at all lags .    given these observations ,",
    "it is expected that the scalar method will do very well in forecasting the scores when data are generated by operator @xmath285 , while it should be not competitive with the vector method if @xmath284 is used .",
    "this conjecture is confirmed in figure  [ fig : ratios ] which shows histograms of the ratios @xmath289 obtained from 1000 simulation runs .",
    "the grey histogram refers to the time series generated by @xmath285 .",
    "it indicates that the scalar method is a bit favorable , as the ratios tend to be slightly larger than one .",
    "contrary to this , a clear superiority of the vector method can be seen when data stem from the sequence generated by @xmath284 . in a majority of the cases , the mse resulting from the vector methods is less than half as large as the corresponding mse obtained by the scalar method .",
    "it should also be mentioned that @xmath6 and @xmath9 where estimated for the proposed method , while they were fixed at the true values @xmath290 and @xmath291 for the scalar predictions .     in for the far(1 ) processes given by the operators @xmath284 ( white ) and @xmath285 ( grey ) . ]      in this section the proposed prediction is compared on far(2 ) processes @xmath292 to the standard predicton of @xcite . for the latter ,",
    "the multiple testing procedure of @xcite was utilized to determine the order @xmath6 of the far model to be fitted . following these authors ,",
    "@xmath9 was chosen as the smallest integer such that the first @xmath9 principal components explain at least @xmath293 of the variance of the data . to ensure that the multiple testing procedure keeps an overall asymptotic level of 10%",
    ", the levels in three subtests ( so testing up to a maximal order @xmath294 ) were chosen to be 5% , 3% and 2% , respectively .",
    "for ease of reference this method will be referred to as the bkr method .",
    "owing to the results of section  [ s : ffar ] , both methods are expected to yield similar results if the order @xmath6 was known and if the same dimension @xmath9 was chosen for the two predictors .",
    "the operators were generated such that @xmath295 and @xmath296 with @xmath297 to ensure stationarity .",
    "the case @xmath298 yields the far(1 ) process .",
    "the operator @xmath17 was chosen at random . more precisely ,",
    "choosing @xmath299 , a @xmath278 matrix of independent , zero - mean normal random variables with corresponding standard deviations @xmath300 was generated .",
    "this matrix was then scaled so that the resulting matrix @xmath273 has induced norm equal to 1 . in every iteration of the simulation",
    "runs @xmath273 was newly generated .",
    "two types of standard deviations for the innovations in were chosen , namely @xmath301 note that if @xmath302 , then @xmath303 if @xmath304 or @xmath305 by the riemann - lebesgue lemma .",
    "this will be reflected in the corresponding matrices by choosing @xmath300 as a decaying sequence in @xmath274 and @xmath275 .",
    "in particular we have chosen @xmath306 for setting ( @xmath3071 ) and @xmath308 for setting ( @xmath3072 ) .",
    ".functional final prediction error ( ffpe ) , mean squared prediction error based on the ffpe criterion ( @xmath309 ) , mean squared prediction error based on bkr ( @xmath310 ) , and the corresponding proportions of variance explained by the chosen number of fpcs ( @xmath311 , @xmath312 ) .",
    "the first row in each setting @xmath313 corresponds to @xmath257 , the second row to @xmath314 .",
    "[ cols= \" < , < , < , < , < , < , < , < , < , < , < , < \" , ]     pm10 concentrations are known to be high at locations suffering from severe temperature inversions such as the basin areas of the alps .",
    "following @xcite , temperature difference between graz ( @xmath315 above sea level ) and kalkleiten ( @xmath316 above sea level ) can be utilized to model this phenomenon .",
    "temperature inversion is often seen as a key factor influencing pm10 concentrations because temperatures increasing with sea level result in a sagging exchange of air , thereby yielding a higher pollutant load at the lower elevation .    to illustrate functional prediction with covariates ,",
    "temperature difference curves of graz and kalkleiten have been included as a dependent variable .",
    "for the overall sample , the first two fpcs of the temperature difference curves describe about @xmath317 of the variance .",
    "hence , fpca was used for covariate dimension reduction , leading to the inclusion of a two - dimensional exogenous regressor ( which is almost equivalent to the true regressor curve ) in the second step of algorithm  [ alg:3 ] .",
    "then a @xmath9-variate varx(@xmath6 ) model was fit with @xmath9 and @xmath6 selected by the functional final prediction error - type criterion adjusted for the covariate : @xmath318 here @xmath319 is the dimension of the regressor vector ( in the present case , @xmath320 ) and @xmath321 is the covariance matrix of the residuals when a model of order @xmath6 and dimension @xmath9 is fit .",
    "the latter method is referred to as fpex .",
    "the corresponding prediction results are summarized in table  [ tab : pm10 ] . a further significant improvement in the mean and median square ( out - of - sample ) prediction error can be observed .",
    "this paper proposes a new prediction methodology for functional time series that appears to be widely and easily applicable .",
    "it is based on the idea that dimension reduction with functional principal components analysis should lead to a vector - valued time series of fpc scores that can be predicted with any existing multivariate methodology , parametric and nonparametric . the multivariate prediction is then transformed to a functional prediction using a truncated karhunen - love decomposition .",
    "the proposed methodology seems to be advantageous for several reasons . among them",
    "is its intuitive appeal , made rigorous for the predominant far(@xmath6 ) case , but also its ease of application as existing software packages can be readily used , even by non - experts .",
    "it is in particular straightforward to extend the procedure to include exogenous covariates into the prediction algorithm .",
    "simulations and an application to pollution data suggest that the proposed method leads to predictions that are always competitive with and often superior to the benchmark predictions in the field .",
    "it is hoped that the present article can spawn interest among researchers working in the active area of functional time series .",
    "it is stated in section  [ s : pr ] that empirical mean and covariance are @xmath47-consistent estimators for their population counterparts for a large class of functional time series .",
    "the following lemma makes this statement precise for far(@xmath6 ) processes .",
    "the notation of section  [ ss : comp ] is adopted .",
    "[ le : a1 ] consider the far(@xmath6 ) model and suppose that assumption  far holds .",
    "further suppose that @xmath142 for some @xmath68 . then ( i ) @xmath322=o(1/n)$ ] .",
    "( ii ) if in addition @xmath323 in @xmath324 , then @xmath325=o(1/n)$ ] .",
    "if follows from proposition  2.1 in @xcite and theorem  3.1 in @xcite that @xmath326 is @xmath327-@xmath49-approximable under ( i ) and @xmath328-@xmath49-approximable under ( ii ) .",
    "@xmath48-@xmath49-approximability is inherited by the projection @xmath329 .",
    "now the proof follows from theorems  5 and 6 in @xcite .      in case of a var(1 ) , step 2 .  of algorithm [ alg:1 ] can be performed with least squares . to explicitly calculate @xmath330 , apply @xmath331 to both sides of @xmath332 to obtain @xmath333 with remainder terms @xmath334 where @xmath335 noting that @xmath336 can always be extended to an orthonormal basis of @xmath327 .",
    "some notation is needed .",
    "set @xmath337 and @xmath338 where @xmath339 and let @xmath340 be the matrix with entry @xmath341 in the @xmath274th row and the @xmath275th column , @xmath342 .",
    "let moreover @xmath343 , @xmath344 , @xmath345 , @xmath346 , @xmath347 and @xmath348 . replacing the eigenfunctions @xmath349 by their sample counterparts @xmath73 , empirical versions of the above variables",
    "are denoted by @xmath78 , @xmath350 , @xmath351 , @xmath352 , @xmath124 and @xmath353 . for a vector @xmath354 ,",
    "the operation @xmath355 creates a @xmath210 matrix , whose @xmath274-th column contains the elements @xmath356 .",
    "define now @xmath357 to arrive at the equations @xmath358 the equations in formally resemble var(1 ) equations .",
    "notice , however , that it is a nonstandard formulation , since the errors @xmath125 are generally not centered and dependent .",
    "furthermore , @xmath125 depends in a complex way on @xmath126 , so that the errors are not uncorrelated with past observations .",
    "the coefficient matrix @xmath124 is also random , but fixed for fixed sample size @xmath10 . in the sequel these effects are ignored .",
    "utilizing some matrix algebra , can be written as the linear regression @xmath359 where @xmath360 .",
    "the ordinary least squares estimator is then @xmath361 and the prediction equation @xmath362 follows directly , defining @xmath363 .",
    "recall the notations introduced above equation . in order to prove the asymptotic equivalence between @xmath121 in and @xmath13 in for the case of far(1 ) functional time series , observe first that @xmath364 where @xmath365 is the @xmath210 matrix with entries @xmath366 determined by the fpc scores",
    "@xmath367 , and @xmath368 signifies the kronecker product . with the help of , the var(1 )",
    "based predictor can be written in the form @xmath369{x^e}^\\prime{\\boldsymbol{z}}^e\\right)\\right)^\\prime{\\boldsymbol{y}}_n^e\\right\\}^\\prime\\hat{\\boldsymbol{v}},\\ ] ] with @xmath370 being the vector of the first @xmath9 empirical eigenfunctions .",
    "on the other hand , defining the @xmath210 matrix @xmath371 by the entries @xmath372 , direct verification shows that takes the form @xmath373{x^e}^\\prime{\\boldsymbol{z}}^e\\right)\\right)^\\prime{\\boldsymbol{y}}_n^e\\right\\}^\\prime\\hat{\\boldsymbol{v}}.\\ ] ] the only formal difference between the two predictors under consideration is therefore in the matrices @xmath365 and @xmath371 .",
    "now , for any @xmath342 , @xmath374 so that @xmath375 implies @xmath376 in the following @xmath168 will be used for the @xmath327 norm , the euclidean norm in @xmath377 and matrix norm @xmath378 , for a square matrix @xmath379 .",
    "let @xmath380\\frac{1}{n-1}{x^e}^\\prime{\\boldsymbol{z}}^e\\bigg).\\ ] ] the orthogonality of the @xmath381 together with pythagoras theorem and bessel s inequality imply that @xmath382 define @xmath383 and notice that @xmath384 and hence @xmath385 .",
    "let @xmath386 .",
    "since @xmath387 , iterative applications of the cauchy - schwarz inequality yield @xmath388 it remains to estimate @xmath389 .",
    "the next step consists of using the fact that , for any @xmath390 , it holds that @xmath391 , provided all inverse matrices exist .",
    "now choose @xmath392 and @xmath393 .",
    "since in the given setting the time series @xmath394 is stationary and ergodic , it can be deduced that @xmath395 with probability one .",
    "thus @xmath396 for large enough @xmath10 , and consequently @xmath397^{-1}(\\hat\\gamma-\\tilde\\gamma ) \\tilde\\gamma^{-1}\\big\\| \\\\[.2 cm ] & \\leq\\big\\|\\tilde\\gamma^{-1}\\big\\|^2\\big\\|\\hat\\gamma-\\tilde\\gamma\\big\\|\\big\\|\\big[i_d+(\\hat\\gamma-\\tilde\\gamma)\\tilde\\gamma^{-1}\\big]^{-1}\\big\\| \\\\[.2 cm ] & \\leq\\frac{\\big\\|\\tilde\\gamma-\\hat\\gamma\\big\\|}{\\hat\\lambda_d^2}\\sum_{\\ell=0}^\\infty\\bigg(\\frac{\\|\\tilde\\gamma-\\hat\\gamma\\|}{\\hat\\lambda_d}\\bigg)^{\\ell}\\\\ & = o_p\\bigg(\\frac 1n\\bigg).\\end{aligned}\\ ] ] it has been assumed here that @xmath398 .",
    "if @xmath399 , then the model has dimension @xmath400 . in this case both estimators will of course be based on at most @xmath401 principal components .",
    "putting together all results , the statement of theorem [ th : equiv_var_far ] is established .      using the results and notations of section  [ ss : fpe ]",
    ", it follows that @xmath167 & = e\\big[\\|\\boldsymbol{y}_{n+1}-\\hat{\\boldsymbol{y}}_{n+1}\\|^2\\big ] + \\sum_{i > d}\\lambda_i.\\end{aligned}\\ ] ] some algebra shows that@xmath402 where the @xmath210 matrices @xmath403 have entry @xmath404 in the @xmath275th column and @xmath274th row , and @xmath405 with @xmath9-variate vectors @xmath406 and @xmath407 taking the respective values @xmath408 and @xmath409 in the @xmath274th coordinate",
    ".    the best linear predictor @xmath11 of @xmath410 based on @xmath411 satisfies @xmath412 \\leq e\\big[\\|\\boldsymbol{y}_{n+1}-(\\boldsymbol\\psi_1 \\boldsymbol{y}_{n}+\\cdots+\\boldsymbol\\psi_p\\boldsymbol{y}_{n - p+1})\\|^2\\big ] = e\\big[\\|\\boldsymbol{s}_n\\|^2\\big]+e\\big[\\|\\boldsymbol{t}_n\\|^2\\big].\\ ] ] the last equality comes from the fact that , due to causality , the components in @xmath407 and in @xmath406 are uncorrelated",
    ". observe next that , by bessel s inequality , @xmath413 = \\sum_{\\ell=1}^de[\\langle \\varepsilon_{n+1},v_\\ell\\rangle^2]\\leq\\sigma^2 .",
    "$ ] it remains to bound @xmath414 $ ] .",
    "for this term , it holds @xmath415 & = e\\bigg[\\sum_{\\ell=1}^d\\bigg(\\sum_{j=1}^p\\sum_{\\ell^\\prime = d+1}^\\infty y_{n+1-j,\\ell^\\prime}\\langle \\psi_j(v_{\\ell^\\prime}),v_\\ell\\rangle\\bigg)^2\\bigg ] \\\\ & \\leq e\\bigg[\\sum_{d=1}^\\infty\\bigg\\langle\\sum_{j=1}^{p}\\sum_{\\ell^\\prime = d+1}^\\infty y_{n+1-j,\\ell^\\prime } \\psi_j(v_{\\ell^\\prime}),v_\\ell\\bigg\\rangle^2\\bigg ] \\\\ & = e\\bigg[\\bigg\\|\\sum_{j=1}^p\\sum_{\\ell^\\prime = d+1}^\\infty y_{n+1-j,\\ell^\\prime } \\psi_j(v_{\\ell^\\prime})\\bigg\\|^2\\bigg],\\end{aligned}\\ ] ] where parseval s identity was applied in the final step . repeatedly using the cauchy - schwarz inequality ,",
    "the last expectation can be estimated as @xmath416 \\big\\langle \\psi_{j}(v_{\\ell}),\\psi_{j^\\prime}(v_{\\ell^\\prime})\\big\\rangle \\\\ & \\leq\\sum_{j , j^\\prime=1}^p \\bigg(\\sum_{\\ell = d+1}^\\infty\\sqrt{\\lambda_{\\ell}}\\| \\psi_{j}(v_{\\ell})\\|\\bigg ) \\bigg(\\sum_{\\ell^\\prime = d+1}^\\infty\\sqrt{\\lambda_{\\ell^\\prime}}\\| \\psi_{j^\\prime}(v_{\\ell^\\prime})\\|\\bigg)\\\\ & \\leq\\sum_{\\ell''=d+1}^\\infty\\lambda_{\\ell''}\\sum_{j , j^\\prime=1}^p \\bigg(\\sum_{\\ell = d+1}^\\infty\\|\\psi_{j}(v_{\\ell})\\|^2\\bigg)^{1/2 } \\bigg(\\sum_{\\ell^\\prime = d+1}^\\infty\\|\\psi_{j^\\prime}(v_{\\ell^\\prime})\\|^2\\bigg)^{1/2 } \\\\ & = \\sum_{\\ell = d+1}^\\infty\\lambda_\\ell\\bigg(\\sum_{j=1}^p\\bigg[\\sum_{\\ell = d+1}^\\infty\\|\\psi_{j}(v_{\\ell})\\|^2\\bigg]^{1/2}\\bigg)^2.\\end{aligned}\\ ] ] collecting all estimates finishes the proof .",
    "alexander aue is associate professor , department of statistics , university of california , davis , ca 95616 ( e - mail : aaue@ucdavis.edu ) .",
    "diogo dubart norinho is graduate student , department of computer science , university college london , london wc1e 6bt , uk ( e - mail : ucabdub@ucl.ac.uk .",
    "siegfried hrmann is charg de cours , department of mathematics , universit libre de bruxelles , b-1050 brussels , belgium ( e - mail : shormann@ulb.ac.be ) .",
    "the authors are grateful to the editor , the associate editor and the reviewers for constructive comments and support .",
    "this research was partially supported by nsf grants dms 0905400 , dms 1209226 and dms 1305858 , communaut franaise de belgique  actions de recherche concertes ( 20102015 ) and the iap research network grant nr .",
    "p7/06 of the belgian government ( belgian science policy ) .",
    "part of this work was completed during a research in pairs stay ( aue and hrmann ) at the mathematical research institute oberwolfach .",
    "panaretos , v.  tavakoli , s. 2013 , ` cramr - karhunen - love representation and harmonic principal component analysis of functional time series ' , _ stochastic processes and their applications _ * 123 * ,  27792807 ."
  ],
  "abstract_text": [
    "<S> this paper addresses the prediction of stationary functional time series . </S>",
    "<S> existing contributions to this problem have largely focused on the special case of first - order functional autoregressive processes because of their technical tractability and the current lack of advanced functional time series methodology . </S>",
    "<S> it is shown here how standard multivariate prediction techniques can be utilized in this context . </S>",
    "<S> the connection between functional and multivariate predictions is made precise for the important case of vector and functional autoregressions . </S>",
    "<S> the proposed method is easy to implement , making use of existing statistical software packages , and may therefore be attractive to a broader , possibly non - academic , audience . </S>",
    "<S> its practical applicability is enhanced through the introduction of a novel functional final prediction error model selection criterion that allows for an automatic determination of the lag structure and the dimensionality of the model . </S>",
    "<S> the usefulness of the proposed methodology is demonstrated in a simulation study and an application to environmental data , namely the prediction of daily pollution curves describing the concentration of particulate matter in ambient air . </S>",
    "<S> it is found that the proposed prediction method often significantly outperforms existing methods . + </S>",
    "<S> * keywords : * dimension reduction ; final prediction error , forecasting , functional autoregressions ; functional principal components , functional time series ; particulate matter , vector autoregressions + * msc 2010 : * primary 62m10 , 62m20 ; secondary 62p12 , 60g25 </S>"
  ]
}