{
  "article_text": [
    "sparse principal components analysis ( pca ) is a relatively new and popular technique for simultaneous dimension reduction and variable selection in high - dimensional data analysis [ e.g. , @xcite ] .",
    "it combines the central idea of classic ( or ordinary ) pca [ @xcite ] with the notion of sparsity : it seeks linear transformations that reduce the dimension of the data , while depending on a small number of variables , but retain as much variation as possible . in the population setting , these linear transformations correspond to the projectors of the @xmath0-dimensional principal subspaces , spanned by the eigenvectors of the population covariance matrix .",
    "the appeal of sparsity is that it not only enhances interpretability , but it can yield consistent estimates when sparsity is truly present in the population , even in high dimensions [ @xcite ] .    the development of sparse pca has taken a brisk pace over the past decade .",
    "methodological developments include regularized estimators based on penalizing or constraining the variance maximization formulation of pca [ @xcite ] , regression or low - rank approximation [ @xcite ] , convex relaxations [ daspremont , bach and el  ghaoui ( @xcite ) , @xcite ] , two - stage procedures based on diagonal thresholding [ @xcite ] and algorithmic variations of iterative thresholding [ @xcite ] .",
    "theoretical developments including consistency , rates of convergence , minimax risk bounds for estimating eigenvectors and principal subspaces and detection have been established under various statistical models [ @xcite , @xcite ( @xcite ) , @xcite ] .",
    "the presence of a sparse `` truth '' has been an explicit assumption in the theoretical analysis of sparse pca and is often an implicit assumption in its methodological development . here",
    "the `` truth '' refers to the leading @xmath0-dimensional principal subspace .",
    "this naturally raises questions about the properties of sparse pca methods and how they depend on the assumption of sparsity . under what conditions",
    "can the relevant variables be selected consistently if the truth is assumed to be sparse ?",
    "if the truth is not sparse , and/or not unique , what can be said about the results of sparse pca ?",
    "the first question is essentially concerned with variable selection consistency , or _ sparsistency_. the second question is a bit more slippery , because it essentially requires us to assume nothing beyond independence of the observations . in other words ,",
    "the second question is concerned with _",
    "agnostic inference properties _ of an estimation method . in this paper",
    ", we investigate variable selection consistency and agnostic inference properties of the recently proposed _ fantope projection and selection _",
    "( fps ) method due to @xcite .",
    "fps formulates the sparse pca problem as a semidefinite program ( sdp ) whose solution is a sparse estimate of the projector of the principal subspace .",
    "it extends the so - called dspca formulation of @xcite from the one - dimensional ( @xmath1 ) case to the multidimensional ( @xmath2 ) case , and it presents a change in perspective by focusing on projectors rather than individual eigenvectors .",
    "fps is appealing for both theoretical and computational reasons .",
    "since it directly estimates the projector of the @xmath0-dimensional principal subspace , there is no need for iterative deflation [ e.g. , @xcite ] , and hence an sdp need only be solved once rather than @xmath0 separate times as in dspca .",
    "@xcite developed an efficient alternating direction method of multipliers algorithm to compute fps , and established @xmath3 consistency of fps under very mild conditions on the population and input matrices .",
    "most notably , fps does not require the stringent spiked covariance model assumption ( i.e. , the population covariance matrix is a sparse low - rank matrix plus identity ) that is required by many competing methods such as diagonal thresholding .",
    "this makes fps applicable to a much wider range of problems , including the important case of correlation matrices where diagonal thresholding can not even be used .",
    "( see section  [ sec : sparsistency ] for another example . )",
    "however , the variable selection and agnostic inference properties of fps remain unknown .",
    "sparsistency is the ability of an estimator to accurately select the correct subset of variables when applied to a random sample generated from a model where only a subset of variables is assumed to be relevant .",
    "conditions under which sparsistency holds provide important insights about both the estimator and the model .",
    "they have been studied extensively in other high - dimensional inference problems such as linear regression [ @xcite ] and gaussian graphical model selection [ @xcite ] .",
    "in contrast , theoretical analyses of sparse pca have mainly focused on consistency and rates of convergence in matrix norm , with relatively less progress on variable selection .",
    "an exception is @xcite , who analyzed dscpca under a stringent spiked covariance model with @xmath4 , where the population covariance matrix is block diagonal and its leading eigenvector is assumed to have a small number of nonzero entries of constant magnitude .",
    "their work is an important first step , but it leaves open whether or not their stringent conditions can be loosened and it also does not address the @xmath5 case .",
    "in the first part of this paper , we investigate the sparsistency of fps under general conditions .",
    "our main results ( theorems [ thm : deterministic ] and [ thm : probabilistic - main ] ) give broad sufficient conditions under which fps can exactly recover the relevant variables . roughly , the conditions are that ( 1 ) the relevant variables are not too correlated with the irrelevant variables ( limited correlation ) , and ( 2 ) the leverages ( diagonals of the projector ) of the relevant variables are large enough .",
    "interestingly , these conditions are analogous to so - called ( 1 ) `` irrepresentability '' and ( 2 ) `` @xmath6-min '' conditions for variable selection consistency of the lasso [ @xcite ] . to our knowledge , this is the first sparsistency result for principal subspaces . when @xmath4 , it generalizes the results of @xcite in several directions , the most important of which is that it relaxes their block - diagonal condition on the population covariance matrix .",
    "the second part of this paper addresses the question of assumption - free interpretation of sparse pca within a framework that we call _ agnostic inference_. our goal is to provide both analysis and interpretation of sparse pca with essentially no assumptions beyond independence of observations .",
    "the terminology is borrowed from the learning theory literature where the chief concern is estimating a classifier or regression function without assumptions on the model [ @xcite ] ; however , much of our perspective is influenced by earlier work on maximum likelihood under misspecification [ @xcite ] , interpretations obtained by extending the maximum likelihood principle [ @xcite ] , and the notion of _ persistence _ of high - dimensional linear predictors proposed by @xcite .",
    "our point is that although fps is derived under the assumption of sparsity , its results can still be interpreted even when sparsity does not hold .",
    "the main result ( theorem  [ thm : agnostic - fps ] ) is that without assuming sparsity or identifiability , fps provides a sparse , linear dimension - reducing transformation that is close to the best possible in terms of maximizing the _",
    "predictive covariance_.    the remainder of the paper is organized as follows .",
    "section  [ sec : prelim ] provides the technical background and conditions that are necessary to state our results ",
    "divided between sections  [ sec : sparsistency ] ( sparsistency ) and [ sec : agnostic ] ( agnostic inference ) .",
    "we discuss these results in section  [ sec : discussion ] , and defer their proofs to the .",
    "finally , we collect our notation below for our readers convenience .      for two matrices @xmath7 with conformable dimensions",
    ", @xmath8 denotes the trace inner product . for a vector @xmath9 and @xmath10 $ ] , @xmath11 is the @xmath12 norm if @xmath13 ; when @xmath14 , @xmath15 is the number of nonzero entries of @xmath16 ; when @xmath17 , @xmath18 . for a matrix @xmath19 , and index sets",
    "@xmath20 $ ] and @xmath21 $ ] , @xmath22 denotes the @xmath23 submatrix of @xmath24 consisting of rows in @xmath25 and columns in @xmath26 , and @xmath27 ( @xmath28 ) denotes the submatrix consists of corresponding rows ( columns ) .",
    "given @xmath29 $ ] and @xmath19 , the matrix @xmath30-pseudonorm @xmath31 is defined as @xmath32 . as usual , the spectral norm of @xmath24 is denoted @xmath33 and the frobenius norm is @xmath34 . if @xmath24 is a symmetric matrix , @xmath35 denotes the @xmath36th largest eigenvalue of @xmath24 .",
    "we will use @xmath37 to denote the @xmath38 underlying true covariance matrix , whose ordered eigenvalues are @xmath39 . for a square matrix @xmath24 , @xmath40 denotes its diagonal vector . for a vector @xmath16",
    ", @xmath41 is the support of @xmath16 ( the index set corresponding to nonzero entries ) .",
    "let @xmath42 be a symmetric matrix with spectral decomposition @xmath43 where @xmath44 are eigenvalues and @xmath45 is an orthonormal basis of eigenvectors .",
    "the @xmath0-dimensional principal subspace of @xmath37 is the subspace spanned by @xmath46 .",
    "it is unique if and only if the spectral gap @xmath47 , and its projector ( orthogonal projection matrix ) is @xmath48 where @xmath49 is the orthonormal matrix with columns @xmath50 .",
    "every subspace has a unique projector and so we will consider the principal subspace and @xmath51 to be equivalent , and we will also assume that @xmath0 is known or fixed in advance .",
    "estimation of the principal subspace requires at minimum that it be well - defined .",
    "when this is the case , we can consider @xmath51 to be a mapping @xmath52 and so it makes sense to consider indices of the variables that @xmath51 depends on .",
    "since @xmath51 is positive semidefinite , this is equivalent to the indices of the nonzero diagonal entries of @xmath51 , because row / column @xmath53 of @xmath51 is nonzero if and only if @xmath54 .",
    "@xmath37 satisfies the _ sparse principal subspace _ condition with support set @xmath55 if    @xmath56    ( [ sps ] ) is the minimal requirement for sparse principal subspace estimation , and the assumption will only be used in section  [ sec : sparsistency ] in our investigation of sparsistency .",
    "the spectral gap condition ensures that the principal subspace is identifiable , and the support set definition states that the principal subspace does not depend on variables outside of @xmath55 .",
    "this corresponds to a notion of subspace sparsity introduced by @xcite called @xmath57 _ row sparsity _ , and it can be shown that @xmath58 for any orthonormal basis @xmath59 of the principal subspace [ @xcite ] .      when ( [ sps ] ) is assumed , the main statistical inference problem considered in this paper is , in a general setting , to estimate @xmath55 from a symmetric noisy version @xmath60 of @xmath37 .",
    "we then extend the interpretation and analytical properties of sparse pca solutions without assuming ( [ sps ] ) . in both parts ,",
    "the estimation accuracy depends on the noisiness of @xmath60 as an approximation to @xmath37 , which will be quantified by an entrywise tail bound on @xmath61    as motivated by principal component analysis , it may be helpful to think of  @xmath37 as the covariance of a @xmath62-dimensional random vector and @xmath63 as sample covariance matrix of a random sample of size @xmath64 , but that is not strictly necessary for our theoretical analysis .",
    "in fact , our sparsistency results do not even have to assume that @xmath37 or @xmath60 are positive semidefinite . in the following ,",
    "we describe two probabilistic models that imply a strong entrywise tail bound on @xmath65 .",
    "[ exa : sample - cov ] let @xmath66 be i.i.d .",
    "random vectors with @xmath67 and let @xmath60 be the sample covariance matrix : @xmath68 where @xmath69 .",
    "we assume throughout this paper that @xmath70 by bernstein s inequality [ @xcite , chapter  2.2 ] if @xmath71 has sub - gaussian tails in that there exists constants @xmath72 such that @xmath73\\qquad \\mbox{for all } v\\neq 0,\\ ] ] then there is an absolute constant @xmath74 such that @xmath60 satisfies , for @xmath75 , @xmath76 in other words , the maximum entrywise error is bounded by @xmath77 with high probability",
    ". this fact will be the starting point of subsequent analysis of the sparsistency of the fps estimator introduced in section  [ sec : fps ] .",
    "the tail bound ( [ eq : max - tail - bound ] ) is well known and a proof of a stronger result that implies ( [ eq : max - tail - bound ] ) can be found in @xcite , lemma 3.2.2 .",
    "[ exa : clique ] here , we give an example that does not involve an i.i.d .",
    "random sample and the rate of error bound on @xmath60 depends only on @xmath62 . consider a random graph model with @xmath62 nodes where edges appear independently with probability @xmath78 for all @xmath79 .",
    "let @xmath24 be the random adjacency matrix such that @xmath80 according to the presence / absence of edge , then the pair @xmath81 and @xmath82 satisfies @xmath83 for some universal constant @xmath84 .",
    "this model is related to the planted clique problem where @xmath85 for all @xmath86 , and @xmath87 everywhere else .",
    "the leading eigenvector of @xmath37 is @xmath88 and it is supported on @xmath89 .",
    "our main result implies that fps finds the planted clique with high probability when @xmath90 for some absolute constant @xmath84 .",
    "this is within a factor of @xmath91 of the best known result for polynomial time recovery in the planted clique problem [ @xcite ] .",
    "@xcite give another reduction of the planted clique model to a sparse pca problem .",
    "for simplicity of notation and presentation , we will focus on the case of sample covariance matrix in the rest of this paper .",
    "but most of our sparsistency results are applicable to a broader range of problems as exemplified in the random graph example .",
    "vu et  al .",
    "( @xcite ) recently proposed an estimator for @xmath51 , called _ fantope projection and selection _ ( fps ) , defined as a solution @xmath92 to the following semidefinite program : @xmath93 where @xmath94 is the trace-@xmath0 _ fantope _ , @xmath95 , and @xmath96 is a tuning parameter .",
    "@xcite showed that fps can be efficiently computed by alternating direction method of multipliers [ admm , e.g. , @xcite ] .",
    "when @xmath97 , a solution is given by the projector of the @xmath0-dimensional principal subspace of @xmath60 ( see lemma  [ lem : fantop - projection ] below ) .",
    "the @xmath98 penalty term encourages the solution to be sparse .",
    "moreover , the decomposability of the @xmath98 penalty term [ @xcite ] makes it straightforward to analyze the statistical properties of fps .",
    "in particular , @xcite established a near - optimal frobenius norm error bound for the fps estimator under general conditions . in the next section",
    ", we will show that , if @xmath37 satisfies the ( [ sps ] ) and @xmath60 satisfies the maximum error bound assumption ( [ eq : max - tail - bound ] ) , then under mild conditions , @xmath99=j$ ] with high probability for appropriate choices of @xmath100 .    in general",
    ", the solution to ( [ eq : fps ] ) , and hence the fps estimator may not be unique .",
    "however , we will show that it is unique with high probability when the ( [ sps ] ) and maximum error bound assumption hold .",
    "the argument utilizes the following elastic net version of fps : @xmath101 since the objective is a strongly concave function , the solution of ( [ eq : fps - en ] ) is unique .",
    "a very interesting and important fact is that when @xmath100 and @xmath102 are small enough , if a solution of ( [ eq : fps ] ) is sparse then it must be the unique solution of ( [ eq : fps - en ] ) .",
    "this observation will be proved in the and play a key role in establishing the uniqueness of solution for the original fps problem .",
    "we conclude this section by introducing some basic properties of the fantope , which will be used repeatedly in the proof of main results .",
    "further properties and discussion of the fantope will be given in section  [ sec : agnostic ] .",
    "denote the euclidean projection of a @xmath38 symmetric matrix @xmath24 onto @xmath103 by @xmath104    [ lem : fantop - projection ] let @xmath24 be a symmetric matrix with eigenvalues @xmath105 and orthonormal eigenvectors @xmath106 .    1 .   [ kyfan ] @xmath107 and",
    "the maximum is achieved by the projector of a @xmath0-dimensional principal subspace of @xmath24 . moreover ,",
    "the maximizer is unique if and only if @xmath108 .",
    "[ fantopeprojection ] @xmath109 , where @xmath110 and @xmath111 satisfies the equation @xmath112 .",
    "[ fantopeprojection2 ] if @xmath113 , then @xmath114 uniquely .    a proof of lemma  [ lem : fantop - projection ] is given in section  [ sec : additional - proof ] .",
    "throughout this section , we assume that @xmath37 satisfies ( [ sps ] ) with dimension @xmath0 and support set @xmath115 for some @xmath116 , and that @xmath60 satisfies the maximum error bound condition ( [ eq : max - tail - bound ] ) with some @xmath117 .",
    "the sample covariance matrix is covered as a special case in view of example  [ exa : sample - cov ] .",
    "intuitively , variable selection would be easier if the relevant variables ( those in  @xmath55 ) and noise variables ( those in @xmath118 ) are not too correlated . in the context of sparse linear regression",
    ", such an intuition leads to the famous irrepresentable condition [ @xcite ] . in sparse subspace estimation",
    ", we have the analogous limited correlation condition ( [ lcc ] ) . in order to state the condition concisely ,",
    "we use the following block representation of @xmath37 : @xmath119 similar block representations can be defined for @xmath60 and @xmath120 .    our main technical condition , the limited correlation condition ( [ lcc ] ) is given below .",
    "a symmetric matrix @xmath37 satisfies the _ limited correlation condition _ with constant @xmath121 $ ] if    @xmath122    ( [ lcc ] ) contains the condition assumed by @xcite as a special case , where @xmath123 , and hence ( [ lcc ] ) holds with @xmath124 . another popular model for sparse",
    "pca is the _ spiked covariance model _",
    ", where @xmath125 , @xmath126 , and @xmath123 .",
    "an important difference between ( [ lcc ] ) and the assumptions in previous works is that previous assumptions , for example , the spiked covariance model , usually imply that the relevant variables can be selected with good accuracy by thresholding the diagonal entries , while ( [ lcc ] ) contains situations where such diagonal thresholding intuition does not work . here , we illustrate this difference by a toy example with @xmath127 , @xmath1 , @xmath128 : @xmath129 this @xmath37 satisfies ( [ lcc ] ) with @xmath130 for any @xmath131 , but picking large diagonal entries of @xmath37 does not select the relevant variables .    to our knowledge ,",
    "the ( [ lcc ] ) is the first sufficient condition for consistent sparse pca variable selection without assuming @xmath37 being block - diagonal and is also the first sufficient condition for sparse subspace variable selection consistency .",
    "we state two versions of our main results .",
    "the first is a more general , deterministic result that provides sufficient conditions for uniqueness , false positive control , and false negative control of @xmath132 .",
    "the second specializes the general result to the case where @xmath60 satisfies an entrywise error bound ( [ eq : max - tail - bound ] ) like the sample covariance matrix in example  [ exa : sample - cov ] , and provides probabilistic guarantees for sparsistency of fps .",
    "[ thm : deterministic ] assume @xmath37 satisfies ( [ sps ] ) . if the fps penalty parameter @xmath100 satisfies @xmath133 and @xmath134 then the solution @xmath92 of fps problem ( [ eq : fps ] ) is unique and satisfies@xmath135 . if in addition , either @xmath136 then the fps solution satisfies @xmath137 .",
    "theorem  [ thm : deterministic ] consists of two parts .",
    "the first part provides a set of sufficient conditions [ ( [ eq : deterministic - condition-1 ] ) and ( [ eq : deterministic - condition-2 ] ) ] for no false positives .",
    "the second part gives two additional conditions that individually guarantee no false negatives , and hence exact recovery .",
    "we discuss these parts separately .",
    "( [ eq : deterministic - condition-1 ] ) reveals the motivation for ( [ lcc ] ) .",
    "when ( [ lcc ] ) holds , one can choose @xmath138 so that ( [ eq : deterministic - condition-1 ] ) holds .",
    "on the other hand , ( [ eq : deterministic - condition-2 ] ) puts some upper bound constraint on @xmath100 .",
    "when @xmath60 is random and satisfies the maximum error bound condition ( [ eq : max - tail - bound ] ) , @xmath139 depends on @xmath140 .",
    "then ( [ eq : deterministic - condition-1 ] ) and ( [ eq : deterministic - condition-2 ] ) jointly put a constraint on @xmath141 so that there exists a @xmath100 satisfying both conditions .",
    "( [ eq : deterministic - condition-2 ] ) puts an upper bound on the sparsity penalty parameter @xmath100 .",
    "it may seem counterintuitive since a larger value of @xmath100 will lead to a sparser solution .",
    "in fact , @xmath100 can not be too large because otherwise the @xmath98 penalty term will outweigh the pca objective in the fps problem , leading to a large estimation bias .",
    "consider the example given in ( [ eq : toy - example ] ) with @xmath142 , if @xmath143 and @xmath144 ; the fps solution will return a projection matrix corresponding to eigenvector @xmath145 , which is supported outside of the true subset . in general , when @xmath146 , the fps solution will be a diagonal matrix taking value 1 on diagonal entries corresponding to the @xmath0 largest diagonal entries of @xmath60 , and 0 elsewhere .",
    "the proof of false positive control in theorem  [ thm : deterministic ] , as given in section  [ sec : proof - thm : deterministic ] , consists of two main steps .",
    "the first step ( section  [ sec : proof - existence ] ) is to show that there exists a solution of the fps problem ( [ eq : fps ] ) supported on @xmath55 , using the primal  dual witness ( pdw ) argument [ @xcite ] .",
    "the pdw argument first constructs a sparse solution @xmath147 supported on @xmath55 by solving the fps problem ( [ eq : fps ] ) under additional sparsity constraint @xmath148\\subseteq j$ ] .",
    "then it is shown that when @xmath100 is large enough , with high probability one can find a dual variable @xmath149 such that the primal  dual pair @xmath150 satisfies the kkt condition , and hence is optimal for the original problem .",
    "when the solution is unique , this ensures that the optimizer is supported on @xmath55 .",
    "the challenge here is to establish kkt condition when @xmath37 is not block diagonal , which requires a careful and delicate subspace perturbation analysis in comparing the fps solution and the population projector ( lemmas  [ lem : sparse - solution ] and [ lem : primal - dual - optimality ] ) .",
    "the second step is to show that , under the conditions assumed in the theorem , the sparse solution constructed in the primal  dual witness argument is indeed rank-@xmath0 and also unique .",
    "our proof of uniqueness is novel and makes use of the elastic net version of fps ( [ eq : fps - en ] ) . a key fact used in the proof",
    "is that , for small enough values of @xmath102 , the two problems have the same solution and the uniqueness of fps solution follows essentially from that of the elastic net version .",
    "the details are given in section  [ sec : uniqueness - proof ] .",
    "having established false positive control in theorem  [ thm : deterministic ] , full sparsistency will be established if we can show that the number of false negatives is also zero . in sparsity pattern recovery , the number of false negatives is typically controlled by assuming a lower bound on the magnitude of signals carried by relevant variables . in the context of principal subspace estimation , our first sufficient condition for false negative control ( [ eq : signal - strength-1 ] ) originates from a frobenius norm error bound of fps established in @xcite : @xmath151    the other sufficient condition for controlling false negative ( [ eq : entry - wise - min ] ) is motivated by an assumption used by @xcite for the @xmath1 case where the leading eigenvector is assumed to be @xmath152 ( where @xmath153 is the @xmath154 vector of ones , and the signs of nonzero entries can actually be arbitrary ) and @xmath155 .",
    "let @xmath156 be the @xmath157 matrix of entry - wise signs of @xmath158 .",
    "our condition ( [ eq : entry - wise - min ] ) generalizes that of @xcite in three directions .",
    "first , we allow principal subspaces of dimension @xmath5 .",
    "second , we allow nonzero correlation between the relevant and irrelevant variables , whereas @xcite assumes a block diagonal structure . third , we do not require a generalized spiked covariance model as in @xcite .",
    "the proof of the second part of theorem  [ thm : deterministic ] is given in section  [ sec : additional - proof ] .",
    "[ thm : probabilistic - main ] assume that @xmath37 satisfies ( [ sps ] ) and ( [ lcc ] ) , and that @xmath60 satisfies the maximum error bound ( [ eq : max - tail - bound ] ) with scaling factor @xmath159 . if @xmath160 and the fps penalty parameter @xmath100 in ( [ eq : fps ] ) satisfies @xmath161 then with probability at least @xmath162 , the fps estimate @xmath163 is unique and satisfies @xmath164 . if in addition , either @xmath165 then @xmath166 .",
    "using the maximum error bound condition , with probability at least @xmath162 we have @xmath167 .",
    "this together with the property ( [ lcc ] ) of  @xmath37 establishes ( [ eq : deterministic - condition-1 ] ) . on the other hand , ( [ eq : sample - complexity ] ) ensures that ( [ eq : deterministic - condition-2 ] ) holds . on the other hand , ( [ eq : lower - bound - condition - prob-1 ] ) implies ( [ eq : signal - strength-1 ] ) , and ( [ eq : lower - bound - condition - prob-2 ] ) implies that the choice of @xmath100 satisfies ( [ eq : entry - wise - min ] ) .",
    "the claimed results follow from theorem  [ thm : deterministic ]",
    ".    when the eigenvalues of @xmath37 are constants and do not change with @xmath168 , theorem  [ thm : probabilistic - main ] recovers a rate developed by @xcite as a special case where theorem  [ thm : probabilistic - main ] implies that a sufficient condition for consistent variable selection ( with suitable choice of @xmath100 ) is @xmath169 for a constant @xmath84 [ according to ( [ eq : sample - complexity ] ) and ( [ eq : lower - bound - condition - prob-2 ] ) ] .",
    "@xcite also obtain a sharper sufficient condition @xmath170 , by assuming that the solution is rank 1 .",
    "however , @xcite show that , with high probability , the solution is not rank 1 unless @xmath171 is bounded by a constant .    condition ( [ eq : sample - complexity ] ) suggests that the required sample size needs to increase as @xmath172 increases .",
    "this is because the oracle operator norm error bound of the principal subspace ( i.e. , assuming @xmath55 is known ) has a factor of @xmath172 . in an extremal case ,",
    "when @xmath172 is large and @xmath173 ( @xmath174 ) are much smaller , the estimation error of the leading eigenvector will likely dominate all the remaining spectral gaps , making it hard to recover the remaining eigenvectors .",
    "@xmath175consistent estimation and variable selection inevitably depend on the existence of a `` true '' model . for sparse pca",
    ", this corresponds to the assumption that the @xmath0-dimensional principal subspace of @xmath37 is ( 1 ) identifiable and  ( 2 ) sparse . under this assumption ,",
    "previous work [ e.g. , @xcite ] and the theory presented in section  [ sec : sparsistency ] establish conditions under which consistent estimation and variable selection are possible . while these results can provide useful insights for sparse pca and fps , the conditions may or may not hold in practice . therefore , it is important to understand the statistical inference problem without these assumptions",
    ". this is the _",
    "agnostic inference _ perspective .",
    "can we remove the assumptions of identifiability and sparsity ?",
    "is there an assumption - free interpretation for fps ?    without assuming identifiability , variable selection and estimation consistency are no longer valid objectives , since there is no unique `` true '' parameter to estimate .",
    "for example , when @xmath176 , every @xmath0-dimensional subspace is a principal subspace , and even if there is a unique principal subspace , it may not be sparse . to develop an assumption - free interpretation , we return to the basic objective function of pca .",
    "let @xmath71 be a random vector with covariance matrix @xmath37 .",
    "pca can be interpreted as a covariance maximization technique .",
    "it seeks a rank-@xmath0 projector @xmath177 that maximizes the _ predictive covariance _ :",
    "@xmath178 if we interpret @xmath177 as a dimension - reducing transformation , then @xmath179 is just the total covariance between the input @xmath71 and output @xmath180 .",
    "fps also maximizes covariance , but it replaces the rank-@xmath0 projector constraint on @xmath177 with a fantope constraint and an additional sparsity constraint via the @xmath181-norm .",
    "let @xmath182 by lagrangian duality , this constrained form of fps is equivalent to the penalized form ( [ eq : fps ] ) in the sense that given @xmath60 , for every @xmath183 there is a corresponding @xmath100 such that a solution of ( [ eq : fps ] ) is also a solution of ( [ eq : fps - constraint ] ) and vice - versa .",
    "the corresponding population version of ( [ eq : fps - constraint ] ) is @xmath184    the meaning of @xmath185 may be unclear since it is not necessarily a rank-@xmath0 projector .",
    "however , it turns out that if we regard @xmath177 as a linear transformation @xmath186 , then @xmath177 is a _ smoother matrix _ [ @xcite , section  5.4.1 ] and the fantope coincides with a class of linear smoothers called _ shrinking smoothers _ [ @xcite ] .",
    "the two essential properties of @xmath177 are :    1 .",
    "this is equivalent to the condition that @xmath188 in other words , the sum of squares of the transformation @xmath189 and its residual @xmath190 can not be larger than that of @xmath191 .",
    "a map satisfying this property is called _ firmly nonexpansive_. 2 .   @xmath192 . if @xmath177 is a projector , then @xmath0 is the dimension of the projection space .",
    "it is also equal to @xmath193 $ ] when @xmath194 is a random vector with @xmath195 . by analogy , @xmath196 is the _ effective degrees of freedom _ of @xmath177 [ see @xcite , section  5.4.1 ] .",
    "these two properties are exactly those laid out by @xcite for smoother matrices and shrinking smoothers . in the context of dimension reduction , we call the action of @xmath185 _ shrinking dimension reduction_.    now we turn to the @xmath181 norm constraint in ( [ eq : fps - constraint - pop ] ) . a natural notion of sparsity of a matrix @xmath185 is @xmath197 , the number of nonzero rows . here , we use the @xmath181-norm as an alternative convex measure of sparsity .",
    "for @xmath185 we have , by cauchy ",
    "schwarz , @xmath198 that is , if @xmath197 is small , then @xmath199 must also be small .",
    "our main result in the assumption - free setting is an interpretation of the constrained form of fps and its persistence under no assumptions on @xmath37 .",
    "[ thm : agnostic - fps ] let @xmath200 be i.i.d .",
    "random vectors that satisfy the tail probability bound ( [ eq : x - sub - gaussian ] ) ( i.e. , @xmath71 is sub - gaussian ) . then with probability at least @xmath162 , @xmath201 where @xmath74 is a constant .",
    "our proof of theorem  [ thm : agnostic - fps ] is given in section  [ sec : additional - proof ] .",
    "theorem  [ thm : agnostic - fps ] shows that the predictive covariance of fps comes close to that of the best sparse @xmath177 in the fantope .",
    "this is essentially an assumption - free interpretation .",
    "let @xmath202 be the best rank-@xmath0 and @xmath203-sparse projector .",
    "what can we say about @xmath204 and @xmath205 ? in this case , ( [ eq:1 - 1-bounded - by-2 - 0 ] ) implies that @xmath206 .",
    "thus @xmath205 is in the feasible set of ( [ eq : fps - constraint ] ) if @xmath207 .",
    "if we do not assume any structure on @xmath37 , theorem  [ thm : agnostic - fps ] implies that , with high probability , @xmath208 when @xmath207 .",
    "if we assume in addition that @xmath37 does have a @xmath0-dimensional principal subspace involving at most @xmath203 variables , then the result can be strengthened to @xmath209 here , the assumption that @xmath37 has a sparse principal subspace is still much weaker than the sparse principal subspace condition required by the sparsistency argument in section  [ sec : sparsistency ] , because there is no requirement on uniqueness of the principal subspace . as a simple example",
    ", @xmath210 satisfies the sparsity condition but not the uniqueness condition .",
    "a referee has pointed out to us that there is another interpretation of theorem  [ thm : agnostic - fps ] in terms of the _ continuity _ of the maximal predictive covariance map @xmath211 the proof of theorem  [ thm : agnostic - fps ] implies that @xmath212 so the predictive covariance of fps is relatively stable under perturbations of @xmath37 if @xmath213 is small .",
    "a connection between sparse pca and sparse linear regression has been observed by @xcite .",
    "they established minimax rates for estimation under @xmath3 loss with @xmath12-penalized estimators with suitably defined model parameters and observed that the rates are identical to those for sparse linear regression when the effective noise variance is defined appropriately .",
    "the sparsistency result in the present paper further extends this connection to variable selection . roughly speaking , the previously used spiked covariance model in sparse pca , which assumes that @xmath214 where @xmath49 is @xmath215 orthonormal matrix and @xmath216 is diagonal",
    "[ see , e.g. , @xcite ] , corresponds to the orthogonal design in linear regression , in the sense that the relevant and noise variables are not correlated .",
    "moreover , the @xmath217 term boosts the signal by adding @xmath218 to all the relevant diagonal entries in @xmath37 and , therefore , thresholding based methods usually work well .",
    "the limited correlation condition developed in this paper is analogous to the irrepresentable condition [ @xcite ] for @xmath98-penalized sparse regression ( lasso ) , where convex optimization methods can succeed when the correlation between relevant and noise variables is small .    when the eigenvalues of @xmath37 are fixed , a sufficient condition for consistent variable selection using fps is @xmath219",
    "this is comparable to the corresponding rate developed for @xmath1 by @xcite when the rank of the solution is not assumed to be @xmath220 .",
    "it has been shown by @xcite that the information - theoretic critical rate is @xmath221 .",
    "that is , if @xmath222 , no method can succeed in variable selection .",
    "it remains an open question if there exist polynomial time methods that can consistently select relevant variables in the range @xmath223 . an interesting work in this direction",
    "is that by @xcite , which shows that , for @xmath1 , testing a sparse pca model in this regime is at least as hard as solving the planted clique problem beyond the well - believed computational barrier .",
    "the predictive covariance maximization interpretation of pca leads to a natural characterization of the fantope as the collection of all shrinking smoothers with @xmath0 effective degrees of freedom . without any assumptions on @xmath37 ,",
    "fps gives us a dimension reducing transformation that is sparse while being computationally tractable , and it nearly approaches the best predictive covariance . in practice , it would be useful to estimate the predictive covariance of the fps solution for a particular value of @xmath100 using risk estimates such as cross - validation .",
    "this leads to a data - driven procedure for selecting the best fps tuning parameter @xmath100 .",
    "the detailed design and properties of such a cross - validation method is an important and interesting topic for future work .",
    "[ sec : proof - deterministic ]",
    "this appendix contains detailed technical proofs . in section  [ sec : proof - thm : deterministic ] , we prove the deterministic sparsistency theorem ( theorem  [ thm : deterministic ] ) . other proofs , including those of lemma  [ lem : fantop - projection ] and theorem  [ thm : agnostic - fps ] are given in section  [ sec : additional - proof ] .",
    "the primal  dual witness argument starts from the dual form of the fps problem ( [ eq : fps ] ) . using strong duality ,",
    "we can write ( [ eq : fps ] ) in a equivalent min  max form : @xmath224 & & \\qquad \\iff\\quad\\max_{h\\in\\mathcal{f}^{k}}\\min_{z\\in\\mathbb{b}_p } \\langle s , h \\rangle-\\rho\\langle h , z \\rangle - k\\rho \\nonumber \\\\[1.5pt ] \\label{eq : fps - min - max } & & \\qquad \\iff\\quad\\max_{h\\in\\mathcal{f}^{k}}\\min_{z\\in\\mathbb{b}_p } \\langle s- \\rho z , h \\rangle \\\\[1.5pt ] \\label{eq : fps - max - min } & & \\qquad\\iff\\quad\\min_{z\\in\\mathbb{b}_p}\\max_{h\\in\\mathcal{f}^{k } } \\langle s- \\rho z , h \\rangle,\\end{aligned}\\ ] ] where @xmath225 .",
    "according to the standard karush ",
    "tucker ( kkt ) condition , a pair @xmath226 is optimal for problems ( [ eq : fps - min - max ] ) and ( [ eq : fps - max - min ] ) if and only if @xmath227 \\label{eq : kkt-2 } \\widehat{z}_{ij } & \\in & [ -1,1]\\qquad \\forall i\\neq j , \\widehat { h}_{ij}=0 , \\\\[1.5pt ] \\label{eq : kkt-3 } \\widehat{h}&= & \\mathop{\\arg\\max}_{h\\in\\mathcal{f}^{k } } \\langle s-\\rho \\widehat{z } , h \\rangle.\\end{aligned}\\ ] ]    to proceed with the primal  dual witness argument , we first construct an additionally constrained solution @xmath147 as follows : @xmath228 let @xmath229 be a corresponding optimal dual variable . by lemma  [ lem : sparse - solution ]",
    ", @xmath147 is a rank-@xmath0 projector supported on @xmath55 .",
    "let @xmath230 and @xmath231 be @xmath232 orthogonal matrices consisting of the @xmath0 leading eigenvectors of @xmath233 and @xmath37 , respectively , where @xmath234 and @xmath235 are @xmath236 orthogonal matrices . according to lemma  [ lem : sparse - solution ] ,",
    "there exists a @xmath157 orthonormal matrix @xmath237 such that @xmath238 and @xmath239 .",
    "define a modified primal ",
    "dual pair @xmath240 as follows ( recall that @xmath241 ) : @xmath242 \\label{eq : dual - construct-1 } \\widehat{z}_{jj}&=&\\tilde z_{jj } , \\\\[1.5pt ] \\label{eq : dual - construct-2 } \\widehat{z}_{ij}&=&\\frac{1}{\\rho } \\bigl\\{s_{ij}-\\langle q_{i * } , \\sigma_{j , j } \\rangle \\bigr\\ } , \\qquad   ( i , j)\\in j\\times j^c , \\\\[1.5pt ] \\label{eq : dual - construct-3 } \\widehat{z}_{ij } & = & \\frac{1}{\\rho}w_{ij } , \\qquad ( i , j)\\in \\bigl(j^c\\bigr)^2 , i\\neq j.\\\\[-30pt]\\nonumber\\end{aligned}\\ ] ]    we need to check that @xmath243 is feasible for ( [ eq : fps - min - max ] ) and ( [ eq : fps - max - min ] ) and satisfies the kkt conditions ( [ eq : kkt-1 ] ) to ( [ eq : kkt-3 ] ) .    _ checking feasibility_. the feasibility of @xmath92 is obvious . to check feasibility of @xmath244",
    ", we only need to verify that @xmath245 $ ] for all @xmath246 .",
    "in fact , @xmath247 \\\\[1.5pt ] & \\le & \\frac{1}{\\rho } \\bigl[{\\vert}w{\\vert}_{\\infty,\\infty } + \\bigl { \\vert}(i - q)_{i*}\\bigr{\\vert}\\times{\\vert}\\sigma_{j , j}{\\vert}\\bigr ] \\\\[1.5pt ] & \\le & \\frac{1}{\\rho } \\bigl[{\\vert}w{\\vert}_{\\infty,\\infty } + { \\vert}i - q { \\vert}_f { \\vert}\\sigma_{j^c j}{\\vert}_{2,\\infty } \\bigr ] \\\\[1.5pt ] & \\le & \\frac{1}{\\rho } \\biggl[{\\vert}w{\\vert}_{\\infty,\\infty}+ \\frac{8\\rho s}{\\lambda_k-\\lambda_{k+1 } } { \\vert}\\sigma_{j^c j}{\\vert}_{2,\\infty } \\biggr ] \\le 1,\\end{aligned}\\ ] ] where the last inequality follows from ( [ eq : deterministic - condition-1 ] ) .    _ checking kkt condition _ ( [ eq : kkt-1 ] ) . because @xmath163 only has nonzero entries in @xmath248 , so @xmath240 satisfies ( [ eq : kkt-1 ] ) by construction .",
    "_ checking kkt condition _ ( [ eq : kkt-2 ] ) . for @xmath249 in @xmath248 , ( [ eq : kkt-2 ] ) is satisfied for @xmath240 because the same condition is satisfied for @xmath250 . for @xmath251 , we have @xmath252 and ( [ eq : kkt-2 ] ) follows from the feasibility of @xmath244",
    ".    _ checking kkt condition _ ( [ eq : kkt-3 ] ) .",
    "recall that @xmath120 .",
    "let @xmath253 be the @xmath254 diagonal matrix that agrees with @xmath255 on diagonal entries .",
    "by lemma  [ lem : fantop - projection ] , it suffices to show that @xmath230 spans a @xmath0-dimensional principal subspace of @xmath256 which is established in lemma  [ lem : primal - dual - optimality ] .",
    "now we have shown that @xmath240 is indeed an optimal primal  dual pair for ( [ eq : fps - min - max ] ) and ( [ eq : fps - max - min ] ) , and hence @xmath92 is a solution of ( [ eq : fps ] ) and is also supported only on @xmath55 .",
    "consider the elastic net version of fps in  ( [ eq : fps - en ] ) and its max  min and min ",
    "max forms using dual variable @xmath257 : @xmath258 the kkt condition for optimality of @xmath259 becomes @xmath260 \\qquad \\forall i\\neq j , \\widehat{h}_{ij}=0 , \\\\ \\widehat{h } & = &   \\mathcal{p}_{\\mathcal{f}^{k } } \\biggl(\\frac{1}{\\tau } ( s-\\rho \\widehat{z } ) \\biggr).\\end{aligned}\\ ] ]    let @xmath147 , @xmath229 be the support constrained fps solution in ( [ eq : sparse - solution ] ) and @xmath244 be the dual variable constructed in ( [ eq : dual - construct-1 ] ) to ( [ eq : dual - construct-3 ] ) . we first show that @xmath261 is also optimal for the elastic net version of fps when @xmath102 is small enough .    from the existence proof above and lemma  [ lem : primal - dual - optimality ] , we know that ( i ) @xmath262 , ( ii ) the @xmath0-dimensional principal subspace of @xmath263 is spanned by @xmath264 and ( iii )  @xmath265 .    by the construction of @xmath147 ,",
    "part 3 of lemma  [ lem : fantop - projection ] implies that when @xmath266 we have @xmath267    as a consequence , @xmath261 is also an optimal primal  dual pair for the elastic net fps problem ( [ eq : fps - en ] ) when @xmath102 is in the range specified in ( [ eq : tau - range ] ) .",
    "now we prove uniqueness of @xmath147 as a solution to the fps problem ( [ eq : fps ] ) .",
    "assume that there is another solution @xmath268 such that @xmath269 but @xmath147 is the unique solution to the elastic net fps for @xmath270 small enough , we must have @xmath271 , and hence @xmath272 which is a contradiction ( the first inequality follows from that @xmath268 ) .",
    "the false negative control under condition ( [ eq : signal - strength-1 ] ) is obvious in view of the frobenius norm error bound ( [ eq : frobenius - error ] ) .",
    "now we prove false negative control under the entry - wise condition ( [ eq : entry - wise - min ] ) . according to theorem  [ thm : deterministic ]",
    ", we know that @xmath92 is supported on @xmath55 , and @xmath273 corresponds to the projector of the @xmath0-dimensional principal subspace of @xmath274 where @xmath149 is the optimal dual variable .",
    "then it is sufficient to show that the leading eigenvector of @xmath275 does not have zero entries .",
    "note that @xmath276 and the second part of assumption  ( [ eq : entry - wise - min ] ) implies that @xmath277 .    by the first part of assumption ( [ eq : entry - wise - min ] ) , we have @xmath278 , where @xmath279 .",
    "let @xmath280 be the @xmath157 diagonal matrix such that @xmath281 .",
    "the matrix @xmath282 has all positive entries , and hence by the perron ",
    "frobenius theorem , it has a unique leading eigenvector @xmath283 whose entries are all positive . as a result , the leading eigenvector of @xmath284 is @xmath285 , which does not have zero entries .",
    "[ lem : sparse - solution ] under the assumptions in theorem  [ thm : deterministic ] , let @xmath147 be the solution to the further constrained problem ( [ eq : sparse - solution ] ) .",
    "then @xmath147 is rank @xmath0 and unique .",
    "furthermore , there exist @xmath236 orthonormal matrices @xmath235 , @xmath234 such that :    1 .",
    "@xmath231 and @xmath264 span the @xmath0-dimensional principal subspaces of @xmath37 and @xmath233 , respectively .",
    "2 .   there exists a @xmath157 orthonormal matrix @xmath237 such that @xmath286    consider @xmath287 .",
    "we know that @xmath288 maximizes @xmath289 over all @xmath290 ( the trace-@xmath0 fantope of @xmath291 ) .",
    "we argue that @xmath288 is unique and has rank @xmath0 . by condition ( [ eq : deterministic - condition-1 ] ) and @xmath292 , we have @xmath293 , and hence @xmath294 . on the other hand , by the ( [ sps ] ) condition",
    "it is straightforward to verify that the leading @xmath0 eigenvectors of @xmath158 are those of @xmath37 confined on @xmath55 , and hence @xmath295 for all @xmath296 .",
    "furthermore , since @xmath158 is a principal submatrix of @xmath297 , we have @xmath298",
    ". therefore , @xmath299 by condition ( [ eq : deterministic - condition-2 ] ) .",
    "the first claim follows from part 1 of lemma  [ lem : fantop - projection ] .",
    "the second claim is trivial when @xmath300 .",
    "now we focus on the case @xmath301 . by the ( [ sps ] ) condition we know that the unique @xmath0-dimensional principal subspace of @xmath37 is spanned by @xmath302 where @xmath235 is a @xmath236 orthonormal matrix",
    ". then @xmath235 spans the @xmath0-dimensional principal subspace of @xmath158 .",
    "using the fact that @xmath303 , and applying proposition 2.2 in @xcite , we can choose the right rotations for the columns of @xmath234 and @xmath235 so that @xmath304 using lemma 4.2 of @xcite and cauchy  schwarz , we have @xmath305 the above two inequalities jointly imply that @xmath306    now let @xmath307 be an @xmath157 orthonormal matrix , and similarly @xmath308 .",
    "one can show that , using the same argument as above , @xmath309 and @xmath310 can be chosen such that @xmath311 let @xmath312 , then @xmath313 and @xmath314    [ lem : primal - dual - optimality ] under the assumptions of theorem  [ thm : deterministic ] , let @xmath315 be the optimal primal  dual pair of the additionally constrained fps problem ( [ eq : sparse - solution ] ) .",
    "let @xmath264 , @xmath316 , and @xmath237 be defined as in lemma  [ lem : sparse - solution ] .",
    "let @xmath263 be defined as in ( [ eq : sigma - tilde ] )",
    ". then @xmath317 and @xmath147 is the unique projector of the @xmath0-dimensional principal subspace of @xmath263 .",
    "we start from a decomposition of @xmath263 as follows : @xmath318 \\label{eq : magic - matrix } \\\\[-8pt ] \\nonumber & = & \\pmatrix{s_{jj}-\\rho\\tilde z_{jj } - q\\sigma_{jj } q^t & 0 \\cr 0&\\tilde w } + \\pmatrix { q\\sigma_{jj } q^t & q\\sigma_{jj^c } \\vspace*{3pt}\\cr \\sigma_{j^cj}q^t & \\sigma_{j^cj^c } } \\nonumber \\\\ & = & \\mbox{``noise''}+\\mbox{``signal.''}\\nonumber\\end{aligned}\\ ] ]    it can be directly verified that @xmath319 spans the @xmath320-principal subspace of @xmath321 moreover , ( [ eq : eigen - signal ] ) implies that the eigenvalues of the `` signal '' part in the decomposition ( [ eq : magic - matrix ] ) are the same as those of @xmath37 .    to sum up",
    ", we have so far shown that @xmath264 spans the @xmath0-dimensional principal subspace of the signal part , with spectral gap @xmath322 .",
    "next , we need to show that the @xmath0-dimensional principal subspace remains unchanged after adding the `` noise '' part .    first , the block - diagonal structure of the `` noise '' matrix in ( [ eq : magic - matrix ] ) ensures that @xmath323 spans one of its @xmath0-dimensional spectral subspace ( a @xmath0-dimensional spectral subspace of a @xmath38 symmetric matrix @xmath24 means that if @xmath16 is in this subspace , then @xmath324 is also in this subspace ) .",
    "second , we show that twice the operator norm of the `` noise '' part is smaller than the gap between @xmath0th and @xmath325th eigenvalues of @xmath326 , which is @xmath322 .",
    "in fact , the operator norm of the noise part does not exceed @xmath327 where the bound on @xmath328 comes from lemma  [ lem : sparse - solution ] .",
    "we also have @xmath329 , which is contained within the above bound .",
    "therefore , by standard perturbation theory such as weyl s inequality , the subspace spanned by @xmath323 is the @xmath0-dimensional principal subspace of @xmath263 as long as @xmath330 which means that twice the noise operator norm does not exceed the spectral gap in the signal part .",
    "when the inequality in ( [ eq : eigen - gap - final ] ) is strict , as stated in condition ( [ eq : deterministic - condition-2 ] ) , we know that the @xmath0-dimensional principal subspace of @xmath263 is unique .",
    "proof of lemma  [ lem : fantop - projection ] ( 1 ) see @xcite .",
    "( 2 ) is lemma  4.1 of @xcite .",
    "( 3 ) we have @xmath331 this is maximized over @xmath185 by @xmath332 .",
    "note that by assumption @xmath333 and @xmath334 .",
    "then the claim follows by applying ( 1 ) and ( 2 ) .",
    "proof of theorem  [ thm : agnostic - fps ] let @xmath335 be any solution of @xmath336 then @xmath337 , and ( [ eq : fps - constraint ] ) implies @xmath338 .",
    "combining these two inequalities with the hlder and triangle inequalities yields @xmath339 finally , invoke ( 2 ) to complete the proof .",
    "we thank the editors and referees for their helpful comments ."
  ],
  "abstract_text": [
    "<S> the presence of a sparse `` truth '' has been a constant assumption in the theoretical analysis of sparse pca and is often implicit in its methodological development . </S>",
    "<S> this naturally raises questions about the properties of sparse pca methods and how they depend on the assumption of sparsity . under what conditions </S>",
    "<S> can the relevant variables be selected consistently if the truth is assumed to be sparse ? </S>",
    "<S> what can be said about the results of sparse pca without assuming a sparse and unique truth ? </S>",
    "<S> we answer these questions by investigating the properties of the recently proposed fantope projection and selection ( fps ) method in the high - dimensional setting . </S>",
    "<S> our results provide general sufficient conditions for sparsistency of the fps estimator . </S>",
    "<S> these conditions are weak and can hold in situations where other estimators are known to fail . on the other hand , without assuming sparsity or identifiability , we show that fps provides a sparse , linear dimension - reducing transformation that is close to the best possible in terms of maximizing the predictive covariance . </S>"
  ]
}