{
  "article_text": [
    "neural networks ( anns ) are complex networks imitating the way human cerebral neurons process information to realize parallel information transformation and processing .",
    "anns have been widely employed to solve real life problems related to classification , function approximation , data processing , and robotics .",
    "the training algorithm used to determine various parameters of ann is one of the key factors that influence the performance of anns . among all training algorithms , backpropagation ( bp ) has been widely used , but it suffers from the problem that it is easy to get stuck in the local optima , and its low convergence speed @xcite@xcite . with the advancement of evolutionary algorithms ( eas ) , they are employed to train anns . moreover",
    ", an ea can simultaneously optimize the weights of an ann and it can also evolve the structure of the network so as to achieve desirable performance @xcite .    with different levels of ea involvement , ea - based anns can be classified into two major types :  noninvasive \" and  invasive \" .",
    "the former refers to those methods using ea to evolve network structure in conjunction with bp for weight adaptation .",
    "the latter includes those using ea for both evolving network structure and weight adaptation @xcite . due to the tradition of employing bp for network weight training , ",
    "noninvasive \" methods have been widely studied and many algorithms have been developed with outstanding performance @xcite@xcite . since they rely heavily on bp , they still suffer from the problems of getting stuck in local optima and low convergence speed @xcite .",
    "the  invasive \" methods , however , merely depend on ea for evolving the network .",
    "thus the computation speed is higher than  noninvasive \" methods since the  invasive \" methods can avoid bp fitness evaluation with direct representation of networks . in this paper",
    ", we propose a new algorithm based on chemical reaction optimization ( cro ) @xcite to evolve the network structure and to tune the weights of networks .",
    "cro is a novel chemical reaction - inspired general purpose optimization algorithm .",
    "it is a variable population - based metaheuristics , mimicking the transition of molecules and intermolecular interactions in a chemical reaction .",
    "the transitions and interactions tend to direct molecules toward the lowest potential energy states on the potential energy surface ( pes ) .",
    "thus cro uses the idea of mimicking the objective function landscape with pes and molecules can explore the solution space to find the global optimum due to this tendency .",
    "cro has been proved to be effective in solving many practical problems @xcite@xcite@xcite and simulation results show that anns trained by cro outperform other eas in many classification problems .",
    "the rest of the paper is organized as follows . in section",
    "ii , we briefly present the related work of using ea to train anns . in section iii ,",
    "the problem formulation is presented . the detailed framework and the algorithm based on cro",
    "is given in section iv .",
    "section v presents the simulation results comparing cro - based anns ( croann ) with other anns .",
    "finally we conclude the paper in section vi .",
    "using ea to train anns has become an active research topic .",
    "many eas , e.g. genetic algorithm ( ga ) @xcite , simulated annealing ( sa ) @xcite , and particle swarm optimization ( pso ) @xcite have been used .",
    "yet relatively few  invasive \" methods have been studied to achieve the best performance of ea - based neural networks .",
    "used tabu search ( ts ) for neural network training @xcite , where ts was used to train a fixed neural network with six hidden layer neurons .",
    "the ts solution is given in the form of vectors representing all the weights of the network .",
    "the testing data set was a collection of randomly generated two - dimensional points @xmath0 where @xmath1 $ ] and @xmath2 $ ] .",
    "the output data set was generated by simple mathematical functions .",
    "the result demonstrated that ts - based networks could outperform conventional bp - derived networks .",
    "sa and ga were also implemented for the same data set @xcite .",
    "angeline _ et al .",
    "_ proposed generalized acquisition of recurrent links ( gnarl ) using hybrid - ga to train anns @xcite . instead of using symmetric topology ,",
    "gnarl employs sparse connections of neural networks to represent the network structures .",
    "gnarl uses a mutation operation to evolve the structure and to tune the weights of networks .",
    "gnarl reserves the top 50% individuals in each generation , according to the user - defined fitness function , and performs reproduction by two types of mutation methods : parametric mutation and structural mutation .",
    "the former mutation method changes the network by perturbing the weights with gaussian noise controlled by an annealing temperature @xcite , while the latter mutation method involves the addition or deletion of hidden layer nodes or links .    a constructive algorithm for training cooperative neural network ensembles ( cnne ) , proposed by m. islam _",
    "@xcite , uses a constructive algorithm to evolve neural networks .",
    "cnne relies on the contribution of individuals in the population and uses incremental learning to maintain the diversity among individuals in an ensemble .",
    "incremental learning based on negative correlation could effectively reduce the redundancy generated by individuals searching the same solution space and thus different individuals could learn different aspect of the training data , which could result in a final solution of the ensemble .",
    "cnne is a  noninvasive \" method which relies on proper implementation of bp .",
    "though cnne minimizes optimization problems by utilization of ensembles , it suffers from the  structural climbing problem \" @xcite .",
    "proposed a group search optimizer - based ann ( gsoann ) @xcite , which uses group search optimizer , a population - based optimization algorithm inspired by animal social foraging behavior , to train the networks with least - squared error function as the fitness function .",
    "paulito p. palmes _ et al .",
    "_ proposed mutation - based genetic neural network ( mgnn ) employing a specially designed mutation strategy to perturb the chromosomes representing neural networks @xcite .",
    "mgnn is very similar to gnarl except that it implements selection , encoding , mutation , fitness function , and stopping criteria differently .",
    "mgnn s encoding scheme contributes to a flexible formulation of fitness function and mutation strategy of local adaptation of evolutionary programming , and it implements a stopping criteria using  sliding window \" to track the state of overfitness .",
    "in this paper , we consider the problem of single - hidden - layer feedforward neural network ( slfn ) design for data classification .",
    "we use a topological structure , activation functions of nodes , and connection weights to describe an slfn .",
    "we use @xmath3 $ ] to distinguish the different layers , where layer 0 to 2 are the input layer , the hidden layer , and the output layer , respectively .",
    "@xmath4 stands for the number of neurons in layer @xmath5 .",
    "@xmath6 represents the weight of the connection between the @xmath7 neuron in layer @xmath8 to the @xmath9 neuron in layer @xmath10 .",
    "@xmath11 stands for the bias of the @xmath7 neurons in layer @xmath10 .",
    "1 depicts an example of slfn . in the problem",
    "dataset @xmath12 with @xmath13 samples , the @xmath14 sample @xmath15 is composed of a pattern @xmath16 and the corresponding desired output @xmath17 , where @xmath18 .",
    "thus we use @xmath19 to describe the @xmath20 element of the @xmath14 pattern , and @xmath21 to denote the @xmath9 element of the @xmath14 desired outputs in the dataset . with a given slfn and a pattern @xmath16",
    ", the computed result @xmath22 can be obtained from the following formula @xmath23 where @xmath24 and @xmath25 are the activation functions for hidden layer neurons and output neurons , respectively . for a trained slfn , we can make use of the difference between @xmath17 and @xmath26 to evaluate the performance of the network .        the primary function to evaluate the performance of a conventional bp network is the mean squared error ( mse ) between @xmath17 and @xmath26 .",
    "a small mse means that the performance is good and the network is well - trained .",
    "however , in order to concentrate more on the accuracy of the classification result , here we adopt a new fitness function @xmath27 ( [ ffit ] ) consisting of the normalized mean squared error ( nmse ) @xmath28 ( [ fnmse ] ) and the classification percentage error @xmath29 ( [ fpercent ] ) . their formulas are given as follows : @xmath30 @xmath31 @xmath32 in ( [ ffit ] ) , @xmath33 and @xmath34 are user - defined parameters to balance the weighting on ( [ fnmse ] ) and ( [ fpercent ] ) in the ultimate fitness function and should be set to a small real value between 0 and 1 .",
    "for instance , implementations stressing classification correctness can set @xmath35 and @xmath36 . in this paper",
    ", we also use this setting in the simulation .    for croann",
    ", we divide the samples in a dataset into a training set , a validation set , and a testing set .",
    "we first determine the best \\{@xmath37 } quadlet with the training set according to ( [ ffit ] ) .",
    "then we use the validation set to detect and avoid overfitness ( see section iv.d ) .",
    "since anns should have the ability to process unfamiliar data , we use the testing set to evaluate the accuracy of classification and compare the results with other algorithms .",
    "in this section , we first discuss how cro works .",
    "then we introduce the encoding scheme and operators employed to train the structure and weights of networks for croann .",
    "finally the stopping criteria is given .",
    "cro is a population - based metaheuristic inspired by chemical reactions , mimicking the process of reactions where molecules collide with the walls of the container and with each other . in the process , molecules attempt to reach the stable state .",
    "imagine we put a certain number of molecules in a closed container . at the start of reaction",
    ", molecules with excess energy are unstable .",
    "since there is a natural tendency for a reacting system to stay in a stable state , molecules change their energy state from high to low through a sequence of elementary reactions .",
    "when the reaction stops , we can get molecules with the minimum stable state of energy .",
    "if we consider different energy states as an energy surface , the transition and interaction of molecules can result in a gradual rolling down process on pes and the lowest point is the minimum stable state of energy .",
    "we call the initial molecules  reactants \" and the final  products \" .    in cro , each molecule has a molecular structure , representing a solution of the problem , and two kinds of energy , i.e. potential energy ( pe ) and kinetic energy ( ke ) .",
    "pe stands for the fitness function value and ke describes the tolerance of a molecule to an increase of its energy state .",
    "suppose @xmath38 and @xmath39 are a molecular structure and the fitness function , respectively , then we compute its pe with @xmath40 .",
    "if @xmath41 is the new structure derived from @xmath38 in an elementary reaction , then @xmath42 has to be satisfied . otherwise , the reaction is invalid and the new structure should be rejected .",
    "in other words , ke represents the ability for a molecule to escape from a local minimum .",
    "this rule can also be easily applied to intermolecular elementary reactions and changes may be more vigorous since more ke can be transformed into pe . a central energy buffer is set up for energy conservation and convergence .",
    "we define four types of elementary reactions for cro , namely , on - wall ineffective collision , decomposition , inter - molecular ineffective collision , and synthesis .",
    "these four elementary reactions are defined to cover all possible reactions under the framework of cro .",
    "these four types can be classified into two classes : uni - molecular reactions include the first two types and inter - molecular reactions include the latter two .",
    "a uni - molecular reaction can be triggered when a single molecule collides on a wall of the container .",
    "an inter - molecular reaction happens when two or more molecules collide with each other ( for simplicity , only two molecules are considered in this class of reactions ) .",
    "interested readers can refer to @xcite for the pseudocode of cro .      to accelerate the simulation and to reduce the difficulty in programming , we use two matrices and two vectors to represent different weights and thresholds .",
    "this scheme is similar to that described in @xcite with one key difference . in @xcite",
    ", there is an extra element in each solution which controls the perturbation strength , but we abandon it since cro uses one constant parameter to control the gaussian perturbation .",
    "we call the complete collection of these matrices and vectors a  solution structure \" .",
    "every molecule has a solution structure representing the network structure and determining the current energy state the molecule is at .",
    "this operator is designed to generate a new structure of networks randomly .",
    "each call will generate a new solution structure .",
    "it is achieved by first assigning random numbers to all elements and then scaling them to [ -1.0 , 1.0 ] linearly .",
    "its pseudocode is given in algorithm 1 below :    randomly generate a real number @xmath43 @xmath44 @xmath45      this operator is designed to generate a new solution @xmath46 from a given solution @xmath38 .",
    "its main purpose is to perform a local search for better solutions .",
    "it is done by perturbing one random element in the matrices or vectors in @xmath38 with gaussian perturbation , whose mean is the original number and variation is a user - defined value . its pseudocode is given in algorithm 2 where @xmath47 stands for gaussian perturbation function and @xmath48 is a user - defined variance .",
    "generate a random integer @xmath15 smaller than the total number of elements in a solution find the @xmath14 element @xmath49 in @xmath50 @xmath51      this operator is used to generate two different solutions @xmath52 and @xmath53 based on a given solution @xmath38 .",
    "this operator can help molecules jump out of local minimums by performing severe perturbation on the solution .",
    "it is done by perturbing every element in the matrices and vectors in @xmath38 with gaussian perturbation probabilistically , say 50% .",
    "if , though unlikely to happen , nothing is changed during the first stage of the perturbation , this solution will be perturbed by the neighbor generator function .",
    "its pseudocode is given in algorithm 3 and the variables are as defined in the previous section .",
    "@xmath54 copy @xmath50 to @xmath55 and @xmath56 generate a real @xmath57 between 0 and 1 @xmath51 @xmath58 @xmath59the original @xmath55 or @xmath60      this operator is used to generate a new solution @xmath46 based on two given solutions @xmath61 and @xmath62 .",
    "it is done by randomly choosing elements from both solutions with equal possibilities to form a new solution .",
    "its pseudocode is given in algorithm 4 .",
    "generate a real @xmath57 between 0 and 1 @xmath63counterpart in @xmath64 @xmath63counterpart in @xmath65      we introduce two stopping criteria to croann : maximum function evaluations ( fe ) and overfitness detection .",
    "the maximum fe criterion is a hard limit of croann and no simulation could evaluate the fitness function more than this threshold .",
    "the design of the other stopping criterion , overfitness detection , is based on the observation that good performance with the training samples may not necessarily result in a good performance with the testing samples .",
    "poor overall performance might be obtained due to over - training the system in the training phase . to address this problem",
    ", we employ a  sliding window \" to monitor the presence of overfitness in the network .",
    "croann measures the validation performance of the current best network @xmath66 and compares this performance with the previous best validation performance using @xmath67 at the end of @xmath14 window .",
    "if the previous validation performance is better , then we say this network is  overfitting \" and add 1 to the overfitness counter .",
    "when this overfitness counter reaches a user - defined threshold , croann will terminate .",
    "however , once @xmath68 is smaller than @xmath69 , the overfitness counter is reset to zero .",
    "the pseudocode describing the stopping criteria is given in algorithm 5 below :    croann stop calculate the validation performance @xmath70",
    "@xmath71 @xmath72 store @xmath73 @xmath74 restore the saved best network croann stop",
    "in order to evaluate the performance of croann , the simulation is implemented with c++ and tested with some famous classification datasets from the uci repository @xcite : _ iris _ classification dataset , _",
    "wisconsin breast cancer _ classification dataset and _ pima indians diabetes _ dataset .",
    "they are all derived from real - world problems .",
    "the iris dataset is a standard benchmark for evaluating the performance of anns and has been tested by many neural network algorithms , including some ea - based anns algorithm .",
    "the latter two datasets are used to test the ability of croann to recover from polluted data @xcite .",
    "the datasets are partitioned based on the suggestion given by prechelt @xcite .",
    "each of them is separated into three classes : training class , validation class and testing class with the ratio of 2:1:1 using the simple random sampling method .",
    "first , several tests are conducted to evaluate the impact of changes in different cro parameters , using the _ iris _ classification dataset as benchmark .",
    "then croann is compared with other ea - based ann algorithms proposed in the recent literature to evaluate the performance .",
    "the ratio of occurrence for different elementary reactions of cro and the initial size of energy buffer have direct impact on the final performance of the neural network .",
    "so it is essential to analyze their impact in order to adjust them for later use .",
    "the first experiment includes a set of tests on different cro parameter values , based on the _ iris _ dataset , a benchmark test for machine learning and pattern recognition .",
    "when investigating one parameter , other parameters are set constant .",
    "results of each test are generated by averaging the error rate of testing set in 50 trials .",
    "the analysis results are shown in fig .",
    "results show that a gaussian perturbation variance which is too large makes cro scan through the solution space in a relatively large scale but can not explore small regions carefully while a variance which is too small is likely to result in getting stuck in local optima .",
    "similarly , a small population can not let the molecules fully explore the solution space , while a large population will reduce the possibility of reaction occurrence of individual molecules .",
    "initial energy buffer size , initial molecular kinetic energy , and kinetic energy loss rate control the overall energy in the whole system .",
    "they also cooperate with molecular collision rate , thresholds for decomposition and synthesis to control the ratio of occurrence for different elementary reactions of cro to a proper value .",
    ".croann parameters [ cols= \" > , < , < , < \" , ]     the wisconsin breast cancer dataset contains 699 samples , each of which has real - valued attributes and can be classified into two classes : 458 _ benign _ and 241 _ malignant_. to test the performance of croann , all samples are divided into three parts by simple random sampling method : 349 training samples , 175 validation samples and 175 testing samples .",
    "results from croann and six other anns are listed in table iv .",
    "croann performs superior to sgaann , epann , psoann , gsoann , and mgnn , and it has a similar performance with esann . as compared with gsoann",
    ", croann can give a better standard derivation .",
    "there are also comparisons with recent published results listed in table v. we also compare croann with other machine learning algorithms and the results are given in table v. we can see that croann performs very well and shares the best performance with ganet - best .",
    "the pima indian diabetes dataset contains 768 samples , 500 of which are indicated with sign of diabetes and 268 are without such sign .",
    "there are eight real - valued attributes that can be used to determine whether a patient has the sign of diabetes or not .",
    "this dataset is known as a difficult problem for machine learning for its scarcity of samples and heavy noise pollution .",
    "this dataset is partitioned into 384 training samples , 192 validation samples and the 192 testing samples .",
    "table vi shows the comparison between croann and five other ea - based anns ( mgnn is not included because there is no simulation data provided for this dataset in @xcite ) .",
    "croann again outperforms the rest in terms of the testing set mean error .",
    "results from other machine learning algorithms are tabulated in table vii , which demonstrates that croann achieves a performance that is comparable with the best , i.e. cnne , and greatly outperforms the others .",
    "in this paper , we propose a novel ea - based anns called croann , to train anns , based on cro .",
    "we have shown that croann can optimize the structure as well as the weights of anns simultaneously using stochastic processes . in croann , the structure and weights of a network",
    "is encapsulated in one solution , which is considered as a point in the solution space . in this way",
    ", croann searches the global minimum , which represents the network configuration providing the best performance .",
    "since there are no restrictions on the evolution of the network structure and on the weight adaptation , croann does not suffer from the `` structural hill - climbing '' problem observed in the constructive and pruning approaches of ann @xcite .",
    "simulation results show that croann can outperform other existing ea - based ann algorithms . in the iris dataset and the pima indian diabetes dataset",
    ", croann provides the best testing error rate among all representative ea - based anns .",
    "although croann is not the best in the test with the wisconsin breast cancer dataset , the gap between the result generated by croann and the best one is indeed very small . in the comparisons with other sophisticated machine learning algorithms on the three classification problems",
    ", croann can always provide the best performance .",
    "to summarize , cro is well suited to be incorporated in ann to solve classification problems . in the future",
    ", we will conduct a systematic analysis of variance on the parameters and perform student s t - test to show significance of the results .",
    "moreover , we can further explore the ability of croann to solve some real - world classification problems , and other types of problems including function approximation , data processing , and robotics .",
    "this work is supported in part by the strategic research theme of information technology of the university of hong kong .",
    "lam is also supported in part by the croucher foundation research fellowship .",
    "f. leung , h. lam , s. ling , and p. tam ,  tuning of the structure and parameters of a neural network using an improved genetic algorithm , \" _ ieee trans .",
    "neural netw .",
    "79 - 88 , jan .",
    "2003          a.y.s .",
    "lam , j. xu , and v.o.k .",
    "chemical reaction optimization for the population transition problem in peer - to - peer live streaming , \" in _ proc .",
    "_ , barcelona , spain , pp .",
    "1429 - 1436 , july 2010 .",
    "tsai , j.h .",
    "chou , and t.k .",
    "liu ,  tuning the structure and parameters of a neural network by using hybrid taguchi - genetic algorithm , \" _ ieee trans .",
    "neural netw .",
    "69 - 80 , jan . 2006 .",
    "sexton , r.e .",
    "dorsey , and j.d .",
    "johnson ,  optimization of neural networks : a comparative analysis of the genetic algorithm and simulated annealing , \" _ eur .",
    "589 - 601 , 1999 .",
    "fogel , a.j .",
    "owens , and m.j .",
    "walsh ,  artificial intelligence through a simulation of evolution , \" in _ proc .",
    "2nd cybern .",
    "biophysics cybern .",
    "_ , washington : spartan books , 1965 , pp . 131 - 155 .",
    "e. cantu - paz and c. kamath ,  an empirical comparison of combinations of evolutionary algorithms and neural networks for classification problems , \" _ ieee trans .",
    "man , cybern .- part b : cybern .",
    "915 - 927 , oct . 2005 .",
    "n. garcia - pedrajas , c. hervas - martinez , and d. ortiz - boyer ,  cooperative coevolution of artificial neural network ensembles for pattern classification , \" _ ieee trans .",
    "_ , vol . 9 , no .",
    "3 , pp . 271 - 302 , jun .",
    "t. g. dirtterich ,  an experimental comparison of three methods for constructing ensembles of decision trees : bagging , boosting , and randomization , \" _ mach . learning _",
    "139 - 157 , 2000 ."
  ],
  "abstract_text": [
    "<S> evolutionary algorithms ( eas ) are very popular tools to design and evolve artificial neural networks ( anns ) , especially to train them . </S>",
    "<S> these methods have advantages over the conventional backpropagation ( bp ) method because of their low computational requirement when searching in a large solution space . in this paper , we employ chemical reaction optimization ( cro ) , a newly developed global optimization method , to replace bp in training neural networks . </S>",
    "<S> cro is a population - based metaheuristics mimicking the transition of molecules and their interactions in a chemical reaction . </S>",
    "<S> simulation results show that cro outperforms many ea strategies commonly used to train neural networks .    </S>",
    "<S> artificial neural networks , evolutionary algorithm , chemical reaction optimization . </S>"
  ]
}