{
  "article_text": [
    "distributed coordination is of particular interest in many engineering systems .",
    "two examples are distributed overlay routing or network formation @xcite and medium access control @xcite in wireless communications . in either case ,",
    "nodes need to utilize their resources _ efficiently _ so that a desirable global objective is achieved .",
    "for example , in network formation , nodes need to choose their immediate links so that connectivity is achieved with a minimum possible communication cost , i.e. , minimum number of links .",
    "similarly , in medium access control , users need to establish a _",
    "fair _ scheduling of accessing a shared communication channel so that collisions ( i.e. , situations at which two or more users access the common resource ) are avoided . in these scenarios , achieving _ coordination _ in a distributed and adaptive fashion to an _ efficient _ outcome is of special interest .",
    "the distributed yet coupled nature of these problems , combined with a desire for online adaptation , motivates using models based on game theoretic learning @xcite . in game theoretic learning ,",
    "each agent is endowed with a set of actions and a utility / reward function that depends on that agent s and other agents actions .",
    "agents then learn which action to play based only on their own previous experience of the game ( actions played and utilities received ) .",
    "a major challenge in this setting is that explicit utility function optimization may be impractical .",
    "this may be due to inherent complexity ( e.g. , a large number of players or actions ) , or the lack of any closed form expression for the utility function .",
    "rather , rewards can be measured online . in terms of game",
    "theoretic learning , this eliminates adaptation based on an ability to compute a `` best reply '' .",
    "another obstacle to utility maximization is that from any agent s perspective , the environment includes other adapting agents , and hence is nonstationary .",
    "consequently , actions that may have been effective in the past need not continue to be effective .",
    "motivated by these issues , this paper considers a form of distributed learning dynamics known as _ aspiration learning _ , where agents  satisfice \" rather than  optimize \" .",
    "the aspiration learning scheme is based on a simple principle of  win - stay , lose - shift \" @xcite , according to which a successful action is repeated while an unsuccessful action is dropped .",
    "the success of an action is determined by a simple comparison test of its performance with the player s desirable return ( _ aspiration level _ ) .",
    "the aspiration level is updated to incorporate prior experience into the agent s success criterion . through this learning scheme ,",
    "agents _ learn _ to play their  best \" action .",
    "the history of aspiration learning schemes starts with the pioneering work of @xcite , where satisfaction seeking behavior was used to explain social decision making .",
    "a simple aspiration learning model is presented in @xcite , where games of two players and two actions are considered , and decisions are taken based on the `` win - stay , lose - shift '' rule . in the special case of two - player / two - action _ mutual interest games _ and _ symmetric coordination games _ , respectively , references , @xcite and @xcite show that the payoff - dominant action profile is selected with probability close to one .",
    "similar are the results in @xcite .",
    "however , contrary to @xcite and @xcite , both models incorporate a small perturbation either in the aspiration update @xcite or in the action update @xcite .",
    "recent research efforts on equilibrium selection in games have focused on achieving distributed convergence to _ pareto - efficient _ payoff profiles , i.e. , payoff profiles at which no action change can make a player better off while not making some other player worse off .",
    "for example , reference @xcite introduced an aspiration learning algorithm that converges ( in distribution ) to action profiles that maximize social welfare in multiple player games .",
    "some key characteristics of this algorithm is that agents keep track of their most recent satisfactory action and satisfactory payoff ( benchmark action and payoff ) , and they update their actions by following a `` win - stay lose - shift '' rule , where the aspiration level is defined as the benchmark payoff .",
    "convergence to the pareto - efficient payoffs in two player games also has been investigated by @xcite .",
    "the learning algorithm considered in @xcite has two distinctive features : a ) agents commit on playing a series of actions for a @xmath0-period interval , and b ) agents make decisions according to a `` win - stay lose - shift '' rule , where aspiration levels are computed as the running average payoff over all the previous @xmath0-period intervals .",
    "it is shown that , in two player games , the agents payoffs converge to a small neighborhood of the set of the pareto - efficient payoffs almost surely if @xmath0 is sufficiently large .    in this paper",
    ", we also focus on achieving convergence to efficient payoff profiles ( also part of the pareto - efficient payoff profiles ) in coordination games of large number of players and actions .",
    "agents apply an aspiration learning scheme that is motivated by @xcite .",
    "our goal is to a ) characterize explicitly the asymptotic behavior of the process for generic games of multiple players and actions , and b ) derive conditions under which _ efficient _ payoffs are selected in large coordination games .",
    "our main contribution is the characterization of the asymptotic behavior of the induced markov chain by means of the invariant distributions of an equivalent finite - state markov chain , whenever the experimentation probability becomes sufficiently small .",
    "this equivalence simplifies the analysis of what would otherwise be an infinite state markov process .",
    "these results extend prior analysis on this type of aspiration learning schemes to games of multiple players and actions .",
    "we also specialize the results for a class of games that is a generalized version of so - called _",
    "coordination games_. in particular , we show that , in these games , the unique invariant distribution of the equivalent finite - state markov chain puts arbitrarily large weight on the _ payoff - dominant _ action profiles if the step size of the aspiration - level update becomes sufficiently small .",
    "we finally demonstrate the utility of the learning scheme to network formation games , which is of independent interest , since prior learning schemes on network formation are primarily based on best - response dynamics , e.g. , @xcite .    while convergence to payoff - dominant action profiles in coordination games is desirable , another desirable property is a notion of fairness . in particular , for some coordination games where coincidence of interests is not so strong , such as the _ battle of the sexes _",
    ", ( * ? ? ?",
    "* section  2.3 ) ) , convergence to a single action profile might not be _ fair _ for all agents that would probably rather be in a different action profile . instead",
    ", an alternation between several action profiles might be more desirable , usually described through distributions in the _ joint _ action space .",
    "an example of a class of such coordination games is so - called common - pool games , where multiple users need to coordinate on utilizing a limited common resource .",
    "the proposed aspiration learning algorithm also may provide a distributed and adaptive approach for convergence to fair outcomes in such symmetric coordination games , such as common - pool games .",
    "this property is of independent interest , since it is relevant to several scenarios of distributed resource allocation , such as medium access control in wireless communications @xcite .    in comparison to prior and other current work ,",
    "this paper develops ( and corrects ) the specific model of aspiration learning in @xcite beyond two player games .",
    "the paper goes on to derive specialized results for coordination games involving convergence to efficient action profiles and fairness in symmetric games .",
    "the results in @xcite use a simpler finite state model of aspiration learning and are applicable to almost all games .",
    "the results in @xcite establish convergence to efficient action profiles , but as yet do not specify selection / fairness among these action profiles .",
    "the model of @xcite is more closely related to the present model , but with a different definition of aspiration levels and a different mechanism to perturb aspirations .",
    "the results of convergence to efficiency in @xcite extend beyond coordination games while requiring two player games and do not specify fairness / selection among efficient profiles .",
    "the remainder of the paper is organized as follows .",
    "section  [ s2 ] defines coordination games and presents two special cases of coordination games , namely network formation and common - pool games .",
    "section  [ s3 ] presents the aspiration learning algorithm and its convergence properties in games of multiple players and actions .",
    "section  [ s4 ] specializes the convergence analysis to coordination games and establishes convergence to efficient outcomes .",
    "it also demonstrates the results through simulations in network formation games .",
    "section  [ s5 ] extends the convergence analysis to symmetric coordination games and establishes conditions under which convergence to fair outcomes is also established .",
    "finally , section  [ s6 ] presents concluding remarks .    _",
    "terminology : _ we consider the standard setup of finite strategic - form games .",
    "there is a finite set of _ agents _ or _ players _ , @xmath1 , and each agent has a finite set of actions , denoted by @xmath2 .",
    "the set of action profiles is the cartesian product @xmath3 ; @xmath4 denotes an _ action _ of agent @xmath5 ; and @xmath6 denotes the _ action profile _ or _ joint action _ of all agents .",
    "the _ payoff / utility function _ of player @xmath5 is a mapping @xmath7 .",
    "a strategic - form game , denoted @xmath8 , consists of the sets @xmath9 , @xmath10 and the preference relation induced by the utility functions @xmath11",
    ", @xmath12 .",
    "an action profile @xmath13 is a _ ( pure ) nash equilibrium _ if @xmath14 for all @xmath15 and @xmath16 , where @xmath17 denotes the complementary set @xmath18 .",
    "we denote the set of pure nash equilibria by @xmath19 . in case",
    "the inequality is strict , the nash equilibrium is called a _ strict nash equilibrium_. for the remainder of the paper , the term  nash equilibrium \" always refers to a  pure nash equilibrium . \"",
    "before defining coordination games , we first need to define the notion of _ better reply _ :    the better reply of agent @xmath15 to an action profile @xmath20 is a set valued map @xmath21 such that for any @xmath22 we have @xmath23 .    a _ coordination game _",
    "is defined as follows :    [ d2.2 ] a game of two or more agents is a coordination game if there exists @xmath24 such that the following conditions are satisfied :    1 .   for any @xmath25 and @xmath26 , @xmath27 i.e. , @xmath28 payoff - dominates @xmath29 ; 2 .   for any @xmath30 , there exist @xmath15 and action @xmath31 such that @xmath32 3 .   for any @xmath33 ( if non - empty ) , there exist an action profile @xmath34 and a sequence of distinct agents @xmath35 , such that @xmath36 for all @xmath37 , @xmath38 .",
    "a _ strict coordination game _ refers to a coordination game with the inequality being strict .",
    "the conditions of a coordination game establish a weak form of `` coincidence of interests '' and define a larger class of games than the ones traditionally considered as coordination games , e.g. , @xcite .",
    "for example , according to @xcite , one of the conditions that a coordination game needs to satisfy is that payoff differences among players at any action profile are much smaller than payoff differences among different action profiles .",
    "this condition reflects a form of coincidence of interests .",
    "definition  [ d2.2 ]  ( b ) also establishes a similar form of coincidence of interests , but weaker in the sense that it holds for at least one direction of action change .",
    "note also that existence of nash equilibria is not necessary for a game to be a coordination game .",
    "furthermore , if @xmath39 , then definition  [ d2.2 ] can be written solely with respect to the desirable set of profiles @xmath28 . in that case ,",
    "definition  [ d2.2 ]  ( c ) becomes vacuous since @xmath40 .",
    "a trivial example of a coordination game is the stag - hunt game of table  [ tb : shg ] .",
    "22 & a & b + a & @xmath41 & @xmath42 + b & @xmath43 & @xmath44 +     +    first , there exists a payoff - dominant profile , namely @xmath45 , that can be identified as the desirable set @xmath28 , and satisfies definition  [ d2.2 ]  ( a ) .",
    "also , from any action profile outside @xmath46 , namely @xmath47 or @xmath48 , there is a better reply that improves the payoff for all agents ( i.e. , definition  [ d2.2 ]  ( b ) holds ) .",
    "lastly , for any nash equilibrium profile outside @xmath28 , i.e. , @xmath49 , there is a player ( row or column ) and an action which makes everyone worse off ( i.e. , definition  [ d2.2 ]  ( c ) holds ) .",
    "thus , the stag - hunt game satisfies all the conditions of definition  [ d2.2 ] .",
    "note finally that in some games , there might be multiple choices for the selection of the desirable set @xmath28 .",
    "for example , in the stag - hunt game of table  [ tb : shg ] , an alternative selection of @xmath28 corresponds to the union of the action profiles @xmath45 and @xmath49 . in that case , both properties  ( a ) and ( b ) of definition  [ d2.2 ] hold , while property  ( c ) is vacuous . in other words ,",
    "the stag - hunt game is also a coordination game with respect to the new selection of the desirable set @xmath50 .",
    "[ cl2.1 ] in any coordination game and for any action profile @xmath51 there exists a sequence of action profiles @xmath52 , such that @xmath53 and @xmath54 for some @xmath5 , terminates at an action profile in @xmath55 .    by definition  [ d2.2 ]  ( b )",
    "there exists an agent @xmath15 and an action @xmath56 , such that @xmath57 and @xmath58 for all @xmath59 define @xmath60 . unless @xmath61 , we can repeat the same argument to generate an action profile @xmath62 and so on .",
    "thus , we construct a sequence @xmath63 along which the map @xmath64 is strictly monotone .",
    "however , since @xmath65 is finite , the sequence must necessarily terminate at some @xmath66 for @xmath67 .",
    "note that when @xmath68 , then a direct consequence of claim  [ cl2.1 ] is that coordination games are weakly acyclic games ( cf .",
    ",  @xcite ) .",
    "network formation games are of particular interest in wireless communications due to their utility in modeling distributed topology control @xcite and overlay routing @xcite .",
    "recent developments in distributed learning dynamics , e.g. , @xcite , have also provided the tools for computing efficient solutions for these games in a distributed manner .    to illustrate how a network formation game can be modeled as a coordination game",
    ", we introduce a simple network formation game motivated by @xcite .",
    "let us consider @xmath69 nodes deployed on the plane and assume that the set of actions of each agent @xmath5 , @xmath2 , contains all possible combinations of neighbors of @xmath5 , denoted @xmath70 , with which a link can be established , i.e. , @xmath71 .",
    "links are considered unidirectional , and a link established by node @xmath5 with node @xmath72 , denoted @xmath73 , starts at @xmath72 with the arrowhead pointing to @xmath5 .",
    "a _ graph _",
    "@xmath74 is defined as a collection of nodes and directed links .",
    "define also a _ path _ from @xmath72 to @xmath5 as a sequence of nodes and directed links that starts at @xmath72 and ends to @xmath5 following the orientation of the graph , i.e. , @xmath75 for some positive integer @xmath76 . in",
    "a _ connected _ graph , there is a path from any node to any other node .",
    "let us consider the utility function @xmath7 , @xmath15 , defined by @xmath77 where @xmath78 denotes the number of links corresponding to @xmath79 and @xmath80 is a constant in @xmath81 .",
    "also , @xmath82 where @xmath83 denotes the graph induced by joint action @xmath84 .",
    "the resulting nash equilibria are usually called _ nash networks _ @xcite . as it was shown in proposition  4.2 in @xcite , a network",
    "@xmath85 is a nash network if and only if it is _ critically connected _ ,",
    "i.e. , i ) it is connected , and ii ) for any @xmath86 , @xmath87 is the unique path from @xmath72 to @xmath5 .",
    "for example , the resulting nash networks for @xmath88 agents and unconstrained neighborhoods are shown in  [ fig : nn ] .",
    "agents and @xmath89.,width=211 ]    let us define @xmath28 to be the following set of action profiles @xmath90 which corresponds to the set of _ payoff - dominant _ networks . note that payoff - dominant networks ( if they exist ) are connected with minimum number of links . also , _ not _ all nash networks are necessarily payoff - dominant .",
    "for example , in  [ fig : nn](a ) , assuming that @xmath89 , all players realize the same utility , which is equal to @xmath91 .",
    "this is a strict nash network since each agent can only be worse off by unilaterally changing its links .",
    "it is also the payoff - dominant network . on the other hand ,  [ fig : nn](b )",
    "is a non - strict nash network and is payoff - dominated by  [ fig : nn](a ) .",
    "the utility function corresponds to the _ connections model _ of @xcite and has been used to describe various economic and social contexts such as transmission of information .",
    "it has also been applied for distributed topology control in wireless networks @xcite .",
    "practically , it constitutes a measure of network connectivity , since the maximum utility for node @xmath5 is achieved when there is a path from any other node to @xmath5 .",
    "[ cl2.2 ] the network formation game defined by is a coordination game , provided the set of payoff - dominant networks is non - empty .    for a joint action @xmath92 suppose that an agent @xmath5 picks the _ best reply _ in @xmath93 ( i.e. , the most profitable better reply )",
    "then no other agent becomes worse off , since a best reply for @xmath5 always retains connectivity .",
    "note that this is not necessarily true for any other better reply .",
    "thus , definition  [ d2.2 ]  ( b ) is satisfied . in order to show property  ( c ) , consider any joint action @xmath84 that is a nash network .",
    "if any one agent @xmath94 selects the action @xmath95 of establishing  no links \" , then there exists at least one other agent @xmath96 whose payoff becomes strictly less than the equilibrium payoff ( e.g. , pick @xmath97 such that @xmath98 ) .",
    "this is due to the fact that @xmath84 is critically connected .",
    "continue in the same manner by selecting @xmath99 to be the action of establishing  no links \" , and so on . this way , we may construct a sequence of agents and an action profile which satisfies definition  [ d2.2 ]  ( c ) of a coordination game .    the condition that payoff - dominant networks exist",
    "is not restrictive .",
    "for example , if @xmath100 for all @xmath5 , then the set of _ wheel networks _ ( cf .",
    ",  @xcite ) is payoff dominant .    in a forthcoming section",
    ", we present a distributed optimization approach for achieving convergence to payoff - dominant networks through aspiration learning which is of independent interest .",
    "common - pool games refer to strategic interactions where two or more agents need to decide unilaterally whether or not to utilize a limited common resource . in such interactions , each agent would rather use the common resource by itself than share it with another agent , which is usually penalizing for both .",
    "we define common - pool games as follows :    a common - pool game is a strategic - form game such that for each agent @xmath15 , @xmath101 , with @xmath102 , and @xmath103 -c_{j } + \\tau_{j}\\ , , & \\text{if~}\\alpha_{i}=p_{j } \\text{~and~ } \\exists s\\in{\\mathcal{i}}\\setminus\\{i\\ } \\mbox{~s.t.~ } \\alpha_{s } > \\max_{\\ell\\neq{s}}\\alpha_{\\ell}\\,,\\\\[3pt ] -c_{j}\\ , , & \\text{if~}\\alpha_{i}=p_{j } \\text{~and~ } \\nexists s\\in{\\mathcal{i}}\\mbox{~s.t.~ } \\alpha_{s } > \\max_{\\ell\\neq{s}}\\alpha_{\\ell}\\ , ,   \\end{cases}\\ ] ] where @xmath104 , @xmath105 for all @xmath106 , and @xmath107    this definition of a common - pool game can be viewed as a finite - action analog of continuous - action common - pool games defined in @xcite .",
    "table  [ tb : cpg ] presents an example of a common - pool game of 2 players and 3 actions .",
    "33 & @xmath108 & @xmath109 & @xmath110 + @xmath108 & @xmath111 & @xmath112 & @xmath113 + @xmath109 & @xmath114 & @xmath115 & @xmath116 + @xmath117 & @xmath118 & @xmath119 & @xmath120 +     +    we call `` successful '' any action profile in which one player s action is strictly greater than any other player s action .",
    "any other situation corresponds to a `` failure . '' in common - pool games , we define the set of desirable action profiles @xmath28 , as the set of successful action profiles , i.e. , @xmath121    for example , this set of joint actions corresponds to the off - diagonal action profiles in table  [ tb : cpg ] .",
    "moreover , the set @xmath28 payoff - dominates the set @xmath29 .",
    "any common - pool game is a strict coordination game .",
    "let @xmath28 be defined as in .",
    "note first that for any @xmath122 and @xmath123 , we have @xmath124 for all @xmath15 .",
    "in other words , definition  [ d2.2 ]  ( a ) is satisfied .",
    "moreover , note that any @xmath26 is not a nash equilibrium . for any action profile @xmath125 , pick an agent @xmath5 such that @xmath126 .",
    "let us also assume that @xmath127 for some @xmath128 . if @xmath129 , then agent @xmath5 can increase its utility by selecting action @xmath130 for any @xmath131 .",
    "in that case , the utility of any other agent either increases or remains the same . if , instead , @xmath132 , then agent @xmath5 can increase its utility by selecting action @xmath130 for any @xmath133 . in this case , the utility of any other agent increases .",
    "thus , definition  [ d2.2 ]  ( b ) is also satisfied .",
    "lastly , note that @xmath134 . to check this ,",
    "consider any @xmath26 .",
    "as the previous discussion revealed , there always exist an agent and a better reply for that agent , i.e. , @xmath134 .",
    "thus , definition  [ d2.2 ]  ( c ) is trivially satisfied .",
    "if we imagine that a common - pool game is played repeatedly over time , it would be desirable that i ) failures are avoided , and ii ) agents manage to equally share the time they succeed ( i.e. , access the common resource ) . in other words ,",
    "convergence to a successful state may not be sufficient . instead , a ( possibly time - dependent ) solution that equally divides the time - slots that each user utilizes the common resource would seem more appropriate",
    ".    distributed convergence to such solutions is currently an open issue in packet radio multiple - access protocols ( see , e.g. , ( * ? ? ?",
    "* chapter  5 ) ) . in these scenarios , there are multiple users that compete for access to a single communication channel .",
    "each user needs to decide whether or not to occupy the channel in a given time - slot based only on _ local _ information .",
    "if more than one user is occupying the channel , then a _ collision _ occurs and the user needs to resubmit the data .",
    "an example of such multiple - access protocol is the aloha protocol @xcite , where users decide on transmitting a packet according to a probabilistic pattern . in this line of work ,",
    "the action space of each user consists of multiple power levels of transmission @xcite .",
    "if a user transmits with a power level that is strictly larger than the power level of any other user , then it is able to transmit successfully , otherwise a collision occurs and transmission is not possible .",
    "this game can be formulated in a straightforward manner as a common - pool game .    in a forthcoming section",
    "we provide a distributed solution to this problem using aspiration learning which is of independent interest .",
    "in this section , we define aspiration learning , motivated by @xcite . for some constants @xmath135 , @xmath136 , @xmath137 , @xmath138 , @xmath139 , and @xmath140 , such that @xmath141 the aspiration learning iteration initialized at @xmath142 is described in table  [ tb : al ] .     +    according to this algorithm",
    ", each agent @xmath5 keeps track of an _ aspiration level _ @xmath143 , which measures player @xmath5 s desirable return and is defined as a perturbed fading memory average of its payoffs throughout the history of play .",
    "given the current aspiration level @xmath144 , agent @xmath5 selects a new action @xmath145 .",
    "if the previous action @xmath146 provided utility at least @xmath144 , then the agent is  satisfied \" and repeats the same action , i.e. , @xmath147 .",
    "otherwise , @xmath145 is selected randomly over all available actions , where the probability of selecting again @xmath146 depends on the level of discontent measured by the difference @xmath148 .",
    "the random variables @xmath149 are independent , identically distributed and are referred to as the `` tremble . ''",
    "let @xmath150^{n}$ ] , i.e. , pairs of joint actions @xmath84 and vectors of aspiration levels , @xmath143 , @xmath15 .",
    "the set @xmath65 is endowed with the product topology , @xmath151 $ ] with its usual euclidean topology , and @xmath152 with the corresponding product topology .",
    "we also let @xmath153 denote the borel @xmath154-field of @xmath152 , and @xmath155 the set of probability measures on @xmath153 endowed with the prohorov topology , i.e. , the topology of weak convergence . the algorithm in table  [ tb : al ] defines an @xmath152-valued markov chain .",
    "let @xmath156 $ ] denote its transition probability function , parameterized by @xmath157 .",
    "we refer to the process with @xmath157 as the _",
    "perturbed process_.    we let @xmath158 denote the banach space of real - valued continuous functions on @xmath152 under the sup - norm ( denoted by @xmath159 ) topology .",
    "for @xmath160 we define @xmath161{\\triangleq}\\int_{{{\\mathcal{x}}}}\\mu({\\mathrm{d}}{x})f(x)\\,,\\quad \\mu\\in{{\\mathcal{p}}}({{\\mathcal{x}}})\\,.\\ ] ] it is straightforward to verify that @xmath162 has the feller property , i.e. , @xmath163 for all @xmath160 .",
    "recall that @xmath164 is called an invariant probability measure for @xmath162 if @xmath165 since @xmath152 is a compact metric space and @xmath162 has the feller property it admits an invariant probability measure @xmath166 ( * ? ? ?",
    "* theorem  7.2.3 ) .",
    "we are interested in the asymptotic behavior of the aspiration learning algorithm as the `` experimentation probability '' @xmath167 approaches zero .",
    "we say that a state @xmath168 is _ stochastically stable _ if any collection of invariant probability measures @xmath169 satisfies @xmath170 .",
    "it turns out that the stochastically stable states comprise a finite subset of @xmath152 which is defined next .",
    "a _ pure strategy state _ is a state @xmath171 such that for all @xmath15 , @xmath172 .",
    "the set of pure strategy states is denoted by @xmath173 and @xmath174 denotes its cardinality .",
    "note that the set @xmath173 is isomorphic to @xmath65 and can be identified as such .",
    "as customary , the dirac measure in @xmath155 supported at @xmath168 is denoted by @xmath175 .",
    "the objective in this section is to characterize the set of stochastically stable states .",
    "our main result is summarized in the following theorem :    [ t3.1 ] there exists a unique probability vector @xmath176 such that for any collection of invariant probability measures @xmath177 , we have @xmath178 where convergence is in the weak@xmath179 sense",
    ".    as we show later , @xmath180 in theorem  [ t3.1 ] is the unique invariant distribution of a finite - state markov chain .",
    "[ rm : ergodicity ] the expected asymptotic behavior of aspiration learning can be characterized by @xmath181 and , therefore , @xmath180 . in particular , by birkhoff s individual ergodic theorem , e.g. , ( * ? ? ?",
    "* theorem  2.3.4 ) , and the weak convergence of @xmath166 to @xmath181 , the expected percentage of time that the process spends in any @xmath182 such that @xmath183 is given by @xmath184 as the experimentation probability @xmath167 approaches zero and time increases , i.e. , @xmath185    the proof of theorem  [ t3.1 ] requires a series of propositions , which comprise the remaining of this section .",
    "let @xmath186 denote the transition probability function on @xmath187 corresponding to @xmath188 .",
    "we refer to the process @xmath189 governed by @xmath190 as the _",
    "unperturbed process_. let @xmath191 denote the canonical path space , i.e. , an element @xmath192 is a sequence @xmath193 , with @xmath194 .",
    "we use the same notation for the elements @xmath195 of the space @xmath152 and for the coordinates of the process @xmath196 .",
    "let also @xmath197 denote the unique probability measure induced by @xmath190 on the product @xmath154-algebra of @xmath198 , initialized at @xmath199 , and @xmath200 the corresponding expectation operator .",
    "let also @xmath201 @xmath202 , denote the @xmath154-algebra generated by @xmath203 .",
    "for @xmath204 define the sets @xmath205 b_{t } & { \\triangleq}\\{\\omega\\in\\omega:\\alpha(\\tau)=\\alpha(0)\\ , , \\text{~for all~ } 0\\le\\tau\\le{t}\\}\\,.\\end{aligned}\\ ] ] note that @xmath206 is a non - increasing sequence , i.e. , @xmath207 , while @xmath208 is non - decreasing . recall that the shift operator @xmath209 , @xmath204 , satisfies @xmath210",
    ". therefore @xmath211 .",
    "let @xmath212 and @xmath213 . the set @xmath214 is the event that agents eventually play the same action profile , while @xmath215 is the event that agents never change their actions .",
    "for @xmath216 we let @xmath217 denote the first hitting time of @xmath218 , i.e. , @xmath219    [ p3.1 ] it holds that @xmath220    assume that the process is initialized at @xmath221 .",
    "note that @xmath222 consists of those sample paths which satisfy @xmath223 therefore , we have : @xmath224 where @xmath225 let @xmath226 satisfy @xmath227 then @xmath228 & \\ge h^{nt_{0 } } \\prod_{i\\in{\\mathcal{i}}}\\;\\biggl(1- c\\bigl(\\rho_{i}-u_{i}(\\alpha)\\bigr)^{+ } \\sum_{\\tau = t_{0}+1}^{t } ( 1-\\epsilon)^\\tau\\biggr)\\notag\\\\[5pt ] & \\ge h^{nt_{0 } } \\prod_{i\\in{\\mathcal{i}}}\\;\\left ( 1- ( 1-\\epsilon)\\ , \\frac{\\bigl(\\rho_{i}-u_{i}(\\alpha)\\bigr)^{+ } } { \\overline{\\rho}-\\underline{\\rho}}\\right)\\qquad \\forall t > t_{0}\\,,\\end{aligned}\\ ] ] and since the sequence @xmath229 is non - increasing , also for all @xmath204 . therefore , by continuity from above , we obtain @xmath230 which proves the first claim .",
    "next , define the set @xmath231 and note that @xmath232 , where @xmath233 , @xmath204 , denotes the multistage transition probability function defined by the recursion @xmath234 and @xmath235 .",
    "thus , using the markov property over @xmath0 time blocks of length @xmath236 , we obtain the rough estimate @xmath237 let @xmath238 .",
    "we have already shown that @xmath239 . finite induction on yields @xmath240 we have @xmath241 and thus using the markov property together with the fact that @xmath242 a.s .",
    "on @xmath243 , and setting @xmath244 , we obtain @xmath245 & \\ge\\bigl(1-{{\\mathbb{p}}}_{x}(\\uptau\\bigl(d_{\\ell})>\\ell^{2}\\bigr)\\bigr ) \\inf_{y\\in d_{\\ell}}\\;{{\\mathbb{p}}}_{y}(b_\\infty)\\notag\\\\[5pt ] & \\ge\\left(1-q_{0}^{\\ell}\\right ) \\inf_{y\\in d_{\\ell}}\\;{{\\mathbb{p}}}_{y}(b_\\infty)\\,.\\end{aligned}\\ ] ] it is clear by that @xmath246 as @xmath247 .",
    "therefore both terms on the right hand side of converge to @xmath248 as @xmath247 , and the proof is complete .",
    "[ p3.2 ] there exists a transition probability function @xmath249 on @xmath250 that has the feller property and @xmath251 is supported on @xmath173 for all @xmath168 , and such that    * for all @xmath160 , @xmath252 .",
    "* if @xmath253 is a resolvent of @xmath190 , defined by @xmath254 where @xmath255 , @xmath157 , and @xmath256 , then @xmath257    for @xmath160 and @xmath168 , we have @xmath258= p^{t } f(x)$ ] .",
    "since @xmath211 , then using the markov property we obtain that , for any positive @xmath259 and @xmath260 , @xmath261\\bigr\\rvert}\\notag\\\\[5pt ] & = { \\bigl\\lvert{{\\mathbb{e}}}_{x}\\bigl[\\bigl(f(x_{2t})-f(x_{2t+t'})\\bigr ) \\mathbf{1}_{a_{t}}\\bigr]\\bigr\\rvert } + { \\bigl\\lvert{{\\mathbb{e}}}_{x}\\bigl[\\bigl(f(x_{2t})-f(x_{2t+t'})\\bigr ) \\mathbf{1}_{a^{c}_{t}}\\bigr]\\bigr\\rvert}\\notag\\\\[5pt ] & \\le\\bigl\\lvert { { \\mathbb{e}}}_{x}\\bigl[{{\\mathbb{e}}}\\bigl[\\bigl(f(x_{2t})-f(x_{2t+t'})\\bigr ) \\mathbf{1}_{a_{t}}\\bigm|",
    "\\mathfrak{f}_{t}\\bigr]\\bigr]\\bigr\\rvert + 2 \\mathbb{p}_{x}(a^{c}_{t } ) { { \\lvert}f{\\rvert}_{\\infty}}\\notag\\\\[5pt ] & \\le { { \\mathbb{e}}}_{x}\\bigl[{{\\mathbb{e}}}_{x_{t } } \\bigl[{\\lvertf(x_{2t})-f(x_{2t+t'})\\rvert}\\,\\mathbf{1}_{a_{t}}\\bigr]\\bigr ] + 2 \\mathbb{p}_{x}(a^{c}_{t}){{\\lvert}f{\\rvert}_{\\infty}}\\notag\\\\[5pt ] & \\le \\sup_{z\\in{{\\mathcal{x}}}}\\;{{\\mathbb{e}}}_{z}\\bigl[{\\lvertf(x_{t})-f(x_{t+t'})\\rvert}\\ , \\mathbf{1}_{b_{\\infty}}\\bigr ] + 2 \\mathbb{p}_{x}(a^{c}_{t}){{\\lvert}f{\\rvert}_{\\infty}}\\,.\\end{aligned}\\ ] ] since for any initial condition @xmath199 the dynamics on @xmath215 evolve according to @xmath262 the continuity of @xmath263 ( which is necessarily uniform since @xmath152 is compact ) yields @xmath264\\\\ = \\sup_{t'\\ge0}\\;\\sup_{(\\alpha,\\rho)\\in{{\\mathcal{x}}}}\\ ; { \\bigl\\lvertf\\bigl(\\alpha,\\varrho(t;\\alpha,\\rho)\\bigr ) -f\\bigl(\\alpha,\\varrho(t+t';\\alpha,\\rho)\\bigr)\\bigr\\rvert } \\xrightarrow[t\\to\\infty]{}0\\,.\\end{gathered}\\ ] ] by  and proposition  [ p3.1 ] we obtain @xmath265{}0\\,.\\ ] ] therefore , the sequence @xmath266 is cauchy in @xmath267 , and hence converges in @xmath158 .",
    "let @xmath268 .",
    "then for each @xmath269 , @xmath270 defines a bounded linear functional on @xmath158 .",
    "it is a positive functional since @xmath271 , for @xmath272 , and if @xmath273 denotes the constant function equal to @xmath248 , @xmath274 .",
    "then , by the riesz representation theorem , @xmath275 is a borel probability measure on @xmath152 for each @xmath269 .",
    "denote this by @xmath251 .",
    "since @xmath276 , it follows that @xmath249 has the feller property .",
    "also , by the definition of @xmath249 , we have @xmath277 { } 0 \\qquad \\forall f\\in{{\\mathcal{c}}}({{\\mathcal{x}}})\\,.\\ ] ] this proves ( i ) .    next using a triangle inequality",
    ", we have for each @xmath278 , @xmath279 letting @xmath280 , we obtain @xmath281 and ( ii ) follows by",
    ".    we can decompose the transition probability function of the perturbed process as @xmath282 where @xmath283 is the probability that at least one agent trembles , and satisfies @xmath284 as @xmath285 .",
    "also , define the  lifted \" transition probability function : @xmath286 where @xmath253 was defined in proposition  [ p3.2 ] ( the equality on the right - hand side is evident by fubini ) .",
    "similarly we decompose @xmath287 as @xmath288 here @xmath289 is the transition probability function induced by aspiration learning where exactly one player trembles , and @xmath290 is the transition probability function where at least two players tremble simultaneously .",
    "we have the following proposition .",
    "[ p3.3 ] the following hold ,    * for @xmath160 , @xmath291 . *",
    "any invariant distribution @xmath166 of @xmath162 is also an invariant distribution of @xmath292 . *",
    "any weak limit point in @xmath155 of @xmath166 , as @xmath285 , is an invariant probability measure of @xmath293 .",
    "\\(i ) we have @xmath294 & \\le{{\\lvert}r_{\\lambda } f-\\pi{f}{\\rvert}_{\\infty } } + { { \\lvert}q_{\\lambda}\\pi{f } - q \\pi{f}{\\rvert}_{\\infty}}\\,.\\end{aligned}\\ ] ] the first term on the right hand side of tends to @xmath295 as @xmath280 by proposition  [ p3.2 ] , while the second term does the same by the definition of @xmath287 .",
    "\\(ii ) multiplying both sides of by @xmath253 , we have @xmath296 where @xmath297 denotes the identity operator .",
    "let @xmath166 denote an invariant distribution of @xmath162 .",
    "hence , by , we have @xmath298 and the second claim follows .",
    "\\(iii ) let @xmath181 be a limit point of @xmath166 as @xmath280 .",
    "for any @xmath160 , we have @xmath299 - ( \\hat{\\mu } q\\pi)[f ] = \\bigl(\\hat{\\mu}[f ] - \\mu_{\\lambda}[f]\\bigr ) + \\mu_{\\lambda}\\bigl[p_{\\lambda}^{l } f - q\\pi{f}\\bigr ] + \\bigl(\\mu_{\\lambda}\\bigl[q\\pi{f}\\bigr ] - \\hat{\\mu}\\bigl[q\\pi{f}\\bigr]\\bigr)\\,.\\ ] ] the first and the third terms on the right hand side tend to @xmath295 as @xmath280 along some sequence , by the weak convergence @xmath166 to @xmath181 , while the second term is dominated by @xmath300 - q\\pi[f]{\\rvert}_{\\infty}}$ ] that also tends to @xmath295 by part ( i ) .    for @xmath301 let @xmath302 denote the open @xmath303-neighborhood of @xmath72 in @xmath152 .",
    "for any two pure strategy states , @xmath304 , define @xmath305 for some @xmath306 sufficiently small . by proposition  [ p3.1 ] , @xmath307 is independent of the selection of @xmath303 .",
    "define also the @xmath308 stochastic matrix @xmath309 $ ] .",
    "[ p3.4 ] there exists a unique invariant probability measure @xmath181 of @xmath293 .",
    "it satisfies @xmath310 for some constants @xmath311 , @xmath301 .",
    "moreover , @xmath176 is an invariant distribution of @xmath312 , i.e. , @xmath313 .    by proposition  [ p3.2 ] ,",
    "the support of @xmath249 is @xmath173 , and so is the support of @xmath293 .",
    "thus , for any sufficiently small @xmath306 , @xmath314 since @xmath293 is a feller transition function it admits an invariant probability measure , say @xmath181 .",
    "the support of @xmath181 is also @xmath173 , and , therefore , it has the form of for some constants @xmath311 , @xmath301 .",
    "note also that @xmath315 is a continuity set of @xmath316 , i.e. , @xmath317 .",
    "therefore , by the portmanteau theorem , @xmath318 if we also define @xmath319 , then @xmath320 which shows that @xmath180 is an invariant distribution of @xmath312 , i.e. , @xmath313",
    ".    to establish the uniqueness of the invariant distribution of @xmath293 , recall the definition of @xmath289 . since @xmath173 is isomorphic with @xmath65 , we can identify @xmath301 with an element @xmath321 .",
    "if agent @xmath5 trembles , then all actions in @xmath2 have positive probability of being selected , i.e. , @xmath322 for all @xmath323 and @xmath15 .",
    "it follows by proposition  [ p3.1 ] that @xmath324 for all @xmath16 and @xmath15 .",
    "finite induction then shows that @xmath325 for all @xmath84 , @xmath326 .",
    "it follows that if we restrict the domain of @xmath293 to @xmath173 , then @xmath293 defines an irreducible stochastic matrix .",
    "therefore , @xmath293 has a unique invariant distribution .",
    "theorem  [ t3.1 ] follows from propositions  [ p3.3 ] and [ p3.4 ] .",
    "moreover , proposition  [ p3.4 ] shows that the unique invariant probability measure of @xmath293 agrees with the unique invariant probability distribution of the finite stochastic matrix @xmath312 .    a similar result to proposition",
    "[ p3.3](i ) , based on which theorem  [ t3.1 ] was shown , has also been derived in ( * ? ? ?",
    "* theorem  2 ) . the result in @xcite",
    "though assumes incorrectly that the process @xmath289 satisfies the strong feller property .",
    "note that the proof of proposition  [ p3.3 ] does not make use of any such assumption and provides a corrected analysis for the asymptotic behavior of the aspiration learning scheme presented in @xcite .    in the forthcoming sections ,",
    "we demonstrate the importance of theorem  [ t3.1 ] in characterizing the asymptotic behavior of aspiration learning in large coordination games .",
    "note that prior analysis of this type of aspiration learning , e.g. , in @xcite , was only restricted to two player and two action games .",
    "in this section , we study the asymptotic behavior of the invariant distribution @xmath180 of @xmath312 in strict coordination games when the step size @xmath327 approaches zero .",
    "the aim is to characterize the states in @xmath173 that are stochastically stable with respect to the parameter @xmath327 . to this end , first denote @xmath328 as the set of pure strategy states that correspond to @xmath28 .",
    "clearly , @xmath328 is isomorphic to @xmath28",
    ". also , denote by @xmath329 the set of pure strategy states that correspond to the set of nash action profiles @xmath19 .",
    "we define two constants that are important in the analysis : @xmath330 \\delta_{\\text{max } } & { \\triangleq}\\max_{i\\in{\\mathcal{i}}}\\ ; \\max_{\\alpha\\neq\\alpha'}\\;{\\lvertu_{i}(\\alpha ' ) - u_{i}(\\alpha)\\rvert}\\ , .",
    "\\end{split}\\ ] ] for strict coordination games @xmath331 , and it is the smallest possible payoff decrease from the dominant payoff due to any deviation from the set of actions in @xmath28 .    to facilitate the analysis we let @xmath332 and @xmath333 denote the probability and expectation operator , respectively , on the path space of a markov process @xmath334 starting at @xmath168 at @xmath335 , and governed by the family of transition probabilities @xmath336 . in other words @xmath337 for any @xmath338 .",
    "lemma  [ l4.1 ] below introduces two new hypotheses .",
    "the first hypothesis corresponds to the case at which payoff differences within the same action profile are smaller than payoff differences between dominant and non - dominant action profiles .",
    "the second hypothesis corresponds to the case where each player receives a unique payoff within @xmath28 .",
    "[ l4.1 ] let @xmath8 be a strict coordination game satisfying either one of the following two hypotheses :    1 .",
    "2 .   @xmath340 .",
    "then , there exists a constant @xmath341 such that if @xmath342 then @xmath343{}0 \\qquad\\text{~for all~ } \\bar{s}\\in{{\\bar{\\mathcal{s}}}}\\,,~s\\in{{\\mathcal{s}}}\\setminus{{\\bar{\\mathcal{s}}}}\\,.\\ ] ]    suppose ( h1 ) holds .",
    "select @xmath344 .",
    "let @xmath345 .",
    "without loss of generality suppose agent @xmath248 trembles .",
    "if @xmath346 the process clearly converges to @xmath347 as @xmath348 with probability @xmath248 . therefore , suppose @xmath349 .",
    "note that for @xmath204 we have @xmath350 & \\le ( 1-\\epsilon){\\lvert\\rho_{i}(t)-\\rho_{j}(t)\\rvert } + \\epsilon\\delta^ { * } \\qquad\\text{~for all~ } i , j\\in{\\mathcal{i}}\\,,\\end{aligned}\\ ] ] and since @xmath344 by a straightforward induction argument using we obtain @xmath351 for @xmath15 define @xmath352 and for @xmath353 define the sets @xmath354 let also @xmath355 and @xmath356 recall the definition of @xmath357 in and in order to simplify the notation let @xmath358 , for @xmath353 . note the following : firstly , using , we obtain @xmath359 secondly , since @xmath360 , we obtain @xmath361 it is also evident that @xmath362 where @xmath363 is a metric in @xmath173 .",
    "it is clear from the definition of @xmath190 that if @xmath364 there are two possibilities : if a profile @xmath365 is played , then @xmath143 decreases in value for all @xmath15 , or in other words , that @xmath366 for all @xmath367 .",
    "otherwise , if a profile in @xmath28 is played , then the sample path gets trapped in the domain of attraction of @xmath328 .",
    "this means that if @xmath368 then @xmath369 , where @xmath197 is the probability measure induced by @xmath190 defined in section  [ s3 ] . in this case , and by , we also have @xmath370 thus , using the markov property we obtain , with @xmath371 , @xmath372 conditioning on @xmath373 and using the strong markov property , , and the foregoing , we obtain @xmath374\\right ] \\notag\\\\[5pt ] & \\le \\tilde{{{\\mathbb{e}}}}_{\\bar{s } } \\left[{{\\mathbb{p}}}_{x_{\\uptau_{0}}}(\\uptau_{1}<\\infty ) \\right ] \\notag\\\\[5pt ] & \\le \\sup_{x\\in\\gamma\\cap d_{1}^{c}}\\ ; { { \\mathbb{p}}}_{x}(\\uptau_{1}<\\infty)\\notag\\\\[5pt ] & \\le \\sup_{x\\in\\gamma\\cap d_{1}^{c}}\\ ; p^{t_{0}}(x,\\gamma\\setminus\\bar\\gamma)\\notag\\\\[5pt ] & \\le \\exp\\left ( \\left\\lfloor\\frac{\\delta_{\\text{min } } } { 4\\epsilon\\delta_{\\text{max}}}\\right\\rfloor\\log(1-\\gamma)\\right)\\,.\\end{aligned}\\ ] ] the result then follows by and .    next ,",
    "suppose ( h2 ) holds .",
    "note that in this case @xmath375 for all @xmath376 pick any @xmath377 .",
    "as before we may suppose that agent @xmath248 trembles .",
    "let @xmath378 .",
    "let @xmath379 be the first time that an action profile in @xmath380 has been played at least @xmath381 times .",
    "then , at time @xmath379 the aspiration level of the initially perturbed agent @xmath248 satisfies : @xmath382 while the aspiration level of any agent @xmath15 satisfies @xmath383 for @xmath353 define the sets @xmath384 and let @xmath385 , for @xmath353 .",
    "also define @xmath386 it is straightforward to show that @xmath387 . from this point on , we proceed as in the previous case .    for the lemma that follows we need to define the following constant . for each @xmath33 ,",
    "select any @xmath34 and @xmath388 which satisfy definition  [ d2.2 ]  ( c ) , and define @xmath389 by definition  [ d2.2 ]  ( c ) , @xmath390 .",
    "[ l4.2 ] suppose @xmath391 then , for any strict coordination game @xmath8 for which @xmath392 , there exists a constant @xmath393 such that @xmath394    let @xmath395 , @xmath396 .",
    "suppose @xmath34 and @xmath388 are the action profile and sequence of agents , respectively , corresponding to @xmath397 used in the calculation of @xmath398 .",
    "consider the sample paths @xmath399 satisfying @xmath400 , @xmath401 , @xmath402 , and @xmath403 , for @xmath404 .",
    "we have @xmath405 by , @xmath406 for all @xmath15 and @xmath407 . therefore ,",
    "@xmath408 for @xmath409 and hence we obtain @xmath410 and @xmath411 by , we have @xmath412 by , @xmath413 .",
    "consequently , the result follows by .",
    "we define inductively the following collection of sets @xmath414 for @xmath415 . for example",
    ", @xmath416 includes all pure strategy states for which there exist an agent @xmath5 and an action @xmath417 which satisfies ( i.e. , makes no other player worse off ) and also @xmath418 .",
    "let also @xmath419 denote the maximum @xmath0 for which @xmath420 is non - empty , i.e. , @xmath421 such @xmath419 is well - defined since the set of action profiles @xmath65 is finite .    [ l4.3 ] in any coordination game , the collection of sets @xmath422 forms a partition of @xmath173 .    by definition of the collection @xmath422 , the sets @xmath423 are mutually disjoint .",
    "it remains to show that their union coincides with @xmath173 .",
    "assume not , i.e. , assume that there exists @xmath301 such that @xmath424 . according to the definition of a coordination game and claim  [ cl2.1 ]",
    ", there exists a sequence of action profiles @xmath425 , such that @xmath53 and @xmath426 for some @xmath15 terminates in @xmath46 .",
    "let @xmath427 denote the sequence of pure strategy states which corresponds to @xmath425 .",
    "then , for some @xmath428 we have @xmath429 , i.e. , @xmath430 .",
    "since @xmath430 , then we should also have that @xmath431 .",
    "however , this conclusion contradicts our assumption that @xmath432 .",
    "thus , @xmath433 and therefore the collection of sets @xmath422 defines a partition for @xmath173 .",
    "[ t4.1 ] let @xmath8 be a strict coordination game that satisfies either one of the hypotheses ( h1 ) or ( h2 ) in lemma  [ l4.1 ] , and suppose that @xmath434",
    ". then @xmath435 as @xmath436 for all @xmath437",
    ".    consider the partition of @xmath173 defined by the family of sets @xmath422 .",
    "let @xmath438 denote the sub - stochastic matrix composed of the transition probabilities @xmath439 for @xmath440 and @xmath441 .",
    "in other words @xmath442 $ ] is the block decomposition of @xmath312 subordinate to the partition @xmath443 .",
    "similarly , we define @xmath444 , and let @xmath445 denote the block decomposition of @xmath446 subordinate to the partition @xmath447 of @xmath448 . from : @xmath449 we obtain @xmath450 by lemma  [ l4.1 ] , @xmath451 as @xmath452 , while by lemma  [ l4.2 ] for some positive constant @xmath453 , which does not depend on @xmath327 , we have @xmath454 .",
    "thus , @xmath455{}0\\,,\\ ] ] and we obtain @xmath456 similarly , from the equation @xmath457 , we obtain @xmath458 .",
    "it is straightforward to show , using definition  [ d2.2 ]  ( b ) , that for some positive constant @xmath459 , which does not depend on @xmath327 , we have @xmath460 for all @xmath461 . combining the equations",
    "above we get : @xmath462 & = \\pi_{{{\\mathcal{s}}}^{c}_{0}}\\hat{p}_{{{\\mathcal{s}}}^{c}_{0}{{\\mathcal{s}}}_{0}}\\bm{1}\\\\[5pt ] & = \\pi_{{{\\bar{\\mathcal{s}}}}}\\hat{p}_{{{\\bar{\\mathcal{s}}}}{{\\mathcal{s}}}_{0}}\\bm{1 } + \\pi_{\\tilde{{\\mathcal{s}}}^{*}_{\\phantom{0 } } } \\hat{p}_{\\tilde{{\\mathcal{s}}}^{*}_{\\phantom{0}}{{\\mathcal{s}}}_{0}}\\bm{1 } \\xrightarrow[\\epsilon\\to0]{}0\\,,\\end{aligned}\\ ] ] where in the last line we used lemma  [ l4.1 ] and .",
    "thus , we have shown that @xmath463 as @xmath452 .",
    "we proceed by induction .",
    "suppose @xmath464 as @xmath452 .",
    "then , @xmath465{}0\\,,\\ ] ] which shows that @xmath466 as @xmath452 . by lemma  [ l4.3 ] , the proof is complete .",
    "theorem  [ t4.1 ] combined with theorem  [ t3.1 ] provides a complete characterization of the time average asymptotic behavior of aspiration learning in strict coordination games .      in this section ,",
    "we demonstrate the asymptotic behavior of aspiration learning in coordination games as described by theorems  [ t3.1][t4.1 ] .",
    "consider the network formation game of section  [ sec : nfg ] which , according to claim  [ cl2.2 ] , is a ( non - strict ) coordination game .",
    "although theorem  [ t4.1 ] was only shown for strict coordination games , our intention here is to demonstrate that it also applies to the larger class of ( non - strict ) coordination games .",
    "we consider a set of six nodes deployed on the plane , so that the neighbors of each node are the two immediate nodes ( e.g. , @xmath467 ) .",
    "note that a payoff - dominant set of networks exists and corresponds to the wheel networks , where each node has a single link .",
    "we pick the set @xmath28 of desirable networks as the set of wheel networks .",
    "note that the set @xmath28 satisfies hypothesis ( h2 ) of lemma  [ l4.1 ] .    in order for the average behavior",
    "to be observed @xmath167 and @xmath327 need to be sufficiently small .",
    "we choose : @xmath468 , @xmath469 , @xmath470 , @xmath471 , and @xmath472 . in  [ fig : nfgsim ] , we have plotted a typical response of aspiration learning for this setup , where the final graph and the aspiration level as a function of time are shown .    to illustrate better the response of aspiration learning ,",
    "define the distance from node @xmath473 to node @xmath5 , denoted @xmath474 , as the minimum number of hops from @xmath473 to @xmath5 .",
    "we also adopt the convention @xmath475 and @xmath476 if there is no path from @xmath473 to @xmath5 in @xmath74 . the last graph in  [ fig : nfgsim ] plots , for each node , the running average of the inverse total distance from all other nodes , i.e. , @xmath477 .",
    "this number is zero if the node is disconnected from any other node .",
    "we observe that the payoff - dominant profile ( wheel network ) is played with frequency that approaches one .",
    "in fact , the aspiration level converges to @xmath478 and the inverse total distance converges to @xmath479 , both of which correspond to the wheel network .",
    "in several coordination games , establishing convergence ( in the way defined by theorem  [ t3.1 ] ) to the set of desirable states @xmath328 ( as theorem  [ t4.1 ] showed ) may not be sufficient . for example , in common - pool games of section  [ sec : cpg ] , convergence to @xmath328 does not guarantee that all agents get access to the common resource in a _",
    "fair _ schedule . in the remainder of this section ,",
    "we establish conditions under which _ fairness _ is also established .      in this section",
    ", we provide an approach on characterizing explicitly the invariant distribution of a finite - state , irreducible and aperiodic markov chain .",
    "we use a characterization introduced by @xcite , which has been extensively used for showing stochastic stability arguments for several learning dynamics , see , e.g. , @xcite .",
    "in particular , for finite markov chains an invariant distribution can be expressed as the ratio of sums of products consisting of transition probabilities . these products can be described conveniently by means of graphs on the set of states of the chain .",
    "let @xmath173 be a finite set of states , whose elements are denoted by @xmath480 , @xmath481 , etc .",
    ", and let a subset @xmath482 of @xmath173 .",
    "( @xmath482-graph ) a graph consisting of arrows @xmath483 ( @xmath484 ) is called a @xmath482-graph if it satisfies the following conditions :    1 .",
    "every point @xmath485 is the initial point of exactly one arrow ; 2 .",
    "there are no closed cycles in the graph ; or , equivalently , for any point @xmath486 there exists a sequence of arrows leading from it to some point @xmath487 .",
    "we denote by @xmath488 the set of @xmath482-graphs ; we shall use the letter @xmath489 to denote graphs .",
    "if @xmath490 are nonnegative numbers , where @xmath491 , define the product @xmath492    the following lemma holds :    [ l5.1 ] let us consider a markov chain with a finite set of states @xmath173 and transition probabilities @xmath493 and assume that every state can be reached from any other state in a finite number of steps .",
    "then the stationary distribution of the chain is @xmath494 $ ] , where @xmath495 and @xmath496 .      in this section , using theorem  [ t3.1 ] and lemma  [ l5.1 ] we establish fairness in _ symmetric _ games , defined as follows :    a game @xmath8 characterized by the action profile set @xmath65 is symmetric if , for any two agents @xmath497 and any action profile @xmath321 , the following hold : a ) if @xmath498 , then @xmath499 , and b ) if @xmath500 , then there exists an action profile @xmath501 , such that the following two conditions are satisfied :    1 .",
    "@xmath502 , @xmath503 and @xmath504 for all @xmath505 ; 2 .",
    "@xmath506 , @xmath507 and @xmath508 for any @xmath505 .",
    "define the following equivalence relation between states in @xmath173 :    [ d5.3 ] for any two pure - strategy states @xmath304 such that @xmath509 , let @xmath84 and @xmath510 denote the corresponding action profiles .",
    "we write @xmath511 if there exist @xmath497 , @xmath512 , such that the following two conditions are satisfied :    1 .",
    "@xmath502 , @xmath503 and @xmath504 for all @xmath505 ; 2 .",
    "@xmath506 , @xmath507 and @xmath508 for any @xmath505 .",
    "since there is a one - to - one correspondence between @xmath173 and @xmath65 , we also say that two action profiles @xmath84 and @xmath510 are equivalent , if the conditions of definition  [ d5.3 ] are satisfied .",
    "[ l5.2 ] for any symmetric game and for any two pure - strategy states @xmath304 such that @xmath513 , @xmath514 .",
    "let us consider any two pure strategy states @xmath304 such that @xmath513 .",
    "let also consider any @xmath515-graph @xmath489 , i.e. , @xmath516 .",
    "such a graph can be identified as a collection of paths , i.e. , for some @xmath517 , we have @xmath518 where @xmath519 for some @xmath520 . in the above expression",
    ", the function @xmath521 provides an enumeration of the states that belong to the path @xmath522 .",
    "note that due to the definition of @xmath523-graphs , we should have that @xmath524 for all @xmath525 .",
    "moreover , if @xmath526 , we should also have @xmath527 i.e. , the collection of paths @xmath528 do not cross each other , except at node @xmath72 .",
    "let us consider any other state @xmath529 such that @xmath530 .",
    "since the game is symmetric , for any graph @xmath516 , there exists a unique graph @xmath531 which satisfies @xmath532 where @xmath533 and @xmath534 , @xmath535 , for all @xmath536 .",
    "the transition probability between any two states is a sum of probabilities of sequences of action profiles .",
    "since the game is symmetric , for any such sequence of action profiles which leads , for instance , from @xmath537 to @xmath538 , there exists an equivalent sequence of action profiles which leads from @xmath539 to @xmath540 .",
    "therefore , we should have that : @xmath541 for any @xmath525 , and hence , @xmath542 in other words , there exists an isomorphism between the graphs in the sets @xmath523 and @xmath543 , such that any two isomorphic graphs have the same transition probability .",
    "thus , we have @xmath544 for any two states @xmath545 such that @xmath511 .",
    "lemma  [ l5.2 ] can be used to provide a more explicit characterization of the invariant distribution @xmath180 in several classes of coordination games which are also symmetric , e.g. , common - pool games .",
    "first , recall that in common - pool games we define the set of `` desirable '' or `` successful '' action profiles @xmath28 as in . to characterize more explicitly the invariant distribution @xmath180",
    ", we define the subset of pure - strategy states @xmath546 that correspond to `` successful '' states for agent @xmath5 by @xmath547 in other words , @xmath546 corresponds to the set of pure - strategy states in which the action of agent @xmath5 is strictly larger than the action of any other agent @xmath548 .",
    "we also define @xmath549 .",
    "note that the equivalence relation @xmath550 defines an isomorphism among the states of any two sets @xmath546 and @xmath551 for any @xmath512 .",
    "this is due to the fact that for any state @xmath552 , there exists a unique state @xmath553 such that @xmath554 .",
    "[ l5.3 ] for any common - pool game , @xmath555    as already mentioned , for any @xmath497 such that @xmath512 and for any state @xmath552 , there exists a unique state @xmath553 such that @xmath556 .",
    "therefore , the sets @xmath546 and @xmath551 are isomorphic with respect to the equivalence relation @xmath550 . since a common - pool game is symmetric , from lemma  [ l5.2 ] , we conclude that @xmath557 .",
    "[ t5.1 ] let @xmath8 be a common - pool game which satisfies hypothesis ( h1 ) of lemma  [ l4.1 ] .",
    "there exists a constant @xmath558 such that for any @xmath559 , @xmath560{}\\frac{1}{n},$ ] for all @xmath561    first , recognize that the sets @xmath562 are mutually disjoint , and @xmath563 then , by theorem  [ t4.1 ] , and for any @xmath564 , we have @xmath565 as @xmath566",
    "lastly , by lemma  [ l5.3 ] , the conclusion follows .    in other words",
    ", we have shown that the invariant distribution @xmath180 puts equal weight on either agent `` succeeding , '' which establishes a form of _ fairness _ over time .",
    "moreover , it puts zero weight on states outside @xmath328 ( i.e. , states which correspond to collisions ) as @xmath567 .",
    "theorems  [ t3.1 ] and [ t5.1 ] provide a characterization of the asymptotic behavior of aspiration learning in common - pool games as @xmath167 and @xmath327 approach zero .",
    "in fact , according to remark  [ rm : ergodicity ] , the expected percentage of time that the aspiration learning spends in any one of the pure strategy sets @xmath546 should be equal as the perturbation probability @xmath568 and @xmath569 ( i.e. , fairness is established ) . moreover , the expected percentage of `` failures '' ( i.e. , states outside @xmath328 ) approaches zero as @xmath569 .",
    "we consider the following setup for aspiration learning : @xmath570 @xmath571 @xmath572 @xmath573 and @xmath574 also , we consider a common - pool game of 2 players and 4 actions , where @xmath575 , @xmath576 and @xmath577 .",
    "note that the maximum payoff difference within the same action profile is @xmath578 , and the minimum payoff difference between @xmath28 and @xmath579 is @xmath580 .",
    "therefore , the hypotheses of theorem  [ t5.1 ] are clearly satisfied since @xmath581 and @xmath582 . under this setup ,  [ fig : cpgsim ]",
    "demonstrates the response of aspiration learning .",
    "we observe , as theorem  [ t5.1 ] predicts , that the frequency with which either agent succeeds approaches @xmath583 as time increases . also ,",
    "the frequency of collisions ( i.e. , the joint actions in which neither agent succeeds ) approaches zero as time increases .",
    "we introduced an aspiration learning algorithm and analyzed its asymptotic behavior in games of multiple players and actions .",
    "the main contribution of this analysis was the establishment of a relation between the time average behavior of the induced infinite - state markov chain with the invariant distribution of a finite - state markov chain .",
    "the establishment of this relation allowed for characterizing the asymptotic properties of aspiration learning when applied to generic coordination games .",
    "in particular , we showed that over time , the efficient payoff profiles are played ( in expectation ) with a frequency that can become arbitrarily large . this analysis extended ( and corrected ) prior results on aspiration learning which primarily focused on games of two players and two actions .",
    "we further demonstrated these results through simulations on network formation games , where distributed convergence to efficient networks is of particular interest .",
    "finally , we provided conditions under which fair outcomes can be established in symmetric coordination games where coincidence of interest among players is not so strong .",
    "for example , we showed that in common - pool games , where multiple players compete over utilizing a limited resource , the expected frequency at which the common resource is exploited successfully is equally divided among players as time increases , which establishes a form of fairness ."
  ],
  "abstract_text": [
    "<S> we consider the problem of distributed convergence to efficient outcomes in coordination games through dynamics based on aspiration learning . under aspiration learning , a player continues to play an action as long as the rewards received exceed a specified aspiration level . here , the aspiration level is a fading memory average of past rewards , and these levels also are subject to occasional random perturbations . </S>",
    "<S> a player becomes dissatisfied whenever a received reward is less than the aspiration level , in which case the player experiments with a probability proportional to the degree of dissatisfaction . </S>",
    "<S> our first contribution is the characterization of the asymptotic behavior of the induced markov chain of the iterated process in terms of an equivalent finite - state markov chain . </S>",
    "<S> we then characterize explicitly the behavior of the proposed aspiration learning in a generalized version of coordination games , examples of which include network formation and common - pool games . </S>",
    "<S> in particular , we show that in generic coordination games the frequency at which an efficient action profile is played can be made arbitrarily large . </S>",
    "<S> although convergence to efficient outcomes is desirable , in several coordination games , such as common - pool games , attainability of fair outcomes , i.e. , sequences of plays at which players experience highly rewarding returns with the same frequency , might also be of special interest . </S>",
    "<S> to this end , we demonstrate through analysis and simulations that aspiration learning also establishes fair outcomes in all symmetric coordination games , including common - pool games .    </S>",
    "<S> coordination games , aspiration learning , game theory    68t05 , 91a26 , 91a22 , 93e35 , 60j05 , 91a80 </S>"
  ]
}