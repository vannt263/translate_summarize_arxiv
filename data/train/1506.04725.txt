{
  "article_text": [
    "testing whether two random variables are identically distributed without imposing any parametric assumptions on their distributions is important in a variety of scientific applications . these include data integration in bioinformatics @xcite ,",
    "benchmarking for steganography @xcite and automated model checking @xcite . such problems are addressed in the statistics literature via two - sample tests ( also known as homogeneity tests ) .",
    "traditional approaches to two - sample testing are based on distances between representations of the distributions , such as density functions , cumulative distribution functions , characteristic functions or mean embeddings in a reproducing kernel hilbert space ( rkhs ) @xcite .",
    "these representations are infinite dimensional objects , which poses challenges when defining a distance between distributions . examples of such distances include the classical kolmogorov - smirnov distance ( sup - norm between cumulative distribution functions ) ; the maximum mean discrepancy ( mmd ) @xcite , an rkhs norm of the difference between mean embeddings , and the @xmath0-distance ( also known as energy distance ) @xcite , which is an mmd - based test for a particular family of kernels @xcite .",
    "tests may also be based on quantities other than distances , an example being the kernel fisher discriminant ( kfd ) @xcite , the estimation of which still requires calculating the rkhs norm of a difference of mean embeddings , with normalization by an inverse covariance operator .",
    "in contrast to consistent two - sample tests , heuristics based on pseudo - distances , such as the difference between characteristic functions evaluated at a single frequency , have been studied in the context of goodness - of - fit tests @xcite .",
    "it was shown that the power of such tests can be maximized against fully specified alternative hypotheses , where test power is the probability of correctly rejecting the null hypothesis that the distributions are the same .",
    "in other words , if the class of distributions being distinguished is known in advance , then the tests can focus only at those particular frequencies where the characteristic functions differ most .",
    "this approach was generalized to evaluating the empirical characteristic functions at multiple distinct frequencies by @xcite , thus improving on tests that need to know the single `` best '' frequency in advance ( the cost remains linear in the sample size , albeit with a larger constant ) .",
    "this approach still fails to solve the consistency problem , however : two distinct characteristic functions can agree on an interval , and if the tested frequencies fall in that interval , the distributions will be indistinguishable .    in section [ sec : distances ] of the present work , we introduce two novel distances between distributions , which both use a parsimonious representation of the probability measures .",
    "the first distance builds on the notion of differences in characteristic functions with the introduction of _ smooth characteristic functions _ , which can be though as the analytic analogues of the characteristics functions .",
    "a distance between smooth characteristic functions evaluated at a single random frequency is almost surely a distance ( definition [ rand : metric ] formalizes this concept ) between these two distributions . in other words",
    ", there is no need to calculate the whole infinite dimensional representation - it is almost surely sufficient to evaluate it at a single random frequency ( although checking more frequencies will generally result in more powerful tests ) .",
    "the second distance is based on analytic mean embeddings of two distributions in a characteristic rkhs ; again , it is sufficient to evaluate the distance between mean embeddings at a single randomly chosen point to obtain almost surely a distance . to our knowledge , this representation is the first mapping of the space of probability measures into a finite dimensional euclidean space ( in the simplest case , the real line ) that is almost surely an injection , and as a result almost surely a metrization .",
    "this metrization is very appealing from a computational viewpoint , since the statistics based on it have linear time complexity ( in the number of samples ) and constant memory requirements .",
    "we construct statistical tests in section [ sec : test ] , based on empirical estimates of differences in the analytic representations of the two distributions .",
    "our tests have a number of theoretical and computational advantages over previous approaches .",
    "the test based on differences between analytic mean embeddings is a.s .",
    "consistent for all distributions , and the test based on differences between smoothed characteristic functions is a.s .",
    "consistent for all distributions with integrable characteristic functions ( contrast with [ 7 ] , which is only consistent under much more onerous conditions , as discussed above ) .",
    "this same weakness was used by @xcite in justifying a test that integrates over the _",
    "frequency domain ( albeit at cost quadratic in the sample size ) , for which the quadratic - time mmd is a generalization @xcite .",
    "compared with such quadratic time tests , our tests can be conducted in linear time  hence , we expect their power / computation tradeoff to be superior .",
    "we provide several experimental benchmarks ( section [ sec : experiments ] ) for our tests .",
    "first , we compare test power as a function of computation time for two real - life testing settings : amplitude modulated audio samples , and the higgs dataset , which are both challenging multivariate testing problems .",
    "our tests give a better power / computation tradeoff than the characteristic function - based tests of @xcite , the previous sub - quadratic - time mmd tests @xcite , and the quadratic - time mmd test . in terms of power when unlimited computation time is available , we might expect worse performance for the new tests , in line with findings for linear- and sub - quadratic - time mmd - based tests @xcite .",
    "remarkably , such a loss of power is not the rule : for instance , when distinguishing signatures of the higgs boson from background noise @xcite ( higgs dataset ) , we observe that a test based on differences in smoothed empirical characteristic functions outperforms the quadratic - time mmd .",
    "this is in contrast to linear- and sub - quadratic - time mmd - based tests , which by construction are less powerful than quadratic - time mmd .",
    "next , for challenging artificial data ( both high - dimensional distributions , and distributions for which the difference is very subtle ) , our tests again give a better power / computation tradeoff than competing methods .",
    "in this section we consider mappings from the space of probability measures into a sub - space of real valued analytic functions .",
    "we will show that evaluating these maps at @xmath1 randomly selected points is almost surely injective for any @xmath2 . using this result",
    ", we obtain a simple ( randomized ) metrization of the space of probability measures .",
    "this metrization is used in the next section to construct linear - time nonparametric two - sample tests .    to motivate our approach ,",
    "we begin by recalling an integral family of distances between distributions , denoted maximum mean discrepancies ( mmd ) @xcite .",
    "the mmd is defined as @xmath3,\\ ] ] where @xmath4 and @xmath5 are probability measures on @xmath6 , and @xmath7 is the unit ball in the rkhs @xmath8 associated with a positive definite kernel @xmath9 .",
    "a popular choice of @xmath10 is the gaussian kernel @xmath11 with bandwidth parameter @xmath12 .",
    "it can be shown that the mmd is equal to the rkhs distance between so called mean embeddings , @xmath13 where @xmath14 is an embedding of the probability measure @xmath4 to @xmath8 , @xmath15 and @xmath16 denotes the norm in the rkhs @xmath8 .",
    "when @xmath10 is translation invariant , i.e. , @xmath17 , the squared mmd can be written ( * ? ? ?",
    "* corollary 4 ) as @xmath18 where @xmath19 denotes the fourier transform , @xmath20 is the inverse fourier transform ,  and @xmath21 , @xmath22 are the characteristic functions of @xmath4 , @xmath5 , respectively .  from ( * ?",
    "* theorem 9 ) , a kernel is called _ characteristic _ when @xmath23 any bounded , continuous , translation - invariant kernel whose inverse fourier transform is almost everywhere non - zero is characteristic @xcite . by representation ,",
    "it is clear that mmd with a characteristic kernel is a metric .",
    "[ [ pseudometrics - based - on - characteristic - functions . ] ] pseudometrics based on characteristic functions .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    a practical limitation when using the mmd in testing is that empirical estimates are expensive to compute , these being the sum of two u - statistics and an empirical average , with cost quadratic in the sample size .",
    "we might instead consider a finite dimensional approximation to the mmd , achieved by estimating the integral , with the random variable @xmath24 where @xmath25 are sampled independently from the distribution with a density function @xmath26 .",
    "this type of approximation is applied to various kernel algorithms under the name of _ random fourier features _ @xcite .",
    "in the statistical testing literature , the quantity @xmath27 predates the mmd by a considerable time , and was studied in @xcite , and more recently revisited in @xcite .",
    "our first proposition is that @xmath28 can be a poor choice of distance between probability measures , as it fails to distinguish a large class of measures .",
    "the following result is proved in the appendix .",
    "[ prop : notsufficient ] let @xmath29 and let @xmath25 be a sequence of real valued i.i.d . random variables with a distribution which is absolutely continuous with respect to the lebesgue measure .",
    "for any @xmath30 , there exists an uncountable set @xmath31 of mutually distinct probability measures ( on the real line ) such that for any @xmath32 , @xmath33 .",
    "we are therefore motivated to find distances of the form ( [ eq : unsmoothedcharfuncdist ] ) that can distinguish larger classes of distributions , yet remain efficient to compute .",
    "these distances are characterized as follows :    [ rand : metric ] a random process @xmath34 with the values in @xmath35 , indexed with pairs from the set of probability measures @xmath36 @xmath37 is said to be a random metric if it satisfies all the conditions for a metric with qualification ` almost surely ' .",
    "formally , for all @xmath38 , random variables @xmath39 must satisfy    1 .",
    "@xmath40 a.s .",
    "2 .   if @xmath41 , then @xmath42 a.s , if @xmath43 then @xmath42 a.s .",
    "3 .   @xmath44 a.s .",
    "4 .   @xmath45 a.s .",
    "are distances on @xmath36 , but it does imply that they are almost surely distances for all arbitrary finite subsets of @xmath36 . ]    from the statistical testing point of view , the coincidence axiom of a metric @xmath34 , @xmath42 if and only if @xmath41 , is key , as it ensures consistency against all alternatives .",
    "the quantity @xmath27 in ( [ eq : unsmoothedcharfuncdist ] ) violates the coincidence axiom , so it is only a random pseudometric ( other axioms are trivially satisfied ) .",
    "we remedy this problem by replacing the characteristic functions by smooth characteristic functions :    a smooth characteristic function @xmath46 of a measure @xmath4 is a characteristic function of @xmath4 convolved with an analytic smoothing kernel @xmath47 , i.e. @xmath48    the analogue of @xmath27 for smooth characteristic functions is simply @xmath49 where @xmath25 are sampled independently from the absolutely continuous distribution ( returning to our earlier example , this might be @xmath50 if we believe this to be an informative choice ) .",
    "the following theorem , proved in the appendix , demonstrates that the smoothing greatly increases the class of distributions we can distinguish .    [ th : charemb ] let @xmath47 be an analytic , integrable kernel with an inverse fourier transform strictly greater than zero .",
    "then , for any @xmath2 , @xmath51 is a random metric on the space of probability measures with integrable characteristic functions , and @xmath52 is an analytic function .",
    "this result is primarily a consequence of analyticity of smooth characteristic functions and the fact that analytic functions are well behaved. there is an additional , practical advantage to smoothing : when the variability in the difference of the characteristic functions is high , and these differences are local , smoothing distributes the difference in cfs more evenly in the frequency domain ( a simple illustration is in fig . [",
    "fig : var ] , appendix ) , making them easier to find by measurement at a small number of randomly chosen points .",
    "this accounts for the observed improvements in test power in section [ sec : experiments ] , over differences in unsmoothed cfs .",
    "[ [ metrics - based - on - mean - embeddings . ] ] metrics based on mean embeddings .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the key step which led us to the construction of a random metric @xmath51 is the convolution of the original characteristic functions with an analytic smoothing kernel .",
    "this idea need not be restricted to the representations of probability measures in the frequency domain .",
    "we may instead directly convolve the probability measure with a positive definite kernel @xmath10 ( that need not be translation invariant ) , yielding its mean embedding into the associated rkhs , @xmath53 we say that a positive definite kernel @xmath54 is analytic on its domain if for all @xmath55 , the feature map @xmath56 is an analytic function on @xmath57 . by using embeddings with _ characteristic and analytic _ kernels , we obtain particularly useful representations of distributions . as for the smoothed cf case , we define @xmath58 the following theorem ensures that @xmath59 is also a random metric .",
    "[ th : analyticmean ] let @xmath10 be an analytic , integrable and characteristic kernel .",
    "then for any @xmath2 , @xmath60 is a random metric on the space of probability measures ( and @xmath14 is an analytic function ) .",
    "note that this result is stronger than the one presented in theorem [ th : charemb ] , since is is not restricted to the class of probability measures with integrable characteristic functions .",
    "indeed , the assumption that the characteristic function is integrable implies the existence and boundedness of a density . recalling the representation of mmd in",
    ", we have proved that it is almost always sufficient to measure difference between @xmath14 and @xmath61 at a finite number of points , provided our kernel is characteristic and analytic . in the next section",
    ", we will see that metrization of the space of probability measures using random metrics @xmath60 , @xmath51 is very appealing from the computational point of view .",
    "it turns out that the statistical tests that arise from those metrics have linear time complexity ( in the number of samples ) and constant memory requirements .",
    "in this section , we provide two linear - time two - sample tests : first , a test based on analytic mean embeddings , and then a test based on smooth characteristic functions .",
    "we further describe the relation with competing alternatives .",
    "proofs of this chapter s propositions are in the appendix [ ap : prrofs ] .",
    "* difference in analytic functions * in the previous section we described the random metric based on a difference in analytic mean embeddings , @xmath62 if we replace @xmath14 with the empirical mean embedding @xmath63 it can be shown that for any sequence of unique @xmath64 , under the null hypothesis , as @xmath65 , @xmath66 converges in distribution to a sum of correlated chi - squared variables . even for fixed @xmath64 ,",
    "it is very computationally costly to obtain quantiles of this distribution , since this requires a bootstrap or permutation procedure .",
    "we will follow a different approach based on hotelling s",
    "@xmath67-statistic @xcite .",
    "the hotelling s @xmath67-squared statistic of a normally distributed , zero mean , gaussian vector @xmath68 , with a covariance matrix @xmath69 , is @xmath70 .",
    "the compelling property of the statistic is that it is distributed as a @xmath71-random variable with @xmath1 degrees of freedom . to see a link between @xmath67 and equation ( [ eq : sumchi ] ) , consider a random variable @xmath72 :",
    "this is also distributed as a sum of correlated chi - squared variables . in our case",
    "@xmath73 is replaced with a difference of normalized empirical mean embeddings , and @xmath69 is replaced with the empirical covariance of the difference of mean embeddings .",
    "formally , let @xmath74 denote the vector of differences between kernels at tests points @xmath75 , @xmath76 we define the vector of mean empirical differences @xmath77 and its covariance matrix @xmath78 .",
    "the test statistic is @xmath79 the computation of @xmath80 requires inversion of a @xmath81 matrix @xmath82 , but this is fast and numerically stable : @xmath1 will typically be small and is in our experiments less than 10 .",
    "the next proposition demonstrates the use of @xmath80 as a two - sample test statistic .",
    "[ prop : hotelling ] let @xmath83 a.s . and",
    "let @xmath84 and @xmath85 be i.i.d .",
    "samples from @xmath4 and @xmath5 respectively .",
    "then the statistic @xmath80 is a.s .",
    "asymptotically distributed as a @xmath71-random variable with @xmath1 degrees of freedom ( as @xmath86 with @xmath34 fixed ) . if @xmath87 a.s .",
    ", then a.s .",
    "for any fixed @xmath88 , @xmath89 as @xmath86 .",
    "we now apply the above proposition in obtaining a statistical test .",
    "[ test ] calculate @xmath80 .",
    "choose a threshold @xmath90 corresponding to the @xmath91 quantile of a @xmath71 distribution with @xmath1 degrees of freedom , and reject the null hypothesis whenever @xmath80 is larger than @xmath90 .",
    "there are a number of valid sampling schemes for the test points @xmath25 to evaluate the differences in mean embeddings : see section [ sec : experiments ] for a discussion .",
    "* difference in smooth characteristic functions * from the convolution definition of a smooth characteristic function it is not clear how to calculate its estimator in linear time .",
    "however , we show in the next proposition that a smooth characteristic function can be written as an expected value of some function with respect to the given measure , which can be estimated in a linear time .",
    "[ lemma : compchar ] let @xmath10 be an integrable translation - invariant kernel and @xmath92 its inverse fourier transform",
    ". then the smooth characteristic function of @xmath4 can be written as @xmath93    it is now clear that a test based on the smooth characteristic functions is similar to the test based on mean embeddings .",
    "the main difference is in the definition of the vector of differences @xmath74 : @xmath94 the imaginary and real part of the @xmath95 are stacked together , in order to ensure that @xmath96 , @xmath82 and @xmath80 as all real - valued quantities .",
    "[ prop : hotelling2 ] let @xmath83 and let @xmath84 and @xmath85 be i.i.d .",
    "samples from @xmath4 and @xmath5 respectively .",
    "then the statistic @xmath80 is almost surely asymptotically distributed as a @xmath71-random variable with @xmath97 degrees of freedom ( as @xmath86 with @xmath1 fixed ) . if @xmath98 , then almost surely for any fixed @xmath88 , @xmath99 as @xmath86 .",
    "* other tests*. the test @xcite based on empirical characteristic functions was constructed originally for one test point and then generalized to many points - it is quite similar to our second test , but does not perform smoothing ( it is also based on a @xmath67-hotelling statistic ) .",
    "the block mmd @xcite is a sub - quadratic test , which can be trivially linearized by fixing the block size , as presented in the appendix . finally , another alternative is the mmd , an inherently quadratic time test .",
    "we scale mmd to linear time by sub - sampling our data set , and choosing only @xmath100 points , so that the mmd complexity becomes @xmath101 .",
    "note , however , that the true complexity of mmd involves a permutation calculation of the null distribution at cost @xmath102 , where the number of permutations @xmath103 grows with @xmath104",
    ". see appendix [ sec : othertests ] for a detailed description of alternative tests .",
    "in this section we compare two - sample tests on both artificial benchmark data and on real - world data .",
    "we denote the smooth characteristic function test as ` smooth cf ' , and the test based on the analytic mean embeddings as ` mean embedding ' .",
    "we compare against several alternative testing approaches : block mmd ( ` block mmd ' ) , a characteristic functions based test ( ` cf ' ) , a sub - sampling mmd test ( ` mmd(@xmath105 ) ' ) , and the quadratic - time mmd test ( ` mmd(n ) ' ) .    *",
    "experimental setup . * for all the experiments , @xmath106 is the dimensionality of samples in a dataset , @xmath104 is a number of samples in the dataset ( sample size ) and @xmath1 is number of test frequencies .",
    "parameter selection is required for all the tests .",
    "the table summarizes the main choices of the parameters made for the experiments .",
    "the first parameter is the test function , used to calculate the particular statistic .",
    "the scalar @xmath12 represents the length - scale of the observed data .",
    "notice that for the kernel tests we recover the standard parameterization @xmath107 .",
    "the original cf test was proposed without any parameters , hence we added @xmath12 to ensure a fair comparison - for this test varying @xmath12 is equivalent to adjusting the variance of the distribution of frequencies @xmath75 . for all tests ,",
    "the value of the scaling parameter @xmath12 was chosen so as to maximize test power on a held - out training set : details are described in appendix [ sec : params ] .",
    "we chose not to optimize the sampling scheme for the mean embedding and smooth cf tests , since this would give them an unfair advantage over the block mmd , mmd(@xmath105 ) and cf tests .",
    "the block size in the block mmd test and the number of test frequencies in the mean embedding , smooth cf , and cf tests , were always set to the same value ( not greater than 10 ) to maintain exactly the same time complexity .",
    "note that we did _ not _ use the popular median heuristic for kernel bandwidth choice ( mmd and b - test ) , since it gives poor results for the blobs and am audio datasets @xcite .",
    "we do not run mmd(n ) test in the simulation 1 and on the amplitude modulated music since the sample size is @xmath108 , i.e. , too large for a quadratic - time test with permutation sampling for the test critical value .",
    "it is important to verify that type i error is indeed at the design level , set at @xmath109 in this paper .",
    "this is verified in the appendix figure [ fig : h0 ] .",
    "also shown in the plots is the @xmath110 percent confidence intervals for the results , as averaged over 4000 runs .",
    "[ tab : params ]    [ cols=\"<,^,^,^\",options=\"header \" , ]     [ [ real - data-1-higgs - dataset ] ] real data 1 : higgs dataset , + + + + + + + + + + + + + + + + + + + + + + + + + + +    _ @xmath111 , @xmath104 varies , @xmath112 .",
    "_ the first experiment we consider is on the uci higgs dataset @xcite described in @xcite - the task is to distinguish signatures of processes which produce higgs bosons from background processes which do not .",
    "we consider a two - sample test on certain extremely low - level features in the dataset - kinematic properties measured by the particle detectors , i.e. , the joint distributions of the azimuthal angular momenta @xmath113 for four particle jets .",
    "we denote by @xmath4 the jet @xmath113-momenta distribution of the background process ( no higgs bosons ) , and by @xmath5 the corresponding distribution for the process that produces higgs bosons ( both are distributions on @xmath114 ) . as discussed in (",
    "2 ) , @xmath113-momenta , unlike transverse momenta @xmath115 , carry very little discriminating information for recognizing whether higgs bosons were produced or not .",
    "therefore , we would like to test the null hypothesis that the distributions of angular momenta @xmath4 ( no higgs boson observed ) and @xmath5 ( higgs boson observed ) might yet be rejected .",
    "the results for different algorithms are presented in the figure [ fig : higgs ] .",
    "we observe that the joint distribution of the angular momenta is in fact a discriminative feature .",
    "sample size varies from 1000 to 12000 .",
    "the smooth cf test has significantly higher power than the other tests , including the quadratic - time mmd , which we could only run on up to @xmath116 samples due to computational limitations .",
    "the leading performance of the smooth cf test is especially remarkable given it is several orders of magnitude faster then the quadratic - time mmd(n ) , which is both expensive to compute , and requires a costly permutation approach to determine the significance threshold .        [",
    "[ real - data-2-amplitude - modulated - music ] ] real data 2 : amplitude modulated music , + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    _ @xmath117 , @xmath118 , @xmath112 .",
    "_ amplitude modulation is the earliest technique used to transmit voice over the radio . in the following experiment observations",
    "were one thousand dimensional samples of carrier signals that were modulated with two different input audio signals from the same album , song @xmath4 and song @xmath5 ( further details of these data are described in ( * ? ? ?",
    "* section 5 ) ) . to increase",
    "the difficulty of the testing problem , independent gaussian noise of increasing variance ( in the range @xmath119 to @xmath120 ) was added to the signals .",
    "the results are presented in the figure [ fig : mean ] .",
    "compared to the other tests , the mean embedding and smooth cf tests are more robust to the moderate noise contamination .     and",
    "@xmath5.,scaledwidth=86.0% ]    [ [ simulation-1-high - dimensions ] ] simulation 1 : high dimensions , + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    _ @xmath106 varies , @xmath118 , @xmath121 .",
    "_ it has been recently shown , in theory and in practice , that the two - sample problem gets more difficult as the number of the dimensions increases on which the distributions do not differ @xcite . in the following experiment , we study the power of the two - sample tests as a function of dimension of the samples .",
    "we run two - sample test on two datasets of gaussian random vectors which differ _ only _ in the first dimension , @xmath122 where @xmath123 is a @xmath106-dimensional vector of zeros , @xmath124 is a @xmath106-dimensional identity matrix , and @xmath125 is a diagonal matrix with @xmath126 on the diagonal .",
    "the number of dimensions ( d ) varies from 50 to 1000 ( dataset i ) and from 50 to 2500 ( dataset ii ) .",
    "the power of the different two - sample tests is presented in figure [ fig : varandmean ] . the mean embedding test yields best performance for both datasets , where the advantage is especially large for differences in variance .",
    "[ [ simulation-2-blobs ] ] simulation 2 : blobs , + + + + + + + + + + + + + + + + + + + +    _ @xmath127 , @xmath104 varies , @xmath128 . _",
    "[ sec : blobs ] the blobs dataset is a grid of two dimensional gaussian distributions ( see figure [ fig : blobs ] ) , which is known to be a challenging two - sample testing task .",
    "the difficulty arises from the fact that the difference in distributions is encoded at a much smaller lengthscale than the overall data . in this experiment",
    "both @xmath4 and @xmath5 are a four by four grid of gaussians , where @xmath4 has unit covariance matrix in each mixture component , while each component of @xmath5 has a non unit covariance matrix .",
    "it was demonstrated by @xcite that a good choice of kernel is crucial for this task .",
    "figure [ fig : blobs ] presents the results of two - sample tests on the blobs dataset .",
    "the number of samples varies from 50 to 14000 ( mmd(n ) reached test power one with @xmath129 ) .",
    "we found that the mmd(n ) test has the best power as function of the sample size , but the worst power / computation tradeoff .",
    "by contrast , random distance based tests have the best power / computation tradeoff .     and",
    "amplified so the standard deviation in this direction is 2.,scaledwidth=86.0% ]",
    "which uses a single frequency @xmath130 as a function of this frequency ; * middle * : @xmath131 depicted in the same way ; * right * : @xmath132 which uses a single location @xmath130 as a function of this location . the measures @xmath133 used",
    "are illustrated in figure [ fig : blobs ] - these are grids of gaussian distributions discussed in detail in section [ sec : blobs ] . ]",
    "( @xmath134 is number of repetitions ) around the design test size of @xmath109 .",
    ", scaledwidth=86.0% ]",
    "for some @xmath135 , there exists an interval @xmath136 $ ] with measure @xmath137 .",
    "define @xmath138 for @xmath139 and zero elsewhere . by polya s",
    "theorem , @xmath140 is an uncountable family of characteristic functions that are the same on the complement of @xmath136 $ ] , which has measure @xmath141 . for @xmath142 , @xmath143 in some neighborhood of @xmath144 ,",
    "hence the measures associated with those characteristic functions are different .",
    "the probability that all @xmath145 sit in the complement of interval @xmath136 $ ] is @xmath146 and such an event implies that @xmath147 .",
    "first we give a proposition that characterizes limits of analytic functions .",
    "[ anal ] if @xmath148 is a sequence of real valued , uniformly bounded analytic functions on @xmath149 converging pointwise to @xmath92 , then @xmath92 is analytic .",
    "now we characterize the rkhs of an analytic kernel .",
    "similar results were proved for specific classes of kernels in ( * ? ? ?",
    "* theorem 1 ) , ( * ? ? ?",
    "* corollary 3.5 ) .",
    "[ lem : analytich ] if @xmath10 is a bounded , analytic kernel on @xmath150 , then all functions in the rkhs @xmath151 associated with this kernel are analytic .    since @xmath149 is separable then by ( * ? ? ?",
    "* lemma 4.33 ) hilbert space @xmath151 is separable . by moore - aronszajn theorem @xcite there",
    "exist a set @xmath152 of linear combinations of functions @xmath153 , which is dense in @xmath151 and @xmath151 is a set of functions which are pointwise limits of cauchy sequences in @xmath152 . for each @xmath154 let @xmath155 be a sequence of functions converging in the hilbert space norm to @xmath92 . since @xmath148 is convergent there exists @xmath156 such that @xmath157 @xmath158 .",
    "for all @xmath104 there exist a uniform bound on @xmath159 norm @xmath160 since @xmath10 is bounded , by the ( * ? ? ?",
    "* lemma 4.33 ) , there exists @xmath161 such that for any @xmath154 , @xmath162 .",
    "therefore for all @xmath104 @xmath163 finally , using proposition [ anal ] we conclude that @xmath92 is analytic .",
    "this concludes the proof of lemma [ lem : analytich ] .",
    "next , we show that analytic functions are well behaved.    [ lem : disc ] let @xmath164 be absolutely continuous measure on @xmath149 ( wrt .",
    "the lebesgue measure ) .",
    "non - zero , analytic function @xmath92 can be zero at most at the set of measure 0 , with respect to the measure @xmath164 .",
    "if @xmath92 is zero at the set with a limit point then it is zero everywhere",
    ". therefore @xmath92 can be zero at most at a set @xmath165 without a limit point , which by definition is a discrete set ( distance between any two points in @xmath165 is greater then some @xmath30 ) .",
    "discrete sets have zero lebesgue measure ( as a countable union of points with zero measure ) .",
    "since @xmath4 is absolutely continuous then @xmath166 is zero as well .",
    "next , we show how to construct random distances .",
    "[ main : lemma ] let @xmath167 be an injective mapping from the space of the probability measures into a space of analytic functions on @xmath149 . define @xmath168(t_j ) -\\left[\\lambda q\\right](t_j ) \\big|^2\\ ] ] where @xmath25 are real valued i.i.d .",
    "random variables from a distribution which is absolutely continuous with respect to the lebesgue measure .",
    "then , @xmath169 is a random metric .",
    "let @xmath170 and @xmath171 be images of measures @xmath4 and @xmath5 respectively .",
    "we want to apply lemma [ lem : disc ] to the analytic function @xmath172 , with the measure @xmath173 , to see that if @xmath43 then @xmath174 a.s .",
    "to do so , we need to show that @xmath43 implies that @xmath92 is non - zero .",
    "since mapping to @xmath167 is injective , there must exists at least one point @xmath175 where @xmath92 is non - zero . by continuity of @xmath92 , there exists a ball around @xmath175 in which @xmath92 is non - zero .",
    "we have shown that @xmath43 implies @xmath174 a.s . which in turn implies that @xmath176 a.s . if @xmath177 then @xmath178 and @xmath179 .    by the construction @xmath180 and for any measure @xmath181 , @xmath182 a.s .",
    "since the triangle inequality holds for any vectors in @xmath183 .",
    "we are ready to proof theorem [ th : analyticmean ] .",
    "since @xmath10 is characteristic the mapping @xmath184 is injective .",
    "since @xmath10 is a bounded , analytic kernel on @xmath150 , the lemma [ lem : analytich ] guarantees that @xmath14 is analytic , hence the image of @xmath167 is a subset of analytic functions .",
    "therefore , we can use lemma [ main : lemma ] to see that @xmath185 is a random metric .",
    "this concludes the proof of theorem [ th : analyticmean ] .",
    "we first show that smooth characteristic functions are unique to distributions .",
    "[ lem : meanemb ] if @xmath47 is an analytic , integrable , translation invariant kernel with an inverse fourier transform strictly greater then zero and @xmath4 has integrable characteristic function , then the mapping @xmath186 is injective and @xmath187 is element of the rkhs @xmath188 associated with @xmath47 .    for the integrable characteristic function @xmath113",
    "we define a functional @xmath189 given by formula @xmath190 since @xmath191 , @xmath192 is linear .",
    "we check that @xmath192 is bounded ; let @xmath193 be a unit ball in the hilbert space .",
    "@xmath194 by riesz representation theorem there exist @xmath195 such that @xmath196 . by reproducing property @xmath197",
    "is given by equation @xmath198 .",
    "with each probability measure @xmath4 with an integrable characteristic function @xmath21 we associate the smooth characteristic function with @xmath199    we will prove that @xmath200 is injective .",
    "we will show that , @xmath201 implies @xmath41 .",
    "@xmath202 we apply inverse fourier transform to this convolution and get @xmath203 where @xmath204 , @xmath205 and @xmath206 .",
    "since inverse fourier transform is injective on the space of the integrable characteristic functions , and all @xmath207 are integrable cfs , then application of the inverse fourier transform does not enlarge the null space of eq . .",
    "since @xmath208 , @xmath209 everywhere , implying that the mapping @xmath200 is injective .",
    "this concludes the proof of lemma [ lem : meanemb ] .",
    "next , we show that smooth characteristic function is analytic .",
    "[ lem : analytichh ] if @xmath47 is an analytic , integrable kernel with an inverse fourier transform strictly greater then zero and @xmath4 has an integrable characteristic function then the smooth characteristic function @xmath52 is analytic .    by lemma [ lemma : compchar ] , all functions in the rkhs associated with @xmath47 are analytic and by [ lem : meanemb ] @xmath52 is an element of this rkhs .",
    "we are ready to proof theorem [ th : charemb ] .",
    "since @xmath47 is an analytic , integrable kernel with an inverse fourier transform strictly greater then zero then by the lemma [ lem : meanemb ] the mapping @xmath210 is injective and @xmath211 is an element of the rkhs associated with @xmath47 .",
    "the lemma [ lem : analytichh ] shows that @xmath14 is analytic .",
    "therefore we can use lemma [ main : lemma ] to see that @xmath212 is a random metric .",
    "this concludes the proof of theorem [ th : charemb ]    [ [ proof - of - lemma - lemmacompchar ] ] proof of lemma [ lemma : compchar ] + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    by fubini s theorem we get @xmath213.\\end{aligned}\\ ] ] use of fubini s theorem is justified , since the iterated integral is finite @xcite[theorem 8.8 b ] i.e. @xmath214    [ [ proof - of - proposition - prophotelling ] ] proof of proposition [ prop : hotelling ] + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the probability space of random variables @xmath215 and @xmath216 is a product space i.e sequence of @xmath75 s is defined on the space @xmath217 and the sequence of @xmath218 s is defined on the space @xmath219 .",
    "we will show that for almost all @xmath220 , @xmath80 converges to @xmath71 distribution with @xmath1 degrees of freedom .",
    "we define @xmath221 if there exist @xmath222 , such that @xmath223 , then we set @xmath224 .",
    "otherwise , if @xmath225 then @xmath226 converges to a multivariate gaussian vector with covariance matrix @xmath227 ( the variance @xmath228 is finite so we use standard multivariate clt ) .",
    "therefore @xmath229 has asymptotically @xmath71 distribution with @xmath1 degrees of freedom ( by the clt and slutsky s theorem ) .",
    "consider @xmath230 if @xmath231 then for all @xmath232 , @xmath233 , which implies that @xmath234 .",
    "if @xmath235 then @xmath236 to see that , first we show that @xmath237 converges in probability to the positive definite matrix @xmath238 .",
    "indeed , each entry of the matrix @xmath239 converges to the matrix @xmath240 , hence entires of the matrix @xmath238 , given by a continuous function of the entries of @xmath240 , are limit of the sequence @xmath237 .",
    "similarly @xmath241 converges in probability to the vector @xmath242 .",
    "since @xmath243 ( @xmath238 is positive definite ) , then @xmath244 , being a continuous function of the entries of @xmath245 and @xmath246 , converges to @xmath247 . on",
    "the other hand @xmath248 converges to zero and the proposition follows . finally since @xmath249 almost surely then @xmath235 for almost all @xmath220 .",
    "we have showed that the proposition hold for almost all @xmath250 .",
    "indeed it does not hold if it happens that for some @xmath222 , @xmath223 or @xmath251 for @xmath43 . but",
    "both those events have zero measure .    [ [ proof - of - proposition - prophotelling2 ] ] proof of proposition [ prop : hotelling2 ] + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the poof is analogue to the proof of the proposition [ prop : hotelling ] .",
    "for two measures @xmath4 , @xmath5 the population @xmath252 can be written as @xmath253    an mmd - based test uses as its statistic an empirical estimator of the squared population mmd , and rejects the null if this is larger than a threshold @xmath90 corresponding to the @xmath91 quantile of the null distribution . the minimum variance unbiased estimator of mmd is @xmath254 the test threshold @xmath90 is costly to compute .",
    "the null distribution of @xmath255 is an infinite weighted sum of chi - squared random variables , where the weights are eigenvalues of the kernel with respect to the ( unknown ) distribution @xmath4 .",
    "a bootstrap or permutation procedure may be used in obtaining consistent quantiles of the null distribution , however the cost is @xmath256 if we have @xmath103 permutations and @xmath104 data points ( @xmath103 is usually in the hundreds , at minimum ) . as an alternative consistent procedure ,",
    "the eigenvalues of the joint gram matrix over samples from @xmath4 and @xmath5 may be used in place of the population eigenvalues ; the fastest quadratic - time mmd test uses a gamma approximation to the null distribution , which works well most of the times , but has no consistency guarantees @xcite .",
    "an alternative to the quadratic - time mmd test is a b - test ( block - based test ) : the idea is to break the data into blocks , compute a quadratic - time statistic on each block , and average these quantities to obtain the test statistic .",
    "more specifically , for an individual block , laying on the main diagonal and starting at position @xmath257 , the statistic @xmath258 is calculated as @xmath259 the overall test statistic is then @xmath260    the choice of @xmath261 determines computation time - at one extreme is the linear - time mmd suggested by @xcite where we have @xmath262 blocks of size @xmath263 , and at the other extreme is the usual full mmd with @xmath119 block of size @xmath104 , which requires calculating the test statistic on the whole kernel matrix in quadratic time . in our case , the size of the block remains constant as @xmath104 increases , and is greater than 2 .",
    "this is very similar to the case proposed by @xcite , and the consistency of the test is not affected .",
    "b - test of @xcite assumes that @xmath264 together with @xmath104 , which implies that the statistic @xmath265 defined in under the null distribution satisfies @xmath266 for asymptotic variance @xmath267 $ ] that can easily be estimated directly or by considering the empirical variance of the statistics computed within each of the blocks .",
    "note that the same asymptotic variance @xmath268 is obtained in the case of a quadratic - time statistic @xcite  albeit convergence rate being a faster @xmath269 in that case .",
    "indeed , is obtained directly from the leading term of the variance of each block - based statistic being @xmath270 .",
    "therefore , the p - value for b - test is approximated as @xmath271 , where @xmath272 is the standard normal cdf .",
    "when @xmath261 remains constant as @xmath104 increases , it can be shown that the variance of each block - based statistic is exactly @xmath273 , and thus we obtain by clt that @xmath274 therefore , a slight change to p - value needs to be applied when @xmath268 is estimated directly : @xmath275 .",
    "if , however , one simply uses the empirical variance of the individual statistics computed within each block , the procedure is unaffected .",
    "we split our data set into two disjoint sets , training and testing set , and optimize parameters on the training set .",
    "we did nt come up with an automated testing procedure , instead we plotted the p - values of tests for different scales .",
    "the figure @xmath276 presents such a plot for three different tests .",
    "the p - values were obtained by running the test several times ( 20 to 50 ) for each data scaling @xmath277 .",
    "note that in the case of simulations we just generated new training dataset for each repetition for a given data scaling .",
    "for the music dataset we generated new noises for each scaling and for the higgs dataset we have used bootstrap .",
    "the last method is applicable to real life problems i.e. we split our data into training and test part and then bootstrap from the training part ."
  ],
  "abstract_text": [
    "<S> we propose a class of nonparametric two - sample tests with a cost linear in the sample size . </S>",
    "<S> two tests are given , both based on an ensemble of distances between analytic functions representing each of the distributions . </S>",
    "<S> the first test uses smoothed empirical characteristic functions to represent the distributions , the second uses distribution embeddings in a reproducing kernel hilbert space . </S>",
    "<S> analyticity implies that differences in the distributions may be detected almost surely at a finite number of randomly chosen locations / frequencies . </S>",
    "<S> the new tests are consistent against a larger class of alternatives than the previous linear - time tests based on the ( non - smoothed ) empirical characteristic functions , while being much faster than the current state - of - the - art quadratic - time kernel - based or energy distance - based tests . </S>",
    "<S> experiments on artificial benchmarks and on challenging real - world testing problems demonstrate that our tests give a better power / time tradeoff than competing approaches , and in some cases , better outright power than even the most expensive quadratic - time tests . </S>",
    "<S> this performance advantage is retained even in high dimensions , and in cases where the difference in distributions is not observable with low order statistics . </S>"
  ]
}