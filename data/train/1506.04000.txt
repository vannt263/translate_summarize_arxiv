{
  "article_text": [
    "gaussian process models are attractive for machine learning because of their flexible nonparametric nature . by combining a gp prior with different likelihoods ,",
    "a multitude of machine learning tasks can be tackled in a probabilistic fashion @xcite .",
    "there are three things to consider when using a gp model : approximation of the posterior function ( especially if the likelihood is non - gaussian ) , computation , storage and inversion of the covariance matrix , which scales poorly in the number of data ; and estimation ( or marginalization ) of the covariance function parameters .",
    "a multitude of approximation schemes have been proposed for efficient computation when the number of data is large .",
    "early strategies were based on retaining a sub - set of the data @xcite .",
    "@xcite introduced an inducing point approach , where the model is augmented with additional variables , and @xcite used these ideas in a variational approach .",
    "other authors have introduced approximations based on the spectrum of the gp @xcite , or which exploit specific structures within the covariance matrix @xcite , or by making unbiased stochastic estimates of key computations @xcite . in this work ,",
    "we extend the variational inducing point framework , which we prefer for general applicability ( no specific requirements are made of the data or covariance function ) , and because the variational inducing point approach can be shown to minimize the kl divergence to the posterior process @xcite .    to approximate the posterior function and covariance parameters , markov chain monte - carlo ( mcmc ) approaches provide asymptotically exact approximations . @xcite and @xcite examine schemes which iteratively sample the function values and covariance parameters .",
    "such sampling schemes require computation and inversion of the full covariance matrix at each iteration , making them unsuitable for large problems .",
    "computation may be reduced somewhat by considering variational methods , approximating the posterior using some fixed family of distributions @xcite , though many covariance matrix inversions are generally required .",
    "recent works @xcite have proposed inducing point schemes which can reduce the computation required substantially , though the posterior is assumed gaussian and the covariance parameters are estimated by ( approximate ) maximum likelihood .",
    "table [ tab : existing ] places our work in the context of existing variational methods for gps .",
    "this paper presents a general inference scheme , with the only concession to approximation being the variational inducing point assumption .",
    "non - gaussian posteriors are permitted through mcmc , with the computational benefits of the inducing point framework .",
    "the scheme jointly samples the inducing - point representation of the function with the covariance function parameters ; with sufficient inducing points our method approaches full bayesian inference over gp values and the covariance parameters .",
    "we show empirically that the number of required inducing points is substantially smaller than the dataset size for several real problems .    @ l c c c c @ & @xmath0 & * sparse * & * posterior * & * hyperparam . * + williams & barber@xcite ( also * ? ? ?",
    "* ; * ? ? ?",
    "* ) & probit / logit & & gaussian ( assumed ) & point estimate + @xcite & gaussian & & gaussian ( optimal ) & point estimate + @xcite & softmax & & gaussian ( assumed ) & point estimate + @xcite & any factorized & & mixture of gaussians & point estimate + @xcite & probit & & gaussian ( assumed ) & point estimate + this work & any factorized & & free - form & free - form +",
    "the model is set up as follows .",
    "we are presented with some data inputs @xmath1 and responses @xmath2 .",
    "a latent function is assumed drawn from a gp with zero mean and covariance function @xmath3 with ( hyper- ) parameters @xmath4 .",
    "consistency of the gp means that only those points with data are considered : the latent vector @xmath5 represents the values of the function at the observed points @xmath6 , and has conditional distribution @xmath7 , where @xmath8 is a matrix composed of evaluating the covariance function at all pairs of points in @xmath9 .",
    "the data likelihood depends on the latent function values : @xmath0 . to make a prediction for latent function value test points @xmath10 ,",
    "the posterior function values and parameters are integrated : @xmath11 in order to make use of the computational savings offered by the variational inducing point framework @xcite , we introduce additional input points to the function @xmath12 and collect the responses of the function at that point into the vector @xmath13 . with some variational posterior @xmath14 , new points are predicted similarly to the exact solution @xmath15 this makes clear that the approximation is a stochastic process in the same fashion as the true posterior : the length of the predictions vector @xmath16 is potentially unbounded , covering the whole domain .    to obtain a variational objective ,",
    "first consider the support of @xmath17 under the true posterior , and for @xmath5 under the approximation . in the above ,",
    "these points are subsumed into the prediction vector @xmath16 : from here we shall be more explicit , letting @xmath5 be the points of the process at @xmath9 , @xmath17 be the points of the process at @xmath12 and @xmath16 be a large vector containing all other points of interest here is considered finite but large enough to contain any point of interest for prediction .",
    "the infinite case follows @xcite , is omitted here for brevity , and results in the same solution . ] .",
    "all of the free parameters of the model are then @xmath18 , and using a variational framework , we aim to minimize the kullback - leibler divergence between the approximate and true posteriors : @xmath19 = \\!\\underset{q({\\mathbf f^\\star},{\\mathbf f},{\\mathbf u},{\\boldsymbol \\theta})}{-\\mathbb e}\\left[\\log\\frac{p({\\mathbf f^\\star}{\\,|\\,}{\\mathbf u},{\\mathbf f},{\\boldsymbol \\theta})p({\\mathbf u}{\\,|\\,}{\\mathbf f},{\\boldsymbol \\theta})p({\\mathbf f},{\\boldsymbol \\theta}{\\,|\\,}{\\mathbf y})}{p({\\mathbf f^\\star}{\\,|\\,}{\\mathbf u},{\\mathbf f},{\\boldsymbol \\theta})p({\\mathbf f}{\\,|\\,}{\\mathbf u},{\\boldsymbol \\theta})q({\\mathbf u},{\\boldsymbol \\theta})}\\right]\\ ] ] where the conditional distributions for @xmath16 have been expanded to make clear that they are the same under the true and approximate posteriors , and @xmath20 and @xmath21 have been omitted for clarity .",
    "straight - forward identities simplify the expression , @xmath22\\\\ & = -\\mathbb e_{q({\\mathbf f},{\\mathbf u},{\\boldsymbol \\theta})}\\left[\\log\\frac{p({\\mathbf u}{\\,|\\,}{\\boldsymbol \\theta})p({\\boldsymbol \\theta})p({\\mathbf y}{\\,|\\,}{\\mathbf f})}{q({\\mathbf u},{\\boldsymbol \\theta})}\\right ] + \\log p({\\mathbf y})\\ , , \\label{eq : bound } \\end{split}\\ ] ] resulting in the variational inducing - point objective investigated by @xcite , aside from the inclusion of @xmath4 .",
    "this can be rearranged to give the following informative expression @xmath23   } { c } \\right ] - \\log c + \\log p({\\mathbf y}).\\ ] ] here @xmath24 is an intractable constant which normalizes the distribution and is independent of @xmath25 . minimizing",
    "the kl divergence on the right hand side reveals that the optimal variational distribution is @xmath26 + \\log p({\\mathbf u}{\\,|\\,}{\\boldsymbol \\theta } ) + \\log p({\\boldsymbol \\theta } ) - \\log c. \\label{eq : qhat}\\ ] ] for general likelihoods , since the optimal distribution does not take any particular form , we intend to sample from it using mcmc .",
    "this is feasible using standard methods since it is computable up to a constant , using @xmath27 computations . to sample effectively ,",
    "the following are proposed .",
    "[ [ whitening - the - prior ] ] whitening the prior + + + + + + + + + + + + + + + + + + +    noting that the problem appears similar to a standard gp for @xmath17 , albeit with an interesting ` likelihood ' , we make use of an ancillary augmentation @xmath28 , with @xmath29 .",
    "this results in the optimal variational distribution @xmath30 + \\log p({\\mathbf v } ) + \\log p({\\boldsymbol \\theta } ) - \\log c \\label{eq : qhat_v}\\ ] ] previously @xcite this parameterization has been used with schemes which alternate between sampling the latent function values ( represented by @xmath31 or @xmath17 ) and the parameters @xmath4 .",
    "our scheme uses hmc across @xmath31 and @xmath4 jointly , whose effectiveness is examined throughout the experiment section .    [ [ quadrature ] ] quadrature + + + + + + + + + +    the first term in is the expected log - likelihood . in the case of factorization across the data - function pairs , this results in @xmath32 one - dimensional integrals .",
    "for gaussian or poisson likelihood these integrals are tractable , otherwise they can be approximated by gauss - hermite quadrature . given the current sample @xmath31 ,",
    "the expectations are computed w.r.t .",
    "@xmath33 , with : @xmath34 where the kernel matrices @xmath35 are computed similarly to @xmath8 , but over the pairs in @xmath36 respectively . from here",
    ", one can compute the expected likelihood and it is subsequently straight - forward to compute derivatives in terms of @xmath37 and @xmath38 .    [ [ reverse - mode - differentiation - of - cholesky ] ] reverse mode differentiation of cholesky + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    to compute derivatives with respect to @xmath4 and @xmath12 we use reverse - mode differentiation ( backpropagation ) of the derivative through the cholesky matrix decomposition , transforming @xmath39 into @xmath40 , and then @xmath41 .",
    "this is discussed by @xcite , and results in a @xmath42 operation ; an efficient cython implementation is provided in the supplement .",
    "a natural question is , what strategy should be used to select the inducing points @xmath12 ? in the original inducing point formulation @xcite , the positions @xmath12 were treated as parameters to be optimized .",
    "one could interpret them as parameters of the _ approximate prior covariance _ @xcite .",
    "the variational formulation @xcite treats them as parameters of the variational approximation , thus protecting from over - fitting as they form part of the variational posterior . in this work , since we propose a bayesian treatment of the model , we question whether it is feasible to treat @xmath12 in a bayesian fashion .",
    "since @xmath17 and @xmath12 are auxiliary parameters , the form of their distribution does not affect the marginals of the model .",
    "the term @xmath43 has been defined by the consistency with the gp in order to preserve the posterior - process interpretation above ( i.e. @xmath17 should be points on the gp ) , but we are free to choose @xmath44 . omitting dependence on @xmath4 for clarity , and choosing w.l.o.g .",
    "@xmath45 , the bound on the marginal likelihood , similarly to is given by @xmath46\\ , .",
    "\\label{eq : boundz}\\ ] ] the bound can be maximized w.r.t @xmath44 by noting that the term only appears inside a ( negative ) kl divergence : @xmath47 $ ] .",
    "substituting the optimal @xmath48 reduces to @xmath49\\right]\\,,\\ ] ] which can now be optimized w.r.t .",
    "since no entropy term appears for @xmath50 , the bound is maximized when the distribution becomes a dirac s delta . in summary , since we are free to choose a prior for @xmath12 which maximizes the amount of information captured by @xmath17 , the optimal distribution becomes @xmath51 .",
    "this formally motivates optimizing the inducing points @xmath12 .",
    "[ [ derivatives - for - mathbf - z ] ] derivatives for @xmath12 + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    for completeness we also include the derivative of the free form objective with respect to the inducing point positions . substituting the optimal distribution @xmath52 into to give @xmath53 and then differentiating we obtain @xmath54 \\right].\\ ] ]",
    "since we aim to draw samples from @xmath55 , evaluating this free form inducing point gradient using samples seems plausible but challenging .",
    "instead we use the following strategy . + * 1 . fit a gaussian approximation to the posterior . *",
    "we follow @xcite in fitting a gaussian approximation to the posterior .",
    "the positions of the inducing points are initialized using k - means clustering of the data .",
    "the values of the latent function are represented by a mean vector ( initialized randomly ) and a lower - triangular matrix @xmath56 forms the approximate posterior covariance as @xmath57 . for large problems",
    "( such as the mnist experiment ) , stochastic optimization using adadelta is used .",
    "otherwise , lbfgs is used .",
    "after a few hundred iterations with the inducing points positions fixed , they are optimized in free - form alongside the variational parameters and covariance function parameters .",
    "+ * 2 . initialize the model using the approximation . *",
    "having found a satisfactory approximation , the hmc strategy takes the optimized inducing point positions from the gaussian approximation .",
    "the initial value of @xmath31 is drawn from the gaussian approximation , and the covariance parameters are initialized at the ( approximate ) map value . + * 3 . tuning hmc * the hmc algorithm has two free parameters to tune , the number of leapfrog steps and the step - length .",
    "we follow a strategy inspired by @xcite , where the number of leapfrog steps is drawn randomly from 1 to @xmath58 , and bayesian optimization is used to maximize the expected square jump distance ( esjd ) , penalized by @xmath59 . rather than",
    "allow an adaptive ( but convergent ) scheme as @xcite , we run the optimization for 30 iterations of 30 samples each , and use the best parameters for a long run of hmc . +",
    "* 4 . run tuned hmc to obtain predictions * having tuned the hmc , it is run for several thousand iterations to obtain a good approximation to @xmath60 .",
    "the samples are used to estimate the integral in equation .",
    "the following section investigates the effectiveness of the proposed sampling scheme .",
    "this section illustrates the effectiveness of hamiltonian monte carlo in sampling from @xmath60 .",
    "as already pointed out , the form assumed by the optimal variational distribution @xmath60 in equation   resembles the joint distribution in a gp model with a non - gaussian likelihood .    for a fixed @xmath4 ,",
    "sampling @xmath31 is relatively straightforward , and this is possible to be done efficiently using hmc @xcite or elliptical slice sampling @xcite . a well tuned hmc has been reported to be extremely efficient in sampling the latent variables , and this motivates our effort into trying to extend this efficiency to the sampling of hyper - parameters as well .",
    "this is also particularly appealing due to the convenience offered by the proposed representation of the model .    the problem of drawing samples from the posterior distribution over @xmath61 has been investigated in detail in @xcite . in these works , it has been advocated to alternate between the sampling of @xmath31 and @xmath4 in a gibbs sampling fashion and condition the sampling of @xmath4 on a suitably chosen transformation of the latent variables .",
    "for each likelihood model , we compare efficiency and convergence speed of the proposed hmc sampler with a gibbs sampler where @xmath31 is sampled using hmc @xmath4 is sampled using the metropolis - hastings algorithm . to make the comparison fair , we imposed the mass matrix in hmc and the covariance in mh to be isotropic , and any parameters of the proposal",
    "were tuned using bayesian optimization . unlike in the proposed hmc sampler , for the gibbs sampler we did not penalize the objective function of the bayesian optimization for large numbers of leapfrog steps , as in this case hmc proposals on the latent variables are computationally cheaper than those on the hyper - parameters .",
    "we report efficiency in sampling from @xmath60 using effective sample size ( ess ) and time normalized ( tn)-ess . in the supplement we include convergence plots based on the potential scale reduction factor ( psrf ) computed based on ten parallel chains ; in these each chain is initialized from the vb solution and individually tuned using bayesian optimization .",
    "we first use the _ image _ dataset @xcite to investigate the benefits of the approach over a gaussian approximation , and to investigate the effect of changing the number of inducing points , as well as optimizing the inducing points under the gaussian approximation .",
    "the data are 18 dimensional : we investigated the effect of our approximation using both ard ( one lengthscale per dimension ) and an isotropic rbf kernel .",
    "the data were split randomly into 1000/1019 train / test sets ; the log predictive density over ten random splits is shown in figure [ fig : image ] .",
    "following the strategy outlined above , we fitted a gaussian approximation to the posterior , with @xmath12 initialized with k - means .",
    "figure [ fig : image ] investigates the difference in performance when @xmath12 is optimized using the gaussian approximation , compared to just using k - means for @xmath12 . whilst our strategy is not guaranteed to find the global optimum , it is clear that it improves the performance .",
    "the second part of figure [ fig : image ] shows the performance improvement of our sampling approach over the gaussian approximation .",
    "we drew 10,000 samples , discarding the first 1000 : we see a consistent improvement in performance once @xmath62 is large enough . for small @xmath62",
    ", the gaussian approximation appears to work very well .",
    "the supplement contains a similar figure for the case where a single lengthscale is shared : there , the improvement of the mcmc method over the gaussian approximation is smaller but consistent .",
    "we speculate that the larger gains for ard are due to posterior uncertainty in the lengthscale parameters , which is poorly represented by a point in the gaussian / map approximation .",
    "performance of the method on the _ image _ dataset , with one lengthscale per dimension . left , box - plots show performance for varying numbers of inducing points and @xmath12 strategies .",
    "optimizing @xmath12 using the gaussian approximation offers significant improvement over the k - means strategy .",
    "right : improvement of the performance of the gaussian approximation method , with the same inducing points .",
    "the method offers consistent performance gains when the number of inducing points is larger .",
    "the supplement contains a similar figure with only a single lengthscale.,title=\"fig : \" ]   performance of the method on the _ image _ dataset , with one lengthscale per dimension .",
    "left , box - plots show performance for varying numbers of inducing points and @xmath12 strategies .",
    "optimizing @xmath12 using the gaussian approximation offers significant improvement over the k - means strategy .",
    "right : improvement of the performance of the gaussian approximation method , with the same inducing points .",
    "the method offers consistent performance gains when the number of inducing points is larger .",
    "the supplement contains a similar figure with only a single lengthscale.,title=\"fig : \" ]    the ess and tn - ess are comparable between hmc and the gibbs sampler .",
    "in particular , for @xmath63 inducing points and the rbf covariance , ess and tn - ess for hmc are @xmath64 and @xmath65 and for the gibbs sampler are @xmath66 and @xmath67 .",
    "for the ard covariance , ess and tn - ess for hmc are @xmath68 and @xmath67 and for the gibbs sampler are @xmath69 and @xmath70 .",
    "convergence , however , seems to be faster for hmc , especially for the ard covariance ( see the supplement ) .",
    "we apply our methods to log gaussian cox processes @xcite : doubly stochastic models where the rate of an inhomogeneous poisson process is given by a gaussian process . the main difficulty for inference lies in that the likelihood of the gp requires an integral over the domain , which is typically intractable . for low dimensional problems , this integral can be approximated on a grid ; assuming that the gp is constant over the width of the grid leads to a factorizing poisson likelihood for each of the grid points . whilst some recent approaches allow for a grid - free approach @xcite , these usually require concessions in the model , such as an alternative link function , and do not approach full bayesian inference over the covariance function parameters .",
    "[ [ coal - mining - disasters ] ] coal mining disasters + + + + + + + + + + + + + + + + + + + + +    on the one - dimensional coal - mining disaster data .",
    "we held out 50% of the data at random , and using a grid of 100 points with 30 evenly spaced inducing points @xmath12 , fitted both a gaussian approximation to the posterior process with an ( approximate ) map estimate for the covariance function parameters ( variance and lengthscale of an rbf kernel ) . with gamma priors on the covariance parameters we ran our sampling scheme using hmc , drawing 3000 samples .",
    "the resulting posterior approximations are shown in figure [ fig : coal ] , alongside the true posterior using a sampling scheme similar to ours ( but without the inducing point approximation ) .",
    "the free - form variational approximation matches the true posterior closely , whilst the gaussian approximation misses important detail .",
    "the approximate and true posteriors over covariance function parameters are shown in the right hand part of figure [ fig : coal ] , there is minimal discrepancy in the distributions .    over 10 random splits of the data ,",
    "the average held - out log - likelihood was @xmath71 for the gaussian approximation and @xmath72 for the free - form mcmc variant ; the average difference was @xmath73 , and the mcmc variant was always better than the gaussian approximation .",
    "we attribute this improved performance to marginalization of the covariance function parameters .",
    "the posterior of the rates for the coal mining disaster data . left : posterior rates using our variational mcmc method and a gaussian approximation .",
    "data are shown as vertical bars .",
    "right : posterior samples for the covariance function parameters using mcmc .",
    "the gaussian approximation estimated the parameters as ( 12.06 , 0.55).,title=\"fig : \" ]   the posterior of the rates for the coal mining disaster data . left : posterior rates using our variational mcmc method and a gaussian approximation .",
    "data are shown as vertical bars .",
    "right : posterior samples for the covariance function parameters using mcmc .",
    "the gaussian approximation estimated the parameters as ( 12.06 , 0.55).,title=\"fig : \" ]    efficiency of hmc is greater than for the gibbs sampler ; ess and tn - ess for hmc are @xmath74 and @xmath75 and for the gibbs sampler are @xmath76 and @xmath77 .",
    "also , chains converge within few thousand iterations for both methods , although convergence for hmc is faster ( see the supplement ) .    [",
    "[ pine - saplings ] ] pine saplings + + + + + + + + + + + + +    the advantages of the proposed approximation are prominent as the number of grid points become higher , an effect emphasized with increasing dimension of the domain .",
    "we fitted a similar model to the above to the pine sapling data @xcite .",
    "[ fig : pines ]    pine sapling data . from left to right : reported locations of pine saplings ; posterior mean intensity on a 32x32 grid using full mcmc ; posterior mean intensity on a 32x32 grid ( with sparsity using 225 inducing points ) , posterior mean intensity on a 64x64 grid ( using 225 inducing points ) .",
    "the supplement contains a larger version of this figure.,scaledwidth=97.0% ]    we compared the sampling solution obtained using 225 inducing points on a 32 x 32 grid to the gold standard full mcmc run with the same prior and grid size .",
    "figure [ fig : pines ] shows that the agreement between the variational sampling and full sampling is very close .",
    "however the variational method was considerably faster . using a single core on a desktop computer required @xmath78 seconds to obtain 1 effective sample for a well tuned variational method whereas it took @xmath79 seconds for well tuned full mcmc .",
    "this effect becomes even larger as we increase the resolution of the grid to 64 x 64 , which gives a better approximation to the underlying smooth function as can be seen in figure [ fig : pines ] .",
    "it took @xmath80 seconds to obtain one effective sample for the variational method , but now gold standard mcmc comparison was computationally extremely challenging to run for even a single hmc step .",
    "this is because it requires linear algebra operations using @xmath81 flops with @xmath82 .",
    "to do multi - class classification with gaussian processes , one latent function is defined for each of the classes .",
    "the functions are defined a - priori independent , but covary a posteriori because of the likelihood .",
    "@xcite studies a sparse variational approximation to the softmax multi - class likelihood restricted to a gaussian approximation . here ,",
    "following @xcite , we use a robust - max likelihood . given a vector @xmath83 containing @xmath84 latent functions evaluated at the point @xmath85 , the probability that the label takes the integer value @xmath86 is @xmath87 as @xcite discuss , the ` soft ' probit - like behaviour is recovered by adding a diagonal ` nugget ' to the covariance function . in this work ,",
    "@xmath88 was fixed to @xmath89 , though it would also be possible to treat this as a parameter for inference .",
    "the expected log - likelihood is @xmath90 = p \\log ( \\epsilon ) + ( 1-p)\\log ( \\epsilon/(k-1))$ ] , where @xmath91 is the probability that the labelled function is largest , which is computable using one - dimensional quadrature .",
    "an efficient cython implementation is contained in the supplement .",
    "[ [ toy - example ] ] toy example + + + + + + + + + + +    to investigate the proposed posterior approximation for the multivariate classification case , we turn to the toy data shown in figure [ fig : multiclass_toy ] .    0.3   a toy multiclass problem . left : the gaussian approximation , colored points show the simulated data , lines show posterior probability contours at 0.3 , 0.95 , 0.99 . inducing points positions shows as black points .",
    "middle : the free form solution with 10,000 posterior samples .",
    "the free - form solution is more conservative ( the contours are smaller ) .",
    "right : posterior samples for @xmath31 at the same position but across different latent functions .",
    "the posterior exhibits strong correlations and edges.,title=\"fig : \" ]    0.3   a toy multiclass problem .",
    "left : the gaussian approximation , colored points show the simulated data , lines show posterior probability contours at 0.3 , 0.95 , 0.99 . inducing points positions",
    "shows as black points .",
    "middle : the free form solution with 10,000 posterior samples .",
    "the free - form solution is more conservative ( the contours are smaller ) .",
    "right : posterior samples for @xmath31 at the same position but across different latent functions .",
    "the posterior exhibits strong correlations and edges.,title=\"fig : \" ]    0.35   a toy multiclass problem .",
    "left : the gaussian approximation , colored points show the simulated data , lines show posterior probability contours at 0.3 , 0.95 , 0.99 . inducing points positions",
    "shows as black points .",
    "middle : the free form solution with 10,000 posterior samples .",
    "the free - form solution is more conservative ( the contours are smaller ) .",
    "right : posterior samples for @xmath31 at the same position but across different latent functions .",
    "the posterior exhibits strong correlations and edges.,title=\"fig : \" ]    we drew 750 data points from three gaussian distributions .",
    "the synthetic data was chosen to include non - linear decision boundaries and ambiguous decision areas .",
    "figure [ fig : multiclass_toy ] shows that there are differences between the variational and sampling solutions , with the sampling solution being more conservative in general ( the contours of 95% confidence are smaller ) .",
    "as one would expect at the decision boundary there are strong correlations between the functions which could not be captured by the gaussian approximation we are using . note the movement of inducing points away from k - means and towards the decision boundaries .",
    "efficiency of hmc and the gibbs sampler is comparable . in the rbf case ,",
    "ess and tn - ess for hmc are @xmath92 and @xmath93 and for the gibbs sampler are @xmath94 and @xmath95 . in the ard case ,",
    "ess and tn - ess for hmc are @xmath96 and @xmath97 and for the gibbs sampler are @xmath98 and @xmath99 . for both cases ,",
    "the gibbs sampler struggles to reach convergence even though the average acceptance rates are similar to those recommended for the two samplers individually .    [",
    "[ mnist ] ] mnist + + + + +    the mnist dataset is a well studied benchmark with a defined training / test split .",
    "we used 500 inducing points , initialized from the training data using k - means .",
    "a gaussian approximation was optimized using minibatch - based optimization over the means and variances of @xmath100 , as well as the inducing points and covariance function parameters .",
    "the accuracy on the held - out data was 98.04% , significantly improving on previous approaches to to classify these digits using gp models .    for binary classification",
    ", @xcite reported that their gaussian approximation resulted in movement of the inducing point positions toward the decision boundary .",
    "the same effect appears in the multivariate case , as shown in figure [ fig : sixes ] , which shows three of the 500 inducing points used in the mnist problem .",
    "the three examples were initialized close to the many six digits , and after optimization have moved close to other digits ( five and four ) .",
    "the last example still appears to be a six , but has moved to a more ` unusual ' six shape , supporting the function at another extremity .",
    "similar effects are observed for all inducing - point digits .",
    "left : three k - means centers used to initialize the inducing point positions .",
    "center : the positions of the same inducing points after optimization .",
    "right : difference .",
    ", title=\"fig:\",scaledwidth=30.0% ] left : three k - means centers used to initialize the inducing point positions",
    ". center : the positions of the same inducing points after optimization .",
    "right : difference .",
    ", title=\"fig:\",scaledwidth=30.0% ] left : three k - means centers used to initialize the inducing point positions .",
    "center : the positions of the same inducing points after optimization .",
    "right : difference .",
    ", title=\"fig:\",scaledwidth=30.0% ]    having optimized the inducing point positions with the approximate @xmath101 , and estimate for @xmath4 , we used these optimal inducing points to draw samples from @xmath31 and @xmath4 .",
    "this did not result in an increase in accuracy , but did improve the log - density on the test set from -0.068 to -0.064 . evaluating",
    "the gradients for the sampler took approximately 0.4 seconds on a desktop machine , and we were easily able to draw 1000 samples .",
    "this dataset size has generally be viewed as challenging in the gp community and consequently there are not many published results to compare with .",
    "one recent work @xcite reports a 94.05% accuracy using variational inference and a gp latent variable model .",
    "we have presented an inference scheme for general gp models .",
    "the scheme significantly reduces the computational cost whilst approaching exact bayesian inference , making minimal assumptions about the form of the posterior .",
    "the improvements in accuracy in comparison with the gaussian approximation of previous works has been demonstrated , as has the quality of the approximation to the hyper - parameter distribution .",
    "our mcmc scheme was shown to be effective for several likelihoods , and we note that the automatic tuning of the sampling parameters worked well over hundreds of experiments .",
    "this paper shows that mcmc methods are feasible for inference in large gp problems , addressing the unfair sterotype of ` slow ' mcmc .",
    "convergence of the samplers on the image dataset is reported in fig .",
    "[ fig : convergence : image ] and shows the evolution of the psrf for the twenty slowest parameters for hmc and the gibbs sampler in the case of rbf and ard covariances .",
    "the figure shows that hmc consistently converges faster than the gibbs sampler for both covariances , even when the ess of the slowest variable is comparable .",
    "image dataset - evolution of the psrf of the twenty least efficient parameter traces for hmc ( blue ) and the gibbs sampler ( red ) . left panel : rbf case - minimum ess and tn - ess for hmc are @xmath64 and @xmath65 and for the gibbs sampler are @xmath66 and @xmath67 .",
    "right panel : ard case - minimum ess and tn - ess for hmc are @xmath68 and @xmath67 and for the gibbs sampler are @xmath69 and @xmath70 . , title=\"fig:\",scaledwidth=25.0% ]   image dataset - evolution of the psrf of the twenty least efficient parameter traces for hmc ( blue ) and the gibbs sampler ( red ) . left panel : rbf case - minimum ess and tn - ess for hmc are @xmath64 and @xmath65 and for the gibbs sampler are @xmath66 and @xmath67 .",
    "right panel : ard case - minimum ess and tn - ess for hmc are @xmath68 and @xmath67 and for the gibbs sampler are @xmath69 and @xmath70 .",
    ", title=\"fig:\",scaledwidth=25.0% ]       coal dataset - evolution of the psrf of the twenty least efficient parameter traces for hmc ( blue ) and the gibbs sampler ( red ) .",
    "minimum ess and tn - ess for hmc are @xmath74 and @xmath75 and for the gibbs sampler are @xmath76 and @xmath77",
    ". , scaledwidth=25.0% ]    convergence of the samplers on the toy multi - class dataset is reported in fig .",
    "[ fig : convergence : multiclass ] .",
    "hmc converges much faster than the gibbs sampler even though efficiency measured through ess is comparable .",
    "multiclass dataset - evolution of the psrf of the twenty least efficient parameter traces for hmc ( blue ) and the gibbs sampler ( red ) . left panel : rbf case - minimum ess and tn - ess for hmc are @xmath92 and @xmath93 and for the gibbs sampler are @xmath94 and @xmath95 .",
    "right panel : ard case - minimum ess and tn - ess for hmc are @xmath96 and @xmath97 and for the gibbs sampler are @xmath98 and @xmath99 . , title=\"fig:\",scaledwidth=25.0% ]   multiclass dataset - evolution of the psrf of the twenty least efficient parameter traces for hmc ( blue ) and the gibbs sampler ( red ) . left panel : rbf case - minimum ess and tn - ess for hmc are @xmath92 and @xmath93 and for the gibbs sampler are @xmath94 and @xmath95 .",
    "right panel : ard case - minimum ess and tn - ess for hmc are @xmath96 and @xmath97 and for the gibbs sampler are @xmath98 and @xmath99 . , title=\"fig:\",scaledwidth=25.0% ]      a larger version of figure [ fig : pines ] .",
    "top right : gold standard mcmc 32x32 grid .",
    "bottom left : variational mcmc 32x32 grid .",
    "bottom right : variational mcmc 64x64 grid , with 225 inducing points in the non - exact case.,scaledwidth=97.0% ]"
  ],
  "abstract_text": [
    "<S> gaussian process ( gp ) models form a core part of probabilistic machine learning . </S>",
    "<S> considerable research effort has been made into attacking three issues with gp models : how to compute efficiently when the number of data is large ; how to approximate the posterior when the likelihood is not gaussian and how to estimate covariance function parameter posteriors . </S>",
    "<S> this paper simultaneously addresses these , using a variational approximation to the posterior which is sparse in support of the function but otherwise free - form . </S>",
    "<S> the result is a hybrid monte - carlo sampling scheme which allows for a non - gaussian approximation over the function values and covariance parameters simultaneously , with efficient computations based on inducing - point sparse gps . </S>",
    "<S> code to replicate each experiment in this paper will be available shortly . </S>"
  ]
}