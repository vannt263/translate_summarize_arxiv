{
  "article_text": [
    "here we present an algorithm to compute projections of channels onto exponential families of fixed interactions .",
    "the decomposition is geometrical , and it is based on the idea that , rather than joint distributions , the quantities we work with are channels , or conditionals ( or markov kernels , stochastic kernels , transition kernels , stochastic maps ) .",
    "our algorithm can be considered a channel version of the iterative scaling of ( joint ) probability distributions , presented in @xcite .",
    "exponential and mixture families ( of joints and of channels ) have a duality property , shown in section [ fam ] . by fixing some marginals",
    ", one determines a mixture family . by fixing ( boltzmann - type ) interactions",
    ", one determines an exponential family .",
    "these two families intersect in a single point , which means that ( theorem [ dualmk ] ) _ there exists a unique element which has the desired marginals and the desired interactions_.    as a consequence , theorem [ dualmk ] translates projections onto exponential families ( which are generally hard to compute ) to projections onto fixed - marginals mixture families ( which can be approximated by an iterative procedure ) .",
    "section [ algo ] explains how this is done .",
    "projections onto exponential families are becoming more and more important in the definition of measures of statistical interaction , complexity , synergy , and related quantities .",
    "in particular , the algorithm can be used to compute decompositions of mutual information , as for example the ones defined in @xcite and @xcite , and it was indeed used to compute all the numerical examples in @xcite .",
    "another application of the algorithm is explicit computations of complexity measure as treated in @xcite , @xcite , and @xcite .",
    "examples of both applications can be found in section [ applic ] .",
    "for all the technical details about the iterative scaling algorithm in its traditional version , we refer the interested reader to chapters 3 and 5 of @xcite .",
    "all proofs can be found in the appendix .",
    "we take the same definitions and notations as in @xcite , except that we let the output be multiple .",
    "more precisely , we consider a set of @xmath0 input nodes @xmath1 , taking values in the sets @xmath2 , and a set of @xmath3 output nodes @xmath4 , taking values in the sets @xmath5 .",
    "we write the input globally as @xmath6 , and the output globally as @xmath7 .",
    "we denote by @xmath8 the set of real functions on @xmath9 , and by @xmath10 the set of probability measures on @xmath11 .",
    "let @xmath12 and @xmath13 .",
    "we call @xmath14 the space of functions which only depend on @xmath15 and @xmath16 : @xmath17    we can model the channel from @xmath11 to @xmath9 as a markov kernel @xmath18 , that assigns to each @xmath19 a probability measure on @xmath9 ( for a detailed treatment , see @xcite ) . here",
    "we will consider only finite systems , so we can think of a channel simply as a transition matrix ( or stochastic matrix ) , whose rows sum to one .",
    "@xmath20 the space of channels from @xmath11 to @xmath9 will be denoted by @xmath21 .",
    "we will denote by @xmath11 and @xmath9 also the corresponding random variables , whenever this does not lead to confusion .",
    "conditional probabilities define channels : if @xmath22 and the marginal @xmath23 is strictly positive , then @xmath24 is a well - defined channel .",
    "viceversa , if @xmath25 , given @xmath26 we can form a well - defined joint probability : @xmath27    to extend the notion of divergence from probability distributions to channels , we need an `` input distribution '' :    let @xmath26 , let @xmath28 . then : @xmath29    let @xmath30 be joint probability distributions on @xmath31 , and let @xmath32 be the kl - divergence . then the following `` chain rule '' holds : @xmath33",
    "suppose we have a family @xmath34 of channels , and a channel @xmath18 that may not be in @xmath34 .",
    "then we can define the `` distance '' between @xmath18 and @xmath34 in terms of @xmath35 .",
    "let @xmath36 be an input distribution .",
    "the divergence between a channel @xmath18 and a family of channels @xmath34 is given by : @xmath37 if the minimum is uniquely realized , we call the channel @xmath38 the _ ri - projection _ of @xmath18 on @xmath34 ( and simply `` an '' ri - projection if it is not unique )",
    ".    the families considered here are of two types , dual to each other : linear and exponential .",
    "for both cases , we take the closures , so that the minima defined above always exist .",
    "a _ mixture family _ of @xmath21 is a subset of @xmath21 defined by one or several affine equations , i.e. , the locus of the @xmath18 which satisfy a ( finite ) system of equations in the form : @xmath39 for some functions @xmath40 , and some constants @xmath41 .",
    "[ [ example . ] ] example .",
    "+ + + + + + + +    consider a channel @xmath42 .",
    "we can form the marginal : @xmath43 the channels @xmath44 such that @xmath45 form a mixture family , defined by the system of equations ( for all @xmath46 , @xmath47 ) : @xmath48 where the function @xmath49 is equal to 1 for @xmath50 , and zero for any other case .",
    "more in general , let @xmath51 be a ( finite - dimensional ) linear subspace of @xmath52 , and let @xmath25 . then : @xmath53 is a mixture family , which we call _ generated by @xmath18 and @xmath51_.    a ( closed ) _ exponential family _ of @xmath21 is ( the closure of ) a subset of @xmath21 of channels in the form : @xmath54 where @xmath55 satisfies affine constraints , @xmath18 is fixed , and : @xmath56 so that the channel is correctly normalized .",
    "this is a sort of multiplicative equivalent of mixture families , as the exponent satisfies constraints similar to .",
    "[ [ example.-1 ] ] example .",
    "+ + + + + + + +    let @xmath51 be a ( finite - dimensional ) linear subspace of @xmath52 , and let @xmath25 .",
    "then the closure : @xmath57 is an exponential family , which again we call _ generated by @xmath18 and @xmath51_.    this family is in some sense `` dual '' to the family in . the duality is expressed more precisely by the following result .    [ dualmk ] let @xmath51 be a subspace of @xmath52 .",
    "let @xmath26 be strictly positive .",
    "let @xmath58 be a strictly positive `` reference '' channel .",
    "let @xmath59 and @xmath60 . for @xmath61 ,",
    "the following conditions are equivalent :    1 .",
    "2 .   @xmath63 , and @xmath64 .",
    "3 .   @xmath65 , and @xmath66 .    in particular , @xmath67 is unique , and it is exactly @xmath68 .",
    "geometrically , we are saying that @xmath69 , the ri - projection of @xmath18 on @xmath34 .",
    "we call the mapping @xmath70 the _ ri - projection operator _ , and the mapping @xmath71 the _ i - projection operator _ these are the channel equivalent of the i - projections introduced in @xcite and generalized in @xcite .",
    "the result is illustrated in figure [ fig : dualmk ] .    .",
    "the point @xmath67 at the intersection minimizes on @xmath34 the distance from @xmath18 , and minimizes on @xmath72 the distance from @xmath73 . ]    as suggested by figure [ fig : dualmk ] , i- and ri - projections on exponential families satisfy a pythagoras - type equality .",
    "for any @xmath74 , with @xmath34 exponential family : @xmath75 this statement follows directly from the analogous statement for probability distribution found in @xcite , after applying the chain rule .",
    "the algorithm can be considered as a channel equivalent of the iterative scaling procedure for joint distributions , which can be found in chapter 5 of @xcite . translated into our language , that theorem says the following :    [ jointit ] let @xmath76 be mixture families of joint distributions with nonempty intersection @xmath51 .",
    "denote by @xmath77 the @xmath78-projection of a joint @xmath79 onto the family @xmath80 .",
    "consider the sequence that starts at @xmath81 and is defined iteratively by : @xmath82    then @xmath83 converges , and the limit point is the @xmath78-projection of @xmath81 onto @xmath51 , i.e. if we call : @xmath84 then @xmath85 , and for any @xmath86 : @xmath87    our result depends on the theorem above , in the following way .",
    "we define a marginal procedure for channels , which in general depends on the choice of an input distribution .",
    "we define mixture families of channels with fixed marginals in a way compatible with the equivalent for joints .",
    "we then define scalings of channels , and prove that they give the desired result at the joint level .",
    "this makes it possible to translate the statement of theorem [ jointit ] to an analogous statement for channels , theorem [ convergence ] .",
    "unless otherwise stated , all the input distributions here will be assumed strictly positive .",
    "all our proofs can be found in the appendix .",
    "consider an input distribution @xmath26 .",
    "let @xmath88 ,",
    "j\\subseteq [ m ] , j\\ne\\emptyset$ ] .",
    "we define the _ marginal operator _ for channels as : @xmath89 given the input @xmath36 .",
    "[ jeq ] defined as above , @xmath90 is exactly the conditional probability for the marginal @xmath91 . in other words",
    ", @xmath92 has marginal @xmath90 if and only if @xmath93 has marginal @xmath91 .",
    "consider an input distribution @xmath26 .",
    "let @xmath88 , j\\subseteq [ m]$ ] , @xmath94 .",
    "we define the mixture families @xmath95 as : @xmath96 where the @xmath97 are prescribed channel marginals .",
    "analogously , let @xmath98 be a probability distribution in @xmath99 .",
    "we define the mixture families : @xmath100    proposition [ jeq ] says that , for any @xmath25 , for any ( strictly positive ) @xmath26 , and for any @xmath88 , j\\subseteq [ m]$ ] : @xmath101    [ presc ] @xmath95 is exactly the set @xmath102 of equation , where as @xmath51 we take the space @xmath14 of functions which only depend on the nodes in @xmath103 .    just as in @xcite , the i - projections for single marginals can be obtained by scaling . for joint distributions",
    "the scaling is done in this way : if @xmath104 is a `` prescribed '' marginal , then : @xmath105 will have the prescribed marginals , and even be the i - projection of @xmath36 on @xmath106 .",
    "i.e. , @xmath107 , and : @xmath108 for the proof , see chapter 3 and section 5.1 of @xcite .    in the case of channels ,",
    "the scaling is instead done in two steps .",
    "we define the ( unnormalized ) _ @xmath109-scaling _ as the operator @xmath110 , mapping @xmath18 to : @xmath111    we have that @xmath112 is _ not _ an element of @xmath113 , as in general it is not even in @xmath21 ( i.e. a correctly normalized channel ) .",
    "however , at the joint level this corresponds exactly to the joint scaling :    [ jlevel ] let @xmath26 , @xmath25 , and @xmath114 . then : @xmath115    this implies that @xmath116 is the @xmath78-projection of @xmath117 to the family @xmath118 .",
    "we define the _ normalized @xmath109-scaling _ as the operator @xmath119 , mapping @xmath18 to : @xmath120 where : @xmath121    at the joint level , this corresponds to scaling of the input in the following way :    [ inscale ] let @xmath26 , @xmath25 , and @xmath114 .",
    "then : @xmath122}^{p } \\operatorname{\\sigma}_{ij}^{p \\bar k } pk\\;.\\ ] ]    this implies that @xmath123 is the @xmath78-projection of @xmath124 to the family with prescribed input @xmath23 . for brevity ,",
    "let s call this family @xmath125}(p)$ ] .",
    "now @xmath126 is an element of @xmath21 , but still _ not _ of @xmath113 .",
    "however , if we iterate the operator @xmath127 , the resulting sequence will converge to the projector on @xmath113 .",
    "more in general , the following result holds :    [ convergence ] for @xmath128 , let @xmath129 $ ] be subsets of @xmath130 $ ] and @xmath131 $ ] be nonempty subsets of @xmath132 $ ] .",
    "take an input distribution @xmath26 and a channel @xmath133 .",
    "define the mixture families of prescribed marginals : @xmath134 and their intersection , which is also a mixture family ( nonempty , as it contains at least @xmath135 ) : @xmath136 choose a ( different ) channel @xmath137 and consider the sequence of normalized scalings starting at @xmath138 and defined iteratively by : @xmath139 then :    * @xmath140 converges to a limit channel : @xmath141 * the limit @xmath142 is the @xmath78-projection of @xmath138 on @xmath72 , i.e. @xmath143 and : @xmath144    the proof can be found in the appendix .    to apply the theorem [ convergence ] in our algorithm , we choose as initial channel @xmath138 exactly the reference channel @xmath73 of theorem [ dualmk ] , usually the uniform channel . as @xmath135 we take exactly the `` prescription channel '' @xmath18 of theorem [ dualmk ] , i.e. the channel which has the desired marginals .",
    "the result of the iterative scaling will be the ri - projection of @xmath18 on the desired exponential family .",
    "the algorithm presented here permits to compute the decompositions of mutual information between inputs and outputs in @xcite and @xcite .",
    "we give here examples of computations of _ pairwise synergy _ as an ri - projection for channels , as described in @xcite .",
    "it is not within the scope of this article to motivate this measure , we rather want to show how it can be computed .    let @xmath18 be a channel from @xmath145 to @xmath9 .",
    "let @xmath26 be a strictly positive input distribution .",
    "we define in @xcite the synergy of @xmath18 as : @xmath146 where @xmath147 is the ( closure of the ) family of channels in the form : @xmath148 where : @xmath149 and : @xmath150 according to theorem [ dualmk ] , the ri - projection of @xmath18 on @xmath147 is the unique point @xmath67 of @xmath147 which has all the prescribed marginals : @xmath151 and can therefore be computed by iterative scaling , either of the joint distribution ( as it is traditionally done , see @xcite ) , or of the channels ( our algorithm ) .    here",
    "we present a comparison of the two algorithms , implemented similarly and in the same language ( mathematica ) .",
    "the red dots represent our ( channel ) algorithm , and the blue dots represent the joint rescaling algorithm .",
    "for the easiest channels ( see figure [ fig : xor ] ) , both algorithm converge instantly .",
    "a more interesting example is a randomly generated channel ( figure [ fig : rand ] ) , in which both method need 5 - 10 iterations to get to the desired value .",
    "however , the channel method is slightly faster .",
    "the most interesting example is the synergy of the and gate , which should be zero according to the procedure @xcite . in that article",
    ", we mistakenly wrote a different value , that here we would like to correct ( it is zero ) .",
    "the convergence to zero is very slow , of the order of @xmath152 ( figure [ fig : and ] ) .",
    "it is clearly again slightly faster for the channel method in terms of iterations .    , the joint method ( blue ) proportionally to @xmath153 . ]",
    "it has to be noted , however , that rescaling a channel requires more elementary operations than rescaling a joint distribution .",
    "because of this , one single iteration with our method takes longer than with the joint method .",
    "( as explained in section [ algo ] , a scaling for the channel corresponds to two scalings for the joint . ) in the end , despite the need of fewer iterations , the total computation time of a projection with our algorithm can be longer ( depending on the particular problem ) .",
    "for example , again for the synergy of the and gate , we can plot the computation time as a function of the accuracy ( distance to actual value ) , down to @xmath154 .",
    "the results are shown in figure [ fig : comp ] .        to get to the same accuracy , though",
    ", the channel approach used less iterations . in summary ,",
    "our algorithm is better in terms of iteration complexity , but generally worse in terms of computing time .",
    "iterative scaling can also be used to compute measures of complexity , as defined in @xcite , @xcite , and in section 6.9 of @xcite . for simplicity ,",
    "consider two inputs @xmath155 , two outputs @xmath156 and a generic channel between them .",
    "in general , any sort of interaction is possible , which in terms of graphical models ( see @xcite ) can be represented by diagrams such as those in figure [ fig : graphs1 ] .",
    ".5   and @xmath157 are indeed correlated , but only indirectly , via the inputs .",
    "b ) the graphical model corresponding to a non - complex system .",
    ", title=\"fig : \" ] [ fig : graph2 ]    .45   and @xmath157 are indeed correlated , but only indirectly , via the inputs .",
    "b ) the graphical model corresponding to a non - complex system .",
    ", title=\"fig : \" ] [ fig : gp1 ]    any line in the graph indicates an interaction between the nodes .",
    "in @xcite the outputs are assumed to be conditionally independent , i.e. they do not directly interact ( or , their interaction can be _ explained away _ by conditioning on the inputs ) .",
    "in this case the graph looks like figure [ fig : graphs1]a , and the maginals to preserve are those of the family of pairs @xmath158 , @xmath159 with : @xmath160 , @xmath161 , @xmath162 , @xmath163 .",
    "suppose now that @xmath156 correspond to @xmath155 at a later time .",
    "in this case it is natural to assume that the system is not complex if @xmath164 does not depend ( directly ) on @xmath165 , and @xmath157 does not depend ( directly ) on @xmath166 .",
    "intuitively , in this case `` the whole is exactly the sum of its parts '' . in terms of graphical models",
    ", this means that our system is represented by figure [ fig : graphs1]b , meaning that the subsets of nodes in question are now only the ones given by @xmath167 , @xmath161 , @xmath168 , @xmath163 .",
    "these channels ( or joints ) form an exponential family ( see @xcite ) which we call @xmath169 .",
    ".5 a , with correlation between the outputs .",
    "b ) the non - complex model of figure [ fig : graphs1]b , with correlation between the outputs . , title=\"fig : \" ] [ fig : graph32 ]    .45 a , with correlation between the outputs .",
    "b ) the non - complex model of figure [ fig : graphs1]b , with correlation between the outputs . , title=\"fig : \" ] [ fig : gp2 ]    suppose now , though , that the outputs are not conditionally independent anymore , because of some `` noise '' ( see @xcite and @xcite ) .",
    "this way the interaction structure would look like figure [ fig : graphs3 ] , i.e. the `` complete '' subset given by @xmath170 with @xmath171 and @xmath172 .",
    "in particular , a non - complex but `` noisy '' system would be represented by figure [ fig : graphs3]b , and have subsets of nodes given by the pairs @xmath158 , @xmath173 with : @xmath167 , @xmath161 , @xmath168 , @xmath163 , @xmath174 , @xmath175 .",
    "such channels form again an exponential family , which we call @xmath176 .",
    "we would like now to have a measure of complexity for a channel ( or joint ) . in @xcite , the measure of complexity",
    "is defined as the divergence from the family @xmath169 represented in figure [ fig : graphs1]b .",
    "we will call such a measure @xmath177 . in case of noise , however , it is argued in @xcite and @xcite that the divergence should be computed from the family @xmath176 represented in [ fig : graphs3]b ( for example , as written in the cited papers , because such a complexity measure should be required to be upper bounded by the mutual information between @xmath11 and @xmath9 ) .",
    "we will call such a measure @xmath178 .",
    "both divergences can be computed with our algorithm . as an example",
    ", we have considered the following channel : @xmath179 with : @xmath180 here @xmath181 represents a node of `` unknown input noise '' that adds correlation between the outputs ( of unknown form ) when if it is not observed .",
    "we have chosen @xmath182 and @xmath183 , and a uniform input probability @xmath36 .",
    "after marginalizing out @xmath181 ( obtaining then an element of the type of figure [ fig : graphs3]a ) , we can compute the two divergences :    * @xmath184 . *",
    "@xmath185 .",
    "this could indicate that @xmath177 is incorporating part the correlation of the output nodes due to the `` noise '' , and therefore probably overestimating the complexity , at least in this case .",
    "one could nevertheless also argue that @xmath178 can underestimate complexity , as we can see in the following `` control '' example .",
    "consider the channel : @xmath186 with : @xmath187 which is represented by the graph in figure [ fig : graphs1]a . if the difference between @xmath177 and @xmath178 were just due to the noise , then for our new channel @xmath188 and @xmath189 should be equal .",
    "this is not the case :    * @xmath190 . * @xmath191 .",
    "the divergences are still different .",
    "this means that there is an element @xmath192 in @xmath176 , which does _ not _ lie in @xmath169 , for which : @xmath193    the difference is this time smaller , which could mean that noise still does play a role , but in rigor it is hard to say , since none of these quantities is linear , and divergences do not satisfy a triangular inequality .",
    "we do not want to argue here in favor or against any of these measures .",
    "we would rather like to point out that such considerations can be done mostly after explicit computations , which can be done with iterative scaling .",
    "5    csiszr , i. and shields , p. c. . , 1(4):417528 , 2004    goodman , j. . : 916 , 2002 .",
    "olbrich , e. , bertschinger , n. , and rauh , j. . , 17(5):35013517 , 2015    perrone , p. and ay , n. .",
    ", 35(2 ) , 2016 .    ay , n. . ,",
    "17 , 24322458 , 2015 .",
    "oizumi , m. , tsuchiya , n. , and amari , s. , 2015 .",
    "amari , s. springer , 2016 .",
    "kakihara , y. .",
    "world scientific , 1999 .",
    "csiszr , i. .",
    ", 3:146158 , 1975 .",
    "csiszr , i. and mat , f. .",
    ", 49:14741490 , 2003 .",
    "amari , s. .",
    ", 47(5):17011709 , 2001 .",
    "lauritzen , s. l. .",
    "oxford , 1996 .",
    "williams , p. l. and beer , r. d. . , 2010 .",
    "amari , s. and nagaoka , h. . , 1982 .",
    "amari , s. and nagaoka , h. .",
    "oxford , 1993 .",
    "@xmath194 : choose a basis @xmath195 of @xmath51 .",
    "define the map @xmath196 , with : @xmath197 and : @xmath198 then : @xmath199 + \\operatorname{\\mathbb{e}}_p[\\log z_\\theta]\\;.\\ ] ] deriving ( where @xmath200 is w.r.t .",
    "@xmath201 ) : @xmath202 + \\operatorname{\\mathbb{e}}_p\\left [ \\dfrac{\\operatorname{\\partial}_j z_\\theta}{z_\\theta } \\right]\\;.\\ ] ] the term in the last brackets is equal to : @xmath203 so that now reads : @xmath204 + \\operatorname{\\mathbb{e}}_{pk_\\theta}[f_j]\\;.\\ ] ] this quantity is equal to zero for every @xmath205 if and only if @xmath206 . now if @xmath207 is a minimizer , it satisfies , and so @xmath206 .",
    "viceversa , suppose @xmath206 , so that it satisfies for every @xmath205 . to prove that it is a global minimizer , we look at the hessian : @xmath208 this is precisely the covariance matrix of the joint probability measure @xmath209 , which is positive definite .",
    "@xmath210 : for every @xmath211 , we have : @xmath212\\;.   \\end{aligned}\\ ] ] if @xmath213 , then : @xmath214 = d_p(m||k ' ) + \\operatorname{\\mathbb{e}}_{pm}\\left [ \\log\\dfrac{k'}{k_0 } \\right]\\;.\\ ] ] by definition of @xmath34 , the logarithm in the last brackets belongs to @xmath51 , and since @xmath211 : @xmath215 = \\operatorname{\\mathbb{e}}_{pk}\\left [ \\log\\dfrac{k'}{k_0 } \\right ] = \\operatorname{\\mathbb{e}}_{pk'}\\left [ \\log\\dfrac{k'}{k_0 } \\right]\\;.\\ ] ] inserting in : @xmath216 = d_p(m||k ' ) + d_p(k'||k_0)\\;.\\ ] ] since @xmath217 , shows that @xmath67 is a minimizer . since @xmath218 is strictly convex in the first argument",
    ", its minimizer is unique .      for @xmath55 in @xmath14 :",
    "@xmath220 = \\sum_{x , y } p(x)\\,k(x;y)\\,f(x , y ) = \\sum_{x_i , y_j } p(x_i)\\,k(x_i;y_j)\\,f(x_i;y_j)\\;,\\ ] ] and just as well : @xmath221 = \\sum_{x , y } p(x)\\,\\bar{k}(x;y)\\,f(x , y ) = \\sum_{x_i , y_j } p(x_i)\\,\\bar{k}(x_i;y_j)\\,f(x_i;y_j)\\;.\\ ] ] the definition in ( with strict positivity of @xmath36 ) requires exactly that : @xmath222=\\operatorname{\\mathbb{e}}_{p\\bar{k}}[f]\\ ] ] for every @xmath223 . using and",
    ", the equality becomes : @xmath224 for every @xmath55 in @xmath14 , which means that @xmath225 .",
    "take as initial distribution @xmath233 and form as in theorem [ jointit ] the sequence @xmath83 of @xmath78-projections . according to theorem [ jointit",
    "] , this sequence converges to the @xmath78-projection of @xmath234 on @xmath51 . since @xmath235}(p)$ ] , this projection will have input marginal equal to @xmath23 , and so we can write it as @xmath236 for some uniquely defined channel @xmath142 .",
    "we have , for @xmath237 : @xmath238 so in particular , for the subsequence of even - numbered terms also : @xmath239 this subsequence is defined iteratively by : @xmath240}^{p } \\operatorname{\\sigma}_{i_jj_j}^{p \\bar k } q^{2(j-1)}\\;.\\ ] ] propositions [ jlevel ] and [ inscale ] imply then that : @xmath241 for every @xmath205 , where @xmath242 is the sequence defined in the statement of theorem [ convergence ]",
    ". therefore this sequence converges : @xmath243    since @xmath244 for all @xmath245 , @xmath246 for all @xmath245 because of , which by definition means that @xmath247 .",
    "moreover , @xmath248 is the @xmath78-projection @xmath79 of @xmath234 on @xmath51 , which means that : @xmath249 using the chain rule of the kl - divergence , we get : @xmath250 which means that @xmath142 is the @xmath78-projection of @xmath138 on @xmath72 ."
  ],
  "abstract_text": [
    "<S> here we define a procedure for evaluating kl - projections ( i- and ri - projections ) of channels </S>",
    "<S> . these can be useful in the decomposition of mutual information between input and outputs , e.g. to quantify synergies and interactions of different orders , as well as information integration and other related measures of complexity .    </S>",
    "<S> the algorithm is a generalization of the standard iterative scaling algorithm , which we here extend from probability distributions to channels ( also known as transition kernels ) .    </S>",
    "<S> * keywords : * markov kernels , hierarchy , i - projections , divergences , interactions , iterative scaling , information geometry . </S>"
  ]
}