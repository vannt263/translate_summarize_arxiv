{
  "article_text": [
    "the cosmic microwave background ( cmb ) radiation is one of the most pristine sources of information about the early universe available to us . since its discovery in 1964 @xcite , the amount of information available to us about the cmb has increased at a rapid pace through series of ground - based , sub - orbital and satellite experiments .",
    "the recently released _",
    "temperature sky maps @xcite is just the latest example of how the present challenge in the field of cosmology is one of overabundance rather than shortage of data .    to extract cosmological parameters from these ever growing data sets requires increasingly sophisticated and efficient algorithms , both due to larger data volumes and to more stringent requirements to statistical precision .",
    "for example , the _ cobe_-dmr sky maps published twenty years ago @xcite comprised @xmath3 pixels , and could be analyzed using exact brute - force likelihood techniques ( e.g. , * ? ? ? * ) , with a computational scaling of @xmath4 .",
    "the _ wmap _",
    "sky maps published ten years ago comprised @xmath5 pixels @xcite , at which point faster and approximate methods had to be used for parameter estimation @xcite .",
    "however , for _ wmap _ the error budget was still dominated by cosmic variance on large angular scales and instrumental noise on small angular scales , and confusion with galactic and extra - galactic emission was minimal , allowing for very simple component separation methods @xcite . for _ planck",
    "_ , the total number of data points in nine frequency bands is @xmath6 , and instrumental noise never dominates the uncertainties at any angular scales , as small - scale astrophysical confusion becomes important at multipoles @xmath7 @xcite . as a result , an unprecendented study of all important sources of uncertainty , including instrumental , systematic and astrophysical , was required for _ planck _ to reach its ambitious goals @xcite .    with the advent of these massive mega - pixel data sets , a number different analysis strategies have been developed to robustly extract cosmological parameters with acceptable computational cost .",
    "as of today , the preferred option for full - sky high - resolution experiments such as _ planck _ and _ wmap _ is to divide the analysis into two separate components according to large and small angular scales , and merge the two at the likelihood level . on large angular scales",
    ", they use a gibbs sampling based @xcite blackwell - rao estimator @xcite that takes into account the full non - gaussian structure of the true cmb likelihood , while on small angular scales , they use faster approaches ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) coupled to an analytic multivariate gaussian ( and/or log - normal ) likelihood approximation .",
    "the computational cost of this hybrid approach is dominated by spherical harmonics transforms , and therefore scales as @xmath8 , which is acceptable even for large data sets .",
    "however , there is an unsolved problem associated with this hybrid approach , and that is how to merge the two likelihood components into a single all - scale expression ; correlations between the smallest scales in the large - scale likelihood and the largest scales in the small - scale likelihood should in principle be accounted for . as of today",
    ", no fully satisfactory solution to this exists in the cmb literature , though various approaches were explored during the planck analysis .    having a computational scaling of @xmath8",
    ", the gibbs sampling approach could in principle be employed for all angular scales , thus eliminating the need for any hybrid approximation .",
    "unfortunately , in practice this method is in its current implementation limited to low angular scales for two reasons : first , joint cmb analysis and component separation is currently implemented in terms of pixel - based fits of physical foreground models , requiring all frequency bands to have the same angular resolution , dictated by the coarsest resolution in a given data set .",
    "second , although the computational scaling for the gibbs sampler is acceptable , the prefactor is high .",
    "the 2013 _ planck _ likelihood employed 100000 gibbs samples in order to achieve robust blackwell - rao convergence , and each of those samples required @xmath92000 conjugate gradient iterations ( and twice as many spherical harmonic transforms ) to converge , for a total cost of 500000 cpu hours . naively scaling this to full _ planck _",
    "resolution suggest a final cost of @xmath10 cpu hours .",
    "the main result of the present paper is a statistically well motivated block factorization of the cmb power spectrum likelihood that is applicable to several of these problems .",
    "specifically , we show that for sets of random variables that can be arranged sequentially in such a way that all correlations have a finite range within the sequence , the full joint probability distribution may be written in terms of lower - dimensional marginals .",
    "the arch - typical example of such a distribution is a multivariate gaussian with a strictly banded covariance matrix , and we therefore call the general ( non - gaussian but conditionally limited ) case also `` banded '' .",
    "with this statistical identity ready at hand , we first suggest a statistically well - motivated likelihood hybridization scheme that takes properly into account correlations between the low- and high-@xmath1 regimes , and , second , we show how the convergence rate of the blackwell - rao estimator can be improved by factorizing the full high - dimensional multivariate posterior into a set of lower - dimensional distributions , each of which converges much faster than the full distribution .",
    "this approach differs from the direct gaussianization technique proposed by @xcite in that the underlying probabilistic structure ( e.g. , shapes of marginal and @xmath11-point correlations ) is conserved ; in principle , the only modification to the full likelihood enforced by our new approach is that assumed negligible correlations are explicitly set to zero .",
    "we begin with a general joint probability density @xmath12 for a set of random variables , @xmath13 , with @xmath14 .",
    "we choose one specific sequential ordering of these variables ( out of all the possible orderings ) , and use the definition of a conditional to write the joint distribution as a product of univariate conditionals , @xmath15 we then assume that our variables only have a conditional probability dependence on their immediate neighbors in the sequence , i.e. , that the probability distribution is _ tri - diagonally _ banded , @xmath16 thus , this simple derivation shows that a strictly ( tri - diagonally ) banded probability distribution may be factorized recursively into a product of uni- and bivariate marginals .    before applying this expression to cmb likelihood approximation , we note that even if the joint probability distribution do not have correlations exclusively between neighboring variables , it may still be possible to factorize it , provided at least some correlations may be ignored .",
    "for instance , suppose we can ignore all but the nearest _ two _ neighbors ; in that case , the joint distribution will factorize into a product of uni- , bi- and trivariate marginals .",
    ", for the official _",
    "cmb data set , estimated by monte carlo sampling .",
    "note that any two - point correlations are contained within a band of @xmath17 , suggesting that the cmb likelihood may be approximated as a banded probability distribution . to factorize the cmb likelihood into lower - dimensional elements , we partition the full multipole range into a set of disjoint blocks such that all non - zero covariance elements are embedded within a tri - diagonal block structure , indicated here by colored squares . ]    in its most basic representation , a cmb data set , @xmath18 , may be modelled as @xmath19 where @xmath20 is the true sky signal and @xmath21 represents instrumental noise . both the signal and noise",
    "are usually assumed to be zero - mean gaussian variables with covariances @xmath22 and @xmath23 , respectively .",
    "the noise covariance matrix is typically given by external knowledge about the instrumental noise characteristics and the scanning strategy of a given experiment .",
    "the signal covariance matrix , on the other hand , is generally unknown , and must be estimated from the data .",
    "however , given the fact that we only have one observable sky available , it is impossible to estimate the @xmath24 elements in @xmath22 from the @xmath25 elements in @xmath18 without imposing strong priors on its structure .",
    "the most commonly accepted prior is simply that the cmb sky is isotropic and homogeneous ( e.g. , * ? ? ?",
    "it is therefore convenient to expand @xmath20 in spherical harmonics , such that @xmath26 where @xmath27 is a unit vector pointing to a given position on the sky , @xmath28 are the spherical harmonics , and @xmath29 are the corresponding spherical harmonics coefficients . then the signal covariance matrix may be written as @xmath30 where @xmath31 is known as the angular power spectrum .",
    "the main goal of most cmb experiments is precisely to measure the cmb power spectrum , and the most straightforward way to do so is by maximum - likelihood estimation . since we have assumed that both signal and noise are gaussian distributed , the cmb power spectrum likelihood simply reads @xmath32 where @xmath33 is the covariance matrix given in equation  [ eq : covmat ] expressed in pixel domain .",
    "note that @xmath31 denotes the set of all power spectrum coefficients , and the likelihood therefore spans an @xmath34-dimensional space .",
    "as already mentioned , brute - force evaluation of equation  [ eq : likelihood ] scales computationally as @xmath4 , and is therefore feasible only for very low angular resolutions .",
    "much of the cmb analysis literature therefore revolves around finding computationally tractable approximations to this expression .    in order to build up",
    "some intuition about the correlation structure of @xmath35 , it is useful to plot the correlation matrix @xmath36 figure  [ fig : bandmat ] shows this matrix for the official _",
    "low-@xmath1 cmb data , as evaluated from @xmath37 monte carlo samples generated with a cmb gibbs sampler @xcite .",
    "in this case , there are significant correlations between all elements at @xmath38 , while at @xmath39 any correlations are well contained inside a band of @xmath40 ; any correlations beyond @xmath41 are well below 1% .",
    "higher - order correlations are significantly smaller than these two - point correlations .            for typical sky cuts and instrumental noise characteristics ,",
    "the basic cmb likelihood can therefore be approximated as a banded probability distribution with a bandwidth of @xmath42 , and can therefore in principle be factorized by equation  [ eq : master ] . however , as currently written this expression only applies to a strictly tri - diagonal covariance matrix . to circumvent this problem ,",
    "we therefore introduce an auxiliary block structure that embeds all non - negligible elements within a larger tri - diagonal structure , as illustrated by the colored blocks in figure  [ fig : bandmat ] .",
    "that is , we define a set of multipole blocks such that @xmath43 , @xmath44 ,  , @xmath45 .",
    "thus , each univariate marginal in equation  [ eq : master ] is replaced with a multivariate distribution of dimension @xmath46 , and each bivariate marginal is replaced with a multivariate distribution of dimension @xmath47 .",
    "this block - wise factorization constitutes the main result of this paper , and in the following sections we will apply this to two concrete problems in cmb likelihood estimation .",
    "lcccccc[h!t ] @xmath48 & @xmath49 & @xmath49 & @xmath50 & @xmath49 & @xmath50 + @xmath51 & @xmath52 & @xmath52 & @xmath53 & @xmath54 & @xmath55 + @xmath56 & @xmath57 & @xmath57 & @xmath58 & @xmath57 & @xmath55 + @xmath59 & @xmath60 & @xmath60 & @xmath58 & @xmath60 & @xmath55 + @xmath61 & @xmath62 & @xmath63 & @xmath64 & @xmath65 & @xmath66 + @xmath67 $ ] & @xmath68 & @xmath69 & @xmath64 & @xmath68 & @xmath55",
    "as already mentioned , both _ planck _ and _ wmap _ have adopted so - called `` hybrid '' likelihood approximations , combining a gibbs sampling based blackwell - rao estimator at large angular scales with a gaussian ( and/or log - normal ) pseduo cross - spectrum approximation at small angular scales .",
    "these two components are merged into a single expression at the log - likelihood level .",
    "the _ planck _",
    "likelihood simply adds the two log - likelihoods @xcite , adopting a so - called `` sharp transition '' between the low- and high-@xmath1 regimes , schematically illustrated in the left panel of figure  [ fig : regions ] .",
    "this is the simplest possible approach , and assumes that any correlations across the transition multipole are negligible .",
    "wmap _ likelihood makes a different choice , by including the off - diagonal terms between the low- and high-@xmath1 blocks in the ( gaussian plus log - normal ) high-@xmath1 likelihood , as illustrated in the middle panel of figure  [ fig : regions ] .    in this section",
    ", we introduce a new and statistically better motivated approach than either of two employed by _ planck _ and _ wmap _ , taking advantage of the block factorization derived in equation  [ eq : master ] .",
    "the first step in our approach is to partition the full multipole range between @xmath70 and @xmath34 into three disjoint regions , @xmath71 , @xmath72 and @xmath73 , corresponding to a low-@xmath1 region , a transition region and a high-@xmath1 region , respectively .",
    "the width of the transition region is chosen to be at least as wide as the effective bandwidth of the @xmath31 covariance matrix ( see figure  [ fig : bandmat ] ) . with this partitioning ,",
    "we now specialize equation  [ eq : master ] to the case with @xmath74 regions ; @xmath75 note that this approximation is exact under the assumption of vanishing correlations between the low- and high-@xmath1 regions , which can be ensured simply by letting the transition region be sufficiently wide .",
    "this estimator is schematically illustrated in the right panel of figure  [ fig : regions ] .",
    "equation  [ eq : hybrid ] has a simple intuitive interpretation : the log - likelihood is simply the sum of a low- and a high-@xmath1 contribution , defined such that they overlap over a sufficiently wide multipole range that all non - negligible correlations are included .",
    "however , because the diagonal block in the transition region is included twice , both by the low- and the high-@xmath1 likelihood , one must subtract the corresponding marginal for the transition region once to avoid double - counting ( this is also an immediate consequence of equation 1 , under the assumption that @xmath76 , i.e. the low-@xmath1 region is conditionally independent of the high-@xmath1 region given the transition region ) . note that any estimator for the transition likelihood may be used for the correction term , typically by extracting the relevant range from either the low- or the high-@xmath1 likelihoods .    to assess the importance of the specific strategy adopted for hybridization , we modify the ( 7-year ) _ wmap _ likelihood to include each of the three solutions , and derive constraints on the standard @xmath77cdm model using _ wmap _ data only .",
    "the transition multipole is set to @xmath78 for the sharp transition case , whereas the transition region is defined as @xmath79 for the new hybrid scheme .",
    "the _ wmap _",
    "blackwell - rao estimator is used both for the low-@xmath1 and the transition regions in the latter case .",
    "we adopt @xmath80 and @xmath81 as our primary parameters , and adopt cosmomc @xcite as our mcmc engine .",
    "the resulting one - dimensional marginals are shown in figure  [ fig : trans_plot ] for all three cases , and posterior mean summary statistics are given in table [ tab : trans_tab ] .        with a largest relative difference between any two cases of @xmath2 , these results demonstrate that the standard six - parameter @xmath77cdm model is highly robust with respect to assumptions about the correlations across the transition regime .",
    "similar conclusions were found when performing an identical analysis for the the recently released _",
    "likelihood @xcite , and this motivated the choice of a sharp transition for that particular implementation . for future experiments and analyses",
    "we nevertheless recommend the hybrid approach presented here , for two main reasons .",
    "first , our expression provides a statistically well motivated solution whose validity may be monitored directly through the @xmath31 covariance matrix ; without the same level of statistical rigour , detailed simulations are more critical for the other two approaches , and these should in principle be repeated both when the data set or the parametric model is changed .",
    "second , this expression is implementationally trivial once both low- and high-@xmath1 likelihoods are available , and there is therefore no practical reason for not including these correlations , even if their impact may be small .",
    "as mentioned in section  [ sec : introduction ] , both the _ planck _ and _ wmap _ low-@xmath1 likelihoods @xcite employs a specific blackwell - rao ( br ) estimator to produce an accurate likelihood approximation that accounts for all correlations and non - gaussian structures @xcite .",
    "the main advantages of this estimator are 1 ) computational speed , 2 ) implementational simplicity , and 3 ) support for seamless marginalization over systematic effects and component separation errors through gibbs sampling @xcite .",
    "this estimator may be explained intuitively as follows : suppose it is possible to construct an experiment that provides a perfect full - sky noiseless image of the cmb sky , @xmath82 .",
    "for that experiment , the only source of uncertainty on @xmath31 is cosmic variance , and the exact cmb likelihood in equation  [ eq : likelihood ] reduces to an inverse gamma distribution , @xmath83 here we have defined @xmath84 to be the realization specific power spectrum of @xmath85 .",
    "however , for any real experiment there are additional sources of uncertainty beyond cosmic variance , for instance from instrumental noise and foreground contamination , and @xmath86 is no longer a delta function . in order to account for this additional uncertainty",
    ", one must weight the ideal likelihood in equation  [ eq : br0 ] with respect to @xmath86 , @xmath87 at first glance , this integral appears difficult to evaluate , as it involves millions of degrees of freedom .",
    "however , this is precisely where the cmb gibbs sampler enters the picture . as explained in detail by @xcite ,",
    "the output from this algorithm is a set of samples drawn directly from @xmath86 , accounting for both instrumental noise and foreground errors .",
    "thus , the integral can be simply evaluated by monte carlo integration as a sum over these samples , @xmath88 this is the cmb power spectrum blackwell - rao estimator , which is guaranteed to converge to the true likelihood in the limit of @xmath89 .       in one dimension ( top and left panels )",
    ", it is @xmath90 in two dimensions ( central panel ) , and @xmath91 in @xmath92 dimensions .",
    "this implies that the number of monte carlo samples required to reach convergence for the cmb br estimator scales exponentially with @xmath34 .",
    "the evaluation of the 2-d likelihood at a specific point in parameter space ( _ red cross _ ) will be much more sensitive to the number of samples than the corresponding evaluations in the respective marginalized parameter spaces ( _ red lines _ ) . ]    while the blackwell - rao estimator is guaranteed to converge to the correct answer , it is not obvious how fast it does so , as measured in terms of number of samples required for convergence , @xmath93 .",
    "further , since the computational cost of a single gibbs sample is typically on the order of several cpu hours @xcite , depending on the angular resolution and/or signal - to - noise ratio of the data set under consideration , it is important to understand this scaling before attempting a full - scale analysis .",
    "indeed , @xcite showed that @xmath93 scales exponentially with @xmath34 , effectively limiting its operational range to @xmath9470 .",
    "the main goal of the present section is to improve on this limit , and extend the br estimator to high @xmath1 s .    to understand the origin of the exponential scaling , we show in figure  [ fig : dimensions ] a simple two - dimensional gaussian distribution mapped by a monte carlo sampler .",
    "the top and left panels show the respective one - dimensional marginals .",
    "the blackwell - rao estimator establishes a smooth approximation to these distributions by assigning a kernel of finite width to each individual monte carlo sample ( illustrated by blue contours / gaussians ) before taking the average over all samples .",
    "suppose now that the width of the one - dimensional kernel is 10% of the width of the marginal distribution ; in that case , one needs @xmath910 samples in order to cover the marginal once .",
    "in two dimensions , however , one needs @xmath9@xmath95 samples to cover the full joint distribution once , since the ratio now is only 10% in each of the two directions . more generally , in @xmath92 dimensions one would need @xmath9@xmath96 samples .",
    "this is a variation of the well - known `` curse of dimensionality '' , which says that the number of points required to cover an @xmath92-dimensional space scales exponentially with @xmath92 .",
    "the br estimator given in equation  [ eq : br2 ] converges well up to @xmath97 with only a few thousand samples for _ wmap _",
    "@xcite , while for _ planck _ it is found to be robust up to @xmath98 with 100000 samples @xcite . to extend to even higher @xmath1 s by brute force would soon require a prohibitively large number of samples , as the computational cost for the gibbs sampling step of the latter case is already half a million cpu hours .",
    "fortunately , the block factorization presented in section  [ sec : factorization ] may be used to define an alternative and computationally much cheaper algorithm :    1 .",
    "partition the full @xmath34-dimensional @xmath35 into a sequence of lower - dimensional blocks , @xmath99 , for instance of width @xmath0 .",
    "2 .   use the standard br estimator to estimate the marginal likelihood for each block and each neighboring set of two blocks .",
    "merge these block marginals into a single all-@xmath1 estimator through the block factorization in equation  [ eq : master ] .",
    "thus , our new likelihood approximation can be written succinctly on the following form , @xmath100 note that all the likelihood evaluations on the right side of this expression involve a maximum of @xmath101 dimensions , as opposed to @xmath102 for the full joint br estimator , effectively lifting the curse of dimensionality .        before the block factorized br estimator",
    "can be used for real analysis , it is necessary to assess its accuracy and convergence properties .",
    "to this aim , we analyze two different simulations with the above machinery , adopting the convergence analysis methodology of @xcite , but implementing a few minor changes to improve the reliability of the convergence statistics .",
    "monte carlo samples are produced with commander @xcite .",
    "the first simulation consists of a full - sky high - resolution ( @xmath103 , @xmath104 , 14 gaussian beam ) data set with uniform noise ( 65@xmath105 rms per pixel ) .",
    "the main advantage of this case is that the @xmath31 likelihood ( equation  [ eq : likelihood ] ) factorizes in @xmath1 , and can be evaluated analytically , @xmath106 where @xmath107 is the angular power spectrum of the noisy sky map , and @xmath108 is the ensemble averaged noise power spectrum .",
    "the second simulation consists of a low - resolution ( @xmath109 , @xmath110 , @xmath111 fwhm gaussian beam ) data set with the _ wmap _ kq85 sky cut imposed , removing 25% of the sky .",
    "white noise of 5@xmath105 rms is added to each pixel , resulting in a signal - to - noise of unity at @xmath98 .",
    "the main purpose of this simulation is to study the effect of correlations between different multipoles arising from the sky cut through comparison with brute - force pixel - space likelihood evaluation .",
    "however , because of the brute - force evaluations , this case is necessarily limited to low angular resolution .    the cmb signal is drawn from a gaussian distribution with a covariance given by the best - fit _ wmap _",
    "@xmath77cdm power spectrum , @xmath112 @xcite . in each case",
    ", we fit a two - parameter amplitude - tilt ( @xmath113@xmath92 ) model on the form @xmath114 where @xmath115 , simply by mapping out @xmath116 over a two - dimensional grid . for @xmath117 ,",
    "this choice of pivot multipole ensures a low degree of correlation between @xmath113 and @xmath92 .    to assess both convergence and accuracy , we adopt the following measure of difference between two likelihoods , @xmath118 and @xmath119 @xcite",
    ", @xmath120 one can show that if @xmath118 and @xmath119 are two bivariate gaussian distributions with the same covariance matrix , @xmath121 , but different means , @xmath122 and @xmath123 , then @xmath124 where @xmath125 is the cumulative standard normal distribution function . from this",
    ", one finds that a @xmath126 shift in a gaussian distribution corresponds to @xmath127 . in the following ,",
    "we therefore define two distributions to agree if @xmath128 .    for the accuracy assessment",
    ", we simply compare the block factorized br likelihood with the exact case .",
    "convergence assessment , however , is done by drawing two disjoint sample subsets from the full set of available monte carlo samples , compute the br estimator from each subset , and compare the resulting likelihoods .",
    "we then increase the number of samples in the two subsets , @xmath93 , until @xmath129 is consistently lower than 0.05 even when adding 100 additional samples ; the latter criterion is imposed in order to avoid chance agreement .",
    "finally , we repeat this calculation a certain number of times with different sample subsets ( but drawn from the same full sample set ) , and report the median of the resulting values of @xmath93 as the final estimate of the number of samples required for convergence .",
    "figure  [ fig : hirespoc ] shows @xmath116 evaluated from the high - resolution full - sky simulation for nine different values of @xmath34 with four different likelihood expressions ; analytic , standard br , and two variations of the block - factorized br estimator .",
    "a total of @xmath130 samples are included in the two latter , a choice that is set to highlight the fundamental difference between the various cases . in particular , since there are no correlations between any multipoles in this case , all four approaches are in principle exact , and the only difference among the four cases are their relative convergence rates .    for @xmath131 , we see that all four estimator agree to very high accuracy .",
    "however , from @xmath132 the full - range br likelihood starts to diverge . at @xmath133 ,",
    "it is separated from the analytic result by more than @xmath134 . in this case , the sum in equation  [ eq : br2 ] is strongly dominated by the one sample that happens to have the lowest power spectrum scatter about some best - fit mode , and the resulting distribution is simply an imprint of the cosmic variance kernel ( equation  [ eq : br0 ] ) for that sample .    , @xmath135 , and @xmath136 , respectively . ]    .",
    "the samples come from running commander on a full - sky simulation .",
    "we show the median of the number of samples needed for convergence for a given @xmath137 , along with the best - fit regression line in @xmath138-space .",
    "the median is computed from 10 ( top ) and 1024 ( bottom ) runs where the samples are scrambled between each run .",
    "the regression lines are dotted when they extend past the available data points .",
    "the high number of runs per data point for the bottom plot is also the reason for the more sparse sampling - each data point represented a very high computational cost , and so the number of data points were reduced.,title=\"fig : \" ] .",
    "the samples come from running commander on a full - sky simulation .",
    "we show the median of the number of samples needed for convergence for a given @xmath137 , along with the best - fit regression line in @xmath138-space .",
    "the median is computed from 10 ( top ) and 1024 ( bottom ) runs where the samples are scrambled between each run .",
    "the regression lines are dotted when they extend past the available data points .",
    "the high number of runs per data point for the bottom plot is also the reason for the more sparse sampling - each data point represented a very high computational cost , and so the number of data points were reduced.,title=\"fig : \" ]    the block factorized br estimators remain valid to higher @xmath34 , demonstrating how the `` curse of dimensionality '' is lifted by breaking the full parameter space into smaller regions that are easier to handle .",
    "in particular , the case with @xmath139 agrees with the analytic case even at @xmath133 to @xmath9@xmath140 .    in figure  [",
    "fig : maskedpoc ] we show similar results for the low - resolution simulation for which 25% of the sky is removed by masking , but this time comparing with the brute - force pixel - based likelihood estimator , and this time using @xmath141 samples .",
    "again , we see that all cases agree to better than @xmath126 , even for the factorized br estimator with @xmath142 , demonstrating the accuracy of both the full and the factorized br estimators , even with very small block sizes and for the fairly large _ wmap _ mask .",
    "next , in the top panel of figure  [ fig : convergence_hires_fullsky ] we plot the number of samples required for convergence according to the above criterion for the high - resolution full - sky simulation described above , and in the bottom panel we show the same , but after applying the _ wmap _ mask , in order to introduce a realistic multipole correlation structure .",
    "the upper vertical limit in these plots is set by the finite number of samples included in the analysis .    in all cases",
    "we see the same qualitative behaviour : reducing the dimensionality of the br estimator through block factorization greatly improves the convergence rate by reducing the required number of samples by orders of magnitude at high @xmath1 s . for instance , for the full - sky case and with a block size of @xmath143 , only @xmath144 samples are required in order to reach convergence up to @xmath145 , whereas the full br estimator would require @xmath146 . for the 25%",
    "wmap _ mask , about @xmath147 samples are required for @xmath148 , while it is difficult to establish any sensible estimate for the full br estimator in this case .",
    "( note that the high-@xmath1 projection for the latter case , marked by a dashed line , is based on linear extrapolation from a few low-@xmath1 points , since convergence was not reached at all within the current sample set at higher multipoles .",
    "this projection is therefore associated with a very large systematic uncertainty . )",
    "the main result presented in this paper is a statistically well motivated block factorization of the cmb power spectrum likelihood .",
    "because the spherical harmonics are nearly orthogonal over the large sky coverages achieved by current cmb satellite experiments such as _ planck _ and _ wmap _ , any correlations between different @xmath31s are localized in multipole space . under the assumption that these probabilistic dependencies have a strictly finite range ,",
    "the full cmb likelihood may be reduced into a product of lower - dimensional marginals .",
    "we have applied this result to two outstanding problems in cmb analysis .",
    "first , we use this expression to derive a well - motivated hybrid cmb likelihood estimator , merging an exact low-@xmath1 component with an approximate high-@xmath1 component , that accounts for correlations between the two regions .",
    "although a detailed analysis of the _ wmap _ likelihood shows that these correlations are negligible for the _ wmap _ sky cut and the six - parameter @xmath77cdm model , we nevertheless recommend this new estimator for future experiments and analyses , both because its implementation is trivial , and because it provides additional safety when analyzing non - standard models .",
    "second , we have shown how the same expression may be used to accelerate the convergence rate of the blackwell - rao cmb likelihood estimator by orders of magnitude at high @xmath1s .",
    "this is achieved by factorizing the full parameter space into subspaces that each individually converge faster , and then merging these sub - blocks into a full - range estimator at the likelihood level using the block factorization formula .",
    "it should be noted that these results rely directly on the assumption of vanishing long - range correlations . while this assumption holds to a very high accuracy for the basic cmb signal plus noise data model , it is in",
    "general not valid when including systematic effects in the analysis .",
    "perhaps the two most important examples are correlated beam uncertainties and unresolved extra - galactic point sources , each of which extend through all @xmath1 s ( e.g. , * ? ? ?",
    "fortunately , these long - range degrees of freedom may be modelled in terms of a small number of power spectrum templates , each with an unknown amplitude .",
    "one can therefore marginalize over these by sampling the unknown amplitudes as nuiscance parameters , similar to what was done for high-@xmath1 astrophysical parameters in the 2013 _ planck _ likelihood @xcite .",
    "finally , we note that the block factorization presented in section  [ sec : factorization ] is a completely general statistical result that holds exactly for any banded probability distribution , and we therefore expect it to also find applications outside the cmb field .",
    "this project was supported by the erc starting grant stg2010 - 257080 .",
    "part of the research was carried out at the jet propulsion laboratory , california institute of technology , under a contract with nasa .",
    "some of the results in this paper have been derived using the healpix @xcite software and analysis package .",
    "bennett , c.  l. , halpern , m. , hinshaw , g. , et al .  2003a , , 148 , 1 bennett , c.  l. , hill , r.  s. , hinshaw , g. , et al .",
    "2003b , , 148 , 97 chu , m. , eriksen , h.  k. , knox , l. , et al .",
    "2005 , , 71 , 103002 eriksen , h.  k. , odwyer , i.  j. , jewell , j.  b. , et al .  2004 , , 155 , 227 eriksen , h.  k. , jewell , j.  b. , dickinson , c. , banday , a.  j. , grski , k.  m. , & lawrence , c. r. 2007 , , 676 , 19 grski , k.  m.  1994 , , 430 , l85 grski , k. m. , hivon , e. , banday , a. j.,wandelt , b. d. , hansen , f. k. , reinecke , m. , bartelman , m. 2005 , , 622 , 759 hinshaw , g. , spergel , d.  n. , verde , l. , et al .",
    "2003 , , 148 , 135 hinshaw , g. , larson , d. , komatsu , e. , et al .  2012 , arxiv:1212.5226 hivon , e. , grski , k.  m. , netterfield , c.  b. , et al .",
    "2002 , , 567 , 2 jarosik , n. , bennett , c.  l. , dunkley , j. , et al .",
    "2011 , , 192 , 14 jewell , j. , levin , s. , & anderson , c. h. 2004 , , 609 , 1 lewis , a. , & bridle , s.  2002 , , 66 , 103511 penzias , a.  a. , & wilson , r.  w.  1965 , , 142 , 419 planck collaboration i 2013 , [ 1303.5062 ] planck collaboration xii 2013 , [ 1303.5072 ] planck collaboration xv 2013 , [ 1303.xxxx ] planck collaboration xvi 2013 , [ 1303.5076 ] planck collaboration xxiii 2013 , [ 1303.5083 ] planck collaboration xxiv 2013 , [ 1303.5084 ] rocha , g. , contaldi , c.  r. , bond , j.  r. , & grski , k.  m.  2011 , , 414 , 823 rudjord ,  . ,",
    "groeneboom , n.  e. , eriksen , h.  k. , et al .",
    "2009 , , 692 , 1669 smoot ,",
    "g.  f. , bennett , c.  l. , kogut , a. , et al .",
    "1992 , , 396 , l1 verde , l. , peiris , h.  v. , spergel , d.  n. , et al .",
    "2003 , , 148 , 195 wandelt , b.  d. , larson , d.  l. , & lakshminarayanan , a.  2004 , , 70 , 083511"
  ],
  "abstract_text": [
    "<S> we investigate sets of random variables that can be arranged sequentially such that a given variable only depends conditionally on its immediate predecessor . for such sets , </S>",
    "<S> we show that the full joint probability distribution may be expressed exclusively in terms of uni- and bivariate marginals . under the assumption that the cmb power spectrum likelihood only exhibits correlations within a banded multipole range , @xmath0 </S>",
    "<S> , we apply this expression to two outstanding problems in cmb likelihood analysis . </S>",
    "<S> first , we derive a statistically well - defined hybrid likelihood estimator , merging two independent ( e.g. , low- and high-@xmath1 ) likelihoods into a single expression that properly accounts for correlations between the two . applying this expression to the wmap likelihood </S>",
    "<S> , we verify that the effect of correlations on cosmological parameters in the transition region is negligible in terms of cosmological parameters for wmap ; the largest relative shift seen for any parameter is @xmath2 . </S>",
    "<S> however , because this may not hold for other experimental setups ( e.g. , for different instrumental noise properties or analysis masks ) , but must rather be verified on a case - by - case basis , we recommend our new hybridization scheme for future experiments for statistical self - consistency reasons . </S>",
    "<S> second , we use the same expression to improve the convergence rate of the blackwell - rao likelihood estimator , reducing the required number of monte carlo samples by several orders of magnitude , and thereby extend it to high-@xmath1 applications . </S>"
  ]
}