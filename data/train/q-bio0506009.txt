{
  "article_text": [
    "most hypotheses which involve the idea that information sharing is reflected in coordinated activity across neural units invoke a very specific notion of coordinated activity , namely strict synchrony : the units should be doing exactly the same thing ( e.g. , spiking ) at exactly the same time .",
    "investigators then measure coordination by measuring how close the units come to being strictly synchronized ( e.g. , variance in spike times ) .    from an informational point of view , there is no reason to favor strict synchrony over other kinds of coordination .",
    "one neuron consistently spiking 50 ms after another is just as informative a relationship as two simultaneously spiking , but such stable phase relations are missed by strict - synchrony approaches . indeed , whatever the exact nature of the neural code , it uses temporally extended patterns of activity , and so information sharing should be reflected in coordination of those patterns , rather than just the instantaneous activity .",
    "there are three common ways of going beyond strict synchrony : cross - correlation and related second - order statistics , mutual information , and topological generalized synchrony .",
    "the cross - correlation function ( the normalized covariance function ; this includes , for present purposes , the joint peristimulus time histogram @xcite ) , is one of the most widespread measures of synchronization .",
    "it can be efficiently calculated from observable series ; it handles statistical as well as deterministic relationships between processes ; by incorporating variable lags , it reduces the problem of phase locking .",
    "fourier transformation of the covariance function @xmath0 yields the cross - spectrum @xmath1 , which in turn gives the spectral coherence @xmath2 , a normalized correlation between the fourier components of @xmath3 and @xmath4 .",
    "integrated over frequencies , the spectral coherence measures , essentially , the degree of linear cross - predictability of the two series .",
    "( @xcite applies spectral coherence to coordinated neural activity . ) however , such second - order statistics _ only _ handle linear relationships . since neural processes are known to be strongly nonlinear , there is little reason to think these statistics adequately measure coordination and synchrony in neural systems .",
    "mutual information is attractive because it handles both nonlinear and stochastic relationships and has a very natural and appealing interpretation .",
    "unfortunately , it often seems to fail in practice , being disappointingly small even between signals which are known to be tightly coupled @xcite .",
    "the major reason is that the neural codes use distinct patterns of activity over time , rather than many different instantaneous actions , and the usual approach misses these extended patterns .",
    "consider two neurons , one of which drives the other to spike 50 ms after it does , the driving neuron spiking once every 500 ms .",
    "these are very tightly coordinated , but whether the first neuron spiked at time @xmath5 conveys little information about what the second neuron is doing at @xmath5  it s not spiking , but it s not spiking most of the time anyway .",
    "mutual information calculated from the direct observations conflates the `` no spike '' of the second neuron preparing to fire with its just - sitting - around `` no spike '' . here",
    ", mutual information could find the coordination if we used a 50 ms lag , but that wo nt work in general .",
    "take two rate - coding neurons with base - line firing rates of 1 hz , and suppose that a stimulus excites one to 10 hz and suppresses the other to 0.1 hz . the spiking rates thus share a lot of information , but whether the one neuron spiked at @xmath5 is uninformative about what the other neuron did then , and lagging wo nt help .",
    "generalized synchrony is based on the idea of establishing relationships between the states of the various units .",
    "`` state '' here is taken in the sense of physics , dynamics and control theory : the state at time @xmath5 is a variable which fixes the distribution of observables at all times @xmath6 , rendering the past of the system irrelevant @xcite .",
    "knowing the state allows us to predict , as well as possible , how the system will evolve , and how it will respond to external forces @xcite .",
    "two coupled systems are said to exhibit generalized synchrony if the state of one system is given by a mapping from the state of the other .",
    "applications to data employ state - space reconstruction @xcite : if the state @xmath7 evolves according to smooth , @xmath8-dimensional deterministic dynamics , and we observe a generic function @xmath9 , then the space @xmath10 of time - delay vectors @xmath11 $ ] is diffeomorphic to @xmath12 if @xmath13 , for generic choices of lag @xmath14 .",
    "the various versions of generalized synchrony differ on how , precisely , to quantify the mappings between reconstructed state spaces , but they all appear to be empirically equivalent to one another and to notions of phase synchronization based on hilbert transforms @xcite . thus all of these measures accommodate nonlinear relationships , and are potentially very flexible . unfortunately , there is essentially no reason to believe that neural systems have deterministic dynamics at experimentally - accessible levels of detail , much less that there are deterministic relationships among such states for different units .    what we want",
    ", then , but none of these alternatives provides , is a quantity which measures predictive relationships among states , but allows those relationships to be nonlinear and stochastic .",
    "the next section introduces just such a measure , which we call `` informational coherence '' .",
    "there are alternatives to calculating the `` surface '' mutual information between the sequences of observations themselves ( which , as described , fails to capture coordination ) .",
    "if we know that the units are phase oscillators , or rate coders , we can estimate their instantaneous phase or rate and , by calculating the mutual information between those variables , see how coordinated the units patterns of activity are .",
    "however , phases and rates do not exhaust the repertoire of neural patterns and a more general , common scheme is desirable .",
    "the most general notion of `` pattern of activity '' is simply that of the dynamical state of the system , in the sense mentioned above .",
    "we now formalize this .    assuming the usual notation for shannon information @xcite , the information content of a state variable @xmath3 is @xmath15 $ ] and the mutual information between @xmath3 and @xmath4 is @xmath16 $ ] . as is well - known , @xmath16 \\leq \\min{h[x],h[y]}$ ] .",
    "we use this to normalize the mutual state information to a @xmath17 scale , and this is the _ informational coherence _ ( ic ) .",
    "@xmath18}{\\min{h[x],h[y]}}~ , ~ ~ \\mathrm{with}\\ 0/0 = 0~.\\end{aligned}\\ ] ]    @xmath19 can be interpreted as follows .",
    "@xmath16 $ ] is the kullback - leibler divergence between the joint distribution of @xmath3 and @xmath4 , and the product of their marginal distributions @xcite , indicating the error involved in ignoring the dependence between @xmath3 and @xmath4 .",
    "the mutual information between predictive , dynamical states thus gauges the error involved in assuming the two systems are independent , i.e. , how much predictions could improve by taking into account the dependence .",
    "hence it measures the amount of _ dynamically - relevant _ information shared between the two systems . @xmath19",
    "simply normalizes this value , and indicates the degree to which two systems have coordinated _ patterns _ of behavior ( cf .",
    "@xcite , although this only uses directly observable quantities ) .",
    "as mentioned , the state space of a deterministic dynamical system can be reconstructed from a sequence of observations .",
    "this is the main tool of experimental nonlinear dynamics @xcite ; but the assumption of determinism is crucial and false , for almost any interesting neural system . while classical state - space reconstruction wo nt work on stochastic processes , such processes _ do _ have state - space representations @xcite , and , in the special case of discrete - valued , discrete - time series , there are ways to reconstruct the state space .    here",
    "we use the cssr algorithm , introduced in @xcite ( code available at ` http://bactra.org/cssr ` ) .",
    "this produces _ causal state models _",
    ", which are stochastic automata capable of statistically - optimal nonlinear prediction ; the state of the machine is a minimal sufficient statistic for the future of the observable process@xcite .",
    "the basic idea is to form a set of states which should be ( 1 ) markovian , ( 2 ) sufficient statistics for the next observable , and ( 3 ) have deterministic transitions ( in the automata - theory sense ) .",
    "the algorithm begins with a minimal , one - state , iid model , and checks whether these properties hold , by means of hypothesis tests . if they fail , the model is modified , generally but not always by adding more states , and the new model is checked again .",
    "each state of the model corresponds to a distinct distribution over future events , i.e. , to a statistical pattern of behavior . under mild conditions , which do not involve prior knowledge of the state space",
    ", cssr converges in probability to the unique causal state model of the data - generating process @xcite . in practice",
    ", cssr is quite fast ( linear in the data size ) , and generalizes at least as well as training hidden markov models with the em algorithm and using cross - validation for selection , the standard heuristic @xcite .",
    "one advantage of the causal state approach ( which it shares with classical state - space reconstruction ) is that state estimation is greatly simplified . in the general case of nonlinear state estimation ,",
    "it is necessary to know not just the form of the stochastic dynamics in the state space and the observation function , but also their precise parametric values and the distribution of observation and driving noises . estimating",
    "the state from the observable time series then becomes a computationally - intensive application of bayes s rule @xcite .    due to the way causal states are built as statistics of the data , with probability 1",
    "there is a finite time , @xmath5 , at which the causal state at time @xmath5 is certain .",
    "this is not just with some degree of belief or confidence : because of the way the states are constructed , it is impossible for the process to be in any other state at that time .",
    "once the causal state has been established , it can be updated recursively , i.e. , the causal state at time @xmath20 is an explicit function of the causal state at time @xmath5 and the observation at @xmath20 .",
    "the causal state model can be automatically converted , therefore , into a finite - state transducer which reads in an observation time series and outputs the corresponding series of states @xcite .",
    "( our implementation of cssr filters its training data automatically . )",
    "the result is a new time series of states , from which all non - predictive components have been filtered out .",
    "our algorithm for estimating the matrix of informational coherences is as follows . for each unit , we reconstruct the causal state model , and filter the observable time series to produce a series of causal states",
    ". then , for each pair of neurons , we construct a joint histogram of the state distribution , estimate the mutual information between the states , and normalize by the single - unit state informations .",
    "this gives a symmetric matrix of @xmath19 values .",
    "even if two systems are independent , their estimated ic will , on average , be positive , because , while they should have zero mutual information , the empirical estimate of mutual information is non - negative .",
    "thus , the significance of ic values must be assessed against the null hypothesis of system independence .",
    "the easiest way to do so is to take the reconstructed state models for the two systems and run them forward , independently of one another , to generate a large number of simulated state sequences ; from these calculate values of the ic .",
    "this procedure will approximate the sampling distribution of the ic under a null model which preserves the dynamics of each system , but not their interaction .",
    "we can then find @xmath21-values as usual .",
    "we omit them here to save space .",
    "@xmath22 @xmath23    there is broad agreement @xcite that analyses of networks should not just be an analysis of pairs of neurons , averaged over pairs .",
    "ideally , an analysis of information sharing in a network would look at the over - all structure of statistical dependence between the various units , reflected in the complete joint probability distribution @xmath24 of the states .",
    "this would then allow us , for instance , to calculate the @xmath25-fold multi - information , @xmath26 \\equiv d(p||q)$ ] , the kullback - leibler divergence between the joint distribution @xmath24 and the product of marginal distributions @xmath27 , analogous to the pairwise mutual information @xcite .",
    "calculated over the predictive states , the multi - information would give the total amount of shared dynamical information in the system .",
    "just as we normalized the mutual information @xmath28 $ ] by its maximum possible value , @xmath29 , h[x_2]}$ ] , we normalize the multi - information by its maximum , which is the smallest sum of @xmath30 marginal entropies : @xmath31 \\leq \\min_{k}{\\sum_{i\\neq k}{h[x_n]}}\\ ] ] unfortunately , @xmath24 is a distribution over a very high dimensional space and so , hard to estimate well without strong parametric constraints .",
    "we thus consider approximations .",
    "the lowest - order approximation treats all the units as independent ; this is the distribution @xmath27 .",
    "one step up are tree distributions , where the global distribution is a function of the joint distributions of pairs of units .",
    "not every pair of units needs to enter into such a distribution , though every unit must be part of some pair .",
    "graphically , a tree distribution corresponds to a spanning tree , with edges linking units whose interactions enter into the global probability , and conversely spanning trees determine tree distributions . writing @xmath32 for the set of pairs @xmath33 and abbreviating @xmath34 by @xmath35 , one has @xmath36 where the marginal distributions @xmath37 and the pair distributions @xmath38 are estimated by the empirical marginal and pair distributions .",
    "we must now pick edges @xmath32 so that @xmath39 best approximates the true global distribution @xmath24 .",
    "a natural approach is to minimize @xmath40 , the divergence between @xmath24 and its tree approximation .",
    "chow and liu @xcite showed that the maximum - weight spanning tree gives the divergence - minimizing distribution , taking an edge s weight to be the mutual information between the variables it links .",
    "there are three advantages to using the chow - liu approximation .",
    "( 1 ) estimating @xmath39 from empirical probabilities gives a consistent maximum likelihood estimator of the ideal chow - liu tree @xcite , with reasonable rates of convergence , so @xmath39 can be reliably known even if @xmath24 can not . ( 2 ) there are efficient algorithms for constructing maximum - weight spanning trees , such as prim s algorithm @xcite , which runs in time @xmath41 .",
    "thus , the approximation is computationally tractable .",
    "( 3 ) the kl divergence of the chow - liu distribution from @xmath27 gives a lower bound on the network multi - information ; that bound is just the sum of the mutual informations along the edges in the tree : @xmath31 \\geq d(t||q ) = \\sum_{(i , j ) \\in e_t}{i[x_i;x_j ] } \\label{eqn : chow - liu - approx - to - multi - info}\\ ] ] even if we knew @xmath24 exactly , eq .",
    "[ eqn : chow - liu - approx - to - multi - info ] would be useful as an alternative to calculating @xmath42 directly , evaluating @xmath43 for all the exponentially - many configurations @xmath44 .",
    "it is natural to seek higher - order approximations to @xmath24 , e.g. , using three - way interactions not decomposable into pairwise interactions @xcite .",
    "but it is hard to do so effectively , because finding the optimal approximation to @xmath24 when such interactions are allowed is np @xcite , and analytical formulas like eq .",
    "[ eqn : chow - liu - approx - to - multi - info ] generally do not exist @xcite .",
    "we therefore confine ourselves to the chow - liu approximation here .    _",
    "we use simulated data as a test case , instead of empirical multiple electrode recordings , which allows us to try the method on a system of over 1000 neurons and compare the measure against expected results .",
    "the model , taken from @xcite , was originally designed to study episodes of gamma ( 3080hz ) and beta ( 1230hz ) oscillations in the mammalian nervous system , which often occur successively with a spontaneous transition between them .",
    "more concretely , the rhythms studied were those displayed by _ in vitro _",
    "hippocampal ( ca1 ) slice preparations and by _ in vivo _ neocortical eegs .",
    "the model contains two neuron populations : excitatory ( ampa ) pyramidal neurons and inhibitory ( gaba@xmath45 ) interneurons , defined by conductance - based hodgkin - huxley - style equations .",
    "simulations were carried out in a network of 1000 pyramidal cells and 300 interneurons .",
    "each cell was modeled as a one - compartment neuron with all - to - all coupling , endowed with the basic sodium and potassium spiking currents , an external applied current , and some gaussian input noise .",
    "the first 10 seconds of the simulation correspond to the gamma rhythm , in which only a group of neurons is made to spike via a linearly increasing applied current .",
    "the beta rhythm ( subsequent 10 seconds ) is obtained by activating pyramidal - pyramidal recurrent connections ( potentiated by hebbian preprocessing as a result of synchrony during the gamma rhythm ) and a slow outward after - hyper - polarization ( ahp ) current ( the m - current ) , suppressed during gamma due to the metabotropic activation used in the generation of the rhythm . during the beta rhythm , pyramidal cells ,",
    "silent during gamma rhythm , fire on a subset of interneurons cycles ( fig .",
    "[ fig : rastergram ] ) .    fig .",
    "[ fig : heat - maps - first - stage ] compares zero - lag cross - correlation , a second - order method of quantifying coordination , with the informational coherence calculated from the reconstructed states .",
    "( in this simulation , we could have calculated the actual states of the model neurons directly , rather than reconstructing them , but for purposes of testing our method we did not . )",
    "cross - correlation finds some of the relationships visible in fig .",
    "[ fig : rastergram ] , but is confused by , for instance , the phase shifts between pyramidal cells .",
    "( surface mutual information , not shown , gives similar results . ) informational coherence , however , has no trouble recognizing the two populations as effectively coordinated blocks .",
    "the presence of dynamical noise , problematic for ordinary state reconstruction , is not an issue .",
    "the average ic is @xmath46 ( or @xmath47 if the inactive , low - numbered neurons are excluded ) .",
    "the tree estimate of the global informational multi - information is @xmath48 bits , with a global coherence of @xmath49 . the right half of fig .",
    "[ fig : heat - maps - first - stage ] repeats this analysis for the beta rhythm ; in this stage , the average ic is @xmath50 , and the tree estimate of the global multi - information is @xmath51 bits , though the estimated global coherence falls very slightly to @xmath52 .",
    "this is because low - numbered neurons which were quiescent before are now active , contributing to the global information , but the over - all pattern is somewhat weaker and more noisy ( as can be seen from fig .",
    "[ fig : rastergram]@xmath23 . )",
    "so , as expected , the total information content is higher , but the overall coordination across the network is lower .",
    "informational coherence provides a measure of neural information sharing and coordinated activity which accommodates nonlinear , stochastic relationships between extended patterns of spiking .",
    "it is robust to dynamical noise and leads to a genuinely multivariate measure of global coordination across networks or regions .",
    "applied to data from multi - electrode recordings , it should be a valuable tool in evaluating hypotheses about distributed neural representation and function ."
  ],
  "abstract_text": [
    "<S> most nervous systems encode information about stimuli in the responding activity of large neuronal networks . </S>",
    "<S> this activity often manifests itself as dynamically coordinated sequences of action potentials . since multiple electrode recordings are now a standard tool in neuroscience research , it is important to have a measure of such network - wide behavioral coordination and information sharing , applicable to multiple neural spike train data . </S>",
    "<S> we propose a new statistic , _ informational coherence _ , which measures how much better one unit can be predicted by knowing the dynamical state of another . </S>",
    "<S> we argue informational coherence is a measure of association and shared information which is superior to traditional pairwise measures of synchronization and correlation . to find the dynamical states , we use a recently - introduced algorithm which reconstructs effective state spaces from stochastic time series . </S>",
    "<S> we then extend the pairwise measure to a multivariate analysis of the network by estimating the network multi - information . </S>",
    "<S> we illustrate our method by testing it on a detailed model of the transition from gamma to beta rhythms .    much of the most important information in neural systems is shared over multiple neurons or cortical areas , in such forms as population codes and distributed representations @xcite . on behavioral time scales , </S>",
    "<S> neural information is stored in temporal patterns of activity as opposed to static markers ; therefore , as information is shared between neurons or brain regions , it is physically instantiated as coordination between entire sequences of neural spikes . </S>",
    "<S> furthermore , neural systems and regions of the brain often require coordinated neural activity to perform important functions ; acting in concert requires multiple neurons or cortical areas to share information @xcite . </S>",
    "<S> thus , if we want to measure the dynamic network - wide behavior of neurons and test hypotheses about them , we need reliable , practical methods to detect and quantify behavioral coordination and the associated information sharing across multiple neural units . these would be especially useful in testing ideas about how particular forms of coordination relate to distributed coding ( e.g. , that of @xcite ) . </S>",
    "<S> current techniques to analyze relations among spike trains handle only pairs of neurons , so we further need a method which is extendible to analyze the coordination in the network , system , or region as a whole . here </S>",
    "<S> we propose a new measure of behavioral coordination and information sharing , _ informational coherence _ </S>",
    "<S> , based on the notion of dynamical state .    </S>",
    "<S> section [ sec : synch - measures ] argues that coordinated behavior in neural systems is often not captured by existing measures of synchronization or correlation , and that something sensitive to nonlinear , stochastic , predictive relationships is needed . </S>",
    "<S> section [ sec : states - and - ic ] defines informational coherence as the ( normalized ) mutual information between the dynamical states of two systems and explains how looking at the states , rather than just observables , fulfills the needs laid out in section [ sec : synch - measures ] . since we rarely know the right states </S>",
    "<S> a prori _ , section [ sec : reconstruction ] briefly describes how we reconstruct effective state spaces from data . </S>",
    "<S> section [ sec : estimating - states - and - ic ] gives some details about how we calculate the informational coherence and approximate the global information stored in the network . </S>",
    "<S> section [ sec : example ] applies our method to a model system ( a biophysically detailed conductance - based model ) comparing our results to those of more familiar second - order statistics . in the interest of space </S>",
    "<S> , we omit proofs and a full discussion of the existing literature , giving only minimal references here ; proofs and references will appear in a longer paper now in preparation . </S>"
  ]
}