{
  "article_text": [
    "adaptive filters have been widely used in active noise control , channel equalization , adaptive inverse control , echo cancellation , noise cancellation , linear prediction and system identification .",
    "least mean squares ( lms ) , recursive least squares ( rls ) and their variations are the widely used adaptive algorithms @xcite-@xcite .",
    "the least mean squares ( lms ) algorithm adjusts the filter coefficients to minimize the least mean squares of the error signal . compared to recursive least squares ( rls ) algorithm , the lms algorithm has a slower convergence speed , however it does not involve any matrix operations .",
    "therefore , the lms algorithm requires fewer computational resources and memory than the rls algorithm .",
    "the implementation of the lms algorithm also is less complicated than the rls algorithm .    in many applications",
    ", the unknown system response can be assumed to be sparse , which means only a small fraction of the coefficients are different from zero .",
    "for example , sparse echo cancellation @xcite , internet telephony , sparse nonlinear channel estimation @xcite , sparse system estimation @xcite-@xcite .",
    "zero - attracting filters exploiting the sparse nature of the system can improve the performance of the adaptive filter .",
    "a similar idea is used in linear prediction of speech in @xcite to obtain better coding properties by enhancing the sparsity of residual and predictor .",
    "motivated by lasso @xcite and recent progress in compressive sensing @xcite,@xcite , the zero - attracting lms ( za - lms ) algorithm and reweighted zero - attracting lms ( rza - lms ) were proposed in @xcite , for sparse system identification .",
    "za - lms is a combination of lms algorithm with @xmath0-norm penalty of the coefficient vector .",
    "the reweighted @xmath0-norm minimization algorithm is first proposed in @xcite to enhance the sparsity of the system .",
    "convergence analysis of za - lms is given in @xcite .",
    "to reduce the eigenvalue spread of the input signal correlation matrix , za - lms and rza - lms are extended to the transform domain in @xcite .",
    "the lms type algorithms generally provide a more accurate solution , with less mis - adjustment when the noise is gaussian , however , they are very sensitive to the outliers . on the other hand , though the convergence rate of the lad type algorithms is low , they are robust to the outliers , such as the @xmath1-stable noise .",
    "the @xmath1-stable noise model is a more generalized model @xcite-@xcite , such that the gaussian model can be seen as an special case of @xmath1-stable model by setting the characteristic exponent @xmath2 .    in this paper , the performance of the least mean absolute deviation ( lad ) @xcite , zero - attracting lad ( za - lad ) @xcite and reweighted zero - attracting lad ( rza - lad ) algorithms are evaluated for linear time varying system identification under the non - gaussian ( @xmath1-stable ) noise environments .",
    "the rest of the paper is organized as follows .",
    "the lad , za - lad and rza - lad algorithms are given in section ii .",
    "a brief discussion about the parameter selection is provided in section iii . in section iv",
    ", we evaluate the performance of the three algorithms for linear time varying system identification under gaussian and non - gaussian noise environments .",
    "conclusions of our work and some further research directions are provided in section v.",
    "notations : @xmath3 is the system input signal at time @xmath4 , @xmath5 is the coefficient vector and @xmath6 is the desire signal at time @xmath4 , the superscripts @xmath7 denotes the transpose and @xmath8 denotes the sign function . @xmath9 and @xmath10 denote the absolute value and @xmath0-norm , respectively .",
    "the output of the lad algorithm is given by @xmath11 the cost function is formulated as @xmath12 where @xmath13 is the error signal .",
    "using the stochastic gradient approach , the filter weights are estimated iteratively by @xmath14 where @xmath15 substituting ( [ eq_4 ] ) into ( [ eq_3 ] ) , we obtain the updating equation for the lad algorithm : @xmath16 where @xmath17 is the step size that should be chosen carefully to ensure convergence .",
    "the lad algorithm is summarized in algorithm [ alg1 ] .",
    "inputs : @xmath17 initialization : @xmath18 @xmath19 @xmath20 @xmath21      the output of the za - lad algorithm is @xmath22 for the za - lad algorithm , @xmath0 norm penalty is used to explore the sparse nature of the filter coefficients .",
    "the cost function is formulated as @xmath23 where @xmath24 is the error signal , @xmath25 is the regularization parameter .    using the stochastic gradient approach ,",
    "the filter weights are estimated iteratively by @xmath26 where @xmath27 substituting ( [ eq_9 ] ) into ( [ eq_8 ] ) , we obtain the updating equation for the za - lad algorithm : @xmath28 where @xmath29 is the stepsize and @xmath30 .",
    "the za - lad algorithm is summarized in algorithm [ alg2 ] .",
    "inputs : @xmath31 initialization : @xmath32 @xmath33 @xmath34 @xmath35      the output of the rza - lad algorithm is @xmath36    compared with @xmath0 norm , log - sum penalty function is a better approximation for the sparsity of the filter coefficients . for the rza - lad algorithm ,",
    "log - sum penalty function is used as the coefficient penalty function .",
    "the cost function is formulated as @xmath37_m|\\big ) \\nonumber\\\\ & = & \\left| e_3(n ) \\right| + \\alpha_3 \\sum_{m=1}^{m } \\log \\big(1 + \\varepsilon_3 |[\\mathbf{w}_3(n)]_m|\\big),\\end{aligned}\\ ] ] where @xmath38 is the error signal , @xmath39 is the regularization parameter , @xmath40 is a positive number and @xmath41 is the number of elements in weight vector @xmath42 .    using the stochastic gradient approach ,",
    "the filter weights are estimated iteratively by @xmath43 and @xmath44 where @xmath45 and the @xmath46th element of @xmath47 is given by @xmath48_m =   \\frac { \\textrm{sgn } \\big([\\mathbf{w}_3(n)]_m\\big)}{1+\\varepsilon_3 \\left|[\\mathbf{w}_3(n)]_m \\right|},\\ ; 1 \\leq m \\leq m.\\ ] ] substituting ( [ eq_14 ] ) into ( [ eq_13 ] ) , we obtain the updating equation for the rza - lad algorithm .",
    "the updating equation is given by @xmath49 where @xmath50 is the stepsize and @xmath51 and @xmath47 is given in ( [ eq_15a ] ) .",
    "the rza - lad algorithm is summarized in algorithm [ alg3 ] .",
    "inputs : @xmath52 initialization : @xmath53 @xmath54 @xmath55 @xmath56",
    "the za - lad and rza - lad algorithms are regularization based adaptive algorithms . for the regularization based adaptive algorithms , the cost function",
    "is defined by combining the the @xmath57 norm of the error signal with the @xmath58 norm penalty of the coefficient vector .",
    "regularization plays a fundamental role in adaptive filtering , however the better performance is not obtained if the regularization parameter is not chosen properly .",
    "according to ( [ eq_10 ] ) and ( [ eq_15 ] ) , the parameter @xmath59 denotes the importance of the @xmath0 norm term or the intensity of attraction .",
    "so a large @xmath59 results in a faster convergence since the intensity of attraction increases as @xmath59 increases .",
    "however , steady - state misalignment increases as @xmath59 increases .",
    "therefore , the parameter @xmath59 are determined by the trade - off between adaptation speed and adaptation quality in particular applications .    one possible way to find the optimal regularization parameter @xmath1 for four lms - type algorithms",
    "is given in @xcite .",
    "the performance of the regularization based algorithms may be improved by using an adaptive regularization factor . to verify the performance of the lad type algorithms , a fixed regularization factor is used .",
    "we introduce the parameter @xmath63 in ( [ eq_12 ] ) in order to provide stability and to ensure that a zero - valued component in @xmath5 does not strictly prohibit a nonzero estimate at the next step .",
    "as empirically demonstrated in section [ s4 - 1 ] , the rza - lad algorithm is robust to the choice of @xmath62 .",
    "as mentioned in @xcite , @xmath62 should be set slightly smaller than the expected nonzero magnitudes of @xmath5 .",
    "in this section , the performance of the proposed method is assessed via computer simulations .",
    "two experiments are designed to demonstrate the steady - state performance , convergence rate , and tracking ability of thelad , za - lad , rza - lad , lms , za - lms and rza - lad algorithms . for comparison purposes",
    ", we also implement the lms , rza - lms and za - lms algorithms .",
    "mean square deviation ( msd ) is taken as a metric , which is defined as @xmath64 where @xmath65 is the coefficient estimated in the @xmath66th independent trial and @xmath67 is the real coefficient of the system .",
    "the noise is non - gaussian with an @xmath1-stable distribution .",
    "the matlab @xmath1-stable distribution toolbox written by mark s. veillette is used to generate the noise with @xmath1-stable distribution .",
    "the characteristic exponent @xmath68 , the symmetry parameter @xmath69 , the location parameter @xmath70 and the dispersion ( also called the scale ) @xmath71 .",
    "the six filters ( lad , za - lad , rza - lad , lms , za - lms and rza - lad ) are run 500 times independently .",
    "the parameters are given in table [ table1 ] .",
    ".simulation parameters [ cols=\"^,^,^,^,^\",options=\"header \" , ]      to verify the effect of parameter @xmath62 , the following experiment is considered .",
    "a linear time invariant system with 16 coefficients is considered .",
    "initially , we set the @xmath72 tap with value 1 and the others to zero , making the system have a sparsity of 1/16 .",
    "the performance of lad , za - lad and rza - lad with different @xmath62 is compared when the input signal is a white gaussian random sequence with variance of 1 and the generalized signal - to - noise ratio ( gsnr ) as defined in @xcite is 10 @xmath73 .",
    "the average msd is shown in fig.[fig1_r ] .    .",
    "the input signal is gaussian , noise is @xmath1 stable , characteristic exponent @xmath74 , gsnr = 10db , 500 independent trials.,scaledwidth=50.0% ]    from fig.[fig1_r ] , we can see that , the performance of za - lad and rza - lad is similar when @xmath75 is used .",
    "it is because for a small @xmath62 , @xmath76 .",
    "the update functions for za - lad ( [ eq_10 ] ) and rza - lad ( [ eq_15 ] ) are equivalent , then the rza - lad algorithm reduces to the conventional za - lad algorithm . compared with lad and za - lad , better steady - state performance is obtained for the rza - lad algorithm when @xmath77 or @xmath78 .",
    "another observation is that , the convergence rate of rza - lad is slower when a larger @xmath62 is used .",
    "it is because , @xmath62 is in the denominator part of the third term of the rza - lad update function ( [ eq_15 ] ) .",
    "a larger @xmath62 will make the step - size in the third term of ( [ eq_15 ] ) smaller .",
    "the convergence rate is slower due to the relatively smaller step - size .      in the second experiment ,",
    "similar to the simulation setup used in @xcite , a linear time varying system with 16 coefficients is considered .",
    "initially , we set the @xmath72 tap with value 1 and the others to zero , making the system have a sparsity of 1/16 . after 3000 iterations , all the odd taps",
    "are set to 1 , while all the even taps remains to be zero , i.e. , a sparsity of 8/16 .",
    "after 6000 iterations all the even taps are set with value -1 while all the odd taps are maintained to be 1 , leaving a completely non - sparse system .",
    "the performance of lad , za - lad , rza - lad , lms , za - lms and rza - lms is compared when the input signal is a white gaussian random sequence with variance of 1 and the generalized signal - to - noise ratio ( gsnr ) is 10 @xmath73 .",
    "the average msd is shown in fig.[fig1_g ] .        from fig.[fig1_g ] , we can see that , when the system is very sparse ( before the 3000@xmath79 iterations ) , rza - lad yields fastest convergence rate and best steady - state performance compared with lad and za - lad . and the msd of the lad and za - lad algorithms are similar .",
    "after the 3000@xmath79 iteration , as the number of non - zero efficients increases to eight , a similar performance is obtained for lad and za - lad , while the rza - lad algorithm maintains the best performance among the three algorithms .",
    "after 6000@xmath79 iterations , the system is completely non - sparse , the performance of all the three algorithms are similar . since the impulsive noise is considered , all the three lms type algorithms are not converged .",
    "the system in the second experiment is the same as the second one , except the switching times are set to the 20000@xmath79 iteration and 40000@xmath79 iteration .",
    "the performance of lad , za - lad , rza - lad , lms , za - lms and rza - lms is compared when the input signal @xmath80 is now a correlated signal generated by @xmath81 , where @xmath82 is a white gaussian noise .",
    "the average msd is shown in fig.[fig2_ng ] .",
    "stable , characteristic exponent @xmath74 , gsnr = 10db , 500 independent trials.,scaledwidth=50.0% ]    from fig.[fig2_ng ] we can see that , when the system is very sparse ( before the 20000@xmath79 iterations ) , both za - lad and rza - lad yield faster convergence rate and better steady - state performance than lad . and the msd of the rza - lad algorithm is lower than the za - lad algorithm . after the 20000@xmath79 iteration ,",
    "as the number of non - zero efficients increases to eight , the performance of the za - lad deteriorated , a similar performance is obtained for lad and za - lad , while the rza - lad algorithm maintains the best performance among the three algorithms .",
    "after th 40000@xmath79 iteration , the system is completely non - sparse , the performance of the lad and rza - lad algorithms are similar .",
    "the convergence rate of the rza - lad algorithm is low compared with lad .",
    "all the three lms type algorithms are still not converged due to the impulsive noise .",
    "in this paper , the performance of lad , za - lad and rza - lad are evaluated for linear time varying system identification under the gaussian and non - gaussian ( @xmath1-stable ) noise environments .",
    "the coefficients are updated using fixed stepsize .",
    "better performance is obtained for the za - lad type algorithms by exploiting the sparse nature of the system .",
    "rza - lad performs best among all the three algorithms , even when the system is non - sparse .",
    "furthermore , the za - lad type algorithms are robust to the non - gaussian noise with @xmath1-stable distribution .",
    "there are two potential further research directions , the first one is evaluating the performance of za - lad type algorithms using variable stepsize and automatically adapted regularization parameters to ensure reliable performance across a broad array of signals .",
    "the second one is extending the za - lad type algorithms to nonlinear system such as nonlinear system identification using volterra filter .",
    "arenas - garcia , j. and figueiras - vidal , a.r .",
    ": adaptive combination of proportionate filters for sparse echo cancellation .",
    "ieee trans . on audio , speech , and language processing .",
    "17(6 ) : pp .",
    "1087 - 1098 ( 2009 )    kalouptsidis , n. et al .",
    ": adaptive algorithms for sparse nonlinear channel estimation . in statistical signal processing . in proc .",
    "ieee workshop on statistical signal processing , cardiff , wales , uk : pp .",
    "221 - 224 ( 2009 )      slavakis , k. kopsinis , y. and theodoridis . s. : adaptive algorithm for sparse system identification using projections onto weighted @xmath0 balls . in proc .",
    "ieee icassp , texas , usa : pp .",
    "3742 - 3745 ( 2010 )                        obrien , m.s .",
    "sinclair , a.n . and kramer . s.m .",
    ": high resolution deconvolution using least - absolute - values minimization . in proc .",
    "ieee , ultrasonics symposium , honolulu , hi , usa , vol.2 , pp .",
    "1151 - 1156 ( 1990 )"
  ],
  "abstract_text": [
    "<S> in this paper , the @xmath0 norm penalty on the filter coefficients is incorporated in the least mean absolute deviation ( lad ) algorithm to improve the performance of the lad algorithm . </S>",
    "<S> the performance of lad , zero - attracting lad ( za - lad ) and reweighted zero - attracting lad ( rza - lad ) are evaluated for linear time varying system identification under the non - gaussian ( @xmath1-stable ) noise environments . </S>",
    "<S> effectiveness of the za - lad type algorithms is demonstrated through computer simulations .    </S>",
    "<S> least mean absolute deviation ( lad ) , zero - attracting lad , reweighted zero - attracting lad , sparse system identification , @xmath0 norm , @xmath1-stable noise </S>"
  ]
}