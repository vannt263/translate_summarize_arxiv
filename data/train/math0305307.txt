{
  "article_text": [
    "within the financial industry forward rate curves play a central role in fixed - income derivative pricing and risk management .",
    "notwithstanding , such curves are not empirical or directly measurable objects but rather useful abstract concepts from where observed prices can be derived . moreover , given a finite set of market prices , we can construct in general an infinite number of compatible forward rate curves .",
    "to avoid such ambiguity several approaches have been proposed in the literature trying to capture a reasonable  or natural  functional form within the set of compatible possibilities .    in historical order",
    "the first kind of methods proposed to solve this problem make use of the so - called parametric approach . in this approach a particular functional form for the forward rate curve",
    "is assumed leaving a certain number of free parameters to be fixed from the calculation of a given set of quoted prices .",
    "an extensive literature exists advocating for this approach",
    ". we can cite as examples the works of mcculloc @xcite , vasicek and fong @xcite , chambers , carleton and waldman @xcite , shea @xcite , nelson and siegel @xcite and more recently the works of svensson @xcite , fisher , nychka and zervos @xcite and waggoner @xcite .",
    "in most of these works we notice the privileged role played by polynomial and exponential splines as the preferred functional forms for the forward rate curves .",
    "the second kind of methods has been termed in the literature as non - parametric or maximum - smoothness approach . here instead of advocating for an a priori functional form for the forward rate a given measure of smoothness is chosen and then the forward rate curve is obtained as the one maximizing this measure subject to the constraints imposed by market prices .",
    "examples where these methods have been investigated include the works of adams and van deventer @xcite , delbaen and lorimier @xcite , kian guan lim , qin xiao et al @xcite , frishling and yamamura @xcite and yekutieli @xcite . in these",
    "works three different smoothing measures have been proposed .",
    "we also have the works of forsgren @xcite and kwon @xcite that generalize these methodologies and clarify the connection between splines and certain smoothness measures .",
    "finally we point out the work of wets , bianchi and yang @xcite that can be located somewhere in - between both approaches since here the number of functional parameters is finite ( albeit arbitrarily large ) and the functional behavior is restricted to a subfamily of @xmath0 curves .",
    "the purpose of this article is two - fold : firstly we want to present an efficient maximum - smoothing algorithm that handles the presence of spreads and implements the positivity constraint .",
    "secondly we want to investigate the predictive power of a linear combination of two quadratic measures , namely the one proposed by delbaen et al . and frishling et al . @xcite and the one by adams and van deventer @xcite . here",
    "it is worth remarking that once the compatibility with market prices is fulfilled the only guiding principle that should be taken as definition of reasonable  or natural  is the predictive power and not other ad - hoc criteria .    in this article",
    "we will only use as constraining data coupon bearing bonds .",
    "the inclusion of treasury bills , zero coupon bonds or bill futures is straightforward and amounts to adding the corresponding linear constraints . since our objective in this article is focusing on an algorithm dealing with non - linear constraints and inequality constraints ( spreads and positivity constraint ) we have not included such data .",
    "with these objectives in mind we organize the article as follows :    in section [ sec algorithm ] we present the objective function that we will use throughout the article and we establish the basic notation . in this section",
    "we present a sketch of the complete algorithm leaving the details for the appendices . in section",
    "[ sec results ] we present the results of the article including examples where the absence of the positivity constraint or the adequate spreads leads to negative rates . here",
    "we present also a study of the predictive power of the one - parametric family of smoothness measures that include as extreme cases the measures used by delbaen et al .",
    ", frishling et al . and adams and van deventer . finally in section [ sec conclusions ] we present the conclusions",
    "a bond , @xmath1 , is an instrument that gives future coupons , @xmath2 , at time stages @xmath3 and a final payment , @xmath4 we mean the complete last cash flow of bond @xmath1 , typically that includes a principal plus a last coupon . ] .",
    "the bond price , @xmath5 , can be determined from the discrete forward rate curve , @xmath6 , as follows @xmath7 where @xmath8 is the length of the time period between time stage @xmath9 and @xmath10 ( in our implementation we have used @xmath11 day ) .",
    "the objective function , or smoothing measure , is defined as a linear combination of the one used by delbaen et al . and frishling et al .",
    "( df ) @xcite and the one used by adams and van deventer ( ad ) @xcite @xmath12 the first term in eq.([objective ] ) ( df ) is a discrete approximation of the integral of the square of the first derivative of @xmath13 and the second term ( ad ) is a discrete approximation of the integral of the square of the second derivative of @xmath13 .",
    "this objective function is to be minimized subject to the consistency constraints@xmath14 where @xmath15 is to be determined along with @xmath13 and where we have used the definitions @xmath16 with @xmath17 @xmath18 the respective bid and ask prices of bond @xmath1 ( @xmath19 ) .",
    "note that the equality constraint given by eq.([eq_const ] ) is just eq.([bondprice ] ) rewritten taking logarithms and using the definitions ( [ definitions ] ) .",
    "eq.([ineq_constr ] ) introduces two inequality constraints .",
    "the first one is the positivity constraint over the forward rate curve and the second one is the requirement that the single price given by eq.([bondprice ] ) must lie in - between the bid and ask prices .",
    "we define the spread of bond @xmath1 as the quantity @xmath20 .",
    "we take the largest time to maturity in eq.([objective ] ) equal to the largest time to maturity in the constraining dataset , namely@xmath21    the constraints reflecting bond prices ( [ bondprice ] ) have been rewritten in a way such that they become linear when no coupons are present ( @xmath22 for a zero  coupon bond @xmath1 ) .",
    "constraints given by eqs.([eq_const ] ) and ( [ ineq_constr ] ) are moved to the objective function defining@xmath23   -\\mu\\sum _ { r=1}^{n}\\ln\\left (   f_{r}\\right )   -\\tilde{\\mu}\\sum_{j=1}^{m}\\left ( \\ln\\left (   \\rho_{j}-\\rho_{j}^{b}\\right )   + \\ln\\left (   \\rho_{j}^{a}-\\rho _ { j}\\right )   \\right )   , \\label{total_obj}\\ ] ] with the lagrange multipliers @xmath24 @xmath25 and the logarithmic barriers with parameters @xmath26 and @xmath27 ( in the solution procedure we take @xmath28 , @xmath29 ) .",
    "the use of log barriers to deal with inequality constraints is a standard methodology in interior point methods for optimization problems @xcite .",
    "an explanation of this methodology adapted to our problem is given in appendix a. briefly the minimization algorithm is structured as follows:@xmath30   } \\text { , } \\tilde{\\mu}^{\\left [   0\\right ]   } \\text { and set ( } f,\\rho\\text { ) } = \\text{(}f^{\\left [   0\\right ]   } , \\rho^{\\left [   0\\right ] } \\text { ) } \\nonumber\\\\ &   \\text{with the seed ( } f^{\\left [   0\\right ]   } , \\rho^{\\left [   0\\right ] } \\text { ) satisfying the inequality constraints ( \\ref{ineq_constr } ) .",
    "let } k=0.\\nonumber\\\\ \\text{step 1}\\text { : }   &   \\text{make a second order approximation of } z\\text { at ( } f^{\\left [   k\\right ]   } , \\rho^{\\left [   k\\right ]   } \\text { ) ( see appendix a).}\\nonumber\\\\[0.03 in ] \\text{step 2}\\text { : }   &   \\text{determine the newton step ( } \\hat{f}^{\\left [ k+1\\right ]   } , \\hat{\\rho}^{\\left [   k+1\\right ]   } \\text { ) using dynamic programming ( see appendix b).}\\nonumber\\\\[0.03 in ] \\text{step 3}\\text { : }   &   \\text{modify ( } \\hat{f}^{\\left [   k+1\\right ]   } , \\hat{\\rho}^{\\left [   k+1\\right ]   } \\text { ) to get a solution ( } f^{\\left [ k+1\\right ]   } , \\rho^{\\left [   k+1\\right ]   } \\text { ) that satisfies the inequality}\\nonumber\\\\ &   \\text{constraints ( \\ref{ineq_constr } ) .",
    "update log barriers ( } \\mu^{\\left [ k\\right ]   } \\rightarrow0\\text { and } \\tilde{\\mu}^{\\left [   k\\right ]   } \\rightarrow0\\text { as } k\\rightarrow\\infty\\text { ) and}\\nonumber\\\\ &   \\text{calculate the values of } w\\text { and of constraints ( \\ref{eq_const } ) .",
    "check if a termination criterion}\\nonumber\\\\ &   \\text{is satisfied , otherwise let } k = k+1\\text { and go to step 1 ( see appendix c ) . } \\label{algorithm}\\ ] ] computing times involved in step 2 are summarized in subsection b.1 .",
    "the solution typically stabilizes in approximately 6 iterations as can be seen in fig.([graph2 ] ) . on a pentium 4 , 2.4 ghz computer",
    "the algorithm coded in c++ takes around 1/4 sec . to compute a forward rate curve like any of the ones seen in fig.([graph1 ] ) .",
    "( [ objective ] ) ) vs. newton iterations ( starting from the seeds : @xmath31}=0.04/year$ ] , @xmath32}=(\\rho_{j}^{a}+\\rho_{j}^{b})/2 $ ] ) .",
    "the plots correspond to the forward rate curves of fig.([graph1 ] ) .",
    "the converge behavior is affected by the termination criterion dictated by eq.([criterion ] ) and by the initial values of the log barriers dictated by eq.([values ] ) .",
    "note the exponential convergence of the algorithm in the first steps .",
    "note also that on friday 06 the minimal solution for ad measure is , for all practical purposes , a straight line . , width=566 ]     year@xmath33 , @xmath34 ) while the ones in plot ( b ) correspond to the ad measure ( @xmath35 , @xmath36 year@xmath37 ) . in both plots",
    "we have used a tolerance spread of 1% ( @xmath38 ) .",
    ", width=566 ]",
    "in this section we present some examples of the behavior of algorithm ( [ algorithm ] ) and we investigate the predictive power of the resulting forward rate curves . in fig.([graph1 ] ) we present a series of forward rate curves calculated using df and ad smoothing measures . there",
    "we can see that for both measures the resulting curves share some similar traits like the positions of most peaks and dips .",
    "clear differences between both sets of curves are found at their end - points and in their behavior in the presence of high spreads . in the set of curves obtained from the df measure we have vanishing first derivative at the end - points and curves that tend to constants for high spreads .",
    "for the ad measure we have vanishing second and third derivatives at end - points @xcite and curves for high enough spreads given by straight lines . from the financial point of view these features are , in principle , just different aesthetic possibilities . in order to choose a particular measure",
    "the guiding principles should be , in the first place , the fulfillment of the consistency constraints given by eqs.([eq_const]-[ineq_constr ] ) and after this is guaranteed the predictive performance .",
    "let us start now with the analysis of consistency .",
    "as can be found in @xcite if we do not consider the positivity constraint the local minima of objective ( [ objective ] ) are given by exponential splines with exponents @xmath39 or polynomial splines of order 2 or 4 when @xmath40 or @xmath41 are respectively zero .",
    "the main problem with these exponential or polynomial splines is that there is no warranty that they fulfill the positivity constraint .",
    "negative rates are not admissible in the absence of arbitrage opportunities and the risk of obtaining this unwanted feature is illustrated in fig.([graph3 ] ) . in this figure",
    "we have concentrated on the swedish bond data on monday , july 09 , 2001 .",
    "there we have tested three spread patterns for both df and ad measures with and without the positivity constraint . from these plots",
    "it is evident that the inclusion of spreads in the calculation of the forward rates is a necessary ingredient that can have a major impact in the resulting functional behavior .",
    "once we have an algorithm that insures the consistency of forward rates we can concentrate on the predictive accuracy of different measures .",
    "however , before starting the analysis of this issue let us make a brief digression to comment a point regarding measure ( [ objective ] ) . if we want to have both @xmath41 and @xmath40 different from zero and we want to compare the effects of each term it is important to realize that df and ad measures scale differently under changes of time units . in other words , @xmath41 and @xmath40 have different units . a practical way to define their units is to consider the objective @xmath42 as an adimensional quantity . by doing so and remembering that @xmath13 has units of inverse time , it is immediate to obtain that @xmath41 has units of time@xmath33 and @xmath40 units of time@xmath37 .",
    "the importance of keeping this in mind becomes apparent in results like the ones presented in figs.([graph4 ] ) and ( [ graph5 ] ) .",
    "figs.([graph4 ] ) and ( [ graph5 ] ) summarize our results regarding the predictive accuracy of the algorithm as a function @xmath43 there it is clear that the characteristic time span where df and ad compete is not the day or the century , but clearly the year . to construct these figures",
    "we have calculated the forward rate curves for different values of @xmath44 when one bond is removed from the constraining dataset .",
    "the price of this missing bond is used afterwards as a benchmark to test the accuracy of the resulting curves .",
    "since we are interested in the statistical performance we have done such comparison for 335 consecutive trading days starting on wednesday , november 08 , 2000 and ending on thursday , march 07 , 2002 .",
    "we are also interested in studying the impact of spreads in the constraining dataset over the predictive accuracy . therefore we present our results for three spread patterns , namely constant spreads of 0% , 0.5% and 1% in the constraining dataset .",
    "fig.([graph4 ] ) concentrates on the predictive accuracy for the first 9 bonds of table ( [ table2 ] ) and fig.([graph5 ] ) presents the same analysis for the remaining 2 bonds of table ( [ table2 ] ) .",
    "these last 2 bonds are the ones with the larger maturities in the complete dataset . in particular for the last one with the largest maturity we have to decide upon the methodology to extrapolate the forward curve outside the range of the constraining dataset .",
    "therefore in fig.([graph5 ] ) we present the prediction accuracy using constant extrapolation from the last maturity in the constraining dataset and @xmath42-generated  extrapolation that consists in utilizing the @xmath42-optimal forward rate curve even outside the range of the constraining dataset .     and [ table2 ] ) and allowing for a large 5 % in the rest ( almost unconstraining this subset ) . doing so we observe in ( b1 ) and ( b2 ) curves that are still negative in the region where the nominal cash flows of the 0-spread bonds take place .",
    ", width=604 ]    .",
    "these prediction errors are obtained removing the indicated bonds from the constraining dataset .",
    "the statistical sample comprise the forward curves corresponding to 335 consecutive trading days starting on wednesday , november 08 , 2000 and ending on thursday , march 07 , 2002 . within this set of days",
    "we calculate the relative errors of the predicted prices given by @xmath45 where @xmath46 is the actual price of the bond given by the market and @xmath47 is the price predicted using the prices of all other bonds in the dataset .",
    "hence in plots ( a1 ) , ( a2 ) and ( a3 ) we show the average of the absolute value of these errors and the plots ( b1 ) , ( b2 ) and ( b3 ) just their average .",
    "consecutive columns show averages obtained using spreads of @xmath48 , @xmath49 and @xmath50 in the constraining dataset .",
    "the set of predicted bonds shown in this figure is given by the first 9 bonds of table [ table2 ] .",
    "the remaining bonds of this table ( the last two bonds with the larger maturities ) are analyzed in fig.([graph5 ] ) .",
    "note how prediction errors tend to decrease when spreads are considered albeit not significantly ( compare with fig.([graph5 ] ) ) .",
    "note also that in the absence of spreads the df measure ( @xmath51 ) systematically exhibits a better performance than the ad one ( @xmath52)).,width=566 ]    ) but now the plots correspond to the last two bonds of table [ table2 ] that are the ones with larger maturities within our dataset ( we have also included the bond with the shortest maturity to facilitate the comparison with fig.([graph4 ] ) ) . for the bond with the largest maturity",
    "the forward curve has to be extrapolated to reach its last cash flows ( see fig.([graph3 ] ) ) .",
    "we consider two extrapolation possibilities , one where we continue the forward rate curve from the last cash flow as a constant and the other where the curve is dictated by the minimization of functional @xmath42 even beyond the last cash flow in the constraining dataset .",
    "note that constant extrapolation gives better results than w - generated extrapolation in the region close to the ad measure ( @xmath52 ) . again like in fig.([graph4 ] ) we observe that in the absence of spreads the df measure ( @xmath51 ) systematically exhibits a better performance than the ad one .",
    "the most striking difference with fig.([graph4 ] ) is that here we observe that for these two long maturing bonds the introduction of spreads drastically reduce the prediction error all along the family of measures.,width=566 ]",
    "in this article we have presented a non - linear dynamic programming algorithm designed for the calculation of positive definite forward rate curves using data with or without spreads .",
    "we have included multiple details of the algorithm aiming at practitioners not familiar with the techniques of dynamic programming .",
    "we have illustrated the results of this algorithms using the swedish bond data for a one - parametric family of smoothness measures .",
    "the results and conclusions are the following :    * the proposed algorithm calculates forward rate curves in real time and admits , without time or complexity penalizations , the use of any non - linear _ local _ objective function . since it also handles non - linear constraints it is possible to include within the constraining dataset any derivative products with prices bearing some dependence on forward rates .",
    "* this is the first algorithm proposed in the literature that implements the positivity constraint in the maximum smoothness framework . to the knowledge of the authors",
    "the only other work that implements such constraint outside this framework is the one of wets et al . @xcite .",
    "the proposal in @xcite has the advantage of using simple linear programming but do not consider the presence of spreads minimizing instead the sum over the _ modulus _ of the difference between calculated prices and market prices .",
    "* for the objective functions and constraining datasets like the ones we have used or more generally for the ones studied in @xcite the algorithm proposed in that reference offers better computing times at the expense of ignoring the positivity constraint .",
    "essentially the complexity in @xcite is @xmath53 and in ours is @xmath54 @xmath55 where @xmath54 is the number of time steps and @xmath56 the number of constraints .",
    "for that reason , when this class of objective functions and constraining datasets are used , a well coded algorithm might try first the proposal in @xcite ( improving its treatment of the spreads using e.g. log - barriers ) and later , only if the result is not positive definite , use our approach .",
    "* since the optimization problem we are solving is non - convex we can not discard the presence of several local minima ( this is independent of the presence of the positivity constraint @xcite ) .",
    "however , we have tested our algorithm starting from different seeds and in all cases we have arrived at the same minima .",
    "these tests included hundreds of searches with initial log - prices given by @xmath57   } = \\rho_{j}^{b}+x\\left ( \\rho_{j}^{a}-\\rho_{j}^{b}\\right )   $ ] ( with @xmath58   $ ] a flat random variable ) and initial forward rates given by several constant and oscillating functions . with this comment",
    "we only want to convey our practical experience and by no means we intend to say that we have exhaustively explored the presence or absence of local minima . *",
    "it is clear that one way to avoid negative rates is just by increasing spreads by hand .",
    "the advantage of using real spreads at a given moment is that one can be sure of being consistent with market prices .",
    "if one observes in fig.([graph3 ] ) the large forward rate variations taking place for different spread patterns no doubts should remain about the relevance of a careful treatment of this issue . * from fig.([graph5 ] ) we conclude that the inclusion of spreads can remarkably improve the accuracy of resulting forward curves in the prediction of market prices of long maturing bonds . in @xcite it was pointed out that the inclusion of spreads notably improved the smoothness of the forward curve . to the knowledge of the authors",
    "this is the first time it is shown that their presence also improves the prediction accuracy .",
    "for that reason we believe that spreads should be considered even when the market data does not provide such information . in that case",
    "the approach should consist in using a cross validation technique to asses the optimal spread minimizing an error criterion based in the prediction of market prices ( for cross validation methods see for example @xcite ) .",
    "one possibility is using the well know leave - one - out  cross - validation to select the optimal spread much in the spirit suggested by figs.([graph4 ] ) and ( [ graph5 ] ) . given a constraining set of @xmath59 products ,",
    "the method actually consists in obtaining @xmath59 forward rate curves for a given spread , each curve leaving out one of the constraining products ( bonds in our case ) but using only the @xmath59 omitted products to compute an error criterion like @xmath60 thus we obtain a quantitative criterion to select an optimal level of spread .",
    "for example for our dataset and our family of models ( measures ) it can be conjectured from figs.([graph4 ] ) and ( [ graph5 ] ) that this optimal level of spread is typically around 0.5% . *",
    "figs.([graph4 ] ) and ( [ graph5 ] ) strongly suggest that for low spread patterns df measure is more accurate than ad one . for bigger spreads results",
    "do not clearly favor any particular measure .",
    "j. m. acknowledges the financial support from _ tekniska hgskolan , inst .",
    "fr fysik och mtteknik _ ,",
    "linkping university , thanks j. shaw ( barclays ) for calling his attention to ref .",
    "@xcite , f. delbaen for ref .",
    "@xcite and j. m. eroles ( hsbc ) for proof reading the manuscript .",
    "j. m. would also like to thank dr . per olov lindberg for the hospitality during his stay at the division of optimization ; linkping university .",
    "the authors would like to thank the referees for their helpful comments on the manuscript .",
    "since the objective function @xmath61 in eq.([total_obj ] ) is a non - quadratic function of @xmath62 , @xmath63 we will use an iterative quadratic approximation ( newton steps ) to find its minima .",
    "we start with feasible seeds @xmath64   } $ ] and @xmath57   } $ ] fulfilling inequality constraints ( [ ineq_constr ] ) and we set up initial log - barriers coefficients @xmath65   } > 0 $ ] and @xmath66 } > 0.$ ] for newton iteration number @xmath67 we define @xmath68   }   &   : = f_{r}^{\\left [   s-1\\right ]   } + \\delta _ { r}^{\\left [   s\\right ]   } , \\nonumber\\\\ \\hat{\\rho}_{j}^{\\left [   s\\right ]   }   &   : = \\rho_{j}^{\\left [   s-1\\right ] } + \\sigma_{j}^{\\left [   s\\right ]   } , \\label{dynamic_sol}\\ ] ] with @xmath69 the hat over @xmath13 and @xmath70 indicate that at each step @xmath67 , @xmath71   } $ ] and @xmath72   } $ ] are the minima of the quadratic approximation and may not fulfill the inequality constraints ( [ ineq_constr ] ) . to assure constraints ( [ ineq_constr ] )",
    "are fulfilled a final redefinition @xmath71   } \\rightarrow f^{\\left [   s\\right ]   } , $ ] @xmath72   } \\rightarrow \\rho^{\\left [   s\\right ]   } $ ] is necessary after each newton step .",
    "this redefinition is explained in appendix c. expanding @xmath61 up to second order in @xmath73   } $ ] and @xmath74   } $ ] we write@xmath75   }   &   : = \\left .",
    "z\\left (   \\hat{f}^{\\left [   s\\right ] } , \\hat{\\rho}^{\\left [   s\\right ]   } , \\lambda^{\\left [   s\\right ]   } \\right ) \\right\\vert _ { o\\left (   2\\right )   } \\nonumber\\\\ &   = \\frac{1}{2}\\delta^{\\left [   s\\right ]   t}q^{\\left [   s\\right ]   } \\delta^{\\left [   s\\right ]   } + \\delta^{\\left [   s\\right ]   t}b^{\\left [   s\\right ] } \\lambda^{\\left [   s\\right ]   } + \\delta^{\\left [   s\\right ]   t}c^{\\left [   s\\right ] } + \\lambda^{\\left [   s\\right ]   t}a^{\\left [   s\\right ]   } \\nonumber\\\\ &   + \\sigma^{\\left [   s\\right ]   t}\\lambda^{\\left [   s\\right ]   } + \\frac{1}{2}\\sigma^{\\left [   s\\right ]   t}m^{\\left [   s\\right ]   } \\sigma^{\\left [   s\\right ] } + \\sigma^{\\left [   s\\right ]   t}d^{\\left [   s\\right ]   } + b^{\\left [   s\\right ]   } , \\label{quadratic}\\ ] ] where @xmath76   } $ ] collects all terms not depending on @xmath77   } $ ] , @xmath78   } $ ] or @xmath79   } $ ] .",
    "we will use square brackets around newton step indices and parenthesis around dynamic programming ones .",
    "let us now work - out the matrices involved in the quadratic approximation . from eqs.([objective ] ) and ( [ quadratic ] ) we immediately obtain@xmath80   t}q^{\\left [   s\\right ]   } \\delta^{\\left [   s\\right ]   } + \\delta^{\\left [   s\\right ]   t}c^{\\left [   s\\right ] } \\\\ &   = \\gamma\\sum_{r=1}^{n-1}\\left (   \\frac{f_{r+1}^{\\left [   s-1\\right ]   } -f_{r}^{\\left [   s-1\\right ]   } } { \\xi_{r}}\\right )   \\left (   \\frac{\\delta _ { r+1}^{\\left [   s\\right ]   } -\\delta_{r}^{\\left [   s\\right ]   } } { \\xi_{r}}\\right ) \\xi_{r}+\\frac{\\gamma}{2}\\sum_{r=1}^{n-1}\\left (   \\frac{\\delta_{r+1}^{\\left [ s\\right ]   } -\\delta_{r}^{\\left [   s\\right ]   } } { \\xi_{r}}\\right )   ^{2}\\xi_{r}\\\\ &   + 4\\varphi\\sum_{r=2}^{n-1}\\left (   \\frac{1}{\\left (   \\xi_{r-1}+\\xi_{r}\\right ) } \\left (   \\frac{1}{\\xi_{r}}f_{r+1}^{\\left [   s-1\\right ]   } + \\frac{1}{\\xi_{r-1}}f_{r-1}^{\\left [   s-1\\right ]   } \\right )   -\\frac{1}{\\xi_{r}\\xi_{r-1}}f_{r}^{\\left [   s-1\\right ]   } \\right ) \\\\ &   \\times\\left (   \\frac{1}{\\left (   \\xi_{r-1}+\\xi_{r}\\right )   } \\left (   \\frac { 1}{\\xi_{r}}\\delta_{r+1}^{\\left [   s\\right ]   } + \\frac{1}{\\xi_{r-1}}\\delta _ { r-1}^{\\left [   s\\right ]   } \\right )   -\\frac{1}{\\xi_{r}\\xi_{r-1}}\\delta _ { r}^{\\left [   s\\right ]   } \\right )   \\xi_{r}\\\\ &   + 2\\varphi\\sum_{r=2}^{n-1}\\left (   \\frac{1}{\\left (   \\xi_{r-1}+\\xi_{r}\\right ) } \\left (   \\frac{1}{\\xi_{r}}\\delta_{r+1}^{\\left [   s\\right ]   } + \\frac{1}{\\xi _ { r-1}}\\delta_{r-1}^{\\left [   s\\right ]   } \\right )   -\\frac{1}{\\xi_{r}\\xi_{r-1}}\\delta_{r}^{\\left [   s\\right ]   } \\right )   ^{2}\\xi_{r},\\end{aligned}\\ ] ] and defining @xmath81   }   &   : = \\sum_{j=1}^{m}\\lambda_{j}^{\\left [ s\\right ]   } \\left [   \\hat{\\rho}_{j}^{\\left [   s\\right ]   } -\\ln\\left ( v_{j}^{\\left [   s\\right ]   } \\right )   + \\sum_{r=1}^{r_{n_{j}}^{\\left (   j\\right ) } -1}\\hat{f}_{r}^{\\left [   s\\right ]   } \\xi_{r}\\right ]   , \\label{cosntr}\\\\ y_{\\mu}^{\\left [   s\\right ]   }   &   : = -\\mu^{\\lbrack s-1]}\\sum_{r=1}^{n}\\ln\\left ( \\hat{f}_{r}^{\\left [   s\\right ]   } \\right )   , \\label{barrier1}\\\\ y_{\\tilde{\\mu}}^{\\left [   s\\right ]   }   &   : = -\\tilde{\\mu}^{\\left [   s-1\\right ] } \\sum_{j=1}^{m}\\left (   \\ln\\left (   \\rho_{j}^{\\left [   s\\right ]   } -\\rho_{j}^{b}\\right )   + \\ln\\left (   \\rho_{j}^{a}-\\rho_{j}^{\\left [   s\\right ]   } \\right ) \\right )   , \\label{barrier2}\\ ] ] we have @xmath81   }   &   = \\sum_{j=1}^{m}\\lambda_{j}^{\\left [ s\\right ]   } \\left [   \\rho_{j}^{\\left [   s-1\\right ]   } + \\sigma_{j}^{\\left [ s\\right ]   } -\\ln\\left (   v_{j}^{\\left [   s-1\\right ]   } \\right )   + \\sum _ { r=1}^{r_{n_{j}}^{\\left (   j\\right )   } -1}\\left (   f_{r}^{\\left [   s-1\\right ] } + \\delta_{r}^{\\left [   s\\right ]   } \\right )   \\xi_{r}\\right . \\\\ &   \\left .",
    "-\\frac{1}{v_{j}^{\\left [   s-1\\right ]   } } \\sum\\limits_{i=1}^{n_{j}-1}\\sum_{r = r_{i}^{\\left (   j\\right )   } } ^{r_{n_{j}}^{\\left (   j\\right )   } -1}\\alpha_{ij}\\exp\\left (   \\sum_{z = r_{i}^{\\left (   j\\right )   } } ^{r_{n_{j}}^{\\left (   j\\right )   } -1}f_{z}^{\\left [   s-1\\right ]   } \\xi_{z}\\right ) \\delta_{r}^{\\left [   s\\right ]   } \\xi_{r}+o\\left (   \\delta_{r}^{\\left [   s\\right ] } { } ^{2}\\right )   \\right ]   , \\\\ y_{\\mu}^{\\left [   s\\right ]   }   &   = -\\mu^{\\left [   s-1\\right ]   } \\sum_{r=1}^{n}\\left (   \\frac{\\delta_{r}^{\\left [   s\\right ]   } } { f_{r}^{\\left [   s-1\\right ] } } -\\frac{1}{2}\\left (   \\frac{\\delta_{r}^{\\left [   s\\right ]   } } { f_{r}^{\\left [ s-1\\right ]   } } \\right )   ^{2}+o\\left (   \\delta_{r}^{\\left [   s\\right ]   } { } ^{3}\\right )   \\right )   , \\\\ y_{\\tilde{\\mu}}^{\\left [   s\\right ]   }   &   = -\\tilde{\\mu}^{\\left [   s-1\\right ] } \\sum_{j=1}^{m}\\left (   \\ln\\left (   \\rho_{j}^{\\left [   s-1\\right ]   } -\\rho_{j}^{b}\\right )   + \\ln\\left (   \\rho_{j}^{a}-\\rho_{j}^{\\left [   s-1\\right ]   } \\right ) \\right ) \\\\ &   + \\tilde{\\mu}^{\\left [   s-1\\right ]   } \\sum_{j=1}^{m}\\left (   \\frac{1}{\\rho _ { j}^{a}-\\rho_{j}^{\\left [   s-1\\right ]   } } -\\frac{1}{\\rho_{j}^{\\left [ s-1\\right ]   } -\\rho_{j}^{b}}\\right )   \\sigma_{j}^{\\left [   s\\right ]   } \\\\ &   + \\frac{\\tilde{\\mu}^{\\left [   s-1\\right ]   } } { 2}\\sum_{j=1}^{m}\\left [   \\frac { 1}{\\left (   \\rho_{j}^{\\left [   s-1\\right ]   } -\\rho_{j}^{a}\\right )   ^{2}}+\\frac{1}{\\left (   \\rho_{j}^{\\left [   s-1\\right ]   } -\\rho_{j}^{b}\\right )   ^{2}}\\right ]   \\sigma_{j}^{\\left [   s\\right ]   2}+o\\left (   \\sigma_{j}^{\\left [ s\\right ]   3}\\right )   .\\end{aligned}\\ ] ] thus defining @xmath82{cc}1 & x\\leq r\\leq y\\\\ 0 & \\mathrm{otherwise}\\end{array } \\right .   , \\qquad\\delta_{i , j}:=\\left\\ { \\begin{array } [ c]{cc}1 & i = j\\\\ 0 & \\mathrm{otherwise}\\end{array } \\right .   , \\ ] ] from above expansions and eq.([quadratic ] ) we immediately obtain@xmath83   }   &   = \\xi_{r}\\chi\\left (   r,1,r_{n_{j}}^{\\left ( j\\right )   }",
    "-1\\right )   -\\frac{1}{v_{j}^{\\left [   s-1\\right ]   } } \\sum \\limits_{i=1}^{n_{j}-1}\\alpha_{ij}\\exp\\left (   \\sum_{z = r_{i}^{\\left (   j\\right ) } } ^{r_{n_{j}}^{\\left (   j\\right )   } -1}f_{z}^{\\left [   s-1\\right ]   } \\xi _ { z}\\right )   \\xi_{r}\\chi\\left (   r , r_{i}^{\\left (   j\\right )   } , r_{n_{j}}^{\\left (   j\\right )   } -1\\right )   ,",
    "\\\\ a_{j}^{\\left [   s\\right ]   }   &   = \\rho_{j}^{\\left [   s-1\\right ]   } -\\ln\\left ( v_{j}^{\\left [   s-1\\right ]   } \\right )   + \\sum_{r=1}^{r_{n_{j}}^{\\left (   j\\right ) } -1}f_{r}^{\\left [   s-1\\right ]   } \\xi_{r},\\\\ q_{r , x}^{\\left [   s\\right ]   }   &   = 4\\varphi\\left [   \\left (   \\frac{\\left ( 1-\\delta_{r,1}\\right )   \\left (   1-\\delta_{r,2}\\right )   \\xi_{r-1}}{\\left ( \\xi_{r-2}+\\xi_{r-1}\\right )   ^{2}\\xi_{r-1}^{2}}+\\frac{\\left (   1-\\delta _ { r , n}\\right )   \\left (   1-\\delta_{r,1}\\right )   \\xi_{r}}{\\xi_{r}^{2}\\xi _ { r-1}^{2}}+\\frac{\\left (   1-\\delta_{r , n-1}\\right )   \\left (   1-\\delta _ { r , n}\\right )   \\xi_{r+1}}{\\left (   \\xi_{r}+\\xi_{r+1}\\right )   ^{2}\\xi_{r}^{2}}\\right )   \\delta_{r , x}\\right . \\\\ &   + \\frac{\\xi_{r+1}\\delta_{r+2,x}}{\\left (   \\xi_{r}+\\xi_{r+1}\\right )   ^{2}\\xi_{r+1}\\xi_{r}}+\\frac{\\xi_{x+1}\\delta_{r-2,x}}{\\left (   \\xi_{x}+\\xi _ { x+1}\\right )   ^{2}\\xi_{x+1}\\xi_{x}}-\\frac{\\left (   1-\\delta_{r,1}\\right ) \\xi_{r}\\delta_{r+1,x}}{\\left (   \\xi_{r-1}+\\xi_{r}\\right )   \\xi_{r-1}\\xi_{r}^{2}}\\\\ &   \\left .   -\\frac{\\left (   1-\\delta_{x,1}\\right )   \\xi_{x}\\delta_{r-1,x}}{\\left (   \\xi_{x-1}+\\xi_{x}\\right )   \\xi_{x-1}\\xi_{x}^{2}}-\\frac{\\left ( 1-\\delta_{r , n}\\right )   \\xi_{r}\\delta_{r-1,x}}{\\left (   \\xi_{r-1}+\\xi _ { r}\\right )   \\xi_{r}\\xi_{r-1}^{2}}-\\frac{\\left (   1-\\delta_{x , n}\\right ) \\xi_{x}\\delta_{r+1,x}}{\\left (   \\xi_{x-1}+\\xi_{x}\\right )   \\xi_{x}\\xi_{x-1}^{2}}\\right ] \\\\ &   + \\gamma\\left [   \\left (   1-\\delta_{r , n}\\right )   \\frac{\\xi_{r}}{\\xi_{r}^{2}}\\delta_{r , x}+\\left (   1-\\delta_{r,1}\\right )   \\frac{\\xi_{r-1}}{\\xi_{r-1}^{2}}\\delta_{r , x}-\\frac{\\xi_{r}}{\\xi_{r}^{2}}\\delta_{r+1,x}-\\frac{\\xi_{x}}{\\xi _ { x}^{2}}\\delta_{r-1,x}\\right ]   + \\frac{\\mu^{\\left [   s-1\\right ]   } } { f_{r}^{\\left [   s-1\\right ]   2}}\\delta_{r , x},\\\\ c_{r}^{\\left [   s\\right ]   }   &   = -\\frac{\\mu^{\\left [   s-1\\right ]   } } { f_{r}^{\\left [   s-1\\right ]   } } + \\gamma\\frac{f_{r}^{\\left [   s-1\\right ] } -f_{r-1}^{\\left [   s-1\\right ]   } } { \\xi_{r-1}}\\frac{\\left (   1-\\delta _ { r,1}\\right )   } { \\xi_{r-1}}\\xi_{r-1}-\\gamma\\frac{f_{r+1}^{\\left [   s-1\\right ] } -f_{r}^{\\left [   s-1\\right ]   } } { \\xi_{r}}\\frac{\\left (   1-\\delta_{r , n}\\right ) } { \\xi_{r}}\\xi_{r}\\\\ &   + 4\\varphi\\left (   \\frac{f_{r}^{\\left [   s-1\\right ]   } -f_{r-1}^{\\left [ s-1\\right ]   } } { \\xi_{r-1}}+\\frac{f_{r-2}^{\\left [   s-1\\right ]   } -f_{r-1}^{\\left [   s-1\\right ]   } } { \\xi_{r-2}}\\right )   \\frac{\\left (   1-\\delta _ { r,1}\\right )   \\left (   1-\\delta_{r,2}\\right )   } { \\left (   \\xi_{r-2}+\\xi _ { r-1}\\right )   ^{2}\\xi_{r-1}}\\xi_{r-1}\\\\ &   + 4\\varphi\\left (   \\frac{f_{r+2}^{\\left [   s-1\\right ]   } -f_{r+1}^{\\left [ s-1\\right ]   } } { \\xi_{r+1}}+\\frac{f_{r}^{\\left [   s-1\\right ]   } -f_{r+1}^{\\left [ s-1\\right ]   } } { \\xi_{r}}\\right )   \\frac{\\left (   1-\\delta_{r , n}\\right )   \\left ( 1-\\delta_{r , n-1}\\right )   } { \\left (   \\xi_{r}+\\xi_{r+1}\\right )   ^{2}\\xi_{r}}\\xi_{r+1}\\\\ &   -4\\varphi\\left (   \\frac{f_{r+1}^{\\left [   s-1\\right ]   } -f_{r}^{\\left [ s-1\\right ]   } } { \\xi_{r}}+\\frac{f_{r-1}^{\\left [   s-1\\right ]   } -f_{r}^{\\left [ s-1\\right ]   } } { \\xi_{r-1}}\\right )   \\frac{\\left (   1-\\delta_{r,1}\\right )   \\left ( 1-\\delta_{r , n}\\right )   } { \\left (   \\xi_{r-1}+\\xi_{r}\\right )   \\xi_{r}\\xi_{r-1}}\\xi_{r},\\\\ m_{j , k}^{\\left [   s\\right ]   }   &   = \\tilde{\\mu}^{\\left [   s-1\\right ]   } \\left [ \\frac{1}{\\left (   \\rho_{j}^{a}-\\rho_{j}^{\\left [   s-1\\right ]   } \\right )   ^{2}}+\\frac{1}{\\left (   \\rho_{j}^{b}-\\rho_{j}^{\\left [   s-1\\right ]   } \\right )   ^{2}}\\right ]   \\delta_{j , k},\\\\ d_{j}^{\\left [   s\\right ]   }   &   = \\tilde{\\mu}^{\\left [   s-1\\right ]   } \\left ( \\frac{1}{\\rho_{j}^{a}-\\rho_{j}^{\\left [   s-1\\right ]   } } + \\frac{1}{\\rho_{j}^{b}-\\rho_{j}^{\\left [   s-1\\right ]   } } \\right )   .\\end{aligned}\\ ] ]",
    "given positive definite symmetric matrices @xmath84 and @xmath85 and the @xmath54-vector @xmath86 and @xmath56-vector @xmath87 , we want to minimize the objective function @xmath88 subject to the set of constraints@xmath89 thus defining a new objective function @xmath90 for any @xmath91 we define@xmath92{cc}1 & x\\neq0\\\\ 0 & x=0 \\end{array } \\right .   , \\ ] ] where e.g. @xmath93 , @xmath94 for diagonal , tridiagonal matrices respectively .",
    "when @xmath95 and @xmath96 then @xmath61 can be solved efficiently with dynamic programming . to set up the notation and for those readers not familiar with dynamic programming let us here briefly explain the basics of this well known method @xcite .",
    "we start defining @xmath97 where @xmath98 satisfies@xmath99 for @xmath100 we use the inductive hypothesis@xmath101 where @xmath102 @xmath103 and @xmath104 .",
    "the final step of this backwards process consists in obtaining @xmath105 and @xmath106 satisfying@xmath107 from eqs.([extrema ] ) and ( [ hypothesis ] ) we obtain @xmath108 and plugging eq.([first ] ) into eq.([fundamental ] ) we obtain@xmath109 hence obtaining@xmath110 where@xmath111{cc}1 & x\\geq0,\\\\ 0 & x<0 .",
    "\\end{array } \\right.\\ ] ]    now from eq.([lagrange ] ) we obtain @xmath112 or@xmath113 and using eq.([g ] ) we obtain@xmath114 finally from @xmath106 and eq.([sig ] ) we obtain @xmath115 and then using eqs.([first ] ) and ( [ q]-[c ] ) we obtain @xmath116 moving forward in the @xmath117 index .",
    "the time necessary to compute all @xmath118 is proportional to@xmath119 to calculate the inverse of @xmath120 , that is a @xmath121 symmetric matrix , we require a computing time proportional to@xmath122 all @xmath123 and @xmath124 require , respectively , computing times proportional to@xmath125 finally the calculation of @xmath126 requires a computing time proportional to @xmath127 in the forward rate calculation typically we have @xmath128 ( in eq.([objective ] ) we have @xmath129 ) and therefore the maximum delay would be given by @xmath130",
    "the newton step obtained from eq.([dynamic_sol ] ) may not fulfill the inequality constraints ( [ ineq_constr ] ) . to satisfy such constraints we search for a @xmath131}$ ] in the interval @xmath132   $ ] such that@xmath133   } + \\alpha^{\\lbrack s]}\\delta_{r}^{\\left [   s\\right ] } \\geq0,\\qquad\\rho_{j}^{b}\\leq\\rho_{j}^{\\left [   s-1\\right ]   } + \\alpha^{\\lbrack s]}\\sigma_{j}^{\\left [   s\\right ]   } \\leq\\rho_{j}^{a}.\\label{constr_alpha}\\ ] ] in order to do this we first determine the maximum @xmath134}$ ] in the interval @xmath132   $ ] satisfying eq.([constr_alpha ] ) .",
    "that is , given the sets of points@xmath135   } \\leq0\\right\\ }   , \\\\",
    "\\tilde{a}_{\\leq } &   : = \\left\\ {   j,\\text { } \\hat{\\rho}_{j}^{\\left [   s\\right ] } \\leq\\rho_{j}^{b}\\right\\ }   , \\\\ \\tilde{a}_{\\geq",
    "} &   : = \\left\\ {   j,\\text { } \\hat{\\rho}_{j}^{\\left [   s\\right ] } \\geq\\rho_{j}^{a}\\right\\ }   , \\end{aligned}\\ ] ] we have@xmath136}=\\min\\left (   \\min\\limits_{r\\in a}\\left (   \\frac { -f_{r}^{\\left [   s-1\\right ]   } } { \\delta_{r}^{\\left [   s\\right ]   } } \\right ) , \\min\\limits_{j\\in\\tilde{a}_{\\leq}}\\left (   \\frac{\\rho_{j}^{b}-\\rho _ { j}^{\\left [   s-1\\right ]   } } { \\sigma_{j}^{\\left [   s\\right ]   } } \\right ) , \\min\\limits_{j\\in\\tilde{a}_{\\geq}}\\left (   \\frac{\\rho_{j}^{a}-\\rho _ { j}^{\\left [   s-1\\right ]   } } { \\sigma_{j}^{\\left [   s\\right ]   } } \\right )   \\right ) , \\ ] ] and then we take@xmath137}:=\\beta\\alpha_{\\max}^{[s]},\\ ] ] with @xmath138 ( in our implementation we have taken @xmath139 that is a standard election in the optimization literature ) .",
    "once we have @xmath140}$ ] we define@xmath141   } &   = f_{r}^{\\left [   s-1\\right ]   } + \\alpha^{\\lbrack s]}\\delta_{r}^{\\left [   s\\right ]   } , \\nonumber\\\\ \\rho_{j}^{\\left [   s\\right ]   } &   = \\rho_{j}^{\\left [   s-1\\right ]   } + \\alpha^{\\lbrack s]}\\sigma_{j}^{\\left [   s\\right ]   } , \\nonumber\\\\ \\mu^{\\left [   s\\right ]   } &   = \\max(\\psi\\left (   \\alpha^{\\lbrack s]}\\right ) \\mu^{\\left [   s-1\\right ]   } , \\mu_{\\min}),\\nonumber\\\\ \\tilde{\\mu}^{\\left [   s\\right ]   } &   = \\max(\\psi\\left (   \\alpha^{\\lbrack s]}\\right )   \\tilde{\\mu}^{\\left [   s-1\\right ]   } , \\tilde{\\mu}_{\\min } ) , \\label{update}\\ ] ] where @xmath142 and @xmath143 are positive small values guaranteeing that matrices @xmath91 and @xmath144 are positive definite and @xmath145\\rightarrow r^{+}$ ] is a monotonically decreasing function satisfying @xmath146 . in our implementation we have taken @xmath147 of the form@xmath148 with @xmath149 , @xmath150 .",
    "the value of @xmath151 controls how fast barriers are reduced . in our implementation",
    "we have found that adequate values for @xmath151 range @xmath152 .",
    "the value of @xmath153 controls the non - linearity of @xmath147 .",
    "we have found that the simple linear response provides good performance albeit convergence time is not significantly affected for @xmath153 in the range @xmath154    in this way we iterate the algorithm @xmath155 times until a given termination criterion is met .",
    "defining@xmath156   }   &   : = \\ln\\left (   w\\left (   f^{[s]}\\right ) \\right )   -\\ln\\left (   w\\left (   f^{[s-1]}\\right )   \\right )   , \\\\",
    "\\epsilon^{\\left [   s\\right ]   }   &   : = \\max\\limits_{j}\\left (   \\left\\vert \\rho _ { j}^{\\left [   s\\right ]   } -\\ln\\left (   v_{j}^{\\left [   s\\right ]   } \\right ) + \\sum_{r=1}^{r_{n_{j}}^{\\left (   j\\right )   } -1}f_{r}^{\\left [   s\\right ]   } \\xi_{r}\\right\\vert \\right )   , \\end{aligned}\\ ] ] we have chosen the following termination criterion@xmath157}<w_{zero}~~\\mathrm{or}~~\\left (   \\left\\vert \\delta_{\\ln w}^{\\left [   s\\right ] } \\right\\vert < \\delta_{\\ln w}^{\\max}~~\\mathrm{and}~~\\left\\vert \\delta_{\\ln w}^{\\left [   s-1\\right ]   } \\right\\vert < \\delta_{\\ln w}^{\\max}\\right )   \\right ] \\right .",
    "\\nonumber\\\\ &   \\left .",
    "\\mathrm{and}~~n_{it}>n_{\\min}~~\\mathrm{and}~~\\mu^{\\left [   s\\right ] } < \\mu_{\\max}~~\\mathrm{and}~~\\tilde{\\mu}^{\\left [   s\\right ]   } < \\tilde{\\mu}_{\\max}~~\\mathrm{and}~~\\epsilon^{\\left [   s\\right ]   } < \\epsilon_{\\max}\\right\\ } . \\label{criterion}\\ ] ] in our implementation we have taken@xmath158 }   &   = 10^{-1},\\quad\\mu_{\\min}=10^{-10},\\quad\\mu_{\\max}=10^{-6},\\nonumber\\\\ \\tilde{\\mu}^{[0 ] }   &   = 10^{+1},\\quad\\tilde{\\mu}_{\\min}=10^{-10},\\quad \\tilde{\\mu}_{\\max}=10^{-6},\\nonumber\\\\ \\epsilon_{\\max }   &   = 10^{-8},\\quad\\delta_{\\ln w}^{\\max}=10^{-2},\\quad w_{zero}=10^{-9},\\nonumber\\\\ n_{\\min }   &   = 5,\\quad n_{\\max}=60 . \\label{values}\\ ] ]    obviously there is considerable latitude to change the heuristic values assigned to the above parameters .",
    "let us finish this appendix making some comments regarding their robustness .",
    "@xmath159 is there to guarantee a minimum number of iterations so as to have @xmath160   } $ ] and @xmath161 } $ ] well defined and to avoid premature termination in the improbable case where the other criteria incorrectly suggest convergence . for this purpose",
    "is enough to take @xmath162 .",
    "@xmath163 serves as a maximum limit to secure termination even if convergence is not achieved and therefore an alarm should be provided whenever @xmath164 . from our experience",
    "we observe that is more than enough to take @xmath165    @xmath166 sets our precision to consider a given forward rate curve as a straight line .",
    "the order of magnitude of @xmath166 should be taken much lower than the typical order of magnitude of the observed optimal @xmath167 the value of the optimal @xmath42 depends not only on the constraining data but also on @xmath41 and @xmath168 we have adopted the practise of spanning the range @xmath169 year@xmath170 taking @xmath171 year@xmath33 , @xmath36 year@xmath37 and the range @xmath94 year@xmath172 taking @xmath173 year@xmath33 , @xmath174 year@xmath37 . with this convention",
    "we have found reasonable to take @xmath175 for the full range @xmath176    @xmath177 sets the maximum variation of @xmath178 between newton steps that is accepted before termination .",
    "note that in ( [ criterion ] ) to have ( @xmath179 } \\right\\vert < \\delta_{\\ln w}^{\\max}~~\\mathrm{and}~~\\left\\vert \\delta_{\\ln w}^{\\left [   s-1\\right ]   } \\right\\vert < \\delta_{\\ln w}^{\\max}$ ] ) is a necessary but not sufficient condition for termination . therefore sending @xmath180 has no major impact and is equivalent to rely only on the error @xmath181   } $ ] and the barrier coefficients as indicators of convergence . on the contrary excessively reducing @xmath182 can generate unnecessary iterations .",
    "we have tested that for values satisfying @xmath183 we do not have any drastic increase in convergence time .",
    "@xmath184 controls the error in the constraints and is the most important parameter in ( [ criterion ] ) .",
    "a too large value of @xmath185 reduces the accuracy of the result and a too small value can give rise to unnecessary iterations .",
    "we have observed acceptable results for @xmath184 in the range @xmath186    @xmath187 and @xmath188 control the maximum allowed values for the logarithmic barriers implementing the positivity and spread constraints respectively .",
    "large values for these parameters can make log barriers to have a residual influence in the feasible region",
    ". acceptable values of these maximum weights range in @xmath189 and @xmath190 as explained above keeping parameters @xmath142 and @xmath143 positive guarantees that matrices @xmath91 and @xmath144 are positive definite which in turn is sufficient to guarantee the existence of an optimal solution in each iteration .",
    "hence @xmath142 and @xmath143 should be chosen as small as possible without interfering with the numerical stability of the algorithm . using double precision in our program",
    "we have found the values given in ( [ criterion ] ) as a good compromise .",
    "finally @xmath191}$ ] and @xmath192}$ ] are the initial values for the log barriers coefficients .",
    "taking very large values for @xmath191}$ ] and @xmath192}$ ] increases the convergence time because we need more time to reduce the barriers .",
    "taking too small values for @xmath191}$ ] and @xmath192}$ ] also increases the convergence time because time is wasted exploring unfeasible solutions .",
    "moreover , we have observed that convergence and stability is improved if the contributions to the objective of the two barrier terms are kept balanced .",
    "this is achieved setting @xmath192}\\simeq\\frac{n}{2m}\\mu^{\\lbrack0]}$ ] ( see eqs.([barrier1 ] ) and ( [ barrier2 ] ) ) and using the same updating factor @xmath193}\\right )   $ ] for @xmath194}$ ] and @xmath195   } $ ] ( see eq.([update ] ) ) . keeping @xmath192}\\simeq\\frac { n}{2m}\\mu^{\\lbrack0]}$ ]",
    "we have found that the time of convergence is stable for @xmath191}$ ] in the range @xmath196}\\lesssim10^{+1}.$ ]",
    "in this work we have used the following data tables and conventions .",
    "[ c]|l||l|l|l|l|l|l|l|l|l|l|l|@xmath197bond ( so ) & 1033 & 1042 & 1035 & 1044 & 1038 & 1037 & 1040 & 1043 & 1034 & 1045 & 1041 + & 4.86 & 4.92 & 5.06 & 5.15 & 5.26 & 5.27 & 5.355 & 5.4 & 5.395 & 5.46 & 5.655 + & 4.905 & 4.965 & 5.11 & 5.2 & 5.05 & 5.325 & 5.4 & 5.455 & 5.23 & 5.51 & 5.7 + & 4.885 & 4.945 & 5.095 & 5.185 & 4.93 & 5.295 & 5.395 & 5.435 & 5.24 & 5.495 & 5.68 + & 4.865 & 4.92 & 5.06 & 5.15 & 4.93 & 5.275 & 5.355 & 5.41 & 5.415 & 5.465 & 5.65 + & 4.835 & 4.885 & 5.025 & 5.125 & 4.93 & 5.25 & 5.355 & 5.405 & 5.405 & 5.465 & 5.66 + & 4.84 & 4.89 & 5.045 & 5.145 & 4.93 & 5.27 & 5.38 & 5.43 & 5.43 & 5.495 & 5.69 + & 4.825 & 4.87 & 5.035 & 5.13 & 4.93 & 5.25 & 5.36 & 5.415 & 5.41 & 5.475 & 5.66 + & 4.805 & 4.85 & 5.015 & 5.11 & 4.93 & 5.23 & 5.34 & 5.395 & 5.395 & 5.455 & 5.64 + & 4.8 & 4.86 & 5.005 & 5.1 & 4.93 & 5.22 & 5.335 & 5.38 & 5.395 & 5.445 & 5.64 + & 4.77 & 4.83 & 4.97 & 5.06 & 4.93 & 5.165 & 5.27 & 5.34 & 5.34 & 5.39 & 5.585 +    the price of the bond @xmath1 is calculated using the formula@xmath198 where @xmath199 is the nominal amount ( sek 40 millions for all bonds in table [ table1 ] ) , @xmath200 is the coupon rate ( given in table [ table2 ] ) , @xmath201 is the total number of remaining coupons ( each paid at time @xmath202 ) , @xmath203 is the quoted rate given in table [ table1 ] and @xmath204 is a time difference between @xmath202 and the settlement day .",
    "this time difference is calculated according to the isma 30e/360 convention defined as follows : given two dates @xmath205 and @xmath206 , their isma 30e/360 time difference @xmath207 is given by@xmath208    [ c]|c||c|c| & @xmath209 & @xmath210 + & 05/05/2003 & 10.25 + & 15/01/2004 & 5 + & 09/02/2005 & 6 + & 20/04/2006 & 3.5 + & 25/10/2006 & 6.5 + & 15/08/2007 & 8 + & 05/05/2008 & 6.5 + & 28/01/2009 & 5 + & 20/04/2009 & 9 + & 15/03/2011 & 5.25 + & 05/05/2014 & 6.75 +                            kian guan lim , qin xiao , and jimmy ang , _ estimating forward rate curve in pricing interest rate derivatives _ , derivatives use , trading & regulation , an international journal of the futures and options association uk , vol .",
    "6 no.4 , pp .",
    "299 - 305 , 2001 .",
    "kwon , oh kang , _ a general framework for the construction and the smoothing of forward rate curves_. qfrg , university of technology , sydney .",
    "http://www.business.uts.edu.au/finance/qfr/cfrg_papers.html , march 2002 ."
  ],
  "abstract_text": [
    "<S> in this article we present a non - linear dynamic programming algorithm for the computation of forward rates within the maximum smoothness framework . </S>",
    "<S> the algorithm implements the forward rate positivity constraint for a one - parametric family of smoothness measures and it handles price spreads in the constraining dataset . </S>",
    "<S> we investigate the outcome of the algorithm using the swedish bond market showing examples where the absence of the positive constraint leads to negative interest rates . </S>",
    "<S> furthermore we investigate the predictive accuracy of the algorithm as we move along the family of smoothness measures . among other things </S>",
    "<S> we observe that the inclusion of spreads not only improves the smoothness of forward curves but also significantly reduces the predictive error . </S>"
  ]
}