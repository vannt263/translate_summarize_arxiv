{
  "article_text": [
    "here we extend alg . [ algc ] , given in section [ secderivate ] , for channels where the encoder has specific information about the delayed output . in this case , the input probability is given by @xmath209 , where @xmath30 is the feedback , and @xmath27 is deterministic .",
    "in other words , we solve the optimization problem given by @xmath210 the optimization problem is associated to fig .",
    "[ dch ] .",
    "[ ] [ ] [ 0.8]@xmath211[][][1]@xmath7[][][0.8]@xmath48[][][1]@xmath177[][][0.8]decoder [ ] [ ] [ 1]@xmath212 [ ] [ ] [ 1]@xmath213[][][0.8]encoder        the proof for this case is similar to that of theorem [ thc ] , except the steps that follow from lemmas [ rfix ] and [ qfix ] .",
    "lemma [ rfix ] proves the existence of an argument @xmath72 that maximizes the directed information where @xmath119 is fixed .",
    "the modification of this lemma is presented here , where we find the argument @xmath72 that maximizes the directed information where @xmath209 is fixed ; the proof is omitted .",
    "therefore , the maximization over @xmath72 where @xmath209 is fixed is given by @xmath214    lemma [ qfix ] proves the existence of an argument @xmath119 that maximizes the directed information where @xmath72 is fixed .",
    "we replace this lemma by lemma [ qfixd ] .",
    "[ qfixd ] . for fixed @xmath72 ,",
    "there exists @xmath215 that achieves @xmath216 and given by @xmath217 where @xmath218 and @xmath96^{\\frac{p(y^n||x^n)\\prod_{j = i+1}^{n}{r(x_j|x^{j-1},z^{j - d})}}{\\sum_{a_{i , d , z}}\\prod_{j=1}^{i - d}{p(y_j|x^{j},y^{j-1})}}}}.\\end{aligned}\\ ] ]    we find the products of @xmath209 that achieve maximum for the directed information . for convenience ,",
    "let us use for short : @xmath219 , and @xmath220 . as in lemma [ isconcave ]",
    "we can omit that @xmath23 is concave in @xmath221 .",
    "furthermore , the constraints of the optimization problem are affine , and we can use the lagrange multipliers method with the karush - kuhn - tucker conditions .",
    "we define the lagrangian as : @xmath222 now , for every @xmath125 we find @xmath223 s.t . , @xmath224\\right)}+\\nu_{i,(x^{i-1},z^{i - d})}\\nonumber\\\\ & = \\sum_{a_{i , d , z}}\\prod_{j=1}^{i-1}{r_{j}}\\sum_{x_{i+1}^n , y_{i - d+1}^n}{\\left(p(y^n||x^n)\\prod_{j = i+1}^{n}{r_{j } } \\left[\\log{\\frac{q(x^n|y^n)}{\\prod_{j = i+1}^{n}{r_j}}}-\\log{\\prod_{j=1}^{i-1}{r_{j}}}-\\log{r_i}-1\\right]\\right)}+\\nu_{i,(x^{i-1},z^{i - d})}\\nonumber\\\\ & = 0,\\nonumber\\end{aligned}\\ ] ] where the set @xmath225 stands for all output sequences @xmath226 s.t .",
    "the function in the delay maps them to the same sequence @xmath98 , which is the feedback .",
    "note that since @xmath227 does not depend on @xmath228 , we can take the product out of the sum .",
    "furthermore , since @xmath229 is a function of @xmath230 we can divide the whole equation by the product above , and get a new @xmath231 .",
    "moreover , we can see that three of the expressions in the sum , i.e. , @xmath232 , do not depend on @xmath233 , thus leaving their coefficient in the equation to be @xmath234 hence we obtain : @xmath235 -\\log{r_i}-\\log\\nu^{**}_{i,(x^{i-1},z^{i - d})}=0,\\nonumber\\end{aligned}\\ ] ] where @xmath236 therefore , we are left with the expression : @xmath237 where @xmath238^{\\frac{p(y^n||x^n)\\prod_{j = i+1}^{n}{r(x_j|x^{j-1},z^{j - d})}}{\\sum_{a_{i , d , z}}\\prod_{j=1}^{i - d}{p(y_j|x^{j},y^{j-1})}}}}.\\end{aligned}\\ ] ] as in section [ secderivate ] , we can see that for all @xmath126 , @xmath223 is dependent on @xmath72 and @xmath239 , and @xmath240 is a function of @xmath72 alone .",
    "thus , we use the _ backwards maximization _ method . after calculating @xmath223 for all @xmath241",
    ", we obtain @xmath242 that maximizes the directed information where @xmath72 is fixed , i.e. , @xmath215 and the lemma is proven .",
    "as mentioned , by replacing lemmas [ rfix ] , [ qfix ] by those given here , we can follow the outline of theorem [ thc ] and conclude the existence of an alternating maximization procedure , i.e. , we can compute @xmath243 that is equal to @xmath244 , where @xmath245 is the value of @xmath246 in the @xmath66th iteration in the extended algorithm .",
    "one more step is required in order to prove the extension of alg .",
    "[ algc ] to the case presented here ; the existence of @xmath247 .",
    "this part is presented in appendix [ proofthup ] .",
    "here , we prove the existence of an upper bound , @xmath247 , that converges to @xmath159 from above simultaneously with the convergence on @xmath246 to it from below , as in alg .",
    "[ algc ] . to this purpose , we present and prove few lemmas that assist in obtaining our main goal .",
    "we start with lemma [ upbound1 ] that gives an inequality for the directed information .",
    "this inequality is used in lemma [ upbound2 ] to prove the existence of our upper bound which lemma [ upbound2tight ] proves to be tight .",
    "theorem [ thupbound ] combines lemmas [ upbound2 ] , [ upbound2tight ] .      for any @xmath249 , @xmath250 , @xmath252 where in ( a )",
    ", @xmath253 and @xmath254 are the pmfs of @xmath114 that corresponds to @xmath255 and @xmath256 , and ( b ) follows from the non negativity of the divergence .",
    "thus , the lemma is proven .        to prove this lemma , we first use lemma [ upbound1 ] . for every @xmath259 ,",
    "@xmath260 where ( a ) follows lemma [ upbound1 ] , ( b ) follows from maximizing an expression over @xmath261 , and ( c ) follows from the fact that the expression in the under - brace is a function of @xmath262 , and we can take it out of the summation over @xmath261 and use @xmath263 .",
    "the rest of the steps are the same as ( b ) and ( c ) , where we refer to a different @xmath180 .",
    "since the inequality above is true for every @xmath249 , we can use it on @xmath264 that achieves @xmath159 , and thus for every @xmath250 @xmath265 this is also true for every @xmath250 , and hence for the minimum over all @xmath250 , and we obtain @xmath266 and the lemma is proven .        in lemma [ upbound2 ] , we showed only half of the proof of the theorem , i.e. , the existence of an upper bound . to prove this lemma",
    ", we need to show that this inequality is tight . for that purpose",
    ", we use the lagrange multipliers method with the kkt conditions with respect to all @xmath268s .",
    "we can use the kkt conditions since the directed information is a concave function in all @xmath268s , as seen in lemma [ whyalt ] .",
    "we define the lagrangian as @xmath269 now , for every @xmath268 , we have @xmath270 setting @xmath271 we are left with two cases . for @xmath272 the kkt conditions requires us to set @xmath273 and we obtain @xmath274 whereas for @xmath275 we set @xmath276 and the equality becomes an inequality .",
    "we now analyze our results for the case where @xmath272 .",
    "first , we note that for @xmath18 we have that @xmath277 and thus constant for every @xmath261 . as a result ,",
    "for @xmath278 we have @xmath279 that again , is constant for every @xmath280 .",
    "we can move backwards and obtain that for @xmath19 , @xmath281 using the analysis above , we find an expression for @xmath159 where @xmath267 achieves it . in the following equations we can assume that @xmath282 , since otherwise , for the specific @xmath283 , the expression for @xmath159 will contribute @xmath284 to the summation .",
    "@xmath285 where ( a ) is due to the analysis above for @xmath19 .",
    "we showed that the upper bound is tight , and thus the lemma is proven .",
    "as showed in lemma [ upbound2 ] , there exists an upper bound for @xmath159 .",
    "lemma [ upbound2tight ] showed that this upper bound is tight , when using the pmf @xmath267 that achieves @xmath159 .",
    "thus , the theorem is proven .",
    "_ generalization of theorem [ thupbound ] _ we generalize theorem [ thupbound ] to the case where the feedback is a delayed function of the output ( as presented in appendix [ appfd ] ) .",
    "we recall , that the optimization problem for this model is @xmath210 while solving this optimization problem , we defined the following set : @xmath225 ; namely , all output sequences @xmath226 s.t .",
    "the function in the delay sends them to the same sequence @xmath98 .",
    "we use this notation for the upper bound . in that case , the upper bound is of the form @xmath286 the proof for this upper bound is omitted due to its similarity to the case where @xmath287 for all @xmath126 , i.e. , theorem [ thupbound ] .",
    "moreover , one can see that this is a generalization , since if indeed @xmath287 , then @xmath288 has only one sequence , @xmath289 , and the equation for @xmath247 coincides with the one in theorem [ thupbound ] ."
  ],
  "abstract_text": [
    "<S> [ abs ] we extend the blahut - arimoto algorithm for maximizing massey s directed information . </S>",
    "<S> the algorithm can be used for estimating the capacity of channels with delayed feedback , where the feedback is a deterministic function of the output . in order to do so </S>",
    "<S> , we apply the ideas from the regular blahut - arimoto algorithm , i.e. , the alternating maximization procedure , onto our new problem . </S>",
    "<S> we provide both upper and lower bound sequences that converge to the optimum value . </S>",
    "<S> our main insight in this paper is that in order to find the maximum of the directed information over causal conditioning probability mass function ( pmf ) , one can use a backward index time maximization combined with the alternating maximization procedure . </S>",
    "<S> we give a detailed description of the algorithm , its complexity , the memory needed , and several numerical examples .    </S>",
    "<S> alternating maximization procedure , backwards index time maximization , blahut - arimoto algorithm , causal conditioning , channels with feedback , directed information , finite state channels , ising channel , trapdoor channel .    </S>",
    "<S> introduction [ secintro ] in his seminal work , shannon @xcite showed that the capacity of a memoryless channel is given as the optimization problem @xmath0 since the set of all @xmath1 is not of finite cardinality , an optimization method is required to find the capacity @xmath2 . </S>",
    "<S> in order to obtain an efficient way to calculate the global maximum in ( [ shancap ] ) , the well - known blahut - arimoto algorithm ( referred to as baa ) was introduced by blahut @xcite and arimoto @xcite in 1972 . </S>",
    "<S> the main idea is that we can calculate the optimum value using the equality @xmath3 i.e. , we can maximize over @xmath1 and @xmath4 , instead of just @xmath1 alone . </S>",
    "<S> the maximization is then achieved using the alternating maximization procedure . </S>",
    "<S> the convergence of the alternating maximization procedure to the global maximum was proven by csiszar and tusnady @xcite , and later by yeung @xcite .    in this paper </S>",
    "<S> , we find an efficient way to estimate the capacity of channels with feedback . </S>",
    "<S> it was shown by massey @xcite , kramer @xcite , tatikonda and mitter@xcite , permuter , weissman , and goldsmith @xcite , and kim @xcite , that the expression @xmath5 has an important role in characterizing the feedback capacity , where @xmath6 is the _ directed information _ , and @xmath7 is a _ causally conditioned _ pmf ( definitions in section [ secpre ] ) given by @xmath8 since in the maximization we deal with causally conditioned pmfs , trying to follow the regular baa will result in difficulties . </S>",
    "<S> this is due to the fact that a causal conditioned pmf is the result of multiplications of conditioned pmfs as seen in ( [ causal ] ) . </S>",
    "<S> while in the regular baa we maximize over @xmath9 , and thus the constraints are simply @xmath10 and @xmath11 , in our extended problem we have no efficient way of writing all the constraints necessary for a causally conditioned pmf . </S>",
    "<S> in fact , we need @xmath12 simple constraints , one for each product of @xmath13 . </S>",
    "<S> another difficulty is that although the equality @xmath14 holds , we can not translate the given problem into @xmath15 since @xmath16 influence all terms @xmath17 . </S>",
    "<S> a solution could be to maximize backwards from @xmath18 to @xmath19 over @xmath16 , and it can be shown that in each maximization , the non - causal probability @xmath20 is determined only by the previous @xmath21 for @xmath22 . in our solution , we maximize the entire expression @xmath23 as a function of @xmath24 . each time we maximize over a specific @xmath16 starting from @xmath18 and moving backwards to @xmath19 , where all but @xmath16 are fixed . </S>",
    "<S> before we present the extension of the baa to the directed information , let us present some of the other extensions of this algorithm . in 2004 , matz and duhamel@xcite proposed two blahut - arimoto - type algorithms that often converge significantly faster than the standard blahut - arimoto algorithm , which relied on following the natural gradient rather than maximizing per variable </S>",
    "<S> . during that year , rezaeian and grant @xcite generalized the regular baa for multiple access channels , and dupuis , yu , and willems extended the baa for channels with side information @xcite . </S>",
    "<S> they used the fact that the input is a deterministic function of the auxiliary variable and the side information , and then extended the input alphabet . </S>",
    "<S> another solution to the side information problem was given by el gamal and heegard@xcite , where they did not expand the alphabet , but included an additional step to optimize over @xmath25 . </S>",
    "<S> also , the baa was used by egorov , markavian , and pickavance @xcite to decode reed solomon codes . in 2005 </S>",
    "<S> dauwels @xcite showed how the baa can be used to calculate the capacity of continuous channels . </S>",
    "<S> dauwels s main idea is the use of sequential monte - carlo integration methods known as the `` particle filters '' . in 2008 </S>",
    "<S> vontobel , kavci , arnold , and loeliger@xcite extended the regular baa to estimate the capacity of finite state channels where the input is markovian . </S>",
    "<S> sumszyk and steinberg @xcite gave a single letter characterization of the capacity of an information embedding channel and provided a ba - type algorithm for the case where the channel is independent of the host given the input .    </S>",
    "<S> recently , few papers about the maximization of the directed information using control theory and dynamic programming were published . in @xcite , </S>",
    "<S> yang , kavcic and tatikonda maximized the directed information to estimate the feedback capacity of finite - state machine channels where the state is a deterministic function of the previous state and input . </S>",
    "<S> chen and berger @xcite maximized the directed information for the case where the state of the channel is known to the encoder and decoder in addition to the feedback link . </S>",
    "<S> later , permuter , cuff , van roy and weissman @xcite maximized the directed information and found the capacity of the trapdoor channel with feedback . in @xcite , </S>",
    "<S> gorantla and coleman estimated the maximum of directed information where they considered a dynamical system , whose state is an input to a memoryless channel . </S>",
    "<S> the state of the dynamical system is affected by its past , an exogenous input , and causal feedback from the channel s output .    </S>",
    "<S> the remainder of the paper is organized as follows . in section [ secpre ] </S>",
    "<S> we present the notations we use throughout the paper , and give the outline for the alternating maximization procedure as given by yeung@xcite . in section [ secdescription ] </S>",
    "<S> we give a description of the algorithm for solving the optimization problem- @xmath26 , calculate the complexity of the algorithm and memory needed , and compare it with those of the regular baa . in section [ secderivate ] </S>",
    "<S> we derive the algorithm using the alternating maximization procedure , and show the convergence of our algorithm to the optimum value . </S>",
    "<S> numerical examples for channel capacity with feedback are presented in section [ secex ] . in appendix [ appfd ] </S>",
    "<S> we give a wider angle on the feedback channel problem , where the feedback of the channel is a deterministic function @xmath27 of the output with some delay @xmath28 ; namely , we derive the algorithm for the optimization problem @xmath29 where @xmath30 and @xmath31 . in appendix [ proofthup ] </S>",
    "<S> we prove an upper bound for @xmath32 which converges to the directed information from above and helps determining the stoping iteration of the algorithm .    </S>",
    "<S> preliminaries [ secpre ]    directed information and causal conditioning in this section we present the definitions of directed information and causally conditioned pmf , originally introduced by massey@xcite ( who was inspired by marko s work @xcite on bidirectional communication ) and by kramer @xcite . </S>",
    "<S> these definitions are necessary in order to address channels with memory . </S>",
    "<S> we denote by @xmath33 the vector @xmath34 . </S>",
    "<S> usually we use the notation @xmath35 for short . </S>",
    "<S> further , when writing a pmf we simply write @xmath36 . </S>",
    "<S> let us denote as @xmath37 the probability mass function ( pmf ) of @xmath38 _ causally conditioned _ on @xmath39 , given by @xmath40 here we have to establish that when @xmath41 , the vector @xmath42 . </S>",
    "<S> two straight forward properties of the causal conditioning pmf that we use throughout the paper are @xmath43 and @xmath44 another elementary property is the chain rule for directed information @xmath45 the definitions above lead to the causally conditioned entropy @xmath46 , which is given by @xmath47}.\\ ] ] moreover , the directed information from @xmath38 to @xmath48 is given by @xmath49 it is possible to show , that we can write the directed information as such : @xmath50 we refer to this form when using the alternating maximization procedure since @xmath51 are the variables we optimize over where @xmath7 is fixed . for convenience , </S>",
    "<S> we use from now on the notation of @xmath52 when required . with these definitions </S>",
    "<S> , we follow the alternating maximization procedure given by yeung@xcite in order to maximize the directed information .    alternating maximization procedure here , we present the alternating maximization procedure on which our algorithm is based . </S>",
    "<S> let @xmath53 be a real function , and let us consider the optimization problem given by @xmath54 we denote by @xmath55 the point that achieves @xmath56 , and by @xmath57 the one that achieves @xmath58 . </S>",
    "<S> the algorithm is defined by iterations , where in each iteration we maximize over one of the variables . </S>",
    "<S> let @xmath59 be an arbitrary point in @xmath60 . </S>",
    "<S> for @xmath61 let @xmath62 and let @xmath63 be the value if the present iteration . </S>",
    "<S> the following lemma describes the conditions the problem needs to meet in order for @xmath64 to converge to @xmath65 as @xmath66 goes to infinity .    </S>",
    "<S> [ lemconv ] . </S>",
    "<S> let @xmath53 be a real , concave , bounded from above function that is continuous and has continuous partial derivatives , and let the sets @xmath67 which we maximize over , be convex . </S>",
    "<S> further , assume that @xmath55 and @xmath57 for all @xmath68 . </S>",
    "<S> under these conditions , @xmath69 .    in section [ secdescription ] </S>",
    "<S> we give a detailed description of the algorithm that computes @xmath70 based on the alternating maximization procedure . in section [ secderivate ] </S>",
    "<S> we show that the conditions in lemma [ lemconv ] hold , and therefore the algorithm we suggest , which is based on the alternating maximization procedure , converges to the global optimum .    </S>",
    "<S> description of the algorithm [ secdescription ] in this section , we describe an algorithm for maximizing the directed information . </S>",
    "<S> in addition , we compute the complexity of the algorithm per iteration , and compare it to the complexity of the regular baa . the memory calculation is also given .    </S>",
    "<S> the algorithm for channel with feedback in algorithm [ algc ] , we present the steps required to maximize the directed information where the channel @xmath7 is fixed and the delay is @xmath71 .    * start from a random point @xmath72 . usually we start from a uniform distribution , i.e. , @xmath73 for every @xmath74 + * starting from @xmath18 , calculate @xmath75 using the formula + @xmath76 where @xmath77^{p(y_i|x^i , y^{i-1})\\prod_{j = i+1}^{n}{r(x_j|x^{j-1},y^{j-1})p(y_j|x^j , y^{j-1})}}},\\label{riform}\\ ] ] and do so backwards until @xmath19 . + </S>",
    "<S> * once you have @xmath75 for all @xmath78 , compute @xmath79 . + * compute @xmath72 using the formula @xmath80 * calculate @xmath81 , where @xmath82 * return to ( b ) if @xmath83 . </S>",
    "<S> * @xmath84 .    </S>",
    "<S> note that the regular baa has a structure similar to that of algorithm [ algc ] , where step ( b ) is an additional backward loop . </S>",
    "<S> its purpose is to maximize over the input causal probability , which is not necessary in the regular baa .    </S>",
    "<S> now , let us present a special case and a few extensions for alg . </S>",
    "<S> [ algc ] .    * </S>",
    "<S> _ regular baa , i.e. , @xmath85_. for @xmath85 , the algorithm suggested here agrees with the original baa , where instead of steps ( b ) , ( c ) we have @xmath86 and step ( d ) is replaced by @xmath87 the bounds @xmath88 agree with the regular baa as well , and are of the form @xmath89 * _ feedback with general delay @xmath28_. we can generalize the algorithm in order to compute @xmath90 , where the feedback is the output with delay @xmath28 . in that case , in step ( b ) we have @xmath91^{\\prod_{j = i - d+1}^{n}{p(y_j|x^{j},y^{j-1})}\\prod_{j = i+1}^{n}{r(x_j|x^{j-1},y^{j - d})}}},\\label{rform1}\\end{aligned}\\ ] ] and step ( d ) will be replaced by @xmath92 the bounds @xmath88 are of the form @xmath93 * _ feedback as a function of the output with general delay_. in appendix [ appfd ] , we generalize the algorithm in order to compute @xmath94 , where the feedback @xmath95 is a deterministic function of the delayed output . </S>",
    "<S> the expression characterizes the capacity of channels with time - invariant feedback@xcite . in that case , in step ( b ) we have @xmath96^{\\frac{p(y^n||x^n)\\prod_{j = i+1}^{n}{r(x_j|x^{j-1},z^{j - d})}}{\\sum_{a_{i , d , z}}\\prod_{j=1}^{i - d}{p(y_j|x^{j},y^{j-1})}}}},\\label{rformfd}\\end{aligned}\\ ] ] where we define the set @xmath97 as the set of output sequences that @xmath27 transforms to @xmath98 , and step ( d ) will be replaced by @xmath99 the bounds @xmath88 are of the form @xmath100    note , that for @xmath101 , the vector @xmath102 , hence @xmath103 , and @xmath104 also note that when @xmath105 , @xmath106 , @xmath107 , and @xmath108 . in each of the cases above ( @xmath101 or @xmath109 ) , in step ( d ) we have @xmath110 and we obtain a different version of the regular baa for channel capacity , where the maximization is done over all @xmath111 instead of over @xmath112 at once . </S>",
    "<S> furthermore , if @xmath113 then case ( 3 ) agrees with all the equations of case ( 2 ) .    </S>",
    "<S> complexity and memory needed here , we give an expression for the computation complexity of one iteration in the algorithm , and then compare it to regular baa . </S>",
    "<S> this will be done in two parts , one for each step in the iteration .    </S>",
    "<S> * complexity of computing @xmath72 as given in ( [ qform ] ) . for each @xmath114 , we need @xmath115 multiplications for a specific @xmath116 and use the denominator computed for every other @xmath116 , thus obtaining @xmath117 operations . doing so for all @xmath114 achieves @xmath118 . </S>",
    "<S> * complexity of computing @xmath119 . </S>",
    "<S> first , we compute the complexity of each @xmath75 as given in ( [ riform ] ) , assuming that an exponent is a constant number of computations , i.e. , @xmath120 . </S>",
    "<S> simple computations will conclude that the entire numerator takes about @xmath121 computations . </S>",
    "<S> the denominator is a summation over @xmath122 variables , and as with @xmath72 , we can use the denominator for every other @xmath123 . </S>",
    "<S> hence , we obtain @xmath124 computations for every @xmath125 . </S>",
    "<S> summing over @xmath126 will achieve @xmath127 computations . </S>",
    "<S> multiplying all @xmath75s is a constant number of computations for every @xmath128 . </S>",
    "<S> finally , in order to compute @xmath119 we need @xmath129 computations .    to conclude , each iteration requires about @xmath130 computations .    </S>",
    "<S> comparing to regular baa : since baa computes the capacity of memoryless channels , we only need to compute @xmath131 and @xmath132 . in much the same way , we can have its complexity and achieve @xmath133 computations . </S>",
    "<S> however , if we want to compare it to baa for channels with memory , we replace @xmath134 , @xmath135 but , @xmath136 and so we obtain @xmath137 computations . </S>",
    "<S> the memory needed for the algorithm is very much dependent on the manner in which one implements the algorithm . </S>",
    "<S> however , the obligatory memory needed is for @xmath138 , and @xmath139 and its products ; thus we need at least @xmath140 cells of type double . </S>",
    "<S> computation complexity and memory needed are presented in table [ complexmemory ] .    </S>",
    "<S> .memory and operations needed for regular and extended baa for channel coding with feedback . [ cols=\"^,^,^\",options=\"header \" , ]     [ tt2 ]    an examination of the truth table in table [ tt1 ] yields the formula for @xmath141 as @xmath142 note that in table [ tt1 ] , the input series @xmath143 and @xmath144 are not possible since the output is not one of the bits in the box ; thus we may assign to @xmath141 whatever value we choose , in order to simplify the formula . </S>",
    "<S> as for the conditional probability @xmath145 , we assume that @xmath146 , and because of the channel s symmetry the outcome for @xmath147 is easily calculated . </S>",
    "<S> looking at table [ tt2 ] , we can see that the formula for @xmath148 is given by @xmath149 where we know that @xmath141 is a function of @xmath150 , and @xmath151 denotes and .    now that we have @xmath152 , we use alg . </S>",
    "<S> [ algc ] for estimating the capacity of the channel as we run the algorithm to find the upper and lower bound for every @xmath153 , where @xmath154 note that ( [ cupp ] ) is calculated via alg . </S>",
    "<S> [ algc ] and @xmath146 due to the channel s symmetry . </S>",
    "<S> however , calculating ( [ clow ] ) is more difficult , since we have to maximize over all the probabilities @xmath119 , and at the same time minimize over the initial state . </S>",
    "<S> hence , we use another lower bound denoted by @xmath155 , for which @xmath119 is fixed and is the one that achieves the maximum at ( [ cupp ] ) , and we only minimize over @xmath156 . clearly , @xmath157 . </S>",
    "<S> fig . </S>",
    "<S> [ tdgraph ] presents the capacity estimation , and the upper and lower bound , as a function of the block length @xmath12 .    </S>",
    "<S> [ ] [ ] [ 0.8]@xmath158 [ ] [ ] [ 0.8]@xmath159 [ ] [ ] [ 0.8]@xmath160 [ ] [ ] [ 1]@xmath12[][][1]value[][][0.8 ] true cap .     and the true capacity of the trapdoor channel with 2 states and feedback with delay 1.,width=226 ]    in @xcite , the capacity of the trapdoor channel is calculated analytically , and given by @xmath161 we see from the simulation that the upper and lower bounds of the capacity approach the limit in ( [ captd ] ) , and the estimated capacity at block length @xmath162 is @xmath163 .    </S>",
    "<S> directed information rate as a different estimator for the capacity we now consider an estimator to the feedback capacity of an fsc by calculating @xmath164 . </S>",
    "<S> the justification for this estimator is based on the following lemma .    . </S>",
    "<S> if @xmath165 exists , then @xmath166 i.e. , @xmath167    if we suppose that the limit above exists , then @xmath168 where ( a ) follows from the fact that if the limit of the sequence @xmath169 exists , then the average of the sequence converges to the same limit . </S>",
    "<S> further , a result from @xcite provides that if the joint process @xmath170 is stationary , then the limit @xmath165 exists .    </S>",
    "<S> fig . </S>",
    "<S> [ tddiffgraph ] presents the directed information rate estimator using the lemma above , and its comparison to the true capacity .    </S>",
    "<S> [ ] [ ] [ 1]@xmath12[][][1]value[][][0.8]@xmath171[][][0.8 ]   true cap .     calculated using alg . </S>",
    "<S> [ algc ] and the horizontal line is the analytical calculation , for the trapdoor channel with 2 states and feedback with delay 1.,width=188 ]    one can see that the convergence of @xmath164 is faster than @xmath159 and the upper and lower bounds as seen in fig . </S>",
    "<S> [ tdgraph ] , and achieves the value @xmath172 when we calculate the @xmath173 difference . </S>",
    "<S> furthermore , the convergence of the directed information rate stabilizes faster .    </S>",
    "<S> m - state trapdoor channel we generalize the trapdoor channel to an m - state one .    </S>",
    "<S> [ ] [ ] [ 1]input[][][1]channel [ ] [ ] [ 1]output[][][1]@xmath174[][][1]m cells        in the previous example we had @xmath175 cells in the box , one for the state bit , and one for the input bit </S>",
    "<S> . one can consider the state to be the number of 1 s in the channel before a new input is inserted . </S>",
    "<S> we can expand this notation , by letting the box contain more than 2 cells as presented in fig . </S>",
    "<S> [ mstatetd ] . here </S>",
    "<S> , the state at any given time will express the number of @xmath176s that are in the box at that time , and each cell has even probability to be chosen for the output . in this case , </S>",
    "<S> @xmath177 cells in the box are equivalent to @xmath177 states of the channel . by that definition </S>",
    "<S> we can see that the state @xmath141 as a function of past input , output , and the initial state is given by @xmath178 moreover , for calculating the channel probability @xmath179 , we add @xmath141 to @xmath180 and divide the sum by the number of cells , i.e. , @xmath181 now that we have @xmath182 , we use alg . </S>",
    "<S> [ algc ] for calculating @xmath159 for every @xmath183 . </S>",
    "<S> fig . </S>",
    "<S> [ td3graph ] presents the directed information rate estimator @xmath164 for the trapdoor channel with @xmath184 cells .    </S>",
    "<S> [ ] [ ] [ 1]@xmath12[][][1]value     for the trap door channel with 3 cells and feedback with delay 1.,width=188 ]    note , that in fig </S>",
    "<S> . [ td3graph ] we achieve the value @xmath185 in the @xmath173 difference , thus we can assume that the capacity of a 3-state trapdoor channel is approximately @xmath186 .    </S>",
    "<S> influence of the number of cells on the capacity to summarize the trapdoor channel example , we examine the way the number of cells affects the capacity .    [ ] [ ] [ 0.8]@xmath187 [ ] [ ] [ 0.8]@xmath188 [ ] [ ] [ 1]cells[][][1]value     over the number of cells in the trapdoor channel with feedback with delay 1.,width=188 ]    the estimation use is the directed information rate , with @xmath162 . in fig . </S>",
    "<S> [ tdcelldelay ] we can see that the capacity decreases as the number of cells increases and approaches zero .    </S>",
    "<S> the ising channel the ising model is a mathematical model of ferromagnetism in statistical mechanics . </S>",
    "<S> it was originally proposed by the physicist wilhelm lenz who gave it as a problem to his student ernst ising after whom it is named . </S>",
    "<S> the model consists of discrete variables called spins that can be in one of two states . </S>",
    "<S> the spins are arranged in a lattice or graph , and each spin interacts only with its nearest neighbors .    the ising channel is based on its physical model , and simulates intersymbol interference where the state of the channel at time @xmath126 is the current input , and the output is determined by the input at time @xmath189 .    [ ] [ ] [ 1]@xmath190 [ ] [ ] [ 1]@xmath191 [ ] [ ] [ 1]@xmath192 [ ] [ ] [ 1]@xmath193 [ ] [ ] [ 0.7]@xmath194        the channel ( without feedback ) was introduced by berger and bonomi @xcite and is depicted in fig . </S>",
    "<S> [ isingchannel ] . in their paper , they proved the existence of bounds for the no - feedback case . </S>",
    "<S> in addition , they showed that the zero - error capacity without feedback is @xmath195 .    </S>",
    "<S> ising channel with delay @xmath196 we estimate the capacity of the ising channel with feedback .    [ ] [ ] [ 0.8]@xmath158 [ ] [ ] [ 0.8]@xmath159 [ ] [ ] [ 0.8]@xmath197 [ ] [ ] [ 1]@xmath12 [ ] [ ] [ 0.8]value   [ ] [ ] [ 1]@xmath12 [ ] [ ] [ 0.8]value    since the output at time @xmath126 is determined by the input at times @xmath198 , we define the channel pmf as @xmath199 . therefore , the feedback at time @xmath126 must be the output at time @xmath200 , since we can not have @xmath201 before @xmath202 is sent . </S>",
    "<S> thus , looking at the ising channel with delay @xmath71 is not a practical example , and we did not examine it . </S>",
    "<S> we ran our algorithm on the ising channel , with delayed feedback of @xmath196 ; the results are presented in fig . </S>",
    "<S> [ isinggraph2 ] . in fig . </S>",
    "<S> [ isinggraph2 ] ( a ) , we obtain @xmath203 , and in ( b ) we achieve @xmath204 in the @xmath173 difference .    </S>",
    "<S> the effects the delay has on the capacity here we investigate how the delay influences the capacity . </S>",
    "<S> we do so by computing the directed information rate estimator of the ising channel with blocks of length @xmath205 , over the feedback delay @xmath206 . the formulas for estimating the capacity when the delay is bigger than 1 is given in section [ secdescription ] , equations ( [ rform1 ] ) , ( [ qform1 ] ) . in fig . </S>",
    "<S> [ isingdelay ] we can see that , as expected , the capacity decreases as the delay increases . </S>",
    "<S> this is due to the fact that we have less knowledge of the output to use .    </S>",
    "<S> [ ] [ ] [ 0.8]@xmath187 [ ] [ ] [ 0.8]@xmath207 [ ] [ ] [ 1]delay of feedback[][][1]value     over the delay of the feedback on the ising channel.,width=188 ]    conclusions in this paper , we generalized the classical baa for maximizing the directed information over causal conditioning , i.e. , calculating @xmath208 the optimizing the directed information is necessary for estimating the capacity of an fsc with feedback . as we attempted to solve this problem we found that difficulties arose regarding the causal conditioning probability we tried to optimize over . </S>",
    "<S> we overcame this barrier by using an additional backwards loop to find all components of the causal conditioned probability , separately .    </S>",
    "<S> another application of optimizing the directed information is to estimate the rate distortion function for source coding with feed forward as presented in @xcite , @xcite , @xcite . in our future work @xcite </S>",
    "<S> , we address the source coding with feedforward problem , and derive bounds for stationary and ergodic sources . </S>",
    "<S> we also present and prove a ba - type algorithm for obtaining a numerical solution that computes these bounds . </S>"
  ]
}