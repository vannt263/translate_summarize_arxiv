{
  "article_text": [
    "fig.1 . a schematic of parallel computation algorithm and required number of numerical steps . @xmath136",
    "represent fft in first , second etc .",
    "cpus respectively .",
    "righ - hand - side schematically shows calculation of vectors @xmath86 ( see text ) .",
    "power distributions of 5 wdm channels after propagation of pseudorandom sequences of gaussian pulses over @xmath137 km . only small part of the total computational interval of @xmath138 is shown ."
  ],
  "abstract_text": [
    "<S> an efficient numerical algorithm is presented for massively parallel simulations of dispersion - managed wavelength - division - multiplexed optical fiber systems . </S>",
    "<S> the algorithm is based on a weak nonlinearity approximation and independent parallel calculations of fast fourier transforms on multiple cpus . </S>",
    "<S> the algorithm allows one to implement numerical simulations @xmath0 times faster than a direct numerical simulation by a split - step method , where @xmath1 is a number of cpus in parallel network .     </S>",
    "<S> _ ocis codes : _ 060.2330 , 060.5530 , 060.4370 , 190.5530 , 260.2030 . + a wavelength - division - multiplexed ( wdm ) dispersion - managed ( dm ) optical fiber system is the focus of current research in high - bit - rate optical communications . high capacity of optical transmission is achieved using both wavelength multiplexing and dispersion management . </S>",
    "<S> ( see e.g. ref . </S>",
    "<S> @xcite ) . </S>",
    "<S> wavelength multiplexing allows the simultaneous transmission of several information channels , modulated at different wavelengths , through the same optical fiber . </S>",
    "<S> a dispersion - managed @xcite optical fiber systems are designed to achieve low ( or even zero ) path - averaged group - velocity dispersion ( gvd ) by periodically alternating the sign of the dispersion along an optical fiber . </S>",
    "<S> this dramatically reduces pulse broadening . </S>",
    "<S> second - order gvd ( dispersion slope ) effects and path - averaged gvd effects cause optical pulses in distinct wdm channels to move with different group velocities . </S>",
    "<S> consequently modeling of wdm systems requires simulating a long time interval . </S>",
    "<S> enormous computation resources are necessary to capture accurately the nonlinear interactions between channels which deteriorates bit - rate capacity . </S>",
    "<S> the large computational resources required to simulate wdm transmission over transoceanic distances make parallel computation necessary . here </S>",
    "<S> an efficient numerical algorithm is developed for massive parallel computation of wdm systems . </S>",
    "<S> the required computational time is inversely proportional to the number of parallel processors used . </S>",
    "<S> this makes feasible a full scale numerical simulation of wdm systems on a workstation cluster with a few hundred processors .    neglecting polarization effects and stimulated raman scattering and brillouin scattering , </S>",
    "<S> the propagation of wdm optical pulses in a dm fiber is described by a scalar nonlinear schrdinger equation ( nls ) : @xmath2\\sigma^n_{k=1}\\delta(z - z_k)\\big ) a \\nonumber \\\\ </S>",
    "<S> i g(z)a , \\label{nls1}\\end{aligned}\\ ] ] where @xmath3 is the propagation distance along an optical fiber , @xmath4 is the slow amplitude of light ; @xmath5 and @xmath6 are the first and second - order gvd respectively which are periodic functions of @xmath3 ; @xmath7 is the nonlinear coefficient ; @xmath8 is the nonlinear refractive index ; @xmath9 is the carrier wavelength ; @xmath10 is the effective fiber area ; @xmath11 are amplifier locations ; @xmath12 is the amplifier spacing ; and @xmath13 is the loss coefficient . </S>",
    "<S> note that distributed amplification can be also included in @xmath14 without changing the following analysis .    </S>",
    "<S> the change of variables @xmath15 results in the nls with the @xmath3-dependent nonlinear coefficient @xmath16 : @xmath17 by applying fourier transform @xmath18 to eq . </S>",
    "<S> @xmath19 , changing variables @xmath20   \\big)}$ ] and integrating eq . </S>",
    "<S> @xmath19 over @xmath3 from @xmath21 to @xmath3 one obtains the following integro - differential equation : @xmath22,\\omega , z , z_0\\big),\\end{aligned}\\ ] ] where @xmath23,\\omega , z , z_0\\big)= \\frac{1}{(2\\pi)^2}\\int d\\omega_1 d\\omega_2d\\omega_3 \\int^{z}_{z_0}dz '   \\nonumber \\\\ \\times \\ , c(z')\\ , \\hat v^{(z')}(\\omega_1,z ' )   \\hat v^{(z')}(\\omega_2 , z ' )   \\hat v^{*\\,(z')}(\\omega_3,z ' ) \\nonumber \\\\ </S>",
    "<S> \\times    \\exp{\\big ( -\\frac{i}{2}\\int^{z'}_{0}dz '' [ \\omega^2   \\beta_2(z'')+\\frac{\\omega^3}{3}\\beta_3(z '' ) ]   \\big ) }   \\nonumber \\\\ \\times </S>",
    "<S> \\delta(\\omega_1+\\omega_2-\\omega-\\omega_3 ) , \\nonumber \\\\ </S>",
    "<S> \\hat v^{(z)}(\\omega , z ) \\equiv </S>",
    "<S> \\hat v(\\omega , z ) \\nonumber \\\\ </S>",
    "<S> \\times \\exp{\\big(\\frac{i}{2}\\int^{z}_{0}dz ' [ \\omega^2   \\beta_2(z')+\\frac{\\omega^3}{3}\\beta_3(z ' ) ]   \\big)}.\\end{aligned}\\ ] ] if the nonlinearity is small : @xmath24 , where @xmath25 is a characteristic nonlinear length , @xmath26 is the dispersion length ; @xmath27 and @xmath28 are typical pulse amplitude and width respectively </S>",
    "<S> . then one can conclude that @xmath29 is a slow function of @xmath3 on any scale @xmath30 because all of the fast dependence of @xmath31 is already included in the term </S>",
    "<S> @xmath32   \\big)}$ ] ( see reference  @xcite ) . </S>",
    "<S> this term is nothing more than an exact solution of the linear part of eq . </S>",
    "<S> @xmath19 . in first approximation </S>",
    "<S> one can neglect the slow dependence of @xmath33 on @xmath3 in the interval @xmath34 , i.e. one can replace @xmath35 $ ] by @xmath36 $ ] in the nonlinear term @xmath37 ( @xmath38 is an arbitrary nonnegative integer number ) . </S>",
    "<S> this substitution allows to rewrite eq . </S>",
    "<S> @xmath39 in the following form : @xmath40,\\omega,(m+1)l , ml\\big )   +   o(\\frac{l}{z_{nl}})^2.\\end{aligned}\\ ] ] the term @xmath41 indicates the order of accuracy of this approximation . </S>",
    "<S> eq . </S>",
    "<S> @xmath42 enables one to find @xmath43 given @xmath44 . </S>",
    "<S> thus one can recover recover @xmath45 using the definition of @xmath46 . </S>",
    "<S> however for wdm simulation , the accuracy @xmath41 is not always sufficient . </S>",
    "<S> the next order approximation is obtained by including the first order correction , @xmath47 in the nonlinear term , @xmath37 : @xmath48,\\omega , z , ml ) + o(\\frac{l}{z_{nl}})^3,\\label{psi1a } \\\\ \\hat \\psi^{(1)}(\\omega , z)\\equiv \\hat </S>",
    "<S> \\psi(\\omega , m l)+i r(\\hat \\psi[\\omega , ml],\\omega , z , ml ) . </S>",
    "<S> \\label{psi1b}\\end{aligned}\\ ] ] equations @xmath49 and @xmath50 form a closed set for the approximate calculation of @xmath43 given @xmath51 , where @xmath52 is the accuracy of the approximate solution which is controlled by the appropriate choice of @xmath53 . </S>",
    "<S> the main obstacle in the numerical integration of eqs . @xmath49 and @xmath50 is the computation of the integral term @xmath54,\\omega , z , ml\\big)$ ] which generally requires @xmath55 operations for each iteration , where @xmath56 is the number of grid points in @xmath57 or @xmath58-space and @xmath1 is the number of grid points for integration over @xmath3 . </S>",
    "<S> next one presents a very efficient numerical algorithm for calculations @xmath54,\\omega , z , ml\\big)$ ] .    in @xmath58-space eq . </S>",
    "<S> @xmath59 becomes @xmath60,\\omega , z , </S>",
    "<S> ml\\big ) \\big ) \\nonumber\\\\ = \\int^{z}_{ml}dz ' \\ , c(z ' )   { \\bf g}^{(z')}\\big ( v^{(z')}(t , z')\\big),\\end{aligned}\\ ] ] where @xmath61 is the inverse fourier transform over @xmath57 ; @xmath62   \\big ) } \\hat v^{(z)}(\\omega , z)$ ] in the @xmath57-space . </S>",
    "<S> it follows from eqs . @xmath63 </S>",
    "<S> that the numerical procedure for calculation of @xmath64 requires the following eight steps :    \\(i ) the inverse fourier transform of @xmath65   \\big)}$ ] for every value of @xmath3 @xmath66    \\(ii ) a calculation of @xmath67 from @xmath68 .    </S>",
    "<S> \\(iii ) the forward fourier transform of @xmath69    \\(iv ) a numerical integration ( summation ) of @xmath70   \\big)}\\hat v^{(z')}(\\omega)$ ] over @xmath71 ( from @xmath72 to @xmath73 ) for every values of @xmath57 and @xmath74 this integration gives @xmath75 according to eq . </S>",
    "<S> @xmath50 .    </S>",
    "<S> \\(v ) the inverse fourier transform of @xmath76   \\big)}$ ] for every value of @xmath3 , @xmath77 . </S>",
    "<S> ( note that in contrast with step ( i ) it is necessary to take into account the dependence of @xmath78 on @xmath3 ) .    </S>",
    "<S> ( vi)-(viii ) these steps are similar to steps ( ii)-(iv ) except that the new value of @xmath79 is used which was obtained in step ( v ) .    </S>",
    "<S> the forward and inverse fourier transforms can be performed with the fast fourier transform ( fft ) which requires @xmath80 numerical operations . </S>",
    "<S> steps ( i)-(iii ) need only the value of @xmath81 these steps can be performed independently and simultaneously in a network of @xmath1 central processor units ( cpus ) , shown schematically in fig . </S>",
    "<S> 1 . the number of cpus , @xmath82 coincides with the number of grid points for integration over @xmath3 . </S>",
    "<S> thus the effective computational time equals to time necessary to perform @xmath83 operations on complex numbers in one cpu . </S>",
    "<S> below to estimate effective computational time one always refers to the number of numerical operations in one cpu if all calculations can be implemented simultaneously in different cpus without communication between them .    </S>",
    "<S> the resulting values of @xmath67 ( after step ( iii ) ) are a set of @xmath1 vectors @xmath84 consisting of @xmath56 complex numbers each . </S>",
    "<S> every vector @xmath85 is stored in the memory of the @xmath38th cpu ( or in memory assigned to @xmath38th cpu in shared memory network ) . to perform step ( iv ) </S>",
    "<S> one replaces these vectors by the new vectors @xmath86 : @xmath87 . here </S>",
    "<S> a simple parallel algorithm is given . note </S>",
    "<S> that this algorithm can be improved but this improvement is outside the scope of this letter . </S>",
    "<S> it is assumed that @xmath1 is a power of 2 : @xmath88 , @xmath89 is an integer . </S>",
    "<S> the proposed algorithm requires @xmath89 substeps . </S>",
    "<S> the vectors @xmath90 are results of @xmath91th substep stored in memory . </S>",
    "<S> so that @xmath92 . </S>",
    "<S> the first substep is to sum up every pair of vectors : @xmath93 to get @xmath94 . </S>",
    "<S> this summation requires @xmath56 operations . by induction </S>",
    "<S> one can see that after @xmath91 substeps , @xmath95 for @xmath96 @xmath97 for @xmath98 @xmath99 for @xmath100 note that m vectors are now grouped in @xmath101 blocks with the appropriate summation inside each block . to perform the @xmath102th substep , </S>",
    "<S> it is necessary to double the block size . </S>",
    "<S> this can be done by adding the last element of each odd block to each element of next even block . to do this , </S>",
    "<S> one first creates in memory of @xmath103 copies of the last element of each odd block , which requires @xmath104 operations in a parallel cpu network . </S>",
    "<S> ( a number of copies can be doubled by memory forking after each @xmath56 operations ) . to complete the @xmath102th substep </S>",
    "<S> , it is now enough to simultaneously add @xmath103 copies to each element in the even block , requiring @xmath56 operations . </S>",
    "<S> the total number of operations for step ( iv ) is @xmath105n = m_e(m_e+1)/2 $ ] . </S>",
    "<S> steps ( v)-(viii ) can be done in about @xmath106 $ ] operations . </S>",
    "<S> ( in step ( viii ) it is only necessary to calculate @xmath107 requiring @xmath108 $ ] operations ) . thus the total number of operation for steps ( i)-(viii ) is : @xmath109\\sim n[4 log_2(n)+\\frac{log_2(m)^2}{2}].\\end{aligned}\\ ] ] direct solution of @xmath19 by a split - step method with the same accuracy ( for the same size of numerical step , @xmath110 and the same number of points @xmath56 in @xmath57 space ) requires @xmath111 operations . comparing this with @xmath112 one </S>",
    "<S> can conclude that the proposed parallel algorithm allows one to do numerical simulations with the same numerical accuracy about @xmath0 times faster using a network of @xmath1 parallel cpus . </S>",
    "<S> however the proposed algorithm is about 2 times slower if only one cpu is used .    </S>",
    "<S> numerical simulations of the wdm system were performed using both the split - step method for nls @xmath19 and using the numerical algorithm given by eqs . @xmath49 and @xmath113 to demonstrate the accuracy of the proposed numerical scheme . </S>",
    "<S> simulations were performed for 5 wdm channels ( 20 gb / s per channel ) over a typical transoceanic distance of @xmath114 . </S>",
    "<S> the channel spacing was @xmath115 the gvd periodically alternates between spans of standard monomode fiber ( @xmath116 @xmath117 @xmath118 , length @xmath119 ) and dispersion compensating fiber ( @xmath120 @xmath121 @xmath122 , length @xmath123 ) so that the average gvd is zero . </S>",
    "<S> fiber losses and amplifiers were not considered . </S>",
    "<S> however they can be easily included in the coefficient @xmath124 a pseudo - random binary sequence of length 20 was used for every wdm channel . </S>",
    "<S> the boundary conditions are periodic in time . </S>",
    "<S> each binary  1 \" was represented by an initially zero - chirp gaussian pulse ( return to zero format ) of @xmath125 width and peak power @xmath126 at the beginning @xmath127 of the fiber line which is taken at the middle of standard monomode fiber span . </S>",
    "<S> the integration length @xmath53 ( see eqs . </S>",
    "<S> @xmath128 ) is set to be equal to @xmath129 ; @xmath130 and @xmath131 fig . </S>",
    "<S> 2 shows the pulse power distribution ( simultaneously in all 5 channels ) after propagating @xmath132 obtained from both the split - step and the proposed parallel algorithm . </S>",
    "<S> the differences in power distribution between these two simulations are less than 1@xmath133 so the two curves are indistinguishable in fig . </S>",
    "<S> 2 . </S>",
    "<S> numerical simulations were performed on usual workstation without use of parallel computations . </S>",
    "<S> the objective of this numerical example is to demonstrate the relative accuracy of numerical algorithm . </S>",
    "<S> hardware implementation of the parallel simulation for numerical algorithm @xmath49 and @xmath134 is beyond the scope of this letter .    </S>",
    "<S> one can conclude that the proposed parallel numerical algorithm allows one to implement numerical simulations of eq . </S>",
    "<S> @xmath135 about @xmath0 times faster than a direct numerical simulation of that eq . by the split - step method with the same accuracy </S>",
    "<S> the absence of communications between parallel cpus during the computation of the fft allows one to implement the proposed massive parallel algorithm on workstation clusters .    </S>",
    "<S> the author thanks m. chertkov , i.r . </S>",
    "<S> gabitov and s. tretiak for helpful discussions .    </S>",
    "<S> support was provided by the department of energy , under contract w-7405-eng-36 .    </S>",
    "<S> e - mail address : lushnikov@cnls.lanl.gov .    </S>",
    "<S> d.  leguen , s.  del burgo , l.  moulinard , d.  grot , m.  henry , f.  favre , t.  georges , ofc / iooc99 , 21 - 26 feb . 1999 , </S>",
    "<S> san diego , ca , usa    l.f . </S>",
    "<S> mollenauer , p.v . </S>",
    "<S> mamyshev , j. gripp , m.j . </S>",
    "<S> neubelt , n. mamysheva , l. grner - nielsen and t. veng , optics letters , * 25 * , 704 ( 2000 ) .    c. lin , h. kogelnik and l.g . </S>",
    "<S> cohen , opt . </S>",
    "<S> lett . , * 5 * , 476 ( 1980 ) .    c. kurtzke , ieee phot . </S>",
    "<S> tech . </S>",
    "<S> lett . , </S>",
    "<S> * 5 * , 1250 ( 1993 ) .    </S>",
    "<S> a.r . </S>",
    "<S> chraplyvy , a.h . </S>",
    "<S> gnauck , r.w . </S>",
    "<S> tkach and r.m . </S>",
    "<S> derosier , ieee phot . </S>",
    "<S> tech . </S>",
    "<S> lett . , * 5 * , 1233 ( 1993 ) .    </S>",
    "<S> n.j . smith , f.m.knox , n.j . </S>",
    "<S> doran , k.j . </S>",
    "<S> blow and i. bennion , electron . </S>",
    "<S> lett . </S>",
    "<S> , * 32 * , 54 ( 1996 ) .    i.r </S>",
    "<S> gabitov and s.k . </S>",
    "<S> turitsyn , opt . </S>",
    "<S> lett . , </S>",
    "<S> * 21 * , 327 ( 1996 ) .    </S>",
    "<S> i. gabitov and s.k . </S>",
    "<S> turitsyn , jetp lett . , * 63 * , 861 ( 1996 ) .    p.m. lushnikov , opt . lett . * </S>",
    "<S> 26 * , 1535 ( 2001 ) . </S>"
  ]
}