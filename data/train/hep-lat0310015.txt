{
  "article_text": [
    "the joint cp - pacs / jlqcd collaborations have been carrying out  @xcite three - flavor full qcd simulations with the iwasaki rg gauge action and the non - perturbatively @xmath0 improved wilson quark action using the polynomial hybrid monte carlo ( phmc ) method  @xcite on a variety of computers .",
    "one of the computers is the earth simulator(es ) at the es center  @xcite .",
    "it was made available under the project `` study of the standard model of elementary particles on the lattice with the earth simulator '' which was approved as one of the `` epoch making simulation projects '' of the es center .",
    "here we report on the performance of our phmc program on es .",
    "the es consists of 640 processing nodes ( pn ) connected by a one - dimensional crossbar switch with 12.3 gb / s bi - directional bandwidth .",
    "each pn is an smp with 8 vector - type arithmetic processors ( ap ) , each with a peak speed of 8gflops . among several programming models ,",
    "we employ micro - tasking by hand parallelization for 8 ap s of a single pn and mpi for communications between pn s .",
    "our phmc code was originally developed at kek for hitachi sr8000 and sustains as much as 40% of peak speed as a whole .",
    "on the es , however , the performance turned out to be of order 10% . to explore an effective coding style ,",
    "we take the basic mult subroutine for wilson - dirac matrix - vector multiplication , and measure the performance for seven types of coding on a single ap .",
    "results range from 10 to 73% for a @xmath1 lattice .",
    "the highest performance is achieved by a code written originally for a vector - parallel machine , fujitsu vpp500 . in this code , sites are one - dimensionalized on the @xmath2 plane and divided by four to realize a large vector length without list vectors .",
    "other codes with smaller vector lengths ( e.g. that for hitachi sr8000 in which only @xmath3 direction is vectorized ) show low performance of at most 30% .",
    "hence we rewrite the entire code with the coding style used for vpp500 .",
    "we subdivide the whole lattice on the @xmath4 plane and assign each region to a pn and parallelize with mpi . in one pn ,",
    "loop indices in the @xmath5 and @xmath6 directions and an even / odd flag for four vector loops mentioned above are one - dimensionalized and divided by eight .",
    "this enables the compiler to do an automatic parallelization ( micro - tasking ) of all appropriate do - loops .",
    "since performance depends on the lattice size and the number of pn , we examine three cases , a ) @xmath7 on @xmath8 pn ( @xmath9 per pn ) , b ) @xmath10 on @xmath11 pn ( @xmath12 per pn ) , and c ) @xmath13 on @xmath14 pn ( @xmath15 per pn ) .",
    "= 6.7 cm    the vector processing performance on a single ap is an important fundamental .",
    "our program includes redundant arithmetic calculations at edges in the @xmath16 direction to realize a long vector length .",
    "therefore we distinguish the performance reported by the system analyzer  ` ftrace' and that calculated theoretically for an effective part excluding the redundant operations .",
    "for the latter , total flops equals 1296flops@xmath17#sites .",
    "the redundant part costs @xmath18% of peak performance .",
    "figure  [ fig : mult-1ap-24 ] shows the single ap performance of mult for case b ) plotted versus @xmath19 .",
    "we test two codes . in the original one ,",
    "contributions from 8 directions are calculated in one large do - loop and are summed up later . in the revised code , which intends to overlay arithmetic operations and communications in future ,",
    "the large loop is divided into two loops for @xmath20 and @xmath21 directions .",
    "the array structure is also different .",
    "the revised code runs about 15% slower .",
    "we suppose that this is partly caused by a slow startup of do - loops .",
    "in general , the vector performance of the revised mult code reaches 55 - 65% for all three cases .",
    "however , it drops by about 10% when the vector length just exceeds a multiple of the size of vector registers , 256 .",
    "= 6.7 cm    the cost of automatic parallelization is another important point , because smp of vector processors is a distinctive feature of pn .",
    "figure  [ fig : mult - mt ] shows the efficiency against # ap for @xmath22 and @xmath23 lattices .",
    "the micro - tasking parallelization costs 3 to 4% which is not so high , while memory copy to implement boundary conditions is relatively heavy , being 4% for 1 ap and 7% for 8 ap s .      in our code one pn issues an mpi_irsend for a gathered data and an adjacent pn issues an mpi_irecv and then scatters the received data .",
    "this enables us to construct long messages .",
    "the message size ranges from 0.34 mb to 1.64 mb , and the throughput ranges from 1.62gb / sec to 5.56gb / sec .",
    "these numbers are consistent with a mpi performance report from the es center .",
    "communication performance drops by 20% due to buffer copy for gather / scatter , _",
    "e.g. , _ the throughput for the longest message drops to 4.35gb / sec .",
    "= 6.7 cm    = 6.7 cm    in order to show how various overheads affect the overall efficiency , we show in fig .",
    "[ fig : mult - upto72 ] the mult performance starting from 1 ap up to 72 ap s ( 9 pn s ) .",
    "the lattice volume per ap is fixed to @xmath24 , which corresponds to a @xmath25 lattice on a @xmath14 pn array .",
    "the performance of 61.6% for 1 ap finally drops to 35.4% for 72 ap s .",
    "the main cause of the drop is a slow speed of mpi communications which are not overlaid with arithmetic operations , and secondly the cost of memory copy in one pn , which together costs 40% relative to the total execution time .",
    "the fraction becomes higher when volume / node becomes smaller ; for a @xmath26 lattice on a @xmath8 pn array , 60% of execution time is spent for communications and memory copy .",
    "figure  [ fig : scaling ] shows the sustained speed in gflops versus # ap for a @xmath27 lattice . in this case ,",
    "lattice size is fixed for all measurements . for 1 ap , 4.12gflops ( 52% efficiency )",
    "is achived , and 164.83gflops for 72 ap s is about 40 times that for 1 ap . in other words , the parallelization efficiency is 40/72 @xmath28 56% .",
    ".profile ( % ) , performance ( gflops ) per 1ap and total efficiency ( % ) of the phmc program .",
    "[ cols= \" < , > , > , > , > \" , ]     table  [ tab : phmc ] shows the profile of the entire phmc program as provided by the es profiler .",
    "the arithmetic calculations and boundary copy / communications in mult are the two heaviest routines .",
    "the multiplication of the inverse clover term ( mlci ) and bicgstab ( bicg ) are the third and fourth heaviest , which are relatively light .",
    "therefore , for the next round of simulations on a @xmath13 lattice , we plan to overlay arithmetic operations and communications in the mult routine .",
    "as table  [ tab : phmc ] shows the phmc program runs on the es with an efficiency of 2540% for our four target lattice sizes .",
    "currently we are executing on a @xmath26 lattice at @xmath29 .",
    "the efficiency of 31% on the es is comparable to that on other machines , 35% on sr8000/f1 at kek with 32 nodes , 44% on vpp5000 at tsukuba with 8 nodes , and 20% on cp - pacs with 500 nodes .",
    "we would like to thank the es center for the approval of our project , k.  itakura for continuous technical support , and rcnp , osaka university for some cpu time of sx5 at early stage of this project .",
    "this work is supported in part by grants - in - aid of the ministry of education no.15204015 ."
  ],
  "abstract_text": [
    "<S> we report on coding and performance of our polynomial hybrid monte carlo program on the earth simulator . at present </S>",
    "<S> the entire program achieves 2540% efficiency . </S>",
    "<S> an analysis of overheads shows that a tuning of inter - node communications is required for further improvement . </S>"
  ]
}