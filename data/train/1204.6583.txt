{
  "article_text": [
    "in classification problems , the goal is to predict output labels for given input vectors . for this purpose ,",
    "a decision function defined on the input space is estimated from training samples .",
    "the output value of the decision function is used for the label prediction . in binary classification problems ,",
    "the label is predicted by the sign of the decision function .",
    "many learning algorithms use loss functions to measure the penalty of misclassifications .",
    "the decision function minimizing the empirical mean of the loss function over training samples is employed as the estimator @xcite .",
    "for example , hinge loss , exponential loss and logistic loss are used for support vector machine ( svm ) , adaboost and logistic regression , respectively . especially in the binary classification tasks , statistical properties of learning algorithms based on loss functions are well - understood due to intensive recent works .",
    "see @xcite for details .    as another approach",
    ", the maximum - margin criterion is also applied for the statistical learning . under the maximum - margin criterion , the best separating hyperplane between the two output labels",
    "is employed as the decision function . in hard - margin svm @xcite , a convex - hull of input vectors for each binary label",
    "is defined , and the maximum - margin between the two convex - hulls is considered . for the non - separable case , @xmath0-svm provides a similar picture @xcite . in @xmath0-svm , the so - called reduced convex - hull which is a subset of the original convex - hull",
    "is used for the learning .",
    "a reduced convex - hull is defined for each label , and the best separating hyperplane between the two reduced convex - hulls is employed as the decision function .",
    "not only polyhedral sets such as the convex - hull of finite input points but also ellipsoidal sets are applied for classification problems @xcite . in this paper ,",
    "the set used in the maximum - margin criterion is referred to as _ uncertainty set_. this term is borrowed from robust optimization in mathematical programming @xcite .",
    "there are some works in which the statistical properties of the learning based on the uncertainty set are studied .",
    "for example , @xcite proposed minimax probability machine ( mpm ) using the ellipsoidal uncertainty sets , and studied statistical properties under the worst - case setting . in the statistical learning using uncertainty set ,",
    "the main concern is to develop optimization algorithms under the maximum margin criterion @xcite .",
    "so far , statistical properties of the learning algorithm using uncertainty sets have not been intensively studied compared to the learning using loss functions .",
    "the main purpose of this paper is to study the learning algorithm using the uncertainty set .",
    "we focus on the relation between the loss function and the uncertainty set .",
    "we show that the uncertainty set is described by using the conjugate function of the loss function . for given uncertainty set ,",
    "we construct the corresponding loss function .",
    "we study the statistical properties of the learning algorithm using the uncertainty set by applying theoretical results on the loss function approach .",
    "then , we establish the statistical consistency of learning algorithms using the uncertainty set .",
    "we point out that in general the maximum margin criterion for a fixed uncertainty set does not provide accurate decision functions .",
    "we need to introduce a parametrized uncertainty set by the one - dimensional parameter which specifies the size of the uncertainty set .",
    "we show that a modified maximum margin criterion with the parametrized uncertainty set recovers the statistical consistency .",
    "the paper is organized as follows . in section [ sec : preliminaries ] , we introduce the existing method based on the uncertainty set . in section [ sec : loss_and_uncertainty ] , we investigate the relation between loss functions and uncertainty sets .",
    "section [ sec : revision_uncertainty_set ] is devoted to illustrate a way of revising the uncertainty set to recover nice statistical properties . in section",
    "[ sec : learning_algorithm ] , we present a kernel - based learning algorithm with uncertainty sets . in section [ sec : statistical_properties ] , we prove that the proposed algorithm has the statistical consistency .",
    "numerical experiments are shown in section [ sec : numerical_studies ] .",
    "we conclude in section [ sec : conclusion ] .",
    "some proofs are shown in appendix .",
    "we summarize some notations to be used throughout the paper .",
    "the indicator function is denoted as @xmath1 , i.e. , @xmath1 equals @xmath2 if @xmath3 is true , and @xmath4 otherwise .",
    "the column vector @xmath5 in the euclidean space is described in bold face .",
    "the transposition of @xmath5 is denoted as @xmath6 .",
    "the euclidean norm of the vector @xmath5 is expressed as @xmath7 .",
    "for a set @xmath8 in a linear space , the convex - hull of @xmath8 is denoted as @xmath9 or @xmath10 .",
    "the number of elements in the set @xmath8 is denoted as @xmath11 .",
    "the expectation of the random variable @xmath12 w.r.t .",
    "the probability distribution @xmath13 is described as @xmath14 $ ] .",
    "we will drop the subscript @xmath13 as @xmath15 $ ] , when it is clear from the context .",
    "the set of all measurable functions on the set @xmath16 is denoted by @xmath17 or @xmath18 for short .",
    "the supremum norm of @xmath19 is denoted as @xmath20 .",
    "for the reproducing kernel hilbert space @xmath21 , @xmath22 is the norm of @xmath23 defined from the inner product @xmath24 on @xmath21 .",
    "we define @xmath16 as the input space and @xmath25 as the set of binary labels .",
    "suppose that the training samples @xmath26 are drawn i.i.d . according to a probability distribution @xmath13 on @xmath27 .",
    "the goal is to estimate a decision function @xmath28 from a set of functions @xmath29 , such that the sign of @xmath30 provides an accurate prediction of the unknown binary label associated with the input @xmath31 under the probability distribution @xmath13 . in other word , for the estimated decision function @xmath32",
    ", the probability of @xmath33 is expected to be as small as possible . in this article , the composite function of the sign function and the decision function , @xmath34 , is referred to as classifier .      in binary classification problems ,",
    "the prediction accuracy of the decision function @xmath32 is measured by the 0 - 1 loss @xmath35 which equals @xmath2 when the sign of @xmath30 is different from @xmath36 and @xmath4 otherwise .",
    "the average prediction performance of the decision function @xmath32 is evaluated by the expected 0 - 1 loss , i.e. , @xmath37.\\end{aligned}\\ ] ] the bayes risk @xmath38 is defined as the minimum value of the expected 0 - 1 loss over all the measurable functions on @xmath16 , @xmath39 bayes risk is the lowest achievable error rate under the probability @xmath13 .",
    "given the set of training samples , @xmath40 , the empirical 0 - 1 loss is denoted by @xmath41 the subscript @xmath42 in @xmath43 is dropped if it is clear from the context .    in general , minimization of @xmath43 is considered as a hard problem @xcite .",
    "the main difficulty is considered to come from non - convexity of the 0 - 1 loss @xmath35 as the function of @xmath32 .",
    "hence , many learning algorithms use a surrogate loss of the 0 - 1 loss in order to make the computation tractable .",
    "for example , svm uses the hinge loss , @xmath44 , and adaboost uses the exponential loss , @xmath45 .",
    "both the hinge loss and the exponential loss are convex in @xmath32 , and they provide an upper bound of the 0 - 1 loss .",
    "thus , the minimizer under the surrogate loss is also expected to minimize the 0 - 1 loss .",
    "the quantitative relation between the 0 - 1 loss and the surrogate loss was studied by @xcite .",
    "to avoid overfitting of the estimated decision function to training samples , the regularization is considered . by adding the regularization term such as the squared norm of the decision function to the empirical surrogate loss",
    ", the complexity of the estimated classifier is restricted .",
    "the balance between the regularization term and the surrogate loss is adjusted by the regularization parameter   @xcite .",
    "then , the deviation of the empirical 0 - 1 loss and the expected 0 - 1 loss is controlled by the regularization .",
    "when both the regularization term and the surrogate loss are convex , the computational tractability of the statistical learning is retained .      besides statistical learning using loss functions , there is another approach to the classification problems , i.e. , statistical learning based on the so - called _",
    "uncertainty set_. we briefly introduce the basic idea of the uncertainty set .",
    "we assume that @xmath16 is a subset of euclidean space .    in robust optimization problems",
    "@xcite , the uncertainty set describes uncertainties or ambiguities included in optimization problems .",
    "the parameter in the optimization problem may not be precisely determined . instead of the precise information",
    ", we have an uncertainty set which probably includes the parameter in the optimization problem .",
    "the worst - case setting is employed to solve the robust optimization problem with the uncertainty set .",
    "the statistical learning with uncertainty set is considered as an application of the robust optimization to classification problems . in classification problems ,",
    "the uncertainty set is designed such that most training samples are included in the uncertainty set with high probability .",
    "we prepare an uncertainty set for each binary label . for example , @xmath46 and @xmath47 are the confidence regions such that the conditional probabilities , @xmath48 and @xmath49 , are equal to @xmath50 . as the other example , the uncertainty set @xmath46 ( resp .",
    "@xmath47 ) consists of the convex - hull of input vectors in training samples having the positive ( resp .",
    "negative ) label .",
    "the convex - hull of data points is used in hard margin svm   @xcite . the ellipsoidal uncertainty set is also used for the robust classification under the worst - case setting  @xcite . based on the uncertainty set , we estimate the linear decision function @xmath51 . here , we consider the _ minimum distance problem _ @xmath52 let @xmath53 and @xmath54 be optimal solutions of .",
    "then , the normal vector of the decision function , @xmath55 , is estimated by @xmath56 , where @xmath57 is a positive real number .",
    "figure [ fig : uncertaintyset_approach ] illustrates the estimated decision boundary . when both @xmath46 and @xmath47 are compact subsets satisfying @xmath58 , the estimated normal vector can not be the null vector .",
    "the minimum distance problem appears in the hard margin svm @xcite , @xmath0-svm @xcite and the learning algorithms proposed by @xcite . in section [ subsec : uncertainty_set_nusvm ] , we briefly introduce the relation between @xmath0-svm and the minimum distance problem . in minimax probability machine ( mpm )",
    "proposed by @xcite , the other criterion is applied to estimate the linear decision function , though the ellipsoidal uncertainty set plays an important role also in their algorithm .     and @xmath47 . ]",
    "the minimum distance problem is equivalent with the maximum margin principle @xcite .",
    "when the bias term @xmath59 in the linear decision function is estimated such that the decision boundary bisects the line segment connecting @xmath53 and @xmath54 , the estimated decision boundary achieves the maximum margin between the uncertainty sets , @xmath60 . according to @xcite ,",
    "we explain how the maximum margin is connected with the minimum distance .",
    "suppose that @xmath46 and @xmath47 are convex subsets and that @xmath58 holds .",
    "then , the margin of two uncertainty sets along the direction of @xmath55 is given as @xmath61 the maximum margin criterion is described as @xmath62 the equality above follows from the minimum norm duality @xcite .",
    "we study the relation between loss functions and uncertainty sets .",
    "first , we introduce the relation in @xmath0-svm according to @xcite and @xcite .",
    "then , we present an extension of @xmath0-svm to investigate a generalized relation between loss functions and uncertainty sets .",
    "suppose that the input space @xmath16 is a subset of euclidean space @xmath63 .",
    "we consider the linear decision function , @xmath51 , where the normal vector @xmath64 and the bias term @xmath65 are to be estimated based on observed training samples . by applying the kernel trick @xcite",
    ", we obtain rich statistical models for the decision function , while keeping the computational tractability .    in @xmath0-svm ,",
    "the classifier is estimated as the optimal solution of @xmath66 where @xmath67 is a prespecified constant which has the role of the regularization parameter . as @xcite pointed out",
    ", the parameter @xmath0 controls the margin errors and number of support vectors . in @xmath0-svm ,",
    "a variant of the hinge loss , @xmath68 , is used as the surrogate loss . in the original formulation of @xmath0-svm , the non - negativity constraint , @xmath69 ,",
    "is introduced . as shown by @xcite",
    ", we can confirm that the non - negativity constraint is redundant .",
    "indeed , for an optimal solution @xmath70 , we have @xmath71 where the last inequality comes from the fact that the parameter , @xmath72 , is a feasible solution of . as a result",
    ", we have @xmath73 for @xmath74 .",
    "we briefly show that the dual problem of yields the minimum distance problem in which the reduced convex - hulls of training samples are used as uncertainty sets .",
    "see @xcite for details .",
    "the problem is equivalent with @xmath75 then , the lagrangian function is defined as @xmath76 where @xmath77 are non - negative lagrange multipliers . for the observed training samples ,",
    "we define @xmath78 and @xmath79 as the set of sample indices for each label , i.e. , @xmath80 by applying min - max theorem , we have @xmath81 where the last equality is obtained by changing the variable from @xmath82 to @xmath83 . for the positive ( resp .",
    "negative ) label , we introduce the uncertainty set @xmath46 ( reps .",
    "@xmath47 ) defined by the reduced convex - hull , i.e. , @xmath84 when the upper limit of @xmath85 is less than one , the reduced convex - hull is a subset of the convex - hull of training samples .",
    "we find that solving the problem is identical to solving the minimum distance problem under the uncertainty set of the reduced convex - hulls , @xmath86 the representation based on the minimum distance problem provides an intuitive understanding of the learning algorithm .",
    "we consider general loss functions , and study the relation between the loss function and the corresponding uncertainty set .",
    "again , the decision function is defined as @xmath51 on @xmath63 .",
    "let @xmath87 be a convex and non - decreasing function .",
    "for the training samples , @xmath88 , we propose a learning method in which the decision function is estimated by solving @xmath89 the regularization effect is introduced by the constraint @xmath90 , where @xmath91 is the regularization parameter which may depend on the sample size .",
    "the statistical learning using is regarded as an extension of @xmath0-svm . to see this , we define @xmath92 .",
    "let @xmath70 be an optimal solution of for a fixed @xmath67 . by comparing the optimality conditions of and",
    ", we can confirm that the problem with @xmath93 has the same optimal solution as @xmath0-svm .    in the similar way as @xmath0-svm",
    ", we derive the uncertainty set associated with the loss function @xmath94 in .",
    "we introduce the slack variables @xmath95 satisfying the inequalities @xmath96 .",
    "then , the lagrangian function of is given as @xmath97 where @xmath98 and @xmath99 are the non - negative lagrange multipliers .",
    "the optimality conditions , @xmath100 and the non - negativity of @xmath82 lead to the constraint on lagrange multipliers , @xmath101 we define the conjugate function of @xmath102 as @xmath103 then , by applying min - max theorem , we have @xmath104 in section [ sec : statistical_properties ] , we present a rigorous proof that under some assumptions on @xmath105 , the min - max theorem works in the above lagrangian function , i.e. , there is no duality gap . for each binary label , we define the parametrized uncertainty sets , @xmath106 $ ] and @xmath107 $ ] , by @xmath108=\\bigg\\{\\sum_{i\\in{m_o}}\\alpha_i\\x_i   \\,:\\ , \\alpha_i\\geq0 , \\,\\sum_{i\\in{m_o}}\\alpha_i=1,\\    \\frac{1}{m}\\sum_{i\\in{m_o}}\\ell^*(m\\alpha_i)\\leq{c } \\bigg\\}.    \\label{eqn : uncertainty - set}\\end{aligned}\\ ] ] then , the optimization problem in is represented by @xmath109,\\,\\z_n\\in\\ucal_n[c_n],\\ c_p,\\,c_n\\in\\rbb .",
    "\\label{eqn : rcm - representation}\\end{aligned}\\ ] ] let @xmath110 and @xmath111 be the optimal solution of @xmath112 and @xmath113 in .",
    "let @xmath114 be an optimal solution of @xmath55 in .",
    "the saddle point of the above min - max problem provides the relation between the @xmath110 , @xmath111 and @xmath114 . some calculation yields that , when @xmath115 holds , any vector such that @xmath116 satisfies the kkt condition of . on the other hand ,",
    "when @xmath117 holds , @xmath114 is given by @xmath118 .",
    "hence , an optimal solution of the normal vector in the linear decision function is given as @xmath119 we show a sufficient condition that the equality @xmath120 holds .",
    "suppose that @xmath121\\cap\\ucal_n[c_n]$ ] is nonempty for all @xmath122 and @xmath123 , whenever @xmath121 $ ] and @xmath124 $ ] are both nonempty .",
    "then , clearly @xmath125\\cap\\ucal_n[c_n]$ ] is the optimal choice of the objective function in . in @xmath0-svm with a small @xmath74 , the reduced convex - hulls satisfy @xmath58 , and hence , @xmath120 and @xmath126 hold .",
    "the bias term @xmath59 in the linear decision function is not directly obtained from the optimal solution of without knowing the explicit form of the loss function @xmath94 . a simple way of estimating the bias term is to choose @xmath127 , which provides the decision boundary bisecting the line segment connecting @xmath110 and @xmath111 . in the learning algorithm proposed in section [ sec : learning_algorithm ]",
    ", the bias term is estimated by minimizing the error rate @xmath128 since the estimated normal vector @xmath114 is substituted in the above objective function , the optimization is tractable .",
    "based on the argument above , we propose the learning algorithm using uncertainty sets in figure [ fig : simple_learning_algorithm ] .",
    "it is straightforward to apply the kernel method to the algorithm . in order to study statistical properties of the learning algorithm based on uncertainty sets , we need more elaborate description on the algorithm .",
    "details are presented in section [ sec : learning_algorithm ] .",
    "we show some examples of uncertainty sets associated with popular loss functions . in the following examples , the index sets , @xmath78 and @xmath79 , are defined by for the training samples @xmath88 , and let @xmath129 and @xmath130 be @xmath131 and @xmath132 , respectively .",
    "as explained above , the problem is reduced to @xmath0-svm by defining @xmath92 .",
    "the conjugate function of @xmath94 is given as @xmath133,\\\\           \\infty , & \\alpha\\not\\in[0,2/\\nu],\\\\          \\end{cases }   \\end{aligned}\\ ] ] and the associated uncertainty set is defined by @xmath134&=    \\begin{cases }     \\displaystyle     \\bigg\\ {     \\sum_{i\\in{m_o}}\\alpha_i\\x_i\\,:\\,\\sum_{i\\in{m_o}}\\alpha_i=1,\\ ,     0\\leq\\alpha_i\\leq\\frac{2}{m\\nu},\\,i\\in{m_o }     \\bigg\\ } , & c\\geq0,\\\\     \\displaystyle     \\emptyset , & c<0 .     \\end{cases}\\quad",
    "\\end{aligned}\\ ] ] for @xmath135 , the uncertainty set consists of the reduced convex - hull of training samples , and it does not depend on the parameter @xmath57 .",
    "in addition , the negative @xmath57 is infeasible .",
    "hence , in the problem , optimal solutions of @xmath122 and @xmath123 are given as @xmath136 , and the problem is reduced to the simple minimum distance problem .",
    "[ example : uncertainty_truncated_quadratic ] now consider @xmath137 .",
    "the conjugate function is @xmath138 for @xmath139 , we define @xmath140 and @xmath141 as the empirical mean and the empirical covariance matrix of the samples @xmath142 , i.e. , @xmath143 suppose that @xmath141 is invertible .",
    "then , the uncertainty set corresponding to the truncated quadratic loss is given as @xmath134    & =     \\bigg\\ {     \\sum_{i\\in{m_o}}\\alpha_i\\x_i\\,:\\,\\sum_{i\\in{m_o}}\\alpha_i=1,\\ ,    \\alpha_i\\geq{0},\\,i\\in{m_o},\\,\\sum_{i\\in{m_o}}\\alpha_i^2\\leq \\frac{4(c+1)}{m }    \\bigg\\}\\\\    & =     \\bigg\\ {    \\z\\in\\conv\\{\\x_i : i\\in{m_o}\\}\\,:\\ ,    ( \\z-\\bar{\\x}_o)^t\\widehat{\\sigma}_o^{-1}(\\z-\\bar{\\x}_o)\\leq\\frac{4(c+1)m_o}{m }    \\bigg\\}.    \\end{aligned}\\ ] ] to prove the second equality , let us define the matrix @xmath144 . for @xmath145 satisfying the constraints , the equality @xmath146 holds , where @xmath147 .",
    "then , the singular value decomposition of the matrix @xmath148 and the constraint @xmath149 yield the second equality . a similar uncertainty set",
    "is used in minimax probability machine ( mpm ) @xcite and maximum margin mpm @xcite , though the constraint , @xmath150 , is not imposed in these learning methods .",
    "the loss function @xmath151 is used in adaboost @xcite .",
    "the conjugate function is equal to @xmath152 hence , the corresponding uncertainty set is defined as @xmath153=     \\bigg\\ {     \\sum_{i\\in{m_o}}\\alpha_i\\x_i\\,:\\,\\sum_{i\\in{m_o}}\\alpha_i=1,\\ ,     \\alpha_i\\geq{0},\\,i\\in{m_o},\\ ,     \\sum_{i\\in{m_o}}\\alpha_i\\log\\frac{\\alpha_i}{1/m_o}\\leq{}c+1+\\log\\frac{m_o}{m}\\bigg\\ }    \\end{aligned}\\ ] ] for @xmath139 . in the uncertainty set , the kullback - leibler divergence from the weight @xmath154 to the uniform weight is bounded above .    in this section",
    ", we derived parametrized uncertainty sets associated with convex loss functions .",
    "inversely , if the uncertainty set is represented as the form of , there exists the corresponding loss function .",
    "when we consider statistical properties of the classifier estimated based on the uncertainty set , we can study the equivalent estimator derived from the corresponding loss function .",
    "we have many theoretical tools to analyze such estimators .",
    "however , if the uncertainty set does not have the expression of , the corresponding loss function would not exist . in this case , we can not apply the standard theoretical tools to understand statistical properties of learning algorithms based on such uncertainty sets .",
    "one way to remedy the drawback is to revise the uncertainty set so as to possess the corresponding loss function .",
    "the next section is devoted to study a way of revising the uncertainty set .",
    "given a parametrized uncertainty set , generally there does not exist the loss function which corresponds to the uncertainty set . in this section , we present a way of revising the uncertainty set such that there exists a corresponding loss function .",
    "we consider two kinds of representations for parametrized uncertainty sets : one is vertex representation , and the other is level - set representation .",
    "let @xmath155 and @xmath156 be index sets defined in , and we define @xmath131 and @xmath132 .",
    "for @xmath139 , let @xmath157 be a closed , convex , proper function on @xmath158 , and @xmath159 be the conjugate function of @xmath157 .",
    "the argument of @xmath159 is represented by @xmath145 .",
    "vertex representation _ of the uncertainty set is defined as @xmath160=   \\bigg\\ {   \\sum_{i\\in{m_o}}\\alpha_i\\x_i\\,:\\ ,   l_o^*(\\alphabold_o)\\leq{c }   \\bigg\\},\\quad{}o\\in\\{p , n\\}.    \\label{eqn : uncertainty - vertex - rep}\\end{aligned}\\ ] ] in example [ example : uncertainty_truncated_quadratic ] , the function @xmath161 is employed . on the other hand ,",
    "let us define @xmath162 as a closed , convex , proper function , and @xmath163 be the conjugate of @xmath164 .",
    "the _ level - set representation _ of the uncertainty set is defined by @xmath160=   \\bigg\\ {   \\sum_{i\\in{m_o}}\\alpha_i\\x_i\\,:\\ ,    h_o^*\\big(\\sum_{i\\in{m_o}}\\alpha_i\\x_i\\big)\\leq{c }   \\bigg\\},\\quad{}o\\in\\{p , n\\}.    \\label{eqn : uncertainty - levelset - rep } \\ ] ] the function @xmath163 may depend on the population distribution .",
    "we suppose that @xmath163 does not depend on the sample points , @xmath165 . in example",
    "[ example : uncertainty_truncated_quadratic ] , the second expression of the uncertainty set involves the convex function @xmath166 .",
    "this function does not satisfy the assumption , since @xmath163 depends on training samples via @xmath140 and @xmath141 .",
    "instead , the function @xmath167 with the population mean @xmath168 and the population covariance matrix @xmath169 meets the condition .",
    "when @xmath168 and @xmath169 are replaced with the estimated parameters based on a prior knowledge or a set of samples independent of the training samples , @xmath170 , the function @xmath163 with the estimated parameters still satisfies the condition we imposed above .      in popular learning algorithms using uncertainty sets such as hard - margin svm , @xmath0-svm and maximum margin mpm ,",
    "the decision function is estimated by solving the minimum distance problem with @xmath171 $ ] and @xmath172 $ ] , where @xmath173 and @xmath174 are prespecified constants . in order to investigate the statistical properties of the learning algorithm using uncertainty sets , we consider the primal expression of a variant of the minimum distance problem .    in section [ sec : loss_and_uncertainty ] , we derived the problem as the dual form of . here",
    ", we consider the following optimization problem to obtain the loss function corresponding to given uncertainty sets having the vertex representation , @xmath175\\cap\\conv\\{\\x_i : i\\in{m_p}\\},\\\\    \\displaystyle      \\phantom{\\st}\\      \\z_n\\in\\ucal_n[c_n]\\cap\\conv\\{\\x_i : i\\in{m_n}\\}.    \\end{array }   \\label{eqn : opt - based - on - uncertainty_set}\\end{aligned}\\ ] ] in the above problem the constraints , @xmath176 , are added , since the corresponding uncertainty set has the same constraint .",
    "we derive the primal problem corresponding to via the min - max theorem .",
    "a brief calculation yields that is equivalent to @xmath177 if there is no duality gap , the corresponding primal formulation of is given as @xmath178 where @xmath179 is defined as @xmath180 for @xmath139 . in the primal expression , @xmath181 and @xmath182",
    "are regarded as the loss function for the decision function @xmath183 on training samples . in general , however , the loss function is not represented as the empirical mean over training samples .",
    "thus , we can not apply the standard theoretical tools to investigate statistical properties such as bayes risk consistency for the learning algorithm based on or . on the other hand ,",
    "if the problem is described as the empirical loss minimization , we can study statistical properties of the algorithm by applying the statistical theory developed by @xcite . to link",
    "the uncertainty set approach with the empirical loss minimization , we consider a revision of the uncertainty set .",
    "we propose a way of revising uncertainty sets such that the primal form is represented as minimization of the empirical mean of a loss function .",
    "remember that the additivity of the function is kept unchanged in the conjugate function , i.e. , @xmath184 .",
    "revision of uncertainty set defined by vertex representation : : :    suppose that the uncertainty set is described by . for    @xmath139 , we define @xmath185-dimensional    vectors @xmath186 and    @xmath187 . for the convex function",
    "@xmath188 , we define    @xmath189 by    @xmath190 the revised uncertainty set    @xmath191,\\,o\\in\\{p , n\\}$ ] is defined as    @xmath192=             \\bigg\\ {             \\sum_{i\\in{m_o}}\\alpha_i\\x_i\\,:\\ ,             \\sum_{i\\in{m_o}}\\alpha_i=1,\\,\\alpha_i\\geq 0,\\,i\\in{m_o},\\ ,             \\frac{1}{m}\\sum_{i\\in{m_o}}\\bar{\\ell}^*(\\alpha_im)\\leq{c }             \\bigg\\}.             \\end{aligned}\\ ] ] revision of uncertainty set defined by level - set representation : : :    suppose that the uncertainty set is described by and that the mean of    the input vector @xmath5 conditioned on the positive ( resp .",
    "negative ) label is given as    @xmath193 .",
    "the null vector is    denoted as @xmath194 .",
    "we define the function    @xmath195 by    @xmath196 the revised uncertainty set    @xmath191 , o\\in\\{p , n\\}$ ] is defined as    @xmath192=\\bigg\\ {             \\sum_{i\\in{m_o}}\\alpha_i\\x_i\\,:\\ ,             \\sum_{i\\in{m_o}}\\alpha_i=1,\\,\\alpha_i\\geq 0,\\,i\\in{m_o},\\ ,             \\frac{1}{m}\\sum_{i\\in{m_o}}\\bar{\\ell}^*(\\alpha_im)\\leq{c},\\ ,             \\bigg\\}.             \\end{aligned}\\ ] ] we apply the parallel shift of training    samples so as to be @xmath197 or    @xmath198 .",
    "we explain the reason why the revised uncertainty set is defined as above . in the revision ,",
    "the uncertainty set is kept unchanged , when the function @xmath199 is described in the additive form .",
    "the precise description is presented in the following theorem .",
    "[ theorem : conservation_law ] let @xmath200 be convex functions , and @xmath201 be the function defined by for given @xmath202 and @xmath203 .",
    "suppose that @xmath204 is a closed , convex , proper function such that @xmath205 and @xmath206 for @xmath207 hold .    1 .",
    "suppose that the equality @xmath208 holds for all non - negative @xmath209 .",
    "then , the equality @xmath210 holds .",
    "2 .   suppose that the equality @xmath211 holds for all @xmath212 .",
    "then , the equality @xmath210 holds .",
    "we prove the first statement . from the definition of @xmath201 and the assumption on @xmath213 , the equality @xmath214 holds for @xmath207 .",
    "suppose @xmath215 .",
    "the assumption on @xmath202 and @xmath203 leads to @xmath216 .",
    "hence , we have @xmath217 .",
    "the second statement of the theorem is straightforward .",
    "theorem [ theorem : conservation_law ] implies that the transformation of @xmath199 to @xmath218 is a projection onto the set of functions with the additive form .",
    "in addition , the second statement of theorem [ theorem : conservation_law ] denotes that the projection is uniquely determined when we impose the condition that the values on the diagonal @xmath219 are unchanged .",
    "next , we explain the validity of the formula .",
    "we want to find a function @xmath220 such that @xmath221 is close to @xmath222 in some sense .",
    "we substitute @xmath223 into @xmath224 . in the large sample limit , @xmath225 is approximated by @xmath226 .",
    "suppose that @xmath227 is represented as @xmath228 .",
    "then , we obtain .    for the revised uncertainty sets @xmath229 $ ] and @xmath230 $ ] , the corresponding primal problem of @xmath231,\\ \\z_n\\in\\bar{\\ucal}_n[c_n ]   \\label{eqn : revised - uncertainty - problem}\\end{aligned}\\ ] ] is given as @xmath232 the revision of the uncertainty sets leads to the empirical mean of the revised loss function @xmath233 .",
    "when we study statistical properties of the estimator given by the optimal solution of , we can apply the standard theoretical tools , since the objective in the primal expression is described by the empirical mean of the revised loss functions .",
    "we show some examples to illustrate how the revision of the uncertainty set works .",
    "[ example : quad - uncertainty - set - to - loss ] let @xmath234 be the convex function @xmath235 , where @xmath236 is a positive definite matrix .",
    "the revised function defined by is given as @xmath237 for @xmath212 .",
    "then , we have @xmath238 when both @xmath239 and @xmath240 are the identity matrix , the equality @xmath241 holds .",
    "let @xmath242 be @xmath243 .",
    "then , the revised uncertainty set is given as @xmath244    & =    \\bigg\\ {    \\sum_{i\\in{m_o}}\\alpha_i\\x_i\\ , :    \\sum_{i\\in{m_o}}\\alpha_i=1,\\,\\alpha_i\\geq0\\,(i\\in{m_o } ) ,     \\sum_{i\\in{m_o}}\\alpha_i^2 \\leq   \\frac{cm}{k }    \\bigg\\}.    \\end{aligned}\\ ] ] for @xmath139 , let @xmath140 and @xmath141 be the empirical mean and the empirical covariance matrix , @xmath245 if @xmath141 is invertible , we have @xmath246    =    \\bigg\\ {    \\z\\in\\conv\\{\\x_i : i\\in{m_o}\\}\\,:\\ ,    ( \\z-\\bar{\\x}_o)^t\\widehat{\\sigma}_o^{-1}(\\z-\\bar{\\x}_o)\\leq    \\frac{c{}mm_o}{k }    \\bigg\\}.    \\end{aligned}\\ ] ] in the learning algorithm based on the revised uncertainty set , the estimator is obtained by solving @xmath247,\\,\\z_n\\in\\bar{\\ucal}_n[c_n ] \\\\    & \\longleftrightarrow    \\min_{c_p , c_n,\\z_p,\\z_n } c_p+c_n+\\frac{m^2\\lambda}{4k}\\|\\z_p-\\z_n\\|\\ \\",
    "\\st\\     \\z_p\\in\\bar{\\ucal}_p\\bigg[\\frac{4c_p k}{m^2}\\bigg],\\ ,    \\z_n\\in\\bar{\\ucal}_n\\bigg[\\frac{4c_n k}{m^2}\\bigg ] .",
    "\\end{aligned}\\ ] ] the corresponding primal expression is given as @xmath248    [ example : quad - levelset - uncertainty - set - to - loss ] we define @xmath249 for @xmath139 by @xmath250 where @xmath168 is the mean vector of the input vector @xmath5 conditioned on each label and @xmath236 is a positive definite matrix . in practice ,",
    "the mean vector is estimated by using a prior knowledge which is independent of the training samples @xmath251 .",
    "suppose that @xmath252 .",
    "then , for @xmath212 , the revision of leads to @xmath253 where @xmath254 and @xmath255 are constant numbers .",
    "thus , we have @xmath246    & =    \\bigg\\ {    \\sum_{i\\in{m_o}}\\alpha_i\\x_i \\,:\\ ,    \\sum_{i\\in{m_o}}\\alpha_i=1,\\,\\alpha_i\\geq0\\ , ( i\\in{m_o}),\\ ,    \\sum_{i\\in{m_o}}\\alpha_i^2\\leq    \\frac{c - b_1}{mb_2 }    \\bigg\\}\\\\    & =    \\bigg\\ {    \\z\\in\\conv\\{\\x_i : i\\in{m_o}\\ } \\,:\\ ,    ( \\z-\\bar{\\x}_o)^t\\widehat{\\sigma}_o^{-1}(\\z-\\bar{\\x}_o)\\leq    m_o\\cdot\\frac{c - b_1}{mb_2 }    \\bigg\\ } ,    \\end{aligned}\\ ] ] where @xmath140 and @xmath141 are the estimators of the mean vector and the covariance matrix based on training samples @xmath170 .",
    "the corresponding loss function is obtained in the same way as example [ example : quad - uncertainty - set - to - loss ] . figure [ fig : revised - ellipsoidal_uncertainty - set ]",
    "illustrates an example of the revision of the uncertainty set . in the left panel",
    ", the uncertainty set does not match the distribution of the training samples .",
    "the revised uncertainty set in the right panel seems to well approximate the dispersal of the training samples .    [ cols=\"^,^ \" , ]",
    "in this paper , we studied the relation between the loss function approach and the uncertainty set approach in binary classification problems .",
    "we showed that these two approaches are connected to each other by the conjugate property based on the legendre transformation .",
    "given a loss function , there exists a corresponding parametrized uncertainty set . in general , however , uncertainty set does not correspond to the empirical loss function .",
    "we presented a way of revising the uncertainty set such that there exists an empirical loss function .",
    "then , we proposed a modified maximum - margin algorithm based on the parametrized uncertainty set .",
    "we proved the statistical consistency of the learning algorithm .",
    "numerical experiments showed that the revision of the uncertainty set often improves the prediction accuracy of the classifier .    in our proof of the statistical consistency ,",
    "the hinge loss used in @xmath0-svm is excluded .",
    "@xcite proved the statistical consistency of @xmath0-svm with a nice choice of the regularization parameter .",
    "we are currently investigating the relaxation of the assumptions of our theoretical result so as to include the hinge loss function and other popular loss functions such as the logistic loss .",
    "as for the statistical modeling , the relation between the loss function approach and the uncertainty set approach can be a useful tool . in optimization and control theory ,",
    "the modeling based on the uncertainty set is frequently applied to the real - world data ; see the modeling in robust optimization and related works @xcite .",
    "we believe that the learning algorithm with the revision of the uncertainty set can bridge a gap between statistical modeling based on some intuition and nice statistical properties of the estimated classifiers .",
    "tk was partially supported by grant - in - aid for young scientists ( 20700251 ) . at",
    "was partially supported by grant - in - aid for young scientists ( 23710174 ) .",
    "ts was partially supported by mext kakenhi 22700289 and the aihara project , the first program from jsps , initiated by cstp .",
    "first , we prove the existence of an optimal solution . according to the standard argument on the kernel estimator",
    ", we can restrict the function part @xmath32 to be the form of @xmath256 then , the problem is reduced to the finite - dimensional problem , @xmath257 let @xmath258 be the objective function of .",
    "let us define @xmath259 be the linear subspace in @xmath260 spanned by the column vectors of the gram matrix @xmath261 .",
    "we can impose the constraint @xmath262 , since the orthogonal complement of @xmath259 does not affect the objective and the constraint in .",
    "we see that assumption [ assump : universal_kernel ] and the reproducing property yield the inequality @xmath263 . due to this inequality and the assumptions on the function @xmath94",
    ", the objective function @xmath258 is bounded below by @xmath264 hence , for any real number @xmath57 , the inclusion relation @xmath265 holds .",
    "note that the vector @xmath266 satisfying @xmath267 and @xmath268 is restricted to a compact subset in @xmath260 .",
    "we shall prove that the subset is compact , if they are not empty .",
    "we see that the two sets above are closed subsets , since both @xmath269 and @xmath270 are continuous . by the variable change from @xmath271 to @xmath272 ,",
    "@xmath273 is transformed to the convex function @xmath274 defined by @xmath275 the subgradient of @xmath102 diverges to infinity , when @xmath276 tends to infinity .",
    "in addition , @xmath102 is a non - decreasing and non - negative function .",
    "then , we have @xmath277 the same limit holds for @xmath278 .",
    "hence , the level set of @xmath274 is closed and bounded , i.e. , compact . as a result ,",
    "the level set of @xmath273 is also compact .",
    "therefore , the subset is also compact in @xmath279 .",
    "this implies that has an optimal solution .",
    "next , we prove the duality between and .",
    "since has an optimal solution , the problem with the slack variables @xmath280 , @xmath281 also has an optimal solution and the finite optimal value .",
    "in addition , the above problem clearly satisfies the slater condition ( * ? ? ? * assumption 6.2.4 ) . indeed , at the feasible solution , @xmath282 and @xmath283 , the constraint inequalities are all inactive for positive @xmath91 .",
    "hence , proposition 6.4.3 in @xcite ensures that the min - max theorem holds , i.e. , there is no duality gap .",
    "then , in the same way as , we obtain with the uncertainty set as the dual problem of .",
    "we show proofs of lemmas in section [ subsec : convergence+to+optimal_expected_loss ] .",
    "let @xmath284 be the subset @xmath285 , then we have @xmath286 .",
    "due to the non - negativity of the loss function @xmath94 , we have @xmath287 for given @xmath288 satisfying @xmath289 , we define the function @xmath290 by @xmath291 we derive a lower bound @xmath292 . since @xmath102 is a finite - valued convex function on @xmath293 , the subdifferential @xmath294 is given as @xmath295 formulas of the subdifferential are presented in theorem 23.8 and theorem 23.9 of @xcite .",
    "we prove that there exist @xmath296 and @xmath297 such that @xmath298 holds .",
    "since the second condition in assumption [ assumption : expectedloss_consistency ] holds for the convex function @xmath94 , the union @xmath299 includes all the positive real numbers .",
    "hence , there exist @xmath300 and @xmath301 satisfying @xmath302 and @xmath303 .",
    "then , for @xmath304 , the null vector is an element of @xmath305 .",
    "since @xmath290 is convex in @xmath306 , the minimum value of @xmath290 is attained at @xmath307 .",
    "define @xmath308 as a real number satisfying @xmath309 since @xmath289 is assumed , both @xmath300 and @xmath301 are less than @xmath308 due to the monotonicity of the subdifferential .",
    "then , the inequality @xmath310 holds for all @xmath311 and all @xmath288 such that @xmath312 .",
    "hence , for any measurable function @xmath19 and @xmath313 , we have @xmath314 as a result , we have @xmath315 .",
    "corollary 5.29 of @xcite ensures that the equality @xmath316:f\\in\\hcal\\}=\\inf\\{\\ebb[\\ell(\\rho - yf(x))]:f\\in{}l_0\\ }   \\end{aligned}\\ ] ] holds for any @xmath317 .",
    "thus , we have @xmath318 for any @xmath317",
    ". then , the equality @xmath319 holds . under assumption",
    "[ assump : non - deterministic - assumption ] and assumption [ assumption : expectedloss_consistency ] , we have @xmath320 due to lemma [ lemma : risk_boundedness ] .",
    "then , for any @xmath321 , there exist @xmath322 and @xmath323 such that @xmath324 and @xmath325 hold . for all @xmath326 we have @xmath327 on the other hand",
    ", it is clear that the inequality @xmath328 holds .",
    "hence , eq . holds .      under assumption",
    "[ assump : non - deterministic - assumption ] , the label probabilities , @xmath329 and @xmath330 , are positive .",
    "we assume that the inequalities @xmath331 hold . applying chernoff bound",
    ", we see that there exists a positive constant @xmath332 depending only on the marginal probability of the label such that holds with the probability higher than @xmath333 .",
    "lemma [ lemma : existence_opt_sol ] ensures that the problem has optimal solutions @xmath334 .",
    "the first inequality in , i.e. , @xmath335 , is clearly satisfied .",
    "then , we have @xmath336 from the reproducing property of the rkhss . the definition of the estimator and the non - negativity of @xmath94 yield that @xmath337",
    "then , we have @xmath338 next , we consider the optimality condition of @xmath339 . according to the calculus of subdifferential introduced in section 23 of @xcite , the derivative of the objective function with respect to @xmath340 leads to an optimality condition , @xmath341 the monotonicity and non - negativity of the subdifferential and the bound of @xmath20 lead to @xmath342 the above expression means that there exist numbers in the subdifferential such that the inequality holds , where @xmath343 denotes the @xmath129-fold sum of the set @xmath344 . let @xmath345 be a real number satisfying @xmath346 , i.e. , all elements in @xmath347 are greater than @xmath348 .",
    "then , @xmath349 should be less than @xmath345 . in the same way , for @xmath350 satisfying @xmath351",
    ", we have @xmath352 .",
    "the existence of @xmath345 and @xmath350 is guaranteed by assumption [ assumption : expectedloss_consistency ] .",
    "hence , the inequalities @xmath353 hold , in which @xmath354 is used in the second inequality .",
    "define @xmath355 as a real number such that @xmath356 inequalities in lead to @xmath357 hence , we can choose @xmath355 satisfying @xmath358 .",
    "suppose that @xmath359 holds for @xmath360 .",
    "then , the inequalities @xmath361 hold with the probability higher than @xmath333 for @xmath360 . by choosing an appropriate positive constant @xmath362",
    ", we obtain .",
    "since @xmath363 holds for @xmath23 such that @xmath364 , we have the following inequality @xmath365 in the same way as the proof of lemma 3.4 in @xcite , hoeffding s inequality leads to the upper bound .",
    "is the direct conclusion of and .",
    "lemma [ lemma : convergence_regularized_exp_loss ] assures that , for any @xmath366 , there exists sufficiently large @xmath367 such that @xmath368 holds for all @xmath369 .",
    "thus , there exist @xmath370 and @xmath371 such that @xmath372 and @xmath373 hold for @xmath374 . due to the law of large numbers",
    ", the inequality @xmath375 holds with high probability , say @xmath376 , for @xmath377 . the boundedness property in lemma [ lemma : estimator_bound ] leads to @xmath378 for @xmath379 .",
    "in addition , by the uniform bound shown in lemma [ lemma : uniform_convergence_loss ] , the inequality @xmath380 holds with probability @xmath381 .",
    "hence , the probability such that the inequality @xmath382 holds is higher than @xmath383 for @xmath379 .",
    "let @xmath384 be @xmath385 .",
    "then , for any @xmath366 , the following inequalities hold with probability higher than @xmath386 for @xmath387 , @xmath388 the second inequality above is given as @xmath389      for a fixed @xmath340 such that @xmath390 , the loss function @xmath391 is classification - calibrated @xcite , since @xmath392 holds .",
    "hence @xmath393 in assumption [ assumption : loss_bayesrisk_consistency ] satisfies @xmath394 , @xmath395 for @xmath396 , and @xmath393 is continuous and strictly increasing in @xmath397 $ ] .",
    "in addition , for all @xmath23 and @xmath65 , the inequality @xmath398-",
    "\\inf_{f\\in\\hcal , b\\in\\rbb}\\ebb[\\ell(\\rho - y(f(x)+b ) ) ]   \\end{aligned}\\ ] ] holds .",
    "details are presented in theorem 1 and theorem 2 of @xcite .",
    "here we used the equality @xmath399:f\\in\\hcal , b\\in\\rbb\\ }    = \\inf\\{\\ebb[\\ell(\\rho - y(f(x)+b))]:f\\in{}l_0,b\\in\\rbb\\ } ,    \\end{aligned}\\ ] ] which is shown in corollary 5.29 of @xcite .",
    "hence , we have @xmath400-    \\inf_{f\\in\\hcal , b\\in\\rbb}\\ebb[\\ell(\\widehat{\\rho}-y(f(x)+b))]\\\\    & =    \\rcal(\\widehat{f}+\\widehat{b},\\widehat{\\rho } )    -    \\inf_{f\\in\\hcal , b\\in\\rbb}\\rcal(f+b,\\widehat{\\rho } ) ,    \\end{aligned}\\ ] ] since @xmath401 holds due to .",
    "we assumed that @xmath402 converges to @xmath403 in probability .",
    "then , for any @xmath321 , the inequality @xmath404 holds with high probability for sufficiently large @xmath405 .",
    "thus , @xmath406 converges to zero in probability .",
    "the inequality @xmath407 and the assumption on the function @xmath408 ensure that @xmath409 converges to @xmath38 in probability , when @xmath405 tends to infinity . as a result , for any @xmath366 , @xmath410 holds with probability higher than @xmath411 with respect to the probability distribution of @xmath412 , where @xmath413 satisfies @xmath414 for any @xmath366 .",
    "next , we study the relation between @xmath415 and @xmath416",
    ". the sample size of @xmath417 is @xmath418 . for any fixed @xmath23",
    ", we define the set of 0 - 1 valued functions , @xmath419 .",
    "the vc - dimension of @xmath420 equals to one . indeed , for two distinct points @xmath421 such that @xmath422 , the event such that @xmath423 and @xmath424 is impossible .",
    "hence , for any @xmath321 and any @xmath23 , the inequality @xmath425 holds with probability higher than @xmath426 with respect to the joint probability of training sample @xmath417 .",
    "note that @xmath427 depends only on @xmath418 , @xmath428 and the vc - dimension of @xmath420 .",
    "thus , @xmath429 is independent of the choice of @xmath23 .",
    "remember that @xmath415 depends only on the data set @xmath412 .",
    "due to the law of large numbers , the inequality @xmath430 holds with probability higher than @xmath431 with respect to the probability distribution of @xmath417 conditioned on @xmath412 .",
    "since the 0 - 1 loss is bounded , it is possible to choose @xmath432 independent of @xmath433 . from the uniform convergence property",
    ", the following inequality also holds @xmath434 with probability higher than @xmath426 with respect to the probability distribution of @xmath417 conditioned on the observation of @xmath412 .",
    "in addition , we have @xmath435 given the training samples @xmath412 satisfying , the inequalities @xmath436 hold with probability higher than @xmath437 with respect to the probability distribution of @xmath417 conditioned on the observation of @xmath412 .",
    "hence , as for the conditional probability , we have @xmath438 remember that @xmath432 and @xmath427 do not depend on @xmath412 .",
    "hence , as for the joint probability of @xmath412 and @xmath417 , we have @xmath439 the above inequality implies that @xmath440 converges to @xmath38 in probability , when @xmath405 and @xmath418 tend to infinity .",
    "for @xmath441 and @xmath442 , we can directly confirm that the lemma holds . in the following , we assume @xmath443 and @xmath390 .",
    "we consider the following optimization problem involved in @xmath393 , @xmath444 the objective function is a finite - valued convex function on @xmath293 , and diverges to infinity when @xmath276 tends to @xmath445 .",
    "hence , there exists an optimal solution .",
    "let @xmath446 be an optimal solution of .",
    "the optimality condition is given as @xmath447 we assumed that both @xmath448 and @xmath449 are positive and that @xmath450 holds .",
    "hence , both @xmath451 and @xmath452 should not be zero .",
    "indeed , if one of them is equal to zero , the other is also zero .",
    "hence , we have @xmath453 and @xmath454 .",
    "these inequalities contradict @xmath455 .",
    "then , we have @xmath456 and @xmath457 , i.e. , @xmath458 . in addition , we have @xmath459 since @xmath460 holds on @xmath461 , the second derivative of the objective in satisfies the positivity condition , @xmath462 for all @xmath276 such that @xmath463 and @xmath464 .",
    "therefore , @xmath465 is uniquely determined . for a fixed @xmath466",
    ", the optimal solution can be described as the function of @xmath340 , i.e. , @xmath467 . by the implicit function theorem , @xmath468 is continuously differentiable with respect to @xmath340",
    ". then , the derivative of @xmath393 is given as @xmath469 the convexity of @xmath470 for @xmath471 leads to @xmath472 hence , we have @xmath473 for @xmath450 and @xmath443 . as a result",
    ", we see that @xmath393 is non - decreasing as the function of @xmath340 .",
    "we use the result of @xcite . for a fixed @xmath340 ,",
    "the function @xmath474 is continuous for @xmath475 , and the convexity of @xmath94 leads to the non - negativity of @xmath474 .",
    "moreover , the convexity and the non - negativity of @xmath102 lead to @xmath476 for @xmath477 and @xmath390 , where @xmath478 and @xmath479 are positive for @xmath480 .",
    "the above inequality and the continuity of @xmath481 ensure that there exists @xmath276 satisfying @xmath482 for all @xmath483 such that @xmath484 .",
    "we define the inverse function @xmath485 by @xmath486 for @xmath484 . for a fixed @xmath390 ,",
    "the loss function @xmath391 is classification - calibrated @xcite .",
    "hence , lemma 3 in @xcite leads to the inequality @xmath487 for @xmath484 .",
    "define @xmath488 by @xmath489 from the definition of @xmath490 , @xmath491 is well - defined for all @xmath492 . since @xmath493 holds , we have @xmath494 .",
    "in addition , @xmath479 is non - decreasing as the function of @xmath340 .",
    "thus , we have @xmath495 for all @xmath390 and @xmath496 .",
    "then , we can choose @xmath497 it is straightforward to confirm that the conditions of assumption [ assumption : loss_bayesrisk_consistency ] are satisfied .                      d.  j. crisp and c.  j.  c. burges . a geometric interpretation of @xmath0-svm classifiers . in s.",
    "a. solla , t.  k. leen , and k .-",
    "mller , editors , _ advances in neural information processing systems 12 _ , pages 244250 .",
    "mit press , 2000 .",
    "j.  s. nath and c.  bhattacharyya .",
    "maximum margin classifiers with specified false positive and false negative error rates . in c.",
    "apte , b.  liu , s.  parthasarathy , and d.  skillicorn , editors , _ proceedings of the seventh siam international conference on data mining _ , pages 3546 .",
    "siam , 2007 ."
  ],
  "abstract_text": [
    "<S> in binary classification problems , mainly two approaches have been proposed ; one is loss function approach and the other is uncertainty set approach . the loss function approach is applied to major learning algorithms such as support vector machine ( svm ) and boosting methods . </S>",
    "<S> the loss function represents the penalty of the decision function on the training samples . in the learning algorithm , the empirical mean of the loss function </S>",
    "<S> is minimized to obtain the classifier . against a backdrop of the development of mathematical programming , nowadays learning algorithms based on loss functions </S>",
    "<S> are widely applied to real - world data analysis . in addition </S>",
    "<S> , statistical properties of such learning algorithms are well - understood based on a lots of theoretical works . </S>",
    "<S> on the other hand , the learning method using the so - called uncertainty set is used in hard - margin svm , mini - max probability machine ( mpm ) and maximum margin mpm . in the learning algorithm , firstly , the uncertainty set is defined for each binary label based on the training samples . </S>",
    "<S> then , the best separating hyperplane between the two uncertainty sets is employed as the decision function . </S>",
    "<S> this is regarded as an extension of the maximum - margin approach . </S>",
    "<S> the uncertainty set approach has been studied as an application of robust optimization in the field of mathematical programming . </S>",
    "<S> the statistical properties of learning algorithms with uncertainty sets have not been intensively studied . in this paper </S>",
    "<S> , we consider the relation between the above two approaches . </S>",
    "<S> we point out that the uncertainty set is described by using the level set of the conjugate of the loss function . based on such relation , we study statistical properties of learning algorithms using uncertainty sets . </S>"
  ]
}