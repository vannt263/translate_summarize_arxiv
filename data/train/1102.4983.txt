{
  "article_text": [
    "in this note , we study lower bounds on the empirical minimization algorithm . to explain the basic setup of this algorithm , let @xmath6 be a probability space and set @xmath7 to be a random variable taking values in @xmath8 , distributed according to @xmath9 .",
    "we are interested in the _ function learning _",
    "( noiseless ) problem , in which one observes @xmath10 independent random variables @xmath11 , distributed according to @xmath9 , and the values @xmath12 of an unknown target function @xmath0 .",
    "the goal is to construct a procedure that uses the data @xmath13 with a _ risk _ as close as possible to the best one in @xmath1 .",
    "that is , we want to construct a statistic @xmath14 such that for every @xmath10 , with high @xmath15-probability , @xmath16 where the risk of @xmath17 is defined by @xmath18 and @xmath19 is the loss function that measures the pointwise error between @xmath0 and @xmath17 .",
    "the residue @xmath20 somehow captures the complexity or richness of the class @xmath1 and the risk of a statistic @xmath21 is the conditional expectation @xmath22 .",
    "it is well known ( see , e.g. , @xcite ) that if the class @xmath1 is not too large , for example , if it satisfies some kind of uniform central limit theorem , @xmath0 is bounded by @xmath23 and @xmath24 is reasonable , then there are upper bounds on @xmath20 that are of the form @xmath25 , where @xmath26 is a complexity term that is independent of @xmath10 .",
    "the algorithm that is used to produce the function @xmath21 is the empirical risk minimization algorithm , in which one chooses a function in @xmath1 that minimizes the empirical risk function @xmath27 in @xmath1 .",
    "there is a well developed theory concerning ways in which the complexity term may be controlled , using various parameters associated with the geometry of the class ( cf .",
    "@xcite and references therein ) .",
    "it turns out that this type of error rate , @xmath28 , is very pessimistic in many cases .",
    "in fact , if the class is small enough , then , under some structural assumptions ( see , e.g. , @xcite ) , @xmath20 can be much smaller  of the order of @xmath29 .    in this note",
    ", we are going to focus on `` small classes '' in which empirical minimization performs poorly , despite the size of the class .",
    "recently , it has been shown ( cf .",
    "@xcite ) that under mild assumptions on @xmath24 and @xmath1 , if there is more than a single function in @xmath30 then the following holds : for every @xmath10 large enough , there will be a perturbation @xmath31 of @xmath0 ( with respect to the @xmath32-norm ) for which @xmath33 has a unique minimizer in @xmath1 , but where the empirical minimization algorithm performs poorly trying to predict @xmath31 on samples of cardinality @xmath10 . to be more exact , relative to the target @xmath31 , with @xmath15-probability at least @xmath34 , @xmath35 where @xmath36 is a constant depending only on @xmath1 .",
    "although it is reasonable to expect that the larger the set @xmath37 is , the more likely it is that the empirical minimization algorithm will perform poorly , this does not follow from the analysis in @xcite .",
    "therefore , our goal here is to provide a bound on the constant @xmath36 in that does take into account the complexity of the set of minimizers @xmath37 .",
    "just as in @xcite , our method of analysis can be applied to a wide variety of losses . however , for the sake of simplicity , we will only present here what is arguably the most important case  that in which the risk is measured relative to the squared loss , @xmath38 .    to explain our result , we need several definitions from empirical processes theory .",
    "other standard notions we require from the theory of gaussian processes can be found in @xcite .",
    "for every set @xmath39 , let @xmath40 be the canonical gaussian process indexed by @xmath1 ( i.e. , with the covariance structure @xmath41 ) and set @xmath42  the expectation of the supremum of the gaussian process indexed by @xmath1 .",
    "also , for every integer @xmath10 and @xmath5 , let @xmath43 where @xmath44 are standard , independent gaussian random variables and @xmath45 are independent , distributed according to @xmath9 .",
    "it is well known that if @xmath1 is a class consisting of uniformly bounded functions , then it is a @xmath9-donsker class if and only if for every @xmath46 , @xmath47 tends to @xmath48 as @xmath10 tends to infinity ( cf .",
    "@xcite , page 301 ) . for any @xmath49 , let @xmath50 that is , the oscillation in a ball around @xmath17 .",
    "the quantity @xmath51 is a natural upper bound for some intrinsic quantity of the problem we study here ( cf .  lemma  [ lemma : sym ] ) .",
    "let @xmath37 be as above  the set of loss functions @xmath52 that minimize the risk in @xmath1 ",
    "select @xmath53 for which @xmath54 and consider the following subset of excess loss functions : @xmath55    it turns out that the desired constant in ( [ eq:01 ] ) can be bounded from below by two parameters : the expectation of the supremum of the canonical gaussian process indexed by @xmath4 and the oscillation around @xmath56 . in particular , if @xmath4 is a rich set and one of the minimizers of @xmath57 is isolated , then for any @xmath10 large enough , the error of the empirical minimizer with respect to a wisely selected target ( denoted by @xmath58 in what follows ) which is a perturbation of @xmath0 will be at least @xmath59 .",
    "the core idea of this work is that a small , wisely chosen perturbation of a target function @xmath0 with multiple oracles ( functions achieving @xmath60 ) is badly estimated by the empirical risk minimization procedure ( for further discussion of this fact , we refer the reader to @xcite ) .",
    "although the general philosophy of the proof presented here is similar to the proof from @xcite , it is much simpler .",
    "and , in fact , it seems that the method used in the proof from @xcite can not be directly extended to obtain the sharper estimate on the constant as we do here .",
    "naturally , this result recovers the previous estimates on lower bounds for the empirical risk minimization algorithm from @xcite    next , a word about notation . throughout",
    ", all absolute constants will be denoted by @xmath61 and @xmath62 etcetera .",
    "their values may change from line to line .",
    "if @xmath63 has a unique minimizer in @xmath1 , then we denote it by @xmath56 .",
    "if the minimizer is not unique , then we will fix one function in the set of minimizers and denote it by @xmath56 . for every @xmath49 ,",
    "let @xmath64 be the excess loss function associated with the target @xmath0 .",
    "for every @xmath65 , set @xmath66 and define @xmath67 .",
    "it is standard to verify ( cf .",
    "@xcite or theorem  [ cor : otherminimizerincorona ] in what follows ) that @xmath56 is a minimizer of @xmath68 and that under mild convexity assumptions on @xmath24 that clearly hold if @xmath24 is the squared loss , it is the unique minimizer in @xmath1 of @xmath69 .",
    "if @xmath11 is an independent sample selected according to @xmath9 , we set @xmath70 and let @xmath71 .",
    "thus , @xmath72 is the expectation of the supremum of the empirical process indexed by @xmath1 . finally ,",
    "when the target function is @xmath73 , we will denote the function produced by the empirical risk minimization algorithm by @xmath74  which is one element of the set @xmath75    finally , if @xmath76 is a normed space , we denote its unit ball by @xmath77 , the inner product of @xmath78 will be denoted by @xmath79 and the corresponding norm by @xmath80 .",
    "let us now formulate our main result .",
    "[ thm : main ] let @xmath81 , which is @xmath82-pre - gaussian ( cf .",
    "@xcite ) , and assume that @xmath83 .",
    "set @xmath24 to be the squared loss and put @xmath84 .",
    "there exist some absolute constants @xmath85 and @xmath86 and an integer @xmath87 for which the following holds . for every @xmath88 , with @xmath15-probability at least @xmath85 , @xmath89 where @xmath5 is such that for every integer @xmath88 , @xmath90 and @xmath91 .",
    "thus , two parameters control the behavior of the constant in ( [ eq:01 ] ) : the complexity of the set of excess loss functions of the oracles of @xmath0 and the parameter @xmath5 .",
    "when one of the oracles @xmath56 of @xmath0 is isolated , one can take @xmath5 as an absolute constant .",
    "this leads to a lower bound of the order of @xmath92 , which is optimal in the sense that an upper bound can be obtained of the order of @xmath93 for some set @xmath94 such that @xmath95 ( see , e.g. , @xcite or @xcite ) .",
    "in other settings , the lower bound obtained in theorem [ thm : main ] may fail to match exactly with an upper bound .",
    "for instance , in settings where the oscillation function @xmath96 of all the oracles @xmath56 of @xmath0 decreases to zero very slowly and at the same convergence rate , the factor @xmath97 should break down the lower bound , whereas it seems that it should not appear in the lower bound . from a technical point of view , this comes from the fact that we did not take into account the complexity `` around '' the points in @xmath98 ( cf .",
    "theorem  [ thm : gauss - proc ] and equation  ( [ eq : final1 ] ) in what follows ) .",
    "finally , the noiseless model considered here is the worst case scenario to prove the lower bound . indeed",
    ", adding some noise to the target function would increase the lower bound .",
    "the core of the proof is to find a set that can `` compete '' with a set @xmath99 that contains @xmath56 , in the sense that the empirical excess risk function @xmath100 will be more negative on the set than it can possibly be on @xmath101 .",
    "once this task is achieved , it is obvious that the empirical risk minimization algorithm will produce a function @xmath102 which is outside @xmath101 and , thus , with a certain probability , @xmath103 > r.\\end{aligned}\\ ] ]    hence , the proof consists of two parts .",
    "first , we will show that the empirical excess risk function @xmath104 is likely to be very negative on @xmath4 and we will then find some @xmath105 on which the oscillations in @xmath101 are small .    the first result we need is the following lower estimate on the expectation of the excess loss relative to the target @xmath106 , according to the distance of @xmath17 from @xmath56 .",
    "this proposition is based on the fact that the functional @xmath107 inherits a strong convex structure from the norm and was proven in @xcite in a far more general situation .",
    "[ cor : otherminimizerincorona ] let @xmath108 and @xmath109 .",
    "there exists an absolute constant @xmath36 such that for any function @xmath110 , if @xmath111 @xmath112 and @xmath113 then @xmath114    recall that @xmath115 is the set of excess loss functions associated with the true minimizers of @xmath116 in @xmath1 .",
    "we will show that if @xmath117 is a finite set , then for @xmath10 large enough , with a non - trivial @xmath15-probability there will be some @xmath118 for which the empirical error @xmath119 is very negative ( for a well chosen @xmath120 ) .",
    "[ thm : gauss - proc ] there exist constants @xmath121 and @xmath122 , depending only on the @xmath123-diameter of @xmath124 , for which the following holds .",
    "if @xmath125 is a finite subset of @xmath4 that contains @xmath48 , then there exists an integer @xmath126 such that for every integer @xmath127 , with @xmath15-probability at least @xmath128 , @xmath129 where @xmath130 and @xmath131 is the expectation of the canonical gaussian process associated with @xmath125 .",
    "let @xmath132 and recall that each @xmath133 has mean zero .",
    "consider the random vector @xmath134 and let @xmath135 be independent copies of @xmath136 ( i.e. , @xmath137 ) . by the vector - valued central limit theorem ( see ,  e.g. , @xcite ) , @xmath138 converges weakly to the canonical gaussian process indexed by @xmath125 , which we denote by @xmath139 .",
    "fix @xmath140 and @xmath141 , to be given later , for which @xmath142 is such that @xmath143 .",
    "set @xmath144 to be such that for @xmath127 , @xmath145 which clearly exists by weak convergence .",
    "since @xmath146 it follows that , with probability at least @xmath128 , @xmath147 it remains to show that one may take @xmath148 .",
    "indeed , by the symmetry of the gaussian process , it follows that ( for this choice of @xmath149 ) @xmath150 let @xmath151 and @xmath152 .",
    "since @xmath153 , it follows that if @xmath154 , then it is clear that @xmath155 .",
    "otherwise , using the concentration property of @xmath156 around its mean ( see , e.g. , @xcite ) and since @xmath157 ( where @xmath158 is an absolute constant ) , there exists an absolute constant @xmath159 such that @xmath160 } \\bigr]\\leq(\\mathbb{e}z)/4.\\end{aligned}\\ ] ] therefore , @xmath161}+\\mathbh{1}_{[(\\mathbb{e}z)/4\\leq z \\leq\\mathbb{e } z + a\\sigma]}+\\mathbh{1}_{[z\\geq\\mathbb{e}z + a\\sigma ] } \\bigr ) \\bigr)\\\\ & \\leq&(\\mathbb{e}z)/2+(\\mathbb{e}z)(1+c_0a)\\operatorname{pr}\\bigl((\\mathbb { e}z)/4\\leq z\\bigr).\\end{aligned}\\ ] ] thus , @xmath162^{-1}$ ] and so @xmath163^{-1}:=c$ ] ( which is an absolute constant ) , implying that , with probability greater than @xmath128 , @xmath164    next , observe that for small values of @xmath165 ( as we will have in our construction ) , @xmath166 is a good approximation of @xmath167 with respect to the @xmath123-norm .",
    "indeed , @xmath67 and @xmath64 ; hence , for every @xmath49 , @xmath168 thus , if one selects @xmath169 , then , with probability greater than @xmath128 , @xmath170    fix a finite set @xmath171 for which @xmath172 and @xmath173 .",
    "clearly , such a set exists because @xmath4 is a pre - gaussian as a subset of the pre - gaussian class @xmath174 .",
    "let @xmath175 . recall that a bounded class of functions @xmath1 is @xmath9-_donsker _ if and only if for every @xmath176 , there exist @xmath46 and an integer @xmath177 such that for every @xmath178 .",
    "also , note that @xmath179 .",
    "let @xmath180 , where @xmath181 is an absolute constant , to be fixed later , and set @xmath5 and @xmath182 to be such that for @xmath183 , @xmath184 ( such @xmath5 and @xmath182 necessarily exist because @xmath1 is @xmath9-donsker ) .    the next lemma is standard and follows from a symmetrization argument combined with slepian s lemma .",
    "its proof may be found in , for example , @xcite .",
    "[ lemma : sym ] there exists an absolute constant @xmath36 for which the following holds . for any @xmath185 such that @xmath186 and any @xmath187 , @xmath188 where @xmath44 are independent , standard gaussian variables .",
    "we are now ready to control the oscillation of the empirical excess risk function in the set @xmath189 .",
    "[ thm : uclt ] let @xmath128 , @xmath190 and @xmath120 be defined as in theorem  [ thm : gauss - proc ] , and let @xmath5 and @xmath182 be as above .",
    "there exists an absolute constant @xmath122 such that for any integer @xmath191 , with @xmath15-probability at least @xmath192 , @xmath193 where @xmath194    by theorem [ cor : otherminimizerincorona ] , for any @xmath195 , if @xmath110 is such that @xmath196 , then @xmath197 where @xmath198 and @xmath199 were defined in theorem  [ cor : otherminimizerincorona ] .",
    "thus , @xmath200 where @xmath201 .",
    "hence , by lemma  [ lemma : sym ] , for @xmath183 , @xmath202 provided that @xmath203 .",
    "thus , for an appropriate choice of @xmath181 ( e.g. , @xmath204 would do ) and setting @xmath205 ( which is smaller than @xmath206 ) , it is evident that @xmath207 therefore , with @xmath15-probability at least @xmath192 , @xmath208 as claimed .",
    "we can now prove our main result .",
    "proof of theorem [ thm : main ] by theorem [ thm : gauss - proc ] applied to the set @xmath125 , there exists some integer @xmath126 such that for every @xmath127 , with @xmath15-probability at least @xmath128 , @xmath209 where @xmath128 and @xmath190 are two absolute constants .    by theorem  [ thm : uclt ] , for any integer @xmath191 , with @xmath15-probability at least @xmath192 , @xmath210    hence , combining equations ( [ eq : final1 ] ) and ( [ eq : final2 ] ) , with @xmath15-probability at least @xmath211 , the excess risk of @xmath212 is such that @xmath213 \\leq -c_2h(q^\\prime)/(\\sqrt{n})$ ] , while for every function @xmath110 with @xmath214 , the empirical excess risk satisfies @xmath215 .",
    "therefore , the empirical risk minimization algorithm has an excess risk ( conditionally on the data @xmath198 ) larger than @xmath216 , with probability greater than @xmath211 , as claimed .",
    "this research was supported in part by australian research council discovery grant dp0559465 and by israel science foundation grant 666/06 .",
    "lecu , g. ( 2007 ) .",
    "suboptimality of penalized empirical risk minimization in classification . in _",
    "20th annual conference on learning theory , colt07 _ ( g. bshouty , ed . ) .",
    "_ lnai _ * 4539 * 142156 .",
    "berlin : springer ."
  ],
  "abstract_text": [
    "<S> we present an argument based on the multidimensional and the uniform central limit theorems , proving that , under some geometrical assumptions between the target function @xmath0 and the learning class @xmath1 , the excess risk of the empirical risk minimization algorithm is lower bounded by @xmath2 where @xmath3 is a canonical gaussian process associated with @xmath4 ( a well chosen subset of @xmath1 ) and @xmath5 is a parameter governing the oscillations of the empirical excess risk function over a small ball in @xmath1 . </S>"
  ]
}