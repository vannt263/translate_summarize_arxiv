{
  "article_text": [
    "in many scientific applications , data is often naturally expressed as a matrix , and computational problems on such data are reduced to standard matrix operations including matrix multiplication , @xmath14-regression , and low rank matrix approximation .    in this paper",
    "we analyze several approximation algorithms with respect to these operations .",
    "all of our algorithms share a common underlying framework which can be described as follows : let @xmath10 be an input matrix that we may want to apply a matrix computation on it to infer some useful information about the data that it represents . the main idea is to work with a sample of @xmath10 ( a.k.a .",
    "sketch ) , call it @xmath8 , and hope that the obtained information from @xmath8 will be in some sense close to the information that would have been extracted from @xmath10 .    in this generality , the above approach ( sometimes called `` monte - carlo method for linear algebraic problems '' ) is ubiquitous , and is responsible for much of the development in fast matrix computations  @xcite .    as we sample @xmath10 to create a sketch @xmath8 , our goal is twofold : ( _ i _ ) guarantee that @xmath8 resembles @xmath10 in the relevant measure , and ( _ ii _ ) achieve such a @xmath8 using as few samples as possible .",
    "the standard tool that provides a handle on these requirements when the objects are real numbers , is the chernoff bound inequality .",
    "however , since we deal with matrices , we would like to have an analogous probabilistic tool suitable for matrices .",
    "quite recently a non - trivial generalization of chernoff bound type inequalities for matrix - valued random variables was introduced by ahlswede and winter  @xcite .",
    "such inequalities are suitable for the type of problems that we will consider here .",
    "however , this type of inequalities and their variants that have been proposed in the literature @xcite all suffer from the fact that their bounds depend on the dimensionality of the samples .",
    "we argue that in a wide range of applications , this dependency can be quite detrimental .",
    "specifically , whenever the following two conditions hold we typically provide stronger bounds compared with the existing tools : ( _ a _ ) the input matrix has low intrinsic dimensionality such as rank or stable rank , ( _ b _ ) the matrix samples themselves have low rank .",
    "the validity of condition ( _ a _ ) is very common in applications from the simple fact that viewing data using matrices typically leads to redundant representations .",
    "typical sampling methods tend to rely on extremely simple sampling matrices , i.e. , samples that are supported on only one entry  @xcite or samples that are obtained by the outer - product of the sampled rows or columns  @xcite , therefore condition ( _ b _ ) is often natural to assume . by incorporating the rank assumption of the matrix samples on the above matrix - valued inequalities we are able to develop a `` dimension - free '' matrix - valued chernoff bound . see theorem  [ thm : chernoff : matrix_valued : low_rank ] for more details .",
    "fundamental to the applications we derive , are two probabilistic tools that provide concentration bounds of certain random matrices .",
    "these tools are inherently different , where each pertains to a different sampling procedure . in the first",
    ", we multiply the input matrix by a random sign matrix , whereas in the second we sample rows according to a distribution that depends on the input matrix . in particular , the first method is oblivious ( the probability space does not depend on the input matrix ) while the second is not .",
    "the first tool is the so - called subspace johnson - lindenstrauss lemma .",
    "such a result was obtained in @xcite ( see also  ( * ? ? ?",
    "* theorem  1.3 ) ) although it appears implicitly in results extending the original johnson lindenstrauss lemma ( see  @xcite ) .",
    "the techniques for proving such a result with possible worse bound are not new and can be traced back even to milman s proof of dvoretsky theorem  @xcite .",
    "[ lem : jl_subspace ] ( subspace jl lemma @xcite ) let @xmath15 be a linear subspace of dimension @xmath16 and @xmath17 .",
    "let @xmath18 be a @xmath19 random sign matrix rescaled by @xmath20 , namely @xmath21 with equal probability .",
    "then @xmath22 where @xmath23 are constants .",
    "the importance of such a tool , is that it allows us to get bounds on the necessary dimensions of the random sign matrix in terms of the _ rank _ of the input matrices , see theorem  [ thm : matrixmult ] ( _ i.a _ ) .",
    "while the assumption that the input matrices have low rank is a fairly reasonable assumption , one should be a little cautious as the property of having low rank is not robust .",
    "indeed , if random noise is added to a matrix , even if low rank , the matrix obtained will have full rank almost surely . on the other hand",
    ", it can be shown that the added noise can not distort the frobenius and operator norm significantly ; which makes the notion of _ stable rank _ robust and so the assumption of low stable rank on the input is more applicable than the low rank assumption .",
    "given the above discussion , we resort to a different methodology , called matrix - valued chernoff bounds .",
    "these are non - trivial generalizations of the standard chernoff bounds over the reals and were first introduced in  @xcite .",
    "part of the contribution of the current work is to show that such inequalities , similarly to their real - valued ancestors , provide powerful tools to analyze randomized algorithms .",
    "there is a rapidly growing line of research exploiting the power of such inequalities including matrix approximation by sparsification  @xcite ; analysis of algorithms for matrix completion and decomposition of low rank matrices  @xcite ; and semi - definite relaxation and rounding of quadratic maximization problems  @xcite .",
    "the quality of these bounds can be measured by the number of samples needed in order to obtain small error probability .",
    "the original result of ( * ? ? ?",
    "* theorem  19 ) shows that if @xmath24 is distributed according to some distribution over @xmath25 matrices with zero mean matrix . ] , and if @xmath26 are independent copies of @xmath24 then for any @xmath2 , @xmath27 where @xmath28 holds almost surely and @xmath29 is an absolute constant .",
    "notice that the number of samples in ineq .",
    "depends logarithmically in @xmath30 . in general , unfortunately , such a dependency is inevitable : take for example a diagonal random sign matrix of dimension @xmath30 .",
    "the operator norm of the sum of @xmath12 independent samples is precisely the maximum deviation among @xmath30 independent random walks of length @xmath12 . in order to achieve a fixed bound on the maximum deviation with constant probability , it is easy to see that @xmath12 should grow logarithmically with @xmath30 in this scenario .    in their seminal paper , rudelson and vershynin",
    "provide a matrix - valued chernoff bound that avoids the dependency on the dimensions by assuming that the matrix samples are the _ outer product _",
    "@xmath31 of a randomly distributed vector @xmath32  @xcite .",
    "it turns out that this assumption is too strong in most applications , such as the ones we study in this work , and so we wish to relax it without increasing the bound significantly . in the following theorem we replace this assumption with that of having _",
    "low rank_. we should note that we are not aware of a simple way to extend theorem  @xmath33 of  @xcite to the low rank case , even constant rank .",
    "the main technical obstacle is the use of the powerful rudelson selection lemma , see  @xcite or lemma  @xmath34 of  @xcite , which applies only for rademacher sums of outer product of vectors .",
    "we bypass this obstacle by proving a more general lemma , see lemma  [ lem : e_p_vs_sum_of_squares ] .",
    "the proof of lemma  [ lem : e_p_vs_sum_of_squares ] relies on the non - commutative khintchine moment inequality  @xcite which is also the backbone in the proof of rudelson s selection lemma . with lemma  [ lem : e_p_vs_sum_of_squares ] at our disposal ,",
    "the proof techniques of  @xcite can be adapted to support our more general condition .",
    "[ thm : chernoff : matrix_valued : low_rank ] let @xmath35 and @xmath24 be a random symmetric real matrix with @xmath36 and @xmath37 almost surely .",
    "assume that each element on the support of @xmath24 has at most rank @xmath38 .",
    "set @xmath39 . if @xmath40 holds almost surely , then @xmath41 where @xmath42 are i.i.d .",
    "copies of @xmath24 .",
    "see appendix , page  .",
    "the above theorem can not be improved in terms of the number of samples required without changing its form , since in the special case where the rank of the samples is one it is exactly the statement of  theorem  @xmath33 of @xcite , see  ( * ? ? ?",
    "* remark  @xmath43 ) .",
    "| c | c | c | c | c | + _ assumption on the sample _ @xmath24 & # _ of samples _ ( @xmath12 ) & failure prob . &",
    "_ references _ & _ comments _",
    "+ @xmath44 a.s . &",
    "@xmath45 & @xmath46 & @xcite & hoeffding + @xmath44 a.s . ,",
    "@xmath47 & @xmath48 & @xmath46 & @xcite & bernstein + @xmath49 a.s .",
    ", @xmath50 , @xmath36 & @xmath51 & @xmath52 & @xcite & rank one + @xmath49 , @xmath53 a.s .",
    ", @xmath36 & @xmath54 & @xmath55 & theorem  [ thm : chernoff : matrix_valued : low_rank ] & low rank +    we highlight the usefulness of the above main tools by first proving a `` dimension - free '' approximation algorithm for matrix multiplication with respect to the spectral norm ( section  [ sec : apps : matrix_mult ] ) .",
    "utilizing this matrix multiplication bound we get an approximation algorithm for the @xmath14-regression problem which returns an approximate solution by randomly projecting the initial problem to dimensions linear on the rank of the constraint matrix ( section  [ sec : apps : l2_regression ] ) .",
    "finally , in section  [ sec : apps : low_rank ] we give improved approximation algorithms for the low rank matrix approximation problem with respect to the spectral norm , and moreover answer in the affirmative a question left open by the authors of  @xcite .",
    "the next discussion reviews several definitions and facts from linear algebra ; for more details , see  @xcite .",
    "we abbreviate the terms independently and identically distributed and almost surely with i.i.d . and a.s . , respectively .",
    "we let @xmath56 be the @xmath57-dimensional sphere .",
    "a _ random gaussian _",
    "matrix is a matrix whose entries are i.i.d .",
    "standard gaussians , and a _ random sign _",
    "matrix is a matrix whose entries are independent bernoulli random variables , that is they take values from @xmath58 with equal probability . for a matrix @xmath59 , @xmath60 , @xmath61 , denote the @xmath62th row , @xmath63th column , respectively . for a matrix with rank @xmath38 , the singular value decomposition ( svd ) of @xmath10 is the decomposition of @xmath10 as @xmath64 where @xmath65 , @xmath66 where the columns of @xmath67 and @xmath68 are orthonormal , and @xmath69 is @xmath70 diagonal matrix .",
    "we further assume @xmath71 and call these real numbers the _ singular values _ of @xmath10 .",
    "by @xmath72 we denote the best rank @xmath16 approximation to @xmath10 , where @xmath73 and @xmath74 are the matrices formed by the first @xmath16 columns of @xmath67 and @xmath68 , respectively .",
    "we denote by @xmath75 the spectral norm of @xmath10 , and by @xmath76 the frobenius norm of @xmath10 .",
    "we denote by @xmath77 the moore - penrose pseudo - inverse of @xmath10 , i.e. , @xmath78 .",
    "notice that @xmath79 .",
    "also we define by @xmath80 the _ stable rank _ of @xmath10 .",
    "notice that the inequality @xmath81 always holds .",
    "the orthogonal projector of a matrix @xmath10 onto the row - space of a matrix @xmath82 is denoted by @xmath83 .",
    "by @xmath84 we define the best rank-@xmath16 approximation of the matrix @xmath85 .",
    "all the proofs of this section have been deferred to section  [ sec : proofs ] .",
    "the seminal research of  @xcite focuses on using non - uniform row sampling to speed - up the running time of several matrix computations .",
    "the subsequent developments of  @xcite also study the performance of monte - carlo algorithms on primitive matrix algorithms including the matrix multiplication problem with respect to the frobenius norm .",
    "sarlos  @xcite extended ( and improved ) this line of research using random projections .",
    "most of the bounds for approximating matrix multiplication in the literature are mostly with respect to the frobenius norm  @xcite . in some cases",
    ", the techniques that are utilized for bounding the frobenius norm also imply _ weak _ bounds for the spectral norm , see  ( * ? ? ?",
    "* theorem  4 ) or  ( * ? ? ?",
    "* corollary  11 ) which is similar with part ( _ i.a _ ) of theorem  [ thm : matrixmult ] .    in this section",
    "we develop approximation algorithms for matrix multiplication with respect to the spectral norm . the algorithms that will be presented in this section",
    "are based on the tools mentioned in section  [ sec : tools ] . before stating our main dimension - free matrix multiplication theorem ( theorem  [ thm : matrixmult ] )",
    ", we discuss the best possible bound that can be achieved using the current known matrix - valued inequalities ( to the best of our knowledge ) .",
    "consider a direct application of ineq .  , where a similar analysis with that in proof of theorem  [ thm : matrixmult ] ( _ ii _ ) would allow us to achieve a bound of @xmath86 on the number of samples ( details omitted ) .",
    "however , as the next theorem indicates ( proof omitted ) we can get linear dependency on the stable rank of the input matrices gaining from the `` variance information '' of the samples ; more precisely , this can be achieved by applying the matrix - valued bernstein inequality  see e.g.  @xcite , ( * ? ? ?",
    "* theorem  3.2 ) or  ( * ? ?",
    "* theorem  2.10 ) .",
    "let @xmath87 and let @xmath0 , @xmath88 both having stable rank at most @xmath89 .",
    "the following hold :    a.   let @xmath18 be a @xmath90 random sign matrix rescaled by @xmath20",
    ". denote by @xmath91 and @xmath92 .",
    "if @xmath93 then @xmath94 b.   let @xmath95 , where @xmath96 be a probability distribution over @xmath97 $ ] .",
    "if we form a @xmath98 matrix @xmath8 and a @xmath99 matrix @xmath9 by taking @xmath100 i.i.d .",
    "( row indices ) samples from @xmath101 , then @xmath102    notice that the above bounds depend linearly on the stable rank of the matrices and logarithmically on their dimensions .",
    "as we will see in the next theorem we can remove the dependency on the dimensions , and replace it with the stable rank . recall that in most cases matrices _ do _ have low stable rank , which is much smaller that their dimensionality .",
    "[ thm : matrixmult ] let @xmath87 and let @xmath0 , @xmath88 both having rank and stable rank at most @xmath38 and @xmath89 , respectively .",
    "the following hold :    a.   let @xmath18 be a @xmath90 random sign matrix rescaled by @xmath20",
    ". denote by @xmath91 and @xmath92 .",
    "a.   if @xmath103 then @xmath104 @xmath105 b.   if @xmath106 then @xmath107 b.   let @xmath95 , where @xmath96 be a probability distribution over @xmath97 $ ] .",
    "if we form a @xmath98 matrix @xmath8 and a @xmath99 matrix @xmath9 by taking @xmath108 i.i.d .",
    "( row indices ) samples from @xmath101 , then @xmath102    in part ( _ ii _ ) , we can actually achieve the _ stronger _ bound of @xmath109 @xmath110 ( see proof ) .",
    "however , for ease of presentation and comparison we give the above displayed bound .",
    "part ( _ i.b _ ) follows from ( _ i.a _ ) via a simple truncation argument .",
    "this was pointed out to us by mark rudelson  ( personal communication ) . to understand the significance and the differences between the different components of this theorem",
    ", we first note that the probabilistic event of part ( _ i.a _ ) is superior to the probabilistic event of ( _ i.b _ ) and ( _ ii _ ) . indeed , when @xmath111 the former implies that @xmath112 for every @xmath32 , which is stronger than @xmath113 .",
    "we will _ heavily _",
    "exploit this fact in section  [ sec : app : spectral ] to prove theorem  [ thm : lowrank ] ( _ i.a _ ) and ( _ ii _ ) .",
    "also notice that part ( _ i.b _ ) is essential computationally inferior to ( _ ii _ ) as it gives the same bound while it is more expensive computationally to multiply the matrices by random sign matrices than just sampling their rows .",
    "however , the advantage of part ( _ i _ ) is that the sampling process is _ oblivious _ ,",
    "i.e. , does not depend on the input matrices .",
    "in this section we present an approximation algorithm for the least - squares regression problem ; given an @xmath114 , @xmath115 , real matrix @xmath10 of rank @xmath38 and a real vector @xmath116 we want to compute @xmath117 that minimizes @xmath118 over all @xmath119 . in their seminal paper",
    "@xcite , drineas et al .",
    "show that if we non - uniformly sample @xmath120 rows from @xmath10 and @xmath121 , then with high probability the optimum solution of the @xmath19 sampled problem will be within @xmath122 close to the original problem .",
    "the main drawback of their approach is that finding or even approximating the sampling probabilities is computationally intractable .",
    "sarlos  @xcite improved the above to @xmath123 and gave the first @xmath124 relative error approximation algorithm for this problem .    in the next theorem",
    "we eliminate the extra @xmath125 factor from sarlos bounds , and more importantly , replace the dimension ( number of variables ) @xmath126 with the rank @xmath38 of the constraints matrix @xmath10 .",
    "we should point out that independently , the same bound as our theorem  [ thm : ell2_regression ] was recently obtained by clarkson and woodruff  @xcite ( see also  @xcite ) .",
    "the proof of clarkson and woodruff uses heavy machinery and a completely different approach . in a nutshell",
    "they manage to improve the matrix multiplication bound with respect to the frobenius norm .",
    "they achieve this by bounding higher moments of the frobenius norm of the approximation viewed as a random variable instead of bounding the _ local _ differences for each coordinate of the product .",
    "to do so , they rely on intricate moment calculations spanning over four pages , see  @xcite for more .",
    "on the other hand , the proof of the present @xmath14-regression bound uses only basic matrix analysis , elementary deviation bounds and @xmath13-net arguments .",
    "more precisely , we argue that theorem  [ thm : matrixmult ] ( _ i.a _ ) immediately implies that by randomly - projecting to dimensions linear in the intrinsic dimensionality of the constraints , i.e. , the rank of @xmath10 , is sufficient as the following theorem indicates .",
    "[ thm : ell2_regression ] let @xmath0 be a real matrix of rank @xmath38 and @xmath116 .",
    "let @xmath127 be the @xmath14-regression problem , where the minimum is achieved with @xmath128 .",
    "let @xmath129 , @xmath18 be a @xmath90 random sign matrix rescaled by @xmath20 and @xmath130 .    *",
    "if @xmath131 , then with high probability , @xmath132 * if @xmath133 , then with high probability , @xmath134    the above result can be easily generalized to the case where @xmath121 is an @xmath135 matrix @xmath11 of rank at most @xmath38 ( see proof ) .",
    "this is known as the generalized @xmath14-regression problem in the literature , i.e. , @xmath136 where @xmath11 is an @xmath135 rank @xmath38 matrix .",
    "a large body of work on low rank matrix approximations  @xcite has been recently developed with main objective to develop more efficient algorithms for this task .",
    "most of these results study approximation algorithms with respect to the frobenius norm , except for @xcite that handle the spectral norm .    in this section",
    "we present two @xmath122-relative - error approximation algorithms for this problem with respect to the spectral norm , i.e. , given an @xmath114 , @xmath137 , real matrix @xmath10 of rank @xmath38 , we wish to compute @xmath138 , which minimizes @xmath139 over the set of @xmath114 matrices of rank @xmath16 , @xmath140 .",
    "the first additive bound for this problem was obtained in  @xcite . to the best of our knowledge the best relative bound was recently achieved in  ( * ? ? ?",
    "* theorem  1 ) .",
    "the latter result is not directly comparable with ours , since it uses a more restricted projection methodology and so their bound is weaker compared to our results .",
    "the first algorithm randomly projects the rows of the input matrix onto @xmath12 dimension . here , we set @xmath12 to be either @xmath141 in which case we get an @xmath122 error guarantee , or to be @xmath142 in which case we show a @xmath143 error approximation . in both cases",
    "the algorithm succeeds with high probability .",
    "the second approximation algorithm samples non - uniformly @xmath144 rows from @xmath10 in order to satisfy the @xmath122 guarantee with high probability .",
    "the following lemma ( lemma  [ lem : rayleigh_implies_lowrank ] ) is essential for proving both relative error bounds of theorem  [ thm : lowrank ] .",
    "it gives a sufficient condition that any matrix @xmath8 should satisfy in order to get a @xmath122 spectral low rank matrix approximation of @xmath10 for _ every _",
    "@xmath16 , @xmath145 .",
    "[ lem : rayleigh_implies_lowrank ] let @xmath10 be an @xmath146 matrix and @xmath2 . if there exists a @xmath98 matrix @xmath8 such that for every @xmath147 , @xmath148 , then @xmath149 for _ every _ @xmath150 .",
    "the theorem below shows that it s possible to satisfy the conditions of lemma  [ lem : rayleigh_implies_lowrank ] by randomly projecting @xmath10 onto @xmath141 or by non - uniform sampling i.i.d .",
    "@xmath151 rows of @xmath10 as described in parts ( _ i.a _ ) and ( _ ii _ ) , respectively .",
    "[ thm : lowrank ] let @xmath129 and let @xmath152 be a real @xmath146 matrix of rank @xmath38 with @xmath153 .",
    "a.   a.   let @xmath18 be a @xmath90 random sign matrix rescaled by @xmath20 and set @xmath91 .",
    "if @xmath133 , then with high probability @xmath154 for _ every _ @xmath155 .",
    "b.   let @xmath18 be a @xmath90 random gaussian matrix rescaled by @xmath20 and set @xmath91 .",
    "if @xmath156 , then with high probability @xmath157 b.   let @xmath158 be a probability distribution over @xmath97 $ ] .",
    "let @xmath8 be a @xmath98 matrix that is formed ( row - by - row ) by taking @xmath12 i.i.d .",
    "samples from @xmath101 and rescaled appropriately .",
    "if @xmath159 , then with high probability @xmath160 for _ every _ @xmath155 .    we should highlight that in part ( _ ii _ ) the probability distribution @xmath101 is in general hard to compute .",
    "indeed , computing @xmath161 requires computing the svd of @xmath10 . in general , these values are known as statistical leverage scores  @xcite . in the special case where @xmath10 is an edge - vertex matrix of an undirected weighted graph then @xmath101",
    ", the probability distribution over edges ( rows ) , corresponds to the effective - resistance of the @xmath62-th edge  @xcite .",
    "theorem  [ thm : lowrank ] gives an @xmath122 approximation algorithm for the special case of low rank matrices .",
    "however , as discussed in section  [ sec : tools ] such an assumption is too restrictive for most applications . in the following theorem",
    ", we make a step further and relax the rank condition with a condition that depends on the stable rank of the residual matrix @xmath162 .",
    "more formally , for an integer @xmath163 , we say that a matrix @xmath10 has a _",
    "@xmath16-low stable rank tail _",
    "iff @xmath164 .",
    "notice that the above definition is useful since it contains the set of matrices whose spectrum follows a power - law distribution and those with exponentially decaying spectrum .",
    "therefore the following theorem combined with the remark below ( partially ) answers in the affirmative the question posed by  @xcite : is there a relative error approximation algorithm with respect to the spectral norm when the spectrum of the input matrix decays in a power law ?",
    "[ thm : lowrank : low_stable_tail ] let @xmath129 and let @xmath10 be a real @xmath146 matrix with a _",
    "@xmath16-low stable rank tail_. let @xmath18 be a @xmath90 random sign matrix rescaled by @xmath20 and set @xmath91 .",
    "if @xmath165 , then with high probability @xmath166    the @xmath167 bound can be improved to a relative @xmath122 error bound if we return as the approximate solution a slightly higher rank matrix , i.e. , by returning the matrix @xmath168 , which has rank at most @xmath169 ( see ( * ? ? ?",
    "* theorem  @xmath170 ) ) .",
    "[ [ part - a ] ] part ( _ a _ ) : + + + + + + + + + + +    in this section we show the first , to the best of our knowledge , non - trivial spectral bound for matrix multiplication .",
    "although the proof is an immediate corollary of the subspace johnson - lindenstrauss lemma ( lemma  [ lem : jl_subspace ] ) , this result is powerful enough to give , for example , tight bounds for the @xmath14 regression problem .",
    "we prove the following more general theorem from which theorem  [ thm : matrixmult ] ( _ i.a _ ) follows by plugging in @xmath133 .",
    "[ thm : matrixmult : restated ] let @xmath0 and @xmath88 .",
    "assume that the ranks of @xmath10 and @xmath11 are at most @xmath38 .",
    "let @xmath18 be a @xmath90 random sign matrix rescaled by @xmath20 .",
    "denote by @xmath171 and @xmath172 .",
    "the following inequality holds @xmath173 @xmath174 where @xmath23 are constants .",
    "( of theorem  [ thm : matrixmult : restated ] ) let @xmath175 , @xmath176 be the singular value decomposition of @xmath10 and @xmath11 respectively .",
    "notice that @xmath177 , where @xmath178 and @xmath179 is the rank of @xmath10 and @xmath11 , respectively .",
    "let @xmath180 two arbitrary unit vectors .",
    "let @xmath181 and @xmath182 . recall that @xmath183 @xmath184 we will bound the last term for any arbitrary vector",
    ". denote with @xmath185 the subspace the subspace generated by the columns of @xmath10 , and @xmath186 the subspace generated by the rows of @xmath10 . ]",
    "@xmath187 of @xmath188 .",
    "notice that the size of @xmath189 .",
    "applying lemma  [ lem : jl_subspace ] to @xmath185 , we get that with probability at least @xmath190 that @xmath191 therefore we get that for any unit vectors @xmath192 : @xmath193 where the first equality follows from the parallelogram law , the first inequality follows from equation  , and the last inequality since @xmath194 are unit vectors . by similar considerations we get that @xmath195 . by linearity of @xmath18",
    ", we get that @xmath196 notice that @xmath197 , hence @xmath198 .    [",
    "[ part - b ] ] part ( _ b _ ) : + + + + + + + + + + +    we start with a technical lemma that bounds the spectral norm of any matrix @xmath10 when it s multiplied by a random sign matrix rescaled by @xmath20 .",
    "[ lem : rudelson ] let @xmath10 be an @xmath114 real matrix , and let @xmath18 be a @xmath90 random sign matrix rescaled by @xmath20 . if @xmath199 , then @xmath200    without loss of generality assume that @xmath201 .",
    "then @xmath202 . let @xmath203 be a @xmath90 gaussian matrix .",
    "then by the gordon - chevt inequality in ( * ? ? ?",
    "* proposition  @xmath204 ,  p.  @xmath205 ) . ]",
    "@xmath206 the gaussian distribution is symmetric , so @xmath207 and @xmath208 , where @xmath207 is a gaussian random variable have the same distribution . by jensen s inequality and the fact that @xmath209 , we get that @xmath210 .",
    "define the function @xmath211 by @xmath212 .",
    "the calculation above shows that @xmath213 .",
    "since @xmath214 is convex and @xmath215-lipschitz as a function of the entries of @xmath216 , talagrand s measure concentration inequality for convex functions yields @xmath217 setting @xmath218 in the above inequality implies the lemma .    now using the above lemma together with theorem  [ thm : matrixmult ] ( _ i.a _ ) and a simple truncation argument we can prove part ( _ i.b _ ) .",
    "( of theorem  [ thm : matrixmult ] ( _ i.b _ ) ) without loss of generality assume that @xmath219 . set @xmath220 .",
    "set @xmath221 , @xmath222 . since @xmath223 , @xmath224 by triangle inequality",
    ", it follows that @xmath225 choose a constant in theorem  [ thm : matrixmult ] ( _ i.a _ ) so that the failure probability of the right hand side of   does not exceed @xmath226 , where @xmath227 .",
    "the same argument shows that @xmath228 and @xmath229 .",
    "this combined with lemma  [ lem : rudelson ] applied on @xmath230 and @xmath231 yields that the sum in   is less than @xmath232 .",
    "also , since @xmath233 , the sum in   is less that @xmath234 . combining the bounds for  ,   and concludes the claim .",
    "by homogeneity normalize @xmath10 and @xmath11 such that @xmath219 . notice that @xmath235 .",
    "define @xmath236 , where @xmath237 . also define a distribution over matrices in @xmath238 with @xmath30 elements by @xmath239{ll } 0 & b^\\top_{(i ) } a_{(i ) } \\\\",
    "a^\\top_{(i ) } b_{(i ) } & 0 \\end{array } \\right]\\right ) } } = p_i.\\ ] ] first notice that @xmath240{ll } 0 & b^\\top_{(i ) } a_{(i ) } \\\\   a^\\top_{(i ) } b_{(i ) } & 0 \\end{array } \\right]}\\cdot p_i \\\\           &   =   &   \\sum_{i=1}^{n}{\\left[\\begin{array}[c]{ll } 0 & b^\\top_{(i ) } a_{(i ) } \\\\",
    "a^\\top_{(i ) } b_{(i ) } & 0 \\end{array } \\right ] } \\\\      &   =   &    \\left[\\begin{array}[c]{ll } 0 & b^\\top a \\\\   a^\\top b & 0 \\end{array } \\right].\\end{aligned}\\ ] ] this implies that @xmath241 .",
    "next notice that the spectral norm of the random matrix @xmath24 is upper bounded by @xmath242 almost surely .",
    "indeed , @xmath243}}{\\ensuremath{\\left\\|\\dfrac{a^\\top_{(i ) } b_{(i)}}{p_i}\\right\\|_2}}\\\\              &    =   &   s\\sup_{i\\in{[n ] } } { \\ensuremath{\\left\\|\\dfrac{a_{(i)}^\\top}{{\\ensuremath{\\left\\|a_{(i)}\\right\\|_2 } } } \\dfrac{b_{(i)}}{{\\ensuremath{\\left\\|b_{(i)}\\right\\|_2 } } } \\right\\|_2 } } = s \\cdot 1\\\\                      &    =   &   \\sum_{i=1}^{n}{{\\ensuremath{\\left\\|a_{(i)}\\right\\|_2}}{\\ensuremath{\\left\\|b_{(i)}\\right\\|_2 } } }",
    "\\ \\leq   \\   { \\ensuremath{\\left\\|a\\right\\|_{\\text{\\rm f}}}}{\\ensuremath{\\left\\|b\\right\\|_{\\text{\\rm f } } } } \\\\                      &    =    & \\sqrt{{\\ensuremath{\\mathrm{\\textbf{\\footnotesize sr}}\\left(a\\right)}}{\\ensuremath{\\mathrm{\\textbf{\\footnotesize sr}}\\left(b\\right ) } } }",
    "\\ ( { \\ensuremath{\\mathrm{\\textbf{\\footnotesize sr}}\\left(a\\right ) } } + { \\ensuremath{\\mathrm{\\textbf{\\footnotesize sr}}\\left(b\\right ) } } ) /2,\\end{aligned}\\ ] ] by definition of @xmath101 , properties of norms , cauchy - schwartz inequality , and arithmetic / geometric mean inequality .",
    "notice that this quantity ( since the spectral norms of both @xmath244 are one ) is at most @xmath89 by assumption . also notice that every element on the support of the random variable @xmath24 , has rank at most two .",
    "it is easy to see that , by setting @xmath245 , all the conditions in theorem  [ thm : chernoff : matrix_valued : low_rank ] are satisfied , and hence we get @xmath246 indices from @xmath97 $ ] , @xmath247 , such that with high probability @xmath248{ll } 0 & \\frac1{p_{i_j } } b^\\top_{(i_j ) } a_{(i_j ) }",
    "\\\\   \\frac1{p_{i_j}}a^\\top_{(i_j ) } b_{(i_j ) } & 0 \\end{array } \\right]}\\\\   -   \\left[\\begin{array}[c]{ll } 0 & b^\\top a \\\\",
    "a^\\top b & 0 \\end{array } \\right]\\|_2 & \\leq & { \\ensuremath{\\varepsilon}}.\\end{aligned}\\ ] ] the first sum can be rewritten as @xmath249 where @xmath250{llll } \\frac1{\\sqrt{p_{i_1}}}a_{(i_1)}^\\top & \\frac1{\\sqrt{p_{i_2}}}a_{(i_2)}^\\top & \\dots & \\frac1{\\sqrt{p_{i_t}}}a_{(i_t)}^\\top \\end{array } \\right]^\\top$ ] and @xmath251{llll } \\frac1{\\sqrt{p_{i_1}}}b_{(i_1)}^\\top & \\frac1{\\sqrt{p_{i_2}}}b_{(i_2)}^\\top & \\dots & \\frac1{\\sqrt{p_{i_t}}}b_{(i_t)}^\\top \\end{array } \\right]^\\top$ ] .",
    "this concludes the theorem .",
    "( of theorem  [ thm : ell2_regression ] ) similarly as the proof in  @xcite .",
    "let @xmath152 be the svd of @xmath10 .",
    "let @xmath252 , where @xmath253 and @xmath254 . also let @xmath255 , where @xmath256 .",
    "our goal is to bound this quantity @xmath257 it suffices to bound the norm of @xmath258 , i.e. , @xmath259 .",
    "recall that given @xmath260 the vector @xmath261 is uniquely defined . on the other hand ,",
    "vector @xmath258 depends on the random projection @xmath18 .",
    "next we show the connection between @xmath258 and @xmath261 through the `` normal equations '' .",
    "@xmath262 where @xmath263 , and used this fact to derive ineq .  .",
    "a crucial observation is that the @xmath264 is perpendicular to @xmath261 .",
    "set @xmath265 in theorem  [ thm : matrixmult ] , and set @xmath266 , and @xmath267 .",
    "notice that @xmath268 , hence with constant probability we know that @xmath269 .",
    "it follows that @xmath270 .",
    "a similar argument ( set @xmath271 and @xmath272 in theorem  [ thm : matrixmult ] ) guarantees that @xmath273 . recall that @xmath274 , since @xmath275 with high probability",
    "therefore , taking euclidean norms on both sides of equation   we get that @xmath276 summing up , it follows from equation   that , with constant probability , @xmath277 this proves ineq .  .",
    ".   follows directly from the bound on the norm of @xmath258 repeating the above proof for @xmath278 .",
    "first recall that @xmath279 is in the row span of @xmath10 , since @xmath280 and the columns of @xmath68 span the row space of @xmath10 . similarly for @xmath281 since the row span of @xmath282 is contained in the row - span of @xmath10 .",
    "indeed , @xmath283 .",
    "( of lemma  [ lem : rayleigh_implies_lowrank ] ) by the assumption and using lemma  [ lem : rayleight_to_eig ] we get that @xmath284 for all @xmath285 . let @xmath286 be the projection matrix onto the first @xmath16 right singular vectors of @xmath8 , i.e. , @xmath287 .",
    "it follows that for every @xmath288 @xmath289 using that @xmath290 implies @xmath291 , left side of the hypothesis , courant - fischer on @xmath292 ( see eqn .  ) , eqn .  , and properties of singular values , respectively .    [ [ proof - of - theoremthmlowrank - i ] ] proof of theorem  [ thm : lowrank ] ( _ i _ ) : + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    [ [ part - a-1 ] ] part ( _ a _ ) : + + + + + + + + + + +    now we are ready to prove our first corollary of our matrix multiplication result to the problem of computing an approximate low rank matrix approximation of a matrix with respect to the spectral norm ( theorem  [ thm : lowrank ] ) .",
    "set @xmath293 where @xmath18 is a @xmath294 random sign matrix .",
    "apply theorem  [ thm : matrixmult ] _ ( i.a ) _ on @xmath10 we have with high probability that @xmath295 combining lemma  [ lem : rayleigh_implies_lowrank ] with ineq .",
    "concludes the proof .",
    "[ [ part - b-1 ] ] part ( _ b _ ) : + + + + + + + + + + +    the proof is based on the following lemma which reduces the problem of low rank matrix approximation to the problem of bounding the norm of a random matrix .",
    "we restate it here for reader s convenience and completeness  ( * ? ? ?",
    "* lemma  8) , ( see also  ( * ? ? ?",
    "* theorem  @xmath170 ) or @xcite ) .",
    "[ lem : low_rank : stoc09 ] let @xmath296 , @xmath297 and @xmath18 be _ any _ @xmath90 matrix . if the matrix @xmath298 has full column rank , then the following inequality holds , @xmath299    notice that the above lemma , reduces the problem of spectral low rank matrix approximation to a problem of approximation the spectral norm of the random matrix @xmath300 .",
    "first notice that by setting @xmath156 we can guarantee that the matrix @xmath298 will have full column rank with high probability .",
    "actually , we can say something much stronger ; applying theorem  [ thm : matrixmult ] ( _ i.a _ ) with @xmath301 we can guarantee that all the singular values are within @xmath302 with high probability .",
    "now by conditioning on the above event ( @xmath298 has full column rank ) , it follows from lemma  [ lem : low_rank : stoc09 ] that @xmath303 using the sub - multiplicative property of matrix norms , and that @xmath304 .",
    "now , it suffices to bound the norm of @xmath305 . recall that @xmath306 where @xmath203 is a @xmath90 random gaussian matrix , it is well - known that the distribution of the random matrix @xmath307 ( by rotational invariance of the gaussian distribution ) has entries which are also i.i.d .",
    "gaussian random variables .",
    "now , we can use the following fact about random sub - gaussian matrices to give a bound on the spectral norm of @xmath308 .",
    "indeed , we have the following    ( * ? ? ?",
    "* proposition  2.3)[thm : subgaussian_norm ] let @xmath308 be a @xmath309 random matrix whose entries are independent mean zero gaussian random variables .",
    "assume that @xmath310 , then @xmath311 for any @xmath312 , where @xmath313 is a positive constant .",
    "apply union bound on the above theorem with @xmath314 be a sufficient large constant and on the conditions of lemma  [ lem : low_rank : stoc09 ] , we get that with high probability , @xmath315 _ and _ @xmath316 .",
    "hence , lemma  [ thm : subgaussian_norm ] combined with the above discussion implies that @xmath317 where @xmath318 is an absolute constant .",
    "rescaling @xmath13 by @xmath319 concludes theorem  [ thm : lowrank ] ( _ i.b _ ) .    [",
    "[ proof - of - theoremthmlowrank - ii ] ] proof of theorem  [ thm : lowrank ] ( _ ii _ ) + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    here we prove that we can achieve the same relative error bound as with random projections by just sampling rows of @xmath10 through a judiciously selected distribution . however , there is a price to pay and that s an extra logarithmic factor on the number of samples , as is stated in theorem  [ thm : lowrank ] , part ( _ ii _ ) .",
    "( of theorem  [ thm : lowrank ] ( _ ii _ ) ) the proof follows closely the proof of  @xcite .",
    "similar with the proof of part ( _ a _ ) .",
    "let @xmath320 be the singular value decomposition of @xmath10 .",
    "define the projector matrix @xmath321 of size @xmath322 .",
    "clearly , the rank of @xmath323 is equal to the rank of @xmath10 and @xmath323 has the same image with @xmath10 since every element in the image of @xmath10 and @xmath323 is a linear combination of columns of @xmath67 .",
    "recall that for any projection matrix , the following holds @xmath324 and hence @xmath325 .",
    "moreover , @xmath326 . let @xmath327 be a probability distribution on @xmath97 $ ] , where @xmath328 is the @xmath62-th row of @xmath67 .",
    "define a @xmath90 random matrix @xmath216 as follows : pick @xmath12 samples from @xmath101 ; if the @xmath62-th sample is equal to @xmath329})$ ] then set @xmath330 .",
    "notice that @xmath216 has exactly one non - zero entry in each row , hence it has @xmath12 non - zero entries .",
    "define @xmath331 .",
    "it is easy to verify that @xmath332 .",
    "apply theorem  [ thm : chernoff : matrix_valued : low_rank ] ( alternatively we can use ( * ? ? ?",
    "* theorem  3.1 ) , since the matrix samples are rank one ) on the matrix @xmath323 , notice that @xmath333 and @xmath334 , @xmath335 , hence the stable rank of @xmath323 is @xmath38 .",
    "therefore , if @xmath336 then with high probability @xmath337 it suffices to show that ineq .",
    "is equivalent with the condition of lemma  [ lem : rayleigh_implies_lowrank ] .",
    "indeed , @xmath338 since @xmath339 implies @xmath340 , @xmath341 , and @xmath342 . by re - arranging terms",
    "we get equation   and so the claim follows .",
    "[ [ proof - of - theoremthmlowranklow_stable_tail ] ] proof of theorem  [ thm : lowrank : low_stable_tail ] : + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    similarly with the proof of theorem  [ thm : lowrank ] ( _ i.b _ ) . by following the proof of part ( _ i.b _ ) , conditioning on the event that @xmath298 has full column rank in lemma  [ lem : low_rank : stoc09 ] , we get with high probability that @xmath343 using the fact that if @xmath298 has full column rank then @xmath344 and @xmath345 . now observe that @xmath346 .",
    "since @xmath347 , using theorem  [ thm : matrixmult ] ( _ i.b _ ) with @xmath169 , we get that @xmath348 with high probability .",
    "rescaling @xmath13 concludes the proof .",
    "many thanks go to petros drineas for many helpful discussions and pointing out the connection of theorem  [ thm : matrixmult ] with the @xmath14-regression problem .",
    "the second author would like to thank mark rudelson for his valueable comments on an earlier draft and also for sharing with us the proof of  theorem  [ thm : matrixmult ] ( _ i.b _ ) .",
    "drvw06    s.  arora , e.  hazan , and s.  kale . .",
    "in _ proceedings of the international workshop on randomization and approximation techniques ( random ) _ , pages 272279 , 2006 .",
    "d.  achlioptas and f.  mcsherry . . ,",
    "54(2):9 , 2007 .",
    "r.  ahlswede and a.  winter . .",
    ", 48(3):569579 , 2002 .",
    "r.  bhatia . , volume 169 .",
    ", first edition , 1996 .    c.  boutsidis , m.  w. mahoney , and p.  drineas . . in _ proceedings of the acm - siam symposium on discrete algorithms ( soda ) _ ,",
    "pages 968977 , 2009 .",
    "a.  buchholz . .",
    ", 319(1 - 16):116 , january 2001 .",
    "k.  l. clarkson . .",
    "in _ proceedings of the acm symposium on computational geometry ( socg ) _ , pages 3948 , 2008 .",
    "e.  cands and j.  romberg . .",
    ", 23(3):969 , 2007 .",
    "k.  l. clarkson and d.  p. woodruff . .",
    "in _ proceedings of the symposium on theory of computing ( stoc ) _ , pages 205214 , 2009 .",
    "p.  drineas and r.  kannan . . in _ proceedings of the acm - siam symposium on discrete algorithms ( soda ) _ , pages 223232 , 2003",
    "p.  drineas , r.  kannan , and m.  w. mahoney . .",
    ", 36(1):132157 , 2006 .",
    "p.  drineas , r.  kannan , and m.  w. mahoney . . ,",
    "36(1):158183 , 2006 .",
    "p.  drineas , r.  kannan , and m.  w. mahoney . .",
    ", 36(1):184206 , 2006 .",
    "p.  drineas and m.  w. mahoney . .",
    "available at  http://arxiv.org/abs/1005.3097[arxiv:1005.3097 ] , may 2010 .",
    "p.  drineas , m.  w. mahoney , and s.  muthukrishnan . .",
    "in _ proceedings of the acm - siam symposium on discrete algorithms ( soda ) _ , pages 11271136 , 2006 .",
    "p.  drineas , m.  w. mahoney , s.  muthukrishnan , and t.  sarlos . .",
    "available at http://arxiv.org/pdf/0710.1435[arvix:0710.1435 ] , may 2009 .",
    "a.  deshpande and l.  rademacher . .",
    "in _ proceedings of the symposium on foundations of computer science ( focs ) _ , 2010 .",
    "a.  deshpande , l.  rademacher , s.  vempala , and g.  wang . .",
    "in _ proceedings of the acm - siam symposium on discrete algorithms ( soda ) _ , pages 11171126 , 2006 .",
    "p.  drineas and a.  zouzias . .",
    "available at  http://arxiv.org/abs/1006.0407[arxiv:1006.0407 ] , june 2010 .",
    "a.  frieze , r.  kannan , and s.  vempala . .",
    ", 51(6):10251041 , 2004 .",
    "d.  gross , y .- k .",
    "liu , s.  t. flammia , s.  becker , and j.  eisert . .",
    "available at  http://arxiv.org/abs/0909.3304[0909.3304 ] , september 2009 .",
    "d.  gross . .",
    "available at  http://arxiv.org/abs/0910.1879[arxiv:0910.1879 ] , december 2009 .",
    "g.  h. golub and c.  f. van loan . .",
    ", third edition , october 1996 .",
    "n.  halko , p.  g. martinsson , and j.  a. tropp . .",
    "available at  http://arxiv.org/abs/0909.4061[arxiv:0909.4061 ] , sep .",
    "2009 .    f.  lust - piquard .",
    "ingalits de khintchine dans @xmath349 . , 303(7):289292 , 1986 .",
    "f.  lust - piquard and g.  pisier . . ,",
    "29(1 - 2):241260 , december 1991 .",
    "m.  ledoux and m.  talagrand . ,",
    "volume  23 of _ ergebnisse der mathematik und ihrer grenzgebiete ( 3)_. springer - verlag , 1991 . .",
    "a.  magen . . ,",
    "38(1):139153 , 2007 .",
    ", 5(4):2837 , 1971 .",
    "n.  h. nguyen , t.  t. do , and t.  d. tran . .",
    "in _ proceedings of the symposium on theory of computing ( stoc ) _ , pages 215224 , 2009 .    a.  nemirovski . . ,",
    "109(2):283317 , 2007 .",
    "b.  recht . .",
    "available at  http://arxiv.org/abs/0910.0651v2[arxiv:0910.0651 ] , october 2009 .",
    "v.  rokhlin , a.  szlam , and m.  tygert . .",
    ", 31(3):11001124 , 2009 .",
    "m.  rudelson . .",
    ", 164(1):6072 , 1999 .",
    "m.  rudelson and r.  vershynin . . ,",
    "54(4):21 , 2007 .",
    "m.  rudelson and r.  vershynin . . ,",
    "62(1 - 2):17071739 , 2009 .",
    "t.  sarlos . .",
    "in _ proceedings of the symposium on foundations of computer science ( focs ) _ , pages 143152 , 2006 .    a.  man - cho so . .",
    "in _ proceedings of the acm - siam symposium on discrete algorithms ( soda ) _ , pages 12011209 , 2009 .",
    "a.  man - cho so . . ,",
    "december 2009 .",
    "g.  w. stewart and j.  g. sun . .",
    "academic press , june 1990 .",
    "d.  a. spielman and n.  srivastava . .",
    "in _ proceedings of the symposium on theory of computing ( stoc ) _ , pages 563568 , 2008 .",
    "j.  a. tropp . .",
    "available at  http://arxiv.org/abs/1004.4389[arxiv:1004.4389 ] , april 2010 .",
    "a.  wigderson and d.  xiao . .",
    ", 4(1):5376 , 2008 .",
    "the next lemma states that if a symmetric positive semi - definite matrix @xmath8 approximates the rayleigh quotient of a symmetric positive semi - definite matrix @xmath10 , then the eigenvalues of @xmath8 also approximate the eigenvalues of @xmath10 .",
    "[ lem : rayleight_to_eig ] let @xmath35 .",
    "assume @xmath10 , @xmath8 are @xmath322 symmetric positive semi - definite matrices , such that the following inequality holds @xmath350 then , for @xmath351 the eigenvalues of @xmath10 and @xmath8 are the same up - to an error factor @xmath13 , i.e. , @xmath352    the proof is an immediate consequence of the courant - fischer s characterization of the eigenvalues .",
    "first notice that by hypothesis , @xmath10 and @xmath8 have the same null space .",
    "hence we can assume without loss of generality , that @xmath353 for all @xmath351 .",
    "let @xmath354 and @xmath355 be the eigenvalues ( in non - decreasing order ) of @xmath10 and @xmath8 , respectively .",
    "the courant - fischer min - max theorem  @xcite expresses the eigenvalues as @xmath356 where the minimum is over all @xmath62-dimensional subspaces @xmath357 .",
    "let the subspaces @xmath358 and @xmath359 where the minimum is achieved for the eigenvalues of @xmath10 and @xmath8 , respectively .",
    "then , it follows that @xmath360 and similarly , @xmath361 therefore , it follows that for @xmath362 , @xmath363      for notational convenience , let @xmath364 and define @xmath365",
    ". moreover , let @xmath366 be copies of a ( matrix - valued ) random variables @xmath367 , we will denote @xmath368 by @xmath369}}$ ] .",
    "our goal is to give sharp bounds on the moments of the non - negative random variable @xmath370 and then using the moment method to give concentration result for @xmath370 .",
    "first we give a technical lemma of independent interest that bounds the @xmath371-th moments of @xmath370 as a function of @xmath371 , @xmath38 ( the rank of the samples ) , and the @xmath372-th moment of the random variable @xmath373 .",
    "more formally , we have the following    [ lem : e_p_vs_sum_of_squares ] let @xmath374 be i.i.d .",
    "copies of @xmath24 , where @xmath24 is a symmetric matrix - valued random variable that has rank at most @xmath38 almost surely . then for every @xmath375 @xmath376}}{{\\ensuremath{\\left\\|\\sum_{j=1}^{t } {   m_j^2 } \\right\\|_2}}^{p/2}},\\ ] ] where @xmath377 is a constant that depends on @xmath371 .",
    "we need a non - commutative version of khintchine inequality due to f. lust - piquard  @xcite , see also  @xcite and  ( * ? ? ?",
    "* theorem  5 ) .",
    "we start with some preliminaries ; let @xmath378 and denote by @xmath379 the @xmath371-th schatten norm space@xmath380the banach space of linear operators ( or matrices in our setting ) in @xmath381 equipped with the norm @xmath382 where @xmath383 are the singular values of @xmath10 , see  ( * ? ? ? * chapter  iv ,  p.92 ) for a discussion on schatten norms .",
    "notice that @xmath384 , hence we have the following inequality @xmath385 for any @xmath386 . notice that when @xmath387 , then @xmath388 .",
    "therefore , in this case , the schatten norm is essentially the spectral norm .",
    "we are now ready to state the matrix - valued khintchine inequality .",
    "see e.g.  @xcite or  ( * ? ? ?",
    "* lemma  8) .",
    "[ thm : non - comm : khintchine ] assume @xmath389 .",
    "then there exists a constant @xmath377 such that for any sequence of @xmath12 symmetric matrices @xmath390 , with @xmath391 , such that the following inequalities hold @xmath392}}{\\left\\|\\sum_{i=1}^{t}{{\\ensuremath{\\varepsilon}}_i m_i } \\right\\|_{\\mathrm{c}_p^n}^p}\\right)^{1/p } \\leq b_p \\left\\|\\left(\\sum_{i=1}^{t}{m_i^2}\\right)^{1/2 } \\right\\|_{\\mathrm{c}_p^n}\\ ] ] where for every @xmath393}$ ] , @xmath394 is a bernoulli random variable .",
    "moreover , @xmath377 is at most @xmath395 .",
    "( of lemma  [ lem : e_p_vs_sum_of_squares ] ) the proof is inspired from  ( * ? ? ?",
    "* theorem  3.1 ) .",
    "let @xmath375 .",
    "first , apply a standard symmetrization argument ( see  @xcite ) , which gives that @xmath396}}{\\left\\|\\frac1{t}\\sum_{i=1}^{t}{m_i } - \\operatorname{\\mathbb{e}}{m}\\right\\|^p_2}\\right)^{\\frac1{p}}\\\\   \\leq 2 \\left(\\operatorname{\\mathbb{e}}_{m_{[t]}}\\operatorname{\\mathbb{e}}_{{\\ensuremath{\\varepsilon}}_{[t]}}{{{\\ensuremath{\\left\\|\\frac1{t}\\sum_{i=1}^{t}{{\\ensuremath{\\varepsilon}}_i m_i}\\right\\|_2}}^p}}\\right)^{\\frac{1}{p}}.\\ ] ] indeed , let @xmath397 denote independent bernoulli variables .",
    "let @xmath398 be independent copies of @xmath24 .",
    "we essential estimate the @xmath371-th root of @xmath399 , @xmath400}}{{\\ensuremath{\\left\\| \\frac1{t } \\sum_{i=1}^{t } m_i -\\operatorname{\\mathbb{e}}m \\right\\|_2}}^p } \\right)^{1/p}\\ ] ] notice that @xmath401}}}{\\left(\\frac1{t}\\sum_{i=1}^{t } \\widetilde{m_i}\\right)}$ ] .",
    "we plug this into and apply jensen s inequality , @xmath402 } } { { \\ensuremath{\\left\\|\\operatorname{\\mathbb{e}}_{\\widetilde{m}_{[t ] } } { \\frac1{t}\\sum_{i=1}^{t } m_i -\\frac1{t}\\sum_{i=1}^{t } \\widetilde{m_i } } \\right\\|_2}}^p } \\right)^{1/p}\\\\             & \\leq & \\left (   \\operatorname{\\mathbb{e}}_{m_{[t ] } } { \\operatorname{\\mathbb{e}}_{\\widetilde{m}_{[t ] } } { { \\ensuremath{\\left\\| \\frac1{t}\\sum_{i=1}^{t } m_i -\\frac1{t}\\sum_{i=1}^{t } \\widetilde{m_i } \\right\\|_2}}^p } } \\right)^{1/p}.\\end{aligned}\\ ] ] now , notice that @xmath403 is a symmetric matrix - valued random variable for every @xmath393}$ ] , i.e. , it is distributed identically with @xmath404 .",
    "thus @xmath405}}{\\operatorname{\\mathbb{e}}_{\\widetilde{m}_{[t]}}{\\operatorname{\\mathbb{e}}_{{\\ensuremath{\\varepsilon}}_{[t]}}{{\\ensuremath{\\left\\|\\frac1{t } \\sum_{i=1}^{t } { \\ensuremath{\\varepsilon}}_i(m_i - \\widetilde{m_i})\\right\\|_2}}^p } } } \\right)^{1/p}.\\ ] ] denote @xmath406 and @xmath407 .",
    "then @xmath408 , and @xmath409 .",
    "thus , we obtain that @xmath410}}{\\operatorname{\\mathbb{e}}_{{\\ensuremath{\\varepsilon}}_{[t ] } } { \\left\\| \\frac1{t}\\sum_{i=1}^{t}{\\ensuremath{\\varepsilon}}_i m_i\\right\\|^p_2 } } \\right)^{1/p}.\\ ] ] now by the khintchine s inequality the following holds for any _ fixed _ symmetric matrices @xmath42 .",
    "@xmath411 } } { \\ensuremath{\\left\\|\\frac1{t}\\sum_{j=1}^{t}{{\\ensuremath{\\varepsilon}}_j m_j } \\right\\|_2}}^p \\right)^{\\frac1{p } }                  & \\leq &   \\frac1{t}\\left(\\operatorname{\\mathbb{e}}_{{\\ensuremath{\\varepsilon}}_{[t ] } } \\left\\|\\sum_{j=1}^{t}{{\\ensuremath{\\varepsilon}}_j m_j } \\right\\|^p_{c_p } \\right)^{\\frac1{p } } \\nonumber \\\\                & \\leq &   \\frac1{t } b_p \\left\\|\\left(\\sum_{j=1}^{t}{m_j^2}\\right)^{1/2}\\right\\|_{c_p}\\nonumber\\\\                & \\leq &   \\frac{(rt)^{1/p } b_p}{t } { \\ensuremath{\\left\\|\\left(\\sum_{j=1}^{t}{m_j^2}\\right)^{\\frac1{2}}\\right\\|_2}}\\nonumber\\\\                &    =   & \\frac{(rt)^{1/p } b_p}{t } { \\ensuremath{\\left\\|\\sum_{j=1}^{t}{m_j^2}\\right\\|_2}}^{\\frac1{2 } } \\label{ineq : final},\\end{aligned}\\ ] ] taking @xmath412 outside the expectation and using the left part of ineq .  , ineq .  ,",
    "the right part of ineq .   and",
    "the fact that the matrix @xmath413 has rank at most @xmath414 .",
    "now raising ineq .   to the @xmath371-th power on both sides and then take expectation with respect to @xmath26 , it follows from ineq .",
    "that @xmath415}}{{\\ensuremath{\\left\\|\\sum_{j=1}^{t}{m_j^2}\\right\\|_2}}^{p/2}}.\\ ] ] this concludes the proof of lemma  [ lem : e_p_vs_sum_of_squares ] .",
    "now we are ready to prove theorem  [ thm : chernoff : matrix_valued : low_rank ] .",
    "first we can assume without loss of generality that @xmath416 almost surely losing only a constant factor in our bounds .",
    "indeed , by the spectral decomposition theorem any symmetric matrix can be written as @xmath417 .",
    "set @xmath418 and @xmath419 .",
    "it is clear that @xmath420 , @xmath421 and @xmath422 .",
    "triangle inequality tells us that @xmath423 and one can bound each term of the right hand side separately .",
    "hence , from now on we assume that @xmath424 a.s .. now use the fact that for every @xmath425}$ ] , @xmath426 since @xmath427 s are positive semi - definite and @xmath28 almost surely . summing up all the inequalities we get that @xmath428",
    "it follows that @xmath429}}{{\\ensuremath{\\left\\|\\sum_{j=1}^{t}{m_j^2}\\right\\|_2}}^{p/2}}\\\\          & \\leq & r t^{1-p } ( 2b_p)^p\\gamma^{p/2 } \\operatorname{\\mathbb{e}}_{m_{[t]}}{{\\ensuremath{\\left\\|\\sum_{j=1}^{t}{m_j}\\right\\|_2}}^{p/2 } } \\\\          &    =   &   \\frac{rt(2b_p\\sqrt{\\gamma})^p}{t^{p/2 } } \\operatorname{\\mathbb{e}}_{m_{[t]}}{{\\ensuremath{\\left\\|\\frac1{t}\\sum_{j=1}^{t}{m_j}\\right\\|_2}}^{p/2 } } \\\\          &    =   & \\frac{rt(2b_p\\sqrt{\\gamma})^p}{t^{p/2 } } \\operatorname{\\mathbb{e}}_{m_{[t]}}{{\\ensuremath{\\left\\|\\frac1{t}\\sum_{j=1}^{t}{m_j } -\\operatorname{\\mathbb{e}}m + \\operatorname{\\mathbb{e}}m\\right\\|_2}}^{p/2 } } \\\\          & \\leq & \\frac{rt(2b_p\\sqrt{\\gamma})^p}{t^{p/2 } } \\left(\\left(\\operatorname{\\mathbb{e}}{{\\ensuremath{\\left\\|\\frac1{t}\\sum_{j=1}^{t}{m_j } - \\operatorname{\\mathbb{e}}m\\right\\|_2}}^{\\frac{p}{2}}}\\right)^{\\frac{2}{p } } + 1 \\right)^{\\frac{p}{2}}\\\\          & \\leq & \\frac{rt(2b_p\\sqrt{\\gamma})^p}{t^{p/2 } } \\left(\\left(\\operatorname{\\mathbb{e}}{{\\ensuremath{\\left\\|\\frac1{t}\\sum_{j=1}^{t}{m_j } -\\operatorname{\\mathbb{e}}m\\right\\|_2}}^{p}}\\right)^{\\frac1{p } } + 1 \\right)^{\\frac{p}{2 } } \\\\          &    =   & \\frac{rt(2b_p\\sqrt{\\gamma})^p}{t^{p/2 } } \\left(e_p^{1/p } + 1 \\right)^{p/2 } , \\ ] ] using lemma  [ lem : e_p_vs_sum_of_squares ] , ineq",
    ".  , minkowski s inequality , jensen s inequality , definition of @xmath399 and the assumption @xmath430 .",
    "this implies the following inequality @xmath431 using that @xmath432 , @xmath433 .",
    "let @xmath434 .",
    "then it follows from the above inequality that @xmath435 .",
    "it follows that , then @xmath436 . otherwise @xmath437 . ]",
    "@xmath438 . also notice that @xmath439 now for any @xmath35 , @xmath440 by the moment method we have that @xmath441 where @xmath442 is an absolute constant .",
    "now assume that @xmath40 and then set @xmath443 , where @xmath444 is a sufficient large constant , at the infimum expression in the above inequality , it follows that @xmath445 we want to make the base of the above exponent smaller than one .",
    "it is easy to see that this is possible if we set @xmath446 where @xmath447 is sufficiently large absolute constant .",
    "hence it implies that the above probability is at most @xmath55 .",
    "this concludes the proof ."
  ],
  "abstract_text": [
    "<S> in this paper we develop algorithms for approximating matrix multiplication with respect to the spectral norm . </S>",
    "<S> let @xmath0 and @xmath1 be two matrices and @xmath2 . </S>",
    "<S> we approximate the product @xmath3 using two sketches @xmath4 and @xmath5 , where @xmath6 , such that @xmath7 with high probability . </S>",
    "<S> we analyze two different sampling procedures for constructing @xmath8 and @xmath9 ; one of them is done by i.i.d . </S>",
    "<S> non - uniform sampling rows from @xmath10 and @xmath11 and the other by taking random linear combinations of their rows . </S>",
    "<S> we prove bounds on @xmath12 that depend only on the intrinsic dimensionality of @xmath10 and @xmath11 , that is their rank and their stable rank .    for achieving bounds that depend on rank </S>",
    "<S> when taking random linear combinations we employ standard tools from high - dimensional geometry such as concentration of measure arguments combined with elaborate @xmath13-net constructions . for bounds that depend on the smaller parameter of stable rank this technology itself seems weak . </S>",
    "<S> however , we show that in combination with a simple truncation argument it is amenable to provide such bounds . to handle similar bounds for row sampling , we develop a novel matrix - valued chernoff bound inequality which we call low rank matrix - valued chernoff bound . </S>",
    "<S> thanks to this inequality , we are able to give bounds that depend only on the stable rank of the input matrices .    </S>",
    "<S> we highlight the usefulness of our approximate matrix multiplication bounds by supplying two applications . </S>",
    "<S> first we give an approximation algorithm for the @xmath14-regression problem that returns an approximate solution by randomly projecting the initial problem to dimensions linear on the rank of the constraint matrix . </S>",
    "<S> second we give improved approximation algorithms for the low rank matrix approximation problem with respect to the spectral norm . </S>"
  ]
}