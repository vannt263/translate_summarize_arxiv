{
  "article_text": [
    "stochastic parsing models capturing contextual constraints beyond the dependencies of probabilistic context - free grammars ( pcfgs ) are currently the subject of intensive research .",
    "an interesting feature common to most such models is the incorporation of contextual dependencies on individual head words into rule - based probability models .",
    "such word - based lexicalizations of probability models are used successfully in the statistical parsing models of , e.g. , , , or .",
    "however , it is still an open question which kind of lexicalization , e.g. , statistics on individual words or statistics based upon word classes , is the best choice .",
    "secondly , these approaches have in common the fact that the probability models are trained on treebanks , i.e. , corpora of manually disambiguated sentences , and not from corpora of unannotated sentences . in all of the cited approaches ,",
    "the penn wall street journal treebank @xcite is used , the availability of which obviates the standard effort required for treebank training ",
    "hand - annotating large corpora of specific domains of specific languages with specific parse types . moreover , common wisdom is that training from unannotated data via the expectation - maximization ( em ) algorithm @xcite yields poor results unless at least partial annotation is applied",
    ". experimental results confirming this wisdom have been presented , e.g. , by and for em training of hidden markov models and pcfgs .    in this paper , we present a new lexicalized stochastic model for constraint - based grammars that employs a combination of head - word frequencies and em - based clustering for grammar lexicalization .",
    "furthermore , we make crucial use of em for estimating the parameters of the stochastic grammar from unannotated data .",
    "our usage of em was initiated by the current lack of large unification - based treebanks for german .",
    "however , our experimental results also show an exception to the common wisdom of the insufficiency of em for highly accurate statistical modeling .",
    "our approach to lexicalized stochastic modeling is based on the parametric family of log - linear probability models , which is used to define a probability distribution on the parses of a lexical - functional grammar ( lfg ) for german . in previous work on log - linear models for lfg by ,",
    "pseudo - likelihood estimation from annotated corpora has been introduced and experimented with on a small scale . however , to our knowledge , to date no large lfg annotated corpora of unrestricted german text are available .",
    "fortunately , algorithms exist for statistical inference of log - linear models from unannotated data @xcite .",
    "we apply this algorithm to estimate log - linear lfg models from large corpora of newspaper text . in our largest experiment ,",
    "we used 250,000 parses which were produced by parsing 36,000 newspaper sentences with the german lfg . experimental evaluation of our models on an exact - match task ( i.e. percentage of exact match of most probable parse with correct parse ) on 550 manually examined examples with on average 5.4 analyses gave 86% precision .",
    "another evaluation on a verb frame recognition task ( i.e. percentage of agreement between subcategorization frames of main verb of most probable parse and correct parse ) gave 90% precision on 375 manually disambiguated examples with an average ambiguity of 25 .",
    "clearly , a direct comparison of these results to state - of - the - art statistical parsers can not be made because of different training and test data and other evaluation measures .",
    "however , we would like to draw the following conclusions from our experiments :    * the problem of chaotic convergence behaviour of em estimation can be solved for log - linear models . * em does help constraint - based grammars , e.g. using about 10 times more sentences and about 100 times more parses for em training than for training from an automatically constructed parsebank can improve precision by about 10% .",
    "* class - based lexicalization can yield a gain in precision of about 10% .    in the rest of this paper we introduce incomplete - data estimation for log - linear models ( sec .",
    "[ secim ] ) , and present the actual design of our models ( sec .",
    "[ secprops ] ) and report our experimental results ( sec .",
    "[ secexps ] ) .",
    "a log - linear distribution @xmath0 on the set of analyses @xmath1 of a constraint - based grammar can be defined as follows : @xmath2 where @xmath3 is a normalizing constant , @xmath4 is a vector of log - parameters , @xmath5 is a vector of property - functions @xmath6 for @xmath7 , @xmath8 is the vector dot product @xmath9 , and @xmath10 is a fixed reference distribution .    the task of probabilistic modeling with log - linear distributions is to build salient properties of the data as property - functions @xmath11 into the probability model . for a given vector @xmath12 of property - functions ,",
    "the task of statistical inference is to tune the parameters @xmath13 to best reflect the empirical distribution of the training data .",
    "standard numerical methods for statistical inference of log - linear models from fully annotated data ",
    "so - called complete data  are the iterative scaling methods of and . for data consisting of unannotated sentences  so - called incomplete data  the iterative method of the em algorithm @xcite has to be employed . however , since even complete - data estimation for log - linear models requires iterative methods , an application of em to log - linear models results in an algorithm which is expensive since it is doubly - iterative .",
    "a singly - iterative algorithm interleaving em and iterative scaling into a mathematically well - defined estimation method for log - linear models from incomplete data is the i m algorithm of . applying this algorithm to stochastic constraint - based grammars , we assume the following to be given : a training sample of unannotated sentences @xmath14 from a set @xmath15 , observed with empirical probability @xmath16 , a constraint - based grammar yielding a set @xmath17 of parses for each sentence @xmath14 , and a log - linear model @xmath18 on the parses @xmath19 for the sentences in the training corpus , with known values of property - functions @xmath12 and unknown values of @xmath13 .",
    "the aim of incomplete - data maximum likelihood estimation ( mle ) is to find a value @xmath20 that maximizes the incomplete - data log - likelihood @xmath21 , i.e. , @xmath22 closed - form parameter - updates for this problem can be computed by the algorithm of fig . [ algo ] , where @xmath23 , and @xmath24 is the conditional probability of a parse @xmath25 given the sentence @xmath14 and the current parameter value @xmath13 .",
    "the constancy requirement on @xmath26 can be enforced by adding a `` correction '' property - function @xmath27 :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ choose @xmath28 and @xmath29 for all @xmath30 . +",
    "then @xmath31 for all @xmath30 .",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    note that because of the restriction of @xmath1 to the parses obtainable by a grammar from the training corpus , we have a log - linear probability measure only on those parses and not on all possible parses of the grammar",
    ". we shall therefore speak of mere log - linear measures in our application of disambiguation .      for incomplete - data estimation ,",
    "a sequence of likelihood values is guaranteed to converge to a critical point of the likelihood function @xmath32 .",
    "this is shown for the i m algorithm in .",
    "the process of finding likelihood maxima is chaotic in that the final likelihood value is extremely sensitive to the starting values of @xmath13 , i.e. limit points can be local maxima ( or saddlepoints ) , which are not necessarily also global maxima . a way to search for order in",
    "this chaos is to search for starting values which are hopefully attracted by the global maximum of @xmath32 .",
    "this problem can best be explained in terms of the minimum divergence paradigm @xcite , which is equivalent to the maximum likelihood paradigm by the following theorem .",
    "let @xmath33 = \\sum_{x \\in    \\mathcal{x } } p(x ) f(x)$ ] be the expectation of a function @xmath34 with respect to a distribution @xmath35 :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ the probability distribution @xmath36 that minimizes the divergence @xmath37 to a reference model @xmath10 subject to the constraints @xmath38 = q[\\nu_i ] , i = 1 , \\ldots , n$ ] is the model in the parametric family of log - linear distributions @xmath39 that maximizes the likelihood @xmath40 $ ] of the training data , the expectation @xmath41 $ ] corresponds to the empirical expectation @xmath42 $ ] .",
    "if we observe incomplete data @xmath43 , the expectation @xmath41 $ ] is replaced by the conditional expectation @xmath44 $ ] given the observed data @xmath14 and the current parameter value @xmath45 . ] .",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    reasonable starting values for minimum divergence estimation is to set @xmath46 for @xmath47 .",
    "this yields a distribution which minimizes the divergence to @xmath10 , over the set of models @xmath35 to which the constraints @xmath38 = q[\\nu_i ] , i = 1 , \\ldots , n$ ] have yet to be applied .",
    "clearly , this argument applies to both complete - data and incomplete - data estimation . note that for a uniformly distributed reference model @xmath10 , the minimum divergence model is a maximum entropy model @xcite . in sec .",
    "[ secexps ] , we will demonstrate that a uniform initialization of the i m algorithm shows a significant improvement in likelihood maximization as well as in linguistic performance when compared to standard random initialization .",
    "the basic 190 properties employed in our models are similar to the properties of which incorporate general linguistic principles into a log - linear model .",
    "they refer to both the c(onstituent)-structure and the f(eature)-structure of the lfg parses .",
    "examples are properties for    * c - structure nodes , corresponding to standard production properties , * c - structure subtrees , indicating argument versus adjunct attachment , * f - structure attributes , corresponding to grammatical functions used in lfg , * atomic attribute - value pairs in f - structures , * complexity of the phrase being attached to , thus indicating both high and low attachment , * non - right - branching behavior of nonterminal nodes , * non - parallelism of coordinations .",
    "our approach to grammar lexicalization is class - based in the sense that we use class - based estimated frequencies @xmath48 of head - verbs @xmath49 and argument head - nouns @xmath50 instead of pure frequency statistics or class - based probabilities of head word dependencies .",
    "class - based estimated frequencies are introduced in as the frequency @xmath51 of a @xmath52-pair in the training corpus , weighted by the best estimate of the class - membership probability @xmath53 of an em - based clustering model on @xmath52-pairs , i.e. , @xmath54 . as is shown in in an evaluation on lexical ambiguity resolution , a gain of about 7%",
    "can be obtained by using the class - based estimated frequency @xmath48 as disambiguation criterion instead of class - based probabilities @xmath55 . in order to make the most direct use possible of this fact",
    ", we incorporated the decisions of the disambiguator directly into 45 additional properties for the grammatical relations of the subject , direct object , indirect object , infinitival object , oblique and adjunctival dative and accusative preposition , for active and passive forms of the first three verbs in each parse .",
    "let @xmath56 be the verbal head of grammatical relation @xmath57 in parse @xmath25 , and @xmath58 the nominal head of grammatical relation @xmath57 in @xmath25 .",
    "then a lexicalized property @xmath59 for grammatical relation @xmath57 is defined as @xmath60 the property - function @xmath59 thus pre - disambiguates the parses @xmath61 of a sentence @xmath14 according to @xmath48 , and stores the best parse directly instead of taking the actual estimated frequencies as its value . in sec .",
    "[ secexps ] , we will see that an incorporation of this pre - disambiguation routine into the models improves performance in disambiguation by about 10% .",
    "[ cols=\"^ \" , ]      +      in our experiments , we used an lfg grammar for german for parsing unrestricted text . since training was faster than parsing , we parsed in advance and stored the resulting packed c / f - structures",
    ". the low ambiguity rate of the german lfg grammar allowed us to restrict the training data to sentences with at most 20 parses .",
    "the resulting training corpus of unannotated , incomplete data consists of approximately 36,000 sentences of online available german newspaper text , comprising approximately 250,000 parses .    in order to compare the contribution of unambiguous and ambiguous sentences to the estimation results",
    ", we extracted a subcorpus of 4,000 sentences , for which the lfg grammar produced a unique parse , from the full training corpus .",
    "the average sentence length of 7.5 for this automatically constructed parsebank is only slightly smaller than that of 10.5 for the full set of 36,000 training sentences and 250,000 parses .",
    "thus , we conjecture that the parsebank includes a representative variety of linguistic phenomena .",
    "estimation from this automatically disambiguated parsebank enjoys the same complete - data estimation properties as training from manually disambiguated treebanks .",
    "this makes a comparison of complete - data estimation from this parsebank to incomplete - data estimation from the full set of training data interesting .      to evaluate our models , we constructed two different test corpora .",
    "we first parsed with the lfg grammar 550 sentences which are used for illustrative purposes in the foreign language learner s grammar of . in a next step ,",
    "the correct parse was indicated by a human disambiguator , according to the reading intended in .",
    "thus a precise indication of correct c / f - structure pairs was possible .",
    "however , the average ambiguity of this corpus is only 5.4 parses per sentence , for sentences with on average 7.5 words . in order to evaluate on sentences with higher ambiguity rate , we manually disambiguated further 375 sentences of lfg - parsed newspaper text .",
    "the sentences of this corpus have on average 25 parses and 11.2 words .",
    "we tested our models on two evaluation tasks .",
    "the statistical disambiguator was tested on an `` exact match '' task , where exact correspondence of the full c / f - structure pair of the hand - annotated correct parse and the most probable parse is checked .",
    "another evaluation was done on a `` frame match '' task , where exact correspondence only of the subcategorization frame of the main verb of the most probable parse and the correct parse is checked .",
    "clearly , the latter task involves a smaller effective ambiguity rate , and is thus to be interpreted as an evaluation of the combined system of highly - constrained symbolic parsing and statistical disambiguation .",
    "performance on these two evaluation tasks was assessed according to the following evaluation measures :    @xmath62 , + @xmath63 .",
    "`` correct '' and `` incorrect '' specifies a success / failure on the respective evaluation tasks ; `` do nt know ''",
    "cases are cases where the system is unable to make a decision , i.e. cases with more than one most probable parse .      for each task and each test corpus , we calculated a random baseline by averaging over several models with randomly chosen parameter values . this baseline measures the disambiguation power of the pure symbolic parser .",
    "the results of an exact - match evaluation on the helbig - buscha corpus is shown in fig .",
    "[ figexacteval ] .",
    "the random baseline was around 33% for this case .",
    "the columns list different models according to their property - vectors .",
    "`` basic '' models consist of 190 configurational properties as described in sec .",
    "[ basicprops ] .",
    "`` lexicalized '' models are extended by 45 lexical pre - disambiguation properties as described in sec .",
    "[ lexprops ] .",
    "`` selected + lexicalized '' models result from a simple property selection procedure where a cutoff on the number of parses with non - negative value of the property - functions was set .",
    "estimation of basic models from complete data gave 68% precision ( p ) , whereas training lexicalized and selected models from incomplete data gave 86.1% precision , which is an improvement of 18% .",
    "comparing lexicalized models in the estimation method shows that incomplete - data estimation gives an improvement of 12% precision over training from the parsebank .",
    "a comparison of models trained from incomplete data shows that lexicalization yields a gain of 13% in precision .",
    "note also the gain in effectiveness ( e ) due to the pre - disambigution routine included in the lexicalized properties .",
    "the gain due to property selection both in precision and effectiveness is minimal .",
    "a similar pattern of performance arises in an exact match evaluation on the newspaper corpus with an ambiguity rate of 25 .",
    "the lexicalized and selected model trained from incomplete data achieved here 60.1% precision and 57.9% effectiveness , for a random baseline of around 17% .    as shown in fig .",
    "[ figparteval ] , the improvement in performance due to both lexicalization and em training is smaller for the easier task of frame evaluation . here",
    "the random baseline is 70% for frame evaluation on the newspaper corpus with an ambiguity rate of 25 .",
    "an overall gain of roughly 10% can be achieved by going from unlexicalized parsebank models ( 80.6% precision ) to lexicalized em - trained models ( 90% precision ) .",
    "again , the contribution to this improvement is about the same for lexicalization and incomplete - data training .",
    "applying the same evaluation to the helbig - buscha corpus shows 97.6% precision and 96.7% effectiveness for the lexicalized and selected incomplete - data model , compared to around 80% for the random baseline .",
    "optimal iteration numbers were decided by repeated evaluation of the models at every fifth iteration . fig .",
    "[ figiters ] shows the precision of lexicalized and selected models on the exact match task plotted against the number of iterations of the training algorithm . for parsebank training ,",
    "the maximal precision value is obtained at 35 iterations .",
    "iterating further shows a clear overtraining effect . for incomplete - data estimation",
    "more iterations are necessary to reach a maximal precision value .",
    "a comparison of models with random or uniform starting values shows an increase in precision of 10% to 40% for the latter . in terms of maximization of likelihood",
    ", this corresponds to the fact that uniform starting values immediately push the likelihood up to nearly its final value , whereas random starting values yield an initial likelihood which has to be increased by factors of 2 to 20 to an often lower final value .",
    "the most direct points of comparison of our method are the approaches of and . in the first approach ,",
    "log - linear models on lfg grammars using about 200 configurational properties were trained on treebanks of about 400 sentences by maximum pseudo - likelihood estimation .",
    "precision was evaluated on an exact match task in a 10-way cross validation paradigm for an ambiguity rate of 10 , and achieved 59% for the first approach . achieved a gain of 1% over this result by including a class - based lexicalization .",
    "our best models clearly outperform these results , both in terms of precision relative to ambiguity and in terms of relative gain due to lexicalization .",
    "a comparison of performance is more difficult for the lexicalized pcfg of which was trained by em on 450,000 sentences of german newspaper text .",
    "there , a 70.4% precision is reported on a verb frame recognition task on 584 examples .",
    "however , the gain achieved by due to grammar lexicalizaton is only 2% , compared to about 10% in our case .",
    "a comparison is difficult also for most other state - of - the - art pcfg - based statistical parsers , since different training and test data , and most importantly , different evaluation criteria were used .",
    "a comparison of the performance gain due to grammar lexicalization shows that our results are on a par with that reported in .",
    "we have presented a new approach to stochastic modeling of constraint - based grammars .",
    "our experimental results show that em training can in fact be very helpful for accurate stochastic modeling in natural language processing .",
    "we conjecture that this result is due partly to the fact that the space of parses produced by a constraint - based grammar is only `` mildly incomplete '' , i.e. the ambiguity rate can be kept relatively low .",
    "another reason may be that em is especially useful for log - linear models , where the search space in maximization can be kept under control .",
    "furthermore , we have introduced a new class - based grammar lexicalization , which again uses em training and incorporates a pre - disambiguation routine into log - linear models . an impressive gain in performance",
    "could also be demonstrated for this method .",
    "clearly , a central task of future work is a further exploration of the relation between complete - data and incomplete - data estimation for larger , manually disambiguated treebanks .",
    "an interesting question is whether a systematic variation of training data size along the lines of the em - experiments of for text classification will show similar results , namely a systematic dependence of the relative gain due to em training from the relative sizes of unannotated and annotated data .",
    "furthermore , it is important to show that em - based methods can be applied successfully also to other statistical parsing frameworks .",
    "we thank stefanie dipper and bettina schrader for help with disambiguation of the test suites , and the anonymous acl reviewers for helpful suggestions .",
    "this research was supported by the pargram project and the project b7 of the sfb 340 of the dfg ."
  ],
  "abstract_text": [
    "<S> we present a new approach to stochastic modeling of constraint - based grammars that is based on log - linear models and uses em for estimation from unannotated data . </S>",
    "<S> the techniques are applied to an lfg grammar for german . </S>",
    "<S> evaluation on an exact match task yields 86% precision for an ambiguity rate of 5.4 , and 90% precision on a subcat frame match for an ambiguity rate of 25 . </S>",
    "<S> experimental comparison to training from a parsebank shows a 10% gain from em training . </S>",
    "<S> also , a new class - based grammar lexicalization is presented , showing a 10% gain over unlexicalized models . </S>"
  ]
}