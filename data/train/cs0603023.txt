{
  "article_text": [
    "the realization of fully autonomous robots will require algorithms that can learn from direct experience obtained from visual input .",
    "vision systems provide a rich source of information , but , the piecewise - continuous ( pwc ) structure of the perceptual space ( e.g. video images ) implied by typical mobile robot environments is not compatible with most current , on - line reinforcement learning approaches .",
    "these environments are characterized by regions of smooth continuity separated by discontinuities that represent the boundaries of physical objects or the sudden appearance or disappearance of objects in the visual field .",
    "there are two broad approaches that are used to adapt existing algorithms to real world environments : ( 1 ) discretizing the state space with fixed  @xcite or adaptive  @xcite grids , and ( 2 ) using a function approximator such as a neural - network  @xcite , radial basis functions ( rbfs )  @xcite , cmac  @xcite , or instance - based memory  @xcite .",
    "fixed discrete grids introduce artificial discontinuities , while adaptive ones scale exponentially with state space dimensionality .",
    "neural networks implement relatively smooth global functions that are not capable of approximating discontinuities , and rbfs and cmacs , like fixed grid methods , require knowledge of the appropriate local scale .",
    "instance - based methods use a _ neighborhood _ of explicitly stored experiences to generalize to new experiences .",
    "these methods are more suitable for our purposes because they implement local models that in principle can approximate pwc functions , but typically fall short because , by using a fixed neighborhood radius , they assume a uniform sampling density on the state space .",
    "a fixed radius prevents the approximator from clearly identifying discontinuities because points on both sides of the discontinuity can be averaged together , thereby blurring its location .",
    "if instead we use a fixed number @xmath0 of neighbors ( in effect using a variable radius ) the approximator has arbitrary resolution near important state space boundaries where it is most needed to accurately model the local dynamics . to use such an approach ,",
    "an appropriate metric is needed to determine which stored instances provide the most relevant information for deciding what to do in a given situation  @xcite .",
    "apart from the pwc structure of the perceptual space , a robot learning algorithm must also cope with the fact that instantaneous sensory readings alone rarely provide sufficient information for the robot to determine where it is ( localization problem ) and what action it is best to take . some form of short - term memory is needed to integrate successive inputs and identify the underlying environment states that are otherwise only _ partially observable_.    in this paper , we present an algorithm called piecewise continuous nearest sequence memory ( pc - nsm ) that extends mccallum s instance - based algorithm for discrete , partially observable state spaces , nearest sequence memory ( nsm ;  @xcite ) , to the more general pwc case . like nsm , pc - nsm stores all the data it collects from the environment , but uses a continuous metric on the history that allows it to be used in real robot environments without prior discretization of the perceptual space .    an important priority in this work is minimizing the amount of _ a priori _ knowledge about the structure of the environment that is available to the learner .",
    "typically , artificial learning is conducted in simulation , and then the resulting policy is transfered to the real robot .",
    "building an accurate model of a real environment is human - resource intensive and only really achievable when simple sensors are used ( unlike full - scale vision ) , while overly simplified models make policy transfer difficult  @xcite .",
    "for this reason , we stipulate that the robot must learn directly from the real world . furthermore , since gathering data in the real world is costly , the algorithm should be capable of efficient autonomous exploration in the robot perceptual state space without knowing the amount of exploration required in different parts of the state space ( as is normally the case in even the most advanced approaches to exploration in discrete  @xcite , and even in metric  @xcite state spaces ) .",
    "the next section introduces pc - nsm , section  [ sec : exp ] presents our experiments in robot navigation , and section  [ sec : discussion ] discusses our results and future directions for our research .",
    "in presenting our algorithm , we first briefly review the underlying learning mechanism , @xmath1-learning , then describe nearest sequence memory which extends @xmath1-learning to discrete pomdps , and forms the basis of our pc - nsm .",
    "the basic idea of @xmath1-learning , originally formulated for finite discrete state spaces , is to incrementally estimate the value of state - action pairs , @xmath1-values , based on the reward received from the environment and the agent s previous @xmath1-value estimates .",
    "the update rule for @xmath1-values is q_t+1(s_t , a_t ) = ( 1- ) q_t(s_t , a_t ) + where @xmath2 is the @xmath1-value estimate at time @xmath3 of the state @xmath4 and action @xmath5 , @xmath6 is a learning rate , and @xmath7 $ ] a discount parameter .",
    "@xmath1-learning requires that the number of states @xmath8 be finite and completely observable .",
    "unfortunately , due to sensory limitations , robots do not have direct access to complete state information , but , instead , receive only observations @xmath9 , where @xmath10 is the set of possible observations .",
    "typically , @xmath10 is much smaller than the set of states @xmath11 causing _ perceptual aliasing _ where the robot is unable to behave optimally because states requiring different actions look the same .    in order to use @xmath1-learning and similar methods under these more general conditions , some mechanism is required to estimate the underlying environmental state from the stream of incoming observations .",
    "the idea of using the history of all observations to recover the underlying states forms the core of the nsm algorithm , described next .",
    "nsm tries to overcome perceptual aliasing by maintaining a chronologically ordered list or history of interactions between the agent and environment .",
    "the basic idea is to disambiguate the aliased states by searching through the history to find those previous experience sequences that most closely match its recent situation .    at each time step",
    "@xmath3 the agent stores an experience triples @xmath12 of its current action , observation , and reward by appending it to history @xmath13 of previous experiences , called _ observation state _ for @xmath14 in mccallum s original notation to avoid confusion with the accepted definition of `` state '' as observation sequences do not correspond to process states . ] .    in order to choose an action at time @xmath15 , the agent finds , for each possible action @xmath16 , the @xmath0 observation states in the history that are most similar to the current situation .",
    "mccallum  @xcite defines similarity by the length of the common history [ eq : nsm - metric ] n(h_t , h_t^ ) =    0 & t = 0 t^ = 0 ( a_t , o_t , r_t ) ( a_t^ , o_t^ , r_t^ ) + 1+n(h_t-1,h_t^-1 ) & ( a_t , o_t , r_t ) = ( a_t^ , o_t^ , r_t^ ) .    which counts the number of contiguous experience triples in the two observation states that match exactly , starting at @xmath3 and @xmath17 and going back in time .",
    "we rewrite the original @xmath18 into a functionally equivalent , but more general form using the distance measure is not a metric . ]",
    "@xmath19^{-1}$ ] to accommodate the metric we introduce in the next section .",
    "the @xmath0 observation states @xmath20-nearest to @xmath21 for each possible action @xmath16 at time @xmath15 form a neighborhood @xmath22 that is used to compute the @xmath1-value for the corresponding action by : [ eq : qval ] q(h_t , a ) = ( 1|n_a^h_t| _ h_tn_a^h_t q(h_t ) ) , where @xmath23 is a local estimate of @xmath24 at the state - action pair that occurred at time @xmath3 .",
    "after an action has been selected according to @xmath1-values ( e.g. the action @xmath25 with the highest value ) , the @xmath26-values are updated : [ eq : update ] q(h_i ) : = ( 1-)q(h_i ) + ( r_i + _ a q(h_t , a ) ) ,  h_i n__t^h_t .",
    "nsm has been demonstrated in simulation , but has never been run on real robots . using history to resolve perceptual",
    "aliasing still requires considerable human programming effort to produce reasonable discretization for real - world sensors . in the following",
    "we avoid the issue of discretization by selecting an appropriate metric in the continuous observation space .",
    "@xmath27 @xmath28 @xmath29 @xmath30 @xmath31 @xmath32    @xmath33 @xmath34 @xmath35 perform @xmath36 perform @xmath25 @xmath37randz@xmath38 @xmath39 @xmath40    the distance measure used in nsm ( equation  [ eq : nsm - metric ] ) was designed for discrete state spaces . in the continuous perceptual space where our robot must learn , this metric is inadequate since most likely all the triples @xmath41 will be different from each other and @xmath42 will always equal 1 .",
    "therefore , to accommodate continuous states , we replace equation  [ eq : nsm - metric ] with the following discounted metric : [ eq : discounted - metric ] ( h_t , h_t^ ) = _",
    "= 0^min(t , t^ ) ^ ||o_t -- o_t^-||_2 , where @xmath43 $ ] .",
    "this metric takes an exponentially discounted average of the euclidean distance between observation sequences . note",
    "that , unlike equation  [ eq : nsm - metric ] , this metric ignores actions and rewards .",
    "the distance between action sequences is not considered because there is no elegant way to combine discrete actions with continuous observations , and because our primary concern from a robotics perspective is to provide a metric that allows the robot to localize itself based on observations .",
    "reward values are also excluded to enable the robot to continue using the metric to select actions even after the reinforcement signal is no longer available ( i.e. after some initial training period ) .",
    "algorithm  [ alg : tra ] presents pc - nsm in pseudocode .",
    "the functions randz(a , b ) and randr(c , d ) produce a uniformly distributed random number in @xmath44\\in\\mathbb{z}$ ] and @xmath45\\in\\mathbb{r}$ ] respectively , and @xmath46 $ ] determines the greediness of the policy . the algorithm differs most importantly from nsm in using the discounted metric ( line 8) , and in the way exploratory actions in the @xmath47-greedy policy are chosen ( line 12 ) .",
    "the exploratory action is the action whose neighborhood has the highest average distance from the current observation - state , i.e. the action about which there is the least information .",
    "this policy induces what has been called _",
    "balanced wandering _  @xcite .",
    "if the @xmath26-values are only updated during interaction with the real environment , learning can be very slow since updates will occur at the robot s control frequency ( i.e.  the rate at which the agent takes actions ) .",
    "one way to more fully exploit the information gathered from the environment is to perform updates on the stored history between normal updates .",
    "we refer to these updates as _ endogenous _ because they originate within the learning agent , unlike normal , _ exogenous _ updates which are triggered by `` real '' events outside the agent .    during learning ,",
    "the agent selects random times @xmath48 , and updates the @xmath26-value of @xmath49 according to equation  [ eq : update ] where the maximum @xmath1-value of the next state @xmath50 is computed using equation  [ eq : qval ] ( see lines 1821 in algorithm  [ alg : tra ] ) .",
    "this approach is similar to the dyna architecture  @xcite in that the history acts as a kind of model , but , unlike dyna , the model does not generate new experiences , rather it re - updates those already in the history in a manner similar to experience replay  @xcite .",
    "we demonstrate pc - nsm on a mobile robot task where a csem robotics smartease + robot must use video input to identify and navigate to a target object while avoiding obstacles and walls .",
    "because the camera provides only a partial view of the environment , this task requires the robot to use its history of observations to remember both where it has been , and where it last saw the target if the target moves out of view .",
    "[ sec : uair ] the experiments were conducted in the 3x4 meter walled arena shown in figure  [ fig : scenario ] .",
    "the robot is equipped with two ultrasound distance sensors ( one facing forward , one backward ) , and a vision system based on the axis 2100 network camera that is mounted on top of the robot s 28 cm diameter cylindrical chassis .",
    "learning was conducted in a series of trials where the robot , obstacle(s ) , and target ( blue teapot ) were placed at random locations in the arena . at the beginning of each trial , the robot takes a sensor reading and sends , via wireless , the camera image to a _ vision computer _ , and the sonar readings to a _ learning computer_. the vision computer extracts the @xmath51-@xmath52 coordinates of the target in the visual field by calculating the centroid of @xmath53 pixels of the target color ( see figure[fig : policy ] ) , and passes them on to the learning computer , along with a predicate @xmath54 indicating whether the target is visible . if @xmath54 is false , @xmath51=@xmath52=0 .",
    "the learning computer merges @xmath55 , and @xmath54 with the forward and backward sonar readings , @xmath56 and @xmath57 , to form the * inputs * to pc - nsm : an observation vector @xmath58 , where @xmath51 and @xmath52 are normalized to @xmath59 $ ] , and @xmath56 and @xmath57 are normalized to @xmath60 $ ] .",
    "pc - nsm then selects one of 8 * actions * : turn left or right by either @xmath61 or @xmath62 , and move forward or backward either 5 cm or 15 cm ( approximately ) .",
    "this action set was chosen to allow the algorithm to adapt to the scale of environment  @xcite .",
    "the selected action is sent to the robot , the robot executes the action , and the cycle repeats . when the robot reaches the goal , the goal is moved to a new location , and a new trial begins .",
    "the entire interval from sensory reading to action execution is 2.5 seconds , primarily due to camera and network delays . to accommodate this relatively low control frequency , the maximum velocity of the robot",
    "is limited to 10 cm / s . during the dead time between actions ,",
    "the learning computer conducts as many endogenous updates as time permits .",
    "* learned control policy . *",
    "each row shows a different situation in the environment along with its corresponding learned policy . in the top",
    "row the robot is positioned directly in front of the target object .",
    "the crosses in the camera image mark detected pixels of the target color , and the circle indicates the assumed direction towards the target . the policy for this situation",
    "is shown in terms of the visual coordinates , i.e. only the @xmath51-@xmath52 camera view coordinates of the high dimensional policy are shown .",
    "each point in the policy graph indicates , with an arrow , the direction the robot should move if the circle , shown in the image is at that point in the visual field ( left arrow means move left , right = right , up = forward , down = backwards , and no arrow = stand still .",
    "for instance , in this case , the robot should move forward because the circle lies in a part of the policy with an up arrow . in the bottom row",
    "the robot is almost touching the target . here",
    "the policy is shown in terms of the subspace spanned by the two ultrasound distance sensors found at the fore and aft of the robot .",
    "the @xmath57-axis is the distance from the robot to the nearest obstacle in front , the @xmath56-axis behind .",
    "when the robot is with its back to an obstacle , and the way forward is clear ( upper left corner of policy graph ) , it tends to go forward .",
    "when the way forward is obstructed , but there is nothing behind the robot ( lower right corner ) , the robot tends to turn or move backward . , title=\"fig:\",scaledwidth=48.0% ] * learned control policy . *",
    "each row shows a different situation in the environment along with its corresponding learned policy .",
    "in the top row the robot is positioned directly in front of the target object .",
    "the crosses in the camera image mark detected pixels of the target color , and the circle indicates the assumed direction towards the target . the policy for this situation",
    "is shown in terms of the visual coordinates , i.e. only the @xmath51-@xmath52 camera view coordinates of the high dimensional policy are shown .",
    "each point in the policy graph indicates , with an arrow , the direction the robot should move if the circle , shown in the image is at that point in the visual field ( left arrow means move left , right = right , up = forward , down = backwards , and no arrow = stand still .",
    "for instance , in this case , the robot should move forward because the circle lies in a part of the policy with an up arrow . in the bottom row",
    "the robot is almost touching the target . here",
    "the policy is shown in terms of the subspace spanned by the two ultrasound distance sensors found at the fore and aft of the robot .",
    "the @xmath57-axis is the distance from the robot to the nearest obstacle in front , the @xmath56-axis behind .",
    "when the robot is with its back to an obstacle , and the way forward is clear ( upper left corner of policy graph ) , it tends to go forward . when the way forward is obstructed , but there is nothing behind the robot ( lower right corner ) , the robot tends to turn or move backward . ,",
    "title=\"fig:\",scaledwidth=48.5% ] * learned control policy . *",
    "each row shows a different situation in the environment along with its corresponding learned policy . in the top",
    "row the robot is positioned directly in front of the target object .",
    "the crosses in the camera image mark detected pixels of the target color , and the circle indicates the assumed direction towards the target . the policy for this situation",
    "is shown in terms of the visual coordinates , i.e. only the @xmath51-@xmath52 camera view coordinates of the high dimensional policy are shown .",
    "each point in the policy graph indicates , with an arrow , the direction the robot should move if the circle , shown in the image is at that point in the visual field ( left arrow means move left , right = right , up = forward , down = backwards , and no arrow = stand still . for instance , in this case , the robot should move forward because the circle lies in a part of the policy with an up arrow . in the bottom row",
    "the robot is almost touching the target . here",
    "the policy is shown in terms of the subspace spanned by the two ultrasound distance sensors found at the fore and aft of the robot .",
    "the @xmath57-axis is the distance from the robot to the nearest obstacle in front , the @xmath56-axis behind .",
    "when the robot is with its back to an obstacle , and the way forward is clear ( upper left corner of policy graph ) , it tends to go forward . when the way forward is obstructed , but there is nothing behind the robot ( lower right corner ) , the robot tends to turn or move backward . , title=\"fig:\",scaledwidth=48.0% ] *",
    "learned control policy . *",
    "each row shows a different situation in the environment along with its corresponding learned policy . in the top",
    "row the robot is positioned directly in front of the target object .",
    "the crosses in the camera image mark detected pixels of the target color , and the circle indicates the assumed direction towards the target . the policy for this situation",
    "is shown in terms of the visual coordinates , i.e. only the @xmath51-@xmath52 camera view coordinates of the high dimensional policy are shown .",
    "each point in the policy graph indicates , with an arrow , the direction the robot should move if the circle , shown in the image is at that point in the visual field ( left arrow means move left , right = right , up = forward , down = backwards , and no arrow = stand still . for instance , in this case , the robot should move forward because the circle lies in a part of the policy with an up arrow . in the bottom row the robot is almost touching the target . here",
    "the policy is shown in terms of the subspace spanned by the two ultrasound distance sensors found at the fore and aft of the robot .",
    "the @xmath57-axis is the distance from the robot to the nearest obstacle in front , the @xmath56-axis behind .",
    "when the robot is with its back to an obstacle , and the way forward is clear ( upper left corner of policy graph ) , it tends to go forward .",
    "when the way forward is obstructed , but there is nothing behind the robot ( lower right corner ) , the robot tends to turn or move backward .",
    ", title=\"fig:\",scaledwidth=48.5% ]    pc - nsm uses an @xmath47-greedy policy ( algorithm  [ alg : tra ] , line 13 ) , with @xmath47 set to 0.3 .",
    "this means that 30% of the time the robot selects an exploratory action .",
    "the appropriate number of nearest neighbors , @xmath0 , used to select actions , depends upon the noisiness of the environment .",
    "the lower the noise , the smaller the @xmath0 that can be chosen . for the amount of noise in our sensors",
    ", we found that learning was fastest for @xmath63 .",
    "a common practice in toy reinforcement learning tasks such as discrete mazes is to use minimal reinforcement so that the agent is rewarded only when it reaches the goal .",
    "while such a formulation is useful to test algorithms in simulation , for real robots , this sparse , delayed reward forestalls learning as the agent can wander for long periods of time without reward , until finally happening upon the goal by accident .",
    "often there is specific domain knowledge that can incorporated into the reward function to provide intermediate reward that facilitates learning in robotic domains where exploration is costly  @xcite .",
    "the reward function we use is the sum of two components , one is obstacle - related , @xmath64 , and the other is target - related , @xmath65 : [ eq : reward ] r = _ r _ + _ r _ @xmath65 is largest when the robot is near to the goal and is looking directly towards it , smaller when the target is visible in the middle of the field of view , even smaller when the target is visible , but not in the center , and reaches its minimum when the target is not visible at all .",
    "@xmath64 is negative when the robot is too close to some obstacle , except when the obstacle is the target itself , visible by the robot .",
    "it is important to note that the coefficients in equation  [ eq : reward ] are specific to the robot and not the environment .",
    "they represent a one - time calibration of pc - nsm to the robot hardware being used .",
    "[ sec : er ] after taking between 1500 and 3000 actions the robot learns to avoid walls , reduce speed when approaching walls , look around for the goal , and go to the goal whenever it sees it .",
    "this is much faster compared to neural network based learners , e.g.  @xcite , where 4000 episodes were required ( resulting in more than 100000 actions ) to solve a simpler task in which the target was always within the perceptual field of the robot . neither do we need a virtual model environment and manual quantization of the state space like in  @xcite . to our knowledge ,",
    "our results are the fastest in terms of learning speed and use least quantization effort compared to all other methods to date , though we were unable to compare results directly on the hardware used by these competing approaches .    in the beginning of learning ,",
    "corners pose serious difficulty causing the robot to get stuck and receive negative reinforcement for being too close to a wall .",
    "when the robot accidentally turns towards the target , it will quickly lose track of it .",
    "as learning progresses , the robot is able to recover ( usually within one action ) when an exploratory action causes it to turn away and loose sight of the target .",
    "the discounted metric allows the robot to use its history of real - valued observation states to remember that it had just seen the target in the recent past .",
    "figure  [ fig : policy ] shows the learned policy for this task .",
    "since the robot state space is perception - based ( not @xmath51-@xmath52 coordinates on the floor as is the case in rl textbook examples ) , changing the position of the obstacles or target does not impede robot performance .",
    "figure  [ fig : reward ] shows learning in terms of immediate and average reward for a typical sequence of trials lasting a total of approximately 70 minutes .",
    "the dashed vertical lines in the two graphs indicate the beginning of a new trial .",
    "as learning progresses the robot is able to generalize from past experience and more quickly find the goal .",
    "after the first two trials , the robot starts to accumulate reward more rapidly in the third , after which the fourth trial is completed with very little deliberation .",
    "figure  [ fig : scenario ] illustrates two such successful trials .    *",
    "pc - nsm learning performance*. ( a ) the plot shows the reward the robot receives at each time - step during learning .",
    "( b ) the plot shows the reward at each time - step averaged over all previous time - steps within the same trial .",
    "the dashed lines indicate the beginning of a new trial where the target is moved to a new location . ]",
    "\\(a ) ( b )      ( a ) ( b )",
    "we have developed a instance - based algorithm for mobile robot learning and successfully implemented it on an actual vision - controlled robot .",
    "the use of a metric state space allows our algorithm to work under weaker requirements and be more data - efficient compared to previous work in continuous reinforcement learning  @xcite .",
    "using a metric instead of a discrete grid is a considerable relaxation of the programmer s task , since it obviates the need to guess the correct scale for all the regions of the state space in advance .",
    "the algorithm explores the environment and learns directly on a mobile robot without using a hand - made computer model as an intermediate step , works in piecewise continuous perceptual spaces , and copes with partial observability .",
    "the metric used in this paper worked well in our experiments , but a more powerful approach would be to allow the algorithm to select the appropriate metric for a given environment and task automatically . to choose between metrics , a criterion should be defined that determines which of a set of _ a priori _ equiprobable metrics @xmath66 fits the given history of experimentation better .",
    "a useful criterion could be , for example , a generalization of the criteria used in the mccallum s u - tree algorithm  @xcite to decide whether a state should be split .",
    "the current algorithm uses discrete actions so that there is a convenient way to group observation states .",
    "if the action space were continuous , the algorithm lacks a natural way to generalize between actions .",
    "a metric on the action space @xmath67 could be used within the observation - based neighborhood delimited by the current metric @xmath20 .",
    "the agent could then randomly sample possible actions at the query point @xmath21 and obtain q - values for each sampled action by computing the @xmath67-nearest neighbors within the @xmath20-neighborhood .",
    "future work will explore this avenue .        c.  anderson .",
    "-learning with hidden - unit restarting . in s.",
    "j. hanson , j.  d. cowan , and c.  l. giles , editors , _ advances in neural information processing systems 5 _ , pages 8188 , san mateo , ca , 1993 .",
    "morgan kaufmann .",
    "m.  iida , m.  sugisaka , and k.  shibata .",
    "application of direct - vision - based reinforcement learning to a real mobile robot with a ccd camera . in _ proc . of arob ( intl symp . on artificial life and robotics ) 8th _ , pages 8689 , 2003 .",
    "s.  kakade , m.  kearns , and j.  langford .",
    "exploration in metric state spaces . in _",
    "machine learning , proceedings of the twentieth international conference ( icml 2003 ) , august 21 - 24 , 2003 , washington , dc , usa_. aaai press , 2003 .",
    "r.  a. mccallum .",
    "instance - based state identification for reinforcement learning . in g.",
    "tesauro , d.  touretzky , and t.  leen , editors , _ advances in neural information processing systems _ , volume  7 , pages 377384 . the mit press , 1995 .    r.  a. mccallum .",
    "learning to use selective attention and short - term memory in sequential tasks . in p.",
    "maes , m.  mataric , j .- a .",
    "meyer , j.  pollack , and s.  w. wilson , editors , _ from animals to animats 4 : proceedings of the fourth international conference on simulation of adaptive behavior , cambridge , ma _ , pages 315324 .",
    "mit press , bradford books , 1996 .",
    "a.  w. moore .",
    "the parti - game algorithm for variable resolution reinforcement learning in multidimensional state - spaces . in j.  d. cowan , g.  tesauro , and j.  alspector , editors , _ advances in neural information processing systems _ , volume  6 , pages 711718 .",
    "morgan kaufmann publishers , inc . , 1994 .",
    "s.  pareigis",
    ". adaptive choice of grid and time in reinforcement learning . in",
    "_ nips 97 : proceedings of the 1997 conference on advances in neural information processing systems 10 _ , pages 10361042 , cambridge , ma , usa , 1998 .",
    "mit press .",
    "r.  schoknecht and m.  riedmiller . learning to control at multiple time scales . in o.",
    "kaynak , e.  alpaydin , e.  oja , and l.  xu , editors , _ artificial neural networks and neural information processing - icann / iconip 2003 , joint international conference icann / iconip 2003 , istanbul , turkey , june 26 - 29 , 2003 , proceedings _ ,",
    "volume 2714 of _ lecture notes in computer science_. springer , 2003 .",
    "w.  d. smart and l.  p. kaelbling .",
    "practical reinforcement learning in continuous spaces . in _ proc .",
    "17th international conf . on machine learning _ , pages 903910 .",
    "morgan kaufmann , san francisco , ca , 2000 .",
    "r.  s. sutton .",
    "first results with dyna , an integrated architecture for learning , planning and reacting . in _ proceedings of the aaai spring symposium on planning in uncertain , unpredictable , or changing environments _ , 1990 .",
    "r.  s. sutton .",
    "generalization in reinforcement learning : successful examples using sparse coarse coding . in d.",
    "s. touretzky , m.  c. mozer , and m.  e. hasselmo , editors , _ advances in neural information processing systems 8 _ , pages 10381044 .",
    "cambridge , ma : mit press , 1996 ."
  ],
  "abstract_text": [
    "<S> we address the problem of autonomously learning controllers for vision - capable mobile robots . </S>",
    "<S> we extend mccallum s ( 1995 ) nearest - sequence memory algorithm to allow for general metrics over state - action trajectories . </S>",
    "<S> we demonstrate the feasibility of our approach by successfully running our algorithm on a real mobile robot . </S>",
    "<S> the algorithm is novel and unique in that it ( a ) explores the environment and learns directly on a mobile robot without using a hand - made computer model as an intermediate step , ( b ) does not require manual discretization of the sensor input space , ( c ) works in piecewise continuous perceptual spaces , and ( d ) copes with partial observability . </S>",
    "<S> together this allows learning from much less experience compared to previous methods .    ,    ,    and    reinforcement learning , mobile robots . </S>"
  ]
}