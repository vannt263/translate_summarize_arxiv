{
  "article_text": [
    "as humans we are able to visually recognize and name a large variety of object categories .",
    "a rough estimation of @xcite suggests that we know approximately @xmath0 different visual categories , which corresponds to learning five categories per day , on average , in our childhood .",
    "moreover , we are able to learn the appearance of a new category using few visual examples  @xcite . despite the impressive success of current machine vision systems  @xcite , the performance is still far from being comparable to human generalization abilities .",
    "current machine learning methods , especially when applied to visual recognition problems , often need several hundreds or thousands of training examples to build an appropriate _ classification model _ or _",
    "classifier_. transfer learning techniques try to reduce this still existing gap between human and machine vision .",
    "the importance of efficient learning with few examples can be illustrated by analyzing current large - scale datasets for object recognition , such as _ labelme _",
    "[ fig : zipfbox ] shows the relative number of object categories in labelme that possess a specific number of labeled instances .",
    "a large percentage ( over @xmath1% ) of all categories only have one single labeled instance .",
    "therefore , even in datasets which include an enormous number of images and annotations in total , the lack of training data is a more common problem than one might expect .",
    "the plot also shows that the number of object categories with @xmath2 labeled instances follows an exponential function @xmath3 , which is additionally illustrated in the right log - log plot of  [ fig : zipf ] .",
    "this phenomenon is known as _ zipf s law _",
    "@xcite and can be found in language statistics and other scientific data .    with current state - of - the - art approaches we are able to build robust object detectors for tasks with a large set of available training images , like detecting pedestrians or cars  @xcite .",
    "however , if we want to extract a richer semantic representation of an image , such as trying to predict different visual attributes of a car ( model type , specific identity , etc . )",
    ", we are likely not able to rely on a sufficient number of images for each new category . therefore , high - level visual recognition approaches frequently suffer from weak training representations .",
    "problems related to a lack of learning examples are not restricted to visual object recognition tasks in real - world scenes , but are also prevalent in many industrial applications . collecting more training data is often expensive , time - consuming or practically impossible . in the following ,",
    "we give three examples tasks in which such a problem arises .    one important example is face identification  @xcite , where the goal is to estimate the identity of a person from a given image .",
    "for example , such a system has to be trained with images of each person being allowed to access a protected security area .",
    "obtaining hundreds of training images for each person is thus impractical , especially because the appearance should vary and include different illumination settings and clothing , which leads to a time - consuming procedure .",
    "similar observations hold for writer or speaker verification  @xcite and speech or handwritten text recognition  @xcite .",
    "another interesting application scenario is the prediction of user preferences in shop systems .",
    "the goal is to estimate the probability that a client likes a new product , given some previous product selections and ratings .",
    "if a machine learning system quickly generalizes from a few given user ratings and achieves a high performance in suggesting good products to buy , it is more probable that the client will use this shop frequently . in this application area , solving the problem of learning with few training examples is simply a question of cost .",
    "the economical importance of the problem can be seen in the _ netflix prize _",
    "@xcite , which promised one million dollars for a new algorithm which improves the rating accuracy of a dvd rental system .",
    "this competition has lead to a large amount of machine learning research related to _ collaborative filtering _ , which is a special case of _ knowledge transfer _ and is explained in more detail in section  [ sec : knowledgetransfer ] .    a prominent and widely established field of application for machine learning and computer vision is _ automatic visual inspection ( avi ) _  @xcite .",
    "to achieve a high quality of an industrial production , several work pieces have to be checked for errors or defects . due to the required speed and the high cost of manual quality control ,",
    "the need for automatic visual defect localization arises . whereas images from non - defective data can be easily obtained in large numbers , training images for all kinds of defects",
    "are often impossible to collect . a solution to solve this problem is to handle it as an _ outlier detection _ or _ one - class classification _ problem . in this case , learning data only consists of non - defective examples and is used afterward to detect examples not belonging to the underlying distribution of the data .      what are the challenges and the problems of traditional machine learning methods in scenarios with few training examples ?",
    "first of all , we have to clarify our notion of `` few '' .",
    "common to all traditional machine learning methods are their underlying assumptions , which were formulated by @xcite .",
    "the first postulate states :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ * postulate 1 : * _ `` in order to gather information about a task domain , a representative sample of patterns is available . '' ( * ? ? ?",
    "* section  1.3 , p. 9 ) _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    therefore , a scenario with few training examples can be defined as a classification task that violates this assumption by having an insufficient or non - representative sample of patterns .",
    "of course , this notion depends on the specific application and on the complexity of the task under consideration .",
    "one of the difficulties is the high variability in the data of high - level visual learning tasks .",
    "some images from an object category database are given in  [ fig : imgcatdifficulty ] .",
    "a classification system has to cope with background clutter , different viewpoints , illumination changes and in general with a large diversity of the category ( intra - class variance ) . on the one hand , this can only be performed with a large amount of flexibility in the underlying model of the category , such as using a large set of features extracted from the images and a complex classification model . on the other hand",
    ", learning those models requires a large number of ( representative ) training examples .",
    "these conditions turn learning with few examples into a severe problem especially for high - level recognition tasks .",
    "the trade - off between a highly flexible model and the number of training examples required can be explained quite intuitively for polynomial regression : consider a set of @xmath4 sample points of a one - dimensional @xmath5-order polynomial .",
    "the order of the polynomial is a measure of the complexity of the function . for the noise - free case ,",
    "we need @xmath6 examples to get an exact and unique solution . in contrast",
    ", noisy input data requires a higher number of examples to estimate a good approximation of the underlying polynomial .",
    "this direct dependency to the model complexity becomes more severe if we increase the input dimension @xmath7 .",
    "the number of coefficients that have to be estimated , and analogous the number of examples required , grows polynomial in @xmath8 according to @xmath9  ( * ? ? ?",
    "* exercise 1.16 ) .",
    "this immense increase in the amount of required training data is known in a broader as the _ curse of dimensionality_.    a deeper insight and an analysis for classification rather than regression tasks is offered by the theoretical bounds derived in statistical learning theory for the error of a classification model or hypothesis @xmath10  @xcite .",
    "assume that a learner selects a hypothesis from a possibly infinite set of hypotheses @xmath11 which achieves zero error on a given sampled training set of size @xmath4 .",
    "the attribute `` sampled '' refers to the assumption that the training set is a sample from the underlying joint distribution of examples and labels of the task . due to this premise",
    "the following bound is not valid for one - class classification .",
    "a theorem proved by ( * ? ? ? * corrolary 3.4 ) states that with probability of @xmath12 the following number of training examples is sufficient for achieving an error below @xmath13 : @xmath14 the term @xmath15 denotes the _ vc dimension _ of @xmath11 and can be regarded as a complexity measure of the set of available models or hypotheses .",
    "for example , the class of all @xmath7-dimensional linear classifiers including a bias term has a vc dimension of @xmath16  ( * ? ? ?",
    "* section  4.11 , example 2 ) .",
    "let us now take a closer look on the bound in eq .  .",
    "if we fix the maximum error @xmath13 and choose an appropriate small value for @xmath17 , we can see that the sufficient number of training examples depends linearly on the vc dimension @xmath18 .",
    "this directly corresponds to our previous example of polynomial regression , because the vc dimension of @xmath7-variate polynomials of up to order @xmath5 is exactly @xmath19 @xcite .",
    "nearly all machine learning algorithms can be formulated as optimization problems , whether in a direct way , such as done by _ support vector machines ( svm ) _",
    "@xcite , or in an indirect manner like _ boosting _",
    "@xcite approaches . from this point of view",
    ", we can say that learning with few examples inherently tries to solve an ill - posed optimization problem .",
    "therefore , it is not possible to find a suitable well - defined solution without incorporating additional ( prior ) information .",
    "the role of prior information is to ( indirectly ) reduce the set of possible hypotheses .",
    "for example , if we know in advance that for a classification task only the color of an object is important , _ e.g._,if we want to detect expired meat , only a small number of features have to be computed and a lower dimensional linear classifier can be used . in this situation",
    "the vc dimension is reduced , which results in a lower bound for the sufficient number of training examples ( _ cf_eq .  ) .    introducing common prior knowledge into the machine learning part of a visual recognition system",
    "is often done by regularization techniques that penalize non - smooth solutions  @xcite or the `` complexity '' of the classification model .",
    "examples of such techniques are @xmath20-regularization , also known as _ tikhonov regularization _",
    "@xcite , or @xmath21-regularization , which is mostly related to methods trying to find a sparse solution  @xcite .    other possibilities to incorporate prior knowledge include _ semi - supervised learning _ and _ transductive learning _ , which utilize large sets of unlabeled data to support the learning process  @xcite .",
    "unlabeled data can help to estimate the underlying data manifold and , therefore , are able to reduce the number of model parameters",
    ". however , the use of unlabeled data is not studied in this thesis .",
    "machine learning tasks related to computer vision always require a large amount of prior knowledge . for a specific task ,",
    "we indirectly incorporate prior knowledge into the classification system by choosing image preprocessing steps , feature types , feature reduction techniques , or the classifier model .",
    "this choice is mostly based on prior knowledge manually obtained by a software developer from previous experiences on similar visual classification tasks .",
    "for instance , when developing an automatic license plate reader , ideas can be borrowed from optical text recognition or traffic sign detection .",
    "increasing expert prior knowledge decreases the number of training examples needed by the classification system to perform a learning task with a sufficient error rate .",
    "however , this requires a large manual effort .",
    "the goal of some techniques presented in this work is to perform transfer of prior knowledge from previously learned tasks to a new classification task in an automated manner , which is known as _ transfer learning _ and which is a special case of _ knowledge transfer_. the advantage compared to traditional machine learning methods , or _ independent learning _ , is that we do not have to build new classification systems from the scratch or by large manual effort .",
    "previously known tasks used to obtain prior knowledge are referred to as _ support tasks _ and a new classification task only equipped with few training examples is called _",
    "target task_. in the following , we concentrate on _ inductive transfer learning _",
    "@xcite , which assumes that we have labeled data for the target and the support tasks .",
    "especially interesting are situations where a large number of training examples for the support tasks exists and prior knowledge can be robustly estimated .",
    "other terms for transfer learning are _ learning to learn _",
    "@xcite , _ lifelong learning _ and _ interclass transfer",
    "_  @xcite .",
    "the concept of transfer learning is also one of the main principles that explain the development of the human perception and recognition system  @xcite .",
    "for example , it is much easier to learn spanish if we are already able to understand french or italian .",
    "the knowledge transfer concept is known in the language domain as language transfer or linguistic interference  @xcite .",
    "we already mentioned that a child quickly learns new visual object categories in an incremental manner without using many learning examples .",
    "[ fig : simpleillu ] shows some images of a transfer learning scenario for visual object recognition .",
    "generalization from a single example of the new animal category , on the right hand side of  [ fig : simpleillu ] , is possible due to a large set of available memories ( images ) of related animal categories .",
    "these categories often share visual properties , such as typical object part constellations ( head , body and four legs ) or similar fur texture .",
    "developing transfer learning techniques and ideas requires answering four different questions : _ `` what , how , from where and when to transfer?''_. first of all , the type of knowledge which will be transferred from support tasks to a new target task has to be defined , _",
    "e.g. _ , information about common suitable features .",
    "detailed examples are listed in a paragraph of section  [ sec : whathow ] .",
    "the transfer technique applied to incorporate prior knowledge into the learning process of the target task strictly depends on this definition but is not determined by it .",
    "for example , the relevance of features for a classification task can be transferred using generalized linear models  @xcite or random decision forests .",
    "prior knowledge is only helpful for a target task if the support tasks are somehow related or similar . in some applications",
    "not all available previous tasks can be used as support tasks , because they would violate this assumption . giving an answer to the question",
    "_ `` from where to transfer ? '' _ means that the learning algorithm has to select suitable support tasks from a large set of tasks .",
    "these learning situations are referred to as learning in _ heterogeneous environments_. of course , we expect that additional information incorporated by transfer learning always improves the recognition performance on a new target task , because it is the working hypothesis of transfer learning in general . however , in machine learning there is no guarantee at all that the model learned from a training set is also appropriate for all unseen examples of the task , _ i.e._there are no deterministic warranties concerning the generalization ability of a learner .. ] therefore , knowledge transfer can fail and lead to worse performance compared to independent learning .",
    "this event is known as _ negative transfer _ and happens in everyday life .",
    "for example , if we use `` false friends '' when learning a new language , _ e.g._,german speakers are sometimes confused about `` getting a gift '' because the word `` gift '' is the german word for poison , which is likely not a thing you are happy to get .",
    "situations in which negative transfer might occur are difficult to detect .",
    "besides transfer learning , another type of knowledge transfer is _ multitask learning _ which learns different classification tasks jointly .",
    "combined estimation of model parameters can be very helpful , especially if a set of tasks are given , with each having only a small number of training examples .",
    "in contrast to transfer learning there is no prior knowledge obtained in advance , but the model parameters of each task are coupled together .",
    "for example , given a set of classification tasks , relevant features can be estimated jointly and all classifiers are learned independently with the reduced set of features . a possible",
    "application is collaborative filtering as mentioned in section  [ sec : motivation ] . in the following thesis",
    ", we stick to transfer learning but borrow some ideas from multitask learning approaches .",
    "[ fig : concepts ] summarizes the conceptual difference between independent learning , transfer learning , and multitask learning .",
    "furthermore , we illustrate the principle of _ multi - class transfer learning_. in contrast to all other approaches , transfer within a single multi - class task is considered rather than between several ( binary ) classification tasks .",
    "to emphasize this fact , we use the term target class rather than target task .",
    "the main difficulty is that the target class has to be distinguished from the support class , even though information was transferred and exploits their similarity .",
    "it should be noted that another area of knowledge transfer is _ transductive transfer learning _ , which concentrates on transferring knowledge from one application domain to another .",
    "for example , the goal is to recognize objects in low quality webcam images with the support of labeled data from photos made by digital cameras .",
    "related terms are _ sample - selection bias _ , _ covariate shift _ , and _ domain adaptation",
    "_  @xcite .",
    "there is a large body of literature trying to handle the problem of learning with few examples .",
    "a lot of work concentrates on new feature extraction methods , or classifiers , which show superior performance to traditional methods especially for few training examples  @xcite .",
    "the few examples problem was also tackled by introducing manual prior knowledge of the application domain , such as augmenting the training data by applying artificial transformations  @xcite also known as _",
    "data manufacturing_.    in the following , we concentrate on methods related to the principle of transfer learning and multitask learning as introduced in the previous section . other similar surveys and literature reviews can be found in the work of @xcite from a computer vision perspective and the journal paper of @xcite , which gives a comprehensive overview of the work done in the machine learning community .",
    "there is also a textbook by  @xcite covering the broader area of _ meta - learning _ , and the edited book of  @xcite , about the early developments of learning to learn .",
    "the type of knowledge which is transferred from support tasks to a target task is often directly coupled with the method used to incorporate this additional information .",
    "therefore , we give a combined literature review on answers to both questions .    [ [ sec : congealing ] ] learning transformations + + + + + + + + + + + + + + + + + + + + + + + +    one of the most intuitive types of knowledge which can be transferred between categories is application - specific transformations or distortions .",
    "while in data manufacturing methods these transformations have to be defined using manual supervision , transfer learning methods learn this information from support tasks .",
    "for example , a face recognition application can significantly benefit from transformations transferred from other persons using optical flow techniques @xcite . estimating latent transformations and distortions of images ( e.g. illumination changes , rotations , translations ) within a category is proposed by @xcite and @xcite using an optimization approach .",
    "their approach called _ congealing _ tries to minimize the joint entropy of gray - value histograms in each pixel with a greedy strategy .",
    "the obtained transformations can be directly applied to the images of a target task and used in a nearest neighbor approach for text recognition .",
    "restricting and regularizing the complexity of the class of transformations during estimation is important for a good generalization , because it additionally introduces generic prior knowledge .",
    "the original congealing approach proposed a heuristic normalization step directly applied during optimization .",
    "an extension without explicit normalization , and a study of different complexity measures , can be found in the work of @xcite and @xcite .",
    "@xcite use congealing to align images of cars or faces with local features .",
    "[ [ shared - kernel - or - similarity - measure ] ] shared kernel or similarity measure + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    how we compare objects and images strictly depends on the current task .",
    "a distance metric or a more general similarity measure can be an important part of a classification system , e.g. in nearest neighbor approaches .",
    "the term _ kernel _ is a related concept which also measures the similarity between input patterns .",
    "hence , a distance metric or a kernel function is an important piece of prior knowledge which can be transferred to new tasks .",
    "the early work of @xcite used neural network techniques to estimate a similarity measure for a specific task .",
    "in general , the idea of estimating an appropriate metric from data is a research topic on its own called _ metric learning _  @xcite .",
    "a common idea is to find a metric which minimizes distances between examples of a single category ( intra - class ) and maximizes distances between different categories ( inter - class ) .",
    "applying a similarity measure to another task is straightforward when using a nearest neighbor classifier .",
    "@xcite used the metric learning algorithm of @xcite , which allows online learning and estimates the correlation matrix of a mahalanobis distance .",
    "metric learning for visual identification tasks is presented by  @xcite .",
    "they show how to find discriminative local features which can be used to compare different instances of an object category , _ e.g._,distinguishing between specific instances of a car . in this work , metric learning",
    "is based on learning a binary classifier which estimates the probability that both images belong to the same object instance .",
    "the obtained similarity measure can be used for visual identification with only one single training example for each instance .",
    "another application of metric learning is domain adaptation as explained in section  [ sec : knowledgetransfer ] .",
    "the paper of @xcite presents a new database for testing domain adaptation methods and also gives results of a metric learning algorithm .",
    "in contrast , @xcite propose to perform domain adaptation by applying gaussian process regression on scores of examples near the decision boundary .",
    "[ [ shared - features ] ] shared features + + + + + + + + + + + + + + +    visual appearance can be described in terms of features such as color , shape and local parts .",
    "thus , it is natural to transfer information about the relevance of features for a given task .",
    "this idea can be generalized to shared base classifiers which allow modeling feature relevance .",
    "@xcite study combining perceptron - like classifiers of the support and target task . due to the decomposition into multiple base classifiers ( weak learners ) ,",
    "ensemble methods and especially boosting approaches  @xcite are suitable for this type of knowledge transfer .",
    "@xcite extend the standard boosting framework by integrating task - level error terms .",
    "weak learners which achieve a small error on all tasks should be preferred to very specific ones .",
    "a similar concept is used in the work of @xcite , who propose learning multiple binary classifiers jointly with a boosting extension called _",
    "jointboost_. the algorithm tries to find weak learners shared by multiple categories .",
    "this also leads to a reduction of the computation time needed to localize an object with a sliding - window approach .",
    "experiments of @xcite show that the number of feature evaluations grows logarithmic in the number of categories , which is an important benefit compared to independent learning .",
    "an extension of this approach to kernel learning can be found in  @xcite .",
    "@xcite exploits category hierarchies and performs feature sharing by combining hyperplane parameters .",
    "[ [ shared - latent - space ] ] shared latent space + + + + + + + + + + + + + + + + + + +    finding discriminative and meaningful features is a very difficult task .",
    "therefore , the assumption of shared features between tasks is often not valid for empirically selected features used in the application . @xcite",
    "propose assuming an underlying latent feature space which is common to all tasks .",
    "they use the method of @xcite to estimate a feature transformation from support tasks derived from caption texts of news images . to estimate relevant features , the subsequent work  @xcite propose an eigenvalue analysis and the use of unlabeled data .",
    "latent feature spaces can be modeled in a bayesian framework using gaussian processes , which leads to the so called _ gaussian process latent variable model ( gp - lvm ) _  @xcite .",
    "incorporating the idea of a shared latent space into the framework allows using various kinds of noise models and kernels  @xcite .",
    "[ [ constellation - model - and - hierarchical - bayesian - learning ] ] constellation model and hierarchical bayesian learning + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    an approach for knowledge transfer between visual object categories was presented by @xcite .",
    "their method is inspired by the fact that a lot of categories share common object parts which are often also in the same relative position .",
    "based on a generative constellation model  @xcite they propose using maximum a posteriori estimation to obtain model parameters for a new target category .",
    "the prior distribution of the parameters corresponding to relative part positions and part appearance is learned from support tasks .",
    "the underlying idea can also be applied to a shape based approach  @xcite .",
    "a prior on parameters shared between tasks is an instance of hierarchical bayesian learning .",
    "@xcite used this concept to transfer covariance estimates of parts of the feature set .",
    "[ [ joint - regularization ] ] joint regularization + + + + + + + + + + + + + + + + + + + +    a lot of machine learning algorithms such as svm are not directly formulated in a probabilistic manner but as optimization problems .",
    "these problems often include regularization terms connected to the complexity of the parameter , which would correspond to a prior distribution in a bayesian setting .",
    "the equivalent to hierarchical bayesian learning as described in the last paragraph is a joint regularization term shared between tasks .    @xcite",
    "propose using _",
    "trace norm regularization _ of the weight matrix in a multi - class svm approach .",
    "they show that this regularization is related to the assumption of a shared feature transformation and task - specific weight vectors with independent regularization terms . instead of transferring knowledge between different classification tasks",
    "this work concentrates on transfer learning in a multi - class setting , _",
    "i.e._multi - class transfer .",
    "sharing a low dimensional data representation for multitask learning is the idea of @xcite .",
    "the proposed optimization problem learns a feature transformation and a weight vector jointly and additionally favors sparse solutions by utilizing an @xmath22 regularization .",
    "multitask learning with kernel machines was first studied by @xcite .",
    "their idea is to reduce the multitask problem to a single task setting by defining a combined kernel function or _",
    "multitask kernel _ and a new regularizer .",
    "[ [ shared - prior - on - latent - functions ] ] shared prior on latent functions + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the framework of gaussian processes allows modeling a prior distribution of an underlying latent function for each classifier  @xcite using a kernel function .",
    "if we want to learn a set of classifiers jointly in a multitask setting , an appropriate assumption is that all corresponding latent functions are sampled from the same prior distribution .",
    "@xcite suggest learning the hyperparameters of the kernel function jointly by maximizing the marginal likelihood .",
    "this idea was also applied to image categorization tasks  @xcite . a more powerful way of performing transfer learning is studied with multitask kernels originally introduced by @xcite .",
    "@xcite use a parameterized multitask kernel that is the product of a base kernel comparing input features and a task kernel modeling the similarity between tasks and using meta or _ task - specific features_. task similarities can also be learned without additional meta features by estimating a non - parametric version of the task kernel matrix  @xcite .",
    "a theoretical study of the generalization bounds induced by this framework can be found in @xcite .",
    "@xcite propose an algorithm and model to learn the fully non - parametric form of the multitask kernel in a hierarchical bayesian framework .",
    "the _ semi - parametric latent factor model ( slfm ) _ of @xcite is directly related to a multitask kernel .",
    "the latent function for each task is modeled as a linear combination of a smaller set of underlying latent functions .",
    "therefore , the full covariance matrix has a smaller rank , which directly corresponds to the rank assumption of other transfer learning ideas  @xcite . a more general framework which allows modeling arbitrary dependencies between examples and tasks using a graph - theoretic notation",
    "is presented by  @xcite .",
    "[ [ semantic - attributes - and - similarities ] ] semantic attributes and similarities + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    transfer learning with very few examples of the target task can be difficult due to the lack of data to estimate task relations and similarities correctly .",
    "especially if no training data ( neither labeled nor unlabeled ) is available , other data sources have to be used to perform transfer learning .",
    "this scenario is known as _ zero - shot learning _ and uses the concept of _ learning with attributes _ ,",
    "an area which received much attention in recent years .",
    "the term attribute refers to category - specific features .",
    "@xcite use a large database of human - labeled abstract attributes of animal classes ( e.g. brown , stripes , water , eats fish ) .",
    "one idea is to train several attribute classifiers and use their output as new meta features .",
    "this representation allows recognizing new categories without real training images only by comparison with the attribute description of the category .",
    "the knowledge transferred from support tasks is the powerful discriminative attribute representation which was learned with all training data .",
    "a similar idea is presented by @xcite for zero - shot learning based on task - specific features .",
    "a theoretical investigation of zero - shot learning with an attribute representation is given by @xcite and concentrates on analysis with the concept of _ probable approximate correctness ( pac ) _",
    "@xcite .    instead of relying on human - labeled attributes ,",
    "internet sources can be used to mine attributes and relations .",
    "the papers of @xcite compare different kinds of linguistic sources , such as wordnet  @xcite , google search , yahoo and flickr .",
    "a large - scale evaluation of their approach can be found in @xcite .",
    "attribute based recognition can help to generalize to new tasks or categories @xcite which is otherwise difficult using a training set only equipped with ordinary category labels .",
    "attributes can also help to boost the performance of object detection rather than image categorization as shown in @xcite .",
    "their transfer learning approach heavily relies on model sharing of object parts between categories .",
    "@xcite use a generalization of maximum covariance analysis to find a shared latent subspace of different data modalities .",
    "this can also be applied to transfer learning with attributes by regarding the attribute representation as a second modality . beyond zero - shot learning",
    "semantic similarities are also used to guide regularization  @xcite .",
    "[ [ context - information ] ] context information + + + + + + + + + + + + + + + + + + +    up to now , we only covered transfer learning in which knowledge is used from visually similar object categories or tasks .",
    "however , dissimilar categories can also provide useful information if they can be used to derive contextual information .",
    "for example it is likely to find a keyboard next to a computer monitor , which can be a valuable information for an object detector .",
    "methods using contextual information always exploit dependencies between categories and tasks and are therefore a special case for knowledge transfer approaches .",
    "@xcite propose training a set of object detectors simultaneously with an extended version of the _ adaboost _ algorithm  @xcite and can be regarded as an instance of multitask learning . in each round of the boosting algorithm",
    "the map of detection scores is updated and used as an additional feature in subsequent rounds .",
    "a similar idea is presented by @xcite for _ semantic segmentation _ , which labels each pixel of the image as one of the learned categories .",
    "the work of @xcite pursues the same line of research , but clearly separates the support and target tasks .",
    "in a first step geometric properties of image areas are estimated .",
    "the resulting labeling into planar , non - planar , and porous objects , as well as ground and sky areas can be used to further assist local detectors as high - level features .",
    "contextual relationships between different categories can also be modeled directly with a _",
    "conditional markov random field ( crf ) _ as done by @xcite and @xcite on a region - based level for semantic segmentation .",
    "automatically selecting appropriate support tasks from a large set is a difficult sub - task of transfer learning .",
    "therefore , most of the previous work presented in this thesis so far assumes that support tasks are given in advance .",
    "an exception is the early work of @xcite , which proposes the _ task clustering _ algorithm .",
    "similarities between two tasks are estimated by testing the classifier learned on one task using data from the other task .",
    "afterward , clustering can be performed with the resulting task similarity matrix .",
    "@xcite select relevant features for a target task by comparing the weights of the svm hyperplane with each of the available tasks .",
    "therefore , the algorithm selects a similar but more robustly estimated feature representation .",
    "the work of @xcite performs transfer learning with logistic regression classifiers and models the likelihood of each task as a mixture of the target task likelihood and a likelihood term which is independent of all other tasks . due to the task - dependent weight",
    ", the algorithm can adapt to heterogeneous environments .",
    "in general , selecting support tasks is a model selection problem , therefore , techniques like leave - one - out are used  @xcite .",
    "heterogeneous tasks can also be handled within the regularization framework of @xcite by directly optimizing a clustering of the tasks  @xcite .",
    "we gave a summary of current work done in the area of visual transfer learning .",
    "although there is a huge number of papers dealing with the problem of transferring information between tasks ( or domains ) , many of the methods share the same assumptions and underlying basic ideas ."
  ],
  "abstract_text": [
    "<S> transfer learning techniques are important to handle small training sets and to allow for quick generalization even from only a few examples . </S>",
    "<S> the following paper is the introduction as well as the literature overview part of my thesis related to the topic of transfer learning for visual recognition problems . </S>"
  ]
}