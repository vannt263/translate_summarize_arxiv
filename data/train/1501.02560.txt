{
  "article_text": [
    "due to the increase of imperfect data , the process of decision making is becoming harder . in order to face this ,",
    "the data analysis is being applied in various fields .",
    "clustering is mostly used in data mining and aims at grouping a set of similar objects into clusters . in this context , many clustering algorithms exist and are categorized into two main families : + the first family involves the partitioning methods based on density such as @xmath0-means algorithm @xcite that is widely used thanks to its convergence speed .",
    "it partitions the data into @xmath0 clusters represented by their centers .",
    "the second family includes the hierarchical clustering methods such as the top - down and the hierarchical ascendant clustering ( hac ) @xcite .",
    "this latter consists on constructing clusters recursively by partitioning the objects in a bottom - up way .",
    "this process leads to good result visualizations .",
    "nevertheless , it has a non - linear complexity .",
    "all these standard methods deal with certain and precise data .",
    "thus , in order to facilitate the decision making , it would be more appropriate to handle uncertain data . here , we need a soft clustering process that will take into account the possibility that objects belong to more than one cluster .",
    "in such a case , several methods have been established . among them , the fuzzy c - means @xcite which consists in assigning a membership to each data point corresponding to the cluster center , and the weights minimizing the total weighted mean - square error .",
    "this method constantly converges .",
    "patently , evidential @xmath1-means ( ecm ) @xcite , @xcite is deemed to be a very fateful method .",
    "it enhances the fcm and generates a credal partition from attribute data .",
    "this method deals with the clustering of object data .",
    "accordingly , the belief @xmath0-modes method @xcite is a popular method , which builds k groups characterized by uncertain attribute values and provides a classification of new instances .",
    "schubert has also found a clustering algorithm @xcite which uses the mass on the empty set to build a classifier .",
    "our objective in this paper is to develop a belief hierarchical clustering method , in order to ensure the membership of objects in several clusters , and to handle the uncertainty in data under the belief function framework .",
    "this remainder is organized as follows : in the next section we review the ascendant hierarchical clustering , its concepts and its characteristics . in section 3 ,",
    "we recall some of the basic concepts of belief function theory .",
    "our method is described in section 4 and we evaluate its performance on a real data set in section 5 .",
    "finally , section 6 is a conclusion for the whole paper .",
    "this method consists on agglomerating the close clusters in order to have finally one cluster containing all the objects @xmath2 ( where @xmath3 .",
    "+ let s consider @xmath4 the set of clusters .",
    "if @xmath5 , @xmath6 . thereafter",
    ", throughout all the steps of clustering , we will move from a partition @xmath7 to a partition @xmath8 .",
    "the result generated is described by a hierarchical clustering tree ( dendrogram ) , where the nodes represent the successive fusions and the height of the nodes represents the value of the distance between two objects which gives a concrete meaning to the level of nodes conscripted as `` indexed hierarchy '' .",
    "this latter is usually indexed by the values of the distances ( or dissimilarity ) for each aggregation step .",
    "the indexed hierarchy can be seen as a set with an ultrametric distance @xmath9 which satisfies these properties : _",
    "i ) _ @xmath10 .",
    "+ _ ii ) _ @xmath11 .",
    "+ _ iii ) _ @xmath12 .",
    "+ the algorithm is as follows :    * initialisation : the initial clusters are the n - singletons .",
    "we compute their dissimilarity matrix .",
    "* iterate these two steps until the aggregation turns into a single cluster : * * combine the two most similar ( closest ) elements ( clusters ) from the selected groups according to some distance rules . * * update the matrix distance by replacing the two grouped elements by the new one and calculate its distance from each of the other classes .",
    "once all these steps completed , we do not recover a partition of @xmath13 clusters , but a partition of @xmath14 clusters .",
    "hence , we had to point out the aggregation criterion ( distance rules ) between two points and between two clusters .",
    "we can use the euclidian distance between @xmath15 objects @xmath16 defined in a space @xmath17 .",
    "different distances can be considered between two clusters : we can consider the minimum as follows : @xmath18 with @xmath19 .",
    "the maximum can also be considered , however , the minimum and maximum distances create compact clusters but sensitive to `` outliers '' .",
    "the average can also be used , but the most used method is ward s method , using huygens formula to compute this : @xmath20 where @xmath21 and @xmath22 are numbers of elements of @xmath23 and @xmath24 respectively and @xmath25 the centers .",
    "then , we had to find the couple of clusters minimizing the distance : @xmath26",
    "in this section , we briefly review the main concepts that will be used in our method that underlies the theory of belief functions @xcite as interpreted in the transferable belief model ( tbm ) @xcite . let s suppose that the frame of discernment is @xmath27 .",
    "@xmath28 is a finite set that reflects a state of partial knowledge that can be represented by a basis belief assignment defined as : @xmath29\\\\ \\sum_{a\\subseteq \\omega}\\limits m(a)=1 \\end { array}\\ ] ] the value @xmath30 is named a basic belief mass ( bbm ) of @xmath31 . the subset @xmath32 is called focal element if @xmath33 .",
    "one of the important rules in the belief theory is the conjunctive rule which consists on combining two basic belief assignments @xmath34 and @xmath35 induced from two distinct and reliable information sources defined as : @xmath36 the dempster rule is the normalized conjunctive rule : @xmath37    in order to ensure the decision making , beliefs are transformed into probability measures recorded @xmath38 , and defined as follows @xcite : @xmath39",
    "in order to set down a way to develop a belief hierarchical clustering , we choose to work on different levels : on one hand , the object level , on the other hand , the cluster level . at the beginning , for @xmath15 objects we have ,",
    "the frame of discernment is @xmath40 and for each object belonging to one cluster , a degree of belief is assigned .",
    "let @xmath41 be the partition of @xmath15 objects .",
    "hence , we define a mass function for each object @xmath42 , inspired from the @xmath0-nearest neighbors @xcite method which is defined as follows : @xmath43 where @xmath44 , @xmath45 and @xmath46 are two parameters we can optimize  @xcite , @xmath9 can be considered as the euclidean distance , and the frame of discernment is given by @xmath47 .    in order to move from the partition of @xmath15 objects to a partition of @xmath48 objects we have to find both nearest objects @xmath49 to form a cluster .",
    "eventually , the partition of @xmath48 clusters will be given by @xmath50 where @xmath51 .",
    "the nearest objects are found considering the pignistic probability , defined on the frame @xmath52 , of each object @xmath42 , where we proceed the comparison by pairs , by computing firstly the pignistic for each object , and then we continue the process using @xmath53 .",
    "the nearest objects are chosen using the maximum of the pignistic values between pairs of objects , and we will compute the product pair one by one .",
    "@xmath54    then , this first couple of objects is a cluster . now consider that we have a partition @xmath55 of @xmath13 clusters @xmath56 . in order to find the best partition @xmath8 of @xmath14 clusters",
    ", we have to find the best couple of clusters to be merged .",
    "first , if we consider one of the classical distances @xmath57 ( single link , complete link , average , etc ) , presented in section  [ cah ] , between the clusters , we delineate a mass function , defined within the frame @xmath52 for each cluster @xmath58 with @xmath59 by : @xmath60 where @xmath61 . then , both clusters to merge are given by : @xmath62 and the partition @xmath8 is made from the new cluster @xmath63 and all the other clusters of @xmath7 .",
    "the point by doing so is to prove that if we maximize the degree of probability we will have the couple of clusters to combine .",
    "of course , this approach will give exactly the same partitions than the classical ascendant hierarchical clustering , but the dendrogram can be built from @xmath38 and the best partition ( _ i.e. _ the number of clusters ) can be preferred to find",
    ". the indexed hierarchy will be indexed by the sum of @xmath38 which will lead to more precise and specific results according to the dissimilarity between objects and therefore will facilitate our process .",
    "hereafter , we define another way to build the partition @xmath8 . for each initial object @xmath42 to classify",
    ", it exists a cluster of @xmath55 such as @xmath64 .",
    "we consider the frame of discernment @xmath65 , @xmath66 , which describes the degree that the two clusters could be merged , can be noted @xmath67and we define the mass function : @xmath68 in order to find a mass function for each cluster @xmath69 of @xmath7 , we combine all the mass functions given by all objects of @xmath69 by a combination rule such as the dempster rule of combination given by equation  .",
    "then , to merge both clusters we use the equation   as before .",
    "the sum of the pignisitic probabilities will be the index of the dendrogram , called @xmath38 index .",
    "experiments were first applied on diamond data set composed of twelve objects as describe in figure  [ diamondres].a and analyzed in @xcite .",
    "the dendrograms for both classical and belief hierarchical clustering ( bhc ) are represented by figures  [ diamondres].b and  [ diamondres].c .",
    "the object 12 is well considered as an outlier with both approaches . with the belief hierarchical clustering",
    ", this object is clearly different , thanks to the pignistic probability . for hac ,",
    "the distance between object 12 and other objects is small , however , for bhc , there is a big gap between object 12 and others .",
    "this points out that our method is better for detecting outliers .",
    "if the objects 5 and 6 are associated to 1 , 2 , 3 and 4 with the classical hierarchical clustering , with bhc these points are more identified as different .",
    "this synthetic data set is special because of the equidistance of the points and there is no uncertainty .    [ cols=\"^,^ \" , ]",
    "ultimately , we have introduced a new clustering method using the hierarchical paradigm in order to implement uncertainty in the belief function framework .",
    "this method puts the emphasis on the fact that one object may belong to several clusters .",
    "it seeks to merge clusters based on its pignistic probability .",
    "our method was proved on data sets and the corresponding results have clearly shown its efficiency .",
    "the algorithm complexity has revealed itself as the usual problem of the belief function theory . our future work will be devoted to focus on this peculiar problem .",
    "schubert , j. : clustering belief functions based on attracting and conflicting metalevel evidence . in : bouchon - meunier , b. , foulloy , l. , yager , r. ( eds . ) intelligent systems for information processing : from representation to applications .",
    "elsevier science ( 2003 )        zouhal , l.m . ,",
    "denux , t. : an evidence - theoric @xmath0-nn rule with parameter optimization .",
    "ieee transactions on systems , man , and cybernetics - part c : applications and reviews 28(2 ) , 263271 ( mai 1998 )"
  ],
  "abstract_text": [
    "<S> in the data mining field many clustering methods have been proposed , yet standard versions do not take into account uncertain databases . </S>",
    "<S> this paper deals with a new approach to cluster uncertain data by using a hierarchical clustering defined within the belief function framework . </S>",
    "<S> the main objective of the belief hierarchical clustering is to allow an object to belong to one or several clusters . to each belonging </S>",
    "<S> , a degree of belief is associated , and clusters are combined based on the pignistic properties . experiments with real uncertain data show that our proposed method can be considered as a propitious tool .    </S>",
    "<S> , hierarchical clustering , belief function , belief clustering </S>"
  ]
}