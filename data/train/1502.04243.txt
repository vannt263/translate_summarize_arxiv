{
  "article_text": [
    "an important common challenge facing retailers is to understand customer preferences in the presence of stockouts .",
    "when an item is out of stock , some customers will leave , while others will substitute a different product . from the transaction data collected by retailers",
    ", it is challenging to determine exactly what the customer s original intent was , or , because of customers that leave without making a purchase , even how many customers there actually were .",
    "the task that we consider here is to infer both the customer arrival rate , including the unobserved customers that left without a purchase , and the substitution model , which describes how customers substitute when their preferred item is out of stock .",
    "furthermore , we wish to infer these from sales transaction and stock level data , which data are readily available for many retailers .",
    "these quantities are a necessary input for inventory management and assortment planning problems .",
    "stockouts are a common occurrence in some retail settings , such as bakeries and flash - sale retailers @xcite .",
    "not properly accounting for the data truncation caused by stockouts can lead to poor stocking decisions .",
    "navely estimating demand as the number of items sold underestimates the demand of items that stock out , while overestimating the demand of their substitutes .",
    "this could lead the retailer to set the stock for the substitute items too high , while leaving the stock of the stocked - out item too low , potentially losing customers and revenue .",
    "there are several key features of our model and inference that make it successful in problem settings where prior work in the area has not been .",
    "first , prior work has assumed the arrival rate to be constant within each time period @xcite .",
    "our model allows for arbitrary nonhomogeneous arrival rate functions , which is important for our bakery case study where sales have strong peaks at lunch time and between classes .",
    "second , prior work has required a particular choice model @xcite , whereas our model can incorporate whichever choice model is most appropriate",
    ". there are a wide variety of choice models , econometric models describing how a customer chooses one of several alternatives , with different properties and which are applicable in different settings .",
    "third , we model multiple customer segments , each with their own substitution models which can be used to borrow strength across data from multiple stores .",
    "fourth , unlike prior work which has used point estimates , our inference is fully bayesian . because we do full posterior inference , we are able to compute the posterior predictive distributions for decision quantities of interest , such as lost sales due to stock unavailability .",
    "this allows us to incorporate the uncertainty in estimation directly into uncertainty in our decision quantities , thus leading to more robust decisions .",
    "our contributions are four - fold .",
    "first , we develop a bayesian hierarchical model that uses the censoring caused by stockouts and their induced substitutions to gain useful insight from transaction data .",
    "our model is flexible and powerful enough to be useful in a wide range of retail settings .",
    "second , we show how recent advances in mcmc for topic models can be adapted to our model to provide a sampling procedure that scales to large transaction databases .",
    "third , we provide a simulation study which shows that we can recover the true generating values and which demonstrates the scalability of the inference procedure .",
    "finally , we make available actual retail transaction data from a bakery and use these data for a case study showing how the model and sampling work in a real setting . in the case study we evaluate the predictive power of the model , and show that our model can make accurate out - of - sample predictions whereas the baseline method can not .",
    "we finally show how the methods developed here can be useful for decision making by producing a posterior predictive distribution of the bakery s lost sales due to stock unavailability .",
    "we begin by introducing the notation that we use to describe the observed data .",
    "we then introduce the nonhomogeneous model for customer arrivals , followed by a discussion of various possible choice models .",
    "section [ sec : mixtures ] discusses how multiple customer segments are modeled .",
    "finally , section [ sec : likelihood ] introduces the likelihood model and section [ sec : prior ] discusses the prior distributions .      we suppose that we have data from a collection of stores @xmath0 . for each store , data come from a number of time periods @xmath1 , throughout each of which time varies from @xmath2 to @xmath3 . for example , in our experiments a time period was one day .",
    "we consider a collection of items @xmath4 .",
    "we suppose that we have two types of data : purchase times and stock levels .",
    "we denote the number of purchases of item @xmath5 in time period @xmath6 at store @xmath7 as @xmath8 .",
    "then , we let @xmath9 be the observed purchase times of item @xmath5 in time period @xmath6 at store @xmath7 . for notational convenience ,",
    "we let @xmath10 be the collection of all purchase times for that store and time period , and let @xmath11 be the complete set of arrival time data .    we denote the known initial stock level as @xmath12 and assume that stocks are not replenished throughout the time period .",
    "that is , @xmath13 and equality implies a stockout . as before ,",
    "we let @xmath14 and @xmath15 represent respectively the collection of initial stock data for store @xmath7 and time period @xmath6 , and for all stores and all time periods .    given @xmath16 and @xmath12",
    ", we can compute a stock indicator as a function of time .",
    "we define this indicator function as @xmath17    the generative model for these data will be that customers arrive at the store according to some arrival process .",
    "each customer belongs to a particular segment , and chooses an item to purchase ( or no - purchase ) based on the preferences of his or her segment and the available stock .",
    "when the customer purchases item @xmath5 , the arrival time is recorded in @xmath18 .",
    "when a customer leaves without making a purchase , for instance because his or her preferred item is out of stock , the arrival time is not recorded .",
    "we now present the two main components of this model : the customer arrival process and the choice model .",
    "we model the times of customer arrivals using a nonhomogeneous poisson process ( nhpp ) .",
    "an nhpp is a generalization of the poisson process that allows for the intensity to be described by a function @xmath19 as opposed to being constant .",
    "we assume that the intensity function has been parameterized , with parameters @xmath20 potentially different for each store @xmath7 .",
    "the most basic parameterization is @xmath21 , producing a homogeneous poisson process of rate @xmath22 . as another example",
    ", we can produce an intensity function that rises to a peak and then decays by letting @xmath23 which is the derivative of the hill equation @xcite .    the posterior of @xmath20 will be inferred . to do this we use the log - likelihood function for nhpp arrivals , which for arrival times @xmath24 over interval @xmath25 $ ] is : @xmath26 where @xmath27 .",
    "our model can incorporate any integrable rate function .",
    "we let @xmath28 represent the complete collection of rate function parameters to be inferred .",
    "whether or not a customer purchases an item and which item they purchase depends on the stock availability as well as some choice model parameters which we describe below .",
    "we define @xmath29 to be the probability that a customer purchases product @xmath5 given the current stock @xmath30 and choice model parameters @xmath31 and @xmath32 .",
    "then , we denote the no - purchase probability as @xmath33 the index @xmath34 indicates the parameters for a particular customer segment , which we will discuss in section [ sec : mixtures ] .",
    "posterior distributions for the parameters @xmath31 and @xmath32 are inferred .",
    "choice models are econometric models describing a customer s choice between several alternatives , often derived from a utility maximization problem .",
    "different assumptions and utility models lead to different choice models , which ultimately lead to a different form of the purchase probability @xmath29 .",
    "our model accommodates any choice model for which the purchase probabilities can be expressed as a function of the current stock .",
    "we now discuss how several common choice models fit into this framework , and we use these choice models in our simulation and data experiments .",
    "the multinomial logit ( mnl ) is a popular choice model with parameters @xmath35 specifying a preference distribution over products , that is , @xmath36 and @xmath37 .",
    "each customer selects a product according to that distribution .",
    "when an item goes out of stock , substitution takes place by transferring purchase probability to the other items proportionally to their original probability , including to the no - purchase option .",
    "this model requires a proportion @xmath38 of arrivals be no - purchases when all items are in stock .",
    "the mnl choice probabilities are : @xmath39 the mnl model parameter @xmath32 is not identifiable when the arrival function is also unknown , a serious disadvantage of this model @xcite .",
    "the exogenous choice model overcomes many of the shortcomings of the mnl model , including the issue of parameter identifiability . according to the exogenous proportional substitution model @xcite , a customer samples a first choice from the preference distribution @xmath31 .",
    "if that item is available , he or she purchases the item .",
    "if the first choice is not available , with probability @xmath40 the customer leaves as no - purchase . with the remaining @xmath32 probability",
    ", the customer picks a second choice according to a preference vector that has been re - weighted to exclude the first choice .",
    "specifically , if the first choice was @xmath41 then the probability of choosing @xmath5 as the second choice is @xmath42 .",
    "if the second choice is in stock it is purchased , otherwise the customer leaves as no - purchase .",
    "the purchase probability is @xmath43 posterior distributions for both @xmath31 and @xmath32 are inferred .",
    "allowing for the no - purchase option only in the event of stockouts means that the inferred arrival rate will be that of customers who actually would have purchased an item had all items been in stock .",
    "it would be possible for the exogenous model to include a proportion of customers that make no purchase even with full stock , as is required by the mnl model .",
    "however , inasmuch as these customers make no contribution to sales regardless of stock , it serves no purpose in the ultimate goal of understanding the effect of stock on sales .",
    "nonparametric models describe preferences as an ordered set of items .",
    "let @xmath31 be an ordered subset of the items @xmath44 .",
    "customers purchase @xmath45 if it is in stock .",
    "if not , they purchase @xmath46 if it is in stock .",
    "if not , they continue substituting down @xmath31 until they reach the first item that is available . if none of the items in @xmath31 are available , they leave as a no - purchase .",
    "the purchase probability for this model is then @xmath47 for the first in - stock item in @xmath31 , and @xmath2 otherwise .",
    "because this model requires all customers to behave exactly the same , it is most useful when customers are modeled as coming from a number of different segments @xmath34 , each with its own preference ranking @xmath31 .",
    "this is precisely what we do in our model , as we describe in the next section . for the nonparametric model",
    "the rank orders for each segment @xmath31 are fixed and it is the distribution of customers across segments that is inferred . we do not generally need to consider all possible rank orders , as we discuss in the next section .",
    "we model customers as each coming from one of @xmath48 segments @xmath49 , each with its own choice model parameters @xmath31 and @xmath32 .",
    "let @xmath50 be the customer segment distribution for store @xmath7 , with @xmath51 the probability that an arrival at store @xmath7 belongs to segment @xmath34 , @xmath52 , and @xmath53 .",
    "as with other variables , we denote the collection of segment distributions across all stores as @xmath54 .",
    "similarly , we denote the collections of choice model parameters across all segments as @xmath55 and @xmath56 .    for the nonparametric choice model , each of these segments would have a different rank ordering of items and multiple segments",
    "are required in order to have a diverse set of preferences . for the mnl and exogenous choice models ,",
    "customer segments can be used to borrow strength across multiple stores .",
    "all stores share the same underlying segment parameters @xmath55 and @xmath56 , but each store s arrivals are represented by a different mixing of these segments , @xmath50 . this model allows us to use data from all of the stores for inferring the choice model parameters , while still allowing stores to differ from each other by having a different mixture of segments .    with the nonparametric choice model , using a segment for each ordered subset of @xmath44 would likely result in more parameters",
    "than could be reasonably inferred for @xmath57 even moderately large .",
    "our inference procedure would be most appropriate for nonparametric models with one or two substitutions ( that is , ordered subsets of size 2 or 3 ) , which could still capture a wide range of behaviors .",
    "we now describe in detail the generative model for how customer segments , choice models , stock levels , and the arrival function all interact to create transaction data .",
    "consider store @xmath7 and time period @xmath6 .",
    "customers arrive according to the nhpp for this store .",
    "let @xmath58 represent all of the arrival times ; these are unobserved , as they may include no - purchases .",
    "each arrival has probability @xmath51 of belonging to segment @xmath34 .",
    "they then purchase an item or leave as no - purchase according to the choice model @xmath59 .",
    "if the @xmath41th arrival purchases an item then we observe that purchase at time @xmath60 ; if they leave as no - purchase we do not observe that arrival at all .",
    "the generative model for the observed data @xmath61 is thus :    for store @xmath0 and time period @xmath62    * sample arrivals @xmath63 . * for arrival @xmath64 : * * sample segment as @xmath65 .",
    "* * with probability @xmath66 purchase item @xmath5 , or no purchase with @xmath67 .",
    "* * if item @xmath68 purchased , add the time to @xmath69 .",
    "we denote the probability that an arrival at time @xmath70 purchases item @xmath5 as @xmath71 .",
    "an important quantity for the likelihood is the _ observed purchase rate _ , which is the arrival rate times the purchase probability : @xmath72 this is the rate at which customers purchase item @xmath5 , incorporating stock availability and customer choice .",
    "the corresponding mean function is @xmath73 .",
    "the following theorem gives the likelihood function corresponding to this generative model .",
    "[ thm : loglklhd ] the log - likelihood function of @xmath61 is @xmath74    remarkably , the result is that which would be obtained if we treated the purchases for each item as independent nhpps with rate @xmath75 , the observed purchase rate from ( [ eq : obsp ] ) . in reality",
    ", they are not independent nhpps inasmuch as they depend on each other via the stock function @xmath76 .",
    "the key element of the proof is that while the purchase processes depend on each other , they do not depend on the no - purchase arrivals .",
    "the proof is given in the appendix .",
    "also in the appendix we show how the mean function @xmath77 can be expressed in terms of @xmath78 and thus computed efficiently , provided the rate function is integrable .      finally , we specify a prior distribution for each of the latent variables : @xmath79 , @xmath54 , and @xmath55 and @xmath56 as required by the choice model .",
    "the variables @xmath54 , @xmath55 , and @xmath56 are all probability vectors , so the natural choice is to assign them a dirichlet or beta prior : @xmath80 in our experiments , we used uniform priors by setting the hyperparameters to vectors of @xmath47 .",
    "similarly , a natural choice for the prior distribution of @xmath79 is a uniform distribution for each element : @xmath81 in our experiments we chose the interval @xmath82 large enough to not be restrictive .",
    "we use mcmc techniques to simulate posterior samples , specifically the stochastic gradient riemannian langevin dynamics ( sgrld ) algorithm of @xcite .",
    "sgrld was developed for posterior inference in topic models , to which our model is conceptually similar .",
    "it uses a stochastic gradient that does not require the full likelihood function to be evaluated in every mcmc iteration , which is critical for doing posterior inference on a potentially very large transaction database .",
    "we first transform each of the probability variables using the expanded - mean parameterization @xcite .",
    "consider the latent variable @xmath54 , which has as constraints @xmath83 and @xmath84 .",
    "take @xmath85 a random variable with support on @xmath86 , and give @xmath85 a prior distribution consisting of a product of @xmath87 distributions : @xmath88 the posterior sampling is done over variables @xmath85 by mirroring any negative proposal values about @xmath2 .",
    "we then set @xmath89 .",
    "this parameterization is equivalent to sampling on @xmath54 with a @xmath90 prior , but does not require the probability simplex constraint .",
    "the same transformation is done to @xmath31 and @xmath32 .",
    "let @xmath91 represent the complete collection of transformed latent variables whose posterior distribution we are inferring . from state @xmath92 on mcmc iteration @xmath93",
    ", the next iteration moves to the state @xmath94 according to @xmath95 the iteration performs a gradient step plus normally distributed noise , using the natural gradient of the log posterior , which is the manifold direction of steepest descent using the metric @xmath96 . using bayes theorem ,",
    "the posterior gradient can be decomposed into the likelihood gradient and the prior gradient , and we use a stochastic gradient approximation for the likelihood gradient .",
    "on mcmc iteration @xmath93 , rather than use all @xmath97 time periods to compute the gradient we use a uniformly sampled collection of time periods @xmath98 .",
    "the gradient approximation is then @xmath99 the iterations will converge to the posterior samples if the step size schedule is chosen such that @xmath100 and @xmath101 @xcite . in our simulations and experiments",
    "we used three time periods for the stochastic gradient approximations .",
    "we followed @xcite and took @xmath102 , with parameters @xmath103 , @xmath104 , and @xmath105 chosen using cross - validation over a grid to minimize out - of - sample perplexity .",
    "we drew 10,000 samples from each of three chains initialized at a local maximum _ a posteriori _ solution found from a random sample from the prior .",
    "we verified convergence using the gelman - rubin diagnostic after discarding the first half of the samples as burn - in @xcite , and then merged samples from all three chains to estimate the posterior .     used in the set of simulations ( 3 stores @xmath106 10 simulations ) , the corresponding estimate of the posterior mean .",
    "the bottom panel shows the same result for each value of @xmath51 used ( 3 stores @xmath106 2 segments @xmath106 10 simulations ) . for a range of generating parameter values ,",
    "the posterior distributions were centered on the true values . ]     from a simulation with the corresponding number of time periods , across the 3 values of @xmath34 where the true value equaled @xmath107 .",
    "the top panel shows the posterior mean for each of the simulations across the different number of time periods .",
    "the bottom panel shows the interquartile range ( iqr ) of the posterior .",
    "as the amount of available data increased , the posterior distributions became increasingly concentrated on the true values . ]",
    "we use a collection of simulations to illustrate and analyze the model and the inference procedure .",
    "we use a variety of rate functions and choice models throughout the simulations to demonstrate this flexibility of the model .",
    "first we use the simulations to verify that the posterior concentrates around the true generating values for a wide selection of arrival rate functions , choice models , and model parameters",
    ". then we use simulations to investigate the dependence on the amount of data used in the inference .",
    "the simulations show that the posterior variance decreases as the size of the training data set increases , which is remarkable inasmuch as the reduction of uncertainty came with no additional computational cost because of the stochastic gradient approximation for the likelihood .",
    "the first set of simulations used the homogeneous rate function @xmath21 and the exogenous choice model given in ( [ eq : exo ] ) , with @xmath108 stores , @xmath109 segments , and @xmath110 items .",
    "the choice model parameters were fixed at @xmath111 , @xmath112 $ ] , and @xmath113 $ ] . for each of 10 simulated data sets ,",
    "the segment distributions @xmath114 were chosen independently at random from a uniform dirichlet distribution and the arrival rates @xmath22 were chosen independently at random from a uniform distribution on @xmath115 $ ] .",
    "for each store , we simulated @xmath116 time periods , each of length @xmath117 and with the initial stock for each item chosen uniformly between @xmath2 and @xmath118 , independently at random for each item , time period , and store .",
    "purchase data were then generated according to the generative model in section [ sec : likelihood ] .",
    "figure [ fig : sim1_5 ] shows the posterior means estimated from the mcmc samples across the @xmath119 repeats of the simulation , each with different segment distributions and rate parameters .",
    "this figure shows that across the full range of parameter values used in these simulations the posterior mean was close to the true , generating value .    in the second set of simulations we used the hill rate function with the nonparametric choice model , with 3 items .",
    "we used all sets of preference rankings of size @xmath47 and @xmath120 , which for @xmath121 items requires a total of @xmath122 segments .",
    "we simulated data for a single store , with the segment proportion @xmath123 set to @xmath107 for preference rankings @xmath124 , @xmath125 , and @xmath126 : the first segment prefers item @xmath47 and will leave with no purchase if item @xmath47 is not available , the second segment prefers item @xmath47 but is willing to substitute to item @xmath120 , and the third segment prefers item @xmath121 but is willing to substitute to item @xmath120 .",
    "the segment proportions for the remaining @xmath127 preference rankings were set to zero . with this simulation",
    "we study the effect of the number of time periods used in the inference , @xmath128 .",
    "@xmath128 was taken from @xmath129 , and for each of these values 10 simulations were done .",
    "as in figure [ fig : sim1_5 ] , the posterior densities for the segment proportions were concentrated near their true values .",
    "figure [ fig : sim3_3 ] shows how the posteriors depended on the number of time periods of available data .",
    "the top panel shows that the posterior means for the non - zero segment proportions tended closer to the true value as more data were made available .",
    "the bottom panel shows the actual concentration of the posterior , where the interquartile range of the posterior decreased with the number of time periods . because we use a stochastic gradient approximation , using more time periods came at no additional computational cost : we used 3 time periods for each gradient approximation regardless of the available number",
    "we now provide the results of the model applied to real transaction data . as part of our case study",
    ", we evaluate the predictive power of the model and sample the posterior distribution of lost sales due to stockouts .",
    "we obtained one semester of sales data from the bakery at 100 main marketplace , a cafe located at mit , for a collection of cookies : oatmeal , double chocolate , and chocolate chip .",
    "the data set included all purchase times for 151 days ; we treated each day as a time period ( 11:00 a.m. to 7:00 p.m. ) , and there were a total of 4084 purchases .",
    "stock data were not available , only purchase times , so for the purpose of these experiments we set the initial stock for each time period equal to the number of purchases for the time period - thus every item was treated as stocked out after its last recorded purchase",
    ". this may be a reasonable assumption for these cookies given that they are perishable baked goods which are meant to stock out by the end of the day , but in any case the experiments still provide a useful illustration of the method .",
    "the empirical purchase rate for the cookies , shown in figure [ fig : lnch_2 ] , was markedly nonhomogeneous : there is a broad peak at lunch time and two sharp peaks at common class ending times .",
    "we modeled the rate function with a combination of the hill function @xmath130 ( [ eq : hill ] ) and a fixed function consisting of only two peaks at the two afternoon peak times , @xmath131 , obtained via a spline .",
    "the hill function has three parameters , and then a fourth parameter provided the weight of the fixed peaks that were added in : @xmath132 .",
    "we fit the model separately with the exogenous and nonparametric choice models .",
    "figure [ fig : lnch_2 ] shows 20 posterior samples for the model s predicted average purchase rate over all time periods , which equals @xmath133 , from the fit with the nonparametric choice model .",
    "these samples show that the model provides an accurate description of the arrival rate .",
    "the variance in the samples provides an indication of the uncertainty in the model , which further motivates the use of the posterior predictive distribution over a point estimate for making predictions .",
    "figure [ fig : lnch_3 ] shows the posterior density for the substitution rate @xmath56 , obtained by fitting the model with the exogenous choice model .",
    "the substitution rate is very low , indicating that most customers left without a purchase if their preferred cookie was not in stock .",
    "the posterior distribution of the item preference vector is given in [ fig : lnch_4 ] .",
    "chocolate chip cookies were the strong favorite , followed by double chocolate and lastly oatmeal .",
    "the next set of experiments establish that the model has predictive power on real data .",
    "we evaluated the predictive power of the model by predicting out - of - sample purchase counts during periods of varying stock availability .",
    "we took the first @xmath134 of time periods ( 120 time periods ) as training data and did posterior inference .",
    "the latter 31 time periods were held out as test data , the goal being to use data from the first part of the semester to make predictions about the latter part .",
    "we considered each possible level of stock unavailability , _",
    "i.e. _ , @xmath135 $ ] , @xmath136 $ ] , etc . for each stock level",
    ", we found all of the time intervals in the test periods with that stock .",
    "the prediction task was , given only the time intervals and the corresponding stock level , to predict the total number of purchases that took place during those time intervals in the test periods .",
    "the actual number of purchases is known and thus predictive performance can be evaluated .",
    "there were no intervals where only chocolate chip cookies were out of stock , but predictions were made for every other stock combination .",
    "this is a meaningful prediction task because good performance requires being able to accurately model both the arrival rate as a function of time and how the actual purchases then depend on the stock .",
    "we compare predictive performance to a baseline model that has previously been proposed for this problem by @xcite , which is the maximum likelihood model with a homogeneous arrival rate and the mnl choice model .",
    "we discuss this and other related works in more detail in section [ sec : relatedwork ] .    for the mnl baseline ,",
    "the parameter @xmath137 is unidentifiable and can not be estimated .",
    "we fit the model for each fixed @xmath138 , and show here the results with the value of @xmath137 that minimized the out - of - sample absolute deviation between the model expected number of purchases and the true number of purchases , which was @xmath139 .",
    "that is , we show here the results that would have been obtained if we had known _ a priori _ the best value of @xmath137 , and thus show the best possible performance of the baseline .    for our model , for each choice model ( nonparametric and exogenous ) posterior samples obtained from the mcmc procedure were used to estimate the posterior predictive distribution for the number of purchases under each stock level . for the maximum likelihood baseline , we used simulation to estimate the distribution of purchase counts conditioned on the point estimate model .",
    "these posterior densities , smoothed with a kernel density estimate , are given in figure [ fig : pred_1 ] . despite their very different natures , the predictions made by the exogenous and nonparametric models are quite similar , and both have",
    "posterior means close to the true values for all stock levels . the baseline maximum likelihood model with a homogeneous arrival rate and mnl choice performs very poorly .",
    "our purpose in inferring the model is to use it to make better stocking decisions .",
    "an important starting point is to use the inferred parameters to estimate what the sales would have been had there not been any stockouts .",
    "this allows us to know how much revenue is being lost with our current stocking strategy .",
    "we estimated posterior densities for the number of purchases of each item across 151 time periods , with full stock .",
    "figure [ fig : pred_4 ] compares those densities to the actual number of cookie purchases in the data .",
    "for each of the cookies , the actual number of purchases was significantly less than the posterior density for purchases with full stock , indicating that there were substantial lost sales due to stockouts . with the nonparametric model , the difference between the full - stock posterior mean and the actual number of purchases was 791 oatmeal cookies , 707 double chocolate cookies , and 1535 chocolate chip cookies .",
    "figure [ fig : lnch_3 ] shows that customers were generally unwilling to substitute , which would have contributed to the lost sales .",
    "the primary work on this problem , estimating demand and substitution from sales transaction data with stockouts and unobserved no - purchases , was done by @xcite .",
    "they model customer arrivals using a homogeneous poisson process within each time period , meaning the arrival rate is constant throughout each time period .",
    "customers then choose an item , or an unobserved no - purchase , according to the mnl choice model .",
    "they derive an em algorithm to solve the corresponding maximum likelihood problem . in the prediction task of section [ sec : predictions ] we compared our results with this model as the baseline and found that it was unable to make accurate predictions with our case study data .",
    "our model overcomes several limitations of this model , thereby substantially advancing the power of the inference and the settings in which the model can be used .",
    "first , figure [ fig : lnch_2 ] shows that the arrivals are significantly nonhomogeneous throughout the day , and modeling the arrival rate as constant throughout the day is likely the reason the baseline model failed the prediction task .",
    "the work in @xcite proposes extending their model to a nonhomogeneous setting by choosing sufficiently small time periods that the arrival rate can be approximated as piecewise constant .",
    "however , with the level of nonhomogeneity seen in figure [ fig : lnch_2 ] it is implausible that accurate estimation could be done for the number of segments ( and thus separate rate parameters ) required to model the arrival rate with a piecewise - constant function .",
    "second , our model does not require using the mnl choice model , which avoids the issue with the parameter @xmath140 being unidentifiable .",
    "this parameter represents the proportion of arrivals that do not purchase anything even when all items are in stock , and is not something that a retailer would necessarily know .",
    "finally , we take a bayesian approach to inference and produce posterior predictive distributions . this becomes especially important in this setting where the parameters themselves are of secondary interest to using the model to make predictions about lost revenue and to make decisions about stocking strategies .",
    "other work in this area includes @xcite , where customer arrivals are modeled with a homogeneous poisson process and purchase probabilities are modeled explicitly for each stock combination , as opposed to using a choice model .",
    "their model does not scale well to a large number of items as the likelihood expression includes all stock combinations found in the data .",
    "the work of @xcite is extended in @xcite to incorporate nonparametric choice models , for which maximum likelihood estimation becomes a large - scale concave program that must be solved via a mixed integer program subproblem .",
    "there is a large body of work on estimating demand and choice in settings different than that which we consider here , such as discrete time @xcite , panel or aggregate sales data @xcite , negligible no purchases @xcite , and online learning with simultaneous ordering decisions @xcite .",
    "these models and estimation procedures do not apply to the setting that we consider here , which is retail transaction data with stockouts and unobserved no - purchases ; @xcite provide a review of the various threads of research in the larger field of demand and choice estimation .",
    "our work fits into a growing body of work in advancing the use of statistics in areas of business .",
    "these areas include marketing @xcite , market analysis @xcite , demand forecasting @xcite , and pricing @xcite .",
    "these works , and ours , address a real need for rigorous statistical methodologies in business , as well as a substantial opportunity for impact .",
    "we have developed a bayesian model for inferring primary demand and consumer choice in the presence of stockouts .",
    "the model can incorporate a realistic model of the customer arrival rate , and is flexible enough to handle a variety of different choice models .",
    "our model is conceptually related to topic models like latent dirichlet allocation @xcite .",
    "variants of topic models are regularly applied to very large text corpora , with a large body of research on how to effectively infer these models . that research was the source of the stochastic gradient mcmc algorithm that we used , which allows inference from even very large transaction databases .",
    "the simulation study showed that when data were actually generated from the model , we were able to recover the true generating values .",
    "it further showed that the posterior bias and variance decreased as more data were made available , an improvement that came without any additional computational cost due to the stochastic gradient .    in the case study we applied the model and inference to real sales transaction data from a local bakery .",
    "the daily purchase rate in the data was clearly nonhomogeneous , with several peak periods .",
    "these data clearly demonstrated the importance of modeling nonhomogeneous arrival rates in retail settings . in a prediction task that required accurate modeling of both the arrival rate and the choice model",
    ", we showed that the model was able to make accurate predictions and significantly outperform the baseline approach .",
    "finally , we showed how the model can be used to estimate lost sales due to stockouts .",
    "the posterior provided evidence of substantial lost cookie sales .",
    "the model and inference procedure we have developed provide a new level of power and flexibility that will aid decision makers in using transaction data to make smarter decisions .",
    "we are grateful to the staff at 100 main marketplace at the massachusetts institute of technology who provided data for this study .",
    "b.  letham , w.  sun , and a.  sheopuri .",
    "latent variable copula inference for bundle pricing from retail transaction data . in _ proceedings of the 31st international conference on machine learning _ ,",
    "icml14 , 2014 .",
    "we consider the complete arrivals @xmath143 , which include both the observed arrivals @xmath144 as well as the unobserved arrivals that left as no - purchase , which we here denote @xmath145 .",
    "we define an indicator @xmath146 equal to @xmath5 if the customer at time @xmath60 purchased item @xmath5 , or @xmath2 if this customer left as no - purchase . for store @xmath7 and time period @xmath6 , @xmath147 \\mid \\boldsymbol{\\tilde{t}^{\\sigma , l } } , \\boldsymbol{\\eta}^{\\sigma}\\right ) \\\\ & \\quad \\times \\prod_{j=1}^{\\tilde{m}^{\\sigma , l } } p(\\tilde{t}^{\\sigma , l}_j \\mid \\tilde{t}^{\\sigma , l}_{<j } , \\boldsymbol{\\eta}^{\\sigma } ) p(\\tilde{i}^{\\sigma , l}_j",
    "\\mid \\tilde{t}^{\\sigma , l}_{<j } , \\boldsymbol{\\theta}^{\\sigma } , \\boldsymbol{\\phi } , \\boldsymbol{\\tau } , \\boldsymbol{n})\\\\ % & =   \\exp(-\\lambda(\\tilde{t}_{\\tilde{m}^{\\sigma , l}},t \\mid \\boldsymbol{\\eta}^{\\sigma } ) ) \\prod_{j=1}^{\\tilde{m}^{\\sigma , l } } p(\\tilde{t}^{\\sigma , l}_j \\mid \\tilde{t}^{\\sigma , l}_{j-1 } , \\boldsymbol{\\eta}^{\\sigma } ) \\pi_{\\tilde{i}^{\\sigma , l}_j}(\\tilde{t}^{\\sigma , l}_j)\\\\ & = \\exp(-\\lambda(\\tilde{t}_{\\tilde{m}^{\\sigma , l}},t \\mid \\boldsymbol{\\eta}^{\\sigma } ) ) \\\\ & \\quad \\times \\lambda(\\tilde{t}^{\\sigma , l}_1 \\mid \\boldsymbol{\\eta}^{\\sigma})\\exp(-\\lambda(0,\\tilde{t}^{\\sigma , l}_1 \\mid \\boldsymbol{\\eta}^{\\sigma } ) ) \\pi_{\\tilde{i}^{\\sigma , l}_1}(\\tilde{t}^{\\sigma , l}_1)\\\\ & \\quad \\times \\prod_{j=2}^{\\tilde{m}^{\\sigma , l } } \\lambda(\\tilde{t}^{\\sigma , l}_j \\mid \\boldsymbol{\\eta}^{\\sigma})\\exp(-\\lambda(\\tilde{t}^{\\sigma , l}_{j-1},\\tilde{t}^{\\sigma , l}_j \\mid \\boldsymbol{\\eta}^{\\sigma } ) ) \\pi_{\\tilde{i}^{\\sigma , l}_j}(\\tilde{t}^{\\sigma , l}_j)\\\\ & = \\exp(-\\lambda(0,t \\mid \\boldsymbol{\\eta}^{\\sigma } ) ) \\prod_{i=0}^n \\prod_{j : \\tilde{i}^{\\sigma , l}_j = i } \\lambda(\\tilde{t}^{\\sigma , l}_j \\mid \\boldsymbol{\\eta}^{\\sigma } ) \\pi_{i}(\\tilde{t}^{\\sigma , l}_j ) \\\\ & = \\exp(-\\lambda(0,t \\mid \\boldsymbol{\\eta}^{\\sigma } ) ) \\prod_{i=0}^n \\prod_{j=1}^{m^{\\sigma , l}_i } \\lambda(t^{\\sigma , l}_{i , j } \\mid \\boldsymbol{\\eta}^{\\sigma } ) \\pi_{i}(t^{\\sigma , l}_{i , j})\\\\ & = \\left(\\exp(-\\tilde{\\lambda}_0^{\\sigma , l}(0,t ) ) \\prod_{j=1}^{m^{\\sigma , l}_0 } \\tilde{\\lambda}_0^{\\sigma , l}(t^{\\sigma , l}_{0,j } )   \\right ) \\\\ & \\quad \\times \\left (   \\prod_{i=1}^n \\exp(-\\tilde{\\lambda}_i^{\\sigma , l}(0,t ) ) \\prod_{j=1}^{m^{\\sigma , l}_i } \\tilde{\\lambda}_i^{\\sigma , l}(t^{\\sigma , l}_{i , j } ) \\right).\\end{aligned}\\ ] ] we have then that @xmath148 since the last integrand is exactly the joint density for the arrivals from an nhpp with rate @xmath149 , and so integrates to @xmath47 .",
    "we now show how @xmath77 can be expressed analytically in terms of @xmath78 . for convenience , in this section we suppress in the notation the dependence of the stock on past arrivals and initial stock levels and will write @xmath76 as simply @xmath30 .",
    "we consider each of the time intervals where the stock @xmath30 is constant .",
    "let the sequence of times @xmath151 demarcate the intervals of constant stock .",
    "that is , @xmath25 = \\bigcup_{r=1}^{q^{\\sigma , l}-1 } [ q^{\\sigma , l}_r , q^{\\sigma , l}_{r+1}]$ ] and @xmath30 is constant for @xmath152 for @xmath153 .",
    "then , @xmath154 with this formula , the likelihood function can be computed for any parameterization @xmath155 desired so long as it is integrable ."
  ],
  "abstract_text": [
    "<S> when an item goes out of stock , sales transaction data no longer reflect the original customer demand , since some customers leave with no purchase while others substitute alternative products for the one that was out of stock . </S>",
    "<S> here we develop a bayesian hierarchical model for inferring the underlying customer arrival rate and choice model from sales transaction data and the corresponding stock levels . </S>",
    "<S> the model uses a nonhomogeneous poisson process to allow the arrival rate to vary throughout the day , and allows for a variety of choice models . </S>",
    "<S> model parameters are inferred using a stochastic gradient mcmc algorithm that can scale to large transaction databases . </S>",
    "<S> we fit the model to data from a local bakery and show that it is able to make accurate out - of - sample predictions , and to provide actionable insight into lost cookie sales . </S>"
  ]
}