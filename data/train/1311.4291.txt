{
  "article_text": [
    "the problem of recovering an unknown low - rank matrix @xmath2 from the linear constraint @xmath3 , where @xmath4 is the linear transformation and @xmath5 is the measurement , has been an active topic of recent research with a range of applications including collaborative filtering ( the netflix probolem ) @xcite , multi - task learning @xcite , system identification @xcite , and sensor localization @xcite .",
    "one method to solve this inverse problem is to solve the matrix rank minimization problem : @xmath6 which becomes a mathematical task of minimizing the rank of @xmath7 such that it satisfies the linear constraint . with the application of nuclear norm which is the tightest convex approach to the rank function",
    ", one can relax the non - convex np - hard problem ( [ pm1 ] ) to a tractable , convex one ( see * ? ? ?",
    "* ; * ? ? ?",
    "an alternative model of this inverse problem is the minimum rank approximation problem : @xmath8 where @xmath9 is known in advance , and @xmath10 is the true data to be reconstructed .",
    "the model in ( [ pm ] ) has been widely studied in the literature ( see * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "in fact , this formulation can not only work for the exact recovery case ( @xmath3 ) , but also suit for the noisy case ( @xmath11 ) , where @xmath12 denotes the noise by which the measurements are corrupted .",
    "although the model ( [ pm ] ) is based on a priori knowledge of the rank of @xmath10 , an incremental search over @xmath13 , which increases the complexity of the solution by at most factor @xmath13 , can be applied when the minimum rank @xmath13 is unknown .",
    "particularly , if an upper bound on @xmath13 is available , we can use a bisection search over @xmath13 since the minimum of ( [ pm ] ) is monotonously decreasing in @xmath13 .",
    "then the factor can reduce to @xmath14 . several effective algorithms based on ( [ pm ] )",
    "have been proposed , such as optspace @xcite , space evolution and transfer ( set ) @xcite , atomic decomposition for minimum rank approximation ( admira ) @xcite and the iterative hard thresholding ( iht ) introduced in @xcite . among these algorithms ,",
    "iterative hard thresholding algorithm is an easy - to - implement and fast method , which also shows the strong performance guarantees available with methods based on convex relaxation .",
    "recently , many researchers focus on the recovery problem in the high dimensional space , which has many applications in computer vision and graphics such as image inpainting @xcite and video inpainting . more specifically , by using the @xmath0-rank as a sparsity measure of a tensor ( or multidimensional array ) , this inverse problem can be transformed into the mathematical task of recovering an unknown low @xmath0-rank tensor @xmath15 from its linear measurements @xmath16 via a given linear transformation @xmath17 with @xmath18 .",
    "some related works can be found in @xcite , @xcite , @xcite , @xcite and @xcite . in all these studies",
    ", the authors mainly discussed the following tensor recovery model : @xmath19 where @xmath20 is the decision variable , @xmath21 is the mode-@xmath22 unfolding ( the notation will be given in section 2 ) of @xmath23 , @xmath24 s are the weighted parameters which satisfy @xmath25 and @xmath26 .",
    "note that ( [ p11 ] ) can be regarded as an extension of ( [ pm1 ] ) in the high dimensional space @xmath27 and it is a difficult non - convex problem due to the combination nature of the function rank(@xmath28 ) . in order to solve it , the common method is replacing rank(@xmath28 ) by its convex envelope to get a convex tractable approximation and developing effective algorithms to solve the convex approximation , including fp - lrtc ( fixed point continuation method for low @xmath0-rank tensor completion ) @xcite , tensor - hc ( hard completion ) @xcite , and adm - tr(e ) ( alternative direction method algorithm for low-@xmath0-rank tensor recovery ) @xcite .",
    "additionally , @xcite investigated the exact recovery conditions for the low @xmath0-rank tensor recovery problems via its convex relaxation . and lately , @xcite studied the problem of robust low @xmath0-rank tensor recovery in a convex optimization framework , drawing upon recent advances in robust principal component analysis and tensor completion .    in this paper",
    ", we consider a new alternative recovery model extended from problem ( [ pm ] ) , which is called as _",
    "minimum @xmath0-rank approximation _",
    "( mnra ) : @xmath29 where @xmath30 is the @xmath0-rank of the ture data @xmath31 to be restored .",
    "note that this formulation has not been discussed in tensor space in the literature to our knowledge and it also includes both the noisy case ( @xmath32 ) and noiseless case ( @xmath16 ) .",
    "one of its special cases is the _",
    "low @xmath0-rank tensor completion _ ( lrtc )",
    "problem : @xmath33 where @xmath23 , @xmath34 are @xmath35-way tensors with identical size in each mode , and @xmath36 ( or @xmath37 ) denotes the tensor whose @xmath38-th component equal to @xmath39 ( or @xmath40 ) if @xmath41 and zero otherwise . to solve ( [ p1 ] )",
    ", we propose an iterative hard thresholding algorithm , which is easy to implement and very fast .",
    "particularly , we prove that for the noiseless case the iterative sequence generated by the proposed algorithm is globally linearly convergent to the true data @xmath31 with the rate @xmath1 under some conditions , while for the noisy case the distance between the iterative sequence and the true data @xmath31 is decreased quickly associated with the noise @xmath12 .",
    "additionally , combining an effective heuristic for determining @xmath0-rank , we can also apply the proposed algorithm to solve mnra when @xmath0-rank of @xmath31 is unknown in advance .",
    "some preliminary numerical results are reported and demonstrate the efficiency of the proposed algorithms .",
    "the rest of our paper is organized as follows . in section 2",
    ", we first briefly introduce some preliminary knowledge of tensor .",
    "then , we propose an iterative hard thresholding algorithm to solve the minimum @xmath0-rank approximation problem in section 3 and the convergence analysis of the proposed algorithm will be presented in section 4 . in section 5 and section 6 ,",
    "we give some implementation details and report some preliminary numerical results for low @xmath0-rank tensor completion , respectively .",
    "conclusions are given in the last section .",
    "in this section , we briefly introduce some essential nomenclatures and notations used in this paper ; and more details can be found in @xcite .",
    "scalars are denoted by lowercase letters , e.g. , @xmath42 ; vectors by bold lowercase letters , e.g. , @xmath43 ; and matrices by uppercase letters , e.g. , @xmath44 .",
    "an _ n_-way tensor is denoted as @xmath45 , whose elements are denoted as @xmath46 , where @xmath47 and @xmath48 .",
    "let us denote tensor space by @xmath49 for convenience , i.e. , @xmath50 .",
    "then , for any @xmath51 , the inner product is defined as @xmath52 obviously , the tensor space @xmath53 becomes a hilbert space with the above definition of the inner product , and the corresponding frobenius - norm is @xmath54 .",
    "the mode-@xmath22 fibers are all vectors @xmath55 obtained by fixing the indexes of @xmath56 , which are analogue of matrix rows and columns .",
    "the mode-@xmath22 unfolding of @xmath57 , denoted by @xmath21 , arranges the mode-@xmath22 fibers to be the columns of the resulting matrix .",
    "the tensor element @xmath58 is mapped to the matrix element @xmath59 , where @xmath60 which infers @xmath61 , where @xmath62 .",
    "we also follow @xcite to define the @xmath0-rank of a tensor @xmath63 by @xmath64 in the following parts of this paper , we say @xmath65 is an ( @xmath66)-rank tensor , if for any @xmath22 , the rank of mode-@xmath22 unfolding of @xmath65 is not greater than @xmath67 , i.e. , @xmath68 for all @xmath22 . it should be pointed out that this definition is different from the notation of a  rank-(@xmath66 ) tensor \" in @xcite , which represents a tensor with the rank of each mode-@xmath22 unfolding is exactly @xmath67 . the best ( @xmath69)-rank approximation @xmath70 of a tensor @xmath65",
    "is defined as the following : @xmath71    the @xmath22-mode ( matrix ) product of a tensor @xmath72 with a matrix @xmath73 is denoted by @xmath74 and is of size @xmath75 .",
    "it can be expressed in terms of unfolded tensors : @xmath76    additionally , for any transformation @xmath77 , @xmath78 denotes the operator norm of the transformation @xmath77 ; and for any vector @xmath79 , we use diag@xmath80 to denote a diagonal matrix with its @xmath22-th diagonal element being @xmath81 .",
    "in this section , we will derive an iterative hard thresholding algorithm to solve problem ( [ p1 ] ) . as a fast and efficient algorithm ,",
    "iterative hard thresholding algorithm has been widely applied in various fields . @xcite and @xcite first independently proposed the iterative hard thresholding algorithm to solve the compressed sensing recovery problem .",
    "later , blumensath and davies presented a theoretical analysis of the iterative hard thresholding algorithm when applied to the compressed sensing recovery problem in @xcite . through the analysis",
    ", they showed that the simple iterative hard thresholding algorithm has several good properties , including near - optimal error guarantees , robustness to observation noise , short memory requirement and low computational complexity .",
    "also , it requires a fixed number of iterations and its performance guarantees are uniform .",
    "recently , @xcite used acceleration methods of choosing the step - size appropriately to improve the convergence speed of the iterative hard thresholding algorithm .",
    "when it came to matrix space from vector space , @xcite studied the convergence/ recoverability properties of the fixed point continuation algorithm and its variants for matrix rank minimization .",
    "particularly , in @xcite , the authors proposed an iterative hard thresholdinging algorithm and discussed its convergence . at each iteration of their iterative hard thresholding algorithm ,",
    "the authors first performed a gradient step @xmath82 where @xmath83 denotes the @xmath84-th iteration of @xmath7 , @xmath85 denotes the ( @xmath86)-th iteration of @xmath87 and @xmath88 is the adjoint operator of @xmath89 that is a linear transformation operating from @xmath90 to @xmath49 . then , they applied hard thresholding operator to the singular values of @xmath85 , i.e. , they only kept the largest @xmath13 singular values of @xmath85 , to get @xmath91 .",
    "it is easy to see that @xmath91 is actually the best @xmath13-rank approximation to @xmath85 . more specifically , by using @xmath92 to denote the hard thresholding operator with threshold @xmath13 for @xmath7 , the iterative scheme of their algorithm is as follows : @xmath93 lately , in @xcite",
    ", they studied acceleration schemes via memory - based techniques and randomized , @xmath12-approximate matrix projections to decrease the computational costs in the recovery process . in this paper , inspired by the work of @xcite",
    ", we will develop an iterative hard thresholding algorithm for minimum @xmath0-rank approximation ( [ p1 ] ) . in the following , we will do some theoretical analysis of problem ( [ p1 ] ) in order to derive the iterative scheme of our algorithm .",
    "firstly , we consider the objective function @xmath94 in ( [ p1 ] ) .",
    "similar to that in @xcite , we introduce a surrogate objective function @xmath95 instead of function @xmath96 : @xmath97 where @xmath98 , @xmath99 $ ] , @xmath26 and @xmath100 are auxiliary variables in the domain of function @xmath101 .",
    "it is obvious that @xmath102 and if @xmath103 , @xmath104 , @xmath105 for all @xmath106 , where @xmath78 denotes the operator norm of linear operator @xmath77 .",
    "so , function @xmath101 is said to majorize @xmath96 .",
    "let @xmath107 be the @xmath84-th iteration and the @xmath108-th iteration @xmath109 be the minimum of the function @xmath101 by setting its later @xmath35 variables to @xmath110 , i.e. , @xmath111 . if @xmath103 , we have @xmath112 where the first inequality follows from the assumption that @xmath103 , and the second inequality follows from that @xmath109 is the minimum of @xmath113 .",
    "thus , it can be clearly seen that if @xmath103 , fixing the latter @xmath35 variables in ( [ sfunction ] ) and optimizing ( [ sfunction ] ) with respect to the first variable will then decrease the value of the original objective function @xmath96 . in other words , if @xmath103 , the iterative scheme solving problem ( [ p1 ] ) could be : @xmath114 where @xmath115 denotes the set @xmath116 .",
    "note that , @xmath117 can be written as : @xmath118 then , it is easy to see that the solution of the following problem ( without the constraints @xmath119 for any @xmath120 ) : @xmath121 is @xmath122 and the value of @xmath101 at this time is equal to @xmath123 therefore , the minimum of @xmath124 with the constraints @xmath119 for any @xmath120 is then achieved at the best rank-@xmath125 approximation of @xmath126 , i.e. , @xmath127 where @xmath128 means the best rank-@xmath125 approximation of @xmath129 .",
    "however , for a tensor @xmath129 , its best rank-@xmath125 approximation is hard to be obtained in general .",
    "thus , here we use another form to replace the exact best @xmath125-rank approximation .",
    "our method is first to compute the best rank-@xmath67 approximation of @xmath130 for each @xmath22 , then update @xmath126 by the convex combination of the refoldings of these rank-@xmath67 matrices , i.e. , @xmath131 where @xmath132 denotes the mode-@xmath22 unfolding of a tensor @xmath63 for any @xmath133 : @xmath134 and @xmath135 denotes the adjoint operator of @xmath136 .",
    "now , we are ready to present the iterative hard thresholding algorithm for solving ( [ p1 ] ) as below and its convergence analysis will be presented in the next section .    [ cols=\"<\",options=\"header \" , ]",
    "in this section , we concentrate on the convergence of algorithm 3.1 . let @xmath137 with @xmath138 being the true data to be restored , and it is known that @xmath139 is an @xmath140-rank tensor , i.e. , the rank of the mode-@xmath22 unfolding of @xmath139 is not greater than @xmath67 for each @xmath141 .",
    "algorithm 3.1 is used to recover the true data @xmath139 .",
    "next , we will prove the following inequality to characterize the performance of the proposed algorithm : @xmath142 where @xmath107 denotes the @xmath84-th iteration generated by algorithm 3.1 and @xmath143 denotes the rate at which the sequence converges to @xmath139 .",
    "the analysis begins by giving the following concepts , including the restricted isometry constant ( ric ) of a linear transformation and singular value decomposition ( svd ) basis of a matrix .",
    "[ df - t - ric ] let @xmath144 .",
    "the restricted isometry constant @xmath145 of a linear transformation @xmath146 @xmath147 with order @xmath148 is defined as the smallest constant such that @xmath149 holds for any @xmath150-rank tensor @xmath23 , i.e. , rank@xmath151 for all @xmath141 .",
    "assume that the @xmath13-rank matrix @xmath152 has the svd @xmath153 .",
    "@xmath154 is called an svd basis for the matrix @xmath152 .",
    "it s easy to see that the elements in the subspace spanned by the svd basis are all @xmath13-rank matrices . based on these definitions , we give the following important lemma , which paves the way towards the convergence of algorithm 3.1 .",
    "[ lemma - con ] suppose @xmath155 is the best @xmath13-rank approximation to the matrix @xmath87 and @xmath156 is an svd basis of @xmath7",
    ". then , for any @xmath13-rank matrix @xmath152 and svd basis @xmath157 of @xmath152 , we have @xmath158 where @xmath159 is any orthonormal set of matrices satisfying @xmath160 , and @xmath161 is the projection of @xmath7 onto the subspace spanned by @xmath159 ..    now we prove the convergence of algorithm 3.1 under proper conditions .",
    "[ convergence1 ] let @xmath138 be the original data to be restored with @xmath137 , and @xmath139 is an @xmath140-rank tensor .",
    "set @xmath162 and @xmath163 , where @xmath164 means rounding up .",
    "suppose that @xmath165 , and let @xmath166 be the ric of @xmath89 with order @xmath167 and @xmath168 . if @xmath169 , then the iterative sequence @xmath170 generated by algorithm 3.1 is linearly convergent to the original data @xmath139 with rate @xmath1 , i.e. , @xmath171 moreover , iterating the above inequality , we have @xmath172    to facilitate , we denote @xmath173 and @xmath174 for all @xmath175 in the proof . since @xmath138 is a @xmath140-rank tensor , we have @xmath176 is an @xmath67-rank matrix , i.e. , the rank of @xmath176 is not greater than @xmath67 .",
    "note that from algorithm 3.1 , @xmath177 is also an @xmath67-rank matrix for all @xmath178 .",
    "thus , for each @xmath175 , there exist the svd basises for @xmath176 and @xmath179 , denoted by @xmath180 and @xmath181 , respectively . and",
    "let @xmath182 denote an orthonormal basis of the subspace span@xmath183 .",
    "then the subspace spanned by @xmath182 , containing @xmath176 and @xmath179 , is a set of @xmath184-rank matrices . setting @xmath185 to be the projection of @xmath186 onto the subspace spanned by @xmath182 .",
    "then , @xmath187 .    based on the aforementioned notations and the iterative scheme of algorithm 3.1",
    ", we have @xmath188 where the first and third inequality follow from the triangle inequality , the second equality follows from @xmath189 , and the last inequality follows from lemma [ lemma - con ] .",
    "furthermore , for each index @xmath175 , we have @xmath190 therefore , in order to prove ( [ thm-0 ] ) , i.e. , @xmath191 , we need to estimate the upper bounds of the three terms in the right - hand side of ( [ thm-2 ] ) , respectively .",
    "the specifical analysis is as follows :    * ( * estimation on the upper bound of the term * @xmath192 ) + utilizing the non - expansion of projection operator , it s simple to estimate an upper bound of term @xmath193 in the right - hand side of ( [ thm-2 ] ) .",
    "this is given by @xmath194 ) + note that , for any matrix @xmath195 with appropriate size , @xmath187 .",
    "thus , @xmath196 is a @xmath197-rank tensor , where @xmath198 .",
    "then , based on ( [ df - t - ric ] ) , @xmath199 which implies that @xmath200 therefore , we can obtain that @xmath201 * ( * estimation on the upper bound of * @xmath202 ) + let the svd of @xmath203 be @xmath204 , where @xmath205 is the vector of the singular values of @xmath203 with @xmath206 . we decompose @xmath207 into a sum of vectors @xmath208 , where disjoint index sets @xmath209 and the sparsity of each @xmath210 is @xmath67 ( except possibly @xmath211 ) . then , @xmath212 is the part of @xmath203 corresponding to the @xmath67 largest singular values , @xmath213 is the part corresponding to the next @xmath67 largest singular values , and so on .",
    "thus , we have @xmath214 , where @xmath215 is an @xmath67-rank matrix , @xmath216 , and @xmath162 .",
    "then , we have @xmath217 where the first inequality follows from the triangle inequality , and the second inequality follows from the following facts : @xmath218    therefore , by utilizing the results of items ( a ) , ( b ) , and ( c ) , i.e. , by combining ( [ thm-3 ] ) , ( [ thm-4 ] ) and ( [ thm-5 ] ) , we can further obtain that for each index @xmath175 @xmath219 then , let @xmath220 and @xmath221 . substituting ( [ thm-6 ] ) into ( [ thm-1 ] ) , we have @xmath222 where the second inequality follows from that fact that @xmath223 for all @xmath22 .    by the assumption that @xmath169 , we have @xmath224 iterating this inequality , we obtain @xmath172    the proof is complete .    *",
    "remark : * note that we use a parameter @xmath98 as the step - size in algorithm 3.1 .",
    "actually , @xmath225 is scoped . in the conditions of theorem [ convergence1 ] , we assume @xmath226 to ensure that @xmath227 , since @xmath228 . actually , observing ( [ thm-7 ] ) given in the proof of theorem [ convergence1 ] , in order to ensure the convergence of the iterative sequence , we only need @xmath229 , which implies that @xmath230 .",
    "thus , @xmath228 implies that @xmath231 , which is enough to guarantee the convergence of algorithm 3.1 .",
    "note that theorem [ convergence1 ] considers the exact recovery case .",
    "however , it is possible to apply algorithm 3.1 to recover the data corrupted by noise .",
    "next , we will give the recoverability result of algorithm 3.1 for the noisy case .",
    "[ convergence2 ] let @xmath138 be the original data to be restored with @xmath232 , where @xmath233 denotes the noise , and @xmath139 is an @xmath140-rank tensor .",
    "set @xmath162 and @xmath163 , where @xmath164 means rounding up .",
    "suppose that @xmath165 , and let @xmath166 be the ric of @xmath89 with order @xmath167 and @xmath168 . if @xmath169 , then algorithm 3.1 will recover an approximation @xmath234 satisfying @xmath235 where @xmath236 is a constant , which only depends on @xmath225 , @xmath67 and @xmath237 ( @xmath238 ) .    the proof is similar to the one of theorem [ convergence1 ] .",
    "the main difference is to add one term involving the noise @xmath12 .",
    "first , we can also obtain ( [ thm-1 ] ) , i.e. , @xmath239 in the noisy case ( @xmath232 ) , we have the following result with similar deduction to ( [ thm-2 ] ) , which only adds one term about @xmath12 : @xmath240 the number of the right - hand terms increases 1 , but the estimation of the remaining three terms are the same with ( a ) , ( b ) , ( c ) in the proof of theorem [ convergence1 ] .",
    "thus , we just need to estimate the additional term @xmath241 .",
    "@xmath242 where the first inequality follows from the cauchy - schwarz inequality , the second inequality follows definition [ df - t - ric ] and the fact that @xmath243 is a @xmath197 tensor , where @xmath198 , and the third inequality follows that @xmath244 and @xmath245 , where @xmath246 denotes the operator norm of @xmath135 .",
    "then , by using ( a ) , ( b ) , ( c ) in the proof of theorem [ convergence1 ] and ( [ thm-23 ] ) , we have @xmath247 substituting ( [ thm-24 ] ) into ( [ thm-21 ] ) and setting @xmath220 , @xmath221 , we can obtain @xmath248 then , by the assumption that @xmath169 , we can obtain @xmath249 where @xmath236 is a constant which only depends on @xmath225 , @xmath67 and @xmath237 ( @xmath250 ) .",
    "iterating this inequality , we have @xmath251    the proof is complete .",
    "_ problem settings_. the random low @xmath0-rank tensor completion problems without noise we considered in our numerical experiments are generated as in @xcite , @xcite and @xcite . for creating a tensor @xmath252 with @xmath0-rank @xmath30 ,",
    "we first generate a core tensor @xmath253 with i.i.d .",
    "gaussian entries ( @xmath254 ) . then , we generate matrixes @xmath255 , with @xmath256 whose entries are i.i.d . from @xmath257 and set @xmath258 with this construction , the @xmath0-rank of @xmath34 equals @xmath30 almost surely .",
    "we also conduct numerical experiments on random low @xmath0-rank tensor completion problems with noisy data . for the noisy random low @xmath0-rank tensor",
    "completion problems , the tensor @xmath252 is corrupted by a noise tensor @xmath259 with independent normally distributed entries .",
    "then , @xmath34 is taken to be @xmath260 where @xmath207 is the noise level .",
    "we use @xmath261 to denote the sampling ratio , i.e. , a percentage @xmath261 of the entries to be known and choose the support of the known entries uniformly at random among all supports of size @xmath262 . the values and the locations of the known entries of @xmath34",
    "are used as input for the algorithms .",
    "+ _ predicting @xmath0-rank_. in practice , the @xmath0-rank of the optimal solution is usually unknown .",
    "thus , we need to estimate the @xmath0-rank appropriately during the iterations . inspired by the work @xcite , we propose a heuristic for determining @xmath0-rank @xmath263 .",
    "we start with @xmath264 . in the @xmath84-th iteration ( @xmath265 ) , for each @xmath22 , we first choose @xmath67 as the number of singular values of @xmath266 which are greater than @xmath267 , where @xmath268 is the largest singular value of @xmath266 and @xmath269 is a given tolerance .",
    "since the given tolerance sometimes truncates too many singular values , we need to increase @xmath67 occasionally .",
    "note that from the iterative scheme ( [ scheme1 ] ) , we have that @xmath270 at the optimal point @xmath271 .",
    "thus , we increase @xmath67 by 1 whenever the frobenius norm of @xmath272 increased .",
    "our numerical experience indicates the efficiency of this heuristic for determining @xmath263 .",
    "+ _ singular value decomposition_. computing singular value decomposition is the main computational cost even if we use a state - of - the - art code propack @xcite , especially when the rank is relatively large .",
    "therefore , for random low @xmath0-rank tensor completion problems without noise , we use the monte carlo algorithm lineartimesvd developed by @xcite to compute an approximate svd , which was also used in @xcite , @xcite and @xcite to reduce the computational cost .",
    "this lineartimesvd algorithm returns an approximation to the largest @xmath273 singular values and the corresponding left singular vectors of a matrix @xmath274 in linear @xmath275 time .",
    "we outline it below .",
    "thus , the outputs @xmath276 , @xmath277 are approximations to the largest @xmath273 singular values and @xmath278 , @xmath277 are approximations to the corresponding left singular vectors of the matrix @xmath279 .",
    "the parameter settings we used in lineartimesvd algorithm are similar to those in @xcite .",
    "to balance the computational time and accuracy of svd of @xmath280 , we choose a suitable @xmath281 with @xmath62 for each mode-@xmath22 .",
    "all @xmath282 s are set to @xmath283 for simplicity .",
    "for the predetermined parameter @xmath273 , in the @xmath84-th iteration , we let @xmath273 equal to the predetermined @xmath67 for each mode-@xmath22 .    on the other hand , for random low @xmath0-rank tensor completion problems with noisy data , to guarantee the accuracy of the solution",
    ", we will use the matlab command @xmath284 = \\mathrm{svd}(x,'\\mathrm{econ}')$ ] to compute full svd in our algorithms although it may cost more time than the lineartimesvd algorithm does .",
    "in this section , we apply algorithm 3.1 to solve the low @xmath0-rank tensor completion problem ( [ p - add1 ] ) .",
    "we use ihtr - lrtc to denote the algorithm in which the @xmath0-rank is specified , and iht - lrtc to denote the algorithm in which the @xmath0-rank is determined by the heuristic described in section 5 .",
    "we test ihtr - lrtc and iht - lrtc on both simulated and real world data with the missing data , and compare them with the latest tensor completion algorithms , including fp - lrtc @xcite , tensor - hc @xcite , adm - tr(e ) @xcite and horpca ( higher - order robust principal component analysis ) @xcite . the tucker decomposition algorithm based on the idea of alternating least squares from the _ n - way toolbox for matlab _",
    "@xcite is also included , for which we use the correct @xmath0-rank @xmath285 (  n - way - e \" ) and the higher @xmath0-rank @xmath286 (  n - way - ie \" ) .",
    "all numerical experiments are run in matlab 7.14 on a hp z800 workstation with an intel xeon(r ) 3.33ghz cpu and 48 gb of ram .    for random low @xmath0-rank tensor completion problems without noise , the relative error @xmath287",
    "is used to estimate the closeness of @xmath288 to @xmath34 , where @xmath288 is the  optimal \" solution produced by the algorithms and @xmath34 is the original tensor .    for random low @xmath0-rank tensor completion problems with noisy data ,",
    "we follow @xcite to measure the performance by the normalized root mean square error ( nrmse ) on the complementary set @xmath289 : @xmath290 where @xmath291 is as in ( [ noise ] ) and @xmath292 denotes the cardinality of @xmath289 .",
    "the stopping criterion we used for ihtr - lrtc and iht - lrtc in all our numerical experiments is as follows : @xmath293 where tol is a moderately small number , since when @xmath294 gets close to an optimal solution @xmath295 , the distance between @xmath294 and @xmath126 should become very small .    in ihtr - lrtc and iht - lrtc , we choose the initial iteration to be @xmath296 and set @xmath297 .",
    "the weighted parameters @xmath24 are set to @xmath298 for simplicity .",
    "additionally , the parameter @xmath299 in predicting @xmath0-rank is set to @xmath300 for noiseless cases and @xmath301 for noisy cases . in fp - lrtc",
    ", we set @xmath302 , @xmath303 , @xmath304 , @xmath305 , @xmath306 . in tensor - hc , we set the regularization parameters @xmath307 , @xmath120 to @xmath308 and @xmath225 to @xmath309 . in adm - tr(e ) , the parameters are set to @xmath310 . for horpca , we follow @xcite to keep @xmath311 constant and set @xmath312 .",
    "the regularization parameter is different from that in authors paper .",
    "it is given by authors in their matlab code for tensor completion , which can be downloaded from https://sites.google.com/site/tonyqin/research . ]",
    "it is stopped when the maximum of the relative primal and dual residuals decreased to below @xmath314 .    in fig.1 , we first numerically compare the recovery results with different values of @xmath225 by testing ihtr - lrtc and iht - lrtc on random noiseless low @xmath0-rank tensor completion problems with the tensor of size @xmath315 and @xmath0-rank @xmath316 .",
    "the sampling ratio is set to 0.3 and 0.6 , respectively .",
    "it s worth noting that though the assumption @xmath231 is given for ensuring convergence by theoretical analysis , we find that ihtr - lrtc and iht - lrtc can be convergent with choosing @xmath225 in a more broad interval , which can be seen in the figure ( @xmath225 is chosen from @xmath317 to @xmath318 ) .",
    "additionally , it is obvious that the larger @xmath225 becomes , the less time it costs to recover a tensor with lower relative error .",
    "therefore , considering these situations , we can choose a larger @xmath225 to guarantee the low error and less iterations .",
    "specifically , we will set @xmath319 for the remaining tests in this paper .",
    "then , we compare ihtr - lrtc with iht - lrtc on random noiseless low @xmath0-rank tensor completion problems with the tensor of size @xmath320 and @xmath0-rank @xmath321 .",
    "the sampling ratio is set to 0.3 and 0.6 , respectively .",
    "we plot the logarithm of the relative error between the @xmath294 and the true tensor @xmath34 versus the iteration number for algorithms ihtr - lrtc and iht - lrtc in fig.2 for each problem setting . from this figure",
    ", we can see that iht - lrtc decreases @xmath322 slower than ihtr - lrtc due to the heuristic of determining @xmath13 . additionally , for ihtr - lrtc",
    ", log@xmath323 is approximately a linear function of the iteration number @xmath84 ; for iht - lrtc , it also approximately a linear function after several iterations .",
    "this implies that the theoretical results in theorem [ convergence1 ] approximately hold in practice .",
    "-rank tensor completion problems with the tensor of size @xmath320 and @xmath0-rank @xmath321 .",
    "the sampling ratio is set to 0.3 and 0.6 , respectively.,height=226 ]     |l|llcl|l|llcl|   + & & & & & & & & & + & & & & & & & & & + & & & & & & & & & + & & & & & & & & & + & & & & & & & & & + & & & & & & & & & + & & & & & & & & & + & & & & & & & & & + & & & & & & & & & +    & & & & & & & & & + & & & & & & & & & + & & & & & & & & & + & & & & & & & & & + & & & & & & & & & + & & & & & & & & & + & & & & & & & & & + & & & & & & & & & +    & & & & & & & & & + & & & & & & & & & + & & & & & & & & & + & & & & & & & & & + & & & & & & & & & + & & & & & & & & & + & & & & & & & & & + & & & & & & & & & +    table 1 presents the different settings for random noiseless low @xmath0-rank tensor completion problems and the recovery performance of different algorithms .",
    "the order of the tensors varies from three to five , and we also vary the @xmath0-rank and the sampling ratio @xmath261 . for each problem setting ,",
    "we solve 10 randomly created tensor completion problems .",
    "iter , rel.err and time(s ) stands for the average iterations , the average relative error and the average time ( seconds ) for each problem setting , respectively . from the results in table 1",
    ", we can easily see that it costs less time with lower @xmath0-rank and higher sampling ratio @xmath261 . by comparing the results of different algorithms ,",
    "it is easy to see that ihtr - lrtc and iht - lrtc always perform better than other algorithms in both relative error and cpu time .",
    "note that though iht - lrtc converges a little slower than ihtr - lrtc since it needs more iterations and time to determine @xmath0-rank , the recoverability of iht - lrtc can be comparable with that of ihtr - lrtc , which indicates the efficiency of the heuristic for determining @xmath0-rank . for the problem with relative large size ( e.g. , @xmath324 , @xmath325 , @xmath326 ) , we can see that ihtr - lrtc and iht - lrtc can save much more time to recover a tensor .",
    "additionally , it s worth noting that n - way - e also has a good performance for all the problem settings , but n - way - ie performs poorly for these problems though we just use a little higher @xmath0-rank .",
    "this situation indicates that the _ n - way toolbox _ depends strongly on the knowledge of the @xmath0-rank and the tensor",
    "may no longer be recovered with the inexact @xmath0-rank .    then , we test the first seven different algorithms ( n - way - ie is poorer than other algorithms obviously by table 1 ) on random noiseless low @xmath0-rank tensor completion problems with the tensor of fixed size @xmath327 and different @xmath0-ranks @xmath328 ( here we set @xmath329 for convenience ) . fig.3 depict the average results of 10 independent trials corresponding to different @xmath0-rank @xmath328 for randomly created noiseless tensor completion problems .",
    "the sampling ratios is set to @xmath330 . as indicated in fig.3 ,",
    "ihtr - lrtc and iht - lrtc are always faster and more robust than others , and provide the solutions with lower relative error .",
    "we further test the algorithms on random noisy low @xmath0-rank tensor completion problems .",
    "table * 2 * presents the numerical performance . in the table",
    ", we report the mean of nrmses , iterations and execution times over 10 independent trials .",
    "then , we set the noise level @xmath331 . from the results , we can easily see that ihtr - lrtc and iht - lrtc are comparable with other algorithms in terms of nrmse and cpu time .",
    "|l|llcl|l|llcl|   + & & & & & & & & & + & & & & & & & & & + & & & & & & & & & + & & & & & & & & & + & & & & & & & & & + & & & & & & & & & + & & & & & & & & & + & & & & & & & & & + & & & & & & & & & +    & & & & & & & & & + & & & & & & & & & + & & & & & & & & & + & & & & & & & & & + & & & & & & & & & + & & & & & & & & & + & & & & & & & & & + & & & & & & & & & +    & & & & & & & & & + & & & & & & & & & + & & & & & & & & & + & & & & & & & & & + & & & & & & & & & + & & & & & & & & & + & & & & & & & & & + & & & & & & & & & +    _ inpainting of color images via low @xmath0-rank tensor completion_. next , we further evaluate the performance of ihtr - lrtc and iht - lrtc on image inpainting @xcite .",
    "color images can be expressed as third - order tensors .",
    "if the image is of low @xmath0-rank , or numerical low @xmath0-rank , we can solve the image inpainting problem as a low @xmath0-rank tensor recovery problem . in our test ,",
    "we first compute the best rank-@xmath332 approximation of a color image to obtain an numerical low @xmath0-rank image .",
    "then , we randomly remove the values of some of the pixels of the numerical low @xmath0-rank image , and want to fill in these missing values .",
    "* remark * : the best rank-@xmath333 approximation is used as a tool for dimensionality reduction and signal subspace estimation . several algorithms for this purpose have been proposed in the literature , e.g. , the higher - order orthogonal iteration ( hooi ) @xcite . more details can be seen in @xcite .",
    "note that the _ n - way toolbox for matlab _ is also an effective and convenient tool of computing the best rank-@xmath333 approximation .",
    "however , considering to be fair and reasonable , we here use the _ matlab tensor toolbox _ by @xcite , which is an another famous tool for tensor computation , to compute the best rank-@xmath333 approximation .",
    "using matlab notation , for a tensor @xmath334 , @xmath335)$ ] returns the best rank-@xmath333 approximation of @xmath23 . additionally , the parameter @xmath299 in predicting @xmath0-rank is set to @xmath336 to guarantee the better prediction of @xmath0-rank for the practical problems .",
    "fig.4 and fig.5 respectively present the recovered images for the best rank-@xmath337 and rank-@xmath338 approximation of the original @xmath339 image by different algorithms ( here , adm - tr(e ) and n - way - ie perform poorer than others obviously , so their results are no longer reported ) .",
    "the sampling ratio is set to 0.3 .",
    "we also report the numerical results in table 3 .",
    "although the recovered images of these five algorithms are similar visually to each other , the results in table 3 show that ihtr - lrtc and iht - lrtc are more effective than others , especially for the problem with high @xmath0-rank .",
    "more specifically , for the best rank-@xmath337 approximation of the original image , all the algorithms can recover the image well by using only 30% of pixels and ihtr - lrtc is much faster than others .",
    "for the best rank-@xmath338 approximation of the original image , we can see that the relative errors of recovered images by fp - lrtc , tensor - hc and horpca are very large due to the relatively high @xmath0-rank .",
    "however , ihtr - lrtc and iht - lrtc can also perform well .",
    "+   +      +   +",
    "in this paper , we considered a new alternative recovery model ` mnra ' and proposed an appropriate iterative hard thresholding algorithm to solve it with giving upper bound of @xmath0-rank in advance .",
    "the convergence analysis of the proposed algorithm was also presented . by using an effective heuristic of determining @xmath0-rank",
    ", we can also apply the proposed algorithm to solve mnra with unknown @xmath0-rank in advance .",
    "some preliminary numerical results on lrtc were reported . through the theoretical analysis and numerical experiments",
    ", we can draw some encouraging conclusions :    * the model of mnra in this paper is creative in low @xmath0-rank tensor recovery .",
    "mnra includes both noiseless and noisy case .",
    "and although the model needs the @xmath0-rank of the original data as prior information , we have proposed a heuristic for determining @xmath0-rank and this method turned to be efficient . *",
    "the iterative hard thresholding algorithm proposed in this paper is easy to implement .",
    "it has a very simple iterative scheme and only one parameter @xmath225 , which can be easily estimated from theoretical analysis and can be chosen broadly in practice . *",
    "the iterative hard thresholding algorithm is extremely fast .",
    "actually , the iterative sequence generated by the proposed algorithm is globally linearly convergent with the rate @xmath1 for the noiseless case . in our numerical experiments , these theoretical results can be confirmed .",
    "* ihtr - lrtc and iht - lrtc are still effective for the tensor with high @xmath0-rank .",
    "thus , they may have wider applications in practice .",
    "it is interesting to investigate how to determine @xmath0-rank more effectively in practice .",
    "we believe that the iterative hard thresholding algorithm combining with the appropriate method for predicting @xmath0-rank can be used to solve more general tensor optimization problems .",
    "moreover , the nonconvex sparse optimization problems and the related algorithms in vector or matrix space have been widely discussed in the literature @xcite .",
    "it is worth investigating how to apply the iterative hard thresholding algorithm to solve the nonconvex model in the tensor space .",
    "we would like to thank silvia gandy for sending us the code of adm - tr(e ) , and thank marco signoretto for sending us the code of tensor - hc .",
    "this work was partially supported by the national natural science foundation of china ( grant no .",
    "11171252 and no.11201332 ) .",
    "shi , z. q. , han , j. q. , zheng , t. r. & li , j.(2013 ) guarantees of augmented trace norm models in tensor recovery _",
    "proceedings of the twenty - third international joint conference on artificial intelligence _ , aaai press , 16701676    signoretto , m. , lathauwer , l. de . & suykens , j. a. k. ( 2010 ) nuclear norms for tensors and their use for convex multilinear estimation _ internal report 10 - 186 , esat - sista , k.u .",
    "leuven , leuven , belgium .",
    "lirias number : 270741 _"
  ],
  "abstract_text": [
    "<S> the problem of recovering a low @xmath0-rank tensor is an extension of sparse recovery problem from the low dimensional space ( matrix space ) to the high dimensional space ( tensor space ) and has many applications in computer vision and graphics such as image inpainting and video inpainting . in this paper </S>",
    "<S> , we consider a new tensor recovery model , named as minimum @xmath0-rank approximation ( mnra ) , and propose an appropriate iterative hard thresholding algorithm with giving the upper bound of the @xmath0-rank in advance . </S>",
    "<S> the convergence analysis of the proposed algorithm is also presented . </S>",
    "<S> particularly , we show that for the noiseless case , the linear convergence with rate @xmath1 can be obtained for the proposed algorithm under proper conditions . </S>",
    "<S> additionally , combining an effective heuristic for determining @xmath0-rank , we can also apply the proposed algorithm to solve mnra when @xmath0-rank is unknown in advance . </S>",
    "<S> some preliminary numerical results on randomly generated and real low @xmath0-rank tensor completion problems are reported , which show the efficiency of the proposed algorithms . </S>",
    "<S> iterative hard thresholding ; low-@xmath0-rank tensor recovery ; tensor completion ; compressed sensing </S>"
  ]
}