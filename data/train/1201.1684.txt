{
  "article_text": [
    "we study the three - user multi - way relay channel ( mwrc ) with correlated sources , where each user transmits its data to the other two users via a single relay , and where the users messages can be correlated .",
    "the mwrc is a canonical extension of the extensively studied two - way relay channel ( twrc ) , where two users exchange data via a relay  @xcite . adding users to",
    "the twrc can change the problem significantly  @xcite .",
    "the mwrc has been studied from the point of view of channel coding and source coding .    in channel coding problems ,",
    "the sources are assumed to be independent , and the channel noisy .",
    "the problem is to find the capacity , defined as the region of all _ achievable _ channel rate triplets ( bits per channel use at which the users can encode / send on average ) . for the gaussian mwrc with independent sources , gndz et al .",
    "@xcite obtained asymptotic capacity results for the high snr and the low snr regimes . for the finite - field mwrc with independent sources , ong et al .",
    "@xcite constructed the _ functional - decode - forward _ coding scheme , and obtained the capacity region . for the general mwrc with independent sources ,",
    "however , the problem remains open to date .    in source coding problems ,",
    "the sources are assumed to be correlated , but the channel noiseless .",
    "the problem is to find the region of all _ achievable _ source rate triplets ( bits per message symbol at which the users can encode / send on average ) .",
    "the source coding problem for the three - user mwrc was solved by wyner et al .",
    "@xcite , using _ cascaded slepian - wolf _ source coding  @xcite .    in this paper , we study both source and channel coding in the same network , i.e. , transmitting correlated sources through noisy channels ( cf .",
    "our recent work  @xcite on the mwrc with correlated sources and orthogonal uplinks ) . for most communication scenarios ,",
    "the source correlation is fixed by the natural occurrence of the phenomena , and the channel is the part that engineers are `` unwilling or unable to change ''  @xcite .",
    "given the source and channel models , we are interested in finding the limit of how fast we can feed the sources through the channel . to this end , define _ source - channel rate _  @xcite ( also known as bandwidth ratio  @xcite ) as the average channel transmissions used per source tuple . our aim is then to derive the minimum source - channel rate required such that each user can _ reliably _ and _ losslessly _ reconstruct the other two users messages .    in the multi - terminal network",
    ", it is well known that separating source and channel coding , i.e. , designing them independently , is not always optimal ( see , e.g. , the multiple - access channel @xcite ) .",
    "designing good joint source - channel coding schemes is difficult , let alone finding an optimal one .",
    "gndz et al .",
    "@xcite considered a few networks with two senders and two receivers , and showed that source - channel separation is optimal for certain classes of source structure . in this paper , we approach the mwrc in a similar direction .",
    "we show that source - channel separation is optimal for three classes of source / channel combinations , by constructing coding schemes that achieve the minimum source - channel rate .",
    "recently , mohajer et al .",
    "@xcite solved the problem of linear deterministic relay networks with correlated sources .",
    "they constructed an optimal coding scheme , where each relay injectively maps its received channel output to its transmitted channel input .",
    "while this scheme is optimal for deterministic networks , such a scheme ( e.g. , the amplify - forward scheme in the additive white gaussian noise channel ) suffers from noise propagation in noisy channels and has been shown to be suboptimal for the mwrc with independent sources  @xcite .",
    "we consider the mwrc depicted in figure  [ fig : mwrc ] , where three users ( denoted by 1 , 2 , and 3 ) exchange messages through a noisy channel with the help of a relay ( denoted by 0 ) .",
    "for each node @xmath0 , we denote its source by @xmath1 , its input to the channel by @xmath2 , and its received channel output by @xmath3 .",
    "we let @xmath4 , as the relay has no source .",
    "we consider correlated and discrete - memoryless sources for the users , where @xmath5 , @xmath6 , and @xmath7 are generated according to some joint probability mass function @xmath8 the channel consists of a finite - field _ uplink _ from the users to the relay , which takes the form @xmath9 and a finite - field _ downlink _ from the relay to each user @xmath10 , which takes the form @xmath11 where @xmath12 , for all @xmath0 , for some finite field @xmath13 of cardinality @xmath14 with the associated addition @xmath15 . here",
    ", @xmath16 can be any prime power .",
    "we assume that the noise @xmath17 is not uniformly distributed , i.e. , its entropy @xmath18 ; otherwise , it will randomize the channel , and no information can be sent through .",
    "each user sends @xmath19 source symbols to the other two users ( simultaneously ) in @xmath20 channel uses .",
    "we refer to the @xmath19 source symbols of user @xmath21 as its message , denoted by @xmath22 , w_i[2 ] , \\dotsc,$ ] @xmath23)$ ] , where each symbol triplet @xmath24 , w_2[u ] , w_3[u])$ ] for @xmath25 is generated independently according to .",
    "the channel is memoryless in the sense that the channel noise @xmath17 for all nodes and all channel uses are independent , and the distribution @xmath26 is fixed for all channel uses .",
    "the source - channel rate , i.e. , the number of channel uses per source triplet , is denoted by @xmath27 .",
    "we assume that each user has all its @xmath19 source symbols prior to the @xmath20 channel uses source triplets and @xmath20 channel uses per unit time ) , we can always transmit in blocks .",
    "we first wait for @xmath19 source triplets to be generated , and then use the channel @xmath20 times to transmit these source symbols ( while waiting for the next @xmath19 triplets generation ) , and so on . taking the number of blocks to be sufficiently large , the source - channel rate can be made as close to @xmath28 as desired . ] , and consider the following block code of source - channel rate @xmath28 :    1 .",
    "the @xmath29-th transmitted channel symbol of each node @xmath30 depends on its message and its previously received channel symbols , i.e. , @xmath31 = f_{d , t } ( { \\ensuremath{\\boldsymbol{w}}}_d , y_d[1 ] , y_d[2 ] , \\dotsc , y_d[t-1])$ ] , for all @xmath0 and for all @xmath32 .",
    "each user @xmath21 estimates the messages of the other users from its own message and all its received channel symbols , i.e. , user @xmath21 decodes the messages from users @xmath33 and @xmath34 as @xmath35 , for all distinct @xmath36 .",
    "we denote @xmath37,y_i[2],$ ] @xmath38)$ ] .",
    "for source symbols or @xmath20 for channel symbols is clear from context . ]",
    "note that utilizing _",
    "feedback _ is permitted in our system model .",
    "this is commonly referred to as the _ unrestricted _ mwrc ( cf .",
    "the _ restricted _ mwrc @xcite ) .",
    "we will see later that for the classes of source / channel combinations for which we find the minimum source - channel rate , feedback is not used .",
    "this means that feedback provides no improvement to source - channel rate for these cases .",
    "user @xmath21 makes a decoding error if @xmath39 .",
    "we define @xmath40 as the probability that one or more users make a decoding error , and say that source - channel rate @xmath27 is _ achievable _ if the following is true : for any @xmath41 , there exists at least one block code of source - channel rate @xmath42 with @xmath43 .",
    "the aim of this paper is to find the infimum of achievable source - channel rates , denoted by @xmath44 . for the rest of the paper",
    ", we refer to @xmath44 as the minimum source - channel rate .",
    "theoretical interest aside , the finite - field channel considered in this paper shares two important properties with the awgn channel ( commonly used to model wireless environments ) .",
    "firstly , the channel is linear , i.e. , the channel output is a function of the sum of all inputs .",
    "secondly , the noise is additive .",
    "sharing these two properties , optimal coding schemes derived for the finite - field channel shed light on how one would code in awgn channels .",
    "for example , the optimal coding scheme derived for the finite - field mwrc with independent sources  @xcite is used to prove capacity results for the awgn mwrc with independent sources  @xcite .",
    "we will now state the main result of this paper .",
    "the technical terms ( in italics ) in the theorem will be defined in section  [ sec : definition ] following the theorem .",
    "[ theorem : main ] the minimum source - channel rate is given by @xmath45 if the sources have any one of the following :    1 .",
    "_ almost - balanced conditional mutual information _ , or 2 .",
    "_ skewed conditional entropies _ ( on any _ symmetrical _ finite - field channel ) , or 3 .   _",
    "their common information equals their mutual information_.    for cases 1 and 2 , we derive the achievability ( upper bound ) of @xmath44 using existing ( i ) slepian - wolf source coding and ( ii ) functional - decode - forward channel coding for independent sources .",
    "we abbreviate this pair of source and channel coding scheme by sw / fdf - is .",
    "we derive a lower bound using cut - set arguments . while the achievability for these two cases is rather straightforward",
    ", what we find interesting is that using the scheme for independent messages is actually optimal for two classes of source / channel combinations .",
    "furthermore , although the source - channel rates achievable using sw / fdf - is can not be expressed in a closed form , we are able to derive closed - form conditions for two classes of sources where the achievability of sw / fdf - is matches the lower bound .    in sw",
    "/ fdf - is , the source coding  while compressing  destroys the correlation among the sources , and hence channel coding for independent sources is used . for case 3 ,",
    "the sources have their common information equal their mutual information , meaning that each source is able to identify the parts of the messages it has in common with other source(s ) . for this case",
    ", we again use slepian - wolf source coding , but we conserve the parts that the sources have in common .",
    "we then design a _",
    "channel coding scheme that takes the common parts into account . here",
    ", the challenge is to optimize the functions of different parts that the relay should decode .",
    "we show that the new coding scheme is able to achieve @xmath44 .    for all three cases ,",
    "the coding schemes are derived based on the separate source - channel coding architecture .",
    "also , for cases 1 and 3 , @xmath44 is found when only the sources satisfy certain conditions , and this is true independent of the underlying finite - field channel , i.e. , any @xmath16 and any noise distribution .      in this section ,",
    "we define the technical terms in theorem  [ theorem : main ] .",
    "[ def : symmetrical ] a finite - field mwrc is _ symmetrical _ if @xmath46 otherwise , we say that the channel is _",
    "we can think of @xmath47 as the noise level on the downlink from the relay to user @xmath21 .",
    "so , a symmetrical channel requires that the downlinks from the relay to all the users are equally noisy .",
    "we do not impose any condition on the uplink noise level , @xmath48 .",
    "[ def : abcmi ] the sources are said to have _ almost - balanced conditional mutual information _",
    "( abcmi ) if @xmath49 for all distinct @xmath36 .",
    "otherwise , the sources are said to have _ unbalanced conditional mutual information_.    putting it another way , for unbalanced sources , we can always find a user @xmath50 , such that @xmath51 for some @xmath52 and distinct @xmath53 .",
    "[ def : sce ] sources with unbalanced conditional mutual information are said to have _ skewed conditional entropies _ ( sce ) if , in addition to , @xmath54 for the same @xmath55 as in .",
    "[ def : sce ] sources with unbalanced conditional mutual information are said to have _ skewed conditional entropies _",
    "( sce ) if , in addition to , @xmath56 for the same @xmath55 as in .",
    "lastly , we define _ common information _ in the same spirit as gcs and krner  @xcite . for two users ,",
    "gcs and krner defined common information as a value on which two users can _ agree _ ( using the terminology of witsenhausen  @xcite ) .",
    "the common information between two random variables can be as large as mutual information ( in the shannon sense ) , but no larger .",
    "the concept of common information was extended to multiple users by tyagi et al .",
    "@xcite , where they considered a value on which all users can agree . in this paper",
    ", we further extend common information to values on which different subsets of users can agree .",
    "we now formally define a class of sources , where their common information equals their mutual information .",
    "[ def : cc ] three correlated random variables @xmath57 are said to have their common information equal their mutual information if there exists four random variables @xmath58 , @xmath59 , @xmath60 , and @xmath61 such that @xmath62 for some deterministic functions @xmath63 , and @xmath64 @xmath65    we give graphical interpretations using information diagrams for sources that have abcmi and sce in appendix  [ appendix : abcmi ] , and examples of sources that have abcmi and their common information equal their mutual information in section  [ section : conclusion ] .",
    "definitions  [ def : abcmi ] and [ def : sce ] are mutually exclusive , but definitions  [ def : cc ] and [ def : abcmi ] ( or [ def : cc ] and [ def : sce ] ) are not .",
    "this means correlated sources that have their common information equal their mutual information must also have either abcmi , sce , or unbalanced mutual information without sce .",
    "this leads to the graphical summary of the results of theorem  [ theorem : main ] in figure  [ fig : results ] .",
    "the rest of this paper is organized as follows : we show a lower bound and an upper bound ( achievability ) to @xmath44 in section  [ section : lower ] . in section  [ section : upper-1 - 2 ] , we show that for cases 1 and 2 in theorem  [ theorem : main ] , the lower bound is achievable . in section  [",
    "section : upper-3 ] , we propose a coding scheme that takes common information into account , and show the source - channel rate achievable using this new scheme matches the lower bound .",
    "we conclude the paper with some discussion in section  [ section : conclusion ] .",
    "denote the rhs of as @xmath66    we first show that @xmath67 is a lower bound to @xmath44 . using cut - set arguments  @xcite",
    ", we can show that if source - channel rate @xmath42 is achievable , then    @xmath68 , \\label{eq : upper-2}\\end{aligned}\\ ] ]    @xmath69 , \\label{eq : upper-2}\\end{aligned}\\ ] ]    for all distinct @xmath36 . here follows from mohajer et al .",
    "( 11)(12 ) ) and follows from ong et al .",
    "* section iii ) .",
    "re - arranging the equation gives the following lower bound to all achievable source - channel rates @xmath42and hence also to @xmath44 :    [ lemma : lower - bound ] for any three - user finite - field mwrc with correlated sources , the minimum source - channel rate is lower bounded as @xmath70    we now present the result of sw / fdf - is coding scheme that first uses slepian - wolf source coding for the noiseless mwrc with correlated sources  @xcite , followed by functional - decode - forward for independent sources ( fdf - is ) channel coding for the mwrc  @xcite .",
    "this scheme achieves the following source - channel rates :    [ lemma : achievability - separate ] for any three - user finite - field mwrc with correlated sources , sw / fdf - is achieves all source - channel rates in @xmath71 , where @xmath72 where @xmath73 is the set of real numbers .",
    "so , the minimum source - channel rate is upper bounded as @xmath74 @xmath75 where @xmath73 is the set of real numbers .",
    "so , the minimum source - channel rate is upper bounded as @xmath74    the proof is based on random coding arguments and can be found in appendix  [ appendix : slepian - wolf - fdf ] .",
    "the variables @xmath76 are actually the channel code rates , i.e. , the number of message bits transmitted by user @xmath21 per channel use .    from lemmas  [ lemma : lower - bound ] and [ lemma : achievability - separate ] , we have the following result :    [ corollary : separation ] for a three - user finite - field mwrc , if @xmath77 , then @xmath78 , meaning that the minimum source - channel rate is known and is achievable using sw / fdf - is .",
    "[ remark : separation ] the collection of source / channel combinations that satisfy corollary  [ corollary : separation ] forms a class where the minimum source - channel rate is found , in addition to theorem  [ theorem : main ]",
    ". the challenge , however , is to characterize  in closed form  classes of source / channel combinations for which @xmath77 . for this , we need to guarantee the existence of three positive numbers @xmath79 and @xmath80 satisfying the inequalities in for every @xmath81 .",
    "next , we will show that for cases 1 and 2 in theorem  [ theorem : main ] , sw / fdf - is achieves all source - channel rates @xmath81 .",
    "in this subsection , we will show that if the sources have abcmi , then @xmath77 .",
    "since any @xmath82 relies on the existence of channel code rates @xmath83 , we first show the following proposition :    [ proposition : existence ] consider sources with abcmi . given any source - channel rate @xmath84 , and any positive number @xmath85",
    ", we can always find positive @xmath79 and @xmath80 such that @xmath86    it can be shown that choosing @xmath87 + \\frac{\\delta}{4 } , \\label{eq : case-1-r1-r3}\\ ] ] @xmath88 + \\frac{\\delta}{4 } , \\label{eq : case-1-r1-r3}\\end{aligned}\\ ] ] for all distinct @xmath36 satisfies . the expression in the square brackets is non - negative due to the abcmi condition .    with this result ,",
    "we now prove case 1 of theorem  [ theorem : main ] .",
    "we need to show that any source - channel rate @xmath81 is achievable , i.e. , the source - channel rate @xmath89 for any @xmath85 , lies in @xmath71 . here , @xmath90 is independent of @xmath91 and @xmath92 .    for a source - channel rate in ,",
    "we choose @xmath93 as in . substituting  into , the second inequality in is satisfied . also , ",
    "imply the first inequality in .",
    "hence , @xmath82 .",
    "this proves case 1 in theorem  [ theorem : main ] .",
    "@xmath94      we need to show that if the sources have sce and the channel is symmetrical , then the source - channel rate in is achievable for any @xmath85 . recall that sources that have sce must have unbalanced conditional mutual information , for which we can always re - index the users as @xmath95 , @xmath96 , and @xmath97 satisfying for some fixed @xmath98 .    for achievability in lemma  [ lemma : achievability - separate ]",
    ", we first show the existence of @xmath99 satisfying the following conditions :    [ proposition : existence-2 ] consider sources with unbalanced mutual information . given any source - channel rate @xmath84 , and any positive number @xmath85 , we can always find positive @xmath100 and @xmath101 such that @xmath102 for @xmath98 defined in .",
    "constraint implies the following : @xmath103 @xmath104    first , we can always choose a positive number @xmath105 as in .",
    "in addition , we choose @xmath106 + \\frac{\\delta}{4 } , \\label{eq : case-2-r2}\\\\ \\kappa r_c & = h(w_c|w_a , w_b ) + \\frac{1}{2 } \\big [ i(w_b;w_c|w_a ) + i(w_a;w_c|w_b ) - i(w_a;w_b|w_c)\\big ] + \\frac{\\delta}{4}. \\label{eq : case-2-r3}\\end{aligned}\\ ] ] @xmath107 + \\frac{\\delta}{4 } , \\label{eq : case-2-r2}\\\\ \\kappa r_c & = h(w_c|w_a , w_b ) + \\frac{1}{2 } \\big [ i(w_b;w_c|w_a)\\nonumber \\\\ & \\quad   + i(w_a;w_c|w_b ) - i(w_a;w_b|w_c)\\big ] + \\frac{\\delta}{4}. \\label{eq : case-2-r3}\\end{aligned}\\ ] ]    substituting into , we get ; substituting into , we get . summing different pairs from , , and ,",
    "we get .",
    "furthermore , for a symmetrical channel , we can define @xmath108 so , for sce and for symmetrical channels imply that the source - channel rate in equals @xmath109 hence , we only need to show that the source - channel rate is achievable for any @xmath110 .",
    "we first choose @xmath100 and @xmath101 as in , , and , respectively . from  , we get    @xmath111 - \\frac{\\delta}{2 } \\label{eq : b } \\\\ &   < \\kappa \\big[\\log f - \\max\\ { h(n_0 ) , h_\\text{downlink } \\ } \\big ] , \\label{eq:2a-1}\\\\ \\kappa ( r_a + r_b ) & = h(w_a , w_b|w_c ) + \\frac{\\eta}{2 } + \\frac{\\delta}{2 } \\label{eq : c } \\\\ & \\leq h(w_b , w_c|w_a )   - \\frac{\\eta}{2 } + \\frac{\\delta}{2 } \\label{eq : d}\\\\ & <   \\kappa \\big[\\log f - \\max\\ { h(n_0 ) , h_\\text{downlink } \\ } \\big ] , \\label{eq:2a-2}\\\\ \\kappa ( r_a + r_c ) & = h(w_a , w_c|w_b ) + \\frac{\\eta}{2 } + \\frac{\\delta}{2 } \\label{eq : e}\\\\ &   \\leq h(w_b , w_c|w_a )   - \\frac{\\eta}{2 } + \\frac{\\delta}{2 } \\label{eq : f } \\\\ &",
    "<    \\kappa \\big[\\log - \\max\\ { h(n_0 ) , h_\\text{downlink } \\ } \\big ] , \\label{eq:2a-3}\\end{aligned}\\ ] ]    where and follow from ; , , and follow from . , which is determined by the sources correlation , is strictly greater than zero ; see its definition in .",
    "] this means the second inequality in is satisfied . from  , we know that the first inequality in is also satisfied . hence , the source - channel rate is indeed achievable for any @xmath110 .",
    "@xmath94      in this section , we give an example showing that sw / fdf - is can be suboptimal .",
    "consider the following sources : @xmath112 , @xmath113 , and @xmath114 , where @xmath115 is uniformly distributed in @xmath116 , @xmath117 is uniformly distributed in @xmath118 , and @xmath119 are each uniformly distributed in @xmath120 . in addition , all @xmath121 and @xmath122 are mutually independent . here",
    ", each @xmath122 represents common information between @xmath123 and @xmath124 .    for the channel ,",
    "let the finite field be @xmath125 and @xmath15 be modulo-@xmath126 addition , i.e. , @xmath127 .",
    "furthermore , let @xmath128 for @xmath129 , and @xmath130 for @xmath131 ; let @xmath132 for @xmath133 , and @xmath134 for @xmath135 , for all @xmath10 .    for this source and channel combinations , we have @xmath136 , @xmath137 , @xmath138 @xmath139 , @xmath140 , @xmath141 , @xmath142 , for all @xmath10 .",
    "one can verify that these sources have unbalanced conditional mutual information and do not have sce .    in this example ,",
    "suppose that @xmath144 is achievable using sw / fdf - is .",
    "from lemma  [ lemma : achievability - separate ] , there must exists three positive real numbers @xmath145 , @xmath146 , and @xmath80 such that @xmath147 @xmath148 from , we must have that @xmath149 and @xmath150 .",
    "these imply @xmath151 . hence , and can not be simultaneously true .",
    "this means the source - channel rate 1.05 is not achievable using sw / fdf - is .",
    "the sources described here have their common information equal their mutual information .",
    "we will next propose an alternative scheme that is optimal for this class of sources .",
    "the following proposed scheme achieves all source - channel rates @xmath152 for this source / channel combination , meaning that the minimum source - channel rate for this example is @xmath153 .",
    "so sw / fdf - is is strictly suboptimal for this source / channel combination .",
    "while the achievability for cases 1 and 2 uses existing source and channel coding schemes , for case 3 ( i.e. , sources that have their common information equal their mutual information ) , we will use an existing source coding scheme and design a new channel coding scheme to achieve all source - channel rates @xmath81 .",
    "while case 3 in general requires a new achievability scheme , these sources may have abcmi ( as shown in figure  [ fig : results ] ) .",
    "for such cases , optimal codes can also be obtained using the coding scheme for case 1 .    in this section , without loss of generality , we let @xmath154 this means we can re - write @xmath67 as follows : @xmath155    as mentioned earlier , we will use a separate - source - channel - coding architecture , where we first perform source coding and then channel coding .",
    "we will again use random coding arguments . more specifically",
    ", we will use random linear block codes for channel coding .",
    "we encode each @xmath156 , v_\\mathcal{s}[2 ] , \\dotsc , v_\\mathcal{s}[m])$ ] to @xmath157 , which is a length-@xmath158 finite - field ( of size @xmath16 ) vector , for all @xmath159 ( see definition  [ def : cc ] for the definition of @xmath160 ) .",
    "we also encode each @xmath161 to @xmath162 , which is a length-@xmath163 finite - field vector .",
    "so , each message @xmath161 is encoded into four subcodes , e.g. , @xmath164 is encoded into @xmath165 . some subcodes  the common parts  are shared among multiple sources .    using the results of distributed source coding  @xcite , if @xmath19 is sufficiently large and if @xmath166 then we can decode @xmath165 to @xmath164 , @xmath167 to @xmath168 , and @xmath169 to @xmath170 with an arbitrarily small error probability .",
    "we show the proof in appendix  [ app : gk - source - coding ] .",
    "after source coding , user 1 has @xmath171 .",
    "in order for it to decode @xmath172 , it must receive @xmath173 from the other users through the channel .",
    "similarly , users 2 and 3 must each obtain subcodes that they do not already have through the channel .",
    "in contrast to the source coding used for cases 1 and 2 , here , we have generated source codes where the users share some subcodes .",
    "so , instead of using existing fdf - is channel codes ( designed for independent sources ) , we will design channel codes that take the common subcodes into account .    [",
    "cols=\"^,^,^,^,^,^,^,^\",options=\"header \" , ]     the messages in each column are transmitted simultaneously .",
    "note that in the third row of messages in the table , @xmath174 is split into two parts @xmath175 if and only if @xmath176 .",
    "else , the entire message @xmath174 will be transmitted in the first column , i.e. , together with @xmath177 . since @xmath178",
    ", the message @xmath175 can always fit into the first and third columns , with the remaining `` space '' padded with zero , denoted by @xmath179 .",
    "the message @xmath180 is transmitted in a similar way .",
    "the relay decodes the modulo addition of the messages in each column . if @xmath20 is sufficiently large and if is satisfied , then the relay can reliably decode its intended messages , i.e. , @xmath181 .",
    "the relay broadcasts @xmath182 on the downlink . using lemma  [ lemma : downlnk ]",
    ", we can show that if , , and are satisfied , then each user can reliably decode @xmath182 , from which it can recover the messages of the other two users .",
    "this completes the proof for the achievability of case 3 in theorem  [ theorem : main ] .",
    "note that the lower bound in lemma  [ lemma : lower - bound ] and the achievable source - channel rates in lemma  [ lemma : achievability - separate ] are applicable to the general finite - field mwrc with correlated sources , in addition to cases 1 and 2 in theorem  [ theorem : main ] .",
    "however , the coding technique in section  [ section : upper-3 ] is useful only for sources that have their common information equal their mutual information .    besides the coding schemes considered in this paper",
    ", one could treat the uplink ( as a multiple - access channel with correlated sources ) and the downlink ( as a broadcast channel with receiver side information ) separately to get potentially different achievable source - channel rates . in this case , on the uplink , we let the relay decode all three users messages @xmath183 .",
    "an achievable channel rate region for the two - sender multiple - access channel with correlated sources was found by cover et al .",
    "then , on the downlink , we can use the result by tuncel  @xcite for the relay to transmit @xmath184 to the users taking into account that each user @xmath21 has side information @xmath185 . extending the results of cover et al .",
    "* theorem 2 ) to three senders , for the relay to be able to reliably decode @xmath183 on the uplink , @xmath42 must satisfy the following : @xmath186 , \\\\",
    "\\rightarrow & & \\kappa & > \\frac{h(w_1,w_2,w_3)}{\\log f - h(n_0)}. \\label{eq : cover - tuncel}\\end{aligned}\\ ] ] @xmath187 , \\\\",
    "\\rightarrow & & \\kappa & > \\frac{h(w_1,w_2,w_3)}{\\log f - h(n_0)}. \\label{eq : cover - tuncel}\\end{aligned}\\ ] ] for the case where each user has non - zero message , @xmath188 for all @xmath21 .",
    "comparing to , we see that this coding strategy is strictly suboptimal for all cases 13 in theorem  [ theorem : main ] when @xmath189 .",
    "however , this strategy _ may _ achieve better ( i.e. , lower ) source - channel rates than sw / fdf - is in general .",
    "we leave the derivation of the rates to the reader  it is straightforward given the results by cover et al .  and tuncel .    in this paper",
    ", we have only investigated the separate - source - channel coding architecture without feedback .",
    "though we have identified source / channel combinations where the minimum source - channel rate is found , the problem remains open in general .",
    "two directions which one could explore are _ joint _ source - channel coding and _",
    "feedback_.      we now give a numerical example where none of the coding schemes in this paper achieves the lower bound @xmath67 .",
    "let the sources be @xmath190 , @xmath191 , and @xmath192 where all @xmath193 are independent random variables , and @xmath194 denotes modulo - two addition .",
    "we choose @xmath195 , @xmath196 , @xmath197 , and @xmath198 . for this choice",
    ", we have @xmath199 , @xmath200 , @xmath201 .    for the channel , let @xmath202 , which gives @xmath203 .",
    "we choose @xmath204 and @xmath205 for @xmath206 , giving @xmath207 . for each @xmath10 , we set @xmath208 , @xmath209 , and @xmath210 for @xmath211 , giving @xmath212 .",
    "this means , @xmath213 , and @xmath214 for all @xmath10 .    in this example , @xmath215 .",
    "if @xmath216 is achievable , then ( from lemma  [ lemma : achievability - separate ] ) we must be able to find some @xmath146 and @xmath80 satisfying @xmath217 , and @xmath218 . since the conditions can not be simultaneously met , @xmath216 is not achievable using sw / fdf - is . as the sources common information does not equal their mutual information , we can not use the coding scheme derived in section  [ section : upper-3 ] .",
    "this example shows that the separation schemes derived in this paper can not achieve the lower bound in some cases .",
    "however , to show that separation is suboptimal , one has to explore all possible separation schemes and show that some joint source - channel scheme achieves a better source - channel rate .      in this paper ,",
    "we have identified three classes of source / channel combinations where the minimum source - channel rate is found .",
    "the first class is sources that have almost - balanced conditional mutual information ( abcmi ) .",
    "an example of sources that have abcmi is interchangeable random variables in the sense of chernoff and teicher  @xcite , where `` every subcollection of the random variables has a joint distribution which is a symmetric function of its arguments . ''",
    "this can model sensor networks where the sensors are equally - spaced on a circle to detect a phenomenon occurring at the center of the circle .",
    "however , the abcmi conditions are looser than that of interchangeable random variables as the former only requires that mutual information between any two sources has _ roughly _ the same value ( see appendix  [ appendix : abcmi ] ) , and also , the marginal distribution of the variables can be vastly different .",
    "another class for which we have derived the minimum source - channel rate is when the sources have their common information equal their mutual information .",
    "an example is correlated sources in the sense of han  @xcite , where the sources can be written as @xmath219 , @xmath220 , and @xmath221 , where @xmath222 @xmath223 and @xmath224 are _ mutually independent _ random variables . using sensor networks as an example again , each node here has multiple sensing capabilities , e.g. , temperature , light , sound . as these measurements display different behavior spatially , some remain constant across subsets of sensors , e.g. , nodes 1 and 2 always sense the same temperature but different light intensity .    as for the class of sources with skewed conditional entropies , the conditions appear to be purely mathematical in nature .",
    "the authors would like to thank roy timo for discussions on gcs and krner s common information and other helpful comments .",
    "figure  [ fig : case1 - 2 ] shows the relationship among the entropies and mutual information for the three source messages @xmath5 , @xmath6 , and @xmath7 for the cases described above .",
    "referring to figure  [ fig : case-1 ] , the shaded areas represent the mutual information between any two source messages given the third source message .",
    "for abcmi , we have that any of the three shaded areas must not be bigger than the sum of the other two shaded areas .",
    "suppose that the sources do not have abcmi , then they must have unbalanced conditional mutual information , i.e. , we can find a user @xmath95 where @xmath225 is larger than the sum of @xmath226 and @xmath227 by an amount @xmath98 ( see figure  [ fig : case-2a ] ) .",
    "in addition , for sources that have sce , we also have that for the two messages , @xmath228 and @xmath229 , whose mutual information conditioned on @xmath230 , i.e. , @xmath225 , is larger than the sum of the other two pairs by the amount @xmath55 , their entropy conditioned on @xmath230 , i.e. , @xmath231 , is also greater than that of any other pair ( conditioned on the message of the third user ) by at least @xmath55 . the information diagram for sce is depicted in figure  [ fig : case-2a ] .",
    "we first quote two existing results of ( i ) channel coding for the three - user mwrc with independent messages and ( ii ) source coding with side information .",
    "the following channel - coding setting assumes that the source messages are independent :    [ lemma : channel - coding ] consider the mwrc depicted in figure  [ fig : mwrc ] , where each user @xmath21 s message @xmath123 is uniformly distributed in @xmath232 ( we consider a single copy , i.e. , @xmath233 ) , and where @xmath5 , @xmath6 , and @xmath7 are independent .",
    "the channel is used @xmath20 times according to the block code ( with @xmath233 ) specified in section  [ sec : model ] .",
    "each user can then decode the messages of the other two users ( with its @xmath20 received channel symbols and its own message ) with an arbitrarily small error probability if @xmath20 is sufficiently large , and if @xmath234 for all distinct @xmath36 .",
    "the following source - coding setting assumes that the channel is noiseless :    [ lemma : source - coding ] consider only the three users with their respective length-@xmath19 messages @xmath235 generated according to ( [ eq : source ] ) .",
    "each user @xmath21 encodes its messages @xmath185 to an index @xmath236 , and gives its index to the other two users .",
    "each user can then decode the messages of the other two users ( with the received indices and its own message ) with an arbitrarily small error probability if @xmath19 is sufficiently large , and if @xmath237 for all distinct @xmath36 .",
    "note that wyner et al .",
    "@xcite derived a similar result with an additional constraint on the relay . in their setup , the users present their indices to a relay ; the relay in turn re - encodes and presents its index to the users .",
    "we use slepian - wolf source coding .",
    "each user @xmath21 encodes its length-@xmath19 message @xmath185 to an index @xmath238 , satisfying and , for @xmath10 .",
    "each user @xmath21 randomly generates a dither @xmath239 uniformly distributed in @xmath240 , and forms its encoded message @xmath241 .",
    "the dithers are made known to all nodes .",
    "now , @xmath242 , @xmath243 , and @xmath244 are mutually independent , and each @xmath245 is uniformly distributed in @xmath240 .",
    "we then use fdf - is channel coding for the users to exchange the encoded independent messages @xmath242 , @xmath243 , and @xmath244 via the relay in @xmath20 channel uses . from lemma",
    "[ lemma : channel - coding ] , if is satisfied , then each user @xmath21 can reliably recover @xmath246 and @xmath247 . knowing the dithers , it can also recover @xmath248 and @xmath249 .",
    "from lemma  [ lemma : source - coding ] , if and are satisfied , then each user @xmath21 can reliably recover @xmath250 .    noting that @xmath27 and defining @xmath251 , the conditions for achievability , i.e. , , , and , can be expressed as .",
    "we perform source coding for correlated sources  @xcite@xcite . consider the source message @xmath164 . clearly , @xmath252 , since @xmath253 are deterministic functions of @xmath5 . now since @xmath58 captures all information that @xmath5 and @xmath6 have in common ( because @xmath254 ) , we have @xmath255 .",
    "similarly @xmath256 .",
    "so , we can reliably reconstruct @xmath164 from @xmath171 if @xmath19 is sufficiently large , and if the following inequalities hold  ( * ? ? ? * theorem  2 ) : @xmath257 \\log f ) /m & > h(w_1 , v_{12 } | v_{13 } , v_{123 } ) = h(w_1|w_2,w_3 ) + i(w_1;w_2|w_3 ) , \\\\ ( [ \\ell_{1 } + \\ell_{13 } ] \\log f ) /m & > h(w_1 , v_{13 } |v_{12 } , v_{123 } ) = h(w_1|w_2,w_3 ) + i(w_1;w_3|w_2 ) , \\\\ ( [ \\ell_{1 } + \\ell_{123 } ] \\log f ) /m & > h(w_1 , v_{123 } | v_{12 } , v_{13 } ) = h(w_1|w_2,w_3 ) , \\\\ ( [ \\ell_{12 } + \\ell_{13 } ] \\log f ) /m & > h(v_{12 } , v_{13 } | w_1 , v_{123 } ) = 0 , \\\\ & \\vdots\\\\ ( [ \\ell_{1 } + \\ell_{12 } + \\ell_{13 } ] \\log f ) /m & > h(w_1 , v_{12 } , v_{13 } | v_{123 } ) \\nonumber \\\\ & = h(w_1|w_2,w_3 ) + i(w_1;w_2|w_3 ) + i(w_1;w_3|w_2 ) , \\\\ ( [ \\ell_{1 } + \\ell_{12 } + \\ell_{123 } ] \\log f ) /m & > h(w_1 , v_{12 } , v_{123 } | v_{13 } ) = h(w_1|w_2,w_3 ) + i(w_1;w_2|w_3 ) , \\\\ ( [ \\ell_{1 } + \\ell_{13 } + \\ell_{123 } ] \\log f ) /m & > h(w_1 , v_{13 } , v_{123 } | v_{12 } ) = h(w_1|w_2,w_3 ) + i(w_1;w_3|w_2 ) , \\\\ ( [ \\ell_{12 } + \\ell_{13 } + \\ell_{123}\\ \\log",
    "f ) /m & > h(v_{12 } , v_{13 } , v_{123}|w_1 ) = 0 , \\\\ ( [ \\ell_{1 } + \\ell_{12 } + \\ell_{13 } + \\ell_{123 } ] \\log f ) /m &",
    "= h(w_1 , v_{12 } , v_{13 } , v_{123 } ) = h(w_1).\\end{aligned}\\ ] ] @xmath258 \\log f ) /m & > h(w_1 , v_{12 } | v_{13 } , v_{123})\\\\ & = h(w_1|w_2,w_3)\\\\ & \\quad + i(w_1;w_2|w_3 ) , \\\\ ( [ \\ell_{1 } + \\ell_{13 } ] \\log f ) /m & > h(w_1 , v_{13 } |v_{12 } , v_{123})\\\\ & = h(w_1|w_2,w_3)\\\\ & \\quad   + i(w_1;w_3|w_2 ) , \\\\ ( [ \\ell_{1 } + \\ell_{123 } ] \\log f ) /m & > h(w_1 , v_{123 } | v_{12 } , v_{13})\\\\ & = h(w_1|w_2,w_3 ) , \\\\ ( [ \\ell_{12 } + \\ell_{13 } ] \\log f ) /m & > h(v_{12 } , v_{13 } | w_1 , v_{123 } ) = 0 , \\\\ & \\vdots\\\\ ( [ \\ell_{1 } + \\ell_{12 } + \\ell_{13 } ] \\log f ) /m & > h(w_1 , v_{12 } , v_{13 } | v_{123 } ) \\nonumber \\\\ & = h(w_1|w_2,w_3)\\\\ & \\quad   + i(w_1;w_2|w_3)\\\\ & \\quad   + i(w_1;w_3|w_2 ) , \\\\ ( [ \\ell_{1 } + \\ell_{12 } + \\ell_{123 } ] \\log f ) /m & > h(w_1 , v_{12 } , v_{123 } | v_{13})\\\\ & = h(w_1|w_2,w_3)\\\\ & \\quad   + i(w_1;w_2|w_3 ) , \\\\ ( [ \\ell_{1 } + \\ell_{13 } + \\ell_{123 } ] \\log f ) /m & > h(w_1 , v_{13 } , v_{123 } | v_{12})\\\\ & = h(w_1|w_2,w_3)\\\\ & \\quad   + i(w_1;w_3|w_2 ) , \\\\ ( [ \\ell_{12 } + \\ell_{13 } + \\ell_{123}\\ \\log f ) /m & > h(v_{12 } , v_{13 } , v_{123}|w_1 ) = 0,\\end{aligned}\\ ] ] @xmath259 \\log f ) /m & = h(w_1 , v_{12 } , v_{13 } , v_{123})\\\\ & = h(w_1).\\end{aligned}\\ ] ] here , we need to consider all possible non - empty subsets of @xmath260 on the lhs .",
    "we have omitted some trivial inequalities where the rhs equals zero . repeating the above for sources 2 and 3 , and simplifying , we have .",
    "d.  gndz , e.  tuncel , and j.  nayak , `` rate regions for the separated two - way relay channel , '' in _ proc .",
    "46th allerton conf .",
    "control comput .",
    "( allerton conf . ) _ , monticello , usa , sept . 2326 2008 , pp . 13331340 .    c.  schnurr , s.  stanczak , and t.  j. oechtering , `` achievable rates for the restricted half - duplex two - way relay channel under a partial - decode - and - forward protocol , '' in _ proc .",
    "theory workshop ( itw ) _ , porto , portugal , may 59 2008 , pp . 134138 .",
    "x.  liang , s.  jin , x.  gao , and k .- k .",
    "wong , `` outage performance for decode - and - forward two - way relay network with multiple interferers and noisy relay , '' _ ieee trans .",
    "_ , vol .",
    "61 , no .  2 ,",
    "521531 , feb .",
    "s.  j. kim , b.  smida , and n.  devroye , `` capacity bounds on multi - pair two - way communication with a base - station aided by a relay , '' in _ proc .",
    "inf . theory ( isit ) _ , austin , usa , june 1318 2010 , pp . 425429 .",
    "r.  timo , g.  lechner , l.  ong , and s.  j. johnson , `` multi - way relay networks : orthogonal uplink , source - channel separation and code design , '' _ ieee trans .",
    "_ , vol .  61 , no .  2 , pp .",
    "753768 , feb .",
    "j.  l. massey , `` channel models for random - access systems . '' in _ performance limits in communication theory and practice , nato advances studies institutes series e142 _ , j.  k. skwirzynski , ed.1em plus 0.5em minus 0.4emkluwer academic , 1988 , pp . 391402 .",
    "a.  jain , d.  gndz , s.  r. kulkarni , h.  v. poor , and s.  verd , `` energy - distortion tradeoffs in gaussian joint source - channel coding problems , '' _ ieee trans .",
    "inf . theory _",
    "58 , no .  5 , pp .",
    "31533168 , may 2012 ."
  ],
  "abstract_text": [
    "<S> this paper studies the three - user finite - field multi - way relay channel , where the users exchange messages via a relay . the messages are arbitrarily correlated , and the finite - field channel is linear and is subject to additive noise of arbitrary distribution . </S>",
    "<S> the problem is to determine the minimum achievable source - channel rate , defined as channel uses per source symbol needed for reliable communication . </S>",
    "<S> we combine slepian - wolf source coding and functional - decode - forward channel coding to obtain the solution for two classes of source and channel combinations . furthermore , </S>",
    "<S> for correlated sources that have their common information equal their mutual information , we propose a new coding scheme to achieve the minimum source - channel rate .    </S>",
    "<S> bidirectional relaying , common information , correlated sources , linear block codes , finite - field channel , functional - decode - forward , multi - way relay channel </S>"
  ]
}