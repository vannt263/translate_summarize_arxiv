{
  "article_text": [
    "much attention has been given to a resurgence of neural networks , deep learning ( dl ) in particular , which can be of unsupervised @xcite , supervised @xcite , or a hybrid form @xcite .",
    "significant performance gain has been observed , especially in the presence of large amount of training data , when deep learning techniques are used for image classification @xcite and speech recognition @xcite . on the one hand ,",
    "hierarchical and recursive networks @xcite have demonstrated great promise in automatically learning thousands or even millions of features for pattern recognition ; on the other hand concerns about deep learning have been raised and many fundamental questions remain open .    some potential problems with the current dl frameworks include : reduced transparency and discriminativeness of the features learned at hidden layers @xcite ; training difficulty due to exploding and vanishing gradients @xcite ; lack of a thorough mathematical understanding about the algorithmic behavior , despite of some attempts made on the theoretical side @xcite ; dependence on the availability of large amount of training data @xcite ; complexity of manual tuning during training @xcite .",
    "nevertheless , dl is capable of automatically learning and fusing rich hierarchical features in an integrated framework .",
    "recent activities in open - sourcing and experience sharing @xcite have also greatly helped the adopting and advancing of dl in the machine learning community and beyond .",
    "several techniques , such as dropout @xcite , dropconnect @xcite , pre - training @xcite , and data augmentation @xcite , have been proposed to enhance the performance of dl from various angles , in addition to a variety of engineering tricks used to fine - tune feature scale , step size , and convergence rate .",
    "features learned automatically by the cnn algorithm @xcite are intuitive @xcite .",
    "some portion of features , especially for those in the early layers , also demonstrate certain degree of opacity @xcite .",
    "this finding is also consistent with an observation that different initializations of the feature learning at the early layers make negligible difference to the final classification @xcite .",
    "in addition , the presence of vanishing gradients also makes the dl training slow and ineffective @xcite . in this paper",
    ", we address the feature learning problem in dl by presenting a new algorithm , deeply - supervised nets ( dsn ) , which enforces direct and early supervision for both the hidden layers and the output layer .",
    "we introduce _ companion objective _ to the individual hidden layers , which is used as an additional constraint ( or a new regularization ) to the learning process .",
    "our new formulation significantly enhances the performance of existing supervised dl methods .",
    "we also make an attempt to provide justification for our formulation using stochastic gradient techniques .",
    "we show an improvement of the convergence rate of the proposed method over standard ones , assuming local strong convexity of the optimization function ( a very loose assumption but pointing to a promising direction ) .",
    "several existing approaches are particularly worth mentioning and comparing with . in @xcite ,",
    "layer - wise supervised pre - training is performed .",
    "our proposed method does not perform pre - training and it emphasizes the importance of minimizing the output classification error while reducing the prediction error of each individual layer .",
    "this is important as the backpropagation is performed altogether in an integrated framework . in @xcite ,",
    "label information is used for unsupervised learning .",
    "semi - supervised learning is carried in deep learning @xcite . in @xcite ,",
    "an svm classifier is used for the output layer , instead of the standard softmax function in the cnn @xcite . our framework ( dsn ) , with the choice of using svm , softmax or other classifiers , emphasizes the direct supervision of each intermediate layer . in the experiments , we show consistent improvement of dsn - svm and dsn - softmax over cnn - svm and cnn - softmax respectively .",
    "we observe all state - of - the - art results on mnist , cifar-10 , cifar-100 , and svhn .",
    "it is also worth mentioning that our formulation is inclusive to various techniques proposed recently such as averaging @xcite , dropconnect @xcite , and maxout @xcite .",
    "we expect to see more classification error reduction with careful engineering for dsn .",
    "in this section , we give the main formulation of the proposed deeply - supervised nets ( dsn ) .",
    "we focus on building our infrastructure around supervised cnn style frameworks @xcite by introducing classifier , e.g. svm model @xcite , to each layer . an early attempt to combine svm with dl",
    "was made in @xcite , which however has a different motivation with ours and only studies the output layer with some preliminary experimental results .          [ cols= \"",
    "< , < \" , ]     street view house numbers ( svhn ) dataset consists of @xmath0 digits for training , @xmath1 digits for testing , and @xmath2 extra training samples on @xmath3 color images .",
    "we followed the previous works for data preparation , namely : we select 400 samples per class from the training set and 200 samples per class from the extra set .",
    "the remaining 598,388 images are used for training .",
    "we followed @xcite to preprocess the dataset by local contrast normalization ( lcn ) .",
    "we do not do data augmentation in training and use only a single model in testing .",
    "table [ svhn - table ] shows recent comparable results .",
    "note that dropconnect @xcite uses data augmentation and multiple model voting .",
    "in this paper , we have presented a new formulation , deeply - supervised nets ( dsn ) , attempting to make a more transparent learning process for deep learning .",
    "evident performance enhancement over existing approaches has been obtained .",
    "a stochastic gradient view also sheds light to the understanding of our formulation .",
    "this work is supported by nsf award iis-1216528 ( iis-1360566 ) and nsf award iis-0844566 ( iis-1360568 ) .",
    "we thank min lin , naiyan wang , baoyuan wang , jingdong wang , liwei wang , and david wipf for help discussions .",
    "we are greatful for the generous donation of the gpus by nvidia ."
  ],
  "abstract_text": [
    "<S> our proposed deeply - supervised nets ( dsn ) method simultaneously minimizes classification error while making the learning process of hidden layers direct and transparent . </S>",
    "<S> we make an attempt to boost the classification performance by studying a new formulation in deep networks . </S>",
    "<S> three aspects in convolutional neural networks ( cnn ) style architectures are being looked at : ( 1 ) transparency of the intermediate layers to the overall classification ; ( 2 ) discriminativeness and robustness of learned features , especially in the early layers ; ( 3 ) effectiveness in training due to the presence of the exploding and vanishing gradients . </S>",
    "<S> we introduce `` companion objective '' to the individual hidden layers , in addition to the overall objective at the output layer ( a different strategy to layer - wise pre - training ) . </S>",
    "<S> we extend techniques from stochastic gradient methods to analyze our algorithm . </S>",
    "<S> the advantage of our method is evident and our experimental result on benchmark datasets shows significant performance gain over existing methods ( e.g. all state - of - the - art results on mnist , cifar-10 , cifar-100 , and svhn ) . </S>"
  ]
}