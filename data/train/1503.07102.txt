{
  "article_text": [
    "the problem of selecting appropriate models has been studied extensively in the literature since the work of @xcite , who derived the so - called akaike information criterion ( aic ) .",
    "there are several approaches to solving the model selection problem : information criteria , such as the aic or bic @xcite ; shrinkage methods , such as the lasso @xcite ; bayesian techniques ; among others . with regard to bayesian techniques , @xcite provide a good review of the key works in this field , including @xcite , @xcite , and @xcite .",
    "in addition , a bayesian lasso procedure based on a spike - and - slab prior has attracted much recent attention @xcite . however , although these methods are useful and important , we focus on information criteria in this study .",
    "two of the fundamental information criteria are the aic and the bic . because the aic and its variants are based on the risks of predictive densities with respect to the kullback ",
    "leibler ( kl ) divergence , they are able to select good models in terms of their predictive ability .",
    "in fact , @xcite , @xcite , and others have shown that the model selected by the aic minimizes the prediction error asymptotically .",
    "however , it is known that aic - type criteria do not have the property of consistency ; that is , the probability that the criteria will select the true model does not converge to 1 . on the other hand ,",
    "the bic , based on the bayesian marginal likelihood , does exhibit consistency in certain specific models @xcite , but does not select models as effectively in terms of their predictive ability .",
    "therefore , we propose a hybrid of the aic and the bic that uses the empirical bayesian method , which has the property of consistency , but also selects models well in terms of predictive ability .    our approach is to measure the prediction risk of a predictive density based on the bayesian marginal likelihood from a frequentist point of view .",
    "specifically , we focus on the variable selection problem for normal linear regression models , assuming a prior distribution of the regression coefficients in order to derive the criterion . in section",
    "[ sec : app ] , we consider two prior distributions , namely the normal distribution and the uniform prior distribution . there are three advantages of our method .",
    "first , this is a compromise between the frequentist and bayesian standpoints because it evaluates the frequentist s risk of the bayesian model .",
    "thus , the method should be less influenced by a prior misspecification .",
    "second , the criteria exhibits consistency when selecting the true model . at the same time",
    ", our proposed criteria can select a good model , in the sense that the prediction risk is small , because the criteria are based on the kl divergence .",
    "lastly , a non - informative improper prior can be also used to construct criteria using our approach . if we consider the bayesian risk , which is the kl risk , integrated out with respect to the parameters based on the prior distribution , it diverges when the prior is improper . on the other hand ,",
    "when we assume a uniform improper prior for the regression coefficients in the normal linear regression model , we can formally derive the marginal likelihood . in this case , the resulting marginal likelihood is the so - called residual likelihood @xcite , and the proposed information criterion is equivalent to the residual information criterion ( ric ) of @xcite .",
    "thus , our approach can be considered a theoretical justification of the ric .",
    "the rest of the paper is organized as follows . in section [ sec : gen ] , we provide a unified framework , which we use to derive the proposed information criteria , that can produce various information criteria , including the aic , bic , and ric .",
    "then , we propose a new approach that uses the bayesian marginal likelihood in the general framework , and compare various information criteria that are based on a bayesian model . in section [ sec : app ] , we derive our information criteria for the variable selection problem in a normal linear regression model , assuming a prior distribution of the regression coefficients . in this section",
    ", we also prove the consistency of the criteria . in section [ sec : sim ] , we use simulations to verify the numerical performance of the proposed criteria .",
    "lastly , section [ sec : dis ] concludes the paper .",
    "in this section , we describe the concept of information criteria from a general point of view .",
    "let @xmath0 be an @xmath1-variate observable random vector , with density @xmath2 for a vector of unknown parameters @xmath3 .",
    "let @xmath4 be a predictive density of @xmath5 , where @xmath6 is an @xmath1-variate independent replication of @xmath0 . here",
    ", we evaluate the predictive performance of @xmath4 in terms of the following risk : @xmath7 f(\\y|\\bom)\\dd\\y.\\ ] ] because this is interpreted as a risk with respect to the kl divergence , we call it the kl risk . the spirit of aic suggests that we can provide an information criterion for model selection as an ( asymptotically ) unbiased estimator of the information , as follows : @xmath8 , \\end{split}\\ ] ] which is part of ( [ eqn : klg ] ) ( multiplied by 2 ) , where @xmath9 denotes the expectation , with respect to the distribution , of @xmath10 .",
    "let @xmath11 $ ] .",
    "then , the aic variant based on the predictor @xmath4 is defined by @xmath12 where @xmath13 is an ( asymptotically ) unbiased estimator of @xmath14 and @xmath15 is the value of the function @xmath4 evaluated at @xmath16 .",
    "note that @xmath17 produces the aic and bic for specific predictive densities .",
    "( aic ) use @xmath18 as the maximum likelihood estimator @xmath19 of @xmath3 .",
    "then , @xmath20 is the exact aic , or is the corrected aic suggested by @xcite and @xcite , which is approximated by the aic of @xcite as @xmath21 .",
    "( bic ) use @xmath22 as a proper prior distribution @xmath23 .",
    "it is evident that @xmath24 $ ] .",
    "thus , we have @xmath25 , so that @xmath26 , which is the bayesian marginal likelihood . note that @xmath27 is approximated by @xmath28 .",
    "the criterion @xmath17 in ( [ eqn : icg ] ) can produce not only the conventional aic and bic , but also various other criteria .",
    "hereafter , we consider that @xmath3 is divided as @xmath29 , for a @xmath30-dimensional parameter vector of interest @xmath31 , and a @xmath32-dimensional nuisance parameter vector @xmath33 , respectively .",
    "we assume that @xmath31 has prior density @xmath34 , with hyperparameter @xmath35 .",
    "the model is given as follows : @xmath36 where @xmath33 and @xmath35 are estimated from the data .",
    "an inference based on such a model is called an empirical bayes procedure . here , we consider the predictive density @xmath4 as @xmath37 for some estimators , @xmath38 and @xmath39 .",
    "then , the information given in ( [ eqn : fbig ] ) is @xmath40 and the resulting information criterion is @xmath41 where @xmath42 is an ( asymptotically ) unbiased estimator of @xmath43 $ ] .",
    "there are three motivations for considering the information @xmath44 in ( [ eqn : fbi ] ) and the information criterion @xmath45 in ( [ eqn : fbic ] ) .",
    "first , the precision of the bayesian predictor @xmath46 is characterized by the risk @xmath47 in ( [ eqn : klg ] ) , which is based on a frequentist point of view . on the other hand",
    ", the bayesian risk is defined by @xmath48 which measures the prediction error of @xmath4 , under the assumption that the prior information is correct , where @xmath49 .",
    "the resulting bayesian criteria , such as the pic @xcite or the dic @xcite , are sensitive to a prior misspecification because they depend on the prior information .",
    "however , because @xmath47 can measure the prediction error of the bayesian model from a frequentist standpoint , the resulting criterion @xmath45 is less influenced by a prior misspecification .",
    "second , this criterion has the property of consistency . in section [ sec : app ]",
    ", we derive criteria for the variable selection problem in normal linear regression models , and prove that the criteria select the true model with probability tending to one .",
    "the bic and marginal likelihood are known to exhibit consistency , while most aic - type criteria are not consistent .",
    "however , aic - type criteria can choose a good model in the sense of minimizing the prediction error .",
    "our proposed criterion should include both properties , namely consistency when selecting the parameters of interest @xmath31 , and the tendency to select a good model in terms of its predictive ability .",
    "lastly , we can construct the information criterion @xmath45 even when the prior distribution of @xmath31 is improper , because the information @xmath44 in ( [ eqn : fbi ] ) can be defined formally for the corresponding improper marginal likelihood .",
    "however , because the bayesian risk @xmath50 does not exist for the improper prior , we can not obtain the corresponding bayesian criteria or use the bayesian risk .",
    "note that our criterion is equivalent to the residual information criterion ( ric ) of @xcite if we assume a uniform prior on the regression coefficients .",
    "in general , the marginal likelihood based on an improper prior depends on an arbitrary scalar constant , which can be included in the prior , but that might be problematic when selecting the model .",
    "however , the criterion based on our approach , using a uniform prior , can work as a variable selection criterion .",
    "this is discussed further in remark [ rem : uniform ] in section [ sec : app ] .      to clarify our proposed approach , we first explain related information criteria that are based on a bayesian model .",
    "when the prior distribution @xmath34 is proper , we can treat the bayesian prediction risk @xmath51 in ( [ eqn : klb ] ) .",
    "when @xmath52 is known , the predictive density @xmath53 that minimizes @xmath51 is the bayesian predictive density ( posterior predictive density ) @xmath54 , given by @xmath55 when @xmath56 is unknown , we can consider the bayesian risk of the plug - in predictive density @xmath57 . in this case , the resulting criterion is known as the predictive likelihood @xcite or the pic @xcite .",
    "the deviance information criterion ( dic ) of @xcite and the bayesian predictive information criterion ( bpic ) of @xcite are related criteria based on the bayesian prediction risk @xmath51 .",
    "akaike s bayesian information criterion ( abic ) @xcite is another information criterion based on the bayesian marginal likelihood , given by @xmath58 where the nuisance parameter @xmath33 is not considered .",
    "the abic measures the following kl risk : @xmath59 f_\\pi(\\y|\\bla ) \\dd\\y,\\ ] ] which is not the same as either @xmath60 or @xmath51 . the abic is used to choose the hyperparameter @xmath35 in the same manner as the aic .",
    "however , note that the abic works as a model selection criterion for @xmath31 because it is based on the bayesian marginal likelihood .",
    "in this section , we derive variable selection criteria for normal linear regression models .",
    "first , we consider a collection of candidate models , defined as follows .",
    "let the @xmath61 matrix @xmath62 consist of all explanatory variables , and assume that @xmath63 . in order to define candidate models using the index set @xmath64 ,",
    "suppose that @xmath64 denotes a subset of @xmath65 containing @xmath66 elements ( _ i.e. _ , @xmath67 ) and that @xmath68 consists of @xmath66 columns of @xmath62 indexed by the elements of @xmath64 .",
    "we define the class of candidate models as @xmath69 , namely the power set of @xmath70 , where @xmath70 denotes the full model .",
    "we assume that the true model exists in the class of the candidate models @xmath71 , which is denoted by @xmath72 .",
    "note that the dimension of the true models is @xmath73 , which we abbreviate to @xmath74 .",
    "the candidate model @xmath64 is the linear regression model @xmath75 where @xmath0 is an @xmath76 observation vector of the response variables , @xmath68 is an @xmath77 matrix of the explanatory variables , @xmath78 is a @xmath79 vector of the regression coefficients , and @xmath80 is an @xmath76 vector of the random errors . here , @xmath80 has the distribution @xmath81 , where @xmath82 is an unknown scalar and @xmath83 is a known positive definite matrix .",
    "we consider the problem of selecting the explanatory variables , and assume that the true model can be expressed by each candidate model .",
    "this is the common assumption used to derive an information criterion . under this assumption ,",
    "the true mean of @xmath0 can be written as @xmath84 where @xmath85 is a @xmath79 vector , the @xmath86 components of which are exactly @xmath87 , and the remaining components are not @xmath87 .",
    "hereafter , we omit the model index , @xmath64 , for notational convenience . furthermore , we abbreviate @xmath85 as @xmath31 .    now , we construct the variable selection criteria for the regression model ( [ eqn : regmodel ] ) , which has the form ( [ eqn : fbic ] ) .",
    "we consider the following two situations .    *",
    "[ i ] a normal prior for @xmath31*. we first assume a normal prior distribution for @xmath31 , @xmath88 where @xmath89 is a @xmath90 matrix , suitably chosen with full rank .",
    "examples of @xmath89 are @xmath91 for @xmath92 , when @xmath83 is the identity matrix , as introduced by @xcite , or more simply , @xmath93 . for the moment , we assume that @xmath94 is known .",
    "we discuss how to determine it in section [ subsec : example ] .",
    "because the likelihood is @xmath95 , the marginal likelihood function is @xmath96 where @xmath97 .",
    "note that @xmath98 for @xmath99 ; that is @xmath100 .",
    "then , we take the predictive density as @xmath101 , and the information ( [ eqn : fbi ] ) can be written as @xmath102,\\ ] ] where @xmath103 , @xmath104 , and @xmath9 denotes the expectation with respect to the distribution of @xmath105 for @xmath106 .",
    "note that @xmath31 is the parameter of interest , and @xmath107 is the nuisance parameter corresponding to @xmath33 in the previous section .",
    "then , we propose the following information criterion .    the information @xmath108 in @xmath109 is unbiasedly estimated by the information criterion @xmath110 where @xmath111 that is , @xmath112 .",
    "if @xmath113 converges to a @xmath90 positive definite matrix as @xmath114 , @xmath115 can be approximated as @xmath116 , which is the penalty term of the bic . in that case , @xmath117 is approximately expressed as @xmath118 when @xmath1 is large .",
    "note that only the first term of @xmath117 can work as a variable selection criterion because @xmath119 is the bayesian marginal likelihood .",
    "the difference between them is @xmath120 in other words , @xmath117 has a slight additional penalty , of order @xmath121 .",
    "we compare the performance of the criteria using simulations in section [ sec : sim ] .",
    "alternatively , the kl risk @xmath51 in ( [ eqn : klb ] ) can be used to evaluate the risk of the predictive density @xmath122 because the prior distribution is proper .",
    "then , the resulting criterion is @xmath123 which is an asymptotically unbiased estimator of @xmath124 $ ] , where @xmath125 denotes the expectation with respect to the prior distribution @xmath126 ; that is @xmath127 as @xmath128 .",
    "interestingly , @xmath129 is analogous to the criterion proposed by @xcite , known as the consistent aic , who suggested replacing the penalty term @xmath130 in the aic with @xmath131 .    *",
    "[ ii ] uniform prior for @xmath31*. we next assume a uniform prior for @xmath31 , namely @xmath132 .",
    "although this is an improper prior distribution , we can obtain the marginal likelihood function formally , as follows : @xmath133 which is known as the residual likelihood @xcite .",
    "then , we take the predictive density as @xmath134 , and the information ( [ eqn : fbi ] ) can be written as @xmath135,\\ ] ] where @xmath136 , which is the residual maximum likelihood ( reml ) estimator of @xmath82 , based on the residual likelihood @xmath137 .",
    "next , we propose the information criterion .",
    "the information @xmath138 in @xmath139 is unbiasedly estimated by the infomation criterion @xmath140 where @xmath141 that is , @xmath142 .",
    "note that @xmath143 .",
    "if @xmath144 converges to a @xmath90 positive definite matrix as @xmath128 , @xmath145 can be approximated by @xmath116 .",
    "then , we can approximate the criterion as @xmath146 for large @xmath1 . note that @xmath147 is equivalent to the ric proposed by @xcite . noting that @xmath148",
    ", we can see that the difference between @xmath147 and the ric is @xmath149 ; that is @xmath150 .",
    "note too that the criterion based on @xmath151 and @xmath152 can not be constructed because its kl risk diverges to infinity .",
    "[ rem : uniform ] as discussed in section [ subsec : prop ] , the marginal likelihood based on an improper prior depends on an arbitrary scalar constant , which can , in general , be problematic when selecting the model . however , our @xmath153 , or its equivalent ric , can work as a variable selection criterion .",
    "in order to show that , we compare @xmath153 with the aic and bic . when @xmath154 , the aic and bic for the normal linear regression model can be expressed as @xmath155    where the first three terms are the likelihood part , and the last term @xmath156 is the penalty , which depends on @xmath30 . here , @xmath157 for the aic and @xmath158 for the bic .",
    "then , @xmath159 is the residual sum of squares , defined as @xmath160 . on the other hand , @xmath153 in ( [ eqn : fbic_r ] )",
    "can be rewritten as @xmath161 where the first three terms are the same as those of the aic and bic , and @xmath162 is @xmath163 thus , @xmath162 can represent the penalty for the large model because @xmath164 is asymptotically negligible , and the value in the braces of the first term is positive when @xmath1 is at least moderately large , noting that @xmath165 becomes small as @xmath30 becomes large .      in the derivation of the criteria",
    ", we assumed that the scaled covariance matrix @xmath83 of the vector of error terms is known .",
    "however , it is often the case that @xmath83 is unknown , and is some function of the unknown parameter @xmath166 , namely @xmath167 . in that case ,",
    "@xmath83 in each criterion is replaced with its plug - in estimator @xmath168 , where @xmath169 is some consistent estimator of @xmath166 .",
    "this strategy is also used in many other studies , for example in @xcite , who proposed the ric .",
    "we suggest that the @xmath166 be estimated based on the full model .",
    "the scaled covariance matrix @xmath89 of the prior distribution of @xmath31 is also assumed to be known . in practice ,",
    "its structure should be specified , and we have to estimate the parameter @xmath94 in @xmath89 from the data . in the same manner as @xmath83 , @xmath89 in each criterion is replaced with @xmath170 .",
    "note that @xmath94 should be estimated based on each candidate model under consideration , because the structure of @xmath89 depends on the model .",
    "we propose that @xmath94 is estimated by maximizing the marginal likelihood @xmath171 , after substituting in the estimate @xmath172 .    here , we give three examples for the regression model ( [ eqn : regmodel ] ) : a regression model with constant variance , a variance components model , and a regression model with arma errors .",
    "the second and the third models include the unknown parameter in the covariance matrix .    *",
    "[ 1 ] regression model with constant variance*. when @xmath154 , ( [ eqn : regmodel ] ) represents a multiple regression model with constant variance . in this model",
    ", the scaled covariance matrix @xmath83 does not contain any unknown parameters .",
    "* [ 2 ] variance components model*. consider a variance components model @xcite , described as follows : @xmath173 where @xmath174 is an @xmath175 matrix with @xmath176 , @xmath177 is an @xmath178 random vector with distribution @xmath179 for @xmath180 , @xmath181 is an @xmath76 random vector with @xmath182 for known @xmath183 matrices @xmath184 and @xmath185 , and @xmath186 are mutually independently distributed .",
    "the nested error regression model ( nerm ) is a special case of a variance components model , given by @xmath187 where @xmath188 and @xmath189 are mutually independently distributed as @xmath190 and @xmath191 , respectively , and @xmath192 .",
    "note that the nerm in ( [ eqn : nerm ] ) is given by @xmath193 and @xmath194 , where @xmath195 is the @xmath196-dimensional vector of ones , for the variance components model ( [ eqn : vcmodel ] ) .",
    "this model is often used for clustered data , where @xmath188 is considered the random effect of the cluster @xcite .",
    "for such a model , when we are interested in a specific cluster or in predicting the random effects , an appropriate criterion is the conditional aic , as proposed by @xcite , which is based on the conditional likelihood given the random effects .",
    "however , when we wish to predict the fixed effects , namely @xmath197 , the nerm can be seen as a linear regression model and the random effects are part of the error term .",
    "in other words , we consider @xmath198 , @xmath199 for ( [ eqn : regmodel ] ) , where @xmath200 and @xmath201 for @xmath202 . in this case , our proposed variable selection procedure is useful .    *",
    "[ 3 ] regression model with autoregressive moving average errors*. consider the regression model ( [ eqn : regmodel ] ) , assuming the random errors are generated by an @xmath203 process defined by @xmath204 where @xmath205 is a sequence of independent normal random variables , with mean @xmath87 and variance @xmath206 .",
    "a special case of this model is the regression model with @xmath207 errors , satisfying @xmath208 , @xmath209 , and @xmath210 for @xmath211 .",
    "when we define @xmath212 , the @xmath213-element of the scaled covariance matrix @xmath83 in ( [ eqn : regmodel ] ) is @xmath214 .      in this subsection",
    ", we prove that the proposed criteria exhibit consistency .",
    "our asymptotic framework is that @xmath1 tends to infinity and the true dimension of the regression coefficients @xmath74 is fixed .",
    "following @xcite , we first show that the criteria are consistent for the regression model with constant variance and pre - specified @xmath89 .",
    "then , we extend the result to the regression model with a general covariance matrix and the case where @xmath89 is estimated .    we divide @xmath71 into two subsets , @xmath215 and @xmath216 , where @xmath217 and @xmath218 .",
    "note that the true model @xmath72 is the smallest model in @xmath215 , and that @xmath219 , abbreviated to @xmath220 , where @xmath221 is a @xmath222 vector of the true regression coefficients .",
    "let @xmath223 denote the model selected by some criterion .",
    "following @xcite , we make the following assumptions :    ( a1 ) @xmath224 .",
    "( a2 ) @xmath225 and @xmath226 .",
    "( a3 ) @xmath227 , where @xmath228 .",
    "we can now obtain the asymptotic properties of the criteria for the regression model with constant variance .",
    "[ thm : ordinary ] if assumptions ( a1)(a3 ) are satisfied , @xmath215 is not empty , the @xmath229 s are independent and identically distributed ( iid ) , and @xmath230 in the prior distribution of @xmath78 is pre - specified , then the criteria @xmath117 , @xmath231 , @xmath129 , @xmath153 , and @xmath147 are consistent ; that is @xmath232 as @xmath233 .",
    "the proof of theorem [ thm : ordinary ] is given in appendix [ sec : proof ] .",
    "we next consider the regression model with a general covariance structure and the case where @xmath230 is estimated from the data . in this case ,",
    "@xmath83 and @xmath230 are replaced with their plug - in estimators @xmath168 and @xmath234 , respectively .",
    "[ thm : general ] assume that @xmath235 and @xmath236 tend to 0 in probability as @xmath233 , for all @xmath237 .",
    "in addition , assume that the elements of @xmath238 and @xmath239 are continuous functions of @xmath166 and @xmath240 , respectively , and that @xmath238 and @xmath241 are positive definite in the neighborhood of @xmath242 and @xmath243 , respectively , for all @xmath237 . if assumptions ( a1)(a3 ) are satisfied when @xmath68 and @xmath80 are replaced with @xmath244 and @xmath245 , respectively , @xmath215 is not empty and @xmath246 are iid .",
    "then , the criteria @xmath117 , @xmath231 , @xmath129 , @xmath153 , and @xmath147 are consistent .    for the proof of theorem [ thm : general ] , we use the same techniques as those used in the proof of theorem [ thm : ordinary ] .",
    "in this section , we compare the numerical performance of the proposed criteria , @xmath117 and @xmath153 , with that of conventional criteria , namely the aic , bic , dic , and the marginal likelihood .",
    "we consider two regression models : a regression model with constant variance and a regression model with @xmath207 errors .",
    "these models are taken as examples of the linear model ( [ eqn : regmodel ] ) given in section [ subsec : example ] . in each simulation ,",
    "1000 realizations are generated from ( [ eqn : regmodel ] ) , with @xmath247 or @xmath248 .",
    "that is , the full model is @xmath249 and the true model is @xmath250 or @xmath251 .",
    "all explanatory variables are randomly generated from the standard normal distribution .",
    "the signal - to - noise ratio ( @xmath252 ) is controlled at 1 , 3 , and 5 . for the regression model with ar(1 ) errors , we set the ar parameter as @xmath253 .",
    "when deriving the criterion @xmath117 , we set the prior distribution of @xmath31 as @xmath254 ; that is , @xmath93 .",
    "the unknown parameter @xmath255 in @xmath83 for the ar(1 ) model is estimated using the maximum likelihood estimator based on the full model .",
    "the hyperparameter @xmath94 is estimated by maximizing the marginal likelihood @xmath171 , after substituting in the estimate @xmath256 of @xmath82 .",
    "note that @xmath255 is estimated based on the full model , while @xmath82 and @xmath94 are estimated from each candidate model using the plugged - in version of @xmath257 .    as a competitor for @xmath117",
    ", we consider the criterion that uses @xmath258 only , which is the so - called the marginal likelihood commonly used in bayesian analyses .",
    "another competitor is the dic , which is also popular in bayesian analyses .",
    "when deriving the dic , we consider the same prior distribution of @xmath31 as that assumed when deriving @xmath117 , namely @xmath259 . in fairness to the other criteria , we take @xmath82 as an unknown parameter and do not assume a prior distribution for the derivation of the dic .",
    "let @xmath260 .",
    "when @xmath82 is known , the dic is @xmath261 - d(\\bbet),\\ ] ] where @xmath262 denotes the expectation with respect to the conditional distribution of @xmath31 , given @xmath0 and @xmath263 . because @xmath264 for @xmath265 , the first term of the dic is @xmath266 = & \\ 2\\tr\\big [ \\x^t\\v^{-1}\\x \\ { \\si^2 ( \\x^t\\v^{-1}\\x + \\w^{-1 } ) ^{-1 } + \\bbet\\bbet^t \\ } \\big ] / \\si^2 - 4\\y^t\\v^{-1}\\x\\bbet / \\si^2 \\\\ & + ( { \\rm the } \\ { \\rm term } \\ { \\rm which } \\ { \\rm is } \\ { \\rm irrelevant } \\ { \\rm to } \\ { \\rm the } \\ { \\rm model } ) , \\end{aligned}\\ ] ] and the second term is @xmath267 ( the term that is irrelevant to the model )",
    "then , we use dic(@xmath172 ) , where @xmath172 is substituted into dic(@xmath82 ) .",
    "we consider all subsets of the full model as the class of the candidate models .",
    "the performance of the criteria is measured as the number of simulations that select the true model and the prediction error of the selected model , based on the quadratic loss ; that is @xmath268 , where @xmath269 ( i.e. , the gls estimator ) .    , and the right three figures are for @xmath270.,height=755 ]    [ fig : consistency ]    first",
    ", we confirm that @xmath117 , @xmath153 , and the bic are consistent",
    ". figure [ fig : consistency ] shows the number of simulations that select the true model among 1000 realizations of the regression model with constant variance .",
    "the resuls show that each criterion is consistent .",
    "when the data are noisy ( i.e. , the snr is weak ) , @xmath153 does not perform as well as @xmath117 and the bic in terms of selecting the true model .",
    "although we omit the detail , the results for the ar(1 ) model are similar to those of the model with constant variance .",
    ".the mean of the prediction error among 1000 simulations for the regression model with constant variance .",
    "the true model is @xmath250 .",
    "[ cols=\"^,^,>,>,>,>,>,>\",options=\"header \" , ]     [ tab : pe_ar1_pt2 ]    next , we investigate the performance of the criteria in terms of the prediction error .",
    "tables [ tab : pe_const_pt4][tab : pe_ar1_pt2 ] show the mean value of the prediction error among 1000 simulations . in each case",
    ", we put an asterisk at the minimum value of the prediction error . in many cases",
    ", @xmath117 performs best , except when the sample size is small and the snr is weak .",
    "although the performance of the marginal likelihood ( ml ) is similar to @xmath117 , that of @xmath117 is slightly better . for the noisy and small sample size data , which is the exception to the good performance of @xmath117 , @xmath153 performs very well , except for the regression with constant variance with @xmath251 , shown in table [ tab : pe_const_pt2 ] . in this case , the result in figure [ fig : consistency ] show that @xmath153 can not select the true model . in this case",
    ", @xmath153 tends to select larger models .",
    "@xcite also pointed out that the ric , which is equivalent to @xmath153 , does not perform well when the sample size is small and the snr is weak . on the other hand",
    ", @xmath153 performs best in terms of the prediction .",
    "we have derived variable selection criteria for normal linear regression models relative to the frequentist kl risk of the predictive density , based on the bayesian marginal likelihood .",
    "we have proved the consistency of the criteria and , using simulations , have shown that they perform well in terms of the prediction .",
    "although our theoretical approach is general , the derivation of the criterion depends on the normal distribution . if we assume a conjugate prior distribution for the parameter of interest when deriving the criterion , it is easy to extend our approach to other models .",
    "however , for the class of generalized linear models , which includes the poisson and the logistic regression models , it is difficult to consider a prior distribution where the marginal likelihood can be evaluated analytically . in such models , we have to rely on some computational method , which we leave for future research .",
    "variable selection for the mixed effects models , such as the variance components model ( [ eqn : vcmodel ] ) in section [ subsec : example ] , is another important problem .",
    "as discussed , it is appropriate to consider the kl divergence based on the conditional density , given the random effects , when the objective is to predict the random effects , as in the conditional aic ( caic ) .",
    "an extension of our approach to the caic - type criterion is also left to future research .",
    "* acknowledgments . *",
    "the authors are grateful to the associate editor and the anonymous referee for their valuable comments and helpful suggestions .",
    "the first and second authors were supported , in part , by grant - in - aid for scientific research from the japan society for the promotion of science ( jsps ) .",
    "the third author was supported , in part , by nserc of canada .",
    "in this section , we show the derivations of the criteria . to this end , we first obtain the following lemma , which was shown in section a.2 of @xcite .    [",
    "lem : ratio ] assume that @xmath271 is an @xmath183 symmetric matrix , @xmath272 is an idempotent matrix of rank @xmath30 , and that @xmath273 . then , @xmath274 = { \\tr(\\c ) \\over n - p-2 } -{2\\tr[\\c(\\i_n-\\m ) ] \\over ( n - p)(n - p-2)}.\\ ] ]      it is sufficient to show that the bias correction @xmath275 $ ] is @xmath276 , where @xmath108 is given by ( [ eqn : fbi_pi ] ) .",
    "it follows that @xmath277 first , @xmath278",
    "\\non\\\\ = & \\ \\si^2",
    "\\tr(\\a\\v ) + \\bbe^t\\x^t\\a\\x\\bbe .",
    "\\label{eqn : defpi_1}\\end{aligned}\\ ] ] second , noting that @xmath279 for @xmath280 and that @xmath281 , we obtain @xmath282 \\non\\\\ = & \\ { n \\over \\si^2(n - p-2)}. \\label{eqn : defpi_2}\\end{aligned}\\ ] ] finally , @xmath283 \\non\\\\ = &",
    "\\ n\\times \\left\\ { { \\tr(\\a\\v ) \\over n - p-2 } - { 2\\tr(\\a\\v\\p\\v ) \\over ( n - p)(n - p-2 ) } + { \\bbe^t\\x^t\\a\\x\\bbe \\over \\si^2(n - p-2 ) } \\right\\}. \\label{eqn : defpi_3}\\end{aligned}\\ ] ] the latter equation derives from lemma [ lem : ratio ] . combining ( [ eqn : defpi_1 ] ) , ( [ eqn : defpi_2 ] ) , and ( [ eqn : defpi_3 ] ) , we have @xmath284 therefore , @xmath285 because @xmath286 . then , we obtain @xmath287 . @xmath288      from the fact that @xmath289 and that @xmath290 = i_{\\pi,2}(\\si^{2})$ ] , it suffices to show that @xmath291 is approximated by @xmath292 \\\\ \\approx &",
    "\\ e_\\pi e_{\\bom } [ n\\log(2\\pi\\sih^{2 } ) + \\log|\\v| + p\\log n + p ] + ( n+2 ) = e_\\pi e_{\\bom}(\\ic_{\\pi,2 } ) + ( n+2),\\end{aligned}\\ ] ] when @xmath1 is large .",
    "note that @xmath293 is irrelevant to the model .",
    "it follows that @xmath294 \\\\ = & \\ n + n\\times e_\\bom \\left [ { \\y^t\\v^{-1}\\x(\\x^t\\v^{-1}\\x + \\w^{-1})^{-1}\\w^{-1}(\\x^t\\v^{-1}\\x)^{-1}\\x^t\\v^{-1}\\y \\over \\y^t\\ { \\v^{-1 } -\\v^{-1}\\x(\\x^t\\v^{-1}\\x)^{-1}\\x^t\\v^{-1 } \\ } \\y } \\right ] \\\\",
    "= & \\ n + { n \\over \\si^2(n - p-2 ) } \\times e_\\bom \\left [ \\y^t\\v^{-1}\\x(\\x^t\\v^{-1}\\x + \\w^{-1})^{-1}\\w^{-1}(\\x^t\\v^{-1}\\x)^{-1}\\x^t\\v^{-1}\\y \\right ] \\\\",
    "n + { n \\over \\si^2(n - p-2)}\\times \\big [ \\si^2\\cdot\\tr\\ { ( \\x^t\\v^{-1}\\x + \\w^{-1})^{-1}\\w^{-1 } \\ } \\\\ & + \\bbe^t\\x^t\\v^{-1}\\x(\\x^t\\v^{-1}\\x + \\w^{-1})^{-1}\\w^{-1}\\bbe \\big],\\end{aligned}\\ ] ] and that @xmath295 = \\si^2\\cdot \\tr [ \\x^t\\v^{-1}\\x(\\x^t\\v^{-1}\\x + \\w^{-1})^{-1}].\\ ] ] if @xmath144 converges to a @xmath90 positive definite matrix as @xmath128 , @xmath296 \\rightarrow 0 $ ] and @xmath297 \\rightarrow p$ ] . then",
    ", we have @xmath298 , which we want to show .",
    "we show that the bias correction @xmath299 $ ] is @xmath300 , where @xmath138 is given by ( [ eqn : fbi_r ] )",
    ". then , @xmath301 since @xmath302 and @xmath303 , we have @xmath304 . @xmath288",
    "we only prove the consistency of @xmath117 .",
    "the proof of the consistency of the other criteria can be shown in the same manner .",
    "because we have @xmath305 for any @xmath306 , it suffices to show that @xmath307 , or equivalently , that @xmath308 as @xmath233 for @xmath306 .",
    "when @xmath309 , we obtain @xmath310 where @xmath311 for @xmath312 , @xmath313 , @xmath314 , @xmath315 , and @xmath316 .",
    "we evaluate the asymptotic behaviors of @xmath317 , @xmath318 , and @xmath319 for @xmath320 , and @xmath321 , separately .    .",
    "first , we evaluate @xmath317 .",
    "we decompose @xmath322 , where @xmath323 and @xmath324 .",
    "it follows that @xmath325 then , we have @xmath326 and it follows from the assumption ( a3 ) that @xmath327 because @xmath328 and @xmath329 , we obtain @xmath330 second , we evaluate @xmath318 .",
    "it follows that @xmath331 in addition , @xmath332 . then , @xmath333",
    "lastly , it is easy to see that @xmath334 from ( [ eqn : pf1_u1])([eqn : pf1_u5 ] ) , it follows that @xmath335 for all @xmath336 .    .",
    "first , we evaluate @xmath317 . from @xmath337",
    "it follows that @xmath338 for @xmath339 , from ( [ eqn : pf1_o0 ] ) and @xmath340 , we obtain @xmath341 then , @xmath342 second , we evaluate @xmath318 .",
    "since @xmath343 for all @xmath344 , @xmath345 finally , it is easy to see that @xmath346 from ( [ eqn : pf1_o1])([eqn : pf1_o4 ] ) , it follows that @xmath347 for all @xmath344 .",
    "akaike , h. ( 1973 ) .",
    "information theory and an extension of the maximum likelihood principle . in _",
    "2nd international symposium on information theory _ , ( b.n .",
    "petrov and f. csaki , eds . ) , 267281 , akademiai kiado , budapest .",
    "battese , g.e . , harter , r.m . and fuller , w.a .",
    "( 1988 ) . an error - components model for prediction of county crop areas using survey and satellite data .",
    "_ journal of the american statistical association _ , * 83 * , 2836 .",
    "dellaportas , p. , forster , j.j . and ntzoufras , i. ( 1997 ) . on bayesian model and variable selection using mcmc .",
    "_ technical report , department of statistics , athens university of economics and business _ , athens , greece .",
    "zellner , a. ( 1986 ) . on assessing prior distributions and bayesian regression analysis with g - prior distributions . in _",
    "bayesian inference and decision techniques : essays in honor of bruno de finetti _ , ( goel , p.k . and zellner , a. , eds . ) , 233243 , amsterdam : north - holland / elsevier ."
  ],
  "abstract_text": [
    "<S> we propose information criteria that measure the prediction risk of a predictive density based on the bayesian marginal likelihood from a frequentist point of view . </S>",
    "<S> we derive criteria for selecting variables in linear regression models , assuming a prior distribution of the regression coefficients . </S>",
    "<S> then , we discuss the relationship between the proposed criteria and related criteria . </S>",
    "<S> there are three advantages of our method . </S>",
    "<S> first , this is a compromise between the frequentist and bayesian standpoints because it evaluates the frequentist s risk of the bayesian model . </S>",
    "<S> thus , it is less influenced by a prior misspecification . </S>",
    "<S> second , the criteria exhibits consistency when selecting the true model . </S>",
    "<S> third , when a uniform prior is assumed for the regression coefficients , the resulting criterion is equivalent to the residual information criterion ( ric ) of @xcite .    : </S>",
    "<S> aic ; bic ; consistency ; kullback  leibler divergence ; linear regression model ; residual information criterion ; variable selection . </S>"
  ]
}