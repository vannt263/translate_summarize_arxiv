{
  "article_text": [
    "the authors would like to thank the anonymous reviewers for their helpful comments and suggestions .",
    "frank nielsen ( 5793b870 ) would like to thank dr .",
    "kitano and dr .",
    "tokoro for their support .",
    "additional material is available on - line at http://www.informationgeometry.org/sharmamittal/"
  ],
  "abstract_text": [
    "<S> the sharma - mittal entropies generalize the celebrated shannon , rnyi and tsallis entropies . </S>",
    "<S> we report a closed - form formula for the sharma - mittal entropies and relative entropies for arbitrary exponential family distributions . </S>",
    "<S> we instantiate explicitly the formula for the case of the multivariate gaussian distributions and discuss on its estimation .    the sharma - mittal entropy @xmath0  @xcite of a probability density of a continuous random variable @xmath1 in this note . for multivariate densities @xmath2 , </S>",
    "<S> the integral notation @xmath3 denote the corresponding multi - dimensional integral , so that we write for short @xmath4 . </S>",
    "<S> our results hold for probability mass functions , and probability measures in general . ] </S>",
    "<S> @xmath2 is defined as    @xmath5    this bi - parametric family of entropies tends in limit cases to rnyi entropies @xmath6 ( for @xmath7 ) , tsallis entropies @xmath8 ( for @xmath9 ) , and shannon entropy @xmath10 ( for both @xmath11 ) . </S>",
    "<S> sharma - mittal entropy has previously been studied in the context of multidimensional harmonic oscillator systems  @xcite .    </S>",
    "<S> many usual statistical distributions including the gaussians and discrete multinomials ( that is , normalized histograms ) belong to the exponential families  @xcite . those exponential families play a major role in the field of thermo - statistics  @xcite , and admit the generic canonical decomposition    @xmath12    where @xmath13 denote the inner product , @xmath14 is a strictly convex @xmath15 function characterizing the family ( called the log - normalizer since @xmath16 ) , @xmath17 is the natural parameter denoting the member of the family @xmath18 , @xmath19 is the sufficient statistics , and @xmath20 is an auxiliary carrier measure  @xcite . </S>",
    "<S> the natural parameter space @xmath21 is an open convex set .    </S>",
    "<S> for example , the probability density of a multivariate gaussian @xmath22 centered at @xmath23 with positive - definite covariance matrix @xmath24 is conventionally written as    @xmath25    where @xmath26 denote the determinant of the positive definite matrix . rewriting the density of eq .  </S>",
    "<S> [ eq : gaussian ] to fit the canonical decomposition of eq .  </S>",
    "<S> [ eq : expfam ] , we get    @xmath27    using the matrix trace cyclic property , we have @xmath28 , where @xmath29 denote the matrix trace operator . </S>",
    "<S> it follows that    @xmath30    with @xmath31 and @xmath32 ( and @xmath33 ) . in this decomposition , the natural parameter @xmath34 consists of two parts : a vectorial part @xmath35 , and a symmetric negative definite matrix part @xmath36 . </S>",
    "<S> the inner product of @xmath37 and @xmath38 is defined as @xmath39 . for univariate normal distributions , </S>",
    "<S> the natural parameter @xmath40 is @xmath41 . </S>",
    "<S> the order of the exponential family is the dimension of its natural parameter space @xmath42 . </S>",
    "<S> normal @xmath43-dimensional distributions @xmath44 form an exponential family of order @xmath45 .    </S>",
    "<S> we have @xmath46 , that is @xmath47 , and @xmath48 ( since @xmath49 , @xmath50 and @xmath51 ) . </S>",
    "<S> it follows that the log - normalizer @xmath14 expressed using the canonical natural parameters is    @xmath52    in order to calculate the sharma - mittal entropy of eq .  </S>",
    "<S> [ eq : defsm ] , let @xmath53 so that    @xmath54    let us prove that for an arbitrary exponential family @xmath55    @xmath56\\ ] ]    proof : @xmath57.\\end{aligned}\\ ] ] observe that in eq .  </S>",
    "<S> [ eq : obs1 ] , we require @xmath58 for a valid exponential family distribution . this is the case whenever the natural parameter space @xmath42 is a convex cone ( e.g. , gaussian case ) . </S>",
    "<S> it follows from eq .  </S>",
    "<S> [ eq : m ] that the sharma - mittal entropy of a distribution @xmath59 belonging to an exponential family @xmath60 is    @xmath61)^{\\frac{1-\\beta}{1-\\alpha } }     -1 \\right).\\ ] ]    in particular , when the auxiliary carrier measure @xmath33  @xcite ( including the above - mentioned multivariate gaussian family ) , eq .  </S>",
    "<S> [ eq : smh ] becomes a closed - form formula since @xmath62=e_p[1]=1 $ ] :    @xmath63    we derive in limit cases expressions for the rnyi , tsallis and shannon entropies of an arbitrary exponential family ( with @xmath33 ) : @xmath64    note that the shannon entropy of a member of an exponential family @xmath59 indexed with natural parameter @xmath40 can also be rewritten as @xmath65 with @xmath66 the dual moment coordinates , and @xmath67 the legendre @xmath15 convex conjugate of @xmath14  @xcite .    ) for the @xmath68 covariance matrix @xmath69 ( independent of the mean @xmath23 ) , where @xmath70 denotes the identity matrix . </S>",
    "<S> [ fig : sm],scaledwidth=70.0% ]    let us instantiate the generic formula of eq .  </S>",
    "<S> [ eq : smenok ] to the case of multivariate gaussians with mean parameter @xmath23 and covariance matrix @xmath24 . </S>",
    "<S> we get    @xmath71    independent of @xmath23 ( in 1d , @xmath72 so that @xmath73 ) . indeed , consider the expression @xmath74 in eq .  </S>",
    "<S> [ eq : smh ] for the gaussian log - normalizer of eq .  </S>",
    "<S> [ eq : gaussianf ] . using the fact that @xmath75 for @xmath43-dimensional matrices @xmath76 , the term @xmath77 is    @xmath78    similarly , we have @xmath79    thus by subtracting eq .  </S>",
    "<S> [ eq2 ] to eq </S>",
    "<S> .  [ eq1 ] , we obtain    @xmath80    therefore , we deduce that    @xmath81    hence the result of eq .  </S>",
    "<S> [ eq : entgau ] . for 1d gaussians with standard deviation </S>",
    "<S> @xmath82 , this yields @xmath83 note that the differential sharma - mittal entropy of gaussians may potentially be negative .    </S>",
    "<S> figure  [ fig : sm ] displays the plot of the sharma - mittal entropy for a @xmath68 covariance matrix set to @xmath84 , where @xmath70 denotes the identity matrix .    </S>",
    "<S> we also report respectively the rnyi , tsallis and shannon entropies for multivariate gaussians    </S>",
    "<S> @xmath85    figure  [ fig : rt ] displays the plots of the rnyi ( eq .  [ eq : gaur ] ) and tsallis ( eq .  [ eq : gaut ] ) entropies for a @xmath86-dimensional covariance matrix @xmath87 ( @xmath88 ) .    [ cols=\"^,^ \" , ]     information geometry  @xcite considers the underlying differential geometry induced by a divergence . from the sharma - mittal entropy </S>",
    "<S> , we can derive the sharma - mittal divergence  @xcite between two distributions @xmath89 and @xmath90    @xmath91    note that @xmath92 if and only if @xmath93 , since in that case @xmath94 .    for @xmath11 </S>",
    "<S> , the divergence tends to the renown kullback - leibler divergence . </S>",
    "<S> let @xmath95 denote the @xmath96-divergence  @xcite , related to the hellinger integral of order @xmath96 : @xmath97 . </S>",
    "<S> for @xmath98 , the similarity measure @xmath99 is symmetric and called the bhattacharrya coefficient . </S>",
    "<S> the bhattacharrya coefficient is related to the following squared hellinger distance :    @xmath100    we rewrite compactly the sharma - mittal divergence of eq .  </S>",
    "<S> [ eq : smdef ] as    @xmath101    let us prove that for members @xmath102 and @xmath103 belonging to the _ same _ exponential family @xmath60 , we have @xmath104 , where @xmath105 is a jensen difference divergence  @xcite .    proof : @xmath106 observe that for @xmath107 , @xmath108 since @xmath42 is an open convex set , and therefore the distribution @xmath109 is well - defined in eq .  </S>",
    "<S> [ eq : obs2 ] .    </S>",
    "<S> it follows that the sharma - mittal divergence of distributions belonging to the same exponential family ( even when @xmath110 ) is the following closed - form formula    @xmath111    for multivariate gaussians , let us explicit the jensen difference divergence @xmath112 as the difference of two terms using the @xmath113 coordinate system    @xmath114    and @xmath115 . </S>",
    "<S> let @xmath116 denote by @xmath117 and @xmath118 the corresponding parameters . </S>",
    "<S> using eq .  </S>",
    "<S> [ eq : can ] , we have    @xmath119    it follows that the jensen difference divergence between two gaussian distributions @xmath22 and @xmath120 is given by the closed - form formula    @xmath121    with @xmath122    letting @xmath123 , eq .  </S>",
    "<S> [ eq : smdivgaussian ] can further be rewritten compactly as    @xmath124    thus we obtain a closed - form formula for the sharma - mittal divergence of multivariate gaussians generalizing the rnyi @xmath96-divergences , formerly reported in  @xcite :    @xmath125     ( eq .  [ eq : smg ] ) for univariate normal distributions with respective standard deviation @xmath126 and @xmath127 , and mean difference @xmath128 . </S>",
    "<S> [ fig : smdiv],scaledwidth=65.0% ]    figure  [ fig : smdiv ] shows the plot of the sharma - mittal divergence for univariate normal distributions with respective standard deviation @xmath126 and @xmath127 , and mean difference @xmath128 in eq .  </S>",
    "<S> [ eq : smg ] . in practice , for numerical stability , we prefer to compute the divergence by first computing the jensen difference divergence of eq .  </S>",
    "<S> [ eq : smdivgaussian2 ] , and then applying generic formula of eq .  </S>",
    "<S> [ eq : smdiv ] .    </S>",
    "<S> the underlying distribution is usually not explicitly given so that we need to first estimate the distribution or related quantities like its entropy  @xcite . </S>",
    "<S> leonenko et al .  </S>",
    "<S> @xcite proposed a method to estimate entropies using the @xmath129-nearest neighbor graph ( @xmath129-nn ) of an independently and identically distributed sample set @xmath130 . </S>",
    "<S> however , their method suffers from the curse of dimensionality of computing @xmath129-nn graphs and falls short when dealing with moderate dimensions . for exponential families </S>",
    "<S> , we can estimate the natural parameter of an exponential family using the maximum likelihood estimator ( mle ) that admits the unique global optimum  @xcite @xmath131 such that    @xmath132    for multivariate gaussians , from the sufficient statistic @xmath133 we deduce that @xmath134 .    </S>",
    "<S> it follows a simple and fast scheme to estimate the sharma - mittal entropy ( or divergence ) from @xmath135 observations sampled identically and independently from an exponential family distribution : estimate the natural parameter using eq .  </S>",
    "<S> [ eq : mle ] and apply formula of eq .  </S>",
    "<S> [ eq : smenok ] .    to conclude , </S>",
    "<S> let us note that _ any _ arbitrary smooth density can be approximated by an exponential family of order depending on the approximation precision  @xcite ( enforcing no extra auxiliary carrier measure : that is , with @xmath33 ) . </S>",
    "<S> thus we can approximate the sharma - mittal entropy of an arbitrary probability density  @xcite by approximating it to a close exponential family , and then applying the closed - form formula eq .  </S>",
    "<S> [ eq : smenok ] . </S>",
    "<S> we believe that eq .  </S>",
    "<S> [ eq : smenok ] , eq .  </S>",
    "<S> [ eq : entgau ] , eq .  </S>",
    "<S> [ eq : smdiv ] ( numerically stable ) and  [ eq : smg ] will prove useful when experimenting for suitable parameters @xmath136 in various statistical signal tasks  @xcite . </S>"
  ]
}