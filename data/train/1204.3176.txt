{
  "article_text": [
    "an essential part of many scientific problems is to evaluate an integral in a high - dimensional space with the integrand containing a weighting function @xmath0 ( probability distribution function of the configuration @xmath1 ) which is large in some area but close to zero almost everywhere else .",
    "the computational cost of evaluating the integral by conventional quadrature schemes is prohibitive since it demands a huge number of quadrature points inside a high - dimensional space .",
    "this integral can be estimated by the average value of the integrand over a large number of configurations sampled inside the domain randomly , independently and uniformly using monte carlo ( mc ) method .",
    "metropolis and ulam  @xcite ( see  @xcite ) dubbed this simulation method _",
    "monte carlo _ since it uses a huge number of random numbers generated by a computer .",
    "the accuracy of the mc method can be improved by using the importance sampling scheme  @xcite which generates configurations non - uniformly but according to an artificially selected probability density function @xmath2 close to the integrand so that more probability mass is assigned to those configurations with higher probability  @xcite . in order to ensure the sampled configurations are still independent",
    ", it demands the primitive function @xmath3 of @xmath2 and its inverse function @xmath4 .",
    "unfortunately , it is not feasible to find such @xmath2 in most applications of interest . rather than generating independent configurations , the metropolis method  @xcite , which still uses the importance sampling idea , generates ( possibly ) correlated configurations from the original @xmath0 by markov chain .",
    "markov chain makes the algorithm simple and universal .",
    "this method is known as markov chain monte carlo ( mcmc ) method ( see  @xcite ) . since the samples are correlated with each other , the variance of mcmc simulation with the same sample size is larger than the variance of the mc methods using independent configurations . in the rest of this article , we discuss only the mcmc method and refer it as simply the monte carlo ( mc ) method .",
    "the use of averages is common in scientific studies and many quantities related to thermal equilibria are averaged properties measured in real experiments over large numbers of particles and long time intervals . if the ergodic hypothesis applies to the system  @xcite , we can compute those quantities by ensemble averaging instead of time averaging using a partition function , an idea stemming from statistical mechanics .",
    "monte carlo method is a powerful tool based on ensemble averaging idea and so it can be used to calculate the quantities related to the thermal equilibrium state .",
    "a system with fixed particle number @xmath5 , volume @xmath6 , and temperature @xmath7 can be described by canonical ( constant-@xmath8 ) ensemble with the partition function containing only the coordinates of the @xmath5 particles as independent variables .",
    "this description is valid for systems where the quantities of interest depend explicitly only on the location of all the particles .",
    "mc simulations of this system apply a random sequence of displacements to randomly selected particles .",
    "this random selection of particles and displacement is known as a trial move .",
    "the sample sequence it forms generates a ( correlated ) markov chain .",
    "the correlation degree of this sequence depends on the maximal random displacement applied , that is , the step size that determines the acceptance rate of the trial move .",
    "most real experiments are carried out under conditions of controlled pressure and temperature .",
    "thus the isobaric - isothermal ensemble ( constant-@xmath9 ) is widely used in mc simulations where the particle location and the system s volume are randomly modified to visit all possible configurations according to their respective probabilities . here , the step size of the volume - changing trial move also influences the correlation degree of the successive configurations .    in adsorption studies where the chemical potential @xmath10 is fixed , instead of the particle number @xmath5 , the grand - canonical ( constant-@xmath11 ) ensemble is used to calculate the average particle number .",
    "the corresponding mc method includes a displacement trial move and trial insertion and removal of particles with step size usually fixed to one particle , that is , only one particle is tentatively inserted or removed from the volume each time .",
    "the acceptance ratio of particle insertion and removal is very small thus it results in high correlation degree of the related successive configurations .",
    "this correlation degree can not be reduced because the step size is already the minimal divisible unit , a particle .",
    "for the simulation of coexisting phases , important in many engineering applications , the mc algorithms based on the traditional ensembles described above suffer some important drawbacks .",
    "for example , limited computational resources imply that the number of particles used to represent the phase - coexistence system is relatively small .",
    "thus , a large fraction of all particles used reside in the vicinity of the interface between phases .",
    "this induces a bias towards the interfacial properties when ensemble averages are computed , rather than including a balanced representation of the bulk phases .    in the literature",
    "several improvements to the traditional sampling have been proposed . in  @xcite , a gibbs-@xmath8 mc method , where the total particle number , total volume , and temperature are fixed ,",
    "was proposed to alleviate these algorithmic restrictions .",
    "this gibbs-@xmath8 scheme combines @xmath8 , @xmath9 and @xmath11 ensembles for simulating coexisting phases .",
    "this combination skillfully avoids the interface predominance by introducing two subsystems modeled as separate boxes .",
    "this model allows particles to swap from one phase ( box ) to the other while the potential energy between particles from different phases is neglected .",
    "additionally , volume exchange are allowed between the two boxes while the total volume is conserved .",
    "the acceptance ratio of particle swap is very small , as was the case for the grand - canonical ensemble simulation",
    ". this limitation can be particularly severe when the density of one of the phases is very high and becomes important when modeling deposition and separation of dense liquids and solids .",
    "this drawback is avoided in the gibbs - duhem integration method  @xcite .",
    "nevertheless , the gibbs - duhem integration needs the initial point on the coexistence curve , thus it relies on the use of another method that can provide this initial point . if one of the coexisting phases is a crystal , the method proposed in  @xcite improves the acceptance probability of exchanging particles .",
    "there are a lot of successful applications of the mc method based on gibbs ensemble for water systems  @xcite and the oil production and processing  @xcite . in these applications",
    "the solubility of hydrogen sulfide and other corrosive components in the gas - hydrocarbon mixtures is important data .",
    "nevertheless , this solubility is poorly understood due to lack of experimental results . in gibbs-@xmath8 ensemble simulations of two coexisting phases",
    ", there are three kinds of trial moves : particle displacement , volume exchange , and particle swap . in order to reduce variance of the simulation results by decreasing the correlation degree of configurations ,",
    "we adjust the step size for the first two trial moves . a discussion of the relationship between the variance and the step size of particle displacement is given in  @xcite but it is usually difficult to obtain a general rule for such a relationship .",
    "recently  @xcite , the liquid - vapor coexistence of methane is first simulated by the gibbs-@xmath8 mc method and then the variation of mole fraction with pressure in a two - component system at phases coexistence state is studied by the gibbs-@xmath9 mc method proposed in  @xcite , where the total particle number , pressure and temperature are fixed .",
    "when markov chain evolution is used for monte carlo simulation , it is not advisable to sample the system for the quantities of interest after each trial move .",
    "this requires too much memory while the correlation in the data is high ; instead , the system is sampled at intervals ( sampling interval ) .",
    "the larger the sampling interval is , the smaller the correlation degree will be .",
    "the same applies to the variance with fixed sample size ( i.e. , the total number of sampled cycles ) .",
    "the computational cost ( i.e. , simulation time ) is almost proportional to the product of the number of samples collected and the sampling interval .",
    "thus , increasing the sampling interval implies to either increase the simulation time when keeping the number of samples constant or to increase the variance of the results , when keeping the simulation time constant .",
    "nevertheless , our simulation results show that we can achieve a good trade - off between total simulation time and memory usage . in this paper",
    ", we describe the gibbs-@xmath8 mc method and employ it to model the coexisting phases of a lennard - jones ( lj ) fluid . to make the problem tractable for theoretical analysis",
    ", we analyze the influence of the sampling interval and sample size on the variance of the simulation results for an idealized fluid , rather than the lj fluid system .",
    "finally , a general theoretical analysis is proposed to justify and prove some of the empirical observations and rules proposed .",
    "suppose we wish to evaluate the following average : @xmath12 to compute @xmath13 , it is convenient to use the monte carlo method , based on markov chain , to generate correlated configurations @xmath14 after each cycle with a probability density proportional to @xmath0 .",
    "note that only @xmath0 and not its integral , @xmath15 , is used in the mc method .",
    "the configuration @xmath16 at each sampled cycle is used to estimate the expected value @xmath13 by the average value @xmath17 over a large sample set .",
    "the algorithm  @xcite of monte carlo method using markov chain for solving the general integral   can be summarized as follows :    1 .",
    "initialization of configuration;[step1 ] 2 .   for each cycle",
    ": a.   apply trial move algorithm;[step2a ] b.   apply acceptance criterion;[step2b ] 3 .",
    "sample the system at regular intervals ; 4 .",
    "stop after getting sufficient samples for analysis .",
    "[ step4 ]    the initial configuration can be selected randomly from within the domain @xmath18 of the definition of the configuration space .",
    "the markov chain is generated by randomly modifying the current configuration @xmath1 into @xmath19 using the trial move algorithm .",
    "the algorithm outlined in steps  [ step1 ] to  [ step4 ] should satisfy the ergodicity and time - reversal conditions .",
    "the ergodicity condition requires that from the current configuration @xmath1 it is possible to visit any @xmath20 by a limited number of trial moves .",
    "the time - reversal condition requires that the probability for the system to change back to its previous state is larger than zero .",
    "the probability density of the trial move event @xmath21 is denoted by @xmath22 .",
    "the algorithm can be simplified significantly by using the symmetric condition that the probability of the trial move from @xmath1 to @xmath19 is equal to the probability of the reverse move , that is , @xmath23 .",
    "any configuration generated in step  [ step2a ] will be accepted or rejected in step  [ step2b ] based on the following acceptance criterion : the new configuration @xmath19 is accepted if @xmath24 ( random number distributed uniformly inside [ 0 , 1 ] ) is less than @xmath25 or rejected otherwise .",
    "this means that the acceptance probability equals to @xmath26.\\ ] ] this selection for the acceptance probability is based on the detailed balance condition for equilibrium state that can be stated as @xmath27 and also on the fact that @xmath28}{\\min\\left[1 , \\beta^{-1}\\right]}\\equiv\\beta.\\ ] ] we note that the detailed balance condition is a sufficient but not a necessary requirement and that in  @xcite it was shown that the weaker balance condition is a sufficient requirement .",
    "samples are collected in step 3 after the simulation has reached the statistically steady state , that is , after an initial transitional period .",
    "the quantities of interest are estimated from samples collected every @xmath29 cycles .",
    "+   +    in the gibbs-@xmath8 ensemble  @xcite , as described in  @xcite , the probability density function and the related partition function are expressed as : @xmath30}{q_{g}(n , v , t)v\\lambda^{3n}n_1!(n - n_1 ) ! } \\ ] ] and @xmath31 where @xmath7 is the temperature of both boxes , @xmath32 is the particle number inside box 1 , @xmath33 is the volume occupied by box 1 , @xmath34 and @xmath35 are the positions of the @xmath32 and @xmath36 particles normalized by the size @xmath37 and @xmath38 of box 1 and 2 , respectively , @xmath39 is the thermal de broglie wavelength with @xmath40 , @xmath41 is the planck constant , @xmath42 is the boltzmann constant , @xmath43 is the total potential energy of box 1 , namely a summation of pair potential energy @xmath44 contributed by particles @xmath45 and @xmath46 inside box 1 .",
    "the expressions for the probability density function   and the related partition function   are obtained after completing the integration with respect to the momentum variables . for lj fluid",
    ", we have : @xmath47\\ ] ] where @xmath48 and @xmath49 are the coordinates of particle @xmath45 . to simplify our computations , we can substitute eq .   by a truncated potential @xmath50 with correction by @xmath51\\ ] ] for the total energy in the corresponding box due to contributions beyond the cutoff distance @xmath52 . in mc simulations , it is convenient to use non - dimensional quantities .",
    "the resulting non - dimensional system is defined by the following normalized quantities : number density @xmath53 , pressure @xmath54 , temperature @xmath55 , and energy @xmath56 .    as the probability distribution function of eq .",
    "contains three independent variables , three kinds of trial moves are used in the related mc algorithm : particle displacement , volume exchange , and particle swap ( see fig .",
    "[ fig : trial move ] ) .",
    "the acceptance probability for the three kinds of trial moves can be obtained according to the detailed balance condition mentioned above in section  [ ss : basic algorithm ] .",
    ".,scaledwidth=60.0% ]    we simulate phase coexistence of an lj fluid by the monte carlo method based on gibbs-@xmath8 ensemble .",
    "the cutoff distance for the two boxes is fixed at 45% ( smaller than a half ) of the relative box size .",
    "this box sizes are modified in each volume exchange trial move .",
    "one thousand particles are used in our simulation and the initial normalized density of the two boxes is @xmath57 unless otherwise stated .",
    ".,scaledwidth=60.0% ]    .,scaledwidth=60.0% ]    in each cycle a trial move is applied .",
    "this trial move is selected randomly from three possible kinds .",
    "the probability for selecting the displacement trial move is 0.9 , but it is 0.01 for volume exchange and 0.09 for particle swap . after a transitional period ( about @xmath58 cycles for the current simulations ) , we sample the system every 50 cycles , that is , @xmath29=50 .",
    "the initial values of @xmath59 and @xmath60 are chosen to be 0.1 ( see fig .  [",
    "fig : adjust , t=0.9 ] ) . in order to make the acceptance ratios of the related trial moves approach to user - defined values ,",
    "the step sizes are modified according to an auto - adjusting algorithm ( see the source code mentioned in the preface of  @xcite ) using the collected information .",
    "these step sizes are reset at the end of each @xmath61 cycles .",
    "the self - adjusting procedure utilized ensures that by the completion of the initial @xmath62 cycles , the step sizes of the different trial moves are such that the acceptance ratios of those trial moves are approximately the predetermined value ( 0.5 in the current simulations ) .",
    "once the first @xmath62 steps are executed , the step sizes are kept fixed for the remaining of the computation .",
    "these fixed step sizes ensure that the following trial moves are symmetric .",
    "the reported average values are computed using @xmath63 correlated samples .    for @xmath64=0.9",
    ", it shows in figs .",
    "[ fig : rho , t=0.9]-[fig : pre , t=0.9 ] that the normalized density , volume , and pressure of the two boxes reach convergence after the predetermined @xmath58 cycles .",
    "notice that before @xmath62 time steps are complete , the step sizes @xmath59 , @xmath60 are adjusted and the related achieved acceptance ratios are changed correspondingly and finally approach to the predetermined value of 0.5 as shown in fig .",
    "[ fig : adjust , t=0.9 ] .",
    "after @xmath62 cycles , the step sizes are fixed to their latest values and the related acceptance ratios fluctuate around 0.5 as desired .",
    "[ fig : adjust , t=0.9 ] also shows that the acceptance ratio of particle swap between boxes is only about 0.0016 because the density of box 1 is very high ( see fig .",
    "[ fig : rho , t=0.9 ] ) and this situation will get worse as the density increases . as discussed in the introduction , this acceptance ratio can not be improved although it results into high correlation degree of the successive configurations .",
    "the results of @xmath65 for different @xmath64 are shown in fig .",
    "[ fig : temp - rho ] including comparison with results computed using the state of equation  @xcite and mc simulations  @xcite .",
    ".,scaledwidth=60.0% ]        .,scaledwidth=60.0% ]    .,scaledwidth=60.0% ]    .,scaledwidth=60.0% ]    as shown in figs .",
    "[ fig : rho , t=0.9]-[fig : pre , t=0.9 ] , it seems that the statistical noise of the simulation results in the dense - phase box is larger than in the other box .",
    "a similar observation is made in  @xcite .",
    "for example , the simulation results of @xmath66 with initial density still 0.3 are shown in figs .",
    "[ fig : rho , t=1.25]-[fig : pre , t=1.25 ] where we observe that the intensity difference of statistical noise of the two phases is reduced with the decrease of the density difference .",
    "in mc simulations , each sample @xmath14 is a measurement of a random variable @xmath1 with exact but unknown probability distribution , from which we define the expected value @xmath67 .",
    "if the measurements can be taken as independent , the variance @xmath68 of the average quantity @xmath69 is inversely proportional to the size @xmath70 of the sample set .",
    "but , if they are correlated with each other , the variance also depends on the sampling interval @xmath29 between two successive samples .    in the blocking method",
    "@xcite , the following transformation is employed to decrease the sample size till @xmath71 @xmath72 after each blocking step , we get a new value for @xmath73 which will increase during the blocking process and approximates @xmath68 if convergence is achieved .",
    "the value at the convergence point is used to estimate the variance of the average value . if the blocking process does not converge",
    ", the largest value during the blocking process is a lower bound of the variance  @xcite .",
    "convergence will happen if the sample set covers a time span which is several times larger than the maximal correlation interval @xmath74 of the sample set so that the blocking variables @xmath75 at the convergence point are independent gaussian variables .",
    "the subtlety of the blocking method is to decrease the correlation degree of the new sample set @xmath76 making the correlated functions @xmath77 tend to zero .",
    ", @xmath78 and @xmath79 are the pressure variance in boxes  1 and  2 , respectively , @xmath80 and @xmath81 are the density variance in boxes  1 and  2 , respectively , and @xmath82 and @xmath83 are the volume variance in boxes  1 and  2 , respectively.,scaledwidth=50.0% ]    we take the set of samples after each trial move as the full sample set . if the trial move is accepted , the next sample is different from the previous one but if rejected , the configuration remains unchanged and the next sample is the same as the previous one .",
    "the repeated samples induce high correlation degree in the sample set .",
    "these repeated samples are reasonable from the point of view of statistics but contain little useful information .",
    "the lower the correlation degree is , the smaller the variance with a given sample size will be . instead of sampling",
    "after each trial move , we could add a sample to the set after @xmath29 cycles , for example .",
    "the new sample set will be referred to as coarse sample set , which is a subset of the full sample set .",
    "we can reduce the correlation degree of the coarse sample set by increasing @xmath29 and its size is denoted by @xmath70 . in mc simulations , only the coarse sample set is stored and the memory or disk usage requirements can be reduced significantly compared to storing the full sample set if @xmath29 is much larger than one .",
    "the average value and the corresponding variance are calculated using the coarse sample set .    in the above simulation of a lj fluid with @xmath84",
    ", we observed that the statistical noise of the simulation results in the dense - phase box is larger than in the other box .",
    "[ fig : variance , t=0.9 ] shows that the variance results estimated by the blocking method .",
    "the variances of the normalized density , pressure of box 1 with dense phase are larger than those of box 2 ( the final wild fluctuation is due to numerical instabilities when @xmath85 becomes too small ) but their volume variances are the same as the total volume @xmath6 is fixed , which is consistent with the data shown in figs .",
    "[ fig : rho , t=0.9]-[fig : pre , t=0.9 ] .",
    "the cpu time is proportional to the total cycle times @xmath86 which is almost equal to @xmath87 ( @xmath88 but the cycle times @xmath62 before convergence is negligible ) .",
    "we discuss the influence of @xmath70 and @xmath29 on the variance in what follows .",
    "the rules we obtain are expected to be independent on the particular mc simulation used to generate the correlated sample set . for simplicity ,",
    "an artificial ideal system , which is much simpler than the lj system , is used in following simulations .    in the ideal system ,",
    "the particle number @xmath5 is equal to 12 and particle coordinate only takes integral numbers @xmath89 as in the ising model and the probability distribution function is : @xmath90\\exp\\left[-u_2\\left(s_2^{n - n_1}\\right)\\right]}{n_1!(n - n_1)!}\\ ] ] where @xmath91 is a summation spanning over all pairs ( particle s periodic images are neglected here ) located inside the same box and @xmath92 so that the acceptance ratio is not too small . for this model",
    ", we only need the spin trial move and the trial move of particle swap .",
    "the properties of the two boxes are equivalent , thus the correlation degree of their sample sets is the same .",
    "the mc simulation results show that blocking processes of the @xmath93 sample sets from the two boxes are very similar as shown in fig .",
    "[ fig : ising variance ] . during the blocking process",
    ", only the evolution of @xmath94 at the initial stage provides useful information relative to the approaching process to the variance .",
    "as @xmath85 shrinks , the evolution of the blocking process becomes unstable , leading to wild fluctuations .",
    "these fluctuations can be arbitrarily large increasing or decreasing the computed value .",
    "those fluctuations are due to numerical instabilities and only serve to bound the trust worthy region of the blocking computation .",
    "these instability do not cause any problem if @xmath94 converges before losing stability ( see fig .",
    "[ fig : ising variance ]  ( left ) ) .",
    "thus , the value at the convergence point can be used to estimate the corresponding variance .",
    "but , in some cases where @xmath94 can not converge before losing stability ( see fig .",
    "[ fig : ising variance ]  ( right ) ) , the problem is that it is difficult to judge where the separation point of the two stages is located , thus the lower bound of the variance , the largest value before losing stability , is unknown . when using two sample sets with the same correlation degree , their initial stage should be the same and the final stage with drastic fluctuation is random , and so it is easy to find the separation point of the two stages . as shown in fig .",
    "[ fig : ising variance ]  ( left ) using @xmath95 samples , the two curves overlap with each other and deviate after blocking 14 times which is the separation point .",
    "as it converges before the separation point , the variance for this two sample sets is about @xmath96 . in fig .",
    "[ fig : ising variance ]  ( right ) using only @xmath97 samples , the two curves overlap with each other before blocking 10 times which is the separation point but are still not converged at the separation point and so , the lower bound of the variance is about @xmath98 , which is the largest value before losing stability . using different sample sets with similar correlation degrees simplifies the computation of the variance lower bound , nevertheless in real mc simulations this would incur prohibitive computational demands , both memory and cpu time .",
    "thus , as shown in fig .",
    "[ fig : ising variance ] , we propose to use the first maximal point in the blocking process as the separation point and use it to estimate the variance lower bond",
    ". this observation is justified by the fact that @xmath94 is a non - decreasing quantity theoretically , the oscillation shown in figs .",
    "[ fig : variance , t=0.9 ] and  [ fig : ising variance ] can be justified by the loss of stability of the blocking computation .",
    ".variance of markov chain monte carlo simulation results with different sample size @xmath70 and sampling interval @xmath29 [ cols=\"^,^,^,^,^\",options=\"header \" , ]",
    "in section  [ s : variance observation ] , we discussed the relationship between the variance and the sample size @xmath70 and sampling interval @xmath29 have some independent rules , namely eq .  - .",
    "these rules are independent of the blocking method used to calculate the variance and reflect the underlying feature of the statistical rules . in this section a theoretical analysis is presented to validate these observed rules .     of sample sets with different correlation degree.,scaledwidth=100.0% ]",
    "let @xmath99 be the results of a consecutive measurement of a random variable @xmath1 in a monte carlo simulation in thermal equilibrium state , which has the following features  @xcite : @xmath100 where @xmath101 denotes the expected value with respect to these exact but unknown probability distributions . in mc simulations , we estimate the expected value @xmath67 by the average quantity @xmath69 and the variance of @xmath102 is  @xcite : @xmath103\\ ] ] where @xmath104 and @xmath105 . in monte carlo simulation , it is reasonable to assume : @xmath106 where the equality holds when the samples are independent with each other .",
    "[ fig : correlation degree ] shows some representative results of @xmath107 in usual mc simulations .",
    "[ fig : correlation degree ]  ( left ) shows the results of a high - correlation sample set compared to fig .",
    "[ fig : correlation degree ] ( middle ) . in the limit case where each sample is an independent random variable , @xmath107 is equal to a constant for @xmath108 and zero otherwise as shown in fig .",
    "[ fig : correlation degree ]  ( right ) .",
    "we use these schematic models only to show the contour distributions , the monotone interval and the location of maximum value .",
    "these models make it easy to understand the following linear interpolation scheme .",
    "assume that the sample size @xmath70 is much larger than the maximal correlation interval @xmath74 , then the variance @xmath68 with fixed sampling interval @xmath29 is inverse proportional to @xmath70 .    for a given @xmath29 ,",
    "the correlation degree is fixed and we always can make an estimation for the value of @xmath74 where @xmath109 ( see fig .  [",
    "fig : correlation degree ] ) .",
    "then , eq .   is simplified to @xmath110      \\approx\\frac{1}{n}\\left[\\gamma_0 + 2\\sum_{t=1}^{\\tau}\\left(1-\\frac{t}{n}\\right)\\gamma_t\\right]\\ ] ] assuming the sample size @xmath70 is large enough such that @xmath111 , we conclude that @xmath112      \\approx\\frac{1}{n}\\left[\\gamma_0 + 2\\sum_{t=1}^{\\tau}\\gamma_t\\right]\\ ] ] which is consistent with eq .  .",
    "the relationship between the variance and the sample size in theorem 5.1 is well - known for independent sample set but holds for correlated sample set only if the sample size @xmath111 that is satisfied in the data presented in tables  [ tab : variance(n , d)]-[tab : tau(n , d ) ] .",
    "in fact , @xmath111 means that the length of contours with nontrivial value in fig .",
    "[ fig : correlation degree ] is proportional to the sample size @xmath70 and so @xmath113 is also proportional to @xmath70 making @xmath114 inverse proportional to @xmath70",
    ".    given two sample sets with the same sample size @xmath70 but different sampling intervals , @xmath115 and @xmath116 @xmath117 , respectively .",
    "if @xmath111 and @xmath118 , their variances satisfy @xmath119    we first discuss two sample sets : @xmath120 containing @xmath121 samples and @xmath122 containing @xmath70 samples generated once from each @xmath29 samples of @xmath123 .",
    "according to eq .  , we have : @xmath124 as shown in fig .",
    "[ fig : summation model ] , @xmath125 is summation over all vertexes ( without repeating ) of small black quadrilaterals but @xmath126 is summation over only the bottom - left vertexes of larger quadrilaterals , which are marked by red and blue colors and have indexes as @xmath127 $ ] .    in the area",
    "@xmath128 of each blue quadrilateral centered at the maximum value of @xmath129 ( see fig .  [",
    "fig : summation model ] ) , it can be observed that @xmath130 where the equality holds when @xmath131 .",
    "this can be understood by considering one of the blue quadrilaterals in fig .",
    "[ fig : summation model ] ( left ) .",
    "then , realizing that the leftmost summation , @xmath132 , only contains the bottom left - hand corner of the blue quadrilateral , namely a maximum value which lies on the diagonal . multiplying",
    "this maximum value by @xmath29 will be lower or equal to @xmath133 , having @xmath29 maximum terms and other terms with smaller but still positive values .",
    "the second part of the inequality stems from the fact that @xmath134 is multiplying one maximum term , and this will always be greater than @xmath133 , having @xmath134 terms but only @xmath29 terms taking maximum values .    in the area @xmath135 of those red quadrilaterals located always at the monotone interval of @xmath129 , we assume @xmath29 is much smaller than @xmath74 thus the linear interpolation is valid in each small local area with size @xmath29 . for the representative red quadrilateral shown in fig .",
    "[ fig : summation model ] ( right ) with @xmath136 , we have @xmath137 . according to linear interpolation , @xmath138 , @xmath139 and @xmath140 .",
    "thus , we have the following estimation : @xmath141 generally , the following approximation for any arbitrary red quadrilateral is valid : @xmath142 according to eqs",
    ".  - , we have @xmath143 at this point we assume eq .",
    "valid and apply it to the sample set @xmath144 , thus we get ( @xmath145 and @xmath146 ) : @xmath147 where the equality holds also at the same condition of @xmath131 .",
    "substituting eq .   into eq .",
    ", we have @xmath148 now introducing @xmath149 containing @xmath70 samples as @xmath144 but having the same correlation degree as @xmath123 and taking eq .   into consideration ,",
    "we have : @xmath150 with which we finish the proof .    taking the sample set with @xmath115 in eq .   as @xmath151 and the other as @xmath144",
    ", it is easy to see that eq .",
    "is equivalent to eq .",
    "proved here . note that if the sample set @xmath123 ( namely @xmath151 ) has a high correlation degree making the summation over the area @xmath135 dominant ( see fig .  [",
    "fig : correlation degree ]  ( left ) ) , @xmath152 converges to @xmath153 according to eq .  .",
    "but in contrast , @xmath154 if the sample in @xmath123 are independent .",
    "the assumptions of theorem 5.2 are that the sample size @xmath70 is much larger than @xmath74 and sampling interval @xmath29 is much smaller than @xmath74 , which are satisfied in the data shown in tables  [ tab : variance(n , d)]-[tab : tau(n , d ) ] . in real applications , @xmath70 should be much larger than @xmath74 since otherwise the variance of the average value is very high , which makes the average value not trustable . for the selection of @xmath29",
    ", we suggest to let @xmath29 much larger than 1 to reduce the memory usage .",
    "in addition , we also suggest to let @xmath29 much smaller than @xmath74 as otherwise too much correlated information , which is still effective to reduce the variance , would be lost .",
    "thus , the two necessary assumptions can be easily satisfied in real applications .",
    "the influence of the sample size and sampling interval used in mc simulations on the variance of the average quantities is analyzed by numerical results and proved by theoretical analysis . in the case of large sample size",
    ", the variance with fixed sampling interval is inversely proportional to the sample size and the cpu time . for a given cpu time",
    ", the memory or disk usage ( namely the sample size ) can be reduced greatly by increasing the sampling interval and sometimes the corresponding increase in the variance is negligible .    in the implementation of the blocking method , the blocking process is subject to increased fluctuations when the sample size @xmath85 is reduced ; in particular , the fluctuation gets its worst value when @xmath85 approaches to two .",
    "the current results show that the fluctuation starts near the first maximal point obtained during the blocking process .",
    "additionally , the corresponding maximal value can be used as an estimate of the variance if the blocking process converges or as a lower bound estimate of the variance if the blocking process does not converge ."
  ],
  "abstract_text": [
    "<S> in monte carlo simulations , the thermal equilibria quantities are estimated by ensemble average over a sample set containing a large number of correlated samples . </S>",
    "<S> as the stochastic error of the simulation results is significant , it is desirable to understand the variance of the estimation by ensemble average , which depends on the sample size ( i.e. , the total number of samples in the set ) and the sampling interval ( i.e. , cycle number between two consecutive samples ) . </S>",
    "<S> although large sample sizes reduce the variance , they increase the computational cost of the simulation . for a given cpu time , the sample size can be reduced greatly by increasing the sampling interval while the corresponding increase of the variance is negligible . in this work , </S>",
    "<S> we report a few general rules that relate the variance with the sample size and the sampling interval . </S>",
    "<S> these relations were observed in our numerical results . </S>",
    "<S> the main contribution of this work is the theoretical proof of these numerical observations and the set of assumptions that lead to them .    </S>",
    "<S> phase coexistence , gibbs ensemble , molecular simulation , monte carlo simulation , variance estimation , blocking method </S>"
  ]
}