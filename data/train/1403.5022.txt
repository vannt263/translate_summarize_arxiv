{
  "article_text": [
    "many tracking problems ( e.g. , radar - based surveillance ) involve the estimation of the number and states ( e.g. , position , velocity ) of targets through unlabelled measurements .",
    "this problem is complicated by data association , or the unknown correspondence between measurements and targets .",
    "traditionally , there have been two basic approaches to this .",
    "the first is that utilised in methods such as the multiple hypothesis tracker ( mht ) @xcite . in this approach ,",
    "the likelihood of several feasible measurement - target association hypotheses is evaluated , and the solution provided is the target state conditioned the maximum a posteriori ( map ) hypothesis .",
    "this formulation is well - known to provide excellent performance in challenging problems such as those involving closely - spaced targets .",
    "the difficulty is its computational tractability , which limits its application in problems with high false alarm rates , or with association ambiguity between many targets .",
    "the alternate traditional approach has been joint probabilistic data association ( jpda ) @xcite , which adopts a philosophy that the goal is to estimate the target state , and that data association is a nuisance variable .",
    "consequently , association is addressed by taking a total probability expansion over feasible measurement - target association hypotheses , marginalising over the nuisance variable .",
    "jpda provides an improved computation - performance trade - off for problems in which targets remain well - spaced ; however , the method fails in problems in which targets become well - spaced due to a difficulty known as coalescence ( e.g. , @xcite ) .",
    "this is a direct result of the unknown measurement - target association : after targets have become closely - spaced , posterior distributions become strongly multimodal , where the different modes correspond to permutations of targets . in other words , coalescence reveals the underlying nature of the problem as an unlabelled set , motivating application of methods from random finite sets ( rfss ) .",
    "rfs distributions provide an integrated mathematical framework for addressing estimation problems in which states to be estimated and/or observations form sets @xcite .",
    "the difference between random sets and random vectors is that realisations of a random set will generally contain different numbers of elements , and the ordering of the elements is meaningless .",
    "we will use the lower case notation @xmath0 to refer to elements of the underlying state space ( e.g. , position , velocity ) , and upper case @xmath1 to refer to finite sets of state space elements .",
    "rfs distributions encode both uncertainty in cardinality and uncertainty in values into a single representation .",
    "uncertainty of this form could be represented via a cardinality distribution @xmath2 and a series of conditional state distributions @xmath3 ; the relationship between the rfs distribution @xmath4 and these components is : @xmath5 where @xmath6 is the set of all @xmath7-element permutations ( defined below ) , the sum over which ensures permutation invariance , a necessary property for a set distribution .",
    "[ def : completepermutations ] denote by @xmath8 the set of complete permutation functions on @xmath9 : @xmath10    early work in rfss focused on simple representations such as the probability hypothesis density ( phd ) @xcite and cardinalised phd ( cphd ) @xcite , which have provided interesting practical and theoretical results ( e.g. , @xcite ) .",
    "recently , improved performance has been demonstrated through parallel derivations of conjugate prior forms for target tracking using unlabelled rfss @xcite and labelled rfss @xcite . in each case , the form of the exact filter is a linear combination of multi - bernoulli ( mb ) distributions .",
    "the complexity of exact methods is problematic as the number of terms in the linear combination grows exponentially in the number of targets ; this is the problem of data association .",
    "the complexity is addressed in @xcite by seeking a mb distribution that approximates the posterior . observing that the linear combination may be viewed as a marginalisation over a latent association variable @xmath11 ( as in jpda , where @xmath12 is the set of all association hypotheses , to be defined later ) , the methods proposed approximated the probability distribution of data association , @xmath13 .",
    "this resulted in two approaches , the first of which was closely related to jpda and joint integrated pda ( jipda ) @xcite and hence suffered from coalescence , and the second of which was related to the member filter @xcite .",
    "the latter was found to be more robust , but exhibited lower performance when targets were well - spaced .",
    "working within the rfs framework has several advantages .",
    "one of these is the ability to rigorously define measures of distortion caused to an entire multi - object distribution @xmath4 when approximating it by another multi - object distribution @xmath14 via the set kullback - leibler ( kl ) divergence , as defined in ( * ? ? ? * p513 ) : @xmath15 where the set integral is ( * ? ? ?",
    "* p361 ) @xmath16 @xmath17 the set kl divergence in encompasses changes to both the distribution of cardinality , and the target state distribution .",
    "as explored further in section [ ss : backklmin ] , minimisation of kl divergence is the standard approach for finding the distribution in a family that best matches a particular exact distribution .",
    "for example , the phd and cphd filters both calculate the distribution which minimises the set kl divergence within their respective families .",
    "thus , a compelling alternative to the algorithms in @xcite would be to find the mb distribution which minimises the set kl divergence from the exact distribution .",
    "the first contribution of this paper is an efficient , approximate method of finding the mb distribution that minimises the set kl divergence .",
    "the resulting algorithm is shown to be related to set jpda ( sjpda ) @xcite , but is a rfs - based alternative that accommodates uncertainty in the number of targets .",
    "furthermore , the computational complexity of the proposed method is drastically lower than sjpda .",
    "specifically , for problems involving @xmath18 targets , @xmath19 single - target association hypotheses , and @xmath20 joint association hypotheses ( where typically @xmath21 ) , the proposed method requires solution of a network flow linear program ( lp ) with @xmath22 variables , whereas sjpda requires iterative solution of @xmath23 @xmath18-element assignment problems .",
    "since the term _ variational inference _ is widely used to refer to statistical approximations based on optimisation ( e.g. , @xcite ) , we refer to our method as the variational mb ( vmb ) filter .",
    "the optimal sub - pattern assignment ( ospa ) was introduced in @xcite as a distance metric for the difference between two sets of points ; it is defined below .",
    "[ def : ospa ] if @xmath1 , @xmath24 , and @xmath25 , ospa is defined as :    @xmath26^{\\frac{1}{p}}\\ ] ]    where @xmath27 , @xmath28 is a distance function on the single target state space , @xmath29 is a real number indicating the cost of a target not having a corresponding estimate ( or vice versa ) , and @xmath30 is a permutation function in the set @xmath8 of definition [ def : completepermutations ] . if @xmath31 then @xmath32 following the definition above ( thus reversing @xmath7 and @xmath33 ) .",
    "mimimum mean ospa ( mmospa ) estimation has been studied previously in works such as @xcite , and it was the motivation for the objective function used in sjpda .",
    "however , previous works have not satisfactorily addressed the growth in complexity with the number of targets .",
    "the second contribution of this paper is an approximation of the mmospa estimator based on the vmb algorithm ; we refer to the result as the variational mmospa ( vmmospa ) estimator .",
    "this work is based on the form derived in @xcite , which studies unlabelled distributions , and makes the following modelling assumptions .",
    "the multiple target state evolves according to the following time dynamics process :    * targets arrive at each time according to a non - homogeneous poisson point process ( ppp ) with birth intensity @xmath34 , independent of existing targets * targets depart according to independent , identically distributed ( i.i.d . ) markovian processes ; the survival probability in state @xmath0 is @xmath35 * target motion follows i.i.d.markovian processes ; the single - target transition probability density function ( pdf ) is @xmath36    [ ass : dynamics ]    the multiple target measurement process is as follows :    * each measurement is either a false alarm , or a measurement of a single target * each target may give rise to at most one measurement ; probability of detection in state @xmath0 is @xmath37 * false alarm measurements arrive according to a non - homogeneous ppp with intensity @xmath38 , independent of targets and target - related measurements * each target - derived measurement is independent of all other targets and measurements conditioned on its corresponding target ; the single target measurement likelihood is @xmath39    [ ass : measurement ]    under these assumptions , @xcite proves that the following form is a conjugate prior , i.e. , it is preserved by prediction and update : @xmath40 where @xmath41 denotes the sum over all sets @xmath42 which are a subset of the finite set @xmath43 , and @xmath44 is a ppp representing unknown targets , for targets just born . accordingly , a proportion of the targets hypothesised to have arrived by the birth model will go undetected .",
    "these targets , which have _ never _ been detected , are referred to as _ unknown _ targets .",
    "see @xcite for a proof of the result and further discussion . ] with intensity @xmath45 : @xmath46 and @xmath47 is a mb mixture , i.e. , a linear combination of multi - bernoulli distributions , of the form @xmath48 where the notation @xmath49 denotes that the sum is over all disjoint subsets @xmath50 whose union is @xmath43 .",
    "as derived in @xcite , the terms in the sum @xmath11 correspond to different choices of data association , i.e. , different choices of which groups of past measurements correspond to the same targets , referred to as _ global association hypotheses_. the coefficient @xmath51 is the probability of global hypothesis @xmath52 ; consequently , @xmath53 and @xmath54 . for notational convenience",
    ", we consider the single - target hypotheses @xmath55 for all bernoulli components to be indexed through the set @xmath56 , i.e. , @xmath57 .",
    "used by different bernoulli components will be disjoint across all global hypotheses . ]",
    "the hypotheses @xmath58 are of the form : @xmath59 where @xmath60 is the probability of existence under hypothesis @xmath61 , and @xmath62 is the pdf of target state under hypothesis @xmath61 .",
    "in our implementations , we further assume that @xmath63 , although the basic derivation can handle other representations such as particle filters . as shorthand ,",
    "when we refer to a global association hypothesis @xmath52 , we assume that @xmath64 , and hence we also refer to the @xmath65-th constituent single - target event as @xmath66 .",
    "expressions and algorithms for predicting and updating the ppp component and the mb mixture are provided in @xcite .",
    "the greatest difficulty in implementing the filter is the number of components in the mb mixture . in this work ,",
    "we focus on the problem of simplifying the mb mixture into a single mb distribution .",
    "therefore , for the purpose of this paper , we disregard the ppp component and refer to the mb mixture as @xmath4 .      the first goal of this work is to obtain the mb distribution @xmath14 which best matches the full distribution @xmath4 according to some measure .",
    "because of its links to maximum likelihood ( ml ) estimation , a natural choice for the distortion measure in this problem is kl divergence : @xmath67 the following well - known theorems show that minimisation of kl divergence yields intuitive outcomes in common cases .",
    "[ th : expfamilykl ] ( * ? ? ?",
    "* theorem 8.6 , pg 278 ) let @xmath68 be the exponential family corresponding to sufficient statistics @xmath69 , i.e. , distributions of the form @xmath70 , where @xmath71 is the vector of canonical parameters , which are constrained to values for which @xmath72 is normalisable ( i.e. , its integral is finite ) .",
    "then the distribution @xmath73 which minimises the kl divergence @xmath74 from @xmath75 is the one which matches the expected value of the sufficient statistics @xmath69 , i.e. , @xmath76 such that @xmath77=e_{f}[\\tau(x)]$ ] .",
    "particular cases of this theorem include the best fitting gaussian distribution ( choosing the gaussian distribution @xmath78 matching the mean and covariance of @xmath75 ) and multinomial distribution ( choosing the discrete distribution to match the observed frequency of occurrence of each outcome ) . another common example is selection of the best fitting distribution which has a fully factored form , which we state in theorem [ th : klindependence ] .",
    "[ th : klindependence ] ( * ? ? ?",
    "* prop 8.3 , pg 277 ) given a distribution @xmath79 , the distribution with independent components @xmath80 which minimises the kl divergence @xmath74 is found by setting @xmath81 to the marginal distribution of @xmath82 , i.e. , @xmath83 where @xmath84 denotes integration with respect to all elements of @xmath0 other than @xmath82 .",
    "minimisation of kl divergence also motivates standard approaches in rfss such as the probability hypothesis density ( phd ) and cardinalised phd ( cphd ) filters , as the following theorems show .",
    "* theorem 4 ) the ppp distribution @xmath85 which minimises the set kl divergence @xmath74 is the one with @xmath86 , where @xmath87 is the phd ( i.e. , first moment ) of @xmath4 .",
    "thus the phd filter , which approximates the posterior distribution @xmath4 via its first moment @xmath87 , was explained in @xcite as finding the ppp @xmath14 with the smallest kl divergence from @xmath4 .",
    "the following theorem shows that the cphd filter , which approximates the posterior as an i.i.d.cluster process matching the cardinality distribution and setting the spatial distribution to a normalised version of the phd , can similarly be shown to minimise the set kl divergence within the family of i.i.d.cluster processes .",
    "[ th : cphdminkl ] given an exact posterior distribution @xmath4 , the i.i.d.cluster process @xmath88 parameterised by cardinality distribution @xmath89 and spatial pdf @xmath78 which minimises the set kl divergence @xmath74 is the distribution which sets @xmath90 where @xmath2 is the cardinality distribution of @xmath4 , and @xmath91 .",
    "the proof of the theorem is in appendix [ ss : cphdminkl ] .",
    "it can also be shown that the posterior approximation utilised in jpda minimises the kl divergence from the true posterior among the family of gaussian approximations with independent targets ( this is a direct consequence of theorem [ th : expfamilykl ] with an appropriate choice of sufficient statistics ) .",
    "the key in this case is that the distribution is labelled ( implicitly , since the joint state of all targets is considered to be a vector , rather than an unlabelled set ) .",
    "in contrast , the uncertain mapping between the elements of the set @xmath43 and the mb components of the approximating distribution @xmath14 in the unlabelled case prevents application of the standard results , and leaves room for development of new methods .",
    "expectation - maximisation ( em ) @xcite provides a mechanism for performing ml inference on distributions where the ml process would be easy if additional data was included alongside the observations .",
    "restating in terms that match the present context , the problem solved by em is determination of the parameters of @xmath92 to maximise the log likelihood : @xmath93 \\mathrm{d } x } \\ ] ] the variable @xmath94 is an _ unobserved _ or _ latent _ variable , or _ missing data _ : it is included because @xmath92 has a tractable ( e.g. , exponential family ) form whereas @xmath95 does not .",
    "most commonly , @xmath96 consists of empirical observations ; , then @xmath97 . ] in the present work @xmath96 is the exact posterior that we wish to approximate .",
    "the em process alternates between the following steps :    * e - step : calculate the expectation of the missing data distribution @xmath98 .",
    "* m - step : calculate the parameters of @xmath92 which maximise the completed log likelihood @xmath99 .    in @xcite",
    ", it was shown that em can be viewed as a coordinate descent of an upper bound to the negative log likelihood , alternating between minimising with respect to the model parameters and the missing data distribution .",
    "the proof can be stated simply as : @xmath100 \\mathrm{d } x } \\displaybreak[0]\\\\ & = \\int { \\left(\\sum_y q(y|x)\\right ) f(x ) \\log \\frac{\\sum_y q(y|x)}{\\sum_y g_\\theta(x ,",
    "y ) } \\mathrm{d } x } \\displaybreak[0]\\\\ & \\leq \\sum_y\\int { q(y|x)f(x ) \\log \\frac{q(y|x)}{g_\\theta(x , y ) } \\mathrm{d } x } \\displaybreak[0]\\\\ & = \\int{f(x ) \\sum_y q(y|x)\\log q(y|x)\\mathrm{d } x }   - \\sum_y\\int{q(y|x)f(x)\\log g_\\theta(x , y)\\mathrm{d } x } \\displaybreak[0]\\\\ & \\triangleq f(q , g_\\theta)\\end{aligned}\\ ] ] @xmath100 \\mathrm{d } x } \\displaybreak[0]\\\\ & = \\int { \\left(\\sum_y q(y|x)\\right ) f(x ) \\log \\frac{\\sum_y q(y|x)}{\\sum_y g_\\theta(x , y ) } \\mathrm{d } x } \\displaybreak[0]\\\\ & \\leq \\sum_y\\int { q(y|x)f(x ) \\log \\frac{q(y|x)}{g_\\theta(x , y ) } \\mathrm{d } x } \\displaybreak[0]\\\\ & = \\int{f(x ) \\sum_y q(y|x)\\log q(y|x)\\mathrm{d } x } \\notag\\\\ & \\qquad - \\sum_y\\int{q(y|x)f(x)\\log g_\\theta(x , y)\\mathrm{d } x } \\displaybreak[0]\\\\ & \\triangleq f(q , g_\\theta)\\end{aligned}\\ ] ] where the upper bound is due to the log - sum inequality ( * ? ? ?",
    "* p29 ) . assuming that the energy functional @xmath101 is continuously differentiable ( e.g. , assuming an appropriate form of @xmath102 ) , coordinate descent ( i.e.",
    ", alternating between optimising with respect to @xmath103 , and with respect to the parameters of @xmath92 ) is guaranteed to converge to a local minimum ( * ? ? ?",
    "* prop 2.7.1 ) .",
    "further , as shown in ( * ? ? ? * thm 2 ) , upon convergence , we have @xmath104 , in which case the log - sum inequality is tight , and the solution reached is a local optimum of @xmath105 .",
    "the bernoulli distribution is practically similar to the model used in integrated probabilistic data association ( ipda ) @xcite .",
    "likewise , the tomb algorithm proposed in @xcite is practically similar to jipda @xcite , which is in turn an extension of jpda which admits uncertainty in target existence .",
    "the method which we propose in this work is related to kl set jpda ( klsjpda ) and set jpda ( sjpda ) @xcite .",
    "klsjpda seeks to minimise the kl divergence between a modification @xmath106 of the posterior distribution @xmath107 ( where @xmath108 is the vector of the joint state of all targets , and the number of targets @xmath7 is known ) and a gaussian approximation : @xmath109 the objective is minimised with respect to the modified distribution @xmath106 ( permuting elements in such a manner that the kl divergence is reduced but the symmetrised distribution is the same as the symmetrisation of @xmath107 ) , and @xmath110 ( fitting a gaussian to the modified distribution ) .",
    "in appendix [ sec : sjpdaequiv ] , we show that this is equivalent to finding the symmetrised gaussian distribution that minimises the kl divergence from the true symmetrised distribution , i.e. , the same as with fixed cardinality .",
    "sjpda replaces this objective with the trace of the covariance @xmath111 , which is related to the mean ospa metric .",
    "the modification of @xmath107 is also limited such that a fixed permutation is used under each global association hypothesis . in comparison to sjpda / klsjpda , the method proposed in the section [ sec : bfmb ] has the following advantages :    * the motivation and derivation is based on minimising the set kl divergence between a fixed distribution @xmath4 and an approximation @xmath14 , in a manner similar to well - accepted methods for finding best fitting distributions ( e.g. , section [ ss : backklmin ] ) * the formulation naturally handles cases in which the number of targets present is uncertain , whereas klsjpda / sjpda assume that the number of targets is known * the method yields a relaxed optimisation involving a single variable for each single - target hypothesis and each track ( i.e. , bernoulli component ) that can be solved efficiently in low order polynomial time , whereas sjpda involves a distribution of permutations for every global association hypothesis , resulting in time that is exponential in the number of targets    the final difference is especially significant , as it essentially means that sjpda ( and even more so klsjpda ) can only be applied to very small problems , whereas the method proposed scales well with problem size .",
    "having observed in section [ ss : backklmin ] that the standard approach for finding the distribution within a family that best matches a given distribution is to minimise the kl divergence , we set about using this process to obtain the mb distribution @xmath14 which best matches the mbm distribution @xmath4 .",
    "we refer to the resulting algorithm as the best fitting mb ( bfmb ) filter , and the subsequent approximation ( described in sections [ ss : vmb ] to [ ss : efficient ] ) as the variational mb ( vmb ) filter .",
    "[ prob : mb ] find the mb distribution @xmath14 that minimises the kl divergence @xmath112 } \\int{f(x)\\log\\frac{f(x)}{g(x)}\\delta x } =   \\operatornamewithlimits{argmax}_{[g_j ] } \\int{f(x)\\log g(x)\\delta x}\\ ] ] @xmath113 } \\int{f(x)\\log\\frac{f(x)}{g(x)}\\delta x } = \\\\ \\operatornamewithlimits{argmax}_{[g_j ] } \\int{f(x)\\log g(x)\\delta x}\\end{gathered}\\ ] ] where @xmath14 is mb : @xmath114 and the components @xmath115 are similar in form to .    as in",
    ", the right hand side ( rhs ) of is obtained by separating the log of the quotient into the difference of the logs , and observing that the first term is constant with respect to ( wrt ) the variables of minimisation .",
    "the two major difficulties encountered in performing this optimisation are in the complexity of the set integral used in , and in the sum which appears in . comparing the sum in to",
    ", we interpret it as being the result of _ missing data _ , and address it using em ( in section [ ss : vmb ] ) .",
    "first , we address the set integral in .    as described in , the integral",
    "is expanded into a sum over cardinalities , and each bernoulli component appears under each cardinality .",
    "the following theorem shows that the form can be simplified into a nested series of bernoulli integrals , and that the summation in which is over assignments of the elements of the variable cardinality set @xmath43 to bernoulli components @xmath116 $ ] can be simplified to a sum of assignments from the @xmath18 bernoulli components in the exact distribution @xmath4 to the @xmath18 bernoulli components in the simplified mb distribution @xmath14 .",
    "[ th : bfmbequivexact ] the solution of the optimisation @xmath117 } \\sum_{a\\in{\\cal a}}w_a \\idotsint { \\prod_{i=1}^n f_{h_i}(x_i ) \\cdot }   { \\cdot \\log\\sum_{\\pi\\in\\pi_n}\\prod_{i=1}^n g_{\\pi(i)}(x_i ) \\delta x_1\\cdots \\delta x_n}\\ ] ] @xmath118 } \\sum_{a\\in{\\cal a}}w_a \\idotsint { \\prod_{i=1}^n f_{h_i}(x_i ) \\cdot } \\\\ { \\cdot \\log\\sum_{\\pi\\in\\pi_n}\\prod_{i=1}^n g_{\\pi(i)}(x_i )",
    "\\delta x_1\\cdots \\delta x_n } \\end{gathered}\\ ] ] is the same as the solution of problem [ prob : mb ] .    in theorem",
    "[ th : bfmbequivexact ] , @xmath8 is the set of permutations , as in definition [ def : completepermutations ] .",
    "the proof of the theorem is in appendix [ ss : bfmbequivexact ] .",
    "we will see in the following section that the form is suitable for applying em and obtaining a tractable approximate solution .",
    "we propose an approximate solution of based on minimisation of an upper bound of the true objective , following a similar process to the derivation of em in section [ ss : backem ] .",
    "the correspondence between the underlying bernoulli distribution @xmath119 and the bernoulli component @xmath120 in the best - fitting distribution is treated as missing data ; the algorithm proceeds by alternating between estimating the correspondence ( e - step ) , and optimising @xmath121 to best fit the completed distribution ( m - step ) . in the appendix",
    ", we show that the missing data acts in a equivalent manner to klsjpda s selection of an ordered distribution in the same unordered family that is best able to be approximated by the desired distribution family .",
    "there is a separate missing data distribution for each component @xmath52 in the multi - bernoulli mixture ; the distribution under the @xmath52-th component is @xmath122 .",
    "we constrain @xmath123 , and @xmath124 .",
    "accordingly , solution of is equivalent to minimisation of @xmath125\\big)$ ] , where @xmath126\\big ) = -\\sum_{a\\in{\\cal a}}w_a \\idotsint { \\prod_{i=1}^n f_{h_i}(x_i ) } { \\cdot \\log\\left(\\sum_{\\pi\\in\\pi_n}\\prod_{i=1}^n g_{\\pi(i)}(x_i)\\right ) \\delta x_1\\cdots \\delta x_n } \\label{eq : emstep1}\\displaybreak[0 ] \\\\",
    "= & \\sum_{a\\in{\\cal a}}w_a \\left(\\sum_{\\pi\\in\\pi_n}q_a(\\pi)\\right)\\idotsint { \\prod_{i=1}^n f_{h_i}(x_i ) } \\cdot \\log\\left(\\frac{\\sum_{\\pi\\in\\pi_n}q_a(\\pi)}{\\sum_{\\pi\\in\\pi_n}\\prod_{i=1}^n g_{\\pi(i)}(x_i)}\\right ) \\delta x_1\\cdots \\delta x_n   \\label{eq : emstep2}\\displaybreak[0]\\\\ \\leq & \\sum_{a\\in{\\cal a},\\pi\\in\\pi_n}w_a q_a(\\pi)\\idotsint { \\prod_{i=1}^n f_{h_i}(x_i ) }   \\cdot \\log\\left(\\frac{q_a(\\pi)}{\\prod_{i=1}^n g_{\\pi(i)}(x_i)}\\right ) \\delta x_1\\cdots \\delta x_n \\label{eq : emstep3}\\displaybreak[0]\\\\ = & \\sum_{a\\in{\\cal a},\\pi\\in\\pi_n}w_a q_a(\\pi)\\log q_a(\\pi ) -\\sum_{a\\in{\\cal a},\\pi\\in\\pi_n } w_a q_a(\\pi ) \\sum_{i=1}^n \\int{f_{h_i}(x_i)\\log g_{\\pi(i)}(x_i)\\delta x_i } \\label{eq : emstep4}\\displaybreak[0]\\\\ \\leq & \\;t\\cdot\\sum_{a\\in{\\cal",
    "a},\\pi\\in\\pi_n}w_a q_a(\\pi)\\log q_a(\\pi ) -\\sum_{a\\in{\\cal a},\\pi\\in\\pi_n } w_a q_a(\\pi ) \\sum_{i=1}^n \\int{f_{h_i}(x_i)\\log g_{\\pi(i)}(x_i)\\delta x_i } \\label{eq : emstep5}\\\\ \\triangleq & \\tilde{j}_t\\big([g_j],[q_a(\\pi)]\\big ) \\notag\\end{aligned}\\ ] ] @xmath126\\big ) = -\\sum_{a\\in{\\cal a}}w_a \\idotsint { \\prod_{i=1}^n f_{h_i}(x_i ) \\cdot } \\notag \\\\ & { \\cdot \\log\\left(\\sum_{\\pi\\in\\pi_n}\\prod_{i=1}^n g_{\\pi(i)}(x_i)\\right ) \\delta x_1\\cdots \\delta x_n } \\label{eq : emstep1}\\displaybreak[0 ] \\\\",
    "= & \\sum_{a\\in{\\cal a}}w_a \\left(\\sum_{\\pi\\in\\pi_n}q_a(\\pi)\\right)\\idotsint { \\prod_{i=1}^n f_{h_i}(x_i ) \\cdot } \\notag \\\\ & \\cdot \\log\\left(\\frac{\\sum_{\\pi\\in\\pi_n}q_a(\\pi)}{\\sum_{\\pi\\in\\pi_n}\\prod_{i=1}^n g_{\\pi(i)}(x_i)}\\right ) \\delta x_1\\cdots \\delta x_n   \\label{eq : emstep2}\\displaybreak[0]\\\\ \\leq & \\sum_{a\\in{\\cal a},\\pi\\in\\pi_n}w_a q_a(\\pi)\\idotsint { \\prod_{i=1}^n f_{h_i}(x_i ) \\cdot } \\notag \\\\ & \\cdot \\log\\left(\\frac{q_a(\\pi)}{\\prod_{i=1}^n g_{\\pi(i)}(x_i)}\\right ) \\delta x_1\\cdots",
    "\\delta x_n \\label{eq : emstep3}\\displaybreak[0]\\\\ = & \\sum_{a\\in{\\cal a},\\pi\\in\\pi_n}w_a q_a(\\pi)\\log q_a(\\pi ) \\notag\\\\ & -\\sum_{a\\in{\\cal a},\\pi\\in\\pi_n } w_a q_a(\\pi ) \\sum_{i=1}^n \\int{f_{h_i}(x_i)\\log g_{\\pi(i)}(x_i)\\delta x_i } \\label{eq : emstep4}\\displaybreak[0]\\\\ \\leq & \\;t\\cdot\\sum_{a\\in{\\cal a},\\pi\\in\\pi_n}w_a q_a(\\pi)\\log q_a(\\pi ) \\notag\\\\ & -\\sum_{a\\in{\\cal a},\\pi\\in\\pi_n } w_a q_a(\\pi ) \\sum_{i=1}^n \\int{f_{h_i}(x_i)\\log g_{\\pi(i)}(x_i)\\delta x_i } \\label{eq : emstep5}\\\\ \\triangleq & \\tilde{j}_t\\big([g_j],[q_a(\\pi)]\\big ) \\notag\\end{aligned}\\ ] ] as in section [ ss : backem ] , multiplies by @xmath127 and similarly adds @xmath128 , invokes the log - sum inequality , and replaces the log of a product with the sum of logs and simplifies . in , we observe that the first term is negative ( since @xmath129 ) , hence incorporating a multiplier @xmath130 loosens the bound . in statistical physics",
    "this corresponds to the temperature ( e.g. , @xcite ) .",
    "this is discussed further in section [ ss : zerotemp ] .",
    "[ prob : bfmb_em ] bfmb may be solved approximately by minimising the upper bound to the objective of problem [ prob : mb ] : @xmath131,[q_a(\\pi ) ] } \\tilde{j}_t\\big([g_j],[q_a(\\pi)]\\big)\\ ] ] where @xmath132 , @xmath133 , @xmath134 , @xmath135 , and @xmath136 $ ] is the temperature ( a constant to be selected ) .    in most applications of em",
    ", the missing data is estimated for each of a finite number of training samples . in turn , this guarantees that the log - sum inequality is tight at the optimum , and hence that the procedure will converge to a local minimum of the original likelihood function @xmath137)$ ] . in the appendix , we show that if we follow steps similar to those above in a case with fixed cardinality , and we allow missing data to select a different permutation for each joint state ( i.e. , replacing @xmath122 with @xmath138 ) , the result is identical to klsjpda .",
    "accordingly , the estimation of missing data in the formulation above can be seen as equivalent to the selection of a new ordered density in the same ordered family in klsjpda .    in the formulation",
    "above , we constrain the missing data to vary only with the global association hypothesis @xmath52 . as a consequence of this ,",
    "the log - sum inequality is not necessarily tight at the optimum , but the constraint is essential for tractability .",
    "a similar constraint is applied in sjpda ( i.e. , reordering target indices for each hypothesis , not for each target state , albeit utilising a different objective function ) .",
    "the standard method for solving the form of problem [ prob : bfmb_em ] is by block coordinate descent , alternating between minimisation with respect to @xmath116 $ ] ( m - step ) , and @xmath139 $ ] ( e - step ) .",
    "these two steps can be solved as : @xmath140 where @xmath141 is the inverse of the permutation function @xmath30 ( i.e. , if @xmath142 then @xmath143 ) .",
    "if the distributions @xmath116 $ ] are constrained to be bernoulli - gaussian , is replaced by expressions matching the probability of existence , mean and covariance to the expression in .",
    "the bernoulli - gaussian form is convenient since it permits closed - form evaluation of .",
    "specifically , if @xmath144 then the m - step reduces to setting : @xmath145[\\mu_{h_{\\pi^{-1}(j)}}-\\hat{\\mu}_j]^t \\big\\ } \\label{eq : bg_mstep3}\\end{aligned}\\ ] ] @xmath146[\\mu_{h_{\\pi^{-1}(j)}}-\\hat{\\mu}_j]^t \\big\\ } \\label{eq : bg_mstep3}\\end{aligned}\\ ] ] while the integral required in the e - step becomes : @xmath147^t\\hat{\\sigma}_j^{-1}[\\mu_{h_i}-\\hat{\\mu}_j ] + \\log|2\\pi\\hat{\\sigma}_j| \\big\\}\\end{gathered}\\ ] ] @xmath148^t\\hat{\\sigma}_j^{-1}[\\mu_{h_i}-\\hat{\\mu}_j ] + \\log|2\\pi\\hat{\\sigma}_j| \\big\\}\\end{gathered}\\ ] ]      in the preliminary work @xcite , it was observed that the best performance in terms of the original objective ( problem [ prob : mb ] ) occurs when we set @xmath149 . in this case",
    ", e - step reverts to a lp , which can be implemented through finding the most likely assignment @xmath150 for each association hypothesis @xmath11 using methods such as the auction algorithm .",
    "this is referred to as the _ point - estimate _ ( or _ winner takes all _ ) variant of em @xcite . the difference between the cases with @xmath151 and @xmath149 is analogous to the difference between the em algorithm for estimating the parameters of a gaussian mixture ( involving soft assignment of samples to mixture components ) , and the widely - used @xmath152-means algorithm ( hard assignment of samples to mixture components ) .",
    "having found that the case with @xmath149 tends to yield solutions with lower kl divergence , we turn to analyse the structure of the problem in this case .",
    "we find that the geometry of the optimisation problem reveals a family of potential approximations .",
    "[ th : bfmb_pt_form ] problem [ prob : bfmb_em ] can be solved equivalently as : @xmath153 @xmath154 where the polytope @xmath155 is : @xmath156 @xmath157    note that @xmath158 is the sum over all tracks of the probability that a bernoulli component in @xmath4 that is utilising hypothesis @xmath61 is assigned to bernoulli component @xmath159 in @xmath14 .",
    "the proof of theorem [ th : bfmb_pt_form ] is in appendix [ ss : bfmb_pt_form ] .",
    "the objective is the sum of entropies of the simplified bernoulli components , which was proposed as a heuristic in @xcite .",
    "we can return to a form that can be minimised via coordinate descent by analysing the following expression : @xmath160,q(h , j ) )   = -\\sum_{j=1}^n\\int{\\left ( \\sum_{h\\in{\\cal h } } q(h , j ) f_{h}(x )   \\right ) } \\log g_j(x ) \\delta x \\label{eq : relax5}\\ ] ] @xmath161,q(h , j ) ) \\\\",
    "= -\\sum_{j=1}^n\\int{\\left ( \\sum_{h\\in{\\cal h } } q(h , j ) f_{h}(x )   \\right ) } \\log g_j(x ) \\delta x \\label{eq : relax5}\\end{gathered}\\ ] ] the minimum of with respect to @xmath162 $ ] occurs at @xmath163 , hence the minimisation of with respect to @xmath116 $ ] and @xmath158 is equivalent to the minimisation of with respect to @xmath158 .      whereas the optimisation in the statement of problem [ prob : bfmb_em ] involves missing data @xmath122 for every global association hypothesis @xmath11 , theorem [ th : bfmb_pt_form ] provides a form of the objective which depends only on the vastly simplified representation @xmath158 , specifying the weight of single target hypothesis @xmath164 in the new bernoulli component @xmath120 .",
    "this does not necessarily reduce complexity , however , as the feasible set @xmath155 suffers from combinatorial complexity .",
    "it does , however , raise the prospect of tractable approximations based on relaxations of the polytope @xmath155 that admit a compact description .    from",
    ", it is clear that any set of distributions @xmath139 $ ] will yield @xmath158 which satisfies the following constraints : @xmath165 where @xmath166 this suggests a relaxation of the polytope @xmath155 to the compact approximation : @xmath167 @xmath168 the notation @xmath169 is chosen due to its similarity to the marginal polytope approximation that is widely used in variational inference @xcite .",
    "many other approximations are possible , and approximations can be improved incrementally ( e.g. , @xcite ) .",
    "the details of the resulting algorithm are shown in figure [ alg : vmb ] ; we refer to the method as variational mb ( vmb ) .",
    "we assume that @xmath170 is of the form , and constrain @xmath120 to be of the form in order to find the best - fitting mb distribution with bernoulli - gaussian components .",
    "even if the desire is to return a gaussian mixture representation , the bernoulli - gaussian form is recommended for two reasons .",
    "firstly , it permits closed - form evaluation of the objective , which is not otherwise possible .",
    "secondly ( and more importantly ) , it incorporates the desire that bernoulli components be made local in state space .",
    "if the bernoulli - gaussian approximation is not made , there is little penalty for solutions which collect gaussian mixture components that are arbitrarily dissimilar together in the same bernoulli component .",
    "the bernoulli - gaussian form ensures that the simplified components are able to be approximated via a unimodal distribution .",
    "the algorithm is initialised with the marginal probabilities @xmath171 , as can be approximated efficiently using methods such as @xcite ( as described in @xcite ) .",
    "@xmath172 @xmath173 @xmath174 @xmath175 @xmath176 @xmath177 calculate @xmath178 using @xmath179 solve the lp : @xmath180 * return * @xmath181,[\\hat{\\mu}_j],[\\hat{\\sigma}_j],q(h , j)$ ]    finally , we note that the form of is a network flow lp , referred to as a transportation problem @xcite . problems of this type admit rapid solution ; we utilise a forward - reverse variant of the transport auction algorithm described in @xcite .",
    "the second problem we consider is that of finding the minimum mean ospa ( mmospa ) estimate . we modify the standard ospa measure by omitting the leading @xmath182 factor so that the measure _ adds _ over targets rather than _ averaging _ over targets .",
    "[ prob : ospa ] find @xmath183 that solves the optimisation @xmath184 where if @xmath1 , @xmath24 and @xmath25 , @xmath185^{\\frac{1}{p}}\\ ] ]    we formulate the estimate as @xmath186 , where for each @xmath187 , @xmath188 is bernoulli , i.e. , either @xmath189 or @xmath190 .",
    "the following lemma provides an equivalent form of problem [ prob : ospa ] .",
    "[ lem : ospaequiv ] the solution of the following optimisation is the same as that of problem [ prob : ospa ] .",
    "@xmath191 } \\sum_{a\\in{\\cal a}}w_a \\idotsint { \\prod_{i=1}^n f_{h_i}(x_i ) }   { d\\big([x_i],[\\hat{x}_j]\\big)^p   \\delta x_1\\cdots \\delta x_n}\\ ] ] @xmath192 } \\sum_{a\\in{\\cal a}}w_a \\idotsint { \\prod_{i=1}^n f_{h_i}(x_i ) } \\\\ { d\\big([x_i],[\\hat{x}_j]\\big)^p   \\delta x_1\\cdots \\delta x_n } \\end{gathered}\\ ] ] where the optimisation is performed over the bernoulli sets @xmath188 , @xmath193 , and @xmath194,[\\hat{x}_j]\\big ) & = \\min_{\\pi\\in\\pi_n}\\left [ \\sum_{i=1}^n d(x_i,\\hat{x}_{\\pi(i)})^p \\right]^{\\frac{1}{p}}\\end{aligned}\\ ] ]    lemma [ lem : ospaequiv ] results directly from corollary [ cor : setmbdecomp ] ( from appendix [ ss : bfmbequivexact ] ) .",
    "note that for bernoulli sets , evaluates to : @xmath195    we will show that the following problem is closely related .",
    "[ prob : sospa ] find bernoulli sets @xmath196 $ ] that solve the optimisation @xmath191 } \\sum_{a\\in{\\cal a}}w_a \\idotsint { \\prod_{i=1}^n f_{h_i}(x_i ) }   { s_\\gamma\\big([x_i],[\\hat{x}_j]\\big)^p   \\delta x_1\\cdots \\delta x_n}\\ ] ] @xmath192 } \\sum_{a\\in{\\cal a}}w_a \\idotsint { \\prod_{i=1}^n f_{h_i}(x_i ) } \\\\ { s_\\gamma\\big([x_i],[\\hat{x}_j]\\big)^p   \\delta x_1\\cdots \\delta x_n } \\end{gathered}\\ ] ] where @xmath197,[\\hat{x}])$ ] is the softmax approximation of the ospa distance : @xmath198,[\\hat{x}_j])\\right ) =   \\left\\{\\frac{-1}{\\gamma}\\log\\sum_{\\pi\\in\\pi_n } \\exp \\bigg [ -\\gamma\\sum_{i=1}^n d(x_i,\\hat{x}_{\\pi(i)})^p \\bigg]\\right\\}^{\\frac{1}{p}}\\ ] ] @xmath199,[\\hat{x}_j])\\right ) = \\\\ \\left\\{\\frac{-1}{\\gamma}\\log\\sum_{\\pi\\in\\pi_n } \\exp \\bigg [ -\\gamma\\sum_{i=1}^n d(x_i,\\hat{x}_{\\pi(i)})^p \\bigg]\\right\\}^{\\frac{1}{p}}\\end{gathered}\\ ] ]    problem [ prob : sospa ] replaces the minimum in the ospa definition with the function log - sum - exp , which is commonly referred to as _",
    "softmax _ ; this function is illustrated in figure [ fig : softmax ] .",
    "as @xmath200 increases , softmax better approximates the maximum function . note that the definition of @xmath201 implicitly yields a function taking set arguments @xmath202 for @xmath203 , @xmath204 via corollary [ cor : setmbdecomp ] .    .",
    "]      noting the similarity between and problem [ prob : sospa ] , we show that an approximate solution can be found using the vmb algorithm . since problem [ prob : sospa ] is the softmax approximation of problem [ prob : ospa ] , this in turn provides an approximate mmospa estimator .",
    "we refer to the resulting method as variational mmospa ( vmmospa ) .",
    "specifically , with @xmath205 , for each @xmath187 we adopt the parameterisation @xmath206 where @xmath207 , @xmath189 if @xmath208 , and @xmath190 if @xmath209 .",
    "if we set : @xmath210^{\\hat{r}_j } \\cdot & \\\\",
    "\\quad \\cdot \\exp\\{-\\gamma c^2\\}^{1-\\hat{r}_j } & x=\\{x\\ } \\end{cases}\\ ] ] @xmath211^{\\hat{r}_j } \\cdot & \\\\ \\quad \\cdot \\exp\\{-\\gamma c^2\\}^{1-\\hat{r}_j } & x=\\{x\\ } \\end{cases}\\end{gathered}\\ ] ] then the objective of problem [ prob : mb ] corresponds to that of problem [ prob : sospa ] . , which we subsequently incorporate ( although its absence would not affect the location of the minima ) .",
    "note also that @xmath212 is replaced by its softmax approximation in . ]",
    "subsequently , we relax the feasible set to @xmath213 $ ] , substitute into , and repeat the em derivation to incorporate additional missing data @xmath214 , @xmath215 in order to handle the sum in ( as in and ) .",
    "we also divide through by @xmath200 , and incorporate a smoothing term @xmath216 $ ] in order to gradually converge on integral values of @xmath217 as @xmath218 .",
    "would produce integral values , and if @xmath208 , it would remain so for all subsequent iterations . ]",
    "the resulting modified objective is : @xmath219,[\\hat{x}_j],[q_{h , j}(b)],q(h , j ) ) =    \\sum_{j=1}^n\\sum_{h\\in{\\cal h}}q(h , j ) \\bigg\\ { c^2 ( 1-r_h)\\hat{r}_j + c^2 r_h(1-\\hat{r}_j ) \\\\",
    "+ \\frac{1}{\\gamma}r_h\\hat{r}_j\\sum_{b=0}^1 q_{h , j}(b)\\log q_{h , j}(b )   + r_h\\hat{r}_j\\left[q_{h , j}(0)c^2 + q_{h , j}(1)\\int{f_h(x)d(x,\\hat{x}_j)^2 \\mathrm{d } x } \\right]\\bigg\\ } \\\\",
    "+ \\frac{1}{\\gamma}\\sum_{j=1}^n \\left\\ { \\hat{r}_j\\log\\hat{r}_j + ( 1-\\hat{r}_j)\\log(1-\\hat{r}_j ) \\right\\}\\end{gathered}\\ ] ] @xmath219,[\\hat{x}_j],[q_{h , j}(b)],q(h , j ) ) =   \\\\",
    "\\sum_{j=1}^n\\sum_{h\\in{\\cal h}}q(h , j ) \\bigg\\ { c^2 ( 1-r_h)\\hat{r}_j + c^2 r_h(1-\\hat{r}_j ) \\\\ + \\frac{1}{\\gamma}r_h\\hat{r}_j\\sum_{b=0}^1 q_{h , j}(b)\\log q_{h , j}(b )",
    "\\\\ + r_h\\hat{r}_j\\left[q_{h , j}(0)c^2 + q_{h , j}(1)\\int{f_h(x)d(x,\\hat{x}_j)^2 \\mathrm{d } x } \\right]\\bigg\\ } \\\\",
    "+ \\frac{1}{\\gamma}\\sum_{j=1}^n \\left\\ { \\hat{r}_j\\log\\hat{r}_j + ( 1-\\hat{r}_j)\\log(1-\\hat{r}_j ) \\right\\}\\end{gathered}\\ ] ] this can be optimised by block coordinate descent , iteratively applying the following equations : @xmath220 \\notag\\\\ &   + r_h \\hat{r}_j q_{h , j}(1)\\int{f_h(x)d(x,\\hat{x}_j)^2 \\mathrm{d } x } \\label{eq : mm_cost}\\\\ \\hat{x}_j = & \\frac{\\sum_{h\\in{\\cal h } } q(h , j ) r_h q_{h , j}(1 ) \\mu_h}{\\sum_{h\\in{\\cal h } } q(h , j ) r_h q_{h , j}(1 ) } \\\\",
    "\\hat{r}_j = & \\frac{\\alpha_j}{\\alpha_j+\\beta_j } \\\\",
    "\\alpha_j & = \\exp\\bigg\\ { -\\sum_{h\\in{\\cal h } } q(h , j)\\bigg [ \\gamma c^2(1-r_h + r_h q_{h , j}(0 ) ) + r_h\\sum_{b=0}^1 q_{h , j}(b)\\log q_{h , j}(b ) \\notag\\\\   & + \\gamma r_h q_{h , j}(1)\\int{f_h(x)d(x,\\hat{x}_j)^2 \\mathrm{d } x } \\bigg]\\bigg\\ } \\\\ \\beta_j & = \\exp\\bigg\\ { -\\gamma c^2 \\sum_{h\\in{\\cal h}}q(h , j)r_h \\bigg\\}\\end{aligned}\\ ] ] @xmath221\\\\ q(h , j ) = & \\mbox { solution of the lp \\eqref{eq : vmb_lp } using $ c(h , j)$ in \\eqref{eq : mm_cost } } \\notag \\\\ c(h , j ) = &   \\frac{1}{\\gamma}r_h\\hat{r}_j\\sum_{b=0}^1 q_{h , j}(b)\\log q_{h , j}(b ) \\notag\\\\ & + c^2[(1-r_h)\\hat{r}_j + r_h(1-\\hat{r}_j ) + r_h\\hat{r}_j q_{h , j}(0 ) ] \\notag\\\\ &   + r_h \\hat{r}_j q_{h , j}(1)\\int{f_h(x)d(x,\\hat{x}_j)^2 \\mathrm{d } x } \\label{eq : mm_cost}\\displaybreak[0]\\\\ \\hat{x}_j = & \\frac{\\sum_{h\\in{\\cal h } } q(h , j ) r_h q_{h , j}(1 ) \\mu_h}{\\sum_{h\\in{\\cal h } } q(h , j ) r_h q_{h , j}(1 ) } \\displaybreak[0]\\\\ \\hat{r}_j = & \\frac{\\alpha_j}{\\alpha_j+\\beta_j } \\\\",
    "\\alpha_j & = \\exp\\bigg\\ { -\\sum_{h\\in{\\cal h } } q(h , j)\\bigg [ \\gamma c^2(1-r_h + r_h q_{h , j}(0 ) ) \\notag\\\\ & + r_h\\sum_{b=0}^1 q_{h , j}(b)\\log q_{h , j}(b ) \\notag\\\\ & + \\gamma r_h q_{h , j}(1)\\int{f_h(x)d(x,\\hat{x}_j)^2 \\mathrm{d } x } \\bigg]\\bigg\\ } \\\\ \\beta_j & = \\exp\\bigg\\ { -\\gamma c^2 \\sum_{h\\in{\\cal h}}q(h , j)r_h \\bigg\\}\\end{aligned}\\ ] ] in the bernoulli - gaussian case , @xmath222 .",
    "as we gradually increase @xmath218 , both @xmath217 and @xmath214 converge to integral solutions ( as the softmax approximation converges to the maximum function ) .",
    "in order to evaluate the performance of the proposed methods in a challenging scenario , we utilise the experiments from @xcite .",
    "the scenarios involve @xmath223 targets which are in close proximity at the mid - point of the simulation , achieved by initialising at the mid - point and running forward and backward dynamics .",
    "we consider two cases for the mid - point initialisation ( i.e. , @xmath224 ) : @xmath225 where the target state is position and velocity in two dimensions .",
    "snapshots of one dimension of both cases are shown in figure [ fig : scenario ] .",
    "case 1 represents a worst - case scenario for coalescence , since targets are completely indistinguishable ( in position and velocity ) at the mid - point . in case 2",
    ", there is a discernible difference in velocity , hence the effect is expected to be somewhat reduced . in case 1 , targets all exist throughout the simulation ( tracks are not pre - initialised ) . in case 2 , the targets are born at times @xmath226 ( any targets not existing prior to time @xmath224 are born at that time ; consequently , for case 2 with @xmath227 , ten targets are born at time @xmath224 ) .",
    "targets follow a linear - gaussian model with nominally constant velocity , @xmath228 , where @xmath229 , @xmath230 \\otimes \\mathbf{i}_{2\\times 2 } , \\quad \\mathbf{q } = q \\left[\\begin{array}{cc } t^3/3 & t^2/2 \\\\ t^2/2 & t \\end{array}\\right ] \\otimes \\mathbf{i}_{2\\times 2}\\ ] ] and @xmath231 , @xmath151 .    ; top and bottom have the expected number of false alarms set to @xmath232 and @xmath233 respectively.,width=316 ]    target - originated measurements provide position corrupted by gaussian noise with unit variance .",
    "false alarms occur according to a ppp , uniform on the region @xmath234 ^ 2 $ ] .",
    "cases are considered with the expected number of false alarms per scan as @xmath235 , and with @xmath236 , representing a range of snr values .    for each case",
    ", @xmath237 monte carlo trials are executed , each with both randomly generated trajectories , and randomly generated measurements .",
    "each algorithm is tested using the same monte carlo trials .",
    "further details of the simulations and implementation can be found in @xcite .",
    "marginal association probabilities ( e.g. , ) are calculated approximately using the variational method of @xcite .",
    "the recycling method of @xcite is applied to bernoulli components with a probability of existence less than @xmath238 .",
    "the vmb algorithm is applied to clusters of mb components ( tracks ) which share measurements ( i.e. , any hypothesis in the track ) .",
    "the gm - cphd @xcite , momb and tomb algorithms extract estimates by determining the mode of the cardinality distribution ( @xmath239 ) , and outputting the most likely hypothesis of the @xmath239 bernoulli components with the highest probability of existence ( or the gaussian components with the highest weight for cphd ) .",
    "vmb includes the estimate for each simplified bernoulli component @xmath120 of the form which satisfies @xmath240^{-1}\\ ] ] where @xmath241 .",
    "when the bernoulli components are well - spaced ( as vmb generally achieves ) , this can be shown to minimise an upper bound on mospa .",
    "dashed lines show the performance of each algorithm combined with the vmmospa estimator ( again , with @xmath241 ) , which estimates the number of targets and their states via the optimisation procedure in section [ ss : mmospasolution ] .        the scenarios examined are exceptionally challenging due to the large number of targets in close proximity .",
    "while others have considered larger numbers of targets , these are generally positioned uniformly in space and rarely come into close contact . cases such as this can be effectively decoupled into series of single target tracking problems . in the present study , up to 20 targets have effectively the same position and velocity at the mid - point in time , and the dependency between targets is inescapable .",
    "the results are shown in figure [ fig : scenarioresults ] .",
    "the @xmath0-axis shows time in the scenario , while the @xmath94 axis shows the average ospa with @xmath242 and @xmath241 .",
    "the large error suffered by tomb ( solid red ) commencing shortly after time 100 corresponds to coalescence , similar to that experienced by jpda / jipda .",
    "this occurs when targets have been closely spaced , and begin to separate .",
    "tomb maintains a gaussian mixture for each bernoulli component ( track ) , and outputs its estimate as the mean of the highest weighted gaussian mixture component .",
    "after targets have been closely spaced , the gaussian mixture for each target contains components representing all targets .",
    "consequently , the highest weight component for all tracks can fall on the same target , leaving all other targets without estimates .",
    "tomb using the vmmospa estimate ( dashed red ) resolves this difficulty to an extent , restoring performance to at least that obtained by the cphd .",
    "the gain in performance is particularly good in the cases with @xmath243 .",
    "the initialisation used for the method is the estimate that would have been generated using the momb algorithm from the current time step ( which requires minimal calculation beyond that needed for tomb ) .",
    "it is likely that better performance could be obtained through some improved initialisation procedure . when applied alongside algorithms other than tomb , vmmospa results in a minimal change in performance ,",
    "hence these results are not shown .",
    "the results demonstrate the success of vmb in resolving the coalescence phenomenon . in almost all cases ,",
    "vmb outperforms tomb , momb and cphd , and exhibits little deterioration over the error incurred when targets are well - spaced .",
    "the goal of this work was to combine the superior performance of tomb in problems involving well - spaced targets with the robustness of momb in problems involving closely spaced targets : this has been achieved . the small difference between vmb - g ( the gaussian approximation of vmb , in green ) and vmb ( the version retaining a gaussian mixture representation , in blue ) is somewhat surprising .",
    "it is only in the cases with @xmath244 or @xmath245 that there is a discernible difference .",
    "this suggests that a well - chosen gaussian representation is sufficient to represent the posterior uncertainty in all but the lowest snr environments .",
    "the computation times for the methods are compared in table [ tab : runtimes ] .",
    "the table shows the average time to execute a complete monte carlo ( mc ) simulation ( consisting of 201 time steps ) .",
    "complete scenarios with six targets are completed in as little as two seconds , while scenarios with 20 targets are completed in as little as six seconds .",
    "the table demonstrates that vmb is highly tractable , requiring minimal computation beyond the tomb method which it extends ( and , when utilising the gaussian approximating , saving significant computation ) .    *",
    "8@c@ case , @xmath7 & @xmath246 , @xmath247 & cphd & tomb & mm - tomb & momb & vmb - g & vmb + @xmath248 , @xmath249 & @xmath250 , @xmath233 & @xmath251 & @xmath252 & @xmath253 & @xmath252 & @xmath254 & @xmath252 + @xmath248 , @xmath249 & @xmath255 , @xmath233 & @xmath256 & @xmath257 & @xmath258 & @xmath257 & @xmath252 & @xmath257 + @xmath248 , @xmath249 & @xmath259 , @xmath233 & @xmath260 & @xmath261 & @xmath262 & @xmath261 & @xmath263 & @xmath261 + @xmath248 , @xmath249 & @xmath264 , @xmath233 & @xmath265 & @xmath266 & @xmath267 & @xmath258 & @xmath268 & @xmath266 + @xmath248 , @xmath233 & @xmath255 , @xmath233 & @xmath269 & @xmath270 & @xmath271 & @xmath270 & @xmath263 & @xmath272 + @xmath248 , @xmath251 & @xmath250 , @xmath233 & @xmath273 & @xmath274 & @xmath275 & @xmath276 & @xmath261 & @xmath277 + @xmath248 , @xmath249 & @xmath255 , @xmath232 & @xmath278 & @xmath279 & @xmath280 & @xmath270 & @xmath281 & @xmath279 + @xmath254 , @xmath233 & @xmath255 , @xmath233 & @xmath267 & @xmath233 & @xmath265 & @xmath233 & @xmath263 & @xmath233 + @xmath254 , @xmath251 & @xmath250 , @xmath233 & @xmath282 & @xmath258 & @xmath274 & @xmath283 & @xmath249 & @xmath284 + @xmath254 , @xmath249 & @xmath250 , @xmath233 & @xmath270 & @xmath254 & @xmath252 & @xmath254 & @xmath254 & @xmath254 + @xmath254 , @xmath249 & @xmath259 , @xmath233 & @xmath262 & @xmath249 & @xmath281 & @xmath257 & @xmath263 & @xmath249      in order to demonstrate the difference in the accuracy / computation trade - off between the proposed method and existing approaches , we evaluate each on the scenario from the previous section with six targets , @xmath285 and @xmath286 .",
    "we compare vmb - g to sjpda , klsjpda @xcite , and klsjpda using the mmospa estimator @xcite . in order to make a fair comparison",
    ", all methods are preinitialised with the true target positions , and are not seeking to identify new targets through the scenario , or estimate whether targets have departed .",
    "the implementation of sjpda @xcite utilises the same java auction code that was developed for vmb .",
    "klsjpda @xcite is implemented by drawing @xmath287 samples from the joint posterior pdf of the targets .",
    "the mmospa estimator @xcite is evaluated by first executing klsjpda using @xmath288 samples ( to obtain a posterior approximation ) , then using the mmospa estimator with the same samples to obtain estimates .",
    "the results of the scenario are shown in figure [ fig : sjpdaresults ] . the difference in performance between the four methods",
    "is seen to be small .",
    "klsjpda and vmb exhibit very similar performance through the period .",
    "sjpda appears to have slightly better performance than the other methods from time @xmath289 to @xmath290 , while the performance of klsjpda with the mmospa estimator is slightly worse .",
    "the mmospa estimator was observed to increase the spacing between its estimates during this period .",
    "if samples of true target positions were drawn from the joint target distribution at this time , this would be the optimal estimator , but in the specific scenario in which target spacing is much closer than the posterior distribution indicates , a slight performance reduction occurs .",
    "this is because the scenarios are constructed by initialising them at the mid - point in time , causing additional structure in the prior distribution which is not provided to the tracker .",
    "the average execution time for a complete scenario is @xmath291 sec for sjpda , @xmath292 sec for klsjpda , and @xmath293 sec for klsjpda using the mmospa estimator . in comparison ,",
    "the average execution time for vmb - g is @xmath294 sec .    ' , klsjpda shown as ` @xmath295 ' ( with solid line ) , klsjpda with mmospa estimator shown as ` @xmath296 ' ( with dashed line ) , and vmb - g shown as ` @xmath297'.,width=326 ]",
    "this paper has presented a principled , highly efficient , approximate method for finding the mb distribution that minimises the kl divergence from the full rfs distribution . to date , there have been two practical difficulties that have limited application of the jpda / jipda family of trackers to problems involving closely - spaced targets .",
    "the first is the intractability of the calculation of marginal association probabilities ( e.g. , ) .",
    "a highly accurate approximation of these quantities based on variational methods was examined in @xcite .",
    "the second limitation was the problem of coalescence , which has been addressed in this paper in a highly tractable manner .",
    "consequently , we believe that this work represents a significant step forward in the practical applicability of jpda and related methods .",
    "as shown in , minimising kl divergence is equivalent to maximising the likelihood . substituting in the form of the i.i.d.cluster process utilised in the cphd : @xmath298\\delta x \\\\ & = \\int",
    "f(x ) \\log p_g(|x| ) \\delta x + \\int f(x ) \\sum_{x\\in x } \\log g(x ) \\delta x \\\\ & = \\sum_n p_f(n)\\log p_g(n ) + \\int d_f(x ) \\log g(x ) \\mathrm{d } x\\end{aligned}\\ ] ] @xmath299\\delta x \\\\ & = \\int f(x ) \\log p_g(|x| ) \\delta x + \\int f(x ) \\sum_{x\\in x } \\log g(x ) \\delta x \\\\ & = \\sum_n p_f(n)\\log p_g(n ) + \\int d_f(x ) \\log g(x ) \\mathrm{d } x\\end{aligned}\\ ] ] to optimise with respect to @xmath89 and @xmath78 , we take gradients and use lagrange multipliers to apply the constraints that @xmath300 and @xmath301 . subsequently , we find that @xmath90 and @xmath302 , thus the parameters of the i.i.d.cluster process utilised by cphd minimise the set kl divergence .      in this section ,",
    "we prove theorem [ th : bfmbequivexact ] , which shows that the set integral in the kl divergence can be decomposed into a series of nested bernoulli integrals , and that the summation over assignments of elements of the variable cardinality set @xmath43 can be simplified to a summation over permutations between the bernoulli components in @xmath4 and those in @xmath14 .",
    "the proof incorporates the following preliminary steps :    * lemma [ lem : mbintegral ] shows that the multi - target set integral for a mb distribution can be decomposed into a series of bernoulli set integrals * corollary [ cor : setmbdecomp ] observes that this allows use of an alternative definition of a function for which the domain is the bernoulli sets @xmath303 $ ] rather than the union @xmath304 * lemma [ lem : cardconst ] shows that if an alternative form @xmath305 modifies @xmath306 by a multiplicative factor ( e.g. , @xmath307 ) , then the modification simply causes an additive constant ( wrt @xmath308 )    [ lem : mbintegral ] suppose @xmath4 is as defined in , and @xmath306 is an arbitrary set - valued function .",
    "then @xmath309 @xmath310 where , as stated previously , we assume throughout that @xmath64 .",
    "we first prove a case where @xmath311 is a convolution of two distributions @xmath312 and @xmath313 , i.e. , that @xmath314 @xmath315 starting from the left hand side ( lhs ) expression and using the shorthand @xmath316 , and @xmath317 : @xmath318\\\\ = & \\sum_{n=0}^\\infty\\sum_{m=0}^n\\frac{1}{m!(n - m)!}\\int f_1(\\{x_1,\\dots , x_m\\ } ) \\cdot f_2(\\{x_{m+1},\\dots , x_n\\})v(x_n)\\mathrm{d } x_1\\cdots \\mathrm{d } x_n \\label{eq : convstep2 } \\displaybreak[0]\\\\ = & \\sum_{m=0}^\\infty\\sum_{k=0}^\\infty\\frac{1}{m!k!}\\iint f_1(\\{x_1,\\dots , x_m\\ } ) \\cdot f_2(\\{x_{m+1},\\dots , x_{m+k}\\ } ) v(\\{x_1,\\dots , x_{m+k}\\ } ) \\cdot \\mathrm{d } x_1\\cdots \\mathrm{d } x_{m+k } \\label{eq : convstep3 } \\\\",
    "= & \\mbox { rhs } \\notag\\end{aligned}\\ ] ] @xmath319\\\\ = & \\sum_{n=0}^\\infty\\sum_{m=0}^n\\frac{1}{m!(n - m)!}\\int f_1(\\{x_1,\\dots , x_m\\ } ) \\cdot \\notag\\\\   & \\cdot f_2(\\{x_{m+1},\\dots , x_n\\})v(x_n)\\mathrm{d } x_1\\cdots \\mathrm{d } x_n \\label{eq : convstep2 } \\displaybreak[0]\\\\ = & \\sum_{m=0}^\\infty\\sum_{k=0}^\\infty\\frac{1}{m!k!}\\iint f_1(\\{x_1,\\dots , x_m\\ } ) \\cdot \\notag\\\\ & \\cdot f_2(\\{x_{m+1},\\dots , x_{m+k}\\ } ) v(\\{x_1,\\dots , x_{m+k}\\ } ) \\cdot \\notag\\\\   & \\cdot \\mathrm{d } x_1\\cdots \\mathrm{d } x_{m+k } \\label{eq : convstep3 } \\\\ = & \\mbox { rhs } \\notag\\end{aligned}\\ ] ] where replaces the sum of subsets of @xmath316 with a sum of the index subsets of the elements , and observes that the variable of summation is no longer a variable of integration , allowing the sum and integral to be exchanged ; observes that the integral is the same for all @xmath320 with @xmath321 , and that there are @xmath322 ways of choosing @xmath323 with @xmath321 ; and makes a change of variables , defining @xmath324 .",
    "subsequently , the desired result can be obtained by applying the two distribution case @xmath325 times for each @xmath11 .",
    "[ cor : setmbdecomp ] let @xmath303\\triangleq(x_1,\\dots , x_n)$ ] .",
    "suppose that an alternative definition of a set - valued function @xmath326 satisfies @xmath327)=v(x)$ ] for any @xmath303 $ ] such that @xmath304",
    ". then can be equivalently evaluated as : @xmath328\\big )   \\delta x_1\\cdots \\delta x_n}\\ ] ] @xmath329\\big )   \\delta x_1\\cdots",
    "\\delta x_n } \\end{gathered}\\ ] ]    [ lem : cardconst ] suppose @xmath330 , where @xmath331 is an arbitrary function of the cardinality of @xmath43 .",
    "then @xmath332 @xmath333 where @xmath334 is the cardinality distribution corresponding to @xmath4 .",
    "the proof of lemma [ lem : cardconst ] simply separates the log of the product into the sum of logs and simplifies . to be an arbitrary function of @xmath43 . ] with these results in hand , we are ready to prove the main theorem .",
    "following application of lemma [ lem : mbintegral ] , the difference between - and is that the former considers assignment of the elements of @xmath43 to @xmath115 , whereas the latter considers assignment of the @xmath335 zero- or one - element subsets of @xmath43 to @xmath115 .",
    "thus , if @xmath336 , there will be @xmath337 empty subsets @xmath338 in any decomposition @xmath49 .",
    "these could be assigned to the remaining bernoulli distributions ( i.e. , those that were not assigned to non - empty @xmath338 ) in @xmath339 ways , so @xmath340\\right)=c(|x|)g(x)$ ] where @xmath341 . by corollary [ cor : setmbdecomp ] and",
    "lemma [ lem : cardconst ] , the two objectives and are different by a constant ( wrt @xmath116 $ ] ) , so that the solution(s ) attaining the maxima are the same .      to commence , consider the result obtained by substituting into . in this case , when @xmath149 , the problem becomes : @xmath342 ) = \\min_{[g_j]}\\tilde{j}_t([g_j],[q_a(\\pi ) ] ) \\notag \\\\ = & -\\sum_{a\\in{\\cal a},\\pi\\in\\pi_n } w_a q_a(\\pi ) \\sum_{i=1}^n \\int{f_{h_i}(x)\\cdot } \\log\\left(\\sum_{\\tilde{a}\\in{\\cal a},\\tilde{\\pi}\\in\\pi_n } w_{\\tilde{a } } q_{\\tilde{a}}(\\tilde{\\pi } ) f_{h_{\\tilde{\\pi}^{-1}(\\pi(i))}}(x)\\right ) \\delta x \\label{eq : relax1 } \\\\",
    "= & -\\sum_{a\\in{\\cal a},\\pi\\in\\pi_n } w_a q_a(\\pi ) \\sum_{j=1}^n \\int{f_{h_{\\pi^{-1}(j)}}(x)\\cdot } \\log\\left(\\sum_{\\tilde{a}\\in{\\cal a},\\tilde{\\pi}\\in\\pi_n } w_{\\tilde{a } } q_{\\tilde{a}}(\\tilde{\\pi } ) f_{h_{\\tilde{\\pi}^{-1}(j)}}(x)\\right ) \\delta x \\label{eq : relax2 } \\\\",
    "= & -\\sum_{j=1}^n\\int{\\left ( \\sum_{a\\in{\\cal a},\\pi\\in\\pi_n } w_a q_a(\\pi ) f_{h_{\\pi^{-1}(j)}}(x )   \\right ) } \\cdot \\log \\left ( \\sum_{a\\in{\\cal a},\\pi\\in\\pi_n } w_a q_a(\\pi )",
    "f_{h_{\\pi^{-1}(j)}}(x )   \\right ) \\delta x \\label{eq : relax3 } \\\\ = & -\\sum_{j=1}^n\\int{\\left ( \\sum_{h\\in{\\cal h } } q(h , j ) f_{h}(x )   \\right ) } \\cdot \\log \\left ( \\sum_{h\\in{\\cal h } } q(h , j ) f_{h}(x )   \\right ) \\delta x \\label{eq : relax4}\\end{aligned}\\ ] ] @xmath342 ) = \\min_{[g_j]}\\tilde{j}_t([g_j],[q_a(\\pi ) ] ) \\notag \\\\ = & -\\sum_{a\\in{\\cal a},\\pi\\in\\pi_n } w_a q_a(\\pi ) \\sum_{i=1}^n \\int{f_{h_i}(x)\\cdot } \\notag\\\\ & \\quad \\cdot \\log\\left(\\sum_{\\tilde{a}\\in{\\cal a},\\tilde{\\pi}\\in\\pi_n } w_{\\tilde{a } } q_{\\tilde{a}}(\\tilde{\\pi } ) f_{h_{\\tilde{\\pi}^{-1}(\\pi(i))}}(x)\\right ) \\delta x \\label{eq : relax1}\\displaybreak[0 ] \\\\",
    "= & -\\sum_{a\\in{\\cal a},\\pi\\in\\pi_n } w_a q_a(\\pi ) \\sum_{j=1}^n \\int{f_{h_{\\pi^{-1}(j)}}(x)\\cdot } \\notag\\\\ & \\quad \\cdot \\log\\left(\\sum_{\\tilde{a}\\in{\\cal a},\\tilde{\\pi}\\in\\pi_n } w_{\\tilde{a } } q_{\\tilde{a}}(\\tilde{\\pi } ) f_{h_{\\tilde{\\pi}^{-1}(j)}}(x)\\right ) \\delta x \\label{eq : relax2}\\displaybreak[0 ] \\\\",
    "= & -\\sum_{j=1}^n\\int{\\left ( \\sum_{a\\in{\\cal a},\\pi\\in\\pi_n } w_a q_a(\\pi ) f_{h_{\\pi^{-1}(j)}}(x )   \\right ) } \\cdot \\notag \\\\",
    "& \\quad \\cdot \\log \\left ( \\sum_{a\\in{\\cal a},\\pi\\in\\pi_n } w_a q_a(\\pi ) f_{h_{\\pi^{-1}(j)}}(x )   \\right ) \\delta x \\label{eq : relax3}\\displaybreak[0 ] \\\\",
    "= & -\\sum_{j=1}^n\\int{\\left ( \\sum_{h\\in{\\cal h } } q(h , j ) f_{h}(x )   \\right ) } \\cdot \\notag \\\\ & \\quad \\cdot \\log \\left ( \\sum_{h\\in{\\cal h } } q(h , j ) f_{h}(x )   \\right ) \\delta x \\label{eq : relax4}\\end{aligned}\\ ] ] where @xmath343 the step changes the variable of summation from @xmath65 to @xmath344 ( noting that @xmath30 is a bijection ) .",
    "this appendix explores the similarities between bfmb and klsjpda , developing a variant of bfmb that is specialised to the vector case , and showing that it is equivalent to klsjpda . throughout the appendix",
    "we assume that the number of objects @xmath7 is known , and therefore rfs distributions of fixed cardinality are handled as symmetrised vector distributions .",
    "we denote the state of target @xmath65 as @xmath82 , and the joint state of all @xmath7 targets as @xmath108 .",
    "as described in section [ ss : klsjpda ] , klsjpda @xcite seeks to find the ordered distribution @xmath106 in the same unordered family as the original ordered distribution @xmath107 that is best able to be approximated via a gaussian @xmath345 , and the parameters of that gaussian distribution , @xmath346 and @xmath111 .",
    "as an optimisation , this can be written as : @xmath347 where the sum over @xmath30 represents all @xmath348 permutation matrices for the @xmath7 single - target components of @xmath349 ( and hence @xmath350 permutes the targets within @xmath349 according to the permutation matrix @xmath30 ) . as discussed in @xcite , the minimisation with respect to @xmath345 ( or , more specifically @xmath346 and @xmath111 ) simply moment matches the distribution to the mean and covariance of @xmath106 .",
    "it is proven in @xcite that the minimisation with respect to @xmath106 yields : @xmath351      to highlight the similarities of the methods , we develop a variant of the bfmb filter specialised to the vector case ( i.e. , fixed cardinality ) , and permitting the permutation ( the missing information in em ) to vary with the target state @xmath349 ( rather than constraining it to vary only with the global association hypothesis @xmath52 ) . the problem we seek to solve is : @xmath352\\log\\frac{\\sum_\\pi f(\\pi\\boldsymbol{x})}{\\sum_\\pi g(\\pi\\boldsymbol{x})}d\\boldsymbol{x } } \\label{eq : bfmb_vec } \\\\",
    "\\operatorname{subject~to~ } & g(\\boldsymbol{x } ) = \\mathcal{n}\\{\\boldsymbol{x};\\boldsymbol{\\mu},\\boldsymbol{\\sigma}\\ } \\notag\\end{aligned}\\ ] ] we develop a variant of bfmb to exactly solve this case ( i.e. , find a local minimum ) in appendix [ ss : vecbfmbsol ] .",
    "first , we prove that the optimisation in klsjpda is equivalent to .",
    "let @xmath353 be the partial minimisation of over @xmath106 : @xmath354 then @xmath353 is exactly the objective of .",
    "consequently the two optimisation problems and are equivalent .",
    "to begin , we substitute the solution of with respect to @xmath106 ( i.e. , ) into to obtain : @xmath355 \\log\\frac{\\sum_{\\tilde{\\pi } } f(\\tilde{\\pi}\\boldsymbol{x})}{\\sum_{\\tilde{\\pi } } g(\\tilde{\\pi}\\boldsymbol{x } ) } d\\boldsymbol{x } \\intertext{let $ s(\\boldsymbol{x } ) = \\log\\frac{\\sum_{\\tilde{\\pi } } f(\\tilde{\\pi}\\boldsymbol{x})}{\\sum_{\\tilde{\\pi } } g(\\tilde{\\pi}\\boldsymbol{x})}$ and $ t(\\boldsymbol{x})=\\sum_{\\tilde{\\pi}}g(\\tilde{\\pi}\\boldsymbol{x})$ and note that $ s(\\boldsymbol{x})$ and $ t(\\boldsymbol{x})$ are symmetric ,",
    "i.e. , $ s(\\boldsymbol{x})= s(\\pi\\boldsymbol{x})\\;\\forall\\;\\pi$. substituting these functions and changing the variable of integration to $ \\boldsymbol{y}=\\pi\\boldsymbol{x}$ : } & = \\sum_{\\pi}\\int \\left[\\frac{g(\\pi^{-1}\\boldsymbol{y})}{t(\\pi^{-1}\\boldsymbol{y } ) } f(\\boldsymbol{y})\\right ] s(\\pi^{-1}\\boldsymbol{y } ) d\\boldsymbol{y } \\\\ & = \\int \\left[\\frac{\\sum_{\\pi}g(\\pi^{-1}\\boldsymbol{y})}{t(\\boldsymbol{y } ) }   f(\\boldsymbol{y})\\right ] s(\\boldsymbol{y } ) d\\boldsymbol{y } \\\\ & = \\int f(\\boldsymbol{y})s(\\boldsymbol{y } ) d\\boldsymbol{y }   \\intertext{where the final step exploits the fact that the inverse of a permutation is also a permutation , so $ \\sum_{\\pi}g(\\pi^{-1}\\boldsymbol{y})$ is just a reordering of terms in the sum $ \\sum_{\\tilde{\\pi}}g(\\tilde{\\pi}\\boldsymbol{y})=t(\\boldsymbol{y})$. introducing a sum over permutations which evaluates to $ 1 $ and changing variables back to $ \\boldsymbol{x}=\\pi^{-1}\\boldsymbol{y}$ : } & = \\left[\\sum_{\\pi}\\frac{1}{n!}\\right]\\int f(\\boldsymbol{y})s(\\boldsymbol{y } ) d\\boldsymbol{y }   \\\\ & = \\frac{1}{n!}\\sum_{\\pi}\\int f(\\pi\\boldsymbol{x})s(\\pi\\boldsymbol{x } ) d\\boldsymbol{x } \\\\ & = \\frac{1}{n!}\\int\\left[\\sum_{\\pi}f(\\pi\\boldsymbol{x})\\right ] s(\\boldsymbol{x } ) d\\boldsymbol{x}\\end{aligned}\\ ] ] this is the desired result .",
    "now we show how a procedure analogous to section [ ss : vmb ] can be used to exactly solve ( i.e. , find a local minimum ) .",
    "this in turn provides insight into the approximations made in section [ ss : vmb ] . commencing from , separating the log of the quotient into the difference of logs , and dropping the first term",
    "since it does not depend on @xmath345 , we seek to solve the problem @xmath356d\\boldsymbol{x } } \\label{eq : bfmb_vec1 } \\\\",
    "\\operatorname{subject~to~ } & g(\\boldsymbol{x } ) = \\mathcal{n}\\{\\boldsymbol{x};\\boldsymbol{\\mu},\\boldsymbol{\\sigma}\\ } \\notag\\end{aligned}\\ ] ] changing variables to @xmath357 and observing that the argument of the log is symmetric ( @xmath358 as defined above ) and that the integral is constant with respect to the sum over @xmath30 , we arrive at the equivalent problem : @xmath359d\\boldsymbol{y } } \\label{eq : bfmb_vec2 } \\\\",
    "\\operatorname{subject~to~ } & g(\\boldsymbol{y } ) = \\mathcal{n}\\{\\boldsymbol{y};\\boldsymbol{\\mu},\\boldsymbol{\\sigma}\\ } \\notag\\end{aligned}\\ ] ] we define the objective of this problem to be @xmath360 . following the procedure in sections [ ss : backem ] and [ ss : vmb ] , we introduce missing data @xmath361 to describe the mapping between the targets in @xmath107 and those in @xmath345 for each value of @xmath349 using em .",
    "this occurs by introducing the sum over @xmath361 ( which evaluates to @xmath362 ) , and then applying the log - sum inequality : @xmath363f(\\boldsymbol{x})\\log\\frac{\\sum_{\\pi}q_{\\boldsymbol{x}}(\\pi)}{\\sum_{\\pi } g(\\pi\\boldsymbol{x})}d\\boldsymbol{x } } \\label{eq : bfmb_vec3}\\\\ & \\leq \\sum_{\\pi}\\int{q_{\\boldsymbol{x}}(\\pi)f(\\boldsymbol{x})\\log\\frac{q_{\\boldsymbol{x}}(\\pi)}{g(\\pi\\boldsymbol{x})}d\\boldsymbol{x } }   \\label{eq : bfmb_vec4}\\end{aligned}\\ ] ] the log - sum inequality is tight if the ratio @xmath364 is constant ( wrt @xmath30 ) ( * ? ? ?",
    "* p29 ) . solving for @xmath361",
    "we obtain : @xmath365 substituting this into , we find that the ratio @xmath366 which is constant wrt @xmath30 , hence the inequality is tight at the minimum wrt @xmath361 , thus it is tight at convergence ( as discussed in section [ ss : backem ] ) .",
    "finally , we make a change of variable in the upper bound to @xmath357 to obtain an equivalent expression @xmath367 minimising this expression wrt @xmath345 , we find that we need to match @xmath346 and @xmath111 to the mean and covariance of @xmath368 which is identical to .      to summarise , we have shown that :    * the optimisation used by the vector version of bfmb is equivalent to the optimisation used by klsjpda . *",
    "the method of solution for vector bfmb yields identical iterates for @xmath345 and @xmath106 ( defined through the missing data @xmath361 in ) as klsjpda . *",
    "the estimation of missing data in vector bfmb is equivalent to the selection of an ordered density from the same unordered family in klsjpda .",
    "the differences between the vector bfmb developed in this appendix and the version in section [ sec : bfmb ] are as follows :    * whereas the version in this appendix considers the vector case ( where the number of targets is known ) , the version in section [ sec : bfmb ] is formulated through rfs to accommodate uncertainty in the number of objects present .",
    "* since it is intractable to estimate missing data for every joint target state @xmath349 , in section [ sec : bfmb ] , the missing data distribution is constrained to depend only on the global association hypothesis @xmath52 ( replacing @xmath361 with @xmath122 ) .",
    "the consequence of this is that the upper bound is not necessarily tight at the optimal value of @xmath122 , but the approximation is essential for practical tractability .",
    "a similar constraint is applied in sjpda ( i.e. , reordering target indices for each hypothesis , not for each target state ) . *",
    "having constrained the missing data , it is found to be necessary to de - weight the term involving the entropy of the missing data .",
    "while this loosens the upper bound , it is shown experimentally that the minimum that it attains is closer to the minimum of the original objective . setting @xmath149 leads to the point - estimate variant of em @xcite ,",
    "of which the widely - used @xmath152-means algorithm is an instance .",
    "in contrast , sjpda replaces the objective based on kl divergence with the trace of the covariance .",
    "the author thanks profs ba - ngu vo and ba - tuong vo for helpful discussions during the early stages of this work , and the anonymous reviewers for suggestions that helped to clarify many points .",
    "jason l.  williams ( s01m07 ) received degrees of be(electronics)/binftech from queensland university of technology in 1999 , msee from the united states air force institute of technology in 2003 , and phd from massachusetts institute of technology in 2007 .",
    "he worked for several years as an engineering officer in the royal australian air force , before joining australia s defence science and technology organisation in 2007 .",
    "he is also an adjunct senior lecturer at the university of adelaide .",
    "his research interests include target tracking , sensor resource management , markov random fields and convex optimisation ."
  ],
  "abstract_text": [
    "<S> the joint probabilistic data association ( jpda ) filter is a popular tracking methodology for problems involving well - spaced targets , but it is rarely applied in problems with closely - spaced targets due to its complexity in these cases , and due to the well - known phenomenon of coalescence . </S>",
    "<S> this paper addresses these difficulties using random finite sets ( rfss ) and variational inference , deriving a highly tractable , approximate method for obtaining the multi - bernoulli distribution that minimises the set kullback - leibler ( kl ) divergence from the true posterior , working within the rfs framework to incorporate uncertainty in target existence . </S>",
    "<S> the derivation is interpreted as an application of expectation - maximisation ( em ) , where the missing data is the correspondence of bernoulli components ( i.e. , tracks ) under each data association hypothesis . </S>",
    "<S> the missing data is shown to play an identical role to the selection of an ordered distribution in the same ordered family in the set jpda algorithm . </S>",
    "<S> subsequently , a special case of the proposed method is utilised to provide an efficient approximation of the minimum mean optimal sub - pattern assignment estimator . </S>",
    "<S> the performance of the proposed methods is demonstrated in challenging scenarios in which up to twenty targets come into close proximity .    </S>",
    "<S> target tracking , random finite sets , expectation maximization , kullback - leibler divergence , optimum sub - pattern assignment , coalescence , variational inference , bayesian estimation . </S>"
  ]
}