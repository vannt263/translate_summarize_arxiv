{
  "article_text": [
    "principal component analysis ( pca ) is one of the most valuable results from applied linear algebra , and probably the most popular method used for compacting higher dimensional data sets into lower dimensional ones for data analysis , visualization , feature extraction , or data compression @xcite .",
    "pca provides a statistically optimal way of dimensionality reduction by projecting the data onto a lower - dimensional orthogonal subspace that captures as much of the variation of the data as possible .",
    "unfortunately , pca quickly becomes quite expensive to compute for high - dimensional data sets , where both the number of variables and samples is high .",
    "therefore , there is a real need in many applications to accelerate the computation speed of pca algorithms . for large data sets ,",
    "the standard approach is to use an iterative algorithm which computes the components sequentially , and to avoid the global methods which calculate all the components simultaneously .",
    "nipals - pca @xcite is the most frequently used iterative algorithm , and often considered as the standard pca algorithm . however , for large data matrices , or matrices that have a high degree of column collinearity , nipals - pca suffers from loss of orthogonality , due to the errors accumulated in each iteration step @xcite .",
    "therefore , in practice it is only used to estimate the first few components . here , we address both the speed and orthogonality problems , and we offer new solutions which eliminate these shortcomings of the iterative pca algorithms .",
    "we formulate an iterative pca algorithm based on the gram - schmidt re - orthogonalization , which we called gs - pca .",
    "this algorithm is stable from the orthogonality point of view , and if necessary , it can be used to calculate the full set of principal components .",
    "the speed up issue is tackled with a parallel implementation for graphics processing units ( gpus ) . here , we present the gpu parallel implementation of both nipals - pca and gs - pca algorithms .",
    "the numerical results show that the gpu parallel optimized versions , based on cublas ( nvidia ) @xcite , are substantially faster ( up to 12 times ) than the cpu optimized versions based on cblas ( gnu scientific library ) @xcite .",
    "in the following description , the dataset to be analyzed is represented by the @xmath0 matrix @xmath1 .",
    "each column , @xmath2 , @xmath3 , contains all the observations of one attribute .",
    "also , we assume that each column is mean centered , i.e. if @xmath4 are the original vectors then @xmath5 pca transforms the set of input column vectors @xmath6 $ ] into another set of column vectors @xmath7 $ ] , called principal component scores .",
    "this transformation has the property that most of the original data s information content ( or most of its variance ) is stored in the first few component scores .",
    "this allows reduction of the data to a smaller number of dimensions , with low information loss , simply by discarding the last component scores .",
    "each component is a linear combination of the original inputs and each component is orthogonal .",
    "this linear transformation of the matrix @xmath1 is specified by a @xmath8 matrix @xmath9 so that the matrix @xmath1 is factorized as : @xmath10 where @xmath9 is known as the loadings matrix .",
    "there are several pca algorithms in the literature , namely svd ( singular value decomposition ) and nipals ( nonlinear iterative partial least squares ) , which use the data matrix , and power and evd ( eigenvalue decomposition ) which use the covariance of the data matrix @xcite .",
    "svd and evd extract all the principal components simultaneously , while nipals and power calculate them sequentially .",
    "unfortunately , the traditional implementation of pca through svd or evd quickly becomes prohibitive for very large data sets . in this case",
    ", an approximate solution can be more efficiently obtained using the iterative approach based on the nipals algorithm .",
    "the nipals - pca algorithm can be described as following @xcite . in the first step ,",
    "the initial data @xmath1 is copied into the residual matrix @xmath11 .",
    "then , in the next steps the algorithm extracts iteratively one component at a time ( @xmath12 ) by repeated regressions of @xmath13 on scores @xmath14 to obtain improved loads @xmath15 , and of @xmath1 on these @xmath15 to obtain improved scores @xmath14 . after the convergence",
    "is achieved , this process is followed by a deflation of the data matrix : @xmath16 the convergence test consists in comparing two successive estimates of the eigenvalue @xmath17 and @xmath18 .",
    "if the absolute difference @xmath19 is smaller than some small error @xmath20 then the convergence is achieved and the algorithm proceeds to the deflation step . using the nipals - pca algorithm approach , the decomposition of the data matrix @xmath1 takes the form : @xmath21 where @xmath22 $ ] is the matrix formed using the first @xmath23 scores , @xmath24 $ ] is the matrix of the first @xmath23 loadings , and @xmath11 is the residual matrix .",
    "the pseudo - code of the nipals - pca algorithm is given below :",
    "@xmath25    for(@xmath26 ) do    \\ {    @xmath27    @xmath28    for(@xmath29 ) do    \\",
    "{    @xmath30    @xmath31    @xmath32    @xmath33    if(@xmath34 ) then break    @xmath35    }    @xmath36    }    return @xmath37 , @xmath9 , @xmath11      a well known shortcoming of the nipals - pca algorithm is the loss of orthogonality @xcite .",
    "both , the computed scores @xmath14 and the loadings @xmath15 , are supposed to be orthogonal .",
    "however , because of the errors accumulated at each iteration step ( which involves large matrix - vector operations ) this orthogonality is quickly lost and in practice one can compute accurately only the first few components . in order to stabilize the iterative pca computation , from the orthogonality point of view , we propose an algorithm which is based on the gram - schmidt ( gs ) re - orthogonalization process . the classical gs algorithm ( cgs )",
    "recursively constructs a set of orthonormal basis vectors for the subspace spanned by a given set of linearly independent normalized vectors @xcite .",
    "it is well known that the cgs algorithm is also numerically unstable due to rounding errors .",
    "however , the cgs can be easily stabilized by a small modification obtaining the so called modified gram - schmidt ( mgs ) algorithm @xcite . unfortunately , the mgs algorithm can not be expressed by level-2 blas functions ( matrix - vector operations ) and therefore it requires a substantial amount of global communications , when implemented on a parallel computer @xcite .",
    "in contrast , the cgs algorithm can be easily expressed using matrix - vector operations and therefore it is more suitable for parallel implementation .",
    "also , the numerical stability of cgs can be achieved by applying it iteratively @xcite . in the proposed gs - pca algorithm",
    "the re - orthogonalization correction is applied to both the scores and the loadings at each iteration step .",
    "for the pseudo - code formulation of the gs - pca algorithm we prefer to use the truncated svd description , since for @xmath38 the algorithm also returns the full svd decomposition of the input matrix : @xmath39 where @xmath40 and @xmath41 are the first @xmath23 left and respectively rigth eigenvectors , @xmath42 are the corresponding eigenvalues , and @xmath11 is the residual .",
    "one can easily show that @xmath43 and @xmath44 .",
    "the pseudo - code of the gs - pca algorithm can be formulated as following :    @xmath25    for(@xmath26 ) do    \\ {    @xmath45    @xmath46    for(@xmath29 ) do    \\",
    "{    @xmath47    if(@xmath48 ) then    \\ {    @xmath49    @xmath50    }    @xmath51    @xmath52    if(@xmath48 ) then    \\ {    @xmath53    @xmath54    }    @xmath55    @xmath56    if(@xmath57 ) then break    @xmath58    }    @xmath59    }    @xmath60    @xmath61    return @xmath37 , @xmath9 , @xmath11 ( for pca ) or @xmath62 , @xmath63 , @xmath64 ( for svd )    one can see that in every iteration step , if @xmath48 then both the right ( loads ) and the left ( scores ) eigenvectors are re - orthonormalized .",
    "this procedure stabilizes the algorithm but it also increases a little bit the computational effort .",
    "however , this effort will be compensated by the efficiency of the parallel implementation .",
    "the gs - pca algorithm assures the perfect orthogonality of both the loads and the scores .",
    "the errors accumulated in gs - pca are only due to to the desired precision @xmath65 in the estimation of the eigenvalues @xmath66 .",
    "also , for @xmath38 the gs - pca algorithm returns a full svd decomposition of the original matrix @xmath1 , with a maximum error @xmath65 for eigenvalues and perfectly orthogonal left / right eigenvectors .",
    "the newly developed gpus now include fully programmable processing units that follow a stream programming model and support vectorized single and double precision floating - point operations .",
    "for example , the cuda computing environment provides a standard c like language interface to the nvidia gpus @xcite .",
    "the computation is distributed into sequential grids , which are organized as a set of thread blocks .",
    "the thread blocks are batches of threads that execute together , sharing local memories and synchronizing at specified barriers .",
    "an enormous number of blocks , each containing maximum 512 threads , can be launched in parallel in the grid .    in our implementation of nipals - pca and gs - pca algorithms we use cublas , a recent parallel implementation of blas , developed by nvidia on top of the cuda programming environment @xcite .",
    "cublas library provides functions for :    * creating and destroying matrix and vector objects in gpu memory ; * transferring data from cpu mainmemory to gpu memory ; * executing blas on the gpu ; * transferring data from gpu memory back to the cpu mainmemory .",
    "blas defines a set of low - level fundamental operations on vectors and matrices which can be used to create optimized higher - level linear algebra functionality .",
    "highly efficient implementations of blas exist for most current computer architectures and the specification of blas is widely adopted in the development of high quality linear algebra software , such as the gnu scientific library ( gsl ) @xcite .",
    "we have selected gsl cblas , for our host ( cpu ) implementation , due to its portability on various platforms ( windows / linux / osx , intel / amd ) and because it is free and easy to use in combination with gcc ( gnu compiler ) .",
    "the gsl library provides a low - level layer which corresponds directly to the c - language blas standard , referred here as cblas , and a higher - level interface for operations on gsl vectors and matrices .",
    "the cblas ( gnu scientific library ) and respectively cublas ( nvidia cuda ) implementations of the nipals - pca and gs - pca algorithms require the following level 1 , 2 , 3 blas functions ( see the cblas / cublas programming manuals for definition details ) :    cblas ( level 2 ) : gsl_blas_dgemv    cublas ( level 2 ) : cublasdgemv    * computes in double precision the matrix - vector product and sum : @xmath67 @xmath68 and @xmath69 are double precision scalars , and @xmath70 and @xmath71 are double precision vectors .",
    "@xmath72 is a matrix consisting of double precision elements .",
    "matrix @xmath72 is stored in column major format .",
    "cblas ( level 1 ) : gsl_blas_daxpy    cublas ( level 1 ) : cublasdaxpy    * computes the double precision sum : @xmath73 multiplies double precision vector @xmath70 by double precision scalar @xmath74 and adds the result to double precision vector @xmath75 .    cblas ( level 1 ) : gsl_blas_dnrm2    cublas ( level 1 ) : cublasdnrm2    * computes the euclidean norm of a double precision vector @xmath76 : @xmath77    cblas ( level 3 ) : gsl_blas_dger    cublas ( level 3 ) : cublasdger    * computes in double precision the matrix - matrix sum : @xmath78 where @xmath68 is a double precision scalar , @xmath70 is an @xmath79 element double precision vector , @xmath75 is an @xmath80 element double precision vector , and @xmath72 is an @xmath0 matrix consisting of double precision elements .",
    "matrix @xmath72 is stored in column major format .",
    "these are the critical functions / kernels which are efficiently exploited in the parallel cublas implementation .",
    "the other involved functions are for vector / matrix memory allocation and vector / matrix accessing , device ( gpu ) initialization , host - device data transfer and error handling .    in the cublas implementation",
    ", the data space is allocated both on host ( cpu ) mainmemory and on device ( gpu ) memory .",
    "after the data is initialized on host it is transferred on device , where the main parallel computation occurs .",
    "the results are then transferred back on host memory .",
    "the double precision code for cblas and cublas implementations are given in the appendices 1 , 2 , 3 and 4 .",
    "these implementations can be easily modified in order to meet the end user s specifications .",
    "for example the single precision blas data allocation and vector / matrix accessing functions have the prefix gsl_vector_float , gsl_matrix_float etc .",
    "( see cblas / cublas programming manuals for details ) . for convenience ,",
    "we have included a timer which measures the time of all main operations involved by the algorithm .",
    "the cblas ( gsl ) versions can be compiled using the following commands :    ....                g++ -o2 nipals_pca.cpp -lgsl -lgslcblas -lm ....    ....                g++ -o2 gs_pca.cpp -lgsl -lgslcblas -lm ....    an obvious requirement for the cublas ( nvidia ) version is the presence of an nvidia gpu installed .",
    "the gpu must be minimum gtx260 , gtx280 or tesla c1060 , otherwise there is no support for double precision .",
    "also , the nvidia driver , toolkit and sdk must be correctly instaled . in order to compile the cublas ( nvidia ) version we recommend to use the following simple make file :    .... # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # build script for the cublas ( nvidia ) implementation # # of the nipals - pca and gs - pca project                 # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #    # add source files here   # ( comment / uncomment your choice )      executable       : = nipals_pca   # for nipals - pca # executable       : = gs_pca       # for gs - pca    # c / c++ source files ( compiled with gcc / c++ ) # ( comment / uncomment your choice )      cfiles           : = nipals_pca.c # for nipals - pca # cfiles           : = gs_pca.c      # for gs - pca     # additional libraries needed by the project      usecublas        : = 1    # rules and targets    include .. / .. /common",
    "/ common.mk    # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # ....",
    "the numerical tests have been carried out on the following system : amd phenom 9950 cpu ( 2.6ghz ) ; xfx gtx280 gpu ; nvidia linux 64-bit driver ( 177.67 ) ; cuda toolkit and sdk 2.0 ; ubuntu linux 64-bit 8.04.1 , gnu scientific library v.1.11 ; compilers : gcc ( gnu ) , nvcc ( nvidia ) .",
    "the gpu used is a high end graphics card solution with 240 stream processors and 1 gb ddr3 ram , which supports both single and double precision and it is theoretically capable of 1 tflop computational power .    in figure 1",
    "we give the cpu vs gpu execution time as a function of the size of the randomly generated input matrix @xmath1 ( @xmath81 , n = m/2 , k=10 , \\varepsilon = 10^{-7}$ ] ) .",
    "the time gap between cpu and gpu increases very fast by increasing the size of the input matrix , and the cpu time versus the gpu time reaches a maximum for @xmath82 , where the gpu is about 12 times faster than the cpu .",
    "the gs - pca algorithm is only about 5 - 7% slower than the standard niplals - pca algorithm , in both cpu and gpu implementation .",
    "these results also show that the gpu performance is dependent on the scale of the problem .",
    "thus , in order to exploit efficiently the massive parallelism of gpus and to effectively use the hardware capabilities , the problem itself needs to scale accordingly , such that thousands of threads are defined and used in computation .",
    "( @xmath83 , red = nipals - pca , blue = gs - pca ) . ]    in conclusion , we have presented an iterative gs - pca algorithm based on gram - schmidt re - orthogonalization .",
    "the gs - pca algorithm assures the perfect orthogonality of both the loads and the scores , and thus totally eliminates the loss of orthogonality present in the standard nipals - pca algorithm .",
    "also , we have discussed the gpu parallel implementation of both nipals - pca and gs - pca algorithms .",
    "we have shown that the gpu parallel optimized versions , based on cublas ( nvidia ) are substantially faster ( up to 12 times ) than the cpu optimized versions based on cblas ( gnu scientific library ) .",
    "the author acknowledge the financial support from ibi and the university of calgary .",
    "99 bjorck , a . , paige , c .",
    ", ( 1992 ) loss and recapture of orthogonality in the modified gram - schmidt algorithm , _ siam j. matrix anal .",
    "_ , * 13*(1 ) , 176 - 190 .",
    "galassi , m .",
    "_ et  al _ , ( 2006 ) gnu scientific library reference manual - revised second edition ( network theory limited ) .",
    "golub , g.h . , ( 1996 ) matrix computations , 3rd ed .",
    "( john hopkins university press , baltimore ) .",
    "jackson , j.e . , ( 1991 ) a user s guide to principal components ( john wiley and sons , new york ) .",
    "jolliffe , i.t . , ( 2002 ) principal component analysis ( springer , new york , 2002 ) .",
    "kramer , r.,(1998 ) chemometric techniques for quantitative analysis ( crc press , new york ) .",
    "lingen , f.j . , ( 2000 ) efficient gram - schmidt orthonormalisation on parallel computers , _ commun .",
    "numer . meth .",
    "_ , * 16 * , 57 - 66 .",
    "nvidia , ( 2008 ) cuda compute unified device architecture ( programming guide , cublas library ) .",
    "wold , s . ,",
    "esbensen , k . ,",
    "geladi , p . , ( 1987 ) principal component analysis , _ chemometrics and intelligent laboratory systems _ , * 2 * , 37 - 52 .",
    "c / c++ example for the cblas ( gnu scientific library )   // implementation of the nipals - pca algorithm // m. andrecut ( c ) 2008 //   //",
    "compile with : g++ -o3 nipals.cpp -lgsl -lgslcblas -lm",
    "// includes , system     # include < math.h > # include <",
    "time.h >    // includes , gsl & cblas    # include < gsl / gsl_vector.h > # include < gsl / gsl_matrix.h > # include < gsl / gsl_blas.h >    // declarations    int nipals_gsl(int , int , int , gsl_matrix * ,                  gsl_matrix * , gsl_matrix * ) ;    int print_results(int , int , int ,                     gsl_matrix * , gsl_matrix * ,                     gsl_matrix * , gsl_matrix * ) ;    //",
    "main int main(int argc , char * * argv ) { // pca model : x = tp ' + r    // input : x , mxn matrix ( data ) // input : m = number of rows in x // input : n = number of columns in x   //",
    "input : k = number of components ( k<=n )    // output : t , mxk scores matrix   // output : p , nxn loads matrix // output : r , mxn residual matrix    int m = 1000 , m ;   int n = m/2 , n ; int k = 25 ;    printf(\"\\nproblem dimensions : mxn=%dx%d , k=%d\\n \" , m , n , k ) ;    //",
    "initialize srand and clock     srand(time(null ) ) ;    clock_t start = clock ( ) ;     double htime ;       //",
    "initiallize some random test data x    gsl_matrix * x = gsl_matrix_alloc(m , n ) ; for(m=0 ; m < m ; m++ )     {   for(n=0 ; n < n ; n++ )      {    gsl_matrix_set(x , m , n , rand()/(double)rand_max ) ;    }   }       //",
    "allocate memory for t , p , r    gsl_matrix * t = gsl_matrix_alloc(m , k ) ;    gsl_matrix * p = gsl_matrix_alloc(n , k ) ;    gsl_matrix * r = gsl_matrix_alloc(m , n ) ;    htime = ( ( double)clock()-start)/clocks_per_sec ;    printf(\"\\ntime for data allocation : % f\\n \" , htime ) ;     // call the nipals_gsl ( ) function    start = clock ( ) ;    gsl_matrix_memcpy(r , x ) ;      nipals_gsl(m , n , k , t , p , r ) ;    htime = ( ( double)clock()-start)/clocks_per_sec ;    printf(\"\\n\\ntime for nipals - pca computation on host : % f\\n \" , htime ) ;     //",
    "the results are in t , p , r    print_results(m , n , k , x , t , p , r ) ;    // memory clean up and shutdown    gsl_matrix_free(r ) ;    gsl_matrix_free(p ) ;    gsl_matrix_free(t ) ;    gsl_matrix_free(x ) ;       printf(\"\\npress enter to exit ... \\n \" ) ;   getchar ( ) ;    return exit_success ; }    int nipals_gsl(int m , int n , int k , gsl_matrix * t ,                  gsl_matrix * p , gsl_matrix * r ) { // pca model : x = tp ' + r     // input : x , mxn matrix ( data ) // input : m = number of rows in x // input : n = number of columns in x // input : k = number of components ( k<=n )    // output : t , mxk scores matrix   // output : p , nxk loads matrix // output : r , mxn residual matrix ( x is initially copied in r )       // maximum number of iterations    int j = 10000 ;       //",
    "max error    double er = 1.0e-7 ;       //",
    "some useful pointers    double * a = ( double*)calloc(1 , sizeof(a ) ) ;     double * b = ( double*)calloc(1 , sizeof(b ) ) ;        int k , n , j ;     //",
    "mean center the data     gsl_vector * u = gsl_vector_calloc(m ) ;    for(n=0 ; n < n ; n++ )    {   gsl_blas_daxpy(1.0 , & gsl_matrix_column(r , n).vector , u ) ;      }    for(n=0 ; n < n ; n++ )    {   gsl_blas_daxpy(-1.0/n , u , & gsl_matrix_column(r , n).vector ) ;   }    for(k=0 ; k < k ; k++ )   {   gsl_blas_dcopy(&gsl_matrix_column(r , k).vector ,                   & gsl_matrix_column(t , k).vector ) ;     * a = 0.0 ;     for(j=0 ; j < j ; j++ )    {    gsl_blas_dgemv(cblastrans , 1.0 , r ,                    & gsl_matrix_column(t , k).vector ,                    0.0 , & gsl_matrix_column(p , k).vector ) ;       gsl_blas_dscal(1.0/gsl_blas_dnrm2(&gsl_matrix_column(p , k).vector ) ,                    & gsl_matrix_column(p , k).vector ) ;      gsl_blas_dgemv(cblasnotrans , 1.0 , r ,                    & gsl_matrix_column(p , k).vector ,                    0.0 , & gsl_matrix_column(t , k).vector ) ;                   * b = gsl_blas_dnrm2(&gsl_matrix_column(t , k).vector ) ;           if(fabs(*a - * b ) < er*(*b ) ) break ;      * a = * b ;    }   gsl_blas_dger(-1.0 , & gsl_matrix_column(t , k).vector ,                   & gsl_matrix_column(p , k).vector , r ) ;   }           //",
    "clean up memory    free(a ) ;     free(b ) ;    gsl_vector_free(u ) ;    return exit_success ; }     int print_results(int m , int n , int k ,                     gsl_matrix * x , gsl_matrix * t ,                     gsl_matrix * p , gsl_matrix * r ) { int m , n ;    //",
    "if m < 13 print the results on screen     if(m >",
    "12 ) return exit_success ;    printf(\"\\nx\\n \" ) ;    for(m=0 ; m < m ; m++ )   {   for(n=0 ; n < n ; n++ )    {    printf(\"%+f   \" , gsl_matrix_get(x , m , n ) ) ;    }    printf(\"\\n \" ) ;   }    printf(\"\\nt\\n \" ) ;    for(m=0 ; m < m ; m++ )   {   for(n=0 ;",
    "n < k ; n++ )    {    printf(\"%+f   \" , gsl_matrix_get(t , m , n ) ) ;    }    printf(\"\\n \" ) ;   }          gsl_matrix * f = gsl_matrix_alloc(k , k ) ;         gsl_blas_dgemm ( cblastrans , cblasnotrans , 1.0 , t , t , 0.0 , f ) ;    printf(\"\\nt ' * t\\n \" ) ;    for(m=0 ; m < k ; m++ )   {   for(n=0 ;",
    "n < k ; n++ )    {    printf(\"%+f   \" , gsl_matrix_get(f , m , n ) ) ;    }    printf(\"\\n \" ) ;   }    gsl_matrix_free(f ) ;     printf(\"\\np\\n \" ) ;    for(m=0 ; m < n ; m++ )   {   for(n=0 ;",
    "n < k ; n++ )    {    printf(\"%+f   \" , gsl_matrix_get(p , m , n ) ) ;    }   printf(\"\\n \" ) ;   }    gsl_matrix",
    "* g = gsl_matrix_alloc(k , k ) ;         gsl_blas_dgemm ( cblastrans , cblasnotrans , 1.0 , p , p , 0.0 , g ) ;    printf(\"\\np ' * p\\n \" ) ;    for(m=0 ; m < k ; m++ )   {   for(n=0 ;",
    "n < k ; n++ )    {    printf(\"%+f   \" , gsl_matrix_get(g , m , n ) ) ;    }   printf(\"\\n \" ) ;   }    gsl_matrix_free(g ) ;    printf(\"\\nr\\n \" ) ;    for(m=0 ; m < m ; m++ )   {   for(n=0 ; n <",
    "n ; n++ )    {    printf(\"%+f   \" , gsl_matrix_get(r , m , n ) ) ;    }   printf(\"\\n \" ) ;   }    return exit_success ; } ....",
    ".... // c / c++ example for the cublas ( nvidia )   // implementation of nipals - pca algorithm //",
    "// m. andrecut ( c ) 2008     // includes , system     # include < stdio.h > # include < stdlib.h > # include < string.h > # include <",
    "time.h >    // includes , cuda     # include < cublas.h >    //",
    "matrix indexing convention     # define id(m , n , ld ) ( ( ( n ) * ( ld ) + ( m ) ) )    //",
    "declarations     int nipals_cublas(int , int , int ,                     double * , double * ,                     double * ) ;    int print_results(int , int , int ,                     double * , double * ,                     double * , double * ) ;    // main   int main(int argc , char * * argv ) {   // pca model : x = t * p ' + r    // input : x , mxn matrix ( data )   // input : m = number of rows in x // input : n = number of columns in x   // input : k = number of components ( k<=n )    // output : t , mxk scores matrix   // output : p , nxn loads matrix // output : r , mxn residual matrix       int m = 1000 , m ; int n = m/2 , n ; int k = 25 ;           printf(\"\\nproblem dimensions : mxn=%dx%d , k=%d\\n \" , m , n , k ) ;    // initialize srand and clock     srand(time(null ) ) ;    clock_t start = clock ( ) ;     double dtime ;       // initialize cublas    cublasstatus status ;    status = cublasinit ( ) ;    if(status ! = cublas_status_success )    {   fprintf(stderr , \" ! cublas initialization error\\n \" ) ;     return exit_failure ;   }    // initiallize some random test data",
    "x    double * x ;     x = ( double*)malloc(m*n * sizeof(x[0 ] ) ) ;    if(x = = 0 )    {   fprintf(stderr , \" ! host memory allocation error : x\\n \" ) ;     return exit_failure ;   }    for(m = 0 ; m < m ; m++ )    {   for(n = 0 ; n",
    "< n ; n++ )     {    x[id(m , n , m ) ] = rand ( ) / ( double)rand_max ;     }   }    //",
    "allocate host memory for t , p , r    double * t ;     t = ( double*)malloc(m*k * sizeof(t[0 ] ) ) ; ;    if(t = = 0 )   {   fprintf(stderr , \" ! host memory allocation error : t\\n \" ) ;     return exit_failure ;   }    double * p ;     p = ( double*)malloc(n*k * sizeof(p[0 ] ) ) ; ;    if(p = = 0 )   {   fprintf(stderr , \" ! host memory allocation error : p\\n \" ) ;     return exit_failure ;   }    double * r ;     r = ( double*)malloc(m*n * sizeof(r[0 ] ) ) ; ;    if(r = = 0 )   {   fprintf(stderr , \" !",
    "host memory allocation error : r\\n \" ) ;     return exit_failure ;   }               dtime = ( ( double)clock ( ) - start)/clocks_per_sec ;    printf(\"\\ntime for data allocation : % f\\n \" , dtime ) ;          // call nipals_cublas ( )     start = clock ( ) ;     memcpy(r , x , m*n * sizeof(x[0 ] ) ) ;    nipals_cublas(m , n , k , t , p , r ) ;    dtime = ( ( double)clock ( ) - start)/clocks_per_sec ;    printf(\"\\ntime for nipals - pca computation on device : % f\\n \" , dtime ) ;     print_results(m , n , k , x , t , p , r ) ;    // memory clean up     free(r ) ;     free(p ) ;        free(t ) ;      free(x ) ;    // shutdown    status = cublasshutdown ( ) ;    if(status !",
    "= cublas_status_success )     {   fprintf(stderr , \" !",
    "cublas shutdown error\\n \" ) ;    return exit_failure ;   }    if(argc < = 1 || strcmp(argv[1 ] , \" -noprompt \" ) )    {   printf(\"\\npress enter to exit ... \\n \" ) ;     getchar ( ) ;   }    return exit_success ; }     int nipals_cublas(int m , int n , int k ,                     double * t , double * p ,                     double * r ) { // pca model : x = t * p ' + r    // input : x , mxn matrix ( data ) // input : m = number of rows in x // input : n = number of columns in x ( n<=m ) // input : k = number of components ( k < n )    // output : t , mxk scores matrix   // output : p , nxk loads matrix // output : r , mxn residual matrix    // cublas error handling    cublasstatus status ;",
    "// maximum number of iterations     int j = 10000 ;    //",
    "max error    double er = 1.0e-7 ;     int k , n , j ;       // transfer the host matrix x to device matrix dr     double * dr = 0 ;     status = cublasalloc(m*n , sizeof(dr[0 ] ) , ( void**)&dr ) ;    if(status !",
    "= cublas_status_success )   {   fprintf ( stderr , \" ! device memory allocation error ( dr)\\n \" ) ;    return exit_failure ;   }        status = cublassetmatrix(m , n , sizeof(r[0 ] ) , r , m , dr , m ) ;    if(status !",
    "= cublas_status_success )    {   fprintf(stderr , \" ! device access error ( write dr)\\n \" ) ;    return exit_failure ;   }    //",
    "allocate device memory for t , p    double * dt = 0 ;     status = cublasalloc(m*k , sizeof(dt[0 ] ) , ( void**)&dt ) ;    if(status ! = cublas_status_success )   {   fprintf(stderr , \" ! device memory allocation error ( dt)\\n \" ) ;    return exit_failure ;   }       double * dp = 0 ;     status = cublasalloc(n*k , sizeof(dp[0 ] ) , ( void**)&dp ) ;    if(status !",
    "= cublas_status_success )   {   fprintf(stderr , \" ! device memory allocation error ( dp)\\n \" ) ;    return exit_failure ;   }        //",
    "mean center the data     double * du = 0 ;     status = cublasalloc(m , sizeof(du[0 ] ) , ( void**)&du ) ;    if(status !",
    "= cublas_status_success )    {   fprintf(stderr , \" ! device memory allocation error ( du)\\n \" ) ;    return exit_failure ;   }      cublasdcopy(m , & dr[0 ] , 1 , du , 1 ) ;       for(n=1 ; n <",
    "n ; n++ )    {   cublasdaxpy(m , 1.0 , & dr[n*m ] , 1 , du , 1 ) ;   }    for(n=0 ; n < n ; n++ )    {   cublasdaxpy(m , -1.0/n , du , 1 , & dr[n*m ] , 1 ) ;       }       double a , b ;            for(k=0 ; k < k ; k++ )   {   cublasdcopy(m , & dr[k*m ] , 1 , & dt[k*m ] , 1 ) ;     a = 0.0 ;      for(j=0 ; j < j ; j++ )    {    cublasdgemv('t ' , m , n , 1.0 , dr , m , & dt[k*m ] , 1 , 0.0 , & dp[k*n ] , 1 ) ;      cublasdscal(n , 1.0/cublasdnrm2(n , & dp[k*n ] , 1 ) , & dp[k*n ] , 1 ) ;      cublasdgemv('n ' , m , n , 1.0 , dr , m , & dp[k*n ] , 1 , 0.0 , & dt[k*m ] , 1 ) ;      b = cublasdnrm2(m , & dt[k*m ] , 1 ) ;      if(fabs(a - b ) < er*b ) break ;      a = b ;    }   cublasdger(m , n , -1.0 , & dt[k*m ] , 1 , & dp[k*n ] , 1 , dr , m ) ;   }          // transfer device dt to host t    cublasgetmatrix(m , k , sizeof(dt[0 ] ) , dt , m , t , m ) ;    // transfer device dp to host p    cublasgetmatrix(n , k , sizeof(dp[0 ] ) , dp , n , p , n ) ;    // transfer device dr to host r    cublasgetmatrix(m , n , sizeof(dr[0 ] ) , dr , m , r , m ) ;       //",
    "clean up memory     status = cublasfree(dp ) ;    status = cublasfree(dt ) ;    status = cublasfree(dr ) ;    return exit_success ; }     int print_results(int m , int n , int k ,                     double * x , double * t ,                     double * p , double * r ) { int m , n , k ;       // if m < 13 print the results on screen    if(m >",
    "12 ) return exit_success ;    printf(\"\\nx\\n \" ) ;    for(m=0 ; m < m ; m++ )   {   for(n=0 ; n <",
    "n ; n++ )    {    printf(\"%+f   \" , x[id ( m , n , m ) ] ) ;    }    printf(\"\\n \" ) ;   }    printf(\"\\nt\\n \" ) ;    for(m=0 ; m < m ; m++ )   {   for(n=0 ; n",
    "< k ; n++ )    {    printf(\"%+f   \" , t[id(m , n ,",
    "m ) ] ) ;    }   printf(\"\\n \" ) ;   }    double a ;    printf(\"\\nt ' * t\\n \" ) ;    for(m = 0 ; m < k ; m++ )   {   for(n=0 ;",
    "n < k ; n++ )    {    a=0 ;     for(k=0 ; k < m ; k++ )      {     a = a + t[id(k , m , m ) ] * t[id(k , n , m ) ] ;     }    printf(\"%+f   \" , a ) ;    }   printf(\"\\n \" ) ;   }          printf(\"\\np\\n \" ) ;    for(m=0 ; m < n ; m++ )   {   for(n=0 ; n < k ; n++ )    {    printf(\"%+f   \" , p[id(m , n , n ) ] ) ;    }   printf(\"\\n \" ) ;   }    printf(\"\\np ' * p\\n \" ) ;    for(m = 0 ; m < k ; m++ )   {   for(n=0 ; n",
    "< k ; n++ )    {    a=0 ;     for(k=0 ; k < n ; k++ )      {     a = a + p[id(k , m , n ) ] * p[id(k , n , n ) ] ;     }    printf(\"%+f   \" , a ) ;    }   printf(\"\\n \" ) ;   }    printf(\"\\nr\\n \" ) ;    for(m=0 ; m < m ; m++ )   {   for(n=0 ; n <",
    "n ; n++ )    {    printf(\"%+f   \" , r[id ( m , n , m ) ] ) ;    }   printf(\"\\n \" ) ;   }    return exit_success ; } ....",
    "c / c++ example for the cblas ( gnu scientific library )   // implementation of pca - gs algorithm //",
    "// m. andrecut ( c ) 2008    // includes , system     # include < math.h >    # include <",
    "time.h >    // includes , gsl & cblas    # include < gsl / gsl_vector.h >    # include < gsl / gsl_matrix.h >    # include < gsl / gsl_blas.h >    //",
    "declarations    int gs_pca_gsl(int , int , int ,                  gsl_matrix * , gsl_matrix * ,                  gsl_matrix * ) ;    int print_results(int , int , int ,                     gsl_matrix * , gsl_matrix * ,                     gsl_matrix * , gsl_matrix * ) ;    // main int main(int argc , char * * argv ) { // pca model : x = tp ' + r    // input : x , mxn matrix ( data ) // input : m = number of rows in x //",
    "input : n = number of columns in x   // input : k = number of components ( k<=n )    // output : t , mxk scores matrix   // output : p , nxk loads matrix // output : r , mxn residual matrix    int m = 1000 , m ;   int n = m/2 , n ; int k = 10 ;       printf(\"\\nproblem dimensions : mxn=%dx%d , k=%d \" , m , n , k ) ;    // initialize srand and clock     srand(time(null ) ) ;    clock_t start = clock ( ) ;     double htime ;       // initiallize some random test data x    gsl_matrix * x = gsl_matrix_alloc (",
    "m , n ) ;    for(m=0 ; m < m ; m++ )     {   for(n=0 ; n <",
    "n ; n++ )      {    gsl_matrix_set(x , m , n , rand()/(double)rand_max ) ;    }   }       // allocate memory for t , p , r    gsl_matrix * t = gsl_matrix_alloc ( m , k ) ;    gsl_matrix * p = gsl_matrix_alloc ( n , k ) ;    gsl_matrix * r = gsl_matrix_alloc ( m , n ) ;    htime = ( ( double)clock()-start)/clocks_per_sec ;    printf(\"\\ntime for data allocation : % f\\n \" , htime ) ;     // call gs_pca_gsl    start = clock ( ) ;    gsl_matrix_memcpy ( r , x ) ;      gs_pca_gsl(m , n , k , t , p , r ) ;    htime = ( ( double)clock()-start)/clocks_per_sec ;    printf(\"\\n\\ntime for gs - pca host computation : % f\\n \" , htime ) ;     //",
    "the results are in t , p , r    print_results(m , n , k , x , t , p , r ) ;    // memory clean up and shutdown    gsl_matrix_free(r ) ;    gsl_matrix_free(p ) ;    gsl_matrix_free(t ) ;    gsl_matrix_free(x ) ;       printf(\"\\npress enter to exit ...",
    "\\n \" ) ;   getchar ( ) ;    return exit_success ; }     int gs_pca_gsl(int m , int n , int k ,                  gsl_matrix * t , gsl_matrix * p ,                  gsl_matrix * r ) { // pca model : x = tlp ' + r    // input : x , mxn matrix ( data , copied in r ) // input : m = number of rows in x //",
    "input : n = number of columns in x //",
    "input : k = number of components ( k<=n )    // output : t , mxk left eigenvectors   // output : p , nxk right eigenvectors //",
    "output : l , kx1 eigenvalues // output : r , mxn residual       // maximum number of iterations    int j = 10000 ;       //",
    "max error    double er = 1.0e-7 ; double a ;    int n , k , j ;     //",
    "mean center the data     gsl_vector * u = gsl_vector_calloc ( m ) ;    for(n=0 ; n < n ; n++ )    {   gsl_blas_daxpy(1.0 , & gsl_matrix_column(r , n).vector , u ) ;      }    for(n=0 ; n <",
    "n ; n++ )    {   gsl_blas_daxpy(-1.0/n , u , & gsl_matrix_column(r , n).vector ) ;   }    // allocate memory fo eigenvalues    gsl_vector * l = gsl_vector_alloc(k ) ;       //",
    "gs_pca    for(k=0 ; k < k ; k++ )   {   gsl_blas_dcopy(&gsl_matrix_column(r , k).vector ,                   & gsl_matrix_column(t , k).vector ) ;     a = 0.0 ;     for(j=0 ; j < j ; j++ )    {    gsl_blas_dgemv(cblastrans , 1.0 , r ,                    & gsl_matrix_column(t , k).vector ,                    0.0 , & gsl_matrix_column(p , k).vector ) ;       if(k>0 )     {     gsl_blas_dgemv(cblastrans , 1.0 ,                     & gsl_matrix_submatrix ( p , 0 , 0 , n , k).matrix ,                     & gsl_matrix_column(p , k).vector , 0.0 ,                     & gsl_vector_subvector ( u , 0 , k).vector ) ;       gsl_blas_dgemv(cblasnotrans , -1.0 ,                     & gsl_matrix_submatrix ( p , 0 , 0 , n , k).matrix ,                     & gsl_vector_subvector ( u , 0 , k).vector , 1.0 ,                     & gsl_matrix_column(p , k).vector ) ;                      }      gsl_blas_dscal(1.0/gsl_blas_dnrm2(&gsl_matrix_column(p , k).vector ) ,                    & gsl_matrix_column(p , k).vector ) ;      gsl_blas_dgemv(cblasnotrans , 1.0 , r , & gsl_matrix_column(p , k).vector ,                    0.0 , & gsl_matrix_column(t , k).vector ) ;           if(k>0 )     {     gsl_blas_dgemv(cblastrans , 1.0 ,                     & gsl_matrix_submatrix ( t , 0 , 0 , m , k).matrix ,                     & gsl_matrix_column(t , k).vector , 0.0 ,                     & gsl_vector_subvector ( u , 0 , k).vector ) ;       gsl_blas_dgemv(cblasnotrans , -1.0 ,                     & gsl_matrix_submatrix ( t , 0 , 0 , m , k).matrix ,                     & gsl_vector_subvector ( u , 0 , k).vector , 1.0 ,                     & gsl_matrix_column(t , k).vector ) ;                      }      gsl_vector_set(l , k , gsl_blas_dnrm2(&gsl_matrix_column(t , k).vector ) ) ;              gsl_blas_dscal(1.0/gsl_vector_get(l , k ) ,                    & gsl_matrix_column(t , k).vector ) ;      if(fabs(a - gsl_vector_get(l , k ) ) < er*gsl_vector_get(l , k ) ) break ;      a = gsl_vector_get(l , k ) ;    }    gsl_blas_dger ( -gsl_vector_get(l , k ) ,                    & gsl_matrix_column(t , k).vector ,                    & gsl_matrix_column(p , k).vector , r ) ;   }          for(k=0 ; k < k ; k++ )   {   gsl_blas_dscal ( gsl_vector_get(l , k ) ,                    & gsl_matrix_column(t , k).vector ) ;   }    // memory clean up    gsl_vector_free(l ) ;    gsl_vector_free(u ) ;    return exit_success ; }     int print_results(int m , int n , int k ,                     gsl_matrix * x , gsl_matrix * t ,                     gsl_matrix * p , gsl_matrix * r ) {      int m , n ;    / * if m < 13 print the results on screen * /        if(m > 12 ) return exit_success ;    printf(\"\\nx\\n \" ) ;    for(m=0 ; m < m ; m++ )   {   for(n=0 ; n <",
    "n ; n++ )    {    printf(\"%+f   \" , gsl_matrix_get(x , m , n ) ) ;    }   printf(\"\\n \" ) ;   }    printf(\"\\nt\\n \" ) ;    for(m=0 ; m < m ; m++ )   {   for(n=0 ;",
    "n < k ; n++ )    {    printf(\"%+f   \" , gsl_matrix_get(t , m , n ) ) ;    }   printf(\"\\n \" ) ;   }          gsl_matrix * f = gsl_matrix_alloc ( k , k ) ;            gsl_blas_dgemm(cblastrans , cblasnotrans , 1.0 , t , t , 0.0 , f ) ;    printf(\"\\nt ' * t\\n \" ) ;    for(m=0 ; m < k ; m++ )   {   for(n=0 ; n",
    "< k ; n++ )    {    printf(\"%+f   \" , gsl_matrix_get(f , m , n ) ) ;    }   printf(\"\\n \" ) ;   }    gsl_matrix_free(f ) ;     printf(\"\\np\\n \" ) ;    for(m=0 ; m < n ; m++ )   {   for(n=0 ;",
    "n < k ; n++ )    {    printf(\"%+f   \" , gsl_matrix_get(p , m , n ) ) ;    }   printf(\"\\n \" ) ;   }    gsl_matrix * g = gsl_matrix_alloc ( k , k ) ;            gsl_blas_dgemm ( cblastrans , cblasnotrans , 1.0 , p , p , 0.0 , g ) ;    printf(\"\\np ' * p\\n \" ) ;    for(m=0 ; m < k ; m++ )   {   for(n=0 ;",
    "n < k ; n++ )    {    printf(\"%+f   \" , gsl_matrix_get(g , m , n ) ) ;    }   printf(\"\\n \" ) ;   }    gsl_matrix_free(g ) ;    printf(\"\\nr\\n \" ) ;    for(m=0 ; m < m ; m++ )   {   for(n=0 ; n <",
    "n ; n++ )    {    printf(\"%+f   \" , gsl_matrix_get(r , m , n ) ) ;    }   printf(\"\\n \" ) ;   }    return exit_success ; } ....",
    "....                                                                                                    status = cublasshutdown ( ) ; if(status ! = cublas_status_success )     {   fprintf ( stderr , \" ! cublas shutdown error\\n \" ) ;    return exit_failure ;   }    if(argc < = 1 || strcmp(argv[1 ] , \" -noprompt \" ) )    {   printf(\"\\npress enter to exit ... \\n \" ) ;   getchar ( ) ;   }    return exit_success ; }                                                  double * du = 0 ;     status = cublasalloc(m , sizeof(du[0 ] ) , ( void**)&du ) ; if(status !",
    "= cublas_status_success )    {   fprintf ( stderr , \" ! device memory allocation error ( du)\\n \" ) ;    return exit_failure ;   }                      for(j=0 ; j < j ; j++ )    {    cublasdgemv ( ' t ' , m , n , 1.0 , dr , m , & dt[k*m ] , 1 , 0.0 , & dp[k*n ] , 1 ) ;       if(k>0 )     {     cublasdgemv ( ' t ' , n , k , 1.0 , dp , n , & dp[k*n ] , 1 , 0.0 , du , 1 ) ;         cublasdgemv ( ' n ' , n , k , -1.0 , dp , n , du , 1 , 1.0 , & dp[k*n ] , 1 ) ;     }            cublasdscal ( n , 1.0/cublasdnrm2(n , & dp[k*n ] , 1 ) , & dp[k*n ] , 1 ) ;"
  ],
  "abstract_text": [
    "<S> principal component analysis ( pca ) is a key statistical technique for multivariate data analysis . for large </S>",
    "<S> data sets the common approach to pca computation is based on the standard nipals - pca algorithm , which unfortunately suffers from loss of orthogonality , and therefore it s applicability is usually limited to the estimation of the first few components . </S>",
    "<S> here we present an algorithm based on gram - schmidt orthogonalization ( called gs - pca ) , which eliminates this shortcoming of nipals - pca . also , we discuss the gpu ( graphics processing unit ) parallel implementation of both nipals - pca and gs - pca algorithms . </S>",
    "<S> the numerical results show that the gpu parallel optimized versions , based on cublas ( nvidia ) are substantially faster ( up to 12 times ) than the cpu optimized versions based on cblas ( gnu scientific library ) .    </S>",
    "<S> institute for biocomplexity and informatics    university of calgary    2500 university drive nw , calgary    alberta , t2n 1n4 , canada </S>"
  ]
}