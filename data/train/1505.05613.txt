{
  "article_text": [
    "clustering is a fundamental process for applications such as content analysis , information integration , information retrieval , web mining and knowledge discovery .",
    "the proliferation of the internet has driven the need for unsupervised document clustering for analyzing natural language without having to label all possible topics such as in supervised learning approaches .",
    "other web data exists with the potential of generating millions of clusters from billions of examples .",
    "for example , sensor data , images , video , audio , customer data , smart grid data , and , data produced by everyday physical objects such as in the web of things .",
    "clustering algorithms are essential for these emerging web applications , and it is challenging due to heavy computational requirements .",
    "it may be possible to achieve web - scale clustering using a high performance distributed architecture , but the cost is prohibitive for many applications .",
    "additionally , high - performance computing platforms are limited to organizations with large budgets and highly skilled employees .",
    "additionally , achieving fine - grained clustering poses difficulties even with high - performance architectures .",
    "we found no examples of clustering near a billion natural language documents .",
    "information retrieval utilizes document clustering for pre - clustering collections on to multiple machines for distributed search .",
    "collection distribution uses clustering for organization into thematic groups .",
    "these clusters are ranked by collection selection to determine which clusters to search .",
    "collection selection selects a few relevant thematically related clusters for each query and therefore improves search performance .",
    "only the top k documents that are returned by the initial search are considered for further analysis when ranking search results . by contrast",
    ", global pre - clustering of documents captures the thematic association of documents on the basis of shared content .",
    "documents that may not contain the search terms , but are thematically related to documents that are initially retrieved by keywords search , can be found in re - ranking and query expansion . finding hidden thematic relationships based on the initial keyword search provides added motivation to pre - clustering document collections .    we are not aware of any solutions to the problem of clustering a billion web pages on standard hardware , other than by sampling to produce a relatively small number of clusters .",
    "it is not possible to cluster a collection into numerous small clusters with only a small sample .",
    "clustering algorithms require redundancy in the data , and a small number of documents in the sample means reduced redundancy ; this does not allow for discovery of small clusters .",
    "the size of the final clusters produced by sampling is relatively large because of distribution of the entire collection over the learned clusters .",
    "for instance , kulkarni and callan @xcite have clustered a 0.1% sample of clueweb09 into 1,000 clusters and then mapped all 500 million documents onto these clusters , yielding an average cluster size of 500,000 documents .",
    "these large clusters work for collection distribution and selection , but we later demonstrate the advantage of finer grain clusters . furthermore , coarse clusters are not useful for topical clustering of documents , where topic cluster size is expected to be several of orders of magnitude smaller .",
    "fine grained clustering is not achievable through aggressive subsampling in web - scale collections .",
    "this paper introduces the parallel streaming em - tree algorithm for clustering web - scale collections on low - cost standard hardware .",
    "the quality of clustering solutions and processing efficiency depend on the representation of objects .",
    "therefore , we present a useful data model using random projections @xcite for representation of the document collection as binary signatures .",
    "we apply the parallel streaming em - tree clustering algorithm that is specialized for binary signatures to segment the collection into fine grained clusters .",
    "the proposed method is evaluated with the web scale collections , clueweb09 and clueweb12 , containing 500 and 733 million english language documents respectively .",
    "document clusters are often evaluated by comparison to a ground truth set of categories for documents .",
    "no topical labels are available for clueweb , and it is nigh impossible to generate the labels for a large scale web crawl for millions of categories .",
    "we present two novel methods for external cluster validation : ( 1 ) ad hoc relevance ; and ( 2 ) spam classification .",
    "we used relevance judgements from the trec web track in 2010 , 2011 , 2012 and 2013 @xcite and spam classifications created by cormack et .",
    "@xcite for the evaluation of document clustering .",
    "extensive analysis reveals that the clusterings with 500,000 to 700,000 clusters were found to improve the quality using this evaluation .",
    "we emphasize that there are no earlier reports in the open literature of document clustering of this magnitude , and there are no standard benchmark resources or comparative evaluation results .",
    "this paper makes two novel contributions : ( 1 ) we introduce the novel parallel streaming signature em - tree algorithm that can cluster documents at scales not previously reported ; and ( 2 ) we solve the problem of cluster validation in web - scale collections where creating a ground truth set of categories is a near impossible task for human assessors .    section [ sec : related ] discusses related research on web scale clustering .",
    "the generation of a document representation is discussed in section [ sec : representation ] .",
    "section [ sec : complexity ] discusses the complexity of the proposed method and section [ sec : emtree ] introduces the em - tree algorithm .",
    "the evaluation of cluster validity is presented in section [ sec : evaluation ] .",
    "the discussion of potential applications and conclusions are contained in sections [ sec : applications ] and [ sec : conclusion ] .",
    "few clustering algorithms can scale to web collections on modest hardware platforms .",
    "they rely on supercomputing resources or imposed constraints such as : ( 1 ) using a small sample for clustering and then mapping the entire collection to these clusters ; ( 2 ) by reducing the number of target clusters .",
    "mapreduce has been used to implement clustering algorithms by jin @xcite and kumar @xcite .",
    "the experiments created a small number of clusters using small data sets several gigabytes in size .",
    "esteves and rong @xcite investigated using the open source mahout library implemented using mapreduce for the clustering of wikipedia .",
    "the k - means algorithm took two days to cluster the 3.5 million wikipedia articles into 20 clusters using ten compute nodes . in comparison",
    ", we increase _ both _ the number of documents and clusters by two to three orders of magnitude .",
    "we analyze the complexity of k - means and em - tree in section [ sec : complexity ] .",
    "broder et .",
    "@xcite describe an approach to scaling up k - means by the use of inverted indexes .",
    "it reverses the process of assigning points to clusters and assign clusters to points .",
    "the authors also highlight the need for fine - grained clustering of high dimensional data such as web pages , users , and , advertisements .",
    "they indicate that k - means may take thousands of hours to converge when using parallel and distributed programming techniques such as mapreduce .",
    "while the authors show improvements to k - means , they only experiment with clustering millions of examples into thousands of clusters .",
    "we far exceed this scale .",
    "bahmani et .",
    "@xcite present a scalable parallel approach to the @xmath0 seeding approach from k - means++ @xcite called k - means|| .",
    "the experiments run k - means to complete convergence after initialization .",
    "it is often impractical to run the optimization to complete convergence .",
    "typically , more than 95% of the optimization happens in the first few iterations , and further iterations offer very little improvement .",
    "we observed this with em - tree in prior work @xcite .",
    "the em - tree can be seeded with k - means|| . however , for fine - grained clustering of clueweb using signatures we found no advantage to more computationally expensive initializations .",
    "it may be due to the smoothing properties of random projections , but analysis of this is beyond the scope of this paper .",
    "however , when such an approach does help , our approach is complementary to k - means|| .",
    "we can leverage the advantages of both approaches by the use of the scalable initialization procedure and the  scalable  optimization of em - tree .",
    "as with other approaches , the experiments were run on data sets orders of magnitude smaller and much lower dimensionality .",
    "@xcite present the birch algorithm that incrementally constructs a tree as data is streamed from disk .",
    "it is similar to the k - tree algorithm except that not all data points are kept in the tree @xcite .",
    "the birch algorithm performs updates along the insertion path like k - tree . when using signatures it has been reported to cause scalability problems because the signature bits are continually unpacked and repacked @xcite .",
    "furthermore , the em - tree has immutable tree state at each iteration leading to scalable parallel implementations .",
    "we were unable to find any reference to birch clustering near 1 billion documents .",
    "we have only found a few other examples of clustering near a billion examples .",
    "these reports were on lower dimensional data sets of images , image patches , low dimensional generic point sets and weather patterns .",
    "@xcite used 2000 cpus , mapreduce and commodity hardware to cluster 1.5 billion images into 1 million clusters for near duplicate detection . similarly to our approach , they used random projections and a tree structure .",
    "@xcite also describe an approach to clustering images for duplicate detection using a 2000 core cluster for clustering 2 billion images .",
    "bisgin and dalfes @xcite used 16,000 cpus and a top500 supercomputer to cluster weather data into 1000 clusters .",
    "@xcite report on the speedup obtained when using gpus instead of multicore cpus , to cluster billions of low dimensional point sets of upto 8 dimensions .",
    "these approaches rely on high - performance computing resources that put it out of reach for most application developers . additionally , we found no examples of clustering web - scale document collections into fine - grained clusters .",
    "the clueweb collections are some of the largest document collections used for research .",
    "these web crawls have been used at trec to evaluate ad hoc retrieval and other web search tasks .",
    "while it is clear that retrieval systems can scale to these collections , there has been little investigation of clustering such large document collections .",
    "the first task in clustering documents is the definition of a representation .",
    "we use a binary signature representation called topsig @xcite .",
    "it offers a scalable approach to the construction of document signatures by applying random indexing @xcite , or random projections @xcite and numeric quantization .",
    "signatures derived in this manner have been shown to be competitive with state of the art retrieval models at early precision , and also to clustering approaches .",
    "the binary signature vectors faithfully preserve the mutual similarity relationships between documents in the original representation .",
    "the johnson lindenstrauss lemma @xcite states that if points in a high dimensional space are projected into a randomly chosen subspace then the distances between the points are approximately preserved .",
    "the lower dimensionality required is asymptotically logarithmic with respect to the original high dimensional space .",
    "this signature generation is similar to that of simhash @xcite .",
    "however , we use a different term weighting scheme , use signatures more than an order of magnitude larger , and , much sparser random codes . simhash has predominantly been applied to nearest duplicate detection where relatively short signatures are used to find the few nearest neighbors of a document .",
    "we refer the reader to @xcite for the specific details of the signature generation process .",
    "we removed stop words , and stemmed  words  using the porter algorithm .",
    "we use 4096-bit signatures because that choice was previously shown to be sufficient to produce the same quality clustering as the original real - valued representation of documents @xcite .",
    "we derived the signatures used to represent the clueweb collections on a single 16 core machine .",
    "our approach uses a fixed amount of memory while indexing and can process a collection without keeping the entire index in memory .",
    "each document is indexed independently of all other documents leading to massive parallelization .",
    "it is important to understand that the focus of this paper is efficient , scalable document clustering , not to compare different approaches of the generation of document signatures .",
    "there have been numerous approaches reported that fall under the general category of similarity preserving hashing @xcite .",
    "the use of signatures is advantageous for increased computational efficiency of document to document similarity .",
    "it has been shown to provide a one to two orders of magnitude increase in processing speed for document clustering over traditional sparse vector representations with sacrificing quality @xcite . in prior research @xcite",
    ", we used an algorithm similar to k - means designed especially for signatures .",
    "we specifically designed the proposed em - tree algorithm for signatures .",
    "all documents and cluster representatives are binary signatures . therefore , traditional vector space clustering approaches can not be applied to these signatures without expanding them into a larger representation such as integer or floating point vectors .",
    "the em - tree algorithm can cluster vast collections into numerous fine - grained clusters .",
    "k - means is one of the earliest and most popular clustering algorithms due to its quick convergence and linear time complexity .",
    "however , it is not suitable for web scale collections that have a vast diversity of content resulting in an enormous number of topics as elaborated in the previous section .",
    "em - tree solves this previously unaddressed issue .",
    "a @xmath1-way nearest neighbor search tree is a recursive data structure that indexes a set of @xmath2 binary signatures in @xmath3 dimensions where , @xmath4 , @xmath5 and @xmath6 .",
    "each node contains a list of @xmath7s that are @xmath8 pairs .",
    "the @xmath9s are binary signatures that are cluster representatives of the associated subtree .",
    "each iteration of the optimization  updates the @xmath9s .",
    "the @xmath7 values are the nodes associated with the  @xmath9s that are one level deeper .",
    "the data structure also generalizes to all vector space representations in @xmath10 .",
    "however , in the case of the em - tree , we have specialized the data structure for binary signatures .",
    "it is not a traditional  vector space clustering algorithm .    applying the k - means clustering algorithm recursively",
    "is known as tree - structured vector quantization ( tsvq ) @xcite , repeated k - means , or for clusters of size two , bisecting k - means .",
    "it generates a @xmath1-way tree in @xmath10 .",
    "the em - tree algorithm iteratively optimizes a randomly initialized @xmath1-way tree until convergence .",
    "in contrast , the repeated k - means algorithm initially creates @xmath11 clusters using k - means @xcite .",
    "it recursively clusters each partition in a layer wise fashion until reaching a desired tree depth or node size .",
    "the em - tree builds a cluster tree in a different manner @xcite .",
    "the collection is initially partitioned by selecting a random set of data points as cluster prototypes . unlike repeated k - means ,",
    "clustering is not applied at this point . instead",
    ", the data points are recursively distributed to random partitions until a desirable tree depth is reached . at this point ,",
    "the initial tree is complete .",
    "now cluster means are updated in a bottom - up fashion .",
    "the entire process of tree insertion and tree update forms an iteration of the optimization and has been proven to converge @xcite .",
    "the entire tree is recomputed with each insert update cycle .",
    "it is different from repeated k - means in which the optimization process is run to completion at each node before proceeding deeper into the tree .",
    "the em - tree algorithm includes an additional pruning step in for removing empty branches of the tree .",
    "the em - tree is not a standard vector space clustering algorithm in this context .",
    "it works directly with binary vectors where all documents and cluster prototypes are binary vectors .",
    "the em - tree algorithm can optimize any @xmath1-way tree such as a tree produced by a low - cost algorithm with poor cluster quality .",
    "the algorithm can be applied to any subtree in a @xmath1-way tree . in a setting",
    "where a changing data set is being clustered , branches of the tree affected by insertions and deletions can be restructured to the data independently of the rest of the tree .    by a process of insertion ( expectation ) ,",
    "update ( maximization ) , and pruning , the @xmath1-way tree model adapts to the underlying data as the clusters converge as seen in figure [ emtree_pseudo ] .",
    "the procedure initializes the em - tree algorithm where @xmath1 is the tree order , @xmath12 is the set of data points to cluster and @xmath13 is the tree depth .",
    "it produces a height - balanced tree where all leaves are at the same depth .",
    "the procedure inserts a set of vectors into a @xmath1-way tree .",
    "points are inserted by following the nearest neighbor search path , where at each node in the tree , the branch with the nearest key is followed .",
    "the procedure updates the means in the tree according to the current assignment of data points in the leaves .",
    "since we work with binary vectors , the bits in each signature assigned to a given leaf node are unpacked and accumulated into an integer vector .",
    "this vector is then used in two different ways .",
    "it is quantized to a binary vector to form a new cluster mean for a given leaf node , and it is also propagated up the tree so that new cluster centroids higher in the tree can be computed .",
    "the updating of cluster centroids is performed for all levels of the tree .",
    "the procedure removes any branches with no associated data points .",
    "it is completed bottom up where leaf nodes are removed first .",
    "the empty branches are removed once and have completed .",
    "it allows the tree structure to adapt to the data .",
    "the optimization error of em - tree is robust with respect to different initializations when producing fine - grained clusters of clueweb .",
    "a tree with random initialization and subsequent optimization was found to minimize the objective function on par with more computationally expensive approaches .",
    "a 10% sample is used to seed the tree because it is large enough to produce meaningful centroid representations for a large number of clusters .",
    "this initial seeding only consumes a small percentage of the compute time .",
    "additionally , approximation guarantees can be achieved by initializing with the @xmath0 approach as in k - means++ @xcite but analysis of this is beyond the scope of this paper .",
    "in summary , the procedure initializes and iteratively optimizes an em - tree of order @xmath1 that is @xmath13 levels deep .",
    "@xmath14 @xmath15 = false not @xmath15 @xmath16 @xmath17 @xmath18 @xmath19 @xmath20 @xmath15 = true @xmath21 @xmath22      one of the key constraints in scaling clustering algorithms to large data sets is the availability of computer memory . one way to approach",
    "this problem is to adopt a streaming paradigm in which the data points are streamed sequentially , and only a small portion of them are ever kept in memory at one time .",
    "the approach used in the streaming em - tree is only to keep internal nodes in memory .",
    "the data points are collected in accumulators associated with each leaf node .",
    "the data points inserted are , therefore , added into the accumulators but are then discarded .    at each iteration of the optimization ,",
    "all signatures are read from disk and inserted into the tree as seen in figure [ fig : streamingem ] .",
    "bits are unpacked from the signature and added into the accumulators .",
    "a count is kept of the number of points added into the accumulator . when all inserts have been performed , the values of the accumulators in the leaf nodes are propagated up the tree and new centroids are calculated in the step .",
    "the centroids , which are now vectors of integers , are quantized to produce new bit signatures .",
    "the step is then performed .",
    "large datasets can be clustered on a single machine with limited memory .",
    "the algorithm is compute bound on typical hardware architectures the data is streamed from disk .",
    "the em - tree algorithm is particularly amenable to large scale parallel and distributed implementations due to the nature of the optimization process .",
    "the key property that ensures scalability is that the entire tree is immutable at each iteration , and the data points can be concurrently inserted according to the nearest neighbor search path .",
    "the em - tree algorithm has been parallelized using threads via the use of loop parallelization and producer - consumer pipelines .",
    "the accumulators in the leaf nodes are locked when updated , although the chance of multiple threads attempting to update the same accumulator at the same time is small due to the typically very large number of leaf nodes .",
    "this property ensures that em - tree scales almost linearly with regards to the number of threads . for thread implementation ,",
    "we use intel s threading building blocks ( tbb ) .",
    "a parallel threaded implementation can be executed on a single machine in which case the entire tree structure with accumulators is shared between threads .",
    "the bottleneck for the streaming em - tree is the insertion step , primarily because of nearest neighbor calculations and bit unpacking of accumulators .",
    "the and steps require much less computation .",
    "figure [ fig : scaling ] contains the results of an experiment to test how em - tree scales with the number of threads used .",
    "we tested em - tree on a machine with 16 cpu cores .",
    "the speedup is measured relative to the execution in a single thread on the same hardware .",
    "the cpus support two hyper - threads per core , and we have stepped through the number of threads in the program from 1 up to 34 . the top curve in figure [ fig : scaling ] shows the speedup that could be obtained if speedup exactly followed the number of threads .",
    "it is not achievable in practice when using hyper - threads .",
    "the bottom curve shows the actual execution speedup as the number of threads is increased .",
    "the middle curve depicts the number of cores used  when there are fewer threads than cores the threads are mapped to idle cores .",
    "after that point , all cpu cores are utilized , and threads are mapped to busy cores using their second hyper thread . in summary , on a machine with 16 cores we were able to achieve a 16 fold speedup through conventional multi - threading . in this experiment , we used a 10 million document sample from the clueweb09 collection .",
    "it is large enough to eliminate any caching effects and small enough that the single threaded experiment does not take weeks to run .",
    "em - tree and streaming em - tree have been implemented by the authors as part of the open source lmw - tree c++ software package .",
    "the software has been built on linux , mac os and windows .",
    "the clusters and tree structure have been made available online .",
    "it is important to have these openly available since any alternative solution can be compared .",
    "the time complexity of the k - means algorithm is linear with respect to all its inputs , @xmath23 @xcite . where @xmath2 is the number of non - zero entries in the document - term matrix , @xmath11 is the number of clusters , and , @xmath24 is the number of iterations .",
    "the number of iterations , @xmath24 , is limited to some small fixed amount of iterations as k - means converges quickly in practice .",
    "fine grained clustering pushes @xmath11 much closer to @xmath2 . as @xmath25 , the complexity of k - means approaches @xmath26 .",
    "it makes k - means impractical for fine - grained clustering of web - scale collections containing billions of examples .",
    "algorithms using height balanced trees such as em - tree , k - tree @xcite and birch @xcite alleviate this problem by reducing the time complexity associated with the number of clusters to @xmath27 .",
    "the complexity of em - tree approaches @xmath28 as @xmath25 for fine - grained clustering .",
    "three nontrivial computer architecture considerations contribute to the efficiency of our approach .",
    "we process 64 dimensions of the binary signatures at a time in a single cpu operation .",
    "while it is a constant speedup , it is certainly not negligible .",
    "our approach only uses integer operations that are faster and have higher throughput in instructions per cycle on modern processors than floating point operations used for k - means clustering .",
    "furthermore , due to the extremely concise representation of our model as binary signatures , we observe more cache hits .",
    "cache misses are extremely costly on modern processors with main memory accesses taking more than 100 cycles .",
    "when using traditional sparse representations , the cluster means in the root of the tree contain many millions of terms , quickly exceeding processor cache size . with signatures",
    "a cluster means always consumes 4096 bits .    by the use of efficient representations and algorithms we turn the problem of fine grained clustering of the entire web into a tractable problem",
    "we ran the experiments on a dual - socket xeon e5 - 2665 based system .",
    "each cpu package has @xmath29 ghz cpu cores connected in a ring topology , for a total of 16 cpu cores .",
    "the program used 64 gb of memory to store the tree in memory where the data set is 240 gb to 350 gb on disk .",
    "we streamed the data points from a 7200rpm 3 tb sata disk providing 150 mb per second of sequential read performance , although this much bandwidth was not needed .",
    "a mid - range machine like this can be purchased for around 5,000 usd .",
    "the clueweb 09 and 12 collections took approximately two and three days to index and resulted in 240 gb and 350 gb signatures indexes .",
    "it is a concise representation of the collections that measure in tens of terabytes when uncompressed .",
    "the em - tree processes these document signatures to produce document clusters .",
    "clustering the clueweb 09 and 12 signatures took approximately 15 and 20 hours .",
    "both ran for five iterations and produced 700,000 and 600,000 clusters respectively .",
    "most of the literature on cluster evaluation focuses on evaluating document clusters by comparison to the ground truth set of categories for documents .",
    "it poses problems when evaluating large - scale collections containing millions to billions of documents .",
    "human assessors are required to label the entire collection into many thousands of potential topics .",
    "even if a small percentage of the collection is labeled , how does an assessor choose between many thousands of potential topics in a general purpose document collection such as the world wide web ?",
    "the experience of labeling large test collections such as rcv1 @xcite demonstrate that categorizing a general purpose document collection such as the web is a daunting task for humans .",
    "rcv1 contains 800,000 short newswire articles with hundreds of categories .",
    "even though this collection is many orders of magnitude simpler than the web , assessors still struggled with categorization .",
    "there are no topical human generated labels available for clueweb .",
    "therefore , we propose novel uses of external sources of information as proxies for cluster validation : ( 1 ) ad hoc relevance ; and ( 2 ) spam classification . the information retrieval evaluation community has already dealt with the problem of assessor load in ad hoc relevance evaluation via the use of pooling @xcite .",
    "pooling reduces assessor load and topics evaluated are specific and well defined .",
    "the issue of assessor load is alleviated by using ad hoc relevance to assess document clustering , instead of category labels for every document in a collection .",
    "we also present another alternative evaluation based upon spam classification produced in earlier research by cormack et .",
    "al . @xcite .",
    "the goal in this case is to place documents with the same spam score into the same cluster .",
    "it measures how consistent the clustering is with respect to the external measure of spam learned from 4 different spam classifications .",
    "we of course conjecture that there is some underlying topical vocabulary that spammy documents use .",
    "clustering , grouping documents on the basis of shared vocabulary , should show a correlation with spam detection scores  which are , of course , independently derived .    before we present results of these two cluster validation experiments , we present some qualitative evaluation by inspection of partial documents content in the final trees in tables [ table : meetup.com ] , [ table : diseases ] and [ table : infotech ] .",
    "xxxxx * weight loss * & * investment * & * language * & * politics * & * scifi and fantasy * + 9f08ef2b0 & 9f06c7770 & 9f0476410 & 9f0a77b10 & 9f0b871e0 + & & & & + 383 bits  find a meetup group near you weight loss meetups pelham & 463 bits  find a meetup group real estate buying investing meetups alexandria & 386 bits  find a meetup group linguistics meetups aurora & 637 bits  find a meetup group democratic underground meetups portland & 667 bits ",
    "find a meetup group star trek meetups long beach + & & & & + 417 bits  find a meetup group weight loss meetups englewood & 587 bits ",
    "find a meetup group real estate buying investing meetups decatur & 630 bits  find a meetup group near you language lovers meetups naperville & 851 bits ",
    "find a meetup group democratic underground meetups worcester & 706 bits  find a meetup group star trek meetups allendale + & & & & + 474 bits ",
    "find a meetup group fitness meetups staten island & 801 bits  find a meetup group investing meetups lake jackson & 941 bits ",
    "find a meetup group near you scandinavian languages meetups skokie & 1097 bits  find a meetup group near you dennis kucinich meetups lakewood & 1009 bits  find a meetup group near you comic books meetups reading + & & & & + 550 bits  find a meetup group fitness meetups new rochelle & 998 bits",
    " find a meetup group near you real estate buying investing meetups soulsbyville & 1252 bits  find a meetup group near you hungarian language meetups libertyville & 1671 bits ",
    "harford county democrats & 1157 bits  find a meetup group buffy meetups london + & & & & + 580 bits  find a meetup group weight loss meetups clinton & 1752 bits  westfield group mergers and acquisitions alacrastorecom & 1844 bits  algebra 1 terms and practice problems chapter 1 lesson 1 flash cards quizlet & 1911 bits  welsh statutory instruments 2005 & 1804 bits  fleet registration station archive star trek online forums +    xxxxx * hepatitis * & * hiv vaccine * & * treating hiv * & * disease research * & * bacteria * + a5b5f0820 & a5ae2f420 & a5b62af80 & a5b48c900 & a5b88ca20 + & & & & +    xxxxx * logistics software * & * video surveillance * & * computer security * & * wireless * & * enterprise software * + a7aded0e0 & a7b3b8f60 & a7b145970 & a7b5417c0 & a7b361450 + & & & & +      the use of ad hoc relevance judgments for evaluation of document clusters is motivated by the cluster hypothesis @xcite .",
    "it states that relevant documents tend to be more similar to each other than non - relevant documents for a given query .",
    "the cluster hypothesis connects ad hoc information retrieval and document clustering .",
    "documents in the same cluster behave similarly with respect to the same information need @xcite .",
    "it is due to the clustering algorithm grouping documents with similar vocabularies together .",
    "there may also be some higher order correlation due to effects caused by the distributional hypothesis @xcite and limitation of the analysis to the size of the document .",
    "we have evaluated document clustering solutions by creating plots for the optimal ordering of clusters for ad hoc queries .",
    "this ordering is created by an _ oracle collection selection _ ranking that has full knowledge of relevant documents .",
    "clusters are ordered by the number of relevant documents they contain .",
    "cumulative recall is calculated by traversing the cluster ranking in descending order .",
    "then the percentage of recall and documents visited is averaged across all queries .",
    "it represents the optimal ordering of the given clusters for a given query and represents an upper bound on recall ranking performance for the given set of clusters .",
    "relevant documents group together in clusters when the cluster hypothesis holds .",
    "additionally , this grouping is better than expected from randomly distributing documents into clusters of the same size . in summary",
    ", we assign to any clustering solution the performance of the oracle in collection selection .",
    "the better the clustering solution , the better the oracle performs in collection selection .",
    "fewer clusters have to be searched to recall relevant documents .",
    "there are no baselines available for clustering the clueweb collections into the order of a million clusters .",
    "therefore , we compare our clustering solutions against a random partitioning baseline .",
    "a common problem when comparing different clustering solutions is that , by nature , different clustering solutions are produced by different methods .",
    "it can introduce structural bias to the evaluation .",
    "unequal cluster sizes can lead to different performance measurements . in order to compare",
    "any given clustering solution against an equivalent random partitioning , we impose the same cluster structure on the random solution .",
    "we assign documents randomly to the same cluster structure obtained from clustering . in this manner",
    ", there is no structural bias to either solution , random or derived through clustering @xcite .",
    "it eliminates advantages of ineffective clustering solutions .",
    "for example , if almost all documents are placed in a single cluster except for one document being placed in every other cluster , then the large cluster will almost always contain all the relevant documents to any query .",
    "an evaluation that tests to see if collection selection discovers the correct cluster to search concludes that the dysfunctional solution is indeed the best  it almost always finds the right cluster . on the other hand , if the same ineffective solution is compared to a random baseline having the same dysfunctional structure then the clustering solution it is found to offer no improvement .",
    "the performance of different clustering solutions is normalized against the performance of a random baseline solution having an identical clustering structure .",
    "for the clueweb09 collection , we have evaluated three different clustering solutions .",
    "the first approach is by kulkarni and callan @xcite using kl - divergence and k - means and the other two by the em - tree algorithm .",
    "first we describe the approaches , and then compare their quality .",
    "kulkarni and callan describe an experiment with clueweb09 where a 500,000 document sample of the clueweb09 collection was clustered using k - means with a kl - divergence similarity measure to produce 1000 clusters .",
    "it maps the 500 million documents onto these clusters .",
    "we have chosen this analysis for a direct comparison with em - tree , even though it does not represent fine - grained clustering .",
    "however , it allows us to perform a side by side comparison with a published baseline .",
    "we processed the entire clueweb with an em - tree containing two levels of tree nodes of order @xmath30 .",
    "the first level contained 1000 clusters and the second 691,708 clusters due to pruning .",
    "the first level of the em - tree and the method of kulkarni and callan using k - means both produced 1000 clusters .",
    "therefore , these clustering solutions are directly comparable .",
    "we also compare solution quality of fine grained clusters , using the second level of the em - tree .",
    "we demonstrate that using many more fine - grained clusters can produce higher quality clusters with respect to ad hoc relevance .",
    "furthermore , the em - tree can produce higher quality clusters with respect to spam , even at 1000 clusters .",
    "direct comparison kl - divergence k - means approach is not possible for 691,708 clusters .",
    "it does not scale to produce this many clusters .",
    "we generate cumulative recall plots for the three methods using the oracle collection selection approach .",
    "each of the 3 approaches appear in separate plots in figures [ fig : kulkarni_callan_1000 ] , [ fig : emtree_1000 ] , and [ fig : emtree_691708 ] .",
    "we place most relevant documents first using the oracle ranking process .",
    "we visit clusters in descending order of recall .",
    "the average percentage of total documents in the collection contained in the first @xmath2 clusters is display along the x - axis .",
    "the y - axis is the average percentage of recall included in the first @xmath2 clusters when averaged over all topics . visiting a cluster produces a mark on the curve .",
    "so , after seeing 2 marks , 2 clusters have been visited in the order specified by the oracle ranking .",
    "each clustering has a unique random baseline created where the cluster size distribution matches that of each clustering solution . in all cases ,",
    "there is a difference between the random baseline and the clustering solution .",
    "useful learning has occurred .",
    "the baseline normalization removes any effect of random chance or ineffective cluster size distributions .",
    "therefore , all three clustering solutions support the cluster hypothesis that relevant documents tend to cluster together .    in comparison to the 1000 clusters produced by the first level of em - tree in figure [ fig : emtree_1000 ]",
    ", the kl - divergence k - means approach in figure [ fig : kulkarni_callan_1000 ] clearly groups relevant documents better .",
    "it achieves total recall after 2.81% of the collection is visited , whereas the em - tree approach does not achieve this until 4.19% .",
    "figure [ fig : all ] highlights difference between the two approaches .",
    "all three approaches are plotted on one graph without their baselines .",
    "we note that this is not a surprising result ",
    "k - means is superior to em - tree in terms of quantization distortion rates .",
    "however , this is a trade off as it is also much more computationally intensive .",
    "if a particular application requires 1000 clusters then , the kl - divergence with k - means approach is clearly better at grouping relevant documents together .",
    "a 500,000 document sample provides adequate statistics for learning 1000 clusters .",
    "however , suppose that fine granularity clustering is sought after and applying k - means recursive clustering to the entire collection is necessary .",
    "we clustered 500,000 documents into 1000 clusters using a sparse vector approach using k - means , and it took approximately ten hours using a very fast implementation .",
    "if each of these 1000 clusters are clustered into another 1000 clusters using the recursive k - means approach , then each subset requires another 500,000 document sample to learn another 1000 subclusters .",
    "as there are 1000 partitions , taking ten hours per partition , it would take another 10,000 hours to produce a similar number of clusters as em - tree , making this k - means based approach infeasible .",
    "em - tree can produce many more clusters in the second level that group relevant documents much more tightly . in figures [ fig : all ] and [ fig : all_zoom ] , the second level of em - tree reaches total recall after 0.06% of the collection has been visited . because these clusters still group relevant documents together , and support the cluster hypothesis , we also conclude they are highly topical .",
    "relevant documents for a given query are related to the same topic because they satisfy the same information need .",
    "many applications can benefit from identifying small clusters of highly topical documents .",
    "this is discussed further in section [ sec : applications ] .",
    "the random baseline for the second level of em - tree looks surprisingly effective .",
    "it achieves total recall after only visiting 0.22% of the collection .",
    "it is not unexpected because this baseline is close to the worst case possible , where each relevant document for a given query appears in a separate cluster .",
    "there are on average 80 relevant documents per query for the 148 queries .",
    "so the oracle returns an average of 80 clusters per query to achieve 100% recall .",
    "indeed , the first cluster in the random baseline contains 4.3% or 3.44 relevant documents on average .",
    "it decreases to below the average of one document per cluster by the 15th cluster .",
    "some clusters still contain more than one relevant document on average because the cluster size distribution in the random baseline matches that of clustering .",
    "large clusters can receive more relevant documents due to their size .",
    "in contrast , the first em - tree cluster in the second level contains 25% or 20 relevant documents on average .",
    "additionally , the first em - tree cluster contains 450 documents on average , whereas the first random baseline cluster contains 30,000 documents on average .",
    "we calculate precision by dividing the number of relevant documents by the number of documents in the cluster .",
    "the first em - tree cluster has a precision of 4.44% on average , whereas the first random baseline cluster has a precision of 0.011% on average .",
    "it is some 400 times worse .",
    "furthermore , clusters in the random baseline contain a mixture of relevant documents with mostly random off topic documents .",
    "it makes it much harder for a non - oracle ranker to find them using vocabulary statistics .",
    "there are on average 3.4 documents mixed with 30,000 random other off - topic documents in the first cluster in the random baseline .",
    "the plots of em - tree level 1 and 2 in figures [ fig : emtree_1000 ] and [ fig : emtree_691708 ] look very similar when the x - axis scale is ignored .",
    "it indicates that the same relationships between the random baseline and the clustering exist in both levels of the em - tree .",
    "while the same relationships exist , the 2nd level contains 691 times more clusters .",
    "the cluster hypothesis holds in the same way , even though the size of the clusters is reduced by almost three orders of magnitude .",
    "it verifies that there is no significant loss of fidelity in fine granularity clustering .",
    "we conducted the same experiments on clueweb12 .",
    "we were unable to compare the kl - divergence approach @xcite as clusters have not been made available for this recent collection .",
    "however , the same trends for the em - tree emerge as shown in figure [ fig : all_clueweb12 ] . all the corresponding plots were highly similar but were left out for brevity .",
    "the relevance judgments from the 2013 trec web track have much deeper pooling , increasing our confidence in the results .",
    "we have performed another evaluation using external knowledge in the form of spam scores .",
    "these spam scores have been produced by an approach by cormack et .",
    ". @xcite using a supervised learning approach that combines training from 4 different labeled spam datasets .",
    "it quantizes the spam scores into 100 values .",
    "a score of 99 is the least spammy and 0 the most .",
    "for instance , a document score of 99 indicates that 99% of the collection is more spammy than the said document .",
    "these spam scores have shown to be useful for improving search quality in ad hoc retrieval @xcite .",
    "if a perfect document clustering exists with respect to spam , it places documents with the same spam score in the same cluster .",
    "we have evaluated clusters by taking the average spam score for documents in the cluster .",
    "clusters are sorted and traversed by descending spam score .",
    "we then observe the percentage of the documents contained in the clusters visited so far .",
    "again , we create a random baseline for each clustering solution .",
    "as expected , almost all clusters average out to a spam score of 50 .",
    "we display the best possible oracle solution by ordering documents according to their spam score ; i.e. all documents with a score of 99 , then 98 and so on .",
    "it is a straight line where the first ranked cluster contains all documents with a spam score of 99 , then those with a spam score of 98 , and so forth .",
    "therefore , the closer an actual clustering solution comes to this straight line from the top left to the bottom right , the better the clustering is with respect to the spam scores .",
    "we evaluated the same clustering solutions as in section [ sec : relevance ] .",
    "these are kl - divergence k - means @xcite and the two clusterings produced by level 1 and 2 of the em - tree . for the clueweb09 collection",
    ", the results of this experiment can be seen in figure [ fig : spam_clueweb09_emtree_both ] .",
    "each of the clusterings have been plotted separately with random baselines and optimal document ordering in figures [ fig : spam_clueweb09_kulkarni_1000 ] , [ fig : spam_clueweb09_emtree_1000 ] and [ fig : spam_clueweb09_emtree_691708 ] .    the signature - based approach with em - tree better groups documents according to spam score than the kl - divergence k - means approach when producing 1000 clusters .",
    "it is the opposite result to using ad hoc relevance judgments where kl - divergence k - means was superior .",
    "the finer grained clustering in the 2nd level of em - tree allows more pure clusters with respect to spam .    for clueweb12",
    ", we have only created the final plot comparing all approaches for brevity in figure [ fig : spam_clueweb12_emtree_both ] .",
    "we omitted the random baselines because they behave in exactly the same manner .",
    "the plots look very similar for clueweb12 and clueweb09 .",
    "recent results in computer vision demonstrate the usefulness of scalable algorithms for unsupervised feature learning @xcite .",
    "the em - tree algorithm using signatures could be advantageous in computer vision applications where the generation of many clusters from large amounts of data is desirable .",
    "near duplicate object detection is another application for large scale fine - grained clustering , particularly for eliminating near - duplicate web pages .",
    "in fact , we found many near duplicate pages in clueweb 2012 .",
    "most existing cluster based re - ranking and query expansion approaches use fine - grained clusters of the top k results returned by a search engine .",
    "it limits the analysis to documents returned by the search engine , and usually it is based solely on the keywords in the query . with fine - grained clusters of web - scale collections made available by the em - tree algorithm , re - ranking and query expansion of documents that do not contain the keywords becomes a possibility .",
    "common cluster membership associates documents based on other keywords that appear in relevant documents .",
    "such documents do not necessarily contain the query keywords .",
    "another area of information retrieval , collection selection , is apt for the application of our approach .",
    "kulkarni and callan @xcite have demonstrated the effectiveness of selective search using collection selection and 1000 clusters of clueweb09 .",
    "it only visits the first few clusters per query .",
    "our results indicate that relevant results cluster in the same way , even when there are almost three orders of magnitude more clusters .",
    "the clustering solutions produced by em - tree may be useful for scaling classification applications .",
    "nearby points contained in a few fine - grained clusters can be used to build a classifier .",
    "alternatively , clusterings can be used for classification directly .",
    "sub - document clustering is now a possibility for large - scale document collections where splitting documents into fragments creates even more objects to cluster .",
    "it makes sense to split larger documents since such documents probably contain multiple sub - topics .",
    "however , splitting documents in web - scale collections certainly push it well beyond the capability of standard commodity hardware .",
    "however , with em - tree it may be possible to consider such clustering of document fragments .",
    "in this paper , we presented solutions for two major problems in web - scale document clustering  scalable and efficient document clustering and evaluation of cluster validity where categorical labeling of collections is unavailable and unfeasible .",
    "the proposed em - tree algorithm can cluster hundreds of millions of documents into hundreds of thousands of clusters on a single 16 cpu core machine in under 24 hours .",
    "it is standard hardware available to organizations of all sizes . to the best of our knowledge ,",
    "clustering on a single machine at this scale has not been reported in the literature . to the contrary",
    ", the few published attempts at this scale have used high - performance computing resources , well beyond reach of most organizations .",
    "these attempts were also on lower dimensional non - document data . for document clustering",
    ", this is far beyond any examples we have been able to find .",
    "the largest scale approach we could find sampled 500,000 documents to produce 1,000 clusters .",
    "it then maps the 500 million documents onto these clusters @xcite .",
    "the closest comparison we could find is a method that clusters the same order of magnitude of images into a similar order of magnitude of clusters using 2000 cpus @xcite .",
    "however , this approach does not cluster documents that are sparse and high dimensional .",
    "another approach parallelized k - means using 16,000 cpus on a super computer to produce 1,000 clusters of 1 billion much lower dimensional weather data @xcite .",
    "we presented two novel evaluations using ad hoc relevance and spam classifications to assess the validity of clusters where no category labels are available .",
    "this evaluation demonstrated that the fine - grained clustering created by em - tree led to higher quality clusters .",
    "additionally , we expect this approach to clustering to be applicable to many different types of web scale data emerging from sources such as images , sound recordings , sensors , positioning systems , genome research , marketing data and many more fields .",
    "d.  arthur and s.  vassilvitskii .",
    "k - means++ : the advantages of careful seeding . in _ proceedings of the eighteenth annual acm - siam symposium on discrete algorithms _ , pages 10271035 .",
    "society for industrial and applied mathematics , 2007 .",
    "a.  broder , l.  garcia - pueyo , v.  josifovski , s.  vassilvitskii , and s.  venkatesan .",
    "scalable k - means by ranked retrieval . in _ proceedings of the 7th acm international conference on web search and data mining _ , pages 233242 .",
    "acm , 2014 .",
    "de  vries and s.  geva .",
    "k - tree : large scale document clustering . in _ proceedings of the 32nd international acm sigir conference on research and development in information retrieval _ ,",
    "pages 718719 .",
    "acm , 2009 .",
    "xin - jing wang , lei zhang , and ce  liu . duplicate discovery on 2 billion internet images . in _",
    "computer vision and pattern recognition workshops ( cvprw ) , 2013 ieee conference on _ , pages 429436 .",
    "ieee , 2013 .",
    "ren wu , bin zhang , and meichun hsu .",
    "clustering billions of data points using gpus . in _ proceedings of the combined workshops on unconventional high performance computing workshop plus memory access workshop _ ,",
    "pages 16 .",
    "acm , 2009 ."
  ],
  "abstract_text": [
    "<S> the proliferation of the web presents an unsolved problem of automatically analyzing billions of pages of natural language . we introduce a scalable algorithm that clusters hundreds of millions of web pages into hundreds of thousands of clusters . </S>",
    "<S> it does this on a single mid - range machine using efficient algorithms and compressed document representations . </S>",
    "<S> it is applied to two web - scale crawls covering tens of terabytes . </S>",
    "<S> clueweb09 and clueweb12 contain 500 and 733 million web pages and were clustered into 500,000 to 700,000 clusters . to the best of our knowledge , </S>",
    "<S> such fine grained clustering has not been previously demonstrated . </S>",
    "<S> previous approaches clustered a sample that limits the maximum number of discoverable clusters . </S>",
    "<S> the proposed em - tree algorithm uses the entire collection in clustering and produces several orders of magnitude more clusters than the existing algorithms . </S>",
    "<S> fine grained clustering is necessary for meaningful clustering in massive collections where the number of distinct topics grows linearly with collection size . </S>",
    "<S> these fine - grained clusters show an improved cluster quality when assessed with two novel evaluations using ad hoc search relevance judgments and spam classifications for external validation . </S>",
    "<S> these evaluations solve the problem of assessing the quality of clusters where categorical labeling is unavailable and unfeasible .    </S>",
    "<S> [ clustering ] [ clustering , algorithms , similarity measures ] [ concurrent programming , parallel programming , distributed programming ] </S>"
  ]
}