{
  "article_text": [
    "the emergence of cloud computing has led to a proliferation of distributed storage solutions that offer high availability , reliability , and excellent performance .",
    "for example , open - source distributed object stores that offer these benefits include riak @xcite , swift @xcite , hdfs @xcite , cephfs @xcite and quantcast qfs @xcite .",
    "the main idea behind these systems is to replicate and spread the data uniformly across a cluster of servers , in order to increase availability and reliability and take advantage of data declustering ( in other words , process data in parallel across multiple servers ) .",
    "these systems make good use of the available cluster resources , but they are targeted towards specific query workloads . in particular , replication and declustering favor mapreduce - style processing , or any processing that can be easily parallelized . on the other hand ,",
    "declustering hurts the performance of query workloads that perform certain types of joins , the reason being that joins could result in a large volume of data migrations , which can saturate the network and reduce the performance of the underlying distributed data store .",
    "for these types of queries , careful placement of data around the cluster is critical for guaranteeing high availability , reliability and high query throughput .",
    "for example , in a relational data warehouse , queries include multi - way joins to combine fact tables with multiple dimension tables .",
    "materialized views are commonly used to pre - compute query results and must be maintained over time .",
    "thus , data placement in a distributed data warehouse is critical for good query performance and efficient view maintenance ; the latter is particularly important in on - line streaming warehouses @xcite , which are continuously updated ( as opposed to being taken down for refresh once a week or once a month ) and must keep up with the incoming data feeds .",
    "recent work such as cohadoop @xcite enables the co - location of related data , but places the burden on the user or the database administrator to determine what should be co - located and where . in this paper",
    ", we aim to automate this process by proposing algorithms for computing nearly - optimal _ data placement _",
    "strategies for a given workload .    in the simplest version of the problem",
    ", we are given a set of base tables , ad - hoc queries and servers with known capacities .",
    "the goal is to decide where to store the tables and where to evaluate the queries in order to minimize the data communication cost during query evaluation .",
    "we also address other issues that arise in our motivating applications , such as load balancing , replication , materialized views , and complex query plans consisting of sequences of intermediate results that may be computed on different servers .",
    "first , not surprisingly , we show that even the simplest formulation of our data placement problem is np - hard .",
    "second , and more surprisingly , we reduce the data placement problem without replication to graph partitioning . previous work on distributed data placement relied on _ hypergraph _ partitioning to represents different objectives such as minimizing the number of distributed transactions @xcite . as we will show , hypergraph partitioning fails to capture the data communication cost , but our reduction to graph partitioning can do so _ exactly_. we consider this reduction to be the main contribution of this paper .",
    "the reduction to graph partitioning is a desirable result .",
    "graph partitioning is an np - hard problem , but it has been studied extensively . in particular ,",
    "effective and efficient approximation algorithms and a variety of robust libraries are publicly available ( e.g. , metis @xcite and chaco @xcite ) . some problems involving hypergraph partitioning may be approximated by graph partitioning , as was done in @xcite ( see section  [ sec.related ] ) .",
    "on the other hand , by reducing our problem to graph partitioning , we can directly use tools such as metis to provide good approximate data placement plans very efficiently in practice .",
    "had we needed hypergraph partitioning , we would have had to use _ hypergraph _ partitioning software , which , by solving a more general problem , gives worse performance .",
    "third , we present two classes of algorithms for our problem : non - trivial integer program ( ip ) formulations that compute an optimal data placement using an ip solver , and practical approximation algorithms based on our reduction to graph partitioning .",
    "we also extend our algorithms to handle load balancing .",
    "fourth , we turn our attention to the version of the problem with replication and we present two heuristics that use the reduction to graph partitioning as a subroutine",
    ". we also present an ip formulation that can be used to compute an optimal solution .",
    "fifth , we present a detailed empirical evaluation of our algorithms using the tpc - ds decision support benchmark . for many parameter settings , it may take an ip solver weeks ( or longer ) to compute an optimal data placement , whereas our reduction produces nearly - optimal solutions in seconds .",
    "the remainder of the paper is organized as follows .",
    "section [ sec.definition ] formally defines the problem and notation used throughout the paper .",
    "section [ sec.related ] presents related work .",
    "section [ sec.solution ] presents our reduction to graph partitioning .",
    "section [ sec.experiments ] presents an empirical study .",
    "finally , section [ sec.conclusion ] concludes the paper .",
    "to simplify the presentation , first we define a simple version of the problem with queries and base tables .",
    "we will address extended versions , including load balancing and replication , in section  [ sec.solution ] .",
    "[ def.problem ] given    1 .",
    "@xmath0 tables @xmath1 , the @xmath2th table having a nonnegative integral size @xmath3 , 2 .",
    "@xmath4 queries @xmath5 , each referencing one or more tables , where @xmath6 , 3 .",
    "@xmath7 servers @xmath8 , the @xmath9th server having nonnegative integral storage capacity @xmath10 , and 4 .   for each @xmath11 and @xmath12 ,",
    "a nonnegative integral communication cost @xmath13 , which is the cost incurred for transferring whichever part of table @xmath14 is needed in order to evaluate query @xmath11 ( e.g. , after performing any local projections or selections ) ,    assign each table in @xmath15 to one of the @xmath7 servers in @xmath16 so as not to violate any capacities , while minimizing the communication cost paid to process all queries .",
    "placement _ of tables to servers be a mapping @xmath17 .",
    "a placement is _ legal _ if @xmath18 for @xmath19 , i.e. , the total size of all tables placed on server @xmath20 is no greater than @xmath10 .",
    "the cost of a legal placement is defined as follows .",
    "we allow any query to be processed on any server .",
    "the cost of processing query @xmath11 on server @xmath20 is the communication cost of shipping to @xmath20 all those ( required fragments of ) tables in @xmath11 which are not stored on @xmath20 , i.e. , @xmath21 we call this query processing model _ query - site execution_. since our goal is to minimize the overall communication cost , we will assume from now on that we choose the server on which to process @xmath11 so as to minimize the associated communication cost :    [ def.cost ] given a placement @xmath22 , the communication cost of processing query @xmath11 is @xmath23}.\\ ] ]    for example , suppose query @xmath24 joins three tables , @xmath25 , @xmath26 and @xmath27 . for simplicity",
    ", assume that @xmath28 , i.e. , @xmath24 requires equally - sized fragments of each of the three tables .",
    "if @xmath25 and @xmath26 are placed on @xmath29 and @xmath27 is placed on server @xmath30 , then we will evaluate @xmath24 on server @xmath29 rather than @xmath30 .",
    "the former has a communication cost of @xmath31 ( we need to ship this fragment of @xmath27 to @xmath29 ) whereas the latter has a communication cost of @xmath32 ( we need to ship the @xmath25 and @xmath26 fragments to @xmath30 ) .",
    "* note : * while our problem definition assumes that the objects to be stored are tables , we can easily extend it to the case of partitioned tables . in this case , each `` table '' is actually a part of a larger table , and , for instance , a distributed hash join can be modelled as a set of smaller queries that join the corresponding parts .",
    "how to partition the tables for a given workload is an orthogonal problem that has been addressed , e.g. , in @xcite .    given that the table sizes in our problem definition are arbitrary nonnegative integers , determining if there is _ any _ legal placement at all is np - hard , since data placement is at least as hard as partition , even when @xmath33 .",
    "( the proof is straightforward and appears in the appendix . )",
    "since the feasibility question is itself np - hard , there can be no polynomial - time approximation algorithm with any fixed ratio ( unless p@xmath34np ) .",
    "this means that in order to get good algorithms , assuming that p@xmath35np , we will have to allow , in the worst case , some overloading of the servers .",
    "given a set of tables , a set of servers and a set of queries , kayyoor et al .",
    "@xcite studied algorithms for deciding which tables to replicate and where to place those replicas .",
    "their objective is to minimize the average query span , which is the number of servers involved in answering a query .",
    "minimizing the average query span of a query workload is different from minimizing the communication cost for the same workload , as illustrated by the following example .",
    "assume we have three servers and one query associated with six tables , as shown in figure [ fig.kayyoor13data ] .",
    "as illustrated , tables @xmath25 through @xmath36 are larger than @xmath37 and @xmath38 .",
    "assume for simplicity that in order to evaluate the query we need to use all tuples contained in all six tables .",
    "two possible placements are shown in the figure , both of which give a query span of three since the tables needed by the query are spread out on three servers .",
    "however , the communication cost of placement 2 is more than double that of placement 1in both cases , it is best to execute the query on server 1 , but placement 2 additionally requires two large tables , @xmath27 and @xmath36 , to be shipped there . since the algorithm of kayyoor et al . can not differentiate between these two cases , it can not be used to solve our problem .",
    "the authors do present a generalization of the algorithm that considers table sizes , but only to guarantee that the capacities of the servers are not exceeded by the recommended placement .",
    "table sizes play no role in deciding a placement that minimizes the communication cost of the query workload .",
    "curino et al .",
    "@xcite present schism , which minimizes the number of distributed transactions in a given workload by assigning and replicating individual tuples to servers .",
    "the database and the workload are represented as a graph , whose nodes correspond to tuples and whose edges connect tuples accessed in the same transaction .",
    "graph partitioning algorithms are applied to find balanced parts , each part corresponding to one server , that minimize the weight of the cut edges . as in kayyoor",
    "@xcite , this technique does not differentiate between cases where the required tuples for a transaction are distributed across two or more than two servers .",
    "hence , it can not distinguish between the two data placement plans shown in figure [ fig.kayyoor13data ] and can not be used to solve our problem .",
    "in addition to database design , graph partitioning was used to solve data placement problems in parallel computing ; see , e.g. , @xcite . again",
    ", this work optimized for different objectives and can not be adapted to solve our problem .    a natural way of thinking about the data placement problem , which was considered by both kayyoor et al .",
    "@xcite and curino et al .",
    "@xcite , is to model the workload as a hypergraph , in which tables or tuples are represented as nodes , and queries or transactions are represented as hyperedges .",
    "each hyperedge is a set of nodes , corresponding to the tables associated with the particular query .",
    "interestingly , the optimization objectives of kayyoor et al .  and curino et al .  are captured exactly by hypergraph partitioning .",
    "for example , cutting a hyperedge means that the tuples needed by this particular transaction are placed on multiple servers ; therefore , the number of cut hyperedges exactly corresponds to the number of distributed transactions .",
    "however , since graph partitioning can be solved more accurately than the more general problem of hypergraph partitioning , kayyoor et al .  and curino et al .  provide reductions from hypergraph to graph partitioning , but these reductions are not exact . on the other hand , hypergraph partitioning can not capture data communication costs , because it is impossible to assign hyperedge weights appropriately and decide how to distribute a hyperedge weight when that edge is split across multiple servers .",
    "recall the example from section  [ sec.definition ] involving query @xmath24 . in the hypergraph representation",
    ", @xmath24 induces a hyperedge containing @xmath25 , @xmath26 and @xmath27 .",
    "clearly , the weight of this hyperedge , i.e. , the communication cost paid by @xmath24 , depends on the placement of the three tables and of @xmath24 itself .",
    "but it is the algorithm s job to determine this placement , so it is not possible to assign an accurate edge weight a priori .",
    "it is easy to show using adversarial counterexamples that , irrespective of how hyperedge weights are chosen , the data communication cost obtained by hypergraph partitioning can be arbitrarily worse than the optimal cost .",
    "it is surprising that while hypergraph partitioning fails to capture the data communication cost , we can provide an exact reduction of this problem to a standard graph partitioning instance .",
    "as we shall see in section  [ sec.solution ] , our graph construction is different from previous constructions which used hypergraphs , _ even in the special case when the hyperedges all have size two , i.e. , when the hypergraphs are graphs .",
    "_ rather than building a graph with tables as nodes and queries as edges , the trick will be to build a _",
    "graph with queries on one side and tables on the other .",
    "finally , we note that partitioning algorithms for relational databases have been studied extensively in the past , but the focus has been on data declustering and physical design tuning in order to speed up query evaluation by taking advantage of parallelization .",
    "partitioning strategies include range partitioning , hash partitioning , and partitioning based on query cost models @xcite .",
    "furthermore , modern distributed storage systems , such as bigtable @xcite , are not optimized for relational workloads on multiple tables .",
    "in addition , distributed key / value stores , such as amazon dynamo @xcite , hdfs @xcite , riak @xcite , and quantcast qfs @xcite focus primarily on randomly distributing redundant data ( either by replication or erasure coding ) with the main objective being to increase reliability and availability .",
    "recently , systems like cohadoop @xcite have been developed to take advantage of non - random data placement in order to speed up evaluation of certain classes of queries .",
    "nevertheless , these systems focus only on technical issues and leave the responsibility of choosing the placement of data to the database administrator .",
    "our work automates this process .",
    "first , we present an _ exact _",
    "reduction from the data placement problem in definition  1 to graph partitioning ( section  [ sec.reduction ] ) .",
    "then we present an ip formulation that can be used with any ip solver to produce an optimal solution ( section  4.1.1 ) .",
    "next , we generalize our definition to include arbitrary query execution plans with materialized views and intermediate results , and we present a generalized reduction to graph partitioning ( section  [ sec.materialized_views ] ) .",
    "section  4.3 discusses load balancing .",
    "finally , we discuss how to handle replication in section  4.4 .",
    "we give an ip formulation for finding an optimal placement when a fixed number of replicas per table is desired , and we discuss two heuristics that use our reduction to find good placements efficiently .      our version of graph partitioning is defined as follows .",
    "given a node- and edge - weighted graph @xmath39 , and a sequence of @xmath7 nonnegative capacities , partition @xmath40 into @xmath7 parts such that the total weight of the nodes in the @xmath9th part is at most the @xmath9th capacity , so as to minimize the sum of the weights of the ( cut ) edges whose endpoints are in different parts .    throughout this paper ,",
    "the term `` partition '' will always refer to what is known in the literature as an _ ordered partition _ , i.e. , a sequence of disjoint subsets whose union is the universe .",
    "equivalently , an ordered partition of @xmath40 into @xmath7 parts is just a mapping @xmath41 .    more formally , given a graph @xmath39 with edge @xmath42 having weight @xmath43 , node @xmath44 having weight @xmath45 ( the distinction between nodes and edges keeping the notation unambiguous ) , and a sequence @xmath46 of @xmath7 nonnegative integers , let a mapping @xmath47 ( which defines an ordered @xmath7-part partition ) be _ legal _ if @xmath48 , for @xmath19 ; then the goal is to find a legal @xmath49 so as to minimize @xmath50.\\ ] ]    the construction of the graph used for graph partitioning to solve data placement is as follows .",
    "we construct a _ bipartite _",
    "graph in which there is a node for each query on the left side and a node for each table on the right side of the graph .",
    "there is an edge between a query @xmath11 and a table @xmath14 iff @xmath12 .",
    "the weight of an edge is equal to @xmath13 .",
    "the main benefit of this construction comes from the fact that we use , in addition to nodes for tables , a separate node for each query , unlike in kayyoor et al .  and",
    "curino et al .  where there are nodes _ only _ for tables / tuples .",
    "the crucial benefits of our construction vis - a - vis previous papers on data placement are that it preserves the optimal cost _ exactly _ and that it generates _ graphs _ , not hypergraphs .",
    "having hyperedges of arbitrarily large size , hypergraphs are far more general than regular graphs ( whose edges all have size 2 ) , making hypergraph partitioning a far more general , and therefore much more difficult , problem to approximately solve than graph partitioning .",
    "an example of our construction is shown in figure [ fig.bipartite ] , in which the query workload consists of four queries , involving six tables .",
    "the queries are @xmath51 , @xmath52 , @xmath53 , @xmath54 .",
    "tables @xmath55 have size @xmath56 .",
    "tables @xmath57 have size @xmath58 , and each server has capacity @xmath59 . for simplicity , in some subsequent examples",
    "we assume that @xmath60 for all @xmath61 ; however , in practice , it is usually the case that @xmath62 since data - reducing operations such as projections and selections can easily be done locally .     for all @xmath63 . with the given placement , @xmath24",
    "will be executed on server @xmath56 , @xmath64 on server @xmath65 , @xmath66 on server @xmath58 and @xmath67 on server @xmath56.,width=240 ]    [ thm.data_placement ] there is a ( simple ) polynomial - time transformation that takes an instance @xmath68 of data placement and produces an instance @xmath69 of graph partitioning such that ( 1 ) given any feasible solution to @xmath68 , there is a feasible solution to @xmath69 of no greater cost , and ( 2 ) given any feasible solution to @xmath69 , there is a feasible solution to @xmath68 of no greater cost .",
    "furthermore , there are ( trivial ) polynomial - time algorithms which convert between the specified solutions for instances @xmath68 and @xmath69 .",
    "take the given instance @xmath68 of data placement and build an instance @xmath69 of graph partitioning as follows . the graph @xmath39 where @xmath70 .",
    "there is an edge of @xmath71 from @xmath11 to @xmath14 iff @xmath72 ; the weight of this edge is @xmath13 .",
    "the weight @xmath73 of @xmath74 is @xmath3 ; the weight @xmath75 of @xmath76 is 0 .",
    "the @xmath9th capacity is @xmath10 .",
    "this completes the description of @xmath69 .",
    "first , given a feasible solution @xmath22 for @xmath68 , we show how to construct a feasible solution @xmath49 for @xmath69 of no greater cost . given a feasible solution @xmath22 for @xmath68",
    ", we just define @xmath77 for all @xmath78 and define @xmath79 where @xmath80 we want to show first that @xmath49 defines a feasible solution for @xmath69 whose cost is at most the cost of @xmath22 on @xmath68 .    we know that for all @xmath9 , @xmath81 since @xmath82 for all @xmath83 , for all @xmath9 we have @xmath84 therefore the partition is legal and",
    "hence @xmath49 defines a feasible solution for @xmath69 .",
    "the cost of the partition is @xmath85\\ ] ] @xmath86\\ ] ] @xmath87\\ ] ] @xmath88\\ ] ] @xmath89\\ ] ] @xmath90\\ ] ] @xmath91 second , given a feasible solution @xmath49 for @xmath69 , we show how to construct a feasible solution @xmath22 for @xmath68 of no greater cost . given any feasible solution @xmath49 for @xmath69",
    ", we may assume that for all @xmath83 , @xmath92 where @xmath9 is chosen to maximize @xmath93 for modifying @xmath49 in order to attain this property for all @xmath83 can not increase the cost .",
    "now we just define @xmath94 for all @xmath2 .",
    "we must show that @xmath22 defines a feasible solution to @xmath68 whose cost is at most the cost of @xmath49 on @xmath69 .",
    "because @xmath49 is feasible for @xmath69 , we have , for all @xmath9 , @xmath95 therefore the placement for @xmath68 is legal .",
    "the cost of solution @xmath22 for @xmath68 is @xmath96\\right ] \\ ] ] @xmath97\\right ] \\ ] ] @xmath98    it is easy to extend the construction of the bipartite graph to handle query frequencies while computing the communication cost .",
    "let @xmath99 denote the frequency of query @xmath11 . given a placement @xmath22 ,",
    "the communication cost of processing @xmath11 with frequency @xmath100 is @xmath101}.\\ ] ] to change the construction , we need only to update the weight of the edge from query @xmath11 to table @xmath14 from @xmath13 to @xmath102 .",
    "building an ip for data placement can be done as follows .",
    "the ip has a boolean variable @xmath103 if @xmath104 is either a query or a table and @xmath20 is a server , the meaning of which is that @xmath105 if query or table @xmath104 is stored on server @xmath20 and 0 otherwise . for each query or table",
    "@xmath104 , we add the constraint @xmath106 , meaning that every query or table is assigned to exactly one server . for each server",
    "@xmath20 , we have a constraint preventing @xmath20 from being overloaded , namely , @xmath107 , where @xmath108 is ( a ) @xmath109 if @xmath104 is a query or ( b ) equal to the size of the table if @xmath104 is a table . here",
    "@xmath10 is the capacity of @xmath20 .",
    "this completes the description of constraints .    as for the objective function , it is the sum , over @xmath61 such that @xmath110 , of @xmath13 times [ @xmath58 if @xmath11 and @xmath14 are stored on different servers and 0 otherwise ] .",
    "more formally : @xmath111 in which @xmath112 , which equals 0 if and only if both @xmath11 and @xmath14 are stored on server @xmath20 and 1 otherwise , as desired .      definition 1 explicitly distinguishes between base tables and queries . in practice",
    ", frequently - used queries are materialized to improve performance ; hence some queries can also act as tables .",
    "furthermore , complex queries ( e.g. , ones that contain several join , aggregation , selection , and projection predicates ) can be decomposed into several steps .",
    "this gives rise to complex distributed query plans , which may be represented by directed acyclic graphs , in which the intermediate results of one step are pipelined to the next step ( or materialized in a _ temporary table _ and fed to the subsequent step ) . in a distributed setting , each node of the dag can be evaluated on a different server , where intermediate results need to be shipped to the server(s ) responsible for executing subsequent steps , but need not be stored permanently .",
    "query execution plans can be determined by a query optimizer using cost - based and rule - based optimization strategies , so as to minimize the cost of evaluating a query as well as minimizing the amount of intermediate data produced @xcite .",
    "given arbitrary query plans , we want to find the best data placement strategy in order to minimize the total data communication cost required to evaluate all such plans .",
    "fortunately , generalizing our reduction is simple .",
    "the trick is to view tables , queries , intermediate nodes of query execution dags and materialized views _ all _ as views . in the new problem ,",
    "any view @xmath118 can depend on any other @xmath119 .",
    "( earlier , queries depended only on tables , tables depended on nothing , and nothing depended on queries . ) in fact , we will allow each view to be computed on one server and stored possibly on a different one ( however , it is easy to force each view to be computed and stored on one server if so desired ) .",
    "we assume that all servers have enough overflow capacity to temporarily store all intermediate results and query results needed .",
    "we define the generalized data placement problem as follows .",
    "( generalized data placement ) ( or gdp ) . given    1 .",
    "@xmath0 views @xmath120 , the @xmath2th having a nonnegative integral size @xmath3 , 2 .   a set @xmath121 of @xmath7 storage servers , the @xmath9th having nonnegative integral storage capacity @xmath10 , 3 .   a nonnegative integral _ transfer cost",
    "@xmath122 for each view @xmath119 , 4 .   a directed acyclic graph @xmath123 representing the union of execution plans of all views ( an arc @xmath124 meaning that view @xmath118 needs view @xmath119 ) , 5 .   for each @xmath61",
    "such that @xmath125 , a nonnegative integral communication cost @xmath13 ( which is the cost incurred for transferring whatever part of view @xmath119 is needed in order to evaluate view @xmath118 ) ,    find a _ computation server _ @xmath126 and a _ storage server _",
    "@xmath127 for each @xmath119 , to minimize the total communication cost , which is defined to be @xmath128 while satisfying the property that for all @xmath9 , @xmath129    the communication cost includes processing each @xmath118 on its computation server and transferring each @xmath118 from its computation server to its storage server , if necessary .",
    "note that we are assuming , as mentioned above , that @xmath118 can be computed on its computation server in scratch space on that server , since we are not including the size of @xmath118 in the space needed on its computation server .",
    "now we show that a simple generalization of the graph construction for the case of tables and queries works for gdp .",
    "specifically , build a node- and edge - weighted bipartite graph @xmath130 where @xmath131 is a copy of @xmath40 and @xmath132 .",
    "the weight of an edge @xmath133 with @xmath134 is @xmath13 ; the weight of an edge @xmath135 is @xmath136 .",
    "the weight of node @xmath137 is @xmath109 ; the weight of @xmath119 is @xmath3 .",
    "we seek a minimum - edge - weight partition of @xmath138 into @xmath9 parts such that the total node weight of the @xmath9th part is at most @xmath10 .    if @xmath119 is a query , set @xmath139 and @xmath140 to represent the fact that queries are not stored ( effectively , they can be stored for free anywhere ) . if @xmath119 is a base table , set @xmath140 to force the graph partitioning algorithm to not cut the edge @xmath141 and hence to have @xmath142 .",
    "if @xmath119 is a materialized view ( i.e. , a node with both in - arcs and out - arcs which is to be stored ) , set @xmath143 to represent the size of the computed view . if @xmath119 is an intermediate result ( i.e. , a node with both in - arcs and out - arcs which is to be computed but not stored permanently ) , set @xmath139 but let @xmath122 equal the size of the computed intermediate query result .",
    "this construction is a generalization of the previous graph construction for for data placement in the sense that when every view is a table ( i.e. , depends on nothing ) or a query ( i.e. , upon which nothing depends ) , in @xmath138 , nodes @xmath11 on the left side of the bipartite graph and nodes @xmath144 on the right side are adjacent ( only ) to @xmath145 and @xmath14 , respectively , by infinite - weight edges , and hence in any finite - edge - cost partition will be combined together .",
    "once those nodes are combined , @xmath11 with @xmath145 and @xmath14 with @xmath144 , what remains is exactly the weighted graph constructed earlier .",
    "here we give a concrete example to show how one would use the reduction specified above to solve an instance of gdp .",
    "suppose there are seven views @xmath146 , and that the dag @xmath147 is as shown in figure [ fig.gdp_example ] .",
    "nodes @xmath148 are base tables ; node @xmath149 is a query ; node @xmath150 is an intermediate result ; and nodes @xmath151 and @xmath152 are materialized views . because @xmath149 and @xmath150 will not be stored , we set @xmath153 . for the other nodes ,",
    "we use the sizes of the views , e.g. , @xmath154 .",
    "now we discuss the @xmath122 s . since a table should not be moved , we set @xmath140 for @xmath148 .",
    "because an intermediate result is not stored or moved , we set @xmath140 also for @xmath150 .",
    "the remaining @xmath122 s are the costs of transferring the views , hence @xmath155 .    for simplicity ,",
    "let us set @xmath13 to be the size of view @xmath119 .",
    "in other words , whenever view @xmath118 depends on view @xmath119 , we must transfer all of @xmath119 from the storage server of @xmath119 to the computation server of @xmath118 .",
    "unless @xmath139 , which signifies that @xmath119 is not to be stored , @xmath156 will just equal @xmath3 .",
    "this is the case for @xmath157 . in our example , @xmath158 .",
    "let us take @xmath159 for all @xmath83 .",
    "we do not need @xmath160 .",
    "suppose there are @xmath33 servers @xmath161 with capacities @xmath162 .",
    "we now build a bipartite graph with left - hand - side nodes @xmath163 and right - hand - side nodes @xmath164 and the following edges :    * @xmath165 , @xmath166 , @xmath167 , and @xmath168 , all of weight @xmath169 , @xmath170 of weight 10 , @xmath171 of weight 7 , and @xmath172 of weight 0 ; * @xmath173 of weight 8 , @xmath174 of weight 5 , @xmath175 of weight 8 , @xmath176 of weight 4 , @xmath177 of weight 5 , @xmath178 of weight 4 , @xmath179 of weight 8 , @xmath180 of weight 10 , and @xmath181 of weight 7 .",
    "a left - hand - side node @xmath119 has weight @xmath3 ; a right - hand - side node @xmath137 has weight 0 . a picture of this bipartite graph appears in figure [ fig.gdp_example_2 ] .",
    ".,width=172 ]    the goal is to find a partition of the vertex set into @xmath33 parts each of total node weight at most 18 , so as to minimize the cost of the cut edges .",
    "for example , one feasible solution is to put @xmath182 and @xmath183 on server @xmath29 and the remaining nodes ( @xmath184 ) on server @xmath30 . in other words , @xmath185 are computed and stored on server @xmath29 ; @xmath186 are computed and stored on server @xmath30 ; @xmath151 is computed on server @xmath29 and stored on server @xmath30 ; and @xmath152 is computed on server @xmath30 and stored on server @xmath29 .",
    "the edges cut by this partition are :    * @xmath175 , of cost 8 , representing the cost of moving @xmath187 from its storage server @xmath30 to @xmath151 s computation server @xmath29 ; * @xmath174 , of cost 5 , representing the cost of moving @xmath188 from its storage server @xmath29 to @xmath150 s computation server @xmath30 ; * @xmath177 , of cost 5 , representing the cost of moving @xmath188 from its storage server @xmath29 to @xmath152 s computation server @xmath30 ; * @xmath178 , of cost 4 , representing the cost of moving @xmath189 from its storage server @xmath29 to @xmath152 s computation server @xmath30 ; * @xmath170 , of cost 10 , representing the cost of 8 to move @xmath151 from its computation server @xmath29 to its storage server @xmath30 ; * @xmath171 , of cost 7 , representing the cost of 6 to move @xmath152 from its computation server @xmath30 to its storage server @xmath29 ; and * @xmath181 , of cost 7 , representing the cost of moving @xmath152 from its storage server @xmath29 to @xmath149 s computation server @xmath30 .      now we state and prove the key property of the new graph construction . within the theorem and its proof , a `` partition '' refers to an ( ordered ) partition into @xmath7 parts whose @xmath9th part has node weight at most @xmath10 . below the theorem and its proof , we give an example of how the theorem would be used .    [",
    "theorem : gdp ] ( 1 ) given any feasible solution to gdp , there is a partition of @xmath138 of edge cost no greater than the cost of the feasible solution , and ( 2 ) given any partition of @xmath138 , there is a feasible solution to gdp of cost at most the edge cost of the partition . in particular",
    ", the optimal value of gdp equals the minimum edge cost of a partition of @xmath138 .",
    "\\(1 ) take any feasible solution to gdp . for all @xmath9",
    ", we have @xmath190 the cost of this solution is @xmath191    define the natural partition @xmath192 of @xmath193 by @xmath194 .",
    "the node weight of @xmath195 is @xmath196 , which we know is at most @xmath10 .",
    "let @xmath197 , for @xmath198 , denote @xmath195 such that @xmath199 .",
    "the edge cost of the graph partition is @xmath200 @xmath201 which is exactly the cost of the gdp solution , so the proof of ( 1 ) is complete .    for ( 2 ) ,",
    "take any partition @xmath192 of @xmath138 .",
    "we have , for all @xmath9 , @xmath202 .",
    "now define a solution to gdp : for all @xmath2 , let @xmath203 , where @xmath204 , and let @xmath205 , where @xmath206 .",
    "we will prove that this solution is feasible for gdp and that its cost equals the edge cost of the graph partition .",
    "we have @xmath207 .",
    "furthermore , the cost of the defined solution to gdp is @xmath208 which equals @xmath209 this is exactly the edge cost of the graph partition .",
    "note that for any @xmath118 , we can ensure that @xmath210 by setting @xmath211 .",
    "this ensures that in any finite - edge - cost partition , the edge @xmath135 is not cut , which implies that @xmath210 .",
    "therefore , if required , materialized views can be required to be computed and stored on the same server .",
    "the integer program for gdp is similar to that for data placement and is omitted due to space constraints .",
    "the graph partitioning algorithm places data optimally across servers but does not guarantee that queries are also evenly distributed across servers .",
    "it is plausible for an optimal solution to assign the execution of a large percentage of queries to only a small fraction of servers .",
    "fortunately , there is a straightforward way of distributing load across servers .",
    "the idea is to assign to each node of the bipartite graph a 2-dimensional weight @xmath212 and to assign each server a 2-dimensional capacity .",
    "( metis allows 2-dimensional weights and 2-dimensional capacities , which must be satisfied componentwise ) . as before",
    ", @xmath213 is the size of the node .",
    "furthermore , @xmath214 corresponds to the estimated execution cost for all nodes on the left - hand side of the graph that corresponds to queries , materialized views , temporary tables and intermediate results , and @xmath109 for the rest of the nodes in the graph .",
    "each server is assigned a 2-dimensional capacity @xmath215 , in which @xmath216 is the total available storage of server @xmath217 as before , and @xmath218 corresponds to the available execution capacity .",
    "then , once again we can use existing graph partitioning algorithms and libraries ( e.g. , metis @xcite ) to partition the graph , by optimizing both storage and load balancing constraints simultaneously      replication is important in many application settings for fault tolerance and load balancing .",
    "existing systems consider replication of tables in which a predetermined number of replicas for each table is desired ( e.g. , the common three - replica scheme used in modern distributed object stores ) .",
    "we propose two heuristics along these lines .",
    "both heuristics make the assumption that the capacities of all servers @xmath219 are equal to @xmath220 .    given that data placement is np - hard , it is easy to show that data placement with replication is also np - hard .",
    "the problem can be modeled by an integer linear program whose formulation is nontrivial and is described in the appendix .",
    "unfortunately , an optimal solution is very expensive to compute , even for small instances of the problem , as integer linear programs take exponential time in the worst case . in this section",
    "we propose two heuristics for computing good data placements with replication , that utilize the graph partitioning algorithm presented earlier .",
    "formulating a direct reduction to graph partitioning is left for future work . for simplicity",
    "we focus on the case in which no views or intermediate results are present .",
    "extensions are straightforward .    for the first heuristic  algorithm [ alg.heuristic1]we run data placement once to get a placement without replication , assuming each server s capacity is only @xmath221 , where @xmath222 is the desired replication factor .",
    "we assume this capacity is enough to store all the tables .",
    "we then apply this placement strategy @xmath222 times to @xmath222 _ random permutations _ of the server set .",
    "notice that this algorithm might result in some tables being stored multiple times on the same server ( of course , in that case we only keep a single copy ) . clearly , this algorithm has the same complexity as data placement , ignoring the time to generate the random permutations , but can only guarantee at most @xmath222 ( and not exactly @xmath222 ) replicas of each table .",
    "heuristic 1 t , q , s , r [ alg.heuristic1 ] a = ( t , q , s ) + p 1 r p = ( s ) + a p. +   +    in the second heuristic , instead of dividing the space on all servers by @xmath222 and filling a fraction of each server in every replication round , we allocate @xmath223 servers completely for each round ( @xmath7 is the total number of servers ) , and we use these servers to compute a data placement that optimizes the communication cost for a subset of @xmath224 queries ( @xmath4 is the total number of queries ) .",
    "we assume that one replica of all tables can fit in @xmath225 servers ( so that @xmath222 replicas can fit in @xmath7 servers ) . for any nontrivial assignment",
    ", we have @xmath226 , as otherwise all tables will be placed on a single server resulting in @xmath109 communication cost .",
    "algorithm  [ alg.heuristic2 ] implements the second heuristic . in the first round , we run data placement using only the first @xmath225 servers",
    "we then remove from the query set @xmath227 the cheapest @xmath224 queries , denoted @xmath228 .",
    "these cheapest queries ( i.e. , those with the lowest communication cost ) will not be considered in the subsequent replication rounds . in the second round ,",
    "we run data placement using the next @xmath225 servers for the query set @xmath229 .",
    "then , again , we remove the cheapest @xmath224 queries from the remaining queries , and so on . in the algorithm below , @xmath230 for @xmath231 and @xmath232 .",
    "heuristic 2 t , q , s , r [ alg.heuristic2 ] i 1 r p_i = \\{s_1 + a_i-1 , s_a_i } + a = ( t , q , p_i ) + a p_i + q = q \\ { m / r q } +   +    note that we use the full set of tables @xmath15 in each replication round , meaning that every table will be replicated exactly @xmath222 times .",
    "the intuition behind this heuristic is that any optimal solution would in fact partition the queries into @xmath222 parts , such that the queries in each part use one of the replicas of the tables , and do so optimally .",
    "we are essentially trying to approximate an optimal solution by assuming that the @xmath224 cheapest queries in each round will be served by the replicas placed in this round ( which is why the cheapest queries are removed from consideration after each round ) .",
    "interestingly , when the algorithm terminates and returns the placement of all the replicas of each table , a query will be executed on a server which results in the minimum communication cost ; therefore , it is not necessarily true that any query @xmath233 be executed using the @xmath83th copy of the tables placed in the @xmath83th round .",
    "for our experimental evaluation , we implemented the proposed integer programs in ampl and solved them optimally using cplex .",
    "we also implemented an approximation algorithm that converts the given workload into a bipartite graph using our proposed reductions and runs metis on the resulting graph .- way partitioning , 1000 cuts , and 1000 iterations .",
    "we try @xmath234 10 u - factors that correspond to various maximum part size objectives and take the best result that satisfies the part size objectives ( i.e. , server capacity constraints ) .",
    "since metis is fast , the np - hardness of feasibility implies that metis ( like any other polynomial - time algorithm used for data placement ) , must sometimes violate the part - size objectives ( unless p@xmath34np ) . ]",
    "all experiments were run on a server with 32 intel ia-64 1.5mhz cores and 256 gb of main memory , running suse linux 2.6.16 .",
    "many of our experiments involving cplex had not finished running after more than three weeks , so we report the data communication cost of the solution that cplex had converged to at that time .",
    "on the other hand , metis took no more than four minutes in the worst case , to report a good placement .",
    "for very few servers ( i.e. , 2 to 3 ) cplex finds an optimal solution quickly and is therefore preferable to metis ( even in these cases metis is very close to the cplex optimal ) .",
    "hence we do not consider small numbers of servers here .",
    "as the input dataset we use the tpc - ds decision support benchmark @xcite ( we only need the schema , the queries , and the table sizes , but we do not need to actually generate any data ) .",
    "the benchmark contains seven fact tables and 17 dimension tables , as well as 99 predefined queries .",
    "the minimum , maximum and average number of table dependencies per query are 1 , 13 , and 4 , respectively . in other words ,",
    "the queries are fairly well distributed across the spectrum from simple to very complex .",
    "as for the table sizes , we set the fact tables to be one to two orders of magnitude larger than the dimension tables and normalized the sizes .",
    "in particular , fact tables have sizes between 50 and 100 , and dimension tables have sizes between 1 and 10 .",
    "first we test our simple bipartite graph construction with tables and queries only . only the base tables need to be stored .",
    "when the queries are executed , they are executed at the server specified by the placement given by the algorithm and the results are then discarded .",
    "the total communication cost for executing a workload is the sum of the costs for executing each query , and the cost of executing a query is the sum of the sizes of the tables that need to be transferred to the server executing the query . for simplicity",
    ", we assume here that when a table is involved in a particular query , that the whole table needs to be transferred , if it is not already co - located with the query .",
    "next , we randomly assign sizes to all queries ( between 1 and 20 ) and assume that all 99 queries are to be materialized",
    ". then we test the bipartite graph construction for arbitrary execution plans .",
    "we also perform experiments to show the effect of load balancing on the data communication cost .",
    "finally , we run experiments for testing the replication heuristics .    given that the total numbers of tables and queries in tpc - ds are fixed , we created very large randomized datasets for testing the scalability of our solution .",
    "these randomized datasets range from 1000 tables and 1000 queries up to 16000 tables and 16000 queries .",
    "we choose the table sizes randomly from a normal distribution with mean 10 and standard deviation 15 ( ignoring negative values and taking the floor of each generated value ) .",
    "we similarly choose the number of tables per query from a normal distribution with mean 5 and standard deviation 3 .",
    "finally , the tables involved in each query are chosen uniformly at random .    in our experiments",
    "we vary the number of servers from 4 up to 16 , and we also vary the capacity of each server . notice that for a placement to be feasible , there is a minimum required capacity per server , that depends both on the total size of all tables and on the largest table . given our normalized table sizes described above , the tpc - ds dataset ( with no materialized queries ) has a total size of 580 units of space and there exist three tables with size 100 and four tables with size at least 50 .",
    "hence , with 4 servers , if each server had capacity @xmath235 , there would not exist any feasible placement , given that after placing all tables with size 100 , there is not enough space left to place all tables with size 50 . in this case",
    ", we assign each server a capacity of 150 .",
    "similarly , assuming a total of 16 servers , each server must have capacity @xmath236 , but clearly now the largest table can not fit in a single server .",
    "thus , our algorithms assume that the capacity of every server is at least 100 . in this case",
    "the servers are over - provisioned , which means that many servers might be left under - utilized . in other words , adding more servers does not necessarily result in a more distributed solution .",
    "in fact , given this particular dataset , for 8 or more servers the optimal data placement is always the same and only uses only 8 servers . determining the optimal number of servers , server capacity , horizontal partitioning of tables , etc . , is dataset and application dependent , and beyond the scope of this paper .",
    "thus we do not consider it any further .",
    "figures [ fig.tpc-ds2_noviews_4 ] and [ fig.tpc-ds2_noviews_8 ] show the total cost of each placement given by the optimal algorithm ( using cplex ) and the approximate algorithm ( using metis ) , for 4 and 8 servers , respectively , ( results with more than 8 servers are identical to the 8 server case ) and varying server capacities . in the graphs we also plot the communication cost obtained by allowing metis to use 10 units of space more capacity per server than cplex ( denoted metis+10 ) for easier comparison .",
    "we can clearly see from the figures that the cost of the approximate solution is very close to that of the best solution which cplex had achieved within three weeks ( the only runs which completed within three weeks were those with 4 servers ) , for a minuscule fraction of the run time .",
    "metis took under 3 seconds to report a placement , in the worst case . in the graphs we also plot the minimum , maximum and average part size assigned to each server ( i.e. , the total size of the tables assigned to each server )",
    "we can see that for a large number of servers , many servers remain empty , as expected .",
    "another interesting observation is that by allowing metis a modest extra 10 units of space per server over cplex , it can consistently `` beat '' cplex for all numbers of servers .              in order to test the bipartite graph construction for arbitrary execution plans , we randomly assign sizes to each one of the @xmath237 queries in tpc - ds and materialize them .",
    "the total table plus materialized view size of this dataset is 965 .",
    "figures [ fig.tpc-ds2_views_onemove_4 ] , [ fig.tpc-ds2_views_onemove_8 ] , and [ fig.tpc-ds2_views_onemove_16 ] show the results assuming 4 , 8 and 16 servers , respectively .",
    "once again , metis produces very good placements as the number of servers increases .",
    "furthermore , for 4 servers , metis is not able to give particularly good solutions .",
    "but even in this case it took cplex 220 minutes to find an optimal solution , while metis reported the placement in under 3 seconds .                for comparison",
    ", we also run experiments for generalized data placement in which we force the materialized views to be executed and stored on the same server ( essentially by setting the appropriate edges to infinity ) .",
    "we wanted to quantify the impact of allowing the views to move .",
    "the results for 8 servers are shown in figure [ fig.onemove-nomove ] , with `` -nomove '' representing using the same server to store and compute a view .",
    "it is clear that allowing a view to be computed on one server and stored on another has a noticeable impact on the data communication cost .",
    "we next run a set of experiments to test the effect of adding load - balancing constraints , as proposed in section  4.3 , on the data communication cost .",
    "the bipartite graph construction now contains two - dimensional weights for all vertices and each server has two - dimensional weights : capacity and load .",
    "metis allows two - dimensional weights on vertices and , using the ubvec parameter , it is possible to vary the ratio of minimum - to - maximum load across the servers .",
    "figure [ fig.load ] shows a comparison result of using 4 and 8 servers with no load balancing constraints ( no - load ) and four other scenarios ( load-1 through load-4 ) with increasingly stringent minimum - to - maximum load constraints . for this experiment",
    ", we use the simple tpc - ds workload with base tables and queries only .",
    "the distribution of load across the servers is also shown .",
    "the results indicate that with moderate increase in communication cost it is possible to satisfy reasonable load balancing requirements .",
    "we obtained similar results when all 99 tpc - ds queries were materialized .",
    "we also run a set of experiments to test the replication heuristics proposed in section [ sec.replication ] .",
    "for these experiments we vary the number of servers from 4 up to 16 , and the number of replicas per table from 1 to 5 .",
    "then , we compute the total cost of executing the queries after determining the placement using heuristic  1 ( abbreviated h1 ) and heuristic  2 ( abbreviated h2 ) .",
    "we use the simple tpc - ds dataset with no materialized views .",
    "figure [ fig.replication_cost ] shows the results for 4 , 8 , and 16 servers .",
    "we can clearly see that h2 results in significantly better data placement than h1 in all cases . also , notice how the data communication cost drops as the number of replicas increases , as expected .",
    "observe that as we increase the number of servers , the total communication cost increases .",
    "this is reasonable , given that we do not keep the capacities of the servers fixed ( so as not to have underutilized servers ) .",
    "therefore , as the number of servers increases , the total capacity of each server decreases proportionately , and hence the dataset becomes more distributed , resulting in higher communication cost . finally , notice that not all parameter combinations are meaningful , hence there are some missing points in the plots ( for example it is pointless to have replication more than 4 for 4 servers ) .",
    "specifically , for h2 certain configurations result in invalid inputs , given that h2 utilizes @xmath238 of the total number of servers in each round .",
    "figure [ fig.replication_size ] shows the maximum part size assigned to the servers as the replication factor increases .",
    "we can see that both heuristics result in fairly equal maximum part size , although h2 is consistently better than h1 . as a baseline , we also plot the `` desired '' part size , which is the ideal storage overhead on each server , based on the desired replication factor .",
    "the desired part size is usually unattainable , since there exist tables in the dataset with sizes larger than the desired part size .              for our final experiment",
    "we test the scalability of metis using very large randomly - generated workloads .",
    "figures [ fig.rand_queries ] and [ fig.rand_tables ] show the results for a varying number of queries and tables . in the first plot",
    "we keep the number of tables fixed at 1000 and vary the number of queries from 1000 up to 16000 . in the second plot",
    "we keep the number of queries fixed to 8000 and vary the number of tables from 1000 up to 16000 .",
    "we also vary the number of servers from 4 up to 128 .",
    "notice that the more tables we have , the sparser the queries become , resulting in sparser bipartite graphs , which are easier to handle .",
    "but the more queries we have with respect to tables the denser the graphs become , which is expected to result in higher cost .",
    "the reported times are in seconds , and we can see that metis scales very well across all dimensions .              clearly , relying on an optimal algorithm for determining data placement is not practical even for very small datasets and number of servers ( e.g. , 24 tables , 99 materialized queries , and four servers takes 220 minutes to compute ) . on the other hand ,",
    "determining approximate placements using our technique can be done very quickly and results in very high quality results in most cases , especially as the number of servers increases . in other words ,",
    "the reduction of data placement to graph partitioning by means of our bipartite graph construction allowed us to use standard graph partitioning libraries ( metis ) to solve a computationally expensive problem efficiently and with high quality in practice .",
    "in this paper , we presented _ practical _ algorithms for minimizing the data communication cost of evaluating a query workload in a distributed setting .",
    "we reduced the data communication problem to graph partitioning and showed how to treat arbitrary query execution plans that can involve tables , materialized views , and intermediate results .",
    "we also discussed load balancing , and presented two heuristics for handling replication .",
    "we implicitly assumed that queries and view updates arrive asynchronously . an interesting direction for future work is to optimize for workloads in which some queries are always executed together , in some pre - specified order .",
    "this complicates the data placement problem , but may enable solutions with lower data communication costs .",
    "for example , if two views are maintained on the same server and require the same table to be shipped from some other server , then by always updating these two views together we only need to ship said table once per update .",
    "load balancing may also be improved by knowing which queries tend to be executed together .",
    "for instance , if queries @xmath24 and @xmath64 are always executed together and each consists of two steps , then we may prefer to assign their first steps to two different servers , rather than allocating the first steps of both queries to the same server and having the servers allocated to the second steps sit idle until the first steps are done .    10",
    ". https://www.quantcast.com/engineering/qfs .    .",
    "http://basho.com/riak .    .",
    "http://docs.openstack.org/developer/swift .    .",
    "https://tahoe-lafs.org/trac/tahoe-lafs .",
    "s.  agrawal , v.  narasayya , and b.  yang .",
    "integrating vertical and horizontal partitioning into automated physical database design . in _ sigmod _ , pages 359370 , 2004 .",
    "d.  borthakur .",
    "the hadoop distributed file system : architecture and design .",
    "http://hadoop.apache.org/docs/r0.18.0/hdfs_design.pdf , 2007 .",
    "f.  chang , j.  dean , s.  ghemawat , w.  c. hsieh , d.  a. wallach , m.  burrows , t.  chandra , a.  fikes , and r.  e. gruber .",
    "bigtable : a distributed storage system for structured data . in _ osdi _ , pages 1515 , 2006 .",
    "s.  chaudhuri .",
    "an overview of query optimization in relational systems . in _ pods _ , pages 3443 , 1998 .    c.  curino ,",
    "e.  jones , y.  zhang , and s.  madden .",
    "schism : a workload - driven approach to database replication and partitioning .",
    ", 3(1 - 2):4857 , sept . 2010 .",
    "g.  decandia , d.  hastorun , m.  jampani , g.  kakulapati , a.  lakshman , a.  pilchin , s.  sivasubramanian , p.  vosshall , and w.  vogels . dynamo : amazon s highly available key - value store . in _",
    "sosp _ , pages 205220 , 2007 .",
    "m.  y. eltabakh , y.  tian , f.  zcan , r.  gemulla , a.  krettek , and j.  mcpherson .",
    "cohadoop : flexible data placement and its exploitation in hadoop . , 4(9):575585 , june 2011 .",
    "s.  ghandeharizadeh and d.  j. dewitt .",
    "hybrid - range partitioning strategy : a new declustering strategy for multiprocessor database machines . in _ vldb _ , pages 481492 , 1990 .",
    "l.  golab , t.  johnson , s.  j. seidel , and v.  shkapenyuk .",
    "stream warehousing with datadepot . in _",
    "sigmod _ , pages 847854 , 2009 .",
    "b.  hendrickson and t.  g. kolda . graph partitioning models for parallel computing .",
    ", 26(12):15191534 , 2000 .",
    "b.  hendrickson and r.  leland .",
    "chaco : software for partitioning graphs .",
    "http://www.sandia.gov/~bahendr/chaco.html , 1994 .",
    "g.  karypis and v.  kumar .",
    "metis - unstructured graph partitioning and sparse matrix ordering system , version 2.0 .",
    "http://glaros.dtc.umn.edu/gkhome/metis/metis/overview , 1995 .",
    "a.  k. kayyoor , a.  deshpande , and s.  khuller .",
    "data placement and replica selection for improving co - location in distributed environments .",
    ", abs/1302.4168 , 2013 .",
    "liu and s.  shekhar . partitioning similarity graphs : a framework for declustering problems .",
    ", 21:475496 , 1996 .",
    "s.  b. navathe and m.  ra .",
    "vertical partitioning for database design : a graphical algorithm .",
    ", 18(2):440450 , june 1989 .    r.  v. nehme and n.  bruno . automated partitioning design in parallel database systems . in _",
    "sigmod _ , pages 11371148 , 2011 .",
    "j.  rao , c.  zhang , n.  megiddo , and g.  lohman .",
    "automating physical database design in a parallel database . in _ sigmod _ , pages 558569 , 2002 .",
    "t.  p. p.  c. ( tpc ) .",
    "http://www.tpc.org / tpcds/.    d.  c. zilio .",
    ". phd thesis , university of toronto , 1998 .",
    "we reduce partition , which is known to be np - hard , to ( the feasibility version of ) data placement .",
    "partition is this problem : given a sequence @xmath239 of positive integers , is there a subset @xmath240 such that @xmath241 ( @xmath242 ) ? given an instance @xmath239 of positive integers , let @xmath243 . if @xmath244 is odd , then the answer to partition is `` no . ''",
    "if @xmath244 is even , construct an instance of data placement in which there are two servers , each of capacity @xmath245 .",
    "there are @xmath0 tables , the @xmath83th of which has size @xmath246 , and there are no queries .",
    "the key point is that there is a legal placement of the @xmath0 tables onto the two servers , each of capacity @xmath245 , if and only if the answer for partition is `` yes . ''      the ip assumes that we want @xmath247 replicas of each table ( though generalizing to the case of table - dependent replica counts is easy ) .",
    "the @xmath222 replicas of table @xmath14 will be denoted by superscript @xmath248 .",
    "the ip for this case has binary variables @xmath249 for table @xmath14 and server @xmath20",
    ". the variable @xmath249 will be 1 if and only if the @xmath250th replica of table @xmath14 is stored on server @xmath20 .",
    "in addition , for query @xmath11 and server @xmath20 , there is a binary variable @xmath251 , which is 1 if and only if @xmath11 is stored on server @xmath20 .",
    "finally , we also have real ( not binary ) variables @xmath252 for query @xmath11 , table @xmath14 in @xmath11 , and server @xmath20 , where @xmath252 will be 1 if the @xmath250th replica of table @xmath14 is stored on server @xmath20 and query @xmath11 is also stored on server @xmath20 , and 0 otherwise . in other words , @xmath252 will equal @xmath253 . as product is decidedly nonlinear , we will have to show how to achieve @xmath254 , since we can not write such a constraint explicitly .",
    "recall that the objective function is to minimize the sum , over @xmath11 , of the sum of the sizes of all tables in @xmath11 minus the sum of the sizes of the tables of @xmath11 at least one replica of which is stored on the same server as @xmath11 .",
    "hence , we can view the objective as the _ maximization _ of [ the sum , over @xmath11 , of the sizes of the items of @xmath11 at least one replica of which is stored on the same server as @xmath11 ] .",
    "the latter quantity equals @xmath256.\\ ] ] ( we ensure below , for all @xmath14 and @xmath20 , that @xmath257 , so that the @xmath222 replicas of a given table go on different servers . ) hence the objective function is        1 .   for all @xmath11 , @xmath259 , i.e.",
    ", every query gets assigned to exactly one server .",
    "2 .   for all @xmath260 , @xmath261 ,",
    "i.e. , at most one replica of each item goes on a given server .",
    "3 .   for all @xmath262 , @xmath263 , i.e.",
    ", every replica gets assigned to some server .",
    "4 .   for all @xmath264 , @xmath265 , i.e.",
    ", the capacities of the servers are not violated .",
    "we deal with the nonlinearity of the objective function as follows . in @xmath266",
    "we replace `` @xmath267 '' by `` @xmath268 '' , getting @xmath269 the intent of @xmath268 is that it should , at optimality , equal @xmath270 , but how do we ensure that ?",
    "the answer is that we add constraints @xmath271 and @xmath272 , for all @xmath273 .",
    "because the objective function is a maximization , and @xmath268 appears nowhere else , in an optimal solution @xmath268 will be as large as possible , and hence at optimality @xmath274 ."
  ],
  "abstract_text": [
    "<S> with the widespread use of shared - nothing clusters of servers , there has been a proliferation of distributed object stores that offer high availability , reliability and enhanced performance for mapreduce - style workloads . however , relational workloads can not always be evaluated efficiently using mapreduce without extensive data migrations , which cause network congestion and reduced query throughput . </S>",
    "<S> we study the problem of computing data placement strategies that minimize the data communication costs incurred by typical relational query workloads in a distributed setting .    </S>",
    "<S> our main contribution is a reduction of the data placement problem to the well - studied problem of graph partitioning , which is np - hard but for which efficient approximation algorithms exist . </S>",
    "<S> the novelty and significance of this result lie in representing the communication cost exactly and using standard graphs instead of hypergraphs , which were used in prior work on data placement that optimized for different objectives ( not communication cost ) .    </S>",
    "<S> we study several practical extensions of the problem : with load balancing , with replication , with materialized views , and with complex query plans consisting of sequences of intermediate operations that may be computed on different servers . </S>",
    "<S> we provide integer linear programs ( ips ) that may be used with any ip solver to find an optimal data placement . for the no - replication case , we use publicly available graph partitioning libraries ( e.g. , metis ) to efficiently compute nearly - optimal solutions . for the versions with replication , </S>",
    "<S> we introduce two heuristics that utilize the graph partitioning solution of the no - replication case . using the tpc - ds workload </S>",
    "<S> , it may take an ip solver weeks to compute an optimal data placement , whereas our reduction produces nearly - optimal solutions in seconds . </S>"
  ]
}