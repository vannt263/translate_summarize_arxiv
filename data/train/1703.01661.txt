{
  "article_text": [
    "to achieve robust , autonomous operation in unstructured environments , robots must be able to identify relevant objects and features in their surroundings , recognize the context of the situation , and then plan their motions and interactions accordingly .",
    "recent efforts in autonomous manipulation challenges such as the darpa robotics challenge  @xcite and the amazon picking challenge  @xcite resulted in state - of - the - art perception capabilities enabling systems to perceive , reason about , and manipulate their surroundings .",
    "however , existing object identification and pose estimation solutions for closed - loop manipulation tasks are generally ( 1 ) not robust in cluttered environments with partial occlusions , ( 2 ) not able to operate in real - time ( @xmath2 ) , ( 3 ) not sufficiently accurate  @xcite , or ( 4 ) not able to produce accurate pose estimates without sufficient initial seeds @xcite .",
    "we present a novel perception pipeline that tightly integrates deep semantic segmentation and model - based object pose estimation , achieving real - time pose estimates with a median pose error of @xmath0 and @xmath3 . our solution ( which we refer to as _ segicp _ ) uses rgb - depth sensors ( and proprioception information when available ) to provide semantic segmentation of all relevant objects in the scene along with their respective poses ( see figure  [ intro - fig ] ) in a _",
    "highly parallelizable _ architecture .",
    "the main contributions of this manuscript are as follows :    1 .",
    "a _ highly parallelized _ approach to integrated semantic segmentation and multi - hypothesis object pose estimation with @xmath0 accuracy using a single view that operates at @xmath4@xmath5 ( @xmath6@xmath7 hz ) per frame _ without any prior pose seeds_. 2 .   a novel metric to score the quality of point cloud registration , allowing for autonomous and accurate pose initialization over many potential hypotheses . 3 .",
    "an efficient automatic data - collection framework for acquiring annotated semantic segmentation and pose data by using a motion capture system .",
    "4 .   analysis and benchmarking of our segicp pipeline against the automatically annotated object poses .",
    "our approach builds on the substantial literature devoted to robot perception of mobile manipulation task environments and the relevant objects therein .",
    "robot systems must be able to first identify entities that pertain to the task at hand and reason about their relative poses to eventually manipulate and interact with them .",
    "accordingly , we discuss the relevant literature in object recognition and pose estimation .    * object recognition .",
    "* semantic segmentation , which assigns each pixel in an image to one of a set of predefined categories , effectively solves the object recognition problem .",
    "this approach is in contrast to that of many object recognition systems , which only output bounding boxes around objects of interest @xcite .",
    "though the winning team for the 2015 amazon picking challenge used a bag - of - words approach @xcite instead of per - pixel categorization , there are multiple advantages to retaining the spatial position of every object . particularly in robotic applications such as grasping or autonomous driving ,",
    "semantic segmentation enables a higher resolution representation of the arrangement and identities of objects in a cluttered scene , and also effectively addresses self - occlusions .",
    "previous approaches to object recognition have used classifiers with hand - engineered features  @xcite to generate either bounding - box object locations or noisy pixel - wise class predictions , which are then smoothed using crfs  @xcite .",
    "recent work in computer vision has shown that convolutional neural networks ( convnets ) considerably improve image classification  @xcite , and that convnets originally trained for image classification can be successfully re - purposed for dense pixel - wise semantic segmentation  @xcite .",
    "such approaches generally retain the lower level feature detectors of image classification models such as alexnet  @xcite or vgg @xcite and stack on additional layers , such as convolution @xcite , deconvolution @xcite , or dilated convolution @xcite for dense prediction .",
    "* pose estimation .",
    "* while semantic segmentation is able to identify and locate objects in 2d images , pose estimation refines object location by also estimating the most likely 6-dof pose of each identified object .",
    "previous approaches to this task have used template matching , which can recover the pose of highly - textured objects @xcite @xcite using local features such as sift  @xcite . for rgb - d images , use of stable gradient and normal features has been demonstrated with linemod @xcite .",
    "approaches using parts - based models have also been successful  @xcite",
    ". however , these methods are not robust to variations in illumination or to scene clutter  @xcite .    a separate class of point cloud alignment algorithms attempt to solve the global optimization problem @xcite .",
    "however , such approaches rely on surface normals and their performance can degrade when objects are generally flat , have low quantities of informative features , or exhibit potentially ambiguous geometries .",
    "a widely - accepted approach to pose estimation is the class of iterative closest point ( icp ) registration algorithms  @xcite .",
    "these approaches usually require initialization close to the global - minimum solution as these gradient descent methods tend to fall into poor local minima and are not robust to partial or full occlusions @xcite .",
    "most relevant to our work , team mit - princeton demonstrated promising results in the amazon picking challenge using multiple views with a fully convolutional neural network to segment images and fit 3d object models to the segmented 3d point cloud @xcite . however , their pose estimation system was slow ( @xmath8 per object ) and showed high position and angle errors ( @xmath9 and @xmath10 , respectively ) .",
    "we advance this prior work by presenting a novel metric for scoring model registration quality , allowing accurate initial pose estimation through multi - hypothesis registration . from a single rgb - d frame ,",
    "we achieve increased pose - tracking robustness and accuracy .",
    "further , we emphasize an order of magnitude speedup by leveraging a _ highly parallelizable _ design that operates over all objects simultaneously .",
    "we couple these advances with an efficient data collection pipeline that automatically annotates semantic segmentation labels and poses for relevant objects .",
    "overall , segicp achieves estimation errors on the order of @xmath11 and @xmath0 and an overall run - time of @xmath4@xmath5 per object , _ without the need of an initial seed_.",
    "we present segicp , a novel perceptual architecture that handles sensor input in the form of rgb - d and provides a semantic label for each object in the scene along with its associated pose relative to the sensor .",
    "segicp acquires and tracks the 6-dof pose of each detected object , operating at @xmath12 per frame ( @xmath5 during an initialization phase ) with @xmath0 position error and @xmath1 angle error , and can robustly deal with prolonged occlusions and potential outliers in the segmentation with a kalman filter .",
    "segicp achieves this using an object library approach to perception , referencing scanned 3d models of known objects , and performs 3d point cloud matching against cropped versions of these mesh models .",
    "an example of object meshes pertaining to our automotive domain is illustrated in figure  [ segicp - pipeline ] .    in our architecture ,",
    "as outlined in figure  [ segicp - pipeline ] , rgb frames are first passed through a convnet which outputs a segmented mask with pixel - wise semantic object labels .",
    "this mask is then used to crop the corresponding point cloud , generating individual point clouds for each detected object .",
    "icp is used to register each object s point cloud with its full point cloud database model and estimate the pose of the object with respect to the sensor .      instead of the classical segmentation problem , we are concerned with the question , _ how can we generate `` appropriate '' masks over the depth map in order to result in accurate pose estimation ? _ to address this question , we experimented with various convnet architectures that semantically segment known objects of interest .",
    "we explored two different convnet architectures , segnet @xcite and dilatednet @xcite ( further discussed and elaborated in section  [ compare - segs ] ) .",
    "of the two networks , we found that the best model for our segicp pipeline was segnet , a @xmath13-layer , fully convolutional neural network with @xmath14 million parameters .",
    "the network was trained on cropped and downsampled images from the training set ( to 320@xmath15320 pixels ) consisting of eight object classes ( including background ) using the cross entropy criterion coupled with data augmentation consisting of image rotations , crops , horizontal and vertical flips , and color and position jitter .",
    "we further elaborate on the acquisition of the annotated training data in section  [ sec : data - collection ] .",
    "the resulting segmentation is used to extract each object s 3d point cloud from the scene cloud .",
    "the identity of each segmented object ( the object s semantic label ) predicted by segnet is then used to retrieve its corresponding 3d mesh model from the object model library .",
    "the mesh model is converted into a point cloud representation , downsampled , and aligned against its respective segmented object point cloud .",
    "the point cloud alignment task is divided into two phases : _ acquisition _ and _ tracking_. the objective of the acquisition phase is to find the initial optimal alignment between each object s model and its corresponding scene point cloud .",
    "this alignment is used to determine the visible side of the model ( _ model crop _ ) and to initialize the tracking phase , whose objective is to fuse camera and robot motion information to maintain an accurate , real - time pose estimate of the object even during camera motion and potential object occlusions .",
    "it is important to note that a novel contribution of this paper is the model - to - scene alignment metric that is used to determine the quality of the initial registration as well as when to switch between acquisition and tracking phases .    *",
    "the acquisition phase . * the acquisition phase finds the initial optimal alignment and crop of the object s mesh model with the current point cloud .",
    "multiple candidate crops are obtained by rendering the visible object s model at various azimuth and elevation angles and cropping the model to keep only the front face .",
    "each of the candidate crops is initialized at the median position of the object s scene point cloud in order to remove segmentation outliers and prevent icp from converging to incorrect local minima . in parallel",
    ", each candidate crop is run through a few iterations of the tracking phase to achieve a pose hypothesis .",
    "a novel model - to - scene alignment metric ( eq .",
    "[ eqn : nn_metric ] ) is evaluated on each candidate model crop .",
    "the motivation behind the metric is to determine whether a candidate cloud can align well with the object cloud by finding how many points in the candidate cloud have a unique corresponding match in the object s cloud .",
    "letting @xmath16 be the set of points in the candidate crop point cloud and @xmath17 be the set of points in the segmented object scene cloud , the alignment metric is given by : @xmath18 where @xmath19 is the set of points in @xmath16 with unique corresponding points in @xmath17 at most @xmath20 meters away .",
    "to compute the metric , segicp first builds a kd - tree with @xmath17 and perform radius searches with a radius of @xmath20 meters from every point in @xmath16 .",
    "each point in @xmath16 is mapped to the closest point in @xmath17 within @xmath20 that has not been already mapped to another point in @xmath16 , and then is added to @xmath19 .",
    "figure  [ segicp - pipeline ] and [ multi - examples ] show examples of model crops and their respective scores .",
    "we illustrate in figure  [ multi - examples ] that metrics such as the icp fitness score ( a euclidean error score ) and intersection over union ( iou ) between the predicted segmentation and the projection of the registered model into the camera frame do not effectively distinguish good registrations from erroneous ones . in comparison",
    ", our proposed metric ( eq . [ eqn : nn_metric ] ) addresses these immediate shortcomings present on objects with high degrees of symmetry such as the oil bottle .",
    "if any candidate scores are above a threshold @xmath21 , segicp switches to the tracking phase for future frames .",
    "the candidate with the best metric score is then used to initialize the tracking pipeline .        * the tracking phase .",
    "* the candidate model pose and crop with the highest alignment score  ( eq .",
    "[ eqn : nn_metric ] ) are used to initialize the tracking phase . in order to make the tracking procedure robust to imperfections on the boundary of the object s segmentation ,",
    "the object s scene point cloud is further pruned by removing points outside a bounding box of the latest registered model pose .",
    "an implementation of point - to - point icp from pcl  @xcite is used to maintain an accurate , real - time pose estimate of the object .",
    "the pose is used as a measurement update in a kalman filter to track each object s 6-dof pose and velocities . by fusing known camera motions from the available odometry of the robot ,",
    "the filter is able to handle temporary object occlusions and outlier pose estimates .",
    "the alignment metric ( eq . [ eqn : nn_metric ] ) is run on the fit to measure the uncertainty of the current pose measurement and to inform the kalman filter accordingly . if the score goes below a predefined minimum threshold @xmath22 , the kalman filter propagates the objects pose based on odometry ( and until a maximum pose uncertainty ) while switching back to object acquisition mode .",
    "deep neural networks require large numbers of data instances to support the millions of decision parameters . moreover , in the case of semantic segmentation , we need pixel - level annotations on each rgb image in the dataset .",
    "we trained segnet on @xmath23 labeled images of indoor scenes consisting of automotive entities ( e.g. engines , oil bottles , funnels , etc ) .",
    "of these images , about two - thirds were hand - labeled by humans ( using labelme ) while the remaining third was generated through a data collection pipeline using a 3d investigator^tm^ motion capture ( mocap ) system and active markers placed on our cameras and objects ( shown in figure  [ fig : mocap - setup ] ) .",
    "the training images span multiple sensor hardware ( microsoft kinect1 , asus xtion pro live , microsoft kinect2 , and carnegie robotics multisense sl ) each with varying resolutions ( respectively , 640@xmath15480 , 640@xmath15480 , 1280@xmath151024 , and 960@xmath15540 ) .",
    "figure  [ fig : mocap ] illustrates images within our motion capture pipeline , where input rgb and depth images are encoded into png format .",
    "the depth image is transformed into rgb using a 16-bit encoding , with each pixel s rgb channel values corresponding to pixel @xmath24 , @xmath25 , and @xmath26 in camera coordinates .",
    "active markers are placed on the engine stand and on the corner of the table .",
    "known transformations via mocap are then used to segment the image by projecting a scanned object mesh using the transform into the camera optical frame , thus generating annotated segmentation and object pose data .",
    "[ dataset ]",
    "we benchmark segicp on a dataset collected using the mocap system in which we annotate the pose and segmentation using active markers on our sensor and objects . as described in section  [ sec : data - collection ] , the benchmark dataset consists of @xmath27 annotated object poses ; a subset of these are shown in figure  [ dataset_sample ] for both the microsoft kinect1 and kinect2 .      to categorize the influence of the segmentation performance on pose estimation , we explored two architectures for our semantic segmentation network  segnet and dilatednet .",
    "segnet is a computationally efficient autoencoder - decoder architecture for pixel - wise semantic segmentation .",
    "the autoencoder architecture is essential for per - pixel classification , as it enables reconstruction of the inputs from the outputs at each layer , learning how to reconstruct the input before the final classification layer .",
    "the dilatednet architecture makes use of dilated convolution modules to aggregate multi - scale contextual information without losing accuracy .",
    "both network architectures adapt the convolutional layers of the vgg @xcite image classification network , with segnet using the vgg layers as its encoder and dilatednet converting later layers into dilated convolution modules .",
    "weights are initialized during training using a vgg-16 model pretrained on imagenet @xcite .",
    "we train each of these networks with a dataset of over @xmath23 annotated images ( with an average epoch time of about an hour ) and obtain the following performance measures , as listed in table  [ segmentation_quantitative ] .",
    "> c|c|c| & & + & @xmath28 & @xmath29 +    & @xmath30 & @xmath31 +    & @xmath32 & @xmath33 +    a key distinction between the two architectures is that dilatednet was designed for increased recall by incorporating dilated convolution modules whereas segnet appears to achieve higher precision measures .",
    "notable visual differences are illustrated in figure  [ segnet - vs - dilated ] , where the output of segnet and dilatednet is displayed for the same scene .",
    "it is important to note that the _ quality _ of the segmentation has immediate impact on the performance of our point - to - pose registration pipeline for object pose estimation , as the purpose of segmentation is to generate a mask over the depth map to aid registration .",
    "however , the following questions still remain : _ does higher segmentation iou result in better pose ?",
    "higher precision ?",
    "higher recall ?",
    "_ in the next section , we perform several benchmarks to investigate these very questions .",
    "we evaluate segicp on the benchmark dataset generated from a motion capture system .",
    "these images consists of semantic labels known objects in the scene as well as their poses relative to the camera .",
    "* the acquisition and tracking phases . * in our benchmarking",
    ", we used a collection of @xmath34 model crops for each object during the acquisition phase and discovered an overalsl average runtime of @xmath5 over a collection of thirty threads on a six - core i7 - 6850k .",
    "however , note that the time evaluation here is directly dependent on the number of crops and the machine s cpu .",
    "the registration of each of these crops proposed a separate object pose hypothesis ( alike figure  [ multi - examples ] ) , and we used a threshold of @xmath35 to switch into the tracking phase , which continuously updates the object s pose using the optimal crop , operating at about @xmath36 , with @xmath37@xmath38 being the neural network forward propagation ( with nvidia gtx titan x ) .",
    "for the kd - tree radius search to compute the metric , we used @xmath39 .",
    "* benchmarking .",
    "* in figure  [ benchmarking ] , we illustrate the results of evaluating segicp on the benchmarking dataset of @xmath27 object pose annotations . to fully categorize the influence of the segmented mask on the final pose estimation , we ran segicp using the annotated segmentation and the output of the two segmentation neural network architectures : segnet and dilatednet .",
    "these results indicate that segnet achieves higher performance ( @xmath40 ) as compared to dilatednet ( @xmath41 ) .",
    "we categorize failure as exceeding errors of more than @xmath9 in position and @xmath42 in axis and axis angle .",
    "these failures due to segmentation errors and model crop coverage represent a class of highlighted instances in the figure . of the successful scenes",
    ", segicp achieves @xmath0 position error and @xmath1 angle error ; this level of accuracy corresponds to about @xmath43 of all the benchmarked instances .",
    "further performance measures are given in figure  [ benchmarking ] , where we show the distribution of pose estimation errors given segmentation .",
    "the performance of segicp is highly correlated with both sensor technology and calibration .",
    "when considering only the @xmath44 kinect1 instances ( a structured light sensor with better rgb - d calibration ) , segicp achieves success measures of @xmath45 , @xmath46 , and @xmath47 using segmented masks from annotated segmentation , segnet , and dilatednet respectively .",
    "segnet and dilatednet appear to have comparable performance in this case .",
    "however , when calibration is off , in the case of our kinect2 ( which is also a time of flight ) sensor , it is beneficial to bound the number of false - positive pixels ( maximizing precision ) to avoid acquiring misaligned points in the cropped scene cloud . from table",
    "[ segmentation_quantitative ] , segnet and dilatednet achieve precision measures of @xmath48 and @xmath49 respectively . with the kinect2",
    ", we observe success measures of @xmath50 , @xmath43 , and @xmath51 for annotated , segnet , and dilatednet segmentation ; the large inconsistencies with dilatednet is as a result of poor cropped scene clouds due to excessive false - positives in the segmentation ( poor precision ) .",
    "furthermore , the segicp pipeline operates with higher performance on structured light sensors such as the microsoft kinect1 , while time of flight technology , such as the microsoft kinect2 , resulted in slightly lower performance .",
    "we discovered that objects with reflective surfaces ( for instance the oil bottle ) with high levels of geometric symmetry and potential ambiguities result in poor icp fits due to the deformations in the point cloud caused by time of flight sensor technology .",
    "figure  [ fig : deform ] illustrates this particular phenomenon , where large deformities on the surface of the oil bottle is present , resulting in poor point cloud alignment .",
    "lastly , because the architecture uses a segmentation mask ( generated using the rgb frames ) to crop the point cloud , the sensor calibration of the rgb and depth frames is crucial for accurate pose estimation .",
    "we present a novel , _ highly parallelized _ architecture for semantic segmentation and accurate pose estimation ( @xmath0 position error and @xmath3 angle error ) .",
    "our architecture delivers immediate benefits as compared to work in the literature by not requiring an initial guess sufficiently close to the solution and by being inherently parallelizable , allowing us to process multiple objects simultaneously in real time ( @xmath4@xmath5 in tracking and acquisition mode respectively ) .",
    "we elaborated on a motion capture approach to collecting potentially massive sets of annotated segmentation and pose data , allowing our architecture to scale rapidly to more enriched domains .",
    "lastly , we categorized the segmentation - driven method to pose estimation by extensively investigating and benchmarking two different neural network architectures .",
    "we are currently working to refine the perception architecture , extend the framework to incorporate much larger sets of objects , and tie it with integrated task and motion planning for complex interactions in unstructured environments ."
  ],
  "abstract_text": [
    "<S> recent robotic manipulation competitions have highlighted that sophisticated robots still struggle to achieve fast and reliable perception of task - relevant objects in complex , realistic scenarios . to improve these systems perceptive speed and robustness , we present segicp , a novel integrated solution to object recognition and pose estimation . </S>",
    "<S> segicp couples convolutional neural networks and multi - hypothesis point cloud registration to achieve both robust pixel - wise semantic segmentation as well as accurate and real - time 6-dof pose estimation for relevant objects , even in the presence of occlusions and sensor noise . </S>",
    "<S> our architecture achieves @xmath0 position error and @xmath1 angle error in real time _ without _ an initial seed . </S>",
    "<S> we evaluate and benchmark segicp against an annotated dataset generated by motion capture . </S>"
  ]
}