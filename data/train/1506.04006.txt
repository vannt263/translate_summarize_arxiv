{
  "article_text": [
    "the european commission emphasizes open access as a key tool to bring together people and ideas in a way that catalyses science and innovation .",
    "more than ever before , there is a recognized need for digital research infrastructures for all kinds of research outputs , across disciplines and countries .",
    "openaire , the open access infrastructure for research in europe ( http://www.openaire.eu ) , ( 1 ) manages scientific publications and associated scientific material via repository networks , ( 2 ) aggregates open access publications and links them to research data and funding bodies , and ( 3 ) supports the open access principles via national helpdesks and comprehensive guidelines .    data related to those in the openaire information space exist in different places on the web . combining them with openaire will enable new use cases .",
    "for example , understanding changes of research communities or the emergence of scientific topics not only requires metadata about publications and projects , as provided by openaire , but also data about events such as conferences as well as a knowledge model of research topics and subjects ( cf .",
    "@xcite ) .",
    "the availability of data that is free to use , reuse and redistribute ( i.e.  _ open data _ ) is the first prerequisite for analysing such information networks .",
    "however , the diverse data formats and means to access or query data , the use of duplicate identifiers , and the heterogeneity of metadata schemas pose practical limitations on reuse .",
    "linked data , based on the rdf graph data model , is now increasingly accepted as a lingua franca to overcome such barriers  @xcite .",
    "the university of bonn is coordinating the effort of publishing the openaire data as linked open data ( lod ) and linking it to related datasets in the rapidly growing lod cloud .",
    "this effort is further supported by the athena research and innovation center and cnr - isti . besides data about scientific events and subject classification schemes , relevant data sources include public sector information ( e.g. , to find research results based on the latest employment statistics , or to answer questions such as `` how do the eu member states expenses for health research compare to their health care spendings ? '' ) and open educational resources ( `` how soon do emergent research topics gain wide coverage in higher education ? '' ) .",
    "concrete steps towards this vision are ( 1 ) mapping the openaire data model to suitable standard lod vocabularies , ( 2 ) exporting the objects in the openaire information space as a lod graph and ( 3 ) facilitating integration with related lod graphs .",
    "expected benefits include    * enabling semantic search over the outputs of european research projects , * simplifying the way the openaire data can be enriched by third - party services , and consumed by interested data or service providers , * facilitated outreach to related open content and open data initiatives , and * enriching the openaire information space itself by exploiting how third parties will use its lod graph .",
    "the specifically tailored nature of the openaire infrastructure , its large amount of data ( covering more than 11 million publications ) and the frequent updates of the more than 5000 repositories from which the data is harvested pose high requirements on the technology chosen for mapping the openaire data to lod .",
    "we therefore compared in depth three alternative mapping methods , one for each source format in which the data are available : hbase , csv and xml .",
    "section  [ sec : inputdata ] introduces the openaire data model and the three existing data sources .",
    "section  [ sec : vocab ] presents our specification of the openaire data model as an rdf vocabulary .",
    "section  [ sec : requirements ] establishes requirements for the mapping .",
    "section  [ sec : sota ] presents the state of the art for each of the three mapping approaches .",
    "section  [ sec : implementation ] explains our three implementations . in section",
    "[ sec : evaluation ] we evaluate them in comparison , with regard to different metrics induced by the requirements . section  [ sec : rw ] reviews work related to our overall approach ( comparing mappings and producing research lod ) . section  [ sec : conclu ] concludes and outlines future work .",
    "the data model of openaire infrastructure is specified as an entity relationship model ( erm )  @xcite with the following entity categories :    * * main entities * ( cf .",
    "figure  [ fig : er ] ) : result ( publication or dataset ) , person , organization , projects , and datasource ( e.g.  repository , dataset archive or cris ) .",
    "instances of these are continuously harvested from data providers . *",
    "* structural entities * representing complex information about main entities : instances ( of a result in different datasources ) , webresources , titles , dates , identities , and subjects .",
    "* * static entities * , whose metadata do not change over time : funding .",
    "e.g. , once a funding agency has opened a funding stream , it remains static .",
    "* * linking entities * represent relationships between entities that carry further metadata ; e.g. , an entity of type person_result whose property _ ranking _ has the value 1 indicates the first author .",
    "so far , the openaire data have been available in three formats : hbase , csv and xml .",
    "currently , the master source of all openaire data is kept in hbase , a column store based on hdfs ( hadoop distributed file system ) .",
    "hbase was introduced in 2012 when data integration efforts pushed the original postgresql database to its limits : joins became inefficient and parallel processing , as required for deduplication , was not supported .",
    "each row of the hbase table has a unique row key and stores a main entity and a number of related linked entities .",
    "the attribute values of the main entities are stored in the _",
    "< family>:body _ column , where the _",
    "< family > _ is named after the type of the main entity , e.g. , _ result _ , _ person _",
    ", _ project _ , _ organization _ or _ datasource_. the attribute values of linked entities , indicating the relationship between main entities , are stored in dedicated column families _ < family>:<column > _ , where _ <",
    "family > _ is the class of the linked entity and _ < column > _ is the row key of the target entity . both directions of a link are represented .",
    "cell values are serialized as byte arrays according to the protocol buffers  @xcite specification ; for example :    .... message person {      optional metadata metadata = 2 ;      message metadata {          optional stringfield firstname = 1 ;          repeated stringfield secondnames = 2 ;          optional qualifier nationality = 9 ; ... }      repeated person coauthors = 4 ; } ....    the following table shows a publication and its authors . for readability , we abbreviated row keys and spelled out key - value pairs rather than showing their binary serialization .     &  isauthorof : +    & body & body & 30  001::9897  & 30  001::ef29  & 50  001::39b9  +    50  0 01::39 b9  & resulttype= `` publication '' ; title=``the data model of  '' ; dateofacceptance= `` 2012 - 01 - 01 '' ; language=``en '' ; publicationdate= `` 2012 '' ; publisher= `` springer '' ; & & ranking=1 ; & ranking=2 ; & +    30  0 01::98 97  & & firstname=``paolo '' ; lastname=``manghi '' ; & & & ranking=1 ; +    30  0 01::ef 29  & & firstname=``nikos '' ; lastname=``houssos '' ; & & & ranking=2 ; +      csv files aid the computation of statistics on the openaire information space .",
    "hbase is a sparse key value - store designed for data with little or no internal relations .",
    "therefore , it is impossible to run complex queries directly on top of hbase , for example a query to find all results of a given project .",
    "it is thus necessary to transform the data to a relational representation , which is comprehensible for statistics tools and enables effective querying . via an intermediate csv representation ,",
    "the data is imported into a relational database , which is queried for computing the statistics .    in this generation process , each main entity type ( result , project , person , organization , datasource ) is mapped to a csv file of the same name , which is later imported into a relational database table .",
    "each single - valued attribute of an entity ( i d , title , publication year , etc . ) becomes a field in the entity s table .",
    "multi - valued attributes , such as the publication languages of a result , are mapped to relation tables ( e.g.  ` result_languages ` ) that represent a one - to - many relation between entity and attributes .",
    "linked entities , e.g.  the authors of a _ result _ , are represented similarly . as the data itself includes many special characters , for example commas in publication titles , the openaire csv files use ! as a delimiter and wrap cell values into leading and trailing hashes :    .... # dedup_wf_001::39b91277f9a2c25b1655436ab996a76b#!#the data model of the openaire scientific communication e - infrastructure#!#null#!#null#!#springer#!#null#!#null # ! # null#!#null#!#2012#!#2012 - 01 - 01#!#open access#!#open access#!#access#!#null # ! # 0#!#null#!#nulloai : http://helios - eie.ekt.gr:!#publication#10442/13187oai : pumaoai .",
    "isti.cnr.it:cnr.isti/cnr.isti/2012-a2-040#!#1 # ! ....",
    "finally , using csv has the advantage that existing tools such as sqoop can be used , thus reducing the need to develop and maintain customly implemented components on the openaire production system .",
    "openaire features a set of http apis for exporting metadata as xml for easy reuse by web services .",
    "these apis use an xml schema implementation of the openaire data model called oaf ( openaire format ) , where each record represents one entity .",
    "there is one api for searching , and one for bulk access .",
    "for example , the listing below comes from ` http://api.openaire.eu/search/publications ?",
    "openairepublicationid = dedup_wf_001::39b91277f9a2c25b1655436ab996a76b ` and shows the metadata of a publication that has been searched for .",
    "[ source , xml ] ---- < oaf : result >    < title schemename=\"dnet : datacite_title \" classname=\"main title \"     schemeid=\"dnet : datacite_title \" classid=\"main title\" > the data model of the      openaire scientific communication e - infrastructure</title >    < dateofacceptance>2012 - 01 - 01</dateofacceptance >    < publisher > springer</publisher >    < resulttype schemename=\"dnet : result_typologies \" classname=\"publication \"     schemeid=\"dnet : result_typologies \" classid=\"publication\"/ >    < language schemename=\"dnet : languages \" classname=\"english \"     schemeid=\"dnet : languages \" classid=\"eng\"/ >    < format > application / pdf</format >    ...",
    "< /oaf : result > ----    the api for bulk access uses oai - pmh ( the * * o**pen * * a**rchives * * i**nitiative * * p**rotocol for * * m**etadata * * h**arvesting ) to publish metadata and its corresponding endpoint is at http://api.openaire.eu/oai_pmh .",
    "the bulk access api lets developers fetch the whole xml files step by step . for our experiments",
    ", we obtained the xml data directly from the openaire server , as an uncompressed hadoop sequencefile comprising 500 splits of @xmath0300 mb each .",
    "as the schema of the openaire lod we specified an rdf vocabulary by mapping the entities of the er data model to rdf classes and its attributes and relationships to rdf properties .",
    "we reused suitable existing rdf vocabularies identified by consulting the linked open vocabularies search service and studying their specifications .",
    "reused vocabularies include dublin core for general metadata , skos for classification schemes and cerif for research organizations and activities .",
    "we linked new , openaire - specific terms to reused ones , e.g. , by declaring _ result _ a superclass of http://purl.org/ontology/bibo/publication and http://www.w3.org/ns/dcat#dataset .",
    "we keep the uris of the lod resources ( i.e.  entities ) in the http://lod.openaire.eu/data/ namespace .",
    "we modelled them after the hbase row keys . in openaire",
    ", these are fixed length identifiers of the form \\{_typeprefix_}\\{_namespaceprefix _ } : : _ md5hash_. _ typeprefix _ is a two digit code , 10 , 20 , 30 , 40 or 50 , corresponding to the main entity types datasource , organization , person , project and result .",
    "the _ namespaceprefix _ is a unique 12-character identifier of the data source of the entity . for each row , _",
    "md5hash _ is computed from the entity attributes .",
    "the resulting uris look like ` http://lod.openaire.eu/data/result/dedup_wf_001::39b9127 ` + ` 7f9a2c25b1655436ab996a76b ` .",
    "the following listing shows our running example in rdf / turtle syntax .    .... @prefix oad : < http://lod.openaire.eu/data/ > .",
    "@prefix oav : < http://lod.openaire.eu/vocab # > .",
    "# further prefixes omitted ; see ! \\url{http://prefix.cc } ! for their standard bindings .",
    "oad : result/ ... 001::39b9 ... rdf : type oav : result , bibo : publication ;      dcterms : title \" the data model of the openaire scientific communication          e - infrastructure\"@en ;      dcterms : dateaccepted \" 2012 - 01 - 01\"^^xsd : date ;      dcterms : language \" en \" ;      oav : publicationyear 2012 ;      dcterms : publisher \" springer \" ;      dcterms : creator oad : person/ ... 001::9897 ... , oad : person/ ... 001::ef29 ... .",
    "oad : person/ ... 001::9897 ... rdf : type foaf : person ;      foaf : firstname \" paolo \" ; foaf : lastname \" manghi \" ;      oav : isauthorof oad : result/ ... 001::39b9 ... .",
    "oad : person/ ... 001::ef29 ... rdf : type foaf : person ;      foaf : firstname \" nikos \" ; foaf : lastname \" houssos \" ;      oav : isauthorof oad : result/ ... 001::39b9 ... . ....",
    "in cooperation with the other technical partners in the openaire2020 consortium , most of whom had been working on the infrastructure in previous projects for years , we established the following requirements for the lod export :    1 .",
    "the lod output must follow the vocabulary specified in section  [ sec : vocab ] .",
    "the lod must be generated from one of the three existing data sources , to avoid extra pre - processing costs .",
    "3 .   the mapping to lod should be maintainable w.r.t .",
    "planned extensions of the openaire data model ( such as linking publications and data to software ) and the evolution of linked data vocabularies .",
    "[ req : scalability ] the mapping to lod should be orchestrable together with the other existing openaire data provision workflows , always exposing a consistent view on the information space , regardless of the format .",
    "5 .   to enable automatic and manual checks of the consistency and correctness of the lod before its actual publication",
    ", it should be made available in reasonable time in a private space .    to prepare an informed decision on the preferred input format to use for the lod export",
    ", we realised one implementation for each of hbase , csv and xml .",
    "for each possible approach , i.e.  mapping hbase , csv or xml to rdf , we briefly review the state of the art to give an overview of technology we could potentially reuse or build on , whereas section  [ sec : rw ] reviews work related to our overall approach .",
    "we assess reusability w.r.t .",
    "the openaire - specific requirements stated above .    [ sec : hbase - rdf]*hbase * , being a sparse , distributed and multidimensional persistent sorted map , provides dynamic control over the data format and layout .",
    "several works have therefore explored the suitability of hbase as a triple store for semi - structured and sparse rdf data .",
    "sun et al .",
    "adopted the idea of the hexastore indexing technique for storing rdf in hbase  @xcite .",
    "khadilkar et al .",
    "focused on a distributed rdf storage framework based on hbase and jena to gain scalability@xcite .",
    "others have provided mapreduce implementations to process sparql queries over rdf stored in hbase  @xcite .",
    "we are only aware of one work on exposing data from column - oriented stores as rdf .",
    "kiran et al .",
    "provide a method for generating a sparql endpoint , i.e. a standardized rdf query interface , on top of hbase  @xcite .",
    "they map tables to classes , rows to resources , and columns to properties .",
    "their approach do not scale well with increasing numbers of hbase entries , as the results show that the time taken to map hbase data to rdf is in hours for a few million rows  @xcite .",
    "[ sec : csv - rdf]*csv * is widely used for publishing tabular data  @xcite .",
    "the csv on the web w3c working group provides technologies for data dependent applications on the web working with csv .",
    "several existing implementations , including that of anything to triples ( any23 ) , map csv to a generic rdf representation .",
    "customizable mappings are more suitable for our purpose . in tarql ( transformation sparql )",
    ", one can define such mappings in sparql ; tabels ( tabular cells ) and sparqlify use domain - specific languages similar to sparql .",
    "tabels provides auxiliary machinery to filter and compare data values during the transformation process .",
    "sparqlify is mainly designed to map relational databases to rdf but also features the sparqlify - csv module .",
    "* xml * is used for various data and document exchange purposes .",
    "like for csv@xmath1rdf , there are generic and domain - specific xml@xmath1rdf approaches .",
    "breitling implemented a direct , schema - independent transformation , which retains the xml structure  @xcite . turning this generic rdf representation into a domain - specific",
    "one requires post - processing on the rdf side , e.g. , transformations using sparql construct queries . on the other hand , the current version of breitling s approach is implemented in xslt 1.0 , which does not support streaming and is therefore not suitable for the very large inputs of the openaire setting .",
    "klein uses rdf schema to map xml elements and attributes to rdf classes and properties  @xcite .",
    "it does not automatically interpret the parent - child relation between two xml elements as a property between two resources , but a lot of such relationships exist in the openaire xml .",
    "xsparql can transform xml to rdf and back by combining the xquery and sparql query languages to  @xcite ; authoring mappings requires good knowledge of both . by supporting xquery s expressive mapping constructs",
    ", xsparql requires access to the whole xml input via its dom ( document object model ) , which results in heavy memory consumption .",
    "a subset of xquery is suitable for streaming but neither supported by the xsparql implementation nor by the free version of the saxon xquery processor required to run xsparql .",
    "[ sec : hbase - rdf - impl ] as the only existing * hbase@xmath1rdf * implementation does not scale well ( cf .  section  [ sec : hbase - rdf ] ) , we decided to follow the mapreduce paradigm for processing massive amounts of data in parallel over multiple nodes .",
    "we implemented a single mapreduce job .",
    "its mapper reads the attributes and values of the openaire entities from their protocol buffer serialization and thus obtains all information required for the mapping to rdf .",
    "hence no reducer is required .",
    "the map - only approach performs well thanks to avoiding the computationally intensive shuffling .",
    "rdf subjects are generated from row keys , predicates and objects from attribute names and cell values or , for linked entities , from column families / qualifiers .",
    "[ sec : csv - rdf - impl ] mapping the openaire * csv@xmath1rdf * is straightforward : files correspond to classes , columns to properties , and each row is mapped to a resource .",
    "we initially implemented mappings in tarql , sparqlify and tabels ( cf .",
    "section  [ sec : csv - rdf ] ) and ended up preferring tarql because of its good performance and the most flexible mapping language  standard sparql with a few extensions .",
    "as we _ map _ csv@xmath1rdf , as opposed to _ querying _ csv like rdf , we implemented _ construct _ queries , which specify an rdf template in which , for each row of the csv , variables are instantiated with the cell values of given columns .",
    "[ sec : xml - rdf - impl ] to enable easy maintenance of * xml@xmath1rdf * mappings by domain experts , and efficient mapping of large xml inputs , we implemented our own approach .",
    "it employs a sax parser and thus supports streaming .",
    "our mapping language is based on rdf triple templates and on the xpath language for addressing content in xml .",
    "xpath expressions in the subjects or objects of rdf triple templates indicate where in the xml they obtain their values from . to keep xpath expressions simple and intuitive , we allow them to be ambiguous , e.g. , by saying that _ oaf : result / publisher / text ( ) _ ( referring to the text content of the _ publisher _ element of a result ) maps to the _ dcterms : publisher _ property of an _ oav : result _ , and that _ oaf : result / dateofacceptance / text ( ) _ maps to _ dcterms : dateaccepted_. in theory , any combination of _ publisher _ and _ dateofacceptance _ elements would match such a pattern ; however in reality only those nodes that have the shortest distance in the xml document tree represent attributes of the _ same _ openaire entity .",
    "xml filters  @xcite efficiently restrict the xpath expressions to such combinations .",
    "the * time * it takes to transform the complete openaire input data to rdf is the most important performance metric ( requirement  [ req : scalability ] ) .",
    "the * main memory usage * of the transformation process is important because openaire2020 envisages the development of further services sharing the same infrastructure , including deduplication , data mining to measure research impact , classification of publications by machine learning , etc .",
    "one objective metric for * maintainability * is the size of the mapping s source code  after stripping comments and compression , which makes the comparison `` independent of arbitrary factors like lengths of identifiers and amount of whitespace ''  @xcite .",
    "the `` cognitive dimensions of notation '' ( cd ) evaluation framework provides further criteria for systematically assessing the `` usability of information artefacts ''  @xcite .",
    "the following dimensions are straightforward to observe here : _ closeness _ of the notation to the problem ( here : mapping hbase / csv / xml to rdf ) , _ terseness _ ( here measured by code size ; see above ) , _ error - proneness _",
    ", _ progressive evaluation _ ( i.e. whether one can start with an incomplete mapping rule and evolve it to further completeness ) , and _ secondary notation and escape from formalism _ ( e.g. whether reading cues can be given by non - syntactic means such as indentation or comments ) .",
    "the * hbase@xmath1rdf * evaluation ran on a hadoop cluster of 12 worker nodes operated by cnr .",
    "as our * csv@xmath1rdf * and * xml@xmath1rdf * implementations required dependencies not yet installed there , we evaluated them locally : on a virtual machine on a server with an intel xeon e5 - 2690 cpu , having 3.7 gb memory and 250 gb disk space assigned and running linux 3.11 and jdk 1.7 . as we did not have a cluster available , and as the tools employed did not natively support parallelization , we ran the mappings from csv and xml sequentially .",
    "the following table lists our measurements ; further observations follow below .    xlll objective comparison metrics & * hbase * & * csv * & * xml * mapping time(s ) & 1,043 & 4,895 & 45,362 memory ( mb ) & 68,000 & 103 & 130 compressed mapping source code ( kb ) & 4.9 & 2.86 & 1.67 number of input rows / records & 20,985,097 & 203,615,518 & 25,182,730 number of generated rdf triples & 655,328,355 & 654,193,273 & 788,953,122    for * hbase@xmath1rdf * , the peak memory usage of the cluster was 68 gb , i.e. @xmath05.5 gb per worker node . no other mapreduce job was running on the cluster at the same time ; however , the usage figure includes the memory used by the hadoop framework , which schedules and monitors job execution .    the 20 * csv * input files correspond to different entities but also to relationships",
    "this , plus the way multi - valued attributes are represented ( cf .  section  [ sec : input - csv ] ) , causes the high number of input rows .",
    "the size of all files is 33.8 gb . the * xml@xmath1rdf * memory consumption is low because of stream processing .",
    "the time complexity of our mapping approach depends on the number of rules ( here : 118 ) and the size of the input ( here : 144 gb ) . with the complexity of the xml representation , this results in an execution time of more than 12 hours .",
    "the size of the single rdf output file is @xmath091 gb . regarding _ cognitive dimensions _ , the different notations expose the following characteristics ; for lack of space we focus on selected highlights .",
    "_ terseness _ : the high - level csv@xmath1rdf and xml@xmath1rdf languages fare better than the java code required for hbase@xmath1rdf . also , w.r.t . _",
    "closeness _ , they enable more intuitive descriptions of mappings . as the csv@xmath1rdf mappings are based on sparql , which uses the same syntax for rdf triples than the turtle rdf serialization , they look closest to rdf . _",
    "error - proneness _ :",
    "syntactically correct hbase@xmath1rdf java code may still define a semantically wrong mapping . in tarql",
    "s csv@xmath1rdf mappings , many types of syntax and semantics errors can be detected easily .",
    "_ progressive evaluation _ : one can start with an incomplete tarql mapping rule csv@xmath1rdf mapping rule and evolve it towards completeness . _",
    "secondary notation _ : tarql and java support flexible line breaks , indentation and comments , whereas our current xml@xmath1rdf mapping implementation requires one ( possibly long ) line per mapping rule .",
    "overall , this strongly suggests that csv@xmath1rdf is the most maintainable approach .",
    "comparisons of different approaches of mapping data to rdf have mainly been carried out for relational databases as a source  @xcite . similarly to our evaluation criteria , the reference comparison framework of the w3c rdb2rdf incubator group covers mapping creation , representation and accessibility , and support for data integration  @xcite .",
    "hert et al .  compared different rdb2rdf mapping languages w.r.t .",
    "syntactic features and semantic expressiveness  @xcite .    for other linked datasets about research",
    ", we refer to the `` publication '' and `` government '' sectors of the lod cloud , which comprises , e.g. , publication databases such as dblp , as well as snapshots of funding databases such as cordis . from this",
    "it can be seen that openaire is a more comprehensive data source than those published as lod before .",
    "we have mapped a recent snapshot of the openaire data to rdf .",
    "a preliminary dump as well as the definitions of the mappings are available online at http://tinyurl.com/oalod . mapping from hbase is fastest , whereas mapping from csv promises to be most maintainable .",
    "its slower execution time is partly due to the less powerful hardware on which we ran it ; comparing multiple csv@xmath1rdf processes running in parallel to the hbase@xmath1rdf implementation on the cnr hadoop cluster seems promising . based on these findings the openaire2020 lod team will decide on the preferred approach for providing the openaire data as lod ; we will then make the data available for browsing from their openaire entity uris , and for querying via a sparql endpoint .",
    "having implemented almost the whole openaire data model , future steps include interlinking the output with other existing datasets .",
    "e.g. , we so far output countries and languages as strings , whereas dbpedia and lexvo.org are suitable linked open datasets for such terms .",
    "link discovery tools will further enable large - scale linking against existing `` publication '' and `` government '' datasets .",
    "we would like to thank the partners in the openaire2020 project , in particular claudio atzori , alessia bardi , glykeria katsari and paolo manghi , for their help with accessing the openaire data .",
    "this work has been partially funded by the european commission under grant agreement no ."
  ],
  "abstract_text": [
    "<S> openaire , the open access infrastructure for research in europe , comprises a database of all ec fp7 and h2020 funded research projects , including metadata of their results ( publications and datasets ) . </S>",
    "<S> these data are stored in an hbase nosql database , post - processed , and exposed as html for human consumption , and as xml through a web service interface . as an intermediate format to facilitate statistical computations , csv is generated internally . to interlink the openaire data with related data on the web , we aim at exporting them as linked open data ( lod ) . the lod export is required to integrate into the overall data processing workflow , where derived data are regenerated from the base data every day . </S>",
    "<S> we thus faced the challenge of identifying the best - performing conversion approach . </S>",
    "<S> we evaluated the performances of creating lod by a mapreduce job on top of hbase , by mapping the intermediate csv files , and by mapping the xml output . </S>"
  ]
}