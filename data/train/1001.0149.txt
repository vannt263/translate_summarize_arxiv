{
  "article_text": [
    "in this work , we consider the following problem : assume that an unknown symmetric matrix @xmath3 has the structure of a hierarchical matrix ( @xmath4-matrix ) @xcite , that is , certain off - diagonal blocks of @xmath3 are low - rank or approximately low - rank ( see the definitions in sections  [ sec:1dalg ] and [ sec : hmatrix ] ) . the task is to construct @xmath3 efficiently only from a `` black box '' matrix - vector multiplication subroutine ( which shall be referred to as in the following ) . in a slightly more general setting",
    "when @xmath3 is not symmetric , the task is to construct @xmath3 from `` black box '' matrix - vector multiplication subroutines of both @xmath3 and @xmath5 . in this paper , we focus on the case of a symmetric matrix @xmath3 .",
    "the proposed algorithm can be extended to the non - symmetric case in a straightforward way .",
    "our motivation is mainly the situation that @xmath3 is given as the green s function of an elliptic equation . in this case",
    ", it is proved that @xmath3 is an @xmath4-matrix under mild regularity assumptions @xcite . for elliptic equations , methods like preconditioned conjugate gradient , geometric and algebraic multigrid methods ,",
    "sparse direct methods provide application of the matrix @xmath3 on vectors .",
    "the algorithm proposed in this work then provides an efficient way to construct the matrix @xmath3 explicitly in the @xmath4-matrix form .",
    "once we obtain the matrix @xmath3 as an @xmath4-matrix , it is possible to apply @xmath3 on vectors efficiently , since the application of an @xmath4-matrix on a vector is linear scaling .",
    "of course , for elliptic equations , it might be more efficient to use available fast solvers directly to solve the equation , especially if only a few right hand sides are to be solved .",
    "however , sometimes , it would be advantageous to obtain @xmath3 since it is then possible to further compress @xmath3 according to the structure of the data ( the vectors that @xmath3 will be acting on ) , for example as in numerical homogenization @xcite .",
    "another scenario is that the data has special structure like sparsity in the choice of basis , the application of the resulting compressed matrix will be more efficient than the `` black box '' elliptic solver .",
    "let us remark that , in the case of elliptic equations , it is also possible to use the @xmath4-matrix algebra to invert the direct matrix ( which is an @xmath4-matrix in _",
    "e.g. _ finite element discretization ) .",
    "our method , on the other hand , provides an efficient alternative algorithm when a fast matrix - vector multiplication is readily available . from a computational point of view",
    ", what is probably more attractive is that our algorithm facilitates a parallelized construction of the @xmath4-matrix , while the direct inversion has a sequential nature @xcite .    as another motivation",
    ", the purpose of the algorithm is to recover the matrix via a `` black box '' matrix - vector multiplication subroutine",
    ". a general question of this kind will be that under which assumptions of the matrix , one can recover the matrix efficiently by matrix - vector multiplications .",
    "if the unknown matrix is low - rank , the recently developed randomized singular value decomposition algorithms @xcite provide an efficient way to obtain the low - rank approximation through application of the matrix on random vectors .",
    "low - rank matrices play an important role in many applications .",
    "however , the assumption is too strong in many cases that the whole matrix is low - rank .",
    "since the class of @xmath4-matrices is a natural generalization of the one of low - rank matrices , the proposed algorithm can be viewed as a further step in this direction .",
    "a repeatedly leveraged tool in the proposed algorithm is the randomized singular value decomposition algorithm for computing a low rank approximation of a given numerically low - rank matrix .",
    "this has been an active research topic in the past several years with vast literature . for the purpose of this work ,",
    "we have adopted the algorithm developed in @xcite , although other variants of this algorithm with similar ideas can also be used here . for a given matrix @xmath6 that is numerically low - rank",
    ", this algorithm goes as following to compute a rank-@xmath7 factorization .",
    "choose a gaussian random matrix @xmath8 where @xmath9 is a small constant ; form @xmath10 and apply svd to @xmath10 .",
    "the first @xmath7 left singular vectors give @xmath11 ; choose a gaussian random matrix @xmath12 ; form @xmath13 and apply svd to @xmath14 .",
    "the first @xmath7 left singular vectors give @xmath15 ; @xmath16 ( u_2^{{\\mathrm{t } } } r_1)^{\\dagger}$ ] , where @xmath17 denotes the moore - penrose pseudoinverse of matrix @xmath18 @xcite .",
    "the accuracy of this algorithm and its variants has been studied thoroughly by several groups . if the matrix @xmath19-norm is used to measure the error , it is well - known that the best rank-@xmath7 approximation is provided by the singular value decomposition ( svd ) . when the singular values of @xmath6 decay rapidly , it has been shown that algorithm  [ alg : randomsvd ] results in almost optimal factorizations with an overwhelming probability @xcite . as algorithm  [ alg : randomsvd ]",
    "is to be used frequently in our algorithm , we analyze briefly its complexity step by step .",
    "the generation of random numbers is quite efficient , therefore in practice one may ignore the cost of steps @xmath20 and @xmath21 .",
    "step @xmath19 takes @xmath22 of matrix @xmath6 and @xmath23 steps for applying the svd algorithms on an @xmath24 matrix .",
    "the cost of step @xmath25 is the same as the one of step @xmath19 .",
    "step @xmath26 involves the computation of @xmath27 , which takes @xmath28 steps as we have already computed @xmath29 in step @xmath19 . once @xmath27 is ready , the computation of @xmath30 takes additional @xmath31 steps .",
    "therefore , the total complexity of algorithm  [ alg : randomsvd ] is @xmath32 s plus @xmath23 extra steps .",
    "we illustrate the core idea of our algorithm using a simple one - dimensional example .",
    "the algorithm of constructing a hierarchical matrix @xmath3 is a top - down pass .",
    "we assume throughout the article that @xmath3 is symmetric .    for clarity , we will first consider a one dimension example .",
    "the details of the algorithm in two dimensions will be given in section 2 .",
    "we assume that a symmetric matrix @xmath3 has a hierarchical low - rank structure corresponding to a hierarchical dyadic decomposition of the domain .",
    "the matrix @xmath3 is of dimension @xmath33 with @xmath34 for an integer @xmath35 .",
    "denote the set for all indices as @xmath36 , where the former subscript indicates the level and the latter is the index for blocks in each level . at the first level",
    ", the set is partitioned into @xmath37 and @xmath38 , with the assumption that @xmath39 and @xmath40 are numerically low - rank , say of rank @xmath7 for a prescribed error tolerance @xmath41 . at level @xmath42 , each block @xmath43 on the above level is dyadically decomposed into two blocks @xmath44 and @xmath45 with the assumption that @xmath46 and @xmath47 are also numerically low - rank ( with the same rank @xmath7 for the tolerance @xmath41 ) .",
    "clearly , at level @xmath42 , we have in total @xmath48 off - diagonal low - rank blocks .",
    "we stop at level @xmath35 , for which the block @xmath49 only has one index @xmath50 . for simplicity of notation",
    ", we will abbreviate @xmath51 by @xmath52 .",
    "we remark that the assumption that off - diagonal blocks are low - rank matrices may not hold for general elliptic operators in higher dimensions .",
    "however , this assumption simplifies the introduction of the concept of our algorithm .",
    "more realistic case will be discussed in detail in sections  [ sec : peeloff ] and [ sec : peeloffdetails ] .",
    "the overarching strategy of our approach is to peel off the off - diagonal blocks level by level and simultaneously construct their low - rank approximations . on the first level",
    ", @xmath53 is numerically low - rank . in order to use the randomized svd algorithm for @xmath53 ,",
    "we need to know the product of @xmath53 and also @xmath54 with a collection of random vectors .",
    "this can be done by observing that @xmath55 @xmath56 where @xmath57 and @xmath58 are random matrices of dimension @xmath59 .",
    "we obtain @xmath60 by restricting the right hand side of eq .",
    "to @xmath38 and obtain @xmath61 by restricting the right hand side of eq .",
    "to @xmath37 , respectively . the low - rank approximation using algorithm  [ alg : randomsvd ] results in @xmath62 @xmath63 and @xmath64 are @xmath65 matrices and",
    "@xmath66 is an @xmath67 matrix . due to the fact that @xmath3 is symmetric ,",
    "a low - rank approximation of @xmath68 is obtained as the transpose of @xmath53 .    now on the second level , the matrix @xmath3 has the form @xmath69 the submatrices @xmath70 , @xmath71 , @xmath72 , and @xmath73 are numerically low - rank , to obtain their low - rank approximations by the randomized svd algorithm .",
    "similar to the first level , we could apply @xmath3 on random matrices of the form like @xmath74 .",
    "this will require @xmath75 number of matrix - vector multiplications .",
    "however , this is not optimal : since we already know the interaction between @xmath37 and @xmath38 , we could combine the calculations together to reduce the number of matrix - vector multiplications needed .",
    "observe that @xmath76 denote @xmath77 with @xmath78 and @xmath79 the low - rank approximations we constructed on the first level , then @xmath80 therefore , @xmath81 so that we simultaneously obtain @xmath82 and @xmath83 .",
    "similarly , applying @xmath3 on @xmath84 provides @xmath85 and @xmath86 .",
    "we can then obtain the following low - rank approximations by invoking algorithm  [ alg : randomsvd ] .",
    "@xmath87 the low - rank approximations of @xmath71 and @xmath73 are again given by the transposes of the above formulas .",
    "similarly , on the third level , the matrix @xmath3 has the form @xmath88 and define @xmath89 we could simultaneously obtain the product of @xmath90 , @xmath91 , @xmath92 and @xmath93 with random vectors by applying the matrix @xmath3 with random vectors of the form @xmath94 then subtract the product of @xmath95 with the same vectors . again invoking algorithm",
    "[ alg : randomsvd ] provides us the low - rank approximations of these off - diagonal blocks .",
    "the algorithm continues in the same fashion for higher levels .",
    "the combined random tests lead to a constant number of at each level .",
    "as there are @xmath96 levels in total , the total number of matrix - vector multiplications scales logarithmically .",
    "when the block size on a level becomes smaller than the given criteria ( for example , the numerical rank @xmath7 used in the construction ) , one could switch to a deterministic way to get the off - diagonal blocks .",
    "in particular , we stop at a level @xmath97 ( @xmath98 ) such that each @xmath99 contains about @xmath7 entries .",
    "now only the elements in the diagonal blocks @xmath100 need to be determined . this can be completed by applying @xmath3 to the matrix @xmath101 where @xmath102 is the identity matrix whose dimension is equal to the number of indices in @xmath99 .",
    "let us summarize the structure of our algorithm . from the top level to the bottom level , we peel off the numerically low - rank off - diagonal blocks using the randomized svd algorithm .",
    "the matrix - vector multiplications required by the randomized svd algorithms are computed effectively by _ combining _ several random tests into one using the zero pattern of the _ remaining _ matrix . in this way",
    ", we get an efficient algorithm for constructing the hierarchical representation for the matrix @xmath3 .",
    "our algorithm is built on top of the framework of the @xmath4-matrices proposed by hackbusch and his collaborators @xcite .",
    "the definitions of the @xmath4-matrices will be summarized in section  [ sec : algorithm ] . in a nutshell ,",
    "the @xmath4-matrix framework is an operational matrix algebra for efficiently representing , applying , and manipulating discretizations of operators from elliptic partial differential equations .",
    "though we have known how to represent and apply these matrices for quite some time @xcite , it is the contribution of the @xmath4-matrix framework that enables one to manipulate them in a general and coherent way . a closely related matrix algebra is also developed in a more numerical - linear - algebraic viewpoint under the name _ hierarchical semiseparable matrices _ by chandrasekaran , gu , and others @xcite .",
    "here , we will follow the notations of the @xmath4-matrices as our main motivations are from numerical solutions of elliptic pdes .",
    "a basic assumption of our algorithm is the existence of a fast matrix - vector multiplication subroutine .",
    "the most common case is when @xmath3 is the inverse of the stiffness matrix @xmath103 of a general elliptic operator .",
    "since @xmath103 is often sparse , much effort has been devoted to computing @xmath104 by solving the linear system @xmath105 .",
    "many ingenious algorithms have been developed for this purpose in the past forty years .",
    "commonly - seen examples include multifrontal algorithms @xcite , geometric multigrids @xcite , algebraic multigrids ( amg ) @xcite , domain decompositions methods @xcite , wavelet - based fast algorithms @xcite and preconditioned conjugate gradient algorithms ( pcg ) @xcite , to name a few .",
    "very recently , both chandrasekaran _ et al _ @xcite and martinsson @xcite have combined the idea of the multifrontal algorithms with the @xmath4-matrices to obtain highly efficiently direct solvers for @xmath105 .",
    "another common case for which a fast matrix - vector multiplication subroutine is available comes from the boundary integral equations where @xmath3 is often a discretization of a green s function restricted to a domain boundary .",
    "fast algorithms developed for this case include the famous fast multipole method @xcite , the panel clustering method @xcite , and others .",
    "all these fast algorithms mentioned above can be used as the `` black box '' algorithm for our method .    as shown in the previous section ,",
    "our algorithm relies heavily on the randomized singular value decomposition algorithm for constructing the factorizations of the off - diagonal blocks .",
    "this topic has been a highly active research area in the past several years and many different algorithms have been proposed in the literature . here , for our purpose",
    ", we have adopted the algorithm described in @xcite . in a related but slightly different problem ,",
    "the goal is to find low - rank approximations @xmath106 where @xmath107 contains a subset of columns of @xmath6 and @xmath108 contains a subset of rows .",
    "papers devoted to this task include @xcite . in our setting , since we assume no direct access of entries of the matrix @xmath6 but only its impact through matrix - vector multiplications , the algorithm proposed by @xcite is the most relevant choice .",
    "an excellent recent review of this fast growing field can be found in @xcite .    in a recent paper @xcite , martinsson considered also the problem of constructing the @xmath4-matrix representation of a matrix , but he assumed that one can access arbitrary entries of the matrix besides the fast matrix - vector multiplication subroutine . under this extra assumption",
    ", he showed that one can construct the @xmath109 representation of the matrix with @xmath110 matrix - vector multiplications and accesses of @xmath111 matrix entries .",
    "however , in many situations including the case of @xmath3 being the inverse of the stiffness matrix of an elliptic differential operator , accessing entries of @xmath3 is by no means a trivial task . comparing with martinsson s work ,",
    "our algorithm only assumes the existence of a fast matrix - vector multiplication subroutine , and hence is more general .    as we mentioned earlier",
    ", one motivation for computing @xmath3 explicitly is to further compress the matrix @xmath3 .",
    "the most common example in the literature of numerical analysis is the process of numerical homogenization or upscaling @xcite . here",
    "the matrix @xmath3 is often again the inverse of the stiffness matrix @xmath103 of an elliptic partial differential operator .",
    "when @xmath103 contains information from all scales , the standard homogenization techniques fail .",
    "recently , owhadi and zhang @xcite proposed an elegant method that , under the assumption that the cordes condition is satisfied , upscales a general @xmath103 in divergence form using metric transformation .",
    "computationally , their approach involves @xmath112 solves of form @xmath105 with @xmath112 being the dimension of the problem . on the other hand ,",
    "if @xmath3 is computed using our algorithm , one can obtain the upscaled operator by inverting a low - passed and down - sampled version of @xmath3 .",
    "complexity - wise , our algorithm is more costly since it requires @xmath0 solves of @xmath105 .",
    "however , since our approach makes no analytic assumptions about @xmath103 , it is expected to be more general .",
    "we now present the details of our algorithm in two dimensions .",
    "in addition to a top - down construction using the peeling idea presented in the introduction , the complexity will be further reduced using the @xmath109 property of the matrix @xcite .",
    "the extension to three dimensions is straightforward .    in two dimensions , a more conservative partition of the domain",
    "is required to guarantee the low - rankness of the matrix blocks .",
    "we will start with discussion of this new geometric setup .",
    "then we will recall the notion of hierarchical matrices and related algorithms in section  [ sec : hmatrix ] .",
    "the algorithm to construct an @xmath109 representation for a matrix using matrix - vector multiplications will be presented in sections  [ sec : peeloff ] and [ sec : peeloffdetails ] .",
    "finally , variants of the algorithm for constructing the @xmath113 and uniform @xmath113 representations will be described in section  [ sec : peeloffvariants ] .",
    "let us consider an operator @xmath3 defined on a 2d domain @xmath114 with periodic boundary condition .",
    "we discretize the problem using an @xmath115 uniform grid with @xmath116 being a power of @xmath19 : @xmath117 .",
    "denote the set of all grid points as @xmath118 and partition the domain hierarchically into @xmath119 levels ( @xmath120 ) . on each level @xmath42 ( @xmath121 )",
    ", we have @xmath122 boxes denoted by @xmath123 for @xmath124 .",
    "the symbol @xmath125 will also be used to denote the grid points that lies in the box @xmath125 .",
    "the meaning should be clear from the context .",
    "we will also use @xmath126(or @xmath127 ) to denote a general box on certain level @xmath42 .",
    "the subscript @xmath42 will be omitted , when the level is clear from the context .",
    "for a given box @xmath126 for @xmath128 , we call a box @xmath129 on level @xmath130 its parent if @xmath131 .",
    "naturally , @xmath126 is called a child of @xmath129 .",
    "it is clear that each box except those on level @xmath97 will have four children boxes .    for any box @xmath132 on level @xmath42",
    ", it covers @xmath133 grid points . the last level @xmath97",
    "can be chosen so that the leaf box has a constant number of points in it ( _ i.e. _ the difference @xmath134 is kept to be a constant when @xmath116 increases ) .    for simplicity of presentation , we will start the method from level @xmath21 .",
    "it is also possible to start from level @xmath19 .",
    "level @xmath19 needs to be treated specially , as for level @xmath21 .",
    "we define the following notations for a box @xmath132 on level @xmath42 ( @xmath135 ) :    * neighbor list of box @xmath132 .",
    "this list contains the boxes on level @xmath42 that are adjacent to @xmath132 and also @xmath132 itself .",
    "there are @xmath136 boxes in the list for each @xmath132 .",
    "* interaction list of box @xmath132 .",
    "when @xmath137 , this list contains all the boxes on level @xmath21 minus the set of boxes in @xmath138 .",
    "there are @xmath139 boxes in total .",
    "when @xmath140 , this list contains all the boxes on level @xmath42 that are children of boxes in @xmath141 with @xmath142 being @xmath132 s parent minus the set of boxes in @xmath138 .",
    "there are @xmath143 such boxes .",
    "notice that these two lists determine two symmetric relationship : @xmath144 if and only if @xmath145 and @xmath146 if and only if @xmath147 . figs .",
    "[ fig : level3 ] and [ fig : level4 ] illustrate the computational domain and the lists for @xmath148 and @xmath149 , respectively .    .",
    "@xmath150 is the black box .",
    "the neighbor list @xmath151 consists of @xmath152 adjacent light gray boxes and the black box itself , and the interaction list @xmath153 consists of the @xmath139 dark gray boxes.,width=288 ]    .",
    "@xmath154 is the black box .",
    "the neighbor list @xmath155 consists of @xmath152 adjacent light gray boxes and the black box itself , and the interaction list @xmath156 consists of the @xmath143 dark gray boxes.,width=288 ]    for a vector @xmath157 defined on the @xmath158 grid @xmath159 , we define @xmath160 to be the restriction of @xmath157 to grid points @xmath132 . for a matrix @xmath161 that represents a linear map from @xmath159 to itself ,",
    "we define @xmath162 to be the restriction of @xmath3 on @xmath163 .",
    "a matrix @xmath161 has the following decomposition @xmath164 here , for each @xmath42 , @xmath165 incorporates the interaction on level @xmath42 between a box with its interaction list .",
    "more precisely , @xmath165 has a @xmath166 block structure : @xmath167 with @xmath132 and @xmath168 both on level @xmath42 .",
    "the matrix @xmath169 includes the interactions between adjacent boxes at level @xmath97 : @xmath170 with @xmath132 and @xmath168 both on level @xmath97 .",
    "to show that is true , it suffices to prove that for any two boxes @xmath132 and @xmath168 on level @xmath97 , the right hand side gives @xmath171 .",
    "in the case that @xmath172 , this is obvious . otherwise , it is clear that we can find a level @xmath42 , and boxes @xmath173 and @xmath174 on level @xmath42 , such that @xmath175 , @xmath176 and @xmath177 , and hence @xmath171 is given through @xmath178 . throughout the text , we will use @xmath179 to denote the matrix @xmath19-norm of matrix @xmath6 .",
    "our algorithm works with the so - called hierarchical matrices .",
    "we recall in this subsection some basic properties of this type of matrices and also some related algorithms . for simplicity of notations and representation",
    ", we will only work with symmetric matrices .",
    "for a more detailed introduction of the hierarchical matrices and their applications in fast algorithms , we refer the readers to @xcite .",
    "@xmath3 is a ( symmetric ) @xmath113-matrix if for any @xmath180 , there exists @xmath181 such that for any pair @xmath182 with @xmath183 , there exist orthogonal matrices @xmath184 and @xmath185 with @xmath186 columns and matrix @xmath187 such that @xmath188    the main advantage of the @xmath113 matrix is that the application of such matrix on a vector can be efficiently evaluated : within error @xmath189 , one can use @xmath190 , which is low - rank , instead of the original block @xmath162 .",
    "the algorithm is described in algorithm  [ alg : h1matvec ] .",
    "it is standard that the complexity of the matrix - vector multiplication for an @xmath113 matrix is @xmath191 @xcite .",
    "@xmath192 ; @xmath193 ; @xmath194 ;      @xmath3 is a ( symmetric ) uniform @xmath113-matrix if for any @xmath180 , there exists @xmath195 such that for each box @xmath132 , there exists an orthogonal matrix @xmath196 with @xmath197 columns such that for any pair @xmath182 with @xmath183 @xmath198 with @xmath199 .    the application of a uniform @xmath113 matrix to a vector is described in algorithm  [ alg : unifh1matvec ] .",
    "the complexity of the algorithm is still @xmath191 .",
    "however , the prefactor is much better as each @xmath196 is applied only once .",
    "the speedup over algorithm  [ alg : h1matvec ] is roughly @xmath200 @xcite .",
    "@xmath192 ; @xmath201 ; @xmath202 ; @xmath203 ;    @xmath204 ; @xmath194 ;      @xmath3 is an @xmath109 matrix if    * it is a uniform @xmath113 matrix ; * suppose that @xmath205 is any child of a box @xmath132 , then @xmath206 for some matrix @xmath207 .",
    "the application of an @xmath109 matrix to a vector is described in algorithm  [ alg : h2matvec ] and it has a complexity of @xmath208 , notice that , compared with @xmath113 matrix , the logarithmic factor is reduced @xcite .",
    "@xmath192 ; @xmath201 ; @xmath209 ; @xmath210 ; @xmath202 ; @xmath203 ;    @xmath211 ; @xmath212 ; @xmath194 ;    applying an @xmath109 matrix to a vector can indeed be viewed as the matrix form of the fast multipole method ( fmm ) @xcite .",
    "one recognizes in algorithm  [ alg : h2matvec ] that the second top - level * for * loop corresponds to the m2 m ( multipole expansion to multipole expansion ) translations of the fmm ; the third top - level * for * loop is the m2l ( multipole expansion to local expansion ) translations ; and the fourth top - level * for * loop is the l2l ( local expansion to local expansion ) translations .    in the algorithm to be introduced",
    ", we will also need to apply a partial matrix @xmath213 for some @xmath214 to a vector @xmath157 .",
    "this amounts to a variant of algorithm  [ alg : h2matvec ] , described in algorithm  [ alg : partialh2matvec ] .",
    "@xmath192 ; @xmath201 ; @xmath209 ; @xmath210 ; @xmath202 ; @xmath203 ;    @xmath211 ; @xmath212 ;      we assume that @xmath3 is a symmetric @xmath109 matrix and that there exists a fast matrix - vector subroutine for applying @xmath3 to any vector @xmath157 as a `` black box '' .",
    "the goal is to construct an @xmath109 representation of the matrix @xmath3 using only a small number of test vectors .",
    "the basic strategy is a top - down construction : for each level @xmath215 , assume that an @xmath109 representation for @xmath216 is given , we construct @xmath165 by the following three steps :    1 .   _",
    "peeling_. construct an @xmath113 representation for @xmath165 using the peeling idea and the @xmath109 representation for @xmath217 .",
    "_ construct a uniform @xmath113 representation for @xmath165 from its @xmath113 representation .",
    "_ construct an @xmath109 representation for @xmath218 .",
    "the names of these steps will be made clear in the following discussion .",
    "variants of the algorithm that only construct an @xmath113 representation ( a uniform @xmath113 representation , respectively ) of the matrix @xmath3 can be obtained by only doing the peeling step ( the peeling and uniformization steps , respectively ) .",
    "these variants will be discussed in section [ sec : peeloffvariants ] .",
    "after we have the @xmath109 representation for @xmath219 , we use the peeling idea again to extract the diagonal part @xmath169 .",
    "we call this whole process the _ peeling _ algorithm .    before detailing the peeling algorithm , we mention two procedures that serve as essential components of our algorithm .",
    "the first procedure concerns with the uniformization step , in which one needs to get a uniform @xmath113 representation for @xmath165 from its @xmath113 representation , _",
    "i.e. _ , from @xmath220 to @xmath221 , for all pairs of boxes @xmath222 with @xmath223 . to this end",
    ", what we need to do is to find the column space of @xmath224 ,    \\label{eqn : columnspace}\\ ] ] where @xmath225 are the boxes in @xmath226 and @xmath227 .",
    "notice that we weight the singular vectors @xmath228 by @xmath30 , so that the singular vectors corresponding to larger singular values will be more significant .",
    "this column space can be found by the usual svd algorithm or a more effective randomized version presented in algorithm  [ alg : h1tounifh1 ] .",
    "the important left singular vectors are denoted by @xmath196 , and the diagonal matrix formed by the singular values associated with @xmath196 is denoted by @xmath229 .",
    "generate a gaussian random matrix @xmath230 ; form product @xmath231r$ ] and apply svd to it .",
    "the first @xmath197 left singular vectors give @xmath196 , and the corresponding singular values give a diagonal matrix @xmath229 ; @xmath232 ; @xmath233 ;    complexity analysis : for a box @xmath132 on level @xmath42 , the number of grid points in @xmath132 is @xmath234 .",
    "therefore , @xmath235 are all of size @xmath236 and @xmath237 are of size @xmath238 . forming the product @xmath239r$ ]",
    "takes @xmath240 steps and svd takes @xmath241 steps . as there are @xmath242 boxes on level @xmath42 , the overall cost of algorithm  [ alg : h1tounifh1 ] is @xmath243 .",
    "one may also apply to @xmath244 $ ] the deterministic svd algorithm , which has the same order of complexity but with a prefactor about @xmath245 times larger .",
    "the second procedure is concerned with the projection step of the above list , in which one constructs an @xmath109 representation for @xmath246 . here",
    ", we are given the @xmath109 representation for @xmath217 along with the uniform @xmath113 representation for @xmath165 and the goal is to compute the transfer matrix @xmath247 for a box @xmath132 on level @xmath130 and its child @xmath205 on level @xmath42 such that @xmath248 in fact , the existing @xmath249 matrix of the uniform @xmath113 representation may not be rich enough to contain the columns of @xmath250 in its span .",
    "therefore , one needs to update the content of @xmath249 as well . to do that",
    ", we perform a singular value decomposition for the combined matrix @xmath251\\ ] ] and define a matrix @xmath252 to contain @xmath197 left singular vectors .",
    "again @xmath253 should be weighted by the corresponding singular values .",
    "the transfer matrix @xmath247 is then given by @xmath254 and the new @xmath249 is set to be equal to @xmath252 . since @xmath249 has been changed , the matrices @xmath255 for @xmath256 and also the corresponding singular values @xmath257 need to be updated as well .",
    "the details are listed in algorithm  [ alg : unifh1toh2 ] .",
    "form matrix @xmath258 $ ] and apply svd to it .",
    "the first @xmath197 left singular vectors give @xmath252 , and the corresponding singular values give a diagonal matrix @xmath259 ; @xmath260 ; @xmath261 ; @xmath262 ; @xmath263 ;    @xmath264 ;    complexity analysis : the main computational task of algorithm  [ alg : unifh1toh2 ] is again the svd part . for a box @xmath205 on level @xmath42 ,",
    "the number of grid points in @xmath132 is @xmath234 .",
    "therefore , the combined matrix @xmath258 $ ] is of size @xmath265 .",
    "the svd computation clearly takes @xmath266 steps .",
    "taking into the consideration that there are @xmath242 boxes on level @xmath42 gives rise to an @xmath208 estimate for the cost of algorithm  [ alg : unifh1toh2 ] .      with the above preparation",
    ", we are now ready to describe the peeling algorithm in detail at different levels , starting from level @xmath21 . at each level , we follow exactly the three steps listed at the beginning of section  [ sec : peeloff ] .      first in the peeling step , we construct the @xmath113 representation for @xmath267 . for each pair @xmath182 on level @xmath21",
    "such that @xmath183 , we will invoke randomized svd algorithm  [ alg : randomsvd ] to construct the low rank approximation of @xmath268 .",
    "however , in order to apply the algorithm we need to compute @xmath269 and @xmath270 , where @xmath271 and @xmath272 are random matrices with @xmath273 columns . for each box @xmath168 on level @xmath21",
    ", we construct a matrix @xmath108 of size @xmath274 such that @xmath275 computing @xmath276 using @xmath273 s and restricting the result to grid points @xmath183 gives @xmath277 for each @xmath183 .    after repeating these steps for all boxes on level @xmath21",
    ", we hold for any pair @xmath182 with @xmath183 the following data : @xmath278 now , applying algorithm  [ alg : randomsvd ] to them gives the low - rank approximation @xmath279    in the uniformization step , in order to get the uniform @xmath113 representation for @xmath267 , we simply apply algorithm  [ alg : h1tounifh1 ] to the boxes on level @xmath21 to get the approximations @xmath280    finally in the projection step , since we only have @xmath20 level now ( level @xmath21 ) , we have already the @xmath109 representation for @xmath267 .",
    "complexity analysis : the dominant computation is the construction of the @xmath113 representation for @xmath267 .",
    "this requires @xmath281 s for each box @xmath132 on level @xmath21 .",
    "since there are in total @xmath282 boxes at this level , the total cost is @xmath283 s. from the complexity analysis in section  [ sec : peeloff ] , the computation for the second and third steps cost an extra @xmath208 steps .",
    "first in the peeling step , in order to construct the @xmath113 representation for @xmath284 , we need to compute the matrices @xmath285 and @xmath270 for each pair @xmath182 on level @xmath25 with @xmath183 . here",
    "@xmath271 and @xmath272 are again random matrices with @xmath273 columns .",
    "one approach is to follow exactly what we did for level @xmath21 : fix a box @xmath168 at this level , construct @xmath108 of size @xmath286 such that @xmath275 next apply @xmath287 to @xmath108 , by subtracting @xmath276 and @xmath288 .",
    "the former is computed using @xmath273 s and the latter is done by algorithm  [ alg : partialh2matvec ] .",
    "finally , restrict the result to grid points @xmath183 gives @xmath269 for each @xmath183 .",
    "however , we have observed in the simple one - dimensional example in section  [ sec:1dalg ] that random tests can be combined together as in eq .   and .",
    "we shall detail this observation in the more general situation here as following .",
    "observe that @xmath289 , and @xmath290 and @xmath291 for boxes @xmath132 and @xmath168 on level @xmath25 is only nonzero if @xmath292 .",
    "therefore , @xmath293 for @xmath108 coming from @xmath168 can only be nonzero in @xmath141 with @xmath142 being @xmath168 s parent .",
    "the rest is automatically zero ( up to error @xmath41 as @xmath267 is approximated by its @xmath109 representation ) .",
    "therefore , we can combine the calculation of different boxes as long as their non - zero regions do not overlap .    more precisely , we introduce the following sets @xmath294 for @xmath295 with @xmath296 there are @xmath282 sets in total , each consisting of four boxes .",
    "[ fig : level4combined ] illustrates one such set at level @xmath25 . for each set @xmath294 , first construct @xmath108 with @xmath297 then , we apply @xmath287 to @xmath108 , by subtracting @xmath276 and @xmath288 . the former is computed using @xmath273 s and the latter is done by algorithm  [ alg : partialh2matvec ] . for each @xmath298 , restricting the result to @xmath183 gives @xmath285 .",
    "repeating this computation for all sets @xmath294 then provides us with the following data : @xmath299 for each pair @xmath182 with @xmath183 .",
    "applying algorithm  [ alg : randomsvd ] to them gives the required low - rank approximations @xmath300 with @xmath184 orthogonal .     at level @xmath25",
    "this set consists of four black boxes @xmath301 .",
    "the light gray boxes around each black box are in the neighbor list and the dark gray boxes in the interaction list . , width=288 ]    next in the uniformization step , the task is to construct the uniform @xmath113 representation of @xmath284 .",
    "similar to the computation at level @xmath21 , we simply apply algorithm  [ alg : h1tounifh1 ] to the boxes on level @xmath25 to get @xmath280    finally in the projection step , to get @xmath109 representation for @xmath302 , we invoke algorithm  [ alg : unifh1toh2 ] at level @xmath25 .",
    "once it is done , we hold the transfer matrices @xmath247 between any @xmath132 on level @xmath21 and each of its children @xmath205 , along with the updated uniform @xmath113-matrix representation of @xmath284 .    complexity analysis :",
    "the dominant computation is again the construction of @xmath113 representation for @xmath284 . for each group @xmath294",
    ", we apply @xmath3 to @xmath273 vectors and apply @xmath267 to @xmath273 vectors .",
    "the latter takes @xmath208 steps for each application .",
    "since there are @xmath282 sets in total , this computation takes @xmath283 s and @xmath208 extra steps .",
    "first in the peeling step , to construct the @xmath113 representation for @xmath165 , we follow the discussion of level @xmath25 .",
    "define @xmath282 sets @xmath294 for @xmath303 with @xmath304 each set contains exactly @xmath305 boxes .",
    "for each set @xmath294 , construct @xmath108 with @xmath297 next , apply @xmath306 $ ] to @xmath108 , by subtracting @xmath276 and @xmath307r$ ] .",
    "the former is again computed using @xmath273 s and the latter is done by algorithm  [ alg : partialh2matvec ] using the @xmath109 representation of @xmath217 . for each @xmath308 , restricting the result to @xmath183 gives @xmath285 . repeating this computation for all sets",
    "@xmath294 gives the following data for any pair @xmath182 with @xmath223 @xmath309 now applying algorithm  [ alg : randomsvd ] to them gives the low - rank approximation @xmath300 with @xmath184 orthogonal .",
    "similar to the computation at level @xmath25 , the uniformization step that constructs the uniform @xmath113 representation of @xmath165 simply by algorithm  [ alg : h1tounifh1 ] to the boxes on level @xmath42 .",
    "the result gives the approximation @xmath280    finally in the projection step , one needs to compute an @xmath109 representation for @xmath218 . to this end , we apply algorithm  [ alg : unifh1toh2 ] to level @xmath42 .",
    "the complexity analysis at level @xmath42 follows exactly the one of level @xmath25 .",
    "since we still have exactly @xmath282 sets @xmath294 , the computation again takes @xmath283 s along with @xmath208 extra steps .",
    "these three steps ( peeling , uniformization , and projection ) are repeated for each level until we reach level @xmath97 . at this point , we hold the @xmath109 representation for @xmath310",
    ".      finally we construct of the diagonal part @xmath311 more specifically , for each box @xmath168 on level @xmath97 , we need to compute @xmath162 for @xmath172 .",
    "define a matrix @xmath312 of size @xmath313 ( recall that the box @xmath168 on level @xmath97 covers @xmath314 grid points ) by @xmath315 where @xmath102 is the @xmath316 identity matrix .",
    "applying @xmath317 to @xmath312 and restricting the results to @xmath172 gives @xmath162 for @xmath172 .",
    "however , we can do better as @xmath318 is only non - zero in @xmath319 .",
    "hence , one can combine computation of different boxes @xmath168 as long as @xmath319 do not overlap .    to do this ,",
    "define the following @xmath320 sets @xmath294 , @xmath321 @xmath322 for each set @xmath294 , construct matrix @xmath312 by @xmath323 next , apply @xmath317 to @xmath312 . for each @xmath298 , restricting the result to @xmath172 gives @xmath324 .",
    "repeating this computation for all @xmath325 sets @xmath294 gives the diagonal part @xmath169 .",
    "complexity analysis : the dominant computation is for each group @xmath294 apply @xmath3 and @xmath326 to @xmath312 , the former takes @xmath314 s and the latter takes @xmath327 extra steps . recall by the choice of @xmath97 , @xmath328 is a constant .",
    "therefore , the total cost for @xmath325 sets is @xmath329 s and @xmath208 extra steps .",
    "let us now summarize the complexity of the whole peeling algorithm . from the above discussion , it is clear that at each level the algorithm spends @xmath330 s and @xmath208 extra steps .",
    "as there are @xmath331 levels , the overall cost of the peeling algorithm is equal to @xmath331 s plus @xmath332 steps .",
    "it is a natural concern that whether the error from low - rank decompositions on top levels accumulates in the peeling steps .",
    "as observed from numerical examples in section  [ sec : numerical ] , it does not seem to be a problem at least for the examples considered .",
    "we do not have a proof for this though .      in this section ,",
    "we discuss two variants of the peeling algorithm .",
    "let us recall that the above algorithm performs the following three steps at each level @xmath42 .    1 .   _",
    "peeling_. construct an @xmath113 representation for @xmath165 using the peeling idea and the @xmath109 representation for @xmath217 .",
    "_ construct a uniform @xmath113 representation for @xmath165 from its @xmath113 representation .",
    "3 .   _ projection .",
    "_ construct an @xmath109 representation for @xmath218 .    as this algorithm constructs the @xmath109 representation of the matrix @xmath3",
    ", we also refer to it more specifically as the @xmath109 version of the peeling algorithm . in",
    "what follows , we list two simpler versions that are useful in practice    * the @xmath113 version , and * the uniform @xmath113 version .    in the @xmath113 version ,",
    "we only perform the peeling step at each level .",
    "since this version constructs only the @xmath113 representation , it will use the @xmath333 representation of @xmath218 in the computation of @xmath334 within the peeling step at level @xmath335 .    in the uniform @xmath113 version ,",
    "we perform the peeling step and the uniformization step at each level .",
    "this will give us instead the uniform @xmath113 version of the matrix .",
    "accordingly , one needs to use the uniform @xmath333 representation of @xmath336 in the computation of @xmath337 within the peeling step at level @xmath335 .",
    "these two simplified versions are of practical value since there are matrices that are in the @xmath113 or the uniform @xmath113 class but not the @xmath109 class .",
    "a simple calculation shows that these two simplified versions still take @xmath331 s but requires @xmath338 extra steps .",
    "clearly , the number of extra steps is @xmath339 times more expensive than the one of the @xmath109 version .",
    "however , if the fast matrix - vector multiplication subroutine itself takes @xmath332 steps per application , using the @xmath113 or the uniform @xmath113 version does not change the overall asymptotic complexity .    between these two simplified versions ,",
    "the uniform @xmath113 version requires the uniformization step , while the @xmath113 version does not .",
    "this seems to suggest that the uniform @xmath113 version is more expensive .",
    "however , because ( 1 ) our algorithm also utilizes the partially constructed representations for the calculation at future levels and ( 2 ) the uniform @xmath113 representation is much faster to apply , the construction of the uniform @xmath333 version turns out to be much faster .",
    "moreover , since the uniform @xmath113 representation stores one @xmath196 matrix for each box @xmath132 while the @xmath113 version stores about @xmath143 of them , the uniform @xmath113 is much more memory - efficient , which is very important for problems in higher dimensions .",
    "we study the performance of the hierarchical matrix construction algorithm for the inverse of a discretized elliptic operator .",
    "the computational domain is a two dimensional square @xmath114 with periodic boundary condition , discretized as an @xmath158 equispaced grid .",
    "we first consider the operator @xmath340 with @xmath341 being the discretized laplacian operator and the potential being @xmath342 . for all @xmath343 , @xmath344",
    "are independent random numbers uniformly distributed in @xmath345 $ ] .",
    "the potential function @xmath346 is chosen to have this strong randomness in order to show that the existence of @xmath4-matrix representation of the green s function depends weakly on the smoothness of the potential .",
    "the inverse matrix of @xmath103 is denoted by @xmath3 .",
    "the algorithms are implemented using .",
    "all numerical tests are carried out on a single - cpu machine .",
    "we analyze the performance statistics by examining both the cost and the accuracy of our algorithm .",
    "the cost factors include the time cost and the memory cost .",
    "while the memory cost is mainly determined by how the matrix @xmath3 is compressed and does not depend much on the particular implementation , the time cost depends heavily on the performance of subroutine .",
    "therefore , we report both the wall clock time consumption of the algorithm and the number of calls to the subroutine . the subroutine used here is a nested dissection reordered block gauss elimination method @xcite . for an @xmath158 discretization of the computational domain",
    ", this subroutine has a computational cost of @xmath347 steps .",
    "table  [ tab : time ] summarizes the number , and the time cost per degree of freedom ( dof ) for the @xmath113 , the uniform @xmath113 and the @xmath109 representations of the peeling algorithm .",
    "the time cost per dof is defined by the total time cost divided by the number of grid points @xmath348 . for the @xmath113 and the uniform @xmath113 versions , the error criterion @xmath41 in eq .  , eq .   and eq .   are all set to be @xmath349 .",
    "the number of calls to the subroutine is the same in all three cases ( as the peeling step is the same for all cases ) and is reported in the third column of table  [ tab : time ] .",
    "it is confirmed that the number of calls to increases logarithmically with respect to @xmath116 if the domain size at level @xmath97 , i.e. @xmath350 , is fixed as a constant . for a fixed @xmath116 ,",
    "the time cost is not monotonic with respect to @xmath97 .",
    "when @xmath97 is too small the computational cost of @xmath169 becomes dominant .",
    "when @xmath97 is too large , the application of the partial representation @xmath351 to a vector becomes expensive . from the perspective of time cost",
    ", there is an optimal @xmath352 for a fixed @xmath116 .",
    "we find that this optimal level number is the same for @xmath113 , uniform @xmath113 and @xmath109 algorithms .",
    "table  [ tab : time ] shows that @xmath353 for @xmath354 , @xmath355 for @xmath356 , and @xmath357 for @xmath358 .",
    "this suggests that for large @xmath116 , the optimal performance is achieved when the size of boxes on the final level @xmath97 is @xmath359 . in other words , @xmath360 .",
    "the memory cost per dof for the @xmath113 , the uniform @xmath113 and the @xmath109 algorithms is reported in table  [ tab : memory ] .",
    "the memory cost is estimated by summing the sizes of low - rank approximations as well as the size of @xmath169 . for a fixed @xmath116 ,",
    "the memory cost generally decreases as @xmath97 increases .",
    "this is because as @xmath97 increases , an increasing part of the original dense matrix is represented using low - rank approximations .",
    "both table  [ tab : time ] and table  [ tab : memory ] indicate that uniform @xmath113 algorithm is significantly more advantageous than @xmath113 algorithm , while the @xmath109 algorithm leads to a further improvement over the uniform @xmath113 algorithm especially for large @xmath116 .",
    "this fact can be better seen from fig .",
    "[ fig : timememory ] where the time and memory cost per dof for @xmath361 with optimal level number @xmath352 are shown .",
    "we remark that since the number of calls to the subroutine are the same in all cases , the time cost difference comes solely from the efficiency difference of the low rank matrix - vector multiplication subroutines .",
    "we measure the accuracy for the @xmath113 , the uniform @xmath113 and the @xmath109 representations of @xmath3 with its actual value using the operator norm ( @xmath19-norm ) of the error matrix . here",
    ", the @xmath19-norm of a matrix is numerically estimated by power method  @xcite using several random initial guesses .",
    "we report both absolute and relative errors . according to table  [ tab :",
    "error ] , the errors are well controlled with respect to both increasing @xmath116 and @xmath97 , in spite of the more aggressive matrix compression strategy in the uniform @xmath113 and the @xmath109 representations .",
    "moreover , for each box @xmath132 , the rank @xmath362 of the uniform @xmath113 representation is only slightly larger than the rank @xmath363 of the @xmath113 representation .",
    "this can be seen from table  [ tab : rankcomparison ] . here",
    "the average rank for a level @xmath42 is estimated by averaging the values of @xmath362 ( or @xmath363 ) for all low - rank approximations at level @xmath42 .",
    "note that the rank of the @xmath109 representation is comparable to or even lower than the rank in the uniform @xmath113 representation .",
    "this is partially due to different weighting choices in the uniformization step and @xmath109 construction step .    .",
    "numbers and time cost per degree of freedom ( dof ) for the @xmath113 , the uniform @xmath113 and the @xmath109 representations with different grid point per dimension @xmath116 and low rank compression level @xmath97 .",
    "the numbers are by definition the same in the three algorithms . [ cols=\"^,^,^,^,^,^ \" , ]",
    "in this work , we present a novel algorithm for constructing a hierarchical matrix from its matrix - vector multiplication .",
    "one of the main motivations is the construction of the inverse matrix of the stiffness matrix of an elliptic differential operator .",
    "the proposed algorithm utilizes randomized singular value decomposition of low - rank matrices .",
    "the off - diagonal blocks of the hierarchical matrix are computed through a top - down peeling process .",
    "this algorithm is efficient . for an @xmath33 matrix ,",
    "it uses only @xmath0 matrix - vector multiplications plus @xmath364 additional steps .",
    "the algorithm is also friendly to parallelization .",
    "the resulting hierarchical matrix representation can be used as a faster algorithm for matrix - vector multiplications , as well as for numerical homogenization or upscaling .",
    "the performance of our algorithm is tested using two 2d elliptic operators .",
    "the @xmath113 , the uniform @xmath113 and the @xmath109 versions of the proposed algorithms are implemented .",
    "numerical results show that our implementations are efficient and accurate and that the uniform @xmath333 representation is significantly more advantageous over @xmath113 representation in terms of both the time cost and the memory cost , and @xmath109 representation leads to further improvement for large @xmath116 .",
    "although the algorithms presented require only @xmath0 s , the actual number of s can be quite large ( for example , several thousands for the example in section  [ sec : numerical ] ) .",
    "therefore , the algorithms presented here might not be the right choice for many applications .",
    "however , for computational problems in which one needs to invert the same system with a huge of unknowns or for homogenization problems where analytic approaches do not apply , our algorithm does provide an effective alternative .",
    "the current implementation depends explicitly on the geometric partition of the rectangular domain .",
    "however , the idea of our algorithm can be applied to general settings . for problems with unstructured grid ,",
    "the only modification is to partition the unstructured grid with a quadtree structure and the algorithms essentially require no change . for discretizations of the boundary integral operators , the size of an interaction list is typically much smaller as many boxes contain no boundary points .",
    "therefore , it is possible to design a more effective combination strategy with small number of s. these algorithms can also be extended to the 3d cases in a straightforward way , however , we expect the constant to grow significantly .",
    "all these cases will be considered in the future .",
    "l.  l. is partially supported by doe under contract no .",
    "de - fg02 - 03er25587 and by onr under contract no .",
    "n00014 - 01 - 1 - 0674 .",
    "l.  y. is partially supported by an alfred p. sloan research fellowship and an nsf career award dms-0846501 .",
    "the authors thank laurent demanet for providing computing facility and ming gu and gunnar martinsson for helpful discussions .",
    "l.  l. and j.  l. also thank weinan e for support and encouragement .",
    "b.  engquist , o.  runborg , wavelet - based numerical homogenization with applications , in : multiscale and multiresolution methods , vol .",
    "20 of lect .",
    "notes comput .",
    ", springer , berlin , 2002 , pp . 97148 ."
  ],
  "abstract_text": [
    "<S> we develop a hierarchical matrix construction algorithm using matrix - vector multiplications , based on the randomized singular value decomposition of low - rank matrices . </S>",
    "<S> the algorithm uses @xmath0 applications of the matrix on structured random test vectors and @xmath1 extra computational cost , where @xmath2 is the dimension of the unknown matrix . </S>",
    "<S> numerical examples on constructing green s functions for elliptic operators in two dimensions show efficiency and accuracy of the proposed algorithm .    </S>",
    "<S> fast algorithm , hierarchical matrix construction , randomized singular value decomposition , matrix - vector multiplication , elliptic operator , green s function </S>"
  ]
}