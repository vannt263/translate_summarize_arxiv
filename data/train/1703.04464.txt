{
  "article_text": [
    "over the years , the study and characterization of complex systems have become a major research topic in many areas of science  @xcite .",
    "part of this massive interest is due to a common requirement in the modeling and analysis of several natural phenomena existing in the world around us : to understand how relationships between pieces of information give rise to collective behaviors among different scale levels of a system @xcite .",
    "reasons for the appearance of this complexity are countless and are not completely known .",
    "often , in complex systems , the interaction between the components is highly non - linear and/or non - deterministic , which brings several challenges that prevent us from getting a better understanding of the underlying processes that govern the global behavior of such structures @xcite .    with the growing volume of data that is being produced in the world these days ,",
    "the notion of information is more present and relevant in any scale of modern society @xcite . in this scenario , where data plays a central role in science , an essential step in order to learn , understand and assess the rules governing complex phenomena that are part of our world is not only the mining of relevant symbols along this vast ocean of data , but especially the identification and further classification of these patterns @xcite .",
    "after the pieces of information are put together and the relationship between them is somehow uncovered , a clearer picture start to emerge , as in the solution of an intricate puzzle . in this paradigm ,",
    "computational tools for data analysis and simulations are a fundamental component of this data - driven knowledge discovery process @xcite .    in this context ,",
    "random fields are particularly interesting mathematical structures @xcite .",
    "first , it is possible to replace the usual statistical independence assumption by a more realistic conditional independence hypothesis @xcite .",
    "in other words , unlike most classical stochastic models , we can incorporate the dependence between random variables in a formal and elegant way .",
    "this is a key aspect when one needs to study how local interactions can lead to the emergence of global effects .",
    "second , if we constrain the size of the maximum clique to be two , that is , we assume only binary relationships , then we have a pairwise interaction markov model , which is mathematically tractable @xcite . finally , considering that the coupling parameter is invariant and isotropic , all the information regarding the spatial dependence structure of the random field is conveyed by a single scalar parameter , from now on denoted by @xmath0 . in the physics literature ,",
    "this parameter is referred as the inverse temperature of the system , and plays an important role in statistical mechanics and thermodynamics @xcite .",
    "random fields have been used with success in several areas of science from a long time ago @xcite .",
    "recently , information geometry has emerged as an unified approach in the study and characterization of the parametric spaces of random variables by combining knowledge from two distinct mathematical fields : differential geometry and information theory @xcite .",
    "however , most information geometry studies are focused in the classical assumption of independent samples drawn from exponential family of distributions @xcite .",
    "little is known about information geometry on random fields , more precisely , about how the geometric properties of the parametric space of these models are characterized .",
    "although some related work can be found in the literature @xcite , there are still plenty of room for contributions in this field .    along centuries",
    "many researchers have studied the concept of time @xcite . during our investigations ,",
    "some questions that motivated this research were based on the relation between time and complexity : what are the causes to the emergence of complexity in dynamical systems ? is it possible to measure complex behavior along time ?",
    "what is time ? why does time seem to flow in one single direction ? how to characterize time in a complex system ?",
    "we certainly do not have definitive answers to all these questions , but in an attempt to study the effect of time in the emergence of complexity in dynamical systems , this paper proposes to investigate an information - theoretic approach to understand these phenomena in random fields composed by gaussian variables .",
    "our study focuses on the information theory perspective , motivated by the connection between fisher information and the geometric structure of stochastic models , provided by information geometry .",
    "the main goal of this paper is to characterize the information geometry of gaussian random fields , through the derivation of the full metric tensor of the model s parametric space .",
    "basically , we want to sense each component of this riemannian metric @xmath1 as we perform positive and negative displacements in the inverse temperature `` axis '' in order to measure the geometric deformations induced to the underlying manifold ( parametric space ) .",
    "it is known that when the inverse temperature parameter is zero , the model degenerates to a regular gaussian distribution , whose parametric space exhibit constant negative curvature ( hyperbolic geometry ) @xcite .",
    "it is quite intuitive to think that the shape and topology of the parametric space has a deep connection with the distances between random fields operating in different regimes , which is crucial in characterizing the behavior of such systems .",
    "to do so , we propose to investigate how the metric tensor components change while the system navigates through different entropic states .    in summary",
    ", we want to track all the deformations in the metric tensor from an initial configuration a , in which temperature is infinite ( @xmath2 ) , to a final state b , in which temperature is much lower .",
    "additionally , we want to repeat this process of measuring the deformations induced by the metric tensor , but now starting at b and finishing at a. if the sequence of deformations a@xmath3b is different from the sequence of deformations b@xmath3a , it means that the process of taking the random field from an initial lower entropic state a to a final higher entropic state b and bring it back to a induces a natural intrinsic one way direction of evolution : an arrow of time . in practical terms , our proposal consists in using information geometry as a mathematical tool to measure the emergence of an intrinsic notion of time in random fields in which temperature is allowed to deviate from infinity @xcite .    since we are restraining our analysis only to gaussian random fields , which are mathematically tractable ,",
    "exact expressions for the components of the full metric tensor are explicitly derived .",
    "computational simulations using markov - chain monte carlo algorithms @xcite validate our hypothesis that the emergence of an arrow of time in random fields is possibly a consequence of asymmetric deformations in the metric tensor of the statistical manifold when the inverse temperature parameter is disturbed .",
    "however , in searching for a solution to this main problem in question , two major drawbacks have to be overcome : 1 ) the information equality does not hold for @xmath4 , which means that we have two different versions of fisher information ; and 2 ) the computation of the expected fisher information ( the components of the metric tensor ) requires knowledge of the inverse temperature parameter for each configuration of the random field .",
    "the solution for the first sub - problem consists in deriving not one but two possible metric tensors : one using type - i fisher information and another using type - ii fisher information .",
    "for the second sub - problem our solution was to perform maximum pseudo - likelihood estimation in order to accelerate computation by avoiding calculations with the partition function in the joint gibbs distribution .",
    "besides , these two sub - problems share an important connection : it has been verified that the two types of fisher information play a fundamental role in quantifying the uncertainty in the maximum pseudo - likelihood estimation of the inverse temperature parameter @xmath0 through the definition of the asymptotic variance of this estimator @xcite .    in the following ,",
    "we describe a brief outline of the paper . in section 2",
    "we define the pairwise gaussian - markov random field ( gmrf ) model and discuss some basic statistical properties .",
    "in addition , we provide an alternative description of the evolving complex system ( random field ) as a non - deterministic finite automata in which each cell may assume an infinite number of states . in section 3",
    "the complete characterization of the metric tensor of the underlying riemannian manifold in terms of fisher information is detailed .",
    "section 4 discusses maximum pseudo - likelihood , a technique for estimating the inverse temperature parameter given a single snapshot of the random field .",
    "section 5 presents the concept of fisher curve , a geometrical tool to study the evolution of complex systems modelled by random fields by quantifying the deformations induced to the parametric space by the metric tensor .",
    "section 6 shows the computational simulations using markov chain monte carlo ( mcmc ) algorithms , the obtained results and some final remarks .",
    "finally , section 7 presents the conclusions of the paper .",
    "the objective of this section is to introduce the random field model , characterizing some basic statistical properties .",
    "gaussian random fields are important models in dealing with spatially dependent continuous random variables , once they provide a general framework for studying non - linear interactions between elements of a stochastic complex system along time .",
    "one of the main advantages of these models is the mathematical tractability , which allows us to derive exact closed - form expressions for two relevant quantities in this investigation : 1 ) estimators for the inverse temperature parameter ; and 2 ) the expected fisher information matrix ( the riemannian metric of the underlying parametric space manifold ) . according to the hammersley - clifford theorem @xcite , which states the equivalence between gibbs random fields ( global models ) and markov random fields ( local models ) it is possible to characterize an isotropic pairwise gaussian random field by a set of local conditional density functions ( lcdf s ) , avoiding computations with the joint gibbs distribution ( due to the partition function ) .",
    "an isotropic pairwise gaussian markov random field regarding a local neighborhood system @xmath5 defined on a lattice @xmath6 is completely characterized by a set of @xmath7 local conditional density functions @xmath8 , given by :    @xmath9^{2 } \\right\\ }      \\label{eq : gmrf}\\ ] ]    with @xmath10 the parameters vector , where @xmath11 and @xmath12 are respectively the expected value ( mean ) and the variance of the random variables in the field , and @xmath0 is the inverse temperature or coupling parameter , which is responsible for controlling the global spatial dependence structure of the system . note that if @xmath2 , the model degenerates to the usual gaussian model for independent random variables .",
    "a model @xmath13 belongs to the @xmath14 parametric exponential family if it can be expressed as :    @xmath15    where @xmath16 is a vector of natural parameters , @xmath17 is vector of natural sufficient statistics , @xmath18 is an arbitrary function of the parameters and @xmath19 is an arbitrary function of the observations . a model is called curved if the dimensionality @xmath14 of both @xmath20 and @xmath21 ( number of natural sufficient statistics ) is greater than the dimensionality @xmath22 of the parameter vector @xmath23 ( number of parameters in the model ) . for instance , considering a sample @xmath24 of the isotropic pairwise gaussian markov random field model in which @xmath25 denotes the support of the neighborhood system ( i.e , 4 , 8 , 12 , etc . ) , we can express the joint conditional distribution , which is the basis for the definition of the pseudo - likelihood function @xcite , as :    @xmath26 ^ 2   \\right\\ } \\\\",
    "\\nonumber       & = \\left ( 2\\pi\\sigma^2\\right)^{-n/2}exp\\left\\ { -\\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}\\left [ x_{i}^{2 } -2x_{i}\\mu + \\mu^2 - 2\\beta\\sum_{j\\in\\eta_i}(x_{i } - \\mu)(x_{j } - \\mu )   \\right .",
    "\\\\ \\nonumber & \\hspace{6 cm } + \\left .",
    "\\beta^2 \\sum_{j\\in\\eta_i}\\sum_{k\\in\\eta_i}(x_{j } - \\mu)(x_{k } - \\mu)\\right ] \\right\\ } \\\\",
    "\\nonumber      & = exp\\left\\{-\\frac{n}{2}log(2\\pi\\sigma^2 ) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}x_{i}^2 + \\frac{\\mu}{\\sigma^2}\\sum_{i=1}^{n } x_{i } - \\frac{n\\mu^2}{2\\sigma^2 } \\right . \\\\ \\nonumber & \\hspace{4 cm } + \\left . \\frac{\\beta}{\\sigma^2 } \\left [ \\sum_{i=1}^{n}\\sum_{j\\in\\eta_i } x_{i}x_{j } - \\mu \\delta \\sum_{i=1}^{n } x_{i } -\\mu\\sum_{i=1}^{n}\\sum_{j\\in\\eta_i } x_{j } + \\delta \\mu^2 n \\right ] \\right\\ } \\\\",
    "\\nonumber      & \\times   exp\\left\\ { -\\frac{\\beta^2}{2\\sigma^2}\\left [ \\sum_{i=1}^{n } \\sum_{j\\in\\eta_i}\\sum_{k\\in\\eta_i}x_{j}x_{k } - \\mu \\delta \\sum_{i=1}^{n } \\sum_{j\\in\\eta_i}x_{j } - \\mu \\delta \\sum_{i=1}^{n}\\sum_{k\\in\\eta_i}x_{k } + \\delta^2 \\mu^2 n \\right ] \\right\\ } \\\\ \\nonumber \\\\ \\nonumber      & = exp\\left\\ { -\\frac{n}{2}\\left [ log(2\\pi\\sigma^2 ) + \\frac{\\mu^2}{\\sigma^2 } \\right ]",
    "+ \\frac{\\beta\\delta\\mu^2 n}{\\sigma^2}\\left [ 1 - \\frac{\\beta\\delta}{2 } \\right ] \\right\\ } \\\\",
    "\\nonumber       & \\times exp\\left\\ { \\left [ \\frac{\\mu}{\\sigma^2}\\left(1 - \\beta\\delta\\right ) \\right]\\sum_{i=1}^{n}x_{i } -\\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}x_{i}^2 + \\frac{\\beta}{\\sigma^2}\\sum_{i=1}^{n}\\sum_{j\\in\\eta_i}x_{i}x_{j } \\right .",
    "\\\\ \\nonumber & \\left .",
    "\\hspace{4 cm } - \\left [ \\frac{\\beta\\mu}{\\sigma^2}(1 - \\beta\\delta)\\right]\\sum_{i=1}^{n}\\sum_{j\\in\\eta_i}x_{j } - \\frac{\\beta}{2\\sigma^2}\\sum_{i=1}^{n}\\sum_{j\\in\\eta_i}\\sum_{k\\in\\eta_i}x_{j}x_{k }   \\right\\}\\end{aligned}\\ ] ]    by observing the above equation , it is possible to identify the following correspondence :    @xmath27 , -\\frac{1}{2\\sigma^2 } , \\frac{\\beta}{\\sigma^2 } , -\\left [ \\frac{\\beta\\mu}{\\sigma^2}(1 - \\beta\\delta)\\right ] , - \\frac{\\beta}{2\\sigma^2 } \\right ) \\\\ \\nonumber      \\vec{t } = \\left ( \\sum_{i=1}^{n}x_{i } , \\sum_{i=1}^{n}x_{i}^2 , \\sum_{i=1}^{n}\\sum_{j\\in\\eta_i}x_{i}x_{j } , \\sum_{i=1}^{n}\\sum_{j\\in\\eta_i}x_{j } , \\sum_{i=1}^{n}\\sum_{j\\in\\eta_i}\\sum_{k\\in\\eta_i}x_{j}x_{k }   \\right ) \\end{aligned}\\ ] ]    with @xmath28 and    @xmath29 + \\frac{\\beta\\delta\\mu^2 n}{\\sigma^2}\\left [ 1 - \\frac{\\beta\\delta}{2 } \\right]\\ ] ]    note that the model is a member of the curved exponential family , since even though the parametric space is a 3d manifold , the dimensionality of @xmath20 and @xmath21 is more than that ( there is a total of 5 different natural sufficient statistics , more than one for each parameter ) . once again ,",
    "notice that for @xmath2 , the mathematical structure is reduced to the traditional gaussian model where both vectors @xmath20 and @xmath21 are 2 dimensional , perfectly matching the dimension of the parameters vector @xmath30 :    @xmath31 \\right\\}\\ ] ]    where now we have @xmath28 and :    @xmath32\\end{aligned}\\ ] ]    hence , from a geometric perspective , as the inverse temperature parameter in a random field deviates from zero , a complex deformation process transforms the underlying parametric space ( a 2d manifold ) into a completely different structure ( a 3d manifold ) .",
    "it has been shown that the geometric structure of regular exponential family distributions exhibit constant curvature .",
    "it is also known that from an information geometry perspective @xcite , the natural riemannian metric of these probability distribution manifolds is given by the fisher information matrix .",
    "however , little is known about information geometry on more general statistical models , such as random field models . in this paper ,",
    "our primary objective is to study , from an information theory perspective , how changes in the inverse temperature parameter affect the metric tensor of the gaussian markov random field model .",
    "the idea is that by measuring these components ( fisher information ) we are capturing and quantifying an important complex deformation process induced by the metric tensor into the parametric space as temperature is disturbed . our main goal is to investigate how displacements in the inverse temperature parameter direction ( `` @xmath0 axis '' ) affect the metric tensor and as a consequence , the geometry of the parametric space of random fields .",
    "the evolution of a random field from a given initial configuration is a dynamical process that can be viewed as the simulation of a non - deterministic cellular automata in which each cell has a probability to accept a new behavior depending on the behaviors of the neighboring cells in the grid .",
    "essentially , this is what is done by markov chain monte carlo algorithms to perform random walks throughout the state space of a random field model during a sampling process .    in this paper",
    "a cellular automata is considered as a continuous dynamical system defined on a discrete space ( 2d rectangular lattice ) .",
    "the system is governed by local rules defined in terms of the neighborhood of the cells in a way that these laws describe how the cellular automata evolves in time .",
    "a discrete - space cellular automata can be represented as a sextuple @xmath33 , where @xcite :    * @xmath34 is a n - dimensional lattice of the euclidean space @xmath35 , consisting of cells @xmath36 , @xmath37 ; * @xmath38 is a set of states for each cell ( in our model @xmath39 is an infinite continuous set that represents the outcome of a gaussian random variable to express an infinite number of possible behaviors ) ; * an output function @xmath40 maps the state of a cell @xmath36 at a discrete time @xmath41 , denoted by @xmath42 ; * @xmath43 is an initial configuration ( in our model it is a random configuration generated by the outputs of @xmath44 independent gaussian variables ) ; * a neighborhood function @xmath45 yields every cell @xmath36 to a finite sequence @xmath46 so that @xmath47 has @xmath25 distinct cells @xmath48 ( @xmath25 is the support of the neighborhood system ) ; * a transition function @xmath49 describes the rules governing the dynamics of every cell @xmath50 so that : @xmath51    thus , the resulting cellular automata characterization for our particular random field model is given by : @xmath34 is the 2d rectangular lattice , @xmath38 is the real line ( to allow each cell to express an infinite number of possible behaviors ) , an output @xmath52 is performed by sampling from the probability density function of a given cell @xmath36 ( the lcdf of the random field model as given by equation [ eq : gmrf ] ) , the neighborhood function @xmath53 is the usual moore neighborhood ( the 8 nearest neighbors ) and the transition function @xmath54 is defined in terms of the metropolis - hastings acceptance rate .",
    "to do so , let @xmath55 be defined as :    @xmath56    where both @xmath57 and @xmath58 are two different outputs for a cell @xmath59 . in other words ,",
    "@xmath57 and @xmath58 denote two possible values for @xmath60 .",
    "let @xmath61 be the minimum value between 1 and @xmath55 .",
    "then , the transition function is given by :    @xmath62    where the parameter @xmath55 used to compute @xmath63 can be written as :    @xmath64   \\bigg\\}\\end{aligned}\\ ] ]    some observations are important at this point .",
    "first , the rule for the non - deterministic automata can be put in words as : generate a new candidate for the behavior of the cell @xmath59 , compute @xmath55 and accept the new behavior with probability @xmath63 or keep the old behavior with probability @xmath65 .",
    "the crucial part however is the analysis of the transition function in terms of the spatial dependence structure of the random field , controlled by the inverse temperature parameter .",
    "note that , when @xmath66 , the second term of equation ( inside the parenthesis ) vanishes , indicating that the transition function favors behaviors that are similar to the global one , indicated in this model by the expected value or simply the @xmath11 parameter . in this scenario ,",
    "new behaviors are considered probable if they fit the global one . on the other hand ,",
    "when @xmath0 grows significantly , this second term , which is a measure of local adjustment , becomes increasingly relevant to the transition function . in these situations ,",
    "the cells try to adjust their behaviors to the behavior of the nearest neighbors , ignoring the global structure .",
    "figure [ fig : automata ] illustrates two distinct configurations regarding the evolution of a gaussian random field .",
    "the left one corresponds to the initial random configuration in which the inverse temperature parameter @xmath0 is zero .",
    "the right image is the configuration obtained after 200 steps of evolution for @xmath0 starting at zero and with regular and fixed increments of @xmath67 in each iteration .",
    "different colors encode different behaviors for the cells in the grid .",
    "note the major difference between the two scenarios described above .",
    ".,title=\"fig : \" ] .,title=\"fig : \" ]    in summary , our main research goal with this paper is to investigate how changes in the inverse temperature parameter affect the transition function of a non - deterministic cellular automata modeled according to a gaussian random field .",
    "this investigation is focused in the analysis of fisher information , a measure deeply related to the geometry of the underlying random field model s parametric space , since it provides the basic mathematical tool for the definition of the metric tensor ( natural riemannian metric ) of this complex statistical manifold .",
    "in this section , we discuss how information geometry can be applied in the characterization of the statistical manifold of gaussian random fields by the definition of the proper riemannian metric , given by the fisher information matrix .",
    "information geometry has been a relevant research area since the pioneering works of shunichi amari @xcite in the 80 s , developed by the application of theoretical differential geometry methods to the study of mathematical statistics . since then",
    ", this field has been expanded and successfully explored by researchers in a wide range of science areas , from statistical physics and quantum mechanics to game theory and machine learning .",
    "essentially , information geometry can be viewed as a branch of information theory that provides a robust and geometrical treatment to most parametric models in mathematical statistics ( belonging to the exponential family of distributions ) . within this context , it is possible to investigate how two distinct independent random variables from the same parametric model are related in terms of intrinsic geometric features .",
    "for instance , in this framework it is possible to measure distances between two gaussian random variables @xmath68 and @xmath69 .",
    "basically , when we analyse isolated random variables ( that is , they are independent ) , the scenario is extensively known , with the underlying statistical manifolds being completely characterized .",
    "however , little is known about the scenario in which we have several variables interacting with each other ( in other words , the inverse temperature parameter is not null ) . in geometric terms , this imply the emergence of an extra dimension in the statistical manifold , and therefore , in the metric tensor .",
    "we will see in the following subsections that the emergence of this inverse temperature parameter ( @xmath0 ) strongly affects all the components of the original metric tensor .",
    "suppose @xmath70 is a statistical model belonging to the exponential family , where @xmath23 denotes the parameters vector of the model .",
    "then , the collection of all admissible vectors @xmath23 defines the parametric space @xmath71 , which has shown to be a riemannian manifold .",
    "moreover , it has been shown that in the gaussian case , the underlying manifold is a surface with constant negative curvature , defining its geometry as hyperbolic @xcite .",
    "since the parametric space @xmath71 is not an euclidean space , it follows that the manifold is curved .",
    "thus , to make the computation of distances and arc lengths in @xmath71 possible , it is necessary to express an infinitesimal displacement @xmath72 in the manifold in an adaptive or locally way . roughly speaking ,",
    "that is the reason why a manifold must be equipped with a metric tensor , which is the mathematical structure responsible for the definition of inner products in the local tangent spaces . with the metric tensor it is possible to express the square of an infinitesimal displacement in the manifold , @xmath73 , as a function of an infinitesimal displacement in the tangent space , which in case of a 2d manifold is given by a vector",
    "@xmath74 $ ] . assuming a matrix notation we have :    @xmath75    where the matrix of coefficients @xmath76 , @xmath77",
    ", e @xmath78 is the metric tensor .",
    "if the metric tensor is a positive definite matrix , the manifold is is known as riemannian .",
    "note that in the euclidean case , where the metric tensor is the identity matrix ( since the space is flat ) , we have the known pitagorean relation @xmath79 .",
    "since its definition , in the works of sir ronald fisher @xcite , the concept of fisher information has been present in an ubiquitous manner throughout mathematical statistics , playing an important role in several applications , from numerical estimation methods based on the newton - raphson iteration to the definition of lower bounds in unbiased estimation ( cramer - rao lower bound ) .",
    "more recently , with the development of information geometry , another fundamental role of fisher information in statistical models has been discovered : it defines intrinsic geometric properties of the parametric space of a model , by characterizing the metric tensor of the respective manifold . in other words , the fisher information matrix is the natural riemannian metric of the manifold ( parametric space ) , given a statistical model .",
    "roughly speaking , fisher information can be thought as a likelihood analog to entropy , which is often used as a measure of uncertainty , but it is based in probability , not likelihood .",
    "basically , in the context of information theory , fisher information measures the amount of information a random sample conveys about an unknown parameter .",
    "let @xmath80 be a probability density function where @xmath81 is the parametric vector .",
    "the fisher information matrix , which is the natural riemannian metric of the parametric space , is defined as :      it is known from the statistical inference theory that information equality holds for independent observations from the regular exponential family of distributions . in other words , it is possible to compute the expected fisher information matrix of a model by two different but equivalent ways ( since the integration and differentiation operators can be interchangeable ) , defining the condition known as the information equality :    @xmath83 = -e\\left [ \\frac{\\partial^2}{\\partial\\theta_i \\partial\\theta_j } log~p(x ; \\vec{\\theta } ) \\right]\\ ] ]    in this investigation we replace @xmath84 by the local conditional density function of an isotropic pairwise gaussian random field ( equation [ eq : gmrf ] ) . more details on how this lcdf is used to build the pseudo - likelihood function are presented in the next sections of the paper .    however , what we observe is that , given the intrinsic spatial dependence structure of random field models , induced by the existence of an inverse temperature parameter , information equality is not a natural condition . in general ,",
    "when the inverse temperature parameter gradually drifts apart from zero ( temperature deviates from infinity ) , this notion of information `` equilibrium '' fails .",
    "thus , in dealing with random field models , we have to consider two different versions of fisher information , from now on denoted by type - i ( due to the first derivative operator in the log likelihood function ) and type - ii ( due to the second derivative operator ) . eventually ,",
    "when certain conditions are satisfied , these two values of information converge to a unique bound .",
    "one trivial condition for the information equality is to have @xmath85 , which means an infinite temperature ( there is no induced spatial dependence structure since the variables are independent and the model degenerates to a regular exponential family density ) .",
    "therefore , in random fields , these two versions of fisher information play distinct roles , especially in quantifying the uncertainty in the estimation of the inverse temperature parameter , as we will see in future sections .      in this section",
    "we present the derivation of all components of the metric tensor @xmath1 in an isotropic pairwise gaussian markov random field model .",
    "the complete characterization of both versions of the metric tensor , using type - i and type - ii fisher information is discussed in details . for purposes of notation",
    ", we define these tensors as :    @xmath86    and    @xmath87    where @xmath88 is the type - i fisher information matrix and @xmath89 is the type - ii fisher information matrix .      in the following ,",
    "we proceed with the definition of the type - i fisher information matrix .",
    "the first component of @xmath88 is given by :    @xmath90\\ ] ]    where @xmath84 is the replaced by the lcdf of the gaussian random field , given by equation . plugging the equations and computing the derivatives leads to :    @xmath91 ^ 2 \\right\\ } \\label{eq : mu_mu_1 } \\\\",
    "\\nonumber & = \\frac{1}{\\sigma^2}\\left(1 - \\beta\\delta \\right)^2 e\\left\\ { \\frac{1}{\\sigma^2 } \\left [ \\left(x_i - \\mu\\right)^2 - 2\\beta\\sum_{j\\in\\eta_i}\\left ( x_i - \\mu \\right)\\left ( x_j - \\mu \\right ) \\right .",
    "\\\\ \\nonumber & \\hspace{4 cm } \\left .",
    "+ \\beta^2 \\sum_{j\\in\\eta_i}\\sum_{k\\in\\eta_i}\\left ( x_j - \\mu \\right)\\left(x_k - \\mu   \\right ) \\right ] \\right\\ } \\\\ \\nonumber & = \\frac{\\left(1 - \\beta\\delta \\right)^2}{\\sigma^2 } \\left [ 1 - \\frac{1}{\\sigma^2}\\left (   2\\beta\\sum_{j\\in\\eta_i}\\sigma_{ij } - \\beta^2\\sum_{j\\in\\eta_i}\\sum_{k\\in\\eta_i}\\sigma_{jk } \\right ) \\right]\\end{aligned}\\ ] ]    where @xmath25 denotes the support of the neighborhood system ( in our case @xmath92 since we have a second - order system ) , @xmath93 denotes the covariance between the central variable @xmath58 and one of its neighbors @xmath94 and @xmath95 denotes the covariance between two variables @xmath96 and @xmath97 in the neighborhood @xmath98 .",
    "the second component of the @xmath88 metric tensor is :    @xmath99\\ ] ]    which leads to :    @xmath100 ^ 3 \\right\\ } \\\\",
    "\\nonumber & \\hspace{3 cm } -\\frac{(1 - \\beta\\delta)}{2\\sigma^4}e\\left\\ { \\left ( x_i - \\mu \\right ) - \\beta\\sum_{j\\in\\eta_i}\\left(x_j - \\mu \\right ) \\right\\ }     \\end{aligned}\\ ] ]    note that second term of equation is zero since :    @xmath101 - \\beta\\sum_{j\\in\\eta_i}e\\left [ x_j - \\mu\\right ] = 0 - 0 = 0\\ ] ]    the expansion of the first term in leads to :    @xmath102 ^ 3 \\right\\ } & = e\\left [ \\left ( x_i - \\mu \\right)^3 \\right ] \\\\",
    "\\nonumber & - 3\\beta\\sum_{j\\in\\eta_i}e\\left [ ( x_i - \\mu ) ( x_i - \\mu ) ( x_j - \\mu ) \\right ] \\\\",
    "\\nonumber & + 3\\beta^2 \\sum_{j\\in\\eta_i}\\sum_{k\\in\\eta_i}e\\left [ ( x_i - \\mu ) ( x_j - \\mu ) ( x_k - \\mu ) \\right ] \\\\",
    "\\nonumber & - \\beta^3 \\sum_{j\\in\\eta_i}\\sum_{k\\in\\eta_i}\\sum_{l\\in\\eta_i}e\\left ( ( x_j - \\mu ) ( x_k - \\mu ) ( x_l - \\mu ) \\right]\\end{aligned}\\ ] ]    note that the first term of is zero for gaussian random variables since every central moment of odd order is null . according to the isserlis theorem @xcite",
    ", it is trivial to see that in fact all the other terms are null .",
    "therefore , @xmath103 .",
    "we now proceed to the third component of @xmath88 , defined by :    @xmath104\\ ] ]    replacing the equations and manipulating the resulting expressions leads to :    @xmath105   \\\\ \\nonumber & - 2\\beta\\sum_{j\\in\\eta_i}\\sum_{k\\in\\eta_i}e\\left [ ( x_i - \\mu ) ( x_j - \\mu ) ( x_k - \\mu ) \\right ] \\\\",
    "\\nonumber & + \\beta^2 \\sum_{j\\in\\eta_i}\\sum_{k\\in\\eta_i}\\sum_{l\\in\\eta_i}e\\left [ ( x_j - \\mu ) ( x_k - \\mu ) ( x_l - \\mu ) \\right ] \\bigg\\}\\end{aligned}\\ ] ]    once again , all the higher - order moments are a product of an odd number of gaussian random variables so by the isserlis s theorem they all vanish , resulting in @xmath106 . for the next component , @xmath107 , we have :    @xmath108 = 0\\ ] ]    since @xmath103 and changing the order of the product does not affect the expected value .",
    "proceeding to the fifth component of the metric tensor @xmath88 we have to compute :    @xmath109\\ ] ]    which is given by :    @xmath110 ^ 2 \\right\\ } \\\\",
    "\\nonumber & = \\frac{1}{4\\sigma^4 } - \\frac{1}{2\\sigma^6}e\\left\\ { \\left [ ( x_i - \\mu ) - \\beta\\sum_{j\\in\\eta_i}(x_j - \\mu ) \\right]^2 \\right\\ } \\\\ \\nonumber & \\hspace{1 cm } + \\frac{1}{4\\sigma^8}e\\left\\ { \\left [ ( x_i - \\mu ) - \\beta\\sum_{j\\in\\eta_i}(x_j - \\mu ) \\right]^4 \\right\\}\\end{aligned}\\ ] ]    in order to simplify the calculations , we expand each one of the expected values separately .",
    "the first expectation leads to the following equality :    @xmath111 ^ 2 \\right\\ } =   \\sigma^2 - 2\\beta\\sum_{j\\in\\eta_i}\\sigma_{ij } + \\beta^2 \\sum_{j\\in\\eta_i}\\sum_{k\\in\\eta_i}\\sigma_{jk}\\ ] ]    in the expansion of the second expectation term note that :    @xmath112 ^ 4 \\right\\ } = e\\left [ ( x_i - \\mu)^4 \\right ] - 4\\beta\\sum_{j\\in\\eta_i}e\\left [ ( x_i - \\mu)^3 ( x_j - \\mu ) \\right ] \\nonumber \\\\ & \\hspace{3 cm } + 6\\beta^2 \\sum_{j\\in\\eta_i}\\sum_{k\\in\\eta_i}e\\left [ ( x_i - \\mu)^2 ( x_j - \\mu ) ( x_k - \\mu ) \\right ] \\\\",
    "\\nonumber & \\hspace{3 cm } - 4\\beta^3 \\sum_{j\\in\\eta_i}\\sum_{k\\in\\eta_i}\\sum_{l\\in\\eta_i } e\\left[(x_i - \\mu ) ( x_j - \\mu ) ( x_k - \\mu ) ( x_l - \\mu ) \\right ] \\\\ \\nonumber & \\hspace{3 cm } + \\beta^4 \\sum_{j\\in\\eta_i}\\sum_{k\\in\\eta_i}\\sum_{l\\in\\eta_i}\\sum_{m\\in\\eta_i}e\\left[(x_j - \\mu ) ( x_k - \\mu ) ( x_l - \\mu ) ( x_m - \\mu ) \\right]\\end{aligned}\\ ] ]    leading to five novel expectation terms . using the isserlis theorem for gaussian distributed random variables , it is possible to express the higher - order moments as functions of second - order moments .",
    "therefore , after some algebra we have :    @xmath113 + \\frac{1}{\\sigma^8}\\left [ 3\\beta^2 \\sum_{j\\in\\eta_i}\\sum_{k\\in\\eta_i}\\sigma_{ij}\\sigma_{ik } \\right .",
    "\\\\ \\nonumber \\\\ \\nonumber & \\hspace{2 cm } \\left .",
    "- \\beta^3 \\sum_{j\\in\\eta_i}\\sum_{k\\in\\eta_i}\\sum_{l\\in\\eta_i}\\left ( \\sigma_{ij}\\sigma_{kl } + \\sigma_{ik}\\sigma_{jl } + \\sigma_{il}\\sigma_{jk } \\right ) \\right .",
    "\\\\ \\nonumber & \\hspace{2 cm } \\left .",
    "+ \\beta^4 \\sum_{j\\in\\eta_i}\\sum_{k\\in\\eta_i}\\sum_{l\\in\\eta_i}\\sum_{m\\in\\eta_i}\\left ( \\sigma_{jk } \\sigma_{lm } + \\sigma_{jl}\\sigma_{km } + \\sigma_{jm}\\sigma_{kl } \\right )   \\right ] \\end{aligned}\\ ] ]    the next component of the metric tensor is :    @xmath114\\ ] ]    which is given by :    @xmath115 \\times \\right . \\\\ \\nonumber & \\hspace{3 cm } \\left . \\left",
    "[ \\frac{1}{\\sigma^2}\\left((x_i - \\mu ) - \\beta\\sum_{j\\in\\eta_i}(x_j - \\mu )   \\right)\\left ( \\sum_{j\\in\\eta_i}(x_j - \\mu ) \\right ) \\right ] \\right\\ } \\\\",
    "\\nonumber & = -\\frac{1}{2\\sigma^4 } e\\left\\ {   \\left [ ( x_i - \\mu ) - \\beta\\sum_{j\\in\\eta_i}(x_j - \\mu ) \\right]\\left [ \\sum_{j\\in\\eta_i}(x_j - \\mu ) \\right ] \\right\\ } \\\\ \\nonumber & \\hspace{2 cm } + \\frac{1}{2\\sigma^6}e\\left\\{\\left [ ( x_i - \\mu ) - \\beta\\sum_{j\\in\\eta_i}(x_j - \\mu ) \\right]^3 \\left [ \\sum_{j\\in\\eta_i}(x_j - \\mu ) \\right ]   \\right\\}\\end{aligned}\\ ] ]    the first expectation can be simplified to :    @xmath116\\left [ \\sum_{j\\in\\eta_i}(x_j - \\mu ) \\right ] \\right\\ } = \\sum_{j\\in\\eta_i}\\sigma_{ij } - \\beta\\sum_{j\\in\\eta_i}\\sum_{k\\in\\eta_i}\\sigma_{jk}\\ ] ]    the expansion of the second expectation leads to :    @xmath117 ^ 3 \\left [ \\sum_{j\\in\\eta_i}(x_j - \\mu ) \\right ]   \\right\\ } = \\\\ \\nonumber & e\\left\\ { \\left [ \\sum_{j\\in\\eta_i}(x_j - \\mu ) \\right ] \\left [ ( x_i - \\mu)^3 - 3\\beta\\sum_{j\\in\\eta_i}(x_i - \\mu)^2 ( x_j - \\mu ) \\right .",
    ". \\\\ \\nonumber \\\\ \\nonumber & \\hspace{4 cm } \\left .",
    "+ 3\\beta^2 \\sum_{j\\in\\eta_i}\\sum_{k\\in\\eta_i}(x_i - \\mu)(x_j - \\mu)(x_k - \\mu ) \\right .",
    "\\\\ \\nonumber   & \\hspace{5 cm } \\left .",
    "-\\beta^3 \\sum_{j\\in\\eta_i}\\sum_{k\\in\\eta_i}\\sum_{l\\in\\eta_i}(x_j - \\mu)(x_k - \\mu)(x_l - \\mu ) \\right ] \\right\\}\\end{aligned}\\ ] ]    thus , by applying the isserlis equation to compute the higher - order cross moments as functions of second - order moments , and after some algebraic manipulations , we have :    @xmath118 - \\frac{1}{2\\sigma^6}\\left [ 6\\beta\\sum_{j\\in\\eta_i}\\sum_{k\\in\\eta_i}\\sigma_{ij}\\sigma_{ik } \\right . \\\\ \\nonumber \\\\ \\nonumber & \\hspace{3 cm } \\left .",
    "- 3 \\beta^2 \\sum_{j\\in\\eta_i}\\sum_{k\\in\\eta_i}\\sum_{l\\in\\eta_i}\\left ( \\sigma_{ij}\\sigma_{kl } + \\sigma_{ik}\\sigma_{jl } + \\sigma_{il}\\sigma_{jk } \\right ) \\right .",
    "\\\\ \\nonumber & \\hspace{3 cm } \\left .",
    "+ \\beta^3 \\sum_{j\\in\\eta_i}\\sum_{k\\in\\eta_i}\\sum_{l\\in\\eta_i}\\sum_{m\\in\\eta_i } \\left ( \\sigma_{jk}\\sigma_{lm } + \\sigma_{jl}\\sigma_{km } + \\sigma_{jm}\\sigma_{kl } \\right ) \\right]\\end{aligned}\\ ] ]    moving forward to the next components , it is easy to verify that @xmath119 and @xmath120 , since the order of the products in the expectation is irrelevant for the final result . finally , the last component of the metric tensor @xmath88 is defined as :    @xmath121\\ ] ]    which is given by :    @xmath122 ^ 2 \\left [ \\sum_{j\\in\\eta_i}(x_j - \\mu ) \\right]^2",
    "\\right\\ } \\\\",
    "\\nonumber & = \\frac{1}{\\sigma^4 } e\\left\\ { \\left [ ( x_i - \\mu)^2 - 2\\beta \\sum_{j\\in\\eta_i } ( x_i - \\mu)(x_j - \\mu ) + \\beta^2 \\sum_{j\\in\\eta_i}\\sum_{k\\in\\eta_i } ( x_j - \\mu)(x_k - \\mu ) \\right ] \\times \\right . \\\\ \\nonumber & \\left .",
    "\\hspace{5 cm } \\left [ \\sum_{j\\in\\eta_i}\\sum_{k\\in\\eta_i } ( x_j - \\mu)(x_k - \\mu ) \\right ] \\right\\ } \\\\ \\nonumber & = \\frac{1}{\\sigma^4 } e \\left\\ { \\sum_{j\\in\\eta_i}\\sum_{k\\in\\eta_i}(x_i - \\mu)(x_i - \\mu)(x_j - \\mu)(x_k - \\mu ) \\right .",
    "\\\\ \\nonumber & \\hspace{2 cm } \\left . - 2\\beta\\sum_{j\\in\\eta_i}\\sum_{k\\in\\eta_i } \\sum_{l\\in\\eta_i}(x_i - \\mu)(x_j - \\mu)(x_k - \\mu)(x_l - \\mu ) \\right .",
    "\\\\ \\nonumber & \\hspace{3 cm } \\left .",
    "+ \\beta^2 \\sum_{j\\in\\eta_i } \\sum_{k\\in\\eta_i } \\sum_{l\\in\\eta_i } \\sum_{m\\in\\eta_i}(x_j - \\mu)(x_k - \\mu ) ( x_l - \\mu ) ( x_m - \\mu )   \\right\\}\\end{aligned}\\ ] ]    using the isserlis formula and after some algebra , we have :    @xmath123\\end{aligned}\\ ] ]    therefore , we conclude that the type - i fisher information matrix of an isotropic pairwise gaussian random field model has the following structure :    @xmath124    where @xmath125 , @xmath126 , @xmath127 and @xmath128 are the coefficients used to define how we compute an infinitesimal displacement in the manifold ( parametric space ) around the point @xmath129 :    @xmath130    with this we have completely characterized the type - i fisher information matrix of the isotropic pairwise gaussian random field model ( metric tensor for the parametric space ) .",
    "note that , from the structure of the fisher information matrix we see that the parameter @xmath11 is orthogonal to both @xmath131 and @xmath0 . in the following ,",
    "we proceed with the definition of the type - ii fisher information matrix .      in the following ,",
    "we provide a brief discussion based on @xcite about the information equality condition , which is a valid property for several probability density function belonging to the exponential family . for purposes of simplification",
    "we consider the uniparametric case , knowing that the extension to multiparametric models is quite natural .",
    "let @xmath132 be a random variable with a probability density function @xmath133 .",
    "note that :    @xmath134\\ ] ]    by the product rule we have :    @xmath135 = -\\frac{1}{p(x;\\theta)^2}\\left [ \\frac{\\partial}{\\partial\\theta}p(x;\\theta ) \\right]^2 + \\frac{1}{p(x;\\theta)}\\frac{\\partial^2}{\\partial\\theta^2}p(x;\\theta)\\ ] ]    which is leads to    @xmath136 ^ 2 + \\frac{1}{p(x;\\theta)}\\frac{\\partial^2}{\\partial\\theta^2}p(x;\\theta)\\ ] ]    rearranging the terms and applying the expectation operator gives us :    @xmath137 = -e\\left [ \\frac{\\partial^2}{\\partial\\theta^2 } log~p(x;\\theta ) \\right ] + e\\left [ \\frac{1}{p(x;\\theta)}\\frac{\\partial^2}{\\partial\\theta^2}p(x;\\theta ) \\right]\\ ] ]    by the definition of expected value , the previous expression can be rewritten as :    @xmath137 = -e\\left [ \\frac{\\partial^2}{\\partial\\theta^2 } log~p(x;\\theta ) \\right ] + \\int \\frac{\\partial^2}{\\partial\\theta^2 } p(x;\\theta ) dx\\ ] ]    under certain regularity conditions , it is possible to differentiate under the integral sign by interchanging differentiation and integration operators , which implies in :    @xmath138    leading to the information equality condition . according to @xcite , these regularity conditions can fail for two main reasons : 1 ) the density function @xmath139 may not tail off rapidly enough to ensure the convergence of the integral ; 2 ) the range of integration ( the set in @xmath132 for which @xmath139 is non - zero ) may depend on the parameter @xmath140 .",
    "however , note that in the general case the integral defined by equation is exactly the difference between the two types of fisher information , or in a more geometric perspective , between the respective components of the metric tensors @xmath88 and @xmath89 :    @xmath141 - \\\\ \\nonumber   & \\left\\ { - e\\left [ \\frac{\\partial^2}{\\partial\\theta_i \\partial\\theta_j } log~p(x;\\vec{\\theta } ) \\right ] \\right\\ } \\\\ \\nonumber \\\\ \\nonumber & = i_{\\theta_i \\theta_j}^{(1)}(\\vec{\\theta } ) - i_{\\theta_i \\theta_j}^{(2)}(\\vec{\\theta})\\end{aligned}\\ ] ]    we will see in the experiments that these measures ( fisher information ) , more precisely @xmath142 and @xmath143 , play an important role in signaling changes in the system s entropy along an evolution of the random field .      by using the second derivative of the log likelihood function , we can compute an alternate metric tensor , given by the type - ii fisher information matrix .",
    "the first component of the tensor @xmath89 is :    @xmath144\\ ] ]    which is given by :    @xmath145 \\right\\ }   = \\frac{1}{\\sigma^2}\\left ( 1 - \\beta\\delta \\right)^2\\end{aligned}\\ ] ]    where @xmath146 is the size of the neighborhood system .",
    "the second component is defined by :    @xmath147\\ ] ]    resulting in    @xmath148 = \\frac{1}{\\sigma^4}(1 - \\beta\\delta)\\left [ 0 - 0 \\right ] = 0\\end{aligned}\\ ] ]    similarly , the third component of the metric tensor is null , since we have :    @xmath149 \\\\ \\nonumber & = \\frac{1}{\\sigma^2}e\\left\\ { \\delta \\left [ ( x_i - \\mu ) - \\beta\\sum_{j\\in\\eta_i}(x_j - \\mu ) \\right ] + ( 1 - \\beta\\delta)\\left [ \\sum_{j\\in\\eta_i}(x_j - \\mu ) \\right ] \\right\\ } \\\\ \\nonumber & = 0 + 0 = 0\\end{aligned}\\ ] ]    proceeding to the fourth component , it is straightforward to see that @xmath150 , since changing the order of the partial derivative operators is irrelevant to the final result . for now , note that both @xmath151 and @xmath152 are approximations to @xmath153 ( equation [ eq : mu_mu_1 ] ) and @xmath154 ( equation [ eq : sigma_sigma_1 ] ) neglecting quadratic and cubic terms of the inverse of the parameter @xmath131 , respectively .",
    "thus , we proceed directly to the fifth component , given by :    @xmath155 \\\\ \\nonumber & = - e \\left\\ { \\frac{\\partial}{\\partial\\sigma^2}\\left [ -\\frac{1}{2\\sigma^2 } + \\frac{1}{2\\sigma^4 } \\left ( x_i - \\mu -\\beta\\sum_{j\\in\\eta_i}(x_j - \\mu ) \\right)^2 \\right ] \\right\\ } \\\\ \\nonumber & = - e \\left\\ { \\frac{1}{2\\sigma^4 } - \\frac{1}{\\sigma^6}\\left [ ( x_i - \\mu ) - \\beta\\sum_{j\\in\\eta_i}(x_j - \\mu ) \\right]^2 \\right\\ } \\\\",
    "\\nonumber & = \\frac{1}{2\\sigma^4 } - \\frac{1}{\\sigma^6}\\left [ 2\\beta\\sum_{j\\in\\eta_i } \\sigma_{ij } - \\beta^2 \\sum_{j\\in\\eta_i}\\sum_{k\\in\\eta_i}\\sigma_{jk } \\right]\\end{aligned}\\ ] ]    the next component of the metric tensor @xmath89 is :    @xmath156 \\\\ \\nonumber & = - e \\left\\ { \\frac{\\partial}{\\partial\\sigma^2}\\left [ \\frac{1}{\\sigma^2 } \\left ( x_i - \\mu - \\beta\\sum_{j\\in\\eta_i}(x_j - \\mu ) \\right)\\left ( \\sum_{j\\in\\eta_i}(x_j - \\mu ) \\right ) \\right ]   \\right\\ } \\\\ \\nonumber & = \\frac{1}{\\sigma^4}\\left [ \\sum_{j\\in\\eta_i}\\sigma_{ij } - \\beta \\sum_{j\\in\\eta_i}\\sum_{k\\in\\eta_i}\\sigma_{jk } \\right]\\end{aligned}\\ ] ]    which is , again , an approximation to @xmath157 ( equation [ eq : sigma_beta_1 ] ) obtained by discarding higher - order functions of the parameters @xmath131 and @xmath0 .",
    "it is straightforward to see that the next two components of @xmath89 are identical to their symmetric counterparts , that is , @xmath158 and @xmath159 .",
    "finally , we have the last component of the fisher information matrix :    @xmath160\\end{aligned}\\ ] ]    which is given by :    @xmath161 \\right\\ } \\\\",
    "\\nonumber & = \\frac{1}{\\sigma^2}e\\left [ \\left ( \\sum_{j\\in\\eta_i}(x_j - \\mu ) \\right ) \\left ( \\sum_{j\\in\\eta_i}(x_j - \\mu ) \\right ) \\right ] \\\\",
    "\\nonumber & = \\frac{1}{\\sigma^2}\\sum_{j\\in\\eta_i}\\sum_{k\\in\\eta_i}\\sigma_{jk}\\end{aligned}\\ ] ]    once again , note that @xmath162 is an approximation to @xmath163 ( equation [ eq_beta_beta_1 ] ) where higher - order functions of the parameters @xmath131 and @xmath0 are suppressed .",
    "it is clear that the difference between the components of the two metric tensors @xmath88 and @xmath89 is significant when the inverse temperature parameter is not null . on the other hand ,",
    "the global structure of @xmath89 is essentially the same of @xmath88 , implying that the definition of @xmath73 is identical to the previous case , but with different coefficients for @xmath164 , @xmath165 , @xmath166 and @xmath167 .",
    "note also that when the inverse temperature parameter is fixed at zero , both metric tensors converge to :    @xmath168    where @xmath146 is a constant defining the support of the neighborhood system .",
    "this is exactly the fisher information matrix of a traditional gaussian random variable @xmath169 ( excluding the third row and column ) , as it would be expected .      in order to simplify the notations and also to make computations faster , the expressions for the components of the metric tensors @xmath88 and @xmath89 can be rewritten in a matrix - vector form using a tensor notation .",
    "let @xmath170 be the covariance matrix of the random vectors @xmath171 , obtained by lexicographic ordering the local configuration patterns @xmath172 for a snapshot of the system ( a static configuration @xmath173 ) . in this work ,",
    "we choose a second - order neighborhood system , making each local configuration pattern a @xmath174 patch .",
    "thus , since each vector @xmath175 has 9 elements , the resulting covariance matrix @xmath170 is @xmath176 .",
    "let @xmath177 be the sub - matrix of dimensions @xmath178 obtained by removing the central row and central column of @xmath170 ( these elements are the covariances between the central variable @xmath179 and each one of its neighbors @xmath180 ) .",
    "also , let @xmath181  be the vector of dimensions @xmath182 formed by all the elements of the central row of @xmath170 , excluding the middle one ( which denotes the variance of @xmath179 actually ) .",
    "[ fig : cov_matrix ] illustrates the process of decomposing the covariance matrix @xmath170 into the sub - matrix @xmath177 and the vector @xmath181   in an isotropic pairwise gmrf model defined on a second - order neighborhood system ( considering the 8 nearest neighbors ) .     into @xmath177 and @xmath181 on a second - order neighborhood system ( @xmath146 ) .",
    "* by expressing the components of the metric tensors in terms of kronocker products , it is possible to compute fisher information in a efficient way during computational simulations . ]",
    "given the above , we can express the elements of the fisher information matrix in a tensorial form using kronecker products .",
    "the following definitions provide a computationally efficient way to numerically evaluate @xmath88 exploring tensor products .",
    "let an isotropic pairwise gaussian markov random field be defined on a lattice @xmath6 with a neighborhood system @xmath5 of size @xmath25 ( usual choices for @xmath25 are even values : 4 , 8 , 12 , 20 , 24 , ... ) .",
    "assuming that the set @xmath183 denotes the global configuration of the system at iteration @xmath41 , and both @xmath181 and @xmath177 are defined according to figure [ fig : cov_matrix ] , the components of the metric tensor @xmath88 ( fisher information matrix ) can be expressed as :    @xmath184\\ ] ]    @xmath185 \\\\ \\nonumber & + \\frac{1}{\\sigma^8}\\left [ 3\\beta^2 \\left\\| \\vec{\\rho } \\otimes \\vec{\\rho } \\right\\|_{+ } - 3 \\beta^3 \\left\\| \\vec{\\rho } \\otimes \\sigma_{p}^{- } \\right\\|_{+ } + 3\\beta^4 \\left\\| \\sigma_{p}^{- } \\otimes \\sigma_{p}^{- } \\right\\|_{+ }   \\right ] \\nonumber\\end{aligned}\\ ] ]    @xmath186 \\\\ \\nonumber & - \\frac{1}{2\\sigma^6 } \\left [ 6\\beta \\left\\| \\vec{\\rho } \\otimes \\vec{\\rho } \\right\\|_{+ } - 9 \\beta^2 \\left\\| \\vec{\\rho } \\otimes \\sigma_{p}^{- } \\right\\|_{+ } + 3\\beta^3 \\left\\| \\sigma_{p}^{- } \\otimes \\sigma_{p}^{- } \\right\\|_{+ }   \\right ] \\nonumber\\end{aligned}\\ ] ]    @xmath187\\ ] ]    where @xmath188 denotes the summation of all the entries of the vector / matrix @xmath76 ( not to be confused with the norm ) and @xmath189 denotes the kronecker ( tensor ) product .",
    "similarly , we can express the components of the metric tensor @xmath89 in this form .",
    "let an isotropic pairwise gaussian markov random field be defined on a lattice @xmath6 with a neighborhood system @xmath5 of size @xmath25 ( usual choices for @xmath25 are even values : 4 , 8 , 12 , 20 , 24 , ... ) .",
    "assuming that the set @xmath183 denotes the global configuration of the system at iteration @xmath41 , and both @xmath181 and @xmath177 are defined according to figure [ fig : cov_matrix ] , the components of the metric tensor @xmath89 ( fisher information matrix ) can be expressed as :    @xmath190    @xmath191\\ ] ]    @xmath192\\ ] ]    @xmath193    from the above equations it is clear to see that the components of @xmath89 are approximations to the components of @xmath88 , obtained by discarding the higher - order terms ( the cross kronecker products vanish ) .",
    "entropy is one of the most ubiquitous concepts in science , with applications in a large number of research fields . in information theory ,",
    "shannon entropy is the most widely know statistical measure related to a random variable , since it often characterizes a degree of uncertainty about any source of information @xcite .",
    "similarly , in statistical physics , entropy plays an important role in thermodynamics , being a relevant measure in the the study and analysis of complex dynamical systems @xcite . in this paper , we try to understand entropy in a more geometrical perspective , by means of its relation to fisher information .",
    "our definition of entropy in a gaussian random field is done by repeating the same process employed to derive the fisher information matrices . knowing that the entropy of random variable x is defined by the expected value of self - information , given by @xmath194 , we have the following definition .",
    "let a pairwise gmrf be defined on a lattice @xmath6 with a neighborhood system @xmath5 .",
    "assuming that the set of observations @xmath183 denote the global configuration of the system at time @xmath41 , then the entropy @xmath195 for this state @xmath196 is given by :    @xmath197 & = \\frac{1}{2}\\left [ log\\left ( 2\\pi\\sigma^2 \\right ) + 1\\right ] \\\\",
    "\\nonumber & - \\frac{1}{\\sigma^2 } \\left [ \\beta\\sum_{j \\in \\eta_i}\\sigma_{ij } - \\frac{\\beta^2}{2}\\sum_{j \\in \\eta_i}\\sum_{k \\in \\eta_i}\\sigma_{jk } \\right]\\end{aligned}\\ ] ]    note that , for @xmath85 the expression is reduced to the entropy of a simple gaussian random variable , as it would be expected . by using the tensor notation ,",
    "we have :    @xmath198 = h_{g } - \\left [ \\frac{\\beta}{\\sigma^{2}}\\left\\| \\vec{\\rho } \\right\\|_{+ } - \\frac{\\beta^{2}}{2 } i_{\\beta\\beta}^{(2)}(\\vec{\\theta } ) \\right ]      \\label{eq : entropy}\\end{aligned}\\ ] ]    where @xmath199 denotes the entropy of a gaussian random variable with mean @xmath11 and variance @xmath131 , and @xmath200 is a component of the fisher information matrix @xmath89 . in other words ,",
    "entropy is related to fisher information .",
    "we will see in the experimental results that the analysis of fisher information can bring us insights in predicting whether the entropy of the system is increasing or decreasing .",
    "a fundamental step in our simulations is the computation of the fisher information matrix ( metric tensor components ) and entropy , given an output of the random field model .",
    "all these measures are function of the model parameters , more precisely , of the variance and the inverse temperature . in all the experiments conducted in this investigation ,",
    "the gaussian random field parameters @xmath11 and @xmath131 are both estimated by the sample mean and variance , respectively , using the maximum likelihood estimatives . however , maximum likelihood estimation is intractable for the inverse temperature parameter estimation ( @xmath0 ) , due to the existence of the partition function in the joint gibbs distribution .",
    "an alternative , proposed by besag @xcite , is to perform maximum pseudo - likelihood estimation , which is based on the conditional independence principle .",
    "the basic idea with this proposal is to replace the independence assumption by a more flexible conditional independence hypothesis , allowing us to use the local conditional density functions of the random field model in the definition of a likelihood function , called pseudo - likelihood .",
    "it has been shown that maximum likelihood estimators are asymptotically efficient , that is , the uncertainty in the estimation of unknown parameters is minimized . in order to quantify the uncertainty in the estimation of the inverse temperature parameter ,",
    "it is necessary to compute the asymptotic variance of the maximum pseudo - likelihood estimator .",
    "we will see later that the components @xmath201 and @xmath200 of both tensors @xmath88 and @xmath89 are crucial in quantifying this uncertainty .",
    "first , we need to define the pseudo - likelihood function of a random field model .",
    "let an isotropic pairwise markov random field model be defined on a rectangular lattice @xmath6 with a neighborhood system @xmath5 .",
    "assuming that @xmath183 denotes the set corresponding to the observations at a time @xmath41 ( a snapshot of the random field ) , the pseudo - likelihood function of the model is defined by :    @xmath202    where @xmath203 .",
    "the pseudo - likelihood function is the product of the local conditional density functions throughout the field viewed as a function of the model parameters . for an isotropic pairwise gaussian markov random field ,",
    "the pseudo - likelihood function is given by plugging equation into equation :    @xmath204^{2 }      \\label{eq : gmrf_pl}\\ ] ]    by differentiating equation with respect to @xmath0 and properly solving the pseudo - likelihood equation , we obtain the following estimator for the inverse temperature parameter :    @xmath205}{\\displaystyle\\sum_{i=1}^{n}\\left [ \\displaystyle\\sum_{j \\in \\eta_i}\\left ( x_{j } - \\mu   \\right ) \\right]^{2 } }      \\label{eq : betampl}\\ ] ]    assuming that the random field is defined on a retangular 2d lattice where the cardinality of the neighborhood system is fixed ( @xmath25 ) , the maximum pseudo - likelihood estimator for the inverse temperature parameter can be rewritten as :    @xmath206    which means that we can also compute this estimative from the covariance matrix of the configuration patterns . in other words , given a snapshot of the system at an instant @xmath41 , @xmath196 , all the measures we need are based solely in the matrix @xmath170 .",
    "therefore , in terms of information geometry , a sequence of gaussian random field outputs in time can be summarized into a sequence of covariance matrices . in computational terms , it means a huge reduction in the volume of data .    in our computational simulations , we fix initial values for the parameters @xmath11 , @xmath131 and @xmath0 , and at each iteration an infinitesimal displacement in the inverse temperature ( @xmath0 axis ) is performed .",
    "a new random field output is generated for each iteration and in order to avoid any degree of supervision throughout the process of computing the entropy and both fisher information metrics of each configuration , the unknown model parameters are properly estimated from data .",
    "however , in estimating the inverse temperature parameter of random fields via maximum pseudo - likelihood , a relevant question emerges : how to measure the uncertainty in the estimation of @xmath0 ?",
    "is it possible to quantify this uncertainty ?",
    "we will see that both versions of fisher information play a central role in answering this question .",
    "it is known from the statistical inference literature that both maximum likelihood and maximum pseudo - likelihood estimators share an important property : asymptotic normality @xcite .",
    "it is possible , therefore , to characterize their behavior in the limiting case by knowing the asymptotic variance .",
    "a limitation from maximum pseudo - likelihood approach is that there is no result proving that this method is asymptotically efficient ( maximum likelihood estimators have been shown to be asymptotically efficient since in the limiting case their variance reaches the cramer - rao lower bound ) .",
    "it is known that the asymptotic variance of the inverse temperature parameter in an isotropic pairwise gmrf is given by @xcite :    @xmath207 ^ 2 } = \\frac{1}{i_{\\beta\\beta}^{(2)}(\\vec{\\theta } ) } + \\frac{1}{i_{\\beta\\beta}^{(2)}(\\vec{\\theta})^{2}}\\left(i_{\\beta\\beta}^{(1)}(\\vec{\\theta } ) - i_{\\beta\\beta}^{(2)}(\\vec{\\theta } ) \\right)\\ ] ]    showing that in the information equilibrium condition , that is , @xmath208 , we have the traditional cramer - rao lower bound , given by the inverse of the fisher information .",
    "a very simple interpretation of this equation indicates that the uncertainty in the estimation of the inverse temperature parameter is reduced when @xmath201 is minimized and @xmath200 is maximized .",
    "essentially , it means that most local patterns must be aligned to the expected global behavior and , in average , the local likelihood functions should not be flat ( indicating that there is a small number of candidates for @xmath0 ) .      by computing @xmath209 , @xmath210 and @xmath195 , we have access to three important information theoretic measures regarding a global configuration @xmath173 of the random field .",
    "we call the 3d space generated by these 3 measures , the information space . a point in this space",
    "represents the value of that specific component of the metric tensor , @xmath211 , when the system s entropy value is @xmath212 .",
    "this allows us to define the fisher curves of the system .",
    "let an isotropic pairwise gmrf model be defined on a lattice @xmath6 with a neighborhood system @xmath5 and @xmath213 be a sequence of outcomes ( global configurations ) produced by different values of @xmath214 ( inverse temperature parameters ) for which @xmath215 .",
    "the fisher curve from @xmath76 to @xmath77 is defined as the parametric curve @xmath216 that maps each configuration @xmath217 to a point @xmath218 in the information space :    @xmath219    where @xmath220 and @xmath221 denote the @xmath222 components of the metric tensors @xmath88 and @xmath89 , respectively , and @xmath212 denotes the entropy .    the motivation behind the fisher curve is the development of a computational tool for the study and characterization of random fields . basically , the fisher curve of a system is the parametric curve embedded in this information - theoretic space obtained by varying the inverse temperature parameter @xmath0 from an initial value @xmath223 to a final value @xmath224 .",
    "the resulting curve provides a geometrical interpretation about how the random field evolves from a lower entropy configuration a to a higher entropy configuration b ( or vice - versa ) , since the fisher information plays an important role in providing a natural metric to the riemannian manifold of a statistical model @xcite .",
    "we will call the path from a global system configuration a to a global system configuration b as the _ fisher curve _ ( from a to b ) of the system , denoted by @xmath225 . instead of using the notion of time as parameter to build the curve @xmath226",
    ", we parametrize @xmath226 by the inverse temperature parameter @xmath0 . in geometrical terms",
    ", we are trying to measure the deformation in the metric tensor of the stochastic model ( local geometric property ) induced by a displacement in the inverse temperature parameter direction .",
    "we are especially interested in characterizing random fields by measuring and quantifying their behavior as the inverse temperature parameter deviates from zero , that is , when temperature leaves infinity . as mentioned before",
    ", the isotropic pairwise gmrf model belongs to the regular exponential family of distributions when the inverse temperature parameter is zero ( @xmath227 ) . in this case",
    ", it has been shown that the geometric structure , whose natural riemannian metric is given by the fisher information matrix ( metric tensor ) , has constant negative curvature ( hyperbolic geometry ) .",
    "besides , fisher information can be measured by two different but equivalent ways ( information equality ) .    as the inverse temperature increases , the model starts to deviate from this known scenario , and the original riemannian metric does not correctly represents the geometric structure anymore ( since there is an additional parameter ) . the manifold which used to be 2d ( surface )",
    "now slowly is transformed ( deformed ) to a different structure . in other words , as this extra dimension is gradually emerging ( since @xmath0 not null ) , the metric tensor is transformed ( the original @xmath228 fisher information matrix becomes a @xmath174 matrix ) .",
    "we believe that the intrinsic notion of time in the evolution of a random field composed by gaussian variables is caused by the irreversibility of this deformation process , as the results suggest .",
    "in this section , we present some experimental results using computational methods for simulating the dynamics and evolution of gaussian random fields .",
    "all the simulations were performed by applying markov chain monte carlo ( mcmc ) algorithms for the generation of random field outcomes based on the specification of the model parameters . in this paper , we make intensive use of the metropolis - hastings algorithm @xcite , a classic method in the literature .",
    "all the computational implementations are done using the python anaconda platform , which includes several auxiliary packages for scientific computing .",
    "the main objective here is to measure @xmath220 , @xmath221 and @xmath212 along a mcmc simulation in which the inverse temperature parameter @xmath0 is controlled to guide the global system behavior .",
    "initially , @xmath0 is set to @xmath229 , that is , the initial temperature is infinite . in the following ,",
    "@xmath0 is linearly increased , with fixed increments @xmath230 , up to an upper limit @xmath231 .",
    "after that , the exact reverse process is performed , that is , the inverse temperature @xmath0 is linearly decreased using the same fixed increments ( @xmath232 ) all the way down to zero . with this procedure",
    ", we are actually performing a positive displacement followed by a negative displacement along the inverse temperature parameter `` direction '' in the parametric space . by sensing each component of the metric tensor ( fisher information ) at each point",
    ", we are essentially trying to capture the deformation in the geometric structure of the statistical manifold ( parametric space ) throughout the process .",
    "the simulations were performed using the following parameter settings : @xmath233 , @xmath234 ( initial value ) , @xmath235 , @xmath236 , @xmath237 and 1000 iterations . at the end of a single mcmc simulation , 2.1 gb of data is generated , representing 1000 random field configurations of size @xmath238 .",
    "[ fig : gmrf_configs ] shows some samples of the random field during the evolution of the system .",
    "is first increased from zero to 0.5 and then decreased from 0.5 to zero . ]      the goal of this investigation is to analyse the behavior of the metric tensor of the statistical manifold of a gaussian random field by learning everything from data , including the inverse temperature parameter @xmath0 . at each iteration of the simulation , the values of @xmath11 and",
    "@xmath12 are updated by computing the sample mean and sample variance , respectively .",
    "the inverse temperature parameter is updated by computing the maximum pseudo - likelihood estimative .    in order to sense the local geometry of the parametric space during the random field dynamics ,",
    "we have computed the values of all the components of the metric tensor at each iteration of the simulation . since we are dealing with both forms of fisher information ( using the square of the first derivative and the negative of the second derivative ) to investigate the information equality condition , both @xmath88 and @xmath89 tensors",
    "are being estimated .",
    "[ fig : fisher ] shows a comparison between each component of @xmath88 with its corresponding component in @xmath89 along the entire simulation . at this point",
    ", some important aspects must be discussed .",
    "first , these results show that the components @xmath239 , @xmath240 and @xmath241 are practically negligible in comparison to @xmath242 in terms of magnitude .",
    "second , while the differences @xmath243 , @xmath244 and @xmath245 are also negligible , the difference @xmath246 is very significant , especially for larger values of @xmath0 . and",
    "third , note that even though the total displacement in the inverse temperature direction adds up to zero ( since @xmath0 is updated from zero to 0.5 and back ) , @xmath242 is highly asymmetric , which indicates that the deformations induced by the metric tensor to the statistical manifold when entropy is increasing are different than those when entropy is decreasing .",
    "tensor and the red lines represent the components of the @xmath89 tensor .",
    "the first row shows the graphs of @xmath247 versus @xmath248 and @xmath249 versus @xmath250 .",
    "the second row shows the graphs of @xmath251 versus @xmath252 and @xmath201 versus @xmath200 .",
    "note that , from an information geometry perspective , the most relevant component in this geometric deformation process of the statistical manifold is the one regarding the inverse temperature parameter",
    ". two important aspects that must be remarked are : 1 ) there is a large divergence between @xmath201 and @xmath200 , that is , the information equality condition fails when @xmath0 deviates from zero ; 2 ) although the total displacement in the @xmath0 `` axis '' adds up to zero , @xmath201 is highly asymmetric , which indicates that the deformations induced by the metric tensor to the statistical manifold when entropy is increasing are different from those when entropy is decreasing . ]    in practical terms , what happens to the metric tensor can be summarized as : by moving forward @xmath253 units in the @xmath0 `` axis '' we sense an effect that is not always the inverse of the effect produced by a displacement of @xmath254 units in the opposite direction .",
    "in other words , moving towards higher entropy states ( when @xmath0 increases ) is different from moving towards lower entropy states ( when @xmath0 decreases ) .",
    "this effect , which resembles the conceptual idea of a hysteresis phenomenon @xcite , in which the future output of the system depends on its history , is illustrated by a plot of the fisher curve of the random field along the simulation . making a analogy with a concrete example ,",
    "it is like the parametric space were made of a plastic material , that when pressured by a force deforms itself .",
    "however , when the pressure is vanishing , a different deformation process takes place to recover the original shape .",
    "[ fig : fishercurve_beta ] shows the estimated fisher curves @xmath255 for @xmath256 ( the blue curve ) and @xmath257 for @xmath258 ( the red curve ) regarding each component of the metric tensor .",
    "this natural orientation in the information space induces an arrow of time along the evolution of the random field .",
    "in other words , the only way to go from a to b by the red path would be running the simulation backwards .",
    "note , however , that when moving along states whose variation in entropy is negligible ( for example , a state a in the same plane of constant entropy ) the notion of time is not apparent . in other words , it is not possible to know whether we are moving forward or backwards in time , simply because at this point the notion of time is not clear ( time behaves similar to a space - like dimension since it is possible to move in both directions in this information space , once the states a and a are equivalent in terms of entropy , because there is no significant variation of @xmath195 ) . during this period ,",
    "it the perception of the passage of time is not clear , since the deformations induced by the metric tensor into the parametric space ( manifold ) are reversible for opposite displacements in the inverse temperature direction .",
    "note also that , from a differential geometry perspective , the torsion of the curve seems to be related to the divergence between the two types of fisher information .",
    "when @xmath201 diverges from @xmath200 the fisher curve leaves the plane of constant entropy .",
    "the results suggest that the torsion of the curve at a given point could be related to the notion of the passage of time : large values suggest that time seems to be `` running faster '' ( large change in entropy ) while small values suggest the opposite ( if we are moving through a plane of constant entropy then time seems to be `` frozen '' ) .    .",
    "* the parametric curve was built by varying the inverse temperature parameter @xmath0 from @xmath229 ( state a ) to @xmath259 ( state b ) and back .",
    "the results show that moving along different entropic states causes the emergence of a natural orientation in terms of information ( an arrow of time ) .",
    "this behavior resembles the conceptual idea of the phenomenon known as hysteresis . ]",
    "following the same strategy , the fisher curves regarding the remaining components were generated .",
    "fig : fishercurve_mu ] , [ fig : fishercurve_sigma ] and [ fig : fishercurve_sigbeta ] illustrates the obtained results .",
    "note , however , that the notion of time is not captured in these curves . by looking at these measurements",
    "we can not say whether the system is moving forwards or backwards in time , even for large variations on the inverse temperature parameter .",
    "since the fisher curves @xmath225 and @xmath260 are essentially the same , the path from a ( @xmath2 ) to b ( @xmath261 ) is the inverse of the path from b to a.    . *",
    "the parametric curve was built by varying the inverse temperature parameter @xmath0 from @xmath229 ( state a ) to @xmath259 ( state b ) and back . in this case",
    "the arrow of time is not evident since the two curves , @xmath225 and @xmath260 , are essentially the same .",
    "the parametric curve was built by varying the inverse temperature parameter @xmath0 from @xmath229 ( state a ) to @xmath259 ( state b ) and back . in this case",
    "the arrow of time is not evident since the two curves , @xmath225 and @xmath260 , are essentially the same . ]    .",
    "* the parametric curve was built by varying the inverse temperature parameter @xmath0 from @xmath229 ( state a ) to @xmath259 ( state b ) and back .",
    "once again , in this case the arrow of time is not evident since the two curves , @xmath225 and @xmath260 , are essentially the same . ]",
    "this section describes the main results obtained in this paper , focusing on the interpretation of the proposed mathematical model of hysteresis for the study of complex systems : the fisher curve of a random field .",
    "basically , when temperature is infinite ( @xmath2 ) entropy fluctuates around a minimum base value and the information equality prevails . from an information geometry perspective",
    ", a reduction in temperature ( increase in @xmath0 ) causes a series of changes in the geometry of the parametric space , since the metric tensor ( fisher information matrix ) is drastically deformed in an apparently non - reversible way , inducing the emergence of a natural orientation of evolution ( arrow of time ) .    by quantifying and measuring an arrow of time in random fields , a relevant aspect that naturally arises concerns the notions of past and future .",
    "suppose the random field is now in a state a , moving towards an increase in entropy ( that is , @xmath0 is increasing ) . within this context , the analysis of the fisher curves suggests a possible interpretation : past is a notion related to a set of states @xmath262 whose entropies are below the current entropic plane of the state a. equivalently , the notion of past could also be related to a set of states @xmath263 whose entropies are above the current entropic plane , provided the random field is moving towards a lower entropy state .",
    "again , let us suppose the random field is in a state a and moving towards an increase in entropy ( @xmath0 is increasing ) .",
    "similarly , the notion of future refers to a set of states @xmath264 whose entropies are higher than the entropy of the current state a ( or equivalently , future could also refer to the set of states @xmath265 whose entropies are lower than a , provided that the random field is moving towards a decrease in entropy ) . according to this possible interpretation , the notion of future",
    "is related to the direction of the movement , pointed by the tangent vector at a given point of the fisher curve . if along the evolution of the random field",
    "there is no significant change in the system s entropy , then time behaves similar to a spatial dimension , as illustrated by fig .",
    "[ fig : past_future ] .",
    "in this paper , we addressed the problem of characterizing the emergence of an arrow of time in gaussian random field models . to intrinsically investigate the effect of the passage of time , we performed computational simulations in which the inverse temperature parameter is controlled to guide the system behavior throughout different entropic states .",
    "investigations about the relation between two important information theoretic measures , entropy and fisher information , led us to the definition of the fisher curve of a random field , a parametric trajectory embbeded in an information space , which characterizes the system behavior in terms of variations in the metric tensor of the statistical manifold .",
    "basically , this curve provides a geometrical tool for the analysis of random fields by showing how different entropic states are `` linked '' in terms of fisher information , which is , by definition , the metric tensor of the underlying random field model parametric space .",
    "in other words , when the random field moves along different entropic states , its parametric space is actually being deformed by changes that happen in fisher information matrix ( the metric tensor ) . in this scientific investigation",
    "we observe what happens to this geometric structure when the inverse temperature parameter is modified , that is , when temperature deviates from infinity , by measuring both entropy and fisher information .",
    "an indirect subproblem involved in the solution of this main problem was the estimation of the inverse temperature parameter of a random field , given an outcome ( snapshot ) of the system . to tackle this subproblem",
    ", we used a statistical approach known as maximum pseudo - likelihood estimation , which is especially suitable for random fields , since it avoids computations with the joint gibbs distribution , often computationally intractable .",
    "our obtained results show that moving towards higher entropy states is different from moving towards lower entropy states , since the fisher curves are not the same .",
    "this asymmetry induces a natural orientation to the process of taking the random field from an initial state a to a final state b and back , which is basically the direction pointed by the arrow of time , since the only way to move in the opposite direction is by running the simulations backwards . in this context",
    ", the fisher curve can be considered a mathematical model of hysteresis in which the natural orientation is given by the arrow of time .",
    "future works may include the study of the fisher curve in other random field models , such as the ising and q - state potts models .",
    "haddad wm .",
    "temporal asymmetry , entropic irreversibility , and finite - time thermodynamics : from parmenides - einstein time - reversal symmetry to the heraclitan entropic arrow of time .",
    "2012;14(3):407455 .",
    "levada alm .",
    "learning from complex systems : on the roles of entropy and fisher information in pairwise isotropic gaussian markov random fields .",
    "entropy , special issue on information geometry .",
    ".              roberts go .",
    "markov chain concepts related to sampling algorithms . in : gilks wr , richardson s , spiegelhalter dj , editors .",
    "markov chain monte carlo in practice ( edited by gilks , w. r. , richardson , s. and spiegelhalter , d. j. ) .",
    "chapman & hall / crc ; 1996 ."
  ],
  "abstract_text": [
    "<S> random fields are useful mathematical objects in the characterization of non - deterministic complex systems . </S>",
    "<S> a fundamental issue in the evolution of dynamical systems is how intrinsic properties of such structures change in time . in this paper </S>",
    "<S> , we propose to quantify how changes in the spatial dependence structure affect the riemannian metric tensor that equips the model s parametric space . </S>",
    "<S> defining fisher curves , we measure the variations in each component of the metric tensor when visiting different entropic states of the system . </S>",
    "<S> simulations show that the geometric deformations induced by the metric tensor in case of a decrease in the inverse temperature are not reversible for an increase of the same amount , provided there is significant variation in the system s entropy : the process of taking a system from a lower entropy state a to a higher entropy state b and then bringing it back to a , induces a natural intrinsic one - way direction of evolution . in this context , </S>",
    "<S> fisher curves resemble mathematical models of hysteresis in which the natural orientation is pointed by an arrow of time . </S>"
  ]
}