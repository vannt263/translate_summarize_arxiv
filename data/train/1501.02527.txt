{
  "article_text": [
    "cities are often defined by their domineering cultural characteristics .",
    "however , particular cities themselves harbor a large variety of different cultural districts . streets separated by just a few blocks may give very different impressions .",
    "these implicit boundaries and classifications are not documented on official maps , and usually are only learned with much time and experience living in a particular city .",
    "+      we believe that having a sense of these districts is valuable to a much wider population .",
    "some examples are : + 1 .",
    "new businesses : for business - owner or entrepreneur looking to open a new restaurant or expand to a different location , knowing which areas of a city harbor restaurants very similar to or different than that particular business is doubtless a valuable insight .",
    "newcomers : for tourists , people moving in , or anyone else new to the city , it is often an arduous and daunting task to get a sense of things such as where they are most likely to find a good thai restaurant , _ the _ block to go for dim sum , or the best area for a dressy , upscale dinner with good wine .",
    "anyone looking to explore : even people who have already have a sense of the city can be surprised by a hole - in - the - wall cafe or undiscovered area .",
    "the lda model we describe can identify the most highly weighted classification for a particular area as well as secondary classifications .",
    "this allows it to uncover more hidden characteristics of a particular area besides the most highly weighted .",
    "this is interesting information in itself , but can also be used as a backbone of a recommendation system .",
    "if a person really enjoys a particular area of town , this model could discover and rank other non - obvious areas that share similar traits .",
    "+ from a cognitive science point of view , we think trying to model these questions is an interesting experiment to test the accuracy of methods like lda and probabilistic mixture models to model human cognition .",
    "recent cognitive science research has had major successes in probabilistic generative models of human cognition [ 12 , 13 ] .",
    "specifically , research by tenenbaum shows strong support for bayesian concept learning [ 14 ] and sanborn et .",
    "use dirichlet process mixture models for category learning that emulates human learning [ 15 ] .",
    "using techniques like these in this paper , we try to recreate the kind of map a local might build up in their head over time of the different subsections of their city .",
    "+      the yelp academic dataset was released in 2013 and has grown to include over 42,000 businesses with over 1 million reviews [ 9 ] .",
    "the dataset has been used in academic papers for sentiment analysis , word layout systems , and recommendation engines , among other research areas .",
    "the quality and sheer size of the dataset is of high value to our research and its natural language user reviews are pivotal to our cultural detection and classification system .",
    "latent dirichlet allocation ( lda ) , first introduced by blei et .",
    "al . in 2003",
    "[ 1 ] , has been applied to numerous and diverse fields : from computer vision [ 2,3 ] to recommendation systems [ 4 ] to spam filtering [ 5 ] .",
    "lda hypothesizes that a collection of documents @xmath0 can be treated as a `` bag of words '' where each document d is generated by the following process , given hyperparameters @xmath1 and @xmath2 : +    1 .",
    "assume each topic @xmath3 has a fixed distribution over all words in d that is @xmath4 2 .",
    "choose the document s topic distribution @xmath5 3 .",
    "to generate each word w : + a. choose a topic @xmath6 from @xmath7 + b. choose a word @xmath8 from @xmath9 +    using this model , lda is able to learn the topic mixtures , @xmath10 , for the documents on which it is trained , in an unsupervised manner . +      to implement lda , we used tools from the python library gensim , which provides functionality to analyze semantic structure in texts [ 6 ] . based off of the results of the expectation maximization algorithm used by huang et .",
    "al . [ 7 ] to determine the optimal number of topics for yelp restaurant reviews in phoenix , we chose @xmath11 = 50 as the number of topics to extract .",
    "we used hyperparameters @xmath1 and @xmath2 with symmetric 1.0/k priors .",
    "+ we cleaned the reviews to remove punctuation , numbers , and a list of stopwords made up of the  english stop words \" list in the scikit - learn python library [ 11 ] . additionally , we specified that after this initial cleaning , the model should only consider the 40,000 median frequency words .",
    "this eliminated words that only appeared a handful of times , as well as generic food - related words that appeared many times .",
    "these words provide little information gain and removing them dramatically increased the convergence time of our lda training . + we trained the model on all restaurant reviews ( around 1.1 million ) from las vegas .",
    "training uses the online inference algorithm described by hoffman et .",
    "al . [ 8 ] and results in an lda topic model object that can be queried with new , unseen documents to return an optimal topic distribution .",
    "we used our model to predict topic weights for each restaurant .",
    "in addition , the model contains the static word distributions @xmath12 for each topic .",
    "our lda model produced 50 topics .",
    "each topic is a collection of word - weight couples .",
    "words with high corresponding weight values are most representative of the topic .",
    "the topic word weights are normalized such that @xmath13 .",
    "table i is a small sampling of selected topics our model generated .",
    "table i displays the topic # , the label we chose for the topic based on its word distribution , and the word distribution .",
    "the full list of topics and their weights can be viewed in the appendix .",
    "+     2 & mexican food & 0.043*tacos + 0.037*taco + 0.026*asada + 0.024*carne + 0.024*mexican + 0.019*burrito + 0.010*salsa + 0.009*fries + 0.009*beans + 0.008*roberto s + 7 & night club & 0.013*music + 0.012*fun + 0.009*club + 0.007*cool + 0.006*party + 0.006*lounge + 0.005*group + 0.005*floor + 0.004*dance + 0.004*girls + 45 & casino & 0.027*hotel + 0.022*casino + 0.020*room + 0.010*stay + 0.009*downtown + 0.006*staying + 0.006*pool + 0.005*street + 0.005*stayed + 0.005*fremont +     topic + pho vietnam restaurant & 4 , 15 & pho , vietnamese , rolls , broth + myxx hookah lounge & 7 , 15 & music , fun , club , cool , party + romano s macaroni grill & 24 , 30 & pasta , italian , bread , server +      we used our trained lda model to predict topic distributions for each of the 3855 restaurants .",
    "a restaurant s topic distribution is a collection of coupled topic numbers and corresponding weights .",
    "topics with high corresponding weight values are most representative of the restaurant .",
    "these topic distribution weights are normalized such that @xmath14 . a sampling of topic predictions is shown in table ii .",
    "we tested several clustering methods in order to group restaurants into appropriate clusters .",
    "we assume that culinary districts in a city are characterized by closeness and similarity of restaurants . in our model ,",
    "therefore , we represent each restaurant as a combined vector of its coordinate position and its lda assigned topic weight distribution .",
    "this vector has 52 dimensions , 2 of which represent the spatial location of the restaurant , and 50 of which represent the restaurant s lda topic weights .",
    "+ 1 . scaling procedure + since the spatial coordinates and topic weights are measured in different spaces , their values are on different scales . to prevent our results being arbitrarily skewed by these different units of scale",
    ", we used a scaling procedure , multiplying the topic weight distributions by a constant @xmath15 .",
    "+ by varying @xmath15 , we can give the topic weights more or less influence over the clustering .",
    "when @xmath16 , the clustering is equivalent to clustering based only on location . as s increases , topic weights are given more control over the clustering .",
    "when @xmath17 , the clustering is done purely by topic similarity . +",
    "our goal was to find an @xmath15 such that close - together clusters of restaurants would be grouped into a single cluster , and points on the outer edges of these clusters would identify themselves with the cluster that best matched their topic distribution . in this way , we allow for a chinese restaurant to escape a nearby cluster of italian restaurants . +",
    "since we are not using pure spatial features , our clustering may result in some clusters overlapping and interweaving .",
    "this allows our model to be representative of the real world of cultural mixing and fuzzy cultural boundaries .",
    "+ to determine a reasonable scaling factor @xmath15 , we constructed a plausible scenario . a chinese restaurant ( @xmath18 )",
    "lies betwen two clusters - a primarily italian cluster ( @xmath19 ) and a primarily chinese cluster ( @xmath20 ) .",
    "@xmath18 is 0.25 mi from @xmath19 s center and 0.75 mi from @xmath20 s center .",
    "we want to choose an s such that dist(@xmath18,@xmath20 ) @xmath21 dist(@xmath18 , @xmath19 ) : that is , we want the chinese restaurant to be classified into the  chinese restaurant \" cluster despite it being closer in spatial coordinates to the italian cluster s center .",
    "the @xmath22 function is a euclidean distance metric between the two vectors .",
    "let s assume @xmath20 and @xmath18 share the same topic distributions , and that @xmath18 and @xmath19 share zero topics in their distributions .",
    "we define @xmath23 as    @xmath24    so our goal is to find an s such that :    @xmath25    @xmath26    @xmath27    this calculation of s , however , assumes topic distributions are all - or - none , when in fact most restaurants are a mixture of a few topics .",
    "in fact , we determined the mean # of topics assignments a restaurant received to be 5 .",
    "we found a typical restaurant to have 1 dominating topic comprising of at least 0.5 of the weight and 4 subtopics comprising of the rest of the weight .",
    "we performed a more advanced analysis of the same scenario and found @xmath28 .",
    "the analysis can be found in equation 10 of the appendix .",
    "normalization + since some topics are inherently more common than other topics due to the high prevalence of some restaurant types , we wanted to avoid our model becoming unfairly skewed by very common topics such as a  pizza \" topic .",
    "there are 355 pizza restaurants in las vegas , comprising of 9.2% of all las vegas restaurants . to avoid the scenario where all clusters are labeled as  pizza \" simply because of the uniformly large number of these restaurants across all clusters , we vertically normalize the topic weights for each restaurant .",
    "we define @xmath29 to be a 50 dimensional topic weight vector of a restaurant , and @xmath30 to be the number of restaurants .",
    "we define    @xmath31    where @xmath32    this normalization can be thought of as dividing out the background of a city s restaurant distribution , ensuring clusters will be dominated by notable exceptions to the average : we do nt want to point out that pizza restaurants are pretty much evenly distributed in high quantities all around vegas , but rather discover when they , or another type of restaurant , are appear in _ notably _ high quantities .",
    "we then horizontally re - normalize each topic vector so that the values remain at the same scale .",
    "determining the number of clusters + to determine the optimal number of clusters , we first used the elbow method , which looks at the percentage of variance explained as a function of the number of clusters .",
    "the idea is that we should choose a number of clusters such that adding more clusters does nt significantly improve the modeling .",
    "we performed clustering with @xmath20 = 5 to @xmath20 = 35 clusters and plotted the variance quantity @xmath33 vs. @xmath20 , where @xmath33 is the sum of the normalized intra - cluster sums of sqaures [ 16 ] .",
    "figure 1 shows a plot of log(@xmath33 ) vs. @xmath20 .",
    "+            the elbow method involves visually choosing the elbow of the graph where the slope changes most drastically .",
    "we determined our elbow happens at @xmath20 = 30 .",
    "however , determining the elbow of a graph is not a well - defined process , and in fact this is one of the known weaknesses of the elbow method .",
    "+ because of the shortcomings of the elbow method , we also used the gap statistic [ 10 , 16 ] to determine the optimal @xmath20 with which to cluster .",
    "the gap statistic is a way to to standardize the comparison of the  variance explained \" metric used in the elbow method .",
    "the gap statistic takes the approach of standardizing the variance explained against a null reference distribution of the data ( distribution with no apparent underlying clusters ) .",
    "the gap statistic method involves calculating the difference between the variance explained for the dataset and the variance explained for the null reference distribution .",
    "this difference is known as the gap statistic .",
    "the @xmath20 value that yields the greatest gap statistic ( greatest difference in variance ) is the optimal @xmath20 value for clustering the data .",
    "figure 2 shows the results of the gap statistic .",
    "+ the gap statistic predicts that @xmath20=30 is the optimal number of clusters for our data .",
    "this confirms our identification of the elbow was indeed correct .",
    "+      \\1 .",
    "k - means clustering + k - means clustering is a clustering algorithm that will find @xmath20 centroids to cluster a data set .",
    "the k - means algorithm converges on a centroid distribution that minimizes the sum of squares of distances between cluster centroids and the corresponding data points that are classified by them .",
    "+ using tools from the python library scikit - learn [ 11 ] , we performed k - means clustering on all 3855 vegas restaurants with random k++ means initialization and 300 iterations , specifying @xmath34 and @xmath35 .",
    "we used a euclidean distance metric for our clustering and classification of the 52-dimensional restaurant vectors .",
    "the result of this clustering can be seen in figure 3 .",
    "+        the clusters have a median of 148 members in each with a standard deviation of 80 .",
    "gaussian mixture model + a gaussian mixture model ( gmm ) is a probabilistic generative model that assumes all the data points are generated from a mixture of a finite number of gaussian distributions .",
    "the gmm , in principle , is a weighted sum of @xmath20 component gaussian densities .",
    "each gaussian distribution can be thought of as a cluster that can classify data points . + also using tools from the python library scikit - learn , we trained our gmm with the expectation maximization algorithm on 3855 restaurants specifying @xmath34 and @xmath35 .",
    "the result of this clustering can be seen in figure 3 .",
    "the gmm clusters have a median of 200 members each , with mean 158 members and standard deviation of 40 members .",
    "+ most notably , gmm clusters varied from the k - means clusters in shape : the k - means clusters were nearly always spherical in shape , due to k - means minimizing distance . the gaussian mixture model , however , is not limited to spherical clusters , as the gaussian distributions that define its clusters are shaped by variances in each dimension .",
    "this results in some elongated elliptical clusters .",
    "+ some of the clusters consists of only restaurants that lie on a particular street . it may be that this behavior is actually beneficial .",
    "oftentimes cultural districts within a city are highly street based , and the gmm model is flexible enough to detect clusters like this . the result of the gmm clustering can be seen in figure 4 .",
    "+          once we determine relevant spatial and topical clusters , we are tasked with labeling the clusters . to determine the labeling of a cluster , we take the average topic vector for all restaurants in the cluster .",
    "we then chose the top _ two _ topics that describe a cluster and use their human - attributed labels .",
    "these labels overlayed atop their cluster distributions are shown in figure 5 .",
    "+ we chose to display the top two labels to uncover not only the most frequent topic within a cluster but also underlying categories which might be less obvious .",
    "+            using our gaussian mixture clustering , we were able to enhance these labeling with appropriate orientations . since each cluster",
    "is represented by a gaussian with two dimensional variances , we are able to rotate the labelings to align with the direction of maximum gaussian variance .",
    "these rotated labels have a tendency to orient with streets .",
    "the waffles / brunch label in the top left displays this rather useful property .",
    "these oriented labels overlayed atop their gaussian cluster distributions are shown in figure 6 .",
    "+      while clustering restaurants on space and topics illuminates a city s many cultural centers , it does not show how a specific topic is distributed throughout the city .",
    "to show this distribution for a given topic we plotted topic similarity in a heatmap .",
    "we ran our lda inference on a novel restaurant s reviews . from this we got a topic distribution of that novel restaurant .",
    "we divided the city into a a 20x20 grid of squares .",
    "for each square we calculated the average topic similarity from the center of the square to all restaurants in the city .",
    "we used a gaussian weight metric to scale topic similarity by proximity . for each square we calculated a similarity metric @xmath36",
    "where :    @xmath37     + where +    @xmath38 : :    = all restaurants in city @xmath39 : :    = the topic distribution of the novel restaurant @xmath40 : :    = euclidian distance metric for topic distributions    and    @xmath41    where +    @xmath42 : :    @xmath43 the center of the square @xmath44 : :    @xmath45 @xmath46 : :    @xmath43 the position of the restaurant    @xmath47    our similarity metric takes in to account topic similarity of the novel restaurant to each other restaurant in the city .",
    "we use a gaussian weight to scale these topic similarities by distance .",
    "this allows restaurants near the square s center location to have most of the influence over the square s color .",
    "we calculated our similarity metric for every square in our grid and colored our heat map red for high values and blue for low values .",
    "the results of our heatmap generated by comparing the topics of the restaurant `` pho vietnamese restaurant '' to the restaurants of vegas are shown in figure 7 .",
    "+        in figure 8 , the x indicates the actual location of the the restaurant ( the similarity calculations were conducted without including this restaurant ) .",
    "our heat map shows that the restaurant is in an area of high topic similarity , which is accurate ( pho vietnamese restaurant is located in las vegas s chinatown district ) .",
    "we found that the resultant lda topics ( appendix table iii ) were well - defined and descriptive .",
    "we observed that the words within a given topic fit well into a particular category of food type or culture , and we had very little trouble labeling them based off of the given words and weights .",
    "additionally , the topics themselves seem reasonably distinct from each other with only few overlapping topics .",
    "the general area in which we saw the most overlap was the buffet restaurants topic .",
    "topics # 0 , # 5 , # 48 each concerned buffet restaurants .",
    "however , looking at the words in each , we were able to distinguish  seafood / buffet \" ( # 5 ) and  upscale / buffet\"(#0 ) from a more general ",
    "buffet \" topic ( # 48 ) .",
    "+ looking at the k - means clustering of las vegas restaurants , we observed that our clustering classifies areas defined beforehand : it put labels of `` pho '' and `` dim sum '' on chinatown , and `` luxe '' , `` steakhouse '' , `` upscale '' , and `` seafood / buffet '' over the strip .",
    "interestingly , it also split these clusters into smaller subclusters , for example separating a  dim sum \" and  ramen \" cluster from a  pho \" and  soup \" cluster .",
    "this behavior may or may not be ideal : it may be identifying actual sub - districts , or future work may involve a final cluster merge step in which two clusters close in distance and topic similarity can be merged in to a single cluster .",
    "+ the gmm also distinguished these already - known cultural areas .",
    "the shape and sizes of the clusters themselves were slightly more varied and often alligned with a particular street .",
    "this ability is interesting because oftentimes districts may be very street - based .",
    "+ as all this learning was unsupervised , we are very interested in finding a metric to quantitatively determine accuracy across these different models .",
    "one potential way to do this would be to conduct cognitive studies with people who live in or are familiar with particular cities .",
    "for example , we could compare our map with maps described by las vegas residents , or get a measure of how accurate they believe our map is .",
    "+      the automatic spatial and topical clustering and labeling approach outlined above is a general method that can be applied to any city .",
    "figure 8 in the appendix shows the results of labeling 2 other cities ( phoenix and endinberg ) with this method .",
    "these maps can be analytical tools with various applications including but not limited to determining new restaurant placement , understanding cultural regions of a city , discovering unexplored areas of one s city , choosing where to live , or what route to take on a stroll to the park .      like the automatic cultural labeling method",
    ", the topic heat map can be used as a useful analytics tool .",
    "this map can be used to determine certain cultural hotbeds , both known and hidden . a hidden cultural hotbed may present a market opportunity for continued growth .",
    "the topical heat map of a city may be an especially valuable asset to a new restaurant or chain looking to strategize where exactly to place a new store location .",
    "the heat map could actually be used to perform a detailed analysis on what kind of location , for different types of restaurants , is optimal ( see iv part 3 for more detail ) .",
    "\\1 ) using the timestamps on reviews , it is possible to filter reviews based on when they were written .",
    "this would allow for creating dynamic maps using reviews within a moving a time window to see how culture changes : how new clusters emerge , split and merge .",
    "+ 2 ) in our study we use the elbow method and gap statistic to predetermine an appropriate number of clusters to use . instead",
    ", it may prove valuable to use a nested chinese restaurant process to learn a hierarchy of clusters and subclusters .",
    "for example , this could split chinatown into various subclusters under the general chinese cluster .",
    "this could be used to label the graphs at various scales and zoom levels . additionally , using a chinese restaurant process as part of a nonparametric mixture model",
    "would allow the model to flexibly add more clusters as needed , and may be more likely to find the optimal number of clusters .",
    "+ 3 ) the similarity heatmap we developed along with yelp star ratings could be used to analyze what kind of placement makes a restaurant successful .",
    "for example , it may be that placing a restaurant right in the center of an area of very high similarity creates direct competition and comparison that is actually detrimental . at the same time",
    ", it may be that placing a restaurant in an area where it is completely out of place is also a bad idea .",
    "a detailed analysis of where restaurants with varying star ratings fall on a similarity heatmap could provide valuable insight to businesses about what kind of placement is optimal .",
    "+ 4 ) using yelp user data and the classifications from this model , it is possible to create a recommendation system .",
    "recommendations could be general areas or specific restaurant suggestions : for example , if a user likes several restaurants in a specific area / cluster , we imagine recommending to them another restaurant or area that shares similar topics .",
    "we would like to thank joshua tenenbaum for supporting this work .    99    d. blei , a. ng , m. jordan , latent dirichlet allocation , journal of machine learning research 3 , pp . 993 , 2003 . y. wang , p. sabzmeydani , g. mori , semi - latnt dirichlet allocation : a hierarchical model for human action recognition , in lecture notes in computer science , vol .",
    "4814 , a. elgammal , b. rosenhahn , r. klette , eds .",
    ", heidelberg : springer berlin , 2007 , pp . 240 .",
    "m. lienou , h. maitre , m. datcu , lienou , m. , semantic annotation of satellite images using latent dirichlet allocation , geoscience and remote sensing letters , ieee , vol . 7 , pp .",
    "28 , july 2009 .",
    "r. krestel , p. fankhauser , w. nejdl , latent dirichlet allocation for tag recommendation , proc .",
    "of the third acm conf .",
    "on recommender systems , new york , 2009 , pp .",
    "61 . i. br , j. szab , a. benczr , latent dirichlet allocation in web spam filtering , proc . of the 4th int .",
    "workshop on adversarial information retrieval on the web , new york , 2008 , pp .",
    "gensim python library , https://radimrehurek.com / gensim/. j. huang , s. rogers , e. joo , improving restaurants by extracting subtopics from yelp reviews , presented at iconference , berlin , 2014 .",
    "m. hoffman , d. blei , f. bach , online learning for latent dirichlet allocation , in advances in neural information processing systems 23 , 2010 .",
    "yelp academic dataset , https://www.yelp.com/academic_dataset .",
    "r. tibshirani , g. walther , t. hastie , estimating the number of clusters in a data set via the gap statistic , j. r. statist .",
    "63 , part 2 , pp . 411 , 2001 .",
    "f. pedregosa , g. varoquaux , a. gramfort , v. michel , b. thirion , o. grisel , m. blondel , p. prettenhofer , r. weiss , v. dubourg , j. vanderplas , a. passos , d. cournapeau , m. brucher , m. perrot , e. duchesnay , scikit - learn : machine learning in python , journal of machine learning , vol .",
    "12 , pp . 2825 , 2011 .",
    "t. griffiths , n. chater , c. kemp , a. perfors , and j. b. tenenbaum , probabilistic models of cognition : exploring representations and inductive biases , trends in cognitive sciences , vol .",
    "14 , pp . 357 , 2010 . n. chater , j. b. tenenbaum , and a. yuille , probabilistic models of cognition : conceptual foundations , trends in cognitive sciences , vol . 10 , 2006 . j. b. tenenbaum , rules and similarity in concept learning , advances in neural information processing systems 12 , pp .",
    "59 , 2000 .",
    "a. n. sanborn , t. l. griffiths , d. j. navarro , psychological review , vol .",
    "117 , pp . 1144 , 2010 .",
    "the data science lab , finding the k in k - means clustering , december 2013 , https://datasciencelab.wordpress.com/2013/12/27/finding-the-k-in-k-means-clustering/",
    "| m0.8 cm | m2 cm | m14 cm |     + * topic # * & * human label * & * word distribution * +   _ continued from previous page _ + * topic # * & * human label * & * word distribution * +   + 0 & buffet / upscale & 0.027*wicked + 0.026*spoon + 0.015*buffet + 0.014*dishes + 0.012*cosmopolitan + 0.011*stone + 0.011*mac + 0.011*marrow + 0.011*bone + 0.010*gelato +"
  ],
  "abstract_text": [
    "<S> topic models are a way to discover underlying themes in an otherwise unstructured collection of documents . in this study </S>",
    "<S> , we specifically used the latent dirichlet allocation ( lda ) topic model on a dataset of yelp reviews to classify restaurants based off of their reviews . </S>",
    "<S> furthermore , we hypothesize that within a city , restaurants can be grouped into similar  clusters  based on both location and similarity . </S>",
    "<S> we used several different clustering methods , including k - means clustering and a probabilistic mixture model , in order to uncover and classify districts , both well - known and hidden ( i.e. cultural areas like chinatown or hearsay like  the best street for italian restaurants \" ) within a city . </S>",
    "<S> we use these models to display and label different clusters on a map . </S>",
    "<S> we also introduce a topic similarity heatmap that displays the similarity distribution in a city to a new restaurant . </S>"
  ]
}