{
  "article_text": [
    "steganography is the field that deals with the problem of sending a message from a sender a to a recipient b through a channel that can be read by a so - called warden , in a way that the warden does nt suspect that the message is there .",
    "steganographic techniques exist for hiding messages in images , audio , videos , and other media .",
    "in particular , text steganography studies information hiding on texts .",
    "there are many techniques for this , as summarized on @xcite @xcite .",
    "one of the simplest steganographic methods for texts works by encoding a fixed amount of bits per word , using a table that maps words to codes , and vice versa .",
    "a disadvantage of this trivial technique is that the text will be obviously random at a syntactic level , as words are generated in a way that is independent of context .",
    "there are other simple methods that store data in the text format , using spacing , capitalization , font or html tags .",
    "for example snow @xcite hides information in tabs and spaces at the end of each line , that are usually not visible on text viewers . also , some techniques start from a base text ( the covertext ) , and modify it in some way : for example by switching words to near synonymous , or by changing sentences from their original grammatical structure to another one that preserves the meaning .",
    "the technique shown in @xcite hides information by modifying words in a way that resembles ortographical or typographical errors .",
    "there are other techniques that rely on translation @xcite .    in other cases ,",
    "texts are generated using a grammar model ; this kind of system has the advantage of producing texts that make sense at a grammatical level , although not at a semantical level .",
    "and there are techniques , like the one described on this paper , that are based on using markov models to generate texts that encode some hidden message on them .",
    "weihui dai et al .",
    "@xcite @xcite explore a method for encoding data on this way ; @xcite shows a simple implementation of a similar concept .",
    "this article explores a specific method for using markov chains for text steganography .",
    "how this method compares to other similar methods and how it works is explored further in the next sections .",
    "a reference implementation of the method described here is also included in the open source program markovtextstego @xcite .",
    "many methods for text steganography that are not based on markov chains are known .",
    "an example is nicetext @xcite , which shows a way to encode ciphertext to text , that uses custom styles , context free grammars and dictionaries .",
    "the approach used in @xcite @xcite is based on markov chains . when encoding , some data is provided as input , and the system generates a text as output using a given markov chain .",
    "the stegotexts are generated in a way that simulates that they were generated by the markov chain .    however , to avoid complex calculations , the markov model is simplified by assuming that all probabilities from a given state to any other state are equal .",
    "this can change the quality of the texts generated by the markov chain significantly .",
    "for example words like `` the '' and `` naturally '' are both potential starts of a phrase , but the former should be much more frequent than the latter ; and this difference is not preserved by the simplification .",
    "other markov - based models or similar models require of similar simplifications of the markov chain , typically by making all outbound probabilities of each state equal ( as in the previous example ) , or by replacing them by other ones , either explicitly or implicitly through the operation of the encoding algorithm @xcite @xcite @xcite .",
    "the method described here aims to be an answer to the question of whether it is possible to preserve the probabilities in the markov models to higher levels of accuracy .",
    "the method is not optimally precise , but it generates texts that use a language model that is a good approximation of the provided markov model .",
    "a markov chain is a model for a stochastic process .",
    "a sequence of random variables @xmath0 with values from a finite set @xmath1 is a markov chain , if it has the markov properties @xcite @xcite :    limited horizon property :    @xmath2    time invariant property :    @xmath3    the first property means that the markov chain does nt have memory of any states , beyond the last one .",
    "the second property means that the conditional probabilities for all states do not depend on the position ( time ) on the sequence .",
    "diagrams like the one shown in fig .",
    "[ fig : examplechain ] are frequently used to represent the transitions in markov chains .",
    "all nodes in the graph represent states ( elements of @xmath1 ) , and any arrow from @xmath4 to @xmath5 with a value of @xmath6 means that @xmath7 .",
    "we call any state @xmath5 an outbound state of @xmath4 , if there is an arrow from @xmath4 to @xmath5 . for every @xmath4 that does nt have an arrow to another @xmath5 state , @xmath8 .",
    "'' marks both the start and end of a sentence.,height=177 ]    markov chains and models are frequently used to model language @xcite ; when that s the case , states in the chain are used , for example , to represent words , characters , or n - grams .",
    "also , markov models are used in steganography ( as described above ) , and in steganalysis @xcite @xcite .",
    "a markov language model may be useful to compute probabilities for phrases , from the n - gram probabilities .",
    "for example given the markov chain shown in fig .",
    "[ fig : examplechain ] , if the process were to start from `` @xmath9 '' ( symbol that we use both for start and end of a sentence ) , the probability of generating the text composed by the sequence of words or states @xmath10 $ ] would be @xmath11 .",
    "these models can also be used to generate random texts . for this",
    ", a random source is used that can pick a next state @xmath5 with probability @xmath12 , given the current state @xmath4 . the algorithm for generating",
    "the random text starts by setting `` @xmath9 '' to be the current state ; then , in every iteration it uses the random source to pick the next word , which also becomes the new current state in the next iteration . to generate a single sentence",
    ", the process can be made to stop when the state `` @xmath9 '' is reached .",
    "although markov chains only have memory of a single previous state , every state can be a bigram , or an n - gram .",
    "this way , a markov language model can have memory for more than a single word .",
    "although this article only describes the steganographic method based on states that are single words , the reference implementation of the system @xcite allows using both unigrams and bigrams as states , and it is possible to extend it to support n - grams with @xmath13 . table  [ fig : tablefiles ] compares the results of the encoding procedure when using unigrams and bigrams .    a markov language model with states as single words can be computed from the frequencies of all bigrams , and all unigrams in a text :    @xmath14    where @xmath15 is the number of occurrences of word @xmath16 followed immediately by word @xmath17 in the text , and @xmath18 is the number of occurrences of word @xmath16 .    as discussed above , some steganographic methods are based on these markov language models .",
    "the language model is usually simplified in some way ; for example @xcite sets all @xmath19 with @xmath20 fixed , to a fixed @xmath21 .    in these models ,",
    "once the simplification is done , the markov chain is used to encode data into text ; every word stores some fixed or variable amount of bytes , and every bigram in the generated text is required to have conditional probability @xmath22 in the markov chain . a decoding algorithm that reverses the process , transforming the text into data ,",
    "is also defined .",
    "the approach shown in this article avoids much of the simplification in the probabilities of the markov chain .",
    "although there is still some precision loss in the model , for the most part , the proportions between the frequencies of different n - grams are preserved , specially for long texts .",
    "a main objective in this article is to describe two functions , @xmath23 , and @xmath24 , that are used to create a text out of a data input , and to get the original data out of an encoded text . in steganography literature",
    ", it would be said that @xmath23 generates a stegotext out of the input plaintext , while @xmath24 does the reverse process .",
    "the @xmath23 function is not cryptographically secure ; it assumes that its input is a plaintext , or some data that has already been encrypted using an independent system . in the latter case , @xmath23 s input can be called ciphertext .",
    "we require the encoding function to be invertible ; that is , for every input data",
    "@xmath25 and @xmath26 , @xmath27 only if @xmath28 .",
    "also @xmath24 is the inverse of @xmath23 , so for every input @xmath29 , @xmath30 .",
    "the encoding function @xmath23 is required to work on all the domain of data @xmath29 ; the required domain of @xmath24 however needs only be the image of @xmath23 .",
    "the @xmath23 and @xmath24 functions will be built out of simpler functions , for fixed - size encoding and decoding .",
    "these functions are @xmath31 @xmath32 , and @xmath33 . both the markov chain and the starting symbol are actually required for these functions too , but they are left out for simplicity .",
    "only when it is required for the purposes of the explanation , a third argument is added to both functions , for the start symbol : @xmath31 @xmath34 @xmath35 , and @xmath36 .    in this system",
    ", the size of @xmath29 in bits is known beforehand both for @xmath37 and for @xmath38 .",
    "the requirements for both functions are weaken compared to those for their non - fixed counterparts ; it is required that for every input data @xmath25 and @xmath26 such that @xmath39 , @xmath40 only if @xmath28 ( where @xmath41 is the size of @xmath29 in bits ) .",
    "this weaker restriction means that the encoder may produce the same text for two different data inputs , only if they have different lengths , as can be seen in the examples in table  [ fig : tableencodings ] .",
    "also , @xmath42 with @xmath43 .",
    "a basic component for encoding and decoding is the function named @xmath44 , that maps all outbound states from a given state , to subranges of a given range .",
    "these subranges are a partition of the original range .",
    "@xmath45\\ ] ]    where @xmath46 is a markov chain , @xmath47 is a state in @xmath1 , and @xmath48 is a range of natural numbers @xmath49 $ ] .",
    "the result is a list that must have some properties that are described below .",
    "the behavior of this function is that it maps outbound states of a markov chain to subranges of a given range , in a way that approximately matches the proportion between the sizes of the different subranges , to the proportion between the probabilities of the respective states .",
    "for example , if @xmath46 is the chain in fig .",
    "[ fig : examplechain ] , @xmath50 , and @xmath51 $ ] , the expected result would be @xmath52 ) , ( s_{2 } , [ 2 , 3])]$ ] .",
    "that is , because each outbound state has a 0.5 probability , it has to get half of the full range .",
    "if @xmath53 , the expected result would be @xmath54),$ ] @xmath55)]$ ] , where again the length of the subranges matches the proportion of their respective probabilities .",
    "this partitioning method will be used in an iterative way , both for encoding and for decoding .",
    "[ fig : exampleencoding ] ( in section  [ encodingsection ] ) shows how this is done , although the details of the operation are described in the next sections .",
    "the returned value for @xmath44 in equation  [ subrangeseq ] is a list of pairs @xmath56 , where @xmath5 is a state such that @xmath57 , and @xmath58 is a subrange of @xmath48 .",
    "the subranges of @xmath48 returned by @xmath44 are a partition of @xmath48 .",
    "a good implementation of the function generates a mapping between subranges @xmath58 and states @xmath5 , such that the fraction of the total range length for each @xmath58 is approximately equal to the probability of the respective state @xmath5 .",
    "that is :    @xmath59    where the length of a range is defined to be @xmath60 ) = b - a + 1 $ ] .    the property in equation  [ conditionlen ] is not a strict requirement , as even without this condition the encoding function will still generate texts that are decoded correctly .",
    "however , only when this condition is held the texts that are generated will be approximately described by the original markov language model .    *",
    "[ condition of minimal length ] * the following condition is actually required for the steganographic system to work , however .",
    "any time that there are at least two different states @xmath61 and @xmath62 such that @xmath63 and @xmath64 ( that is , every time @xmath47 has at least two outbound states ) , it is required that @xmath44 returns a list with at least two elements .",
    "this restriction is necessary for ensuring that both the encoder and decoder methods halt for all inputs .",
    "it might produce precision loss in many cases however , as in the following example : an @xmath47 state has two outbound states @xmath61 and @xmath62 , with conditional probabilities @xmath65 and @xmath66 , and the input range to process is @xmath67 $ ] .    in this case , it would seem that the best output would map @xmath61 to the full range : the returned value for this would be @xmath52)]$ ]",
    ". however this value does nt hold the condition of minimal length , as the list has a single element , despite @xmath47 having more than one outbound state .    because of this , the only valid results for this example would be @xmath68),$ ] @xmath69)]$ ] and a symmetrical one ( same ranges but switching states ) . as can be seen ,",
    "these valid options are worse approximations to the input conditional probabilities , than just mapping @xmath61 to the full state ; however the condition described disallows this better approximation .",
    "the following three functions are used by the encoding and decoding methods .",
    "as described , @xmath44 returns a list that maps ranges to states .",
    "the function @xmath70 uses the list to return the subrange that is assigned to a given state :    @xmath71 = subranges(mc , s_{k } , r )   \\mbox { such that } s_{k } = s_{l } \\end{aligned}\\ ] ]    the function @xmath72 returns the subrange in the list that contains a given number :    @xmath73 = subranges(mc , s_{k } ,   r )   \\mbox { such that } number \\in r_{k}\\end{aligned}\\ ] ]    the function @xmath74 returns the state that is assigned to the subrange returned by @xmath72 :    @xmath75 = subranges(mc , s_{k } , r ) \\mbox { such that } number \\in r_{k } \\end{aligned}\\ ] ]    these functions will be used in the next sections .      the function @xmath74 , can also be seen as a function that encodes data to a single word .",
    "given a markov chain , a state , a range and a number ( the input data ) , it finds the corresponding state or word in the chain for that number . related to that ,",
    "@xmath72 also defined above , returns the subrange that corresponds to the word returned by @xmath74 .",
    "based on these two functions , a sequence of states @xmath76 and a sequence of ranges @xmath77 can be generated as described in the following two equations .",
    "these sequences are computed given a markov chain @xmath46 , an initial state @xmath78 ( typically `` @xmath9 '' ) , an input data ( @xmath79 ) , and an initial range @xmath80 ( typically @xmath81 $ ] , where n is the length of the data to store ) :    @xmath82    @xmath83    both sequences are defined to be finite ( as we want to encode data to a finite sequence of words ) ; the final element for both is @xmath84 such that @xmath85 .",
    "this means that we stop encoding when the sequence of words describes a single number .",
    "finally , @xmath86 $ ] .",
    "this encoding process works by partitioning an input range in a way that matches the outbound states of a given state , and then selecting the outbound state whose subrange contains the number to encode . after this",
    "is done , the selected subrange and state are used as the input for the next iteration of the algorithm .",
    "when the process finishes , the encoded text is the sequence of states that the algorithm went through .",
    "the @xmath44 function is restricted by the condition of minimal length in section  [ mappingprob ] to always split a range in more than one subrange , whenever possible ; therefore the iteration of this process will produce ranges that are smaller and smaller .",
    "( even though it is possible that a markov chain that is computed from a text contains states with only one outbound state , those will eventually lead to @xmath9 , which will have more than one outbound state . )",
    "also all subranges must contain at least one element , so the iterative generation of subranges converges to a subrange of length @xmath87 .    because the selected subrange length converges to @xmath87 ,",
    "the process has to finish , and when it finishes there is a subrange around a single number ( the original input ) and a list of states ( words ) . for every input data ,",
    "there is a final result .    for every number @xmath29 of size @xmath88",
    ", this final result can be seen as a path that points to @xmath29 , as every state in the word sequence tells which subrange to choose from the partitions generated by @xmath44 .",
    "using this path intuition , it can be seen that if @xmath25 and @xmath26 are two different numbers of the same size @xmath88 , their encoded texts are necessarily different , as they lead to different numbers . in the same way , the decoding system can find @xmath29 using the text as a path to the length @xmath87 subrange .     into",
    "@xmath89 $ ] .",
    "this and more examples can be seen in table  [ fig : tableencodings].,height=113 ]    fig .",
    "[ fig : exampleencoding ] shows this partitioning process .",
    "the example in the figure uses the range @xmath90 $ ] , with the numbers encoded in binary .",
    "if we use the markov chain shown in fig .",
    "[ fig : examplechain ] and we start from @xmath9 , in a first step the range has to be split in half , because the probabilities for the two states @xmath61 and @xmath62 are both @xmath91 . the subrange assigned to @xmath62 can then be split in two other parts , now for the states @xmath92 and @xmath93 , but the proportions are @xmath94 and @xmath95 in this case .",
    "this shows that if we were to encode the binary number @xmath96 , with a fixed size @xmath97 bits , we would get the text @xmath89 $ ] .",
    "if we were trying to encode the binary number @xmath98 , we would need to continue partitioning the range for @xmath99 , until there is only a single number in the last subrange .",
    "table  [ fig : tableencodings ] shows the output of @xmath37 for a number of inputs .",
    "the reference implementation @xcite was used , and the results may vary in other implementations , depending on specific details of the range partitioning algorithm .",
    "all examples use the markov chain shown in fig .",
    "[ fig : examplechain ] , with `` @xmath9 '' as the starting state . in particular ,",
    "it is possible to see that @xmath96 indeed encodes to @xmath89 $ ] , as described above .",
    ".table of example encodings using @xmath37 .",
    "two different inputs can encode to the same text only if they have a different length , as happens with @xmath100 and @xmath101 .",
    "the frequencies of the different bigrams approximate the probabilities @xmath102 from the markov chain , and this approximation gets better as @xmath88 grows ( because the space is bigger , and because of the condition of minimal length , which has a higher effect in smaller inputs ) .",
    "also this table shows a very low capacity , because the markov chain used is very small .",
    "more comments about capacity in section  [ conclusion ] . [ cols=\"^,^,^\",options=\"header \" , ]      decoding of fixed - size data is based on @xmath70 , which was described on section  [ mappingprob ] .",
    "it was previously described as a function that returns the subrange that is assigned to a given state ; but it can also be seen as a decoder from states to numbers . in this way ,",
    "the function @xmath103 decodes a single word state @xmath104 , given that the previous state was @xmath105 .",
    "the decoded value is not a number , but a range of numbers : @xmath49 $ ] where both @xmath16 and @xmath17 are natural numbers .",
    "given an input sequence of states or words @xmath106 ( where @xmath107 is taken to be the initial state used for encoding ) and an initial range @xmath80 ( typically @xmath81 $ ] ) we define the sequence of ranges @xmath77 as :    @xmath108    the output of the @xmath38 is the value of the range @xmath109 , where @xmath84 is the first @xmath110 such that @xmath111 . since",
    "when that happens the range covers a single number , the decoding process can just return that number .",
    "a valid output is nt guaranteed for all texts ( sequences of words ) , only for words that have been generated by using the @xmath37 process described above .",
    "the decoding process works because it follows the same path that the encoder process followed when generating the text , and this path leads to the original input data .",
    "the encoder writes a sequence of words while refining subranges until finding a range that has length @xmath87 .",
    "the decoding process follows the states written by the encoder , which lead to exactly the same sequence of subranges .",
    "this means that @xmath38 will reach the input of @xmath37 , when feed with the output of @xmath37 .",
    "this makes @xmath38 acts as the inverse for @xmath37 , for fixed @xmath88 .",
    "an additional property of @xmath38 as it is defined here is that if @xmath112 , then also @xmath113 , where @xmath114 is any text and `` @xmath115 '' is the list concatenation operation .",
    "this is because the fixed decoding algorithm finishes computing the value for @xmath116 when the last subranges converge to a single number , and that happens at the same place in the text sequence for @xmath117 and for @xmath118 .",
    "this property is useful because it allows us to concatenate encoded texts , and they can be decoded directly as the decoder can tell where every text starts and ends .",
    "this is applied to the variable encoding algorithm discussed in section  [ variableencoding ] .",
    "a direct implementation of the algorithms described above would require that many operations are applied to the @xmath88 bit ranges in every iteration of encoding and decoding .",
    "for example , in every iteration of the fixed - size decoding algorithm , a call to @xmath44 needs to be done with a range of numbers with @xmath88 bits of size , until the length of the selected range is @xmath87 ( so that the range matches the original input ) .",
    "this is very inefficient both regarding memory usage and processing time .",
    "it is possible to avoid processing on the full @xmath88 bits on every iteration , by making some changes to the underlying algorithms .",
    "some data with length @xmath88 can be processed more efficiently if only a short , moving window of a few bits is processed in every iteration .",
    "we define @xmath119 :    @xmath120    where @xmath121 $ ] is defined to be a range where @xmath16 and @xmath17 are two numbers that can be expressed in up to @xmath122 bits , and @xmath123 computes @xmath124 $ ] , with @xmath125 identical to @xmath16 in all its leftmost @xmath122 bits , and @xmath126 in the remaining bits , and with @xmath127 identical to @xmath17 in all its leftmost @xmath122 bits , and @xmath87 in the remaining bits .",
    "this means that we can use @xmath128 to convert short ranges like @xmath129 $ ] ( in binary ) to the longer 4 bit range @xmath130 $ ] , if @xmath131 .",
    "an efficient implementation of @xmath119 returns all subranges in short form , for any input .",
    "when the ranges have to be split in a way that requires infinite or long precision ( for example if there are two states , with @xmath132 and @xmath133 ) , this is only possible if a precision limit is set in the implementation .",
    "this precision limit can be set to mean that regardless of the input of @xmath119 , there is a maximum number of bits that can be used for the partitioning process .",
    "for example , with @xmath134 and the probabilities described above , the ranges returned could be : [ 00000000 , 01001101 ] for @xmath61 , and [ 01001110 , 11111111 ] for @xmath62 . in this case",
    ", @xmath61 really has about 0.305 of the numbers of the total range , so using 8 of the 100 bits is a good approximation .",
    "if we were to use only 4 bits in @xmath119 for this case , it would return : [ 0000 , 0100 ] for @xmath61 , and [ 0101 , 1111 ] for @xmath62 . in this case",
    "@xmath61 maps to about 0.312 numbers of the total range ; this is a slightly worse approximation , but it might be better as it requires using only half the amount of bits .    both for encoding and decoding , a bit stream data structure will be needed . for encoding , this stream of bits will be read ; when decoding , it will be used to write the data output , in a bitwise fashion .",
    "when encoding , in every iteration @xmath119 will require a small number of bits to be read from the bit stream .",
    "as soon as those bits are read , they can be discarded from the bit stream .",
    "also , @xmath119 will generate new ranges in every call , and in every iteration these ranges will be more precise , that is , ranges that cover a smaller amount of numbers .",
    "this means that the subranges will require more bits to be stored .",
    "however , if the precision for @xmath119 is set to a finite value ( as described above ) , the number of bits at the right of the range that differ from each other will be at most @xmath21 , for some @xmath21 .",
    "this means that with every iteration , the ranges will grow in size @xmath88 , but the leftmost bits will at the same time converge bitwise to the same values ( for range @xmath49 $ ] , leftmost bits of @xmath16 and @xmath17 will be identical ) .",
    "the leftmost bits can then be discarded , as they are already known to match the leftmost bits in the input data .",
    "this process ensures that in every iteration of encoding , @xmath119 only has to deal with a moving window that has a limited number of bits , related to the precision set to the system in the implementation .",
    "similarly for decoding ; in very iteration , the range that @xmath119 returns will grow in size ( as measured in bits ) .",
    "however , while the range grows in size , the leftmost bits converge , so they can be removed , and added to an output bit stream . when the process finishes , the output bit stream will contain the full output of the decoding algorithm : all the bits of the converged range .        ' '' ''    ' '' ''    image & chain states & encoding time & decoding time & encoded size & @xmath135 + example.zip & unigrams & 91.8 s & 95.3 s & 81 kb & 6.7 + ( 12 kb ) & & _ ( 0.1 kb / s ) _ & _ ( 0.1 kb / s ) _ & _ ( zip : 32 kb ) _ & _ ( zip : 2.7 ) _ + & bigrams & 53.7 s & 55.8 s & 149 kb & 12.4 + & & _ ( 0.2 kb / s ) _ & _ ( 0.2 kb / s ) _ & _ ( zip : 58 kb ) _ & _ ( zip : 4.8 ) _ + example.jpg & unigrams & 141 s & 150.5 s & 119 kb & 6.3 + ( 19 kb ) & & _ ( 0.1 kb / s ) _ & _ ( 0.1 kb / s ) _ & _ ( zip : 38 kb ) _ & _ ( zip : 3.2 ) _ + ( zip : 12 kb ) & bigrams & 93.4 s & 97.6 s & 216 kb & 11.4 + & & _ ( 0.2 kb / s ) _ & _ ( 0.2 kb / s ) _ & _ ( zip : 66 kb ) _ & _ ( zip : 5.5 ) _ + example.png & unigrams & 299.6",
    "s & 317.2 s & 269 kb & 6.9 + ( 39 kb ) & & _ ( 0.1 kb / s ) _ & _ ( 0.1 kb / s ) _ & _ ( zip : 104 kb ) _ & _ ( zip : 2.7 ) _ + & bigrams & 194.1 s & 181.8 s & 494 kb & 12.7 + & & _ ( 0.2 kb / s ) _ & _ ( 0.2 kb / s ) _ & _ ( zip : 188 kb ) _ & _ ( zip : 4.8 )",
    "the encoding and decoding process described above only allows to decode data from a text , given that the size of the data is known beforehand .",
    "however , requiring the recipient of a steganographic system to know the size of the hidden data before it is decoded is not optimal .",
    "an extension of the encoding and decoding methods for variable - size data solves this problem .    for variable size encoding and decoding it",
    "is required that an integer @xmath122 is shared beforehand .",
    "this number is not the data size , but the size used for a header ; it is typically a small value like 16 or 32 .",
    "texts @xmath136 and @xmath137 are encoded as shown below , using the three arguments version of @xmath37 .",
    "the header is encoded first , into @xmath136 .",
    "this is done using the fixed - data encoding algorithm , with the fixed size @xmath122 that is known both for encoder and decoder :    @xmath138    @xmath139    once the header was encoded , the actual data is encoded into @xmath137 .",
    "we use @xmath20 as starting symbol , to ensure that there is nt an interruption in the flow of the generated text between the last symbol in @xmath136 and the first one in @xmath137 :    @xmath140    @xmath141    finally , @xmath142 is defined simply as :    @xmath143    that is , the encoded data is just the header text followed by the data text .",
    "as @xmath20 was used as starting symbol for generating @xmath137 , there will be no interruption in the flow between both texts .    for decoding an input @xmath117 ,",
    "we define :    @xmath144    that is , @xmath38 is used to extract the length information from the header , using the shared value @xmath122 .",
    "@xmath145    @xmath146    @xmath147    finally , @xmath24 can be defined :    @xmath148    it can be seen that when @xmath149 , it follows that : @xmath150 , @xmath151 , @xmath152 , and @xmath153 .",
    "for this reason , @xmath154 , which means that @xmath24 is the right decoding function .",
    "it is also possible to extend @xmath23 , without changing this last property , in this way :    @xmath155    where @xmath156 is the last word in @xmath137 , and @xmath157 generates a random text that ends in period , using the markov chain and starting from the given state .",
    "this can be used to ensure that all texts generated by @xmath23 have a final sentence that is complete , and finishes with period . adding",
    "any text wo nt affect the decoding at all , as explained in section  [ decodingfixed ] .",
    "depending on the kind of data that is being transmitted , it might be useful to encode into @xmath136 the length of the data in bytes , instead of encoding it in bits .",
    "also , the way the length is actually represented into bits matters ; if big endian is used to represent a multi - byte length into bytes , short encoded lengths will start with a sequence of 0 bits ; this could produce the encoded texts to always start with the same words , or with a small variety of different words ( because all leftmost bits are zero ) .",
    "for this reason , either little endian or a representation that reverses the bits of big endian would be preferable .",
    "this article presented a steganographic method based on markov chains that differs from other similar models in the way precision loss in the language model is avoided .",
    "a reference implementation for this method was also presented .",
    "the examples shown in table  [ fig : tableencodings ] could seem to show that the system has very low capacity .",
    "however this is only because of the markov chain used ; if the system uses a small markov chain , it will have low capacity , but if it uses a bigger markov chain it will typically have a higher capacity .",
    "preliminary results of empirical tests using a big markov chain computed from an actual literary text show that the encoded data takes the size of about 6 - 7 times the size of the original data , with an @xmath88 value that is big enough ( for very small @xmath88 , yet bigger than a few bytes , this factor can be higher , e.g. around 9 ) . because the produced output is a text , it can be compressed with a high ratio ; the compressed size of the texts is about 2 times the size of the original data . however , these results require a more complete and thorough analysis",
    ".    other possibilities for further research are : to combine this method to other known language based steganographic systems , for producing an overall better steganographic text generation method ; to analyze what is the actual , measured performance for this new algorithm , and how this new algorithm compares to other existing algorithms , in terms of stegoanalysis .            topkara m. , topkara u. , atallah m.j . : information hiding through errors , a confusing approach .",
    "proceedings of the spie international conference on security , steganography , and watermarking of multimedia contents .",
    "( 2007 )        dai , w. , yu , y. , deng , b. : bintext steganography based on markov state transferring probability .",
    "2nd international conference on interaction sciences : information technology , culture and human ( icis 09 ) .",
    "( 2009 )                    taskiran , c.m . ,",
    "topkara , u. , topkara , m , delp , e.j . : attacks on lexical natural language steganography systems .",
    "spie international conference on security , steganography , and water - marking of multimedia contents ."
  ],
  "abstract_text": [
    "<S> a text steganography method based on markov chains is introduced , together with a reference implementation . </S>",
    "<S> this method allows for information hiding in texts that are automatically generated following a given markov model . </S>",
    "<S> other markov - based systems of this kind rely on big simplifications of the language model to work , which produces less natural looking and more easily detectable texts . </S>",
    "<S> the method described here is designed to generate texts within a good approximation of the original language model provided .    , markov chain , markov model , text , linguistics </S>"
  ]
}