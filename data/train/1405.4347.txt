{
  "article_text": [
    "zero - sum games ( hereafter zsg s ) have played a central role in the development of game theory with seminal contributions from borel  @xcite , von neumann  @xcite and shapley  @xcite .",
    "the latter considered discounted dynamic zsg s and identified conditions under which they have a unique equilibrium value while patek and bertsekas  @xcite extended these results to more general stochastic shortest path ( ssp ) games .",
    "s have many applications including pursuit - evasion / hunter - prey problems , heads - up poker and many so - called parlor games .",
    "many other important problems , however , can be also modeled as zsg s .",
    "these problems generally have an adversarial feature and applications can be found in many fields ranging from business to cyber - security and disease control .",
    "in addition , risk - averse control problems can also be cast as zsg s by assuming that nature plays an adversarial role and selects probability distributions , payoffs or some other feature with a view to thwarting the decision - maker .",
    "this is a common approach in the multi - armed bandit literature , for example , where the goal is to tradeoff the benefits of exploitation and exploration .",
    "while standard dynamic programming ( dp ) methods such as value and policy iteration have been adapted to solve zsg s , these methods quickly become computationally intractable as the game size grows . indeed many important games of interest are too large to be solved exactly and require approximate solution approaches .",
    "texas holdem heads - up poker , for example , is a popular game and only very recently has the `` limit '' version of this game been effectively solved ; see bowling et al .",
    "@xcite as well as sandholm  @xcite for an overview of approaches to tackling incomplete information zsg s .",
    "many dp problems are also intractable and this of course has led to the development of the approximate dp literature .",
    "adversarial versions of these problems will clearly then also be intractable .",
    "it is therefore important that we be able to construct good sub - optimal policy pairs ( one for each player ) for large - scale zsg s .",
    "it is generally straightforward to simulate a policy pair and therefore obtain an unbiased estimate for the game value of this pair . but how far is this value from the ( optimal ) game value ?",
    "while we ca nt answer this question unless we can solve for the game value itself , we can obtain bounds .",
    "in particular , if we fix one player s strategy we can then solve for the other player s best response and the resulting value will be a bound on the game value . unfortunately , solving for the best response is a dp problem that will itself often be intractable .",
    "the goal of this paper is to demonstrate how tight bounds on these best - response problems can be found so that we can still obtain lower and upper bounds on the optimal game value .",
    "we do this via the recently developed information relaxation approach to obtaining dual bounds for dp problems . moreover , strong duality ( see part ( c ) of prop . [",
    "prop : bss ] ) tells us that if we start off with a policy pair that is close to optimal , then it should be possible to obtain tight lower and upper bounds on the optimal game value .",
    "such bounds would then `` certify '' just how good the sub - optimal policy pair is .",
    "the information relaxation approach was introduced independently by brown , smith and sun  @xcite ( hereafter bss ) and rogers  @xcite who in turn were partly motivated by earlier work  @xcite on the pricing of high - dimensional american options . over the past",
    "several years there have been many successful applications of this methodology ; see for example  @xcite .",
    "this paper was also motivated in part by beveridge and joshi  @xcite who used the aforementioned work on american options to compute dual bounds for zero - sum optimal stopping games .",
    "this paper explains how dual bounds can be computed for general zsg s as long as these games have an optimal value .",
    "the remainder of this paper is organized as follows .",
    "we formulate the zsg problem and review the results of shapley  @xcite in section [ sec : zsg - form ] and then review the information relaxation approach for constructing dual bounds in section [ sec : inforelax - review ] .",
    "we provide two examples in section [ sec : egs ] : a simple 2-period matrix game and an industrial - waste inspection game .",
    "we briefly discuss some specific issues and challenges that arise in the context of zsg s in section [ sec : adv ] and we conclude in section [ sec : conc ] .",
    "we now provide a brief overview of zsg s and we mainly use the same notation and simplifying assumptions ( finite action sets and state spaces , common information , simultaneous moves etc . ) of section 7.2 of bertsekas and tsitsiklis  @xcite .",
    "we note , however , that there is no problem relaxing these assumptions .",
    "we essentially only require that the zsg has a value in order to apply the information relaxation approach to compute dual bounds .",
    "there are two players : we refer to player _ a _ as the _ maximizer _ and player _ b _ as the _ minimizer_. at each time @xmath0 and in each state @xmath1 , _ a _ and _ b _ choose actions @xmath2 and @xmath3 from finite constraint sets , @xmath4 and @xmath5 , respectively .",
    "transition probabilities are of the form @xmath6 and one - stage costs are given by @xmath7 .",
    "the players use randomized strategies to select @xmath2 and @xmath3 in each period .",
    "in particular , in state @xmath1 player _ a _ chooses a probability distribution @xmath8 over the set @xmath4 while _ b _ chooses a probability distribution @xmath9 .",
    "the system therefore moves from state @xmath1 to state @xmath10 with probability @xmath11 and the stage cost is @xmath12 a policy @xmath13 for player _ a _ is a sequence of functions , @xmath14 , so that _ a _ selects his time @xmath15 , state @xmath1 action via the probability distribution , @xmath16 which is defined over @xmath4 .",
    "similarly we use @xmath17 to denote a policy for player _",
    "b_. stationary policies have the form @xmath18 and @xmath19 and we simply refer to them as @xmath20 and @xmath21 , respectively .",
    "the cost - to - go for policies @xmath22 and @xmath23 starting from state @xmath1 is @xmath24\\ ] ] where @xmath25 is the discount factor .",
    "we use @xmath26 to denote the cost - to - go corresponding to the pair of stationary policies @xmath27 .",
    "we can then define the min - max and max - min costs as @xmath28 we note that @xmath29 and @xmath30 are the optimal game values corresponding to the different orders in which _ a _ and _ b _ must choose their policies .",
    "the question then arises as to whether @xmath29 and @xmath30 are equal in which case we could define this common value to be the equilibrium value of the game .",
    "the key result of shapley  @xcite is that this is indeed the case .",
    "in particular , there exists a pair of stationary policies @xmath31 such that @xmath32 where we use @xmath33 to denote the equilibrium value of the game . given ( [ eq : shap ] ) it is not surprising that the various solution approaches to dynamic programming problems , e.g. value iteration and policy iteration , have natural analogs for zsg s .",
    "details are given in @xcite and for more general stochastic - shortest path games in @xcite . since",
    "finite horizon games can formulated as infinite horizon games by including time @xmath0 as a state variable , shapley s result also applies to these games resulting in optimal policies that are now time - dependent .",
    "let @xmath34 be any pair of stationary policies .",
    "then it follows immediately from ( [ eq : shap ] ) that @xmath35 where @xmath36 is _ b _ s best response to @xmath37 and @xmath38 is _ a _",
    "s best response to @xmath39 . in theory both @xmath36 and @xmath38 can be found using standard dp methods such as value or policy iteration . for large scale games , however",
    ", computing @xmath36 and @xmath38 ( as well as @xmath33 ) may be intractable . for games of practical interest , this is often the case and so the best we can hope to do is to find good policies , @xmath34 , and bounds on @xmath36 and @xmath38 . in particular",
    ", we would like to find a good lower bound , @xmath40 , for @xmath36 and a good upper bound , @xmath41 , for @xmath38 .",
    "in that case we will have @xmath42 and it follows that @xmath43 $ ] is an interval containing the true value of the game , @xmath33 .",
    "we note that finding a good lower bound for @xmath36 is not straightforward since @xmath36 is the solution of a dp where the goal is to _ minimize _ costs . any feasible policy for this dp",
    "therefore yields an upper bound for @xmath36 so that the inequality goes in the wrong direction .",
    "a similar observation applies to finding an upper bound for @xmath38 . in the next section",
    "we describe how the desired bounds , @xmath40 and @xmath41 , can be computed using the concept of information relaxations and dual penalties .",
    "the information relaxation approach was developed independently by bss  @xcite and rogers  @xcite but the former is more general and we follow their approach here . in particular we will consider a dp problem where the objective is to minimize total expected costs .",
    "this is the _ primal _ dp problem and it is exactly this problem .",
    "the approach is identical so we will consider only the construction of the dual lower bound here . ]",
    "that we face in trying to obtain a lower bound on @xmath36 as described above . in order to keep the notation manageable , however",
    ", we will drop all references to @xmath37 as well as any reference to the players .",
    "we will begin with finite horizon problems after which we will describe how infinite horizon problems can be handled .",
    "it will also be convenient to introduce some alternative notation to that developed in section [ sec : zsg - form ] .",
    "in particular , we will now use @xmath44 to denote a generic state and we will assume a state transition function @xmath45 where @xmath46 is the time @xmath0 state , @xmath47 is the action chosen at time @xmath0 and @xmath48 is a random variable . without loss of generality",
    "we can take the @xmath49 s to be iid @xmath50 random , we can take @xmath49 to be the random variable that is used to generate the state transition in accordance with @xmath51 where we are keeping player a s policy fixed at @xmath37 .",
    "while these probabilities depend on the action , @xmath47 , we can still take the @xmath49 s to be iid and have the state dependence handled via the transition function , @xmath52 .",
    "] variables .",
    "the information relaxation approach considers policies that use more information than is available to primal feasible policies .",
    "specifically , we consider policies that can use advance information about realizations of the uncertain sequence , @xmath53 ; such policies are generally not feasible to the primal dp . by finding the best such policies",
    ", we can obtain lower bounds on the optimal value of the primal dp .",
    "to make this idea concrete , we let @xmath54 denote the underlying filtration associated with @xmath53 , i.e. , @xmath55 , where @xmath56 denotes the @xmath57-algebra generated by @xmath58 at time @xmath0 .",
    "we will describe additional information structures via another filtration @xmath59 .",
    "we say @xmath60 is a _ relaxation of @xmath54 _ if @xmath61 for all @xmath0 , i.e. , under @xmath60 , at least as much ( and perhaps more ) is known about @xmath53 at all times @xmath0 .",
    "we let @xmath62 denote expectations conditional on @xmath63",
    ". we will refer to realizations of @xmath53 as _ scenarios _ or _",
    "dual paths_. a policy is adapted to a filtration @xmath54 ( or @xmath54-adapted ) if the actions taken by the policy in every period @xmath0 are measurable with respect to @xmath56 .",
    "we let @xmath64 denote the set of feasible , @xmath54-adapted policies .",
    "if @xmath60 is a relaxation of @xmath54 , then @xmath65 : a policy that is @xmath54-adapted is also @xmath60-adapted",
    ".    given a finite horizon , @xmath66 , the expected time @xmath67 cost for any @xmath68 can be written as @xmath69.\\end{aligned}\\ ] ] we let @xmath70 denote the optimal cost at time @xmath0 from state @xmath46 and we also define @xmath71 and @xmath72 .",
    "bss then consider penalty functions @xmath73 that depend on actions and uncertainties .",
    "they call a penalty function @xmath74 _ dual feasible _ if @xmath75\\leq 0 $ ] for all @xmath68 .",
    "the following result recaps some of the main results from bss .",
    "[ prop : bss ] * ( bss 2010 ) * let @xmath60 be any information relaxation of @xmath54 .",
    "* _ weak duality : _ for any dual feasible penalty @xmath74 , @xmath76.\\end{aligned}\\ ] ] * _ dual feasible penalties : _ let @xmath77 be any sequence of measurable functions @xmath78 . then the penalty given by @xmath79-{\\mathbb{e}_{{\\scriptscriptstyle{\\mathcal{g}_{t}}}}}[h_{t+1}(s_{t+1}(x_t , v_t , w_{t+1}))]$ ] is dual feasible . * _ strong duality : _ when @xmath80 , inequality ( [ eq : bss_weak_duality ] ) holds with equality",
    ".    part ( a ) of prop .",
    "[ prop : bss ] shows that we can get lower bounds with any information relaxation and any dual feasible penalty .",
    "for example , when @xmath60 is the perfect information ( pi ) relaxation where the entire scenario @xmath81 is revealed at @xmath67 , we can simply simulate scenarios and select actions `` path - wise '' that minimize the sum of costs plus penalties .",
    "the expected value of the optimal costs ( plus penalties ) then provides a lower bound on the optimal value @xmath82 .",
    "more specifically , assuming a pi relaxation we can compute an unbiased lower bound on @xmath83 by simulating @xmath84 scenarios , @xmath85 for @xmath86 , and then solving for @xmath87 where the @xmath46 s satisfy ( [ eq : bss - state - dyn ] ) with @xmath88 replacing @xmath49 .",
    "( with a pi relaxation the `` inf '' in ( [ eq : bss_weak_duality ] ) can be moved inside the expectation which then yields a `` dual '' or ",
    "problem of the form ( [ eq : dual ] ) . ) an unbiased lower bound for @xmath89 is then given by @xmath90 .",
    "part ( b ) of prop .",
    "[ prop : bss ] is useful because it provides a method for constructing dual feasible penalties : we can obtain a dual feasible penalty by adding up differences in conditional expectations of a sequence of any `` generating functions '' @xmath77 .",
    "finally , part ( c ) of prop .",
    "[ prop : bss ] shows that this approach , in theory , provides tight bounds : by taking the generating functions to be the optimal cost functions , the optimal value from the relaxed problem equals the optimal cost of the primal dp . of course , when applying the method , we would typically not know @xmath91 , but we can take the generating functions to be approximate value functions , @xmath92 , and , by parts ( a ) and ( b ) of prop . [",
    "prop : bss ] , we will nonetheless obtain a lower bound on the optimal cost .",
    "the closer @xmath92 is to @xmath91 the better we expect the lower bound to be . as mentioned in section [ sec : intro ] , there have now been many successful applications of this methodology in the dp literature .",
    "we note that bss also showed that for any given penalty , @xmath93 , information relaxations of @xmath54 that reveal less information yield tighter dual bounds .",
    "as stated earlier , shapley  @xcite solved the infinite horizon discounted zsg problem ( which includes the finite - horizon zsg as a special case ) .",
    "while the information relaxation approach was originally developed for finite horizon dp s , brown and haugh  @xcite show how it can easily be extended to infinite horizon discounted problems .",
    "the industrial - waste inspection game below , however , is not an infinite horizon discounted zsg but is instead a stochastic - shortest path ( ssp ) zsg .",
    "an ssp zsg is a non - discounted infinite horizon game that has a terminal absorbing state that may or may not be reached in a finite number of time periods .",
    "the theory of such games is more delicate and was treated by patek and bertsekas  @xcite .",
    "one potential difficulty that arises in constructing dual bounds for ssp zsg s is in simulating a dual sample path , @xmath94 .",
    "the absorption time , @xmath66 , is random and policy - dependent and so it s not clear how to simulate such a path . the weak - form approach of rogers  @xcite and brown and haugh  @xcite",
    "show how this problem can be resolved .",
    "in particular , dual sample paths are simulated under a reference transition probability measure , @xmath95 , that is _ action - independent_. we can use such a @xmath95 to generate dual sample paths as long as all such paths terminate after a finite number of periods @xmath95-almost surely .",
    "appropriate radon - nikodym terms are then used to adjust the objective function in ( [ eq : dual ] ) to account for this change of measure .",
    "it is also necessary to ensure certain absolute - continuity conditions are either satisfied or handled appropriately .",
    "space constraints prevent us from expanding further on these issues but see  @xcite for further details .",
    "we now consider two examples where we use the information relaxation methodology to construct dual bounds on the optimal game values .",
    "these are simply illustrative examples so in each case we were in fact able to solve for both the best responses , @xmath96 and @xmath97 , as well as the optimal game value .",
    "the dual bounds were constructed in each case using the value function , @xmath98 , corresponding to some fixed policy pair @xmath99 , to construct dual feasible penalties .",
    "we do note , however , that approximate value functions can be constructed in many different ways and that it may be advantageous to use different approximate value functions ( and indeed different transition measures , @xmath95 ) to construct the dual penalties required for each of the two dual bounds .",
    "we first consider a single - period two - person zsg where the payoff is defined by an @xmath100 matrix , @xmath101 . in this",
    "game players _ a _ and _ b _ simultaneously select a row , @xmath2 , and a column , @xmath3 , respectively , after which _ b _ pays _ a _ the value @xmath102 . a mixed policy , @xmath103 , for player _ a _ is an @xmath104 vector such that _ a _ chooses the @xmath2-th row with probability @xmath105 .",
    "similarly , we let @xmath74 denote player _",
    "b _ s mixed policy .",
    "the expected payoff to _ a _ is then @xmath106 .",
    "consider now a @xmath107-period dynamic game defined by the following three matrix games : @xmath108 the state variable @xmath109 determines the game @xmath110 that is played at time @xmath0 , and @xmath111 is then the payoff of that game when _ a _ and _ b _ choose the @xmath112 row and @xmath113 column , respectively .",
    "the game begins at @xmath67 with _ a _ and _ b _ playing @xmath114",
    "so the initial state is @xmath115 .",
    "the game to be played at @xmath116 is determined by _ a _ and _ b _ s actions and a transition probability , @xmath117 .",
    "these transition probabilities are determined by the @xmath118 element in the matrices @xmath119 which are defined as : @xmath120 for example , if _ a _ chooses the second row and _ b _ chooses the first column when playing @xmath114 at time @xmath121 , then _",
    "b _ pays _ a _ @xmath122 units and at time @xmath116 they will play game @xmath123 with probability @xmath124 and game @xmath125 with probability @xmath126 . the optimal policies and value functions for this 2-period zsg",
    "are easily calculated using standard techniques and are given in table [ table : matrixgameoptimaltable ] .",
    ".optimal policies and game values [ cols=\"^,^,^,^,>\",options=\"header \" , ]     we can compute an upper bound for the fair value of the game , @xmath127 , by fixing _ b _ s policy , @xmath39 , and then solving for _ a _",
    "s best response . if we take @xmath128 ' ,   \\ , \\hat{\\nu}_{1 } = v_{1}^{*}(x_{1})\\right\\}$",
    "] , then the solution to _ a _",
    "s best response dp problem is given by : @xmath129 where @xmath130 we note that @xmath131 is of course an upper bound on the value of game , @xmath132 .",
    "for more complex games we would not be able to compute @xmath133 but we can instead use the information relaxation approach to compute an upper bound , @xmath134 , on @xmath133 .",
    "suppose we use a pi relaxation and construct the penalty @xmath135 $ ] where @xmath136 is an approximate value function for player _ a _ s dp .",
    "for illustrative if _ a _ always chooses first row and _ b _ always chooses first column when playing @xmath137 . ]",
    "purposes , we take @xmath138 .",
    "we estimated the dual bound by simulating 10,000 values of @xmath139 , solving the deterministic dual inner problem ( [ eq : dual ] ) for each value , and then averaging the results .",
    "this yielded an estimated upper bound @xmath140 with a standard error of @xmath141 .",
    "as an aside , we note that if we instead used the dual optimal penalty @xmath142 $ ] , each dual inner problem ( [ eq : dual ] ) yields an upper bound @xmath143 , thereby demonstrating strong duality ( for _ b _ s fixed policy , @xmath39 ) as defined by part ( c ) of prop .",
    "[ prop : bss ] .",
    "note also that if we fixed _",
    "b _ s policy at the optimal @xmath144 and repeated the numerical calculations above , then we would obtain @xmath145 .",
    "corresponding lower bounds for @xmath146 can be obtained analogously by fixing _",
    "s policy and solving or lower bounding player _",
    "b _ s resulting dp .",
    "we now consider a somewhat more practical game as discussed in patek  @xcite .",
    "the two players are a manufacturer ( player _ a _ ) who produces industrial waste that must be dumped every night , and an inspector ( player _ b _ ) who wants to catch the manufacturer dumping .",
    "there is a finite number of geographically disparate sites where industrial waste can be dumped .",
    "_ a _ must dump waste at one of these sites every night while avoiding detection by _",
    "b_. in order to detect dumping activity on a given night , _ b _ must inspect the same site where _ a _ is dumping that night and even then , there is a nonzero probability of failing to catch _",
    "a_. in particular ,",
    "conditional on _ a _ and _ b _ selecting the same site , the probability of detection depends upon two considerations :    1 .",
    "the closer the current dumping site is to the preceding dumping site , the greater the probability of detection .",
    "( this models the environment s limited ability to absorb waste . )",
    "the closer the current inspection site is to the preceding inspection site , the greater the probability of detection .",
    "( this models the fact that the inspector needs less time to travel and so he has more time to look for dumping activity . )",
    "if _ b _ detects _ a _ dumping two nights in a row , then _ a _ is put out of business .",
    "s objective is to maximize its time in business , while _ b _ s objective is to minimize _ a _ s time in business . in deciding where to dump /",
    "inspect each night , we assume that both players know where the dumping and inspection occurred on the previous night .",
    "they also know whether or not _ a _ was caught dumping the previous night .",
    "patek  @xcite shows that this game has an optimal equilibrium value .",
    "we can formulate this as a zero - sum ssp game as follows .",
    "let @xmath147 represent the @xmath84 sites where waste may be dumped .",
    "we let @xmath148 denote the site where _ a _ dumped at time @xmath149 and we let @xmath150 denote the site inspected by _",
    "b _ at that time . let @xmath151 be a boolean variable which is true if _ a _ was caught dumping at time @xmath149 .",
    "the state vector @xmath152 then describes the state of the system at time @xmath0 .",
    "there are are not possible unless @xmath153 .",
    "hence there are @xmath154 possible states . ]",
    "@xmath155 possible non - absorbing states and one absorbing state .",
    "given the manufacturer _ a _ has not yet been shut down at time @xmath0 , _ a _ chooses to dump at site @xmath156 and _ b _ chooses to search at site @xmath157 .",
    "the probability that _ a _ will then be detected on night @xmath0 is @xmath158 , & \\text{if } u_{t } = v_{t}\\\\ 0 , & \\text{otherwise}\\end{cases}\\ ] ] where @xmath159 denotes the distance between sites @xmath160 and @xmath161 , @xmath162 is the max distance between any two sites , @xmath163 are worst - case and ideal probabilities of detection , and @xmath164 and @xmath165 are positive weighs .",
    "if @xmath166 false , then the system transitions to state @xmath167 with probability @xmath168 , otherwise the system transitions to @xmath169 .",
    "if @xmath166 true , then the game transitions to the absorbing terminal state with probability @xmath168 , otherwise the system transitions to @xmath169 .",
    "the cost is 1 in each time period regardless of the controls applied .",
    "we consider a case with @xmath170 sites that are located on a straight line with equal distances between successive sites and where @xmath171 .",
    "we assume parameter values of @xmath172 , @xmath173 , @xmath174 and @xmath175 .",
    "we begin with suboptimal policies , @xmath176 , where _ a _ and _ b _ select sites uniformly each night and we compute the game value , @xmath177 , for these policies .",
    "we then use the so - called naive policy iteration algorithm ( see @xcite and @xcite ) to update these policies .",
    "we use @xmath178 to denote the policies resulting from @xmath15 rounds of policy iterations and the corresponding game values are denoted by @xmath179 . after each policy iteration",
    ", we also computed each player s best response to the other player s strategy . for more complex games in general , these best responses",
    "can not be calculated and so we therefore also computed dual bounds to these best response value functions .",
    "the dual bounds were computed using dual feasible penalties constructed according to part ( b ) of prop .",
    "[ prop : bss ] with @xmath180 taken equal to @xmath181 .",
    "the results are displayed since in that case the bounds were very far apart  the dual lower and upper bounds were 128.8 and 365.5 , respectively  and would disrupt the scale of the plot . ] in figure [ fig : dumpgame ] for the initial state @xmath182 and were computed using the weak - form dual approach of rogers  @xcite and brown and haugh  @xcite .",
    "we used a @xmath95 that was action - independent and that moved from any state ( except the absorbing state ) to any other state with equal probability , i.e. with probability @xmath183 .",
    "the terminal state was naturally also @xmath95-absorbing .",
    "note that the dual bounds become very tight after just two rounds of policy iteration .",
    "this of course is consistent with strong duality , i.e. part ( c ) of prop .",
    "[ prop : bss ] , which suggests that dual penalties constructed from better approximations to the value function should yield tighter dual bounds .",
    "there are several interesting features of the information relaxation approach that are even more pronounced for zsg s .",
    "we briefly discuss some of these features now .",
    "many of the most interesting zsg s are imperfect information games where one or both players have private information .",
    "the best response of each player is then a partially observed markov decision process ( pomdp ) .",
    "it is well known ( see @xcite ) that these problems can be converted into dp problems and so these games fit into our framework .",
    "that said , there is considerably more flexibility regarding the choice of information relaxation , @xmath60 , and we can expect this flexibility to greatly influence the tractability of the dual inner problems .",
    "moreover , there is no reason to require that the dual lower and upper bounds are constructed using the same set of simulated paths , penalties , control variates ( to reduce monte - carlo uncertainty ) or transition measures , @xmath95 . indeed , depending on the form of transition measures used to generate dual inner problems , it may be necessary to use different penalties for the dual lower and upper bounds .",
    "we also note that adversarial versions of control problems can be considerably harder to handle .",
    "for example , brown and haugh  @xcite use lagrangian relaxation methods to construct dual penalties and therefore obtain tight dual bounds for an intractable multiclass queuing problem .",
    "this approach might not work , however , for most adversarial versions of this problem where nature gets to choose the ( presumably ) state - dependent arrival rates .",
    "this introduces new challenges in finding suitable approximate value functions / dual penalties for these problems .",
    "we have shown how the information relaxation approach for dp problems can be applied to intractable zsg s where the best response dp problems are also intractable .",
    "this is an active area of research within the dp literature with many interesting questions still to be addressed and new challenges that arise in the context of large - scale zsg s ."
  ],
  "abstract_text": [
    "<S> dynamic zero - sum games are an important class of problems with applications ranging from evasion - pursuit and heads - up poker to certain adversarial versions of control problems such as multi - armed bandit and multiclass queuing problems . </S>",
    "<S> these games are generally very difficult to solve even when one player s strategy is fixed , and so constructing and evaluating good sub - optimal policies for each player is an important practical problem . in this paper , we propose the use of information relaxations to construct dual lower and upper bounds on the optimal value of the game . </S>",
    "<S> we note that the information relaxation approach , which has been developed and applied successfully to many large - scale dynamic programming problems , applies immediately to zero - sum game problems . </S>",
    "<S> we provide some simple numerical examples and identify interesting issues and complications that arise in the context of zero - sum games . </S>"
  ]
}