{
  "article_text": [
    "recognizing human activities in sequences , known as human activity recognition ( har ) , is a key research topic in human - computer interaction and human behaviour analysis in ubiquitous computing  @xcite . among diverse human activities ,",
    "some are more frequent or are of different durations than others . for example",
    ", an older person may spend prolonged periods in bed or sitting as opposed to ambulating .",
    "consequently , class imbalance is an inherent nature of har problems .",
    "time series signals in real applications are often collected from multiple sensors with very different characteristics .",
    "thus it is very challenging to design good features and classifiers to recognize these activities both rapidly and accurately .",
    "a typical workflow of har methods for sequential data collected from wearable sensors contains the following steps : preprocessing , segmentation , feature extraction , and classification .",
    "many works focus on handcrafting effective features such as signal - based feature  @xcite , transform - based feature  @xcite , and multilevel features  @xcite .",
    "another research avenue focus on using powerful learning algorithms to train classifiers ( using existing features ) such as support vector machines ( svms )  @xcite , boosting  @xcite , and temporal probabilistic graphical models such as hidden markov model ( hmm )  @xcite , conditional random field ( crf )  @xcite , and semi - markov models  @xcite .    due to the success of deep learning  @xcite ,",
    "researchers have recently started to learn har features in an end - to - end fashion instead of handcrafting them .",
    "@xcite learned the layers of the autoencoder network with restricted boltzmann machine for har .",
    "@xcite proposed convolutional neural networks ( cnns ) based approaches to automatically extract discriminative features for har .",
    "@xcite utilized cnns to model audio data for ubiquitous computing ( ubicomp ) applications .",
    "@xcite investigated different types of deep neural networks for har with wearable sensors data .",
    "@xcite explored temporal deep neural networks for active biometric authentication .",
    "these deep learning based methods outperform other methods due to their ability of learning better features ( than handcrafted ones ) .",
    "however , regardless of using hand - crafted features or learned features , the input sensor data are always segmented into sections , typically using a sliding - window .",
    "it is hoped that each segment only contains a single activity .",
    "given a sliding - window , one label is generated for all samples within the window . while widely used , this has several of issues .",
    "first , the sliding - window procedure creates the difficulty of defining the best window length , sampling stride and window labeling strategy ; second , the samples in one window may not always share the same ground truth label .",
    "people resort to intuitions and heuristics such as majority voting to force all samples to take one label .",
    "this inevitably loses original information , and causes label inconsistency  true labels or ground truth of some samples are not the label of the window and this can misguide the classifier ; third , for current sliding window based cnn methods , the window size for testing data must be the same as the window size used during the training .",
    "the imposes two issues .",
    "if users believe a different window size is better for the new testing data , they either have to stick to the old window size during the training ( which they believe will be worse ) , or re - train the model from scratch .",
    "there are cumbersome ways to introduce some flexibility to the window size , but at the cost of speed and efficiency .",
    "then the need to accumulate data over a fixed window can lead to intolerable prediction delays in real - time applications .    to manage these issues ,",
    "we propose a method that can efficiently predict labels for each individual sample ( which we call dense labeling ) without any window based labeling procedure .",
    "our contributions are three folds : i ) we are the first to do dense labeling for har and avoid the label inconsistency problem caused by all sliding window approaches ; ii ) our algorithm based on a fully convolutional network is much more efficient than cnn counter - parts , and can handle sequences of arbitrary length without window size restrictions ; and iii ) we release a new daily activity dataset collected from hospitalised older people which we believe will be beneficial to the community .",
    "human activity recognition can be viewed as a sequence labeling problem that estimates the class label of activity at each time step in long - term time sequence .",
    "the recent human activity recognition methods usually use a sliding window of fixed length to segment the data , and assign a class label to the window base on the label of the last time step ( data point ) in the window as shown in fig .",
    "[ fig : labeling ] .",
    "however , segmenting a continuous sensor stream is a difficult task , and the exact boundaries of an activity are difficult to define  @xcite . as illustrated in fig .",
    "[ fig : labeling ] , for example , the time series sensor data contains six - class activities . with a fixed sliding window size",
    ", one may annotate the class of sliding window @xmath0 as _ sit to stand_. however , the window @xmath0 contains four - class activities .",
    "class _ lying , get up _ , and _ sitting _ are lost in window @xmath0 . instead of making inaccurate segments that miss activity boundaries",
    ", we propose to train and predict the class of every sample densely and without window based labeling as shown in the bottom of fig .",
    "[ fig : labeling ] .",
    "our method is efficient because we are able to directly generate the dense labels of the sequence in one forward pass of our network without the need for subsampling .            in the traditional method , generating windows for every time step , which we denote as setting the sampling stride to 1 , will also achieve dense predictions of the sequence .",
    "however this straight forward and naive approach will result in a huge number of windows and the prediction process will became intractably slow . specifically , in a conventional cnn based window classification approach , every window requires one forward pass of the cnn network , and it is clearly prohibitively expensive when handling a huge number of windows .",
    "this is also the reason that window based methods usually use a large sliding window stride to generate windows to subsample the labels of the original data . here",
    ", we introduce the fully convolutional network ( fcn ) for this task .",
    "we are able to achieve dense and accurate predictions and , at the same time , only require low computational cost for the prediction process ; these attributes make our approach an effective solution in practice .",
    "[ fig : cnn_fcn ] illustrates the different prediction procedure of conventional cnn and the fcn based network .",
    "we show the computational performance of the proposed method in fig .",
    "[ fig : time ] .",
    "the main component of the proposed method is a fully convolutional network , which is an extension of convolution neural networks  @xcite .",
    "cnns can be viewed as an enhancement of the standard multi - layer perceptron ( mlp ) , where the main difference is the addition of convolutional layers .",
    "the basic components in cnn contain convolutional operation , pooling operation , and activation function .",
    "when cnns are constructed by stacking these basic components layer by layer , a network that only contains the nonlinear filter is called _ fully convolutional network _ ( fcn )  @xcite , which was originally introduced for the task of semantic segmentation .",
    "one advantage of fcn is that it can take input of arbitrary size and produce dense prediction with efficient inference and learning . to our best knowledge",
    ", we are the first to apply fcn into dense sequence labeling for har .      here",
    "we describe how to exploit fcn architecture for the problem of dense labeling of human activity sequences .",
    "the dense labeling estimates the likelihood of a particular activity at a certain time , and then predicts the class of the activity accordingly .",
    "the duration ",
    "length  of activities vary for different sequences and there are multiple activities in the input sequence , consequently , it is a naturally dense prediction task which requires label prediction for each sample in the sequence",
    ". therefore , it is reasonable to treat the recognition of human activity as a dense labeling problem , where the value at each location points out the confidence of each activity .    the proposed network architecture",
    "is shown in fig .",
    "[ fig : architecture ] .",
    "the input to the network is time sequential data and its output is a dense confidence map of the input sequence .",
    "we denote the input of layer @xmath1 at location @xmath2 of filter channel @xmath3 as @xmath4 , where @xmath5 is the feature index , @xmath6 indicates the sample s location in total @xmath7 length sequence , and @xmath8 represents the index of filter",
    ". the input layer is connected by a convolution layer .",
    "for each filter @xmath3 of layer @xmath1 , the output map is defined as a convolution with a kernel of size @xmath9 followed by the addition of a constant `` bias '' term . to capture local temporal context",
    "we restrict each trainable filter with a small size .",
    "let @xmath10 be the components of the kernel and @xmath11 the constant bias for channel @xmath3 of the layer @xmath1 , respectively .",
    "then the output of a specific location @xmath2 in channel @xmath3 of the layer @xmath1 is obtained by : @xmath12 where @xmath13 is the activation function that introduces the nonlinearity into neural networks and allows it to learn more complex models . in this paper , we use rectified linear units ( relu )  @xcite as nonlinearity function in all activation layers .",
    "relu has been shown to work very well on deep convolutional networks for object recognition , and enforces small amounts of sparsity in neural networks .",
    "the relu activation function is defined as @xmath14 .",
    "the convolution and activation layers are followed by a pooling layer .",
    "the purpose of pooling layer is to makes it robust to small variations in previously learned features , which is similar to a smoothing operation .",
    "the pooling strategy used in this work is maxpooling , which computes maximum value in each neighborhood at different positions . recalling the architecture described in fig .",
    "[ fig : architecture ] , we have kernel size @xmath15 , @xmath16 , and the number of filter @xmath17 in the first convolutional layer , and @xmath18 , @xmath19 , @xmath17 in the first pooling layer .    as illustrated in fig .",
    "[ fig : architecture ] , we repeat those three layers ( _ i.e. _ , convolution , relu , and maxpooling ) , in sequence , six times with the same parameters . then , the output is passed through a dropout layer that randomly set part of the inputs to zero with a specific probability . finally ,",
    "unlike the previous work using cnn architecture , we replace the fully connected layers with convolutional layers .",
    "there are @xmath20 filters in the final convolutional layer , where @xmath20 is the number of classes of human activities .",
    "the kernel size of each filter is @xmath21 .",
    "the last layer of the proposed network is a dense softmax loss layer .",
    "each unit of the output reflects a class membership probability : @xmath22 .",
    "thus , we can obtain the posterior probability of the dense sequence classification results .",
    "in contrast with the previous cnn methods for human activity recognition  @xcite , we make three important modifications to make the network architecture more suitable for the task of dense labelling of sequences for har : i ) we replace the fully connected layers with the convolutional layers where the kernel size is @xmath23 and stride is @xmath24@xmath25 is the dimension of input sequence ; ii ) for our convolution and pooling layers , we use padding to compensate for the kernel size and ensure the output is the same size as the input as shown in fig .",
    "[ fig : architecture ] , although the number of filters may change ; and iii ) we apply the dense softmax loss layer which densely impose the softmax classification loss for the predictions of all time steps .",
    "this is different from the existing methods which apply the conventional softmax loss layer for the prediction of a single window .",
    "the replacement of convolutional layer is very efficient for processing large length time sequences for dense labeling compared with applying sliding windows with the classical cnns . in computer vision tasks , an fcn is able to classify the whole image at once by considering the summation of loss function of each individual loss at a specific location .",
    "however , in human activity recognition problems , we are not able to take the entire long - term time sequence as input to the network .",
    "it is computationally and practically prohibitive due to the infinite nature of a time series . in the proposed method",
    ", we separate a long - term time sequence into several overlapped subsequences to perform dense prediction .",
    "we define  _ subsequence _ as a partition of length @xmath26 of contiguous samples from a whole sequence with starting position at @xmath27 . by doing sliding step",
    ", we can extract a large amount of overlapped subsequences from a long - term time sequence . notice that , the proposed _ subsequence _ is not the same as sliding window , because we do dense prediction for each sample in a subsequence_i.e . _ : one label for each sample  instead of predicting the sliding window as a whole_i.e .",
    "_ : one label for all samples .",
    "generally , the length of a subsequence can be far greater than the length of a window .",
    "we can define subsequences with sufficiently large length as shown in fig .",
    "[ fig : time ] .",
    "the parameters of the proposed network are trained end - to - end by minimizing the negative log - likelihood function over the training set , summing over all samples in the subsequences : @xmath28 where @xmath29 represents individual loss of @xmath30-th sample ( data point corresponding to one time step ) in a subsequence .",
    "the minimization of the loss function is performed by the stochastic gradient descent ( sgd ) algorithm , where we update the parameters after every subsequence .",
    "we compute the gradient over a mini - batch of the training subsequences .",
    "we randomly generate a number of subsequences to construct the mini - batch training data .",
    "once the parameters of the proposed fcn are learned from training data , we use the following two steps to get dense labeling of the whole time sequence .",
    "first , the dense confidence maps of each subsequence can be obtained by putting it into the proposed fully convolutional network .",
    "second , because the two adjacent subsequences have overlapped samples , we have to find the overlapped samples and calculate the average probability of being a certain human activity . the final dense prediction ( labeling ) of time sequence",
    "is done by taking the argmax of the average probability of each samples .",
    "this prediction scheme is able to classify the whole subsequence at once , which is more efficient than using a sliding window scheme with a stride of one to perform dense labelling .",
    "we conduct extensive experiments to verify the performance of the proposed dense labeling method for har . for fcn ,",
    "the learning rate for the whole network is set to @xmath31 initially , decreased to @xmath32 after 100 iterations , and training is stopped at 150 iterations .",
    "the network weights are learned using the mini - batch ( set to 10 ) sgd algorithm .",
    "the length of _ subsequence _ is set to 60 for training , and 100 for testing .",
    "if there is enough memory , the length of subsequence can be set from 1 to the maximum length of whole test sequence .",
    "the adjacent subsequences has 50@xmath33 overlap . for sliding window based methods , we set the window size to 24 .",
    "experiments were run on a machine with one gpu ( nvidia geforce gtx 770 ) , all model are implemented using matlab .",
    "we test three non neural network classification techniques ( _ i.e. _ , quadratic discriminant analysis ( qda ) , k - nearest neighbours ( knn , @xmath34 )  @xcite , and support vector machine ( svm )  @xcite ) , and three neural network methods ( baseline convolution network  baseline , convolutional neural network based method by  @xcite  cnn , and three - layer multi - layer perceptron  mlp ) .",
    "the baseline convolutional network contains four convolution - relu - max pooling layers , and follows one fully connected layer and softmax layer .",
    "the baseline , cnn and mpl use a traditional sliding window to generate classifications .",
    "the learning rate and iteration parameters of baseline are set to be the same as ours .",
    "we evaluate the proposed approach on two public benchmark datasets and a new dataset released by our group .",
    "* opportunity dataset . *",
    "the _ opportunity _ activity recognition dataset  @xcite comprises a set of daily activities collected in a sensor - rich environment . in this paper , we address two kinds of recognition problems : _ locomotion _ and _ gestures _ , and use the same subset employed in the _ opportunity _ challenge  @xcite to train and test our models . we evaluate the performance on _ all data , subject 1 , subject 2 _ and _ subject 3 _ of the dataset .",
    "we use the same training and testing set as in  @xcite .",
    "locomotion includes 5 classes with _ null_-class , and gestures contains 18 classes with _ null_-class . each sample",
    "is comprised of 113 real valued attributes , which are scaled to @xmath35 $ ] for training and prediction ; the same data pre - processing method will be applied to the following two datasets .",
    "* hand gesture dataset . * the _ hand gesture _",
    "dataset @xcite records arm movements of two people ( subject 1 and 2 ) performing a continuous sequence of different types gestures of daily living .",
    "the dataset has 12 classes with _",
    "null _ class , and the _ null _ class refers to periods with no specific activity .",
    "the used data includes three three - axis accelerometers and three two - axis gyroscopes recoding at a sampling rate of 32  hz .",
    "each sample has 15 real valued sensor attributes in total . for training , testing and validation",
    ", we use the same approach as that by @xcite to generate the data .",
    "* hospital dataset .",
    "* to verify the performance of the proposed method , we collect our own har dataset using an inertial sensor with one accelerometer and one gyroscope .",
    "each sample has 5 real valued attributes in total .",
    "we recorded 8 activities from 12 volunteer hospitalised older patients . during the trial",
    ", participants were informed only of the types of activities they were required to perform and the inertial sensor was worn loosely over their garments above the sternum area to monitor the activities , which include : _ lying , stand up ( sit to stand ) , sitting , walking , lie down ( get into bed ) , sit down ( stand to sit ) , get up , _ and _ standing_. all data is recorded at a frequency of approximately 10  hz .",
    "for all the compared methods , we set the data of the first eight volunteers as training data , the data from the following three volunteers are used for testing , and the last one for validation .",
    "this dataset will be publicly available at our project website .",
    "two kinds of evaluation protocols are employed in this paper : classification and misalignment measures .",
    "[ [ classification - measures ] ] classification measures : + + + + + + + + + + + + + + + + + + + + + + + +    we use three widely used metrics : weighted f - measure ( @xmath36 ) , wighted f - measure with _ null _ class ( @xmath37 ) , and accuracy ( ac ) . due to class imbalance in the three datasets ,",
    "we calculate the f - measure weighted according to their sample proportion : @xmath38 where @xmath39 is the class index and @xmath40 with @xmath41 the number of samples of the @xmath39-th class , @xmath20 the total number of samples and @xmath42 denotes precision while @xmath43 represents recall . here",
    "measures performance without _ null _ class .",
    "[ [ misalignment - measures ] ] misalignment measures : + + + + + + + + + + + + + + + + + + + + + +    the classification measures may be misleading when recognizing actions from continuously recorded data . therefore , we explicitly report different types of errors : _ true positive  tp , true negative  tn , overfill / underfill , insertion _ , and _ fragmentation / deletion / substitution _",
    "@xcite to measure predicted label misalignments with the ground truth .",
    "errors when the start or stop time of a predicted label is earlier or later than the actual time is an _ overfill _ while a predicted label is later or earlier than the actual is an _ underfill _ measure . predicting an action when there is _ null _",
    "activity is an _",
    "insertion_. _ fragmentation _ denotes the errors of predicting a _ null _ class in between an uninterrupted activity class .",
    "_ deletion _ represents the errors of assigning a _ null _ label when there is an activity .",
    "_ substitution _ measures the errors when an activity is misclassified as a different class other than _",
    "null_.      comparing the performance of different methods across different studies is difficult for many reasons such as the differing types of experimental protocols and feature used . to make a fair comparison , for qda , knn , and svm methods , we used a sliding window with a fixed step size for extracting the mean value of the sensor on each window as features .",
    "the obtained results are quite similar to the benchmark results on the opportunity dataset  @xcite .",
    "[ fig : conmat ]    we report the classification performance on the three datasets in tab .",
    "[ tab : opp ] , tab .",
    "[ tab : hand ] and tab .",
    "[ tab : ade ] , respectively . in general , cnn - based methods perform better on almost all dataset than non - cnn methods and our approach , denoted _ ours _ , performs better than all cnn - based methods .",
    "specially , class imbalance leads to overesting the performance of gesture recognition .",
    "_ ours _ obtains @xmath44 with wighted f - measure on _ all _ opportunity data , which is @xmath45 higher than cnn methods proposed by  @xcite .",
    "a similar conclusion can be drawn in _ subject 1 _ and _ subject 2 _ for gesture recognition .",
    "those results demonstrate that the proposed dense labeling method is more robust with respect to the class imbalance problem .",
    "note that , we do not report @xmath37 in tab .",
    "[ tab : ade ] because _ null _ class is not included in our hospital dataset .",
    "although the proposed approach obtains slight higher accuracy than the baseline method on the new dataset , the @xmath36 value of _ ours _ in tab .",
    "[ tab : ade ] is significantly higher than other methods .",
    "we advocate that the weighted f - measure can reflect the real performance of unbalanced datasets .",
    "[ fig : conmat ] shows the confusion matrix on _ hospital _ dataset generated by our method and cnn method proposed in @xcite .",
    "[ fig : time ]    fig .",
    "[ fig : misalign ] shows the misalignment performance on the three datasets . for clarity ,",
    "we only report the overall performance of all opportunity data in fig .  [",
    "fig : misalign](a ) , and combine several measures to one term in contrast with the benchmark in  @xcite .",
    "it can be seen that _",
    "ours _ achieves the best performance among the compared methods .",
    "this demonstrates that the dense labeling method more accurately finds the starting and ending point of activities in the temporarily dependent sequence data . because we do not contain a",
    "null _ class in the hospital dataset , _ fragmentation / deletion / substitution _ term in fig .",
    "[ fig : misalign ] ( c ) only represents the error of _ substitution_.    we report the computational performance of three convolutional neural networks on _ opportunity _ datasets in fig .",
    "[ fig : time ] .",
    "our approach only consumes one - fifth and one - tenth of the time cnn  @xcite and baseline approach consumed .",
    "there are 118,750 and 33,273 samples with 113-d features in _ all _ and _ subject 1 _ of _ opportunity _ dataset .",
    "note the maximum length of subsequence , 3000 , is limited by the machine used for experiments ( 4 gb memory , single gpu ) .",
    "in this paper , we have presented a new , efficient dense labeling approach for human activity recognition using a fully convolutional network . unlike existing methods",
    ", the proposed approach does not rely on a heuristics sliding - window step for producing ambiguous labels of the window ",
    "often a source of error during training and prediction .",
    "our approach uses a fully convolutional network framework based on the well - studied deep convolutional neural network theory and therefore making it easy to train and infer the dense labels of sequences as well as benefit from generalization and robustness to extract feature end - to - end .",
    "we also release a new dataset publicly for human activity recognition .",
    "the promising results demonstrate the effectiveness and efficiency of the proposed dense labeling approach ."
  ],
  "abstract_text": [
    "<S> recognizing human activities in a sequence is a challenging area of research in ubiquitous computing . </S>",
    "<S> most approaches use a fixed size sliding window over consecutive samples to extract features  either handcrafted or learned features  and predict a single label for all samples in the window . </S>",
    "<S> two key problems emanate from this approach : i ) the samples in one window may not always share the same label . consequently , using one label for all samples within a window inevitably lead to loss of information ; ii ) the testing phase is constrained by the window size selected during training while the best window size is difficult to tune in practice .    </S>",
    "<S> we propose an efficient algorithm that can predict the label of each sample , which we call _ dense labeling _ , in a sequence of human activities of arbitrary length using a fully convolutional network . </S>",
    "<S> in particular , our approach overcomes the problems posed by the sliding window step . </S>",
    "<S> additionally , our algorithm learns both the features and classifier automatically . </S>",
    "<S> we release a _ new _ daily activity dataset based on a wearable sensor with hospitalized patients . </S>",
    "<S> we conduct extensive experiments and demonstrate that our proposed approach is able to outperform the state - of - the - arts in terms of classification and label misalignment measures on three challenging datasets : opportunity , hand gesture , and our new dataset . </S>"
  ]
}