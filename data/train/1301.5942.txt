{
  "article_text": [
    "in this paper confidence intervals for the mutual information of two random variables with finite alphabets are established . while they are not particularly tight , they are the first where no further restrictions have to be considered , neither on being in an asymptotic regime nor on the underlying joint probability distribution . by quantization of random variables with a non finite alphabet",
    "it is also possible to find the lower bound of the confidence interval of the mutual information of such random variables .",
    "the simplicity of these confidence intervals also allows to give an upper bound on the necessary sample size when the confidence interval width , the confidence level , and the alphabet sizes are fixed .",
    "let @xmath0 , @xmath1 , @xmath2 , @xmath3 be two pairs of finite discrete random variables , with joint probability distributions @xmath4 here @xmath5 and @xmath6 and it is w.l.o.g",
    ". assumed that @xmath7 and that @xmath8 .",
    "the marginal probability distributions are @xmath9 , @xmath10 , @xmath11 and @xmath12 , where the marginals are calculated from the joint probalility distributions as usual .",
    "the shannon entropy @xcite is defined as @xmath13 and the joint entropy @xcite as @xmath14 all @xmath15s are natural if not stated otherwise .",
    "@xmath16 is defined as the binary entropy function @xmath17 the mutual information @xcite is defined as @xmath18 w.l.o.g .",
    "it is assumed , that @xmath19 , what can be done because the mutual information is symmetric ( @xmath20 ) , and therefore by renaming the variables if necessary it can be assumed that @xmath19 always holds .",
    "the variational distance between two probability distributions is defined as @xmath21 and similarly for the marginal distributions .",
    "it can be easily seen , that @xmath22 $ ] for any two probability distributions . the empirical joint distribution for an i.i.d",
    ". sequence of pairs @xmath23 , sampled from a distribution @xmath24 , is defined as @xmath25 where @xmath26 and @xmath27 is the kronecker delta .",
    "the following two bounds will be used to construct the confidence interval for mutual information and are stated here as two lemmas .",
    "[ lemma1 ] let @xmath28 and @xmath29 be two pairs of random variables taking values on the same range , with joint probability distributions @xmath24 and @xmath30 .",
    "let @xmath31 if @xmath32 , then it holds that @xmath33    [ lemma2 ] for any @xmath34 @xmath35    the first bound was found by zhang ( * ? ? ?",
    "* theorem 2 ) . in the next section",
    "this bound will be slightly improved and generalized for the usage here , using a result of ho and yeung ( * ? ? ?",
    "* theorem 6 ) .",
    "the second bound was originally found by weissman et al .",
    "* theorem 2.1 ) and slightly modified by ho and yeung ( * ? ? ?",
    "* lemma 3 ) to have no dependence on the true distribution .",
    "first , ( [ eqn_zhang ] ) is improved to yield :      the proof widely follows the lines of the proof of ( [ eqn_zhang ] ) in zhang ( * ? ? ?",
    "* eq . ( 2 ) ) , but replaces the entropy difference bound of zhang ( * ? ? ?",
    "4 ) by the corresponding bound in ho and yeung ( * ? ? ?",
    "* theorem 6 ) , what makes the new bound valid for any @xmath38 and also for any @xmath39 instead of @xmath40 . beyond this , some slight changes in the proof of zhang lead to a tighter bound .    first it is shown that @xmath41 @xmath42 in an analogous way it can be shown that @xmath43 .",
    "for @xmath44 then it holds : @xmath45 + 3 \\mathcal{h}(\\tfrac{\\epsilon}{2 } ) \\nonumber\\end{aligned}\\ ] ] in ( [ eqn_mi_app ] ) eq .",
    "( [ eqn_mi ] ) was used . in ( [ eqn_ho_th6 ] )",
    "the bound of ho and yeung ( * ? ? ?",
    "* theorem 6 ) was applied together with the assumption @xmath19 and therefore , by the assumption @xmath44 , with @xmath46 .    for @xmath47 the well known bounds on mutual information and entropy @xcite , @xmath48 and @xmath49",
    "are first used to show that @xmath50 what immediately implies latexmath:[\\ ] ] then for ( @xmath74 is the ceiling operator ) @xmath75 it holds that @xmath76    if @xmath77 then the probability of being within the bounds is trivially one , therefore @xmath78 is restricted to be less @xmath55 .",
    "then obviously only the first part of ( [ eqn_zhang_improved ] ) @xmath79 + 3 \\mathcal{h}(\\tfrac{\\epsilon}{2})\\ ] ] applies , where @xmath44 .",
    "it is easy to show , that this term is strictly increasing for @xmath80 .",
    "therefore there is only one solution for @xmath80 of equation ( [ th3_eqn1 ] ) which is just the desired maximal variational distance between the true and the empirical joint distribution .",
    "this @xmath38 is also the minimum root as stated in the theorem . then solving ( [ eqn_ci_proof1 ] ) for @xmath69",
    ", after the substitution of @xmath81 by @xmath70 , yields @xmath82 and therefore @xmath75 cleary suffices to guarantee @xmath76    the next theorem is an improvement of theorem [ theorem2 ] , that uses the entropy optimization procedures of ( * ? ? ?",
    "* theorems 2 and 3 ) , which depend on the actual empirical distribution , instead of the worst case entropy difference bound ( * ? ? ?",
    "* theorem 6 ) .",
    "[ theorem4 ] for any @xmath57 $ ] and @xmath58 , @xmath59 with @xmath19 let @xmath61 and let @xmath83 where the solutions for the entropy optimization problems are given in ( * ? ? ?",
    "* theorems 2 and 3 ) .",
    "then it holds that @xmath84    since @xmath85 as well as @xmath86 are @xmath87 , as shown in the proof of theorem [ zhang_improved ] , it is obvious that @xmath88 by the argumentation of the proof of theorem [ theorem2 ] again @xmath61 is fixed , and it follows that @xmath89",
    "theorem [ theorem3 ] can be seen as an upper bound for @xmath69 ( the number of samples ) , which is tight when theorem [ theorem2 ] is used to determine the confidence interval .",
    "this is explained by the fact , that the absolute entropy difference bound that was used to construct the confidence intervals is completely independent of the actual empirical distribution @xmath90 .",
    "also , by using the entropy difference bounds , the dependence between the entropies @xmath91 , @xmath92 and @xmath93 was ignored , since for example the worst case distribution @xmath94 is not necessarily the marginal of the worst case distribution @xmath90 , what makes the mutual information difference bound less tight again .    taken together , one can see that there is much room left for improvement . by this",
    ", @xmath69 of theorem [ theorem3 ] is an upper bound on the necessary smaples size .",
    "a first improvement of this situation was given in theorem  [ theorem4 ] .",
    "an approach for making also use of the dependence between the entropies is given as a conjeture and only for two binary random variables in @xcite .",
    "besides this in the preprint  @xcite , an algorithm for finding the lower bound of the confidence interval for a binary and an arbitrary finite random variable is given .",
    "this bound is tight in terms of the maximal variational distance between the empirical and the true joint distribution .",
    "in this section the different possibilities for the construction of the confidence intervals , which just have been discussed are compared in two numerical examples . in these particular examples",
    "it can be seen that the lower bound conjectured in @xcite ( called method 1 ) matches the lower bound of preprint  @xcite ( called method 2 ) which gives a further indication for the correctness of at least the lower bound in @xcite ( though there is still no proof available ) .",
    "the following setup is used : a binary symmetric channel ( bsc ) with input variable @xmath0 and output variable @xmath1 is given , where the bit error rate ( @xmath95 ) is equal to @xmath96 and the input probabilities @xmath97 .",
    "the joint probabilities therefore are @xmath98 in this case the true mutual information is known to be @xmath99 ( unlike in the sections before , in this section all @xmath15s are to the base 2 ) . then , taking @xmath100 samples from @xmath24 yielded the following exemplary empirical distribution @xmath101 now fixing the confidence level @xmath102 the predescribed methods could be used to estimate the confidence interval . before this is done , a good approximation to the best possible confidence interval is determined , where best possible interval is defined as having minimal interval width . therefore samples of size @xmath69 are sampled @xmath103 times from @xmath24 , yielding an exemplary empirical sampling cumulative distribution function ( cdf ) of @xmath104 ( shown in fig .",
    "[ fig : mi_cdf ] ) , which should be a sufficiently good approximation to the real sampling cdf of @xmath104 , due to the high number of samples .    [ c][c]@xmath104 [ c][c]empirical sampling cdf     then , since it can be seen from the empirical sampling cdf of @xmath104 that the sampling probability density function ( pdf ) is close to being unimodal and symmetric , the approximation to the smallest possible confidence interval is given by the @xmath105 and the @xmath106 of the empirical sampling cdf of @xmath104 ( both marked in fig .",
    "[ fig : mi_cdf ] ) .    in table",
    "[ table1 ] the results of the two methods described in section  [ sec_results ] ( theorem [ theorem2 ] and [ theorem4 ] ) and of method 1 and 2 , applied to @xmath90 , are given .    .",
    "[ cols=\"^,^,^,^ \" , ]",
    "the authors would like to thank the dfg for supporting their research with spp1395 in the projects hu634_7 and sti155_3 .    1",
    "t.  m. cover and j.  a. thomas , _ elements of information theory _",
    "new york : wiley , 2006 .",
    "z. zhang , `` estimating mutual information via kolmogorov distance , '' _ ieee trans .",
    "inform . theory _",
    "9 , pp . 32803282 , sep . 2007 . s .-",
    "w . ho and r.  w. yeung , `` the interplay between entropy and variational distance , '' _ ieee trans .",
    "inform . theory _",
    "59065929 , dec . 2010 .",
    "a.  g. stefani , j.  b. huber , c. jardin and h. sticht , `` towards confidence intervals for the mutual information between two binary random variables , '' in _ proc .",
    "workshop computational systems biology ( wcsb 2012 ) _ , ulm , germany , jun . 46 , 2012 . t. weissman , e. ordentlich , g. seroussi , s. verd and m.j .",
    "weiberger , `` inequalities for the l1 deviation of the empirical distribution , '' tech .",
    "rept . , hp laboratories palo alto , hpl-2003 - 97 ( r.1 ) , jun . 2003 .",
    "o.  g. othersen , a.  g. stefani , j.  b. huber and h. sticht , `` application of information theory to feature selection in protein docking , '' _ j mol model .",
    "_ , vol .  18 , no .",
    "4 , pp . 12851297 , jul . 2012 . a.  g. stefani , j.  b. huber , c. jardin and h. sticht , `` a tight lower bound on the mutual information of a binary and an arbitrary finite random variable in dependence of the variational distance , '' available at http://arxiv.org/abs/1301.5937 ."
  ],
  "abstract_text": [
    "<S> `` this paper is eligible for the student paper award ''    by combining a bound on the absolute value of the difference of mutual information between two joint probability distributions with a fixed variational distance , and a bound on the probability of a maximal deviation in variational distance between a true joint probability distribution and an empirical joint probability distribution , confidence intervals for the mutual information of two random variables with finite alphabets are established . </S>",
    "<S> different from previous results , these intervals do not need any assumptions on the distribution and the sample size . </S>"
  ]
}