{
  "article_text": [
    "one of the hurdles for identifying clinically meaningful patterns in medical data is the fact that much of that data is sparsely , irregularly , and asynchronously observed , rendering it a poor substrate for many pattern recognition algorithms .",
    "a large class of this problematic data in medical records is time - stamped categorical data such as billing codes .",
    "an icd-9 billing code , for example , with value 714.0 ( rheumatoid arthritis ) gets attached to a patient record every time the patient makes contact with the healthcare system for a problem or activity related to her arthritis .",
    "this could be an outpatient doctor visit , a laboratory test , a physical therapy visit , the discharge event of an inpatient stay , or any other billable event .",
    "these events occur at times that are in general independent from events for other conditions .",
    "we would like to learn things from the patterns of these clinical contact events both within and between diseases , but their ( often sparse and ) irregular nature makes it difficult to apply standard learning algorithms to them . to abstract away this problem , we consider the data as streams of events , one stream per code or other categorical label .",
    "we model each stream as a modulated renewal process and use the process s modulation function as the abstract representation of the continuous , longitudinal intensity of the patient s contact with the healthcare system for a particular problem at any point in time .",
    "the resulting inference problem is to estimate a probability density over the renewal process parameters and intensity functions given the raw event data .",
    "the practical utility of using a continuous function density to couple standard learning algorithms to sparse and irregularly observed continuous variables has been previously demonstrated @xcite .",
    "unfortunately , the method of inferring such densities for continuous variables is not applicable to categorical variables .",
    "this paper presents a method that achieves the inference for categorical variables .",
    "our method models the log intensity functions non - parametrically as gaussian processes , and uses markov chain monte carlo ( mcmc ) to infer a posterior distribution over intensity functions and model parameters given the events ( section [ sec : mrp ] ) .",
    "there are several existing approaches to making this inference ( section [ sec : prior - work ] ) , but all of the approaches we found have either flexibility or scalability problems with our clinical data .",
    "for example , clinical event streams can be bursty , and some existing methods are unable to adapt to or adequately represent this .    in this paper",
    "we demonstrate using synthetic data that our approach has accuracy , efficiency , and flexibility advantages over the best existing method ( section [ sec : experiment - synthetic ] ) .",
    "we further demonstrate these properties using synthetic data that mimics our clinical data , under conditions that no existing method that we know of is able to satisfactorily operate ( section [ sec : experiment - synthetic ] ) .",
    "finally , we use our method to infer continuous abstractions over real clinical data ( section [ sec : experiment - clinical ] ) .",
    "a renewal process models random events by assuming that the interevent intervals are independent and identically distributed ( iid ) .",
    "a modulated renewal process model drops the iid assumption and adds a longitudinal intensity function that modulates the event rate with respect to time .",
    "we consider a set of event times @xmath0 to form an event stream that can be modeled by a modulated renewal process . for this work",
    "we choose a modulated gamma process @xcite , which models the times @xmath1 as @xmath2 where @xmath3 is the gamma function , @xmath4 is the shape parameter , @xmath5 is the modulating intensity function , and @xmath6 .",
    "equation is a generalization of the homogeneous gamma process @xmath7 , which models the interevent intervals @xmath8 as positive iid random variables : @xmath9 where @xmath10 takes the place of a now - constant @xmath11 , and can be thought of as the time scale of event arrivals .",
    "the intuition behind is that the function @xmath12 warps the event times @xmath13 into a new space where their interevent intervals become draws from the homogeneous gamma process of .",
    "that is , the warped intervals @xmath14 are modeled by @xmath15 .    for our purposes ,",
    "a gamma process is better than the simpler and more common poisson process because a gamma process allows us to model the relationship between neighboring events , instead of assuming them to be independent or memoryless .",
    "specifically , parameterizing @xmath16 models a bursty process , @xmath17 models a more regular or refractory , and @xmath18 produces the memoryless poisson process .",
    "clinical event streams can behave anywhere from highly bursty to highly regular .",
    "we model the log intensity function @xmath19 as a draw from a gaussian process prior with zero mean and the squared exponential covariance function @xmath20 where @xmath21 sets the magnitude scale and @xmath22 sets the time scale of the gaussian process .",
    "we choose the squared exponential because of its smoothness guarantees that are relied upon by our inference algorithm , but other covariance functions could be used .    in our application",
    "the observation period generally starts at @xmath23 , and ends at @xmath24 , and no events occur at these endpoints .",
    "consequently , we must add terms to to account for these partially observed intervals . for efficiency in inference",
    ", we estimate the probabilities of these intervals by assuming that @xmath25 and @xmath26 are drawn from a homogeneous @xmath27 process in the warped space . the probability of the leading interval @xmath28 is then approximated by @xmath29 , which is equivalent to @xmath30 .",
    "the trailing interval is treated similarly .",
    "our full generative model is as follows :    1 .",
    "[ item : hyperpriors ] @xmath31 + @xmath32 + @xmath33 + @xmath34",
    "[ item : gp]@xmath35 using 3 .",
    "[ item : transform]@xmath36 4 .",
    "[ item : integral ] @xmath6 5 .",
    "@xmath30 ; @xmath37 6 .",
    "[ item : inversion ] @xmath38    step [ item : hyperpriors ] places a prior on @xmath22 that prefers smaller values , and uninformative priors on @xmath39 and @xmath21 .",
    "we set @xmath34 to avoid an identifiability problem .",
    "( @xcite set @xmath40 to avoid this problem .",
    "while that setting has some desirable properties , we ve found that setting @xmath34 avoids more degenerate solutions at inference time . )",
    "given a set of times @xmath1 , we use mcmc to simultaneously infer posterior distributions over the intensity function @xmath41 and the parameters @xmath39 , @xmath21 , and @xmath22 ( algorithm [ alg : inference ] ) , where for simplicity we denote @xmath42 . on each round",
    "we first use slice sampling with surrogate data ( * ? ? ?",
    "* code publicly available ) to compute new draws of @xmath43 , @xmath21 , and @xmath44 using as the likelihood function ( with additional factors for the incomplete intervals at the ends ) .",
    "we then sample the gamma shape parameter @xmath39 with metropolis - hastings moves under the same likelihood function .",
    "one challenge of this direct inference is that it requires integrating @xmath45 , which is difficult because @xmath46 does not have an explicit expression . under certain conditions ,",
    "the integral of a gaussian process has a closed form @xcite , but we know of no closed form for the integral of a log gaussian process .",
    "instead , we compute the integral numerically , relying on the smoothness guarantees provided by the covariance function to provide high accuracy .",
    "the efficiency bottleneck of the update is the @xmath47 complexity of updating the gaussian process @xmath48 at @xmath49 locations , due to a matrix inversion .",
    "naively , we would compute @xmath48 at all @xmath50 of the observed @xmath13 , with additional points as needed for accuracy of the integral . to improve efficiency ,",
    "we do not directly update @xmath48 at the @xmath13 , but instead at @xmath51 uniformly spaced points , where . we then interpolate the values @xmath52 from the values of @xmath53 as needed .",
    "we set the number of points @xmath51 by the accuracy required for the integral .",
    "this is driven by our estimate of the smallest likely gaussian process time scale @xmath54 , at which truncate the prior on @xmath22 to guarantee @xmath55 .",
    "the efficiency of the resulting update is @xmath56 , with @xmath51 depending only on the ratio @xmath57 .",
    "it helps that the time constant driving @xmath51 is the scale of changes in the modulating intensity function instead of the scale of interevent intervals , which is usually much smaller . in practice , we ve found @xmath58 to work well for nearly all of our medical data examples , regardless of the observation time span , resulting in an update that is linear in the number of observed points .",
    "additionally , the regular spacing in @xmath59 means that its covariance matrix generated by is a symmetric positive definite toeplitz matrix , which can be inverted or solved in a compact representation as fast as @xmath60 @xcite .",
    "we did not include this extra efficiency in our implementation , however .",
    "event times @xmath1 , regular grid @xmath59 , current function @xmath48 and parameters @xmath21 , @xmath22 , and @xmath39 updated @xmath48 , @xmath61 , @xmath62 , @xmath21 , @xmath22 , and @xmath39 with likelihood @xmath63 update @xmath64 , using slice sampling compute @xmath52 by smooth interpolation of @xmath53 @xmath65 compute @xmath66 from @xmath67 numerically compute @xmath63 = @xmath68 using update @xmath39 and @xmath63 with metropolis - hastings and",
    ".33     .33     .33     .33     .33     .33     there is a growing literature on finding patterns among clinical variables such as laboratory tests that have both a timestamp and a value ( see * ? ? ?",
    "* and its references for examples ) , but we are not aware of any existing work exploring unsupervised , data - driven abstractions of the purely time - domain clinical event streams that we address here .",
    "there is much prior work on methods similar to ours that infer intensity functions for modulated renewal processes .",
    "the main distinction between these methods lies in the way they handle the form and integration of the intensity function @xmath46 .",
    "approaches include using kernel - smoothing @xcite , using parametric intensity functions @xcite , using discretized bins within which the intensity is considered constant @xcite , or using a form of rejection sampling called _ thinning _ @xcite that avoids the integration altogether .",
    "the binned time approach is straightforward , and we share its use of gaussian processes for the log intensity function . however , there is an inherent information loss in the piecewise - constant intensity function approximation .",
    "moreover , its computational complexity is cubic with the number of bins in the period of observation . for our data , with events at 1-day or finer time resolution over up to a 15 year observation period ,",
    "this method is prohibitively inefficient . a variant of this approach that uses variable - sized bins @xcite has been applied to medical data @xcite .",
    "this variant is very efficient , but is restricted to a poisson process ( fixed @xmath18 ) , and the inferred intensity functions are neither intended to nor particularly well suited to forming an accurate abstraction over the raw discrete events .",
    "thinning is a clever method , but it is limited by the requirement of a bounded hazard function , which prevents it from being used with bursty gamma processes ( which have a hazard function unbounded at zero ) .",
    "one thinning method has also adopted the use of gaussian processes @xcite , but is much less efficient than our algorithm , with time complexity cubic in the number of events that would occur if the maximum event intensity were constant over the entire observation time span . for event streams with a small dynamic range of intensities , this is not a big issue , but our medical data sequences can have a dynamic range of several orders of magnitude .",
    "our method therefore has efficiency and flexibility advantages over existing methods , and we will demonstrate in the experiments that it also has accuracy advantages .",
    "in these experiments , we will refer to our inference method as the _ direct method _ because it uses direct numerical integration , as opposed to thinning , which avoids computing the integral at all .",
    "we tested the ability of both methods to recover known intensity functions and shape parameters from synthetic data .",
    "we then used the direct method to infer latent intensity functions from streams of clinical events .",
    ".49     .49     our first experiments were with the three parametric intensity functions below , carefully following @xcite and @xcite .",
    "we generated all data using the warping model described in ( section [ sec : mrp ] ) , with shape parameter @xmath69 .",
    "1 .   @xmath70 over the interval @xmath71}$ ] , 48 events",
    "@xmath72 on @xmath73 $ ] , 29 events . 3 .",
    "@xmath74 is the piecewise linear curve shown in figure [ fig : parametric - synthetic ] , on the interval @xmath75 $ ] , 230 events .",
    "we express these as normalized intensities @xmath76 , which have units of `` expected number of events per unit time '' , because they are more interpretable than the raw intensities and they are comparable to the previous work done using poisson processes , where @xmath18 .    we compared the direct method to thinning on these datasets .",
    "@xcite compared thinning to the kernel smoothing and binned time methods ( all assuming a poisson process ) , and @xcite compared thinning to binned time , assuming a gamma process with constrained @xmath17 .",
    "both found thinning to be at least as accurate as the other methods in most tests .",
    "we computed the rms error of the true vs.  the median normalized inferred intensity , the log probability of the data given the model , and the inference run time under 1000 burn - in and 5000 inference mcmc iterations .    on these datasets",
    "the direct method was more accurate than thinning for the recovery of both the intensity function and the shape parameter , and more efficient by up to two orders of magnitude ( figure [ fig : parametric - synthetic ] and table [ tab : parametric - synthetic ] ) .",
    "the results for thinning are consistent with those previously reported @xcite .",
    ".performance on synthetic data .",
    "rms : root - mean - squared error ; lp : log probability of data given the model ; rt : run time in seconds .",
    "best results for each measure are bolded . [ cols=\"<,^,^,^,^,^,^ \" , ]     additionally , the confidence intervals from the direct method are subjectively more accurate than from the thinning method .",
    "( that is , the 95% confidence intervals from the direct method contain the true function for about 95% of its length in each case ) .",
    "this is particularly important in the case of small numbers of events .",
    "as might be expected , we found the results for @xmath77 to be sensitive to the prior distribution on @xmath22 , given the small amount of evidence available for the inference .",
    "following @xcite and @xcite , we used a log - normal prior with a mode near @xmath78 , tuned slightly for each method to achieve the best results .",
    "we also allowed thinning to use a log - normal prior with appropriate modes for @xmath79 and @xmath80 , to follow precedent in the previous work , although it may have conferred a small advantage to thinning .",
    "we used the weaker exponential prior on those datasets for the direct method .",
    "our next experiments were on synthetic data generated to resemble our medical data .",
    "we tested several configurations over wide ranges of parameters , including some that were not amenable to any known existing approach ( such as the combination of @xmath16 , high dynamic range of intensity , and high ratio of observation period to event resolution , figure [ fig : synthetic ] ) .",
    "the inferred intensities and gamma parameters were consistently accurate .",
    "estimates of the confidence intervals were also accurate .",
    ".49     .49         lastly , we applied the direct method to sequences of billing codes representing clinical events . after obtaining irb approval , we extracted all codes from five patient records with the greatest number of such codes in the deidentified mirror of our institution s electronic medical record .",
    "we arranged the codes from each patient record as streams of events grouped at both the top level of the icd-9 disease hierarchy ( groups of broadly related conditions ) , and at the level of the individual disease .",
    "for the streams of grouped events , we included an event if its associated icd-9 code fell within the range of the given top - level division .",
    "for example , any event with a code in the range [ 390  459.81 ] was considered a _ cardiovascular _ event .",
    "while intensity functions are only strictly additive for poisson processes , we still find the curves of grouped events to be informative .",
    "we inferred intensity functions for each of these event streams ( figures [ fig : clinical ] and [ fig : summary ] ) .",
    "each curve was generated using 2000 burn - in and 2000 inference iterations in about three minutes using unoptimized matlab code on a single desktop cpu .",
    "the results of both types have good clinical face validity .",
    "the set of top - level intensity functions make clear that there is much underlying structure in these originally irregular and asynchronous medical events that can now be investigated with standard learning methods ( figure [ fig : summary ] ) . for example",
    ", there are obvious dependencies in this patient between congenital , cardiovascular , and pulmonary conditions , and then a dependence of those with a genitourinary condition emerges around year 3 .",
    "such structure can be investigated at both the patient and the population level using these abstractions .",
    "we have made two contributions with this paper .",
    "first , we presented a direct numeric method to infer a distribution of continuous intensity functions from a set of episodic , irregular , and discrete events .",
    "this direct method has increased efficiency , flexibility , and accuracy compared to the best prior method .",
    "second , we presented results using the direct method to infer a continuous function density as an abstraction over episodic clinical events , for the purposes of transforming the raw event data into a form more amenable to standard machine learning algorithms .",
    "the clinical interpretation of these intensity functions is that increased intensity represents increased frequency of contact with the healthcare system , which usually means increased _ instability _ of that condition . in some cases ,",
    "it may also mean increased _ severity _ of the condition , but not always . if a condition acutely increases in severity , this represents an instability and will probably generate a contact event .",
    "on the other hand , if a condition is severe but stably",
    "so , it may or may not require high - frequency medical contact .    a method to construct similar curves from observations with both a time and a continuous value has been previously reported @xcite , and we have presented here a method to construct them from observations with a time plus a categorical label .",
    "these two data types represent the majority of the information in a patient record ( if we consider words and concepts in narrative text to be categorical variables ) , and opens up many possibilities for finding meaningful patterns in large medical datasets .",
    "the practical motivation for this work is that once we have the continuous function densities , we can use them as inputs to a learning problem in the time domain ( such as identifying trajectories that may be characteristic of a particular disease ) , or by aligning many such curves in time and looking for useful patterns in their cross - sections ( which to our knowledge has not yet been reported ) .",
    "we discovered incidentally that a presentation such as figure [ fig : summary ] appears to be a promising representation for efficiently summarizing a complicated patient s medical history and communicating that broad summary to a clinician .",
    "the presentation could allow drilling - down to the intensity plots of the specific component conditions and then to the raw source data .",
    "( the usual method of manually paging through the often massive chart of a patient to get this information can be a tedious and frustrating process . )",
    "one could also imagine presenting the curves of not the raw icd-9 divisions , but the inferred latent factors underlying them , and drilling down into the rich combinations of test results , medications , narrative text , and discrete billing events that comprise those latent factors .",
    "we plan to investigate these possibilities in future work .",
    "this work was funded by grants from the edward mallinckrodt , jr .",
    "foundation and the national institutes of health 1r21lm011664 - 01 .",
    "clinical data was provided by the vanderbilt synthetic derivative , which is supported by institutional funding and by the vanderbilt ctsa grant ultr000445 .",
    "r.  p. adams , i.  murray , and d.  j.  c. mackay .",
    "tractable nonparametric bayesian inference in poisson processes with gaussian process intensities . in _ proceedings of the 26th international conference on machine learning _ , 2009 .",
    "t.  a. lasko , j.  c. denny , and m.  a. levy .",
    "computational phenotype discovery using unsupervised feature learning over noisy , sparse , and irregular clinical data .",
    "_ plos one _ , 80 ( 6):0 e66341 , 2013 .",
    "doi : 10.1371/journal.pone.0066341 .",
    "p.  a.  w. lewis .",
    "recent results in the statistical analysis of univeraite point processes . in p.",
    "a.  w. lewis , editor , _ stochastic point processes ; statistical analysis , theory , and applications_. wiley interscience , 1972 .",
    "p.  martinsson , v.  rokhlin , and m.  tygert . a fast algorithm for the inversion of general toeplitz matrices . _ computers & mathematics with applications _ , 500 ( 56):0 741 752 , 2005 .",
    "doi : http://dx.doi.org/10.1016/j.camwa.2005.03.011 .",
    "i.  murray and r.  p. adams .",
    "slice sampling covariance hyperparameters of latent gaussian models . in j.",
    "lafferty , c.  k.  i. williams , r.  zemel , j.  shawe - taylor , and a.  culotta , editors , _ advances in neural information processing systems _ , 2010 .",
    "j.  weiss and d.  page .",
    "forest - based point process for event prediction from electronic health records . in _",
    "machine learning and knowledge discovery in databases _",
    ", volume 8190 of _ lecture notes in computer science _ , pages 547562 .",
    "springer berlin heidelberg , 2013 .",
    "doi : 10.1007/978 - 3 - 642 - 40994 - 3_35 ."
  ],
  "abstract_text": [
    "<S> the episodic , irregular and asynchronous nature of medical data render them difficult substrates for standard machine learning algorithms . </S>",
    "<S> we would like to abstract away this difficulty for the class of time - stamped categorical variables ( or _ events _ ) by modeling them as a renewal process and inferring a probability density over continuous , longitudinal , nonparametric intensity functions modulating that process . </S>",
    "<S> several methods exist for inferring such a density over intensity functions , but either their constraints and assumptions prevent their use with our potentially bursty event streams , or their time complexity renders their use intractable on our long - duration observations of high - resolution events , or both . </S>",
    "<S> in this paper we present a new and efficient method for inferring a distribution over intensity functions that uses direct numeric integration and smooth interpolation over gaussian processes . </S>",
    "<S> we demonstrate that our direct method is up to twice as accurate and two orders of magnitude more efficient than the best existing method ( thinning ) . </S>",
    "<S> importantly , the direct method can infer intensity functions over the full range of bursty to memoryless to regular events , which thinning and many other methods can not . </S>",
    "<S> finally , we apply the method to clinical event data and demonstrate the face - validity of the abstraction , which is now amenable to standard learning algorithms .    </S>",
    "<S> = 1 </S>"
  ]
}