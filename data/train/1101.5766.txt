{
  "article_text": [
    "finding image representations with a dimensionality reduction while maintaining relevant information for classification , remains a major issue .",
    "effective approaches have recently been developed based on locally orderless representations as proposed by koendering and van doom @xcite .",
    "they observed that high frequency structures are important for recognition but do not need to be precisely located .",
    "this idea has inspired a family of descriptors such as sift @xcite or hog @xcite , which delocalize the image information over large neighborhoods , by only recording histogram information .",
    "these histograms are usually computed over wavelet like coefficients , providing a multiscale image representation with several wavelets having different orientation tunings .",
    "this paper introduces a new geometric image representation obtained by grouping coefficients that have co - occurrence properties across an image class .",
    "it provides a locally orderless representation where sparse descriptors are delocalized over groups which optimize the coefficient co - occurrences , and can be interpreted as a form of parcellization @xcite .",
    "section [ geom - sec ] reviews wavelet image representations and the notion of sparse geometry through significant sets .",
    "section [ mixt - sec ] introduces our co - occurrence grouping model which is optimized with a maximum likelihood approach .",
    "groups are computed from a training sequence in section [ bandfosec ] , using a bernoulli mixture approximation .",
    "applications to face image compression are shown in section [ comp - sec ] and the application of this representation is illustrated for mnist image classifications in section [ mnist - sec ] .",
    "sparse signal representations are obtained by decomposing signals over bases or frames @xmath0 which take advantage of the signal regularity to produce many zero coefficients .",
    "a sparse representation is obtained by keeping the significant coefficients above a threshold @xmath1 , @xmath2 the original signal can be reconstructed with a dual family @xmath3 , and the resulting sparse approximation is @xmath4 .",
    "wavelet transforms compute signal inner products with several mother wavelets @xmath5 having a specific direction tuning , and which are dilated by @xmath6 and translated by @xmath7 : @xmath8 .",
    "separable wavelet bases are obtained with @xmath9 mother wavelets @xcite , in which case the total number @xmath10 of wavelets is equal to the image size .",
    "let @xmath11 be the cardinal of the set @xmath12 . in absence of prior information on @xmath12 ,",
    "the number of bits needed to code @xmath12 in @xmath13 is @xmath14 .",
    "one can also verify @xcite that the number of bits required to encode the values of coefficients in @xmath12 is proportional to @xmath11 and is smaller than @xmath15 so that the coding budget is indeed dominated by @xmath15 which carries most of the image information .",
    "in a supervised classification problem , a geometric model defines a prior model of the probability distribution @xmath16 . there is a huge number @xmath17 of subsets @xmath12 in @xmath13 . estimating the probability @xmath18 from a limited training set thus requires using a simplified prior model .",
    "a signal class is represented by a random vector whose realizations are within the class and whose significance sets @xmath12 are included in @xmath13 .",
    "a mixture model is introduced with co - occurrence groups @xmath19 of constant size @xmath20 , which define a partition of the overall index set @xmath13 @xmath21    co - occurrence groups @xmath19 are optimized by enforcing that all coefficients have a similar behavior in a group and hence that @xmath22 is either almost empty or almost equal to @xmath19 with a high probability .",
    "the mixture model assumes that the distributions of the components @xmath22 are independent .",
    "the distribution @xmath23 is assumed to be uniform among all subsets of @xmath19 of cardinal @xmath24 .",
    "let @xmath25 be its distribution , @xmath26 this co - occurrence model is identified with a maximum log - likelihood approach which computes @xmath27",
    "given a training sequence of images @xmath28 that belong to a class , we optimize the group co - occurrence by approximating the maximum likelihood with a bernoulli mixture .",
    "let @xmath29 be the significant set of @xmath30 .",
    "the log likelihood is calculated with @xmath31 the maximization of this expression is obtained using the stirling formula which approximates the first term by the entropy of a bernoulli distribution .",
    "let us write @xmath32 and @xmath33 , the bernoulli probability distribution associated to @xmath34 .",
    "let us specify the groups @xmath19 by the inverse variables @xmath35 such that @xmath36 .",
    "it results that @xmath37    the distribution @xmath38 is generally unknown and must therefore be estimated .",
    "the estimation is regularized by approximating this distribution with a piecewise constant distribution @xmath39 over a fixed number of quantization bins , that is small relatively to the number of realizations @xmath40 . the likelihood ( [ conasdnwosn ] )",
    "is thus approximated by a likelihood over the bernoulli mixture , which is optimized over all parameters : @xmath41 the following algorithm , minimizes ( [ conasdnwosn3 ] ) by updating separately the bernoulli parameters @xmath42 , the distribution @xmath39 and the grouping variables @xmath35 .",
    "the minimization algorithm begins with a random initialization of groups @xmath19 of same size @xmath20 .",
    "the empirical histograms @xmath39 are initialized to uniform distributions .",
    "the algorithm iterates the following steps :    * step 1 : given @xmath43 and @xmath44 compute @xmath45 which minimizes ( [ conasdnwosn3 ] ) by minimizing @xmath46 * step 2 : update @xmath44 to minimize ( [ conasdnwosn3 ] ) as the normalized histogram of the updated parameters @xmath47 over a predefined number of bins . *",
    "step 3 : update the group indexes @xmath48 to minimize ( [ conasdnwosn3 ] ) by minimizing @xmath49 for groups of constant size @xmath50 .",
    "this algorithm is guaranteed to converge to a local maxima because each step further increases the log - likelihood .",
    "in fact , it is the equivalent of the @xmath51-means algorithm adapted to the mixture model considered here .",
    "to illustrate the efficiency of this grouping strategy , it is first applied to the compression of face images that have been approximately registered .",
    "a database of 170 face images were used for training and a different set of 30 face images were used for testing .",
    "figure [ cooc ] shows the optimal co - occurrence groups obtained over wavelet coefficients by applying the maximum log - likelihood algorithm on the training set .",
    "the encoding cost of the significance map using the optimized model is equal to minus the log - likelihood of this model .",
    "figure [ bitrate ] shows the evolution of the average bit budget needed to encode the significance maps with the bernoulli mixture over optimized co - occurrence groups , depending upon the groups size @xmath20 .",
    "the optimal group size which maximizes the log - likelihood and hence minimizes the encoding cost over all group sizes is @xmath52 .     as a function of @xmath53 . _",
    "dashed _ : bit rate ( equal to minus the log likelihood , in bits per pixel ) using the optimal groups of size @xmath20 as a function of @xmath53 . ]    when @xmath20 is equal to the image size , there is a single group and the encoding is thus equivalent to a standard image coding using no prior information on the class . the bit rate",
    "is also compared with a bernoulli mixture computed with a partition into square groups @xmath19 , as a function of @xmath20 .",
    "figure [ bitrate ] shows that the optimized co - occurrence grouping improves the bit rate by 20 % relatively to the case where there is a single group , and also with respect to the fixed square groups , which means that the optimal grouping provides a geometric information which is stable across the image class .",
    "the optimal group size @xmath52 also gives an estimation of the image deformations that are due to variations of scaling and eye positions and to intrinsic variations of faces in the database .",
    "this section shows the classification ability of our geometric representation despite the presence of strong variability in the images .",
    "the test is performed using the standard mnist database of digits .",
    "this database is relatively simple and without any modification of the image representation an svm classifier can reach @xmath54 of error with a training set of 60,000 images .",
    "this section shows that our geometric co - occurence model can learn with much less training elements and for more complex images .    to take into account texture variation phenomena , which are a central difficulty for geometric models , a white noise texture is introduced .",
    "a digit image @xmath55 $ ] is transformed into a random digit @xmath56 = ( f[n ] + c ) w[n]$ ] where @xmath57 $ ] is a normalized gaussian white noise .",
    "the significance maps of these digits are simply obtained with a thresholding as shown in figure [ mnist_figure ] .",
    "it yields a binary image with a low density binary texture on the digit background and high density texture on the digit support .",
    "visually , the digit is still perfectly recognizable despite the texture variability . with @xmath58 training images an svm with a polynomial kernel yields a very low recognition rate of * 21% * on a different set of @xmath59 test images .",
    "figure [ mnist_figure ] shows the optimal co - occurrence groups of size @xmath60 computed with the minimization algorithm of section [ bandfosec ] . despite the geometric variability ,",
    "the algorithm is able to extract co - occurrence groups that do correspond to the digit structures and their deformations . to each digit @xmath61 ,",
    "corresponds an optimized co - occurrence grouping @xmath62 .",
    "let @xmath63 be the likelihood of the significance map @xmath12 of @xmath64 with the grouping model @xmath62 .",
    "an svm classifier is trained on the feature vector @xmath65 , of dimension @xmath66 with groups of size @xmath60 . with @xmath58 training images",
    "this classifier yields a recognition rate of * 9% * on a different set of @xmath59 test images . a simple maximum likelihood classifier ( map ) associates to each test image @xmath64 the digit class @xmath67 with @xmath58 training examples , this simple classifier yields a recognition rates of 18% for random digits , which is already better than the svm applied on the original pixels .",
    "this paper introduces a new approach to define the geometry of a class of images computed over a sparse representation , using co - occurrence groups .",
    "these co - occurrence groups are computed with a maximum log likelihood estimation calculated over optimized bernoulli mixture model .",
    "an algorithm is introduced to optimize the group computation .",
    "the application to face image compression shows the efficiency of this encoding approach , and the ability to compute co - occurrence groups that provide stable information across the class .",
    "a classification test is performed over textured digits , which shows that the algorithm can take into account texture geometry and provide much better classification rates than a standard pixel based image representation .",
    "b. thirion , g. flandin , p. pinel , a. roche , p. ciuciu , and j .- b .",
    " dealing with the shortcomings of spatial normalization : multi - subject parcellation of fmri datasets \" .",
    "brain mapp . , 27(8):678 - 693 , aug .",
    "s. mallat ,  a wavelet tour of signal processing , the sparse way \" , academic press , 3rd edition , 2008 ."
  ],
  "abstract_text": [
    "<S> a geometric model of sparse signal representations is introduced for classes of signals . </S>",
    "<S> it is computed by optimizing co - occurrence groups with a maximum likelihood estimate calculated with a bernoulli mixture model . </S>",
    "<S> applications to face image compression and mnist digit classification illustrate the applicability of this model . </S>"
  ]
}