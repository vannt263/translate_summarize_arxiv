{
  "article_text": [
    "in most classical machine learning models we exploit the global statistical properties of the data without analysis of their exact local geometry ( svm , neural networks ) . on the other hand  density based methods ( bayes , em )  are conceptually different approaches which often lead to completely local decision criteria .",
    "our approach belongs to the hybrid approaches @xcite which typically try to combine supervised and unsupervised techniques in one , uniform model . some of these methods are combinations of clustering and classification techniques in either direct form @xcite or using the complex , hierarchical structures of alternating algorithms @xcite .    in this paper",
    ", we introduce the kernel building method which exploits the local data geometry using cluster - based space partition and includes it in the constructed feature space projection .",
    "we show that even the use of k - means ( with @xmath3 ) for the partitioning part gives interesting results .",
    "our approach can be seen as a special case of the metric learning problem which does not change the formulation of the optimization problem being solved . as a result",
    "we give a simple and efficient method which can be easily used with existing implementations of svm .",
    "the constructed approach not only gives better classification results then svm but , what is of crucial importance from the practical point of view , , , good  results are obtained with less complex tuning required as compared to the usage of rbf or mahalanobis rbf kernels ( see figure  [ fig : grids ] for example of grid search results on australian dataset ) .",
    "it is worth noting that proposing models and methods which reduce the complexity of metaparameters tuning is of crucial importance for practical applications .",
    "one the one hand , such optimziation can be too expensive ( hierarchical , ensemble based classification  @xcite , active learning scenarios  @xcite ) and on the other researchers from other disciplines often ignore its importance  @xcite .     and",
    "@xmath4 , from left : simple rbf kernel , mahalanobis kernel , k - means + mahalanobis kernel , our method , title=\"fig:\",width=94 ]   and @xmath4 , from left : simple rbf kernel , mahalanobis kernel , k - means + mahalanobis kernel , our method , title=\"fig:\",width=94 ]   and @xmath4 , from left : simple rbf kernel , mahalanobis kernel , k - means + mahalanobis kernel , our method , title=\"fig:\",width=94 ]   and @xmath4 , from left : simple rbf kernel , mahalanobis kernel , k - means + mahalanobis kernel , our method , title=\"fig:\",width=94 ] +   and @xmath4 , from left : simple rbf kernel , mahalanobis kernel , k - means + mahalanobis kernel , our method , title=\"fig:\",width=340 ]    we also introduce an @xmath5 index ( described in detail in evaluation section ) able to measure how easy is to tune the model which requires some set of metaparameters and use it to evaluate our approach .",
    "it can be used both for visualization of this characteristics ( similarly to how roc curves visualize clasifier s accuracy ) and for comparision of different models ( similarly to auc roc measure ) . for examples one can refer to fig .",
    "[ fig : prob ] in the evaluation section .",
    "paper is structured as follows : first , we describe our method and prove that it leads to a valid mercer s kernel .",
    "then we analyze some practical issues connected with algorithm implementation and usage and conclude with comparative empirical evaluation performed using datasets from uci database .",
    "in the recent years there is a growing interest in the fields of metric learning @xcite . among others , mahalanobis metric learning for the rbf svm has been proposed @xcite .",
    "more computationally feasible solutions , which are similarly justified include performing preprocessing step .",
    "one of such approaches is a search for the smallest volume bounding ellipsoid @xcite which is used to define the mahalanobis kernel .",
    "our approach is similar to this idea as it also performs a preprocessing in order to find some data characteristics but instead of optimization procedure we use cheap clustering technique .",
    "many researchers investigated possible fusion of and svms ",
    "these approaches spans from using k - means to reduce the training set size @xcite through reduction of support vectors count @xcite to even incorporating the process of finding centroids directly into the optimization problem @xcite .",
    "however , in our work k - means is used in a completely different manner , only as a selection method for the partition . instead of reducing amount of available information ( by either removing training samples or support vectors ) it introduces additional kind of knowledge into the process .",
    "another branch of related approaches are ensembles - based models .",
    "recently proposed hcoc @xcite model alternates between classification and clustering steps on many levels of tree - like structure .",
    "it also exploits some additional kind of knowledge  which classes are the most likely to be confused .",
    "other authors also showed that clustering can be used to simply divide the problem into smaller ones solved independently by a separate classifiers @xcite . in c@xmath0rbf , instead of splitting data and analyzing the output of several classifiers , we propose to include all the information gained through clustering into one , generic classifier .",
    "the basic idea of our approach is to allow the dependence of the projection type on the local point s neighborhood by transforming each point into some multivariate gaussian : @xmath6 on this level of generality it leads to numerically complex problems . as we need to calculate some @xmath7 for every point in space . and",
    "calculate inverses of some matrices for every pair @xmath8 of points from a dataset @xmath9 .    to make the problem more feasible we assume that we have a partition of the space into @xmath1 pairwise - disjoint sets @xmath10 and @xmath7 depends only on the @xmath11 s belonging to an element of the partition .",
    "consequently we fix @xmath12 and consider the feature space projection @xmath13 note that we still need to define @xmath14 for each element @xmath15 . to avoid complex numerical optimization",
    "we simply use the empirical covariance of @xmath9 restricted to @xmath15 : @xmath16 the only thing left to consider is how to construct the partition @xmath15 . to simplify this ( in general infinite - dimensional ) problem we assume ] that @xmath15 is given as the k - means partition of @xmath17 .",
    "summarizing we define the transformation @xmath18 by the following steps :    [ cols= \" < , < \" , ]     we illustrate the choice of the feature space projection on the two - dimensional projection of iris dataset restricted to two first classes on fig .",
    "[ fig : iris ] .",
    "it is worth noting that our method is somewhat similar to the ideas behind metric learning ",
    "the distance in the feature space should be better fitted to the geometry of the data .",
    "c@xmath0rbf algorithm tries to adapt this measure to the given set of points but without incorporating it inside the optimization process .",
    "this more numerically efficient approach modifies the projection based on the neighborhood of the point in space .",
    "there still remains the question how to choose the number @xmath1 and the partition @xmath15 . in the evaluation section",
    "we show that @xmath3 is a good choice for typical datasets , and discuss some problems and benefits connected with fixing @xmath19 . clearly , in general one can select partition in an arbitrary way , including search through possible solutions of some optimization problem . in our work",
    "we focus on the simple and natural construction coming from the k - means clustering of the whole @xmath9 .",
    "contrary to some models we use the existing kernel space , and only modify the projection function @xmath20 .",
    "we define the feature space as multivariate gaussians , but contrary to rbf or mahalanobis rbf we allow the variation of the gaussians covariance over the space . since we use the hilbert @xmath21 kernel space , we do not have to prove that the feature space projection is correct .",
    "the only thing we need is the formula for the scalar product of two points projections : @xmath22 \\cdot { \\mathcal{n}}(m_2,\\sigma_2)[x ] dx.\\ ] ] the formula for the @xmath23 scalar product of two gaussians is known and can be easily deduced from the formula for the sum of two independent normal variables @xcite .",
    "we provide the direct derivation here for the sake of completeness .",
    "let @xmath24 and positive self - adjoint matrices @xmath25 be given .",
    "we put @xmath26 then @xmath27 = & ( x - m)^t{s}(x - m)+(m_1-m_2)^tw(m_1-m_2 ) .",
    "\\end{aligned}\\ ] ]    one can easily see that the main difficulty lies in proving that the parts , , without ",
    "@xmath11 on left and right - hand sides of coincide .",
    "now the left hand side of can be rewritten in the following form : @xmath28 =   -(m_1^t{s}_1+m_2^t{s}_2)({s}_1+{s}_2)^{-1 }   ( { s}_1m_1+{s}_2m_2)\\\\ \\phantom{= } + m_1^t{s}_1({s}_1+{s}_2)^{-1}({s}_1m_1+{s}_2m_1)\\\\ \\phantom{= } + m_2^t{s}_2({s}_1+{s}_2)^{-1}({s}_1m_2+{s}_2m_2)\\\\[1ex ] =   m_1'{s}_1({s}_1+{s}_2)^{-1}{s}_2(m_1-m_2 ) \\\\",
    "\\phantom{= } -m_2'{s}_2({s}_1+{s}_2)^{-1}{s}_1(m_1-m_2 ) .",
    "\\end{array}\\ ] ]    clearly @xmath29^{-1}= \\\\[0.5ex ] = { s}_1(i+{s}_2^{-1}{s}_1)^{-1 } = { s}_1({s}_2^{-1}{s}_2+{s}_2^{-1}{s}_1)^{-1 } \\\\[0.5ex ] = { s}_1({s}_2+{s}_1)^{-1}{s}_2 . \\end{array}\\ ] ] analogously @xmath30 , which means that reduces to @xmath31 .",
    "we recall that for @xmath32 and positive self - adjoint matrix @xmath33 by @xmath34 we denote the normal density with mean @xmath35 and covariance matrix @xmath33 , that is @xmath36:={\\mbox{$\\frac{1}{\\sqrt{(2\\pi)^d\\det \\sigma}}$ } } \\exp ( -\\tfrac{1}{2}\\|x - m\\|_{\\sigma}^2),\\ ] ] where @xmath37 denotes the square of mahalanobis norm of @xmath38 given by @xmath39 .",
    "let @xmath24 and let @xmath40 be positive self - adjoint matrices on @xmath17 .",
    "we have @xmath22 \\cdot { \\mathcal{n}}(m_2,\\sigma_2)[x ] dx = { \\mbox{$\\frac{1}{\\sqrt{(2\\pi)^{d}\\det(\\sigma_1+\\sigma_2)}}$ } } \\exp(-\\tfrac{1}{2}\\|m_1-m_2\\|^2_{\\sigma_1+\\sigma_2}).\\ ] ]    we put @xmath41 and define @xmath35 as in the previous lemma . by we have @xmath42 = \\exp(-\\tfrac{1}{2}\\|x - m\\|^2_{(\\sigma_1^{-1}+\\sigma_2^{-1})^{-1 } } ) \\cdot \\exp(-\\tfrac{1}{2}\\|m_1-m_2\\|^2_{\\sigma_1+\\sigma_2 } ) , \\end{array}\\]]which implies that @xmath43 \\cdot { \\mathcal{n}}(m_2,\\sigma_2)[x]dx\\ ] ] @xmath44 @xmath45 since @xmath46 dx & = ( 2\\pi)^{d/2}\\sqrt{\\det ( ( \\sigma_1^{-1}+\\sigma_2^{-1})^{-1})}\\\\ & = \\frac{(2\\pi)^d\\sqrt{\\det \\sigma_1 \\det \\sigma_2}}{(2\\pi)^{d/2}\\sqrt{\\det(\\sigma_1+\\sigma_2 ) } }     \\end{aligned}\\ ] ] we obtain the assertion of the proposition .    [",
    "theorem : kernel ] consider a function @xmath18 and put @xmath47 then @xmath48 is a valid kernel in the mercer s sense for each @xmath49 .",
    "it is a direct consequence of the fact that @xmath48 is a scalar product in the feature space @xmath50 for feature projection defined by @xmath51    this concept generalizes two mentioned rbf kernels in a natural way .    for constant function @xmath52 our approach",
    "reduces to the classical rbf and for @xmath53 ( or for @xmath54 and the trivial -based calculation ) to the mahalanobis rbf ( up to the scaling factor ) .    in practical usage",
    ", some of the @xmath55 may be not invertible , and as a consequence we can not compute @xmath56 . to deal with this case",
    "we ca use the typical regularization approach .",
    "[ obs : invertible ] if for some @xmath11 the matrix @xmath7 is not invertible it is sufficient to replace @xmath7 with @xmath57 for any fixed @xmath58 and positive matrix @xmath59 ( then @xmath7 becomes positive , and consequently invertible ) .    in practice as @xmath59 we use the covariance of the whole dataset @xmath9 , since we want to retain the geometry of the data .",
    "we also select @xmath60 as small as it is reasonably possible in order to preserve our method characteristics instead of mimicking the mahalanobis rbf approach .",
    "one more interesting border case of our kernel is restriction on the classes of @xmath7 used , which leads to the computational complexity reduction .",
    "we can restrict the set of possible @xmath7 to some subset of positive self - adjoint matrices in order to obtain simpler ( more efficient ) kernel . by restricting to the radial gaussians we have @xmath61 and consequently kernel formula for",
    "fixed @xmath4 simplifies to @xmath62 which has a computational complexity of standard rbf kernel even though each point can have its own local variance .",
    "for fixed @xmath4 we may drop the constant factors from the definition of the kernel in theorem  [ theorem : kernel ] which leads to a more numerically efficient formula : @xmath63 which is implemented in algorithm  [ alg : kernelbuilding ] . as it has been already shown in observation",
    "[ obs : invertible ] , in case the sum of covariances is not invertible , we can simply substitute those matrices with convex combination with the covariance of the whole set ( if it is invertible , or with identity matrix otherwise ) with some small @xmath60 .",
    "data @xmath9 , kernel parameter @xmath4 , @xmath64 cluster(@xmath9 ) @xmath65 * for * @xmath66 @xmath67 * for small[multiblock footnote omitted ] * @xmath58 @xmath68 * for * @xmath69 @xmath70 * for * @xmath69 @xmath71 * where * @xmath72 @xmath73 @xmath74    complexity of our kernel building algorithm depends on the complexity of k - means and the matrix operations ( determinant , inversion ) applied to the sums of local covariance matrices . in the naive implementation matrix inversion s complexity is @xmath75 , where @xmath76 is the problem s dimensionality , so the whole process adds @xmath77 complexity term to the preprocessing . as we are assuming that @xmath76 is relatively small ( so there is a need of using rbf like kernel ) ,",
    "this cost is negligible as this process is needed only once per given data and parameter @xmath4 .",
    "it is worth noting that for fixed @xmath1 the complexity of this step is asymptotically equal to the computation of mahalanobis rbf kernel .",
    "moreover , c@xmath0rbf does not complicate ( or deconvexify ) the optimization process itself , which is the case in mahalanobis metric learning svm and other modifications using the additional optimization routines .    in order to avoid repeated recalculation of the determinants and inversions during cross - validation procedure",
    "we can exploit the fact that the only element that is dependent on the choice of @xmath4 is the exponent value .",
    "after easy calculations we arrive at the kernel value conversion formula @xmath78 as the kernel building part of c@xmath0rbf does not use labels of samples , we can build upon all the available data , including unlabeled samples as well as data from the testing set .",
    "consequently we do not have to run it with each fold during cross - validation separately , but rather cluster the whole data and then train separate svms on the data subsets .",
    "this feature can be especially exploited when applied in active learning setting @xcite where we have large amount of unlabeled examples .",
    "evaluation was performed using nine datasets from uci repository  @xcite , briefly summarized in table  [ tab : data ] .",
    "all points were linearly scaled to the @xmath79 $ ] interval for fair comparision with regular rbf .",
    "as one can see , almost all considered datasets show significant differences in internal geometry between clusters detected by k - means algorithm ( sixth column )",
    ". only crashes data seems to be quite homogeneous .",
    "experiments were performed using code written in python with use of ` scikit - learn ` library .",
    "k - means algorithm was seeded with k - means++  @xcite 10 times and clustering yielding the smallest energy was selected . all data",
    "( including test cases ) was used during the clustering step ( as unlabeled examples ) .",
    "all experiments were performed in 10-fold cross - validation mode .",
    "we start our evaluation with reporting the best accuracy obtained by all tested models .",
    "table  [ tab : accuracy ] shows that proposed method achieves similar results to the rbf kernel and mahalanobis rbf . in some cases",
    "c@xmath0rbf behaves significantly better ( australian , diabetes , breast - cancer ) and for some worse than referencing kernels ( crashes , heart , liver - disorders ) .",
    "these results are the consequence of many aspects  including the choice of the simplest clustering algorithm , naive empirical covariance estimation .",
    "they show , however , that in terms of achieved overall accuracy using c@xmath0rbf leads to comparable results to rbf / mrbf based classification . to show , that our approach is fundamentally different from applying k - means as a data partitioning scheme , and training separate mrbf based models in each cluster , we also report results of such model ( dentoed as m@xmath80rbf , meaning that it first runs k - means and in each cluster trains separate cluter s covariance based svm ) .",
    "we investigated how different clustering techniques behave in such task .",
    "we performed experiments for gaussian mixture model ( gmm ) and dirichlet process gaussian mixture model ( dpgmm ) with number of clusters varying from @xmath81 to @xmath82 .",
    "table  [ tab : gmm ] shows differences between accuracy obtained for k - means based method and gaussian mixture models .",
    "one can make at least two important observations here .",
    "first , k - means performs surprisingly well as compared to more advanced clustering methods .",
    "second , for some datasets ( like liver - disorders ) gmm based solution brings considerable increase in classification quality ( which outperforms also rbf and mrbf , refer to table  [ tab : accuracy ] ) .",
    "this may lead to the conclusion , that different types of clustering methods can exploit various types of knowledge .",
    "the choice of a good clustering technique requires an additonal analysis of data , internal cross - validation etc .",
    "so in further parts of our investigations we focus only on k - means based approach to show its wide applicability .",
    "however , reader should bear in mind , that different methods are possible .",
    "the most interesting effect of using the proposed method is easier metaparameters selection . in practice",
    ", many applied researchers ( for example in cheminformatics @xcite ) neglect the metaparameters optimization and use its default values . in most of existing svm libraries ( including ` libsvm ` , ` weka ` ) , the default value of the @xmath83 metaparameter is @xmath84 .",
    "table  [ tab : accuracyc1 ] shows accuracy obtained by considered models once we narrow down to the optimization of only @xmath4 .",
    "c@xmath2rbf obtaines significantly better results than both rbf and mrbf in most cases .",
    "it achieves worse performance than rbf kernel only in two tests , where also mrbf behaved worse , which simply shows , that in these datasets , covariance based geometry is not a good kernel building base .    to further invesigate this phenomen we also performed experiments with very limited search of parameters .",
    "we fixed @xmath85 and searched through just @xmath86 values of @xmath4 ( @xmath87 for @xmath88 ) . in table  [",
    "tab : wins ] we report percentage of wins ( times that c@xmath0rbf obtained better accuracy ) for each of such small tests .",
    "one can notice , that in such small metaparameters ranges , proposed method outperformed rbf and mrbf in almost all cases .",
    "such results are of great importance in applications where we often have limited resources and many internal cross - validation based parameters selection are not possible .",
    "one such application could be the active learning scenario , or the hierarchical / ensamble based models .     for conducted experiments for c@xmath2rbf based on k - means algorithm , regular rbf kernel and mrbf.,title=\"fig:\",scaledwidth=30.0% ]   for conducted experiments for c@xmath2rbf based on k - means algorithm , regular rbf kernel and mrbf.,title=\"fig:\",scaledwidth=30.0% ]   for conducted experiments for c@xmath2rbf based on k - means algorithm , regular rbf kernel and mrbf.,title=\"fig:\",scaledwidth=30.0% ]     for conducted experiments for c@xmath2rbf based on k - means algorithm , regular rbf kernel and mrbf.,title=\"fig:\",scaledwidth=30.0% ]   for conducted experiments for c@xmath2rbf based on k - means algorithm , regular rbf kernel and mrbf.,title=\"fig:\",scaledwidth=30.0% ]   for conducted experiments for c@xmath2rbf based on k - means algorithm , regular rbf kernel and mrbf.,title=\"fig:\",scaledwidth=30.0% ]     for conducted experiments for c@xmath2rbf based on k - means algorithm , regular rbf kernel and mrbf.,title=\"fig:\",scaledwidth=30.0% ]   for conducted experiments for c@xmath2rbf based on k - means algorithm , regular rbf kernel and mrbf.,title=\"fig:\",scaledwidth=30.0% ]   for conducted experiments for c@xmath2rbf based on k - means algorithm , regular rbf kernel and mrbf.,title=\"fig:\",scaledwidth=30.0% ]",
    "observe that in practice , even if we perform metaparameters optimization , we never find the real optimum ( in terms of tunable parameters ) and therefore the increase in resolution of the grid results in finding better classification results .",
    "thus the comparison of two svm - based classification methods with just comparing the best result found on the grid can be misleading .",
    "we propose a measure which in our opinion is more reliable ",
    "estimation of the probability of finding results which are better than a fixed parameter value @xmath89 .",
    "consider the typical case in rbf svm when our function depends on two parameters @xmath83 and @xmath4 .",
    "let us fix a grid @xmath90 ( cartesian product of considered @xmath83 s and @xmath4 s ) and consider the function @xmath91 the above function measures the probability of finding results better then @xmath89 . as this kind of measure",
    "better exploits the model s ability to work well with limited grid size , it directly corresponds to its applicability in training time limited scenarios .",
    "it is of great practical importance for the models which are build from many models ( ensembles , hierarchical models ) as well as in the active learning setting , when one has to retrain it repeatedly .",
    "we approximate this probability by the fraction of parameters pairs in the considered grid , which yield results at least @xmath89 .",
    "@xmath92    plots of corresponding @xmath93 functions in fig .",
    "[ fig : prob ] illustrate that rbf and mrbf kernels are very similar in context of how hard is to find the parameters yielding good results .",
    "also m@xmath94rbf behaves in a very similar fashion , yielding in most cases  results between the one given by rbf and mrbf . in the same time",
    "c@xmath2rbf offers noticeably higher probability of yielding comparable results .",
    "this observation is purely empirical and the justification of this phenomenon remains for us an open question .",
    "areas under the @xmath93 curves ( auc ) are shown in table  [ tab : auc ] .",
    "for simplicy , we consider only curves for @xmath89 at least as big , as the worst score achieved by all models ( as @xmath5 for smaller values is constantly equal to @xmath84 for each model ) . in all conducted experiments",
    ", c@xmath2rbf shows significant improvement over competitive approaches , confirming our claim , that c@xmath0rbf based on k - means can be used to simplify the process of metaparameters selection .",
    "in this paper we have presented the method of including information regarding local problem s geometry inside the definition of the gaussian kernel . from theoretical point of view",
    "proposed method leads to the correct kernel in the mercer s sense which is based on feature space projection transforming data points into various multivariate gaussian density functions .    from practical perspective",
    ", our method s kernel building complexity is asymptotically equivalent to the mahalanobis rbf kernel and is similarly cheap in computation during classification .",
    "obtained results show that our method behaves similarly to rbf and mrbf kernels .",
    "however , c@xmath0rbf yields better results with higher probability in terms of selecting the typical svm parameters , @xmath83 and @xmath4 .",
    "we have also shown empirically that proposed approach is fundamentally different from splitting the problem into subproblems using some clustering method and building separate model for each of them .",
    "c@xmath0rbf uses clustering in order to augment the data representation with additional knowledge instead of reducing the amount of information available .",
    "this shows the conceptual distinction of our approach from previous models .",
    "it is also worth noting that even though we used in our experiments , proposed method can be seen as more general framework , where assignment of @xmath7 can be the result of an arbitrary complex process . in particular",
    ", it would be interesting to further investigate space partitioning given by other supervised , linear classifiers instaed of clustering methods .",
    "arthur , d. , vassilvitskii , s. : k - means++ : the advantages of careful seeding . in : proceedings of the eighteenth annual acm - siam symposium on discrete algorithms , soda 07 , pp",
    ". 10271035 .",
    "society for industrial and applied mathematics ( 2007 )        cong , y. , yang , x.g . , lv , w. , xue , y. : prediction of novel and selective tnf - alpha converting enzyme ( tace ) inhibitors and characterization of correlative molecular descriptors by machine learning approaches .",
    "journal of molecular graphics & modelling * 28*(3 ) , 23644 ( 2009 )    gu , q. , han , j. : clustered support vector machines . in : proceedings of the international conference on artificial intelligence and statistics , _ journal of machine learning research : w&cp _ , vol .  31 ( 2013 )",
    "jackowski , k. , krawczyk , b. , woniak , m. : improved adaptive splitting and selection : the hybrid training method of a classifier based on a feature space partitioning .",
    "international journal of neural systems ( 2014 )    lipka , n. , stein , b. , anderka , m. : cluster - based one - class ensemble for classification problems in information retrieval . in : proceedings of the 35th international acm sigir conference on research and development in information retrieval , sigir 12 , pp .",
    "acm ( 2012 )    liu , y. , caselles , v. : improved support vector machines with distance metric learning . in : j.  blanc - talon , r.  kleihorst , w.  philips , d.  popescu , p.  scheunders ( eds . )",
    "advances concepts for intelligent vision systems , _ lecture notes in computer science _ ,",
    "vol . 6915 , pp .",
    "springer berlin heidelberg ( 2011 )    ma , x. , luo , p. , zhuang , f. , he , q. , shi , z. , shen , z. : combining supervised and unsupervised models via unconstrained probabilistic embedding . in : proceedings of the twenty - second international joint conference on artificial intelligence , ijcai11 , pp .",
    "aaai press ( 2011 )      sakiyama , y. , yuki , h. , moriya , t. , hattori , k. , suzuki , m. , shimada , k. , honma , t. : predicting human liver microsomal stability with machine learning techniques .",
    "journal of molecular graphics & modelling * 26*(6 ) , 90715 ( 2008 )          wang , j. , wu , x. , zhang , c. : support vector machines based on k - means clustering for real - time business intelligence systems .",
    "international journal of bussiness intelligence and data minning * 1*(1 ) , 5464 ( 2005 )"
  ],
  "abstract_text": [
    "<S> in the classical gaussian svm classification we use the feature space projection transforming points to normal distributions with fixed covariance matrices ( identity in the standard rbf and the covariance of the whole dataset in mahalanobis rbf ) . in this paper </S>",
    "<S> we add additional information to gaussian svm by considering local geometry - dependent feature space projection . </S>",
    "<S> we emphasize that our approach is in fact an algorithm for a construction of the new gaussian - type kernel .    </S>",
    "<S> we show that better ( compared to standard rbf and mahalanobis rbf ) classification results are obtained in the simple case when the space is preliminary divided by k - means into two sets and points are represented as normal distributions with a covariances calculated according to the dataset partitioning . </S>",
    "<S> we call the constructed method c@xmath0rbf , where @xmath1 stands for the amount of clusters used in k - means . </S>",
    "<S> we show empirically on nine datasets from uci repository that c@xmath2rbf increases the stability of the grid search ( measured as the probability of finding good parameters ) . </S>"
  ]
}