{
  "article_text": [
    "support vector machines(svms ) @xcite have emerged as powerful tools for classification problems .",
    "the key to accurate classification using svms is the choice of _ kernel functions_(for definition of kernel function please see @xcite ) .",
    "this issue was first studied in lanckriet et .",
    "@xcite where the problem of multiple kernel learning(mkl ) was first introduced .",
    "they have been successfully applied to a variety of domains e.g. text , object recognition @xcite , protein structures@xcite . even though the idea was to explore the space of all possible linear combinations of the specified kernels , the functional framework associated with it could only select the best kernel from the set of specified kernels .",
    "recently , many other approaches have been proposed to overcome this limitation@xcite .",
    "while some of them select all the kernels and some have sparse solutions that choose a subset of the specified kernels in a weighted combination , none of them have explicit control over sparsity .    due to lack of explicit control , in many application scenarios non - sparse solutions",
    "end up selecting some bad kernels also which leads to reduction in the discriminative power of the combination kernel .",
    "we show experimental evidence of this phenomenon .",
    "one might argue that if the kernels given were all good kernels , this problem will not persist .",
    "but that does not take away the fact that a lower - accuracy good kernel can still bring down the accuracy of a better kernel . in most of the recent publications",
    ", we do not get a glimpse of the original problem as the space of kernels explored is very small and most of the kernels have almost equal power of representation .    while sparse solutions@xcite overcome this particular problem to an extent by having some inherent ability to select a combination of a subset of the specified kernels , once again",
    ", there is no way to control the sparsity of the solution .",
    "this inherits most of the problems of selecting one and selecting all kernels due to the lack of control .",
    "the most relevant problem is that it misses out on some important features by selecting lesser number of kernels than optimal . in the case of applications like object recognition",
    ", the necessity of non - sparse solutions have been brought to light @xcite .",
    "this is due to the fact that different kernels represent different features necessary for the task and dropping some of them or most of them will lead to a bad combination kernel .",
    "these flaws are shown in our experiments as well .",
    "this work builds a variable sparsity solution that has explicit control over the number of kernels selected overcoming all these problems .",
    "we show the effect and need of strict control of sparsity through our experiments on the application of object recognition . along the way",
    ", we have also extended the mkl framework to nu - svms which allow us better control of the number of support vectors and training error as well .    in the following section [ sec : rel ] , we present a review of the existing work on mkl .",
    "section [ sec : bmkl ] introduces the cskl formulation for the c - svm and @xmath3-svm .",
    "section [ sec : algo ] presents the algorithms to solve the proposed formulations .",
    "section [ sec : expts ] demonstrates the usefulness of the cskl formulation on a toy dataset and the caltech101 real world object categorization dataset .",
    "multiple kernel learning(mkl ) was initially proposed by lanckriet et .",
    "they introduced an semi - definite programming(sdp ) approach to solve for the combination kernel .",
    "as sdp becomes intractable with increase in size and number of kernels , bach et.al @xcite reformulated mkl by considering each feature as a block and applying the @xmath4 norm across the blocks and @xmath5 norm within each block . for this formulation several algorithms@xcite were proposed to speed up the optimization process .",
    "@xcite provides an semi - infinite linear programming(slip ) based algorithm which decreases the training time to large extent .",
    "simplemkl @xcite proposed by rakotomamonjy et.al . derived a formulation which is equivalent to the block @xmath4 norm based formulation and provided a reduced gradient descent based algorithm that is faster than the slip algorithm proposed previously .",
    "the dual of the simplemkl formulation is given by , @xmath6    while all these approaches discussed a sparse solution to mkl , on understanding the need for non - sparse solutions , researchers have been exploring the space of non - sparse formulations in recent times . to acheive non - sparsity , @xcite",
    "group the kernels and apply @xmath7 norm across the groups and @xmath4 norm within the groups .",
    "they have also proposed a mirror descent algorithm for solving mkl formulations which is much faster than simplemkl .",
    "especially when number of kernels are high .",
    "kloft et.al.@xcite apply general @xmath8 norm to kernels and they show that non - sparse mkl generalizes much better than sparse mkl .",
    "the dual of the @xmath8 norm based mkl formulation as proposed by kloft et.al . looks like @xmath9 they have shown that when @xmath10 , the formulation is equivalent to simplemkl and as @xmath1 moves to @xmath11 , it explores non - sparse solutions .",
    "but the value of @xmath1 lacks a direct meaning or implication to the number of kernels selected .",
    "even though the details of sparse and non - sparse solutions have been explored , none of these formulations have explicit control of sparsity for their solutions . as we have demonstrated in the experiments section , strict control of sparsity",
    "is highly valuable .",
    "hence we propose a formulation , where we can parametrically control the total number of kernels selected and an efficient reduced gradient descent based algorithm to solve it .",
    "we have also experimentally shown that our formulation will be able to better state - of - the - art performance on the caltech101@xcite dataset for object categorization through strict control of sparsity .",
    "in this section we introduce the new controlled sparsity kernel learning ( cskl ) formulation and prove that this formulation can explicitly control the sparsity of kernel selection through a parameter @xmath2 .",
    "we derive the cskl formulation by modifying the dual of mkl  @xcite .",
    "lets start with the mkl dual  @xcite    @xmath12    where @xmath13 .",
    "+ denote by @xmath14 and @xmath15 .",
    "as @xmath16 is convex , one can interchange the min and the max .",
    "now , the dual looks like @xmath17 define @xmath18 then the constraint @xmath19 can be rewritten as @xmath20 .",
    "the @xmath21 norm can be represented as @xmath22 for any @xmath23 .",
    "given this , equation ( [ eqn : mkl ] ) can be restated as @xmath24    this formulation ( [ eqn : dual_mkl ] ) results in a sparse selection of kernels as shown in @xcite .",
    "similarly , equation ( [ eqn : kloft_dual_1 ] ) can also be rewritten as , @xmath25 the above formulation([eqn : kloft_dual ] ) is referred to as @xmath26 mkl throughout this paper . even though above formulation ( [ eqn : kloft_dual ] ) uses generic norm over @xmath27",
    ", there is no guarantee of explicit control over sparsity . in next section",
    ", we derive our cskl formulation by modifying the norm on @xmath27 .",
    "let @xmath28 denote the space of @xmath29 dimensional vectors with all components positive , i.e. @xmath30 . let @xmath31 be the @xmath32th largest component of @xmath33 , i.e. @xmath34 consider the following convex function on @xmath35 , @xmath36 where @xmath2 is a positive integer less than @xmath29 .",
    "we present our first claim by this theorem    [ theorem : b_norm ] if @xmath37 such that @xmath38 and @xmath39 defined as before then @xmath40 and at optimality @xmath41 and @xmath42    we begin by constructing the lagrangian of the problem @xmath43 where the lagrange multipliers are @xmath44 and @xmath45 .",
    "apart from the feasibility conditions on @xmath46 and the non - negativity constraints on the lagrange multipliers @xmath47 and @xmath48 the kkt conditions reads as @xmath49 the proof hinges on that fact that @xmath50 satisfies the kkt conditions .",
    "we note that both @xmath51 and @xmath52 can not be simultaneously positive . if @xmath53 , then could be obtained by setting @xmath54 and @xmath55 . as @xmath54 then @xmath56 . again if @xmath57 , then could be obtained by setting @xmath58 and @xmath59 . as @xmath60 then @xmath61 .",
    "interestingly note that if @xmath62 as both @xmath63 and @xmath64 .",
    "let us now suppose that @xmath65 the constraint @xmath66 can now be written as @xmath67 due to observations made before it is straightforward to see that @xmath68 and @xmath69 .",
    "one can always choose feasible @xmath70 such that @xmath71 this establishes the fact that @xmath65 indeed satisfies the kkt conditions and for which @xmath72 if @xmath73 .",
    "as kkt conditions are necessary and sufficient for this problem @xcite we see that at optimality @xmath74 obtained by substituting the @xmath46 obtained before .",
    "this completes the proof .    by introducing @xmath75 to the dual ( as in eqn .",
    "[ eqn : dual_mkl ] ) we get the following cskl formulation , @xmath76 note that cskl formulation ( [ eqn : bmkl_svm ] ) explicitly controls the sparsity of kernel selection by varying @xmath2 as is evident from theorem [ theorem : b_norm ] .      a variant of svm is the @xmath3-svm @xcite where parameter @xmath77 is replaced by a parameter @xmath78 $ ] . here , the parameter @xmath3 is lower bound on the fraction of number support vector and an upper bound on the fraction of margin errors . in this section",
    "we extend our cskl formulation to @xmath3-svm .",
    "the dual of @xmath3-svm is given by , @xmath79 introducing mkl to the dual of @xmath3-svm and rewriting it similar to equation ( [ eqn : dual_mkl ] ) .",
    "@xmath80 we now introduce our cskl formulation in the setting of @xmath3-svm .",
    "@xmath81 the above formulation is denoted as @xmath3-cskl throughout this paper .",
    "we present an alternating optimization scheme for solving the @xmath82 formulation . for a fixed @xmath46 ,",
    "we solve the following maximization for @xmath83 ,    @xmath84    note that in above problem @xmath46 should satisfy the conditions @xmath85 .",
    "we can use standard sequential minimal optimization(smo ) solver for the above problem .",
    "once optimal @xmath86 is calculated , we compute @xmath27 as , @xmath87 .",
    "next step is to solve for @xmath88 we can find the optimal @xmath46 by solving @xmath75 using reduced gradient descent or a linear programming based gradient descent .",
    "@xmath89 input : @xmath90 number of kernels @xmath91 @xmath92 solve @xmath83 using smo solver with kernel @xmath93 compute @xmath94 solve @xmath75 using reduced gradient descent or linear programming based solver @xmath95 @xmath96 @xmath97    @xmath98 set @xmath99 set @xmath100 , @xmath101 compute @xmath102 for @xmath103 compute the descend direction @xmath104 set @xmath105 @xmath106 @xmath107 @xmath108 @xmath109 @xmath110 @xmath111 @xmath112 ,",
    "@xmath113 @xmath114 compute @xmath115 linesearch along @xmath116 for @xmath117 $ ] @xmath118    in algorithm [ algo : redgrad ] , we present our reduced gradient algorithm to solve @xmath46 . the svm solver is used to obtain @xmath119 ( see algorithm [ algo : redgrad ] ) . the descent direction @xmath116 is defined as per algorithm [ algo : descentdir ] . in the case of @xmath3-cskl , the value of @xmath120 while in the case of c - cskl it is @xmath121 while the rest of the framework remains the same .",
    "input : kernel weights @xmath46 , selected pivot @xmath47 , calculated gradients @xmath122 output : optimal descent direction @xmath123 @xmath124 @xmath124 @xmath124 @xmath125 @xmath126    due to our assumptions on @xmath127 , in both the cases , @xmath119 is convex and differentiable with lipschitz gradient wrt .",
    "@xmath46 @xcite . for such functions",
    "the reduced gradient method converges with bounds as defined in @xcite .",
    "we also present a linear programming based approach to solve for @xmath75 .",
    "we use some standard lp solver to solve the following linear program for finding descent direction @xmath116 for @xmath75 .",
    "@xmath128 where @xmath129 and step size @xmath130 can be found by using line search .",
    "@xmath46 is updates as @xmath131 .",
    "though this algorithm is found to converge for @xmath132 we have no bounds on its convergence as yet .",
    "to illustrate the benefits of cskl formulation , we give results on synthetic data and the caltech101 @xcite real - world object categorization dataset .",
    "we compare cskl algorithm with simplemkl [ equation [ eqn : smkl_dual ] ] which is a sparse selection algorithm , and @xmath0 mkl [ equation [ eqn : kloft_dual_1 ] ] with @xmath133 which is a non - sparse selection algorithm .      in this section ,",
    "we describe the datasets we used for our experiments .",
    "to show the effect of noisy kernels , we generated @xmath134 kernels out of which @xmath135 are informative kernels and @xmath136 are noisy kernels .",
    "to build these kernels , we sampled @xmath137 datapoints with dimension @xmath138 from two independent gaussian distributions with covariance as the identity matrix and different means(@xmath139 and @xmath140 , datapoints sampled from different gaussians are assumed to belong to different classes ) .",
    "we generated four kernels ( two gaussian(@xmath141 and @xmath141 ) and two polynomial kernels(@xmath141 and @xmath141 ) ) for each dimension seprately and all together(@xmath142 ) .",
    "on top of this , we also added two carefully chosen noisy kernels to this kernel set .",
    "the caltech101 dataset has 102 categories of images such as airplanes , cars , leopards , etc .",
    "it has been shown by @xcite that multiple image descriptors aid in the generalization ability of the learnt classifier . using the method followed by @xcite , we extract the following 4 descriptors : phowcolor , phowgray , geometricblur and selfsimilarity .",
    "each descriptor gives rise to a distance matrix .",
    "we create multiple gaussian kernels for each descriptor by varying the gaussian width parameter used to generate the kernel .",
    "we currently used 5 width values in the log space of -4 to 0 .",
    "hence we arrive at a total of 20 kernels .",
    "the number of binary classification problems are 5151 and 102 , for 1-vs-1 and 1-vs - rest classification approaches respectively .",
    "the key result we wish to establish is that by suitable variation of parameter @xmath2 in cskl , one can combine good kernels and eliminate noisy kernels and achieve better generalization than other mkl formulations .",
    "+ in the synthetic dataset setting @xmath143 will facilitate sparse selection , and @xmath144 facilitates a complete non - sparse selection . as shown in the figure [ toy ] , the cskl formulation clearly outperforms both sparse and non - sparse mkl by setting @xmath145 .",
    "it is clear that setting @xmath145 in cskl gives better generalization performance than both @xmath146 and @xmath147 .",
    "this clearly shows neither sparse nor complete non - sparse is good for this dataset .",
    "cskl is the only formulation which can capture all good kernels but still eliminate the noisy kernels by tuning parameter @xmath2 .",
    "+ in order to demonstrate that neither sparse nor non - sparse solutions are always the best in real - world datasets , we take all the binary 1-vs-1 and 1-vs - rest classifiers in caltech101 dataset and compare simplemkl and @xmath148 mkl solutions .",
    "figures [ fig : kloft_vs_simplemkl ] , [ fig : kloft_vs_simplemkl_1vrest ] show the ratio of improvement in accuracy of @xmath148 mkl over simplemkl .",
    "it is evident from the figures that neither of the algorithms are always the best .",
    "thus , depending on the binary classification problem , we need to have different controls on the sparsity to achieve the state - of - the - art performance .",
    "clearly this motivates that , to achieve the desired sparsity , we can use the cskl formulation instead of either simplemkl or @xmath148 mkl . in next section",
    "we show how cskl can achieve better performance than other algorithms .",
    "we apply our cskl algorithm , and compare its performance against the other state - of - the - art algorithms simplemkl and @xmath149 mkl .",
    "we take the highest accuracy achieved by cskl across various values of parameter @xmath2 for the comparison .",
    "figures [ fig : bmkl_vs_kloft_1v1 ] , [ fig : bmkl_vs_simplemkl_1v1 ] show the ratio of improvement in accuracy of cskl over @xmath148 mkl and simplemkl .",
    "we also present here overall performance of cskl on caltech101 dataset .",
    "figure [ fig : caltech101 ] shows the performance of cskl as @xmath2 is varied . for comparison",
    ", we have shown a straight line which shows the average accuracy achieved by simplemkl and @xmath149 mkl .",
    "figure [ fig : caltech101 ] clearly shows that all the 20 kernels are not necessary , since the cskl accuracy more or less saturates after @xmath150 .",
    "the result also shows that a sparse selection algorithm like simplemkl wo nt be most efficient algorithm in terms of the accuracy achieved .",
    "and the performance of cskl is almost equal to that of @xmath149 mkl , but the latter selects all the provided kernels , while we can achieve competitive accuracy with the former itself at @xmath151 .",
    "note that no other formulation can give this flexibility to users to select exactly four best performing kernels .",
    "it is natural to use use @xmath145 here because number of descriptors used is four .",
    "hence the experiments demonstrated in this section provide a proper justification for the usage of the cskl formulation .      to analyze more on why cskl achives better accuracy , we plot the histogram of number of descriptors selected when cskl outperforms @xmath148 mkl and simplemkl in the binary classification problems .",
    "we see that , in the figures [ fig : simplemkl_vs_bmkl_1v1_hist_bar ] , [ fig : simplemkl_vs_bmkl_1vrest_hist_bar ] , simplemkl only selects one or two descriptors whereas cskl select all the descriptors . for the cases where simplemkl does nt perform the best",
    ", non - sparse combination might be a better choice , and this is emperically confirmed in the figures [ fig : simplemkl_vs_bmkl_1v1_hist_bar ] , [ fig : simplemkl_vs_bmkl_1vrest_hist_bar ] . similarly from the figures [ fig : kloft_vs_bmkl_1v1_hist_bar ] , [ fig : kloft_vs_bmkl_1vrest_hist_bar ] , we see that @xmath148 mkl selects all the descriptors whereas cskl does not select all descriptors most of the cases .",
    "these are expected , since , for the cases where @xmath148 mkl perform low , it may mean that a non - sparse classification is preferable . and",
    "the same is reflected in the figures [ fig : kloft_vs_bmkl_1v1_hist_bar ] , [ fig : kloft_vs_bmkl_1vrest_hist_bar ] .    out of the 5151 binary classification problems in the 1-vs-1 setting",
    ", @xmath3-cskl performed better in 5112 and 1498 cases against @xmath148 mkl and simplemkl respectively . similarly in the 1-vs - rest setting , out of the 102 classification problems , the numbers turned out to be 97 and 91 against @xmath148 mkl and simplemkl respectively .    finally , figure [ fig : caltech101_desc ] shows the number of descriptors selected as @xmath2 is increased .",
    "we can infer that as @xmath2 increased beyond 4 , all the descriptors are selected .",
    "this is also not surprising since all the 4 descriptors used in our experiment are independent and experimentally they have been shown to aid the accuracy of the object categorization problem .",
    "as we have seen , both sparse and non - sparse mkl have their handicaps depending on the classification problem at hand .",
    "niehter of them are always the best .",
    "also , in such problems the time taken to calculate the features is one the biggest bottlenecks .",
    "for all these reasons , a formulation with strict control of sparsity would be the best solution to have .",
    "one can then tune the sparsity parameter @xmath2 and select the best set of kernels for any particular classification problem .",
    "we have described one such formulation in this paper along with the associated solution algorithms .",
    "we have also shown the superior performance of this formulation with respect to both the sparse and non - sparse formulations of mkl for the application problem of object categorization .",
    "lanckriet ,  g.r.g . and cristianini ,  n. and bartlett ,  p. and el ghaoui ,  l. and jordan ,  m.i . ,",
    "learning the kernel matrix with semidefinite programming , journal of machine learning research , vol 5 , pages 27 - 72 , 2004 .",
    "saketha nath jagarlapudi , dinesh govindaraj , raman s , chiranjib bhattacharyya , aharon ben - tal , k. r. ramakrishnan , on the algorithmics and applications of a mixed - norm based kernel learning formulation , proceedings of the neural information processing systems , 2009 .",
    "marius kloft , ulf brefeld , soeren sonnenburg , pavel laskov , klaus - robert mller , alexander zien , efficient and accurate lp - norm multiple kernel learning , proceedings of the neural information processing systems , 2009 ,      maria - elena  nilsback and andrew  zisserman , a visual vocabulary for flower classification , proceedings of the 2006 ieee computer society conference on computer vision and pattern recognition , vol 2 , pages 1447 - 1454 , 2006 .",
    "maria - elena  nilsback and andrew  zisserman , automated flower classification over a large number of classes , proceedings of the sixth indian conference on computer vision , graphics & image processing , 2008 .",
    "bonnans , j. frdric and gilbert , jean charles and lemarchal , claude and sagastizbal , claudia a. , numerical optimization : theoretical and practical aspects ( universitext ) , springer - verlag new york , inc ."
  ],
  "abstract_text": [
    "<S> multiple kernel learning(mkl ) on support vector machines(svms ) has been a popular front of research in recent times due to its success in application problems like object categorization . </S>",
    "<S> this success is due to the fact that mkl has the ability to choose from a variety of feature kernels to identify the optimal kernel combination . </S>",
    "<S> but the initial formulation of mkl was only able to select the best of the features and misses out many other informative kernels presented . to overcome this , </S>",
    "<S> the @xmath0 norm based formulation was proposed by kloft et . </S>",
    "<S> al . </S>",
    "<S> this formulation is capable of choosing a non - sparse set of kernels through a control parameter @xmath1 . </S>",
    "<S> unfortunately , the parameter @xmath1 doesnot have a direct meaning to the number of kernels selected . </S>",
    "<S> we have observed that stricter control over the number of kernels selected gives us an edge over these techniques in terms of accuracy of classification and also helps us to fine tune the algorithms to the time requirements at hand . in this work , we propose a controlled sparsity kernel learning ( cskl ) formulation that can strictly control the number of kernels which we wish to select . </S>",
    "<S> the cskl formulation introduces a parameter @xmath2 which directly corresponds to the number of kernels selected . </S>",
    "<S> it is important to note that a search in @xmath2 space is finite and fast as compared to @xmath1 . </S>",
    "<S> we have also provided an efficient reduced gradient descent based algorithm to solve the cskl formulation , which is proven to converge . through our experiments on the caltech101 object categorization dataset </S>",
    "<S> , we have also shown that one can acheive better accuracies than the previous formulations through the right choice of @xmath2 . </S>"
  ]
}