{
  "article_text": [
    "multilevel monte carlo ( mlmc ) methods pioneered by @xcite and @xcite are now standard for estimation of expectations of functionals of processes defined by stochastic differential equations ( sdes ) .",
    "while the mlmc techniques have origins in the integral operators @xcite and sdes @xcite , they can be applied also in other application domains , where estimates with gradually increasing accuracy are available ; see the recent review by @xcite and references therein .",
    "more recently , so - called debiasing techniques @xcite have attracted a lot of research activity ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) , although similar ideas have been suggested much earlier in more specific contexts ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "these techniques are based on similar ideas as mlmc , but instead of optimal allocation of computational resources for minimising the error , the primary focus is in providing unbiased estimators .",
    "monte carlo inference is straightforward with independent unbiased samples , allowing to construct confidence intervals in a reliable way @xcite .",
    "debiasing techniques may also be employed within a stochastic approximation algorithm @xcite . in particular , in a stochastic gradient descent type algorithm @xcite relevant for instance in maximum likelihood inference @xcite , unbiased gradient estimate implies pure martingale noise , which is supported by a well - established theory  ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "the debiasing techniques involve balancing with cost and variance , which often boils down to similar methods and conditions as those that are used with mlmc .",
    "the connection between mlmc and debiasing techniques has been pointed out earlier at least by @xcite , @xcite and @xcite , but this connection has not been fully explored yet . the purpose of this paper is to further clarify the connection of mlmc and debiasing techniques , within a general framework for unbiased estimators .",
    "many techniques for unbiased estimation other than those considered here have been suggested in the literature .",
    "for instance , there is a whole body of literature for ` perfect sampling ' by markov chains @xcite or with certain classes of sde models @xcite ; see also the recent monograph by @xcite and references therein .",
    "perfect sampling can be used to construct unbiased estimators , but the problem is generally more prestigious and often harder to implement .",
    "see also the recent article by @xcite for discussion of other related unbiased estimation techniques .",
    "the rest of the paper is organised as follows .",
    "the multilevel monte carlo and the previous debiasing methods are presented briefly in sections [ sec : mlmc ] and [ sec : unbiased ] , respectively .",
    "section [ sec : general ] introduces a new general unbiased scheme with an explicit expression for its variance ( theorem [ thm : general ] ) .",
    "the unbiased estimators suggested by @xcite and @xcite are reformulated as specific instances of this scheme , as well as an obvious ` hybrid ' scheme with mlmc and unbiased components ( example [ ex : hybrid - mlmc ] ) .",
    "new unbiased estimators are suggested in section [ sec : new ] .",
    "two of the new schemes , termed stratified and residual sampling estimators , have provably lower variance than simple averages of independent unbiased estimates ( proposition [ prop : consistency ] ) . because stratification is a well - known variance reduction technique",
    ", these estimators may be well - known , but they do not seem to be recorded in the literature yet .",
    "the first main finding of this paper is theorem [ thm : residual - eff ] which shows that the asymptotic variance of two of new schemes are asymptotically equal to that of mlmc under general conditions .",
    "this result suggests that unbiasedness can often be achieved with asymptotically negligible additional variance .",
    "the expected cost of the methods is discussed in section [ sec : cost ] .",
    "the new schemes appear even more appealing after seeing that the expected cost of mlmc and the unbiased schemes are also asymptotically equivalent ( proposition [ prop : mlmc - expected - cost ] ) and therefore the efficiency of an estimator with new schemes can be made arbitrarily close to mlmc ( corollary [ cor : batch - complexity ] ) . the limiting variance formulation in theorem [ thm : residual - eff ] leads into an easily applicable optimisation criteria for the sampling distribution related to the new estimators .",
    "section [ sec : stopping ] presents a further generalisation of the unbiased scheme , which accomodates further conditioning and dependent randomisation schemes based on stopping times .",
    "numerical experiments in section [ sec : numerical ] show how the efficiency bounds predicted by theory are attained in four examples , three of which were also studied by @xcite .",
    "the paper is concluded by a discussion about the implications of the findings in section [ sec : conclusions ] . some practical guidelines and possible future topics",
    "are discussed as well .",
    "the starting point of mlmc is the existence of estimators @xmath0 , which are increasingly accurate approximations of a given ` target ' random variable @xmath1 , whose expectation is of interest .",
    "that is , @xmath2 are random variables with @xmath3 , and the task is to provide a numerical approximation of @xmath4 .",
    "simulation cost of a single realisation of @xmath5 increases in @xmath6 , which calls for optimisation of computational resources .",
    "such a scenario arises for instance in the context of sde models , commonly applied in option pricing .",
    "in such an application , we might have @xmath7 , where @xmath8 is a given ` payoff ' function , and @xmath9 corresponds to the terminal value of the process @xmath10}$ ] solving the sde @xmath11 , \\qquad x_0\\equiv            x_0 ,            \\label{eq : sde}\\ ] ] where @xmath12 stands for the standard brownian motion , the functions @xmath13 are the drift and diffusion parameters of the model , and @xmath14 is the initial value . unless @xmath15 and @xmath16 are of certain specific form , the random variable @xmath9 can not be simulated exactly , but admits easily implementable approximations based on time - discretisation of .",
    "the commonly applied dyadic uniform meshes are defined as follows @xmath17 the _ milstein _ discretisation ( e.g. * ? ? ? * ) corresponding to @xmath18 is defined by letting @xmath19 and iteratively calculating @xmath20,\\end{aligned}\\ ] ] for @xmath21 , where @xmath22 and @xmath23 are independent @xmath24 random variables .",
    "the final value of such an iteration provides an approximation of @xmath9 , so we may set @xmath25 for @xmath26 .",
    "the cost of simulating a single realisation of @xmath5 is of order @xmath27 , and under certain , fairly general , conditions on @xmath28 and @xmath29 , the mean square error @xmath30 with @xmath31 ( e.g. * ? ? ?",
    "the mlmc is an efficient way to use such approximations in order to approximate @xmath4 .",
    "it is based on the following , seemingly trivial observation @xmath32 which suggests that one may construct an estimate of @xmath33 using a separate monte carlo approximation of each term @xmath34 : @xmath35 where @xmath36 and @xmath37 are independent random variables with @xmath38 .",
    "it is direct to check that then @xmath39 .",
    "there are two key requirements which make this useful :    a.   the random variables @xmath37 are usually independent realisations of @xmath40 , where the approximations @xmath5 and @xmath41 are _ dependent _ , such that @xmath42 is typically small when @xmath6 is large . in the context of the sde example discussed above , such a coupling arises naturally when using a common brownian path in the discretisations leading to @xmath5 and @xmath41 .",
    "b.   it allows to _ optimise _ the computational effort devoted to each ` level ' to estimate @xmath34 .",
    "the key benefit is that fewer samples @xmath43 are often necessary with higher @xmath6 , which leads to lower overall cost .",
    "theorem 1 of @xcite quoted below gives the complexity of mlmc under common exponential framework .",
    "it assumes that the expected cost ( computational complexity ) of each term @xmath37 is @xmath44 , so that the expected cost of @xmath45 is @xmath46 .",
    "[ thm : giles ] suppose @xmath47 are independent , and there exist positive @xmath48 with @xmath49 , such that for all @xmath50 , @xmath51 then , there exists a positive constant @xmath52 such that for any @xmath53 there are values @xmath54 and @xmath55 such that the mlmc estimator satisfies the following mean square error ( mse ) bound @xmath56 and satisfies the expected cost bound @xmath57    the sde application discussed above often satisfies the conditions of theorem [ thm : giles ] with @xmath58 @xcite . in a multidimensional sde setting ,",
    "the antithetic truncated milstein @xcite often lead to @xmath58 as well .",
    "this highlights why mlmc has become so popular  it is often possible to attain a ` canonical ' rate @xmath59 , equivalent with a square root error rate @xmath60 , despite the bias , using simple standard discretisation methods .",
    "crude monte carlo , that is , taking a fixed level @xmath54 and averaging independent realisations of @xmath61 , leads to worse rates .",
    "this canonical square root error rate can be attained with similar assumptions using the debiasing techniques which are discussed next .",
    "as with mlmc , assume that @xmath2 and @xmath1 are integrable random variables with @xmath62 . taking @xmath63 , @xmath64 the fundamental observation behind the unbiased schemes is that one may employ randomisation to pick a finite number of terms of the series to construct estimates with expectation @xmath4 .",
    "the two results below due to @xcite propose two such estimators , along with expressions for their second moments .",
    "proofs of theorems [ thm : single - term ] and [ thm : sum ] are shown to follow as a consequence of theorem [ thm : general ] in section [ sec : general ] ( examples [ ex : single ] and [ ex : sum ] ) .",
    "[ cond : single - term ] suppose @xmath65 are uncorrelated random variables with @xmath66 , and @xmath67 is a probability distribution such that @xmath68 for all @xmath69 and @xmath70    [ thm : single - term ] suppose @xmath65 and @xmath67 satisfy condition [ cond : single - term ] , and @xmath71 is independent random variable with @xmath72 , then the _ single - term estimator _ @xmath73",
    "@xcite first suggested the following estimators , but with different conditions and different expression for the variance .",
    "[ cond : sum ] suppose @xmath65 are random variables with @xmath66 , and @xmath67 is a probability distribution such that @xmath74 for all @xmath69 and either one of the following hold :    a.   [ item : coupled - sum ] @xmath40 and @xmath75 b.   [ item : independent - sum ] @xmath65 are uncorrelated and @xmath76    [ thm : sum ] suppose @xmath65 and @xmath67 satisfy condition [ cond : sum ] and @xmath71 is independent with @xmath72 , then the _ sum estimator _",
    "@xmath77 satisfies @xmath78 .",
    "in case of condition [ cond : sum ] , @xmath79 and , in case of condition [ cond : sum ] , @xmath80    we follow @xcite and call the estimator of theorem [ thm : sum ] _ coupled sum _ in case of condition [ cond : sum ] and _ independent sum _ in case of condition [ cond : sum ] .",
    "the consistency assumption in the latter case is slightly milder than that of @xcite , where condition [ cond : sum ] is assumed also in case of uncorrelated @xmath65 .",
    "let us briefly return to the case where @xmath81 and @xmath82 corresponds to the solution of a time - discretised sde with a mesh of @xmath27 points as discussed in section [ sec : mlmc ] .",
    "many approximation schemes admit a weak rate @xmath83 , which implies that @xmath84 ( cf . * ? ? ?",
    "if additionally @xmath85 with @xmath86 , often satisfied for instance by the milstein scheme @xcite or in the multivariate case by the antithetic truncated milstein scheme @xcite , then taking @xmath87 , where @xmath88 satisfies conditions [ cond : single - term ] and [ cond : sum ] .",
    "because the cost of level @xmath6 is of order @xmath27 , this sampling scheme admits a finite expected cost ( see section [ sec : cost ] for details ) .",
    "this shows that unbiased estimators with finite variance and finite expected cost can be obtained with the same conditions under which mlmc admits the canonical error rate . in case of correlated @xmath65 in condition [ cond : sum ] , a slightly more stringent condition about the ` strong rate ' @xmath30 with @xmath31 is required .",
    "@xcite pointed out that averaging @xmath89 independent single term estimators @xmath90 introduced in theorem [ thm : single - term ] corresponds to an estimator of the form @xmath91 where @xmath92 is the number of samples from level @xmath6 , and the expectation of @xmath92 is @xmath93 .",
    "this shows a close connection with mlmc , which corresponds taking @xmath94 ( up to some level @xmath54 ) .",
    "inspired by this remark , and the techniques by @xcite , consider the following general unbiased scheme , with an expression for its variance .",
    "[ thm : general ] suppose @xmath2 and @xmath1 are integrable random variables such that @xmath3 , and let @xmath65 be square integrable random variables such that @xmath95 for all @xmath50 , with @xmath96 .",
    "assume @xmath97 for @xmath98 are independent realisations of the process @xmath65 .",
    "suppose @xmath92 are non - negative square integrable random integers independent of @xmath99 and with @xmath100 for all @xmath50 .",
    "define for @xmath101 @xmath102 if there exist a strictly increasing sequence of positive integers @xmath103 such that @xmath104 and @xmath105 almost surely , then the estimator @xmath106 is unbiased @xmath107 and @xmath108 .",
    "the proof of theorem [ thm : general ] is given in appendix [ app : proof - general ] .",
    "consider first a simple example of theorem [ thm : general ] , where @xmath92 are taken independent .",
    "let @xmath109 be independent with @xmath110 and @xmath111 , and suppose @xmath112 , and let @xmath65 be uncorrelated .",
    "then , @xmath113 if @xmath114 a.s .  and @xmath115/\\mu_i<\\infty$ ]",
    ", then the corresponding estimator is unbiased with variance @xmath116 . in particular , taking @xmath92 poisson with intensity @xmath93 where @xmath117 , then @xmath118 by the borel - cantelli lemma and @xmath119 .",
    "the averages of single term estimators and sum estimators introduced in theorems [ thm : single - term ] and [ thm : sum ] correspond to certain dependence structures of @xmath120 , as we shall see next .",
    "all details in the following examples are given , as the new schemes introduced in section [ sec : new ] will be based on examples [ ex : single ] and [ ex : sum ] below .",
    "[ ex : single ] suppose condition [ cond : single - term ] holds and @xmath121 are independent .",
    "define @xmath122 and call the estimator @xmath123 .",
    "it is easy to see that this corresponds to an average of @xmath89 independent single term estimators @xmath90 .",
    "note that @xmath109 follow a multinomial distribution with parameters @xmath89 and @xmath67 .",
    "we have @xmath124 and @xmath125      = n p_i { \\mathbbm{i}\\left\\{i = k\\right\\ } } + n(n-1 ) p_i p_k,\\ ] ] so @xmath126 . by the independence of @xmath65 , @xmath127 by assumption @xmath128 , and",
    "therefore also @xmath62 , which imply @xmath129 .",
    "the variance satisfies @xmath130 so @xmath131 .",
    "the above also proves theorem [ thm : single - term ] with @xmath132 .",
    "[ ex : sum ] suppose condition [ cond : sum ] holds and define @xmath133 , where @xmath134 are independent with @xmath135 .",
    "this estimator , which we denote @xmath136 , corresponds to average of @xmath89 independent sum estimators @xmath137 .",
    "we have @xmath138 , and for all @xmath139 , @xmath140 so @xmath141 .",
    "we also have @xmath142 because @xmath143 for @xmath144 , so for @xmath145 , @xmath146    if condition [ cond : sum ] holds , we obtain @xmath147 because @xmath148 .",
    "let @xmath149 be from lemma [ lem : subsequence ] in appendix [ app : single - sum ] .",
    "then for any @xmath150 @xmath151 which converges to zero as @xmath152 .",
    "the variance satisfies by dominated convergence @xmath153 we may conclude that the variance of the estimator is @xmath154 .",
    "suppose then that condition [ cond : sum ] holds .",
    "it is straightforward to check that then @xmath155 which satisfies @xmath129 by assumption , and by dominated convergence @xmath156 taking @xmath132 concludes also the proof of theorem [ thm : sum ] .",
    "the last example is an obvious ` hybrid ' scheme involving mlmc and an ` unbiased tail ' scheme , which also falls into the framework of theorem [ thm : general ] .",
    "[ ex : hybrid - mlmc ] assume that @xmath157 and that @xmath158 for @xmath159 , and @xmath160 are positive with @xmath161 , let @xmath134 be independent with @xmath162 , and define @xmath163 for @xmath164 .",
    "the estimator can be written as @xmath165 the first term coincides with an mlmc estimator with @xmath54 levels , with expectation @xmath33 , and the second term is an average of single term estimators of @xmath166 , with @xmath167 samples .",
    "note that we could have used any unbiased scheme in the latter part , provided it satisfies the conditions in theorem [ thm : general ] .",
    "the new sampling schemes discussed next in section [ sec : new ] provide a different view on the balancing ; see in particular theorem [ thm : residual - eff ] .",
    "let us now turn into new practically interesting estimators which correspond to specific choices in theorem [ thm : general ] .",
    "the estimators are based on stratification , a classical variance reduction technique in survey sampling ( e.g. * ? ? ?",
    "* ) , which have also been widely used in monte carlo ; see for instance @xcite .",
    "the following lemma states classical results on stratification ( cf . * ? ? ?",
    "* for proof ) .",
    "[ lem : strat ] let @xmath168 be an integrable random variable and let @xmath169 be exhaustive disjoint events with @xmath170 , and let @xmath171 .",
    "assume @xmath172 are random variables with conditional laws @xmath173 .",
    "the stratified estimator @xmath174 satisfies    a.   [ item : strat - pessimistic ] unbiasedness @xmath175 and @xmath176 . b.   [ item : prop - alloc ] if @xmath177 are independent and @xmath178 ( proportional allocation ) , then @xmath179 and @xmath180 .    in",
    "what follows , assume @xmath67 are positive and such that @xmath181 , and denote @xmath182 .",
    "let us first consider uniform stratification schemes .",
    "for that purpose , recall the definition of the generalised inverse distribution function @xmath183 corresponding to @xmath67 , @xmath184 the well - known inverse distribution function method states that a uniform @xmath185 is transformed by @xmath186 into @xmath187 with @xmath72 .",
    "[ def : stratified ] let @xmath188 , and assume @xmath189 are independent @xmath190 random variables for @xmath191 .",
    "let @xmath192 .",
    "a.   [ item : strat - single ] define @xmath193 , then the estimator @xmath194 defined as in theorem [ thm : general ] is the _ uniformly stratified single term estimator_. b.   [ item : strat - sum ] define @xmath133 , then the estimator @xmath195 defined as in theorem [ thm : general ] is the _ uniformly stratified sum estimator_.    [ def : systematic ] let @xmath196 and define @xmath197 for all @xmath191 , where @xmath198 , and let @xmath199 . define then @xmath92 and @xmath200 as in definition [ def : stratified ] and , respectively , and the corresponding _",
    "systematic sampling single - term estimator _ @xmath201 and _ systematic sampling sum estimator _ @xmath202 .",
    "the consistency and a variance bound for the uniformly stratified and systematic sampling estimators are stated in proposition [ prop : consistency ] , after introducing another slightly different stratification scheme .",
    "[ def : residual ] let @xmath188 , define @xmath203 and let @xmath204 .",
    "define the ` residual ' probability distribution @xmath205 as @xmath206 , and let @xmath207 be independent random variables such that @xmath208 for @xmath209 .",
    "a.   define @xmath210 where @xmath211 , then the estimator @xmath212 defined as in theorem [ thm : general ] is the _ residual sampling single term estimator_.",
    "b.   define @xmath213 , where @xmath214 and @xmath215 , then the estimator @xmath216 defined as in theorem [ thm : general ] is the _ residual sampling sum estimator_.    [ prop : consistency ] the estimators in definitions [ def : stratified ] , [ def : systematic ] and [ def : residual ] satisfy :    a.   assume condition [ cond : single - term ] , then @xmath217 b.   assume condition [ cond : sum ] , then @xmath218    proposition [ prop : consistency ] follows as a consequence of lemma [ lem : strat ] ; the detailed proof is given in appendix [ app : proof - prop - consistency ] .    instead of using @xmath219 in systematic sampling , we could use instead any sequence @xmath220 $ ] , and let @xmath221 , where @xmath185 .",
    "this would not be stratification , but it is direct to check the estimators still retain the same expectation , and also the same pessimistic variance bound .",
    "for instance , using a low - discrepancy sequence @xmath222 would correspond to randomised quasi - monte carlo ( e.g. * ? ? ? * ) .",
    "let us next consider pseudo - code implementations of the stratified and systematic sampling estimators .",
    "algorithm [ alg : stratified ] describes an efficient implementation which produces @xmath223 as in definition [ def : stratified ] . the function defined as but with the lines [ strat - for ] and [ strat - draw ] interchanged produces @xmath223 following definition [ def : systematic ] .",
    "@xmath224 ; @xmath225 [ strat - for ] draw independent @xmath226 [ strat - draw ] @xmath227 @xmath228 ; @xmath229 @xmath230 @xmath231    algorithm [ alg : single - term ] calculates one realisation of @xmath232 or @xmath233 corresponding to the sde setting discussed in section [ sec : mlmc ] , when called with @xmath234 and @xmath235 or @xmath236 , respectively .",
    "similarly , the independent sum and coupled sum estimators @xmath237 or @xmath238 are simulated if @xmath239 or @xmath240 , respectively .",
    "algorithm [ alg : single - term ] could be applied similarly with @xmath241 leading to @xmath242 and @xmath243 , by defining such that @xmath244 satisfy @xmath245 and drawing independent @xmath246 .",
    "@xmath247 ; @xmath248 @xmath249@xmath250 @xmath251^{-1}$ ] @xmath252 [ line : tailprob ] simulate brownian path @xmath253 at mesh @xmath254 .",
    "@xmath255 [ for - single - coupled ] simulate brownian path @xmath253 at mesh @xmath256 .",
    "@xmath257 where @xmath258 is calculated using @xmath253 .",
    "@xmath259 @xmath260 @xmath261    proposition [ prop : consistency ] states that all the new estimators are unbiased , and that the uniformly stratified and residual sampling estimators can not be worse than averages of independent single term and sum estimators , in terms of variance .",
    "in fact , they often have a strictly lower variance , but it is generally difficult to quantify the benefit . the following theorem [ thm : residual - eff ] is often more useful , indicating that residual sampling attains asymptotically the efficiency of mlmc under general conditions .",
    "let us first formulate ` idealised ' mlmc strategies based on limiting allocations @xmath67 .",
    "[ def : mlmc ] assume @xmath67 are non - negative with @xmath262 and define @xmath263 . for any @xmath188 ,",
    "define @xmath203 , @xmath264 , @xmath265 and @xmath266 and denote @xmath267    practical implementations of mlmc are often based on application of stopping rules , which may determine @xmath43 and @xmath268 during simulation .",
    "definition [ def : mlmc ] can be therefore viewed as ` idealised ' version of mlmc , where ( nearly ) optimal allocation strategy @xmath67 is known beforehand , and @xmath43 and @xmath268 are determined in terms of a single ` running - time ' parameter @xmath89 .",
    "[ thm : residual - eff ] below , @xmath269 ( resp .",
    "@xmath270 ) stands for either @xmath212 or @xmath194 ( resp .",
    "@xmath271 or @xmath272 ) .",
    "a.   [ item : residual - eff - single ] assume condition [ cond : single - term ] then @xmath273 b.   [ item : residual - eff - sum ] assume condition [ cond : sum ] , then @xmath274 and if @xmath275 , then @xmath276 .",
    "c.   [ item : eff - independent - sum ] assume condition [ cond : sum ] , then @xmath277    the proof of theorem [ thm : residual - eff ] is given in appendix [ app : proof - eff ] .",
    "[ rem : eff - diff ] the limiting variances in theorem [ thm : residual - eff ] can be significantly lower than the upper bounds given in proposition [ prop : consistency ] , which correspond to averaging independent unbiased estimates .",
    "indeed , @xmath278 , so @xmath279.\\ ] ] likewise , in case of condition [ cond : sum ] , @xmath280 , so @xmath281 note that @xmath282 for all @xmath6 , so all the terms in the sum are positive .",
    "let us next consider the cost of simulating estimators @xmath261 of the form given in theorem [ thm : general ] .",
    "assume each @xmath37 has random cost @xmath283 , such that the total cost of @xmath261 is @xmath284 .",
    "assume also that @xmath285 are independent of @xmath109 and @xmath286 have common mean @xmath287 .",
    "the expected cost can be written down as follows . @xmath288 the following records the immediate fact that the estimators introduced in section [ sec : new ] have the same expected cost as the simple averages of independent estimates .",
    "[ prop : cost - new ] denote the cost of the estimator @xmath269 by @xmath289 , where ` @xmath290 ' is a place holder .",
    "then ,    a.   @xmath291 , b.   @xmath292 .",
    "the following proposition records that the mlmc estimators introduced in definition [ def : mlmc ] have asymptotically equivalent cost than the corresponding unbiased estimators with same limiting allocation strategies    [ prop : mlmc - expected - cost ] the expected cost of mlmc estimators in theorem [ thm : residual - eff ] satisfy @xmath293    the expected cost of @xmath294 is @xmath295 clearly @xmath296 , @xmath297 and @xmath298 as @xmath299 , so if @xmath300 , the result follows by dominated convergence ; otherwise one can use fatou s lemma . the proof with @xmath301 is identical .    @xcite formulate an asymptotic efficiency principle , which states that if @xmath302 are i.i.d .",
    "estimators with finite variance @xmath303 and with expected cost @xmath304 , then the average of such estimators has asymptotic relative efficiency @xmath305^{-1}$ ] . considering this notion of efficiency , let @xmath188 , and consider the following estimator , which is the average of @xmath54 independent realisations of the estimators above , @xmath306 where @xmath307 are independent realisations of @xmath269 , where ` @xmath290 ' is a place holder . by letting @xmath308",
    ", one may consider the asymptotic efficiency of this estimator as in the following result , stating that stratified and residual sampling procedures always improves upon averages of independent single term and sum estimators , and can be made arbitrarily close to mlmc in asymptotic efficiency .",
    "[ cor : batch - complexity ] suppose @xmath188 is fixed , then the following asymptotic efficiency holds when @xmath308 .",
    "a.   if condition [ cond : single - term ] holds and @xmath309 , then the asymptotic efficiency of both @xmath310 and @xmath311 is no worse than that of @xmath312 , and can be made arbitrarily close to that of @xmath313 by choosing @xmath89 sufficiently large .",
    "b.   if condition [ cond : sum ] holds and @xmath314 , then the asymptotic efficiency of both @xmath315 and @xmath316 is no worse than that of @xmath317 , and can be made arbitrarily close to that of @xmath318 by choosing @xmath89 sufficiently large .    strictly speaking , the use of asymptotic efficiency principle requires that the multilevel estimators @xmath317 would be guaranteed to satisfy a functional central limit theorem , which is out of the scope of this work ; see , however , the recent work of @xcite in this direction in a different setting .",
    "if we assume @xmath89 is taken sufficiently large so that the limits in theorem [ thm : residual - eff ] are approximately attained , the asymptotic efficiency principle suggests a rule for tuning the distribution @xmath319 for stratified and residual sampling distributions : the distribution @xmath67 which minimises the asymptitic inverse relative efficiency ( ire ) @xmath320 or @xmath321 maximises the efficiency . for the single term estimator , condition [ cond : single - term ] ,",
    "this leads to @xmath322 the solution to is proportional to @xmath323 , if @xmath324 ( * ? ? ?",
    "* proposition 1 ) .",
    "this is straightforward to implement in practice , because reliable estimates of @xmath325 are easily available .",
    "a straightforward practical procedure , also implemented in the experiments below , is to use the ` empirical optimal distribution ' @xmath326 to define directly the first @xmath54 probabilities @xmath327 and a tail probability @xmath328 .",
    "the tail probabilities @xmath329 for @xmath330 are then defined to follow a parametric distribution which guarantees a finite variance based on theory . in the experiments ,",
    "the tail distribution was chosen to be geometric .    in case of the independent sum estimator , condition",
    "[ cond : sum ] , similar minimisation with @xmath331 in place of @xmath332 yields the asymptotically optimal distribution @xmath333 .",
    "however , contrary to the single - term estimator , @xmath334 must additionally be non - increasing . because is invariant under multiplicative constants on @xmath67",
    ", the independent sum estimator can never outperform the single term estimator , in terms of asymptotic ire .",
    "the coupled sum estimator , condition [ cond : sum ] , leads to the optimisation problem @xmath335 this is more involved for two reasons .",
    "estimation of the terms @xmath336 is not straightforward , but in practice , a reasonable approximation may be obtained by approximating @xmath1 with @xmath61 , where @xmath54 is large .",
    "the optimisation is more challenging as well , and in fact , the optimal strategy may require considering ` subsequence ' estimators , employing level differences @xmath337 , for some subsequence @xmath338 .",
    "this leads to a combinatorial problem , which is discussed by @xcite , who also describe a dynamic programming algorithm which finds such optimal @xmath339 , up to an index @xmath54 .    in some applications",
    "it is not possible to choose @xmath67 which yield both finite variance and finite expected cost .",
    "then , it is not possible to attain a canonical square root convergence rate , but @xcite suggest another approach : choose @xmath67 that ensure finite variance , but which imply infinite expected cost . using a result due to @xcite , they deduce complexity results for unbiased estimators which are close to what are possible with mlmc .",
    "however , quantifying the efficiency in the present setting is not straightforward , so this is left for future work .",
    "the general unbiased scheme proposed in theorem [ thm : general ] is based on independent randomisation , that is , @xmath109 are assumed independent of @xmath47 .",
    "such a scheme is often appropriate in practice , but it is also possible to think of cases where @xmath109 could depend on @xmath47 , for instance in a stopping time fashion .",
    "it is also possible to retain unbiasedness while replacing @xmath340 in the estimator by a conditional expectation .",
    "we consider below a scheme which accomodates both of these generalisations , while retaining unbiasedness .",
    "[ cond : stopping - dependence ] suppose @xmath341 , @xmath342 and @xmath1 are integrable random variables , @xmath343 and @xmath344 , @xmath345 are @xmath16-algebras , and @xmath109 are non - negative integer - valued random variables , such that @xmath346 almost surely for all @xmath50 .",
    "[ cond : stopping - dependence2 ] suppose condition [ cond : stopping - dependence ] holds , and for all @xmath347 :    a.   [ item : general - consistency ] @xmath348 almost surely .",
    "b.   [ item : general - bound ] there exists random variables @xmath349 such that @xmath350 and with @xmath351 . c.   [ item : general - summability ] @xmath352 and @xmath353 almost surely .",
    "[ thm : dependent - randomisation ] assume condition [ cond : stopping - dependence ] holds and denote @xmath354 and @xmath355 whenever well - defined .",
    "a.   [ item : stopping - gen ] if condition [ cond : stopping - dependence2 ] and hold , then @xmath356 .",
    "if also condition [ cond : stopping - dependence2 ] holds , then @xmath107 .",
    "b.   [ item : stopping - indep ] suppose the assumptions of theorem [ thm : general ] hold and @xmath99 are independent of @xmath345 . if @xmath357        < \\infty       \\end{aligned}\\ ] ] for all @xmath358 and @xmath104 for some subsequence @xmath103 , then @xmath107 and @xmath359 .",
    "proof of theorem [ thm : dependent - randomisation ] is given in appendix [ app : proof - dependent - randomisation ] .",
    "theorem [ thm : dependent - randomisation ] is a generalisation of theorem [ thm : general ] , and leads to new , potentially interesting estimators .",
    "for instance , let @xmath193 where @xmath360 are positive random integers independent of @xmath361 . if we take @xmath362 for all @xmath363 , then @xmath364 .",
    "this can be viewed as a residual sampling scheme applied with the ( random ) probability distribution @xmath365 .",
    "analogously , the residual sampling scheme may be viewed through such conditioning , where @xmath366 are deterministic .",
    "it is unclear whether theorem [ thm : dependent - randomisation ] allows estimators that have practical appeal , such as greater efficiency compared with the estimators introduced in section [ sec : new ] .",
    "theorem [ thm : dependent - randomisation ] allows @xmath367 to depend on @xmath99 in a stopping time fashion , analogous to wald s identity .",
    "suppose condition [ cond : stopping - dependence ] holds and @xmath99 are as in theorem [ thm : general ] .",
    "a.   if @xmath92 are stopping times with respect to @xmath47 , that is , @xmath368 and @xmath369 for all @xmath347 , then taking @xmath370 trivial , condition [ cond : stopping - dependence2 ] holds because @xmath371      = { \\mathbb{e}}\\delta_i { \\mathbb{p } } ( j\\le n_i \\mid { \\mathcal{f}}_{i-1}),\\ ] ] and , similarly condition [ cond : stopping - dependence2 ] with @xmath372 . b.   if condition [ cond : single - term ] holds for some probability distribution @xmath67 , then @xmath373 .",
    "the stopping time formulation may prove theoretically useful , and suggests a possibility to be used in conjunction with stopping rules developed in the mlmc context ( e.g. * ? ? ?",
    "however , the practical relevance of such an approach may be limited , because the expectation of a stopping time is often unavailable .",
    "it appears also difficult to derive useful explicit variance expressions when @xmath374 depend on @xmath47 .",
    "the performance of the estimators was studied with four sde examples , where expectations of final - value functionals are estimated .",
    "three of the models were the same as those used by @xcite .",
    "the first is a _ geometric brownian motion _ ( gbm ) @xmath375,\\qquad x_0\\equiv 1,\\ ] ] with @xmath376 and @xmath377 , where @xmath12 is the standard brownian motion .",
    "the target functional is the final value european option @xmath378 , with approximate expected value @xmath379 @xcite .",
    "the second model was a _ cox - ingersoll - ross _",
    "( cir ) model @xmath380,\\qquad x_0\\equiv 0.04,\\ ] ] with @xmath381 , @xmath382 and @xmath383 , and with final value european option @xmath384 . according to extensive simulations ,",
    "the expected value was found to be approximately @xmath385 .",
    "the third model is a bivariate _",
    "model @xmath386 ,      \\qquad      \\begin{array}{rl }          s_0\\equiv 1 \\\\ v_0\\equiv 0 ,      \\end{array}\\ ] ] with @xmath376 , @xmath381 , @xmath382 , @xmath383 , and where @xmath387 are coordinates of a correlated brownian motion with coefficient @xmath388 .",
    "the functional @xmath389 , with expected value @xmath390 @xcite .",
    "the fourth model is an artificial model termed _ modified gbm _ , which has the same volatility term as gbm but a time - dependent drift : @xmath391 ,      \\qquad",
    "x_0 \\equiv 1,\\ ] ] and @xmath392 .",
    "the target functional is the mean , which was found to have an approximate expected value @xmath393 .",
    "this last model is intended to have bigger @xmath394 than the previous examples , highlighting the differences between the new estimators over averages of independent estimators bigger ; see remark [ rem : eff - diff ] .",
    "the implementation developed for the tests is available at https://bitbucket.org/mvihola/unbiased-mlmc . in",
    "all but the heston model , the standard milstein scheme described in section [ sec : mlmc ] was employed .",
    "the antithetic truncated milstein scheme proposed by @xcite was applied with the heston model .",
    "we considered single term , independent sum and coupled sum estimators . with",
    "the first two , the distributions @xmath67 were set to approximately optimal in each model as discussed in section [ sec : cost ] : @xmath327 were set empirically to minimise variance , and the tail probability @xmath395 was calculated from empirical data .",
    "the tail distribution @xmath332 for @xmath164 was geometric , @xmath396 , with the parameter @xmath397 set in all cases to @xmath398 .",
    "this provided a very good fit with the empirical optimal distribution @xmath399 with @xmath400 in case of gbm and @xmath401 with all other examples ; see also theoretical results on the milstein scheme @xcite and the antithetic truncated milstein @xcite . in the coupled sum estimator ,",
    "the differences @xmath402 were all based on a single brownian trajectory .",
    "the optimisation was based on estimators of @xmath403 , where @xmath61 corresponded to discretisation with a mesh of @xmath404 .",
    "an optimal subsequences were found using the dynamic programming algorithm of @xcite .",
    "figure [ fig : res - all ] shows results based on averages of @xmath405 independent runs of each algorithm in each model .",
    "replications of estimators with @xmath406 , @xmath407 , @xmath405 and @xmath408 . ]",
    "the graphs in figure [ fig : res - all ] show the estimated inverse relative efficiency ( ire ) of the methods : the average cost multiplied with average square deviation from the ground truth values as given above .",
    "according to the theory , this quantity is constant with independent averages , and with residual and stratified sampling , it converges to the same limit as the corresponding mlmc estimator .",
    "table [ tab : res ] shows the corresponding numerical values with @xmath409 .    .estimated ires in the experiments with @xmath409 .",
    "[ cols= \" < , < , < , < , < , < , < , < , < , < , < , < , < \" , ]     [ tab : res ]    the experiments appear to align well with the theoretical findings . in all but the last example ,",
    "the mlmc estimator admitted the best ire with small @xmath89 , and the new estimators appear to admit performance in between the independent and the mlmc case .",
    "the performance of the new schemes come close to mlmc performance as @xmath89 increases , as verified by the differences with @xmath409 reported in table [ tab : res ] , which are all relatively small , except for the sum estimators in the modified gbm example indicating still some discrepancy .",
    "note that the mlmc implemented in the experiments is the ` idealised ' version involving same pre - determined allocation strategy as unbiased estimators ( description [ def : mlmc ] ) .",
    "this explains the discrepancy between the mlmc ire reported here and by @xcite , who employ the original version of the mlmc @xcite .",
    "the findings of @xcite suggest that unbiased estimators applied with stopping rules may sometimes improve upon the original mlmc .",
    "the differences in performance of single term and independent sum estimators with gbm , cir and heston examples are all relatively small , as @xmath89 increases , the ires of the new single term and sum estimators become negligible , as anticipated by the theory .",
    "the coupled sum estimator appears to admit greater efficiency with cir and heston examples , but the differences between independent average estimators and the new estimators are small .",
    "the modified gbm example demonstrates that the new estimators can be significantly more efficient .",
    "the numerical values shown in table [ tab : res ] indicate a 1316 fold increase in terms of relative efficiency with the new single term estimators and similar performance with mlmc .",
    "the increase is 1760 fold with sum estimators . in both cases ,",
    "the stratified and systematic sampling estimators appear to perform slightly better than the residual sampling estimator .",
    "this paper presented a general framework for unbiased estimation , which admits previous debiasing schemes as special cases , and accomodates new , lower variance unbiased estimators .",
    "the proposed stratified sampling and residual sampling schemes are promising classes of estimators , as they enjoy good theoretical behaviour  they can not only improve on averages of independent estimates ( proposition [ prop : consistency ] and proposition [ prop : cost - new ] ) , but also can have a significant gain in efficiency as illustrated in the experiments .",
    "indeed , the stratified and residual sampling estimators can be made arbitrarily close to mlmc in efficiency under general assumptions ( theorem [ thm : residual - eff ] and proposition [ prop : mlmc - expected - cost ] ) , highlighting the close connection between the mlmc and the debiasing schemes , and showing that unbiasedness may be achieved with virtually no sacrifice on efficiency .",
    "unbiasedness is an important quality of estimators , when employed as part of stochastic optimisation algorithms @xcite , or in a ` compound sampling ' context @xcite .",
    "it also enables rigorous stopping rules , which can lead to benefits over mlmc stopping rules @xcite .",
    "while the theory presented in this paper does not give guarantees on the limiting efficiency of the systematic sampling , it is expected to behave often similar to stratified and residual sampling schemes , as illustrated by the experiments . however , as stratified and residual sampling enjoy good theoretical properties , and because systematic sampling appears to perform comparatively in practice , stratified or residual sampling are recommended as safer alternatives .",
    "the empirical evidence from the numerical experiments in section [ sec : numerical ] suggests that stratified sampling might sometimes perform slightly better than residual sampling .",
    "this , together with the straightforward implementation of stratified sampling ( algorithm [ alg : stratified ] ) , makes it appealing for practical purposes .",
    "averages of independent realisations of the single term and the independent sum estimators may have different efficiencies in general . in case of residual and stratified sampling , the optimally tuned estimators often coincide in asymptotic efficiency . in general",
    ", the single term estimator always dominates the independent sum estimator in terms asymptotic efficiency , rendering the single term estimator preferable over the independent sum estimator .",
    "based on the experiments , the optimally tuned coupled sum estimator may sometimes lead into significant performance gains .",
    "this suggests also that dependent level estimators might be worth considering in the mlmc context , where independent level estimators are currently widely employed .    despite the potential performance gain of the coupled estimators , the single term estimator ( resp .",
    "standard independent level mlmc ) may still often be preferable , because of its simplicity of implementation , and because of simpler and more robust criterion for tuning @xmath67 ( resp .",
    "@xmath410 ) .",
    "the simplicity of the optimisation criterion suggests , as noted by @xcite , an algorithm which automatically tunes the distribution @xmath67 during repeated simulation of @xmath194 or @xmath212 .",
    "@xcite suggest methods for finding optimal discretisation hierarchies in the mlmc context , which could also be explored in the debiasing context .",
    "the mlmc literature provides many other potential further research topics .",
    "it is yet unclear how well the unbiased estimators can compete with mlmc in scenarios with slower than canonical rate of convergence .",
    "this is touched by @xcite , but quantifying the effect of the new estimators needs further arguments .",
    "it is possible to employ essentially all techniques developed in the context of mlmc with debiasing .",
    "these include , for instance , quasi - monte carlo @xcite , adaptive time steps @xcite or adaptive importance sampling scheme based on a drifted brownian motion @xcite .",
    "this research was supported by the academy of finland ( grants 274740 and 284513 ) , and computational resources were provided by csc , the it center for science , finland .",
    "the author wishes to thank christel geiss , stefan geiss , peter glynn , chang - han rhee , raul tempone and anni toivola for useful discussions and remarks , and the associate editor and the reviewers of _ operations research _ for helpful comments , and for the suggestion to consider dependent randomisation , which led to the developments in section [ sec : stopping ] .",
    "42 [ 1]#1 [ 1]`#1 ` urlstyle [ 1]doi : # 1    s.  agapiou , g.  o. roberts , and s.  j. vollmer .",
    "unbiased monte carlo : posterior estimation for intractable / infinite - dimensional models .",
    "preprint arxiv:1411.7713 , 2014 .",
    "m.  b. alaya and a.  kebaier .",
    "central limit theorem for the multilevel monte carlo euler method .",
    "_ , 250 ( 1):0 211234 , 2015 .    a.  beskos and g.  o. roberts",
    ". exact simulation of diffusions .",
    "probab . _ , 150 ( 4):0 24222444 , 2005 .    v.  s. borkar .",
    "_ stochastic approximation : a dynamical systems viewpoint_. cambridge university press , 2008 .",
    "isbn 978 - 0 - 521 - 51592 - 4 .",
    "n.  collier , a .-",
    "haji - ali , f.  nobile , e.  von schwerin , and r.  tempone .",
    "a continuation multilevel monte carlo algorithm .",
    "_ bit _ , 550 ( 2):0 399432 , 2015 .",
    "b.  delyon , m.  lavielle , and e.  moulines .",
    "convergence of a stochastic approximation version of the em algorithm .",
    "_ , 270 ( 1):0 94128 , 1999 .    s.  dereich and t.  mueller - gronbach .",
    "general multilevel adaptations for stochastic approximation algorithms .",
    "preprint arxiv:1506.05482 , 2015 .",
    "j.  dick , f.  y. kuo , and i.  h. sloan .",
    "high - dimensional integration : the quasi - monte carlo way .",
    "_ acta numer .",
    "_ , 22:0 133288 , 2013 .",
    "r.  douc , o.  capp , and e.  moulines .",
    "comparison of resampling schemes for particle filtering . in _ proc .",
    "image and signal processing and analysis , 2005 .",
    "_ , pages 6469 , 2005 .",
    "w.  feller .",
    "a limit theoerm for random variables with infinite moments .",
    "_ amer . j. math .",
    "_ , 680 ( 2):0 257262 , 1946 .    m.  b. giles .",
    "multilevel monte carlo path simulation .",
    "_ , 560 ( 3):0 607617 , 2008 .",
    "m.  b. giles .",
    "multilevel monte carlo methods .",
    "_ acta numer .",
    "_ , 24:0 259328 , 2015 .",
    "m.  b. giles and l.  szpruch .",
    "antithetic multilevel monte carlo estimation for multi - dimensional sdes without lvy area simulation",
    "_ , 240 ( 4):0 15851620 , 2014",
    ".    m.  b. giles and b.  j. waterhouse .",
    "multilevel quasi - monte carlo path simulation . in _",
    "advanced financial modelling _ , radon series on computational and applied mathematics , pages 165181 .",
    "de gruyter , 2009 .",
    "p.  glasserman .",
    "_ monte carlo methods in financial engineering _ , volume  53 .",
    "springer , 2003 .",
    "p.  glynn .",
    "randomized estimators for time integrals . technical report , dtic document , 1983 .    p.  w. glynn and c .- h .",
    "exact estimation for markov chain equilibrium expectations",
    ". _ j. appl .",
    "_ , 51a:0 377389 , 2014 .",
    "p.  w. glynn and w.  whitt .",
    "the asymptotic efficiency of simulation estimators .",
    "_ , 400 ( 3):0 505520 , 1992 .",
    "p.  w. glynn and w.  whitt .",
    "the asymptotic validity of sequential stopping rules for stochastic simulations .",
    "_ , 20 ( 1):0 180198 , 1992 .    a .-",
    "haji - ali , f.  nobile , e.  von schwerin , and r.  tempone .",
    "optimization of mesh hierarchies in multilevel monte carlo samplers .",
    "partial differ . equ .",
    "_ , 40 ( 1):0 76112 , 2016",
    ".    m.  h. hansen , w.  n. hurwitz , and w.  g. madow .",
    "_ sample survey methods and theory _ , volume 1 methods and applications .",
    "wiley , 1953 .",
    "s.  heinrich .",
    "multilevel monte carlo methods . in _",
    "large - scale scientific computing _ , pages 5867 .",
    "springer , 2001 .",
    "h.  hoel , e.  von  schwerin , a.  szepessy , and r.  tempone .",
    "adaptive multilevel monte carlo simulation . in _ numerical analysis of multiscale computations _",
    ", pages 217234 .",
    "springer , 2012 .",
    "m.  l. huber .",
    "_ perfect simulation_. chapman hall / crc , 2015 .",
    "p.  e. jacob and a.  h. thiery . on nonnegative unbiased estimators .",
    "_ , 430 ( 2):0 769784 , 2015 .    a.  jentzen , p.  e. kloeden , and a.  neuenkirch .",
    "pathwise approximation of stochastic differential equations on domains : higher order convergence rates without global lipschitz coefficients .",
    "_ , 1120 ( 1):0 4164 , 2009 .    c.  kahl and p.  jckel .",
    "fast stong approximation monte carlo schemes for stochastic volatility models .",
    ", 60 ( 6):0 513536 , 2006 .",
    "a.  kebaier and j.  lelong .",
    "coupling importance sampling and multilevel monte carlo using sample average approximation .",
    "preprint arxiv:1510.03590 , 2015 .",
    "p.  e. kloeden and e.  platen .",
    "_ numerical solution of stochastic differential equations_. springer , 1992 .",
    "h.  j. kushner and g.  g. yin .",
    "_ stochastic approximation and recursive algorithms and applications_. number  35 in applications of mathematics : stochastic modelling and applied probability .",
    "springer - verlag , @xmath411 edition , 2003 .",
    "isbn 0 - 387 - 00894 - 2 .",
    "lyne , m.  girolami , y.  atchade , h.  strathmann , and d.  simpson .",
    "on russian roulette estimates for bayesian inference with doubly - intractable likelihoods . _",
    "_ , 300 ( 4):0 443467 , 2015 .",
    "d.  mcleish . a general method for debiasing a monte carlo estimator .",
    "_ monte carlo methods appl .",
    "_ , 170 ( 4):0 301315 , 2011 .",
    "j.  g. propp and d.  b. wilson .",
    "exact sampling with coupled markov chains and applications to statistical mechanics .",
    "_ random structures algorithms _ , 90 ( 1 - 2):0 223252 , 1996 .",
    "rhee and p.  w. glynn . a new approach to unbiased estimation for sde s . in _ proceedings of the winter simulation conference _",
    ", page  17 .",
    "winter simulation conference , 2012 .",
    "rhee and p.  w. glynn .",
    "unbiased estimation with square root convergence for sde models .",
    "res . _ , 630 ( 5):0 10261043 , 2015 .",
    "h.  robbins and s.  monro . a stochastic approximation method . _",
    "the annals of mathematical statistics _ , 22:0 400407 , 1951 .",
    "t.  rychlik . unbiased nonparametric estimation of the derivative of the mean .",
    "_ statist .",
    "_ , 100 ( 4):0 329333 , 1990 .",
    "t.  rychlik . a class of unbiased kernel estimates of a probability density function .",
    "_ appl . math .",
    "( warsaw ) _ , 220 ( 4):0 485497 , 1995 .",
    "h.  strathmann , d.  sejdinovic , and m.  girolami .",
    "unbiased bayes for big data : paths of partial posteriors .",
    "preprint arxiv:1501.03326 , 2015 .",
    "m.  vihola .",
    "unbiased estimators and multilevel monte carlo .",
    "preprint arxiv:1512.01022v2 , 2015 .",
    "m.  vihola , j.  helske , and j.  franks .",
    "importance sampling type correction of markov chain monte carlo and exact approximations .",
    "preprint arxiv:1609.02541 , 2016 .",
    "c.  walter . point process - based monte carlo estimation .",
    "_ statist .",
    "the following lemma is due to an argument by @xcite .",
    "[ lem : subsequence ] suppose @xmath412 as @xmath413 .",
    "then , there exists strictly increasing @xmath149 such that for all @xmath150 , @xmath414    let @xmath415 . if @xmath416 for infinitely many @xmath6 , choose them as @xmath103",
    "otherwise , let @xmath417 be such that @xmath418 for all @xmath419 , and define recursively @xmath420 .",
    "now , for any @xmath421 and any @xmath422 , @xmath423",
    "define @xmath424 and for @xmath425 @xmath426 then by dominated convergence @xmath427 = \\sum_{i=1}^m { \\mathbb{e}}\\delta_i      = { \\mathbb{e}}y_m,\\ ] ] because @xmath428 .    for @xmath429 , by dominated convergence , @xmath430 \\label{eq : single - level - diffs }",
    "\\\\ & = \\frac{1}{{\\mathbb{e}}n_i { \\mathbb{e}}n_k } { \\mathbb{e}}\\bigg[\\sum_{j,\\ell=1}^\\infty \\delta_i^{(j)}\\delta_k^{(\\ell ) } { \\mathbbm{i}\\left\\{j\\le n_i\\right\\}}{\\mathbbm{i}\\left\\{\\ell\\le n_k\\right\\ } }   \\bigg]\\nonumber\\\\ & = \\frac{1}{{\\mathbb{e}}n_i { \\mathbb{e}}n_k } \\bigg({\\mathbb{e}}(\\delta_i \\delta_k ) { \\mathbb{e}}\\bigg[\\sum_{j=1}^\\infty { \\mathbbm{i}\\left\\{j\\le n_i\\right\\}}{\\mathbbm{i}\\left\\{j\\le n_k\\right\\}}\\bigg ]   \\nonumber\\\\ & \\phantom{=\\frac{1}{{\\mathbb{e}}n_i { \\mathbb{e}}n_k}\\bigg(}+ { \\mathbb{e}}\\delta_i { \\mathbb{e}}\\delta_k   { \\mathbb{e}}\\bigg[\\sum_{\\stackrel{j,\\ell=1}{j\\neq \\ell}}^\\infty { \\mathbbm{i}\\left\\{j\\le    n_i\\right\\}}{\\mathbbm{i}\\left\\{\\ell\\le n_k\\right\\ } } \\bigg]\\bigg ) \\nonumber\\\\ & = \\frac{\\big({\\mathbb{e}}(\\delta_i\\delta_k ) -    { \\mathbb{e}}\\delta_i{\\mathbb{e}}\\delta_k\\big){\\mathbb{e}}(n_i\\wedge n_k ) + { \\mathbb{e}}\\delta_i{\\mathbb{e}}\\delta_k{\\mathbb{e}}[n_i n_k]}{{\\mathbb{e}}n_i { \\mathbb{e}}n_k},\\nonumber\\end{aligned}\\ ] ] because @xmath431 .    we deduce that for @xmath432 , @xmath433 = v_{\\ell , m } + ( { \\mathbb{e}}y_m - { \\mathbb{e}}y_\\ell)^2 .",
    "\\label{eq : full - level - diffs}\\end{aligned}\\ ] ] therefore , by assumption @xmath434 is cauchy in @xmath435 . because @xmath436 a.s .",
    ", we have @xmath437 a.s . , and therefore @xmath438 in @xmath435 .",
    "we deduce that @xmath439 .",
    "similarly we find the expression @xmath440 $ ] .",
    "consider , and let @xmath92 correspond to @xmath212 .",
    "we have for any @xmath139 @xmath463 so @xmath464 . in case of @xmath194 , it is not difficult to check that the number of strata @xmath465 partially overlapping @xmath6 , that is , such that @xmath466 but @xmath467 , is at most two , so @xmath468 , and consequently @xmath469 .    denote @xmath470 for @xmath425 , where @xmath109 correspond either to the residual sampling or stratified sampling scheme .",
    "we have ( cf .",
    "example [ ex : single ] ) @xmath471 because @xmath472 as @xmath299 , we deduce that @xmath473 .    on the other hand , @xmath474 is no greater than the corresponding variance of single term estimators , and therefore for @xmath475 , @xmath476 we deduce that for any @xmath477 , due to independence , @xmath478 both terms on the right can be made arbitrarily small by choosing @xmath479 large enough and letting @xmath299 .",
    "an easy calculation shows that @xmath480 where the first term converges to @xmath481 as @xmath299 . for the latter ,",
    "observe that @xmath482 and @xmath483 as @xmath299 , so by dominated convergence the last sum in vanishes .",
    "let us then turn into .",
    "let us calculate first @xmath484 and because @xmath485 we obtain @xmath486 note that @xmath487 , so @xmath488 the first sum in therefore converges to @xmath489 as @xmath299 and the latter sum in converges to zero as above .",
    "consider then @xmath216 .",
    "as above , @xmath490 . in case of @xmath195 ,",
    "at most one stratum @xmath491 contains some @xmath492 and @xmath6 , so @xmath493 .",
    "therefore , @xmath494 for both schemes , and the variance of @xmath495 admits the bound @xmath496 the second term on the left equals @xmath497 , and clearly @xmath498 as @xmath299 .",
    "now , take @xmath103 from lemma [ lem : subsequence ] , then stratification implies that for @xmath150 , @xmath499 we conclude as above by writing for any @xmath500 , @xmath501 because @xmath502 , all terms on the right can be made arbitrarily small by by choosing @xmath503 large enough and letting @xmath299 .",
    "consider and notice that @xmath504 denote @xmath505 , then @xmath506 and by dominated convergence a similar calculation yields @xmath507 which leads to @xmath356 .",
    "for the latter claim , note that only finitely many of @xmath508 are non - zero , so @xmath261 is well - defined , and the result follows by the dominated convergence theorem .    the statement is a generalisation of theorem [ thm : general ] , and the proof follows similarly . indeed ,",
    "condition [ cond : stopping - dependence2 ] and hold by independence , with @xmath372 , so part implies @xmath356 .",
    "denoting @xmath509 , a straightforward calculation similar to yields @xmath510      = \\frac{{\\mathrm{cov}}(\\delta_i,\\delta_k){\\mathbb{e}}(n_i\\wedge n_k\\mid { \\mathcal{f}}_{i , k } )        + { \\mathbb{e}}\\delta_i{\\mathbb{e}}\\delta_k { \\mathbb{e}}(n_i n_k\\mid { \\mathcal{f}}_{i , k})}{{\\mathbb{e}}(n_i\\mid        { \\mathcal{f}}_{i-1}){\\mathbb{e}}(n_k\\mid { \\mathcal{f}}_{k-1})},\\end{aligned}\\ ] ] and as in , @xmath511 .",
    "let us consider first @xmath194 , which is an average of independent random variables @xmath441 which follow , respectively , the conditional distribution of the single term estimator @xmath90 , given the uniform random variable @xmath442 generating @xmath187 takes value in @xmath443 .",
    "the desired variance bound follows from lemma [ lem : strat ] applied with @xmath444 , @xmath445 , @xmath446 and @xmath447 .",
    "the sum estimator @xmath195 is similarly stratified version of the average of @xmath89 independent sum estimators .",
    "the systematic sampling estimators @xmath201 and @xmath202 , are averages as above , but with the difference that the uniformly distributed random variables on @xmath448 which determine @xmath449 are not independent .",
    "we may apply lemma [ lem : strat ] which gives the ( pessimistic ) upper bound on the variance .",
    "the residual sampling @xmath212 and @xmath216 may also be seen as stratification of @xmath90 and @xmath137 , but instead of considering @xmath450 , we now let @xmath451 , where @xmath452 it is direct to verify that @xmath453 .",
    "let @xmath454 be independent random variables with conditional distribution of @xmath90 ( resp .",
    "@xmath137 ) given @xmath455 , respectively , and let @xmath456 be , similarly , independent conditional on @xmath457 .",
    "we may apply lemma [ lem : strat ] with @xmath458 , @xmath459 , @xmath460 , @xmath461 and @xmath462 ."
  ],
  "abstract_text": [
    "<S> multilevel monte carlo ( mlmc ) and unbiased estimators recently proposed by mcleish ( _ monte carlo methods appl . _ , 2011 ) and rhee and glynn ( _ oper . </S>",
    "<S> res . </S>",
    "<S> _ , 2015 ) are closely related . </S>",
    "<S> this connection is elaborated by presenting a new general class of unbiased estimators , which admits previous debiasing schemes as special cases . </S>",
    "<S> new lower variance estimators are proposed , which are stratified versions of earlier unbiased schemes . under general conditions , </S>",
    "<S> essentially when mlmc admits the canonical square root monte carlo error rate , the proposed new schemes are shown to be asymptotically as efficient as mlmc , both in terms of variance and cost . </S>",
    "<S> the experiments demonstrate that the variance reduction provided by the new schemes can be substantial . </S>"
  ]
}