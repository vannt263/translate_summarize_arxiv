{
  "article_text": [
    "uncertainty quantification of complex systems mandates stochastic computations of a multivariate output function @xmath4 that depends on @xmath5 , a high - dimensional random input with a positive integer @xmath1 . for practical applications , encountering hundreds of variables or more",
    "is not uncommon , where a function of interest , defined algorithmically via numerical solution of algebraic , differential , or integral equations , is all too often expensive to evaluate .",
    "therefore , there is a need to develop low - dimensional approximations of @xmath4 by seeking to exploit the hidden structure potentially lurking underneath a function decomposition .",
    "the dimensional decomposition of @xmath6 can be viewed as a finite , hierarchical expansion in terms of its input variables with increasing dimensions , where @xmath7 is a subset with the complementary set @xmath8 and cardinality @xmath9 , and @xmath10 is a @xmath11-variate component function describing a constant or the cooperative influence of @xmath12 , @xmath13 , a subvector of @xmath14 , on @xmath4 when @xmath15 or @xmath16 .",
    "the summation in ( [ 1 ] ) comprises @xmath17 terms , with each term depending on a group of variables indexed by a particular subset of @xmath18 , including the empty set @xmath19 .",
    "this decomposition , first presented by hoeffding @xcite in relation to his seminal work on @xmath20-statistics , has been studied by many other researchers @xcite : sobol @xcite used it for quadrature and analysis of variance ( anova ) @xcite ; efron and stein @xcite applied it to prove their famous lemma on jackknife variances ; owen @xcite presented a continuous space version of the nested anova ; hickernell @xcite developed a reproducing kernel hilbert space version ; and rabitz and alis @xcite made further refinements , referring to it as high - dimensional model representation ( hdmr ) .",
    "more recently , the author s group formulated this decomposition from the perspective of taylor series expansion , solving a number of stochastic - mechanics problems @xcite .    in a practical setting ,",
    "the multivariate function @xmath4 , fortunately , has an effective dimension @xcite much lower than @xmath1 , meaning that @xmath4 can be effectively approximated by a sum of lower - dimensional component functions @xmath10 , @xmath21 .",
    "given an integer @xmath22 , the truncated dimensional decomposition @xmath23 then represents a general @xmath0-variate approximation of @xmath24 , which for @xmath25 includes cooperative effects of at most @xmath0 input variables @xmath26 , @xmath27 , on @xmath4 .",
    "however , for ( [ 2 ] ) to be useful , one must ask the fundamental question : what is the approximation error committed by @xmath28 for a given @xmath29 ?",
    "the answer to this question , however , is neither simple nor unique , because there are multiple ways to construct the component functions @xmath10 , @xmath30 , spawning approximations of distinct qualities .",
    "indeed , there exist two important variants of the decomposition : ( 1 ) referential dimensional decomposition ( rdd ) and ( 2 ) anova dimensional decomposition ( add ) , both representing sums of lower - dimensional component functions of @xmath24 . while add has desirable orthogonal properties , the anova component functions are difficult to compute due to the high - dimensional integrals involved .",
    "in contrast , the rdd lacks orthogonal features with respect to the probability measure of @xmath14 , but its component functions are much easier to obtain . for rdd , an additional question arises regarding the reference point , which , if improperly selected , can mar the approximation .",
    "existing error analysis , limited to the univariate truncation of @xmath24 , reveals that the expected error from the rdd approximation is at least four times larger than the error from the add approximation @xcite .",
    "although useful to some extent , such result alone is not adequate when evaluating multivariate functions requiring higher - variate interactions of input @xcite .",
    "no error estimates exist yet in the current literature even for a bivariate approximation .",
    "therefore , a more general error analysis pertaining to a general @xmath0-variate approximation of a multivariate function should provide much - needed insights into the mathematical underpinnings of dimensional decomposition .",
    "the purpose of this paper is twofold .",
    "firstly , a brief exposition of add and rdd is given in section 2 , including clarifications of parallel developments and synonyms used by various researchers .",
    "the error analysis pertaining to the add approximation is described in section 3 .",
    "secondly , a direct form of the rdd approximation , previously developed by the author s group , is tapped for providing a vital link to subsequent error analysis .",
    "section 4 introduces new formulae for the lower and upper bounds of the expected errors from the bivariate and general @xmath0-variate rdd approximations .",
    "these error bounds , so far available only for the univariate approximation , are used to clarify why add approximations are exceedingly more precise than rdd approximations .",
    "there are seven new results stated or proved in this paper : proposition [ p4 ] , theorems [ t6 ] , [ t7 ] , and corollaries [ c2 ] , [ c3 ] , [ c4a ] , [ c4 ] . proofs of other results can be obtained from the references cited , including a longer version of this paper ( http://www.engineering.uiowa.edu/~rahman/moc_longpaper.pdf ) .",
    "conclusions are drawn in section 5 .",
    "let @xmath31 , @xmath32 , @xmath33 , @xmath34 , and @xmath35 represent the sets of positive integer ( natural ) , non - negative integer , integer , real , and non - negative real numbers , respectively . for @xmath36 , denote by @xmath37 the @xmath38-dimensional euclidean space and by @xmath39 the @xmath38-dimensional multi - index space .",
    "these standard notations will be used throughout the paper .",
    "let @xmath40 be a complete probability space , where @xmath41 is a sample space , @xmath42 is a @xmath43-field on @xmath41 , and @xmath44 $ ] is a probability measure . with @xmath45 representing the borel @xmath43-field on @xmath46 , consider an @xmath46-valued independent random vector @xmath47 , which describes the statistical uncertainties in all system and input parameters of a given stochastic problem .",
    "the probability law of @xmath14 is completely defined by its joint probability density function @xmath48 .",
    "assuming independent coordinates of @xmath14 , its joint probability density @xmath49 can be expressed by a product of marginal probability density functions @xmath50 of @xmath51 , @xmath52 , defined on the probability triple @xmath53 with a bounded or an unbounded support on @xmath34 .    consider a non - negative , multiplicative , otherwise general , weight function @xmath54 , satisfying @xmath55 with the marginal weight functions @xmath56 , @xmath52 . for @xmath7 ,",
    "let @xmath57 define the joint weight function associated with @xmath58 . without loss of generality , assume that the weight functions have been normalized to integrate to @xmath59 let @xmath60 ) , a real - valued , measurable transformation on @xmath61 , define a stochastic response of interest and @xmath62 represent a hilbert space of square - integrable functions @xmath4 with respect to the induced generic measure @xmath63 supported on @xmath46 . the representation in ( [ 1 ] ) is called dimensional decomposition if the component functions @xmath10 , @xmath7 , are uniquely determined from the requirements @xmath64 indeed , integrating ( [ 1 ] ) with respect to the measure @xmath65 , that is , over all variables except @xmath66 , and using ( [ 3 ] ) yields a recursive form    @xmath67    of the decomposition with @xmath68 denoting an @xmath1-dimensional vector whose @xmath69th component is @xmath51 if @xmath70 and @xmath71 if @xmath72 when @xmath73 , the sum in the last line of ( [ 3b ] ) vanishes , resulting in the expression of the constant function @xmath74 in the second line . when @xmath75 , the integration in the last line of ( [ 3b ] ) is on the empty set , reproducing ( [ 1 ] ) and hence finding the last function @xmath76 .",
    "indeed , all component functions of @xmath4 can be obtained by interpreting literally the last line of ( [ 3b ] ) . on inversion , ( [ 3b ] )",
    "results in @xmath77 providing an explicit form of the same decomposition .",
    "it is important to emphasize that the measure involved in expressing the dimensional decomposition in ( [ 3b ] ) or ( [ 3c ] ) may or may not represent the probability measure of @xmath14 . indeed",
    ", different measures will create distinct yet exact representations of @xmath4 , all exhibiting the same structure of ( [ 1 ] ) .",
    "there exist two important variants of dimensional decomposition , described as follows .",
    "the add is generated by selecting the probability measure of @xmath14 as the generic measure , that is , @xmath63=@xmath78 in ( [ 3b ] ) or ( [ 3c ] ) , yielding the recursive form    @xmath79    [ 4 ]    that is commonly found in the anova literature @xcite , although , for the uniform probability measure @xmath80 .",
    "the explicit version takes the form @xmath81 where @xmath82 is the marginal probability density function of @xmath83",
    ". equations ( [ 4 ] ) and ( [ 5 ] ) can also be derived from other perspectives , including commuting projections on a linear space of real - valued functions invoked by kuo _",
    "@xcite for the uniform probability measure .    if @xmath84 is the expectation operator with respect to the measure @xmath78 , then two important properties of the add component functions , inherited from ( [ 3 ] ) , are as follows .",
    "+    the add component functions @xmath85 , @xmath86 , have zero means , i.e. , @xmath87=\\int_{\\mathbb{r}^{|u|}}y_{u , a}(\\mathbf{x}_{u})f_{u}(\\mathbf{x}_{u})d\\mathbf{x}_{u}=0.\\label{6}\\ ] ] [ p1 ]    two distinct add component functions @xmath85 and @xmath88 , where @xmath86 , @xmath89 , and @xmath90 , are orthogonal , i.e. , they satisfy the property , @xmath91=\\int_{\\mathbb{r}^{|u\\cup v|}}y_{u , a}(\\mathbf{x}_{u})y_{v , a}(\\mathbf{x}_{v})f_{u\\cup",
    "v}(\\mathbf{x}_{u\\cup v})d\\mathbf{x}_{u\\cup v}=0.\\label{7}\\ ] ] [ p2 ]    traditionally , ( [ 4 ] ) or ( [ 5 ] ) with @xmath92 , @xmath93 , following independent , standard uniform distributions , has been identified as the anova decomposition @xcite ; however , the author s recent works @xcite reveal no fundamental requirement for a specific probability measure of @xmath14 , provided that the resultant integrals in ( [ 4 ] ) or ( [ 5 ] ) exist and are finite . in this work , the add should be interpreted with respect to an arbitrary but product type probability measure for which it is always endowed with desirable orthogonal properties .",
    "however , the add component functions are difficult to ascertain , because they require calculation of high - dimensional integrals .",
    "consider a reference point @xmath94 and the associated dirac measure @xmath95 .",
    "the rdd is created when @xmath95 is chosen as the generic measure in ( [ 3b ] ) , leading to the recursive form    @xmath96    [ 8 ]    presented as cut - hdmr @xcite , anchored decomposition @xcite , and anchored - anova decomposition @xcite , with the latter two referring to the reference point as the anchor .",
    "xu and rahman introduced ( [ 8 ] ) with the aid of taylor series expansion , calling it dimension - reduction @xcite and decomposition @xcite methods for calculating statistical moments and reliability of mechanical system responses , respectively . again , these various synonyms of the same decomposition exist due to diverse perspectives employed by researchers in disparate fields .",
    "analogous to add , the rdd can also be described explicitly , for instance @xcite , @xmath97 where @xmath98 denotes an @xmath1-dimensional vector whose @xmath69th component is @xmath51 if @xmath99 and @xmath100 if @xmath101 the second argument `` @xmath102 '' introduced in @xmath103 and @xmath104 is a reminder that the rdd component functions depend on the reference point , although @xmath4 does not , as ( [ 9 ] ) is exact .",
    "an important property of the rdd component functions , also inherited from ( [ 3 ] ) , is as follows @xcite .",
    "the rdd component functions @xmath103 , @xmath86 , vanish when any of its own variables @xmath51 with @xmath70 takes on the value of @xmath100 , i.e. , @xmath105 [ p3 ]    clearly , the rdd component functions lack orthogonal features with respect to the probability measure of @xmath14 , but are relatively easy to obtain as they only involve function evaluations at a chosen reference point . however , the rdd component functions can be orthogonal with respect to other inner products @xcite .      when a dimensional decomposition , whether add or rdd or others , of a multivariate function is truncated by retaining only lower - dimensional terms , the result is an approximation .",
    "however , due to the special structure of the decomposition , the approximation is endowed with an error - minimizing property , described in theorem [ t1 ] .",
    "[ @xcite ] for a multivariate function @xmath106 and @xmath29 , if @xmath107 represents an @xmath0-variate approximation , obtained by truncating at @xmath30 the dimensional decomposition of @xmath24 with respect to the generic measure @xmath63 , then its component functions @xmath10 , @xmath30 , are uniquely determined from @xmath108^{2}w(\\mathbf{x})d\\mathbf{x},\\\\ \\mathrm{subject\\ : to } & \\int_{\\mathbb{r}}y_{u}(\\mathbf{x}_{u})w_{i}(x_{i})dx_{i}=0\\;\\mathrm{for}\\ ; i\\in u.\\end{split}\\label{10c}\\ ] ] [ t1 ]    rabitz and alis @xcite proved this theorem in pages 202 - 207 of their paper .",
    "the @xmath109 error in ( [ 10c ] ) committed by a truncated dimensional decomposition is minimized , but only for a specific measure . given a measure ,",
    "no other choices of the component functions of @xmath110 will produce approximations that are better than the one derived from ( [ 10b ] ) .",
    "however , different measures will create different truncated decompositions , resulting in distinct approximation errors .",
    "therefore , selecting a measure is vitally important for determining the approximation quality of a dimensional decomposition .",
    "the @xmath0-variate add approximation @xmath111 , say , of @xmath24 , where @xmath29 , is obtained by truncating the right side of ( [ 4a ] ) at @xmath30 , yielding @xmath112 applying the expectation operator on @xmath24 and @xmath111 from ( [ 4a ] ) and ( [ 11 ] ) , respectively , and noting proposition [ p1 ] , the mean @xmath113=y_{\\emptyset , a}$ ] of the @xmath0-variate add approximation matches the exact mean @xmath114:=\\int_{\\mathbb{r}^{n}}y(\\mathbf{x})f_{\\mathbf{x}}(\\mathbf{x})d\\mathbf{x}=y_{\\emptyset , a}$ ] , regardless of @xmath0 . applying the expectation operator again , this time on @xmath115 , and recognizing proposition [ p2 ] results in splitting the variance @xmath116=\\sum_{{\\textstyle { \\emptyset\\ne u\\subseteq\\{1,\\cdots , n\\}\\atop 1\\le|u|\\le s}}}\\sigma_{u}^{2}=\\sum_{s=1}^{s}\\:\\sum_{{\\textstyle { \\emptyset\\ne u\\subseteq\\{1,\\cdots , n\\}\\atop |u|=s}}}\\sigma_{u}^{2}\\label{12}\\ ] ] of the @xmath0-variate add approximation , where @xmath117 $ ] represents the variance of the _ _",
    "zero-__mean add component function @xmath85 , @xmath118 .",
    "clearly , the approximate variance in ( [ 12 ] ) approaches the exact variance @xmath119=\\sum_{\\emptyset\\ne u\\subseteq\\{1,\\cdots , n\\}}\\sigma_{u}^{2}=\\sum_{s=1}^{n}\\:\\sum_{{\\textstyle { \\emptyset\\ne u\\subseteq\\{1,\\cdots , n\\}\\atop |u|=s}}}\\sigma_{u}^{2},\\label{13}\\ ] ] the sum of all variance terms , when @xmath120 .",
    "a normalized version @xmath121 is often called the global sensitivity index of @xmath4 for @xmath122 @xcite .",
    "define a mean - squared error @xmath123:=\\int_{\\mathbb{r}^{n}}\\left[y(\\mathbf{x})-\\hat{y}_{s , a}(\\mathbf{x})\\right]^{2}f_{\\mathbf{x}}(\\mathbf{x})d\\mathbf{x}\\label{14}\\ ] ] committed by the @xmath0-variate add approximation @xmath111 of @xmath24 . replacing @xmath4 and @xmath124 in ( [ 14 ] ) with the right sides of ( [ 4a ] ) and ( [ 11 ] ) , respectively , and then recognizing propositions [ p1 ] and [ p2 ] yields @xmath125 which completely eliminates the variance terms of @xmath126 that are associated with @xmath0- and all lower - variate contributions , an attractive property of add . by setting @xmath127 ,",
    "the error can be expressed for any truncation of add .      among all possible measures",
    ", the probability measure endows the add approximation with an error - minimizing property , explained as follows .    for a given @xmath0 , the @xmath0-variate add approximation is optimal in the mean - square sense .",
    "[ p4 ]    consider a generic @xmath0-variate approximation @xmath28 of @xmath24 other than the add approximation @xmath111 . since @xmath128 contains only higher than @xmath0-variate terms and @xmath129 contains at most @xmath0-variate terms ,",
    "@xmath130 and @xmath131 are orthogonal , satisfying @xmath132=0 $ ] .",
    "consequently , the second - moment error from an @xmath0-variate approximation @xmath133\\\\   & =   \\mathbb{e}\\left[\\left(y(\\mathbf{x})-\\hat{y}_{s , a}(\\mathbf{x})\\right)^{2}\\right]+\\mathbb{e}\\left[\\left(\\hat{y}_{s , a}(\\mathbf{x})-\\hat{y}_{s}(\\mathbf{x})\\right)^{2}\\right]\\\\   & =   e_{s , a}+\\mathbb{e}\\left[\\left(\\hat{y}_{s , a}(\\mathbf{x})-\\hat{y}_{s}(\\mathbf{x})\\right)^{2}\\right]\\ge e_{s , a},\\label{16}\\end{aligned}\\ ] ] proving the mean - square optimality of the @xmath0-variate add approximation .",
    "therefore , given a truncation , an rdd approximation , regardless of how the reference point is selected , can not be better than an add approximation for calculating variance .",
    "further details of rdd approximation errors are described in the next section .",
    "the @xmath0-variate rdd approximation @xmath134 , say , of @xmath24 , where @xmath29 , is obtained by truncating the right side of ( [ 8a ] ) at @xmath30 , yielding @xmath135 which depends on the reference point , needing the second argument `` @xmath102 '' in @xmath136 . for error analysis",
    ", however , a suitable direct form of ( [ 17 ] ) is desirable .",
    "theorem [ t2 ] supplies such a form , which was originally obtained by xu and rahman @xcite using the taylor series expansion .",
    "the same form was reported later by kuo _",
    "et al_. @xcite .",
    "let @xmath137 , @xmath138 , be a @xmath38-dimensional multi - index with each component representing a non - negative integer .",
    "the multi - index , used in theorem [ t2 ] , obeys the following standard notations : ( 1 ) @xmath139 ; ( 2 ) @xmath140 ; ( 3 ) @xmath141 ; ( 4 ) @xmath142 , @xmath143 .",
    "[ multivariate function theorem @xcite ] for a differentiable multivariate function @xmath106 and @xmath29 , if @xmath144 represents an @xmath0-variate rdd approximation of @xmath24 , then @xmath134 consists of all terms of the taylor series expansion of @xmath24 at @xmath102 that have less than or equal to @xmath0 variables , i.e. , @xmath145 where @xmath146 [ t2 ]    xu and rahman @xcite proved this theorem in pages 1996 - 2000 of their paper when @xmath147 without loss of generality .",
    "the stochastic method associated with the rdd approximation was simply called `` decomposition method '' @xcite .",
    "theorem [ t2 ] implies that the rdd approximation @xmath134 in ( [ 18 ] ) , when compared with the taylor series expansion of @xmath24 , yields residual error that includes only terms of dimensions @xmath148 and higher .",
    "all higher - order @xmath0- and lower - variate terms of @xmath24 are included in ( [ 18 ] ) , which should therefore generally provide a higher - order approximation of a multivariate function than the equation derived from an @xmath0-order taylor expansion .",
    "equations ( [ 17 ] ) and ( [ 18 ] ) both follow the same structure of ( [ 2 ] ) . however , due to the distinct perspectives involved , it is not obvious if these equations represent the same function @xmath136 . a lemma and a theorem recently proved by the author",
    "demonstrate that , indeed , they do @xcite .    when @xmath149 , @xmath150 , and @xmath151 , ( [ 18 ] ) degenerates to the zero - variate rdd approximation @xmath152 the univariate rdd approximation @xmath153 and the bivariate rdd approximation @xmath154 respectively .",
    "[ c0 ]    since the right side of ( [ 26 ] ) comprises only univariate functions , the interpolation or integration of @xmath155 is essentially univariate .",
    "similarly , the right side of ( [ 27 ] ) , which contains at most bivariate functions , requires at most bivariate interpolation or integration of @xmath156 .",
    "therefore , appellation of the terms `` univariate approximation '' and `` bivariate approximation '' for @xmath155 in ( [ 26 ] ) and @xmath156 in ( [ 27 ] ) , respectively , is more appropriate than referring to them as first - order and second - order approximations .",
    "following similar consideration , define another mean - squared error @xmath157 : = \\int_{\\mathbb{r}^{n}}\\left[y(\\mathbf{x})-\\hat{y}_{s , r}(\\mathbf{x};\\mathbf{c})\\right]^{2}f_{\\mathbf{x}}(\\mathbf{x})d\\mathbf{x } \\end{split}\\label{28}\\ ] ] associated with the @xmath0-variate rdd approximation @xmath134 of @xmath24 , which depends on the reference point @xmath102 .",
    "wang @xcite suggested choosing a random reference point uniformly distributed over @xmath158^{n}$ ] and then calculating the error on average .",
    "but , @xmath14 defined here may follow an arbitrary probability law with density @xmath159 ; therefore , selecting the reference point characterized by the probability density @xmath160 is more appropriate , which leads to @xmath161 & : = \\int_{\\mathbb{r}^{n}}e_{s , r}(\\mathbf{c})f_{\\mathbf{x}}(\\mathbf{c})d\\mathbf{c } \\\\ & = \\int_{\\mathbb{r}^{2n}}\\left[y(\\mathbf{x})-\\hat{y}_{s , r}(\\mathbf{x};\\mathbf{c})\\right]^{2}f_{\\mathbf{x}}(\\mathbf{x})f_{\\mathbf{x}}(\\mathbf{c})d\\mathbf{x}d\\mathbf{c } , \\end{split}\\label{29}\\ ] ] as the expected value of the rdd error . simplifying ( [ 29 ] ) in terms of the variance components of @xmath4 , as done for the add error in ( [ 15 ] ) , for arbitrary @xmath0 and @xmath1 may appear formidable . here , the _ zero_-variate ( @xmath149 ) , univariate ( @xmath162 ) , and bivariate ( @xmath163 ) approximation errors for arbitrary @xmath1 will be derived first , followed by error analysis for a general @xmath0-variate approximation . in all cases ,",
    "the derivations require using ( 1 ) the relationships ,    @xmath164    [ 30 ]    that exist between add and rdd component functions and approximations and ( 2 ) sobol s formula @xcite , @xmath165 for select choices of @xmath166 described in the following subsection .",
    "equations ( [ 30a ] ) and ( [ 30b ] ) follow from propositions [ p1 ] , [ p2 ] , and [ p3 ] and definitions of respective component functions in ( [ 4 ] ) and ( [ 8 ] ) , eventually leading to ( [ 30c ] ) .",
    "the term @xmath167 in sobol s formula represents a sum of variance terms contributed by the add component functions that belong to @xmath86 .",
    "theorems [ t4 ] , [ t5 ] , and [ t6 ] show how the expected errors from the _ zero_-variate , univariate , and bivariate rdd approximations , respectively , depend on the variance components of @xmath4 .",
    "let @xmath94 be a random vector with the joint probability density function of the form @xmath168 , where @xmath169 is the marginal probability density function of its @xmath170th coordinate .",
    "then the expected error committed by the zero - variate rdd approximation for @xmath171 is @xmath172=2\\sigma^{2},\\label{31b}\\ ] ] where @xmath173=\\mathbb{e}\\left[y^2(\\mathbf{x})\\right]-y_{\\emptyset , a}^2 $ ] is the variance of @xmath4 .",
    "[ t4 ]    setting @xmath149 in ( [ 29 ] ) and using the expression of @xmath174 from ( [ 25b ] ) , the expected error from the _ zero_-variate rdd approximation becomes @xmath175 & = \\int_{\\mathbb{r}^{2n}}\\left[y(\\mathbf{x})-y(\\mathbf{c})\\right]^{2}f_{\\mathbf{x}}(\\mathbf{x})f_{\\mathbf{x}}(\\mathbf{c})d\\mathbf{x}d\\mathbf{c}\\\\   & = \\int_{\\mathbb{r}^{n}}y^{2}(\\mathbf{x})f_{\\mathbf{x}}(\\mathbf{x})d\\mathbf{x}+\\int_{\\mathbb{r}^{n}}y^{2}(\\mathbf{c})f_{\\mathbf{x}}(\\mathbf{c})d\\mathbf{c}\\\\   & \\;\\;\\;\\ ;    -2\\int_{\\mathbb{r}^{n}}y(\\mathbf{x})f_{\\mathbf{x}}(\\mathbf{x})d\\mathbf{x}\\int_{\\mathbb{r}^{n}}y(\\mathbf{c})f_{\\mathbf{x}}(\\mathbf{c})d\\mathbf{c}\\\\   & =   \\sigma^{2}+y_{\\emptyset , a}^{2}+\\sigma^{2}+y_{\\emptyset , a}^{2}-2y_{\\emptyset , a}^{2}\\\\   & =   2\\sigma^{2 } ,   \\end{split}\\label{31c}\\ ] ] where the third equality exploits the add - rdd relationship in ( [ 30a ] ) .",
    "hence , the theorem is proven .",
    "let @xmath94 be a random vector with the joint probability density function of the form @xmath168 , where @xmath169 is the marginal probability density function of its @xmath170th coordinate .",
    "then the expected error committed by the univariate rdd approximation for @xmath176 is @xmath177={\\displaystyle \\sum_{s=2}^{n}\\left(s^{2}-s+2\\right)\\sum_{{\\textstyle { \\emptyset\\neq u\\subseteq\\{1,\\cdots , n\\}\\atop |u|=s}}}}\\sigma_{u}^{2},\\label{32}\\ ] ] where @xmath178 $ ] is the variance of the _",
    "zero_-mean add component function @xmath85 , @xmath86 .",
    "[ t5 ]    setting @xmath162 in ( [ 29 ] ) , the expected error from the univariate rdd approximation on expansion is a sum @xmath177=i_{1,1}+i_{1,2}+i_{1,3}\\label{33}\\ ] ] of three integrals @xmath179 on @xmath180 , where their first indices represent the univariate approximation .",
    "the first integral @xmath181=y_{\\emptyset , a}^{2}+\\sigma^{2}=y_{\\emptyset , a}^{2}+\\sum_{s=1}^{n}\\:\\sum_{{\\textstyle { \\emptyset\\ne u\\subseteq\\{1,\\cdots , n\\}\\atop |u|=s}}}\\sigma_{u}^{2},\\label{35}\\ ] ] expressed in terms of the variance components , is independent of @xmath0 . however",
    ", the second integral depends on @xmath0 , yielding @xmath182 \\\\   & =   -2\\left(y_{\\emptyset , a}^{2}+\\hat{\\sigma}_{1,a}^{2}\\right ) \\\\   & =   -2y_{\\emptyset , a}^{2}-2{\\displaystyle \\sum_{{\\textstyle { \\emptyset\\ne u\\subseteq\\{1,\\cdots , n\\}\\atop |u|=1}}}}\\sigma_{u}^{2 } , \\end{split}\\label{36}\\ ] ] where the first , second , and fifth lines are obtained ( 1 ) employing the add - rdd relationships in ( [ 30c ] ) for @xmath162 , ( 2 ) recognizing @xmath183 and @xmath184 to be orthogonal , satisfying @xmath185=0 $ ] , and ( 3 ) applying ( [ 12 ] ) for @xmath162 , respectively . using the expression of @xmath186 from ( [ 26 ] ) and noting independent coordinates of @xmath14 and @xmath102 , the expanded third integral becomes @xmath187^{2}f_{\\mathbf{x}}(\\mathbf{x})f_{\\mathbf{x}}(\\mathbf{c})d\\mathbf{x}d\\mathbf{c}\\\\   & =   \\int_{\\mathbb{r}^{2n}}\\biggl[{\\displaystyle \\sum_{i=1}^{n}}y^{2}(x_{i},\\mathbf{c}_{-\\{i\\}})+2{\\displaystyle \\sum_{i=1}^{n-1}\\sum_{j = i+1}^{n}}y(x_{i},\\mathbf{c}_{-\\{i\\}})y(x_{j},\\mathbf{c}_{-\\{j\\}})\\\\   &   \\;\\;\\ ; + ( n-1)^{2}y^{2}(\\mathbf{c})-2(n-1){\\displaystyle \\sum_{i=1}^{n}}y(x_{i},\\mathbf{c}_{-\\{i\\}})y(\\mathbf{c})\\biggr]f_{\\mathbf{x}}(\\mathbf{x})f_{\\mathbf{x}}(\\mathbf{c})d\\mathbf{x}d\\mathbf{c}. \\end{split}\\label{37}\\ ] ] further evaluation of this integral requires exploiting sobol s formula in ( [ 31 ] ) for @xmath188 and @xmath189 , where @xmath190 , @xmath191 , yielding @xmath192    where @xmath193 is the generic @xmath194-variate coefficient for the univariate approximation , obtained by counting the number of @xmath195 or @xmath196 for @xmath197 @xmath198 _ e.g. _ , @xmath199 for @xmath200 , @xmath201 for @xmath202 , and so on @xmath198 due to symmetry .",
    "adding all terms in ( [ 35 ] ) , ( [ 36 ] ) , and ( [ 37b ] ) , with the recognition that @xmath203 , yields ( [ 32 ] ) , completing the proof .",
    "let @xmath94 be a random vector with the joint probability density function of the form @xmath168 , where @xmath169 is the marginal probability density function of its @xmath170th coordinate .",
    "then the expected error committed by the bivariate rdd approximation for @xmath204 is @xmath205={\\displaystyle \\sum_{s=3}^{n}{\\displaystyle \\frac{1}{4}}\\left(s^{4}-2s^{3}-s^{2}+2s+8\\right)\\sum_{{\\textstyle { \\emptyset\\neq u\\subseteq\\{1,\\cdots , n\\}\\atop |u|=s}}}}\\sigma_{u}^{2},\\label{39}\\ ] ] where @xmath178 $ ] is the variance of the _",
    "zero_-mean add component function @xmath85 , @xmath86 .",
    "[ t6 ]    setting @xmath163 in ( [ 29 ] ) , the expected error from the bivariate rdd approximation on expansion is another sum @xmath205=i_{2,1}+i_{2,2}+i_{2,3}\\label{40}\\ ] ] of three @xmath206-dimensional integrals @xmath207 where their first indices represent the bivariate approximation . since the first integral",
    "does not depend on @xmath0 , @xmath208 is the same as @xmath209 .",
    "following a similar reasoning employed for the univariate approximation , the second integral @xmath210 = -2\\left(y_{\\emptyset , a}^{2}+\\hat{\\sigma}_{2,a}^{2}\\right ) \\\\ % & =   -2\\left(y_{\\emptyset , a}^{2}+\\hat{\\sigma}_{2,a}^{2}\\right)\\\\   & =   -2y_{\\emptyset , a}^{2}-2{\\displaystyle \\sum_{{\\textstyle { \\emptyset\\ne u\\subseteq\\{1,\\cdots , n\\}\\atop |u|=1}}}}\\sigma_{u}^{2}-2{\\displaystyle \\sum_{{\\textstyle { \\emptyset\\ne u\\subseteq\\{1,\\cdots , n\\}\\atop |u|=2}}}}\\sigma_{u}^{2 } \\end{split}\\label{43}\\ ] ] contains variance terms associated with at most two variables . using the expression of @xmath211 from ( [ 27 ] ) ,",
    "the expanded third integral becomes @xmath212^{2}f_{\\mathbf{x}}(\\mathbf{x})f_{\\mathbf{x}}(\\mathbf{c})d\\mathbf{x}d\\mathbf{c}\\\\   & =   \\int_{\\mathbb{r}^{2n}}\\biggl\\{\\biggl[{\\displaystyle \\sum_{i=1}^{n-1}\\sum_{j = i+1}^{n}}y(x_{i},x_{j},\\mathbf{c}_{-\\{i , j\\}})\\biggr]^{2}{\\displaystyle + ( n-2)^{2}{\\displaystyle \\biggl[\\sum_{i=1}^{n}y(x_{i},\\mathbf{c}_{-\\{i\\}})\\biggr]^{2}}}\\\\   &   \\;\\;\\ ; + { \\displaystyle \\frac{1}{4}(n-1)^{2}(n-2)^{2}}y^{2}(\\mathbf{c})\\\\   &   \\;\\;\\ ; -2(n-2)\\biggl[{\\displaystyle \\sum_{i=1}^{n-1}\\sum_{j = i+1}^{n}}y(x_{i},x_{j},\\mathbf{c}_{-\\{i , j\\}})\\biggr]\\biggl[{\\displaystyle \\sum_{i=1}^{n}}y(x_{i},\\mathbf{c}_{-\\{i\\}})\\biggr]\\\\   &   \\;\\;\\ ; -(n-1)(n-2)^{2}\\biggl[{\\displaystyle \\sum_{i=1}^{n}}y(x_{i},\\mathbf{c}_{-\\{i\\}})\\biggr]y(\\mathbf{c})\\\\   &   \\;\\;\\ ; + ( n-1)(n-2)\\biggl[{\\displaystyle \\sum_{i=1}^{n-1}\\sum_{j = i+1}^{n}}y(x_{i},x_{j},\\mathbf{c}_{-\\{i , j\\}})\\biggr]y(\\mathbf{c})\\biggr\\ } f_{\\mathbf{x}}(\\mathbf{x})f_{\\mathbf{x}}(\\mathbf{c})d\\mathbf{x}d\\mathbf{c}. \\end{split}\\label{43b}\\ ] ] employing sobol s formula , this time for @xmath188 , @xmath213 , @xmath189 , @xmath214 , @xmath215 , @xmath216 , and @xmath217 , where @xmath218 , @xmath219 , results in @xmath220 producing the generic @xmath194-variate coefficient @xmath221 for the bivariate approximation . adding all terms in ( [ 42 ] ) , ( [ 43 ] ) , and ( [ 44 ] ) , with the understanding that @xmath222 , yields ( [ 39 ] ) , proving the theorem .",
    "the expected error @xmath223 $ ] from the zero - variate rdd approximation , expressed in terms of the error @xmath224 from the zero - variate add approximation , is @xmath225=2e_{0,a},\\;1\\le n<\\infty.\\label{45b}\\ ] ] [ c1 ]    the lower and upper bounds of the expected errors @xmath226 $ ] and @xmath227 $ ] from the univariate and bivariate rdd approximations , respectively , expressed in terms of the errors @xmath228 and @xmath229 from the univariate and bivariate add approximations , are @xmath230   \\le   \\left(n^{2}-n+2\\right)e_{1,a},\\;2\\le n<\\infty,\\ ] ] and @xmath231   \\le   { \\displaystyle \\frac{1}{4}}\\left(n^{4}-2n^{3}-n^{2}+2n+8\\right)e_{2,a},\\;3\\le n<\\infty,\\ ] ] respectively .",
    "[ c2 ]    when @xmath14 comprises independent and identically distributed uniform random variables over @xmath158 $ ] , the results of the _ zero_-variate and univariate rdd approximations presented in theorems [ t4 ] and [ t5 ] coincide with those derived by wang @xcite . however , the results of the bivariate rdd approximation @xmath198 that is , theorem [ t6 ] @xmath198 are new .",
    "theorems [ t5 ] and [ t6 ] demonstrate that on average the error from the univariate rdd approximation eliminates the variance terms associated with the univariate contribution .",
    "for the bivariate rdd approximation , the variance portions resulting from the univariate and bivariate terms have been removed as well .",
    "the univariate and bivariate add approximations also satisfy this important property .",
    "however , the coefficients of higher - variate terms in the rdd errors are larger than unity , implying greater errors from rdd approximations than from add approximations .    from corollary [ c1 ] , the _",
    "zero_-variate rdd approximation on average commits twice the amount of error as does the _ zero_-variate add approximation . since a _ zero_-variate approximation , whether derived from add or rdd , does not capture the random fluctuations of a stochastic response , the error analysis associated with a _",
    "zero_-variate approximation is useless . nonetheless , the _ zero_-variate results are reported here for completeness .",
    "corollary [ c2 ] shows that the expected error from the univariate rdd approximation is at least four times larger than the error from the univariate add approximation .",
    "in contrast , the expected error from the bivariate rdd approximation can be eight times larger or more than the error from the bivariate add approximation . given a truncation , an add approximation is superior to an rdd approximation .",
    "in addition , rdd approximations may perpetrate very large errors at upper bounds when there exist a large number of variables and appropriate conditions .",
    "for instance , consider a contrived example involving a function of @xmath232 variables with a finite variance @xmath233 and the following distribution of the variance terms : @xmath234 , @xmath235 , and @xmath236 .",
    "then , the errors from the univariate and bivariate add approximations are both equal to @xmath237 , which is negligibly small .",
    "in contrast , the error from the univariate rdd approximation reaches @xmath238 , an unacceptably large magnitude already .",
    "furthermore , the error from the bivariate rdd approximation jumps to an enormously large value of @xmath239 .",
    "more importantly , the results reveal a theoretical possibility for a higher - variate rdd approximation to commit a larger error than a lower - variate rdd approximation @xmath198 an impossible scenario for the add approximation .",
    "however , it is unlikely for this odd behavior to be exhibited for realistic functions , where the variances of higher - variate component functions attenuate rapidly or vanish altogether .",
    "nonetheless , a caution is warranted when employing rdd approximations for stochastic analysis of high - dimensional systems .",
    "[ r1 ]      the error analysis presented so far is limited to at most the bivariate approximation . in this subsection ,",
    "the approximation error from a general @xmath0-variate truncation is derived as follows .",
    "let @xmath94 be a random vector with the joint probability density function of the form @xmath168 , where @xmath169 is the marginal probability density function of its @xmath170th coordinate .",
    "then the expected error committed by the @xmath0-variate rdd approximation for @xmath240 @xmath241 is @xmath242={\\displaystyle \\sum_{s = s+1}^{n}\\left[{\\displaystyle 1+\\sum_{k=0}^{s}}{\\displaystyle \\binom{s - s+k-1}{k}^{2}}\\binom{s}{s - k}\\right]\\sum_{{\\textstyle { \\emptyset\\neq u\\subseteq\\{1,\\cdots , n\\}\\atop |u|=s}}}}\\sigma_{u}^{2},\\label{47}\\ ] ] where @xmath178 $ ] is the variance of the _",
    "zero_-mean add component function @xmath85 , @xmath86 .",
    "[ t7 ]    expanding the square in ( [ 29 ] ) , the expected error from the @xmath0-variate rdd approximation is @xmath242=i_{s,1}+i_{s,2}+i_{s,3},\\label{48}\\ ] ] where @xmath243 are three generic @xmath206-dimensional integrals .",
    "the first integral @xmath244 is the same as before .",
    "the second integral @xmath245=-2\\left(y_{\\emptyset , a}^{2}+\\hat{\\sigma}_{s , a}^{2}\\right)\\\\   & =   -2y_{\\emptyset , a}^{2}-2{\\displaystyle \\sum_{s=1}^{s}\\:\\sum_{{\\textstyle { \\emptyset\\ne u\\subseteq\\{1,\\cdots , n\\}\\atop |u|=s}}}}\\sigma_{u}^{2 } \\end{split}\\label{51}\\ ] ] comprises variance terms associated with at most @xmath0 variables .",
    "the third integral , using @xmath246 from ( [ 18 ] ) , takes the form @xmath247^{2}f_{\\mathbf{x}}(\\mathbf{x})f_{\\mathbf{x}}(\\mathbf{c})d\\mathbf{x}d\\mathbf{c},\\\\   & =   b_{s}(0)y_{\\emptyset , a}^{2}+{\\displaystyle \\sum_{s=1}^{n}b_{s}(s)\\sum_{{\\textstyle { \\emptyset\\ne u\\subseteq\\{1,\\cdots , n\\}\\atop |u|=s}}}}\\sigma_{u}^{2 } \\end{split}\\label{52}\\ ] ] with the generic @xmath194-variate coefficient @xmath248 , @xmath249 for the @xmath0-variate approximation yet to be determined .",
    "of @xmath250 such coefficients , the last one , @xmath251 is easier to determine .",
    "it is obtained from the expansion coefficients of the square in ( [ 52 ] ) that are associated with all variance terms of @xmath126 . to determine other coefficients , @xmath248 , @xmath252 , the procedure used before for the univariate or bivariate approximation is unwieldy .",
    "an alternative scheme proposed here stems from the realization that the expressions of those coefficients consist of terms from two sources : ( 1 ) terms that depend solely on @xmath1 , which can be described by a function @xmath253 , say , of @xmath1 ; and ( 2 ) terms that depend on both @xmath1 and @xmath194 , which can be described by another function @xmath254 , say , of @xmath1 and @xmath194 . following this rationale ,",
    "let a generic coefficient be expressed by @xmath255 for any @xmath256 and @xmath257 switching the variables @xmath1 and @xmath194 , ( [ 54 ] ) produces @xmath258 at @xmath259 , ( [ 54 ] ) and ( [ 55 ] ) result in @xmath260 .",
    "either of these two equations at @xmath259 with @xmath260 in mind yields @xmath261 or @xmath262 , where the function @xmath263 is already described in ( [ 53 ] ) .",
    "therefore , the generic @xmath194-variate coefficient @xmath264 where the binomial coefficients should be interpreted more generally than their classical combinatorial definition , for instance , @xmath265 valid for any real number @xmath266 and any integer @xmath267 .",
    "adding all terms in ( [ 50 ] ) , ( [ 51 ] ) , and ( [ 52 ] ) , with the cognizance that @xmath268 for all @xmath269 , yields ( [ 47 ] ) , proving the theorem .",
    "the lower and upper bounds of the expected error @xmath270 $ ] from the @xmath0-variate rdd approximation , expressed in terms of the error @xmath271 from the @xmath0-variate add approximations , are @xmath272\\le\\left[{\\displaystyle 1+\\sum_{k=0}^{s}}\\binom{n - s+k-1}{k}^{2}\\binom{n}{s - k}\\right]e_{s , a},\\label{57}\\ ] ] @xmath2 , where the coefficients of the lower and upper bounds are obtained from @xmath273 and @xmath274 respectively .",
    "[ c3 ]    both theorem [ t7 ] and corollary [ c3 ] are new and provide a general result pertaining to rdd error analysis for an arbitrary truncation .",
    "the specific results of the _ zero_-variate or univariate or bivariate rdd approximation , derived in the preceding subsection , can be recovered by setting @xmath149 or @xmath150 or @xmath151 in ( [ 47 ] ) through ( [ 59 ] ) . from corollary [ c3 ] ,",
    "the expected error from the @xmath0-variate rdd approximation of a multivariate function is at least @xmath3 times larger than the error from the @xmath0-variate add approximation .",
    "in other words , the ratio of rdd to add errors doubles for each increment of the truncation . consequently , add approximations are exceedingly more precise than rdd approximations at higher - variate truncations .",
    "although the relative disadvantage of using rdd over add worsens drastically with the truncation @xmath0 , one hopes that the approximation error is also decreasing with increasing @xmath0 .",
    "for instance , given a rate at which @xmath275 decreases with @xmath11 , what can be inferred on how fast @xmath271 and @xmath270 $ ] decay with respect to @xmath0 ?",
    "corollary [ c4a ] and subsequent discussions provide some insights .    if the variance of the _ zero_-mean add component function @xmath85 diminishes according to @xmath276 , where @xmath86 , and @xmath277 and @xmath278 are two real - valued constants , then    @xmath279    and @xmath280 \\le c{\\displaystyle \\sum_{s = s+1}^{n}\\left[{\\displaystyle 1+\\sum_{k=0}^{s}}{\\displaystyle \\binom{s - s+k-1}{k}^{2}}\\binom{s}{s - k}\\right ] { \\displaystyle \\binom{n}{s}}p^{-s}}\\label{59d}\\ ] ] for @xmath2 . [ c4a ]",
    "when the equality holds in ( [ 59c ] ) , @xmath271 decays strictly monotonically with respect to @xmath0 for any rate parameter @xmath281 .",
    "in contrast , @xmath270 $ ] , according to ( [ 59d ] ) , does not follow suit for an arbitrary @xmath281",
    ". however , there exists a minimum threshold , say , @xmath282 , when crossed , @xmath270 $ ] also decays monotonically .",
    "the threshold can be determined from the condition that @xmath223 = \\mathbb{e}\\left[e_{1,r}\\right]$ ] , resulting in the relationship    @xmath283    between @xmath1 and @xmath282 .",
    "equation ( [ 59e ] ) supports an exact solution of    @xmath284    in terms of @xmath282 , expressed employing the lambert w function @xmath285 and can be inverted easily .",
    "for instance , when @xmath286 , ( [ 59f ] ) yields @xmath287 , the only real - valued solution of interest . depicted in figure [ f1 ] ( left ) ,",
    "@xmath282 derived from ( [ 59f ] ) increases monotonically and strikingly close to linearly with @xmath1 for the ranges of the variables examined .    using the equalities in ( [ 59c ] ) and ( [ 59d ] ) , figure [ f1 ] ( right ) presents plots of two normalized errors ,",
    "@xmath270/\\sigma^2 $ ] and @xmath288 , against @xmath0 , each obtained for @xmath286 and @xmath289 or @xmath290 , where the variance @xmath291 $ ] .",
    "when the rate parameter is sufficiently low ( _ e.g. _ , @xmath292 ) , the expected rdd error initially rises before falling as @xmath0 becomes larger .",
    "the non - monotonic behavior of the rdd error is undesirable , but it vanishes when the rate parameter is sufficiently high ( _ e.g. _ , @xmath293 ) .",
    "no such anomaly is found in the add error for any @xmath281 .     and",
    "@xmath1 ( left ) and normalized rdd and add errors versus @xmath0 for @xmath286 , @xmath289 or @xmath290 ( right ) . ]    the expected error @xmath294 $ ] from the best rdd approximation , expressed in terms of the error @xmath295 from the best add approximation , where the best approximations are obtained by setting @xmath296 , is @xmath297=2^{n}e_{n-1,a},\\;1\\le n<\\infty.\\label{60}\\ ] ] [ c4 ]    due to the factor @xmath17 in ( [ 60 ] ) , the expected error from the best rdd approximation as @xmath298 can be significantly large unless the best add approximation commits an error equal to or smaller than @xmath299 . in reference to corollary [ c4a ] ,",
    "suppose that @xmath276 .",
    "then @xmath294 \\le c(2/p)^n$ ] .",
    "therefore , @xmath294 \\to 0 $ ] as @xmath298 for @xmath300 .",
    "the error analysis presented in this paper pertains to only second - moment characteristics of @xmath24 .",
    "similar analyses or definitions aimed at higher - order moments or probability distribution of @xmath4 can be envisioned , but no closed - form solutions and simple expressions are possible . however ,",
    "if @xmath4 satisfies the requirements of the chebyshev inequality or its descendants @xmath198 a condition fulfilled by many realistic functions @xmath198 then the results and findings from this work can be effectively exploited for stochastic analysis .",
    "see the longer version of the paper for further details .",
    "two variants of dimensional decomposition , namely , rdd and add , of a multivariate function , both representing finite sums of lower - dimensional component functions , were studied .",
    "the approximations resulting from the truncated rdd and add are explicated , including clarifications of parallel developments and synonyms used by various researchers . for the rdd approximation , a direct form , previously developed by the author s group ,",
    "was found to provide a vital link to subsequent error analysis .",
    "new theorems were proven about the expected errors from the bivariate and general rdd approximations , so far available only for the univariate rdd approximation , when the reference point is selected randomly .",
    "they furnish new formulae for the lower and upper bounds of the expected error committed by an arbitrarily truncated rdd , providing a means to grade rdd against add approximations .",
    "the formulae indicate that the expected error from the @xmath0-variate rdd approximation of a function of @xmath1 variables , where @xmath2 , is at least @xmath3 times larger than the error from the @xmath0-variate add approximation .",
    "consequently , add approximations are exceedingly more precise than rdd approximations at higher - variate truncations .",
    "the analysis also finds the rdd approximation to be sub - optimal for an arbitrarily selected reference point , whereas the add approximation always results in minimum error .",
    "therefore , rdd approximations should be used with caveat ."
  ],
  "abstract_text": [
    "<S> the main theme of this paper is error analysis for approximations derived from two variants of dimensional decomposition of a multivariate function : the referential dimensional decomposition ( rdd ) and analysis - of - variance dimensional decomposition ( add ) . </S>",
    "<S> new formulae are presented for the lower and upper bounds of the expected errors committed by bivariately and arbitrarily truncated rdd approximations when the reference point is selected randomly , thereby facilitating a means for weighing rdd against add approximations . </S>",
    "<S> the formulae reveal that the expected error from the @xmath0-variate rdd approximation of a function of @xmath1 variables , where @xmath2 , is at least @xmath3 times greater than the error from the @xmath0-variate add approximation . </S>",
    "<S> consequently , add approximations are exceedingly more precise than rdd approximations . </S>",
    "<S> the analysis also finds the rdd approximation to be sub - optimal for an arbitrarily selected reference point , whereas the add approximation always results in minimum error . </S>",
    "<S> therefore , the rdd approximation should be used with caution . </S>"
  ]
}