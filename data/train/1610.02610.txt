{
  "article_text": [
    "the last years have seen dramatic improvements in robotic capabilities relevant to household tasks such as putting items into a dishwasher  @xcite , folding and ironing clothing  @xcite , and cleaning surfaces  @xcite .",
    "so far , however , robots have not been able to robustly perform household tasks involving liquids , such as pouring a glass of water .",
    "solving such tasks requires both robust control and detection of liquid during the pouring operation .",
    "humans often are not very accurate at this , requiring specialized containers to measure a specific amount of liquid .",
    "instead people often use vague , relative terms such as `` pour me a half cup of coffee '' or `` just a little , please . ''",
    "while there has been recent success in robotics on controlling a manipulator to pour liquids simulated by small balls  @xcite and on detecting liquids using optical flow or deep learning  @xcite , the task of pouring certain amounts of actual liquids has not been addressed .    in this paper",
    ", we introduce a framework that enables robots to robustly pour specific amounts of a liquid into containers typically found in a home environment , such as coffee mugs , cups , glasses , or bowls .",
    "we achieve this in the most general setting , without requiring specialized hardware , such as highly accurate force sensors for measuring the amount of liquid held by a robot manipulator , scales placed under the target container , or sensors designed for detecting liquids .",
    "however , while we avoid requiring specialized environmental augmentation , our investigation is on how accurate a robot could pour under relatively controlled conditions , such as having been able to train on the target containers .",
    "the intuition behind our approach is based on the insight that people strongly rely on visual cues when pouring liquids .",
    "for example , a health study revealed that the amount of wine people pour into a glass is strongly biased by visual factors such as the shape of the glass or the color of the wine  @xcite .",
    "we thus propose a framework that uses visual feedback in a closed - loop pouring controller .",
    "specifically , we train a deep neural network structure to estimate the amount of liquid in a cup from raw visual data .",
    "our network structure has two stages . in the first stage ,",
    "a network detects which pixels in a camera image contain water .",
    "the output of the detection network is fed into another network that estimates the amount of liquid already in the container .",
    "this amount is used as real - time feedback in a pid controller that is tasked to pour a desired amount of water into a cup .    to generate labeled data needed for the neural networks",
    ", we developed an experimental setup that uses a thermal camera calibrated with an rgbd camera to automatically label which pixels in the color frames contain ( heated ) water .",
    "experiments with a baxter robot pouring water into three different containers ( two mugs and one bowl ) indicate that this approach allows us to train deep networks that provide sufficiently accurate volume estimates for the pouring task .",
    "our main contributions in this paper are ( 1 ) an overall framework for determining the amount of liquid in a container for real - time control during a pouring action ; ( 2 ) the use of thermal imagery to generate ground truth data for pixel level labeling of ( heated ) water ; ( 3 ) a deep neural network that uses such labels to detect liquid pixels in raw color images ; ( 4 ) a model - based method to determine the volume of liquid in a target container given pixel - wise liquid detection ; ( 5 ) a neural network to regress to the volume of liquid given pixel - wise liquid detections as input ; and ( 6 ) an extensive evaluation that shows that our methodology is suitable for control by deploying it on a robot for use in a pouring task .",
    "4.cm    4.cm    4.cm    4.cm    there is prior work related to robotic pouring , however , most of it either uses coarse simulations disconnected from real liquid perception and dynamics @xcite or constrained task spaces that bypass the need to perceive and reason directly about liquids @xcite .",
    "additionally , all of these works with the exception of @xcite pour the entire contents of the source container into the target container , with the focus on other factors such as spillage or the overall motion trajectory .",
    "in contrast , in this work we focus primarily on pouring a specific amount of liquid from the source into the target rather than simply emptying the source container into the target . to do this , the robot requires some method for estimating the volume of liquid in the target .",
    "et al._@xcite utilized force sensors in the robot s arm to measure how much had been poured out , however this requires a robot with very precise torque sensors , which are not available on our baxter robot . in our own prior work @xcite we placed a digital scale under the target container . but this method presents many of its own challenges , such as delay in the scale measurement ( often 1 - 2 seconds ) and no information about where the liquid is or how it is moving .",
    "humans , on the other hand , are able to accomplish this task purely from visual feedback , which strongly suggests that robots should be able to as well .",
    "there is some prior work related to directly perceiving liquids from sensory feedback @xcite .",
    "work by yamaguchi and atkeson @xcite utilizes optical flow to detect liquids as they flow from a source into a target container .",
    "however , for the tasks in this paper , the robot must also be able to detect standing water with no motion , for which optical flow is poorly suited .",
    "instead , we build on our own prior work relating to liquid detection in simulation @xcite .",
    "we developed a method utilizing fully - convolutional neural networks @xcite to label pixels in an image as either _ liquid _ or _ not - liquid_. here we utilize the recurrent network with long short - term memory ( lstm ) layers @xcite that we used in that work to detect and label liquid in an image .",
    "in this paper , the robot is tasked with pouring a specific amount of liquid from a source container into a target container .",
    "this task is more difficult than prior work on robotic pouring which primarily focuses on pouring all the contents of the source container into the target container , whereas we focus on pouring only a limited amount from the source . to accomplish this",
    ", the robot must use visual feedback to continuously estimate the current volume of liquid in the target container .",
    "our approach has 3 main components : first the robot detects which pixels in its visual field are liquid and which are not .",
    "next the robot uses these detections to estimate the volume of liquid in the target container . finally , the robot feeds these volume estimates into a controller to pour the liquid into the target .",
    "figure [ fig : model ] shows a diagram of this process .",
    "we structure the problem in this manner as opposed to simply training one end - to - end network as it allows us to train and evaluate each of the individual components of the system , which can give us better insight into its operation .      in order for both the model - based and model - free volume estimation methods to work",
    ", the robot must classify each pixel in the image as _ liquid _ or _ not - liquid_. we developed two methods for acquiring these pixel labels : a thermographic camera in conjunction with heated water , and a fully - convolutional neural network@xcite with color images . while the thermal camera works well for generating pixel labels ,",
    "it is also rather expensive and must be registered to an rgbd sensor . in our prior work @xcite ,",
    "we developed a method for generating pixel labels on simulated data for liquid from color images only , which we briefly describe here .",
    "given color images , we train a convolutional neural network ( cnn ) to label each pixel as _ liquid _ or _ not - liquid _ ( we use the thermal camera to acquire the ground truth labeling ) .",
    "the network is fully - convolutional , that is , all the learned layers are convolution layers with no fully - connected layers .",
    "the output of the network is a heatmap over the image , with real values in the range @xmath0 $ ] , where higher values indicate a higher likelihood of liquid . in @xcite",
    "we tested 3 network structures and found that a recurrent network utilizing a long short - term memory ( lstm ) layer @xcite performs the best of the 3 .",
    "here we use the lstm - cnn from that paper , which is shown in the top row of figure [ fig : model ] .",
    "we refer the reader to @xcite for more details .",
    "( lstm_in3 ) at ( 0.0,9.0 ) ; ( lstm_rec1 ) at ( 0.0,7.225 ) ;    ( lstm_conv1 ) at ( 2.2,8.975 ) ; ( lstm_conv2 ) at ( 3.1,8.975 ) ; ( lstm_conv3 ) at ( 4.0,8.975 ) ; ( lstm_conv4 ) at ( 4.9,8.975 ) ; ( lstm_conv5 ) at ( 5.8,8.975 ) ;    ( lstm_rec_conv1 ) at ( 2.2,7.2 ) ; ( lstm_rec_conv2 ) at ( 3.1,7.2 ) ; ( lstm_rec_conv3 ) at ( 4.0,7.2 ) ;    ( lstm_lstm1 ) at ( 7.0,8.0 ) [ fill = green!60 ] ; ( lstm_fc_conv1 ) at ( 8.4,8.0 ) [ fill = blue!60 ] ; ( lstm_deconv ) at ( 9.3,8.0 ) [ fill = orange!60 ] ; ( lstm_out1 ) at ( 10.2,8.0 ) ;    ( lstm_rec_in2 ) at ( 6.0 , 7.0 ) recurrent + state ; ( lstm_rec_in3 ) at ( 7.45 , 6.7 ) cell + state ;    ( lstm_rec_out2 ) at ( 8.7 , 9.5 ) recurrent + state ; ( lstm_rec_out3 ) at ( 7.45 , 9.3 ) cell + state ;    ( lstm_in3 )  ( lstm_conv1 ) ; ( lstm_conv1 )  ( lstm_conv2 ) ; ( lstm_conv2 )  ( lstm_conv3 ) ; ( lstm_conv3 )  ( lstm_conv4 ) ; ( lstm_conv4 ) ",
    "( lstm_conv5 ) ;    ( lstm_rec1 )  ( lstm_rec_conv1 ) ; ( lstm_rec_conv1 )  ( lstm_rec_conv2 ) ; ( lstm_rec_conv2 )  ( lstm_rec_conv3 ) ;    ( lstm_conv5.east )  ( lstm_lstm1.west ) ; ( lstm_rec_conv3.east )  ( lstm_lstm1.west ) ;    ( lstm_lstm1 )  ( lstm_fc_conv1 ) ; ( lstm_fc_conv1 )  ( lstm_deconv ) ; ( lstm_deconv )  ( lstm_out1 ) ;    ( lstm_rec_in2 )  ( lstm_lstm1.west ) ;    ( lstm_rec_in3 )  ( lstm_lstm1.south ) ; ( lstm_lstm1.east )  ( lstm_rec_out2.215 ) ; ( lstm_lstm1.north )  ( lstm_rec_out3 ) ;    ( concat ) at ( 7.5,3.525 ) [ fill = gray!60 ] ; ( conv6 ) at ( 8.5,3.525 ) ; ( fc_conv1 ) at ( 9.5,3.525 ) [ fill = blue!60 ] ; ( fc_conv2 ) at ( 10.5,3.525 ) [ fill = blue!60 ] ; ( fc_conv3 ) at ( 11.5,3.525 ) [ fill = blue!60 ] ;    ( out1 ) at ( 12.5,3.525 ) ;    ( in3a ) at ( 0.0,4.0 ) ; ( conv1a ) at ( 2.2,3.975 ) ; ( conv2a ) at ( 3.1,3.975 ) ;    ( conv3a ) at ( 4.0,3.975 ) ; ( conv4a ) at ( 4.9,3.975 ) ; ( conv5a ) at ( 5.8,3.975 ) ; ( in3a )  ( conv1a ) ;    ( conv1a )  ( conv2a ) ; ( conv2a )  ( conv3a ) ; ( conv3a )  ( conv4a ) ; ( conv4a )  ( conv5a ) ; ( conv5a )  ( concat ) ;    ( elps_node ) at ( 0.22,2.5 ) [ fill = none , draw = none ] ;    ( in3b ) at ( 0.5,3.0 ) ; ( conv1b ) at ( 2.7,3.075 ) ; ( conv2b ) at ( 3.6,3.075 ) ; ( conv3b ) at ( 4.5,3.075 ) ;    ( conv4b ) at ( 5.4,3.075 ) ; ( conv5b ) at ( 6.3,3.075 ) ; ( in3b )  ( conv1b ) ; ( conv1b )  ( conv2b ) ; ( conv2b )  ( conv3b ) ;    ( conv3b )  ( conv4b ) ; ( conv4b )  ( conv5b ) ; ( conv5b )  ( concat ) ;    ( concat )  ( conv6 ) ; ( conv6 )  ( fc_conv1 ) ; ( fc_conv1 )  ( fc_conv2 ) ; ( fc_conv2 )  ( fc_conv3 ) ; ( fc_conv3 )  ( out1 ) ; ( crop ) at ( 5.0,5.7 ) [ fill = magenta!60,minimum height=0.7 cm ] ; ( lstm_out1.south ) to [ out=270,in=0 ] ( crop.east ) ; ( crop.west ) to [ out=180,in=90 ] ( in3a.north ) ;    ( hmm ) at ( 4.0,0.0 ) [ fill = cyan!60,minimum height=0.7 cm ] ; ( pid ) at ( 8.0,0.0 ) [ fill = violet!60,minimum height=0.7 cm ] ; ( control ) at ( 12.0,0.0 ) ; ( out1.south ) to [ out=270,in=90 ] ( hmm.north ) ; ( hmm.east )  ( pid.west ) ; ( pid.east ) ",
    "( control.west ) ; ( hmm.east )  ( 7.0,0.0 )  ( 7.0,-1.0 )  ( 3.0,-1.0 )  ( 3.0,0.0 )  ( hmm.west ) ; at ( 4.7,0.7 ) * @xmath1 * ; at ( 6.6,0.4 ) * @xmath2 * ;      we propose two different methods for estimating the volume of liquid in a target container .",
    "the first is a model - based method , which assumes we have access to a 3d model of the target container and infers the height of the liquid based on the camera pose and binary pixel labels .",
    "the second is a model - free method that trains a neural network to regress to the volume of liquid in the target container given labeled pixels .",
    "our model - based method for estimating the volume of liquid in a target container assumes we have a 3d model of the container and that we can use the pointcloud from our rgbd sensor to find its pose in the scene . to determine the volume of liquid in the container ,",
    "we first acquire the pixel - wise liquid labels as described in the previous section .",
    "next we use these classifications to compute the height @xmath3 of the liquid in the container at time @xmath4 .",
    "if we assume that the liquid is resting level in the container , then there is a one - to - one correspondence between the height of the liquid and the volume , and we can use the 3d model to compute the volume given the height . we use a discrete bayes s filter with observations only to estimate a probability distribution @xmath5 over @xmath3 for all timesteps @xmath4 .",
    "that is , for all @xmath4 , we wish to estimate @xmath6 where @xmath7 are all the observations ( pixel labels ) up to time @xmath4 .",
    "we make the markovian assumption that each state is conditionally independent of all prior observations given the previous state , thus the resulting bayes filter is equivalent to a corresponding hidden markov model ( hmm ) .",
    "we can estimate the posterior distribution over @xmath3 using bayes s rule as follows : @xmath8    since we make the conditional independence assumption , we can drop the @xmath9 term from the conditional in the observation probability , resulting in @xmath10 .",
    "we compute @xmath11 as the expectation of the previous distribution times the transition probability , i.e , @xmath12 where @xmath13 is the probability of transitioning from height @xmath14 and @xmath15 is the prior probability of @xmath14 at time @xmath16 .",
    "we can compute the observation probability as @xmath17 where @xmath18 is the set of all pixels that see the inside of the target container , as determined by the model s pose in the scene .",
    "we make the naive bayes assumption that each pixel is conditionally independent of all the others given @xmath3 , which , while not technically correct , works well in practice .",
    "to compute the observation probability of an individual pixel @xmath19 being either liquid or not liquid given the height of the surface of the water , we compare what the robot would expect to see if @xmath3 were the true height to what the observed pixel labels are .",
    "to do this , we use the pose of the model to fit a plane to the surface height @xmath3 and project that surface back into the camera s pixel space , generating expected pixel labels .",
    "an example of this is shown in figure [ fig : therm_layout ] .",
    "based on this , we set @xmath20 as follows :    [ cols=\">,>,^,^ \" , ]     where @xmath21 is the expected label at pixel @xmath19 for @xmath3 .",
    "we allow slightly more error for classifying a pixel as _ liquid _ when it is above the level of the water due to the stream of liquid falling from the source container during the pour .",
    "we discretize the height @xmath5 into 1000 values . after computing the distribution over @xmath22 ,",
    "we take the median as the height of the liquid , and thus the volume , at each time step .",
    "our model - free method replaces the object pose inference of the model - based method with a neural network .",
    "the neural network takes in pixel labels and produces a volume estimate .",
    "we use only the output of the detection network described in section [ sec : classification ] for the pixel labels , so we directly feed the heatmap over the pixels into the volume estimation network .",
    "we also evaluate adding as inputs either the color or depth images , which we append channel - wise to the pixel labels before feeding into the network .",
    "we crop the input to the network around the target container .",
    "the output of the volume estimation network is a discrete distribution over a class label .",
    "we treat this as the observation for a discrete bayes filter and process it in a similar manner as the previous section ( notably we make the markovian independence assumption ) .",
    "we can compute the distribution over volumes as @xmath23 where @xmath2 is a distribution over the volume of liquid @xmath24 in the target container at time @xmath4 .",
    "we compute @xmath25 in the same manner as @xmath11 in the previous section .",
    "to compute @xmath26 , we treat the output of the network as a probability distribution over a discrete observation .",
    "there are multiple methods we can utilize to compute the observation probability from the network output .",
    "for example , we could take the maximum probability and consider that value the observed value .",
    "however , a more robust and principled method would be to compute the expectation of the observation probability , taking the entire distribution output by the network into account .",
    "we can compute this as @xmath27 = \\displaystyle\\sum_i p(z_t = i | v_t)p(z_t = i ) \\vspace{-0.2cm}\\ ] ] where @xmath28 is the probability that the observation is @xmath29 given the volume is @xmath24 , and @xmath30 is the probability of state @xmath29 derived from the output of the network .",
    "we use the median of @xmath31 as the volume at time @xmath4 .",
    "we evaluated three different network architectures : a single - frame cnn , a multi - frame cnn , and a recurrent lstm cnn .",
    "we use the caffe deep learning framework @xcite to implement our networks    _ single - frame cnn : _ the single - frame network is a standard cnn that takes as input a single image",
    ". it then passes the image through 5 convolution layers , each of which is followed by a max pooling and rectified linear layer .",
    "every layer has a stride of 1 except for the first 3 max pooling layers .",
    "it passes the result through 3 fully connected layers , each followed by a rectified linear layer .",
    "these last 3 layers are also followed by dropout layers during training , with a drop rate of 10% .",
    "the single - frame network ( cnn ) is similar to the multi - frame network shown in the center row of figure [ fig : model ] , with the exception that it only takes a single frame and does not have the concatenation layer or the convolution layer immediately following it .    _",
    "multi - frame cnn : _ the multi - frame network ( mf - cnn ) is shown in the center row of figure [ fig : model ] .",
    "it takes as input a set of temporally sequential images .",
    "each image is passed independently through the first 5 layers of the network , which are identical to the first 5 convolutional layers in the single - frame network .",
    "next , the result of each image is concatenated channel - wise and passed through another convolution layer ( which is also followed by max pooling and rectified linear layers ) .",
    "this is then fed into 3 fully connected layers , which are identical to the last 3 layers of the single - frame cnn .",
    "_ recurrent lstm cnn : _ the lstm - cnn is identical to the single - frame network , with the exception that we replace the first fully connected layer with the lstm layer .",
    "in addition to the output of the convolution layers , the lstm layer also takes as input the recurrent state from the previous timestep , as well as the cell state from the previous timestep . each gate in the lstm layer is a 256 node fully connected layer .",
    "please refer to figure 1 of @xcite for a detailed layout of the lstm layer .      for this paper , we want to investigate whether , given good real - time feedback , pouring can be performed with a simple controller .",
    "we place a table in front of the robot , and on the table we place the target container .",
    "we fix the source container in the robot s right gripper and pre - fill it with a specific amount of water not given to the robot .",
    "we also fix the robot s arm such that the source container is above and slightly to the side of the target container .    to pour , the robot controls the angle of its wrist joint , thus directly controlling the angle of the source container .",
    "we use a modified pid controller to execute the pour .",
    "the robot first tilts the container to a pre - specified angle ( we use 75 degrees from vertical ) , then begins running the pid controller , using the difference between the target volume and the current volume in the target container as its error signal .",
    "since pouring is a non - reversible task ( liquid can not return to the source once it has left ) , we set the integral gain to 0 , and we set the proportional and derivative gains to @xmath32 and @xmath33 respectively .",
    "once the target volume has been reached , the robot rotates the source container until it is vertical once again .",
    "both our model - based and model - free methods require finding the target container on the table in front of the robot ( though only the model - based needs a 3d model ) . to find the container , we use the robot s rgbd camera to capture a pointcloud of the scene in front of the robot and then utilize functions built - in to the pointcloud library ( pcl ) @xcite to find the plane of the table and cluster the points on top of the table . to acquire the pose for the model - based method",
    ", we use iterative closest points to find the 3d pose of the model in the scene .",
    "next we use this pose to label each pixel in the image as either _ inner _",
    "( inside of the container ) , _ outer _ ( outside of the container ) , or _",
    "neither_.      we use a thermal camera in combination with water heated to approximately 93celsius to get the ground truth pixel labels for the liquid . to register the thermal image to the color image , we use a paper checkerboard pattern attached to a @xmath34 centimeter metal aluminum sheet .",
    "we then direct a small , bright spotlight at the pattern , causing a heat differential between the white and black squares , which is visible as a checkerboard pattern in the thermal image and we use opencv s built - in function for finding corners of a checkerboard to find correspondence points and compute an affine transformation .",
    "we use an adaptive threshold based on the average temperature of the pixels associated with the target container ( which includes the pixels for the liquid in the container ) .",
    "the result of this is a binary image with each pixel classified as either _ liquid _ or _ not - liquid_. figure [ fig : therm_examples ] shows a color image , its corresponding thermal image transformed to the color pixel space , and a simple temperature threshold of the thermal image . note that the thermal camera provides quite reliable pixel labels for liquid detection with minimal false positives      in order to train our networks in the previous section , and to evaluate both our model - based and model - free methods ,",
    "we need a baseline ground truth volume estimation . to generate this baseline",
    ", we utilize the thermal camera in combination with the model - based method described in section [ sec : model - based ] .",
    "however , since this analysis can be done _ a posteriori _ and does not need to be real - time , we can use the benefit of hindsight to improve our estimates , i.e. , future observations can improve the current state estimate . while we acknowledge that this method does not guarantee perfect volume estimates , the combined accuracy of the thermal camera and after - the - fact processing yield robust estimates suitable for training and evaluation .    to compute this baseline",
    "we replace the forward method for hmm inference described in section [ sec : model - based ] with viterbi decoding @xcite .",
    "we replace the summation in equation [ eq : prior ] in the computation of the prior @xmath35 with a @xmath36 to compute the probability of each sequence .",
    "we use a corresponding @xmath37 to compute the previous state from the current state , starting at the last time step and working backwards . at the last time step , we start with the most probable state . thus using this method",
    "we can generate a reliable ground truth estimate of the volume of liquid in the target container over the duration of a pouring sequence to use for training our learning algorithms and evaluating our methodology .",
    "all of our experiments were performed on our rethink robotics baxter research robot , shown in figure [ fig : robot_setup ] .",
    "it is equipped with two 7-dof arms , each with an electric parallel gripper .",
    "for the experiments in this paper , we use exclusively the right arm .",
    "the robot has an asus xtion pro mounted on its upper - torso , directly below its screen , which includes both an rgb color camera and a depth sensor , each of which produce @xmath38 images at 30hz .",
    "mounted on the robot immediately above the xtion sensor is an infrared cameras inc .",
    "8640p thermal imaging camera , which reads the temperature of the image at each pixel and outputs a @xmath39 image at 30hz .      for all experiments ,",
    "the robot poured from the cup shown in its gripper in figure [ fig : robot_setup ] .",
    "we used three target containers , also shown in figure [ fig : robot_setup ] .",
    "we collected a dataset of pours using this setup in order to both train and evaluate our methodologies .",
    "we collected a total of 279 pouring sequences , in which the robot attempted to pour 250ml of water into the target using the thermal camera with the model - based method , with the initial amount in the varied between 300ml , 350ml , and 400ml .",
    "each sequence lasted exactly 25 seconds and was recorded on both the thermal and rgbd cameras at 30hz .",
    "we randomly divided the data 75%-25% into train and evaluation sets .",
    "after the data was collected , we used the thermal images to generate ground truth pixel labels as well as we used the viterbi decoding method described in section [ sec : get_gt ] to generate ground truth volume estimates , which we compare against for the remainder of this section .",
    "we use this ground truth estimate to directly infer @xmath25 used in the hmms described previously .      4.0 cm    ( 4.0,3.5 ) ( 0.0,0.0 )    4.0 cm    ( 4.0,3.5 ) ( 0.0,0.0 )",
    "before we can evaluate our methodologies , we must first verify that our method for generating ground truth volume estimates is accurate .",
    "we can compare a static volume measurement with a scale to static estimates from the thermal camera combined with the model - based method to gauge the accuracy of our method .",
    "figure [ fig : thermal_verification ] shows a comparison between measurements from a scale ( x - axis ) and the corresponding measurement from the thermal camera with model ( y - axis ) for each of the three target containers .",
    "the black dashed line shows a 1:1 correspondence for reference . from the figure it is clear that the model - based method overestimates the volume for each container . in order to make our baseline as accurate as possible ,",
    "we fit a linear model for each container and use that to calibrate the baseline ground truth estimates described in section [ sec : get_gt ] .",
    "next we must verify that the neural network we trained to labels pixels as _ liquid _ or _ not - liquid _ from color images is accurate enough to utilize for volume estimation .",
    "our prior work @xcite showed that neural networks can label liquid pixels in an image reasonably well on data generated by a realistic liquid simulator , and so we expect that this will carry over to the data we collected for these experiments .",
    "we trained the recurrent lstm cnn using the mini - batch gradient descent method adam @xcite with a learning rate of 0.0001 and default momentum values , for 61,000 iterations .",
    "we unrolled the recurrent network during training for 32 frames and used a batch size of 5 .",
    "we scaled the input images to @xmath40 resolution .",
    "the error signal was computed using softmax loss . as in @xcite",
    ", we found the best results are achieved when we first pre - train the network on crops of liquid in the images , and then train on full images .",
    "figure [ fig : detection_verification ] shows the performance of the detection network , and the image in figure [ fig : detection_example ] shows an example of the output of the network .",
    "these results clearly show that our detection network is able to classify pixels with high precision and recall .",
    "this suggests that the network will work well for estimating the volume of liquid in the target container .",
    "we should note , however , that due to the relatively small size of the training set , this detection network will work well only for the tasks described in this paper and will not generalize to other environments or tasks .",
    "2.4 cm    ( 2.0,3.5 ) ( 0.0,0.1 )    6.0 cm    ( 6.0,3.5 ) ( 0.0,0.0 )    3.0 cm    ( 3.0,2.7 ) ( 0.0,0.0 )    2.7 cm    ( 2.7,2.7 ) ( 0.0,0.0 )    2.7 cm    ( 2.7,2.7 ) ( 0.0,0.0 )    for our model - free methodology , every network was trained using the mini - batch gradient descent method adam @xcite with a learning rate of 0.0001 and default momentum values .",
    "each network was trained for 61,000 iterations , at which point performance tended to plateau .",
    "all single - frame networks were trained using a batch size of 32 ; all multi - frame networks with a window of 32 and batch size of 5 ; and all lstm networks with a batch size of 5 and unrolled for 32 frames during training .",
    "the input to each network was a @xmath41 resolution crop of either the liquid detections only , the color image and detections appended channel - wise , or the depth image and detections appended channel - wise .",
    "we discretize the output to 100 values for the range of 0 to 400ml ( none of our experiments use volumes greater than 400ml ) and train the network to classify the volume .",
    "the error signal was computed using the softmax with loss layer built into caffe @xcite . in our data we noticed that approximately @xmath42 of the time during each pouring sequence was spent either before or after pouring had occurred , with little change in the volume .",
    "we found that the best results could be achieved by first pre - training each network on data from the middle of each sequence during which the volume was actively changing , and then training on data sampled from the entire sequence .",
    "we discretize @xmath2 and the output of the network into 20 values and compute @xmath26 used in the model - free method for all @xmath43 and @xmath24 from the output of the networks on the data ( we compute a separate observation probability distribution for each network ) .",
    "figure [ fig : aggregate_all ] shows the root mean squared error in milliliters on the testing data for each method with respect to our baseline ground truth comparison described in section [ sec : get_gt ] .",
    "it should be noted that although both our baseline ground truth estimate and the thermal estimate in figure [ fig : model_based ] are derived from the same data , the difference between the two can be largely attributed to the fact that the baseline method is able to look backwards in time and adjust its estimates , whereas the thermal model - based method can only look forward ( which is necessary for control ) .",
    "for example , in the initial frames of a pour , as the water leaves the source container , it can splash against the side of the target container , causing the forward thermal estimate to incorrectly estimate a spike in the volume of liquid , whereas the baseline method can smooth this spike by propagating backwards in time .",
    "while the error for both model - based methods are relatively small , it is clear that many of the model - free methods are actually better able to estimate the volume of liquid in the target container . surprisingly , the best performing model - free estimation network is the multi - frame network that takes as input only the pixel - wise liquid detections from the detection network .",
    "the networks trained on detections only are the only networks that receive no shape information about the target container ( both the depth and color images contain some information about shape ) , so intuitively , it would be expected that they would be unable to estimate the volume of more than a single container , and thus perform more poorly than the other networks .",
    "however , a lot of the temporal and perceptual information used by our methodology is already provided in the pixel - wise liquid detections , thus temporal information in addition to either color or depth images are not as beneficial to the networks .",
    "we can verify that this is indeed the case by looking at the volume estimates on randomly selected pouring sequences from the test set , one for each target container .",
    "figure [ fig : exs ] shows the volume estimates for the two model - based methods and the multi - frame detection only method as compared to the baseline .",
    "it is clear from the plots that the multi - frame network is better able to match the baseline ground truth than either of the model - based methods .",
    "not only does the multi - frame network outperform the model - based methods , but unlike them , it does not require either an expensive thermal camera or a model of the target container . for these reasons ,",
    "we utilize this method in the next section for carrying out actual pouring experiments with closed - loop visual feedback .",
    "( 7.0,5.5 ) ( 0.0,0.0 )    2.5 cm    2.5 cm    2.5 cm    2.5 cm    2.5 cm    2.5 cm    estimating the volume _ a posteriori _ and using a volume estimator as input to a pouring controller are two very different problems",
    ". a volume estimation method may work well analyzing the data after the pouring is finished , but that does not necessarily mean it is suitable for control .",
    "for example , if the estimator outputs an erroneous value at one timestep , it may be able to correct in the next since the trajectory of the pour does not change .",
    "however , if this happens during a pour and the estimator outputs an erroneous value , this may result in a negative feedback loop in which the trajectory deviates more and more from optimal , leading to more erroneous volume estimates , etc . to verify that our chosen method from the previous section is actually suitable for control",
    ", we need to execute it on a real robot for real - time control .    to test the multi - frame network with detections only , we executed 30 pours on the real robot using the pid controller described in section [ sec : robot_controller ] .",
    "we ran 10 sequences on each of the three target containers . for each sequence , we randomly selected a target volume in @xmath44 milliliters and we randomly initialized the volume of water in the source container as either 300 , 350 , or 400 milliliters , always ensuring at least a 100ml difference between the starting amount in the source and the target amount ( so the robot can not simply dump out the entire source and call it a success ) .",
    "each pour lasted exactly 25 seconds , and we evaluated the robot based on the actual amount of liquid in the target container ( as measured by a scale ) after the pour was finished .",
    "figure [ fig : control_points ] shows a plot of each pour , where the x - axis is the target amount and the y - axis is the actual volume of liquid in the target container after the pour finished .",
    "note that the robot performs approximately the same on all containers .",
    "this is particularly interesting since the volume estimation network is never given any information about the target container , and must simply infer it based on the motion of the liquid .",
    "additionally , almost all of the 30 pours were within 50ml of the target .",
    "in fact , the average error over all the pours was 38ml . for reference , figure",
    "[ fig : refrence_amounts ] shows 50ml differences for each of our 3 containers from the robot s perspective . as is apparent from this figure ,",
    "50ml is a small amount , and a human solving the same task would be expected to have a similar error .",
    "in this paper , we introduce a framework for visual closed - loop control for pouring specific amounts of liquid into a container . to provide real - time estimation of the amount of liquid poured so far ,",
    "we develop a deep network structure that first detects the presence of water in individual pixels of color videos and then estimates the volume based on these detections . to generate the data and labels required to train the deep networks , we collect training videos with heated water observed by both an rgb - d camera and a calibrated thermal camera .",
    "a model - based approach allows us then to estimate the volume of liquid in a container based on the pixel - level water detections .",
    "our experiments indicate that the deep network architecture can be trained to provide real - time estimates from color only data that are slightly better than the model - based estimates using thermal imagery .",
    "furthermore , once trained on multiple containers , our volume estimator does not require a matched shape model of the target container any more .",
    "we incorporated our approach into a pid controller and found that it on average only missed the target amount by 38ml .",
    "while this is not accurate enough for some applications ( e.g. , some industrial settings ) , it is well suited for similar pouring tasks in standard home environments and is on par with what a human would be expected to do in a similar setting . to our knowledge , this is the first work that has combined visual feedback with control in order to pour specific amounts of liquids into everyday containers .",
    "this work opens up various directions for future research .",
    "one important avenue is to develop methods to improve the robustness of the neural networks .",
    "currently , because of the limited size of our dataset , the networks only work properly on the three target containers they were trained on .",
    "we believe that the networks will be able to generalize to arbitrary containers when trained on sufficiently many examples , but this still has to be shown .",
    "other interesting directions include generalization to different liquids , such as pouring a glass of soda or a cup of coffee , representing target amounts in _ relative _ terms , such as in `` pour me a half cup of water '' , and more sophisticated control schemes on top of our perception ."
  ],
  "abstract_text": [
    "<S> pouring a specific amount of liquid is a challenging task . in this paper </S>",
    "<S> we develop methods for robots to use visual feedback to perform closed - loop control for pouring liquids . </S>",
    "<S> we propose both a model - based and a model - free method utilizing deep learning for estimating the volume of liquid in a container . </S>",
    "<S> our results show that the model - free method is better able to estimate the volume . </S>",
    "<S> we combine this with a simple pid controller to pour specific amounts of liquid , and show that the robot is able to achieve an average 38ml deviation from the target amount . to our knowledge , this is the first use of raw visual feedback to pour liquids in robotics . </S>"
  ]
}