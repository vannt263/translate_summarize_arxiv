{
  "article_text": [
    "the optimization problem of finding the subset of a set of imperfect devices that results in the best aggregate device was recently introduced by challet and johnson  @xcite .",
    "it is an abstraction of what will likely be a major difficulty in the construction of systems using nano - scale components , arising when a large fraction of those components may be faulty , so that we can not use a scheme fixed ahead of time for interconnecting them .",
    "this abstraction is a computationally hard optimization problem ; brute force approaches can not be used for large instances of it .",
    "it is also particularly well - suited to testing distributed optimization algorithms .",
    "we propose addressing this problem by associating each device with an adaptive reinforcement - learning ( rl ) agent  @xcite ) that decides whether or not its device will be a member of the subset .",
    "it makes this decision based on its estimate of which choice will give a larger value of its associated * private utility * function which maps the joint choice of all agents into the reals .",
    "for such an approach to work , we must both ensure that the agents do not work at cross - purposes , and that each one has a tractable learning problem to solve .",
    "typically these two desiderata conflict with one another ( utilities that account for whether your action is at cross - purposes to any other agents actions are very complex and therefore difficult to optimize ) .",
    "so we must find the best way to trade them off each other .",
    "a * collective * is any such system containing utility - maximizing agents , together with an overall * world utility * function that rates the possible configurations of the overall system .",
    "the associated design problem is how to configure the collective  and in particular how to set the private utility functions  to best optimize the world utility .",
    "this design problem is related to work in many other fields , including multi - agent systems ( mas s ) , computational economics , mechanism design , reinforcement learning , statistical mechanics , computational ecologies , ( partially observable ) markov decision processes and game theory .",
    "however none of these fields is both applicable in large problems , and directly addresses the _",
    "general _ design problem , rather than a special instance of it .",
    "( see @xcite for a detailed discussion of the relationship between these fields , involving hundreds of references . ) in particular , since detailed modeling of extremely large real - world systems is usually impossible , it is crucial to use design algorithms that tdo not employ such modeling .",
    "we need to make our design leveraging only the simple assumption that the agents learning algorithms are individually reasonably behaved .",
    "recently some advances have been made in doing just that  @xcite .",
    "it is those advances that we propose to apply to the problem of challet and johnson .",
    "let @xmath0 be an arbitrary space whose elements @xmath1 give the joint move of all agents in the collective system .",
    "we wish to search for the @xmath1 that maximizes the provided world utility @xmath2 .",
    "in addition to @xmath3 we are concerned with private utility functions \\{@xmath4 } , one such function for each agent @xmath5 controlling @xmath6 .",
    "we use the notation @xmath7 to refer to all agents other than @xmath5 .    our uncertainty concerning the state of the system",
    "is reflected in a probability distribution over @xmath0 . our ability to control the system consists of setting the value of some characteristic of the collective , e.g. , setting the private utility functions of the agents .",
    "indicating that value of the * global coordinate * by @xmath8 , our analysis revolves around the following * central equation * for @xmath9 , which follows from bayes theorem :    @xmath10    where @xmath11 and @xmath12 are the * intelligence * vectors of the agents with respect to @xmath4 and @xmath3 , respectively .",
    "intelligence , defined as the `` standardization '' of utility functions so that the numeric value they assign to a @xmath1 only reflects their ranking of @xmath1 relative to certain other elements of @xmath0 , is given by : @xmath13 \\ ; , \\end{aligned}\\ ] ]    where @xmath14 is the heaviside function , and where the subscript on the ( normalized ) measure @xmath15 indicates it is restricted to @xmath16 sharing the same non-@xmath5 components as @xmath1 .    note that @xmath17 means that agent @xmath5 is fully rational at @xmath1 , in that its move maximizes the value of its utility , given the moves of the agents .",
    "in other words , a point @xmath1 where @xmath17 for all agents @xmath5 is one that meets the definition of a game - theory nash equilibrium . on the other hand , a @xmath1 at which all components of @xmath18 is a maximum @xmath3 along all coordinates of @xmath1 ( which of course does not mean it is a maximum along an off - axis direction ) .",
    "so if we can get these two points to be identical , then if the agents do well enough at maximizing their private utilities we are assured we will be near an axis - maximizing point for @xmath3 .    to formalize this , consider our decomposition of @xmath9 .",
    "if we can choose @xmath8 so that the third conditional probability in the integrand is peaked around vectors @xmath11 all of whose components are close to 1 , then we have likely induced large ( private utility function ) intelligences .",
    "intuitively , this ensures that the private utility functions have high `` signal - to - noise '' .",
    "if we can also have the second term be peaked about @xmath12 equal to @xmath11 , then @xmath12 will also be large .",
    "it is in the second term that the requirement that the private utility functions be `` aligned with @xmath3 '' arises .",
    "note that our desired form for the second term in equation  [ eq : central ] is assured if we have chosen private utilities such that @xmath11 equals @xmath12 exactly for all @xmath1 .",
    "such a system is said to be * factored*. finally , if the first term in the integrand is peaked about high @xmath3 when @xmath12 is large , then our choice of @xmath8 will likely result in high @xmath3 , as desired . in this letter",
    "we concentrate on the second and third terms , and show how to simultaneously set them to have the desired form .    as an example , any `` team game '' in which all private utility functions equal @xmath3 is factored  @xcite .",
    "however team games often have very poor forms for term 3 in equation  [ eq : central ] , forms which get progressively worse as the size of the collective grows .",
    "this is because for such private utility functions each agent @xmath5 will usually confront a very poor `` signal - to - noise '' ratio in trying to discern how its actions affect its utility @xmath19 , since so many other agent s actions also affect @xmath3 and therefore dilute @xmath5 s effect on its own private utility function .",
    "we now focus on algorithms based on private utility functions \\{@xmath4 } that optimize the signal / noise ratio reflected in the third term , subject to the requirement that the system be factored . to understand how these algorithms work , say we are given an arbitrary function @xmath20 over agent @xmath5 s moves , two such moves @xmath21 and @xmath22 , a utility @xmath23 , a value @xmath8 of the global coordinate , and a move by all agents other than @xmath5 , @xmath24 .",
    "define the associated * learnability * by @xmath25 ^ 2 } { \\int dz_\\eta [ f(z_\\eta)var(u ; z_{\\;\\hat{}\\;\\eta } , z_\\eta ) ] } } \\ ;   .",
    "\\label{eq : learnability}\\end{aligned}\\ ] ] the expectation values in the numerator are formed by averaging over the training set of the learning algorithm used by agent @xmath5 , @xmath26 .",
    "those two averages are evaluated according to the two distributions @xmath27 and @xmath28 , respectively .",
    "( that is the meaning of the semicolon notation . ) similarly the variance being averaged in the denominator is over @xmath26 according to the distribution @xmath29 .",
    "the denominator in equation  [ eq : learnability ] reflects how sensitive @xmath30 is to changing @xmath24 .",
    "in contrast , the numerator reflects how sensitive @xmath30 is to changing @xmath31 .",
    "so the greater the learnability of a private utility function @xmath4 , the more @xmath32 depends only on the move of agent @xmath5 , i.e. , the better the associated signal - to - noise ratio for @xmath5 . intuitively then , so long as it does not come at the expense of decreasing the signal , increasing the signal - to - noise ratio specified in the learnability will make it easier for @xmath5 to achieve a large value of its intelligence .",
    "this can be established formally : if appropriately scaled , @xmath33 will result in better expected intelligence for agent @xmath5 than will @xmath4 whenever @xmath34 for all pairs of moves @xmath35@xcite .",
    "one can solve for the set of all private utilities that are factored with respect to a particular world utility .",
    "unfortunately though , in general a collective can not both be factored and have infinite learnability for all of its agents  @xcite .",
    "however consider * difference * utilities , of the form @xmath36\\end{aligned}\\ ] ] any difference utility is factored  @xcite .",
    "in addition , for all pairs @xmath37 , under benign approximations the difference utility maximizing @xmath38 by @xmath39 up to an overall additive constant , where the expectation value is over @xmath6 .",
    "we call the resultant difference utility the * aristocrat * utility ( @xmath40 ) , loosely reflecting the fact that it measures the difference between a agent s actual action and the average action .",
    "if each agent @xmath5 uses an appropriately rescaled version of the associated @xmath40 as its private utility function , then we have ensured good form for both terms 2 and 3 in equation  [ eq : central ] .",
    "using @xmath40 in practice is sometimes difficult , due to the need to evaluate the expectation value .",
    "fortunately there are other utility functions that , while being easier to evaluate than @xmath40 , still are both factored and possess superior learnability to the team game utility , @xmath19 .",
    "one such private utility function is the * wonderful life * utility ( wlu ) .",
    "the @xmath41 for agent @xmath5 is parameterized by a pre - fixed * clamping parameter * @xmath42 chosen from among @xmath5 s possible moves : @xmath43 wlu is factored no matter what the choice of clamping parameter . furthermore",
    ", while not matching the high learnability of @xmath40 , @xmath41 usually has far better learnability than does a team game , and therefore ( when appropriately scaled ) results in better expected intelligence @xcite .",
    "we now explore the use of collective - based techniques for the problem of combining imperfect objects of challet and johnson .",
    "the canonical version of this problem arises when the objects are all noisy observational devices producing a single real number by sampling a gaussian of fixed width centered on the true value of the number .",
    "the problem is to choose the subset of a fixed collection of such devices to have the average ( over the members of the subset ) distortion as close to zero as possible .",
    "formally , the problem is to minimize @xmath44 where @xmath45 is whether device @xmath46 is or is not selected , and there are @xmath47 devices in the collection , having associated distortions @xmath48 .",
    "we identify @xmath49 with the world utility , @xmath3 ( so that for these experiments , the goal is to minimize @xmath3 , not maximize it ) .",
    "there are @xmath47 individual agents , each setting one of the @xmath50 .",
    "the goal is to give those agents private utilities so that , as they learn to maximize their private utilities , the maximizer of @xmath3 is found .    because we wished to concentrate on the effects of the utilities rather than on the rl algorithms that use them , the agents all used the same ( very ) simple rl algorithm .",
    "( we would expect that even marginally more sophisticated rl algorithms would give better performance . ) at each timestep each agent runs its algorithm on a data set it maintains of action - utility pairs to choose what action to take .",
    "this gives a joint action , which in turn sets the private utility value for each agent .",
    "combined with what action it took at that timestep , that utility value for agent @xmath46 then gets added to the data set maintained by agent @xmath46 .",
    "this is done for all agents and then the process repeats .",
    "the agents used their data sets to choose moves by maintaining a 2-dimensional vector whose components at a given timestep are the agent s estimates of the utility it would receive for taking each of its two possible move .",
    "each agent @xmath46 picks its action at a timestep by sampling a boltzmann distribution whose over the `` energy spectrum '' of @xmath46 s two utility estimates at that time . for simplicity ,",
    "given how short our runs were , the temperature in the boltzmann distribution did not decay in time .",
    "however to reflect the fact that the environment in which an agent is operating changes with time ( as the other agents change their moves ) , and therefore the optimal action changes in time , the two utility estimates are formed using exponentially aged data : for any time step @xmath51 , the utility estimate @xmath46 uses for setting either of the two actions @xmath50 is a weighted average of all the utility values it has received at previous times @xmath52 that it chose that action , with the weights in the average given by an exponential of the values @xmath53 .",
    "finally , to form the agents initial data sets , there is an initialization period in which all actions by all agents are chosen uniformly randomly , with no learning used .",
    "it is after this initialization period ends that the agents choose their actions according to the associated boltzmann distributions .    for all learning algorithms ,",
    "the first 20 time steps constitute the data set initialization period ( note that all learning algorithms must `` perform '' the same during that period , since none are actually in use then ) .",
    "starting at @xmath54 , with each consecutive timestep a fixed fraction of the agents still choosing their actions randomly switch to using their learner algorithms instead , while others continue to take random actions .",
    "this gradual introduction of the learning algorithms is intended to soften the `` discontinuity '' in each agent s environment when the behavior of the other agents start using their learning algorithms and therefore change their moves .",
    "for @xmath55 , four agents turned on their learning algorithms at each time step ( i.e. , by @xmath56 , all agents were using their learners ) .",
    "figure  [ fig : n=100 ] shows the convergence properties of different algorithms in a system with 100 agents .",
    "the results reported are based on 20 different @xmath48 configurations , each performed 50 times ( i.e. , each point on the figure is the average of @xmath57 runs ) .",
    "@xmath3 , @xmath40 , and @xmath41 show the performance of agents using reinforcement learners with those reinforcement signals provided by @xmath3 ( team game ) , aristocrat utility and wonderful life utility respectively .",
    "@xmath58 shows the performance of greedy search where new @xmath50 s are generated at each step and selected if the solution is better than the current best solution . because the runs are only 200 timesteps long , algorithms such as simulated annealing do not outperform simple search : there is simply no time for an annealing schedule .    note that because of the discontinuity in the environment experienced by each agent as the other agents turn on their learning algorithms , the performance of the system as a whole as the agents turn on their learning may degrade initially , before settling down ( e.g. , wlu ) .",
    "qualitatively , systems in which agents use the @xmath3 utility have a difficult time learning , while systems in which agents use @xmath40 perform best and the search algorithm falls between the performance of the two .",
    "we also investigated the scaling properties of each algorithm .",
    "figure  [ fig : scalew ] shows these results ( the @xmath59 average performance over 1000 runs ) along with the associated error bars . as @xmath47 grows",
    "two competing factors come into play . on the one hand , there are more degrees of freedom to use to minimize @xmath3 . on the other hand ,",
    "the problem becomes more difficult : the search space gets larger for @xmath58 , and there is more noise in the system for the learning algorithms . to account for these effects and calibrate the performance values as @xmath47 varies , in the figure we also provide the performance of the algorithm that randomly selects its action ( `` ran '' ) .",
    "note that the difference between the performances of @xmath58 and @xmath40 increases when the system size increases , up to a factor of twenty for @xmath60 .",
    "all algorithms but @xmath40 have slopes similar to that of `` ran '' , demonstrating that they can not use the additional degrees of freedom provided by the larger @xmath47 . only @xmath40 effectively uses the new degrees of freedom , providing gains that are proportionally higher than the other algorithms ( i.e. , the rate at which @xmath40 s performance improves outpaces what is `` expected '' based on the random algorithm s performance ) .",
    "finally , note that many search algorithms ( e.g. , gradient ascent , simulated annealing , genetic algorithms ) can be viewed as collectives .",
    "however conventionally such algorithms use very `` dumb '' agents .",
    "in particular , in exploration - exploitation algorithms , the agents typically make random moves in the exploration step rather than rl to choose the best move .",
    "preliminary results indicate that using rl - based agents to determine the moves in such exploration steps  intuitively , `` agentizing '' the individual variables of a search problem by providing them with adaptive intelligence  can lead to significantly better solutions than are achieved with conventional `` dumb variable '' exploration steps in a myriad of optimization problems  @xcite .",
    "r.  h. crites and a.  g. barto . improving elevator performance using reinforcement learning . in d.",
    "s. touretzky , m.  c. mozer , and m.  e. hasselmo , editors , _ advances in neural information processing systems - 8 _ , pages 10171023 . mit press , 1996 ."
  ],
  "abstract_text": [
    "<S> in this letter we summarize some recent theoretical work on the design of collectives , i.e. , of systems containing many agents , each of which can be viewed as trying to maximize an associated * private utility * , where there is also a * world utility * rating the behavior of that overall system that the designer of the collective wishes to optimize . </S>",
    "<S> we then apply algorithms based on that work on a recently suggested testbed for such optimization problems  @xcite . </S>",
    "<S> this is the problem of finding the combination of imperfect nano - scale objects that results in the best aggregate object . </S>",
    "<S> we present experimental results showing that these algorithms outperform conventional methods by more than an order of magnitude in this domain . </S>"
  ]
}