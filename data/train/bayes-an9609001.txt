{
  "article_text": [
    "conditional independence graphs are of vital importance in the structuring , understanding and computing of high dimensional complex statistical models . for a review of early work in this area ,",
    "see @xcite , the references and the discussion , and also @xcite .",
    "the above mentioned work is concerned with updating in discrete probability networks . for a discussion of updating in networks with continuous random variables ,",
    "see @xcite , for example . for",
    "a general overview of the theory of graphical models , see @xcite .",
    "also relevant to this paper is the work on graphical gaussian models .",
    "@xcite , @xcite and @xcite discuss the properties of such models .",
    "@xcite examine data propagation through a graphical gaussian network , and apply their results to a dynamic linear model ( dlm ) . here , the aim is to link the theory of local computation over graphical gaussian networks to the bayes linear framework for subjective statistical inference , and the many interpretive and diagnostic features associated with that methodology , in particular .",
    "in this paper , a bayes linear approach is taken to subjective statistical inference , making expectation ( rather than probability ) primitive .",
    "an overview of the methodology is given in @xcite .",
    "the foundations of the theory are quite general , and are outlined in the context of second - order exchangeability in @xcite , and discussed for more general situations in @xcite .",
    "bayes linear methods may be used in order to learn about any quantities of interest , provided only that a mean and variance specification is made for all relevant quantities , and a specification for the covariance between all pairs of quantities is made .",
    "no distributional assumptions are necessary .",
    "there are many interpretive and diagnostic features of the bayes linear methodology .",
    "these are discussed with reference to @xmath0 $ ]  ( the bayes linear computer programming language ) in @xcite .",
    "conventional graphical models are defined via strict probabilistic conditional independence @xcite . however , as @xcite demonstrates , all that is actually required is a tertiary operator @xmath1 satisfying some simple properties .",
    "any relation satisfying these properties is known as a _ generalised conditional independence _",
    "bayes linear graphical models are based on what @xcite refers to as _ weak conditional independence_. in this paper , the relation will be referred to as _ adjusted orthogonality _ , in order to emphasise the linear structure underlying the relation .",
    "bayes linear graphical models based upon the concept of adjusted orthogonality are described in @xcite . for completeness , and to introduce some notation useful in the context of local computation , the most important elements of the methodology are summarised here , and the precise form of the adjusted orthogonality relation is defined .    for vectors of random quantities , @xmath2 and @xmath3 ,",
    "define @xmath4 and @xmath5 .",
    "also , for any matrix , @xmath6 , @xmath7 represents the moore - penrose generalised inverse of @xmath6 .    for all vectors of random quantities @xmath8 and @xmath9 , define @xmath10}^{\\{b\\ } } } = & { \\mathrm{cov}_{}\\left(b , d\\right)}{\\mathrm{var}_{}\\left(d\\right)}{\\mbox{}^{\\dagger}}\\\\ { \\mathrm{t}_{[d]}^{\\{b\\ } } } = & { \\mathrm{p}_{[d]}^{\\{b\\}}}{\\mathrm{p}_{[b]}^{\\{d\\}}}\\end{aligned}\\ ] ]    these represent the fundamental operators of the bayes linear methodology .",
    "@xmath11}^{\\{b\\}}}$ ] is the operator which updates the expectation vector for @xmath8 based on the observation of @xmath9 , and @xmath12}^{\\{b\\}}}$ ] updates the variance matrix for @xmath8 based on observation of @xmath9 .",
    "local computation over bayes linear graphical models is made possible by local computation of these operators .",
    "for all vectors of random quantities @xmath8 , @xmath13 and @xmath9 , define @xmath14}^{\\{b\\}}}[d-{\\mathrm{e}_{}\\left(d\\right ) } ] \\label{eq : exbd}\\\\   { \\mathrm{cov}_{d}\\left(b , c\\right ) } = & { \\mathrm{cov}_{}\\left(b-{\\mathrm{e}_{d}\\left(b\\right)},c-{\\mathrm{e}_{d}\\left(c\\right)}\\right ) } \\label{eq : covdbc}\\end{aligned}\\ ] ]    @xmath15 is the _ expectation for @xmath8 adjusted by @xmath9_. it represents the linear combination of a constant and the components of @xmath9 _ closest to _ @xmath8 in the sense of expected squared loss .",
    "it corresponds to @xmath16 when @xmath8 and @xmath9 are jointly multivariate normal .",
    "@xmath17 is the _ covariance between @xmath8 and @xmath13 adjusted by @xmath9 _ , and represents the covariance between @xmath8 and @xmath13 given observation of @xmath9 .",
    "it corresponds to @xmath18 when @xmath8 , @xmath13 and @xmath9 are jointly multivariate normal .    for all vectors of random quantities",
    "@xmath8 , @xmath13 and @xmath9 @xmath19}^{\\{c\\}}}{\\mbox{}^\\mathrm{t}}\\label{eq : covdbc2}\\\\ { \\mathrm{var}_{d}\\left(b\\right ) } = & ( { \\mathrm{i}}-{\\mathrm{t}_{[d]}^{\\{b\\}}}){\\mathrm{var}_{}\\left(b\\right)}\\label{eq : vardb2}\\end{aligned}\\ ] ]    substituting into we get @xmath20}^{\\{c\\}}}d\\right)}\\end{aligned}\\ ] ] which gives , and replacing @xmath13 by @xmath8 gives .    note that shows that @xmath12}^{\\{b\\}}}$ ] is responsible for the updating of variance matrices . adjusted orthogonality is now defined .",
    "[ def : ci ] for random vectors @xmath8 , @xmath13 and @xmath9 @xmath21    @xcite shows that this relation does indeed define a generalised conditional independence property , and hence that all the usual properties of graphical models based upon such a relation hold .",
    "@xcite defines a bayes linear influence diagram based upon the adjusted orthogonality relation . @xcite",
    "illustrate the use of bayes linear influence diagrams in a multivariate forecasting problem .",
    "relevant graph theoretic concepts can be found in the appendix of @xcite .",
    "the terms _ moral graph _ and _ junction tree _ are explained in @xcite .",
    "briefly , an undirected moral graph is formed from a directed acyclic graph by _ marrying _ all pairs of parents of each node , by adding an arc between them , and then dropping arrows from all arcs .",
    "a junction tree is the _ tree _ of _ cliques _ of a _ triangulated _ moral graph .",
    "a tree is a graph without any cycles .",
    "a graph is triangulated if no cycle of length at least four is without a chord .",
    "a clique is a maximally connected subset of a triangulated graph .    in this",
    "paper , attention will focus on undirected graphs .",
    "an undirected graph consists of a collection of nodes @xmath22 for some @xmath23 , together with a collection of undirected arcs . every pair of nodes , @xmath24 is joined by an undirected arc unless @xmath25 . here ,",
    "the standard set theory notation , @xmath26 is used to mean the set of elements of @xmath8 which are not in @xmath6 .",
    "an undirected graph may be obtained from a bayes linear influence diagram by forming the moral graph of the influence diagram in the usual way .",
    "in fact , _ local computation _ ( the computation of global influences of particular nodes of the graph , using only information local to adjacent nodes ) requires that the undirected graph representing the conditional independence structure is a tree . this tree may be formed as the junction tree of a triangulated moral graph , or better , by grouping together related variables `` by hand '' in order to get a tree structure for the graph .",
    "for the rest of this paper , it will be assumed that the model of interest is represented by an undirected tree defined via adjusted orthogonality .",
    "[ lem : covbc ] if @xmath8 , @xmath13 and @xmath9 are random vectors such that @xmath27 , then @xmath28}^{\\{c\\}}}{\\mbox{}^\\mathrm{t}}\\ ] ]    this follows immediately from definition [ def : ci ] and .",
    "[ lem : covxyz ] if @xmath2 , @xmath3 and @xmath29 are random vectors such that @xmath30 , then @xmath31}^{\\{y\\}}}){\\mathrm{cov}_{}\\left(y , z\\right)}\\end{aligned}\\ ] ]    from @xmath32}^{\\{y\\}}}{\\mathrm{cov}_{}\\left(x , y\\right)}{\\mathrm{p}_{[y]}^{\\{z\\}}}{\\mbox{}^\\mathrm{t}}\\quad \\mathrm{by\\ lemma\\ \\ref{lem : covbc}}\\end{aligned}\\ ] ] and the result follows .",
    "[ thm : lc ] if @xmath2 , @xmath3 and @xmath29 are random vectors such that @xmath30 , then @xmath33}^{\\{z\\ } } } = & { \\mathrm{p}_{[y]}^{\\{z\\}}}{\\mathrm{p}_{[x]}^{\\{y\\ } } } \\label{eq : projxz } \\\\ { \\mathrm{t}_{[x]}^{\\{z\\ } } } = & { \\mathrm{p}_{[y]}^{\\{z\\}}}{\\mathrm{t}_{[x]}^{\\{y\\}}}{\\mathrm{p}_{[z]}^{\\{y\\ } } } \\label{eq : transfxz}\\end{aligned}\\ ] ]    @xmath33}^{\\{z\\ } } } = & { \\mathrm{cov}_{}\\left(z , x\\right)}{\\mathrm{var}_{}\\left(x\\right)}{\\mbox{}^{\\dagger}}\\\\ = & { \\mathrm{cov}_{}\\left(z , y\\right)}{\\mathrm{p}_{[y]}^{\\{x\\}}}{\\mbox{}^\\mathrm{t}}{\\mathrm{var}_{}\\left(x\\right)}{\\mbox{}^{\\dagger}}\\quad \\mathrm{by\\ lemma\\    \\ref{lem : covbc}}\\end{aligned}\\ ] ]    which gives .",
    "also @xmath34}^{\\{z\\ } } } = & { \\mathrm{p}_{[x]}^{\\{z\\}}}{\\mathrm{p}_{[z]}^{\\{x\\ } } } \\\\ = & { \\mathrm{cov}_{}\\left(z , x\\right)}{\\mathrm{var}_{}\\left(x\\right)}{\\mbox{}^{\\dagger}}{\\mathrm{cov}_{}\\left(x , z\\right)}{\\mathrm{var}_{}\\left(z\\right)}{\\mbox{}^{\\dagger}}\\\\ = & { \\mathrm{cov}_{}\\left(z , y\\right)}{\\mathrm{p}_{[y]}^{\\{x\\}}}{\\mbox{}^\\mathrm{t}}{\\mathrm{var}_{}\\left(x\\right)}{\\mbox{}^{\\dagger}}{\\mathrm{cov}_{}\\left(x , y\\right)}{\\mathrm{p}_{[y]}^{\\{z\\}}}{\\mbox{}^\\mathrm{t}}{\\mathrm{var}_{}\\left(z\\right)}{\\mbox{}^{\\dagger}}\\end{aligned}\\ ] ] which gives .",
    "theorem [ thm : lc ] contains the two key results which allow local computation over bayes linear belief networks .",
    "the implications of theorem [ thm : lc ] to bayes linear trees should be clear from examination of figure [ fig : path ] . to examine the effect of observing node @xmath2 , it is sufficient to compute the operators @xmath35}^{\\{z\\}}}$ ] and @xmath36}^{\\{z\\}}}$ ] for every node , @xmath29 on the graph , since these operators contain all necessary information about the adjustment of @xmath29 by @xmath2 .",
    "there is a unique path from @xmath2 to @xmath29 which is shown in figure [ fig : path ] .",
    "the direct predecessor of @xmath29 is denoted by @xmath3 .",
    "note further that it is a property of the graph that @xmath30 .",
    "further , by theorem [ thm : lc ] , the transforms @xmath35}^{\\{z\\}}}$ ] and @xmath36}^{\\{z\\}}}$ ] can be computed using @xmath35}^{\\{y\\}}}$ ] and @xmath36}^{\\{y\\}}}$ ] together with information local to nodes @xmath3 and @xmath29 .",
    "this provides a recursive method for the calculation of the transforms , which leads to the algorithm for the propagation of transforms throughout the tree , which is described in the next section .",
    "consider a tree with nodes @xmath37 for some @xmath23 .",
    "each node , @xmath38 , represents a vector of random quantities .",
    "it also has an edge set @xmath39 , where each @xmath40 is of the form @xmath41 for some @xmath42 .",
    "the resulting tree should represent a conditional independence graph over the random variables in question .",
    "it is assumed that each node , @xmath38 has an expectation vector @xmath43 and variance matrix @xmath44 associated with it .",
    "it is further assumed that each edge , @xmath45 has the covariance matrix , @xmath46 associated with it .",
    "this is the only information required in order to carry out bayes linear local computation over such structures .",
    "now consider the effect of adjustment by the vector @xmath2 , which consists of some or all of the components of node @xmath47 for some @xmath48 .",
    "then , starting with node @xmath47 , calculate and store @xmath49}^{\\{b_j\\}}}$ ] and @xmath50}^{\\{b_j\\}}}$ ] . then , for each node @xmath51 calculate and store @xmath52 and @xmath53 , then for each node @xmath54 , do the same , using theorem [ thm : lc ] to calculate @xmath55}^{\\{b_l\\}}}p_{b(k ) }",
    "\\\\ t_{b(l ) } = & { \\mathrm{p}_{[b_k]}^{\\{b_l\\}}}t_{b(k)}{\\mathrm{p}_{[b_l]}^{\\{b_k\\}}}\\end{aligned}\\ ] ] in this way , recursively step outward through the tree , at each stage computing and storing the transforms using the transforms from the predecessor and the variance and covariance information over and between the current node and its predecessor .",
    "once this process is completed , associated with every node , @xmath56 , there are matrices @xmath57}^{\\{b_i\\}}}$ ] and @xmath58}^{\\{b_i\\}}}$ ] .",
    "these operators represent all information about the adjustment of the structure by @xmath2 .",
    "note however , that @xmath2 has not yet been observed , and that expectations , variances and covariances associated with nodes and edges have not been updated .",
    "it is a crucial part of the bayes linear methodology that _ a priori _ analysis of the model takes place , and that the expected influence of potential observables is examined .",
    "examination of the eigen structure of the belief transforms associated with nodes of particular interest is the key to understanding the structure of the model , and the benefits of observing particular nodes .",
    "it is important from a design perspective that such analyses can take place before any observations are made .",
    "see @xcite for a more complete discussion of such issues , and @xcite for a discussion of the technical issues it raises .",
    "after observation of @xmath59 , updating of the expectation , variance and covariance structure over the tree is required .",
    "start at node @xmath47 and calculate @xmath60 ( using [ eq : vardb2 ] ) .",
    "replace @xmath61 by @xmath62 and @xmath63 by @xmath64 .",
    "then for each @xmath65 do the same , and also update the arc between @xmath47 and @xmath66 by calculating @xmath67 ( using lemma [ lem : covxyz ] ) , and replacing @xmath68 by @xmath69 .",
    "again , step outwards through the tree , updating nodes and edges using the transforms previously calculated .",
    "once the expectations , variances and covariances over the structure have been updated , the tree should be pruned .",
    "if the adjusting node was completely observed ( _ i.e. _ @xmath70 ) , then @xmath47 should be removed from @xmath8 , and @xmath39 should have any arcs involving @xmath47 removed .",
    "further , leaf nodes and their edges may always be dropped without affecting the conditional independence structure of the graph .",
    "this is important if a leaf node is partially observed and the remaining variables are unobservable and of little diagnostic interest , since it means that the whole node may be dropped after observation of its observable components .    if a non - leaf node is partially observed , or a leaf node is observed , but its remaining components are observable or of interest , then the graph itself should remain unaffected , but the expectation , variance and covariance matrices associated with the node and its arcs should have the observed ( and hence redundant ) rows and columns removed ( for reasons of efficiency  use of the moore - penrose generalised inverse ensures that no problems will arise if observed variables are left in the system ) .",
    "as data becomes available on various nodes , it should be incorporated into the tree one node at a time . for each node with observations , the transforms should be computed , and then the beliefs updated in a sequential fashion . the fact that such sequential updating provides a coherent method of adjustment is demonstrated in @xcite .",
    "diagnostics for bayes linear adjustments are a crucial part of the methodology , and are discussed in @xcite .",
    "it follows that for local computation over bayes linear networks to be of practical value , methodology must be developed for the local computation of bayes linear diagnostics such as the _ size _ , _ expected size _ and _ bearing _ of an adjustment .",
    "the bearing represents the magnitude and direction of changes in belief .",
    "the magnitude of the bearing , which indicates the magnitude of changes in belief , is known as the _ size _ of the adjustment .",
    "consider the observation of data , @xmath59 , and the partial bearing of the adjustment it induces on some node , @xmath71 . before observation of @xmath2 , record @xmath72 and @xmath73 . also calculate the cholesky factor , @xmath6 of @xmath74 , so that @xmath6 is lower triangular , and @xmath75 .",
    "once the observed value @xmath59 is known , propagate the revised expectations , variances and covariances through the bayes linear tree . the new value of @xmath76 will be denoted @xmath77 .",
    "now the quantity @xmath78 represents the adjusted expectation for an orthonormal basis , @xmath79 for @xmath71 with respect to the _ a priori _ beliefs , @xmath80 and @xmath74 .",
    "therefore , @xmath81 gives the coordinates of the _ bearing _ of the adjustment with respect to that basis .",
    "the size of the partial adjustment is given by @xmath82 where @xmath83 represents the euclidean norm .",
    "the expected size is given by @xmath84 and so the _ size ratio _ for the adjustment ( often of most immediate interest ) is given by @xmath85 a size ratio close to one indicates changes in belief close to what would be expected .",
    "a size ratio smaller than one indicates changes in belief of smaller magnitude than would have been anticipated _ a priori _ , and a size ratio bigger than one indicates changes in belief of larger magnitude than would have been expected _ a priori_. informally , a size ratio bigger than 3 is often taken to indicate a diagnostic warning of possible conflict between _ a priori _ belief specifications and the observed data .",
    "cumulative sizes and bearings may be calculated in exactly the same way , simply by updating several times before computing @xmath81 .",
    "however , to calculate the expected size of the adjustment , in order to compute the size ratio , the cumulative belief transform must be recorded and updated at each stage , using the fact that @xmath86}^{\\{b\\ } } } = & \\ { \\mathrm{i}}- ( { \\mathrm{i}}- { \\mathrm{t}_{[y/]}^{\\{b/\\}}})({\\mathrm{i}}- { \\mathrm{t}_{[x]}^{\\{b\\}}})\\end{aligned}\\ ] ] where @xmath87}^{\\{b/\\}}}$ ] represents the partial transform for @xmath8 by @xmath3 , with respect to the structure already adjusted by @xmath2 . in other words , @xmath88 minus",
    "the transforms at each stage multiply together to give @xmath88 minus the cumulative transform .",
    "see @xcite for a more complete discussion of the multiplicative properties of belief transforms .      to adjust the tree given data at multiple nodes",
    ", it would be inefficient to adjust the entire tree sequentially by each node in turn , if the nodes in question are `` close together '' . here again , ideas may be borrowed from theory for the updating of probabilistic expert systems .",
    "it is possible to propagate transforms and projections from each node for which adjustment is required , to a _",
    "strong root _ , and then propagate transforms from the strong root out to the rest of the tree .",
    "a strong root is a node which separates the nodes for which there is information , from as much as possible of the rest of the tree . in practice , there are many ways in which one can use the strong root in order to control information flow through the tree .",
    "an example of its use is given in section [ sec : nse ] .",
    "in this paper , attention has focussed exclusively on finite collections of quantities , and matrix representations for bayes linear operators .",
    "all of the theory has been developed from the perspective of pushing matrix representations of linear operators around a network .",
    "however , the bayes linear methodology may be formulated and developed from a purely geometric viewpoint , involving linear operators on a ( possibly infinite dimensional ) hilbert space .",
    "this is not relevant to practical computer implementations of the theory and algorithms  hence the focus on matrix formulations in this paper .",
    "however , from a conceptual viewpoint , it is very important , since one sometimes has to deal , in principle , with infinite collections of quantities , or probability measures over an infinite partition .",
    "in fact , all of the theory for local computation over bayes linear belief networks developed in this paper is valid for the local computation of bayes linear operators on an arbitrary hilbert space .",
    "consequently , the results may be interpreted geometrically , as providing a method of pushing linear operators around a bayes linear hilbert space network .",
    "a geometric form of theorem [ thm : lc ] is derived and utilised in @xcite .",
    "figure [ fig : dlm ] shows a bayes linear graphical tree model for the first four time points of a dynamic linear model .",
    "local computation will be illustrated using the example model , beliefs and data from @xcite .",
    "here , @xmath89 represents the vector @xmath90 from that paper .",
    "the model takes the form @xmath91 where @xmath92 , @xmath93 , @xmath94 , @xmath95 and the @xmath96 and @xmath97 are uncorrelated .",
    "first , the nodes and arcs shown in figure [ fig : dlm ] are defined .",
    "then the expectation and variance of each node is calculated and associated with each node , and the covariances between pairs of nodes joined by an arc is also computed , and associated with the arc .",
    "all of the expectations variances and covariances are determined by the model .",
    "for example , node @xmath98 has expectation vector @xmath99 and variance matrix @xmath100 associated with it .",
    "node @xmath101 has expectation vector @xmath102 and variance matrix @xmath103 associated with it .",
    "the arc between @xmath98 and @xmath101 has associated with it the covariance matrix @xmath104 .",
    "note that though the arc is undirected , the `` direction '' with respect to which the covariance matrix is defined is important , and needs also to be stored .",
    "the effect of the observation of @xmath98 on the tree structure will be examined , and the effect on the node @xmath105 in particular , which has _ a priori _ variance matrix @xmath106 associated with it . before actual observation of @xmath98 ,",
    "the belief transforms for the adjustment , may be computed across the tree structure .",
    "the transforms are computed recursively , in the following order .",
    "3 t_x(1)= & (    r 1    ) & t_(1)= & (    rr 0.7 & 0 + 0 & 0    ) & t_(2)= & (    rr 0.692 & -0.692 + 0 & 0    ) + t_(3)= & (    rr 0.684 & -1.342 + 0 & 0    ) & t_(4)= & (    rr 0.674 & -1.949 + 0 & 0    ) & t_x(4)= & (    r 0.417    ) + t_x(3)= & (    r 0.453    ) & t_x(2)= & (    r 0.479    )    the @xmath107 matrices are calculated similarly .",
    "in particular , @xmath108 . _ a priori _ analysis of the belief transforms is possible .",
    "for example , @xmath109 , indicating that observation of @xmath98 is expected to reduce overall uncertainty about @xmath105 by a factor of @xmath110 .",
    "this is also the expected size of the bearing for the adjustment of @xmath105 by @xmath98 .",
    "now , @xmath98 is observed to be @xmath111 , and so the expectations , variances and covariances may be updated across the structure .",
    "for example , beliefs about node @xmath105 were updated so that @xmath112 and @xmath113 after propagation",
    ". also , calculating the bearing for the adjustment of @xmath105 by @xmath114 , using gives @xmath115 .",
    "consequently , the size of the adjustment is @xmath116 and the size ratio is @xmath117 .",
    "once evidence from the observed value of @xmath98 has been taken into account , the @xmath98 node , and the arc between @xmath98 and @xmath101 may be dropped from the graph .",
    "note also that @xmath101 then becomes an unobservable leaf node , which may be of little interest , and so if desired , the @xmath101 node , and the arc between @xmath101 and @xmath118 may also be dropped .",
    "observation of @xmath119 may now be considered . using the updated , pruned tree , projections and transforms for the adjustment by @xmath119",
    "may be calculated and propagated through the tree .",
    "for example , the ( partial ) belief transform for the adjustment of @xmath105 by @xmath119 is @xmath120 . if cumulative diagnostics are of interest , then it is necessary to calculate the cumulative belief transform , @xmath121 .",
    "this has trace @xmath122 , and so the resolution for the combined adjustment of @xmath105 by @xmath98 and @xmath119 is 0.82 .",
    "similarly , the expected size of the cumulative bearing is @xmath122 .",
    "@xmath119 is observed to be @xmath123 .",
    "the new expectations , variances and covariances may then be propagated through the tree .",
    "for example , the new expectation vector for @xmath105 is @xmath124 .",
    "the size of the cumulative bearing is @xmath125 , giving a size ratio of approximately @xmath125 .",
    "again , the tree may be pruned , and the whole process may continue .",
    "an ordered collection of random quantities , @xmath126 is said to be ( second - order ) @xmath23-step exchangeable if ( second - order ) beliefs about the collection remain invariant under an arbitrary translation or reflection of the collection , and if the covariance between any two members of the collection is fixed , provided only that they are a distance of at least @xmath23 apart .",
    "such quantities arise naturally in the context of differenced time series @xcite . @xmath23-step",
    "exchangeable quantities may be written in the form @xmath127 where the @xmath128 are a mean zero @xmath23-step exchangeable collection such that the covariance between them is zero provided they are a distance of at least @xmath23 apart .",
    "@xmath129 represents the underlying mean for the collection , and @xmath128 represents the residual uncertainty which would be left if the underlying mean became known .",
    "introduction of a mean quantity helps to simplify a graphical model for an @xmath23-step exchangeable collection .",
    "for example , figure [ fig:3se ] ( top ) shows an undirected graphical model for a @xmath130-step exchangeable collection .",
    "note that without the introduction of the mean quantity , @xmath129 , all nodes on the graph would be joined , not just those a distance of one and two apart .",
    "figure [ fig:3se ] ( bottom ) shows a conditional independence graph for the same collection of quantities , duplicated and grouped together so as to make the resulting graph a tree .",
    "note that each node contains @xmath130 quantities , and that there is one less node than observables .    in general , for a collection of @xmath131 , @xmath23-step exchangeable quantities",
    ", the variables can be grouped together to obtain a simple chain graph , in the obvious way , so that there are @xmath132 nodes , each containing @xmath23 quantities . the resulting graph for @xmath133-step exchangeable quantities",
    "is shown in figure [ fig:5se ] ( with the first four nodes missing ) .    in @xcite , @xmath130- , @xmath134- , and @xmath133-step exchangeable collections , @xmath135 , @xmath136 and @xmath137",
    "are used in order to learn about the quantities , @xmath138 , @xmath139 and @xmath140 , representing the variances underlying the dlm discussed in the previous section . since the observables sequences are @xmath130- , @xmath134- , and @xmath133-step exchangeable , they may all be regarded as @xmath133-step exchangeable , and so figure [ fig:5se ] represents a graphical model for the variables , where @xmath141 , and @xmath142 .",
    "note that @xmath74 represents ( a known linear function of ) the mean of the @xmath133-step exchangeable vectors , @xmath143 .",
    "each node of the graph actually contains @xmath144 quantities .",
    "for example , the first node shown contains @xmath145 .",
    "note that the fact that quantities are duplicated in other nodes does not affect the analysis in any way .",
    "observation of a particular quantity in one node will reduce to zero the variance of that quantity in any other node , as they will have a correlation of unity . here",
    ", the fact that the moore - penrose generalised inverse is used in the definition of the projections and transforms becomes important .",
    "updating for this model may be locally computed over this tree structure in the usual way .",
    "suppose now that information for quantities @xmath146 to @xmath147 is to become available simultaneously .",
    "this corresponds to information on the first two nodes ( and others , but this will be conveyed automatically ) .",
    "the second node acts as a strong root for information from the first two nodes .",
    "the transform for the first node may be calculated using information on the first node , thus allowing computation of the transform for the second node given information on the first . once the information from the first node has been incorporated into the first two nodes , the transform for the second node given information from the first two nodes may be calculated , and the resulting transform for the second node given information on the first two may be used in order to propagate information to the rest of the tree .",
    "a test system for model building and computation over bayes linear belief networks has been developed by the author using the computer algebra system , described in @xcite and @xcite .  is a very high level object - oriented mathematical programming language , with symbolic computing capabilities , ideal for the rapid prototyping of mathematical software and algorithms .",
    "the test system allows definition of nodes and arcs of a tree , and attachment of relevant beliefs to nodes and arcs .",
    "recursive algorithms allow computation of belief transforms for node adjustment , and propagation of updated means , variances and covariances through the tree .",
    "note that whilst propagating outwards through the tree , updating of the different branches of the tree may proceed in parallel .",
    "provides a `` parallel for '' construct which allows simple exploitation of this fact on appropriate hardware .",
    "simple functions to allow computation of diagnostics for particular nodes also exist .",
    "the algorithms described in this paper are very simple and easy to implement , and very fast compared to many other algorithms for updating in bayesian belief networks .",
    "further , by linking the theory with the machinery of the bayes linear methodology , full _ a priori _ and diagnostic analysis may also take place .",
    "_ a priori _ analysis is particularly important in large sparse networks , where it is often not clear whether or not it is worth observing particular nodes , which may be `` far '' from nodes of interest .",
    "similarly , diagnostic analysis is crucial , both for diagnosing misspecified node and arc beliefs , and for diagnosing an incorrectly structured model .    for those who already appreciate the benefits of working within the bayes linear paradigm ,",
    "the methodology described in this paper provides a mechanism for the tackling of much larger structured problems than previously possible , using local computation of belief transforms , adjustments and diagnostics .",
    "m.  goldstein . revising exchangeable beliefs : subjectivist foundations for the inductive argument . in p.",
    "freeman and a.f.m .",
    "smith , editors , _ aspects of uncertainty : a tribute to d. v. lindley_. wiley , 1994 ."
  ],
  "abstract_text": [
    "<S> in recent years there has been interest in the theory of local computation over probabilistic bayesian graphical models . in this paper , local computation over bayes linear belief networks </S>",
    "<S> is shown to be amenable to a similar approach . </S>",
    "<S> however , the linear structure offers many simplifications and advantages relative to more complex models , and these are examined with reference to some illustrative examples .    </S>",
    "<S> keywords : bayes linear methods ; belief propagation ; dynamic linear models ; exchangeability ; graphical models ; local computation . </S>"
  ]
}