{
  "article_text": [
    "nearest neighbor searching is a fundamental computational problem .",
    "let @xmath11 be a set of @xmath2 points in @xmath12 and let @xmath13 be the ( euclidean ) distance between any two points @xmath14 and @xmath15 .",
    "the problem consists in reporting , given a query point @xmath16 , a point @xmath17 such that @xmath18 , for all @xmath19 and @xmath14 is said to be a `` nearest neighbor '' of @xmath16 . for this purpose , we preprocess @xmath11 into a structure called nn - structure",
    ". however , an exact solution to high - dimensional nearest neighbor search , in sublinear time , requires prohibitively heavy resources .",
    "thus , many techniques focus on the less demanding task of computing the approximate nearest neighbor ( @xmath0-ann ) . given a parameter @xmath20 ,",
    "a @xmath21-approximate nearest neighbor to a query @xmath16 is a point @xmath14 in @xmath11 such that @xmath22 , @xmath23 .",
    "hence , under approximation , the answer can be any point whose distance from @xmath16 is at most @xmath21 times larger than the distance between @xmath16 and its nearest neighbor .",
    "[ [ our - contribution ] ] our contribution + + + + + + + + + + + + + + + +    tree based space partitioning techniques perform well when the dimension is relatively low , but are affected by the curse of dimensionality . to that end , randomized methods like locality sensitive hashing are more efficient when the dimension is high .",
    "one may also apply the johnson - lindenstrauss lemma to improve upon standard space partitioning techniques , but the properties guaranteed are stronger than what is required for efficient approximate nearest neighbor search .",
    "we define a `` low - quality '' mapping to a euclidean space of dimension @xmath24 , such that an approximate nearest neighbor lies among the @xmath1 approximate nearest neighbors in the projected space .",
    "this leads to our main theorem  [ tann ] which offers a new randomized algorithm for approximate nearest neighbor search with the following complexity .",
    "given @xmath2 points in @xmath25 , the data structure which is based on balanced box - decomposition ( bbd ) trees , requires @xmath4 space , and reports an @xmath26-approximate nearest neighbor in time @xmath27 , where function @xmath28 is proportional to @xmath29 for fixed @xmath30 and shall be specified in section  [ sann ] .",
    "the total preprocessing time is @xmath31 . for each query @xmath32",
    ", the preprocessing phase succeeds with probability @xmath33 for any constant @xmath34 .",
    "the low - quality embedding is extended to pointsets with bounded expansion rate @xmath35 ( see section  [ sexpansion ] for exact definitions ) .",
    "the pointset is now mapped to a euclidean space of dimension roughly @xmath36 for large enough @xmath1 .",
    "we also present experiments , based on synthetic datasets that validate our approach and our analysis .",
    "the datasets along with the queries follow the `` planted nearest neighbor model '' which will be specified in section  [ sexperiment ] . in another scenario",
    ", we assume that the near neighbors of each query point follow the gaussian distribution . apart from showing that the embedding has the desired properties in practice",
    ", we also implement our overall approach for computing @xmath0-ann using the ann library and we compare with a lsh implementation , namely e2lsh .",
    "the notation of key quantities is the same throughout the paper .",
    "the paper extends and improves ideas from @xcite .",
    "[ [ paper - organization ] ] paper organization + + + + + + + + + + + + + + + + + +    the next section offers a survey of existing techniques .",
    "section  [ sembed ] introduces our embeddings to dimension lower than predicted by the johnson - linderstrauss lemma .",
    "section  [ sann ] states our main results about @xmath0-ann search .",
    "section  [ sexpansion ] generalizes our discussion so as to exploit bounded expansion rate , and section  [ sexperiment ] presents experiments to validate our approach .",
    "we conclude with open questions .",
    "as it was mentioned above , an exact solution to high - dimensional nearest neighbor search , in sublinear time , requires heavy resources .",
    "one notable solution to the problem @xcite shows that nearest neighbor queries can be answered in @xmath37 time , using @xmath38 space , for arbitrary @xmath39 .",
    "one class of methods for @xmath0-ann may be called data - dependent , since the decisions taken for partitioning the space are affected by the given data points . in @xcite they introduced the balanced box - decomposition ( bbd ) trees . the bbd - trees data structure achieves query time @xmath40 with @xmath41 , using space in @xmath4 , and preprocessing time in @xmath31 .",
    "bbd - trees can be used to retrieve the @xmath42 approximate nearest - neighbors at an extra cost of @xmath43 per neighbor .",
    "bbd - trees have proved to be very practical , as well , and have been implemented in software library ann .",
    "another data structure is the approximate voronoi diagrams ( avd ) .",
    "they are shown to establish a tradeoff between the space complexity of the data structure and the query time it supports @xcite . with a tradeoff parameter @xmath44 ,",
    "the query time is @xmath45 and the space is @xmath46 .",
    "they are implemented on a hierarchical quadtree - based subdivision of space into cells , each storing a number of representative points , such that for any query point lying in the cell , at least one of the representatives is an approximate nearest neighbor .",
    "further improvements to the space - time trade offs for ann , are obtained in @xcite .",
    "one might directly apply the celebrated johnson - lindenstrauss lemma and map the points to @xmath47 dimensions with distortion equal to @xmath48 in order to improve space requirements .",
    "in particular , avd combined with the johnson - lindenstrauss lemma require @xmath49 space which is prohibitive if @xmath50 and query time polynomial in @xmath51 , @xmath3 and @xmath52 . notice that we relate the approximation error with the distortion for simplicity .",
    "our approach ( theorem  [ tann ] ) requires @xmath4 space and has query time sublinear in @xmath2 and polynomial in @xmath3 .    in high dimensional spaces , data dependent data structures are affected by the curse of dimensionality .",
    "this means that , when the dimension increases , either the query time or the required space increases exponentially .",
    "an important method conceived for high dimensional data is locality sensitive hashing ( lsh ) .",
    "lsh induces a data independent space partition and is dynamic , since it supports insertions and deletions .",
    "it relies on the existence of locality sensitive hash functions , which are more likely to map similar objects to the same bucket .",
    "the existence of such functions depends on the metric space . in general , lsh requires roughly @xmath53 space and @xmath54 query time for some parameter @xmath55 . in @xcite",
    "they show that in the euclidean case , one can have @xmath56 which matches the lower bound of hashing algorithms proved in @xcite .",
    "lately , it was shown that it is possible to overcome this limitation with an appropriate change in the scheme which achieves @xmath57 @xcite .",
    "for comparison , in theorem  [ tann ] we show that it is possible to use @xmath4 space , with query time roughly @xmath54 where @xmath28 is now higher than the one appearing in lsh .    exploiting the structure of",
    "the input is an important way to improve the complexity of nearest neighbor search .",
    "in particular , significant amount of work has been done for pointsets with low doubling dimension . in @xcite",
    ", they provide an algorithm for ann with expected preprocessing time @xmath58 , space @xmath59 and query time @xmath60 for any finite metric space @xmath11 of doubling dimension @xmath61 . in @xcite",
    "they provide randomized embeddings that preserve nearest neighbor with constant probability , for points lying on low doubling dimension manifolds in euclidean settings .",
    "naturally , such an approach can be easily combined with any known data structure for @xmath0-ann .",
    "in @xcite they present random projection trees which adapt to pointsets of low doubling dimension . like kd - trees , every split partitions the pointset into subsets of roughly equal cardinality ; in fact , instead of splitting at the median , they add a small amount of `` jitter '' . unlike kd - trees ,",
    "the space is split with respect to a random direction , not necessarily parallel to the coordinate axes .",
    "classic @xmath62-trees also adapt to the doubling dimension of randomly rotated data @xcite .",
    "however , for both techniques , no related theoretical arguments about the efficiency of @xmath0-ann search were given .",
    "in @xcite , they introduce a different notion of intrinsic dimension for an arbitrary metric space , namely its expansion rate @xmath35 ; it is formally defined in section  [ sexpansion ] .",
    "the doubling dimension is a more general notion of intrinsic dimension in the sense that , when a finite metric space has bounded expansion rate , then it also has bounded doubling dimension , but the converse does not hold @xcite .",
    "several efficient solutions are known for metrics with bounded expansion rate , including for the problem of exact nearest neighbor . in @xcite , they present a data structure which requires @xmath63 space and answers queries in @xmath64 .",
    "cover trees @xcite require @xmath65 space and each query costs @xmath66 time for exact nearest neighbors . in theorem  [ tannexp ] , we provide a data structure for the @xmath0-ann problem with linear space and @xmath67 query time , where @xmath68 depends on @xmath35 .",
    "the result concerns pointsets in the @xmath3-dimensional euclidean space .",
    "this section examines standard dimensionality reduction techniques and extends them to approximate embeddings optimized to our setting . in the following ,",
    "we denote by @xmath69 the euclidean norm and by @xmath70 the cardinality of a set .",
    "let us start with the classic johnson and lindenstrauss lemma :    @xcite for any set @xmath71 , @xmath9 there exists a distribution over linear mappings @xmath72 , where @xmath73 , such that for any @xmath74 , @xmath75    in the initial proof @xcite , they show that this can be achieved by orthogonally projecting the pointset on a random linear subspace of dimension @xmath76 . in @xcite , they provide a proof based on elementary probabilistic techniques . in @xcite , they prove that it suffices to apply a gaussian matrix @xmath77 on the pointset .",
    "@xmath77 is a @xmath78 matrix with each of its entries independent random variables given by the standard normal distribution @xmath79 . instead of a gaussian matrix",
    ", we can apply a matrix whose entries are independent random variables with uniformly distributed values in @xmath80 @xcite .",
    "however , it has been realized that this notion of randomized embedding is somewhat stronger than what is required for approximate nearest neighbor searching .",
    "the following definition has been introduced in @xcite and focuses only on the distortion of the nearest neighbor .    [ dnnpres ]",
    "let @xmath81 , @xmath82 be metric spaces and @xmath83 .",
    "a distribution over mappings @xmath84 is a _",
    "nearest - neighbor preserving embedding _ with distortion @xmath85 and probability of correctness @xmath86 $ ] if , @xmath87 and @xmath88 , with probability at least @xmath89 , when @xmath90 is such that @xmath91 is an @xmath0-ann of @xmath92 in @xmath93 , then @xmath94 is a @xmath95-approximate nearest neighbor of @xmath16 in @xmath11 .",
    "while in the ann problem we search one point which is approximately nearest , in the @xmath1 approximate nearest neighbors problem ( @xmath0-@xmath1anns ) we seek an approximation of the @xmath1 nearest points , in the following sense .",
    "let @xmath11 be a set of @xmath2 points in @xmath12 , let @xmath96 and @xmath97 .",
    "the problem consists in reporting a sequence @xmath98 of @xmath1 distinct points such that the @xmath99-th point is an @xmath100 approximation to the @xmath99-th nearest neighbor of @xmath16 .",
    "furthermore , the following assumption is satisfied by the search routine of tree - based data structures such as bbd - trees .",
    "[ assbbd ] let @xmath101 be the set of points visited by the @xmath0-@xmath1anns search such that @xmath102 is the set of points which are the @xmath1 nearest to the query point @xmath16 among the points in @xmath103 .",
    "we assume that @xmath104 , @xmath105 .",
    "assuming the existence of a data structure which solves @xmath0-@xmath1anns , we can weaken definition  [ dnnpres ] as follows .",
    "[ dlocpres ] let @xmath81 , @xmath82 be metric spaces and @xmath83 .",
    "a distribution over mappings @xmath84 is a _ locality preserving embedding _ with distortion @xmath85 , probability of correctness @xmath86 $ ] and locality parameter @xmath1 , if @xmath87 and @xmath88 , with probability at least @xmath89 , when @xmath106 is a solution to @xmath0-@xmath1anns for @xmath16 , under assumption  [ assbbd ] then there exists @xmath107 such that @xmath94 is a @xmath108 approximate nearest neighbor of @xmath16 in @xmath11 .    according to this definition",
    "we can reduce the problem of @xmath0-ann in dimension @xmath3 to the problem of computing @xmath1 approximate nearest neighbors in dimension @xmath109 .",
    "we use the johnson - lindenstrauss dimensionality reduction technique .",
    "@xcite [ lemdg ] there exists a distribution over linear maps @xmath110 s.t .",
    ", for any @xmath111 with @xmath112 :    * if @xmath113 then @xmath114 \\leq exp(\\frac{d'}{2}(1-\\beta^2 + 2 \\ln \\beta),$ ] * if @xmath115 then @xmath116 \\leq exp(\\frac{d'}{2}(1-\\beta^2 + 2 \\ln \\beta).$ ]    we prove the following lemma which will be useful .",
    "[ lemineq ] for all @xmath117 , @xmath9 , the following holds : @xmath118    for @xmath119 , it can be checked that if @xmath9 then , @xmath120 .",
    "this is our induction basis .",
    "let @xmath121 be such that the induction hypothesis holds . then , to prove the induction step @xmath122 @xmath123 since @xmath9 .",
    "a simple calculation shows the following .    for all @xmath124 ,",
    "it holds : @xmath125    [ thmbad ] under the notation of definition  [ dlocpres ] , there exists a randomized mapping @xmath126 which satisfies definition  [ dlocpres ] for @xmath127 , distortion @xmath128 and probability of success @xmath129 , for any constant @xmath34 .",
    "let @xmath11 be a set of @xmath2 points in @xmath12 and consider map @xmath130 where @xmath131 is a matrix as in definition  [ dlocpres ] .",
    "wlog the query point @xmath16 lies in the origin and its nearest neighbor @xmath132 lies at distance @xmath133 from @xmath16 .",
    "we denote by @xmath134 the approximation ratio guaranteed by the assumed data structure . that is the assumed data structure solves the @xmath135-@xmath1anns problem . for each point",
    "@xmath94 , @xmath136 where @xmath137 .",
    "let @xmath138 be the random variable whose value indicates the number of `` bad '' candidates , that is @xmath139 where we define @xmath140 , @xmath141 .",
    "hence , by lemma  [ lemdg ] , @xmath142\\leq n \\cdot exp(\\frac{d'}{2}(1-\\frac{\\beta^2}{\\gamma^2}+2 \\ln \\frac{\\beta}{\\gamma})).\\ ] ] by markov s inequality , @xmath143 \\leq \\frac{\\mathbb{e}[n]}{k } \\implies \\mathrm{pr}[n\\geq k ] \\leq n\\ , \\cdot\\ ,   { exp(\\frac{d'}{2}(1-\\frac{\\beta^2}{\\gamma^2}+2 \\ln \\frac{\\beta}{\\gamma } ) ) } / { k } .\\ ] ] the event of failure is defined as the disjunction of two events : @xmath144 \\ ; \\vee \\ ; [ \\ , l_u \\geq ( \\beta / c)^2 \\frac{d'}{d } \\ , ] , \\ ] ] and its probability is at most equal to @xmath145 + exp(\\frac{d'}{2}(1-(\\beta / c)^2 + 2 \\ln ( \\beta / c ) ) ) , \\ ] ] by applying again lemma  [ lemdg ] .",
    "now , we bound these two terms . for the first we choose @xmath76 such that @xmath146 therefore , @xmath147 \\leq \\frac{\\delta}{2}.\\ ] ]    notice that @xmath148 and due to expression  ( [ genineq ] ) , we obtain @xmath149 . hence , inequality (  [ ineq3a ] ) implies inequality (  [ ineq4a ] ) : @xmath150 therefore , @xmath151 inequalities (  [ ineq3b ] ) , (  [ ineq4b ] ) imply that the total probability of failure is at most @xmath152 .",
    "using lemma  [ lemineq ] for @xmath119 , we obtain , that there exists @xmath76 such that @xmath153 and with probability of success at least @xmath129 , these two events occur :    * @xmath154 * @xmath155    now consider the case when the random experiment succeeds and let @xmath156 a solution of the @xmath135-@xmath1anns problem in the projected space , given by a data - structure which satisfies assumption  [ assbbd ] .",
    "we have that @xmath157 , @xmath158 where @xmath103 is the set of all points visited by the search routine .",
    "now , if @xmath159 then @xmath160 contains the projection of the nearest neighbor .",
    "if @xmath161 then if @xmath162 we have the following : @xmath163 which means that there exists at least one point @xmath164 s.t .",
    "@xmath165 . finally , if @xmath161 but @xmath166 then @xmath167 which means that there exists at least one point @xmath164 s.t .",
    "@xmath165 .",
    "hence , @xmath168 satisfies definition  [ dlocpres ] for @xmath128 .",
    "this section combines tree - based data structures which solve @xmath0-@xmath1anns with the results above , in order to obtain an efficient randomized data structure which solves @xmath0-ann .",
    "bbd - trees @xcite require @xmath4 space , and allow computing @xmath1 points , which are @xmath100-approximate nearest neighbors , within time @xmath169 .",
    "the preprocessing time is @xmath170 .",
    "notice , that bbd - trees satisfy the assumption  [ assbbd ] .",
    "the algorithm for the @xmath0-@xmath1anns search , visits cells in increasing order with respect to their distance from the query point @xmath16 .",
    "if the current cell lies in distance more than @xmath171 where @xmath172 is the current distance to the @xmath1th nearest neighbor , the search terminates .",
    "we apply the random projection for distortion @xmath128 , thus relating approximation error to the allowed distortion ; this is not required but simplifies the analysis .",
    "moreover , @xmath173 ; the formula for @xmath28 is determined below .",
    "our analysis then focuses on the asymptotic behaviour of the term @xmath174 .",
    "[ lembbd ] with the above notation , there exists @xmath175 s.t .",
    ", for fixed @xmath9 , it holds that @xmath176 , where @xmath177 for some appropriate constant @xmath178 .",
    "recall that @xmath179 for some appropriate constant @xmath180 .",
    "the constant @xmath152 is hidden in @xmath181 . since @xmath182 is a decreasing function of @xmath1 , we need to choose @xmath1 s.t.@xmath183 .",
    ". obviously @xmath185 , for some appropriate constant @xmath186 .",
    "then , by substituting @xmath187 we have : @xmath188    we assume @xmath9 is a fixed constant . hence , it is reasonable to assume that @xmath189 .",
    "we consider two cases when comparing @xmath190 to @xmath0 :    * @xmath191 .",
    "substituting @xmath192 into equation (  [ eterm1ne ] ) , the exponent of @xmath2 is bounded as follows : @xmath193 @xmath194 } < { \\rho}.\\ ] ] * @xmath195 . substituting @xmath196 into equation (  [ eterm1ne ] ) , the exponent of @xmath2",
    "is bounded as follows : @xmath193 @xmath197 } <    { \\rho}.\\ ] ]    notice that for both cases @xmath198 .",
    "combining theorem  [ thmbad ] with lemma  [ lembbd ] yields the following main theorem .",
    "[ tann ] given @xmath2 points in @xmath25 , there exists a randomized data structure which requires @xmath4 space and reports an @xmath26-approximate nearest neighbor in time @xmath199 for some appropriate constant @xmath178 .",
    "the preprocessing time is @xmath31 . for each query @xmath32",
    ", the preprocessing phase succeeds with any constant probability .",
    "the space required to store the dataset is @xmath4 .",
    "the space used by bbd - trees is @xmath200 where @xmath76 is defined in lemma  [ lembbd ] .",
    "we also need @xmath201 space for the matrix @xmath131 as specified in theorem  [ thmbad ] . hence ,",
    "since @xmath109 and @xmath202 , the total space usage is bounded above by @xmath4 .",
    "the preprocessing consists of building the bbd - tree which costs @xmath203 time and sampling @xmath131 .",
    "notice that we can sample a @xmath76-dimensional random subspace in time @xmath204 as follows .",
    "first , we sample in time @xmath205 , a @xmath78 matrix where its elements are independent random variables with the standard normal distribution @xmath206 .",
    "then , we orthonormalize using gram - schmidt in time @xmath204 . since @xmath207 , the total preprocessing time is bounded by @xmath31 .    for each query we use @xmath131 to project the point in time @xmath205 .",
    "next , we compute its @xmath208 approximate nearest neighbors in time @xmath209 and we check its neighbors with their real coordinates in time @xmath210 .",
    "hence , each query costs @xmath211 because @xmath212 , @xmath213 .",
    "thus , the query time is dominated by the time required for @xmath0-@xmath1anns search and the time to check the returned sequence of @xmath1 approximate nearest neighbors .    to be more precise , the probability of success , which is the probability that the random projection succeeds according to theorem .",
    "[ thmbad ] , is greater than @xmath129 , for any constant @xmath214 notice that the preprocessing time for bbd - trees has no dependence on @xmath0 .",
    "this section models the structure that the data points may have so as to obtain more precise bounds .",
    "the bound on the dimension obtained in theorem  [ thmbad ] is quite pessimistic .",
    "we expect that , in practice , the space dimension needed in order to have a sufficiently good projection is less than what theorem  [ thmbad ] guarantees . intuitively , we do not expect to have instances where all points in @xmath11 , which are not approximate nearest neighbors of @xmath16 , lie at distance almost equal to @xmath215 . to this end , we consider the case of pointsets with bounded expansion rate",
    ".    let @xmath216 a metric space and @xmath217 a finite pointset and let @xmath218 denote the points of @xmath11 lying in the closed ball centered at @xmath14 with radius @xmath219 .",
    "we say that @xmath11 has @xmath220-expansion rate if and only if , @xmath221 and @xmath222 , @xmath223    [ thmbad2 ] under the notation introduced in the previous definitions , there exists a randomized mapping @xmath126 which satisfies definition  [ dlocpres ] for dimension @xmath224 , distortion @xmath128 and probability of success @xmath129 , for any constant @xmath225 , for pointsets with @xmath226-expansion rate .",
    "we proceed in the same spirit as in the proof of theorem  [ thmbad ] , and using the notation from that proof .",
    "let @xmath227 be the distance to the @xmath228th nearest neighbor , excluding neighbors at distance @xmath229 .",
    "for @xmath230 , let @xmath231 and set @xmath232 .",
    "clearly , @xmath233 & \\leq \\ ; \\sum_{i=0}^{\\infty } |b_p(r_i)|   \\cdot exp(\\frac{d'}{2}(1-\\frac{(1+\\epsilon/2)^2}{r_{i-1}^2}+2 \\ln \\frac{1+\\epsilon/2}{r_{i-1 } } ) ) \\\\ & \\leq \\",
    "; \\sum_{i=0}^{\\infty } c^i \\rho \\cdot exp(\\frac{d'}{2}(1-\\frac{(1+\\epsilon/2)^2}{2^{2i } ( 1+\\epsilon)^2}+2 \\ln \\frac{1+\\epsilon/2}{2^i ( 1+\\epsilon)})).\\end{aligned}\\ ] ] now , using lemma  [ lemineq ] , @xmath142 \\leq \\sum_{i=0}^{\\infty } c^i",
    "\\rho \\cdot exp(-\\frac{d'}{2}0.05 ( i+1 ) \\epsilon^2 ) , \\ ] ] and for @xmath234 ,    @xmath142 \\leq \\rho \\cdot \\sum_{i=0}^{\\infty } c^i \\cdot   ( \\frac{1}{c + \\frac{2\\rho}{k \\delta } } ) ^{i+1 } = \\rho \\cdot \\sum_{i=0}^{\\infty } c^i \\cdot ( \\frac{1}{c})^{i+1 } \\cdot ( \\frac{1}{1 + \\frac{2\\rho}{k c \\delta } } ) ^{i+1}= \\frac{\\rho}{c}\\cdot \\sum_{i=0}^{\\infty } ( \\frac{1}{1 + \\frac{2\\rho}{k c \\delta } } ) ^{i+1 } = \\frac{k \\delta}{2}.\\ ] ] finally , @xmath143\\leq \\frac{\\mathbb{e}[n]}{k } \\leq \\frac{\\delta}{2}.\\ ] ]    employing theorem  [ thmbad2 ] we obtain a result analogous to theorem  [ tann ] which is weaker than those in @xcite but underlines the fact that our scheme shall be sensitive to structure in the input data , for real world assumptions .    [ tannexp ] given @xmath2 points in @xmath25 with @xmath235-expansion rate , there exists a randomized data structure which requires @xmath4 space and reports an @xmath26-approximate nearest neighbor in time @xmath236 , for some constant @xmath68 depending on @xmath35 .",
    "the preprocessing time is @xmath31 . for each query @xmath32",
    ", the preprocessing phase succeeds with any constant probability .",
    ". then @xmath238 and @xmath239})$ ] .",
    "now the query time is @xmath240}+\\ln n ) d\\frac{\\ln c}{\\epsilon^2 } \\ln   n ) = o((c^{1/\\epsilon^3}+\\log n ) d \\frac{\\ln n}{\\epsilon^2 } ) , \\ ] ] for some constant @xmath68 such that @xmath241 .     increases for the `` planted nearest neighbor model '' datasets .",
    "the highest line corresponds to @xmath242 and the dotted line to a function of the form @xmath243 that best fits the data.[fexp ] ]",
    "in the following two sections we present and discuss the two experiments we performed . in the first one we computed the average value of @xmath1 in a worst - case dataset and we validated that it is indeed sublinear . in the second one we made an ann query time comparison against a lsh implementation .",
    "in this section we present an experimental verification of our approach .",
    "we show that the number @xmath1 of the nearest neighbors in the random projection space that we need to examine in order to find an approximate nearest neighbor in the original space depends sublinearly on @xmath2 .",
    "recall that we denote by @xmath244 the euclidean norm .",
    "[ [ dataset ] ] dataset + + + + + + +    we generated our own synthetic datasets and query points to verify our results .",
    "we decided to follow two different procedures for data generation in order to be as complete as possible .",
    "first of all , as in @xcite , we followed the `` planted nearest neighbor model '' for our datasets .",
    "this model guarantees for each query point @xmath16 the existence of a few approximate nearest neighbors while keeping all others points sufficiently far from @xmath16 .",
    "the benefit of this approach is that it represents a typical ann search scenario , where for each point there exist only a handful approximate nearest neighbors .",
    "in contrast , in a uniformly generated dataset , all the points will tend to be equidistant to each other in high dimensions , which is quite unrealistic .    in order to generate such a dataset ,",
    "first we create a set @xmath245 of query points chosen uniformly at random in @xmath12 .",
    "then , for each point @xmath246 , we generate a single point @xmath14 at distance @xmath247 from @xmath16 , which will be its single ( approximate ) nearest neighbor .",
    "then , we create more points at distance @xmath248 from @xmath16 , while making sure that they shall not be closer than @xmath249 to any other query point @xmath250 .",
    "this dataset now has the property that every query point has exactly one approximate nearest neighbor , while all other points are at distance @xmath248 .",
    "we fix @xmath251 , let @xmath252 and the total number of points @xmath253 . for each combination of the above we created a dataset @xmath11 from a set @xmath245 of @xmath254 query points where each query coordinate was chosen uniformly at random in the range",
    "@xmath255 $ ] .",
    "the second type of datasets consisted again of sets of @xmath254 query points in @xmath12 where each coordinate was chosen uniformly at random in the range @xmath255 $ ] . each query point was paired with a random variable @xmath256 uniformly distributed in @xmath257 $ ] and together they specified a gaussian distribution in @xmath12 of mean value @xmath258 and variance @xmath256 per coordinate . for each distribution we drew @xmath2 points in the same set as was previously specified .",
    "[ [ scenario ] ] scenario + + + + + + + +    we performed the following experiment for the `` planted nearest neighbor model '' . in each dataset @xmath11",
    ", we consider , for every query point @xmath16 , its unique ( approximate ) nearest neighbor @xmath17 .",
    "then we use a random mapping @xmath168 from @xmath25 to a euclidean space of lower dimension @xmath259 using a gaussian matrix @xmath77 , where each entry @xmath260 .",
    "this matrix guarantees a low distortion embedding @xcite .",
    "then , we perform a range query centered at @xmath92 with radius @xmath261 in @xmath93 : we denote by @xmath262 the number of points found . then , exactly @xmath262 points are needed to be selected in the worst case as @xmath1-nearest neighbors of @xmath92 in order for the approximate nearest neighbor @xmath263 to be among them , so @xmath264 .    for the datasets with the gaussian distributions we compute again the maximum number of points",
    "@xmath1 needed to visit in the lower - dimensional space in order to find an @xmath0-approximate nearest neighbor of each query point @xmath16 in the original space . in this case the experiment works as follows : we find all the @xmath0-approximate nearest neighbors of a query point @xmath16",
    ". let @xmath265 be the set containing for each query @xmath16 its @xmath0-@xmath1anns .",
    "next , let @xmath266 .",
    "now as before we perform a range query centered at @xmath92 with radius @xmath267 .",
    "we consider as @xmath1 the number of points returned by this query .",
    "[ [ results ] ] results + + + + + + +    the `` planted nearest neighbor model '' datasets constitute a worst - case input for our approach since every query point has only one approximate nearest neighbor and has many points lying near the boundary of @xmath100 .",
    "we expect that the number of @xmath1 approximate nearest neighbors needed to consider in this case will be higher than in the case of the gaussian distributions , but still expect the number to be considerably sublinear .    in figure  [ fexp ]",
    "we present the average value of @xmath1 as we increase the number of points @xmath2 for the planted nearest neighbor model .",
    "we can see that @xmath1 is indeed significantly smaller than @xmath2 .",
    "the line corresponding to the averages may not be smooth , which is unavoidable due to the random nature of the embedding , but it does have an intrinsic concavity , which shows that the dependency of @xmath1 on @xmath2 is sublinear . for comparison",
    "we also display the function @xmath268 , as well as a function of the form @xmath269 which was computed by sage that best fits the data per plot .",
    "the fitting was performed on the points in the range @xmath270 $ ] as to better capture the asymptotic behaviour . in figure  [ fexpgm ]",
    "we show again the average value of @xmath1 as we increase the number of points @xmath2 for the gaussian distribution datasets .",
    "as expected we see that the expected value of @xmath1 is much smaller than @xmath2 and also smaller than the expected value of @xmath1 in the worst - case scenario , which is the planted nearest neighbor model .",
    "increases for the gaussian datasets .",
    "we see how increasing the number of approximate nearest neighbors in this case decreases the value of @xmath1[fexpgm ] ]      in this section we present a naive comparison between our algorithm and the e2lsh @xcite implementation of the lsh framework for approximate nearest neighbor queries .",
    "[ [ experiment - description ] ] experiment description + + + + + + + + + + + + + + + + + + + + + +    we projected all the `` planted nearest neighbor '' datasets , down to @xmath271 dimensions .",
    "we remind the reader that these datasets were created to have a single approximate nearest neighbor for each query at distance @xmath247 and all other points at distance @xmath272 .",
    "we then built a bbd - tree data structure on the projected space using the ann library @xcite with the default settings .",
    "next , we measured the average time needed for each query @xmath16 to find its @xmath0-@xmath1anns , for @xmath273 , using the bbd - tree data structure and then to select the first point at distance @xmath274 out of the @xmath1 in the original space .",
    "we compare these times to the average times reported by e2lsh range queries for @xmath275 , when used from its default script for probability of success @xmath276 .",
    "the script first performs an estimation of the best parameters for the dataset and then builds its data structure using these parameters .",
    "we required from the two approaches to have accuracy @xmath277 , which in our case means that in at least @xmath278 out of the @xmath254 queries they would manage to find the approximate nearest neighbor .    [ [ ann - results ] ] ann results + + + + + + + + + + +    it is clear from figure  [ fann ] that e2lsh is more efficient than our approach by a factor of @xmath279 .",
    "a few points can be raised here .",
    "first of all , we supplied the appropriate range to the lsh implementation , which gave it an advantage , because typically that would have to be computed empirically . to counter that , we allowed our algorithm to stop its search in the original space when it encountered a point that was at distance @xmath274 from the query point .",
    "secondly , e2lsh is heavily optimized and for each dataset it computed optimal parameters that .",
    "our approach was simpler and we found that the bottleneck was in the computation of the closest point out of the @xmath1 returned from the bbd - tree .",
    "we conjecture that we can choose better values for our parameters @xmath76 and @xmath1 .",
    "lastly , the theoretical guarantees for the query time of lsh are better than ours ( but we perform better in terms of space usage ) .    ]",
    "in terms of practical efficiency it is obvious that checking the real distance to the neighbors while performing an @xmath0-@xmath1anns search in the reduced space , is more efficient in practice than naively scanning the returned sequence of @xmath1-approximate nearest neighbors and looking for the best in the initial space .",
    "moreover , we do not exploit the fact that bbd - trees return a sequence and not simply a set of neighbors .",
    "our embedding possibly has further applications .",
    "one possible application is the problem of computing the @xmath1-th approximate nearest neighbor .",
    "the problem may reduce to computing all neighbors between the @xmath99-th and the @xmath280-th nearest neighbors in a space of significantly smaller dimension for some appropriate @xmath281 .",
    "other possible applications include computing the approximate minimum spanning tree or the closest pair of points ."
  ],
  "abstract_text": [
    "<S> the approximate nearest neighbor problem ( @xmath0-ann ) in euclidean settings is a fundamental question , which has been addressed by two main approaches : data - dependent space partitioning techniques perform well when the dimension is relatively low , but are affected by the curse of dimensionality . on the other hand , </S>",
    "<S> locality sensitive hashing has polynomial dependence in the dimension , sublinear query time with an exponent inversely proportional to the error factor @xmath0 , and subquadratic space requirement .    </S>",
    "<S> we generalize the johnson - lindenstrauss lemma to define `` low - quality '' mappings to a euclidean space of significantly lower dimension , such that they satisfy a requirement weaker than approximately preserving all distances or even preserving the nearest neighbor . </S>",
    "<S> this mapping guarantees , with arbitrarily high probability , that an approximate nearest neighbor lies among the @xmath1 approximate nearest neighbors in the projected space . </S>",
    "<S> this leads to a randomized tree based data structure that avoids the curse of dimensionality for @xmath0-ann . </S>",
    "<S> our algorithm , given @xmath2 points in dimension @xmath3 , achieves space usage in @xmath4 , preprocessing time in @xmath5 , and query time in @xmath6 , where @xmath7 is proportional to @xmath8 , for fixed @xmath9 . </S>",
    "<S> it employs a data structure , such as bbd - trees , that efficiently finds @xmath1 approximate nearest neighbors . </S>",
    "<S> the dimension reduction is larger if one assumes that pointsets possess some structure , namely bounded expansion rate . </S>",
    "<S> we implement our method and present experimental results in up to 500 dimensions and @xmath10 points , which show that the practical performance is better than predicted by the theoretical analysis . </S>",
    "<S> in addition , we compare our approach with e2lsh . </S>"
  ]
}