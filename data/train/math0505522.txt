{
  "article_text": [
    "the use of latent variables has several interesting applications in statistics , econometrics and related fields like quantitative marketing .",
    "models like tobit , and probit , ordered probit and multinomial probit are good examples . references and examples",
    "could be found in geweke , keane and runkle ( 1997 ) and allenby and rossi ( 1999 ) , especially for the multinomial probit .",
    "when the estimation process uses some simulation technique , in particular in bayesian analysis , the need of drawing a sample from the distribution of the latent variable naturally arises .",
    "this procedure _ augments _ the observed data @xmath2 with a new variable @xmath3 which will be referred as a _ latent _ variable .",
    "it is usually referred to as _ data augmentation _ ( tanner and wong ( 1987 ) , tanner ( 1991 ) , gelfand and smith ( 1990 ) ) and in other contexts as _ imputation _ ( rubin 1987 ) .",
    "the data augmentation algorithm is used when the likelihood function or posterior distribution of the parameter given the latent data @xmath4 is simpler than the posterior given the original observed data @xmath5 . if the distribution of the variables @xmath4 is a multivariate normal in @xmath6 dimensions and @xmath7 is the indicator function of the set @xmath1 , the drawing is made from the normal distribution truncated to @xmath1 ( and/or @xmath8 ) .",
    "the goal of this paper is to produce exact ( or perfect ) samples from random variables with distributions supported on a @xmath6-dimensional box @xmath1 ; we call _ box _ the cartesian product of @xmath6 bounded intervals .",
    "we construct a discrete - time stationary markov process @xmath9 in the state space @xmath10 whose time - marginal distribution at any time @xmath11 ( that is , the law of @xmath12 ) has a given density distribution @xmath13 with support in  @xmath1 .",
    "the construction is then implemented to perfectly simulate normal vectors of reasonable large dimension truncated to bounded boxes .",
    "the approach is also useful to show uniqueness of the invariant measure for a family of processes in a infinite - dimensional space @xmath14^{z^d}$ ] with truncated gaussian distributions and nearest - neighbor interactions ( in preparation , see also section [ s6 ] ) .",
    "the process is the gibbs sampler popularized by geman and geman ( 1985 ) and used for the truncated normal case by geweke ( 1991 ) and robert ( 1995 ) in a _ markov chain monte carlo ( mcmc ) _ algorithm to obtain samples with distribution approximating  @xmath13 . to describe the process in our setting ,",
    "let @xmath15 and take a box @xmath16 where @xmath17 are bounded intervals : @xmath18\\subset\\r$ ] , @xmath19 .",
    "if @xmath20 is the configuration at time @xmath21 , then at time @xmath11 a site @xmath22 is chosen uniformly in @xmath23 , say @xmath24 and @xmath25 is substituted by a value @xmath26 chosen with the density @xmath13 conditioned to the values of the other coordinates ; that is with the density @xmath27 given by @xmath28 where @xmath29 , @xmath30 , @xmath31 .",
    "note that @xmath32 does not depend on @xmath25 .",
    "the distribution in @xmath10 with density @xmath13 is reversible measure for this dynamics .",
    "we construct a stationary gibbs sampler @xmath9 as a function of a sequence @xmath33 of independent identically distributed and uniform in @xmath34 $ ] random variables and the updating schedule @xmath35 .",
    "we define ( backwards ) stopping times @xmath36 $ ] such that @xmath37 is measurable with respect to the field generated by the uniform random variables and the schedule in @xmath38 $ ] .",
    "the configuration at time @xmath11 depends on the uniform random variables and the schedule in @xmath39 $ ] .",
    "in fact we construct simultaneously processes @xmath40 } , -\\infty$ ] @xmath41 . for each fixed",
    "@xmath42 and @xmath43 , @xmath40},\\,s\\le t<\\infty)$ ] is the gibbs sampler starting at time @xmath42 with configuration @xmath43 . for each fixed @xmath11 , @xmath40},\\,s\\le t,\\,\\zeta\\in\\cb)$ ]",
    "is a _ maximal coupling _ ( see thorisson ( 2000 ) and section [ s2 ] ) .",
    "for each fixed time interval @xmath44 $ ] the process @xmath45 } , s\\le t'\\le t)$ ] is a function of the uniform random variables and the schedule in @xmath38 $ ] and the initial  @xmath43 .",
    "the crucial property of the coupling is that @xmath46 does not depend on @xmath43 .",
    "the construction is a particular implementation of the _ coupling from the past ( cftp ) _ algorithm of propp and wilson ( 1996 ) to obtain samples of a law @xmath47 as a function of a finite ( but random ) number of uniform random variables . for an annotated bibliography on the subject see the web page of wilson ( 1998 ) .",
    "a markov process having @xmath47 as unique invariant measure is constructed as a function of the uniform random variables .",
    "the processes starting at all possible initial states run from negative time @xmath42 to time @xmath48 using the uniform random variables @xmath49 and a fixed updating schedule .",
    "if at time 0 all the realizations coincide , then this common state has distribution @xmath47 .",
    "if they do not coincide then start the algorithm at time @xmath50 using the random variables @xmath51 and continue this way up to the moment all realizations coincide at time 0 ; @xmath52 is the maximal @xmath53 such that this holds .",
    "the difficulty is that unless the dynamics is monotone , one has to effectively couple all ( non countable ) initial states .",
    "murdoch and green ( 1998 ) proposed various procedures to transform the infinite set of initial states in a more tractable finite subset .    in the normal case , for some intervals and covariance matrices , the maximal coupling works without further treatment .",
    "moeller ( 1999 ) studies the continuous state spaces when the process undergoes some kind of monotonicity ; this is not the case here unless all correlations are non negative .",
    "philippe and robert ( 2003 ) propose an algorithm to perfectly simulate normal vectors conditioned to be positive using coupling from the past and slice sampling ; the method is efficient in low dimensions .",
    "consider a @xmath6-dimensional random vector @xmath54 with normal distribution of density @xmath55 where the covariance matrix @xmath56 is a positive definite matrix , @xmath57 is the mean vector and @xmath58 in this case we say that @xmath59 .",
    "we denote @xmath60 the truncation of @xmath61 to the box @xmath62 ; its density is @xmath63 where @xmath64=\\int_{\\cb}f({\\bx})d{\\bx}$ ] is the normalizing constant .",
    "we have tested the algorithm in some examples .",
    "in particular for the matrix @xmath65 where @xmath66 is the identity matrix and @xmath67 . for the box @xmath68^d$ ] and for @xmath69 our method is faster than the rejection method .",
    "the simulations indicate that the computer time for the rejection method grows exponentially with @xmath6 while it grows like @xmath70 for our method . using a simple program in matlab in desktop microcomputer the method permits to simulate up to dimension @xmath71 for the interval @xmath68^d$ ] . for bigger boxes or boxes",
    "apart from the mean , for instance @xmath72^d$ ] , our method is less efficient but in any case it is much faster than the rejection one . since our method uses several times the same uniform variables , to compare the computation time we have counted the number of times each of the one - dimensional uniform random variable is used .",
    "it is expected that if the box @xmath1 is sufficiently small and the covariance matrix has a `` neighbor structure '' , the number of steps may grow as @xmath73 . in this case",
    "one should be able to simulate the state of an infinite dimensional gaussian field in a finite box .",
    "correlated normal vectors can be mapped into independent standard normal vectors using the cholesky transformation ( see chapter xi in devroye ( 1986 ) ) . in the case of truncated normals to a box , the transformation maps the box into a set .",
    "one might simulate standard normals and reject if they do not belong to the transformed box .",
    "the difficulties are similar to those of the rejection method : as the dimension grows , the transformed set has small probability and the expected number of iterations grows exponentially with the dimension .",
    "when the truncating set is not a box , our method generally fails as the coupling event for each coordinate has probability zero in general .",
    "we illustrate this with an example in section [ s6 ] .",
    "another possibility , also discussed by devroye ( 1986 ) is to compute the marginal of the first coordinate , then the second marginal conditioned to the first one and so on .",
    "the problem here is that the computation of the marginals conditioned to all the other coordinates be in @xmath1 may be as complicated as to compute the whole truncated vector .    in section [ s2 ]",
    "we define coupling and maximal coupling . in section [ s3 ]",
    "we describe the stationary theoretical construction of the gibbs sampler and the properties of the coupling .",
    "section [ s4 ] is devoted to the pseudocode of the perfect simulation algorithm . in section [ s5 ]",
    "we compute the functions entering the algorithm for the truncated normal case .",
    "in section [ s6 ] we give some examples and compare our perfect simulation algorithm with the rejection one based on the uniform distribution .",
    "a _ coupling _ of a family of random variables @xmath74 with @xmath75 a label set , is a family of random variables @xmath76 with the same marginals ; that is , such that @xmath77 an event @xmath78 is called _ coupling event _ , if @xmath79 are identical in @xmath78 .",
    "that is , @xmath80 we consider continuous random variables @xmath81 in @xmath82 with densities",
    "@xmath83 satisfying @xmath84 for any coupling event @xmath78 .",
    "this is always true if @xmath75 is countable and in the normal case treated here . when there is a coupling event @xmath78 such that identity holds in the coupling is called _",
    "maximal_. see thorisson ( 2000 ) for a complete treatment of coupling , including historical quotations .",
    "a natural maximal coupling of these variables is the following .",
    "let @xmath85 be the corresponding cumulative distribution functions and define @xmath86 let @xmath87 .",
    "let @xmath88 be a random variable uniformly distributed in @xmath34 $ ] and define @xmath89 where @xmath90 , the _ generalized inverse _ of @xmath91 , is defined by @xmath92 the marginal distribution of @xmath93 is @xmath94 : @xmath95 the process @xmath96 is a maximal coupling for the family @xmath97 .",
    "we use this coupling to construct the dynamics starting from different initial conditions .",
    "then we show how to compute @xmath98 and @xmath99 in the normal case .    in the sequel we use a family of couplings .",
    "let @xmath100 be a decreasing sequence of parameter sets : @xmath101 for all @xmath102 .",
    "let @xmath103}(x)\\equiv 0 $ ] and for @xmath102 , @xmath104}(x):=\\int_{-\\infty}^{x}\\inf_{\\lambda\\in    \\lambda_\\ell}g_\\lambda(y)dy;\\qquad r_{[\\ell]}:=r_{[\\ell]}(\\infty)\\ , .",
    "\\end{aligned}\\ ] ] assume @xmath105}= 1 $ ] .",
    "define @xmath106}(x ) = r_{[\\ell]}(x)-r_{[\\ell-1]}(x)+ r_{[\\ell-1]}$ ] and for @xmath107 define @xmath108}(x)= g_\\lambda(x ) - r_{[\\ell]}(x ) + r_{[\\ell]}$ ] and @xmath109}^{-1}(u ) \\,\\one\\{u\\in[r_{i-1},r_i)\\ } \\,+\\ ,    \\hat g_{\\lambda,[\\ell]}^{-1}(u ) \\,\\one\\{u > r_{[\\ell]}\\ } \\,.\\ ] ] then @xmath110 is a maximal coupling of @xmath111 for each @xmath112 .",
    "the goal is to define a stationary process @xmath9 in @xmath1 with the gibbs sampler dynamics associated to a density @xmath13 in @xmath1 .",
    "the time marginal of the process at any time will be the distribution with the density @xmath13 .",
    "the gibbs sampler is defined as follows .",
    "assume @xmath113 is known , then at time @xmath11 choose a random site @xmath22 uniformly in @xmath23 independently of `` everything else '' .",
    "then set @xmath114 if @xmath115 and for @xmath29 choose @xmath116 with the law @xmath13 conditioned to the values of @xmath117 at the other coordinates : @xmath118 where @xmath119 , @xmath120 $ ] , is the conditional density of the @xmath121th component of @xmath13 given the other coordinates given in .",
    "reversibility follows from the identity @xmath122 for any @xmath123 and @xmath124 continuous functions from @xmath1 to @xmath82 , where @xmath20 and @xmath30 .",
    "in particular the law with density @xmath13 is invariant for the gibbs sampler .",
    "it is indeed the unique invariant measure for the dynamics , as we show in theorem [ p31 ] .    to define a stationary version of the process",
    "we construct a family of couplings for each time interval @xmath44 $ ] .",
    "the coupling starts with all possible configurations at time @xmath42 and uses random variables associated to each time to decide the updating .",
    "all couplings use the same updating variables . to reflect the evolution of all possible configurations",
    "we introduce a process on the set of boxes contained in @xmath1 .",
    "the initial configuration of this process is @xmath1 at time @xmath42 but at times @xmath125 can be ( and will be as @xmath42 decreases ) reduced to a point .",
    "let @xmath126 be a @xmath6-tuple of closed bounded intervals of @xmath82 , @xmath127 , @xmath128 .",
    "we abuse notation calling @xmath126 also the box @xmath129 notice that @xmath126 has dimension less than @xmath6 when @xmath130 is just a point for some @xmath131 .",
    "define @xmath132 these functions depend on @xmath126 only through @xmath133 .",
    "@xmath134 is the probability that if the @xmath121-th coordinate is updated when the other coordinates belong to the set @xmath126 , then a coupled event is attained for the @xmath121 coordinate .",
    "this means that for any configuration @xmath135 , the updated value of the @xmath121-th coordinate will be the same , say @xmath136 ; its law is given by @xmath137 . in this case",
    "the updated interval @xmath138 is also reduced to the point @xmath136 . if the coupled event is not attained @xmath138 is kept as the interval @xmath17 .",
    "the updating random variables consist on two families : @xmath139 , a family of independent variables with uniform distribution in @xmath34 $ ] and @xmath140 , a family of independent variables with uniform distribution in @xmath23 and independent of @xmath141 .",
    "now for each @xmath142 we define a process @xmath143},\\,t\\ge s)$ ] taking values on boxes contained in @xmath1 as function of @xmath144 .",
    "the initial state is @xmath145}=\\cb$ ] and later each coordinate @xmath121 is either a ( random ) point or the full interval @xmath17 .",
    "more precisely , for @xmath142 and @xmath146 set @xmath147}= 0\\ , , \\qquad r_{[s , s]}(x)= 0\\ , , \\qquad d_{[s , s]}(x)=0\\ , , \\qquad \\eta_{[s , s]}=\\cb.\\end{aligned}\\ ] ] fix @xmath148 and assume @xmath149}(x)$ ] , @xmath149}$ ] and @xmath150}$ ] are defined if @xmath151 .",
    "then for @xmath152 and @xmath153 , define @xmath154}&=&r_{\\kappa(t)}(\\eta_{[s , t-1 ] } ) \\\\",
    "r_{[s , t]}(x)&=&r_{\\kappa(t)}(x|\\eta_{[s , t-1 ] } ) , \\\\",
    "d_{[s , t]}(x)&=&r_{[s+1,t]}+r_{[s , t]}(x)-r_{[s+1,t]}(x)\\,\\nonumber \\\\\\label{oi } \\eta_{[s , t]}(k)&=&\\left\\ { \\begin{array}{lll } \\eta_{[s , t-1]}(k)\\,,\\qquad \\mbox{if}\\;k\\neq \\kappa(t),\\\\ \\\\ \\eta_{[s+1,t]}(k ) \\,{\\bf 1}\\{u_t\\leq r_{[s+1,t]}\\}\\,+ \\,d_{[s , t]}^{-1}(u_t)\\,{\\bf 1}\\{r_{[s+1,t]}<u_t\\leq r_{[s , t]}\\}\\ , \\\\ \\qquad\\qquad\\qquad + \\,b(k)\\,{\\bf 1}\\{u_t > r_{[s , t]}\\}\\ , , \\qquad\\mbox { if}\\;k=\\kappa(t ) ; \\end{array } \\right.\\,.\\end{aligned}\\ ] ] in words : @xmath29 defines the coordinate to be updated at time @xmath11 .",
    "@xmath149}$ ] is the probability that the coupling event is attained at coordinate @xmath22 for all the processes starting at times smaller than or equal to @xmath42 .",
    "the coupling event is attained for the process starting at @xmath42 when @xmath155}$ ] . in case",
    "the coupling event is attained , the value of coordinate @xmath121 is given by @xmath156}(u_t)$ ] , for @xmath157 given by the biggest @xmath158 such that @xmath155}$ ] ( second term in ) .",
    "this value is the same for each @xmath159 ( first term in ) .",
    "when the coupling event is not attained ( that is , for @xmath160 ) , the coordinate @xmath121 is kept equal to the full interval @xmath17 ( third term in ) .",
    "it follows from this construction that @xmath161}(\\cdot),\\,r_{[t-1,t]},\\,d_{[t-1,t]})$ ] does not depend on @xmath141 and , for @xmath162 , @xmath163}(\\cdot),\\,r_{[s , t]},\\,d_{[s , t ] } )    \\hbox { is a function of }    ( ( u_n,\\kappa(n)),\\,n = s+1,\\dots , t-1).\\\\    & & \\eta_{[s , t ] } \\hbox { is a function of }    ( r_{[s , t]}(\\cdot),\\,r_{[s , t]},\\,d_{[s , t ] } ) \\hbox { and } ( u_t,\\kappa(t))\\,.\\end{aligned}\\ ] ] the @xmath121th coordinate of @xmath150}$ ] is either the interval @xmath17 or a point .",
    "the process is monotonous in the following sense : @xmath164}\\subset\\eta_{[s , t]}\\hbox { for all } s < t\\,.\\ ] ] in particular , if some coordinate is a point at time @xmath11 for the process starting at @xmath42 , then it will be the _ same _ point for the processes starting at previous times : @xmath165}(k ) \\hbox { is a point , then } \\eta_{[s-1,t]}(k ) = \\eta_{[s , t]}(k),\\qquad",
    "\\hbox { for all } s < t\\,.\\ ] ] for each time @xmath11 define @xmath166}(k ) \\hbox { is a point for all } k\\}\\,.\\ ] ]    using we conclude that @xmath167 is a stopping time for the filtration generated by @xmath168 : the event @xmath169 is a function of @xmath170 .",
    "assume @xmath171 almost surely for all @xmath11 and define the process @xmath172 in @xmath1 by @xmath173 } \\in \\cb\\,.\\ ] ] noticing that @xmath171 is equivalent to @xmath174}\\to 1 $ ] as @xmath175 , we get the following explicit expression for @xmath12 : @xmath176}^{-1}(u_t)\\,{\\bf 1}\\{r_{[n+1,t]}<u_t\\leq r_{[n , t]}\\}\\ , , \\qquad\\mbox { if}\\;k=\\kappa(t ) ;   \\end{array } \\right.\\,.\\end{aligned}\\ ] ]    for each @xmath177 and @xmath178 we now construct the process @xmath40},\\ , t\\ge s)$ ] , the gibbs sampler starting with @xmath43 at time @xmath42 .",
    "let @xmath179 and @xmath180}(x|\\zeta ) = r_{[s , t]}+ g_{\\kappa(t)}(x|\\zeta ) - r_{[s , t]}(x)\\,.\\ ] ] for each @xmath178 define @xmath181}=\\zeta$ ] and for @xmath182 , @xmath183 } ( k)&=&\\left\\ { \\begin{array}{lll }    \\zeta^\\zeta_{[s , t-1]}(k)\\,,\\qquad \\mbox{if}\\;k\\neq \\kappa(t),\\\\    \\\\    \\sum_{n = s}^{t-1 }    \\one\\{u_t\\in[r_{[n+1,t]},r_{[n , t]}]\\}\\ ,",
    "d^{-1}_{[n+1,t]}(u_t)\\\\ \\qquad\\qquad    \\qquad\\qquad + \\ ;    \\one\\{u_t > r_{[s , t]}\\ }    \\,\\hg_{[s , t]}^{-1}(u_t|\\zeta^\\zeta_{[s , t-1]})\\ , ,    \\qquad\\mbox { if } \\;k=\\kappa(t ) \\end{array } \\right.\\,.\\end{aligned}\\ ] ] for each fixed @xmath11 and @xmath121 , @xmath184}(k),\\ , \\zeta\\in\\cb),\\,s\\le t)$ ] is a maximal coupling among the processes starting with all possible configurations @xmath43 at all times @xmath158 .",
    "[ p31 ] assume @xmath171 almost surely for all @xmath185 .",
    "then the process @xmath9 defined in is a stationary gibbs sampler related to @xmath13 .",
    "the distribution of @xmath12 at each time @xmath11 is the distribution with density @xmath13 in @xmath1 .",
    "this distribution is the unique invariant measure for the process and @xmath186 } \\neq \\zeta_t ) \\le \\p(\\tau(t)<s)\\,,\\ ] ] where @xmath187}$ ] is the process starting with configuration @xmath43 at time @xmath42 .",
    "stationarity follows from the construction .",
    "indeed , for all @xmath158 and @xmath188 , @xmath150}$ ] as a function of @xmath189 is identical to @xmath190}$ ] as a function of @xmath191 .",
    "the function @xmath192\\times [ 0,1]^{\\{-\\infty,\\dots , t-1\\}}\\to b(\\kappa(t))$ ] defined in by @xmath193}^{-1}(u)\\,{\\bf      1}\\{r_{[n+1,t]}<u\\leq r_{[n , t]}\\}\\ ] ] satisfies for @xmath29 @xmath194 where we used that @xmath117 is function of @xmath195 and @xmath196 .",
    "this is sufficient to show that @xmath12 evolves according to the gibbs sampler .",
    "to finish we need to show that the distribution with density @xmath13 is the marginal law of @xmath12 . since the updating is performed with the conditioned distribution , the distribution with density @xmath13 is invariant for the gibbs sampler .",
    "since @xmath187}=\\zeta_{[s , t]}$ ] if @xmath197 , then follows .",
    "this also implies that @xmath198 } = \\zeta_t$ ] almost surely . in particular @xmath187}$ ]",
    "converges weakly to the law of @xmath12 as @xmath199 .",
    "the same is true for @xmath200}$ ] .",
    "taking @xmath43 random with law @xmath13 , this proves @xmath12 must have law @xmath13 .",
    "then taking @xmath43 with law @xmath201 invariant for the dynamics , we conclude @xmath202 and the uniqueness of the invariant measure follows .",
    "[ s22 ] a sufficient condition for @xmath171 _ a.s . _",
    "is that @xmath203 for all @xmath121 . in this case @xmath204",
    "decays exponentially fast with @xmath205 .",
    "@xmath206 @xmath207 and @xmath208 , the last time in the past the coordinates have been successively updated to a point independently of the other coordinates . at @xmath209 and successive times @xmath210 , each coordinate of @xmath211}$ ] has been reduced to a point . @xmath212 is finite for all @xmath11 because the event @xmath213 and @xmath208 occurs for infinitely many @xmath42 with probability one .",
    "the same argument shows the exponential decay of the tail of @xmath214 .",
    "[ [ remark ] ] remark + + + + + +    the velocity of convergence of the gibbs sampler to equilibrium depends on the values @xmath215 . in turn , these values depend on the size of the box @xmath1 and the correlations of the distribution @xmath13 . strongly correlated vectors produce small values @xmath216 and hence slow convergence to equilibrium .",
    "the efficiency of the algorithms discussed in the next sections will also depend on these values .",
    "the construction of section [ s3 ] is implemented in a perfect simulation algorithm .",
    "let @xmath126 be a box contained in @xmath1 . for each @xmath19 , let @xmath217 and @xmath218 as in and .",
    "when @xmath219 we denote by @xmath220 the value of @xmath221 .",
    "let @xmath22 be the updating schedule .",
    "it can be either a family of iid chosen uniformly in @xmath23 or the periodic sequence @xmath222_{{\\rm mod } \\",
    "d}+1 $ ] . for",
    "each pair @xmath223 of boxes contained in @xmath1 and @xmath224 , let @xmath225 $ ] be the function defined by @xmath226    let @xmath227 be boxes contained in @xmath1 , @xmath228 $ ] , and @xmath224 . the auxiliary _ coupler function _",
    "@xmath229 is defined by    ` function  ` @xmath230 ` : ` + @xmath231 + @xmath232 + ` if  ` @xmath233 `  is  not  a  point  in  ` @xmath82 `  and  ` @xmath234 ` , ` + `  update  the  ` @xmath121`th  coordinate  of  ` @xmath126 ` :  ` + `  `",
    "@xmath235 + ` end ` + @xmath236 + ` return(`@xmath237 ` ) `    [ [ perfect - simulation - algorithm ] ] * perfect simulation algorithm : * + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    perform the following steps    @xmath238 + ` start(`@xmath239 ` )  ` + ` while  ` @xmath239 `  is  not  a  point  in  ` @xmath1 + `  ` @xmath240 + `  start(`@xmath241 ` ) ` + `  ` @xmath242 + `  while  ` @xmath243 `  update  ` @xmath244 `  using  the  coupler  function  ` @xmath229 ` : ` + `  ` @xmath245 + `  ` @xmath246 + `  end  ` + ` end ` + ` return(`@xmath239 ` ) `    where , for each @xmath247 , `` start(@xmath248 ) '' is the following subroutine    ` start(`@xmath241 ` ) : ` + @xmath249 + ` generate  ` @xmath250 `  a  uniform  random  variable  in  ` @xmath34 $ ] + ` if  ` @xmath251 ` ,  compute  the  ` @xmath252`th  coordinate  of  ` @xmath241 ` : ` + `  ` @xmath253 + ` end `",
    "in this section we implement the construction in the normal case .",
    "we start with elementary facts of the one dimensional normal distribution .    [",
    "[ one - dimension ] ] one dimension + + + + + + + + + + + + +    let @xmath254 be the standard normal density in @xmath82 and @xmath255 the corresponding distribution function .",
    "let @xmath256 be real numbers .",
    "a random variable @xmath3 with mean @xmath0 and variance @xmath257 has _ truncated normal distribution _ to the interval @xmath14 $ ] if its density is given by @xmath258 where @xmath259 and @xmath260 .",
    "this distribution is called @xmath261 .",
    "the truncated normal is the law @xmath262 conditioned to @xmath14 $ ] .",
    "let @xmath263 let @xmath264 $ ] be a family of normal distributions truncated to the interval @xmath14 $ ] : @xmath265 .",
    "let @xmath266 and @xmath267 be defined by @xmath268 and @xmath269 let @xmath270 $ ] and @xmath271\\\\ r(i ) & = & r(b|i)\\,.\\end{aligned}\\ ] ] the next proposition , proven later , gives an explicit way of computing the functions participating in the multicoupling .",
    "it says that the infimum of normal densities with same variances @xmath272 and means in the interval @xmath270 $ ] coincides with the normal with mean @xmath273 up to @xmath274 and with the normal with mean @xmath275 from this point on .",
    "[ p22 ] the integrand in has the following explicit expression @xmath276 as a consequence,@xmath277{\\bf 1}\\{x < x(i,\\sigma)\\}\\nonumber\\\\ & & \\qquad+\\,\\frac{1}{a^{+}}\\left[\\phi\\left(\\frac{x(i,\\sigma)-\\mu^+}{\\sigma}\\right ) -\\phi\\left(\\frac{a-\\mu^+}{\\sigma}\\right)\\right]{\\bf",
    "1}\\{x\\geq x(i,\\sigma)\\}\\nonumber\\\\ & & \\qquad+\\,\\frac{1}{a^{-}}\\left[\\phi\\left(\\frac{x-\\mu^-}{\\sigma}\\right ) -\\phi\\left(\\frac{x(i,\\sigma)-\\mu^-}{\\sigma}\\right)\\right]{\\bf 1}\\{x\\geq x(i,\\sigma)\\}. \\nonumber\\end{aligned}\\ ] ] in particular , @xmath278 + \\frac{1}{a^{-}}\\left[\\phi\\left(\\frac{b-\\mu^-}{\\sigma}\\right )    -\\phi\\left(\\frac{x(i,\\sigma)-\\mu^-}{\\sigma}\\right)\\right]\\ ] ] and for each @xmath279 $ ] there exists a unique @xmath280 $ ] such that @xmath281 .",
    "[ [ remark-1 ] ] remark + + + + + +    @xmath282 .",
    "indeed , @xmath283 because @xmath284 . if @xmath285 , then all densities coincide unless @xmath286 ; by hypothesis this trivial case is excluded .    [ [ multivariate - normal ] ] multivariate normal + + + + + + + + + + + + + + + + + + +    let @xmath287 , @xmath288 be the vector conditioned to @xmath10 and @xmath289 .",
    "the law of @xmath290 conditioned to @xmath291 is the truncated normal @xmath292 , where @xmath293 we see that @xmath294 does not depend on @xmath25 and @xmath295\\,.\\end{aligned}\\ ] ] where @xmath296    [ [ remark-2 ] ] remark + + + + + +    since @xmath297 for all @xmath121 , @xmath171 almost surely for all @xmath185 .",
    "this implies that the truncated multivariate normal case falls under theorem s [ p31 ] hypothesis and our algorithm works .",
    "[ [ back - to - one - dimension .- minimum - of - truncated - normals ] ] back to one dimension .",
    "minimum of truncated normals + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we prove proposition  [ p22 ] . to simplify notation write @xmath298 instead of @xmath299 .",
    "observing that @xmath300 it is sufficient to prove for @xmath301 . the proof is based in the following elementary lemmas .    [ s14 ]",
    "let @xmath302 .",
    "then @xmath303 where @xmath304    [ s16 ] for all @xmath305 it holds @xmath306    [ [ proof - of - proposition - p22 ] ] proof of proposition [ p22 ] + + + + + + + + + + + + + + + + + + + + + + + + + +    let @xmath305 .",
    "by lemma [ s14 ] we have @xmath307 hence @xmath308 if and only if @xmath309\\,.\\end{aligned}\\ ] ] but by lemma [ s16 ] the interval in is empty for all @xmath310 .",
    "this implies that@xmath311 , from where @xmath312 holds for @xmath301 . the corresponding identity ( [ marzo1 ] )",
    "follows by applying lemma [ s14 ] to @xmath313 and @xmath314 .",
    "in this section we compare our method with the rejection method in some examples .",
    "the conclusion is that the rejection method may be better than ours in dimension 2 for some regions but ours becomes better and better as dimension increases .",
    "then we show why the method does not work when the region is not a box .",
    "this discards the following tempting approach : multiply the target vector by a matrix to obtain a vector with iid coordinates .",
    "the transformed vector is much easier to simulate ; the problem is that it is now conditioned to a transformed region .",
    "when the region is not a box , the conditioned vector is not an iid vector and a coupling must be performed .",
    "we show here that in general the corresponding coupling event has probability zero .",
    "[ [ the - rejection - method ] ] the rejection method + + + + + + + + + + + + + + + + + + + +    we compare our algorithm with the following rejection algorithm : simulate a uniformly distributed vector @xmath315 in @xmath316 $ ] . if @xmath317 , then accept @xmath318 .",
    "we use @xmath319 one - dimensional uniform random variables for each attempt of the rejection algorithm .",
    "let @xmath320 be the expected number of one - dimensional uniform random variables generated by our perfect simulation algorithm , each counted the number of times that it is used .",
    "let @xmath321 be the expected number of one - dimensional uniform random variables needed by the rejection algorithm to produce an accepted value .",
    "we divide the experiments in two parts : @xmath322 and @xmath15 .    @xmath323 $ ] .",
    "let @xmath324 and @xmath325 .",
    "we consider two cases .",
    "( i ) boxes @xmath326\\times[0,1]+(x_1,x_2)$ ] , @xmath327 and @xmath328 .",
    "( ii ) boxes @xmath329\\times[0,r]$ ] ( type @xmath330 ) and @xmath331\\times[0,r]$ ] ( type @xmath332 ) , @xmath333 .",
    "the results are shown in table  [ uno ] for ( i ) and in figure [ dos ] for ( ii ) .",
    "@xmath334    @xmath335 $ ] example 1 .",
    "fix @xmath336 and @xmath337 , where @xmath66 is the identity matrix and @xmath338 .",
    "we consider two cases : ( i ) boxes @xmath339^d$ ] , @xmath340 and ( ii ) boxes @xmath341^d$ ] , @xmath342 .",
    "the results are shown in figure [ tres ] and table [ cuatro ] for ( i ) and ( ii ) , respectively .",
    "@xmath343    @xmath335 $ ] example @xmath332 .",
    "let @xmath336 and @xmath344 be the matrix , with `` neighbor structure '' , @xmath345 , with @xmath346 .",
    "we consider boxes of the form @xmath347^d$ ] .",
    "the results are shown in figure [ cinco ] .",
    "[ [ strongly - correlated - variables ] ] strongly correlated variables + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the algorithm slows down very fast with the dimension when the variables are strongly correlated .",
    "an associate editor proposes to consider a gaussian vector truncated to the box @xmath348^d$ ] , with mean @xmath349 and covariance matrix @xmath350 where @xmath66 is the identity matrix and @xmath351 , for small values of @xmath352 . as a measure of the speed of the algorithm",
    "we have computed the coefficient @xmath99 , shown in table  [ x2 ] , as function of @xmath352 and the dimension @xmath6 .",
    "@xmath353    in @xmath322 the coupling time @xmath52 for this example is a geometric with mean @xmath354 , because a it suffices that one of the uniforms is smaller than @xmath99 to get the coupling in the next step . in general , as explained in lemma [ s22 ] , @xmath355 , whose expectation is of the order @xmath356 .",
    "[ [ other - regions ] ] other regions + + + + + + + + + + + + +    let @xmath357 be a @xmath6-dimensional multinormal random vector with zero mean and nonsingular covariance matrix @xmath358 .",
    "let @xmath359\\times\\cdots\\times[a_d , b_d]$ ] .",
    "if we rotate and scale the coordinates to obtain iid @xmath360 s , i.e. , if we put @xmath361 , then the box constraints are of the form @xmath362 , where @xmath363 and @xmath364 . when the constraints are of this form , the conditional distribution of @xmath365 has a probability density function whose support depends on the condition @xmath366 . in such case",
    "the maximal coupling in our approach may have a coupling event with zero probability .",
    "indeed , it could happen that @xmath367 this implies that @xmath368 for all @xmath25 .    to see this in @xmath322 ,",
    "observe that the transformation takes the rectangular box into a parallelogram @xmath369 with sides not in general parallel to the coordinate axes .",
    "we need to simulate the standard bi - dimensional normal @xmath4 conditioned to @xmath369 .",
    "it is still true that the law of @xmath370 conditioned on @xmath371 is normal truncated to the slice of the parallelogram @xmath372 .",
    "it is a matter of geometry that there are different @xmath26 and @xmath373 such that @xmath374 ; this implies the infimum above is zero .",
    "take for instance @xmath324 , @xmath375 and @xmath5 conditioned to the box @xmath347\\times[0,1]$ ] .",
    "then @xmath376 , and the transformed box is a parallelogram @xmath369 with vertices @xmath377 .",
    "we have proposed a construction ( simulation ) of a multidimensional random vector conditioned to a bounded box using a version of the coupling from the past algorithm of the gibbs sampler dynamics . in the case of normal distribution ,",
    "we have given a explicit construction taking advantage of the bell shape of the density and the fact that the distribution of one coordinate given the others is a truncated normal with variance independent of the other coordinates .",
    "this allows to define the coupling set @xmath78 in function of the extreme possible values of the mean and to obtain positive probability for each coupling set @xmath78 as soon as the box is bounded .",
    "one of the consequences of the approach is to show that the gibbs sampler is ergodic in the box if the coupling event has probability uniformly bounded below in each one of the coordinates .",
    "this corresponds to the condition @xmath203 for all coordinate @xmath121 . when the uniform random variable used to update site @xmath121 at some time",
    "falls below @xmath203 , this coordinate coincides for all processes .",
    "the method can be used to show ergodicity of the process in infinite volume when the inverse of the covariance matrix has a neighbor structure ; for instance for tridiagonal matrices . as an example , consider the formal density in @xmath14^{\\z^d}$ ] : @xmath378 which corresponds to an infinite volume gaussian field , each coordinate truncated to @xmath14\\subset \\r$ ] ; @xmath379 is a parameter ( inverse temperature in statistical mechanics ) .",
    "the gibbs sampler can be performed in continuous time and the coupling can be done as in the finite case .",
    "however the updating of site @xmath121 depends only on the configurations in sites @xmath380 such that @xmath381 .",
    "each site gets a definite value ( and not an interval ) when the corresponding uniform random variable falls below a certain value @xmath382)$ ] ( corresponding to @xmath216 , but now it is constant in the coordinates ) . to determine the value of site @xmath121 at time @xmath11 one explores backwards the process ; calling @xmath88 the random variable used for the last time ( say @xmath383 ) before @xmath11 to update site @xmath121 there are two cases : @xmath384 or @xmath385 . if @xmath384 , we do not need to go further back , the value is determined . if @xmath385 , we need to know the values of @xmath386 neighbors at time @xmath383 . repeating the argument",
    ", we construct a `` oriented percolation '' structure which will eventually finish if @xmath387 .",
    "this value is obtained by dominating the percolation structure with a branching process which dies out with probability @xmath99 and produces @xmath386 offsprings with probability @xmath388 .",
    "the value of @xmath99 depends on the length of the interval @xmath14 $ ] and the strength of the interaction governed by @xmath389 .",
    "one can imagine that for small intervals and small @xmath389 things will work .",
    "we thank christian robert and havard rue for discussions .",
    "propp , j. g. and wilson , d. b. ( 1996 ) exact sampling with coupled markov chains and applications to statistical mechanics .",
    "proceedings of the seventh international conference on random structures and algorithms ( atlanta , ga , 1995 ) .",
    "_ random structures algorithms * 9 * _ , no . 1 - 2 , 223252 .",
    "d.  b. wilson .",
    "annotated bibliography of perfectly random sampling with 77arkov chains . in d.",
    "aldous and j.  propp , editors , _ microsurveys in discrete probability _ , volume  41 of _ dimacs series in discrete mathematics and theoretical computer science _ , pages 209220 .",
    "american mathematical society , 1998 .",
    "updated versions can be found at http://dimacs.rutgers.edu/126dbwilson/exact ."
  ],
  "abstract_text": [
    "<S> the target measure @xmath0 is the distribution of a random vector in a box @xmath1 , a cartesian product of bounded intervals . </S>",
    "<S> the gibbs sampler is a markov chain with invariant measure @xmath0 . </S>",
    "<S> a `` coupling from the past '' construction of the gibbs sampler is used to show ergodicity of the dynamics and to perfectly simulate @xmath0 . an algorithm to sample vectors with multinormal distribution truncated to @xmath1 </S>",
    "<S> is then implemented . </S>"
  ]
}