{
  "article_text": [
    "a galaxy is a self - gravitating system where stellar dynamics is governed by newton s law .",
    "it could be naively described as a set of @xmath2 coupled , second - order , non - linear ordinary differential equations , where @xmath3 is the number of stars , which ranges between @xmath4 and @xmath5 @xcite .",
    "solving such an equation set numerically is practically only possible at the very low end of the @xmath3-range , and even so very challenging with current computer hardware .",
    "thus , various techniques are used to simplify the mathematical description of the system ; these are often designed to fit a particular problem in stellar dynamics and yield unphysical results when applied to another problem .",
    "direct @xmath6-body simulation is one of the main techniques used to study gravitational systems in general and galaxies in particular . in this technique ,",
    "the distribution function is sampled at @xmath7 points in a monte - carlo fashion .",
    "this @xmath6 depends on the computational capabilities , and an astrophysical system with @xmath8 stars might be represented numerically by a sample of just @xmath4 `` supermassive '' particles .",
    "this seems to be allowed because of the equivalence principle and the fact that gravitation is scale free , unlike , for example , in molecular dynamics .",
    "however in gravity too this simplification can cause problems , as some dynamical effects depend on number density rather than just mass density .",
    "the most well known @xmath6-dependent effect in stellar dynamics is two - body relaxation .",
    "the relaxation time , the characteristic time for a particle s velocity to change by order of itself due to encounters with other particles , scales with the crossing time roughly as @xmath9 .",
    "thus , the ratio between the relaxation times in a real and a simulated system is of similar order of magnitude to the undersampling factor .",
    "this could be taken into account when interpreting the result of an undersampled simulation , but a poorly sampled distribution function might have other , unexpected , consequences .",
    "galaxies are often described as collisionless stellar systems , which means that the relaxation time is much larger than the timescale of interest ( except perhaps at the very center ) .",
    "this property could be very useful : since a particle s orbit is basically what it would be if it were moving in a smooth gravitational field , we could evaluate the field instead of calculating all stellar interactions , this is cheaper computationally .",
    "another useful property is that galaxies are often spheroidal in shape .",
    "even highly flattened galaxies will have a spherical dark halo component .",
    "thus , a spherical shape could be used as a zeroth order approximation for the gravitational field , and higher order terms could be written using spherical harmonics .",
    "the goal of this paper is to examine two techniques that utilize both these facts .",
    "these are the multipole expansion ( mex ) and the self - consistent field ( scf ) methods .",
    "they historically come from different ideas , and as explained below in detail , they are mathematically distinct . in the context of numerical simulations , however , they serve a similar function : to evaluate the gravitational force on all @xmath6 particles generated by this same collection of particles , in a way that discards spurious small scale structure ( in other words , smooths the field ) .",
    "mex was born of the need to ease the computational burden .",
    "the idea is that given spherical symmetry , gauss s law says that the gravitational force on a particle at radius @xmath10 from the center is simply @xmath11 , towards the center , where @xmath12 is the enclosed ( internal ) mass .",
    "the gravitational constant , @xmath13 , will be omitted in the following text .",
    "this idea was used by @xcite who simulated clusters with up to 100 particles to study phase mixing due to spherical collapse .",
    "this `` spherical shells '' methods is mex of order zero and was also used for the same purpose by @xcite .",
    "the extension of this this idea is that when spherical symmetry breaks , corrections to the force can be expressed by summing higher multiples ( dipole , quadruple , etc . ) of the other particles , both internal and external to @xmath10 .",
    "@xcite used such a code to study a stellar cluster of a 1000 stars embedded in a galactic potential , truncating the expansion at @xmath14 .",
    "@xcite used a variation of this method to study galaxy collision .",
    "these authors employed a grid and conducted simulations of also up to @xmath15 and @xmath14 .",
    "they additionally assumed azimuthal symmetry which reduced the number of terms in the expansion .",
    "@xcite , @xcite , @xcite and @xcite all use variations of this method , with additional features which are partly discussed in secion [ sec : discussion - mex ] .",
    "see also @xcite for a review .",
    "the prehistory of scf is rooted in the problem of estimating a disk galaxy s mass distribution from its rotation curve .",
    "@xcite proposed a mathematical method to generate a surface density profile and a corresponding rotation curve ( related to the potential ) by means of a hankel transform , and introduced a family of such pairs .",
    "@xcite used toomre s idea , but in reverse : to calculate the gravitational field from an arbitrary 2d density , he generated an orthogonal set of density profiles and their corresponding potentials .",
    "this solved two problems ( 1 ) with his orthogonal set it was possible to represent any flat galaxy as a finite linear combination of basis functions , and ( 2 ) unwanted collisional relaxation was curbed due to the smooth nature of the reconstructed gravitational field .",
    "cf . a related method by @xcite .",
    "@xcite introduced a 3d extension of his method , which was called scf by ( * ? ? ?",
    "* hereafter ho92 ) by analogy to a similar technique used in stellar physics @xcite ; further historical developments are discussed in section [ sec : radial - basis ] .    to exploit recent developments in the world of general purpose computing on gpus",
    ", we implemented both scf and mex routines in a code called _ etics _",
    "( acronym for _ expansion techniques in collisionless systems _ ) . in section [ sec : formalism ] we explain the mathematical formalism of both methods and highlight the differences between them . in section 3 we explain the unique challenges in a gpu implementation and measure the code s performance . in section 4",
    "we discuss the accuracy of expansion and direct techniques . in section 5 we present a general discussion and finally summarize in section 6 .",
    "here we clarify some terms used throughout this work :    expansion methods : :    a way to get potential and force by summing a series of terms ; in this    paper either mex or scf .",
    "mex : :    multipole expansion method ( sometimes known in the literature as the    spherical harmonics method ) ; expansion of the angular part .",
    "scf : :    self - consistent field method ; a `` pure '' expansion method since both    angular and radial parts are expanded .",
    "_ etics _ : :    expansion techniques in collisionless systems ; the name of the code we    wrote , which can calculate the force using both mex and scf , using a    gpu .",
    "gpu : :    graphics processing unit ; a chip with highly parallel computing    capabilities , originally designed to accelerate image rendering but is    also used for general - purpose computing .",
    "it often lies on a video    card that can be inserted into an expansion slot on a computer    motherboard .",
    "both mex and scf methods are ways to solve the poisson equation : @xmath16 the formal solution of which is given by the integral : @xmath17 the expression @xmath18 is the green s function of the laplace operator in three dimensions and in free space ( no boundary conditions ) , and the integral is over the whole domain of definition of @xmath19 . in an @xmath6-body simulation , the density field @xmath19",
    "is sampled at @xmath6 discrete points @xmath20 , such that @xmath21 where @xmath22 is the 3d dirac delta function .",
    "direct @xmath6-body techniques evaluate integral ( [ eq : pot-3d ] ) _ directly _ : @xmath23 and thus at each point @xmath24 where the potential is evaluated , require @xmath6 calculations of inverse distance , or @xmath25 if @xmath26 , since there is no self - interaction . in practice , we are interested in evaluating the potential at the same points in which the density field is sampled , and thus a `` full '' solution of the poisson equation requires @xmath27 inverse distance calculations . in both mex and scf the integrand in equation ( [ eq : pot-3d ] ) is expanded as a series of terms , each of which more easily numerically integrable ; this is done in two different ways , lending the two methods quite different properties . in both methods , the reduction in numerical effort comes at the expense of accuracy compared to direct - summation , but this statement is arguable since in practice direct @xmath6-body techniques use a very small number of particle to sample the phase space .    to demonstrate the difference between the two approaches in the following section ,",
    "let us consider a 1d version of integral ( [ eq : pot-3d ] ) ; let us further assume that the density exists in the interval @xmath28 : @xmath29 note that this is not a solution for a 1d poisson equation ( hence the notation @xmath30 instead of @xmath31 ) , but just a simplification we will use to illustrate the properties of each method .",
    "we will conveniently ignore the fact that this integral is generally divergent in 1d , as it does not affect the following discussion . in brief",
    ", mex is a _",
    "taylor_-like expansion of the green s function , while scf is a _",
    "fourier_-like expansion of the density .",
    "this already hints at the most critical difference between the mex and scf : while the former , like a taylor series , is local in nature , the latter is global .",
    "another way to look at it is that in both methods the integrand is written as a series of functions ( of @xmath32 ) with coefficients : in mex one uses the given density to evaluate the functions , while their coefficients are known in advance ; in scf one evaluates coefficients , while the functions are known in advance .",
    "let us define @xmath33 and expand the green s function equivalent in equation ( [ eq : pot-1d ] ) around @xmath34 , we get that for @xmath35 or @xmath36 : @xmath37 while for @xmath38 or @xmath39 we can expand around @xmath40 : @xmath41 the first and second terms of integral ( [ eq : pot-1d ] ) define the functions @xmath42 and @xmath43 ( utilizing the commutativity of the sum and integral operations ) : @xmath44 and thus @xmath45.\\ ] ] while seemingly we made things worse ( instead of one integral to evaluate , we now have a series of integrals ) , the fact that @xmath32 has moved from the integrand to the integral s limit greatly simplifies things .",
    "let the density @xmath46 be sampled at @xmath47 discrete and ordered points @xmath48 ; it is easy to show that @xmath49 in other words , each of these functions is a cumulative sum of simple terms and can be evaluated at all @xmath50 in just one pass , but a sorting operation is required .",
    "let us leave the green s function as it is , and instead expand the density as a generalized fourier series : @xmath51 where @xmath52 is a complete set of real or complex functions ( the basis functions ) ; orthonormality of the basis functions is assumed above .",
    "the integral ( [ eq : pot-1d ] ) becomes : @xmath53 the function set @xmath54 is defined by the above integral .",
    "in essence , we replaced the integral over an arbitrary density @xmath46 with an integral over some predefined ` densities ' @xmath55 .",
    "the advantage is that we can calculate the corresponding potentials , @xmath56 in advance , and then the problem is reduced to numerically determining the coefficients @xmath57 .",
    "the choice of the basis is not unique , and an efficient scf scheme requires that the following :    1 .",
    "the functions @xmath55 and @xmath56 are easy to evaluate numerically .",
    "the sum ( [ eq : scf - sum ] ) convergence quickly , or in other words , @xmath58 is already close to @xmath46 .",
    "the standard form of mex in 3d is @xmath59y_{lm}(\\theta,\\phi)\\label{eq : mex - phi}\\\\ q_{lm}(r ) & = & \\int_{r'<r}r^{\\prime l}\\rho({\\bf r}')y_{lm}^{*}(\\theta',\\phi'){\\rm d}^{3}r'\\label{eq : mex - qlm}\\\\ p_{lm}(r ) & = & \\int_{r",
    "< r'}r^{\\prime-(l+1)}\\rho({\\bf r}')y_{lm}^{*}(\\theta',\\phi'){\\rm d}^{3}r'.\\label{eq : mex - plm}\\end{aligned}\\ ] ] all together there are @xmath60 complex function pairs ( not counting negative @xmath61 , which are complex conjugates of the others ) that need be calculated from the density . since in practice the density field",
    "is made of @xmath6 discrete points , they must be sorted by @xmath10 in order for the above integrals to be evaluated in one pass .    the standard form for scf is : @xmath62 all together there are @xmath63 complex coefficients ( not counting negative @xmath61 ) that need be calculated from the density .",
    "a typical choice is @xmath64 , for which there would be 308 coefficients .",
    "the radial basis functions and coefficients for scf are discussed in the next section .",
    "spherical harmonics are used in both cases to expand the angular part , but alternatives exist , such as spherical wavelets ( e.g. @xcite ) .",
    "mex has two sums ( one infinite ) while scf has three sums ( two infinite ) . in practice",
    ", the radial and angular infinite sums must be cut off at @xmath65 and @xmath66 , respectively .",
    "the finite sum could in principle also be truncated to discard azimuthal information .",
    "simply equating the expressions gives the relation between the two methods : @xmath67=\\sum_{n=0}^{\\infty}a_{nlm}\\phi_{nl}(r),\\ ] ] where @xmath68 is the @xmath69-pole . in case",
    "the system is azimuthally symmetric , @xmath70 for all @xmath71 .",
    "also , the same azimuthal information is carried in positive and negative @xmath61 terms , and they are related to each other by complex conjugation .    if one decompose the density to a spherical average @xmath72 and the non - spherical deviation @xmath73 , then it is easy to show that @xmath74 depends only on the spherical average , while all other term depend only on the deviation . in a spherically symmetric system only @xmath75",
    "is nonzero , and setting @xmath76 yields an accurate result .",
    "while the choice of @xmath66 depends only on the deviation of the system from spherical symmetry , the choice of @xmath65 in scf depends on how well the system is described by the the zeroth radial basis function , and is usually determined by trial and error ( see section [ sec : inifinite - particle ] ) .",
    "it is interesting to note a nontrivial mathematical difference between the two methods .",
    "one can show that the laplacian of equation ( [ eq : mex - phi ] ) is zero when substituting the appropriate expressions for @xmath77 and @xmath78 from the equations ( [ eq : mex - qlm ] ) and ( [ eq : mex - plm ] ) ; the proof is mathematically cumbersome and will not be brought here .",
    "this is surprising , since according to the poisson equation the result should be proportional to the density .",
    "one can not appeal to series truncation to resolve this apparent contradiction ; indeed each term in the formally non - truncated infinite series yields a zero density , despite representing the multipoles as continuous functions .",
    "the solution is that the potential at point @xmath79 has contributions from all internal ( i.e. at @xmath80 ) particles ( represented by @xmath77 ) and all external particles ( represented by @xmath78 ) , but no information about the density at @xmath79 itself .",
    "this is the case also when the potential is constructed by a direct - summation of all gravitational point sources , so one may say mex is similar to direct methods in this sense . in scf , by construction , taking the laplacian of equation ( [ eq : scf - phi ] ) leads right back to the density field ( equation [ eq : poisson ] ) .",
    "one can thus use the coefficients @xmath81 to represent a smoothened field .",
    "one can also use mex for this purpose , if the derivatives of @xmath68 are calculated on a grid or with a spline .",
    "a key difference between mex and scf is the freedom of choice of _ radial _ basis .",
    "there are in fact two function sets : the radial densities @xmath82 and the radial potentials @xmath83 ; they are related via the poisson equation @xmath84 ( in this case @xmath85 only contains derivatives with respect to @xmath10 ) .",
    "the choice of basis is not unique , and the basis functions themselves need not represent physical densities and potentials ( i.e. @xmath86 could be negative ) .",
    "however it is convenient to take the zeroth term ( @xmath87 ) to represent some physical system , and to construct the rest of the set by some orthogonalization method , such as the gram ",
    "schmidt process .",
    "the idea of @xcite was to use a @xcite model as the zeroth term and construct the next orders using the gegenbauer ( ultraspherical ) polynomials and spherical harmonics ( cf .",
    "@xcite who developed a virtually identical method for finite stellar systems using spherical bessel functions for the radial part ) .",
    "ho92 constructed a new radial basis ( also using gegenbauer polynomials ) which zeroth order was a @xcite model ; this is the basis set we adopt in _ etics_. they argued that this basis was more well suited to study galaxies .",
    "more basis sets followed .",
    "@xcite used the idea of @xcite , that the basis does not have to be biorthogonal , to construct as set which zeroth order was oblate . @xcite gave a radial basis set for the more general @xmath88-model ( of which both plummer and hernquist are special cases ) and @xcite introduced a basis for thick disks in cylindrical coordinates .",
    "@xcite , @xcite describe numerical derivation of the radial basis set so that the lowest order matches any initial spherically - symmetric model , so called `` designer basis functions '' . @xcite introduced an analytical set which zeroth order is the perfect sphere of @xcite .",
    "there are several levels of task parallelism available when writing computer code . at one level",
    ", tasks are performed in parallel on different computational units ( such as cpus ) but only one copy of the data exists , which is accessed by all tasks ; this is called a _ shared memory _ scheme .",
    "the tasks are called `` threads '' , and they are generally managed within one `` process '' of the program . a higher level of parallelism is called _ distributed memory _",
    "scheme , where tasks are performed on different units ( often called `` nodes '' ) , but each unit has access only to its own memory ; thus data must be copied and passed . in this case",
    "the parallel tasks are different processes , and cooperation between them is facilitated by a message passing interface ( mpi ) .",
    "the parallel programming model is different between shared and distributed memory ; the former is considered easier since threads can faster and more easily cooperate . a high - performance supercomputer will generally enable parallelism on both levels : these machines are made of multiple nodes , each of which has its own memory and multiple computational units .",
    "graphics processing units ( gpus ) are powerful and cost - effective devices for high performance parallel computing .",
    "they are used to accelerate many scientific calculations , especially in astrophysics , such as dynamics of dense star clusters and galaxy centers ( @xcite ; @xcite ; @xcite ; @xcite ; see review by @xcite ) .",
    "the gpu contains its own memory and many computational units , thus it is a shared memory device .",
    "scf force calculation is particularly easy to parallelize , since the contribution of each particle to the coefficients @xmath81 is completely independent of all other particles .",
    "particle data can be split to smaller chunks ( each could be on a different node ) , from each chunk partial @xmath81-s are calculated . then the partial coefficients summed up and the result communicated to all the nodes .",
    "this was done by ( * ? ? ?",
    "* hereafter h95 ) , whose code used the mpi call ` mpi_allreduce ` to combine the partial coefficients .",
    "this parallelization scheme , however , is not suitable for gpus , as discussed in section [ sec : implementation - scf ] .",
    "mex force calculation is harder to parallelize since the contribution of each particle depends on its position in a sorted list ( by radius ) .",
    "however , in a shared memory scheme this too could be achieved relatively easily as explained in the following section .",
    "the current implementation of the mex method relies on _ thrust _ @xcite , a c++ template library of parallel algorithms which is part of the cuda framework .",
    "it makes parallel programming on a shared memory device ( either a gpu or a multicore cpu ) transparent , meaning that the task is performed in parallel with a single subroutine call , and the device setup and even choice of algorithm is performed by the library . _",
    "thrust _ provides a sorting routine that selects one of two algorithms depending on input type . in the current version of mex and using version 1.6 of _ thrust _ , a general merge sort algorithm @xcite is used .",
    "a flowchart of the entire mex routine is shown in fig .",
    "[ fig : flowchart - mex ] .",
    "the flow is controlled by the cpu , and boxes with double - struck vertical edges indicate a gpu - accelerated operation .",
    "the blue double - struck boxes represent _ thrust _ calls , while the black ones are regular cuda kernel calls .",
    "when a gpu operation is in progress , the cpu flow is paused .",
    "[ fig : memory - mex ] shows the four main memory structures of the program and how the _ thrust _ subroutines and kernels in the program operate on them .",
    "the particle array contains all particle coordinates and also the distance square from the center , which needs to reside in this structure for the sorting operation ( in practice the particle array contains additional data such as i d and velocity , but this is not used by the mex routine ) ; the cache structure contains functions of particle coordinates which are needed to calculate the multipoles .",
    "kernel1 , which is executed once , reads the coordinates , calculates those functions and fills the cache structure .",
    "kernel2 calculates the spherical harmonics at the current @xmath89-level and from that the contribution of the particle to @xmath77 and @xmath78 , which are saved in global memory .",
    "when this kernel returns , the _ thrust _ subroutines are dispatched to perform the cumulative sum .",
    "the `` scan '' ( forward cumulative sum ) and `` r. scan '' ( reverse scan ) are both in fact calls to the ` exclusive_scan ` subroutine , but to perform the reverse scan , we wrap @xmath78 with a special _ thrust _ structure called ` reverse_iterator ` . not shown in the flowchart , the two scan subroutines have to be called @xmath89 times at each @xmath89-level since they work on one @xmath61 value at a time .",
    "kernel3 has both cache and compute operations : it calculates the partial forces in spherical coordinates ( i.e. the @xmath89-order correction to the force ) and/or potentials by evaluating all the spherical harmonics again ( and their derivatives with respect to spherical coordinates ) . later it advances @xmath90 and @xmath91 to the next @xmath89-level ( except at the last iteration ) . finally , the last kernel operates on the force structure and transforms it to cartesian coordinates .",
    "[ fig : pie - mex ] shows the relative time it takes to do the internal operation .",
    "we note that the potential could be calculated at the same time as the force ( in kernel3 ) and stored at another memory structure ( not shown in fig .",
    "[ fig : memory - mex ] ) but is skipped if only forces are needed . alternatively",
    ", only the potential could be calculated ( this is faster since the derivatives of the special functions are not calculated ) .",
    "the choice between calculating force , potential or both is done with c++ templates .",
    "we first briefly explain the serial algorithm used by ho92 .",
    "the force ( and potential ) calculation had two parts : ( 1 ) calculation of all the @xmath81-s ( the plural suffix ` -s ' to emphasize that there are hundreds of coefficients in this 3d structure ) and ( 2 ) calculation of all the forces using the coefficients .    in both parts ,",
    "the particle loop ( the @xmath92-loop ) was the external one , inside of which there are again two main steps . in step ( 1a )",
    "all necessary special functions were calculated using recursion relations .",
    "step ( 2a ) was identical but additionally , the derivatives of those functions were calculated . in step ( 1b )",
    "there was a nested loop ( @xmath93-@xmath89-@xmath61 structure ) in which a particle s contribution to every @xmath81was calculated and added serially . in step ( 2b ) there was also such a loop , which used all the @xmath81-s to calculate the force on each particle .    in the parallel algorithm used by h95",
    ", another part was added between the two parts mentioned above : communicating all partial @xmath81-s from the various mpi processes , adding them up and distributing the results . in practice",
    "it was achieved using just one command , ` mpi_allreduce ` .",
    "there are two main reasons why this algorithm could not be used effectively on a gpu , both are related to the difference between how the gpu and cpu access and cache memory . the first difficulty is performing the sum .",
    "the partial sums from the different parallel threads could in principle be stored on a part of the gpu memory called _ global memory _ , and then summed in parallel .",
    "however a modern gpu can execute tens of thousands of threads per kernel ( note that the concept of a thread in cuda is abstract , and the number of threads by far exceed the number of thread processors on the gpu chip ) , and every partial @xmath81  is @xmath94 kilobyte in size ( depending on @xmath95 and @xmath96 ) .",
    "thus , writing and summing the partial coefficients would require extensive access to global memory , which is slow compared to the actual calculation part .",
    "the second difficulty is that if one thread uses too much memory , for example to store all necessary legendre and gegenbauer polynomials as well as complex exponent ( as is done in the ho92 code ) , this may lead to an issue called _ register spilling _ , where instead of using the very fast register memory , the thread will store the values on the slow global memory , which again we wish to avoid on performance grounds .    to tackle those issues we utilized another type of gpu memory called _",
    "_ shared memory__. this memory is `` on chip '' ( on the multiprocessor circuit rather than elsewhere on the video card ) and has @xmath97 lower latency than global memory .",
    "threads in a cuda program are grouped into blocks , threads in the same block share this fast memory ( hence the name ) .",
    "it is also much less abundant than global memory .",
    "the nvidia tesla k20 gpus have just 64 kilobytes of shared memory per block , while they have 5 gigabytes of global memory .",
    "in order to use shared memory to calculate the coefficients , each thread would serially add contributions from particles to the partial @xmath81-s on shared memory ; then they would be summed up in parallel in each block .",
    "however , there are usually hundreds of different @xmath81-s , as well as tens or hundreds of threads per block ( depending on hardware ; which is required for efficient loading of the gpu ) ; there is not enough shared memory for that ( by far ) . to solve this",
    ", we changed the order of the loops : the external loop is the @xmath89-loop , then comes the @xmath93-loop . for each @xmath98 pair",
    ", a cuda kernel is executed where the @xmath92-loop is performed in parallel on different threads , inside of which the @xmath61-loop is done .",
    "now each threads has to deal with far fewer @xmath81-s ( no more than @xmath99 ) , for which there is usually enough shared memory .",
    "but for the scf routine .",
    "]        a flowchart of the entire scf routine is shown in fig .",
    "[ fig : flowchart - scf ] .",
    "the flow is controlled by the cpu , and boxes with double - struck vertical edges indicate a cuda kernel call .",
    "when a gpu operation is in progress , the cpu flow is paused .",
    "[ fig : memory - scf ] shows the four main memory structures of the program and how the five kernels in the program operate on them .",
    "the particle array contains all particle coordinates ( in practice it contains additional data such as i d and velocity , but this is not used by the scf routine ) ; the cache structure contains functions of particle coordinates which are needed to calculate the basis functions .",
    "kernel1 , which is executed once , reads the coordinates , calculates those functions and fills the cache structure .",
    "kernel2 only operates on the cache structure , it has just one function which is to advance @xmath100 by one level ; thus it needs to be executed at the beginning of each iteration of the @xmath89-loop .",
    "as shown in the flowchart , it is skipped for @xmath101 because kernel1 calculates and caches @xmath102 .",
    "kernel3 has both cache and compute operations : it calculates the current @xmath103 using recursion relations from the cached @xmath104 and @xmath105 and then updates the cache .",
    "later it calculates the spherical harmonics and from that the contribution of the particle to the all @xmath81 in the current ( @xmath93,@xmath89)-level , which are saved in shared memory .",
    "when all threads in the block have finished calculating contributions of the particles assigned to them , they are synchronized and a parallel reduction is performed . since threads from different blocks can not share memory , the data from each block must be transfered to the host machine s memory and the cpu finishes the summation process .    for the force calculation , just a reading the @xmath81-s is required .",
    "the gpu has yet another type of memory which is ideal for storing of coefficient or constant parameters .",
    "it is fittingly called `` constant memory '' , and is as fast as shared memory when every thread in a warp accesses the same memory element .",
    "it is also very limited ( usually to 64 kilobytes per device ) , but the @xmath81structure could still fit there nicely .",
    "once calculation of all the coefficients is complete , it is transferred back to the gpu constant memory to be used to calculate the forces . since only reading the coefficient is required , in kernel4 which calculates the forces and/or potentials by evaluating all the basis functions again ( and their derivatives with respect to spherical coordinates ) , the @xmath92-loop is the external one . to avoid register spilling",
    "we keep the internal loop structure as @xmath89-@xmath93-@xmath61 , and thus we only need to recalculate the complex exponents , which is relatively cheap .",
    "finally , the last kernel operates on the force structure and transforms it to cartesian coordinates . fig .",
    "[ fig : pie - scf ] shows the relative time it takes to do the internal operation .",
    "we note that the potential could be calculated at the same time as the force ( in kernel4 ) and stored at another memory structure ( not shown in fig .",
    "[ fig : memory - scf ] ) but is skipped if only forces are needed . alternatively , only the potential could be calculated ( this is faster since the derivatives of the special functions are not calculated ) . the choice between calculating force , potential or both is done with c++ templates .",
    "we tested the performance of _ etics _ ( both mex and scf ) on a single nvidia tesla k20 gpus on the laohu supercomputer at the naoc in beijing . for comparison",
    ", we also tested the fortran cpu scf code by lars hernquist on the accre cluster at vanderbilt university in nashville , tennessee ( we used a node with intel xeon e5520 cpu ) .",
    "if the initial conditions are not sorted by @xmath10 in advance , the first mex force calculation is more costly than all the following , since the sorting of an already nearly - sorted particle list is faster .",
    "thus , all measurements of the mex code are done after the system is evolved one very short leapfrog time step . fig .",
    "[ fig : scaling ] shows the time it takes to do one full force calculation as a function of @xmath6 , @xmath96 and @xmath95 .",
    "each point represents the mean time of 10 different calculations .",
    "the dispersion is generally very low , with the exception of _ etics_-mex with @xmath106 ; only for which we show error bars .",
    "note that the timing only depends on the number of particles ( and expansion cutoffs ) and not on their spatial distribution .",
    "the cpu and gpu scf codes are both theoretically @xmath107 . at low @xmath6",
    "the gpu is not fully loaded , and _ etics _ performance seems superlinear with @xmath6 .",
    "_ etics_-mex is theoretically @xmath108 , but this again is an asymptotic behavior which is not observed .",
    "the lack of good gpu load for @xmath109 is much more evident than the @xmath110 nature of the algorithm .",
    "the gpu global memory was the limiting factor in how many particles could be used with both methods .",
    "the dotted lines show the performance of _ etics _ using single - precision instead of double .",
    "the speed increase is 61% for scf and 65% for mex , but there is a price to pay in accuracy as noted in section [ sec : single ] .",
    "the speedup factor could be very different for different gpus .",
    "all codes should scale quadratically with @xmath96 , but as the middle panel of fig .",
    "[ fig : scaling ] shows , this behavior is not so clear for _ etics_-mex .",
    "this is due to the extensive memory access this code requires , which rivals the calculation time .",
    "memory latency on gpus is not easy to predict ; due to caching and the way memory is copied in blocks , and the latency depends not only on the amount of memory accessed but also on the memory access pattern .",
    "scf codes theoretically scale linearly with @xmath95 .",
    "a strange behavior of the cpu code is noted : it seems that the time increases with @xmath95 in a `` zigzag '' fashion ( the measurement error of the times is much smaller than this effect , and it is reproducible ) .",
    "this is paradoxical : it takes a shorter time to calculate with @xmath111 than with @xmath112 , even though more operations are required .",
    "it is not simple to understand why this is , but it seems that the compiler performs some optimization on the first @xmath92-loop ( coefficient computation ) that only help when @xmath95 is odd but not when it is even .",
    "the comparison between _ etics_-gpu and hernquist s code is not exactly fair since they use different types of hardware . specifically for hardware we tested , _",
    "etics_-gpu outperforms hernquist s code by a factor of about 20 ( which depends little on all parameters ) .",
    "however , hernquist s code can utilize a multicore cpu ( using mpi ) .",
    "the xeon cpu we used has 4 cores , and two such cpus are mounted on a single accre node .",
    "we could use the fortran code in mpi mode on all 8 effective cores with almost no overhead , and the calculation is accelerated by a factor of 8 .",
    "also , hernquist s code calculate the jerk ( force derivative ) , which _",
    "etics_-gpu does not ; this takes @xmath113 percent of the total time .     and @xmath114 ) ; the total time is 0.15 sec on nvidia tesla k20 gpu .",
    "the results may differ significantly on different hardware and if single - precision is used instead .",
    "the first operation is sort , followed counter - clockwise by initialization of the cache arrays , the @xmath89-loop where each iteration is divided to ( a ) summand calculation , ( b ) cumulative sum and ( c ) partial force calculation .",
    "the final operation is coordinate transformation from spherical to cartesian ]     for a full scf force calculation with _ etics _",
    "( @xmath115 particles , @xmath114 and @xmath116 ) ; the total time is 0.16 sec on nvidia tesla k20 gpu .",
    "the first operation is initialization , followed counter - clockwise by the @xmath89-loop ( in which the @xmath93-loop is nested ) .",
    "the partial force calculation is a single cuda kernel , inside of which all the loops are performed . ]",
    "[ fig : pie - mex ] and [ fig : pie - scf ] show the fraction of time it takes to perform the internal operations for the force calculation for _ etics_-mex and -scf , respectively , both use @xmath115 , @xmath114 and for scf , @xmath116 . for mex , operations inside each iteration of the @xmath89-loop are shown in different shades ( also denoted by letters corresponding to stages 3a , 3b and 3c as explained in section [ sec : implementation - mex ] ) .",
    "the most costly operations are the ones we entrust to _ thrust _ , namely the sorting and cumulative sum . in fig .",
    "[ fig : pie - scf ] the internal structure of each @xmath89-iteration is not shown ( since there are too many internal operations , including the @xmath93-loop ) .",
    "the force calculation is executed as one operation ( a single cuda kernel call ) , and includes the @xmath89-loop nested inside it ( unlike mex where only a partial force was calculated at every @xmath89-iteration , step 3c ) .",
    "two separate questions come up when discussing the accuracy of expansion methods : how well the expansion approximates the @xmath6-body force ( i.e. direct - summation ) , and how well it approximates the smooth force in the limit of infinite particles ( which we will refer to as the `` real '' force in the following discussion ) .",
    "both questions depend on @xmath6 , @xmath96 and ( for scf ) @xmath95 .",
    "a related question is how well the @xmath6-body force approximates the real force , as a function of @xmath6 .",
    "all these questions depend not only on the expansion cutoff and @xmath6 , but on the stellar distribution as well ( e.g. global shape , central concentration , fractality , etc . ) ; this will not be fully explored in this work .",
    "there are two types of error when considering the expansion methods versus the real force , analogous to systematic and random errors .",
    "the first , systematic - like error , comes from the expansion cutoff , this is called the _",
    "bias_. for example , a system which is highly flattened could not be described by keeping just the quadrupole moment , so both mex and scf cut off at @xmath117 would exhibit this type of error , regardless of @xmath6 ( see @xcite ; @xcite for discussion about bias due to softening ) .",
    "the second , random - like error , comes from the finite number of particle and their coarse grainy distribution ; it is the equivalent of @xmath6-body noise ( also referred to as particle noise or sampling noise ) .",
    "ho92 attempted to estimate accuracy of scf by showing convergence of the coefficient amplitudes with increasing @xmath93 for the density profiles of some well known stellar models .",
    "they showed that @xmath118 decayed exponentially or like a power law with @xmath93 , depending on the model .",
    "this analysis was not satisfactory because it applied to the limit of infinite @xmath6 , thus ignoring the random - like error .",
    "furthermore , showing convergence of the coefficients does not give information about the force error .",
    "the bias and the random error are not easy to distinguish .",
    "the bias could be calculated , in principle , only if the true mass density @xmath19 is known , which is not generally the case ; however , it is still useful to look at some particular examples where it is known .    to test the accuracy of the expansions techniques , we used two simple models for the mass density . both our models are ferrers ellipsoids @xcite ( often called ferrers bars ) with index . ] @xmath119 : ` model1 ` is a mildly oblate spheroid with axis ratio of 10:10:9 , ` model2 ` is triaxial with axis ratio of 3:2:1 .",
    "ferrers ellipsoids are often used in stellar dynamics , especially in the modeling of bars ( e.g. @xcite ) .",
    "they have a very simple mass density : @xmath120^{n } & \\mu\\leq a\\\\ 0 & \\mu > a \\end{cases}\\ ] ] where @xmath121 , @xmath122 and @xmath123 are the axes , @xmath124 is the central density , @xmath125 is the index and @xmath126 is the ellipsoidal radius , defined by : @xmath127 the potential due to this family of models is simply a polynomial in @xmath128 if @xmath93 is an integer . the coefficients could be calculated numerically ( also analytically for some cases ) by solving a 1d integral ( * ? ? ? * chapter 2.5 ) . for both our models we used the mathematical software _",
    "sage _ to calculate the coefficients to better than @xmath129 .",
    "the force vector components are trivially derived from the potential polynomial ; this is the `` real '' force .",
    "we created many realization of these two models , ranging from just 100 particles to @xmath130 .",
    "the goal is to compare for each realization the force calculated using mex , scf and direct - summation ( no softening ) , with the real force .",
    "all calculations performed using double - precision , and the direct - summation force is not softened . for each realization",
    "we get a distribution of @xmath6 values of the relative force error , @xmath131 where @xmath132 is the particle s index .",
    "it is not practical to show to full distribution for all cases , so in figs .",
    "[ fig : force - error1 ] and [ fig : force - error2 ] we show the mean , and the full distribution for only selected cases .    the left panel of fig .",
    "[ fig : force - error1 ] shows the mean relative force error @xmath133 in ` model1 ` for direct - summation and mex with even @xmath96 between @xmath134 and @xmath135 ; odd terms are in principle zero if the expansion center coincides with the center of mass , and in practice very small . for this model",
    ", @xmath133 is decreasing with @xmath6 for all cases but @xmath136 ( monopole only ) .",
    "the smallest error is for @xmath137 ( monopole and quadrupole only ) .",
    "unintuitively , adding correction terms _ increases _ the error ( for constant @xmath6 ) , this is because the model s deviation from sphericity is so mild , that the quadruple describes it well enough ; the following terms just capture some of the @xmath6-body noise in the realization and make more harm than good . in the right panel",
    "we show the full log - distribution for selected cases .",
    "the histograms for @xmath138 are made by stacking of @xmath139 realizations , so there are @xmath130 values of @xmath140 in all histograms . in all cases the @xmath140 distributions are close to log - normal",
    "; the logarithmic horizontal axis hides the fact that the distributions on the right are much wider in terms of standard deviation due to a very long and fat tail when viewed in linear space .",
    "note that while the number of particles increased by 1000 , in all cases the error distribution shifted down by just a factor of @xmath141 .",
    "[ fig : force - error2 ] is the same but for the triaxial ` model2 ` . while in the @xmath6-body cases",
    "the @xmath140 distributions are much the same , mex shows a different behavior . the most prominent feature is the bump on right side of the @xmath115 , @xmath114 error distribution , which demonstrates the issue of bias .",
    "most of the particles which make up this bump are located in the lobes of the ellipsoid , where many angular terms are required . when @xmath96 is increased to @xmath135 ,",
    "this bump disappears .",
    "it also is not present in the @xmath138 , @xmath114 case , probably because it is overwhelmed by the random error .",
    "this bump causes the mean error to saturate with particle number , as the left panel shows .",
    "increasing @xmath6 will shift the bulk of the bell curve to the left ( zero ) , but will not quench the bump . at much larger @xmath6 ,",
    "` model1 ` will show the same behavior as the random error becomes smaller than the bias , and the high-@xmath96 cases would outperform @xmath137 .",
    "we repeat this exercise for scf , which has an additional source of bias due to the radial expansion cutoff . fig .",
    "[ fig : force - scf - error2 ] illustrates that point by showing the relative force error distribution in ` model2 ` for scf compared to mex with the same number of particles ( @xmath115 ) and same angular cutoff ( @xmath142 ) . with increasing @xmath95 , the scf error distribution approaches that of mex , demonstrating the point made in section [ sec:3d ] that mex is equivalent to scf with @xmath143 .",
    "it must be noted that the basis set we programmed in _ etics _ is not at all suitable for ferrers models ( which are finite and have a flat core ) , and the apparently slow convergence should not disparage one from using scf , even if it is not known in advance what basis to choose .",
    "the overlap between the relative force error distributions of scf at @xmath116 and mex is 77% . a more intelligent choice of basis function is discussed by @xcite , who used a similar methodology to choose the best basis set for triaxial @xcite @xmath144-models @xcite with @xmath145 from a family of basis sets similar to the ho92 .",
    "the results presented in this section suggest that there is some optimal expansion cutoff , which is different for different models and depends on the number of particles @xcite .",
    "this is analogous to optimal softening in direct - summation force calculations @xcite .",
    "if not enough terms are used , there is a large bias ; if too many terms are used , the particle noise dominates .",
    "@xcite addressed this issue by calculating the variance of each scf coefficient among several realizations of the same triaxial dehnen model , found that for @xmath115 particles , angular terms beyond @xmath146 are dominated by noise ( and that only the first few @xmath93 , @xmath61 terms at that l - level are reliable ) .    the force error discussed above is not directly related to energy diffusion or relaxation , which are reduced due to the smoothing , but not absent .",
    "the mechanism for energy ( and angular momentum ) diffusion in both expansion methods is temporal fluctuation of the multipoles or coefficients ( due to the particle noise ) .",
    "this is somewhat analogous to two - body relaxation in that the potential felt by every particle fluctuates ( although in this case there is no spatial graininess ) .",
    "* in prep . )",
    "examined energy diffusion in a plummer sphere with @xmath147 particles using scf and direct @xmath6-body codes , and found that scf demonstrated a diffusion rate only several times lower , which was close to the rate in a direct technique using near - optimal softening for this @xmath6 .",
    "further reduction was achieved by discarding of expansion terms which are nominally zero in any triaxial system centered around the expansion center .",
    "finally , vasiliev used temporal softening ( ho92 ) , where the coefficients ( and thus the potential ) are updated in longer intervals than the dynamical time step ; this procedure however introduces a global energy errors unless some measures are taken to amend this .            .",
    "the green histogram is also shown in the right panel of fig .",
    "[ fig : force - error2 ] and represent a mex expansion with @xmath142 .",
    "the other histograms represent scf expansions with @xmath142 and varying @xmath95 values as shown . with increasing @xmath95 , the scf error distribution approaches that of mex with the same @xmath96 .",
    "in this case , the model differs greatly from the zeroth order function of the basis set , showing relatively slow convergence . ]",
    "due to their original intended use , gpus are not optimized for double - precision arithmetic ( indeed early gpus completely lacked a double - precision floating - point type ) . in cards that do support double - precision , arithmetic operations",
    "could still be significantly slower than for single . as noted before , in our test we measured a 6065% speed increase when using single - precision .",
    "the nvidia tesla k20 gpus we used have enhanced double - precision performance with respect to other gpus , for which using double - precision may be significantly slower .",
    "those devices are somewhat specialized for scientific use and are thus more expensive ( albeit in many applications still superior to parallel cpu architectures in terms of price / performance ratio due to the low energy consumption ) .",
    "cpus usually take the same time to perform an arithmetic operation in either single- or double - precision , but a program s general performance could be faster in single - precision due to smaller memory load . for the hernquist - scf cpu code , we measured a 6% improvement in speed .",
    "using single - precision however inevitably reduces the accuracy of the calculated force ; here we examine how bad this _ performance - accuracy trade - off _ is .    fig .",
    "[ fig : single ] show the relative force error distributions of single - precision calculations , compared to double .",
    "the relative force error on particle @xmath132 is now defined as : @xmath148 we testes an @xmath115 realization of a hernquist sphere with characteristic scale of one unit .",
    "the top panel shows two scf force calculations : the green histogram ( on the left ) is a low order expansion up to @xmath149 , retaining 36 coefficients ; the red histogram is an expansion up to @xmath150 , retaining 308 coefficients .",
    "the bottom panel similarly shows two mex expansions . in both cases ,",
    "the higher order expansion has relatively large errors . while it is still smaller than the error with respect to the `` real '' force discussed in section [ sec : inifinite - particle ] , its nature is numeric and it could hinder energy conservation .",
    "the relatively large error is not remedied by usual methods to improve accuracy of floating point arithmetic such as kahan summation algorithm @xcite , because the error does not come from accumulation of round off errors . instead , the accuracy bottle neck is the calculation of the spherical harmonics and/or the gegenbauer polynomials .",
    "particles for which those special functions are calculated with large numerical error will have a large force error , but additionally they contribute erroneously to _ all _ the coefficient or multipoles , thus causing some error in the force calculation of all other particles as well .",
    "there are two groups of particles with large relative error in this implementation : particles that are very far away from the center , and particles which happen to lie very close to the @xmath151-axis .",
    "the former group is not so problematic since the absolute force is very small as well as their contribution to the coefficients or multipoles .",
    "the latter group causes large error because the recursion relation used to calculate the associated legendre polynomials : @xmath152 is not upwardly stable because of the @xmath153 factor , which diverges when the polar angle @xmath154 is very small or very close to @xmath155 ( although the polynomials themselves approach zero in these limits ) .",
    "the distributions shown in fig .",
    "[ fig : single ] may vary significantly depending on the model .",
    "for example , ferrers ellipsoids are finite and flat at the center , thus they do not contain the problematic particles described above and have much smaller error in single - precision .",
    "a hernquist sphere is more representative of the general case in galaxies , being infinite and relatively centrally concentrated .",
    "one could conceivably improve the accuracy at single - precision in several ways . in the test",
    "described above everything was calculated in single - precision , apart from some constant coefficients that were only calculated once , in double - precision , and then cast to single .",
    "it may be possible to identify the most sensitive parts of the force calculation and use double - precision just for those , or use pseudo - double - precision ( as in @xcite ) for part of or the entire force calculation routine .",
    "another possibility is to keep using single - precision for everything but prescribe special treatment to those orbits close to the @xmath151-axis .    ) .",
    "the model used is a hernquist sphere with @xmath115 .",
    "the top and bottom panels shows scf and mex force calculations , respectively . in both cases",
    "the left ( green ) histogram is a lower - order expansion as indicated in the legend . ]",
    "expansion techniques , on their own , are best geared to simulate systems with a dominant single center , where it is important to minimize the effects of two - body relaxation , and where the system potential does not change radically ( quickly ) with time .",
    "an ideal class would be long - term secular evolution in a near - equilibrium galaxy .",
    "both methods presented in this work can be used to quickly calculate the gravitational potential and force on each particle in a many - body system , while discarding small scale structure .",
    "mex comes from a taylor - like expansion of the green s function in the formal solution of the poisson equation , while scf is a fourier - like expansion of the density .",
    "both methods are important tools for collisionless dynamics and has been used extensively in astrophysics as discussed in the following sections .",
    "they are comparable in terms of both accuracy and performance . in both methods , there are free parameters to be set :          the choice of length unit ( or model scaling ) affects the accuracy of scf expansion because the zeroth order of the radial basis functions corresponds to a model of a particular scale .",
    "for example , the basis set offered by _ etics",
    "_ corresponds to a @xcite model with scale length @xmath156 .",
    "the main difference for the end - user is that scf smooths the radial direction as well .",
    "this could be an advantage when @xmath6 is very small , since scf will still provide a rather smooth potential , although it might not represent the real potential well at all due to random error . in mex",
    ", particles are not completely unaware of each other , and every time two particles cross each other s shell , there is a discontinuity in the force , which may lead to large energy error when @xmath6 is small .",
    "this shell crossing occurs when two particles change places in the @xmath10-sorted list , and the particles need not be close to each other at all .",
    "both methods have some problems close to the center . in scf ,",
    "the limitation comes from both radial and angular expansions .",
    "the radial expansion cutoff induces a bias if the central density profile does not match the zeroth basis function , and a very non - spherical model would cause force bias at the center as well as the lobes .",
    "the latter is also a problem for mex , which has two additional problems : the discrete nature and inevitably small number of particles when one gets arbitrarily close to the center , as well as the numerical error ( and/or small step size required ) due to having to calculate @xmath157 ( the scf basis function we use are completely regular at the center ) .",
    "it is clear from the literature that scf has been by far more popular . but",
    "despite the above , we do not think that most authors intentionally avoided mex , and that scf was better publicized and became the standard .",
    "mex is rarely used in its full form , but more frequently in the spherically symmetric version , sometimes called the `` spherical shells '' method ; in this case just the monopole term is kept ( @xmath136 ) .",
    "for example , it is used in the poisson solvers of @xcite monte carlo method .",
    "this hints that it might be easy to extends codes like mocca @xcite to non - spherical cases using our version of mex .",
    "this monopole approximation has also been used to study dark and stellar halo growth @xcite .",
    "the extension of the spherical case using spherical harmonics exists in several variations , divided roughly to two classes : grid and gridless codes .",
    "the mex version presented in this work is gridless and follows from @xcite and @xcite .",
    "these authors used cartesian instead of spherical coordinates , and softened the potential at the center .",
    "this softening , albeit similar mathematically , is not equivalent to particle - particle softening in direct @xmath6-body simulations and was just used to prevent divergence at the center .",
    "the first mex code however is by @xcite , who divided the simulation volume into thick shells , and the force on a particle was calculated by summing the multipoles of all shells except its own ( own - shell correction was added ) .",
    "similarly , @xcite used a mex code with @xmath158 to explore galaxy correlation functions ; in their version each shell had six particles , and softened newtonian interaction was used within a shell .",
    "as noted in the introduction , @xcite used a variation with axial symmetry ( up to @xmath14 but with no azimuthal terms , namely @xmath159 ) , with a grid in both @xmath10 and @xmath154 . in a follow up work @xcite the method was extended to 3d geometry .",
    "finally , @xcite used a grid in @xmath10 only , with logarithmic spacing .",
    "he argues that softening sacrifices the higher resolution near the center ( which is one of the primary advantages of the method ) and that a radial grid smooths the potential and prevents shell crossing .",
    "recently , @xcite presented a similar potential solver with a spline instead of a grid .",
    "we note that a virtually identical mathematical treatment to the mex method has been applied to solve the fokker - planck equation under the local approximation ( neglecting diffusion in position ) .",
    "the collisional terms of the fokker - planck equation can be written by means of the rosenbluth potentials @xcite , which are integrals in velocity space very similar in form to equation ( [ eq : pot-3d ] ) .",
    "@xcite assumed azimuthal symmetry and wrote the rosenbluth potentials using the legendre polynomials up to @xmath160 in a way exactly analogous to our equation ( [ eq : mex - phi ] ) .",
    "this treatment was expanded to @xmath161 by @xcite .",
    "as noted in the previous section , scf gained much more popularity than mex .",
    "the scf formalism has had wide use on galaxy - scale problems .",
    "it has been used to model the effect of black hole growth or adiabatic contraction on the structure ( density profile ) of the dark matter halo  ( e.g. * ? ? ?",
    "scf is also an appropriate tool to model the growth of the stellar and dark matter halos  ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ) as well as the mass evolution of infalling satellite galaxies  ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "one of the clearest uses of the scf technique is when the stability of the orbit matters such as in the study of chaos in galactic potentials  ( e.g * ? ? ?",
    "* ; * ? ? ?",
    "* ) , and in the exchange of energy and angular momentum by mean resonances  ( e.g * ? ? ?",
    "* ; * ? ? ?",
    "@xcite compare a number of methods and show that scf is superior for stability work .    the initial motivation for _ this _ work was to follow up on @xcite , who studied supermassive black hole binaries using a restricted technique . in their method ,",
    "the stellar potential was held constant while the black holes were treated separately as collisional particles ; it was thus not self - consistent in terms of the potential .",
    "this class of problems , where there is a small subset of particles that need to be treated collisionally , has already been attempted using an extension of the expansion technique which hybridizes scf and direct aarseth - type gravitational force calculation ; in these extensions , either the black holes are the only collisional particle  ( e.g * ? ? ?",
    "* ; * ? ? ?",
    "* ) , or all centrophilic particles are treated collisionally  @xcite .",
    "mex has not been applied to this particular problem to our knowledge , although it is as well suited as scf .",
    "our scf implementation on gpu outperformed the serial hernquist cpu version by a factor of @xmath162 ( for double - precision ) , but this number depends on the particular gpu and cpu hardware compared .",
    "the cpu code is definitely competitive on multi - core cpus .",
    "intel recently introduced the many integrated core architecture ( known as intel mic ) , which are shared memory boards with the equivalent of tens of cpus . in principle , the fortran scf code for cpus could be adapted for this architecture with little modification , and it will most likely outperform the gpu version . on the other hand , next generation gpus ( such as nvidia s maxwell architecture ) would also deliver performance improving features , and it is not clear which one would win .",
    "the goal of this project is to ultimately enable simulations of @xmath163 , and to perform them fast enough so that many could be performed , exploring large parameter space rather than making a few such large-@xmath6 simulations . to do that",
    ", the code will be adapted to a multi - gpu and multi - node machines using mpi . as noted in section [ sec : implementation - parallel ] , this is easy for scf but not so much for mex .    simultaneously we will attempt to improve the per - gpu performance .",
    "we spent a lot of time trying to optimize this first version of _ etics _",
    ", by no means we guarantee that out implementation is flawless .",
    "some improvement might come from tweaking of the implementation .",
    "for example , we decided not to cache @xmath164 but rather recalculate it in - kernel before every @xmath61-loop ( as a starting point for the recursion relation ) . since the legendre polynomials are `` hard - coded '' and computed very efficiently , it is not immediately clear if caching is a more efficient approach ( it is probably worth while at very high @xmath96 ) .",
    "likewise , we chose to separate the caching operations that are performed once per routine or once per external loop , and execute them as independent kernels , while in principle they could be executed as statements inside the inner kernels ( so called `` kernel fusion '' , which would save kernel execution overhead ) , with an if - statement making sure that the cache operations are performed only if needed .",
    "some possible more fundamental changes include trying to get rid of the sorting operation in mex ; while the most basic approach requires the particle list to be sorted and a cumulative sum performed over the multiples , some alternatives exist such as logarithmic grid ( as in @xcite ) or spline ( as in @xcite ) . also we might find a more sophisticated way to perform the cumulative sum , since we suspect that the _ thrust _ routines are not optimal for our uses .",
    "another improvement might come from the integration side rather than force - calculations , such as implementation of higher order integrator instead of the leapfrog .",
    "hernquist s scf code already contains a 4th order hermite scheme @xcite , which is not hard to implement for gpus , but mex has a fundamental problem with this scheme due to shell crossing , which causes the force derivatives to be discontinuous .",
    "_ etics _ is a powerful code , but as with any computer program , one should understand its limitation .",
    "the code in its current form should not be used for highly flattened system , or where two - body interactions are significant .",
    "the code is momentarily available upon request from the authors , but we plan to make it public , including a module to integrate it with the amuse framework @xcite .",
    "we thank peter berczik , adi nusser , marcel zemp and eugene vasiliev for the interesting and helpful discussions and the referee for useful comments .",
    "ym is grateful for support from the china postdoctoral science foundation through grant no .",
    "2013m530471 . the special gpu accelerated supercomputer laohu at the center of information and computing at national astronomical observatories , chinese academy of sciences , has been used for the simulations ; it was funded by ministry of finance of people s republic of china , under the grant zdy z2008 - 2 , and by the `` recruitment program of global experts '' ( qianren ) for rainer spurzem ( 2013 ) .",
    "rs has been partially supported by nsfc ( national natural science foundation of china ) , grant no ."
  ],
  "abstract_text": [
    "<S> we present gpu implementations of two fast force calculation methods , based on series expansions of the poisson equation . </S>",
    "<S> one is the self - consistent field ( scf ) method , which is a fourier - like expansion of the density field in some basis set ; the other is the multipole expansion ( mex ) method , which is a taylor - like expansion of the green s function . </S>",
    "<S> mex , which has been advocated in the past , has not gained as much popularity as scf . </S>",
    "<S> both are particle - field method and optimized for collisionless galactic dynamics , but while scf is a `` pure '' expansion , mex is an expansion in just the angular part ; it is thus capable of capturing radial structure easily , where scf needs a large number of radial terms . we show that despite the expansion bias , these methods are more accurate than direct techniques for the same number of particles . the performance of our gpu code , which we call _ etics _ , is profiled and compared to a cpu implementation . on the tested gpu hardware , a full force calculation for one million particles took @xmath0 seconds ( depending on expansion cutoff ) , making simulations with as many as @xmath1 particles fast on a comparatively small number of nodes . </S>"
  ]
}