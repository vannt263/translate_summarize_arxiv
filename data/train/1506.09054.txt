{
  "article_text": [
    "at the heart of the theory of compressed sensing is the paradigm that it is possible to recover a sparse vector @xmath4 using few linear measurements . a widely used method to perform",
    "this reconstruction is basis pursuit , i.e. to solve the minimization problem @xmath5 a fundamental result is that @xmath6 gaussian random measurements are sufficient for basis pursuit to succeed at recovering an @xmath0-sparse vector with overwhelmingly high probability@xcite .",
    "the main problem about recovering a signal in this setting is in fact to decide where its support @xmath7 is situated . indeed ,",
    "if we knew the exact position of @xmath7 , we would be able to recover @xmath8 by simply solving a set of linear equations which have more equations than unknowns , provided @xmath9 .",
    "it is thus plausible that if we are given prior information about the location of @xmath7 , it should be possible to use this information to lower the threshold amount of measurements needed to secure recovery . to be precise ,",
    "let us assume that we know that a fraction @xmath10 of the indices in some set @xmath11 are in the support of @xmath8 .",
    "this can be interpreted as the probability for an @xmath12 to be an element of @xmath7 .",
    "we will subsequently denote @xmath13 , for the sake of brevity .",
    "intuitively , it seems that if @xmath10 is high while @xmath14 is not too large , we should be able to find the solution using less equations than if we did not have the initial guess .    at ( current page.south ) ;    this exact situation was investigated in the paper @xcite .",
    "their proposed solution is to use a weighted @xmath2-approach , namely to solve the minimization problem @xmath15 where @xmath16 denotes the weighted @xmath2-norm , given by @xmath17 ( @xmath16 should of course not be confused with the weak @xmath2-norm . )",
    "we can likewise define the reweighted @xmath18-norms for every @xmath19 $ ] ; @xmath20    choosing the weights @xmath21 is a subtle process ",
    "but the general idea is that the weights on the indices @xmath11 should be lower .",
    "this will have the effect that high values @xmath22 on @xmath11 are not penalized , which is desirable if @xmath23 .",
    "many papers have been concerned with the choice @xmath24 ( the setting corresponding to this choice is sometimes called _ modified compressed sensing _",
    "@xcite ) , but the authors of @xcite suggest that one should give oneself the freedom to choose @xmath25 as @xmath26 for some @xmath27 $ ] . here , as well as in the following , @xmath28 refers to the indicator function of a set @xmath29 , i.e. @xmath30    tedious arguments by the authors of @xcite showed that provided that the guess is good ( @xmath31 ) , the required bound on the rip - constant for the matrix @xmath32 needed for robust recovery can be softened , and the stability constants get smaller , if one chooses @xmath33 .",
    "the formulas actually suggest that one either should use @xmath34 or @xmath35 , and never something in between .",
    "numerical experiments do not harmonize with this rule of thumb : in fact , if the guess is bad ( @xmath36 ) , one does significantly better choosing @xmath37 between @xmath38 and @xmath39 than choosing one of the extreme values .",
    "the authors left it as an open problem to analyze how to choose the weight in an optimal way .",
    "it is of course not entirely clear what  optimal",
    " means in this context .",
    "one way of defining it is to say that a weight is optimal if the minimal amount of measurements needed in order for @xmath40 with @xmath41 to recover @xmath8 is as small as possible .",
    "a popular model assumption is that @xmath32 is chosen according to some probability distribution . because of its universality as a limit distribution , the gaussian probability distribution is probably the most canonical one . in this setting",
    ", we search for the minimal amount of measurements required for @xmath42 to succeed with high probability ( the exact meaning of  high  varies from application to application ) .",
    "the first result concerning choosing @xmath37 optimally in this sense was probably theorem 4.3.3 of the phd - thesis @xcite ( the result was also presented in @xcite ) . based on a very sophisticated argument relying on calculating internal and external angles of certain polytopes ,",
    "the authors implicitly provide a threshold @xmath43 so that if one uses @xmath44 gaussian measurements , @xmath40 will succeed with high probability .",
    "one can then minimize @xmath43 with respect to the weight @xmath37 .",
    "since the formulas are very complicated , this is however a tough task .",
    "a result which is simpler to grasp is given in @xcite .",
    "the strategy is basically the same .",
    "one computes a threshold depending on the weight @xmath37 , using the famous ",
    "escape through a mesh lemma  @xcite and then minimizes the threshold with respect to @xmath37 .",
    "although the paper provides powerful and beautiful results on how the optimal threshold is depending on @xmath10 and @xmath45 , it does not directly provide any method of actually calculating the optimal weights .",
    "furthermore , as the authors point out , the result is only about an upper bound on how many measurements one needs .    a different analysis conducted in @xcite .",
    "the authors define a _ weighted null space property _ and prove several interesting result about that property .",
    "in particular , they calculate upper bounds on thresholds on the number of measurements needed to secure that gaussian matrices have that property .",
    "the main technical tool is again gordon s escape through a mesh lemma . as a concrete weighting strategy",
    ", it is proposed to set the weight @xmath46 , since that weight minimizes their bound on the thresholds .",
    "one should note that the weighted null space property is uniform in nature in the sense that it is equivalent to that all signals supported on a set @xmath11 will be recovered from its linear measurements through weighted @xmath2-minimization as long as the quality of the support estimate is good enough - its exact position does not matter .      in this paper , we will consider the full model with @xmath47 subsets @xmath48 of the complete index set @xmath49 $ ] , each one with a corresponding probability @xmath50 .",
    "correspondingly , we will allow for @xmath47 weights @xmath51 , one on each @xmath48 .",
    "we will use the short - hand notation @xmath52 .",
    "the strategy of finding the optimal weights is very similar to the ones of the papers mentioned above  we will calculate a threshold @xmath53 depending on @xmath37 and then prove that @xmath43 is minimized for an optimal set of weights . to be precise , we will in fact not minimize @xmath53 itself , but only an upper and lower bound on @xmath53 .",
    "these bounds are however simultaneously minimized for the same @xmath54 and also lie very close to each other .",
    "we will also investigate how the mentioned bounds on @xmath53 depend on the properties of the sets @xmath48 , @xmath55 .",
    "in particular , we will find the perhaps somewhat remarkable fact that one needs in principle as many measurements to recover an @xmath56-sparse signal given the prior knowledge of the probabilities @xmath57 and the sets @xmath58 as recovering @xmath47 signals @xmath59 separately , where for each @xmath60 , @xmath59 is @xmath61-sparse and known to be supported on the set @xmath48 .",
    "this was already proven for upper bounds on the measurements needed in @xcite , but here , we provide also a statement about lower bounds .    in order to do this , we will use a powerful and recent result from @xcite , namely that the recovery probability of a convex program undergoes a _ phase transition _ as the number of measurements surpass the _ statistical dimension _",
    "@xmath62 of a certain convex cone .",
    "we will provide a way of calculating a set of weights @xmath54 which simultaneously minimizes a tight _ lower and upper _ bound of @xmath62 , and hence to some extent settle the problem regarding the optimal choice of weights when recovering a sparse vector @xmath8 with prior information .",
    "additionally , we will even provide a simple analytical formula for @xmath54 .",
    "the fact that we are using the new notion of statistical dimensions is the main technical difference from the prior work mentioned in the previous section , where the escape through a lemma was the main tool in most cases .",
    "the notation in the paper will be fairly standard .",
    "there are however a few things that should be pointed out . for a vector @xmath63",
    ", @xmath64 is the diagonal matrix in @xmath65 whose diagonal is @xmath25 .",
    "@xmath66 is the positive part of a real number , i.e. @xmath67 if @xmath68 , and otherwise @xmath69 . for @xmath70",
    ", @xmath71 denotes the vector consisting of the signs of the entries of @xmath72 , with the convention @xmath73 .",
    "finally , @xmath74 will have different meanings depending on its argument . if @xmath11 is a real number",
    ", @xmath74 denotes its absolute value . for vectors ,",
    "@xmath74 is the vector formed by taking the absolute value pointwise . if @xmath11 is a set , @xmath74 denotes the number of elements of @xmath11 .",
    "let us begin by fixing the model situation and notation .",
    "@xmath4 is an @xmath0-sparse vector , meaning that only @xmath0 of the entries @xmath75 are unequal to zero .",
    "additional to the prior information that @xmath8 is sparse , we are given some partial information where the support @xmath76 of @xmath8 is situated , expressed by the partition @xmath77 of @xmath78 $ ] . with each set",
    "@xmath48 we associate the two parameters @xmath50 and @xmath79 @xmath80 reflecting its size and the probability for a @xmath81 to be an element in @xmath76 , @xmath82 , respectively . to each @xmath48 , we will assign a weight @xmath83 and always choose the weights according to @xmath84 we will without loss of generality assume that @xmath85 for every @xmath60 ( if @xmath86 for some @xmath60 , the corresponding set of indices @xmath48 can be completely ignored in the recovery process . )",
    "now we define the object which we will search for in the rest of this section : the threshold amounts of measurements needed for the program @xmath40 to be successful .",
    "[ def : thresholds ] let @xmath87 be a gaussian matrix .",
    "then @xmath88 denotes the normalized threshold amount of measurement needed for @xmath89 with @xmath90 to succeed at recovering an @xmath0-sparse vector @xmath8 , i.e. , @xmath89 will succeed with high probability if @xmath91 , and will fail with high probability if @xmath92 .    in the exact same way",
    ", we define for a given support @xmath76 and partition @xmath77 the number @xmath93 as the threshold amount of measurements for @xmath40 with @xmath90 to succeed at recovering a vector @xmath8 supported on @xmath76 . finally , @xmath94 is the value obtained by choosing the weights @xmath25 optimally , i.e. @xmath95 .    note that we are only concerned with exact recovery of exactly sparse signals , and , in particular , leave out cases where noise is involved and the signals are only approximately sparse .",
    "we believe that the requirement of stability and robustness will not significantly increase the measurement threshold , but also think that a thorough analysis of these questions are beyond the scope of this paper .",
    "therefore they are left as possible lines of future research .",
    "the authors of @xcite provided upper bounds @xmath96 and @xmath97 of @xmath88 and @xmath94 , respectively .",
    "( note that our notation is a bit different to theirs : what we call @xmath97 is denoted @xmath98 by them , where @xmath10 and @xmath99 denotes @xmath100 and @xmath101 , respectively . )",
    "they then proceed to prove the following very beautiful result .",
    "3.1 ) [ th : synthesisupperthresholds ] we have @xmath102    theorem [ th : synthesisupperthresholds ] in fact states that in order to find an @xmath103-sparse vector using the prior information , we need about as many measurements as the sum of the measurements needed to separately recover two @xmath104-sparse vectors known to be supported on @xmath48 , respectively .",
    "the authors of @xcite conjectured that the theorem can be generalized to more than @xmath105 sets @xmath48 .",
    "we will do this at the end of the paper , with the addition that the upper bound we provide on @xmath94 in fact also induces a lower bound .",
    "@xmath88 and @xmath93 can be calculated using a few results from @xcite . before stating the results we will use ,",
    "let us define the statistical dimension of a cone @xmath106 .",
    "[ def : statdim ] @xcite let @xmath107 be a closed convex cone and @xmath108 be a gaussian vector .",
    "let furthermore @xmath109 denote the metric projection onto @xmath106 , i.e. @xmath110 the _ statistical dimension _ @xmath62 of @xmath106 is then defined as @xmath111 the statistical dimension of a general convex cone is the statistical dimension of its closure .",
    "the main result of @xcite shows that the statistical dimension of the descent cone @xmath112 of a convex function @xmath113 at @xmath8 , i.e. , @xmath114 can be used to calculate the minimal amount of measurements needed for the program @xmath115 with @xmath41 to succeed at recovering a vector @xmath4 .",
    "it is well known that @xmath116 has @xmath8 as its solution if and only if @xmath117 .",
    "this leads to the following precise result :    ( * ? ? ?",
    "i ) [ th : statdim ] let @xmath107 be a convex cone and @xmath87 a gaussian matrix . then if @xmath118 we have @xmath119 . likewise if @xmath120 then @xmath121 .    in particular , the probability that the solution of the problem @xmath116 is equal to @xmath8 undergoes a phase transition as @xmath122 surpasses @xmath123 , and hence we have @xmath124    the authors of @xcite proceeded to calculate @xmath125 as an exemplary application of a technique they presented .",
    "we will use the same technique to calculate @xmath126 , but before that , let us state the result regarding standard basis pursuit .",
    "define the function @xmath127 through @xmath128 where @xmath129 \\to [ 0,1]$ ] is defined through @xmath130 the statistical dimension of the descent cone of the @xmath2-norm at an @xmath0-sparse @xmath4 satisfies @xmath131 where @xmath132 .",
    "now as promised , we will prove a similar result regarding the weighted basis pursuit @xmath133 .",
    "we will use the same technique as the authors of @xcite and therefore start by calculating the expected length of the projection of a gaussian vector onto a dilation of the subdifferential of the weighted @xmath2-norm .",
    "the _ subdifferential _",
    "@xcite of a convex function @xmath113 at a point @xmath8 is the set of vectors @xmath134",
    "satisfying @xmath135 for this , we will need a fact on subdifferentials of norms .",
    "the following result is folklore in this area , but let us include a proof for completeness .    for any norm @xmath136 on @xmath137 , denote by @xmath138 its dual norm , i.e. @xmath139 then for every @xmath140 @xmath141 where @xmath142 denotes the canonical dual pairing @xmath143 , i.e. , the euclidean scalar product .",
    "we aim to characterize the vectors @xmath144 which have the property @xmath145 by taking the supremum in over all @xmath146 with @xmath147 , we immediately obtain @xmath148 i.e. @xmath149 . by further choosing @xmath150 , we get @xmath151 , which proves @xmath152 , and therefore also @xmath153 .",
    "this further implies that @xmath154 . if on the other hand @xmath153 and @xmath154 , we have @xmath155 and hence is true , and we are done .",
    "now we can calculate @xmath156 for @xmath4 and a gaussian vector @xmath108 .",
    "[ lem : distsubdiffl1weight ] let @xmath4 be arbitrary and @xmath108 a gaussian vector .",
    "we have @xmath157 where @xmath158 , @xmath159 and @xmath160 is defined through equation .",
    "let us start by noticing that the dual norm of @xmath16 is @xmath161 .",
    "the hlder inequality implies that for every @xmath162 , @xmath163 i.e. @xmath164 . by choosing @xmath146 equal to the vector",
    "for which @xmath165 if @xmath60 is the index where @xmath166 is maximal , and else @xmath167 , we even see that equality holds .",
    "this , together with the fact that under the assumption @xmath168 , @xmath169 if and only if @xmath170 , implies that @xmath171 where @xmath172 .",
    "abbreviating @xmath173 , we see that @xmath174 ( the part on @xmath76 of a vector in @xmath175 is fixed to @xmath176 , and the other part can elementwise not be larger than @xmath25 . )",
    "taking the expectation , we obtain @xmath177    for convinence , let us abbreviate @xmath178 also define @xmath179 where @xmath10 and @xmath45 are the parameters associated with @xmath180 and @xmath76 , and @xmath181 .",
    "as the notation suggests , @xmath182 is not far away from @xmath93 .",
    "we will prove this using lemma [ lem : distsubdiffl1weight ] together with the following theorem from @xcite .",
    "[ th : statdimbound ] ( * ? ? ?",
    "let @xmath136 be a norm on @xmath137 .",
    "the the statistical dimension of the descent cone of @xmath136 at a point @xmath4 satisfies @xmath183    [ cor : statdimweightbound ] the statistical dimension of @xmath184 satisfies @xmath185 where @xmath186 and @xmath182 is defined through .",
    "moreover , the following bound independent of @xmath10 holds : @xmath187    we use theorem . to control the error term , by using that @xmath188 for each @xmath60",
    ", we obtain the estimate @xmath189 which is valid for any @xmath72 supported on @xmath76 .",
    "we even have equality if we choose @xmath190 parallel to @xmath191",
    ". we can do this since , as pointed out in @xcite , there is no need to use theorem directly for @xmath8 .",
    "we only have to secure that @xmath72 has the same @xmath192 and @xmath193 as @xmath8 , since @xmath194 only depends on those entities .",
    "it is clear from the considerations in the proof of lemma [ lem : distsubdiffl1weight ] that for this to be true , @xmath72 only has to be supported on @xmath76 and have the same sign pattern as @xmath8 .",
    "it is equally clear that there exist such vectors @xmath72 so that @xmath190 is parallel to @xmath195 .",
    "this proves that it is possible to choose the denominator of the error term in theorem equal to @xmath196 .",
    "to bound the numerator , it suffices to notice that each vector @xmath197 satisfies @xmath198 since @xmath199 for each @xmath200 , and @xmath201 .",
    "to finish the proof of the tighter bound , we note that @xmath202 finally , the bound @xmath203 easily follows from that statement , since @xmath204 using the facts that @xmath205 and @xmath206 .",
    "proposition [ cor : statdimweightbound ] states that @xmath93 is bounded both below and above by the number @xmath208 , up to an error which is independent of the weights @xmath25 .",
    "in particular ,",
    "if we denote @xmath209 , we have @xmath210 hence , @xmath211 is equal to @xmath94 up to an asymptotically vanishing error term . in this section",
    ", we prove that @xmath182 has a unique minimum at some @xmath54 for every @xmath180 .",
    "as the proof will show , it can furthermore easily be found by minimizing a convex function in two variables .",
    "the proof uses several techniques exploited for partly other purposes in @xcite .",
    "[ prop : optimalweight ] assume that @xmath212 and let @xmath4 be a vector supported on @xmath76 . for each partition @xmath213 of @xmath214 $ ]",
    ", there exists weights @xmath215 so that @xmath216 is minimal , where @xmath217 .",
    "the weights @xmath54 are for instance uniquely determined by the additional constraint @xmath218 .",
    "first , we note that the chain rule proves that @xmath219 where we used @xmath220 and the fact that the subdifferential @xmath221 only depends in the sign pattern of @xmath72 . hence , since @xmath8 and @xmath222 have the same sign pattern , @xmath223 . if for @xmath224 , we denote @xmath225 , then @xmath226 and hence @xmath227 therefore , if we prove that there exists a unique @xmath228 so that @xmath229 is minimal , then @xmath230 is the optimal weight we are looking for , since then @xmath231 for any @xmath232 , @xmath233 , from which the optimality of @xmath54 follows .",
    "@xmath54 of course denotes @xmath234 .    in order to prove that there exists a unique minimizer of @xmath235 on @xmath236",
    ", it suffices to prove that @xmath237 is strictly convex , continuous and coercive (  large outside a ball  ) .    _",
    "strict convexity : _ let @xmath238 , @xmath239 , and @xmath240 $ ] .",
    "then ( due to the structure of @xmath241 ) , we know that @xmath242 for some @xmath243 with @xmath244 and @xmath245 .",
    "this implies that @xmath246 the right hand side of the previous equation is an element of @xmath247 .",
    "this since @xmath248 , the action of diagonal matrices does not change the support of a vector , and the chain of inequalities @xmath249 from the latter we deduce that @xmath250 can be written as @xmath251 , where @xmath252 and @xmath253 .",
    "we have thus proven that for every triple @xmath254 , @xmath240 $ ] , @xmath255 therefore , for each fixed @xmath256 , @xmath238 , @xmath239 , we have @xmath257 taking the infimum over @xmath258 and @xmath259 , it follows that for each fixed @xmath260 , the function @xmath261 is convex .",
    "the square of a non - negative convex function is still convex , and since choosing @xmath262 a gaussian and taking the expectation also does not destroy the convexity , and we can conclude that @xmath237 is convex .",
    "it remains to prove strict convexity .",
    "since convexity already has been established , it suffices to prove that there does not exist @xmath263 and @xmath264 with @xmath265 . towards a contradiction ,",
    "assume that this is not true .",
    "then @xmath266 according to what was just proven , the expression over which we are taking the expectation on the right hand side is not smaller than the expression on the left hand side . hence , in order for equality to be true , the two expressions have to be equal almost surely .",
    "but there exists some @xmath267 which does not lie in @xmath268 ( since @xmath269 ) . for this vector ,",
    "strict equality must hold , since @xmath270 . now , since the distance from a vector @xmath260 to a convex set is continuously dependent on @xmath260 , the strict inequality even holds in some @xmath271-ball around @xmath260 , which has positive gaussian measure .",
    "the two expressions are hence not almost surely equal .",
    "this is a contradiction , and hence @xmath237 is strictly convex .",
    "_ continuity : _ we have , for @xmath272 fixed and each @xmath273 @xmath274 @xmath275 denote @xmath276 and @xmath277 , respectively .",
    "the inequality just proved then reads @xmath278 and we may estimate @xmath279 @xmath237 is furthermore locally bounded , since @xmath280 .",
    "hence , @xmath237 is continuous .",
    "_ coercivity : _ we have to prove that @xmath281 implies @xmath282 .",
    "let us first note that due to our global assumption @xmath85 for each @xmath60 , there exists for every @xmath55 an index @xmath283 with @xmath284 .",
    "for each such index and each @xmath273 , we have due to @xmath285 @xmath286 that implies that for any given @xmath287 , there exists an @xmath288 so that @xmath289 . hence ,",
    "if @xmath290 and @xmath291 , we have @xmath292 this implies that if @xmath293 , @xmath294 since @xmath295 , this proves the claim .    now it remains to prove that the minimum is not attained in @xmath296 .",
    "for this , consider choosing all weights equal to @xmath297 , i.e @xmath298 .",
    "lemma [ lem : distsubdiffl1weight ] applied for @xmath299 for all @xmath60 yields @xmath300 we may estimate @xmath301 by @xmath302 therefore @xmath303 for small values of @xmath304 , since @xmath305 by assumption .",
    "having established an abstract result on the existence of optimal weights @xmath37 , we now take a step back to see that the proof of said theorem actually provides a way to concretely calculate them  we only need to minimize the function @xmath235 .",
    "writing @xmath306 as in the proof , we obtain , with the help of lemma [ lem : distsubdiffl1weight ] @xmath307 since @xmath237 has the structure of a sum of convex functions , it can further be minimized by considering one variable at a time .",
    "one could argue that this would have been a much simpler way of proving the assertion of theorem [ prop : optimalweight ] , but the approach used in that proof has more potential to be applied to more general problems , and hence has an interest of its own .",
    "since the convex functions are even differentiable , it is not hard to write down a formula for calculating @xmath308 analytically . as a matter of fact , this has already been carried out in ( * ? ? ?",
    "* prop 4.5 ) .",
    "there , the formula was used in the process of calculating the statistical dimension of @xmath309 .",
    "in particular , it was not recognized as an optimal choice of a weight @xmath51 . for completeness",
    ", we include the statement .",
    "[ cor : optweightcalc ] the optimal weights @xmath310 described in theorem [ prop : optimalweight ] are given through the equations @xmath311 in particular , @xmath54 is independent of @xmath45 .",
    "finally , we can extract one more interesting corollary , which in fact is a counterpart of theorem [ th : synthesisupperthresholds ] in this setting .",
    "using the fact that @xmath312 , one can see that @xmath313 noticing that the term in the sum is actually the function we need to minimize in order to find @xmath314 , we immediately arrive at the following result , which also summarizes the entirety of this paper .",
    "[ th : synthesisthresholds ] let @xmath315 $ ] with @xmath316 , and a partition @xmath213 of @xmath3 $ ] be given .",
    "then there exist weights @xmath54 which minimize @xmath317 and which are unique up to multiplication with a positive scalar .",
    "furthermore , we have @xmath318 and the same also for the lower bounds @xmath319 .",
    "in this section , we present the results of some numerical experiments investigating the practical performance of the weighting strategy described above . before presenting the set up of the experiment ,",
    "let us note that it is numerically relatively easy to calculate the optimal weights described in corollary [ cor : optweightcalc ] ; we simply have to solve the @xmath47 independent , one - dimensional equations @xmath320 for @xmath51 . for this",
    ", we used the matlab routine ` fzero ` . to be able to compare these weights to other strategies , we furthermore normalized them so that @xmath321 . in a similar manner",
    ", one can explicitly write down the optimality condition for @xmath322 to calculate the thresholds induced by given weights @xmath37 .",
    "in the first set of experiments , we consider the case of two sets @xmath323 .",
    "we fix the ambient dimension to @xmath324 and choose random @xmath325-sparse signals - the support of @xmath8 is drawn at random , and the values of @xmath8 on the support are drawn from the standard multivariate normal distribution . then we draw @xmath326 , @xmath327 or @xmath328 , respectively , indices in the support together with @xmath328 , @xmath327 or @xmath326 , respectively , indices outside the support to form a group @xmath329 .",
    "the rest of the indices is then called @xmath330 . hence , in each experiment , @xmath331 and @xmath332 and @xmath333 , respectively .",
    "( @xmath334 is always equal to @xmath39 ) for @xmath335 for different @xmath336 compared with @xmath337 .",
    "[ fig : weights ] ]    -minimization for different weighting strategies and @xmath335 , depending on @xmath336 .",
    "[ fig : dims ] ]    we consider @xmath338 strategies :    1 .",
    "@xmath339 , which corresponds to standard @xmath2-minimization , 2 .",
    "the extreme strategy @xmath340 , 3 .",
    "@xmath341 , i.e. the one from @xcite , 4 .",
    "the one with weights calculated as proposed in this work .",
    "the resulting weights for strategy @xmath342 are depicted in figure [ fig : weights ] for different values of @xmath10 .",
    "the theoretical thresholds @xmath343 were also calculated and are depicted in figure [ fig : dims ] . as can be seen ,",
    "the statistical dimensions for the our choice are in all cases lower compared to the other approaches .",
    "the strategy from @xcite is however very close to the one described in this paper .    to test the performance in practice , we for each @xmath344 $ ] draw a matrix @xmath87 according to the gaussian distribution and solve the reweighted @xmath2-minimization problem @xmath89 with help of the matlab package ` cvx ` @xcite . for each @xmath122 ,",
    "@xmath345 experiments are performed , and a success is declared if the solution of the minimization problem differs no more than @xmath346 in @xmath347-norm from @xmath8 .",
    "the results can be found in figure [ fig : omegas ] .",
    "the figures show that reweighted @xmath2-minimization performs significantly better using the weight chosen as in theorem [ prop : optimalweight ] compared to standard basis pursuit ( @xmath35 ) for all @xmath10 tested .",
    "the same is true for the comparison to the strategy @xmath34 in the cases @xmath348 and @xmath349 , but not in the case @xmath350 .",
    "this was expected , since experiments performed by the authors of @xcite indicated that the choice of @xmath37 gets more important as @xmath10 gets lower .",
    "the difference between our strategy and the one proposed in @xcite is also present , however not significant .    2 .",
    "in each figure , @xmath324 , @xmath351 . @xmath336 is different in each figure  @xmath352 in the upper left figure , @xmath353 in the upper right figure and @xmath333 in the lower figure , respectively .",
    "the calculated optimal weight @xmath354 ( @xmath334 is always equal to @xmath39 ) in the respective experiments are equal to @xmath355 @xmath356 , @xmath357 @xmath358 and @xmath359 @xmath360 .",
    "the error bars are corresponding to 3 standard deviations.[fig : omegas],title=\"fig : \" ] . in each figure , @xmath324 , @xmath351 .",
    "@xmath336 is different in each figure  @xmath352 in the upper left figure , @xmath353 in the upper right figure and @xmath333 in the lower figure , respectively .",
    "the calculated optimal weight @xmath354 ( @xmath334 is always equal to @xmath39 ) in the respective experiments are equal to @xmath355 @xmath356 , @xmath357 @xmath358 and @xmath359 @xmath360 .",
    "the error bars are corresponding to 3 standard deviations.[fig : omegas],title=\"fig : \" ]    . in each figure , @xmath324 , @xmath351 .",
    "@xmath336 is different in each figure  @xmath352 in the upper left figure , @xmath353 in the upper right figure and @xmath333 in the lower figure , respectively .",
    "the calculated optimal weight @xmath354 ( @xmath334 is always equal to @xmath39 ) in the respective experiments are equal to @xmath355 @xmath356 , @xmath357 @xmath358 and @xmath359 @xmath360 .",
    "the error bars are corresponding to 3 standard deviations.[fig : omegas ] ]    in order to see that also the theory for @xmath361 is practically relevant , we make another experiment and consider the case when @xmath362 and @xmath363 .",
    "@xmath32 is chosen as above for different values of @xmath122 .",
    "we consider five different strategies    1",
    ".   set @xmath364 ( i.e. perform standard @xmath2-minimization .",
    "2 .   consider the union @xmath365 as one region with @xmath366 and @xmath367 , and choose the two weights as @xmath368 .",
    "3 .   consider the union @xmath365 as one region with @xmath366 and @xmath367 , and choose the two weights as proposed in this work .",
    "4 .   choose four weights , one for each @xmath48 , with @xmath369 and @xmath370 for @xmath371 .",
    "5 .   choose four weights as proposed in this work .",
    "note that although strategy @xmath342 bare resemblances to the one proposed in @xcite , there are no theoretical results which motivate why it should be used .",
    "in particular , it was not proposed by the authors of that paper , since they only considered the case @xmath105 .",
    "it should only be seen as a heuristic choice for comparison purposes .",
    "the results are depicted in figure [ fig : ks ] .",
    "we see that the optimal strategy proposed in this work involving four sets @xmath48 is perform significantly better than all of the other strategies .    2   instead of forming two sets @xmath365 and @xmath372 .",
    "the sparsity parameter is @xmath373 , @xmath374 and the ambient dimension is @xmath324 .",
    "the optimal weights were in the first case calculated to @xmath375 and in the second case @xmath376 .",
    "the error bars are corresponding to 3 standard deviations .",
    "[ fig : ks],title=\"fig : \" ]",
    "the author wishes to thank prof .",
    "dr . g. kutyniok for carefully proofreading of the paper as well as for fruitful discussions .",
    "he also wants to thank the anonymous reviewers for many useful comments .",
    "the author also acknowledges support from the deutscher akademischer austauschdienst , daad .            y.  gordon . on milman s inequality and random subspaces which escape through a mesh in @xmath377 . in j.",
    "lindenstrauss and v.  milman , editors , _ geometric aspects of functional analysis _ , volume 1317 of _ lecture notes in mathematics _ ,",
    "pages 84106 .",
    "springer berlin heidelberg , 1988 ."
  ],
  "abstract_text": [
    "<S> compressed sensing deals with the recovery of sparse signals from linear measurements . without any additional information , it is possible to recover an @xmath0-sparse signal using @xmath1 measurements in a robust and stable way . </S>",
    "<S> some applications provide additional information , such as on the location of the support of the signal . using this information , </S>",
    "<S> it is conceivable the threshold amount of measurements can be lowered . a proposed algorithm for this task </S>",
    "<S> is _ weighted @xmath2-minimization_. put shortly , one modifies standard @xmath2-minimization by assigning different weights to different parts of the index set @xmath3 $ ] . </S>",
    "<S> the task of choosing the weights is however non - trivial .    </S>",
    "<S> this paper provides a complete answer to the question of an optimal choice of the weights . </S>",
    "<S> in fact , it is shown that it is possible to directly calculate unique weights that are optimal in the sense that the threshold amount of measurements needed for exact recovery is minimized . </S>",
    "<S> the proof uses recent results about the connection between convex geometry and compressed sensing - type algorithms .    </S>",
    "<S> compressed sensing , gaussian matrices , weighted basis pursuit , convex geometry . </S>"
  ]
}