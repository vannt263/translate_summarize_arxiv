{
  "article_text": [
    "hashing is a widely used approach for practical nearest neighbor search in many computer vision applications .",
    "it has been observed that adaptive hashing methods that learn to hash from data generally outperform data - independent hashing methods such as the classical locality sensitive hashing @xcite . in this paper , we focus on a relatively new family of adaptive hashing methods , namely _ online _ adaptive hashing methods @xcite .",
    "these techniques employ online learning in the presence of streaming data , and are appealing due to their low computational complexity and their ability to adapt to changes in the data distribution .    despite recent progress",
    ", a key challenge has not been addressed in online hashing , which motivates this work : the binary representations , the hash table , may become outdated after a change in the hash mapping . to reflect the updates in the hash mapping , the hash table may need to be recomputed frequently , causing inefficiencies in the system such as successive disk i / o , especially when dealing with large datasets .",
    "we thus identify an important question for online adaptive hashing systems : _ when to update the hash table ?",
    "_ previous online hashing solutions do not address this question , as they usually update both the hash mapping and hash table concurrently .",
    ", an image @xmath0 , along with its neighbors in @xmath1 and non - neighbors in @xmath2 , are mapped to binary codes , yielding two distributions of hamming distances . in this example",
    ", @xmath3 has higher quality than @xmath4 since it induces more separable distributions .",
    "the information - theoretic quantity mutual information can be used to capture the separability , which gives a good quality indicator and learning objective for online hashing . ]    to answer the above question , we make the observation that achieving high quality nearest neighbor search is an ultimate goal in hashing systems , and therefore any effort to limit computational complexity should preserve , if not improve , that quality .",
    "therefore , another important question is : _ how to quantify quality ?",
    "_ here , we briefly describe our answer to this question , but first introduce some necessary notation .",
    "we would like to learn a hash mapping @xmath5 from feature space @xmath6 to the @xmath7-dimensional hamming space @xmath8 , whose outputs are @xmath7-bit binary codes .",
    "the goal of hashing is to preserve a neighborhood structure in @xmath6 after the mapping to @xmath8 . given @xmath9 , the neighborhood structure is usually given in terms of a set of its neighbors @xmath1 , and a set of non - neighbors @xmath2 .",
    "we discuss how to derive the neighborhood structure in sec .",
    "[ sec : formulation ] .    as shown in fig .",
    "[ fig : mi ] , the distributions of the hamming distances from @xmath0 to its neighbors and non - neighbors are histograms over @xmath10 .",
    "ideally , if there is no overlap between these two distributions , we can recover @xmath1 and @xmath2 by simply thresholding the hamming distance .",
    "a nonzero overlap results in ambiguity , as observing the hamming distance is no longer sufficient to determine neighbor relationships .",
    "our discovery is that this overlap can be quantified using an information - theoretic quantity , _ mutual information _ , between two random variables induced by @xmath5 .",
    "we then use mutual information to define a novel measure to quantify quality for hash functions in general .    with a quality measure defined",
    ", we answer the motivating question of when to update the hash table . we propose a simple solution by restricting updates to times when there is an estimated improvement in hashing quality , based on an efficient estimation method in the presence of streaming data .",
    "notably , since mutual information is a good general - purpose quality measure for hashing , this results in a general plug - in module for online hashing that does not require knowledge of the learning method .",
    "inspired by this strong result , a natural next question is , _ can we optimize mutual information as an objective to learn hash functions ? _ we answer this by deriving gradient descent rules on the mutual information objective , which can then be used in online stochastic optimization .",
    "mutual information is an appealing hash learning objective that is free of tuning parameters , unlike other formulations that may require thresholds , margins , .",
    "we conduct experiments on three image retrieval benchmarks , including the places205 dataset @xcite , which has 2.5 m images . for four recent online hashing methods ,",
    "our mutual information based update criterion consistently leads to over an order of magnitude reduction in hash table recomputations , while maintaining retrieval accuracy .",
    "moreover , by optimizing the mutual information objective , our online hashing method achieves very competitive retrieval results compared to other online and batch hashing methods , including recent ones based on deep learning .",
    "in this section , we mainly review hashing methods that adaptively updates the hash mapping with incoming data , given that our focus is on online adaptive hashing , for a more general survey on hashing , please refer to @xcite .",
    "huang @xcite propose online kernel hashing , where a stochastic environment is considered with pairs of points arriving sequentially in time . at each step , a number of hash functions are selected based on a hamming loss measure and the parameters are updated via stochastic gradient descent ( sgd ) .",
    "cakir and sclaroff @xcite argue that , in a stochastic setting , it is difficult to determine which hash functions to update as it is the collective effort of all the hash functions that yields a good hash mapping .",
    "hamming loss is considered to infer the hash functions to be updated at each step and a squared error loss is minimized via sgd .    in @xcite ,",
    "binary error correcting output codes ( ecocs ) are employed in learning the hash functions .",
    "this work follows a more general two - step hashing framework @xcite , where the set of ecocs are generated beforehand and are assigned to labeled data as they appear , allowing the label space to grow with incoming data .",
    "then , hash functions are learned to fit the binary ecocs using online boosting .    inspired by the idea of  data sketching \" ,",
    "leng introduce online sketching hashing @xcite where a small fixed - size sketch of the incoming data is maintained in an online fashion .",
    "the sketch retains the frobenius norm of the full data matrix , which offers space savings , and allows to apply certain batch - based hashing methods .",
    "a pca - based batch learning method is applied on the sketch to obtain hash functions .",
    "none of the above online hashing methods offer a solution to decide whether or not the hash table shall be updated given a new hash mapping .",
    "however , such a solution is crucial in practice , as limiting the frequency of updates will alleviate the computational burden of keeping the hash table up - to - date .",
    "although @xcite and @xcite include strategies to select individual hash functions to recompute , they still require computing on all indexed data instances .",
    "recently , some methods employ deep neural networks to learn hash mappings , @xcite and others .",
    "these methods use minibatch - based stochastic optimization , however , they usually require multiple passes over a given dataset to learn the hash mapping , and the hash table is only computed when the hash mapping has been learned .",
    "therefore , current deep learning based hashing methods are essentially batch learning methods , which differ from the online hashing methods that we consider , methods that process streaming data to learn and update the hash mappings on - the - fly while continuously updating the hash table . nevertheless ,",
    "when evaluating our mutual information based hashing objective , we compare against state - of - the - art batch hashing formulations as well , by contrasting different objective functions on the same model architecture .",
    "lastly , ustinova @xcite recently proposed a method to derive differentiation rules for objective functions that require histogram binning , and apply it in learning deep embeddings . when optimizing our mutual information objective , we utilize their differentiable histogram binning technique for deriving gradient - based optimization rules .",
    "note that both our problem setup and objective function are quite different from @xcite .",
    "as mentioned in sec .  [ sec : intro ] , the goal of hashing is to learn a hash mapping @xmath11 such that a desired neighborhood structure is preserved .",
    "we consider an online learning setup where @xmath5 is continuously updated from input streaming data , and at time @xmath12 , the current mapping @xmath13 is learned from @xmath14 .",
    "we follow the standard setup of learning @xmath5 from pairs of instances with similar / dissimilar labels @xcite .",
    "these labels , along with the neighborhood structure , can be derived from a metric , two instances are labeled similar ( neighbors of each other ) if their euclidean distance in @xmath6 is below a threshold .",
    "such a setting is often called unsupervised hashing . on the other hand , in",
    "supervised hashing with labeled data , pair labels are derived from individual class labels : instances are similar if they are from the same class , and dissimilar otherwise .",
    "below , we first derive the mutual information quality measure and discuss its use in determining when to update the hash table in sec .  [ sec : trigger_update ] .",
    "we then describe a gradient - based approach for optimizing the same quality measure , as an objective for learning hash mappings , in sec .",
    "[ sec : mi_objective ] . finally , we discuss the benefits of using mutual information in sec .  [",
    "sec : mi_benefits ] .",
    "we revisit our motivating question : _ when to update the hash table in online hashing ?",
    "_ during the online learning of @xmath13 , we assume a retrieval set @xmath15 , which may include the streaming data after they are received .",
    "we define the hash table as the set of hashed binary codes : @xmath16 . given the adaptive nature of online hashing , @xmath17 may need to be recomputed often to keep pace with @xmath13 ; however , this is undesirable if @xmath18 is large or the change in @xmath13 s quality does not justify the cost of an update .",
    "we propose to view the learning of @xmath13 and computation of @xmath17 as separate events , which may happen at different rates . to this end , we introduce the notion of a _ snapshot _ , denoted @xmath19 , which is occasionally taken of @xmath13 and used to recompute @xmath17 .",
    "importantly , this happens only when the nearest neighbor retrieval quality of @xmath13 has improved , and we now define the quality measure .    given hash mapping @xmath20 , @xmath5 induces hamming distance @xmath21 as @xmath22 consider some instance @xmath23 , and the sets containing neighbors and non - neighbors , @xmath24 and @xmath25 .",
    "@xmath5 induces two conditional distributions , @xmath26 and @xmath27 as seen in fig .",
    "[ fig : mi ] , and it is desirable to have low overlap between them . to formulate the idea , for @xmath5 and @xmath28 , define random variable @xmath29 , and let @xmath30 be the membership indicator for @xmath1 .",
    "the two conditional distributions can now be expressed as @xmath31 and @xmath32 , and we can write the _ mutual information _ between @xmath33 and @xmath34 as @xmath35 where @xmath36 denotes ( conditional ) entropy .",
    "in the following , for brevity we will drop subscripts @xmath5 and @xmath37 , and denote the two conditional distributions and the marginal @xmath38 as @xmath39 , @xmath40 , and @xmath41 , respectively .    by definition",
    ", @xmath42 measures the decrease in uncertainty in the neighborhood information @xmath43 when observing the hamming distances @xmath44 .",
    "we claim that @xmath45 also captures how well @xmath5 preserves the neighborhood structure of @xmath37 .",
    "if @xmath42 attains a high value , which means @xmath43 can be determined with low uncertainty by observing @xmath44 , then @xmath5 must have achieved good separation ( low overlap ) between @xmath39 and @xmath40 .",
    "@xmath46 is maximized when there is no overlap , and minimized when @xmath39 and @xmath40 are exactly identical .",
    "recall , however , that @xmath46 is defined with respect to a single instance @xmath0 ; therefore , for a general quality measure , we integrate @xmath46 over the feature space : @xmath47 @xmath48 captures the expected amount of separation between @xmath39 and @xmath40 achieved by @xmath5 , over all instances in @xmath6 .    in the online setting ,",
    "given the current hash mapping @xmath13 and previous snapshot @xmath19 , it is then straightforward to pose the update criterion as @xmath49 where @xmath50 is a threshold ; a straightforward choice is @xmath51 . however , eq .  [ mi_batch_criteria ] is generally difficult to evaluate due to the intractable integral ; in practice , we resort to monte - carlo approximations to this integral , as we describe next .",
    "we give a monte - carlo approximation of eq .",
    "[ mi_batch_criteria ] .",
    "since we work with streaming data , we employ the reservoir sampling algorithm @xcite , which enables sampling from a stream or sets of large / unknown cardinality . with reservoir sampling ,",
    "we obtain a _ reservoir set _ @xmath52 from the stream , which can be regarded as a finite sample from @xmath53 .",
    "we estimate the value of @xmath54 on @xmath55 as : @xmath56 we use subscript @xmath55 to indicate that when computing the mutual information @xmath46 , the @xmath39 and @xmath40 for a reservoir instance @xmath57 are estimated from @xmath55 . this can be done in @xmath58 time for each @xmath57 , as the discrete distributions can be estimated via histogram binning .",
    "[ fig : tu_plugin ] summarizes our approach .",
    "we use the reservoir set to estimate the quality @xmath59 , and  trigger \" an update to the hash table only when @xmath59 improves over a threshold .",
    "notably , , in that it only needs access to streaming data and the hash mapping itself , independent of the hashing method s inner workings .     from the input stream , and estimate the mutual information criterion @xmath60 . based on its value , tu decides whether a hash table update should be executed . ]      having shown that mutual information is a suitable measure of neighborhood quality , we consider its use as a learning objective for hashing . following the notation in sec .",
    "[ sec : trigger_update ] , we define a loss @xmath61 with respect to @xmath9 and @xmath5 as @xmath62 we model @xmath5 as a collection of parameterized hash functions , each responsible for generating a single bit : @xmath63 $ ] , where @xmath64 , and @xmath65 represents the model parameters . for example , linear hash functions can be written as @xmath66 , and for deep neural networks the bits are generated by thresholding the activations of the output layer .    inspired by the online nature of the problem and recent advances in stochastic optimization , we derive gradient descent rules for @xmath61 .",
    "the entropy - based mutual information is differentiable with respect to the entries of @xmath41 , @xmath39 and @xmath40 , and , as mentioned before , these discrete distributions can be estimated via histogram binning .",
    "however , it is not clear how to differentiate histogram binning to generate gradients for model parameters .",
    "we describe a differentiable histogram binning technique next .",
    "we borrow ideas from @xcite and estimate @xmath39 , @xmath40 and @xmath41 using a differentiable histogram binning technique . for @xmath7-bit hamming distances , we use @xmath67-bin normalized histograms with bin centers @xmath68 and uniform bin width @xmath69 , where @xmath70 by default .",
    "consider , for example , the @xmath71-th entry in @xmath72 , denoted as @xmath73 .",
    "it can be estimated as @xmath74 where @xmath75 records the contribution of @xmath76 to bin @xmath71 .",
    "it is obtained by interpolating @xmath77 using a triangular kernel : @xmath78 , \\\\    ( v_{k+1 } - d_\\phi(\\mathbf{x } , { { \\hat{\\mathbf{x}}}}))/\\delta , & d_\\phi(\\mathbf{x } , { { \\hat{\\mathbf{x } } } } ) \\in [ v_{k } , v_{k+1 } ] , \\\\    0 , & \\text{otherwise . }    \\end{cases}\\ ] ] this binning process admits subgradients : @xmath79 , \\\\",
    "-1/\\delta , & d_\\phi(\\mathbf{x } , { { \\hat{\\mathbf{x } } } } ) \\in [ v_{k } , v_{k+1 } ] , \\\\    0 , & \\text{otherwise . }    \\end{cases }    \\label{eq : delta - dh}\\ ] ]          we now derive the gradient of @xmath46 with respect to the output of the hash mapping , @xmath80 . using standard chain rule , we can first write @xmath81.\\label{eq : diff_mi}\\end{aligned}\\ ] ]    we focus on terms involving @xmath82 , and omit derivations for @xmath83 due to symmetry . for @xmath84",
    ", we have @xmath85 where we used the fact that @xmath86 , with @xmath87 and @xmath88 being shorthands for the priors @xmath89 and @xmath90 .",
    "we next tackle the term @xmath91 in eq .",
    "[ eq : diff_mi ] . from the definition of @xmath82 in eq.[eq : delta ] , we have @xmath92 note that @xmath93 is already given in eq .",
    "[ eq : delta - dh ] .",
    "for the last step , we used the definition of @xmath94 in eq .",
    "[ eq : d_phi ] .",
    "lastly , to back - propagate gradients to @xmath5 s inputs and ultimately model parameters , we approximate the discontinuous sign function with sigmoid , which is a standard technique in hashing , @xcite .",
    "hashing algorithms are often evaluated with standard ranking metrics , such as average precision ( ap ) , discounted cumulative gain ( dcg ) , and normalized dcg ( ndcg ) @xcite .",
    "it is reasonable to question the use of mutual information , when the performance of a hashing system can be monitored directly by such metrics , as they too can be estimated on a reservoir set .",
    "we respond to this question by discussing the benefits of using mutual information .",
    "first , we point out the lower computational complexity of mutual information .",
    "let @xmath95 be the reservoir set size .",
    "computing eq .",
    "[ eq : q_r ] involves estimating discrete distributions via histogram binning , and takes @xmath96 time for each reservoir item , since @xmath97 only takes discrete values from @xmath10 , in contrast , ranking measures such as ap and ndcg have @xmath98 complexity due to sorting , which render them disadvantageous for large @xmath95 , for example when a large reservoir set is required for better approximation .",
    "[ fig : correlation ] demonstrates the pearson correlation coefficients between mi and ap , dcg , and ndcg , for random hash mappings on three benchmark datasets .",
    "although a theoretical analysis is beyond the scope of this work , in practice we find that mi serves as an efficient and general - purpose ranking surrogate .",
    "finally , sec .",
    "[ sec : mi_objective ] showed that the mutual information objective is suitable for direct , gradient - based optimization .",
    "in contrast , optimizing metrics like ap and ndcg is more challenging as they are often non - differentiable or discontinuous .",
    "existing works resort to optimizing surrogates @xcite , as it is unclear how to directly apply gradient - based optimization .",
    "furthermore , mutual information itself is essentially parameter - free , whereas many other hashing formulations require and could be sensitive to tuning parameters , such as thresholds or margins @xcite , quantization strength @xcite , .",
    "we evaluate our approach on three widely used image benchmarks .",
    "we first describe the datasets and experimental setup in sec .",
    "[ sec : setup ] .",
    "we evaluate the mutual information update criterion in sec .",
    "[ sec : eval_tu ] and the mutual information based objective function for learning hash mappings in sec .  [ sec : eval_mihash ] .",
    "our implementations will be publicly available .      * cifar-10 * is a widely - used dataset for image classification and retrieval , containing 60k images from 10 different categories @xcite . for feature representation ,",
    "we use cnn features extracted from the @xmath99 layer of a vgg-16 network @xcite pre - trained on imagenet @xcite .",
    "* places205 * is a subset of the large - scale places dataset @xcite for scene recognition .",
    "places205 contains 2.5 m images from 205 scene categories .",
    "this is a very challenging dataset due to its large size and number of categories , and it has not been studied in the hashing literature to our knowledge .",
    "we extract cnn features from the @xmath99 layer of an alexnet @xcite pre - trained on imagenet , and reduce the dimensionality to 128 using pca .",
    "* labelme*. the 22k labelme dataset @xcite has 22,019 images represented as 512-dimensional gist descriptors .",
    "this is an unsupervised dataset without labels , and standard practice uses the euclidean distance to determine neighbor relationships .",
    "specifically , @xmath100 and @xmath101 are considered neighbor pairs if their euclidean distance is within the smallest 5% in the training set . for a query ,",
    "the closest 5% examples are considered true neighbors .",
    "all datasets are randomly split into a retrieval set and a test set , and a subset from the retrieval set is used for learning hash functions .",
    "specifically , for * cifar-10 * , the test set has 1k images and the retrieval set has 59k .",
    "a random subset of 20k images from the retrieval set is used for learning , and the size of the reservoir is set to 1k . for * places205 * , we sample 20 images from each class to construct a test set of 4.1k images , and use the rest as the retrieval set .",
    "a random subset of 100k images is used to for learning , and the reservoir size is 5k . for * labelme * , the dataset is split into retrieval and test sets with 20k and 2k samples , respectively .",
    "similar to cifar-10 , we use a reservoir of size 1k .    to evaluate the retrieval performance we use mean average precision ( map ) , which has been widely employed in evaluating hashing algorithms . for places205 ,",
    "map is very time - consuming to compute due to its large size .",
    "we compute map only on the top 1000 retrieved examples ( map@1000 ) , as also done in @xcite for other datasets .    for online hashing experiments , we run three randomized trials for each experiment and average the results , where the random splits and ordering of the training sequence are all different in each trial .",
    "we evaluate our mutual information based update criterion , the trigger update module ( tu ) .",
    "we apply tu to all existing online hashing methods known to us : online kernel hashing ( okh ) @xcite , online supervised hashing ( osh ) @xcite , adaptive hashing ( adapthash ) @xcite and online sketching hashing ( sketchhash ) @xcite .",
    "we use publicly available implementations of all methods .    for these experiments , we fix the code length to 32 bits . for each method , we create a corresponding data - agnostic baseline that updates the hash table at a fixed rate , controlled by parameter @xmath102 . after processing every @xmath102 examples , the baseline triggers an update , while tu makes a decision using the mutual information criterion . for each dataset , @xmath102 is set such that the baseline updates 201 times in total .",
    "* results for the trigger update module . * fig .  [",
    "fig : trig_update ] depicts the retrieval map over time for all four online hashing methods considered , on three datasets , with and without incorporating ` tu ` .",
    "we can clearly observe a significant reduction in the number of hash table updates , between one and two orders of magnitude in all cases .",
    "for example , the number of hash table updates is reduced by a factor of @xmath103 for the okh method on labelme , and the smallest reduction factor is @xmath104 , for sketchhash on the places205 dataset .",
    "the quality - based update criterion is particularly important for hashing methods that may yield inferior hash mappings due to noisy data and/or imperfect learning techniques . in other words ,",
    "tu can be used to _",
    "filter _ updates to the hash mapping with negative or small improvement .",
    "this has a stabilizing effect on the map curve , notably for okh and adapthash . for osh , which appears to stably",
    "improve over time , tu nevertheless significantly reduces revisits to the hash table while maintaining its performance .    all results in fig .",
    "[ fig : trig_update ] are obtained using the default threshold parameter @xmath51 , defined in eq .",
    "[ mi_step_criteria ] .",
    "we do not tune @xmath50 in order to show general applicability .",
    "we also discuss the impact of the reservoir set @xmath55 .",
    "there is a trade - off regarding the size of @xmath55 ; a larger @xmath55 leads to better approximation but increases computation .",
    "nevertheless , we observed robust and consistent results with @xmath105 not exceeding 5% of the size of the training stream .",
    "we evaluate the mutual information based hashing objective .",
    "we name our model using the mutual information objective @xmath106 , and train it using stochastic gradient descent ( sgd ) .",
    "this allows it to be applied to both the online setting and batch setting in learning hash functions .    during minibatch - based sgd , to compute the mutual information objective in eq .",
    "[ eq : mi_obj ] and its gradients , we need access to the sets @xmath1 , @xmath2 for each considered @xmath0 , in order to estimate @xmath39 and @xmath40 . for the online setting in sec .",
    "[ sec : eval_mihash_online ] , a standalone reservoir set @xmath55 is assumed as in the previous experiment , and we partition @xmath55 into @xmath107 with respect to each incoming @xmath0 . in this case a batch size of 1 can be used . for the batch setting in sec .  [",
    "sec : eval_mihash_batch ] , for each @xmath0 in a minibatch , @xmath107 are defined within the same minibatch .",
    "[ sec : eval_mihash_online ] we first consider an online setting that is the same as in sec .",
    "[ sec : eval_tu ] .",
    "we compare against other online hashing methods : okh ,",
    "osh , adapthash and sketchhash .",
    "all methods are equipped with the tu module with @xmath51 , which has been demonstrated to work well .    * results for online setting .",
    "* we first show the map curve comparisons in fig .",
    "[ fig : mihash ] . for competing online hashing methods , the curves are the same as the ones with tu in fig .  [",
    "fig : trig_update ] , and we remove markers to avoid clutter . @xmath106  clearly outperforms other online hashing methods on all three datasets , and shows potential for further improvement with more data , as the map curves are not saturated .",
    "the combination of tu and @xmath106  gives a complete online hashing system that enjoys a superior learning objective with a plug - in update criterion that improves efficiency .",
    "we next give insights into the distribution - separating effect from optimizing mutual information . in fig .",
    "[ fig : distance ] , we plot the conditional distributions @xmath39 and @xmath40 averaged on the cifar-10 test set , before and after learning @xmath106  with the 20k training examples . before learning , with a randomly initialized hash mapping , @xmath39 and @xmath40 exhibit high overlap .",
    "after learning , @xmath106  achieves good separation between @xmath39 and @xmath40 : the overlap reduces significantly , and @xmath39 s mass is pushed towards 0 .",
    "the separation is reflected in the large improvement in map .",
    "in contrast with the other methods , our mutual information formulation is parameter - free .",
    "for instance , there is no threshold parameter that requires separating @xmath39 and @xmath40 at a certain distance value . likewise , there is no margin parameter that dictates the amount of separation in absolute terms .",
    "parameters as such usually need to be tuned to fit to data , whereas the optimization of mutual information is automatically guided by the data itself .",
    "[ fig : distance ]      to further demonstrate the potential of @xmath106 , we consider the batch learning setting , and compare against state - of - the - art batch formulations .",
    "the methods include : supervised hashing with kernels ( shk ) @xcite , fast supervised hashing with decision trees ( fasthash ) @xcite , supervised discrete hashing ( sdh ) @xcite , efficient training of very deep neural networks ( vdsh ) @xcite , deep supervised hashing with pairwise labels ( dpsh ) @xcite and deep supervised hashing with triplet labels ( dtsh ) @xcite .",
    "we focus on comparisons on the cifar-10 dataset , which is the canonical benchmark for supervised hashing .",
    "these competing methods have shown to outperform earlier and other work such as @xcite .",
    "our goal is to contrast different learning objectives , and for fair and efficient comparisons , we restrict all models to use the same underlying network , vgg-16 @xcite , pretrained on imagenet . for @xmath106  and two deep hashing methods ( dpsh , dtsh ) ,",
    "the last fully connected layer is finetuned to produce hash codes according to each method s objective .",
    "although performance can be improved by finetuning the entire network , that is not our focus when comparing learning objectives . for vdsh",
    ", we use the full model with 16 layers and 1024 nodes per layer . in shk , fasthash , and sdh , this setup corresponds to using vgg-16 features in their respective formulations .",
    "we follow the experimental setup in @xcite , which uses 5k training examples to learn hash mappings . as standard practice , we report map values for hash code lengths @xmath108 and @xmath109 on cifar-10 .",
    "we use the publicly available code for the comparisons and exhaustively search parameter settings including the default parameters as provided by the authors .",
    "we give details for compared methods in the supplementary material .",
    "all methods are trained to convergence with multiple epochs over the 5k training set . for @xmath106 , the batch size",
    "is set to 100 .",
    "we also report results after training @xmath106  for a single epoch .    * results for batch setting . * in table  [ table : cifar_exp_soa ] , we list results for all methods .",
    "we first note that @xmath106  outperforms competing deep hashing methods ( vdsh , dpsh , dtsh ) significantly , in some cases only using a single training epoch .",
    "this suggests that the mutual information objective is a more effective learning objective for learning hash functions .",
    "such a comparison is fair , since by finetuning the final layer of vgg-16 , all these methods essentially learn linear hash functions using the same input features .",
    "next we discuss the other methods in table  [ table : cifar_exp_soa ] : shk , sdh , and fasthash .",
    "these methods learn non - linear hash functions on the input vgg-16 features , which allows them to generally outperform @xmath106  trained for a single epoch .",
    "but given sufficient training , @xmath106  achieves better results at convergence .",
    "note that the closest competitor , fasthash , is a two - step hashing method based on sophisticated binary code inference and boosted trees , while @xmath106  directly learns linear hash functions .",
    "we advance the state - of - the - art for online hashing in this paper . motivated by the issue of hash table updates in online hashing , we propose to explore quality - based update criteria , and define a quality measure using the mutual information between variables induced by the hash mapping .",
    "this quality measure is efficiently computable and highly correlates with standard ranking metrics , and leads to consistent reduction in hash table updates for four online hashing methods on three benchmark datasets , while maintaining retrieval accuracy .",
    "inspired by these strong results , we further propose a hashing method @xmath106 , by optimizing mutual information as an objective with gradient descent .",
    "in both online and batch settings , @xmath106  achieves superior performance compared to state - of - the - art hashing techniques .",
    "10=-1pt    f.  cakir and s.  sclaroff .",
    "adaptive hashing for fast similarity search . in _ proc .",
    "ieee international conf .  on computer vision ( iccv ) _ , 2015",
    "f.  cakir and s.  sclaroff .",
    "online supervised hashing . in _ proc .",
    "ieee international conf .  on image processing ( icip ) _ , 2015",
    "j.  deng , w.  dong , r.  socher , l .- j .",
    "li , k.  li , and l.  fei - fei .",
    "imagenet : a large - scale hierarchical image database . in _ cvpr _ , 2009 .",
    "a.  gionis , p.  indyk , and r.  motwani .",
    "similarity search in high dimensions via hashing . in _ proc .",
    "international conf .  on very large data bases ( vldb ) _ , 1999 .",
    "y.  gong and s.  lazebnik .",
    "iterative quantization : a procrustean approach to learning binary codes . in _ proc .",
    "ieee conf .  on computer vision and pattern recognition ( cvpr ) _ , 2011",
    "huang , q.  y. yang , and w .- s .",
    "online hashing . in _ proc .",
    "international joint conf .",
    "on artificial intelligence ( ijcai ) _ , 2013 .",
    "a.  krizhevsky and g.  hinton .",
    "learning multiple layers of features from tiny images , 2009 .",
    "a.  krizhevsky , i.  sutskever , and g.  e. hinton .",
    "imagenet classification with deep convolutional neural networks . in _ proc .",
    "advances in neural information processing systems ( nips ) _ , 2012 .",
    "b.  kulis and t.  darrell . learning to hash with binary reconstructive embeddings . in _ proc .",
    "advances in neural information processing systems ( nips ) _ , 2009 .",
    "h.  lai , y.  pan , y.  liu , and s.  yan . simultaneous feature learning and hash coding with deep neural networks . in _ proc .",
    "ieee conf .  on computer vision and pattern recognition ( cvpr ) _ , 2015",
    "c.  leng , j.  wu , j.  cheng , x.  bai , and h.  lu . online sketching hashing . in _ proc .",
    "ieee conf .  on computer vision and pattern recognition ( cvpr ) _ , 2015",
    "li , s.  wang , and w .- c .",
    "feature learning based deep supervised hashing with pairwise labels . in _ proc .  international joint conf .  on artificial intelligence",
    "( ijcai ) _ , 2016 .",
    "g.  lin , f.  liu , c.  shen , j.  wu , and h.  t. shen . structured learning of binary codes with column generation for optimizing ranking measures .",
    ", pages 122 , 2016 .",
    "g.  lin , c.  shen , q.  shi , a.  van  den hengel , and d.  suter . fast supervised hashing with decision trees for high - dimensional data . in _ proc .",
    "ieee conf .  on computer vision and pattern recognition ( cvpr ) _ , 2014",
    "k.  lin , j.  lu , c .- s .",
    "chen , and j.  zhou .",
    "learning compact binary descriptors with unsupervised deep neural networks . in _ proc .",
    "ieee conf .  on computer vision and pattern recognition ( cvpr ) _ , 2016",
    "j.  w. liu , wei  and , r.  ji , y .- g .",
    "jiang , and s .- f .",
    "supervised hashing with kernels . in _ proc .",
    "ieee conf .",
    "on computer vision and pattern recognition ( cvpr ) _ , 2012 .",
    "c.  d. manning , p.  raghavan , and h.  schtze .",
    "introduction to information retrieval .",
    "m.  norouzi and d.  j. fleet .",
    "minimal loss hashing for compact binary codes . in _ proc .",
    "international conf .  on machine learning ( icml ) _ , 2011 .",
    "b.  c. russell , a.  torralba , k.  p. murphy , and w.  t. freeman .",
    "labelme : a database and web - based tool for image annotation .",
    ", 2008 .",
    "f.  shen , c.  s. wei , l.  heng , and t.  shen .",
    "supervised discrete hashing . in _ proc .",
    "ieee conf .  on computer vision and pattern recognition ( cvpr ) _ , 2015",
    "k.  simonyan and a.  zisserman .",
    "very deep convolutional networks for large - scale image recognition . , 2015 .",
    "a.  torralba , r.  fergus , and y.  weiss .",
    "small codes and large image databases for recognition . in _ proc .",
    "ieee conf .  on computer vision and pattern recognition ( cvpr)_. ieee , 2008 .",
    "e.  ustinova and v.  lempitsky .",
    "learning deep embeddings with histogram loss . in _ proc .",
    "advances in neural information processing systems ( nips ) _ , pages 41704178 , 2016 .    j.  s. vitter",
    ". random sampling with a reservoir .",
    ", 11(1):3757 , 1985 .",
    "j.  wang , h.  t. shen , j.  song , and j.  ji .",
    "hashing for similarity search : a survey . .",
    "q.  wang , z.  zhang , and l.  si . ranking preserving hashing for fast similarity search . in _ proc .",
    "international joint conf .  on artificial intelligence ( ijcai ) _ , 2015",
    "y.  wang , xiaofang  shi and k.  m. kitani .",
    "deep supervised hashing with triplet labels . in _ proc .",
    "asian conf .  on computer vision ( accv ) _ , 2016",
    "r.  xia , y.  pan , h.  lai , c.  liu , and s.  yan .",
    "supervised hashing for image retrieval via image representation learning . in",
    "aaai conf .  on artificial intelligence ( aaai )",
    "_ , volume  1 , page  2 , 2014 .",
    "y.  yue , t.  finley , f.  radlinski , and t.  joachims .",
    "a support vector method for optimizing average precision . in _ proc .",
    "acm conf .  on research & development in information retrieval ( sigir ) _ ,",
    "pages 271278 .",
    "acm , 2007 .",
    "z.  zhang , y.  chen , and v.  saligrama .",
    "efficient training of very deep neural networks for supervised hashing . in _ proc .",
    "ieee conf .  on computer vision and pattern recognition ( cvpr ) _ , 2016",
    "f.  zhao , y.  huang , l.  wang , and t.  tan .",
    "deep semantic ranking based hashing for multi - label image retrieval . in _ proc .",
    "ieee conf .  on computer vision and pattern recognition ( cvpr ) _",
    ", 2015 .",
    "b.  zhou , a.  lapedriza , j.  xiao , a.  torralba , and a.  oliva .",
    "learning deep features for scene recognition using places database . in _ proc .",
    "advances in neural information processing systems ( nips ) _ , 2014 .",
    "[ [ section ] ]",
    "we discuss the implementation details of @xmath106 . in the online hashing experiments , for simplicity we model @xmath106  using linear hash functions , in the form of @xmath110 .",
    "the learning capacity of such a model is lower than the kernel - based okh , and is the same as osh , adapthash , and sketchhash , which use linear hash functions as well .    for the batch hashing experiments , as mentioned in sec .",
    "[ sec : eval_mihash_batch ] , for @xmath106  as well as other competing deep hashing methods , we take a vgg-16 network pretrained on imagenet , replace the original output layer with a fully connected layer that has the same number of outputs as the hash bits , and learn this layer .",
    "this also amounts to learning linear hash functions using _",
    "fc7 _ features from vgg-16 .",
    "we train @xmath106  using stochastic gradient descent . in eq .",
    "[ eq : diff_mi ] , we gave the gradients of the mutual information objective @xmath46 with respect to the _ outputs _ of the hash mapping , @xmath111 .",
    "both @xmath46 and @xmath112 are parameter - free . in",
    "order to further back - propagate gradients to the _ inputs _ of @xmath111 and model parameters @xmath113 , we approximate the @xmath114 function using the sigmoid function @xmath115 : @xmath116 where @xmath117 is a scaling parameter , used to increase the  sharpness \" of the approximation .",
    "we find @xmath118 from the set @xmath119 in our experiments .",
    "we note that @xmath118 is not a tuning parameter of the mutual information objective , but rather a parameter of the underlying hash functions .",
    "the design of the hash functions is independent of the mutual information objective and can be separated .",
    "it will be an interesting topic to explore other methods of constructing hash functions , potentially in ways that are free of tuning parameters .",
    "we set up a streaming scenario in our online hashing experiments .",
    "we run three randomized trials for each experiment . in each trial",
    ", we first randomly split the dataset into a retrieval set and a test set as described in sec .",
    "[ sec : setup ] , and randomly sample the training subset from the retrieval set .",
    "the ordering of the training set is also randomly permuted .",
    "the random seeds are fixed , so the baselines and methods with the trigger update module observe the same training sequences .    in a streaming setting",
    ", we also measure the _ cumulative _ retrieval performance during online hashing , as opposed to only the final results . to mimic real retrieval systems where queries arrive randomly , we set 50 randomized checkpoints during the online process .",
    "we first place the checkpoints with equal spacing , then add small random perturbations to their locations .",
    "we measure the instantaneous retrieval map at these checkpoints to get map vs. time curves ( curves shown in fig .  [",
    "fig : mihash ] ) , and compute the area under curve ( auc ) .",
    "auc gives a summary of the entire online learning process , which can not be reflected by the final performance at the end .",
    "we describe parameters used for online hashing methods in the online experiments .",
    "some of the competing methods require parameter tuning , therefore we sample a validation set from the training data and find the best performing parameters for each method .",
    "the size of the validation sets are 2k , 2k and 10k for cifar-10 , labelme and places205 , respectively .",
    "please refer to the respective papers for the descriptions of the parameters .    * * okh * @xcite : the tuple @xmath120 is set to @xmath121 , @xmath121 and @xmath122 for cifar-10 , labelme and places205 , respectively . *",
    "* osh * @xcite : @xmath123 is set to 0.1 for all datasets .",
    "the ecoc codebook @xmath124 is populated the same way as in osh . * * adapthash * @xcite : the tuple @xmath125 is set to @xmath126 , @xmath127 and @xmath126 for cifar-10 , labelme and places205 , respectively . * * sketchhash * @xcite : the pair ( sketch size , batch size ) is set to @xmath128 , @xmath129 and @xmath130 for cifar-10 , labelme and places205 , respectively .",
    "we use the publicly available implementations for the compared methods , and exhaustively search parameter settings including the default parameters as provided by the authors . for dpsh @xcite and dtsh @xcite , we found a setting that worked well for all evaluated hash code lengths : the mini - batch size is set to the default value of 128 , and the learning rate is initialized to 1 and decayed by a factor of 0.9 after every 20 epochs .",
    "additionally , for dtsh , the margin parameter is set to @xmath131 where @xmath7 is the hash code length .",
    "vdsh @xcite uses a heavily customized architecture with only fully - connected layers , and it is unclear how to adapt it to work with standard cnn architectures . in this sense , vdsh is more akin to nonlinear hashing methods such as fasthash @xcite and shk @xcite .",
    "we used the full vdsh model with 16 layers and 1024 nodes per layer , and found the default parameters to perform the best , except that we increased the number of training iterations by an order of magnitude during finetuning .",
    "as stated before , all methods are trained to convergence with multiple epochs over the 5k training set . for @xmath106",
    ", we use a batch size of 100 , and run sgd with initial learning rate of 0.1 and a decay factor of 0.5 every 10 epochs , for 100 epochs .",
    "in table  [ table : time ] we report running time for all methods on the cifar-10 dataset with 20k training examples , including time spent in learning hash functions and the added processing time for maintaining the reservoir set and computing tu .",
    "numbers are recorded on a 2.3ghz intel xeon e5 - 2650 cpu workstation with 128 gb of ddr3 ram .",
    "most of the added time is due to maintaining the reservoir set , which is invoked in each training iteration ; the mutual information update criterion is only checked after processing every @xmath132 examples .",
    "methods with small batch sizes ( osh , batch size 1 ) therefore incur more overhead than methods with larger batches ( sketchhash , batch size 50 ) .",
    "results for other datasets are similar .",
    "we note that in a real retrieval system with large - scale data , the bottleneck likely lies in recomputing the hash tables for indexed data , due to various factors such as scheduling and disk i / o .",
    "we reduce this bottleneck significantly by using tu .",
    "compared to this bottleneck , the increased training time is likely not significant .",
    "table [ table : cifar_exp_time ] reports cpu times for learning 48-bit hash mappings for all batch hashing methods , on the cifar-10 5k training set .",
    "the retrieval map numbers are replicated from table  [ table : cifar_exp_soa ] .",
    "our current matlab implementation of @xmath106  achieves 1.9 seconds per epoch on cpu .",
    "@xmath106  achieves competitive performance with a single epoch , and has a total training time on par with fasthash , while yielding superior performance .",
    "in sec .  [ sec : experiments ] , we reported online hashing experiments where all methods are compared in the same setup with 32-bit hash codes . additionally , we present results using 64-bit hash codes on all three datasets .",
    "the parameters for all methods are found through validation as described in [ sec : para_online ] .",
    "similar to sec  [ sec : eval_tu ] and [ sec : eval_mihash_online ] , we show the comparisons with and without tu for existing online hashing methods in fig .  [",
    "fig : tu64 ] , and plot the map curves for all methods , including @xmath106 , in fig .",
    "[ fig : mihash64 ]",
    ". the 64-bit results are uniformly better than 32-bit results for all methods in terms of map , but still follow the same patterns .",
    "again , we can see that @xmath106  clearly outperforms all competing online hashing methods , and shows potential for improvement given more training data .",
    "we present a parameter study on the parameter @xmath50 , the improvement threshold on the mutual information criterion in tu . in our previous experiments",
    ", we found the default @xmath51 to work well , and did not specifically tune @xmath50 .",
    "however , tuning for a larger @xmath50 could lead to better trade - offs , since small improvements in the quality of the hash mapping may not justify the cost of a full hash table update .    for this study , we vary parameter @xmath50 from @xmath133 to @xmath134 for all methods ( with 32-bit hash codes ) .",
    "@xmath135 reduces to the baseline . on the other hand",
    ", @xmath136 prevents any updates to the initial hash mapping and hash table , and results in only one hash table update ( for the initial mapping ) and typically low performance .",
    "the performance metric we focus on in this study is the cumulative metric , auc , since it better summarizes the entire online learning process than the final performance alone .",
    "we use a custom update schedule for sketchhash : we enforce hash table updates in the early iterations regardless of other criteria , until the number of observed examples reaches the specified size of the  data sketch \" , which sketchhash uses to perform a batch hashing algorithm .",
    "this was observed to be critical for the performance of sketchhash .",
    "therefore , the number of hash table updates for sketchhash can be greater than 1 even for @xmath136 .",
    "we present full results in tables  [ table : trigger_update_cifar ] , [ table : trigger_update_places ] , [ table : trigger_update_labelme ] . in all cases",
    ", we observe a substantial decrease in the number of hash table updates as @xmath50 increases . with reasonable @xmath50 values",
    "( typically around 0 ) , the number of hash table updates can be reduced by over an order of magnitude with no loss in auc .",
    "note that the computation - performance trade - off achieved by the default @xmath51 is always among the best , thereby in practice it can be used without tuning .",
    "we simulate a data - agnostic baseline that updates hash tables at a constant rate , using the update interval parameter @xmath102 . in sec .",
    "[ sec : experiments ] , @xmath102 is set such that the baseline updates a total of 201 times for all datasets .",
    "this ensures that the baseline is never too outdated ( compared to 50 checkpoints at which performance is evaluated ) , but is still fairly infrequent : the smallest @xmath102 in this case is 100 , which means the baselines process at least 100 training examples before recomputing the hash table . for completeness , here we present results using different values of @xmath102 , where all methods again use 32-bit hash codes and the default @xmath51 .",
    "we used a simple rule that avoids unnecessary hash table updates if the hash mapping itself does not change .",
    "specifically , we do not update if @xmath137 , where @xmath19 is the current snapshot and @xmath13 is the new candidate . some baseline entries have fewer updates because of this rule ( adapthash on places205 ) . and as explained before , due to the custom update schedule , sketchhash may have more hash table updates than what is suggested by @xmath102",
    ".    please see tables  [ table : u_cifar ] , [ table : u_labelme ] , [ table : u_places ] for the full results .",
    "in all experiments , we run three random trials and average the results as mentioned before , and the standard deviation of map and auc scores are less than @xmath138 .",
    "generally , using smaller @xmath102 leads to more updates by both the baselines and methods with tu ; recall that @xmath102 is also a parameter of tu which specifies the frequency of checking the update criterion .",
    "methods with the tu module appear to be quite insensitive to the choice of @xmath102 , the number of updates for sketchhash with tu on cifar-10 only increases by 2x while @xmath102 is reduced by 20x , from 1000 to 50 .",
    "we attribute this to the ability of tu to filter out unnecessary updates . across different values of @xmath102 ,",
    "tu consistently brings computational savings while preserving / improving online hashing performance , as indicated by final map and auc ."
  ],
  "abstract_text": [
    "<S> learning - based adaptive hashing methods are widely used for nearest neighbor retrieval . recently , </S>",
    "<S> online hashing methods have demonstrated a good performance - complexity tradeoff by learning hash functions from streaming data . in this paper </S>",
    "<S> , we aim to advance the state - of - the - art for online hashing . </S>",
    "<S> we first address a key challenge that has often been ignored : the binary codes for indexed data must be recomputed to keep pace with updates to the hash functions . </S>",
    "<S> we propose an efficient quality measure for hash functions , based on an information - theoretic quantity , _ mutual information _ , and use it successfully as a criterion to eliminate unnecessary hash table updates . </S>",
    "<S> next , we show that mutual information can also be used as an objective in learning hash functions , using gradient - based optimization </S>",
    "<S> . experiments on image retrieval benchmarks ( including a 2.5 m image dataset ) confirm the effectiveness of our formulation , both in reducing hash table recomputations and in learning high - quality hash functions . </S>"
  ]
}