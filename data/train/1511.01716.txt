{
  "article_text": [
    "in recent years , we have seen extensive development of the theory and methods for _ structured regularization _ , one of the most fundamental techniques to address the `` big data '' challenge .",
    "the basic problem is to minimize the following objective function with two components ( blocks ) : @xmath2,\\ ] ] where @xmath3 is the loss function and @xmath4 is a penalty function that imposes structured regularization to the model .",
    "both functions are usually convex , but may be nonsmooth .",
    "many data mining and machine learning problems can be cast within this framework , and efficient methods were proposed for these problems .",
    "the first group are the _ operator splitting _",
    "methods originating from @xcite and @xcite , and later developed and analyzed by @xcite , among others . their dual versions , known as _ alternating direction methods of multipliers _",
    "( admm ) ( see , @xcite ) , found many applications in signal processing ( see , _ e.g. _ , @xcite , and the references therein ) . sometimes , they are called _ split bregman methods _ ( see , _ e.g. _ , @xcite ) .    the _ alternating linearization method _ ( alin ) of @xcite handles problems of form by introducing an additional improvement test to the operator splitting methods , which decides whether the proximal center should be updated or stay unchanged , and which of the operator splitting formulas should be applied at the current iteration .",
    "its convergence mechanism is different than that of the splitting methods ; it adapts some ideas of bundle methods of nonsmooth optimization @xcite .",
    "the recent application of alin to structured regularization problems in @xcite proved very successful , with fast convergence , good accuracy , and scalability to very large dimensions .",
    "most of existing techniques for structured regularization are designed to handle the two - block problem of form .    in this paper",
    ", we plan to extend the alin framework to optimization problems involving multiple components .",
    "namely , we aim to solve the following problem : @xmath5 with convex ( possibly nondifferentiable ) functions @xmath6 , @xmath7 , where the number of component functions , @xmath8 , may be arbitrarily large .",
    "we only assume that the minimum exists .    in a typical application",
    ", @xmath3 may be the loss function , similar to problem , while the penalty function is a sum of multiple components .",
    "this type of generalization has many practical applications , including low rank matrix completion , compressed sensing , dynamic network analysis , and computer vision .    to the best of the authors knowledge ,",
    "no general convergent versions of operator splitting methods for multiple blocks exist .",
    "a known way is to introduce @xmath8 copies @xmath9 of @xmath10 , and reduce the problem to the two - function case in the space @xmath11 @xcite : @xmath12 with @xmath13 denoting the indicator function of the subspace @xmath9 .",
    "similar ideas were used in stochastic programming , under the name of _ progressive hedging _ @xcite .",
    "a method for three blocks with one function being differentiable was theoretically analyzed in @xcite .",
    "our new algorithm , which we call the _ selective linearization method _ ( slin ) , does not replicate the decision variables .",
    "it generates a sequence of points @xmath14 with a monotonic sequence of corresponding function values @xmath15 . at each iteration",
    ", it linearizes all but one of the component functions and uses a proximal term penalizing for the distance to the last iterate . in a sense , each step is a backward step of the form employed in operator splitting .",
    "the order of processing the functions is not fixed ; the method uses precise criteria for selecting the function to be treated exactly at the current step .",
    "it also employs special rules for updating the proximal center .",
    "these two rules differ our approach from the simultaneously proposed incremental proximal method of @xcite , which applies to smooth functions only , and achieves linear convergence rate in this case .",
    "the algorithm is a multi - block extension of the alternating linearization method for solving two - block nonsmooth optimization problems . global convergence and convergence rate of the new algorithm",
    "are proved .",
    "specifically , the new algorithm is proven to require at most @xmath1 iterations to achieve solution accuracy @xmath0 . in section [",
    "s : method ] , we present the method and prove its global convergence . the convergence rate is derived in section [ s : rate ] .",
    "finally , in section [ s : application ] , we illustrate its operation on structured regularized regression problems involving many blocks .",
    "our method derives from two fundamental ideas of convex optimization : the _ moreau  yosida regularization _ of @xmath16 , @xmath17 and the _ proximal step _ for , @xmath18 in the formulas above , the norm @xmath19 with a positive definite matrix @xmath20 . in applications",
    ", we shall use a diagonal @xmath20 , which leads to major computational simplifications .",
    "the _ proximal point method _ carries out the iteration @xmath21 , @xmath22 and is known to converge to a minimum of @xmath16 , if a minimum exists @xcite .",
    "the main idea of our method is to replace problem with a sequence of approximate problems of the following form : @xmath23 here @xmath22 is the iteration number , @xmath24 is the current best approximation to the solution , @xmath25 is an index selected at iteration @xmath26 , and @xmath27 are _ affine minorants _ of the functions @xmath28 , @xmath29 .",
    "these minorants are constructed as follows : @xmath30 with some points @xmath31 and specially selected subgradients @xmath32 .",
    "thus , problem differs from the proximal point problem in by the fact that only one of the functions @xmath33 is treated exactly , while the other functions are replaced by affine approximations .",
    "the key elements of the method are the selection of the index @xmath34 , the way the affine approximations are constructed , and the update rule for the proximal center @xmath24 . in formula and in the algorithm description below we write simply @xmath35 for @xmath36 .",
    "we denote the function approximating @xmath37 in by @xmath38    * selective linearization ( slin ) algorithm * + * step 0 : * set @xmath39 and @xmath40 , select @xmath41 and , for all @xmath42 , linearization points @xmath43 where the corresponding subgradients @xmath44 exist .",
    "define @xmath45 for @xmath42 .",
    "choose parameters @xmath46 , and a stopping precision @xmath47 .",
    "+ * step 1 : * find the solution @xmath48 of the @xmath49-subproblem and define @xmath50 * step 2 : * if @xmath51 then stop . otherwise , continue .",
    "+ * step 3 : * if @xmath52 then set @xmath53 ( _ descent step _ ) ; otherwise set @xmath54 ( _ null step _ ) . + * step 4 : * select @xmath55 for all @xmath56 , set @xmath57 and @xmath58 ( so that @xmath59 ) . increase @xmath26 by  1 and go to step 1 .",
    "few comments are in order .",
    "since the point @xmath48 is a solution of the subproblem , the vector @xmath60 calculated in is indeed a subgradient of @xmath49 at @xmath48 ; in fact , it is exactly the subgradient that features in the optimality condition for at @xmath48 .",
    "therefore , at all iterations , the functions @xmath61 are minorants of the functions @xmath33 .",
    "this in turn implies that @xmath62 is a lower approximation of @xmath16 .",
    "consequently , @xmath63 in , with @xmath64 equivalent to @xmath24 being the minimizer of @xmath16 .    in practical implementation of the algorithm , the points @xmath65 need not be stored .",
    "it is sufficient to memorize @xmath66 and the subgradients @xmath67 . at step 4",
    ", we then set @xmath68 and @xmath69 for all @xmath70 , while @xmath71 . for @xmath72 these data",
    "are not needed , because the function @xmath73 will not be linearized at the next iteration .    in some cases , the storage of the subgradients @xmath67 may be substantially simplified .",
    "[ e : sumphi ] suppose @xmath74 with convex functions @xmath75 and @xmath76 , @xmath77 .",
    "then every subgradient of @xmath78 has the form @xmath79 , with @xmath80 .",
    "the scalars @xmath81 are sufficient for recovering the subgradients , because the vectors @xmath82 are part of the problem data .",
    "we assume that @xmath83 in step 2 . to prove convergence of the algorithm , we consider two cases : with finitely or infinitely many descent steps .",
    "we first address the finite case and show that the proximal center updated in the last descent step must be an optimal solution to problem . to this end , we prove that if a null step is made at iteration @xmath26 , then the optimal objective function values of consecutive subproblems are increasing and the gap is bounded below by a value determined by @xmath84 we shall also use this result in the proof of convergence rate .",
    "we denote the optimal objective function value of subproblem at iteration @xmath26 by @xmath85    [ lem : increment_bound ] if a null step is made at iteration @xmath26 , then @xmath86 where @xmath87 with an arbitrary @xmath88 .    the change from the @xmath49-subproblem to the @xmath89-subproblem",
    "can be viewed as two steps : first is the change of @xmath90 to @xmath91 , followed by the change of @xmath92 to @xmath73 . by the selection of the subgradient and the resulting construction of @xmath91 , the first operation does not change the solution and the optimal value of the subproblem .",
    "thus the optimal value of satisfies the following equation : @xmath93 since @xmath94 at a null step , and @xmath95 , the second operation can only increase the optimal value of the last problem .",
    "therefore , @xmath96 .",
    "consider the family of relaxations of the @xmath89-subproblem at iteration @xmath97 : @xmath98 with parameter @xmath99 $ ] . in the above relaxation",
    ", the function @xmath73 is replaced by a convex combination of its two affine minorants : one at the point @xmath100 , which is @xmath92 used at iteration  @xmath26 , and the other one at the @xmath26th trial point @xmath48 , with an arbitrary subgradient @xmath101 . due to",
    ", the value of ( [ eqn : f2_relax ] ) with @xmath102 coincides with  @xmath103 .",
    "therefore , the difference between @xmath104 and @xmath103 can be estimated from below by the increase in the optimal value @xmath105 of ( [ eqn : f2_relax ] ) when @xmath106 moves away from zero .",
    "that is , @xmath107}\\hat{q}_k(\\mu ) - \\hat{q}_k(0).\\ ] ]    define @xmath108 .",
    "note that @xmath109 , since @xmath110 for @xmath111 .",
    "we also define @xmath112 , so @xmath113 $ ] . by direct calculation , and with the use of , the solution of ( [ eqn : f2_relax ] ) has the form @xmath114 = z^k_{j_k}+ \\mu d^{-1}\\big(s_{j_{k+1}}^k - g^{k}_{j_{k+1}}\\big).\\ ] ] using the definitions following ( [ eqn : f2_relax ] ) and the fact that @xmath115 , the derivative of @xmath116 can be expressed as follows : @xmath117 in the inequality above , we used the definition of @xmath72 and the fact that the maximum of the differences @xmath118 over @xmath119 is larger than their average .",
    "thus @xmath120 substitution of the definition of @xmath121 yields @xmath122 if a null step is made at iteration @xmath26 , then the update step rule ( [ eqn : n_new_rule1 ] ) is violated .",
    "thus , @xmath123 plugging this lower bound on @xmath124 into ( [ eqn : gap ] ) and using the definition of @xmath125 , we obtain the postulated bound .",
    "finally , we remark that @xmath126 , because @xmath127 .",
    "we also need to estimate the size of the steps made by the method .",
    "[ l : radius ] at every iteration @xmath26 , @xmath128    since @xmath129 and @xmath48 is a solution of the strongly convex problem , we have @xmath130 rearranging , we obtain .",
    "we are now ready to prove optimality in the case of finitely many descent steps .",
    "[ t : finite - descent ] suppose @xmath83 , the set @xmath131 is finite and @xmath132 . let @xmath133 be the largest index such that @xmath134",
    ". then @xmath135 .",
    "we argue by contradiction .",
    "suppose @xmath136 .",
    "if @xmath83 the method can not stop , because @xmath137 , for all @xmath138 . therefore , null steps are made at all iterations @xmath138 , with @xmath139 . by lemma [ lem : increment_bound ] ,",
    "the sequence @xmath140 is nondecreasing and bounded above by @xmath141 .",
    "hence @xmath142 . the right hand side of estimate with @xmath143 for @xmath144 , owing to the monotonicity of @xmath140 , is nonincreasing , and thus the sequence @xmath145 is bounded .",
    "since the subgradients of a convex function are locally bounded , the differences @xmath146 appearing in the definition of @xmath125 in lemma [ lem : increment_bound ] are bounded from above . therefore , @xmath147 . as @xmath148",
    ", we have @xmath149 .    on the other hand",
    ", the inequality @xmath150 implies that @xmath151 for all @xmath152 . since @xmath153 , we have @xmath154 , which contradicts the convergence of @xmath140 to @xmath141 .    we now address the infinite case . note that the update test ( [ eqn : n_new_rule1 ] ) can be expressed as follows : @xmath155    [ t : infinite - descent ] suppose @xmath156 . if the set @xmath157 is infinite , then @xmath158 , for some @xmath159 .",
    "consider iteration @xmath160 ( descent step ) . from the optimality condition for we obtain @xmath161 + d\\big(z^k_{j_k } - x^k\\big),\\ ] ] which yields @xmath162.\\ ] ] then for any point @xmath163 we obtain @xmath164 hence @xmath165 using , we can continue this chain of inequalities as follows @xmath166 thus , adding up ( [ 6-bundle - scalarproduct ] ) for all @xmath167 , @xmath168 , and noting that the null steps do not change the proximal centers , we obtain @xmath169 the term @xmath170 is non - positive , and the last term is bounded by @xmath171 .",
    "thus , several conclusions follow from inequality .",
    "first , the sequence @xmath172 is bounded , because their distances to @xmath173 are bounded . secondly , rewriting ( [ 6-bundle - key ] ) as @xmath174 and letting @xmath175 in @xmath176 , we deduce that @xmath177 consequently , @xmath178 as @xmath179 in @xmath176 . as the null steps do not change the proximal centers , we also have @xmath180 , when @xmath179 .    to prove that the sequence of proximal centers converges to an optimal solution , note that since the infinite sequence @xmath172 is bounded , it has a convergent subsequence whose limit @xmath181 is a minimizer of @xmath182 . without loss of generality , we substitute @xmath181 for @xmath173 in the above derivations , and add ( [ 6-bundle - scalarproduct ] ) for all @xmath167 such that @xmath183 . for any @xmath184",
    "we obtain the following analog of : @xmath185 the right hand side of the last inequality can be made arbitrarily small by choosing @xmath186 from the subsequence converging to @xmath181 .",
    "therefore the entire sequence @xmath172 is convergent to @xmath181 .",
    "we finish this section with a number of conclusions , which will be useful in the analysis of the rate of convergence .    [",
    "l : eta - in - descent ] if there is a descent step at iteration @xmath26 , then @xmath187    by , @xmath188 the optimal value of at iteration @xmath97 can be then estimated as follows : @xmath189 the minimizer on the right hand side is @xmath190 , and we conclude that @xmath191 which proves the left inequality in . to prove the right inequality , we observe that the test for the descent step is satisfied at iteration @xmath26 , and thus @xmath192 the expression on the right hand side can be calculated with the use of , exactly as in the derivations above , which yields @xmath193 this proves the right inequality in",
    ".    we can now summarize convergence properties of the sequences generated by the algorithm .",
    "[ c : summary ] suppose @xmath156 and @xmath83 .",
    "then a point @xmath194 exists , such that :    ii    @xmath195 ;    @xmath196 .",
    "the convergence of @xmath197 to a minimum point @xmath173 has been proved in theorems [ t : finite - descent ] and [ t : infinite - descent ] .",
    "it remains to verify the convergence properties of @xmath145 and @xmath140 .",
    "it follows from lemmas [ lem : increment_bound ] and [ l : eta - in - descent ] that the sequence @xmath198 is nondecreasing . since @xmath199 by construction , this sequence is bounded from above , and thus convergent .",
    "therefore , a limit @xmath200 of @xmath140 exists and @xmath201 .",
    "if the number of descent steps is finite , the equality @xmath202 follows from theorem [ t : finite - descent ] .",
    "if the number of descent steps is infinite , inequality at each descent step @xmath26 yields : @xmath203 passing to the limit over descent steps @xmath204 we conclude that @xmath205 .",
    "consequently , @xmath206 and assertion ( ii ) is true .",
    "the convergence of the sequence @xmath145 to @xmath173 follows from inequality , because @xmath207 and @xmath208 .",
    "our objective in this section is to estimate the rate of convergence of the method . to this end , we assume that @xmath209 at step 2 ( inequality ) and we estimate the number of iterations needed to achieve this accuracy .",
    "we also make an additional assumption about the growth rate of the function @xmath16 .",
    "[ a : growth ] the function @xmath16 has a unique minimum point @xmath173 and a constant @xmath210 exists , such that @xmath211 for all @xmath212 .",
    "assumption [ a : growth ] has a number of implications on the properties of the method .",
    "first , we recall from ( * ? ? ?",
    "7.12 ) the following estimate of the moreau  yosida regularization .",
    "[ l : lemma7.12 ] for any point @xmath212 , we have @xmath213 where @xmath214$},\\\\ -1 + 2 t & \\text{if\\ ; $ t \\ge 1$}. \\end{cases}\\ ] ]    see ( * ? ? ?",
    "7.12 ) .",
    "[ lem : conv_rate_1 ] suppose assumption [ a : growth ] is satisfied . then the stopping test implies that @xmath215    as @xmath216 , the stopping criterion implies that @xmath217 consider two cases .",
    "+ _ case 1 : _ if @xmath218 , then with @xmath219 yields @xmath220 combining this inequality with , we conclude that @xmath221 substitution of the denominator by the upper estimate @xmath222 implies .",
    "+ _ case 2 : _ @xmath223 . then yields @xmath224 with a view to",
    ", we obtain @xmath225 which implies that @xmath226 in this case .",
    "[ l : fd - improvement ] suppose assumption [ a : growth ] is satisfied . then at any iteration @xmath26 we have @xmath227    by lemma [ l : radius ] , @xmath228 to derive a lower bound for the right hand side of the last inequality , we use assumption [ a : growth ] in with @xmath219 .",
    "we obtain @xmath229 by the definition of the moreau ",
    "yosida regularization , for any optimal solution @xmath173 we have @xmath230 and thus @xmath231 substitution to yields @xmath232 which can be manipulated to @xmath233 this can be combined with the first inequality in the proof , to obtain the desired result .    in order to estimate the number of iterations of the method needed to achieve the prescribed accuracy , we need to consider two aspects .",
    "first , we prove linear rate of convergence between descent steps",
    ". then , we estimate the numbers of null steps between consecutive descent steps .    by employing the estimate of lemma [ lem : conv_rate_1 ]",
    ", we can address the first aspect . to simplify notation , with no loss of generality",
    ", we assume that @xmath234 $ ] ( otherwise , we would have to replace @xmath235 with @xmath236 in the considerations below ) .    [ lem : conv_rate_2 ] suppose @xmath237 is the unique minimum point of @xmath16 and assumption [ a : growth ] is satisfied .",
    "then at every descent step @xmath26 , when the update step rule is satisfied , we have the inequality : @xmath238    it follows from the update rule ( [ eqn : n_new_rule1 ] ) that @xmath239 using lemma [ lem : conv_rate_1 ] with @xmath240 , we obtain @xmath241 combining these inequalities and simplifying , we conclude that @xmath242 subtracting @xmath243 from both sides , we obtain the linear rate .",
    "we now pass to the second issue : the estimation of the number of null steps between two consecutive descent steps .",
    "we shall base it on the analysis of the gap @xmath244 .    by virtue of corollary",
    "[ c : summary ] , all points @xmath145 generated by the algorithm are uniformly bounded .",
    "since subgradients of finite - valued convex functions are locally bounded , the subgradients of all @xmath49 are bounded , and thus a constant @xmath245 exists , such that @xmath246 at all null steps . with no loss of generality , we assume that @xmath247 .",
    "[ lem : conv_rate_3 ] if a null step is made at iteration @xmath26 , then @xmath248 where @xmath249    by lemma [ lem : increment_bound ] , we have @xmath250 on the other hand , @xmath251 combining the last two inequalities , we conclude that @xmath252 consider the definition of @xmath125 in lemma [ lem : increment_bound ] . if @xmath253 , then @xmath254 is no greater than the bound , because @xmath255 .",
    "otherwise , @xmath125 is given by the second case in .",
    "since the algorithm does not stop , we have @xmath256 , and thus @xmath257 substitution to yields .",
    "let @xmath258 be three consecutive proximal centers in the algorithm ( @xmath259 ) .",
    "we want to bound the number of iterations with the proximal center @xmath260 .",
    "to this end , we bound two quantities :    \\1 .",
    "the optimal objective value of the _ first _ subproblem with proximal center @xmath260 , whose iteration number we denote by @xmath261 : @xmath262 we need an upper bound for @xmath263 .",
    "\\2 . the optimal objective value of the _ last _ subproblem with proximal center @xmath260 , occurring at iteration @xmath264 :",
    "@xmath265 we need an upper bound for @xmath266 which implies the update rule ( [ eqn : n_new_rule1 ] ) .    in the following we discuss each issue separately .",
    "recall that according to the algorithm , @xmath260 is the optimal solution of the last subproblem with proximal center @xmath267 .",
    "let @xmath268 be the non - linearized component function of the last subproblem with proximal center @xmath267 , whose optimal solution is @xmath260 .",
    "the optimal value of the subproblem is @xmath269    [ lem : null_step_start_bound ] if a descent step is made at iteration @xmath270 , then @xmath271    the left inequality in yields @xmath272 since @xmath273 , we obtain @xmath274 as iteration @xmath270 is a descent step , the update rule ( [ eqn : n_new_rule1 ] ) holds .",
    "thus @xmath275 - \\frac{1}{2}\\big\\|x^{(\\ell ) } - x^{(\\ell-1)}\\big\\|_d^2\\\\   & \\leq \\frac{1}{\\beta}\\big[f(x^{(\\ell-1 ) } ) - f(x^{(\\ell)})\\big ] - \\frac{1}{2}\\big\\|x^{(\\ell ) } - x^{(\\ell-1)}\\big\\|_d^2.\\end{aligned}\\ ] ] combining the last two inequalities we obtain @xmath276 the right inequality in can be now used to substitute @xmath277 on the right hand side to obtain .",
    "we can now integrate our results .",
    "applying lemma [ l : fd - improvement ] , we obtain the following inequality at _ every _ null step with prox center  @xmath260 : @xmath278 from lemma [ lem : null_step_start_bound ] we know that for @xmath279 the initial value of the left hand side ( immediately after the previous descent step ) is bounded from above by the following expression : @xmath280 lemma [ lem : conv_rate_3 ] established a linear rate of decrease of the left hand side .",
    "therefore , the number @xmath281 of null steps with proximal center @xmath260 , if it is positive , satisfies the inequality : @xmath282 consequently , for @xmath279 we obtain the following upper bound on the number of null steps : @xmath283 if the number @xmath281 of null steps is zero , inequality yields @xmath284 elementary calculations prove that both logarithms on the right hand side of are negative , and thus inequality is satisfied in this case as well .",
    "suppose there are @xmath285 proximal centers appearing throughout the algorithm : @xmath286 , @xmath287 , ",
    ", @xmath288 .",
    "they divide the progress of the algorithm into @xmath285 series of null steps . for the first series , similarly to the analysis above",
    ", we use and lemma [ lem : conv_rate_3 ] to obtain the estimate @xmath289 for the last series , we observe that the inequality @xmath290 has to hold at each null step at which the stopping test was not satisfied .",
    "we use it instead of and we obtain @xmath291 we aggregate the total number of null steps for different proximal centers throughout the algorithm and we obtain the following bound : @xmath292   + l \\end{split}\\ ] ] let us recall the definition of @xmath293 in , and denote @xmath294 so that @xmath295 . since @xmath296 , we derive the following inequality for the number of null steps : @xmath297 + l.\\ ] ] let us now derive an upper bound on the number @xmath285 of descent steps . by virtue of and , descent steps are made only if @xmath298 otherwise , the method must stop . to explain it more specifically , if @xmath299 , then @xmath300 . if a descent step were made , @xmath301 .",
    "then @xmath302 .",
    "since @xmath303 , the algorithm would have already stopped , which contradicts our assumption .",
    "it follows from lemma [ lem : conv_rate_2 ] , that @xmath304 therefore , @xmath305 as a result , we have the final bound for the total number of descent and null steps : @xmath306 \\\\ & { \\quad } + \\frac{1}{\\varepsilon c}\\ln\\left ( \\frac{f(x^{(1 ) } ) - \\eta^1}{\\varepsilon}\\right ) + 2\\frac{\\ln(\\beta\\varepsilon ) - \\ln\\big(f(x^{(1 ) } ) - f(x^{*})\\big)}{\\ln ( 1 - \\alpha \\beta ) } + 2 .",
    "\\end{split}\\ ] ] therefore , in order to achieve precision @xmath0 , the number of steps needed is of order @xmath307 this is almost equivalent to saying that given the number of iterations @xmath26 , the precision of the solution is approximately @xmath308 .",
    "in many areas in data mining and machine learning , such as computer vision and compressed sensing , the resulting optimization models consist of a convex loss function and multiple convex regularization functions , called the _ composite prior models _ in @xcite . for example , in compressed sensing , the linear combination of the total variation ( tv ) norm and @xmath309 norm is a popular regularizer in recovering magnetic resonance ( mr ) images .",
    "formally , the models are formulated as follows : @xmath310 where @xmath311 is the loss function to measure the goodness - of - fit of the data , while the functions @xmath312 are regularization terms .",
    "all the functions are convex but not necessarily smooth .",
    "the slin algorithm introduced in our paper can be directly applied to solve the general problem .",
    "it can be further specialized to take advantage of additional features of the functions involved .",
    "in the following subsection we discuss one such specialization .",
    "the problem is defined as follows : @xmath313 where @xmath314 is an @xmath315 matrix , and @xmath316 are fixed parameters .",
    "this model contains two regularization terms : the _ lasso _ penalty @xmath317 , and the _ fused lasso _ penalty @xmath318 .",
    "we name the first function as @xmath319 . in models with a quadratic loss function",
    ", we found it convenient to use the matrix @xmath320 in the proximal term of the method ( * ? ? ?",
    "3 ) .    in order to solve each subproblem",
    ", we need the gradient of @xmath321 and subgradients of the regularization functions , which are readily available .",
    "our method requires their explicit calculation at the initial iteration only ; at later iterations they are obtained implicitly , as described in step 1 of the algorithm .    with these",
    ", we can solve each subproblem iteratively .    * the @xmath311-subproblem .",
    "* skipping the constants , the @xmath311-subproblem has the form : @xmath322 this is a unconstrained quadratic optimization problem and its optimal solution can be obtained by solving the following linear system of equations : @xmath323 it can be very efficiently solved by the preconditioned conjugate gradient method with preconditioner @xmath20 , as discussed in ( * ? ? ?",
    "3 ) , because the condition index of the system is uniformly bounded .",
    "only matrix  vector multiplications are involved , facilitating the use of a sparse structure of @xmath314 .",
    "after the solution is obtained , the gradient of @xmath324 and its linearization can be determined by step 1 of the slin algorithm .    *",
    "the @xmath325-subproblem . *",
    "the subproblem is defined as follows ( ignoring the constants ) : @xmath326 this problem is separable in the decision variables , with the following closed - form solution : @xmath327 here @xmath328 .",
    "the solution of the @xmath325-subproblem gives a new subgradient of @xmath325 at the minimal point .",
    "* the @xmath329-subproblem .",
    "* the subproblem is defined as follows ( ignoring the constants ) : @xmath330 exactly as described in @xcite , this problem can be equivalently formulated as a constrained optimization problem : @xmath331 with an @xmath332 matrix @xmath333 representing the system @xmath334 , @xmath335 .",
    "the lagrangian of problem ( 19 ) has the form @xmath336 where @xmath106 is the dual variable .",
    "the minimum of the lagrangian with respect to @xmath337 is finite if and only if @xmath338 . under this condition ,",
    "the minimum value of the @xmath337-terms is zero and we can eliminate them from the lagrangian .",
    "we arrive to its reduced form , @xmath339 to calculate the dual function , we minimize @xmath340 with respect to @xmath10 . after elementary calculations ,",
    "we obtain the solution @xmath341 substituting it back to the lagrangian , we obtain the following dual problem : @xmath342 this problem can be treated as a box - constrained quadratic programming problem , for which many efficient algorithms are available , for example coordinate - wise optimization ( * ? ? ?",
    "4 ) . due to",
    "the structure of @xmath333 , the computational effort per iteration is linear in the problem dimension .",
    "we consider the following problem @xmath343 where @xmath344 .",
    "this model contains the first function as @xmath345 where parameter @xmath346 and number of groups @xmath347 are pre - specified parameters .",
    "the second part is a sum of regularization terms , each penalty function as @xmath348 where the weights @xmath349 are known parameters .",
    "@xmath350 is the index set of a group of variables and @xmath351 denotes the subvector of @xmath10 with coordinates in @xmath352 .",
    "this group regularizer has been proven useful in high - dimensional statistics with the capability of selecting meaningful groups of features .",
    "the groups could overlap as needed . as the quadratic term has a coefficient of @xmath353 , the diagonal matrix @xmath20 in the proximal term of the method",
    "is set to @xmath354 .    * the @xmath311-subproblem . *",
    "the @xmath311-subproblem has the form : @xmath355 it has the same structure as the @xmath311-subproblem of the general structured fused lasso example , and can be solved in the same way ; just the matrix @xmath20 is different .    *",
    "the @xmath356-subproblem . *",
    "the @xmath356-subproblem is defined as follows ( ignoring the constants ) : @xmath357 where @xmath358 ; with @xmath359 denoting a subgradient of the function @xmath311 , and @xmath360 the subgradients of @xmath361 used in . to simplify notation , from now on",
    "we write @xmath362 for @xmath352 .",
    "the decision variables that are outside of the current group @xmath362 , which we denote @xmath363 , have the following closed - form solution : @xmath364 the variables in the current group @xmath362 can be calculated as follows .",
    "if @xmath365 , the necessary and sufficient optimality condition for is the following equation : @xmath366 we denote @xmath367 this leads to @xmath368 substituting into , after simple manipulations , we obtain the following equation for @xmath369 : @xmath370 since the left hand side of this equation is an increasing function of @xmath369 , we can easily solve it by bisection , if a solution exists .",
    "if the columns if @xmath314 are normalized , then all @xmath371 , and equation can be solved in closed form .",
    "letting @xmath372 on the left hand side , we obtain the condition for the existence of a solution of : @xmath373 if inequality is satisfied , @xmath369 can be found by bisection and @xmath374 follows from .",
    "if is not satisfied , the only possibility is that the optimal solution of is @xmath375 .",
    "in this section , we present some experimental results for problems and .",
    "all these studies are performed on an 1.8 ghz , 4 gb ram computer using matlab .      in tables",
    "[ table : running_time1 ] and [ table : running_time2 ] , we evaluate slin against three its modifications , in order to assess the usefulness of the main features of the method : the dynamic selection of the block to be optimized , and the sufficient improvement test .",
    "the first modification processes the blocks in a fixed order and performs the improvement test after every block , to decide whether to change the current value of @xmath24 . in a way , it is a direct extension of the alternating linearization method ( alin ) of @xcite , and is labeled as such in the tables . the second modification processes the blocks in a fixed order and updates @xmath24 after each cycle of 3 blocks . in the case of two blocks , it would correspond to the douglas  rachford operator splitting method of @xcite ; see @xcite .",
    "we use the name douglas  rachford in the tables .",
    "the third modification processes the blocks in a fixed order and carries out the update of @xmath24 after each block . in the case of two blocks",
    ", it would correspond to the peaceman ",
    "rachford operator splitting method of @xcite , as explained in @xcite , and we use this name in the tables .",
    "we report the average performance of all 4 versions in cases when @xmath376 @xmath377 and when @xmath378 @xmath379 , with different tolerance and regularization parameters .",
    "we run the experiments 10 times with different samples of the matrix @xmath314 and report the average results and their standard deviation . in the tables ,",
    "`` iterations '' denotes the average iteration number with 1000 as the default maximum iteration number ; `` time '' denotes the average cpu run time in seconds ; `` relative error '' is defined as the relative difference between the optimal value ( obtained by matlab `` fminunc '' function ) and those obtained by the slin , alin , and operator splitting methods , respectively .",
    "`` maxiter '' and `` na '' indicate that the algorithm did not converge in the pre - specified number of 1000 iterations .",
    "we observe that to obtain the same accuracy , slin requires significantly fewer iterations and less cpu time than the other versions .",
    "the versions without sufficient progress test ( nave extensions of operator splitting methods ) did not converge in many cases . the version with the fixed block order ( nave extension of the alternating linearization method ) , although convergent , was always significantly slower .",
    "ccccc & & & & +   + @xmath382 & selective linearization & 55(28.75 ) & 0.99(0.47 ) & 0.0156(0.0043 ) + @xmath383 & alternating linearization & 66(31.04 ) & 1.10 ( 0.99 ) & 0.0161(0.0048 ) + & douglas - rachford & 323(78.57 ) & 5.53 ( 0.96 ) & 0.0161(0.0046 ) + & peaceman - rachford & maxiter(0.00 ) & 16.93 ( 0.23 ) & 0.9191(0.2123 ) + @xmath384 & selective linearization & 46 ( 25.79 ) & 1.00 ( 0.41 ) & 0.0154(0.0075 ) + @xmath383 & alternating linearization & 173 ( 31.30 ) & 2.98 ( 0.60 ) & 0.0166(0.0078 ) + & douglas - rachford & 246(42.57 ) & 4.32 ( 0.73 ) & 0.0155(0.0076 ) + & peaceman - rachford & maxiter ( 0.00 ) & 16.75 ( 0.22 ) & na(na ) + @xmath382 & selective linearization & 79(2.30 ) & 1.60 ( 0.07 ) & 0.0004(0.0000 ) + @xmath385 & alternating linearization & 91 ( 3.05 ) & 1.64 ( 0.06 ) & 0.0004(0.0000 ) + & douglas - rachford & 205(10.17 ) & 3.45 ( 0.21 ) & 0.0004(0.0000 ) + & peaceman - rachford & maxiter ( 0.00 ) & 16.57 ( 0.11 ) & na(na ) + @xmath384 & selective linearization & 177(20.68 ) & 3.06(0.88 ) & 0.0028(0.0019 ) + @xmath385 & alternating linearization & 450(50.74 ) & 8.01(4.06 ) & 0.0028(0.0019 ) + & douglas - rachford & 573(59.81 ) & 9.91 ( 0.52 ) & 0.0028(0.0019 ) + & peaceman - rachford & maxiter ( 0.00 ) & 16.60(0.15 ) & na(na ) +    ccccc & & & & +   + @xmath382 & selective linearization & 83(14.15 ) & 11.26(3.01 ) & 0.0393(0.0103 ) + @xmath383 & alternating linearization & 425(13.96 ) & 55.97(4.87 ) & 0.0456(0.0126 ) + & douglas - rachford & maxiter(0.00 ) & 130.39 ( 6.42 ) & 0.0355(0.0096 ) + & peaceman - rachford & maxiter(0.00 ) & 131.19 ( 8.48 ) & na(na ) + @xmath384 & selective linearization & 257 ( 7.97 ) & 36.76(13.93 ) & 0.0165(0.0059 ) + @xmath383 & alternating linearization & 419(8.23 ) & 59.86(1.99 ) & 0.0240(0.0083 ) + & douglas - rachford & maxiter(0.00 ) & 144.31 ( 18.14 ) & 0.0187(0.0059 ) + & peaceman - rachford & maxiter(0.00 ) & 143.93 ( 11.56 ) & 0.0118(0.0063 ) + @xmath382 & selective linearization & 210(27.96 ) & 27.30(18.07 ) & 0.0483(0.0160 ) + @xmath385 & alternating linearization & 290 ( 34.73 ) & 39.86(28.05 ) & 0.0484(0.0160 ) + & douglas - rachford & 291(53.85 ) & 39.51 ( 32.29 ) & 0.0484(0.0160 ) + & peaceman - rachford & maxiter(0.00 ) & 139.38(18.88 ) & na(na ) + @xmath384 & selective linearization & 404 ( 45.45 ) & 49.51(8.04 ) & 0.0550(0.0181 ) + @xmath385 & alternating linearization & 780(63.45 ) & 107.23(5.01 ) & 0.0550(0.0181 ) + & douglas - rachford & 891(53.07 ) & 106.94 ( 12.29 ) & 0.0550(0.0181 ) + & peaceman - rachford & maxiter(0.00 ) & 133.34(7.81 ) & na(na ) +        we compared our method with existing algorithms in the tree - structured , fixed order , and random order cases .",
    "it shows that slin does not care how the groups are partitioned and is applicable to arbitrary group - splitting cases .      in a tree - structured overlapping group lasso problem , described in @xcite ,",
    "the groups correspond to nodes of a tree .",
    "thus , for any two groups , either one is a subset of the other , or they are disjoint . the design matrix and input vector are centered and normalized to have unit @xmath386-norms .",
    "we conduct the speed comparisons between our approach and fista @xcite . from table [ table : running_time4 ]",
    "we can see that the slin algorithm is faster in terms of both iteration number and computational time .",
    "we simulate data for a univariate linear regression model with an overlapping group structure .",
    "the entries are sampled from i.i.d .",
    "normal distributions , @xmath389 , and @xmath390 , with the noise @xmath0 sampled from the standard normal distribution . assuming that the inputs are ordered , we define a sequence of @xmath347 groups of 100 adjacent inputs with an overlap of 10 variables between two successive groups , so that @xmath391 where @xmath392 .",
    "we adopt uniform weights @xmath393 and set @xmath394 .    to demonstrate the efficiency and scalability of the slin algorithm , we compared slin with several specialized methods for overlapping group lasso problems : pdmm of @xcite , sadmm or jacobian admm of @xcite , pa - apg of @xcite and s - apg of @xcite .",
    "all experiments were run sequentially , that is , no parallel processing features were exploited .",
    "we run the experiments 10 times with different samples of the matrix @xmath314 ; we report the average results .",
    "figure [ figure : compare_line ] plots the convergence of the objective function values _ versus _ the number of iterations , for the number of groups @xmath395 .",
    "for the dual methods pdmm and sadmm , we report the values of the augmented lagrangian .",
    "they go from super optimal ( because the iterates are infeasible ) and converge to the optimal value .",
    "the slin algorithm is the fastest , in terms of iterations while pdmm is the second , and sadmm the third .",
    "the two accelerated methods , pa - apg and s - apg , are the slowest in these tests .          in the next stage , we conducted additional comparisons between slin and pdmm on group lasso problems with randomized overlapping , which do not exhibit the regular group structure specified in ( [ eqn : structure_group ] ) .",
    "this type of problem arises in applications such as bioinformatics [ 3 ] , where one uses prior information to model potential overlapping of groups of variables .",
    "for example , in high throughput gene expression analysis , the number of parameters to be estimated is much greater than the sample size .",
    "one often utilizes information including gene ontology to define group overlaps among genes , thereby achieving structured regularization @xcite .",
    "the resulting overlaps are `` arbitrary '' ( depending on the specific gene ontology ) and more complex than the systematic overlapping example described in ( [ eqn : structure_group ] ) .",
    "we generated test cases in which the indices in each of the 100 groups were assigned to the @xmath396 locations .",
    "as a result , the number of overlapping variables between the groups was random , and multiple group membership was possible .",
    "the performance of the two methods on randomized overlapping group lasso problems is summarized in tables [ table : running_time4 ] and [ table : running_time5 ] .",
    "for fair comparison of the methods , we run pdmm on each instance of the problem .",
    "pdmm was set to run to @xmath397 or 2,000 iterations , whichever came first .",
    "we set the tuning parameters @xmath398 , and @xmath399 , respectively .",
    "then slin was set to run until the objective function values obtained were as good as that of pdmm .",
    "we run the experiments 10 times with different samples of the randomly generated groups ; in tables [ table : running_time5 ] and [ table : running_time6 ] we report the average results and their standard deviations . in all cases ,",
    "the number of iterations of slin is much smaller than that of pdmm . in the determined cases , where @xmath400 and @xmath401 , the running time of slin is usually better than that of pdmm . in the under - determined cases , where @xmath402 and @xmath403 , the running time of slin is slightly worse than that of pdmm .    in summary",
    ", we can conclude that slin is a highly efficient and reliable general - purpose method for multi - block optimization of convex nonsmooth functions .",
    "it successfully competes with dedicated methods for special classes of problems .",
    "cccc & & & +   + @xmath404&slin & 280 ( 15.82 ) & 5.42(0.36 ) + @xmath405 & pdmm & 638(21.14 ) & 5.48(0.10 ) + @xmath404&slin & 308 ( 16.11 ) & 5.38(0.43 ) + @xmath406 & pdmm & 836(29.33 ) & 7.19(0.39 ) + @xmath404&slin & 331 ( 13.05 ) & 5.89(0.28 ) + @xmath407 & pdmm & 991 ( 75.51 ) & 8.91(0.81 ) + @xmath404&slin & 306 ( 11.79 ) & 5.53(0.23 ) + @xmath408 & pdmm & 560(59.47 ) & 4.75(0.45 ) + @xmath404&slin & 330 ( 11.19 ) & 5.63(0.22 ) + @xmath409 & pdmm & 620(38.33 ) & 5.65(0.91 ) + @xmath404&slin & 364 ( 13.77 ) & 6.09(0.31 ) + @xmath410 & pdmm & 741(58.06 ) & 4.14 ( 0.47 ) +    cccc & & & +   + @xmath411&slin & 1280 ( 39.73 ) & 17.75(0.60 ) + @xmath405 & pdmm & 1453(157.07 ) & 12.01(1.03 ) + @xmath411&slin & 1119 ( 57.76 ) & 16.61(1.00 ) + @xmath406 & pdmm & 1566(61.63 ) & 13.88(1.63 ) + @xmath411&slin & 973 ( 39.46 ) & 13.72(1.241 ) + @xmath407 & pdmm & 1753 ( 122.27 ) & 16.75(2.64 ) + @xmath411&slin & 792 ( 36.58 ) & 9.79(0.73 ) + @xmath408 & pdmm & 968(39.82 ) & 7.53(0.54 ) + @xmath411&slin & 722 ( 23.46 ) & 8.87(0.65 ) + @xmath409 & pdmm & 1170(102.16 ) & 9.57(1.00 ) + @xmath411&slin & 683 ( 15.66 ) & 8.01(0.41 ) + @xmath410 & pdmm & 1208(70.30 ) & 10.81 ( 1.21 ) +        d.  p. bertsekas .",
    "incremental aggregated proximal and augmented lagrangian algorithms .",
    "technical report report lids-3176 , department of electrical engineering and computer science , massachusetts institute of technology , cambridge , mass . , 2015 .",
    "p.  l. combettes and j .- c .",
    "proximal splitting methods in signal processing . in _",
    "fixed - point algorithms for inverse problems in science and engineering _ , springer optimization and its applications , pages 185212 .",
    "springer , 2011 ."
  ],
  "abstract_text": [
    "<S> we consider the problem of minimizing a sum of several convex non - smooth functions . we introduce a new algorithm called the selective linearization method , which iteratively linearizes all but one of the functions and employs simple proximal steps . </S>",
    "<S> the algorithm is a form of multiple operator splitting in which the order of processing partial functions is not fixed , but rather determined in the course of calculations . </S>",
    "<S> global convergence is proved and estimates of the convergence rate are derived . specifically , the number of iterations needed to achieve solution accuracy @xmath0 is of order @xmath1 . </S>",
    "<S> we also illustrate the operation of the algorithm on structured regularization problems . </S>",
    "<S> + _ keywords : _ nonsmooth optimization , operator splitting , multiple blocks , alternating linearization </S>"
  ]
}