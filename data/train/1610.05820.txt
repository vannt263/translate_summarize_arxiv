{
  "article_text": [
    "machine learning is the foundation of popular internet services such as image and speech recognition and natural language translation .",
    "many companies also use machine learning internally , to improve marketing and advertising , recommend products and services to users , or better understand the data generated by their operations . in all of these scenarios , activities of individual userstheir purchases and preferences , health data , online and offline transactions , photos",
    "they take , commands they speak into their mobile phones , locations they travel toare used as the training data .",
    "internet giants such as google and amazon are already offering `` machine learning as a service . ''",
    "any customer in possession of a dataset and a data classification task can upload this dataset to the service .",
    "the service automatically constructs a model and makes it available to the customer , typically as a black - box api .",
    "for example , a mobile - app maker can use this service to analyze users activities and use the resulting model inside the app to promote in - app purchases to users when they are most likely to respond .",
    "machine - learning services also let data owners expose their models to external users for querying or sell them while keeping the training data confidential .    * _ our contributions . _",
    "* we investigate what , given black - box access to a machine learning model , can be inferred about the model s training dataset .",
    "we start with the fundamental question that we call * membership inference * : given a model and a record , determine whether this record was used to train the model .",
    "this question is interesting from both the machine learning and privacy perspectives .",
    "we answer it quantitatively in the most difficult setting , where the adversary s access to the model is limited to `` black - box '' predict ( ) calls that return the model s output on a given input .    to answer the membership inference question ,",
    "we turn machine learning against itself and train an _ attack model _ whose purpose is to distinguish the target model s behavior on training inputs from its behavior on inputs that it did not encounter during training . in other words",
    ", we turn the membership inference problem into a classification problem .",
    "attacking black - box models , e.g. , those built by commercial `` machine learning as a service '' providers , requires more sophistication than attacking white - box models whose structure and parameters are known to the adversary . to construct the attack model , we invented a new _ shadow training _ technique .",
    "first , we create multiple `` shadow models '' that imitate the behavior of the target model , but for which we know the training datasets and thus the ground truth about membership in these datasets .",
    "we then train the attack model on the inputs and outputs of the shadow models .",
    "we developed several effective methods to generate training data for the shadow models .",
    "the first method uses black - box access to the target model to synthesize this data .",
    "the second method uses statistics about the population from which the target s training dataset was drawn .",
    "the third method assumes that the adversary has access to a noisy version of the target s training dataset",
    ". the first method does not assume any prior knowledge about the distribution of the target model s training data , while the second and third methods allow the attacker to query the target model only _ once _ before inferring whether a given record was in its training dataset .",
    "our inference techniques are generic and not based on any particular dataset or model type .",
    "we evaluate them on neural networks , as well as black - box models trained using amazon ml and google prediction api .",
    "all of our experiments on amazon and google were done without knowing the learning algorithms used by these services , nor the architecture of the resulting models , since they do nt reveal this information to the users . for our evaluation",
    ", we use concrete datasets of images , retail purchases , and location traces , realistic classification tasks , and standard model - training procedures .",
    "in addition to demonstrating that inference attacks are successful , we quantify how their success relates to the classification tasks and to the standard machine learning metrics for overfitting .",
    "our experimental results show that models created using machine - learning - as - a - service platforms can leak a lot of information about their training datasets . for multi - class classification models trained on 10,000-record retail purchase transaction",
    "datasets using google and amazon with default configurations , our membership inference achieves median accuracy of @xmath0 and @xmath1 , respectively .",
    "the attack accuracy against google - trained models is @xmath2 if shadow training is performed on fully synthetic data .    inferring information about the model s training dataset",
    "should not be confused with procedures such as model inversion that use a model s output on a hidden input to infer something about this input  @xcite or extract features that characterize one of the model s classes  @xcite . as explained in  @xcite and section  [ sec : relatedwork ] , model inversion does not produce an actual member of the model s training dataset , nor , given a record , infer whether this record was in the training dataset .",
    "by contrast , the membership inference problem we study in this paper is similar , in spirit , to the problem of identifying the presence of an individual s data in a mixed pool given some statistics about the pool  @xcite .    in summary , this paper demonstrates and quantifies the problem of machine learning models leaking information about their training datasets . to train our attack models",
    ", we developed a new shadow learning technique that works with minimal knowledge about the target model and its training dataset .",
    "finally , we quantify how leakage of membership information is related to model overfitting .",
    "* _ attacks on statistical and machine learning models . _ * in @xcite , knowledge of the parameters of svm and hmm models is used to infer _ general statistical information _ about the training dataset , for example , whether records of a particular race were used during training . by contrast",
    ", our inference attacks work in the black - box setting , without any knowledge of the model s parameters , and infer information about _ specific records _ in the training dataset , as opposed to general statistics .",
    "homer et al .",
    "@xcite developed techniques for inferring the presence of a particular genome in a dataset .",
    "their approach involves comparing the published statistics about this dataset ( in particular , minor allele frequencies ) to the distribution of these statistics in the general population .",
    "by contrast , our inference attacks target trained machine learning models , not specific statistics .",
    "other attacks on machine learning include  @xcite , where the adversary exploits _ changes _ in the outputs of a collaborative recommender system to infer inputs that caused these changes .",
    "these attacks exploit temporal behavior specific to the recommender systems based on collaborative filtering .         +         * _ model inversion .",
    "_ * model inversion  @xcite uses the output of a model applied to a hidden input to infer certain features of this input .",
    "see  @xcite for a detailed analysis of this attack and an explanation of why it does not necessarily entail a privacy breach .",
    "for example , in the specific case of pharmacogenetics analyzed in  @xcite , the model captures the correlation between the patient s genotype and the dosage of a certain medicine .",
    "this correlation is a valid scientific fact that holds for all patients , regardless of whether they were included in the model s training dataset or not .",
    "it is not possible to prevent disclosure due to population statistics  @xcite .    in general ,",
    "model inversion can not be used to determine if a particular record was used as part of the model s training dataset ( i.e. , membership inference ) . given",
    "a record and a model , model inversion works exactly the same way when the record was used to train the model and when it was _ not _ used . in the case of pharmacogenetics",
    "@xcite , model inversion produces almost identical results for members and non - members . due to the slight overfitting of the model ,",
    "the results are a little bit ( 4% ) more accurate for the members , but this accuracy can only measured in retrospect , if the adversary already knows the ground truth ( i.e. , which records are indeed members of the model s training dataset ) . by contrast , our goal is to construct a decision procedure that clearly distinguishes members from non - members .",
    "model inversion has also been applied to face recognition models  @xcite . in this scenario ,",
    "the model s output is set to @xmath3 for class @xmath4 and @xmath5 for the rest , and model inversion is used to construct an input that produces these outputs .",
    "this input is _ not _ an actual member of the training dataset , but simply an average of the features that `` characterize '' the class .",
    "in the face recognition scenarioand _ only _ in this specific scenarioeach output class of the model is associated with a single person .",
    "the training images for this class are different photos of that person .",
    "model inversion constructs an artificial image that is somehow an average of these training images . because they all happen to depict the same person , their average is recognizable ( by a human ) as that person .",
    "critically , model inversion does not produce any _ specific image _ from the training dataset , which is the definition of membership inference .    in any other scenario , where the class involves multiple individuals or objects ,",
    "the results of model inversion are semantically meaningless . to illustrate this , we ran model inversion against a convolutional neural network trained on the cifar-10 dataset , which is a standard benchmark for object recognition models .",
    "each class includes different images of a single object ( e.g. , an airplane ) .",
    "figure  [ fig : modelinversion ] shows the images `` reconstructed '' by model inversion . as expected , they do not correspond to _ any _ image with a recognizable object , let alone an image from the training dataset .",
    "we expect similar results for other models , too . for example",
    ", for the pharmacogenetics model mentioned above , this form of model inversion produces the average of different patients genomes . for the model that classifies location traces into geosocial profiles ( see section  [ locations ] ) , model inversion produces the average of the location traces of different people . in both cases ,",
    "the results of model inversion are not associated with any specific individual , nor any specific input from the training dataset .    in summary ,",
    "model inversion produces the average of the features that characterize an output class .",
    "it does _ not _ ( 1 ) construct a specific member of the training dataset , nor ( 2 ) given an input and a model , determines if this specific input was used to train the model .",
    "* _ model extraction .",
    "_ * model extraction attacks  @xcite aim to extract the parameters of a model trained on private training data .",
    "the attacker s goal is to construct a model whose predictive performance on validation data is similar to target model .",
    "model extraction can be a stepping stone for inferring information about the model s training dataset .",
    "in  @xcite , this is illustrated for a specific type of models called kernel logistic regression ( klr )  @xcite . in klr models ,",
    "the kernel function includes a tiny fraction of the training data ( so called `` import points '' ) directly into the model . since import points are parameters of the model , extracting them results in the leakage of that particular part of the data .",
    "this result is very specific to klr and does not generalize to other types of models since they do not explicitly store training data in their parameters .    even in the case of klr models ,",
    "leakage is not quantified , other than via visual similarity of a few chosen import points and `` the closest ( in l1 norm ) extracted representers . ''",
    "the results are shown only for the mnist handwritten digit dataset , where all members of a class are very similar ( e.g. , all members of the first class are different ways of writing digit 1 ) .",
    "thus , any extracted digit for a class would be similar to all images associated with that class , whether they were in the training set or not .    in summary , @xcite does not provide a general method of solving the membership inference problem , that is , given an input and a model , determine whether this specific input was used to train the model .",
    "* _ privacy - preserving machine learning . _ * existing literature on privacy protection in machine learning focuses mostly on how to learn without direct access to the training data .",
    "secure multiparty computation ( smc ) has been used for learning decision trees  @xcite , linear regression functions  @xcite , association rules  @xcite , naive bayes classifiers  @xcite , and k - means clustering  @xcite .",
    "the goal is to limit information leakage during training .",
    "the training algorithm is the same as in the non - privacy - preserving case , thus the resulting models are as vulnerable to inference attacks as any conventionally trained model .",
    "this also holds for models trained by computing on encrypted data  @xcite .",
    "differential privacy  @xcite has been applied to linear and logistic regression  @xcite , support vector machines  @xcite , risk minimization  @xcite , deep learning  @xcite , learning an unknown probability distribution over a discrete population from random samples  @xcite , and releasing hyper - parameters and classifier accuracy  @xcite . in  @xcite ,",
    "multiple parties jointly train a deep neural - network model while sharing differentially private changes in their respective model parameters . by definition",
    ", differentially private training produces models that do not depend too much on any particular member of the training dataset .",
    "we leave the design of differentially private training procedures that prevent our inference attacks to future work .",
    "machine learning algorithms help us better understand and analyze complex data .",
    "when the model is created using _ unsupervised _ training , the objective is to extract useful features from unlabeled data and build a model that explains its hidden structure .",
    "when the model is created using _ supervised _ training , which is the focus of this paper , the training records ( as inputs of the model ) are assigned labels or scores ( as outputs of the model ) .",
    "the goal is to learn the relationship between the data and the labels and construct a model that can generalize to data records beyond the training set  @xcite .",
    "model - training algorithms aim to minimize the model s prediction error on the training dataset and thus may overfit to this dataset , producing models that perform better on the training inputs than on inputs drawn from the same population but not used during the training .",
    "techniques have been proposed to prevent models from becoming too dependent on their training datasets while minimizing their prediction error  @xcite .",
    "supervised training is often used for classification and other prediction tasks .",
    "for example , a retailer may train a model that predicts a customer s shopping style to offer her suitable incentives , while a medical researcher may train a model to predict which treatment is most likely to succeed given a patient s clinical symptoms or genetic makeup .    * _ machine learning as a service . _ * major internet companies now offer machine learning as a service in their cloud platforms .",
    "examples include google prediction api , amazon machine learning ( amazon ml ) , microsoft azure machine learning ( azure ml ) , and bigml .",
    "these platforms provide simple apis for uploading the data and training and querying models , thus making state - of - the - art machine learning technologies available to any developer .",
    "for example , a developer may create an app that gathers data from users , uploads it into the cloud platform to train a model ( or update an existing model with new data ) , and then uses the model s predictions inside the app to improve its features or better interact with the users .",
    "some platforms even envision data holders training a model and sharing it through the platform s api for profit .",
    "the details of the models and the training are hidden from the data owners . the type of the model may be chosen by the service adaptively , depending on the data and perhaps accuracy on validation subsets .",
    "service providers do not warn users about the consequences of overfitting and provide little or no control over regularization .",
    "for example , google prediction api hides all details , while amazon ml provides only a very limited set of pre - defined options ( l1- or l2-norm regularization ) .",
    "the models can not be downloaded and are accessed only through the service s api .",
    "service providers derive revenue mainly by charging developers for queries through this api .",
    "therefore , for the rest of this paper , we treat `` machine learning as a service '' as a black box .",
    "all inference attacks we demonstrate are performed entirely through the services standard apis .",
    "before dealing with inference attacks , we need to define what privacy means in the context of machine learning or , alternatively , what it means for a machine learning model to breach privacy . a plausible notion of privacy , known as",
    "the `` dalenius desideratum , '' states that the model should reveal no more about the input to which it is applied than would have been known about this input without applying the model .",
    "this can not be achieved by any useful model  @xcite .",
    "a related notion of privacy appears in prior work on model inversion  @xcite : a privacy breach occurs if an adversary can use the model s output to infer the values of unintended ( sensitive ) attributes used as input to the model . as observed in  @xcite , it may not be possible to prevent this `` breach '' if the model is based on scientific or statistical facts about the population .",
    "for example , suppose that training the model has uncovered a high correlation between a person s externally observable phenotype features and genetic predisposition to a certain disease .",
    "this correlation is now a publicly known scientific fact , which allows anyone to infer information about the person s genome after observing that person .    critically , this correlation is true for _ all _ members of a given population .",
    "therefore , the model breaches privacy not just of people whose data was used to create the model , but also of every other person in the population , even those whose data was not used and whose identities are not known to the model creator ( i.e. , this is `` spooky action at a distance '' ) .",
    "there is nothing the model creator can do to protect their privacy , since the correlation underlying the model remains true regardless of the model creator s actions .",
    "to bypass the difficulties inherent in defining and protecting privacy of the general population , we focus on a narrower , more manageable goal : protecting privacy of the individuals whose data was used to train the model .",
    "this motivation is closely related to the original goals of differential privacy .    of course ,",
    "members of the training dataset are members of the population , too .",
    "we investigate what the model reveals about them _ beyond _ what it reveals about an arbitrary member of the population .",
    "our ultimate goal is to measure the specific _ membership risk _ that a person incurs if they allow their data to be used to train a model .",
    "the basic inference attack is * membership inference * , i.e. , determining whether a given data record was part of the model s training dataset .",
    "when a record is fully known to the adversary , learning that it was used to train a particular model is an indication of information leakage through the model , and in some cases can directly lead to a privacy breach .",
    "for example , knowledge that a certain patient s record was used to train a model associated with a disease ( e.g , to determine appropriate medicine dosage or discover the genetic basis of the disease ) can reveal that the patient has this disease .",
    "we investigate this problem in the * black - box * setting where the adversary can only supply inputs to the model and receive the model s output(s ) . in some situations , the model is available to the adversary indirectly .",
    "for example , an app developer may use a machine - learning service to construct a model from the data collected by the app and have the app make api calls to the resulting model . in this case , the adversary would supply inputs to the app ( rather than directly to the model ) and receive the app s outputs ( which are based on the model s outputs ) .",
    "the details of internal model usage vary significantly from app to app . for simplicity and generality",
    ", we will assume that the adversary directly supplies inputs to and receives outputs from the black - box model .",
    "consider a set of labeled data records sampled from some population and partitioned into classes .",
    "we assume that a machine learning algorithm is used to train a classification model for finding the relationship between the content of the data records and their labels . for any input data record",
    ", the model outputs the _ prediction vector _ of probabilities , one per output class , that the record belongs to that class . the class with the highest probability is selected as the predicted label for the data record .",
    "the accuracy of the model is evaluated by measuring how it generalizes beyond its training set and predicts the labels of other data records in the same population .    in general , we want to measure how much the model s output ( i.e. , the prediction vector ) leaks about the individual data records that were used to train the model .",
    "the focus of this paper is a more specific problem : measuring how much the model s output leaks about the membership of a specific data record in the model s training dataset .",
    "we assume that the attacker has black - box access to the model and can obtain the model s prediction vector on any data record .",
    "the attacker knows the format of the inputs and outputs of the model , including their number and the range of values they can take .",
    "we also assume that the attacker either ( 1 ) knows the type and architecture of the machine learning model , as well as the training algorithm , or ( 2 ) has black - box access to a machine learning oracle ( e.g. , `` machine learning as a service '' platform ) that has been used to train the model . in the latter case",
    ", the attacker does _ not _ know a priori the model s structure or meta - parameters .",
    "the attacker may have some background knowledge about the population from which the target model s training dataset was drawn .",
    "for example , he may have independently drawn samples from the population , disjoint from the target model s training dataset .",
    "alternatively , the attacker may know some general statistics about the population , for example , the marginal distribution of feature values .",
    "the setting for our inference attack is as follows .",
    "the attacker is given a data record and black - box access to the target model .",
    "the attack succeeds if the attacker can correctly determine whether this data record was part of the model s training dataset or not .",
    "the accuracy of the attack can be measured via precision ( what fraction of records inferred as members are truly members of the training dataset ) , and recall ( what fraction of the training dataset s members can be correctly inferred as members by the attacker ) .",
    "our membership inference attack exploits the observation that machine learning models often behave differently on the data that they trained on versus the data they `` see '' for the first time .",
    "overfitting is a common reason but not the only one ( see section  [ sec : whyattackworks ] ) .",
    "the objective of the attacker is to construct an _ attack model _ that can recognize such differences in the target model s behavior and use them to distinguish members from non - members of the target model s training dataset based solely on the target model s output .",
    "our attack model is a collection of models , one for each output class of the target model .",
    "this increases accuracy because the target model produces different distributions over output classes depending on the input s true class .    to train our attack model ,",
    "we build multiple `` shadow '' models intended to behave similarly to the target model .",
    "in contrast to the target model , we know the ground truth for each shadow model , i.e. , whether a given record was in its training dataset or not .",
    "therefore , we can use the inputs and outputs of the shadow models to perform supervised training of the attack model and thus teach it how to distinguish the shadow models outputs on members of their training datasets from their outputs on non - members .",
    "formally , let @xmath6 be the target model , and let @xmath7 be its private training dataset which contains labeled data records @xmath8 .",
    "a data record @xmath9 is the input to the model , and @xmath10 is the true label that can take values from a set of classes of size @xmath11 .",
    "the output of the target model is a probability vector of size @xmath11 ( the values are in @xmath12 $ ] and sum up to @xmath3 ) .",
    "let @xmath13 be the attack model .",
    "its input @xmath14 is composed of the prediction vector of size @xmath11 and the true label of the record . since the goal of the attack is decisional membership inference , the inference model is a binary classifier with @xmath15 classes corresponding to `` in '' and `` out '' labels .",
    "figure  [ fig : attack ] illustrates our end - to - end attack process . for a record @xmath16 , we use the target model to compute the prediction @xmath17 .",
    "the distribution of @xmath18 ( classification confidence values ) depends heavily on the true class of @xmath19 .",
    "this is why we pass the true label @xmath20 of @xmath19 in addition to the model prediction @xmath18 to the attack model . given how probabilities in @xmath18 are distributed around @xmath20 , the attack model computes the membership probability @xmath21 .",
    "the main challenge is how to train the attack model to distinguish members from non - members of the target model s training dataset in the black - box setting , where the attacker has no information about the internal parameters of the target model and only limited access to it through the public api . to solve this conundrum",
    ", we developed a new * shadow training * technique that lets us train the attack model on proxy targets for which we do know the training dataset and can thus perform supervised training .",
    "the attacker creates @xmath22 shadow models @xmath23 . each shadow model @xmath4",
    "is trained on a dataset @xmath24 that is of similar format and distributed similarly to the target s training dataset .",
    "these shadow training datasets can be generated using any of methods described in section  [ shadowtrain ] .",
    "of course , we assume that the datasets used for training the attacker s shadow models do not overlap with the private dataset used to train the target model ( @xmath25 ) .    the shadow models must be trained in a similar way to the target model .",
    "this is easy if the attacker already knows which machine learning algorithm ( e.g. , neural networks , svm , logistic regression ) was used to train the target model and has some information about the model s structure ( e.g. , the wiring of a neural network ) .",
    "machine learning as a service is more challenging . here",
    "the type and structure of the target model are not known , but the attacker can use exactly the same service ( e.g. , google prediction api ) to train the shadow model as was used to train the target modelsee figure  [ fig : target_shadow ] .",
    "our empirical results confirm that this is an effective way to construct shadow models .",
    "the larger the number of shadow models , the more accurate the attack model will be . as described in section  [ attacktrain ] ,",
    "the attack model is trained to recognize differences in shadow models behavior when these models operate on inputs from their own training datasets versus inputs they did not encounter during training .",
    "therefore , the more shadow models , the more training material for the attack model .    [ 1 ]    @xmath26 @xmath27 @xmath28 @xmath29    @xmath30 * return * @xmath19 @xmath31 @xmath32 @xmath28 @xmath33 @xmath34 @xmath28 @xmath26    * return * @xmath35          to train shadow models , the attacker needs training datasets that are distributed similarly to the target model s training dataset .",
    "we developed several methods for generating such training datasets .",
    "* _ model - based synthesis . _ * if the attacker does not have access to real training data and does not know any statistics about its distribution , he can generate synthetic training data for shadow models using the target model itself .",
    "the intuition is that data records that are classified by the target model with high confidence should be statistically similar to the target model s training dataset and thus provide good fodder for shadow models .",
    "the synthesis process runs in two phases : ( 1 ) _ search _ , using a hill - climbing algorithm , the space of possible data records to find records that are classified by the target model with high confidence ; ( 2 ) _ sample _ synthetic data from these records .",
    "after this process synthesizes a record , the attacker can repeat it again and again until the training dataset for shadow models is full .",
    "see algorithm  1 for the pseudocode of our synthesis procedure .",
    "first , fix class @xmath36 for which the attacker wants to generate synthetic data .",
    "the first phase is an iterative process .",
    "start by randomly initializing a data record @xmath19 .",
    "assuming that the attacker only knows the syntax of data records , sample the value for each feature uniformly at random from among all possible values of that feature . in each iteration , propose a new record .",
    "a proposed record is _ accepted _ only if it increases the hill - climbing objective : the probability of being classified by the target model as class @xmath36 .",
    "each iteration involves proposing a new candidate record by changing @xmath22 randomly selected features of the latest accepted record @xmath37 .",
    "we initialize @xmath22 to @xmath38 , and divide it by @xmath15 when @xmath39 subsequent proposals are rejected .",
    "this controls the diameter of search around the accepted record to propose a new record .",
    "we set the minimum value of @xmath22 to @xmath40 .",
    "this controls the speed of search for new records with potentially higher classification probability @xmath41 .",
    "the second , sampling phase starts when the target model s probability @xmath41 that the proposed data record is classified as belonging to class @xmath36 is larger than the probabilities for all other classes and also larger than a threshold @xmath42 .",
    "this ensures that the predicted label for the record is @xmath36 , and the target model is sufficiently confident in its label prediction .",
    "we select such record for the synthetic dataset with probability @xmath41 and , if selection fails , repeat until a record is selected .    *",
    "_ statistics - based synthesis .",
    "_ * the attacker may have some statistical information about the population from which the target model s training data is drawn .",
    "for example , the attacker may have prior knowledge of the marginal distributions of different features . in our experiments ,",
    "we generate synthetic training records for shadow models by sampling the value of each feature independently from its own marginal distribution .",
    "the resulting attack models are very effective .    *",
    "_ noisy real data .",
    "_ * finally , the attacker may have access to noisy data that is similar to the target model s training data . in our experiments with location datasets , we simulate this by flipping the ( binary ) values of 10% or 20% randomly selected features , and training the shadow models on the resulting noisy dataset .",
    "this scenario models the case where the training data for the target and shadow models are not sampled from exactly the same population , or else sampled in a non - uniform way .",
    "the main idea behind our shadow training technique is that similar models trained on relatively similar data records using the same service behave in a similar way .",
    "this observation is empirically true and borne out by our experiments in the rest of this paper .",
    "our results show that learning how to infer membership in shadow models training datasets ( for which we know the ground truth and can easily compute the cost function during supervised training ) produces an attack model that can successfully infer membership in the target model s training dataset , too .",
    "we query each shadow model with its own training dataset and with a disjoint test set of the same size .",
    "the outputs on the training dataset are labeled `` in , '' the rest are labeled `` out . ''",
    "now , the attacker has a dataset of records , the corresponding outputs of the shadow models , and in / out labels .",
    "the objective of the attack model is to infer the labels from the records and corresponding outputs .",
    "figure  [ fig : shadows_attack ] illustrates the process of training the attack model .",
    "@xmath43 , we compute the prediction vector @xmath44 and add the record @xmath45 to the attack training set @xmath46 .",
    "let @xmath47 be a set of data records disjoint from the training set of the @xmath4th shadow model .",
    "then , @xmath48 we compute the prediction vector @xmath44 and add the record @xmath49 to the attack training set @xmath46 .",
    "finally , we split @xmath46 into @xmath50 partitions , each associated with a different class label . for each class label @xmath20 , we train a separate model that , given @xmath18 , predicts the @xmath51 or @xmath52 membership status for @xmath19 .    in effect , we convert the problem of recognizing the complex relationship between members of the training dataset and the model s output into a binary classification problem .",
    "binary classification is a standard machine learning task , thus we can use any state - of - the - art machine learning framework or service to build the attack model .",
    "our approach is independent of the specific method used for attack model training .",
    "for example , in section  [ sec : evaluation ] we construct the attack model using neural networks and also using the same black - box google prediction api that we are attacking , in which case we have no control over the model structure , model parameters , or training meta - parametersbut still obtain a working attack model .",
    "we evaluated our attacks on five datasets and several classification tasks .    * _ cifar . _",
    "* cifar-10 and cifar-100 are benchmark datasets used to evaluate image recognition algorithms  @xcite .",
    "cifar-10 is composed of @xmath53 color images in @xmath54 classes , with @xmath55 images per class . in total",
    ", there are @xmath56 training images and @xmath57 test images .",
    "cifar-100 has the same format as cifar-10 , but it has @xmath58 classes containing @xmath59 images each .",
    "there are @xmath60 training images and @xmath58 testing images per class .",
    "we use different fractions of this dataset in our attack experiments , to show the effect of the training dataset size on the performance of the attack .",
    "* _ purchases . _ * the purchase dataset is based on kaggle s `` acquire valued shoppers '' challenge dataset that contains shopping histories for several thousand individuals .",
    "the purpose of the challenge is to design accurate coupon promotion strategies .",
    "each user record contains his or her transactions over a year .",
    "the transactions include many fields such as product name , store chain , quantity , and date of purchase . for our experiments , we derived a simplified purchase dataset , where each record consists of @xmath59 binary features .",
    "each feature corresponds to a product and represents whether the user has purchased it or not .",
    "we construct our classification tasks by clustering the records into multiple classes , each representing a different purchase style . in our experiments , we design @xmath61 different classification tasks with different number of classes @xmath62 . the classification task is to predict the purchase style of a user given the @xmath59-feature vector .",
    "we use @xmath57 randomly selected records from the purchase dataset to train the target model .",
    "the rest of the dataset contributes to the test set and ( if necessary ) the training sets of the shadow models .    *",
    "_ locations . _",
    "* [ locations ] we created a location dataset from the publicly available set of mobile users location `` check - ins '' in the foursquare social network , restricted to the bangkok area and collected from april 2012 to september 2013  @xcite .",
    "the check - in dataset contains @xmath63 users and @xmath64 locations , for a total of @xmath65 check - ins .",
    "we filtered out users with fewer than 25 check - ins and venues with fewer than 100 visits , which left us with @xmath66 user profiles . for each location venue",
    ", we have the geographical position as well as its location type ( e.g. , indian restaurant , fast food , etc . ) .",
    "the total number of location types is @xmath67 .",
    "we partition the bangkok map into areas of size @xmath68 , yielding @xmath69 regions for which we have at least one user check - in .",
    "each record in the resulting dataset has @xmath70 binary features , representing whether the user visited a certain region or location type , i.e. , the semantic and geographical profiles of the user .",
    "the classification task is similar to the purchase dataset .",
    "we cluster the location dataset into @xmath71 classes , each representing a different geosocial type .",
    "the classification task is to find the user s geosocial type given her record .",
    "we use @xmath72 randomly selected records to train the target model .",
    "the rest of the dataset contributes to the test set and the training sets of the shadow models .    *",
    "_ * mnist is a dataset of handwritten digits formatted as @xmath53 images , normalized so that the digits are located at the center of the image .",
    "the dataset is composed of @xmath73 examples  .",
    "we use @xmath57 randomly selected records to train the target model .",
    "* _ adult ( census income ) .",
    "* the uci adult dataset includes @xmath74 records with @xmath75 different attributes such as age , gender , education , marital status , occupation , working hours , native country , etc .",
    "the ( binary ) classification task is to predict whether a person makes over 50k a year based on the census attributes .",
    "we use @xmath57 randomly selected records to train the target model .",
    "we evaluated our inference attacks on three types of target models : two constructed by cloud - based `` machine learning as a service '' platforms and one we implemented locally . in all cases , our attacks treat the models as black boxes . for the cloud services ,",
    "we do not even know the type or structure of the models they create .",
    "we also do not know the values of the hyper - parameters used during the training process .",
    "* _ machine learning as a service .",
    "_ * the first cloud - based machine learning service in our study is google prediction api . with this service ,",
    "the user uploads the dataset and obtains an api for querying the resulting model ; there are no configuration parameters that can be changed by the user .",
    "the other cloud service is amazon ml .",
    "the user can not choose the type of the model but can control a few meta - parameters . in our experiments , we varied the _ maximum number of passes _ over the training data and l2 _ regularization amount_. the former determines the number of training epochs and controls the convergence of model training ; its default value is @xmath54 .",
    "the latter tunes how much regularization is performed on the model parameters in order to avoid overfitting .",
    "we used the platform in two configurations : the default setting ( @xmath54 , @xmath76 ) and ( @xmath58 , @xmath77 ) .    *",
    "_ neural networks .",
    "_ * neural networks have become a very popular approach to large - scale machine learning .",
    "we use torch7 and its nn packages , a popular deep - learning library that has been used and extended by major internet companies such as facebook .    on cifar datasets , we train a standard convolutional neural network ( cnn ) with two convolution and max pooling layers plus a fully connected layer of size @xmath67 and a @xmath78 layer .",
    "we use @xmath79 as the activation function .",
    "we set the learning rate to @xmath80 , the learning rate decay to @xmath81 , and the maximum epochs of training to @xmath58 .    on the purchase dataset ( see section  [ sec : data ] ) , we train a fully connected neural network with one hidden layer of size @xmath67 and a softmax layer .",
    "we use @xmath79 function as the activation function .",
    "we set the learning rate to @xmath80 , the learning rate decay to @xmath81 , and the maximum epochs of training to @xmath82 .",
    "the training set and the test set of each target and shadow model in our experiments are randomly selected from our datasets and have the same size .",
    "the training and test sets of each model are disjoint .",
    "there is no overlap between the datasets of the target model and those of the shadow models , but the datasets used for different shadow models can overlap with each other .",
    "we set the training set size to @xmath57 for the purchase dataset as well as the adult dataset and the mnist dataset , and to @xmath83 for the location dataset .",
    "we vary the size of the training set for the cifar datasets , to measure the difference in the attack s accuracy . for the cifar-10 dataset ,",
    "we choose @xmath84 , @xmath85 , @xmath57 , and @xmath86 . for the cifar-100 dataset , we choose @xmath87 , @xmath88 , @xmath89 , and @xmath90 .",
    "the experiments on the cifar datasets were run locally , against our own models , so we can vary the model s configuration and measure the impact on the attack s accuracy .",
    "the experiments on other datasets ( purchases with @xmath91 classes , locations , adult , and mnist ) were run against models trained using either google or amazon services , where we have zero visibility into their choice of model type and structure and little control over the training process ( see section  [ models ] ) .",
    "the main part of our evaluation focus on the purchase and location datasets . for the purchase dataset",
    ", we built target models on all platforms ( google , amazon , local neural networks ) employing the same training dataset , thus enabling us to compare the leakage from different models .",
    "we used similar training architectures for the attack models across different platforms : either a fully connected neural network with one hidden layer of size 64 with relu ( rectifier linear units ) activation functions and a softmax layer , or a google - trained black - box model .",
    "we set the number of shadow models to @xmath58 for the cifar datasets , @xmath92 for the purchase dataset , @xmath93 for the location dataset , @xmath94 for the mnist dataset , and @xmath92 for the adult dataset .",
    "increasing the number of shadow models would increase the accuracy of the attack but also its cost .",
    "the attacker s goal is to determine whether a given record was part of the target model s training dataset .",
    "we evaluate this attack by executing it on ( randomly reshuffled ) records from the target s training and test datasets . in our attack evaluation ,",
    "we use sets of the same size ( i.e , equal number of members and non - members ) in order to maximize the uncertainty of inference , thus the baseline accuracy is 50% .    we measure the accuracy of the attack using the standard _ precision _ and _ recall _ metrics .",
    "precision measures the fraction of the records inferred as members of the training dataset that are indeed members .",
    "recall measures coverage of the attack , i.e. the fraction of the training records that the attacker can correctly infer as members .",
    "most measurements are reported per class because the accuracy of the attack can vary considerably for different classes .",
    "this is due to the difference in size and composition among classes and highly depends on the dataset .",
    "the test accuracy of our target neural - network models with the largest training datasets ( with @xmath86 and @xmath90 records , respectively ) is @xmath95 and @xmath96 for cifar-10 and cifar-100 , respectively .",
    "the accuracy is low , indicating that the models are heavily overfitted on their training sets .",
    "figure  [ fig : cifar ] shows the results of membership inference attack against the cifar datasets . for both cifar-10 and cifar-100 , the attack performs much better than the baseline , with cifar-100 especially vulnerable .",
    ".accuracy of different ml platforms on purchase dataset ( 100 classes ) . [ cols=\"<,>,>\",options=\"header \" , ]          +      +      +",
    "table  [ table : google ] shows the relationship between attack accuracy and test / train accuracy of the target models .",
    "figure  [ fig : whyitworks ] also illustrates how the target models outputs distinguish members from non - members ( this is the information that our attack exploits ) .",
    "the plots show , for different datasets , how the model s outputs are different for the inputs that were in its training dataset versus the inputs that were not .",
    "specifically , we look at the accuracy of the model in predicting the right label , as well as the prediction uncertainty of the model .",
    "the model accuracy for class @xmath4 is the probability that it classifies an input with label @xmath4 as @xmath4 .",
    "prediction uncertainty is the normalized entropy of the model s prediction vector ( @xmath97 , where @xmath98 is the probability that the input belongs to class @xmath4 , and @xmath99 is the number of classes ) .",
    "the plots show that there is a clear difference between the output ( both accuracy and uncertainty ) of the model on the member inputs versus the non - member inputs , for the cases where our inference attack is successful .",
    "success of membership inference is directly related to ( 1 ) generalizability of the target model , and ( 2 ) diversity of its training data . if the model overfits and does not generalize well to data records beyond its training dataset , or if the training data is not representative , the model leaks information about its training data .",
    "we quantify this relationship in fig .",
    "[ fig : howitworks ] . from the machine learning perspective , overfitting is a well - known problem .",
    "it is harmful because it results in models that lack high predictive power . in this paper , we show another harm of overfitting : the leakage of sensitive information about the training data .    as we explained in section",
    "[ sec : evaluation ] , overfitting is not the only reason why our inference attacks work .",
    "different machine learning models , due to their different structures , `` remember '' different amounts of information about their training datasets .",
    "this leads to different amounts of information leakage even if the models are overfitted to the same degree ( e.g. , see table  [ table : target - accuracy ] ) .",
    "as explained in section  [ sec : whyattackworks ] , multiple factors contribute to machine learning models leaking information about their training datasets .",
    "overfitting is an important ( but not the only ) reason .",
    "of course , overfitting is a canonical problem in machine learning research because it limits the predictive power and generalizability of models .",
    "this means that instead of the usual tradeoff between utility and privacy , machine learning research and privacy research have similar objectives in this case .",
    "regularization techniques , such as dropout  @xcite , can help defeat overfitting .",
    "it has been shown that dropout can also strengthen privacy guarantees in a simple neural network  @xcite .",
    "regularization techniques are also used for objective perturbation in empirical risk minimization in order to develop differentially private machine learning models  @xcite .",
    "( ideal ) well - regularized models should not leak much information about their training data , and our attack can serve as a metric to quantify this . also , models with a trivial structure ( e.g. , xor of some input features ) generalize to the entire universe and do not leak information .    our attacks are less effective if the attacker s access to the full output of the target model is strictly limitedfor example , instead of the confidence scores for all classes , the model reports only the most likely class .",
    "availability of the entire prediction vector highly depends on the needs of the application that uses the model , but in many cases applications need full scores ( e.g. , to classify an input that belongs to several classes ) . in practice , existing machine - learning - as - service platforms return the full vector of precise confidence scores .",
    "if the training process is differentially private  @xcite , the probability of producing a particular model from a training dataset that includes a given record is @xmath100-close to the probability of producing the same model when this record is not included in the training dataset . differentially private models are , by definition , secure against membership inference attacks of the kind developed in this paper , since our attacks operate solely on the outputs of the model , without any auxiliary information .",
    "one obstacle to designing differentially private models is that they may significantly reduce the model s prediction accuracy for small @xmath100 values . in section",
    "[ sec : relatedwork ] , we survey some of the related work in this area .    in the case of machine learning as a service ,",
    "platform operators such as google and amazon have significant responsibility to the users of their services . in their current form",
    ", these services simply accept the data , produce a model of unknown type and structure , and return an opaque api to this model that data owners use as they see fit , without any understanding that by doing so , they may be leaking out their data . learning services do not inform the users about the risks of overfitting or the harm that may result from models trained on inadequate datasets ( for example , with unrepresentative records or too few representatives for certain classes ) .    instead ,",
    "when adaptively choosing a model for a user - supplied dataset , services such as google prediction api and amazon ml should take into account not only the accuracy of the model but also the risk that it will leak information about its training data .",
    "furthermore , they need to explicitly warn users about this risk and provide more visibility into the model and the methods that can be used to reduce the leakage .",
    "our inference attacks can be used as metrics to quantify leakage from a specific model , and also to measure the effectiveness of future privacy protection techniques deployed by machine - learning services .",
    "we have designed , implemented , and evaluated the first membership inference attack against machine learning models , notably black - box models trained in the cloud using google prediction api and amazon ml .",
    "our attack is the first general , _ quantitative _ approach to understanding how machine learning models leak information about their training datasets . when selecting the type of the model to train or a cloud - based learning service to use , our attack can be used as one of the metrics for choosing between models .",
    "our key technical innovation is the shadow training technique that trains an attack model to distinguish the target model s outputs on members vs.  non - members of its training dataset .",
    "we demonstrate that shadow models used in this attack can be effectively created using synthetic or noisy data . in the case of synthetic data generated from the target model itself",
    ", the attack does not require any prior knowledge about the distribution of the target model s training data .    in all of our experiments ,",
    "the classification models we attack are the types of models that many apps use to profile and classify users .",
    "therefore , our results have substantial privacy implications .",
    "we believe that similar attacks can be performed against models trained on even more sensitive ( e.g. , clinical and biomedical ) data , where our techniques can be used to evaluate the privacy risks of machine learning .",
    "big thanks to congzheng song for running model inversion on convolutional neural networks trained on the cifar-10 dataset and producing figure  [ fig : modelinversion ] .",
    "g.  ateniese , l.  v. mancini , a.  spognardi , a.  villani , d.  vitali , and g.  felici , `` hacking smart machines with smarter ones : how to extract meaningful data from machine learning classifiers , '' _ international journal of security and networks _ , vol .",
    "10 , no .  3 , pp .",
    "137150 , 2015 .",
    "m.  barni , p.  failla , r.  lazzeretti , a.  sadeghi , and t.  schneider , `` privacy - preserving ecg classification with branching programs and neural networks , '' _ trans .",
    "forensics and security _ , vol .  6 , no .  2 , pp .",
    "452468 , 2011 .",
    "t.  hastie , r.  tibshirani , j.  friedman , and j.  franklin , `` the elements of statistical learning : data mining , inference and prediction , '' _ the mathematical intelligencer _ ,",
    "27 , no .  2 ,",
    "pp . 8385 , 2005 .",
    "n.  homer , s.  szelinger , m.  redman , d.  duggan , w.  tembe , j.  muehling , j.  v. pearson , d.  a. stephan , s.  f. nelson , and d.  w. craig , `` resolving individuals contributing trace amounts of dna to highly complex mixtures using high - density snp genotyping microarrays , '' _ plos genetics _",
    ", vol .  4 , no .  8 , 2008"
  ],
  "abstract_text": [
    "<S> we investigate how machine learning models leak information about the individual data records on which they were trained . </S>",
    "<S> we focus on the basic membership inference attack : given a data record and black - box access to a model , determine whether the record was in the model s training dataset . to perform membership inference against a target model , </S>",
    "<S> we make adversarial use of machine learning and train our own inference attack model to recognize differences in the target model s predictions on inputs that it trained on versus inputs that it did not use during training .    </S>",
    "<S> we empirically evaluate our inference techniques on classification models trained by commercial `` machine learning as a service '' providers such as google and amazon . using realistic datasets and classification tasks , </S>",
    "<S> we show that these models can be significantly vulnerable to membership inference attacks . </S>"
  ]
}