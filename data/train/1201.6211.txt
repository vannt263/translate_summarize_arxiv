{
  "article_text": [
    "due to the different possible dependence structures that may occur in time series analysis , several bootstrap procedures have been proposed to infer properties of a statistic of interest .",
    "validity of the different bootstrap procedures depends on the probabilistic structure of the underlying stochastic process @xmath0 and/or on the particular statistic considered .",
    "bootstrap schemes for time series rank from those imposing more parametric type assumptions on the underlying stochastic process class to those accounting only for some kind of mixing or weak dependence assumptions . for an overview see @xcite , @xcite , @xcite and @xcite .",
    "a common assumption is that @xmath1 is a _ linear time series _ , that is , that @xmath2 with respect to independent , identically distributed ( i.i.d . )",
    "random variables @xmath3often assumed to have mean zero and finite fourth order moments  and for absolutely summable coefficients @xmath4 ; this is not to be confused with the wold representation with respect to white noise , that is , uncorrelated , errors that all stationary , purely nondeterministic processes possess . if @xmath5 for all @xmath6 , then the linear process is called _",
    "causal_.    stationary autoregressive ( ar ) processes of order @xmath7 are members of the linear class ( [ mainfinity ] ) provided the autoregression is defined on the basis of i.i.d .",
    "model - based bootstrapping in the ar(@xmath7 ) case was among the first bootstrap proposals for time series ; see , for example , @xcite .",
    "the extension to the ar(@xmath8 ) case was inevitable ; this refers to the situation where the strictly stationary process @xmath9 has the following linear infinite order autoregressive representation @xmath10 with respect to i.i.d .",
    "errors @xmath11 having mean zero , variance @xmath12 and @xmath13 ; here the coefficients @xmath14 are assumed absolutely summable and @xmath15 for @xmath16 .",
    "the two representations , ( [ mainfinity ] ) and  ( [ eqarinfty ] ) are related ; in fact , the class  ( [ eqarinfty ] ) is a subset of the linear class  ( [ mainfinity ] ) . furthermore , it can be shown that the linear ar(@xmath8 ) process ( [ eqarinfty ] ) is causal if and only if @xmath17 for latexmath:[$     there is already a large body of literature dealing with applications and properties of the ar - sieve bootstrap . kreiss ( @xcite , @xcite ) established validity of this bootstrap scheme for different statistics including autocovariances and autocorrelations .",
    "@xcite established asymptotic validity of the ar - sieve bootstrap to infer properties of high order autocorrelations , and @xcite established its validity in a multivariate time series context .",
    "the aforementioned results required an exponential decay of the ar coefficients @xmath14 as @xmath19 ; @xcite extended the class of ar(@xmath8 ) processes for which the ar - sieve bootstrap works by allowing a polynomially decay of the @xmath20 coefficients .",
    "furthermore , @xcite introduced a mixing concept appropriate for investigating properties of the ar - sieve bootstrap which is related to the weak dependence concept of @xcite , while @xcite focused on properties of the ar - sieve bootstrap - based confidence intervals .",
    "a basic assumption in the current literature of the ar - sieve bootstrap is that @xmath21 is a linear ar(@xmath8 ) process , that is , @xmath9 is generated by ( [ eqarinfty ] ) with @xmath3 being an i.i.d .",
    "one exception is the case of the sample mean @xmath22 , where @xcite proved validity of the ar - sieve bootstrap also for the case where the assumption of i.i.d .",
    "errors in ( [ eqarinfty ] ) can be relaxed to that of martingale differences , that is , @xmath23 and @xmath24 with @xmath25 the @xmath26-algebra generated by the random variables @xmath27 .",
    "notice that the process ( [ eqarinfty ] ) with innovations forming a martingale difference sequence is in some sense not `` very far '' from the linear process ( [ eqarinfty ] ) with i.i.d . errors .",
    "in fact , some authors call the set - up of model ( [ mainfinity ] ) with martingale difference errors `` weak linearity , '' and the same would hold regarding ( [ eqarinfty ] ) ; see , for example , @xcite .    to elaborate , for a causal linear process the general @xmath28-optimal predictor of @xmath29 based on its past @xmath30 namely the conditional expectation @xmath31 of @xmath32 , is identical to the best _ linear _ predictor  @xmath33 ; here @xmath34 is assumed positive , and @xmath35 denotes orthogonal projection onto the set @xmath36 and @xmath37 , that is , the closed linear span generated by the random variables @xmath38",
    ". the property of linearity of the optimal predictor is shared by causal processes that are only weakly linear .",
    "recently , under the assumption of weak linearity with ( [ eqarinfty ] ) , @xcite claimed validity of the ar - sieve bootstrap for a much wider class of statistics that are defined as smooth functions of means .",
    "however , this claim does not seem to be correct in general .",
    "in particular , our example  [ bootautocov ] of section [ bootfctlinstat ] contradicts theorem 2 of @xcite ; see remark  [ reposkitt ] in what follows .",
    "the aim of the present paper is to explore the limits of the ar - sieve bootstrap , and to give a definitive answer to the question concerning for which classes of statistics , and for which dependence structures , is the ar - sieve bootstrap asymptotically valid .",
    "moreover , we also address the question what the ar - sieve bootstrap really does when it is applied to data stemming from a stationary process not fulfilling strict regularity assumptions such as linearity or weak linearity . in order to do this ,",
    "we examine in detail in section  [ generalar ] processes possessing a so - called general autoregressive representation with respect to white noise errors ; these form a much wider class of processes than the linear ar(@xmath8 ) class described by ( [ eqarinfty ] ) .    our theoretical results in section [ bootfctlinstat ]",
    "provide an effective and simple tool for gauging consistency of the ar - sieve bootstrap .",
    "they imply that for certain classes of statistics the range of the validity of the ar - sieve bootstrap goes far beyond that of the linear class ( [ mainfinity ] ) . on the other hand , for other classes of statistics , like for instance autocorrelations , validity of the ar - sieve bootstrap",
    "is restricted to the linear process class ( [ mainfinity ] ) , while for statistics like autocovariances , the bootstrap is only valid for the linear ar(@xmath8 ) class ( [ eqarinfty ] ) .",
    "but even in the case of the linear autoregression ( [ eqarinfty ] ) with infinite order , the theory developed in this paper provides a further generalization of existing results since it establishes validity of this bootstrap procedure under weaker assumptions on the summability of the coefficients @xmath20 , thus relaxing previous assumptions referring to exponential or polynomial decay of these coefficients .    the remaining of the paper is organized as follows .",
    "section [ generalar ] develops the background concerning the wold - type infinite order ar representation that is required to study the ar - sieve bootstrap , and states the necessary assumptions to be imposed on the underlying process class and on the parameters of the bootstrap procedure .",
    "section [ bootfctlinstat ] presents our main result and discusses its implications by means of several examples . proofs and technical details",
    "are deferred to the .",
    "here , and throughout the paper , we assume that we have observations  @xmath39 stemming from a strictly stationary process @xmath1 . let @xmath40 be an estimator of some unknown parameter @xmath41 of the underlying stochastic process @xmath1 .",
    "suppose that for some appropriately increasing sequence of real numbers @xmath42 the distribution @xmath43 has a nondegenerated limit . the ar - sieve bootstrap proposal to estimate the distribution  @xmath44 goes as follows :    select an order @xmath45 , @xmath46 , and",
    "fit a @xmath7th order autoregressive model to @xmath47 .",
    "denote by @xmath48 , the yule ",
    "walker autoregressive parameter estimators , that is , @xmath49 where for @xmath50 , @xmath51 @xmath52 , @xmath53 and @xmath54 .",
    "let @xmath55 , @xmath56 , be the residuals of the autoregressive fit and denote by @xmath57 the empirical distribution function of the centered residuals @xmath58 , where @xmath59 .",
    "let @xmath60 be a set of observations from the time series @xmath61 where @xmath62 and the @xmath63 s are independent random variables having identical distribution @xmath57 .",
    "let @xmath64 be the same estimator as @xmath65 based on the pseudo - time series @xmath66 and @xmath67 the analogue of @xmath68 associated with the bootstrap process @xmath69 .",
    "the ar - sieve bootstrap approximation of  @xmath70 is then given by @xmath71 .    in the above ( and in what follows ) ,",
    "@xmath72 will denote probability law , expectation , etc . in the bootstrap world",
    "( conditional on the data @xmath73 ) .",
    "note that the use of yule  walker estimators in step 1 is essential and guarantees  among other things  that the complex polynomial @xmath74 has no roots on or within the unit disc @xmath75 , see the discussion before ( [ zeroesahat ] ) , that is , the bootstrap process @xmath76 always is a  stationary and causal autoregressive process .",
    "the question considered in this paper is when can the bootstrap distribution @xmath77 correctly approximate the distribution @xmath78 of interest , and moreover what the ar - sieve bootstrap does if the latter is not the case . to this end , let us first discuss a general autoregressive representation of stationary processes .",
    "recall that by the well - known wold representation , every purely nondeterministic , stationary and zero - mean stochastic process @xmath79 can be expressed as @xmath80 where @xmath81 and @xmath82 is a zero mean , white noise `` innovation '' process with finite variance @xmath83 ; recall that @xmath84 .",
    "less known is that for all purely nondeterministic , stationary and zero - mean time series unique autoregressive coefficients @xmath85 exist that only depend on the autocovariance function of the time series @xmath86 , such that for any @xmath87 , @xmath88 where @xmath89 is stationary and @xmath90 .    under the additional assumption that the coefficients @xmath91 are absolute summable , that is , @xmath92 ,",
    "one then obtains an autoregressive , wold - type representation of the underlying process given by @xmath93 here again @xmath94 denotes a white noise , that is , uncorrelated , process with finite variance @xmath95 which fulfills @xmath96 where @xmath97 denotes the autocovariance function of @xmath98 .    under the absolute summability assumption on the autoregressive coefficients @xmath99conditions for which will be given in lemma [ lemma1 ] in the sequel  we have that @xmath100 ; this implies that the white noise process @xmath101 appearing in ( [ eqwold ] ) coincides with the white noise process @xmath102 in ( [ woldar ] ) .",
    "notice that this does not mean that if we have an arbitrary one sided moving average representation of a time series @xmath86 , even with summable coefficients , that this moving average representation is the wold representation of the process ; see remark [ remark1 ] for an example .",
    "furthermore , let @xmath103 be the spectral density of @xmath1 , that is , @xmath104.\\ ] ] then , from ( [ woldar ] ) one immediately obtains that @xmath105,\\ ] ] which implies that for strictly positive spectral densities @xmath106 the power series @xmath107 has no zeroes with @xmath108 . for more details of the autoregressive wold representation ( [ woldar ] ) see @xcite , lemma  6.4(b ) , ( 6.10 ) and ( 6.12 ) .",
    "it is worth mentioning that in the historical evolution of wold decompositions the autoregressive variant preceded the moving average one .",
    "[ remark1 ] if we consider a purely nondeterministic and stationary time series possessing a standard one - sided moving average representation , and if we additionally assume that the spectral density is bounded away from zero and that the moving average coefficients @xmath109 are absolutely summable , then this would imply that the polynomial @xmath110 has no zeroes with magnitude equal to one .",
    "there may of course exist zeroes within the unit disk .",
    "but since the closed unit disk is compact and @xmath111 represents a holomorphic function there could exist only finitely many zeroes with magnitude less than one . following the technique described in kreiss and neuhaus [ ( @xcite ) , section 7.13 ]",
    "one may switch to another moving average model for which the polynomial has no zeroes within the unit disk .",
    "this procedure definitely changes the white noise process ; for example , if the white noise process in the assumed moving average representation consists of independent random variables , this desirable feature typically is lost when switching to the moving average model with all zeroes within the unit disk removed .",
    "in fact , only the property of uncorrelatedness is preserved .",
    "the modified moving average process allows then for an autoregressive representation of infinite order and this process , because of the uniqueness of the autoregressive representation , coincides with the one in ( [ woldar ] ) .",
    "the following simple example , taken from brockwell and davis [ ( @xcite ) , example 3.5.2 ] illustrates these points .",
    "based on i.i.d .",
    "random variables @xmath3 with mean zero and finite and nonvanishing variance @xmath112 , construct the simple ma(1)-process @xmath113 this ma(1)-model is not invertible to an autoregressive process .",
    "however , a  general autoregressive representation as described above exists . in order to obtain this representation denote by @xmath114 the usual lag - operator and consider @xmath115 as well as @xmath116 .",
    "of course @xmath117 since @xmath118 , we obtain that @xmath119 again @xmath120 is a ( uncorrelated ) white noise process with variance @xmath121 .",
    "moreover , we have @xmath122 obviously @xmath123 which means that ( [ alternativema1 ] ) and not ( [ ma1 ] ) is the wold representation of the time series @xmath86 .",
    "this also means that the modified moving average ( or wold ) representation of the process @xmath86 possesses only uncorrelated innovations @xmath102 instead of independent innovations  @xmath3 .",
    "but the representation ( [ alternativema1 ] ) with uncorrelated innovations has the advantage that it indeed possesses an autoregressive representation of infinite order .",
    "of course , via the described modification , we do not change any property of the process @xmath86 .",
    "but , and this is essential , the modification leading to the general ar(@xmath8)-representation typically destroys a existing independence property of the white noise in a former moving average representation .    to elaborate , the problem of understanding the stochastic properties of the innovation process in linear time series has been thoroughly investigated in the literature .",
    "breidt and davis ( @xcite ) showed that time reversibility of a  linear process is equivalent to the fact that the i.i.d .",
    "innovations @xmath124 are gaussian and used this result to derive for a class of linear processes uniqueness of moving average representations with i.i.d .",
    "non - gaussian innovations and to discuss the stochastic properties of the innovation process appearing in alternative moving average representations for the same process class .",
    "@xcite used such results to initialize autoregressive processes in monte carlo generation of conditional sample paths running autoregressive processes backward in time and @xcite for estimation problems for all - pass time series models .",
    "properties of the innovation process in non - gaussian , noninvertible time series have been also discussed in lii and rosenblatt ( @xcite ) .",
    "as we have seen the variances of @xmath11 and @xmath125 do not coincide and the same is true for the fourth order cumulant @xmath126 which will be of some importance later . using the fact that @xmath125 is defined via a linear transformation on the i.i.d .",
    "sequence @xmath3 we obtain by straightforward computation @xmath127 which only equals @xmath128 in case the latter quantity is equal to @xmath129 , for example , when the @xmath11 are normally distributed .",
    "the normally distributed case always leads to the fact that uncorrelatedness and independence are equivalent , thus implying that the white noise process in the general autoregressive representation always consists of independent and normally distributed random variables which leads for the autoregressive sieve bootstrap in some cases to a considerable simplification as we will see later .    in order to get conditions which ensure the absolute summability of the autoregressive coefficients @xmath130",
    ", one can go back to an important paper by @xcite . informally speaking it is the smoothness of the spectral density @xmath106 which ensures summability of these coefficients . to be more precise",
    ", we have the following result .",
    "[ lemma1 ] if @xmath106 is strictly positive and continuous and if @xmath131 for some @xmath132 , then @xmath133 if @xmath106 is strictly positive and possesses @xmath134 derivatives , then @xmath135    cf .",
    "@xcite , pages 140 and 142 .",
    "the uniquely determined autoregressive coefficients @xmath99 are closely related to the coefficients of an optimal ( in the mean square sense ) autoregressive fit of order  @xmath7 , or equivalently , to prediction coefficients based on the finite past . to be precise , denote the minimizers of @xmath136 by @xmath137 , which of course are solutions of the following yule ",
    "walker linear equations : @xmath138 recall from brockwell and davis [ ( @xcite ) , proposition 5.1.1 ] that the covariance matrix @xmath139 on the left - hand side is for all @xmath7 invertible provided @xmath140 and @xmath141 as @xmath142 .",
    "now by slight modifications of @xcite , theorem 2.2 [ cf .",
    "also pourahmadi ( @xcite ) , theorem 7.22 ] , we obtain the following helpful result relating the coefficients @xmath143 of the @xmath7th order autoregressive fit to the @xmath144 of the general autoregressive representation .",
    "[ lemma2 ] assume that @xmath106 is strictly positive and continuous and that @xmath145 for some @xmath146 . then there exists @xmath147 and @xmath148 ( both depending on @xmath106 only ) such that for all @xmath149 , @xmath150 as well as @xmath151 this means that we typically can achieve a polynomial rate of convergence of @xmath152 toward @xmath153 .",
    "as already mentioned , @xmath140 and @xmath154 as @xmath142 ensure nonsingularity of all autocovariance matrices appearing in the left - hand side of  ( [ arplineareq ] ) .",
    "since these matrices are positive semidefinite this means that under these conditions @xmath139 actually is positive definite .",
    "this in turn with kreiss and neuhaus [ ( @xcite ) , section 8.7 ] implies that the polynomial @xmath155 has no zeroes in the closed unit disk .",
    "we can even prove a slightly stronger result .",
    "[ lemma3 ] assume that @xmath106 is strictly positive and continuous , that @xmath156 and @xmath140",
    ". then there exists @xmath157 and @xmath147 such that for all @xmath149 , @xmath158    the uniform convergence of @xmath159 toward @xmath160 on the closed unit disk immediately implies the following corollary to lemma [ lemma3 ] .",
    "[ corollary1 ] under the assumption of lemma [ lemma3 ] , we have @xmath161    lemma [ lemma3 ] and corollary [ corollary1 ] now enable us to invert the power series  @xmath160 as well as the polynomial @xmath159 .",
    "let us denote @xmath162 and for all @xmath7 large enough ( because of lemma [ lemma3 ] ) @xmath163 from ( [ arpinvert ] ) , one immediately obtains that @xmath164 a further auxiliary result contains the transfer of the approximation property of @xmath165 for @xmath153 to the respective coefficients @xmath166 and @xmath167 of the inverted series .",
    "for such a result , we make use of a weighted version of wiener s lemma ; cf .",
    "@xcite .    [ lemma4 ] under the assumptions of lemma [ lemma3 ] and additionally @xmath168 for some @xmath146 there exists a constant @xmath148 such that for all @xmath7 large enough @xmath169    in a final step of this section , we now move on to estimators of the coefficients @xmath152 .",
    "the easiest one might think of is to replace in ( [ arplineareq ] ) the theoretical autocovariance function by its sample version @xmath170 . denote the resulting yule  walker estimators of @xmath152 by @xmath171 .",
    "these yule ",
    "walker estimators are under the typically satisfied assumption that @xmath172 uniquely determined and moreover fulfill ( by the same arguments already used ) the desired property that @xmath173 thus , we can also invert the polynomial @xmath174 and we denote @xmath175    we require that the estimators @xmath176 converge  even at a  slow rate  to their theoretical counterparts , namely :    ( a1 ) @xmath177 , where @xmath178 denotes a sequence of integers converging to infinity at a rate to be specified .",
    "assumption ( a1 ) , for example , is met if a sufficient fast rate of convergence for the empirical autocovariances toward their theoretical counterparts can be guaranteed .",
    "the convergence property of @xmath179 carries over to the corresponding coefficients @xmath180 of the inverted polynomials [ cf .",
    "( [ arpdachinvert ] ) ] as is specified in the following lemma .",
    "[ lemma5 ] under the assumptions of lemma [ lemma3 ] and , we have uniformly in @xmath181 @xmath182",
    "consider a general class of estimators @xmath183 discussed in @xcite , cf .",
    "example 2.2 ; here @xmath184 and . for this class of statistics",
    ", @xcite proved validity of the ar - sieve bootstrap under the main assumption of an invertible linear process with i.i.d .",
    "innovations for the underlying process @xmath86 ; this means a process which admits an autoregressive representation ( [ eqarinfty ] ) .",
    "the class of statistics given in ( [ kunschstat ] ) is quite rich and contains , for example , versions of sample autocovariances , autocorrelations , partial autocorrelations , yule  walker estimators and the standard sample mean as well .",
    "the necessary smoothness assumptions on the functions @xmath185 and @xmath186 are described below ; these are identical to the ones imposed by @xcite .",
    "( a2 ) @xmath187 has continuous partial derivatives for all @xmath188 in a neighborhood of @xmath189 and the differentials @xmath190 do not vanish .",
    "the function @xmath186 has continuous partial derivatives of order @xmath191 @xmath192 that satisfy a lipschitz condition .",
    "we intend to investigate in this section what the autoregressive sieve bootstrap really mimics if it is applied to statistics of the form ( [ kunschstat ] ) and the observations @xmath73 do _ not _ stem from a linear ar(@xmath8 ) process ( [ eqarinfty ] ) . to be precise , we only assume that we observe @xmath73 from a process satisfying the following assumption ( a3 ) .",
    "( a3 ) @xmath193 is a zero mean , strictly stationary and purely nondeterministic stochastic process .",
    "the autocovariance function @xmath194 satisfies for some @xmath195 specified in the respective results and the spectral density @xmath106 is bounded and strictly positive . furthermore,@xmath196 .",
    "notice that the processes class described by ( a3 ) is large enough and includes several of the commonly used linear and nonlinear time series models including stationary and invertible autoregressive moving - average ( arma ) processes , arch processes , garch processes and so on .",
    "summability of the autocovariance function implies that the spectral density @xmath106 exists , is bounded and continuous",
    ". we also added in ( a3 ) the assumption of finite fourth order moments of the time series .",
    "this assumption seems to be unavoidable due to the autoregressive parameter estimation involved in step  1 of the ar - sieve bootstrap procedure and in regard of assumption ( a1 ) .    from section [ generalar ] ,",
    "we know that if @xmath86 satisfies assumption ( a3 ) then it possesses an autoregressive representation with an uncorrelated white noise process @xmath102 : cf .",
    "( [ woldar ] ) .",
    "because of the strict stationarity of @xmath86 , we have that the time series @xmath102 is strictly stationary as well and thus that the marginal distribution @xmath197 of @xmath198 does not depend on @xmath199 .    theorem [ bootlinstat ]",
    "is the main result of this section . to state it",
    ", we define the _ companion _ autoregressive process @xmath200 where @xmath201 is generated as follows : @xmath202 here @xmath203 consists of i.i.d .",
    "random variables whose marginal distribution of @xmath204 is identical to that of @xmath205 from ( [ woldar ] ) , that is , @xmath206 .",
    "it is worth mentioning that all second order properties of @xmath207 and @xmath86 , like autocovariance function and spectral density , coincide while the probabilistic characteristics beyond second order quantities of both stationary processes are not necessarily the same .",
    "now , let @xmath208 be the same statistic as @xmath209 defined in ( [ kunschstat ] ) but with @xmath210 replaced by @xmath211 , that is , @xmath212    the main message of theorem [ bootlinstat ] is that the ar - sieve bootstrap applied to data @xmath73 in order to approximate the distribution of the statistic  ( [ kunschstat ] ) will generally lead to an asymptotically consistent estimation of the distribution of the statistic @xmath208 .",
    "this implies that for the class of statistics  ( [ kunschstat ] ) , the ar - sieve bootstrap will work _ if and only if _ the limiting distributions of @xmath65 and of @xmath208 are identical .",
    "[ bootlinstat ] assume assumptions ( ) , ( ) , ( ) for @xmath213 and the moment condition @xmath214 [ cf .",
    "( ) for the definition of @xmath191 and ( [ woldar ] ) for the definition of @xmath125 ] , the condition @xmath216 on the order of the approximating autoregression to the data and the following two further assumptions :    the empirical distribution function @xmath217 of the random variables @xmath218 converges weakly to the distribution function @xmath219 of @xmath220 .",
    "the empirical moments @xmath221 converge in probability to @xmath222 for all @xmath223 .    then , @xmath224 as @xmath225",
    ". here @xmath226 , @xmath227 and @xmath228 denotes the kolmogorov distance .",
    "some remarks are in order .",
    "\\(i ) assumption ( a4 ) does imply that we need some conditions on the dependence structure of the random variables @xmath125 .",
    "for instance , a  standard mixing condition on @xmath102 suffices to ensure ( a4 ) : cf .",
    "@xcite , theorem 2.1 .",
    "( ii ) assumption ( a5 ) on the empirical moments is fulfilled if we ensure that sufficiently high empirical moments of the underlying strictly stationary time series @xmath210 itself would converge in probability to their theoretical counterparts .",
    "\\(iii ) as we already pointed out , theorem [ bootlinstat ] states that the ar - sieve bootstrap mimics the behavior of the companion autoregressive process @xmath229 as far as statistics of the form ( [ kunschstat ] ) are considered .",
    "of course if @xmath86 is a linear process with i.i.d . innovations and if the corresponding moving average representation is invertible leading to an infinite order autoregression ( [ eqarinfty ] ) with i.i.d",
    ". innovations , then the ar - sieve bootstrap works asymptotically as is already known .",
    "moreover , for general process satisfying assumption  ( a3 ) , if we are in the advantageous situation that the existing dependence structure of the innovations @xmath102 appearing in the general ar(@xmath8 ) representation  ( [ woldar ] ) does not show up in the limiting distribution of  @xmath65 , then theorem  [ bootlinstat ] implies that the ar sieve bootstrap works .",
    "we will illustrate this point by several examples later on .",
    "now we discuss relevant specializations of theorem [ bootlinstat ] .",
    "notice that the advantage of this theorem is that in order to check validity of the ar - sieve bootstrap , one only needs to check whether the asymptotic distributions of @xmath230 based on the observed time series is identical to the distribution of the statistic @xmath231 based on fictitious observations @xmath232 from the companion process @xmath233 . if and only if this is the case , then the ar - sieve bootstrap works asymptotically .",
    "[ mean ] consider the case of the sample mean @xmath234 and recall that under standard and mild regularity conditions ( e.g. , mixing or weak dependence ) we typically obtain that the sample mean of stationary time series satisfies @xmath235 as @xmath236 where @xmath237 stands for weak convergence .",
    "thus , the asymptotic distribution of the sample mean depends only on the second order properties of the underlying process @xmath98 and since the companion process @xmath233 has the same second order properties as @xmath98 we immediately get by theorem [ bootlinstat ] that the ar - sieve bootstrap asymptotically works in the case of the mean for general stationary time series for which the spectral density is strictly positive . even the strict stationarity is not necessary in this case .",
    "this is a  novel and significant extension of the results of @xcite .",
    "[ bootautocov ] for @xmath238 , let @xmath239 be the sample autocovariance at lag @xmath240 .",
    "let us assume that @xmath241 holds and denote by @xmath242 the fourth order cumulant spectrum of @xmath21 . under standard and mild regularity conditions [ see , for instance , dahlhaus ( @xcite ) , theorem 2.1 and taniguchi and kakizawa ( @xcite ) , chapter 6.1 ] , it is known that @xmath243 where @xmath244 notice that in contrast to the case of the sample mean , the limiting distribution of the sample autocovariance depends also on the fourth order moment structure of the underlying process @xmath1 .",
    "now , to check validity of the ar - sieve bootstrap we have to derive the asymptotic distribution of the sample autocovariances for the companion autoregressive process @xmath229 .",
    "this can be easily done , since the autoregressive polynomial in the general autoregressive representation ( [ woldar ] ) is always invertible ( cf .",
    "corollary [ corollary1 ] ) from which we immediately get a one - sided moving average representation with i.i.d .",
    "innovations @xmath203 for the companion process @xmath229 .",
    "furthermore , the fourth order cumulant spectrum of @xmath229 is given by @xmath245 where @xmath246 and the coefficients @xmath247 are those appearing in ( [ arinvert ] ) ; see section [ generalar ] .",
    "thus , for the sample autocovariance @xmath248 we get from brockwell and davis [ ( @xcite ) , proposition 7.3.1 ] , that @xmath249 where @xmath250\\\\[-8pt ] & = & \\biggl ( \\frac{e { \\varepsilon}_1 ^ 4}{(e { \\varepsilon}_1 ^ 2)^2}-3\\biggr ) ( \\gamma(h))^2\\nonumber\\\\ & & { } + \\sum_{k=-\\infty}^{\\infty } \\bigl(\\gamma(k)^2 + \\gamma ( k+h ) \\gamma(k - h)\\bigr).\\nonumber\\end{aligned}\\ ] ] since the variances @xmath251 and @xmath252 of the asymptotic distributions of @xmath65 and @xmath208 do not coincide in general , we conclude by theorem [ bootlinstat ] that the ar - sieve bootstrap fails for sample autocovariances . notice that this failure is due to the fact that in general the limiting distribution of sample autocovariances depends additionally on the fourth order moment structure @xmath253 of the underlying process @xmath254 , and this structure may substantially differ from that of the companion process @xmath255 .",
    "interestingly enough , even if the underlying process @xmath21 is a linear time series , that is , satisfies ( [ mainfinity ] ) , the ar - sieve bootstrap may fail for the sample autocovariances . to see why , note that from the aforementioned proposition of @xcite , the asymptotic distribution of @xmath209 satisfies @xmath256 where @xmath257 is given by @xmath258 special attention is now due to the factor of the first summand of ( [ varianceofvariance ] ) , which is the fourth order cumulant of the i.i.d .",
    "process @xmath3 .",
    "recall the asymptotic distribution of the sample autocovariances for the companion autoregressive process @xmath229 and especially its variance given in ( [ varianceofvariancetilde ] ) .",
    "the two asymptotic variances given in ( [ varianceofvariance ] ) and ( [ varianceofvariancetilde ] ) are in general not the same since the fourth order cumulant of the two innovation processes @xmath259 and @xmath260 are not necessarily the same .",
    "we refer to remark  [ remark1 ] for an example .",
    "of course , all appearing autocovariances are identical since we do not change the second order properties of the process when switching from @xmath86 to @xmath229 .",
    "the consequence is that for sample autocovariances , the ar - sieve bootstrap generally does not work even for linear processes of the type ( [ mainfinity ] ) and this is true even if the process is causal ; see remark [ remark1 ] and example ( [ ma1 ] ) .",
    "[ reposkitt ] if the innovations @xmath261 in ( [ mainfinity ] ) are not necessarily i.i.d . but",
    "form a martingale difference sequence , then we are in the above described situation more general than in the case of ( [ mainfinity ] ) with i.i.d . innovations and thus the limiting distribution of sample autocovariances and also of statistics of the type ( [ kunschstat ] ) , is not correctly mimicked by the ar - sieve bootstrap .",
    "this contradicts theorem 2 of @xcite .",
    "we conclude this example by mentioning that in the case where the i.i.d .",
    "innovations @xmath11 in ( [ mainfinity ] ) are normally distributed , it follows that the random variables @xmath125 are gaussian as well , and that in both expressions ( [ varianceofvariance ] ) and  ( [ varianceofvariancetilde ] ) the fourth order cumulants appearing as factors in the first summands vanish .",
    "this means that for the special case of gaussian time series fulfilling assumption ( [ mainfinity ] ) the ar - sieve bootstrap works .",
    "[ bootautocorr ] consider the estimator @xmath262 of the autocorrelation @xmath263 . due to the fact that for general processes satisfying assumption ( a3 ) the limiting distribution of @xmath209 depends also on the fourth order moment structure of the underlying process  @xmath98 , cf .",
    "theorem 3.1 of @xcite , which is not mimicked correctly by the companion process @xmath264 , the ar - sieve bootstrap fails .",
    "fortunately the situation for autocorrelations is much better if we switch to linear processes of type ( [ mainfinity ] ) . from theorem 7.2.1 of @xcite",
    ", we obtain that @xmath265 where the asymptotic variance is given by bartlett s formula : @xmath266 as can be seen , in this case the asymptotic variance depends only on the autocorrelation function @xmath267 [ or equivalently on the standardized spectral density @xmath268 of the underlying process @xmath1 .",
    "this means that the first summand in ( [ varianceofvariance ] ) which refers to the fourth order cumulant of the i.i.d .",
    "innovation process @xmath269 in ( [ mainfinity ] ) does not show up in the limiting distribution of sample autocorrelations and this in turn leads to the fact that the asymptotic distribution of sample autocorrelations based on observations stemming from the process @xmath21 is identical to that of the sample autocorrelation based on observations stemming from the companion autoregressive process @xmath255 .",
    "this is true since the companion autoregressive process @xmath255 shares all second order properties of the process @xmath21 .",
    "hence , the ar - sieve bootstrap works for the autocorrelations given data from a linear process ( [ mainfinity ] ) .",
    "we stress here the fact that this result is true regardless whether the representation ( [ mainfinity ] ) allows for an autoregressive inversion or not and holds even though the probabilistic properties of the underlying process @xmath86 and of the autoregressive companion process @xmath229 beyond second order properties are not the same .",
    "[ othestat ] similar to the autocorrelation case , the validity of the ar - sieve bootstrap in the linear class ( [ mainfinity ] ) is shared by many different statistics whose large - sample distribution depends only on the ( first and ) second order moment structure of the underlying process .",
    "examples include the partial autocorrelations or yule  walker estimators of autoregressive coefficients .",
    "[ runningmean ] consider statistics of the type ( [ kunschstat ] ) in the easiest case where @xmath270 , that is , @xmath271 for a function @xmath272 . here",
    ", the practitioner may approach this as the sample mean of observations @xmath273 where @xmath274 .",
    "notice that strict stationarity as well as mixing properties easily carries over from @xmath86 to @xmath275 .",
    "thus , we may apply the ar - sieve bootstrap to the sample mean of @xmath276 , which works under quite general assumptions ; cf .",
    "remark  [ mean ] and theorem  [ bootlinstat ] .",
    "the only crucial assumption for establishing asymptotic consistency of the ar - sieve bootstrap is that we need the property that the spectral density @xmath277 of the transformed time series @xmath275 is strictly positive and continuous .",
    "although this is not a very restrictive condition , it may be difficult to check since there seems to be no general result describing the behavior of spectral densities under nonlinear transformations .",
    "the considerations of examples [ bootautocov ] and  [ bootautocorr ] can be transferred to integrated periodogram estimators for these quantities which lead us to the second large class of statistics that we will discuss .",
    "denote , based on observations @xmath73 , the periodogram @xmath278 defined  by @xmath279,\\ ] ] and consider a general class of integrated periodogram estimators defined  by @xmath280 where @xmath281 denotes an appropriately defined function on @xmath282 $ ] . under the main assumption that the underlying process @xmath1 has the representation ( [ mainfinity ] ) , @xcite investigated asymptotic properties of ( [ integratedperiodogram ] ) and obtained the asymptotic distribution of @xmath283 .",
    "in particular , it has been shown that @xmath284 converges , as @xmath285 , to a  gaussian distribution with zero mean and variance given by @xmath286 notice that substituting @xmath287 by @xmath288 , implies that @xmath289 equals the sample autocovariance of the observations @xmath73 at lag @xmath191 and ( [ varianceintperiodo ] ) would then exactly turn to be the asymptotic variance given in  ( [ varianceofvariance ] ) .    as we will see in theorem [ intperiodoclt",
    "] , the situation for integrated periodograms  ( [ integratedperiodogram ] )  is rather similar to that of empirical autocovariances which are of course special cases of integrated periodograms .",
    "thus , we only discuss briefly this rather relevant class of statistics .",
    "as theorem [ intperiodoclt ] shows , we obtain for this class that the ar - sieve bootstrap asymptotically mimics the behavior of @xmath290 , where @xmath291 is defined as @xmath292 with  @xmath210 replaced by the companion autoregressive time series @xmath211 , that is , @xmath293 .\\ ] ]    [ intperiodoclt ] assume and with @xmath213 and assume that for all @xmath294 @xmath295 ( in probability ) , where @xmath296 denote the empirical autocovariances of the bootstrap observations @xmath297 and where @xmath298\\\\[-8pt ] & & \\hspace*{4.7pt}{}+\\sum_{k=-\\infty}^{\\infty } \\bigl(\\gamma(k ) \\gamma(k - i+j)+\\gamma(k+j)\\gamma(k - i)\\bigr ) \\biggr ] _ { i ,",
    "j=0}^m .\\nonumber\\end{aligned}\\ ] ] then we obtain for all @xmath281 bounded and with bounded variation that ( in probability ) @xmath299\\\\[-8pt ] & & \\qquad\\hspace*{10pt } \\mathcal{l } \\bigl ( \\sqrt{n}\\bigl ( m(\\widetilde i_n,\\varphi)-m(f_x,\\varphi ) \\bigr ) \\bigr)\\bigr ) \\to0 .\\nonumber\\end{aligned}\\ ] ] here @xmath300 $ ] and @xmath301 , cf . step 2 in the definition of the ar - sieve bootstrap procedure .",
    "moreover , the limiting gaussian distribution of @xmath302 possesses the following variance : @xmath303    [ remarkintperiodoclt ] ( i ) assumptions under which ( [ gammaclt ] ) is fulfilled are given in theorem [ bootlinstat ] since sample autocovariances belong to the class ( [ kunschstat ] ) .",
    "\\(ii ) theorem [ intperiodoclt ] implies that if @xmath304 then the ar - sieve bootstrap asymptotically works for the integrated periodogram statistics @xmath289 for time series fulfilling ( [ mainfinity ] ) .",
    "this follows immediately by a comparison of the asymptotic variances ( [ varianceintperiodo ] ) and ( [ varperiodo ] ) .",
    "clearly , the same result holds true if the underlying time series is normally distributed since in this case both innovation processes , @xmath260 and @xmath259 , are gaussian and therefore the fourth order cumulants vanish . in all other cases",
    "the ar - sieve bootstrap does not work in general , since the fourth order cumulant @xmath305 does not necessarily coincide with @xmath306 ; see example [ bootautocov ]",
    ".    relevant statistics for which we can take advantage of the condition @xmath307 are the so - called ratio statistics which are defined by @xmath308 for this class of statistics , @xcite showed that under the same assumptions of a linear process of type ( [ mainfinity ] ) one obtains that @xmath309 has a gaussian limiting distribution with mean zero and variance given by @xmath310\\\\[-8pt ] & & \\eqntext{\\mbox{where } \\displaystyle \\psi(\\lambda ) = \\varphi(\\lambda ) \\int f_x(\\lambda ) \\,d\\lambda -\\int\\varphi(\\lambda ) f_x(\\lambda ) \\,d\\lambda.}\\end{aligned}\\ ] ] thus , exactly as in the case of sample autocorrelations the fourth order cumulant term [ cf . ( [ varianceintperiodo ] ) ] of the i.i.d .",
    "innovation process disappears and therefore again the following corollary to theorem [ intperiodoclt ] is true .",
    "this corollary states that the ar - sieve bootstrap works for ratio statistics under the quite general assumption that the underlying process is a linear time series ( [ mainfinity ] ) with i.i.d .",
    "innovations and a strictly positive spectral density which under model ( [ mainfinity ] ) is always continuous .",
    "[ intperiodocltcor ] under the assumptions of theorem [ intperiodoclt ] , we have that ( in probability ) @xmath311 moreover , the limiting gaussian distribution of @xmath312 possesses the variance given in ( [ varianceratio ] ) .",
    "another class of integrated periodogram estimators is that of nonparametric estimators of the spectral density @xmath106 which are obtained from ( [ integratedperiodogram ] ) if we allow for the function @xmath313 to depend on @xmath314 . in particular , let @xmath315 for some @xmath316 $ ] where @xmath317 is a sequence of positive numbers ( bandwidths ) approaching zero as @xmath318 , @xmath319 and @xmath320 is a kernel function satisfying the following assumption :    ( a6 ) @xmath320 is a nonnegative kernel function with compact support @xmath321 $ ] .",
    "the fourier transform @xmath34 of @xmath320 is assumed to be a symmetric , continuous and bounded function satisfying @xmath322 and @xmath323 .",
    "denote by @xmath324 be the resulting integrated periodogram estimator , that is , @xmath325 notice that the asymptotic properties of the estimator ( [ specdenest ] ) of the spectral density are identical to those of its discretized version @xmath326 , where @xmath327 are the fourier frequencies , as well as of so - called lag - window estimators ; cf .",
    "@xcite .    now , let @xmath328 be the same estimator as ( [ specdenest ] ) based on the ar - sieve bootstrap periodogram @xmath329 .",
    "we then obtain the following theorem .",
    "[ intperiodocltspec ] under the assumptions of theorem [ intperiodoclt ] with @xmath330 and assumption , we have that ( in probability ) @xmath331 moreover , conditionally on @xmath332 , @xmath333\\\\[-8pt ] & & \\qquad \\rightarrow\\cases { 0 , & \\quad if $ n^{-1/5}h \\to0 $ , \\vspace*{2pt}\\cr \\displaystyle\\frac{1}{4\\pi } f_x^{\\prime\\prime}(\\lambda ) \\int u^2 k(u)\\,du , & \\quad if $ n^{-1/5}h \\to1$,}\\nonumber \\ ] ] where @xmath334 denotes the second derivative of @xmath106 and @xmath335 where @xmath336 if @xmath337 or @xmath338 and @xmath339 otherwise .",
    "recall that under assumption ( a3 ) it has been shown under different regularity conditions [ see @xcite ] that @xmath340 converges to a gaussian distribution with mean and variance given by the expression on the right - hand side of ( [ biassd ] ) and ( [ varsd ] ) , respectively .",
    "thus , the above theorem implies that for spectral density estimators like ( [ specdenest ] ) , the ar - sieve bootstrap asymptotically is valid for a very broad class of stationary time series that goes far beyond the linear processes class ( [ mainfinity ] ) .",
    "corollary [ intperiodocltcor ] and theorem [ intperiodocltspec ] highlight an interesting relation between frequency domain bootstrap procedures , like for instance those proposed by @xcite and @xcite and the ar - sieve bootstrap .",
    "notice that the basic assumptions imposed on the underlying process @xmath21 for such a frequency domain bootstrap procedure to be valid are that the underlying process satisfies ( [ mainfinity ] ) with a strictly positive spectral density @xmath106 .",
    "furthermore , validity of such a frequency domain procedure has been established only for those statistics for which their limiting distribution does not depend on the fourth order moment structure of the innovation process @xmath269 in ( [ mainfinity ] ) .",
    "thus , such a frequency domain bootstrap essentially works for statistics like ratio statistics [ ratio ] or nonparametric estimators of the spectral density like ( [ specdenest ] ) .",
    "the results of this section , that is , theorem  [ intperiodoclt ] , corollary [ intperiodocltcor ] and theorem [ intperiodocltspec ] , imply that if the underlying stationary process satisfies ( [ mainfinity ] ) and if the spectral density is strictly positive then the ar - sieve bootstrap works in the same cases in which the frequency domain bootstrap procedures work .",
    "in this paper , we have investigated the range of validity of the ar - sieve bootstrap . based on a quite general wold - type autoregressive representation",
    ", we provided a simple and effective tool for verifying whether or not the ar - sieve bootstrap asymptotically works .",
    "the central question is to what extent the complex dependence structure of the underlying stochastic process shows up in the ( asymptotic ) distribution of the relevant statistical quantities .",
    "if the asymptotic behavior of the statistic of interest based on our data series is identical to that of the same statistic based on data generated from the companion autoregressive process , then the ar - sieve bootstrap leads to asymptotically correct results .    the family of estimators that have been considered ranges from simple arithmetic means and sample autocorrelations to quite general sample means of functions of the observations as well as spectral density estimators and integrated periodograms .",
    "our concrete findings concerning validity of the ar - sieve bootstrap are different for different statistics . generally speaking ,",
    "if the asymptotic distribution of a relevant statistic is determined solely by the first and second order moment structure , then the ar - sieve bootstrap is expected to work .",
    "thus , validity of the ar - sieve bootstrap does not require that the underlying stationary process obeys a linear ar(@xmath8 ) representation ( with i.i.d . or martingale difference errors ) as was previously thought .",
    "indeed , for many statistics of interest , the range of the validity of the ar - sieve bootstrap goes far beyond this subclass of linear processes .",
    "in contrast , we point out the possibility that the ar - sieve bootstrap may fail even though the data series _ is _ linear ; a prominent example is the sample autocovariance in the case of the data arising from a noncausal ar(@xmath7 ) or a  noninvertible  ma(@xmath341 ) model .",
    "finally , our results bear out an interesting analogy between frequency domain bootstrap methods and the ar - sieve method . in the past",
    ", both of these methodologies have been thought to work only in the linear time series setting .",
    "nevertheless , we have just shown the validity of the ar - sieve bootstrap for many statistics of interest without the assumption of linearity , for example , under the general assumption ( a3 ) and some extra conditions . in recent literature",
    ", some examples have been found where the frequency domain bootstrap also works without the assumption of a linear process ; see , for example , the case of spectral density estimators studied by @xcite .",
    "by analogy to the ar - sieve results of the paper , it can be conjectured that frequency domain bootstrap methods might also be valid without the linearity assumption as long as the statistic in question has a  large - sample distribution depending only on first and second order moment properties ; cf .",
    "@xcite for some results in that direction .",
    "proof of lemma [ lemma2 ] from baxter [ ( @xcite ) , theorem 2.2 ] in a slightly more general version given in baxter [ ( @xcite ) , theorem 1.1 ] , we obtain for arbitrary submultiplicative weight or norm functions @xmath342 , that is , @xmath343 for all @xmath344 , that the following bound holds true for all @xmath345 and a constant @xmath148 @xmath346 here , @xmath347 for all @xmath345 and @xmath348 [ cf .",
    "( [ sigmasquare ] ) ] as @xmath349 .    since @xmath350 is submultiplicative for all @xmath146 , cf .",
    "grchenig [ ( @xcite ) , lemma 2.1 ] , we obtain from ( [ bax1 ] ) @xmath351\\\\[-8pt ] & & \\qquad\\quad{}+ \\sum_{k=0}^{p } ( 1+k)^r \\vert a_k \\vert \\biggl\\vert\\frac{1}{\\sigma^2_{\\varepsilon } } - \\frac{1}{\\sigma^2(p ) } \\biggr\\vert\\cdot\\sigma^2(p ) \\nonumber\\\\ & & \\qquad\\le \\frac{c \\sigma^2(0)}{\\sigma^2_{\\varepsilon } } \\biggl ( 1 + \\sum _ { k=0}^{\\infty } ( 1+k)^r \\vert a_k",
    "\\vert\\biggr ) \\cdot\\sum_{k = p+1}^{\\infty } ( 1+k)^r \\vert a_k \\vert , \\nonumber\\end{aligned}\\ ] ] which is the assertion of lemma [ lemma2 ] . to see the last bound ,",
    "observe that because of @xmath352 we can bound @xmath353 by the right - hand side of ( [ bax1 ] ) as well .",
    "proof of lemma [ lemma3 ] as mentioned just in front of the statement of lemma  [ lemma3 ] , we have @xmath155 for all @xmath354 .",
    "now assume that ( [ arpzeroes ] ) is false .",
    "then there exists a sequence @xmath355 , @xmath356 and a sequence @xmath357 of complex numbers with @xmath358 such that @xmath359 let us further assume that we can find a subsequence of @xmath360 which completely stays within the closed unit disk . without loss of generality",
    ", assume that @xmath361 itself has this property . since we have and",
    "because @xmath362 is holomorphic , the minimum principle of holomorphic functions leads to @xmath363 is continuous , thus there exists a @xmath364 with @xmath365 and @xmath366 .    without loss of generality ,",
    "assume that @xmath364 converges to a complex number  @xmath367 with @xmath368 . from the above",
    ", we have @xmath369 writing @xmath370 and having in mind that @xmath159 converges to @xmath160 uniformly on the closed unit disk because of lemma [ lemma2 ] and regarding ( [ apconv ] ) as well as the continuity of @xmath160 we finally obtain @xmath371 which is a contradiction to @xmath372 for all @xmath373 [ cf .",
    "below ( [ azeroesfirst ] ) ] .    since we can not find a subsequence of @xmath374 , completely staying in the unit disk",
    "it exists a subsequence @xmath375 that completely stays in the region @xmath376 . again",
    "assume without loss of generality that @xmath377 and that @xmath374 converges to some @xmath378 which necessarily must fulfill @xmath379 .",
    "we will show that @xmath380 holds , which again is a contradiction to @xmath381 for all @xmath373 . to this end , let us write @xmath382 in the following way : @xmath383\\\\[-8pt ] & & { } + \\sum_{j=1}^{p(k ) } a_j(z_k^j - z_o^j ) - \\sum_{j = p(k)+1}^{\\infty } a_j z_o^j .\\nonumber\\end{aligned}\\ ] ] the first summand on the right - hand side converges to zero by ( [ apk ] ) and the last summand is bounded through @xmath384 as @xmath385 .",
    "the second summand in turn is bounded by @xmath386 for the third and last summand , which reads @xmath387 one obtains by dominated convergence [ recall that @xmath388 for @xmath389 is bounded by @xmath390 and that @xmath391 also convergence to zero .",
    "this concludes the proof of lemma [ lemma3 ] .",
    "proof of lemma [ lemma4 ] under the assumptions the autoregressive coefficients , @xmath153 have the following property : @xmath392\\\\[-8pt ] & : = & \\biggl\\ { ( z_j\\dvtx j\\in \\mathbb{n}_0)\\subset\\mathbb{c } \\big| \\sum_{j=0}^{\\infty } ( 1+j)^r |z_j|",
    "< \\infty\\biggr\\ } .\\nonumber\\end{aligned}\\ ] ] because of @xmath393 @xmath394 ( cf .",
    "corollary [ corollary1 ] ) we have from grchenig [ ( @xcite ) , theorem 6.2 ] that a multiplicative inverse @xmath395 exists . for this result , observe that our weight function @xmath350 satisfies the so - called gelfand ",
    "raikov  shilov ( grs ) condition @xmath396 cf .",
    "@xcite , lemma 2.1 .",
    "since multiplication here is the usual convolution of sequences , we have that @xmath397 , where the coefficients @xmath167 coincide with the power series coefficients of @xmath398 . the assertion @xmath399",
    "then just means that we have for the coefficients @xmath167 in ( [ arpinvert ] ) @xmath400 exactly along the same lines , we obtain [ cf .",
    "( [ arpinvert ] ) ] @xmath401\\\\[-8pt ] & = & ( 1,\\alpha_1(p),\\alpha_2(p),\\ldots ) \\in\\ell_1^v . \\nonumber\\end{aligned}\\ ] ] we have @xmath402 in order to obtain from lemma [ lemma2 ] the desired assertion .",
    "proof of lemma [ lemma5 ] for simplicity , we write @xmath7 instead of @xmath178 . from lemma [ lemma3 ]",
    ", we have that the polynomial @xmath159 has no zeroes with magnitude less than or equal to @xmath403 .",
    "since we easily get from assumption  ( b ) convergence of @xmath404 to @xmath159 uniformly on the closed disk with radius @xmath405 the polynomial @xmath174 does not possess zeroes with magnitude less than or equal to @xmath405 . therefore",
    ", cauchy s inequality for holomorphic functions applies and yields @xmath406    proof of theorem [ bootlinstat ] a careful inspection of the proof of theorem 3.3 in @xcite [ see also the corresponding technical report @xcite ] shows that only the following properties of the underlying , the companion and the fitted autoregressive process really are needed :    @xmath407 in probability ,    @xmath408 in probability ,    @xmath409 in probability ,    @xmath410 is uniformly bounded in probability ,    @xmath411 ,    the empirical moments of @xmath412 converge for orders up to @xmath413 to the moments of @xmath414 ,    the autoregressive representation of infinite order of the process  @xmath229 is invertible ,    yule  walker parameter estimators are used for the autoregressive fit of order @xmath178 to the data @xmath73 .",
    "because of ( a4 ) and ( a5 ) and the easily obtained fact that for the mallows metric  @xmath415 @xmath416 where @xmath417 denotes the empirical distribution function of the centered residuals @xmath412 , @xmath418 of the autoregressive fit and @xmath217 denotes the empirical distribution function of fictitious observations @xmath419 , we obtain ( i ) .",
    "\\(ii ) is obtained exactly along the lines as in corollary 5.6 of @xcite .    to see ( iii ) ,",
    "recall from section [ generalar ] that we have @xmath420 as well as @xmath421 which converges to @xmath129 as @xmath314 goes to infinity .",
    "these two assertions ensure ( iii ) .",
    "\\(iv ) is obtained exactly along these lines by using the fact that @xmath422 , cf .",
    "( 2.29 ) , which also is ( v ) .",
    "furthermore , it is easy to see that the difference of the empirical moments ( up to the necessary order ) of @xmath412 and of @xmath125 converge to zero due to the bounds for @xmath423 , @xmath424 and @xmath425 , cf .",
    "( [ alphapdach ] ) , ( [ alphap ] ) and ( [ alpha ] ) .",
    "together with ( a5 ) , we obtain ( vi ) .",
    "finally , we use for the autoregressive fit yule ",
    "walker parameter estimators and the autoregressive representation of @xmath229 is invertible ( cf . section  [ generalar ] ) .",
    "this concludes the proof of theorem [ bootlinstat ] .",
    "proof of theorem [ intperiodoclt ] also due to the results of @xcite , we obtain that the distribution of @xmath426 , where @xmath291 denotes the periodogram of @xmath314 observations of the autoregressive companion process @xmath229 , cf .",
    "( [ arcompanion ] ) , asymptotically is normal with mean zero and variance @xmath427 thus , it suffices to show that the distribution of the bootstrap approximation of @xmath428 shares the same asymptotic distribution . exactly along the lines of proof of theorem 4.1 in @xcite ( without the additional nonparametric correction considered therein ) which makes use of proposition 6.3.9 of @xcite , we obtain the the desired result .",
    "proof of theorem [ intperiodocltspec ] let @xmath429 and consider @xmath430 . since @xmath431 converges to a gaussian distribution with mean and variance as in ( [ biassd ] ) and ( [ varsd ] ) , respectively , it suffices to show that @xmath432 shares exactly the same asymptotic behavior as @xmath433 .",
    "this however , follows exactly along the same lines as in the proof of theorem 5.1 in @xcite , again without the additional nonparametric correction considered therein .",
    "the authors would like to express their gratitude to two anonymous referees .",
    "their careful reading of a previous version of the paper and their thorough reports led to a considerable improvement of the present paper ."
  ],
  "abstract_text": [
    "<S> we explore the limits of the autoregressive ( ar ) sieve bootstrap , and show that its applicability extends well beyond the realm of linear time series as has been previously thought . </S>",
    "<S> in particular , for appropriate statistics , the ar - sieve bootstrap is valid for stationary processes possessing a general wold - type autoregressive representation with respect to a white noise ; in essence , this includes all stationary , purely nondeterministic processes , whose spectral density is everywhere positive . </S>",
    "<S> our main theorem provides a simple and effective tool in assessing whether the ar - sieve bootstrap is asymptotically valid in any given situation . in effect , the large - sample distribution of the statistic in question must only depend on the first and second order moments of the process ; prominent examples include the sample mean and the spectral density . as a counterexample , we show how the ar - sieve bootstrap is not always valid for the sample autocovariance even when the underlying process is linear .    ,    and    .    </S>"
  ]
}