{
  "article_text": [
    "since hybrid hidden markov model ( hmm)-deep neural network ( dnn ) was introduced to large - vocabulary continuous speech recognition ( lvcsr ) , the accuracy of speech recognition system has made significant performance improvements in idealized environments @xcite@xcite .",
    "such progression urges the development of speech recognition systems that are robust to background noise and channel distortion , as more and more speech applications are deploying on mobile devices .",
    "state - of - the - art robust automatic speech recognition system usually involves intensive specialized domain knowledge @xcite .",
    "but we are more interested in the transformation of feature .",
    "feature learning ( representation learning ) @xcite is a developing field that grows alongside with deep learning .",
    "the aim of feature learning is to learn a certain kind of transformation through which we are able to extract information that makes discrimination much easier for classifiers .",
    "feature learning needs as little feature engineering as possible , and transformed feature is much closer to real underlying factors that generate the original features that we observe .    in this paper",
    ", we made our first attempt to apply the idea of feature learning to robust speech recognition .",
    "although dozens of alternatives have been proposed over the past few decades , mfcc is still the default choice of feature for many speech applications . instead of trying to propose another alternative , we are more interested in learning a better representation of mfcc feature with restricted boltzmann machine ( rbm ) and its variants .",
    "we will explain why rbm may be able to learn a more suitable representation of mfcc for robust speech recognition .",
    "we will also be proposing a new variant of rbm called multivariate gaussian restricted boltzmann machine ( mgrbm ) which is specially designed for modeling the distribution of speech data .",
    "mgrbm is able to capture the evolving characteristic of speech within a context of several frames which is difficult to model with a gaussian restricted boltzmann machine ( grbm ) .",
    "we perform our experiments on the aurora2 corpus and our results show that the learned features are better than the original feature for robust speech recognition .",
    "the boltzman machine is a special kind of markov random field which models the joint probability distribution of the visible variable and hidden variable .",
    "visible and hidden variable are both defined to be multidimensional bernoulli variables .",
    "the distribution can be written as : @xmath0 and @xmath1 @xmath2 in ( [ eq : rbm ] ) is called energy function . @xmath3 .",
    "@xmath4 models the visible - visible , hidden - hidden , and visible - hidden interaction respectively . @xmath5 and @xmath6 are bias vectors .    the restricted boltzmann machine ( rbm ) @xcite is perhaps the most widely - used variant of boltzmann machine .",
    "the energy function of rbm is the simplified version of that in the boltzmann machine by making @xmath7 and @xmath8 .",
    "that is , the energy function of an rbm is : @xmath9 an rbm is typically trained with maximum likelihood estimation . taking the derivative with respect to the logarithm of the product of all the probability of training cases , we can derive the learning algorithm of rbm as follows : @xmath10 the symbol @xmath11 in ( [ eq : rbmw])([eq : rbma])([eq : rbmb ] ) represents an average with respect to the conditional distribution @xmath12 and @xmath13 represents an average with respect to the joint distribution @xmath14 .",
    "the @xmath11 for rbm is generally easy to train because : @xmath15 ( [ eq : rbmeq1])([eq : rbmeq2 ] ) can be derived from the definition of rbm and @xmath16 .",
    "however , @xmath13 is much harder to obtain . to address this problem ,",
    "@xmath13 is usually approximated with @xmath17 as the following : @xmath18 the @xmath17 represents an average with respect to the reconstruction of the visible data .",
    "the reconstruction of visible data is obtained by setting each node in hidden layer value 1 with probability ( [ eq : rbmeq1 ] ) , followed by setting each node in visible layer value 1 with probability ( [ eq : rbmeq2 ] ) .",
    "this is the contrastive divergence algorithm ( cd ) @xcite for training of rbm .",
    "cd has been empirically showed to be adequate for many applications .",
    "to model real - valued data , the gaussian restricted boltzmann machine ( grbm ) has been proposed @xcite@xcite .",
    "the energy function of grbm is typically defined with : @xmath19 in which @xmath20 and @xmath21 models the standard deviation of each visible units .    conveniently , the learning algorithm of grbm is the same with rbm ( [ eq : rbmcdw])([eq : rbmcda])([eq : rbmcdb ] ) .",
    "the conditional probabilities necessary for cd of a grbm are : @xmath22    generally speaking , @xmath21 can be learned from data , but it s difficult with cd .",
    "the training data to be modeled with a grbm are always pre - processed mean 0 and variance 1 and thus @xmath21 can be fixed with 1 and not trained .",
    "the reason why @xmath21 is difficult to train with cd can be explained as follows : when @xmath21 is much smaller than 1 , the visible - hidden effects ( [ eq : grbmeq1 ] ) tends to be large and hidden - visible effects ( [ eq : grbmeq2 ] ) tends to be small .",
    "the result of such effect is that hidden units always tend to be firmly 1 or 0 , and thus undermine the whole training process .",
    "one disadvantage of grbm is its conditional independence assumption .",
    "that is , conditioned on the hidden layer , each visible unit is assumed to follow a gaussian distribution and independent with each other .",
    "however , for natural data such as speech and image , they tend to have local similarity property",
    ". take speech data for example , the smoothness of speech always makes one frame of acoustic feature similar to the frames next to it .",
    "such local similarity property is difficult to capture with grbm and yet contains certain amount of information . to offset this problem",
    ", we propose a variant of grbm called multivariate gaussian restricted boltzmann machine .      th sub - unit in the @xmath23th visible unit with @xmath24th hidden unit",
    "is modeled with @xmath25.,width=302 ]    the multivariate gaussian restricted boltzmann machine ( mgrbm ) is a natural generalization of grbm . the graphical model of a mgrbm is illustrated in figure [ fig : mgrbm ] .",
    "compared with grbm , in which each unit in visible layer is modeled with a gaussian distribution given the hidden layer , a mgrbm assumes that each unit in visible layer is modeled with a multivariate gaussian distribution given the hidden layer .",
    "similar to what @xmath21 models in a grbm , we denote the covariance matrix of each unit in visible layer of a mgrbm with @xmath26 .",
    "consider only the non - degenerate case , @xmath26 is a positive - definite matrix and thus cholesky decomposition can be applied : @xmath27 . since",
    "matrix @xmath28 is full - rank , we can denote @xmath29 . with these notation",
    ", we can define the energy function of an mgrbm as : @xmath30 suppose the number of units in visible layer and hidden layer is @xmath31 and @xmath32 respectively , and each unit in visible layer has @xmath33 dimension .",
    "then @xmath34 each is a @xmath35 vector ; @xmath36 each is a @xmath37 matrix ; @xmath38 each is a @xmath39 matrix ; @xmath6 and @xmath40 are both @xmath41 vectors .",
    "similar to grbm , we can also prove that : @xmath42 and learning algorithm is : @xmath43    in our experiment , we use persistent contrastive divergence ( pcd ) @xcite to train mgrbm .",
    "the algorithm can be written as : @xmath44 with @xmath45 denotes average with respect to fantasy particles .",
    "notice that the problem for updating variances in a grbm which we described in section [ sec : grbm ] still exists in mgrbm . in section [ sec : rbmtrain ]",
    "we will explain how we address this problem in our experiments .",
    "mgrbm is specially designed for speech data to address the problem of grbm described above .",
    "typically , for the task of speech recognition with hybrid hmm - neural network ( nn ) method , a context of several frames of acoustic feature is used for each training case . unlike grbm , mgrbm can explicitly model the evolving characteristics in each context .",
    "how a mgrbm can be used for robust feature extraction is illustrated in figure [ fig : mgrbmasr ] . concretely , suppose the original acoustic feature for each frame has @xmath46 dimensions and each context is chosen to be @xmath47 frames .",
    "then the visible layer of mgrbm has @xmath46 units , each has dimension @xmath47 ; the @xmath33th dimension in the @xmath48th frame acoustic feature corresponds to the @xmath48th dimension of @xmath33th unit .",
    "there are two reasons for the above setting .",
    "first , the correlation modeled across each dimension of acoustic feature would act as a strong regularization in temporal perspective if training data is much different from testing data .",
    "second , as is the case with grbm , mgrbm also has conditional independence assumption .",
    "fortunately , this assumption is indeed satisfied if we use mfcc as acoustic feature , for the step of discrete cosine transform already has the effect of decorrelation .      despite of its prevalence and huge success in phoneme recognition @xcite@xcite and lvcsr",
    "@xcite , deep neural network ( dnn ) stand alone is rarely used as a acoustic model for robust speech recognition .",
    "we believe that this is due to the fact that neural network is a discriminative model , whose optimization objective is better discriminative power and lower classification error . however , for the task of robust speech recognition , especially in mismatched scenario where the training data is clean and the testing data is noisy , the power of such model is greatly degraded due to its poor generalization over highly distorted data .",
    "we assert that dnn performs significantly poorly in very noisy conditions than gmm and we will prove it in our experiments later .",
    "generative models focus on modeling how the data is  generated \" .",
    "natural data such as speech and image are usually high - dimensional , but all that make sense occupy only a subspace ( or lower - dimensional manifold ) .",
    "the learning process of a generative model is essentially to find out this manifold by tuning all its parameters .",
    "background noise from real - life environment , we believe , is substantially different from white noise , because the former kind of signal contains certain characteristic shared with sounds that spread through the air .",
    "for this reason , we consider it more appropriate to model speech with generative models .",
    "so , we would like to find a model that can leverage such advantage of generative models and yet escape from a model with strong assumption like gmm , and rbm is indeed a such model",
    ".    as a generative model , rbm ( and its variants ) makes little assumption of data .",
    "what is more important , it belongs to a family called product of experts ( as opposed to mixture of experts which gmm belongs to ) @xcite .",
    "this makes it exponentially more powerful and less prone to over - fitting .",
    "other than modeling the distribution of data , it provides a natural way to transform feature by applying ( [ eq : rbmeq1])([eq : grbmeq2])([eq : mgrbmeq2 ] ) .",
    "since such transformation takes the whole distribution of data into consideration , the learned feature tends to be much more abstract and expressive .",
    "in this paper , we used the aurora2 data set @xcite for our experiments . in all the experiments described below , we used only clean data set as training data and whole test set as testing data .",
    "we intentionally use acoustic models that are simple and comparable .",
    "two different kinds of acoustic model is utilized in our experiments : hmm - gmm and hybrid hmm - dnn . with each kind of acoustic model ,",
    "we compare the word error rate ( wer ) of three kinds of feature : mfcc ( 12 coefficients + energy + delta + acceleration , 39-dimension ) , grbm - extracted feature ( g - feature ) and mgrbm - extracted feature ( m - feature ) .",
    "we used standard aurora2 setup described in @xcite for baseline system ( gmm + mfcc ) .",
    "we use dnn simply because it is a good classifier and it is much more natural to employ rbm - extracted feature .",
    "we used two kinds of rbm in our experiment , one for feature extraction and the other for pre - training of dnn .",
    "grbm and mgrbm are both trained to extract feature for comparison . for these grbms and mgrbms , pcd with stochastic gradient descent ( sgd ) was used .",
    "the size of a mini - batch is 128 and no momentum was applied .",
    "the number of fantasy particles was the same with the size of a mini - batch , and one full gibbs update was performed for each gradient estimate .",
    "for all the weights and biases , learning rate was 0.001 .",
    "for the updating of @xmath36 in ( [ eq : mgrbm ] ) of an mgrbm , learning rate was 0.0001 . to avoid the problem of updating variances described in section [ sec : grbm ] , we divide each @xmath36 by its trace after each updating .",
    "this step makes all diagonal elements of @xmath36 fixed with one and thus make the learning stable .",
    "all grbm and mgrbm were trained for 400 epochs and each with 1024 hidden units , which made g - feature and m - feature both has 1024 dimensions .",
    "the visible layer corresponds to a context of 9 frame of mfcc feature , which makes the number of visible units of grbm 351 and mgrbm 39@xmath499 .",
    "for all the grbms and rbms that were used for pre - training dnns , cd algorithm ( cd-1 ) , sgd with batch size 128 and a momentum of 0.9 were used .",
    "we trained all the rbms with learning rate of 0.01 for 50 epochs and all the grbms with learning rate of 0.001 for 100 epochs .",
    "all dnns in our experiments have 4 hidden layers , each hidden layer with 1024 units .",
    "dnns were pre - trained with stacked rbm ( section [ sec : rbmtrain ] ) as described in @xcite and fine - tuned with back - propagation algorithm with sgd as described in @xcite .",
    "the learning rate for back - propagation started from 1.0 and was halved if an increase of substitution error on development set was observed during the end of each epoch and all the weights roll - back to the end of last epoch .",
    "when using mfcc feature , the input layer of dnn corresponds to 9 frame of mfcc , which is 351 units .",
    "the output layer has 180 units , with each unit corresponding to each state in hmm .",
    "all data to be trained with dnn are normalized to have mean 0 and variance 1 with respect to each dimension .      all results from our experiments are shown in table 1 .",
    "we averaged wer in all test set across different noises .",
    "the first thing we should notice is that in the lowest snr condition , hmm - dnn performs consistently poorer than hmm - gmm , which confirms our assertion in section [ sec : fl ] .",
    "those wer that exceeded 100% was due to many substitution errors .",
    "from the last three columns , we can clearly see the advantage of g - feature over mfcc and m - feature over g - feature .",
    "although the improvements does not seems to be obvious , the trend that the gap between m - feature and g - feature increases with the decrease of snr is still distinguishable .",
    "the reason why the improvements of m - feature over g - feature is marginal , we believe , is that the model which we trained is still not good enough .",
    "all the learning rates , number of epochs and number of hidden units are chosen heuristically , and 9 frame of context might be not sufficiently long .",
    "so there is no reason to believe we have exploited the full potential of mgrbm .",
    "[ tab : ret ]    .comparison of different features , all numbers are percentage of wer .",
    "g - feat and m - feat here represents g - feature and m - feature respectively . [ cols=\"^,^,^,^,^,^,^ \" , ]     with hmm - gmm as acoustic model ,",
    "the comparison is not so straightforward , because training a 1024-dimensional gmm would leads to severe over - fitting .",
    "so we reduced the dimension of g - feature and m - feature to 39 dimensions with principal component analysis ( pca ) .",
    "notice that this is actually not the right thing to do , because the sum of top 39 eigenvalues only consists 91.8% and 69.7% of sum of all eigenvalues for g - feature and m - feature respectively .",
    "despite of such great loss of information , both g - feature and m - feature performs better than mfcc for all snr levels except the clean speech .",
    "in this paper , we briefly reviewed the definition and learning algorithm of rbm and grbm .",
    "we then propose a new variant of rbm called mgrbm by which we would like to model the covariance of adjacent frames within each context .",
    "after that we offered an explanation of why a feature learned with rbm ( and its variants ) might be able to enhance the performance of robust speech recognition over original feature .",
    "finally we performed our experiments on aurora2 and showed that feature that learned with grbm and mgrbm would indeed improve the average accuracy across environment in every snr condition over original mfcc feature .    throughout the process of feature learning with grbm ,",
    "virtually nothing is presupposed .",
    "this makes it adaptable with any feature generated from a front - end . aside from training a grbm , which can be done off - line ,",
    "the extra cost of the feature transformation is merely a multiplication with a matrix . from this perspective ,",
    "feature learning with grbm is similar to the tandem system @xcite , but the latter is purely supervised .",
    "hence many advantages can be gained if lots of data is accessible but little is labeled .",
    "what is more important , grbm - extracted feature can be used for the tandem system seamlessly , which makes tandem system semi - supervised and thus further enhancement of performance can be achieved .",
    "this work is partially supported by the national basic research program ( 973 program ) of china(2012cb316401 ) , the national natural science foundation of china ( 60928005 , 60805008 , 60931160443 and 61003094 ) , the ph.d .",
    "programs foundation of ministry of education of china ( 200800031015 ) , the upgrading plan project of shenzhen key laboratory and the science and technology r&d funding of the shenzhen municipal ."
  ],
  "abstract_text": [
    "<S> in this paper , we first present a new variant of gaussian restricted boltzmann machine ( grbm ) called multivariate gaussian restricted boltzmann machine ( mgrbm ) , with its definition and learning algorithm . then we propose using a learned grbm or mgrbm to extract better features for robust speech recognition . </S>",
    "<S> our experiments on aurora2 show that both grbm - extracted and mgrbm - extracted feature performs much better than mel - frequency cepstral coefficient ( mfcc ) with either hmm - gmm or hybrid hmm - deep neural network ( dnn ) acoustic model , and mgrbm - extracted feature is slightly better .    * index terms * : restricted boltzmann machine , robust speech recognition , feature learning </S>"
  ]
}