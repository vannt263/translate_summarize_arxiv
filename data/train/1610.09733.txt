{
  "article_text": [
    "many of the most interesting phenomena in the world around us emerge from interactions among many degrees of freedom . in the era of `` big data , '' we are encouraged to think about this more explicitly , describing the state of the system as a point in a space with many dimensions : the state of a cell is defined by the expression level of many genes , the state of a financial market is defined by the prices of many stocks , and so on .",
    "one approach to the analysis of these high dimensional data is to look for a linear projection onto a lower dimensional space .",
    "quantitatively , the best projection is found by diagonalizing the covariance matrix , which decomposes the variations into modes that are independent at second order ; these are the principal components ( pcs ) , and the method is called principal components analysis ( pca ) . in favorable cases , very few modes will capture most of the variance , but it is much more common to find that the eigenvalues of the covariance matrix form a more continuous spectrum , so that any sharp division between important and unimportant dimensions would be arbitrary .    for physical systems in thermal equilibrium , it again is the case that the most interesting phenomena emerge from interactions among many degrees of freedom , but here we have a quantitative language for describing this emergence . in the classical view , we make precise models of the interactions on a microscopic scale , and then statistical mechanics is about calculating the implications of these interactions for the macroscopic behavior of matter . in the modern view , we admit that our microscopic description itself is approximate , incorporating `` effective interactions '' mediated by degrees of freedom that we might not want to describe explicitly , and that the distance scale at which we draw the boundary between explicit and implicit description also is arbitrary .",
    "attention shifts from the precise form of our model to the way in which this model evolves as we move the boundary between degrees of freedom that we describe and those that we ignore @xcite ; the evolution through the space of possible models is described by the renormalization group ( rg ) . a central result of",
    "the rg is that many detailed features of models on a microscopic scale disappear as we coarse  grain our description out to the macroscopic scale , and that in many cases we are left with only a few terms in our models , the `` relevant operators . ''",
    "thus , some of the success of simple models in describing the world comes not from an inherent simplicity , but rather from the fact that macroscopic behaviors are insensitive to most microscopic details ( irrelevant operators ) .",
    "the rg approach to statistical physics suggests that systems in which pca fails to yield a clean separation between high variance and low variance modes may nonetheless be simplified .",
    "indeed , in a system where the many degrees of freedom live on a lattice , with translation invariant interactions , the principal components are fourier modes , and typically we find that the variance of each mode decreases monotonically but smoothly with wavelength . in the momentum space implementation of rg @xcite , we put a cutoff on the wavelength , and ask what happens to the joint distribution of the remaining variables as we move this cutoff , averaging over the short wavelength modes . in this language , the rg is about what happens as we vary the arbitrary distinction between high variance pcs that we keep , and low variance pcs that we ignore .",
    "the goal of this paper is to clarify this connection between pca and rg , so that we can construct rg approaches to more complex , high dimensional systems .",
    "this paper has been written for a volume dedicated to the memory of leo kadanoff .",
    "as has been described many times , the modern development of the rg began with leo s intuitive construction of `` block spins , '' in which he made explicit the idea of averaging over the fluctuations that occur on short wavelengths @xcite . later in his life , kadanoff worked on more complex problems , from the dynamics of cities @xcite to patterns @xcite , chaos @xcite , and singularities @xcite in fluid flows , and more @xcite .",
    "beyond his own work , he was a persistent advocate for the physics community s exploration of complex systems , including biological systems .",
    "we have benefited , directly and indirectly , from his enthusiasm , as well as being inspired by his example .    among leo s",
    "last papers are a series of historical pieces reflecting on his role in the development of the rg , and on statistical physics more generally @xcite .",
    "although much can be said about these papers , surely one message is that the physicist s persistent search for simplification has been rewarded , time and again .",
    "we offer this paper in that spirit , as we try to carry kadanoff s intuition about thinning out microscopic degrees of freedom away from its origins in systems with local interactions .",
    "let us imagine that the system we are studying is described by a set of variables @xmath0 , where the dimensionality @xmath1 is large .",
    "for the purposes of this discussion , `` describing the system '' means writing down the joint probability of all @xmath1 variables , @xmath2 . for simplicity",
    "we define these variables so that they have zero mean , and we ll assume that positive and negative fluctuations are equally likely ( though this is not essential ) .",
    "we start with the guess that the fluctuations are nearly gaussian , so we can write @xmath3 , \\label{p1}\\ ] ] where the coefficient @xmath4 allow us to describe weak kurtosis of the random variables . it may be useful to note that the probability distribution in eq ( [ p1 ] ) is the maximum entropy , and hence least structured , model consistent with the full covariance matrix and the mean kurtosis of all the variables ; in this sense it is a minimal model .",
    "much of what we will say here can generalized to the case where each variable has a different kurtosis , so there is a distinct @xmath5 associated with each term @xmath6 .",
    "if @xmath7 , we are describing a system in which fluctuations are gaussian , and in this limit the matrix @xmath8 is the inverse of the covariance matrix @xmath9 in the conventional application of renormalization group ideas , we can classify non ",
    "gaussian terms as relevant or irrelevant : as we coarse  grain our description from microscopic to macroscopic scales , do departures from a gaussian distribution become more or less important ?",
    "our first goal is to show how we can export this idea to the more general setting , where the kernel @xmath8 does not have any symmetries such as translation invariance . to do this , we start near @xmath7 , and work in perturbation theory .",
    "it is useful to write the eigenvalues @xmath10 and eigenvectors @xmath11 of the matrix @xmath12 , @xmath13 so that the variations in @xmath14 can be decomposed into modes @xmath15 , @xmath16 if @xmath17 then these modes are exactly the principal components . the gaussian term becomes @xmath18 and hence at @xmath19 the variance of each mode is given by @xmath20 .",
    "the average variance of the individual variables is @xmath21 where in the last step we introduce the distribution @xmath22 which becomes smooth in the limit of large @xmath1 , and we note explicitly that there is a largest eigenvalue @xmath23 .",
    "the essential idea is to eliminate the modes that have small variance .",
    "this corresponds to restricting our attention only to modes with @xmath24 _ less _ than some cutoff .",
    "equivalently , it corresponds to decreasing the limit @xmath23 on the integral over eigenvalues , e.g. in eq ( [ meanvar1 ] ) .",
    "this reduces the total variance , but it is natural to choose units in which the variance is fixed , and this implies that as we change the cutoff @xmath23 we have to rescale the values of @xmath25 .",
    "so we replace @xmath26 , and we can determine this scale factor by insisting that the mean variance stay fixed .",
    "again , we are working at small @xmath4 , so we do this calculation at @xmath7 : @xmath27\\\\ & = & \\frac{d}{d\\lambda } \\left[z_\\lambda^2 \\int_0^\\lambda d\\lambda\\ ,   \\frac{\\rho(\\lambda ) } { \\lambda}\\right]\\\\ \\rightarrow \\frac{d\\ln z_\\lambda}{d\\ln \\lambda } & = & - \\frac{1}{2 } \\rho(\\lambda ) \\left [ \\int_0^\\lambda d\\lambda\\ , \\rho(\\lambda ) \\frac{1}{\\lambda}\\right]^{-1 } .\\end{aligned}\\ ] ]    when we reduce the cutoff , we also reduce the number of degrees of freedom in the system .",
    "the average of the quadratic term in the ( log ) probability distribution is automatically proportional to this effective number of degrees of freedom , @xmath28 and this insures , for example , that the entropy of the probability distribution will be proportional to @xmath29 ( extensivity ) . to be sure that this works also for the quartic terms , we write @xmath30 = n_{\\rm eff } \\tilde g   \\left[{1\\over",
    "n}\\sum_{\\rm i } \\left(z_\\lambda \\phi_{\\rm i}\\right)^4\\right],\\ ] ] which defines the effective coupling constant @xmath31 now the scaling of the coefficient @xmath32 is given by @xmath33 .",
    "\\label{dg4}\\ ] ] since this is the difference between two positive terms , we can find either sign for the result .    if the scaling function @xmath34 is positive ,",
    "then as we decrease the cutoff @xmath23 and thus average over more and more of the low variance modes , any small quartic term @xmath32 will become still smaller , and hence the distribution approaches a gaussian .",
    "this seems to make sense , since when we project onto a ( much ) lower dimensional space , each of the variables that remains is a weighted sum of many of the original variables , and we might expect the central limit theorem to enforce approximate gaussianity of the resulting distribution .",
    "but if the scaling function @xmath35 , then as we average over more and more of the lower variance modes , the quartic term becomes more and more important to the structure of the distribution . to use the language of the rg , under these conditions",
    "the quartic term is a relevant operator .",
    "if we consider the case where the density of eigenvalues is a power law , @xmath36 , we find @xmath37 thus , the spectral density of eigenvalues determines the relevance of non  gaussian terms in the distribution .    in the conventional field theoretic examples , where the variables @xmath38 live at positions @xmath39 in a @xmath40 dimensional euclidean space ,",
    "the correlations come from a `` kinetic energy '' term that enforces similarity among neighbors , @xmath41 ^ 2 .\\ ] ] the eigenvectors of @xmath12 are fourier modes , indexed by a wave vector @xmath42 , with eigenvalues @xmath43 .",
    "if the original variables are on a lattice with linear spacing @xmath44 , then there is a maximum eigenvalue @xmath45 , and the density @xmath46 corresponding to @xmath47 . from eq ( [ dg4b ] )",
    "we find @xmath48 the quartic term is relevant if @xmath35 , which corresponds to @xmath49 , as is well known from the conventional rg analysis @xcite ; the extra factor of @xmath50 arises because @xmath23 is a cutoff on the eigenvalue , which is the square of the wavevector .",
    "everything that we have said here can be carried over to the case where each variable is associated with a different coupling @xmath5 in our original model , eq ( [ p1 ] ) .",
    "this corresponds to a maximum entropy description that captures the pairwise correlations among the variables and the kurtosis of each individual variable .",
    "the relevance or irrelevance of each term is controlled , in the same way , by the eigenvalue spectrum .",
    "if we include terms @xmath51 , allowing us to match higher moments of the marginal distributions for each @xmath25 , then as usual these terms are less relevant at larger @xmath52 .",
    "thus far our analysis has been confined to `` power counting . ''",
    "the next step is to integrate out the the low variance degrees of freedom and compute corrections to the coupling constants that are beyond those generated from the spectrum of eigenvalues itself . since we can think about discrete modes ,",
    "we can write @xmath53 , where @xmath54 is the variable describing fluctuations in the `` last mode '' that we have kept in our description , and we want to average over these fluctuations . in the limit of small @xmath4 , @xmath54 is gaussian with @xmath55 , and we find    @xmath56 & & \\rightarrow   { \\bigg\\langle}\\exp\\left [ -   \\frac{\\tilde g}{4 ! } \\frac { n_{\\rm eff}}{n } \\sum_{\\rm i }   z_\\lambda^4 ( \\phi_{\\rm i } + u_{\\rm i }   \\psi)^4\\right]{\\bigg\\rangle}\\\\ & & \\hskip -1.25 in =    \\exp\\left [ - \\frac{\\tilde g}{2 }    \\frac { n_{\\rm eff}}{n } \\sum_{\\rm i }   z_\\lambda^4 { { u_{\\rm i}^2 } \\over \\lambda } \\phi_{\\rm i}^2   - \\frac{\\tilde g}{4 ! } \\frac { n_{\\rm eff}}{n } \\sum_{\\rm i } ( z_\\lambda \\phi_{\\rm i})^4\\   + \\frac{1}{2}\\left ( \\frac{\\tilde g n_{\\rm eff}}{4 ! n}\\right)^2    \\sum_{{\\rm i},{\\rm j } } \\frac { z_\\lambda^8}{\\lambda^2 }   \\left ( 72 \\phi_{\\rm i}^2 \\phi_{\\rm j}^2 u_{\\rm i}^2 u_{\\rm j}^2 + 96 \\phi_{\\rm i}^3 \\phi_{\\rm j } u_{\\rm i } u_{\\rm j}^3\\right ) + \\cdots \\right ] .",
    "\\nonumber\\\\   & & \\end{aligned}\\ ] ]    the first term is a correction to the matrix @xmath12 , analogous to a mass renormalization . in the general case",
    "we not only get corrections to the coefficient of @xmath6 , we also generate terms @xmath57 and @xmath58 . as in the standard discussion",
    ", we will assume that the fields @xmath25 are `` slowly varying '' functions of their index .",
    "more precisely , we will expand the correction terms around the point where @xmath59 , and for now we drop the gradient  like terms @xmath60 , @xmath61 ,  . in this approximation",
    ", we have @xmath62 where in the last step we again use the slow variation of @xmath25 to replace @xmath63 with its average . in the same approximation",
    ", the term @xmath64 vanishes .",
    "the net result is that @xmath65 this is the change in coupling associated with integrating out one mode , which corresponds to a change in the cutoff such that @xmath66 , so we can rewrite eq ( [ dga ] ) as @xmath67 combining with the scaling behavior in eq ( [ dg4 ] ) , we find @xmath68 . \\label{dgc}\\ ] ] in the case where @xmath69 , this generates a fixed point @xmath70 , which is analogous to the wilson  fisher fixed point @xmath71 @xcite .",
    "the calculation we have done here is aimed at showing that the conventional analysis of fixed points can be carried over to this different setting , away from equilibrium statistical physics with local interactions .",
    "we assume that this more complex setting allows for a richer variety of fixed points , which need to be explored .",
    "these arguments suggest that , at least in perturbation theory , much of the apparatus of the renormalization group for translation invariant systems with local interactions can be carried over to more complex systems .",
    "we can define relevant and irrelevant operators , and there is a path to identifying fixed points . the crucial role played by the dimensionality in systems with local interactions is played instead by the spectrum of the matrix @xmath12 .",
    "perhaps most important is that we can carry over the _ concept _ of renormalization",
    ".    faced with real data on a system with many degrees of freedom , we do nt know the matrix @xmath12 .",
    "we do know that , if the system is close to being gaussian , then @xmath12 is close to being the inverse of the covariance matrix @xmath72 , which we can estimate from the data . in systems with translation invariance ,",
    "the eigenvectors of @xmath72 and @xmath12 are the same , which means that coarse graining by eliminating the modes with large eigenvalues of @xmath12 ( momentum shells ) is exactly the same as eliminating the modes with small eigenvalues of @xmath72 .",
    "although this ca nt be true in general , we can try :    \\(1 ) we examine the spectrum of the covariance matrix . if a small number of eigenvalues are separated from the bulk , and capture most of the variance , then the system is genuinely low dimensional and we are done ( pca works ) .",
    "more commonly , we find a continuum of eigenvalues , with no natural separation .",
    "\\(2 ) power  law behavior in a rank  ordered plot of the eigenvalues is analogous to power ",
    "law correlation functions in the usual field theoretic or statistical physics examples . in practice , however , it may be difficult to verify power  law behavior over a very wide range of scales .",
    "\\(3 ) we coarse grain our description by projecting out a fraction of the modes with the smallest eigenvalues of @xmath72 . in effect",
    "this replaces each variable @xmath25 by an average over low variance details , in the spirit of the block spin construction .",
    "\\(4 ) to follow the results of coarse graining , we can measure the moments of the local variables , @xmath73 , or even their full distribution , as was done long ago for monte carlo data by binder @xcite .    as a first example , we have analyzed an experiment on the activity of 160 neurons in a small patch of the vertebrate retina as it responds to naturalistic movies @xcite ; a full description will be given elsewhere , but here we focus on our ability to detect a nontrivial renormalization group flow as we coarse ",
    "grain this system . as in previous analyses of these data , we divide time into small bins ( width @xmath74 ) , and in each bin a single neuron either generates an action potential or remains silent , so that the natural local variables are binary before any coarse graining .",
    "we can then take a state of the entire system to be the 160dimensional vector of these binary variables , but we can also consider @xmath75 successive vectors in time , as in the `` time delayed embedding '' analysis of dynamical systems @xcite : increasing @xmath75 compensates for not observing directly all the relevant degrees of freedom in the system , and gives us access to a higher dimensional description .",
    "given the size of the data set , we can use @xmath76 without creating problems of undersampling , and this gives us @xmath77 dimensions . because the different neurons are different from one another , we normalize each variable @xmath25 to have zero mean and unit variance , and so the covariance matrix is the matrix of correlation coefficients in the raw data .",
    "as we can see at left in fig [ retinal_data ] , the eigenvalues of the correlation matrix have an essentially continuous spectrum , perhaps even showing hints of scale invariance . vs momentum @xmath78 in the usual statistical physics examples . ]",
    "this spectral structure is well outside the range that would be generated by an equally large random sample from uncorrelated variables .    because the raw variables of this system are binary , the normalized fourth moments ( @xmath79 ) can be large and vary substantially from neuron to neuron .",
    "as we coarse  grain , eliminating modes corresponding to small eigenvalues of the covariance matrix , this variation is reduced , as shown at right in fig [ retinal_data ] .",
    "more strikingly , the normalized fourth moments are hardly varying as we move our cutoff beyond the first @xmath80 of the modes , and the median value is stabilizing well above the value of 3 expected for a gaussian distribution .",
    "it is interesting that the range of scales ( fraction of modes included ) over which see an approximately fixed fourth moment is the same as the range over which see approximately power  law behavior of the eigenvalue spectrum .",
    "these results suggest that the joint distribution of activity in this neural network is close to a nontrivial fixed point of the renormalization group transformation .",
    "this is consistent with previous evidence that this system is close to a critical point in the thermodynamic sense @xcite , but the renormalization group analysis connects more fully to our understanding of criticality in equilibrium systems .    as a second example , we consider a set of 4000 assets traded on the new york stock exchange @xcite . in the data , spanning nearly ten years from 1 jan 1990 through 30",
    "apr 1999 , 2445 assets appear for more than 2300 days out of the total of 2356 trading days , and we focus on a random subset of @xmath81 from this group . on each day",
    "@xmath82 an asset @xmath83 opens at price @xmath84 and closes at price @xmath85 ; we define the state of the system on day @xmath82 by the vector of daily returns @xmath86 , with @xmath87 $ ] . at left in fig [ stock_data ]",
    "we see the spectrum of eigenvalues of the correlation matrix .",
    "in contrast to the neural data , the number of samples here is comparable to the dimensionality of the system , so we expect that the spectrum will be substantially affected by random sampling . indeed ,",
    "if we project out the ten percent of modes with the largest variance ( opposite to our rg procedure ) , the resulting spectrum is very close to the predictions of the marchenko ",
    "pastur distribution for covariance matrices constructed from samples of uncorrelated variables @xcite .",
    "if we apply our rg procedure to the raw data , we see a non  monotonic trajectory of the fourth moments , first moving toward the gaussian fixed point and then away ( fig [ stock_data ] , right ) .",
    "the turning point is roughly when we have integrated out all but the last ten percent of high variance modes , which is consistent with the lower @xmath80 of the eigenvalue spectrum being well described by the marchenko ",
    "pastur distribution . indeed ,",
    "if we first exclude the top ten percent of high variance modes , the fourth moments flow very quickly to the gaussian fixed point and the third moments flow to zero ( not shown ) .",
    "the ten percent of high variance modes clearly are not just noise , however , although it is not clear from the data whether their distribution is described by a fixed point of the rg .",
    "we emphasize that the boundary between noise  like and non  noise modes is a property not of the system , but of the finite sample of data ; it is possible that the rg analysis we propose here could be combined with denoising @xcite to give more insight .",
    "the idea that the rg might be useful in more complex systems is a widely held intuition . in particular ,",
    "there have been efforts to move from regular lattices to graphs @xcite , as well as to construct a real space renormalization group for spin glasses @xcite .",
    "what is new here , we think , is that , at least in perturbation theory , we can free ourselves completely from assumptions of locality , which seem so crucial to the usual notions of relevant and irrelevant operators . perhaps more importantly , connecting rg and pca allows us to look at data in a new way , with interesting results in two very different complex systems .",
    "pca is a search for simplification .",
    "the hope is that a system with variables that live in a high dimensional space can be captured by a projection of these variables in to a low dimensional space .",
    "although the rg involves ( repeated ) projections onto lower dimensional spaces , this dimensionality reduction is _ not _ the source of simplification . indeed , when we study models , the full renormalization group transformation involves expanding the system back to restore the original number of degrees of freedom ; admittedly , this is difficult to do with real data .",
    "rg is a search for simplification , not in the space of the system variables but in the space of models .",
    "an interesting connection is to the question of how well different terms in a model are determined by experimental data .",
    "starting with models for biological signaling networks @xcite , sethna and colleagues have argued that models for complex systems typically have a wide range of parameter sensitivities , so that some directions in parameter space have coordinates that are easily determined by data while other directions are almost never determined .",
    "this pattern is quantified by the spectrum of eigenvalues in the fisher information matrix ( fim ) , and in many cases this spectrum is nearly uniform on a logarithmic scale @xcite , a property termed `` sloppiness . ''",
    "many of these models can be written so that parameter spaces are compact , and simplification then is achievable by moving along the ill  determined directions until reaching the edge of the space , leaving a model with one less parameter @xcite .",
    "recent work has shown that conventional statistical physics models do not exhibit sloppiness if experiments involve measurements on the microscopic scale , but that this pattern develops when measurements are restricted to coarse  grained variables @xcite .",
    "the spreading of the fim eigenvalues is controlled by the rg scaling of the different operators out of which the model is constructed , suggesting that the notions of simplification that are inherent to the rg are equivalent to a more data  driven simplification in which we keep only model components that are well determined be experiment .",
    "it is possible that there are even more direct connections between the renormalization group and the learning of probabilistic models @xcite .    in the conventional implementations of the renormalization group , we put variables in order by their length scale , with small length scales at one end and long length scales at the other .",
    "the intuition is that , when interactions are local , smaller scales are less important , or at least less interesting , and so we average over scales shorter than some distance @xmath88 .",
    "the rg then is the exploration of what happens as we change @xmath88 . in more complex systems",
    ", simplification requires us to find a natural coordinate system in state space , and then put these coordinates in order of their likely importance , with fine  grained details at one end and crucial collective degrees of freedom at the other .",
    "the spectrum of the covariance matrix gives us one possible answer to these questions , which we have explored here , but surely there are other possibilities , even in the two examples discussed above .",
    "the more significant idea is that once we have identified an axis along which coarse graining seems to make sense , rather than looking for the right place to put the boundary between what we include and what we ignore , we should use the rg as inspiration to explore the evolution of our description as we move this boundary .",
    "we thank d amodei , mj berry ii , and o marre for making available the data of ref @xcite and m marsili for the data of ref @xcite .",
    "we are especially grateful to g biroli , j ",
    "p bouchaud , mp brenner , cg callan , a cavagna , i giardina , mo magnasco , a nicolis , se palmer , g parisi , and dj schwab for helpful discussions and comments on the manuscript .",
    "work at cuny was supported in part by the swartz foundation .",
    "work at princeton was supported in part by grants from the national science foundation ( phy1305525 , phy1451171 , and ccf0939370 ) and the simons foundation .",
    "99 k pearson , on lines and planes of closest fit to systems of points in space .",
    "_ phil mag _ * 2 , * 559572 ( 1901 ) .",
    "j shlens , a tutorial on principal components analysis , arxiv:1404.1100 [ cs.lg ] ( 2014 ) .",
    "kg wilson , problems in physics with many scales of length .",
    "_ sci am _ * 241 * , 158179 ( 1979 )",
    ". kg wilson and j kogut , the renormalization group and the @xmath89 expansion .",
    "_ phys repts _ * 12 , * 75200 ( 1974 ) .",
    "lp kadanoff , scaling laws for ising models near @xmath90 .",
    "_ physics _ * 2 , * 263272 ( 1966 ) .",
    "lp kadanoff , from simulation model to public policy : an examination of forrester s `` urban dynamics . '' _ simulation _ * 16 , * 261268 ( 1971 ) .",
    "lp kadanoff and h weinblatt , public policy conclusions from urban growth models .",
    "ieee trans sys man cybern _",
    "* smc2 , * 139165 ( 1972 ) .",
    "d bensimon , lp kadanoff , s liang , bi shraiman , and c tang , viscous flows in two dimensions . _",
    "rev mod phys _ * 58,*977999 ( 1986 ) .",
    "tc halsey , mh jensen , lp kadanoff , i procaccia , and bi shraiman , fractal measures and their singularities : the characterization of strange sets .",
    "_ phys rev a _",
    "* 33 , * 11411151 ( 1986 ) ; erratum * 34 , * 1601 ( 1986 ) .",
    "p constantin and lp kadanoff , singularities in complex interfaces .",
    "_ phil trans r soc lond ser a _ * 333 , * 379389 ( 1990 ) .",
    "a bertozzi , m brenner , tf dupont , and lp kadanoff , singularities and similarities in interface flows . in _ trends and perspectives in applied mathatematics ,",
    "springer verlag applied math series vol 100 _ lp sirovich , ed , pp 155208 ( 1994 ) .",
    "sn coppersmith , rd blank , and lp kadanoff , analysis of a population genetics model with mutation , selection , and pleitropy .",
    "_ j stat phys _ * 97 , * 429459 ( 1999 ) .",
    "ml povinelli , sn coppersmith , lp kadanoff , sr nagel , and sc venkataramani , noise stabilization of self  organized memories . _ phys rev e _ * 59 , * 49704982 ( 1999 ) . lp kadanoff , more is the same : mean field theory and phase transitions .",
    "_ j state phys _ * 137 , * 777797 ( 2009 ) .",
    "lp kadanoff , relating theories via renormalization .",
    "_ stud hist phil sci b _ * 44 , * 2239 ( 2013 ) .",
    "lp kadanoff , reflections on gibbs : from statistical physics to the amistad .",
    "_ j stat phys _ * 156 , * 19 ( 2014 ) .",
    "lp kadanoff , innovations in statistical physics . _ annu rev cond matt phys _ * 6 , * 114 ( 2015 ) .",
    "kg wilson and me fisher , critical exponents in 3.99 dimensions .",
    "_ phys rev lett _ * 28 , * 240243 ( 1972 ) .",
    "dj amit and v martin ",
    "mayor , _ field theory , the renormalization group , and critical phenomena .",
    "graphs to computers . _",
    "third edition ( world scientific , 2005 ) .",
    "k binder , finite size scaling analysis of ising model block distribution functions . _ z phys b _ * 43 , * 119140 ( 1981 ) .",
    "g tkaik , o marre , d amodei , e schneidman , w bialek , and mj berry ii , searching for collective behavior in a large network of sensory neurons .",
    "_ plos comput biol _ * 10 , * e1003408 ( 2014 ) . hdi abarbanel , r brown , jj sidorowich , and ls tsimring , the analysis of observed chaotic data in physical systems .",
    "_ rev mod phys _ * 65 , * 13311392 ( 1993 ) .",
    "t mora and w bialek , are biological systems poised at criticality ? _ j stat phys _ * 144 , * 268302 ( 2011 ) .",
    "g tkaik , t mora , o marre , d amodei , se palmer , mj berry ii , and w bialek , thermodynamics and signatures of criticality in a network of neurons .",
    "_ proc natl acad sci ( usa ) _ * 112 , * 1150811513 ( 2015 ) .",
    "m marsili , dissecting financial markets : sectors and states .",
    "_ quantitative finance _ * 2 * , 297302 ( 2002 ) .",
    "f lillo and rn mantegna , variety and volatility in financial markets .",
    "_ phys rev e _ * 62 , * 61266134 ( 2000 ) .",
    "jp bouchaud and m potters , financial applications . in _ the oxford handbook of random matrix theory , _",
    "g akemann , j baik , and p di francesco , eds ( oxford university press , 2011 ) ; arxiv:0910.1205 [ qfin.st ] ( 2009 ) . j bun , r allez , jp bouchaud , and m potters , rotational invariant estimator for general noisy matrices . arxiv:1502.06736 [ condmat.statmech ] ( 2015 ) .",
    "j bun , j ",
    "p bouchaud , and m potters , cleaning large correlation matrices : tools from random matrix theory . arxiv:1610.08104",
    "[ condmat.statmech ] ( 2016 ) .",
    "e aygn and a erzan , spectral renormalization group theory on networks .",
    "_ j phys conf series _ * 319 , * 012007 ( 2011 ) .",
    "m castellana , real  space renormalization group analysis of a non  mean  field spin  glass .",
    "_ epl _ * 95 , * 47014 ( 2011 ) .",
    "mc angelini , g parisi , and f ricci  tersenghi , ensemble renormalization group for disordered systems .",
    "_ phys rev b _ * 87 * 134201 ( 2013 ) .",
    "mc angelini and g biroli , spin glass in a field : a new zero - temperature fixed point in finite dimensions .",
    "_ phys rev lett _ * 114 , * 095701 ( 2015 ) .",
    "ks brown , cc hill , ga calero , cr myers , kh lee , jp sethna , and ra cerione , the statistical mechanics of complex signaling networks : nerve growth factor aignaling .",
    "_ phys biol _ * 1 , * 184195 ( 2004 )",
    ". jj waterfall , fp casey , rn gutenkunst , ks brown , cr myers , pw brouwer , v elser , and jp sethna , sloppy model universality class and the vandermonde matrix .",
    "_ phys rev lett _ * 97 , * 150601 ( 2006 ) .",
    "rn gutenkunst , jj waterfall , fp casey , ks brown , cr myers , and jp sethna , universally sloppy parameter sensitivities in systems biology .",
    "_ plos comput biol _ * 3 , * e189 ( 2007 ) .",
    "mk transtrum , bb machta , and jp sethna , geometry of nonlinear least squares with applications to sloppy models and optimization . _ phys rev e _ * 83 , * 036701 ( 2011 ) .",
    "bb matcha , r chachra , mk transtrum , and jp sethna , parameter space compression underlies emergent theories and predictive models . _",
    "science _ * 342 * , 604607 ( 2013 ) .",
    "p mehta and dj schwab , an exact mapping between the variational renormalization group and deep learning .",
    "arxiv:1410.3831 [ stat.ml ] ( 2014 ) ."
  ],
  "abstract_text": [
    "<S> a system with many degrees of freedom can be characterized by a covariance matrix ; principal components analysis ( pca ) focuses on the eigenvalues of this matrix , hoping to find a lower dimensional description . </S>",
    "<S> but when the spectrum is nearly continuous , any distinction between components that we keep and those that we ignore becomes arbitrary ; it then is natural to ask what happens as we vary this arbitrary cutoff . </S>",
    "<S> we argue that this problem is analogous to the momentum shell renormalization group ( rg ) . following this analogy , we can define relevant and irrelevant operators , where the role of dimensionality is played by properties of the eigenvalue density . </S>",
    "<S> these results also suggest an approach to the analysis of real data . as an example , we study neural activity in the vertebrate retina as it responds to naturalistic movies , and find evidence of behavior controlled by a nontrivial fixed point . </S>",
    "<S> applied to financial data , our analysis separates modes dominated by sampling noise from a smaller but still macroscopic number of modes described by a non  gaussian distribution . </S>"
  ]
}