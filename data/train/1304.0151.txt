{
  "article_text": [
    "let @xmath4 be a sequence of measurable spaces , @xmath5 , @xmath6 , @xmath7 , be a sequence of indicator potentials and @xmath8\\}_{n\\geq 1}$ ] , with @xmath9 a fixed point , be a sequence of markov kernels .",
    "then for the collection of bounded and measurable functions @xmath10 the @xmath11time feynman - kac marginal is : @xmath12 assuming that @xmath13 $ ] is well - defined , where @xmath14 $ ] is the expectation w.r.t .  the law of an inhomogeneous markov chain with transition kernels @xmath15 .",
    "such models appear routinely in the statistics and applied probability literature including :    * abc approximations ( as in , e.g. ,  del moral et al .",
    "( 2012 ) ) * abc approximations of hmms ( dean et al .",
    "2010;jasra et al",
    ".  2012 ) * rare - events problems ( as in , e.g. ,  crou et al .",
    "( 2012 ) )    in order to perform estimation for such models , one often has to resort to numerical methods such as particle filters or mcmc ; see the aforementioned references .",
    "the basic particle filter , at time @xmath16 and given a collection of samples @xmath17 with non - zero potential on @xmath18 , will generate samples on @xmath19 using the markov kernels @xmath15 and then sample with replacement amongst @xmath20 according to the normalized weights @xmath21 .",
    "the key issue with this basic particle filter is that , at any given time , there is no guarantee that any sample @xmath22 lies in @xmath23 , and in some challenging scenarios , the algorithm can ` die - out ' ( or collapse ) , that is , that all of the samples have zero potentials . from an inference perspective , this is clearly an undesirable property and can lead to some poor performances ; for example , many particle filters display time - uniform convergence properties , which has yet to be shown for this class of particle filters . for some classes of examples ,",
    "e.g.  crou et al .",
    "( 2012 ) or del moral et al .",
    "( 2012 ) , there are some adaptive techniques which can reduce the possibility of the algorithm collapsing , but these are not always guaranteed to work in practice . in this article",
    "we develop a particle filter .",
    "this algorithm uses the same sampling mechanism , but the samples are generated until there is a prespecified number that are alive .",
    "this removes the possibility that the algorithm can collapse , but introduces a random cost per time - step .",
    "the algorithm turns out to be an important special case of the work in lee et al .",
    "( 2013 ) and is closely related to le gland & oudjane ( 2004 ) .    the particle filter is analyzed from a theoretical perspective .",
    "in particular , under assumptions , we establish the following results :    1 .",
    "time uniform @xmath24 bounds for the particle filter estimates of @xmath25 2 .",
    "a central limit theorem ( clt ) for suitably normalized and centered particle filter estimates of @xmath25 3 .",
    "an unbiased property of the particle filter estimates of @xmath26 4 .",
    "the relative variance of the particle filter estimates of @xmath26 , assuming @xmath27 , is shown to grow linearly in @xmath16 .    whilst all of these results are classical in the literature on particle filters ( crou et al .",
    "2011 ; del moral 2004 ) , the proof in this new context requires some modifications . in the main ,",
    "these technical adjustments are associated to @xmath28bounds and clts for sums of random variables with a random number of summands ( in the context of 1.-2 . ) .",
    "the technical results in 1.-2 .",
    "not only verify the correctness of the new algorithm , but suggest a substantial improvement over the standard particle filter , at the cost of increased computational time .",
    "the results in 3.-4 .",
    "are of particular interest when using the new particle filter within mcmc methodology ( a particle mcmc ( pmcmc ) algorithm , ( andrieu et al .",
    "there are variety of applications of such pmcmc algorithms , for example , when performing static parameter estimation for abc approximations of hmms .",
    "the results in 3.-4 .",
    "not only allow one to construct new pmcmc algorithms , but also provide theoretical guidelines for their implementation .",
    "it is remarked that some of these results can also be found in le gland & oudjane ( 2006 ) ( with regards to 1 .  2 . ) , except for _ a different estimate _ ; this is a critical difference between the work in this article and that in le gland & oudjane ( 2006 ) . in particular , as mentioned in result 3 .",
    "our estimates of @xmath26 , and in particular @xmath29 are unbiased and this allows one to develop principled mcmc methodology .",
    "we also note that , in le gland & oudjane ( 2006 ) the authors do not give a time - uniform bound .",
    "the structure of this article is as follows . in section [ sec : algo ]",
    "we provide a motivating example , abc approximations of hmms , for the construction of the particle filter , as well as the new particle filter itself . in section [ sec : theory ] our theoretical results are provided along with some interpretation of their meaning . in section [ sec : numerics ] we implement the new particle filter for the motivating example and then develop a basic pmcmc algorithm using the guidelines in section [ sec : theory ] for static parameter estimation associated to abc approximations of hmms . in section",
    "[ sec : summ ] the article is concluded , with some discussion of future work .",
    "the appendix contains technical results for the theory in section [ sec : theory ] and is split into three sections .",
    "we are given a hmm with observations @xmath30 , @xmath31 , hidden states @xmath32 , @xmath33 , @xmath34 given .",
    "we assume : @xmath35 and @xmath36 with @xmath37 a static parameter and @xmath38 lebesgue measure .",
    "we assume @xmath39 is unknown ( even up to an unbiased estimate ) , but one can sample from the associated distribution . in this scenario , one can not apply a standard particle filter ( or many other numerical approximation schemes ) .",
    "dean et al .  ( 2010 ) and jasra et al .  ( 2012 ) introduce the following abc approximation of the joint smoothing density , for @xmath40 : @xmath41 where @xmath42 and @xmath43 is the open ball centered at @xmath44 with radius @xmath45 .",
    "we let @xmath46 be fixed and omit it from our notations ; it is reintroduced later on .",
    "we introduce a feynman - kac representation of the abc approximation described above .",
    "let @xmath47 and define @xmath48 : @xmath49 now introduce markov kernels @xmath15 , @xmath50 $ ] ( @xmath51 are the borel sets ) , with @xmath52 with @xmath53 . then the abc predictor is for @xmath54 : @xmath55 where @xmath56 and @xmath57 = \\int_{\\mathsf{e}^{n } } \\prod_{p=1}^{n-1 } g_p(x_p)m_p(x_{p-1},dx_p ) m_n(x_{n-1},dx_n)\\varphi(x_n ) .",
    "\\label{eq : gamma_n_phi}\\ ] ] this provides a concrete example of the feynman - kac model in section [ sec : intro ] . in light of",
    ", we henceforth refer to @xmath29 as the _ normalizing constant_. this quantity is of fundamental importance in a wide variety of statistical applications , notably in static parameter estimation , as it is equivalent to the marginal likelihood of the observed data @xmath58 in contexts such as the abc approximation presented above , as can be determined from .",
    "now define , for @xmath59 : @xmath60 the standard particle filter works by sampling @xmath61 i.i.d from @xmath62 and setting @xmath63 at times @xmath59 sampling @xmath64 from @xmath65 , _ assuming that the system has not died out_.      we now discuss an idea which will prevent the particle filter from dying out ; see also lee et al .",
    "( 2013 ) and le gland & oudjane ( 2006 ) . throughout",
    "we assume that @xmath66 is not known for each @xmath67 ; if this is known , then one can develop alternative algorithms . at time 1",
    ", we sample @xmath68 i.i.d .  from @xmath62 , where @xmath69 then , define @xmath70 now , at time @xmath71 sample @xmath72 ,",
    "conditionally i.i.d .  from @xmath73 , where @xmath74 this is continued until needed ( i.e.  with an obvious definition of @xmath75 etc ) .",
    "the idea here is that , at every time step , we retain @xmath76 particles with non - zero weight , so that the algorithm never dies out , but with the additional issue that the computational cost per time - step is a random variable . the procedure is described in algorithm  [ alg : alive_pf ] .",
    "we note that the approach in le gland & oudjane ( 2004 ) retains @xmath77 alive particles , i.e. it differs only in step @xmath78 of algorithm  [ alg : alive_pf ] by sampling instead @xmath79 uniformly on @xmath80 .",
    "this seemingly innocuous difference is , however , crucial to the unbiasedness results we develop in the sequel .    1 .   at time 1 . for @xmath81 until @xmath82 is reached such that @xmath83 and @xmath84 : * sample @xmath85 from @xmath62 .",
    "2 .   at time @xmath86 . for @xmath81 until @xmath87 is reached such that @xmath88 and @xmath89 : 1 .",
    "sample @xmath79 uniformly from @xmath90 .",
    "sample @xmath91 from @xmath92 .",
    "we remark that one can show ( del moral , 2004 ) that for @xmath59 , the normalizing constant is given by @xmath93 thus , a natural estimate of the normalizing constant is @xmath94 we note that the estimates of @xmath95 and @xmath96 are different from those considered in le gland & oudjane ( 2006 ) .",
    "this is a critical point as in proposition [ prop : unbiased ] we show that this estimate of the normalizing constant is unbiased which is crucial for using this idea inside mcmc algorithms . in this direction",
    ", one uses the particle filter to help propose values and there is an accept / reject step ; we discuss this approach in section [ sec : pmcmc ] .",
    "again , it is clearly undesirable in an mcmc proposal , if the particle filter will collapse and so , our approach will prove to be very useful in this context .",
    "other than the fact that this filter will not die out , in the context of our motivating example , there is also a natural use of this idea .",
    "this is because , one can envisage the arrival of an outlier or unusual data ; in such scenarios , the alive particle filter will assign ( most likely ) more computational effort for dealing with this issue , which is not something that the standard filter is designed to do .    a final remark is as follows ; in our example @xmath97 and so , as assumed in this article in general , @xmath66 is not known for each @xmath67 .",
    "this removes the possibility of changing measure to @xmath98 ( in the formula for @xmath99 ) , ( with finite dimensional marginal @xmath100 ) @xmath101 call the markov kernels in the product @xmath102 .",
    "this is because the new potential at time @xmath16 is exactly : @xmath103 however , one can simulate from @xmath104 and use an unbiased estimate of @xmath66 for each particle .",
    "that is , we obtain samples @xmath105 from @xmath106 using @xmath107 samples ( total ) from @xmath108 and then we set @xmath109 ( say ) with associated weight @xmath110 .",
    "this particular procedure would then have a fixed number of particles with no possibility of collapsing .",
    "other than the algorithm being convoluted , some particles @xmath111 could be such that @xmath112 $ ] is prohibitively large , even though @xmath113 $ ] is not very large , which provides a reasonable argument against such a scheme .",
    "we will now present some theoretical results for the particle filter in section [ sec : new_smc ] .",
    "this section could be skipped with little loss of continuity in the article ; although we do provide numerical simulations to verify the behaviour that is predicted by the forthcoming theoretical results .",
    "define the following sequence of markov kernels , for @xmath54 : @xmath114 we will make use of the following assumptions :    * ( @xmath115 ) : for each there exist a @xmath116 such that for each @xmath54 , @xmath117 @xmath118 in addition there exist a @xmath119 such that for each @xmath54 , @xmath120 , @xmath121 . * ( @xmath122 ) : for each @xmath123 @xmath124 * ( @xmath125 ) : there exist @xmath126 and @xmath127 such that for any @xmath128 and @xmath129 @xmath130    the final two conditions are @xmath131 in crou et al .",
    "( 2011 ) ; we also use the notation @xmath132 .",
    "these assumptions are exceptionally strong , but we remark that for the scenario of interest , weaker conditions have not been used in the literature",
    ". note that in addition , in the context of abc , the assumptions are essentially qualitative as verifying them is very difficult ( even on compact state - spaces ) as the likelihood density is typically intractable .",
    "however , we still expect the phenomena reported in the below results to hold in some practical situations .",
    "we again remark that our results are relevant for scenarios other than abc .    in order to understand some of the subsequent results , we introduce some notations . for a probability measure on @xmath133 ( denoted @xmath134 ) @xmath135 and bounded measurable real - valued function ( denoted @xmath136 ) @xmath56 , we write @xmath137 . for @xmath56 , @xmath138 . for @xmath56 , @xmath139 . for @xmath140",
    ", @xmath141 denotes the total variation distance . for a non - negative operator on @xmath136 , @xmath142 , and @xmath56 , @xmath143 .",
    "iterates of @xmath107 are written @xmath144 .",
    "we will use the semi - group @xmath145 with @xmath59 , with the convention for @xmath146 , @xmath147 @xmath148 , where @xmath56 ; when @xmath149 , @xmath150 is the identity operator .",
    "we also adopt the notation for @xmath135 @xmath151 , @xmath56 , @xmath146 ; when @xmath149 , @xmath152 is the identity operator .",
    "@xmath153 denotes expectation w.r.t .  the stochastic process which generates the algorithm , with corresponding probability @xmath154 .",
    "it is assumed that @xmath155 .",
    "note the important formula @xmath156\\eta_n(\\varphi)$ ] , @xmath56 .",
    "@xmath157 denotes the normal distribution with mean @xmath158 and variance @xmath159 .",
    "@xmath160 denotes a geometric random variable ( with support @xmath161 ) with success probability @xmath162 .      in this section ,",
    "we consider the long - time behaviour of approximation of the prediction filter @xmath163 in particular , the study of this latter behaviour w.r.t .",
    "the algorithm in section [ sec : old_algo ] , is difficult due to the fact that the algorithm can collapse .",
    "for example , in a slightly different context , it is shown in del moral & doucet ( 2004 ) that the algorithm which can die out has an upper - bound on the @xmath28error which increases with @xmath16 ( and under strong hypotheses as in this article ) . in general , we do not know of any time - uniform result for algorithms which can die out .",
    "below , we restrict @xmath164 $ ] as this is all that is needed for a strong law of large numbers .",
    "the additional technical results associated to theorem [ theo : time_uniform ] can be found in appendix [ app : tech_pred ] .",
    "[ theo : time_uniform ] assume ( @xmath115 ) .",
    "then for any @xmath164 $ ] there exist a @xmath165 such that for any @xmath54 , @xmath166 , @xmath167 : @xmath168^{1/p } \\leq \\frac{c_p\\|\\varphi\\|_{\\infty}}{\\sqrt{n-1}}.\\ ] ]    throughout @xmath169 is a finite positive constant ( that does not depend upon @xmath16 ) whose value may change from line to line .",
    "the proof follows that of theorem 7.4.4 of del moral ( 2004 ) .",
    "we have , using eq .  7.24 of del moral ( 2004 ) @xmath168^{1/p } \\leq \\sum_{q=1}^n\\mathbb{e}[|[\\phi_{q , n}(\\eta_q^{t_q } ) - \\phi_{q , n}(\\phi_q(\\eta_{q-1}^{t_{q-1}}))](\\varphi)|^{p}]^{1/p}.\\ ] ]",
    "wlog we suppose @xmath170 .",
    "now for @xmath171 we define the markov kernel @xmath172 with associated dobrushin coefficient @xmath173 and also set @xmath174 + @xmath175 . then following the calculations of pp.245246 of del moral ( 2004 )",
    ", we have @xmath168^{1/p } \\leq \\sum_{q=1}^n r_{q , n}\\beta(p_{q , n } ) \\mathbb{e}[|[\\eta_q^{t_q } - \\phi_q(\\eta_{q-1}^{t_{q-1 } } ) ] ( \\bar{q}_{q , n}^n(\\varphi))|^p]^{1/p}.\\ ] ] where @xmath176 is defined in pp .",
    "246 of del moral ( 2004 ) and note that @xmath177 .",
    "application of corollary [ cor : cond_lp ] gives : @xmath168^{1/p } \\leq \\frac{c_p}{\\sqrt{n-1 } } \\sum_{q=1}^n r_{q , n}\\beta(p_{q , n}).\\ ] ] the sum on the r.h.s .",
    "can be bounded uniformly in @xmath16 by using standard arguments in crou et al .",
    "( 2011 ) or del moral ( 2004 ) and are hence omitted .",
    "this concludes the proof .      in this section",
    "we consider the asymptotic properties of a suitably normalized and centered estimate of the predictor ; a central limit theorem .",
    "we note that such a result is not a direct corollary of existing clts for particle filters in the literature ( e.g.  del moral ( 2004 ) ) .",
    "the additional technical results associated to theorem [ theo : clt ] can be found in appendix [ app : tech_clt ] .",
    "here we write : @xmath178 the empirical measure of the first @xmath76 sampled particles at time @xmath16 . the convergence in probability ( written @xmath179 ) weak convergence ( written @xmath180 ) results are as @xmath181 .",
    "[ theo : clt ] assume ( @xmath115 ) .",
    "then for any @xmath54 , @xmath10 we have : @xmath182(\\varphi ) \\rightarrow \\mathcal{n}(0,\\sigma^2_n(\\varphi))\\ ] ] where , setting @xmath183 @xmath184/[\\eta_n(g_n)\\eta_{n-1}(g_{n-1 } ) ] \\quad n\\geq 2\\\\ \\sigma^2_1(\\varphi ) & = & \\eta_1(g_1 ) \\eta_1((\\varphi-\\eta_1(\\varphi))^2)\\end{aligned}\\ ] ] or equivalently for any @xmath54 @xmath185 ^ 2 ) .",
    "\\label{eq : clt_asymp_var } \\ ] ]    our proof proceeds via induction . for the case",
    "@xmath186 , by lemma [ lem : clt_main_random_to_deterministic ] we need only deal with the term @xmath187(\\varphi).\\ ] ] then one need only apply the clt for i.i.d",
    ".  bounded random variables ; this yields the result with @xmath188    we assume the result for @xmath189 and consider @xmath16 .",
    "let @xmath190 then we have @xmath182(\\varphi ) =   \\sqrt{t_n-1 } [ \\eta_{n}^{t_n } -\\phi_n(\\eta_{n-1}^{t_{n-1 } } ) ] ( \\varphi_n )   + \\sqrt{t_n-1}\\phi_n(\\eta_{n-1}^{t_{n-1 } } ) ] ( \\varphi_n ) .",
    "\\label{eq : clt_prf1}\\ ] ] now the first term on the r.h.s .",
    "of can be written as @xmath191(\\varphi_n )   & = & \\sqrt{t_n-1 } [ \\eta_{n}^{t_n } -\\phi_n(\\eta_{n-1}^{t_{n-1 } } ) ] ( \\varphi_n ) -    \\sqrt{(n-1)\\eta_n(g_n ) } [ \\eta_{n}^{n-1 } -\\phi_n(\\eta_{n-1}^{t_{n-1 } } ) ] ( \\varphi_n ) \\nonumber \\\\ & & + \\sqrt{(n-1)\\eta_n(g_n ) } [ \\eta_{n}^{n-1 } -\\phi_n(\\eta_{n-1}^{t_{n-1 } } ) ] ( \\varphi_n ) .",
    "\\label{eq : clt_prf2}\\end{aligned}\\ ] ] in addition , the second term on the r.h.s .",
    "of can be written as @xmath192(\\varphi_n ) & = & \\bigg[\\sqrt{\\frac{t_n-1}{t_{n-1}-1}}- \\sqrt{\\frac{\\eta_{n-1}(g_{n-1})}{\\eta_{n}(g_{n})}}\\bigg]\\sqrt{t_{n-1}-1}\\phi_n(\\eta_{n-1}^{t_{n-1 } } ) ] ( \\varphi_n ) \\nonumber \\\\ & &   + \\sqrt{t_{n-1}-1}\\sqrt{\\frac{\\eta_{n-1}(g_{n-1})}{\\eta_{n}(g_{n})}}\\phi_n(\\eta_{n-1}^{t_{n-1 } } ) ] ( \\varphi_n ) \\label{eq : clt_prf3}\\end{aligned}\\ ] ] by lemma [ lem : clt_main_random_to_deterministic ] the first term on the r.h.s .  of converges in probability to zero .",
    "also , by lemma [ lem : conv_t_n ] , theorem [ theo : time_uniform ] ( which provides a strong law of large numbers ) and the induction hypothesis ( @xmath193 ) , the first term on the r.h.s .  of converges in probability to zero .",
    "thus , by a corollary to slutsky s theorem , we can consider the weak convergence of @xmath194(\\varphi_n ) +   \\sqrt{\\frac{\\eta_{n-1}(g_{n-1})}{\\eta_{n}(g_{n})}}\\sqrt{t_{n-1}-1}\\phi_n(\\eta_{n-1}^{t_{n-1 } } ) ] ( \\varphi_n ) : = a(n ) + b(n).\\ ] ] we now consider the characteristic function : @xmath195 = \\mathbb{e}[\\{\\mathbb{e}[\\exp\\{ita(n)\\}|\\mathscr{f}_{n-1}]-e^{-\\tilde{\\sigma}_n^2(\\varphi_n ) t^2/2}\\}\\exp\\{itb(n)\\ } ] +   e^{-\\tilde{\\sigma}_n^2(\\varphi_n ) t^2/2 } \\mathbb{e}[\\exp\\{itb(n)\\ } ] \\label{eq : clt_prf4}\\ ] ] where @xmath196 and @xmath197 is the filtration generated by the particle system up - to time @xmath189 .",
    "we deal with the limit of the expectations on the r.h.s .  of independently .",
    "we will show that @xmath198-e^{-\\tilde{\\sigma}_n^2(\\varphi_n ) t^2/2}$ ] will converge in probability to zero , by using theorem a3 of douc & moulines ( 2008 ) . to that end",
    ", we note that for any @xmath10 , we have that @xmath199\\ ] ] will converge in probability to zero ( for example , by controlling the second moment with the marcinkiewicz - zygmund ( m - z ) inequality ) . then by theorem [ theo : time_uniform ] as @xmath200 converges almost surely to @xmath25 that @xmath201 will converge in probability to @xmath25 . using this result",
    "it follows easily that @xmath202 ^ 2\\ ] ] converges in probability to @xmath203 .",
    "this verifies the first condition of theorem a3 of douc & moulines ( 2008 ) ( eq .",
    "( 31 ) of that paper ) . as @xmath204 is bounded , it is straightforward to verify the second ( lindeberg - type ) condition of theorem 13 of douc & moulines ( 2008 ) ( eq .",
    "( 32 ) of that paper ) .",
    "thus , application of this latter theorem shows that @xmath198-e^{-\\tilde{\\sigma}_n^2(\\varphi_n ) t^2/2}$ ] converges in probability to zero .",
    "then by the induction hypothesis @xmath205/[\\eta_n(g_n)\\eta_{n-1}(g_{n-1})])\\label{eq : clt_prf5}.\\ ] ] thus @xmath206-e^{-\\tilde{\\sigma}_n^2(\\varphi_n ) t^2/2}\\}\\exp\\{itb(n)\\ } \\rightarrow_{\\mathbb{p } } 0.\\ ] ] application of theorem 25.12 of billingsley ( 1995 ) , shows that @xmath207-e^{-\\tilde{\\sigma}_n^2(\\varphi_n ) t^2/2}\\}\\exp\\{itb(n)\\ } ] = 0.\\ ] ] thus , returning to , we consider @xmath208 $ ] . noting and again applying theorem 25.12 of billingsley ( 1995 ) we yield @xmath209 = e^{-[\\sigma^2_{n-1}(q_n(\\varphi_n))]/[\\eta_n(g_n)\\eta_{n-1}(g_{n-1 } ) ] t^2/2}.\\ ] ] thus , we have proved that @xmath182(\\varphi ) \\rightarrow \\mathcal{n}(0,\\sigma^2_n(\\varphi))\\ ] ] where @xmath210/[\\eta_n(g_n)\\eta_{n-1}(g_{n-1})].\\ ] ] the verification of the formula for the asymptotic variance follows standard calculations and is omitted .",
    "[ rem : clt ] the formula for the asymptotic variance of the particle filter in section [ sec : old_algo ] , pp .",
    "304 of del moral ( 2004 ) is @xmath211 ^ 2 ) .",
    "\\label{eq : old_asymp_var}\\ ] ] comparing to the asymptotic variance formula , this latter formula is certainly smaller if for each @xmath212 @xmath213 an alternative interpretation of is if @xmath214 is a markov chain with transition kernels @xmath15 then is @xmath215 whilst this can be difficult to verify in general , if the spaces are @xmath216 , @xmath54 , potentials @xmath217 for each @xmath54 and the markov kernels are such that for each @xmath54 , @xmath218 , @xmath219 for some @xmath220 , then the l.h.s .",
    "of is @xmath221 and the r.h.s .",
    "is @xmath222 ; so in this ideal scenario , the new algorithm asymptotically outperforms the old one with regards to variance . in general",
    ", one might believe that is smaller that the formula , as its leading term ( when @xmath223 ) is smaller and the condition can certainly hold in many examples .",
    "one can also prove a clt for the estimate of the filter ; a direct corollary is , under ( @xmath115 ) , using theorem [ theo : clt ] , we have @xmath224 \\rightarrow\\mathcal{n}(0,\\tilde{\\sigma}^2_n(\\varphi))\\ ] ] where @xmath225)-\\eta_q(q_{q , n}(g_n[\\varphi_n-\\eta_n(g_n\\varphi)]))]^2).\\ ] ] in comparison , the asymptotic variance of the estimate in theorem 4 of le gland & oudjane ( 2006)(which differs to the one in this article ) has asymptotic variance @xmath226)-\\eta_q(q_{q , n}(g_n[\\varphi_n-\\eta_n(g_n\\varphi)]))]^2).\\ ] ] thus there is an asymptotic difference between the two procedures . in general",
    ", our approach is better with regards to asymptotic variance if @xmath227 or using the markov chain interpretation in remark [ rem : clt ] : @xmath228 in general , one can not say which is preferable , but in the case : the spaces are @xmath216 , @xmath54 , potentials @xmath217 for each @xmath54 and the markov kernels are such that for each @xmath54 , @xmath218 , @xmath219 for some @xmath220 , both the l.h.s .  and r.h.s .  of the inequality",
    "are equal .",
    "define the estimate of the normalizing constant : @xmath229 the technical results used in this section can be found in appendix [ app : tech_nc ] .",
    "[ prop : unbiased ] we have for any @xmath54 , @xmath166 and @xmath10 , that @xmath230 = \\gamma_n(\\varphi).\\ ] ]    the proof uses the standard martingale difference decomposition in del moral ( 2004 ) , with some additional expectation properties that need to be proved . the case @xmath186 follows from lemma [ lem : tech_res ] , so we assume @xmath59 .",
    "we remark that for @xmath231 : @xmath232 and hence that @xmath233(q_{p , n}(\\varphi)).\\ ] ] then by lemma [ lem : tech_res ] , it follows that @xmath234(q_{p , n}(\\varphi))|\\mathscr{f}_{p-1 } ] = 0\\ ] ] and hence that @xmath235 = 0\\ ] ] from which we easily conclude the result .",
    "below the term @xmath236 is as in crou et al .",
    "the expressions and interpretations for @xmath237 can be found in section [ sec : assump ] .",
    "in addition , @xmath238 is the @xmath239statistic that is formed from our empirical measure @xmath240 and @xmath241 is the corresponding @xmath242statistic .",
    "in addition @xmath243 for @xmath244 .",
    "[ prop : non_asymp ] assume ( @xmath245 ) .",
    "then for any @xmath59 , @xmath246 @xmath247 \\leq \\frac{4}{n } \\sum_{s=1}^n \\frac{\\tilde{\\delta}_s\\hat{\\delta}_s^{(m)}\\hat{\\beta}_s^{(m)}}{\\eta_s(g_s)}.\\ ] ]    the result follows essentially from crou et al .",
    "( 2011 ) . to modify the proof to our set - up",
    ", we will prove that for @xmath248 ( where the expectation on the l.h.s .",
    "is w.r.t .  the stochastic process that generates the smc algorithm ) @xmath249",
    "\\leq \\big(\\frac{n-1}{n-2}\\big)^n \\mathbb{e}_{\\xi}[\\eta_1^{\\otimes 2 } c_{\\xi_1}q_2^{\\otimes 2 } c_{\\xi_2}\\dots q_n^{\\otimes 2}c_{\\xi_n}(f)]\\label{eq : fund_ineq}\\ ] ] where for each @xmath54 , independently @xmath250 with corresponding joint expectation @xmath251 and @xmath252 , @xmath253 .",
    "once is proved this gives a verification of lemma 3.2 , eq .",
    "( 3.3 ) of crou et al .",
    "( 2011 ) , given this , the rest of the argument then follows proposition 3.4 of crou et al .",
    "( 2011 ) and theorem 5.1 and corollary 5.2 in crou et al .",
    "( 2011 ) ( note that the fact that we have an upper - bound with @xmath254 ( as in crou et al .",
    "( 2011 ) ) does not modify the result ) .",
    "we will write expectations w.r.t .",
    "the probability space associated to the particle system enlarged with the ( independent ) @xmath255 as @xmath256 .",
    "thus , we consider the proof of .",
    "we have @xmath257   =   \\gamma_{n}^{t_n}(1)^2\\mathbb{e}[(\\eta_n^{t_n})^{\\otimes 2}(f)|\\mathscr{f}_{n-1}].\\ ] ] now @xmath258 & = & \\mathbb{e}\\big[\\frac{t_n-2}{t_n-1}(\\eta_n^{t_n})^{\\odot 2}(f ) + \\frac{1}{t_n-1 } \\eta_n^{t_n}(c(f))\\big|\\mathscr{f}_{n-1}\\big]\\\\ & \\leq & \\phi_n(\\eta_{n-1}^{t_{n-1}})^{\\otimes",
    "2}(f ) + \\frac{1}{n-1 } \\phi_n(\\eta_{n-1}^{t_{n-1}})(c(f ) ) \\\\ & \\leq & \\big(\\frac{n-1}{n-2}\\big)\\overline{\\mathbb{e}}_{\\xi}[\\phi_n(\\eta_{n-1}^{t_{n-1}})^{\\otimes 2}(c_{\\xi_n}(f))|\\mathscr{f}_{n-1}]\\end{aligned}\\ ] ] where we have used @xmath259 , @xmath260 and lemmas [ lem : tech_lem ] and [ lem : tech_res ] to obtain the second line .",
    "thus we have that @xmath261 & \\leq &   \\gamma_{n}^{t_n}(1)^2 \\big(\\frac{n-1}{n-2}\\big ) \\overline{\\mathbb{e}}_{\\xi}[\\phi_n(\\eta_{n-1}^{t_{n-1}})^{\\otimes 2}(c_{\\xi_n}(f))|\\mathscr{f}_{n-1 } ] \\\\ & \\leq &   \\gamma_{n-1}^{t_{n-1}}(1)^2 \\big(\\frac{n-1}{n-2}\\big ) \\overline{\\mathbb{e}}_{\\xi}[(\\eta_{n-1}^{t_{n-1}})^{\\otimes 2}(q_n c_{\\xi_n}(f))|\\mathscr{f}_{n-1}].\\end{aligned}\\ ] ] using the above inequality , one can repeat the argument inductively to deduce .",
    "this completes the proof of the proposition .",
    "the significance of the result is simply that if @xmath262 then if @xmath263 the relative variance will be constant in @xmath16",
    ". this will be useful for the pmcmc algorithm in section [ sec : pmcmc ] .",
    "to investigate the alive particle filter , we consider the following linear gaussian state space model ( with all quantities one - dimensional ) : @xmath264 where @xmath265 , and independently @xmath266 .",
    "our objective is to fit an abc approximation of this hmm ; this is simply to investigate the algorithm constructed in this article .",
    "data are simulated from the ( true ) model for @xmath267 time steps and @xmath268 and @xmath269 . for @xmath270 , if @xmath271 , where @xmath272}$ ] ( the uniform distribution on @xmath273 $ ] ) , we have @xmath274 , where @xmath275 . recall @xmath276 and we consider a fixed sequence of @xmath45 which values belong to set \\{5 , 10 , 15 } , i.e. @xmath277 .",
    "we compare the alive particle filter to the approach in jasra et al .",
    "( 2012 ) .",
    "the proposal dynamics are as described in section [ sec : new_smc ] . for the approach in jasra et al .",
    "( 2012 ) , @xmath278 and we resample every time . for the alive particle filter",
    ", we used @xmath279 particles ; this is to keep the computation time approximately equal .",
    "we also estimate the normalizing constant via the alive filter at each time step and compare it with ` exact ' values obtained via the kalman filter in the limiting case @xmath280 . to assess the performance in normalizing constant estimation , the relative variance is estimated via independent runs of the procedure .",
    "our results are constructed as two parts . in the first part",
    ", we compare the performance of two particle filters under different scenarios . in the second part",
    ", we focus on examples where the approach in jasra et al .",
    "( 2012 ) collapses .",
    "all results were averaged over @xmath281 runs .",
    "we note that , with regards to the results in this section and the approach in le gland & oudjane ( 2004 ) ; generally similar conclusions can be drawn with regards to comparison to the approach in jasra et al .",
    "( 2012 ) .",
    "in this part , the analyses of the alive particle filter were completed in approximately 115 seconds and approximately 103 seconds were taken for the approach in jasra et al .",
    "( 2012 ) ( which we just term the particle filter ) .",
    "our results are shown in figures [ fig : fig1]-[fig : fig6 ] .",
    "figure [ fig : fig1 ] displays the log relative error for the alive filter to the particle filter .",
    "we present the time evolution of the @xmath282 log relative error between the ` exact ' and estimated first moment . from our results , the mean log relative error for each panel is @xmath283 .",
    "figure [ fig : fig2 ] plots the absolute @xmath282 error of the alive particle filter error across time .",
    "these results indicate , in the scenarios under study , that both filters are performing about the same time with regards to estimating the filter .",
    "this is unsurprising as both methods use essentially the same information , and the outlying values do not lead to a collapse of the particle filter .",
    "in addition , the behaviour in figure [ fig : fig2 ] , which is predicted in theorem [ theo : time_uniform ] under strong assumptions , appears to hold in a situation where the state - space is non - compact .    in figure [ fig : fig3 ] , we show the time evolution of the log of the normalizing constant estimate for three approaches , i.e. kalman filter ( black `  ' line ) , new abc filter ( red ` -@xmath284- ' line ) and smc method ( blue ` @xmath285 ' line ) .",
    "figure [ fig : fig4 ] displays the ( log ) relative variance of the estimate of the normalizing constant via the alive particle filter , when using the kalman filter as the ground truth . in figure",
    "[ fig : fig3 ] , there is unsurprisingly a bias in estimation of the normalizing constant , as the abc approximation is not exact , i.e. @xmath286 . in figure",
    "[ fig : fig4 ] the linear decay in variance proven in proposition [ prop : non_asymp ] is demonstrated ( although under a log transformation ) .    in figure",
    "[ fig : fig5 ] and [ fig : fig6 ] , we show the number of particles used at each time step ( that is to achieve @xmath77 alive particles ) of the alive filter ( figure [ fig : fig5 ] ) and the number of alive particles for the standard particle filter ( figure [ fig : fig6 ] ) .",
    "both figures illustrate the effect of outlying data , where the alive filter has to work ` harder ' ( i.e.  assigns more computational effort ) , whereas the standard filter just loses particles .      in this part , we keep the initial conditions the same as in the previous section but change the value of @xmath45 . instead of using @xmath277 ,",
    "we set smaller values to @xmath45 , i.e.  @xmath287 ( recall the smaller @xmath45 , the closer the abc approximation is to the true hmm ( theorem 1 of jasra et al .",
    "( 2012 ) ) .",
    "this change makes the standard particle filter collapse whereas the alive filter does not have this problem .",
    "all results were averaged over @xmath281 runs and our results are shown in figures [ fig : fig7]-[fig : fig8 ] .    in figure [",
    "fig : fig7 ] , we present the true simulated hidden trajectory along with a plot of the estimated @xmath288 given by the two particle filters across time when @xmath289 .",
    "as shown in figure [ fig : fig7 ] , the alive filter can provide better estimation versus the old particle filter .",
    "figure [ fig : fig8 ] displays the log relative error of the alive filter to old particle filter , which supports the previous point made , with regards to estimation of the hidden state .",
    "based upon the results displayed , the alive filter can provide good estimation results under the same conditions when the old particle filter collapses .",
    "we now utilize the results in propositions [ prop : unbiased]-[prop : non_asymp ] .",
    "in particular , proposition [ prop : unbiased ] allows us to construct an mcmc method for performing static parameter inference in the context of abc approximations of hmms .    recall section [ sec : motivating_ex ] .",
    "our objective is to sample from the posterior density : @xmath290 where @xmath291 , @xmath292 is as and @xmath293 is a prior probability density on @xmath294 . throughout the section , we set @xmath166 , @xmath295 , but",
    "in general omit dependencies on these quantities . in practice ,",
    "one often seeks to sample from an associated probability on the extended state - space @xmath296 @xmath297 it is then easily verified that for any fixed @xmath37 @xmath298    a typical way to sample from @xmath299 is via the metropolis - hastings method , with proposing to move from @xmath300 to @xmath301 via the probability density : @xmath302 such a proposal removes the need to evaluate @xmath303 which is not available in this context . as is well known e.g.  andrieu et al .",
    "( 2010 ) , such procedures typically do not work very well and lead to slow mixing on the parameter space @xmath294 .",
    "this proposal can be greatly improved by running a particle - filter ( the particle marginal metropolis - hastings ( pmmh ) algorithm ) as in andrieu et al .",
    "( 2010 ) ; that is a metropolis - hastings move that will first move @xmath46 , via @xmath304 and then run the algorithm in section [ sec : old_algo ] picking a whole path , @xmath305 , @xmath306 the sample used with a probability proportional to @xmath307 .",
    "remarkably , this procedure yields samples from via an auxiliary probability density ; the details can be found in andrieu et al .",
    "( 2010 ) , but the apparently _ fundamental _ property is that the estimate of the normalizing constant is unbiased .",
    "note also that the sample from the markov chain @xmath308 also provides a sample from @xmath299 .    as we have seen in the context of both theory and applications",
    ", it appears that the alive filter in section [ sec : new_smc ] out - performs the standard one , for a given computational complexity .",
    "in addition , as seen in proposition [ prop : unbiased ] , the estimate of the normalizing constant is unbiased .",
    "it is therefore a reasonable conjecture that one can construct a new pmmh algorithm , with the alive particle filter investigated previously in this article and that this might perform better ( in some sense ) than the standard pmmh just described .",
    "we remark that the justification of this new pmmh follows from the statements in andrieu & vihola ( 2012 ) ( see also andrieu & roberts ( 2009 ) ) and proposition [ prop : unbiased ] , but we provide details for completeness .",
    "we will define an appropriate target probability to produce samples from , but we first give the algorithm :    1 .",
    "sample @xmath309 from any absolutely continuous distribution .",
    "then run the particle filter ( with parameter value @xmath309 ) in section [ sec : new_smc ] up - to time @xmath16 , storing @xmath310 ( now denoted @xmath311 ) . pick a trajectory @xmath312 , @xmath313 , with probability @xmath314 set @xmath315 .",
    "2 .   propose @xmath316 from a proposal with positive density on @xmath294 ( write it @xmath304).then run the particle filter ( with parameter value @xmath317 ) in section [ sec : new_smc ] up - to time @xmath16 , storing @xmath318 . pick a trajectory @xmath319 with probability @xmath320 set @xmath321 ,",
    "@xmath322 with probability : @xmath323 otherwise set @xmath324 , @xmath325 , @xmath326 and return to the start of 2 .    for readers interested in the numerical implementation",
    ", they can skip to the next section , noting that the @xmath46 samples will come from the posterior ; this is now justified in the rest of the section .",
    "we construct the following auxiliary target probability on the state - space : @xmath327 whilst the state - space looks complicated it corresponds to the static parameter and all the variables ( the states and the resampled indices ) sampled by the alive particle filter up - to time - step @xmath16 and then just the picking of one of the final paths .    for @xmath59",
    "( we omit @xmath46 from our notation ) define @xmath328 @xmath329 } \\ ] ] where for @xmath54 @xmath330 in addition , set @xmath331 . }",
    "\\ ] ]    then the pmmh algorithm just defined samples from the target @xmath332 where @xmath333 , @xmath334 and @xmath335 . using proposition [ prop : unbiased ]",
    ", one can easily verify that for any fixed @xmath37 @xmath336 note also that the samples @xmath308 from @xmath337 are marginally distributed according to @xmath299 .",
    "the associated ergodicity of the new pmmh algorithm follows the construction in andrieu et al .",
    "( 2010 ) and we omit details for brevity .",
    "we consider the following state - space model , for @xmath54 @xmath338 where @xmath339 ( a stable distribution with location parameter 0 , scale @xmath340 , skewness parameter @xmath341 and stability parameter @xmath342 ) and @xmath343 .",
    "we set @xmath344 , with priors @xmath345 , @xmath346 ( @xmath347 is an inverse gamma distribution with mode @xmath348 ) and @xmath349 .",
    "note that the inverse gamma distributions have infinite variance .",
    "we consider the daily ( adjust closing ) index of the s & p 500 index between 03/01/2011 @xmath350 14/02/2013 ( 533 data points ) .",
    "our data are the log - returns , that is , if @xmath351 is the index value at time @xmath16 , @xmath352 .",
    "the data are displayed in figure [ fig : ex3sp500 ] .",
    "the stable distribution may help us to more realistically capture heavy tails prevalent in financial data , than perhaps a standard gaussian .",
    "in most scenarios , the probability density function of a stable distribution is intractable , which suggests that an abc approximation might be a sensitive way to approximate the true model .",
    "we consider two scenarios to compare the standard pmmh algorithm and the new one developed above . in the first situation we set @xmath353 and in the second , @xmath354 , with @xmath355 in both situations . in the first case ,",
    "we make @xmath45 a suitable value as the data are not expected to jump off the same scale as the initial data . in the second",
    ", @xmath45 is significantly reduced ; this is to illustrate a point about the algorithm we introduce .",
    "both algorithms are run for about the same computational time , such that the new pmmh algorithm has 20000 iterations .",
    "the parameters are initialized with draws from the priors .",
    "the proposal on @xmath356 is a normal random walk and for @xmath357 a gamma proposal centered at the current point with proposal variance scaled to obtain reasonable acceptance rates .",
    "we consider @xmath358 and for the new pmmh algorithm this value is lower to allow the same computational time .",
    "our results are presented in figures [ fig : traceplot_old]-[fig : traceplot_col_new ] . in figures [ fig : traceplot_old]-[fig : traceplot_new ] we can see the output in the case that @xmath353 . for all cases , it appears that both algorithms perform very well ; the acceptance rates were around 0.25 for each case .",
    "for the pmmh algorithm the average number of simulations of the data , per - iteration and data - point , were @xmath359 for @xmath360 respectively ( recall we have modified @xmath77 to make the computational time similar to the standard pmmh ) . for",
    "this scenario one would prefer the standard pmmh as the algorithmic performance is very good , with a removal of a random computation cost per iteration .    in figures [ fig : traceplot_col_old]-[fig :",
    "traceplot_col_new ] the output when @xmath361 is displayed . in figure",
    "[ fig : traceplot_col_old ] we can see that the standard pmmh algorithm performs very badly , barely moving across the parameter space , whereas the new pmmh algorithm has very reasonable performance ( figure [ fig : traceplot_col_new ] ) . in this case , @xmath45 is very small , and the standard smc collapses very often , which leads to the undesirable performance displayed .",
    "we note that considerable effort was expended in trying to get the standard pmmh algorithm to work in this case , but we did not manage to do so ( so we do not claim that the algorithm can not be made to work ) .",
    "note also that whilst these are just one run of the algorithms , we have seen this behaviour in many other cases and it is typical in these examples .",
    "the results here suggest that the new pmmh kernel might be preferred in difficult sampling scenarios , but in simple cases it does not seem to be required .",
    "in this article we have investigated the alive particle filter ; we developed and analyzed new particle estimates and derived new and principled mcmc algorithms .",
    "there are several extensions to the work in this article .",
    "firstly , we have presented and analyzed the most standard particle filter ; one can investigate more intricate filters commensurate with the current state of the art .",
    "secondly , our theoretical results appear to hold under much weaker conditions than adopted ( section [ sec : linear_gaussian ] ) ; one could extend the results in this direction .",
    "thirdly , we have presented the most basic pmcmc algorithm ; one can extend to particle gibbs methods and beyond . finally , one can also use the smc theory in this article to interact with that of mcmc theory to investigate the performance of our pmcmc procedures .",
    "the first author was supported by an moe singpore grant .",
    "we thank gareth peters for useful conversations on this work .",
    "the main result of this section is below .",
    "note we use the convention @xmath362 and recall @xmath363 is the filtration generated by the particle system up - to time @xmath16 .",
    "[ cor : cond_lp ] assume ( @xmath115 ) .",
    "then for any @xmath164 $ ] there exists a @xmath165 such that for any @xmath54 , @xmath166 and @xmath10 @xmath364(\\varphi)|^p|\\mathscr{f}_{n-1}]^{1/p } \\leq \\frac{c_p\\|\\varphi\\|}{\\sqrt{n-1 } } \\quad \\mathbb{p}-a.s .. \\ ] ]    the case @xmath186 follows directly from lemma [ lem : tech_lem_neg1 ] and ( @xmath115 ) , so we consider @xmath59 . for @xmath59 , by lemma [ lem : tech_res ] @xmath365 = \\phi_n(\\eta_{n-1}^{t_{n-1}})(\\varphi)$ ] , so conditional upon @xmath197 we are in the setting of lemma [ lem : tech_lem_neg1 ] . by ( @xmath115 )",
    "we can verify that @xmath366 for some deterministic constant @xmath367 and hence application of lemma [ lem : tech_lem_neg1 ] proves the result .",
    "in the following section let @xmath368 be a measurable space and @xmath369 be i.i.d .",
    "random variables on @xmath370 associated to @xmath371 .",
    "let @xmath372 be such that @xmath373 for some @xmath367 .",
    "let @xmath166 and define @xmath374 note that @xmath375 is a negative binomial random variable , with parameters @xmath77 and success probability @xmath376 .",
    "we will consider some @xmath28properties of @xmath377 with @xmath378 .",
    "expectations are written as @xmath153 .",
    "note that one can follow the proof of lemma [ lem : tech_res ] to show that @xmath379 = \\nu(\\varphi).\\ ] ] we then have the following technical results which are used in the main text .",
    "[ lem : tech_lem_neg1 ] for any @xmath164 $ ] there exist a @xmath165 such that for any , @xmath166 , and @xmath378 @xmath380\\big|^p\\bigg]^{1/p } \\leq \\frac{c_p\\|\\varphi\\|_{\\infty}}{\\sqrt{n-1}}.\\ ] ]    throughout @xmath169 is a finite positive constant ( that only depends upon @xmath162 ) whose value may change from line to line .",
    "wlog we will assume that @xmath381 . by the minkowski inequality @xmath382^{1/p } & \\leq & \\mathbb{e}\\bigg[\\big|   \\frac{1}{t-1}\\sum_{i=1}^{t-1 } \\varphi(x^i)-\\frac{n-1}{t-1}\\frac{\\nu(\\mathbb{i}_b\\varphi)}{\\nu(b ) } - \\frac{t - n}{t-1}\\frac{\\nu(\\mathbb{i}_{b^c}\\varphi)}{\\nu(b^c ) } \\big|^p\\bigg]^{1/p } + \\nonumber\\\\ & & \\mathbb{e}\\bigg[\\big|\\frac{n-1}{t-1}\\frac{\\nu(\\mathbb{i}_b\\varphi)}{\\nu(b ) }",
    "+ \\frac{t - n}{t-1}\\frac{\\nu(\\mathbb{i}_{b^c}\\varphi)}{\\nu(b^c)}\\big|^p\\bigg]^{1/p }",
    "\\label{eq : minkow_app_neg1}.\\end{aligned}\\ ] ] lemma [ lem : tech_lem_neg2 ] will control the second term on the r.h.s .",
    "so we focus on the first term on the r.h.s ..    we have @xmath383^{1/p } = \\ ] ] @xmath384 +   \\frac{1}{t-1}\\sum_{i=1}^{t-1 } \\mathbb{i}_{b^c}(x^i)\\big[\\varphi(x^i)-\\frac{\\nu(\\mathbb{i}_{b^c}\\varphi)}{\\nu(b^c)}\\big ] \\big|^p\\bigg]^{1/p}.\\ ] ] now conditioning upon @xmath375 ( so that @xmath76 samples lie in @xmath385 and @xmath386 lie in @xmath387 and we subtract the conditional expectations of the ( conditionally ) independent random variables ) and applying an appropriately modified version of the m - z inequality ( e.g.  chapter 7 of del moral ( 2004 ) ) we have @xmath383^{1/p } \\leq\\ ] ] @xmath388^{1/p}\\ ] ] where we are using the conditional distribution of @xmath389 , given @xmath375 . setting @xmath390",
    "we have that @xmath383^{1/p } \\leq c_p \\bar{c}^{1/p}\\mathbb{e}\\bigg[\\frac{1}{(t-1)^{p/2}}\\big(1+\\frac{t - n}{t-1}\\big)\\bigg]^{1/p}\\ ] ] and then noting @xmath391 , @xmath392 and using @xmath393 for dealing with @xmath394 we have proved that @xmath383^{1/p } \\leq \\frac{c_p\\|\\varphi\\|_{\\infty}}{\\sqrt{n-1}}.\\ ] ] returning to and using lemma [ lem : tech_lem_neg2 ] , along with above result allows us to complete the proof .",
    "[ lem : tech_lem_neg2 ] for any @xmath164 $ ] there exist a @xmath165 such that for any , @xmath166 , and @xmath378 @xmath395^{1/p } \\leq   \\frac{c_p\\|\\varphi\\|_{\\infty}}{\\sqrt{n}}.\\ ] ]    throughout @xmath169 is a finite positive constant ( that only depends upon @xmath162 ) whose value may change from line to line .",
    "wlog we will assume that @xmath381 , so that @xmath396",
    ". then we have that @xmath397.\\end{aligned}\\ ] ] now , via minkowski @xmath398^{1/p } & \\leq & \\frac{|\\nu(\\mathbb{i}_b\\varphi)|n}{\\nu(b)(1-\\nu(b ) ) } \\big\\{\\mathbb{e}\\bigg[\\big(\\frac{1}{(t-1)^p}\\big)\\big|1-\\frac{t\\nu(b)}{n}\\big|^p\\bigg]^{1/p } + \\mathbb{e}\\bigg[\\big|\\frac{\\nu(b)-1}{n(t-1)}\\big|^p\\bigg]^{1/p } \\big\\ } \\label{eq : minkow_app_neg}\\end{aligned}\\ ] ] as @xmath391 , we will focus on controlling the term @xmath399^{1/p}.\\ ] ] if @xmath400 are independent @xmath401 random variables then @xmath399^{1/p } = \\nu(b)\\mathbb{e}\\bigg[\\big|\\frac{1}{n}\\sum_{i=1}^n y^i - \\frac{1}{\\nu(b)}\\big|^p\\bigg]^{1/p}\\ ] ] and applying an appropriately modified version of the m - z inequality ( e.g.  chapter 7 of del moral ( 2004 ) ) we have @xmath399^{1/p } \\leq \\frac{c_p}{\\sqrt{n } } \\big[\\frac{(1-\\nu(b)(1-\\nu(b ) + \\nu(b)^2))}{\\nu(b)^4}\\big]^{1/p}\\ ] ] where we have used the fourth central moment of a geometric random variable and @xmath169 is a constant that only depends upon @xmath162 ( that is independent of @xmath402 or @xmath385 ) . returning to and noting @xmath393 , we have shown that @xmath403^{1/p } \\leq c\\|\\varphi\\|_{\\infty}\\frac{n}{n-1}[\\frac{c_p}{\\sqrt{n } } + \\frac{1}{n}]\\ ] ] from which we can easily conclude .",
    "recall convergence in probability is written @xmath179 and @xmath77 is going to @xmath404 .",
    "in addition , that the convention @xmath362 is used and again recall @xmath363 is the filtration generated by the particle system up - to time @xmath16 .",
    "[ lem : clt_main_random_to_deterministic ] assume ( @xmath115 ) .",
    "then for any @xmath54 , @xmath10 we have : @xmath405(\\varphi ) -    \\sqrt{(n-1)\\eta_n(g_n ) } [ \\eta_{n}^{n-1 } -\\phi_n(\\eta_{n-1}^{t_{n-1 } } ) ] ( \\varphi ) \\rightarrow_{\\mathbb{p } } 0.\\ ] ]    we give the proof for any @xmath59 ; the case @xmath186 follows a similar proof with only notational modifications . throughout the proof @xmath406",
    "is a deterministic constant independent of @xmath16 and @xmath77 whose value may change from line to line .",
    "our proof follows a similar construction to that found in pp .",
    "369 of billingsley ( 1995 ) . to that end",
    ", we have @xmath405(\\varphi ) -    \\sqrt{(n-1)\\eta_n(g_n ) } [ \\eta_{n}^{n-1 } -\\phi_n(\\eta_{n-1}^{t_{n-1 } } ) ] ( \\varphi ) = \\ ] ] @xmath405(\\varphi ) - ( t_n-1)\\sqrt{\\frac{\\eta_n(g_n)}{(n-1 ) } } [ \\eta_{n}^{t_n } -\\phi_n(\\eta_{n-1}^{t_{n-1 } } ) ] ( \\varphi ) + \\label{eq : main_tech_clt_lem1}\\ ] ] @xmath407(\\varphi ) -   \\sqrt{(n-1)\\eta_n(g_n ) } [ \\eta_{n}^{n-1 } -\\phi_n(\\eta_{n-1}^{t_{n-1 } } ) ] ( \\varphi ) .",
    "\\label{eq : main_tech_clt_lem2}\\ ] ] in lemma [ lem : clt_first_conv ] , we have shown that converges in probability to zero ; hence , we focus upon .",
    "to shorten the subsequent notations , we set @xmath408(\\varphi)\\\\ s_n^{n-1}(\\varphi ) & = & ( n-1)[\\eta_{n}^{n-1 } -\\phi_n(\\eta_{n-1}^{t_{n-1 } } ) ] ( \\varphi).\\end{aligned}\\ ] ] then let @xmath409 be given , and consider : @xmath410 @xmath411 @xmath412 now for the latter probability , one can condition upon @xmath197 and apply kolmogorov s inequality noting that the conditional variance of @xmath413 is deterministically upper - bounded by @xmath414 .",
    "hence , we have that @xmath410 @xmath415 noting lemma [ lem : conv_t_n ] and that we can make @xmath416 arbitrarily small , the proof is completed .",
    "[ lem : clt_first_conv ] assume ( @xmath115 ) .",
    "then for any @xmath54 , @xmath10 we have : @xmath405(\\varphi ) - ( t_n-1)\\sqrt{\\frac{\\eta_n(g_n)}{(n-1 ) } } [ \\eta_{n}^{t_n } -\\phi_n(\\eta_{n-1}^{t_{n-1 } } ) ] ( \\varphi ) \\rightarrow_{\\mathbb{p } } 0.\\ ] ]    we give the proof for any @xmath59 ; the case @xmath186 follows a similar proof with only notational modifications . throughout",
    "the proof @xmath406 is a deterministic constant independent of @xmath16 and @xmath77 whose value may change from line to line",
    ". we will prove that @xmath405(\\varphi ) - ( t_n-1)\\sqrt{\\frac{\\eta_n(g_n)}{(n-1 ) } } [ \\eta_{n}^{t_n } -\\phi_n(\\eta_{n-1}^{t_{n-1 } } ) ] ( \\varphi)\\ ] ] will go to zero in @xmath282 . to that end , we rewrite this expression as @xmath417(\\varphi ) \\bigg[\\frac{t_n-1}{\\sqrt{n-1}}\\sqrt{\\eta_n(g_n)}\\bigg ] \\bigg[\\sqrt{\\frac{(n-1)}{(t_n-1)\\eta_n(g_n)}}-1\\bigg].\\ ] ] to simplify the subsequent notations , we define @xmath418.\\ ] ] then a simple application of hlder s inequality gives @xmath419 \\leq \\mathbb{e}\\bigg[\\big|[\\eta_{n}^{t_n } -\\phi_n(\\eta_{n-1}^{t_{n-1 } } ) ] ( \\varphi ) \\bigg[\\frac{t_n-1}{\\sqrt{n-1}}\\sqrt{\\eta_n(g_n)}\\bigg]\\big|^{3/2}\\bigg]^{2/3}\\mathbb{e}[|b(n)|^3]^{1/3}.\\ ] ] we will show that :    1 .",
    "@xmath420(\\varphi ) \\bigg[\\frac{t_n-1}{\\sqrt{n-1}}\\sqrt{\\eta_n(g_n)}\\bigg]\\big|^{3/2}\\bigg]^{2/3}$ ] is upper - bounded by a finite deterministic constant @xmath421 that is independent of @xmath77 .",
    "2 .   @xmath422^{1/3}=0 $ ] .",
    "this will conclude the proof .",
    "* proof of 1*. we have , by another application of hlder , that @xmath423(\\varphi ) \\bigg[\\frac{t_n-1}{\\sqrt{n-1}}\\sqrt{\\eta_n(g_n)}\\bigg]\\big|^{3/2}\\bigg]^{2/3 } \\leq \\mathbb{e}[|[\\eta_{n}^{t_n } -\\phi_n(\\eta_{n-1}^{t_{n-1 } } ) ] ( \\varphi)|^{3}]^{1/3}\\mathbb{e}\\bigg[\\big|\\bigg[\\frac{t_n-1}{\\sqrt{n-1}}\\sqrt{\\eta_n(g_n)}\\big|^{3}\\bigg]^{1/3}\\ ] ] application of corollary [ cor : cond_lp ] gives @xmath423(\\varphi ) \\bigg[\\frac{t_n-1}{\\sqrt{n-1}}\\sqrt{\\eta_n(g_n)}\\bigg]\\big|^{3/2}\\bigg]^{2/3 } \\leq \\frac{c_3\\|\\varphi\\|_{\\infty}}{\\sqrt{n-1}}\\mathbb{e}\\bigg[\\big|\\bigg[\\frac{t_n-1}{\\sqrt{n-1}}\\sqrt{\\eta_n(g_n)}\\big|^{3}\\bigg]^{1/3}. \\label{eq : tech_lem_clt_st_det1}\\ ] ] now turning to the expectation on the r.h.s .",
    "of the inequality , we have @xmath424 =   \\frac{\\eta_n(g_n)^{3/2 } } { ( n-1)^{3/2}}\\mathbb{e}[t_n^3 - 3 t_n^2 + 3t_n -1]\\ ] ] using the fact that , via ( @xmath115 ) @xmath425 for deterministic @xmath426 and standard properties on raw moments of negative binomial random variables , it follows that @xmath427 \\leq c n^3.\\ ] ] thus , we can show that @xmath424^{1/3 } \\leq c\\eta_n(g_n)^{1/2}n^{1/2}.\\ ] ] returning to , we have shown that @xmath423(\\varphi ) \\bigg[\\frac{t_n-1}{\\sqrt{n-1}}\\sqrt{\\eta_n(g_n)}\\bigg]\\big|^{3/2}\\bigg]^{2/3 } \\leq c\\eta_n(g_n)^{1/2}\\ ] ] which completes the proof of 1 .",
    "* proof of 2*. by lemma [ lem : conv_t_n ] and the continuous mapping theorem , we have that @xmath428 .",
    "thus if we can show that for some @xmath429 , @xmath430 < + \\infty$ ] this will allow us to conclude .",
    "for simplicity of calculation , we set @xmath431 .",
    "then , using the fact that @xmath432 we have @xmath433 \\leq \\mathbb{e}\\bigg[\\big|1-\\sqrt{\\frac{\\eta_n(g_n))(t_n-1)}{n-1}}\\big|^4\\bigg].\\ ] ] on expanding the brackets and removing the negative terms , the expectation on the r.h.s .",
    "is upper - bounded by @xmath434 + \\frac{\\eta_n(g_n)^2}{(n-1)^2}\\mathbb{e}[t_n^2 -2t_n + 1].\\ ] ] using the conditional negative binomial property of @xmath435 this expression is equal to @xmath436 -1\\big\\ } + \\frac{\\eta_n(g_n)^2}{(n-1)^2 } \\big\\{\\mathbb{e}\\big[\\frac{n(1-\\phi_n(\\eta_{n-1}^{t_{n-1}})(g_n))}{\\phi_n(\\eta_{n-1}^{t_{n-1}})(g_n)^2 }   + \\frac{n^2}{\\phi_n(\\eta_{n-1}^{t_{n-1}})(g_n)^2 } - \\frac{2n}{\\phi_n(\\eta_{n-1}^{t_{n-1}})(g_n)}+1\\big ] \\big\\}.\\ ] ] applying we easily show that this latter expression is , uniformly in @xmath77 , upper - bounded by a constant @xmath421 .",
    "that is , we have shown that @xmath430 < + \\infty$ ] , which completes the proof of 2 .",
    "this completes the proof .",
    "[ lem : conv_t_n ] assume ( @xmath115 ) .",
    "then for any @xmath54 , we have : @xmath437    we give the proof for any @xmath59 ; the case @xmath186 follows a similar proof with only notational modifications . in theorem",
    "[ theo : time_uniform ] , we have proved that @xmath438 converges almost surely to @xmath25 for @xmath56 . thus we consider @xmath439.\\ ] ] now conditionally upon @xmath197 , @xmath435 is a negative binomial random variable with success probability @xmath440 , so writing @xmath441 as ( conditionally ) independent geometric random variables with the same success probability , we have @xmath439 =   \\mathbb{e}\\bigg[\\mathbb{e}\\bigg[\\big(\\frac{1}{n}\\sum_{i=1}^n [ x_{i , n}(n ) - \\frac{1}{\\phi_n(\\eta_{n-1}^{t_{n-1}})(g_n ) } ] \\big)^2\\bigg|\\mathscr{f}_{n-1}\\bigg]\\bigg].\\ ] ] applying the conditional version of the m - z inequality on the r.h.s .  of the inequality , we have the upper - bound : @xmath442\\ ] ] recalling that @xmath443 for some deterministic constant @xmath426 we conclude that @xmath439 \\leq \\frac{c}{n}.\\ ] ] the proof is completed on recalling that @xmath440 converges almost surely to @xmath444 .",
    "[ lem : tech_res ] we have for any @xmath54 , @xmath166 and @xmath445 , that @xmath446 = \\phi_n(\\eta_{n-1}^{t_{n-1}})(\\varphi)\\ ] ] where @xmath447 .",
    "we have , for any @xmath54 , @xmath166 that @xmath448 is a negative binomial random variable with parameters @xmath76 and success probability @xmath449 and note that from neuts & zacks ( 1967 ) and zacks ( 1980 ) @xmath450 = \\phi_n(\\eta_{n-1}^{t_{n-1}})(\\mathsf{b}_{n})\\label{eq : neg_binom_ineq}.\\ ] ]    now , @xmath451 & = &   \\mathbb{e}\\big[\\frac{1}{t_n-1}\\sum_{i=1}^{t_n-1 } \\varphi(x_n^i)\\big|\\mathscr{f}_{n-1}\\big]\\\\ & = & \\mathbb{e}\\big[\\big(\\frac{1}{t_n-1}\\big)\\big\\{(n-1)\\frac{\\phi_n(\\eta_{n-1}^{t_{n-1}})(\\varphi\\mathbb{i}_{\\mathsf{b}_{n}})}{\\phi_n(\\eta_{n-1}^{t_{n-1}})(\\mathsf{b}_{n } ) } + ( t_n - n)\\frac{\\phi_n(\\eta_{n-1}^{t_{n-1}})(\\varphi\\mathbb{i}_{\\mathsf{b}_{n}^c})}{\\phi_n(\\eta_{n-1}^{t_{n-1}})(\\mathsf{b}_{n}^c ) } \\big\\}\\big|\\mathscr{f}_{n-1}\\big]\\\\ & = &   \\mathbb{e}\\big[\\frac{n-1}{t_n-1}\\frac{\\phi_n(\\eta_{n-1}^{t_{n-1}})(\\varphi\\mathbb{i}_{\\mathsf{b}_{n}})}{\\phi_n(\\eta_{n-1}^{t_{n-1}})(\\mathsf{b}_{n } ) } + \\big(1-\\frac{n-1}{t_n-1}\\big)\\frac{\\phi_n(\\eta_{n-1}^{t_{n-1}})(\\varphi\\mathbb{i}_{\\mathsf{b}_{n}^c})}{\\phi_n(\\eta_{n-1}^{t_{n-1}})(\\mathsf{b}_{n}^c ) } \\big\\}\\big|\\mathscr{f}_{n-1}\\big]\\end{aligned}\\ ] ] where we have used the fact that there are @xmath76 particles that are ` alive ' and @xmath452 that will die and used the conditional distribution of the samples given @xmath435 .",
    "now by , it follows then that @xmath446 =   \\phi_n(\\eta_{n-1}^{t_{n-1}})(\\varphi\\mathbb{i}_{\\mathsf{b}_{n } } ) + \\phi_n(\\eta_{n-1}^{t_{n-1}})(\\varphi\\mathbb{i}_{\\mathsf{b}_{n}^c } ) =   \\phi_n(\\eta_{n-1}^{t_{n-1}})(\\varphi)\\ ] ] which concludes the proof .",
    "we have : @xmath454 = \\ ] ] @xmath455 \\frac{\\phi_n(\\eta_{n-1}^{t_{n-1}})^{\\otimes 2}(\\mathbb{i}_{\\mathsf{b}_{n}^2}\\varphi ) } { \\phi_n(\\eta_{n-1}^{t_{n-1}})^{\\otimes 2}(\\mathsf{b}_{n}^2 ) } + \\label{eq : two_succ}\\ ] ] @xmath456 \\frac{\\phi_n(\\eta_{n-1}^{t_{n-1}})^{\\otimes",
    "2}(\\mathbb{i}_{\\mathsf{b}_{n}\\times \\mathsf{b}_{n}^c}\\varphi ) } { \\phi_n(\\eta_{n-1}^{t_{n-1}})(\\mathsf{b}_{n})\\phi_n(\\eta_{n-1}^{t_{n-1}})(\\mathsf{b}_{n}^c ) } + \\label{eq : one_succ}\\ ] ] @xmath457 \\frac{\\phi_n(\\eta_{n-1}^{t_{n-1}})^{\\otimes 2}(\\mathbb{i}_{(\\mathsf{b}_{n}^c)^2}\\varphi ) } { \\phi_n(\\eta_{n-1}^{t_{n-1}})^{\\otimes 2}((\\mathsf{b}_{n}^c)^2)}. \\label{eq : no_succ}\\ ] ] the three terms on the r.h.s .  arise due to the @xmath458 different pairs of particles which land in @xmath459 , the @xmath460 pairs of different particles which land in @xmath461 and the @xmath462 different pairs of particles which land in @xmath463 ; the factors of @xmath464 arise from the conditional distributions of the particles given @xmath435 ( recalling that conditional on @xmath197 , @xmath435 is a negative binomial random variables parameters @xmath77 and @xmath465 ) .",
    "now for , we have from neuts & zacks ( 1967 ) and zacks ( 1980 ) that @xmath455 = \\phi_n(\\eta_{n-1}^{t_{n-1}})^{\\otimes 2}(\\mathsf{b}_{n}^2).\\ ] ] so that becomes @xmath466 recalling and using the above result , becomes @xmath467 finally , noting that for any @xmath468 @xmath469 $ ] , and thus using the above results that @xmath470 = \\phi_n(\\eta_{n-1}^{t_{n-1}})^{\\otimes 2}(\\mathsf{b}_{n } ) + \\frac{\\phi_n(\\eta_{n-1}^{t_{n-1}})^{\\otimes 2}(\\mathsf{b}_{n})^2}{n-1}\\ ] ] it follows that is equal to @xmath471 hence we have shown @xmath472 & = & \\phi_n(\\eta_{n-1}^{t_{n-1}})^{\\otimes 2}(\\mathbb{i}_{\\mathsf{b}_{n}^2}\\varphi ) + 2\\phi_n(\\eta_{n-1}^{t_{n-1}})^{\\otimes",
    "2}(\\mathbb{i}_{\\mathsf{b}_{n}\\times \\mathsf{b}_{n}^c}\\varphi ) + \\phi_n(\\eta_{n-1}^{t_{n-1}})^{\\otimes 2}(\\mathbb{i}_{(\\mathsf{b}_{n}^c)^2}\\varphi)\\\\ & = & \\phi_n(\\eta_{n-1}^{t_{n-1}})^{\\otimes 2}(\\varphi).\\end{aligned}\\ ] ]                                  , f. & oudjane , n.  ( 2006 ) .",
    "a sequential particle algorithm that keeps the particle system alive . in _",
    "stochastic hybrid systems : theory and safety critical applications _ , ( h. blom & j. lygeros , eds ) , lecture notes in control and information sciences * 337 * , 351389 , springer : berlin ."
  ],
  "abstract_text": [
    "<S> in the following article we develop a particle filter for approximating feynman - kac models with indicator potentials . examples of such models include approximate bayesian computation ( abc ) posteriors associated with hidden markov models ( hmms ) or rare - event problems . </S>",
    "<S> such models require the use of advanced particle filter or markov chain monte carlo ( mcmc ) algorithms e.g.  jasra et al .  </S>",
    "<S> ( 2012 ) , to perform estimation . </S>",
    "<S> one of the drawbacks of existing particle filters , is that they may ` collapse ' , in that the algorithm may terminate early , due to the indicator potentials . in this article , using a special case of the locally adaptive particle filter in lee et al .  </S>",
    "<S> ( 2013 ) , which is closely related to le gland & oudjane ( 2004 ) , we use an algorithm which can deal with this latter problem , whilst introducing a random cost per - time step . </S>",
    "<S> this algorithm is investigated from a theoretical perspective and several results are given which help to validate the algorithms and to provide guidelines for their implementation . </S>",
    "<S> in addition , we show how this algorithm can be used within mcmc , using particle mcmc ( andrieu et al .  </S>",
    "<S> 2010 ) . </S>",
    "<S> numerical examples are presented for abc approximations of hmms </S>",
    "<S> . + * key words : * particle filters , markov chain monte carlo , feynman - kac formulae .    * the alive particle filter *    by ajay jasr@xmath0 , anthony lee@xmath1 , chris yau@xmath2 & xiaole zhang@xmath2    @xmath3department of statistics & applied probability , national university of singapore , singapore , 117546 , sg . </S>",
    "<S> + e-mail:`staja@nus.edu.sg </S>",
    "<S> `    @xmath1department of statistics , university of warwick , coventry , cv4 7al , uk . </S>",
    "<S> + e-mail:`anthony.lee@warwick.ac.uk `    @xmath2department of mathematics , imperial college london , london , sw7 2az , uk . </S>",
    "<S> + e-mail:`c.yau@ic.ac.uk ` , ` x.zhang11@ic.ac.uk ` </S>"
  ]
}