{
  "article_text": [
    "in this era of massive surveys and resulting colossal databases , one of the hottest topics is how to mine relevant information from these data as efficiently as possible .",
    "scientists are very inventive in constructing a wide diversity of methods to reach their goals and deriving scientific results in many ways .",
    "very often , the issue the data analyst faces is not `` how to answer my question ? '' but `` which of the @xmath0 possible methods would solve best my problem ? '' .",
    "two examples of this situation among many are the classification of astronomical objects and the estimation of photometric redshifts . both subjects have recently seen the proliferation of methods used .",
    "for the first , there is nowadays an increasing variety of supervised classifiers ; in the astronomical literature , @xcite represent a few examples .",
    "the gaia variability processing pipeline @xcite applies three methods for the classification of variable stars , bayesian networks , gaussian mixtures and random forest .",
    "often , when many classifiers can be applied for the task , generally well - performing ones might fail on some classes while generally lower - performing others succeed on them , and it would be desirable to join the overall good performance of the former with the class - specific good scores of the latter . for photometric redshifts , many variants for both the empirical and the template - fitting methodologies exist ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) , some of which ( and more ) are considered and tested for use in the euclid photometric redshift pipeline .",
    "it would be useful to be able to merge the complementary advantages of these .",
    "we propose a high - dimensional meta - model to combine the output of several methods , which is generally applicable in every study where several models are possible to use , each of which gives a probability distribution of the parameter of interest as result .",
    "we use the above two specific topics to demonstrate the capabilities of this hierarchical setup , and show promising results on both of our examples .",
    "assume that @xmath1 represents the data about an object of interest .",
    "this can be the multi - filter photometry of a galaxy , the attribute vector ( amplitude , period , fourier coefficients , colour , absolute magnitude ) characterising a variable star , or any other informative set of measured values about a target .",
    "we wish to make inference about a parameter @xmath2 of the object ; in the case of the galaxy , @xmath2 may be the redshift @xmath3 , in the case of the variable star , its type .",
    "suppose moreover that we have @xmath4 alternative methods , each of which yields a probability distribution function ( pdf ) of the parameter @xmath2 : @xmath5 . for photometric redshift estimation",
    ", these methods can be any variants of both the template fitting and the empirical methods , and for classification of variable stars , any supervised method from gaussian mixtures to random forest , svm or neural networks .",
    "the outputs @xmath6 are the probability distributions of the parameter of interest , conditioned on the observed data ; point estimates of the parameter and uncertainties can be defined in multiple ways .",
    "our goal is to learn a new , better pdf @xmath7 of the true parameter value by combining these outputs .",
    "we look for a mapping @xmath8 the most commonly used way for this is weighted averaging of the outputs , often using different weights in different regions of @xmath9 motivated by bayesian model averaging ( bma ) , bayesian model combination ( bmc ) or other ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ) . instead of this",
    ", we propose to learn this mapping by applying a nonparametric machine learning method , similar to wolpert s proposition called `` stacking '' @xcite , which consists of a second - level , hierarchical training on the predictions from the base methods in a cross - validation - like setup .",
    "suppose we have a training set , which we divide into two parts , one called @xmath10 ( training ) and the other @xmath11 ( combination ) .    1 .",
    "we train the @xmath4 base methods on @xmath10 .",
    "we predict the pdfs on @xmath11 , obtaining @xmath4 output pdfs , @xmath12 for each of our objects in @xmath11 .",
    "these can be represented each as the vector of values taken by the pdfs over a grid of @xmath2 , or in some basis function system ( e.g. * ? ? ?",
    "2 .   in the next stage , we train a nonparametric learning method using the concatenated pdfs obtained on the objects of @xmath11 and the corresponding known values of the parameter of interest .",
    "the output of this second - level model is optimally a new probability distribution of the parameter .",
    "thus , we learn the mapping leading to the final estimate @xmath7 .",
    "any machine - learning method can in principle be used here , though there may be technical restrictions on the choice , such as an ability to deal with high - dimensional data if its input pdfs have a high - dimensional representation .",
    "3 .   to predict a combined pdf of the parameter of interest of a new object , we first apply the trained base learners of step 1 to obtain their output pdfs , then feed the concatenated pdfs into the trained combination model to obtain the combined result .",
    "optionally , the partition of the full training set into @xmath10 and @xmath11 can be repeated randomly @xmath13 times , steps 1 - 3 can be performed using the ensemble of the @xmath13 models , and an average estimate of final combined pdfs may replace the single @xmath7 .",
    "with such a procedure , as usually with ensemble methods , we can obtain more stable results , and can gather information about the uncertainty of the estimates due to training set selection .",
    "why can this hierarchical learning be expected to work better than bma or bmc ?",
    "figure  [ avglimitation ] depicts the main reason .",
    "suppose that we have two base methods ( method  1 and 2 ) that yield the probability distributions for the photometric redshift of a test galaxy , shown in black solid and black dashed lines in the figure .",
    "the true redshift of the galaxy , shown as a green spike , may be known from spectroscopic measurements , and indicates a bias for method  2 and a catastrophic error for method  1 .",
    "both of these mistakes can be characteristic of the method used ( due perhaps to the chosen template set for the template - fitting method  2 and the dominance of low - redshift objects in the training sample for the empirical method  1 ) , and not an implication of the data : with an `` optimal '' method ( better - adapted templates for method  2 , a balanced sample for method  1 or a third method ) , we might be able to reduce the bias and obtain the `` best achievable '' estimate with our data ( in blue in the figure ) .",
    "combinations based on averaging can never leave the grey area delimited by the two base pdfs , and can not approximate well the `` best achievable '' pdf .",
    "this remains so using any number of methods , or partitioning of the input space @xmath9 into several regions : at each @xmath1 the combined pdf will remain between the upper and lower envelope of the pdfs from the methods . in other words , systematics common to the methods can not be corrected by using a linear combination .",
    "the nonparametric , unconstrained learning proposed above is in principle able to learn also nonlinear relationships , and thus approximate the nonlinear mapping from method  1 and 2 to the `` best achievable '' .",
    "moreover , the outputs of the base methods are not independent .",
    "they use often the same or overlapping data , and in many cases , are built on similar principles with possibly only small differences ; an example for this is the template - fitting photometric redshift estimating methods using principal component decomposition or different pre - determined sets of templates .",
    "combining two such methods requires accounting for their dependency as well .",
    "this dependence must be modelled when we seek the estimating mapping @xmath14 .",
    "averaging does not take into account this dependence structure , while the above proposed method is able to learn and thus potentially make use of it .",
    "in other words , it may be able to `` learn from the mistakes of all '' .",
    "we tested our procedure using the following general framework .",
    "we selected a known , thoroughly analysed dataset from the literature for both variable star classification and photometric redshift estimation , and drew random partitions over them into three equal parts ( @xmath10 for base training , @xmath11 for combination , @xmath15 for validation of the results ) @xmath13 times ( for variable stars , @xmath16 , for photometric redshifts , @xmath17 ) .",
    "the procedure described in section [ sec : method ] was run on each partition , using a random forest learner @xcite for the combination because of its stability , simplicity , insensitivity to tuning parameters and ability to deal with high - dimensional data . for a fair comparison , we trained the base methods also on the joined @xmath18 set , thus providing them the same amount of training data as to the combination .",
    "we compare the combinations to these `` doubly - trained '' base classifiers , in order to ensure that the improvement by the combination is not simply due to the twice as large training set .",
    "we also computed a combination based on weighted averages , where the weights were taken to be proportional of the fraction of correct predictions by the method on set @xmath11 .",
    "the results presented are averaged over all sets @xmath15 .",
    "the used complete dataset consists of 1661 stars from the hipparcos periodic variable catalog @xcite , and used in @xcite .",
    "the class system was simplified to 15 classes , merging some of them ( such as all subtypes of classical cepheids into cepcl or type - ii cepheids into cept2 ) and omitting very rare classes , obtaining finally classes that had each at least 20 members .",
    "the objects were characterised by attributes derived from their light curve ( period , amplitude , fourier amplitudes and phases , statistical summaries such as skewness and kurtosis ) complemented by visual and near - infrared colours ( for details , see * ? ? ?",
    "we used five base classifiers : c5.0 , random forest ( rf ) , gaussian mixtures ( gm ) , support vector machines ( svm ) and linear discriminant analysis ( lda ) ( a textbook summarizing them is * ? ? ?",
    "* ) .",
    "the classwise performances of the base classifiers ( trained on @xmath18 ) are shown with dashed and dash - dotted lines and empty symbols in the left panel of figure  [ fig2 ] .",
    "in particular , the overall weakest performance of lda and its point of excellence , the classification of the class ew ( contact eclipsing binaries ) are visible .",
    "an ideal combination should preserve this excellence of lda in identifying ew objects , while on the other classes , maintain the overall good performance of the other classifiers .",
    "the hierarchical combination , shown in red solid line , is very close to achieve this : either it exceeds all the base classifiers , or is very near to the maximum accuracy , including on the ew class .",
    "the combination by average in comparison is somewhat weaker on several classes .",
    "the mean global accuracies are 82.5% ( rf ) , 81% ( c5.0 ) , 79.9% ( gm and svm ) , 77.9% ( lda ) for the base classifiers , while they are 82.2% for the averaging combination and 85.8% for the hierarchical combination .",
    "this shows an average improvement of 3.3% by the latter over the best base method .",
    "the detailed results on the 1000 random partitions show that it yields improvement over the best base classifier in all partitions .",
    "the data , containing @xmath19 and @xmath20 photometry of 3331 galaxies , are a subset from field d1 of the wirds - chftls database @xcite , having spectroscopic redshifts from the vimos - vlt deep and ultradeep surveys @xcite .",
    "to estimate the redshift of the objects , we implemented a least squares template fitting algorithm without photometric zero - point calibration , using the cosmos templates with no template calibration or added emission lines , and an empirical rf regression , using only the default tuning parameters proposed by @xcite .",
    "we trained these base methods ( both on only @xmath10 and on @xmath18 ) , the hierarchical and several average - based combinations , and the median of the two point estimates from the base methods .",
    "the weights for the first were defined to be proportional to the fraction of catastrophic outliers on @xmath11 ( see the definition later ) . ] on the 500 random partitions . to produce the presented plots",
    ", we computed several point estimates of @xmath3 from the pdfs provided by the base methods and by the combinations for all objects when they were in set @xmath15 , and selected the best of these ( mean of the pdf for the template fit , median of the pdf for the base rf and the combinations ) . in",
    "what follows , this best point estimate is denoted by @xmath21 , and the spectroscopic redshift which is considered to be the truth by @xmath22 .",
    "the right panel of figure  [ fig2 ] shows the catastrophic outlier rate ( the fraction of objects for which @xmath23 ) against the normalized median absolute deviation ( defined as @xmath24 ) .",
    "the best methods fall therefore at the lower left corner of the plot .",
    "the base methods have by far the largest scatter , and the base rf models ( both trained on @xmath10 and @xmath18 ) the largest outlier fraction .",
    "the improvement that can be obtained by using a larger training set with the same single method can be seen when comparing the two rf models : both the scatter and the outlier fraction decreases notably .",
    "not so much , however , as obtained by variants of the averaging combination ( pink triangle , violet square and yellow dot ) .",
    "the overall best results , however , are furnished by the hierarchical combination , with a further strong decrease of the scatter .",
    "it is remarkable that even though the second - level method in the hierarchical combination is an empirical machine - learning one , and therefore just as hampered by the underrepresentation of high-@xmath3 objects as the base rf , it still improves even on the outlier fraction of the template fitting base method .    .",
    "middle panel : empirical method , with the median of the pdfs as @xmath21 .",
    "right panel : hierarchical combination , with the median of the pdfs as @xmath21 .",
    "the red line corresponds to @xmath25 .",
    "the blue circle emphasizes the neighbourhood where both base methods are biased downwards , which is visibly decreased by the combination.,scaledwidth=99.0% ]    figure  [ fig3 ] shows the @xmath3-dependent systematic bias of the template fits ( left panel ) and the high scatter and outlier rate of the empirical method ( middle panel ) , when compared to the ideal @xmath26 line ( red ) .",
    "the hierarchical combination ( right panel ) corrects the systematic biases of the first , and shrinks the scatter and decreases the number of catastrophic outliers in the high-@xmath3 regime of the second .",
    "thus , it is indeed able to pick the best of both method , while learning to ignore their systematic failures .",
    "moreover , it is able to do what an averaging method can not : around redshifts 1.2 - 1.5 , where both base methods are downward biased ( blue circle ) , it largely removes this bias .",
    "the learning here is based on simultaneous presence of specific patterns in the output of the two methods , not on a straightforward pointwise averaging .",
    "our paper presents a general hierarchical information combination method , which is aimed at the efficient extraction of useful information from the data .",
    "the first level of the hierarchy trains several base methods ( classifiers , regression models or any other statistical or machine - learning model ) producing each a probability distribution of the parameter of interest .",
    "the second level of the hierarchy consists of training a second nonparametric machine - learning model ( e.g. random forest ) on the outputs of the base models .",
    "the experiments on variable star classification and photometric redshift estimation show the following :    * the information in a given training set about a parameter is more efficiently exploited if the set is divided into two parts in order to train a hierarchical combination model using several base models than to train a single model with the complete training set .",
    "* in both of our examples , we achieved always improvement in the global results over the best single model . *",
    "the hierarchical combination is able also to correct systematics and biases where _ all _ the base models are similarly biased . * the hierarchical combination is very general : it can be applied for every study where there are alternative methods providing different views of the data , producing probability distributions as output . * the choice of the combiner , though it must be able to model nonparametric relationships and high - dimensional data , is largely free . in our study , we used random forest , which in addition was little sensitive to tuning parameters .",
    "hastie , t. , tibshirani , r. , & friedman , j. 2009 , the elements of statistical learning : data mining , inference and prediction , 2nd edn .",
    "( springer - verlag , new york ) , e - book : http://www-stat.stanford.edu/@xmath27tibs/elemstatlearn/"
  ],
  "abstract_text": [
    "<S> throughout the processing and analysis of survey data , a ubiquitous issue nowadays is that we are spoilt for choice when we need to select a methodology for some of its steps . </S>",
    "<S> the alternative methods usually fail and excel in different data regions , and have various advantages and drawbacks , so a combination that unites the strengths of all while suppressing the weaknesses is desirable . </S>",
    "<S> we propose to use a two - level hierarchy of learners . </S>",
    "<S> its first level consists of training and applying the possible base methods on the first part of a known set . at the second level </S>",
    "<S> , we feed the output probability distributions from all base methods to a second learner trained on the remaining known objects . using classification of variable stars and photometric redshift estimation as examples , </S>",
    "<S> we show that the hierarchical combination is capable of achieving general improvement over averaging - type combination methods , correcting systematics present in all base methods , is easy to train and apply , and thus , it is a promising tool in the astronomical `` big data '' era . </S>"
  ]
}