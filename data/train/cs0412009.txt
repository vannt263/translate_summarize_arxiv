{
  "article_text": [
    "let @xmath0 denote the space of @xmath1 symmetric matrices . given @xmath2 and @xmath3 , semidefinite programming ( sdp ) problems in standard form",
    "are given as @xmath4 the notation @xmath5 represents the inner product of @xmath6 and @xmath7 , which is equal to @xmath8 .",
    "the constraint @xmath9 denotes that @xmath7 must be symmetric positive semidefinite , which means @xmath10 for any @xmath11 .",
    "similar to linear programming , semidefinite programs have associated dual problems .",
    "the semidefinite program ( [ primal ] ) is said to be in _ primal _ form .",
    "its _ dual _ , which is also a sdp problem , is @xmath12 where @xmath13 is called the _ dual slack matrix_.    semidefinite programming has many applications in many fields ( see @xcite for a list of applications ) . it can also be regarded as an extension of linear programming . as a result , various methods for solving linear programming have been extended to solve sdp . in particular ,",
    "interior - point methods were first extended to sdp by alizadeh @xcite and nesterov and nemirovskii @xcite independently .",
    "the most effective interior - point methods are primal - dual approaches that use information from both primal and dual programs .",
    "most primal - dual interior - point algorithms for sdp proposed fall into two categories : path - following and potential reduction methods .",
    "our algorithm is of the potential reduction kind , in which a potential function is defined and each iterate reduces the potential by at least a constant amount .",
    "it is based on the primal - dual potential reduction method proposed by nesterov and nemirovskii in their book @xcite .",
    "this paper focuses on the sparse case of sdp , where the data matrices @xmath6 and @xmath14 s consist of mostly zero entries .",
    "because most problem data arising in practice are sparse , it is vital for an sdp solver to take advantage of the sparsity and avoid unnecessary computation on zero entries .",
    "the obstacle that prevents effective exploitation of sparsity in an sdp algorithm is that the primal matrix variable is dense regardless of the sparsity of the data . to avoid this problem , benson et al .",
    "proposed a pure dual interior - point method for sparse case @xcite .",
    "later , fukuda et al .  proposed a primal - dual algorithm using partial matrix and matrix completion theory to avoid the dense primal matrix @xcite .",
    "our algorithms follow fukuda et al.s suggestion and uses partial primal matrix to take advantage of sparsity in the primal - dual framework .",
    "in contrast , a recent work by burer is also built upon fukuda et al.s idea of using partial matrix but his algorithm is a primal - dual path - following method based on a new search direction @xcite .    in nesterov and nemirovskii s primal - dual potential reduction method ,",
    "the computation of the search directions requires the gradient and hessian - matrix product of the barrier functions .",
    "the currently common way to compute this gradient is not efficient in some sparse case .",
    "our algorithm applies the idea from automatic differentiation in reverse mode to compute gradient and hessian - matrix product in a more efficient manner for the sparse cases .",
    "additionally , we suggest a technique that evaluates the barrier function value of a partial matrix efficiently in certain cases and an alternative way to compute the search directions when such evaluation is expensive . when the data matrices aggregated sparsity pattern forms a planar graph , our algorithm manages to reduce the time complexity to @xmath15 operations and the space complexity to @xmath16 per sdp iterate . this is a significant improvement from the @xmath17 time complexity and the @xmath18 space complexity per iterate of a typical sdp solver for planar case .",
    "we start with review of necessary material on semidefinite programming , nesterov and nemirovskii s primal - dual potential reduction method , and sparse matrix computation in section [ section_sdp ] , [ section_nesterov ] , and [ section_sparse ] , respectively .",
    "section [ section_dualnewton ] covers the detail of our algorithm s computation of the dual newton direction including how an idea from automatic differentiation in reverse mode is used to evaluate the gradient and hessian - matrix product efficiently .",
    "the computation of the primal projected newton direction as well as the efficient method in computing the determinant of the positive definite completion of a partial matrix are discussed in section [ section_primalnewton ] .",
    "finally , the results of our algorithm on test instances of the problem of finding maximum cut are in section [ section_exp ] .",
    "we refer to @xmath7 and @xmath19 as _ feasible solutions _ if they satisfy the constraints in ( [ primal ] ) and ( [ dual ] ) , respectively .",
    "a _ strictly feasible solution _ is a feasible solution such that @xmath7 ( or @xmath13 ) are symmetric positive definite .",
    "a matrix @xmath20 is _ symmetric positive definite _",
    "@xmath21 if @xmath22 for any @xmath23 . problem ( [ primal ] ) ( resp .",
    "( [ dual ] ) ) is _ strictly feasible _ if it contains a strictly feasible solution",
    ".    let @xmath24 ( resp .",
    "@xmath25 ) denote the set of feasible solutions of ( [ primal ] ) ( resp .",
    "( [ dual ] ) ) , and @xmath26 ( resp .",
    "@xmath27 ) denote the set of strictly feasible solutions of ( [ primal ] ) ( resp .",
    "( [ dual ] ) ) .",
    "the _ duality gap _ , which is the difference between primal and dual objective functions @xmath28 is nonnegative at any feasible solution @xcite . under the assumption that ( [ primal ] ) and ( [ dual ] ) are strictly feasible and bounded , @xmath29 solves ( [ primal ] ) and ( [ dual ] ) if and only if @xmath30 , and @xmath31 .",
    "_ primal - dual potential reduction methods _ solve sdp programs by minimizing the potential function @xmath32 where @xmath33 is a given constant parameter of the algorithm .",
    "any sequence of iterates in which the potential @xmath34 tends to negative infinity converges to ( or at least has an accumulation point at ) a strictly feasible solution @xmath29 such that @xmath31 and hence is optimal @xcite .",
    "most primal - dual potential reduction methods begin at a strictly feasible iterate and compute the next strictly feasible iterate while guaranteeing at least constant decrease in @xmath35 each iteration .",
    "the process is continued until an iterate with duality gap less than or equal to @xmath36 is found , where @xmath37 is a given tolerance .",
    "nesterov and nemirovskii proposed a polynomial - time primal - dual potential reduction method for more general convex programming problems in their book @xcite , which is the basis of our algorithm .",
    "we now proceed to explain the  large step \" version of their method as applied to the sdp ( [ primal ] ) and ( [ dual ] ) .",
    "we call this method the  decoupled primal - dual \" algorithm ( dpd ) since the computation of primal and dual directions are less directly coupled than in other primal - dual interior point methods . given a current strictly feasible iterate @xmath38 , compute the next iterate @xmath39 as follows :    1 .",
    "let + @xmath40 + define the function + @xmath41 + where @xmath42 .",
    "2 .   find the projected newton direction @xmath43 of @xmath44 onto @xmath45 at x , + @xmath46 3 .",
    "let @xmath47 be @xmath48^{1/2}.\\ ] ] 4 .",
    "we then have + @xmath49 + and + @xmath50 - s\\ ] ] + as a primal and dual directions respectively .    by swapping the roles of primal and dual in ( i)-(iv )",
    ", another pair of directions can be achieved as follow :    1 .",
    "let + @xmath51 + define the function + @xmath52 2 .",
    "find the projected newton direction @xmath53 of @xmath54 onto @xmath55 at @xmath13 , @xmath56 3 .",
    "let @xmath57 be @xmath58^{1/2}.\\ ] ] 4",
    ".   then + @xmath59 + and + @xmath60 - x\\ ] ] + are another dual and primal directions , respectively .",
    "find + @xmath61 6 .",
    "finally , set + @xmath62    nesterov and nemirovskii also showed that dpd algorithm achieves a constant reduction in @xmath35 at each iteration even when only two directions @xmath63 and @xmath64 are considered and potential minimization in step ( ix ) is not performed ( that is , when fixing @xmath65 and @xmath66 ) .",
    "a _ sparse _ matrix is a matrix with few nonzero entries .",
    "many problems data encountered in practice are sparse . by exploiting their structures , time and space required to perform operations on them",
    "can be greatly reduced .",
    "many important applications of sdp , such as the problem of finding maximum cut , usually have sparse data , too .",
    "for this reason , we consider the sparse case in this paper .    to be able to discuss the exploitation of sparse data in sdp , background on chordal graph theory is needed , which is addressed in the following section .",
    "let @xmath67 be a simple undirected graph .",
    "a _ clique _ of @xmath68 is a complete induced subgraph of @xmath68 .",
    "a clique @xmath69 is _ maximal _ if its vertex set @xmath70 is not a proper subset of another clique .",
    "let @xmath71 denote the set of all vertices adjacent to a vertex @xmath72 .",
    "a vertex @xmath44 is called _ simplicial _ if all of its adjacent vertices @xmath73 induce a clique .    for any cycle of @xmath68 ,",
    "chord _ is an edge joining two non - consecutive vertices of the cycle .",
    "graph @xmath68 is said to be _ chordal _ if each of its cycles of length 4 or greater has a chord .",
    "one fundamental property of a chordal graph is that it has a simplicial vertex , say @xmath74 and that the subgraph induced by @xmath75 is again chordal , which therefore has a simplicial vertex , say @xmath76 . by repeating this process",
    ", we can construct a _",
    "perfect elimination ordering _ of the vertices , say @xmath77 , such that @xmath78 induces a clique for each @xmath79 .",
    "it was shown by fulkerson and gross that a graph is chordal if and only if it has a perfect elimination ordering @xcite .    given a perfect elimination ordering @xmath77 of a chordal graph , its maximal cliques can be enumerated easily . a maximal clique containing the simplicial vertex @xmath74 is given by @xmath80 and is unique .",
    "a maximal clique not containing @xmath74 is a maximal clique of the chordal subgraph induced on @xmath81 .",
    "therefore , by repeating this reasoning , the maximal cliques @xmath82 , are given by @xmath83 for @xmath84 , that is , the maximal members of @xmath85 .",
    "one property of the sequence of maximal cliques is that it can be reindexed such that for any @xmath86 , there exists a @xmath87 , such that @xmath88 such property is called the _ running intersection property_.    there is a well - known relationships between chordal graph and cholesky factorization of sparse symmetric positive definite matrices . given a symmetric positive definite matrix @xmath7 , its cholesky factor @xmath89 is a lower - triangular matrix such that @xmath90 .",
    "sparsity pattern _ of @xmath7 , which is defined as the set of row / column indices of nonzero entries of @xmath7 , is often represented as a graph @xmath91 , where @xmath92 and @xmath93 .",
    "similarly , the sparsity pattern of @xmath89 can be represented by the graph @xmath94 , where @xmath95 . under _ no numerical cancellations assumption _ , which means no zero entries are resulted from arithmetic operations on nonzero values , it is seen that @xmath96 , with @xmath97 having possibly additional fill - ins .",
    "in addition , @xmath94 is chordal and is said to be a _",
    "chordal extension _ of @xmath67 .",
    "the number of fill - ins in the cholesky factorization depends on the ordering of the row / column indices .",
    "the question of finding the reordering of the row / column indices to yield fewest fill - ins is np - complete . in the best case when @xmath68 is chordal , a perfect elimination ordering yields the cholesky factor with no fill - ins .",
    "it is a well - known fact that in the course of sdp algorithms , the primal variable @xmath7 usually is dense even if the data are sparse while the dual variables @xmath98 and @xmath13 stay sparse . to avoid working with a dense primal variable , fukuda et al .",
    "suggested the use of a partial symmetric matrix for the primal variable in sdp algorithms @xcite .",
    "let @xmath92 .",
    "define the _ aggregate sparsity pattern @xmath99 _ of the data to be @xmath100_{ij } \\neq 0 \\textrm { for some } p",
    "\\in   \\{0,1,\\ldots , m\\}\\}.\\ ] ]    observing ( [ primal ] ) , we see that the values of the objective function and constraint linear functions only depend the entries of @xmath7 corresponding to the nonzero entries of @xmath6 and @xmath14 s .",
    "the remaining entries of @xmath7 affect only whether @xmath7 is positive semidefinite .",
    "in other words , if @xmath7 and @xmath101 satisfy @xmath102 , for any @xmath103 , then    @xmath104    a _ partial symmetric matrix _ is a symmetric matrix in which not all of its entries are specified .",
    "a partial symmetric matrix @xmath105 can be treated as a sparse matrix , having its unspecified entries regarded as having zero values .",
    "hence , a sparsity graph @xmath94 can be used to represent the row / column indices of specified entries of @xmath105 in the same manner as it is used to represent nonzero entries of a sparse matrix .",
    "let @xmath106 denote the set of @xmath1 partial symmetric matrices with entries specified in @xmath97 .",
    "we assume that all diagonal entries are also specified although there are no edges in @xmath107 representing them .",
    "a _ completion _ of a partial symmetric matrix @xmath105 is a matrix @xmath7 of the same size as @xmath105 such that @xmath108 for any @xmath109 .",
    "a _ positive definite completion _ of a partial symmetric matrix is a completion that is positive definite .",
    "the following theorem characterizes when a partial matrix has a positive definite completion .",
    "[ completion_theorem ] let @xmath94 be a chordal graph .",
    "any partial symmetric matrix @xmath110 satisfying the property that @xmath111 is symmetric positive definite for each @xmath112 , where @xmath113 denote the family of maximal cliques of @xmath107 , can be completed to a positive definite matrix .",
    "the following result of fukuda et al .",
    "@xcite shows an efficient way to compute a certain positive definite matrix completion .",
    "given a partial symmetric matrix @xmath105 whose sparsity pattern @xmath94 is chordal , its unique positive definite completion that maximizes the determinant @xmath114 is shown to be @xmath115 where @xmath116 is the permutation matrix such that @xmath117 is the perfect elimination ordering for @xmath118 , @xmath119 ( @xmath120 are sparse triangular matrices , and @xmath121 is a positive definite block - diagonal matrix , both defined below @xcite .",
    "let @xmath122 be an ordering of maximal cliques of @xmath107 that enjoys the running intersection property ( [ running ] ) .",
    "define @xmath123 the factors in ( [ cliquefactor ] ) are given by @xmath124_{ij } = \\left\\{\\begin{array}{ll }           1 , & i = j,\\\\          { [ \\bar{x}^{-1}_{u_r u_r } \\bar{x}_{u_r s_r}]}_{ij } , & i \\in u_r , j \\in s_r , \\\\           0 , & \\textrm{otherwise}\\\\        \\end{array } \\right.\\ ] ] for @xmath125 , and @xmath126 where @xmath127 in addition , the unique determinant - maximizing positive definite completion @xmath128 has the property that @xmath129 in other words , the inverse of the determinant - maximizing completion has the same sparsity pattern as that of the partial matrix .      to exploit sparsity in the data matrices",
    ", our algorithm works in the space of partial matrix @xmath105 for primal variable .",
    "when positive definite completion of @xmath105 is needed , the maximum - determinant completion is used .",
    "we choose this particular completion because it preserves the self - concordance of the barrier function , which follows directly from proposition 5.1.5 of @xcite .",
    "this property guarantees that our algorithm converges to an optimal solution .",
    "the single most computationally - intensive step in any potential reduction method is the computation of search directions . in nesterov and nemirovskii s method described in section [ section_nesterov ] , this computation occurs in steps ( ii ) , ( iv ) , ( vi ) , and ( viii ) .",
    "for this reason , minimizing computation in these steps are emphasized in our algorithm .",
    "we describe our algorithm to compute @xmath53 ( step ( vi ) of the algorithm ) , which is the most computationally - intensive step in the computation of @xmath130 , in this section .",
    "conjugate gradient is used together with an idea from automatic differentiation in reverse mode to compute @xmath53 .",
    "the minimization problem in step ( vi ) is @xmath131 replacing @xmath53 with @xmath132 in ( [ minnt ] ) yields @xmath133 which is equivalent to solving the system @xmath134 for @xmath135 , where @xmath136 . to rewrite ( [ eqz ] ) into standard system of linear equations",
    "form , we first define the functions @xmath137 and @xmath138 then , the system ( [ eqz ] ) is equivalent to @xmath139 our algorithm uses conjugate gradient to solve ( [ eqz2 ] ) to exploit the fact that @xmath140 and @xmath141 can be computed efficiently .      as described above , the conjugate gradient method",
    "when solving for @xmath53 calls for the computation of @xmath140 in each iteration and @xmath141 once .",
    "note first that @xmath142 can be can be computed as follow : cholesky factorize @xmath143 , where @xmath89 is a lower triangular matrix , compute @xmath144 , and finally @xmath145 .",
    "we derive the algorithm to evaluate @xmath146 from the above method of evaluating @xmath142 by imitating automatic differentiation ( ad ) in reverse mode , which is discussed in detail below . to evaluate @xmath147 , notice that it is the derivative of the function @xmath148^t\\mathbf{z}$ ] with respect to @xmath149 .",
    "hence , we can derive the algorithm to evaluate @xmath147 from the algorithm to compute @xmath150 , again by imitating ad in reverse mode .",
    "we emphasize that we do not suggest using ad to automatically compute derivatives of @xmath142 given the algorithm to evaluate @xmath142 as we would not be able to control the space allocation of ad .",
    "rather , we imitate how ad in reverse mode differentiate the algorithm to evaluate @xmath142 , make additional changes to reduce space requirement ( discussed below ) , and then hand - code the resulting algorithm .    _",
    "automatic differentiation _ is a tool that receives a code that evaluates a function as its input and generates a new piece of code that computes the value of the first derivative of the same function at a given point in addition to evaluating the function .",
    "in essence , ad repeatedly applies the chain rule to the given code .",
    "there are two modes in ad , each representing a different approach in applying the chain rule .",
    "_ forward mode _ differentiates each intermediate variable with respect to each input variable from top to bottom .",
    "_ reverse mode _",
    ", on the other hand , differentiates each output variable with respect to each intermediate variable from bottom up , hence the name _",
    "reverse_. note that each entry in a matrix is treated individually .",
    "therefore , one @xmath1 input matrix is treated as @xmath151 input variables @xcite .",
    "one mode is more suitable than the other in different situations .",
    "complexity - wise , forward mode is more appealing when the number of input variables is less than the number of output variables while reverse mode is more appealing when the number of input variables is greater",
    ". let @xmath152 be the computation time of the given code , @xmath153 be the number of input variables of the code , and @xmath154 be the number of output variables .",
    "the code generated by forward mode computes the first derivative in time proportional to @xmath155 while the one generated by reverse mode does so in time proportional to @xmath156 . however , reverse mode has one additional disadvantage : the storage space required may be as large as time complexity of the original code , which can be much larger than the space complexity of the original code , because if a variable is updated many times throughout the evaluation of @xmath157 , its values before and after each such update may be needed .",
    "forward mode does not suffer from this problem because by taking derivatives from top to bottom , the old value of an intermediate variable is not needed after the variable is updated and therefore can be safely overwritten in the same storage space . the storage issue in reverse mode",
    "can be partially fixed by recomputing required values rather than storing them , but this approach may result in significant increase in computation time .    for our problem ,",
    "however , reverse mode can be applied to compute @xmath141 and @xmath140 without increasing storage requirement . by performing reverse mode ad by hand ,",
    "it is seen that all of the intermediate variables can be overwritten safely and thus avoiding the need to store many versions of a variable .",
    "therefore , our method of computing @xmath141 and @xmath140 requires the same order of time and space complexity as the algorithm for evaluating the original function @xmath158 .",
    "analytically , it can be shown that @xmath159 . from the definition of @xmath160 , we see that only entries of @xmath161 in @xmath97 , the chordal extension of the aggregated sparsity pattern @xmath99 , need to be computed in order to compute @xmath162 ( and , consequently , @xmath141 ) .",
    "erisman and tinney showed a method of computing such entries of @xmath161 in the same order of time and space complexity as performing cholesky factorization of @xmath13 in 1975 @xcite .",
    "thus , their method can be used to compute @xmath141 in the same complexity as our proposed method .",
    "nevertheless , our method proves useful as it can be extended to compute the hessian - vector product efficiently .",
    "this idea of imitating reverse ad is not limited to computing derivatives of @xmath142 .",
    "it can also be applied to compute the gradient of @xmath163 with respect to entries in @xmath97 of @xmath13 , which is required in step ( vi ) and ( viii ) of our algorithm .",
    "the most computationally expensive part of evaluating @xmath163 is to cholesky factorize @xmath13 , which is similar to the algorithm for evaluating @xmath142 .",
    "hence , their derivative codes are very similar , and all of the intermediate variables arising from performing reverse mode ad on @xmath163 evaluation algorithm can be safely overwritten , too . the entries in @xmath97 of @xmath164 required in step ( viii ) can also be computed using the same idea since @xmath165 .",
    "we remark that our approach can be much more efficient than the obvious way of obtaining @xmath146 or the gradient of @xmath163 with respect to entries in @xmath97 .",
    "the simple way of obtaining the entries in @xmath97 of @xmath161 is to ( i ) compute the cholesky factorization @xmath166 and then ( ii ) compute the required entries of @xmath161 by using backward and forward substitution to solve linear systems of the form @xmath167 for the required entries of @xmath168 , the @xmath169th column of @xmath161 , where @xmath170 is the @xmath169th column of the identity matrix .",
    "when @xmath13 is sparse , step ( ii ) may be much more computationally expensive than step ( i ) .",
    "one example is when @xmath13 is tridiagonal , in which case , step ( i ) requires only @xmath171 operations while step ( ii ) requires @xmath171 operations per entry of @xmath161 , which can result in a total of @xmath18 operations if the number of nonzeros in @xmath172 is @xmath171 .",
    "on the other hand , our algorithm would require only @xmath171 operations in this case .",
    "although it appears that the sparse - inverse algorithm has not been previously used in semidefinite programming , it has been used elsewhere in the optimization literature .",
    "see , for example , neumaier and groeneveld @xcite .",
    "following the discussion in section [ section_sparse ] , our algorithm works with a partial matrix @xmath105 with specified entries in @xmath97 , a chordal extension of the aggregated sparsity pattern @xmath99 , for primal variable .",
    "for this reason , the computation of the primal search directions are more complicated than the dual ones described in previous section . moreover , as we shall see below , the evaluation of @xmath173 , where @xmath128 is the maximum determinant positive definite completion of @xmath105 and @xmath116 is an arbitrary matrix , appears to be more expensive than performing cholesky factorization .",
    "consequently , the same algorithm used to compute the dual newton direction as described in section [ section_dualnewton ] , which involves evaluation of @xmath173 in each iteration of the conjugate gradient , may not be efficient . therefore in this section ,",
    "we propose a different method for obtaining @xmath43 that avoids excessive evaluation of @xmath173 .",
    "assume @xmath174 is known in addition to @xmath105 ( the detail on the computation of @xmath174 is addressed below in section [ section_xderiv ] ) .",
    "recall from ( [ xinvprop ] ) that @xmath174 has sparsity pattern @xmath97 and therefore is sparse . to compute for @xmath43 , according to step ( ii ) , the problem under consideration is @xmath175 note that @xmath176 is @xmath174 and @xmath177 is @xmath178 .",
    "the kkt condition for the optimum solution to ( [ minn ] ) is @xmath179 or , equivalently , @xmath180 where @xmath181 @xmath182 is a scalar to be determined that enforces the condition @xmath183 @xmath182 . to determine @xmath181 s , eliminate @xmath43 from ( [ lag1 ] ) by taking inner product with @xmath184 @xmath185 on both sides and noting that @xmath186 , yielding the linear system @xmath187 where @xmath160 is defined as in section [ section_dualnewton ] . to rewrite ( [ lagnon ] ) as a standard system of linear equations",
    "form , define the function @xmath188 the system ( [ lagnon ] ) is therefore equivalent to @xmath189 where @xmath190 .",
    "conjugate gradient is then used to solve the system ( [ lag2 ] ) for @xmath191 . after knowing @xmath191",
    ", we can now compute @xmath43 from ( [ lag1 ] ) .    to solve for @xmath181 efficiently with conjugate gradient",
    ", it is important that @xmath192 and @xmath193 are not expensive to evaluate .",
    "this is where @xmath174 becomes useful . from the definition of @xmath194",
    ", we see that we do not need to know the entries outside @xmath97 of the resulting matrices @xmath193 . also , the matrix @xmath174 , unlike @xmath105 , is not a partial matrix , but recall from section [ section_maxdetcompletion ] that @xmath174 has the same sparsity pattern @xmath97 as the partial matrix @xmath105 .",
    "moreover , @xmath195 ( see appendix c of @xcite ) .",
    "therefore , the entries in @xmath97 of @xmath196 can be computed using the idea of automatic differentiation in reverse mode in the same manner as computing @xmath197 , as detailed in section [ section_deriv ] .",
    "the hessian - vector product @xmath192 can also be handled in the same manner as @xmath198 in the dual case .",
    "lastly , the term @xmath128 that is by itself in the quantity @xmath199 of ( [ lag2 ] ) may be replaced by @xmath105 safely as the entries outside @xmath97 of @xmath128 do not affect the equation after the inner product with @xmath184 is taken .      steps ( i)-(ii ) and ( v)-(viii ) of dpd",
    "can be performed using the techniques described in previous sections and the values of the matrices @xmath13 , @xmath105 , and @xmath174 . for step ( ix )",
    ", steepest descent method used to compute step size requires that the algorithm evaluates @xmath200 for a current point @xmath201 to be able to decide when to terminate the steepest descent .",
    "but since we only have the partial matrices @xmath105 , @xmath202 , and @xmath203 , we need to to be able to evaluate @xmath204 after we update @xmath105 as @xmath205 .",
    "computation of @xmath206 is not trivial because , unlike the objective function or the linear constraints , the value of @xmath206 does depend on the entries outside the aggregated sparsity pattern @xmath97 .",
    "we cover an efficient algorithm to compute @xmath206 in this section .",
    "consider a partial symmetric matrix @xmath105 with sparsity pattern @xmath97 , a chordal extension of @xmath99 .",
    "using the factors given in ( [ cliquefactor ] ) , the value of @xmath207 can be evaluated efficiently as follows . because each @xmath208 is unit lower triangular",
    ", its determinant is one .",
    "the determinant of the block diagonal matrix @xmath121 is the product of the determinants of each of its diagonal blocks @xmath209 . observe that @xmath210 in ( [ dsrsr ] ) is the schur complement of @xmath211 in @xmath212 for some permutation matrix @xmath213 .",
    "the determinant of the schur complement is @xmath214 for @xmath215 .",
    "therefore , @xmath216    note that performing cholesky factorization of a positive definite matrix with sparsity pattern @xmath97 requires @xmath217 , where @xmath218 , assuming @xmath117 is a perfect elimination ordering . on the other hand , computing @xmath204 by straightforward application of ( [ logmaxdet ] ) ,",
    "that is , by computing determinants of each @xmath111 and @xmath211 separately , requires @xmath219 operations .",
    "notice that the time required to perform cholesky factorization of a positive definite matrix with sparsity pattern @xmath97 is the lower bound of the computation time of @xmath204 , which occurs when @xmath220 .",
    "for this reason , we seek to find an algorithm that computes @xmath204 in the same order of complexity as that of performing cholesky factorization on the same sparsity pattern .    in the most favorable case",
    "where none of the maximal cliques overlap , straightforward application of ( [ logmaxdet ] ) has the same time complexity as that of cholesky factorization . to see the equivalence of the two algorithms complexity in this case , note that @xmath221",
    "an example of such case is when @xmath105 is block diagonal .",
    "we consider the efficiency of straightforward calculation of ( [ logmaxdet ] ) in the case that the sparsity pattern graph @xmath67 is planar next as this special case arises often in practice .",
    "our analysis assumes that the vertices of @xmath68 are ordered according to the nested dissection ordering .",
    "lipton et al .",
    "introduce generalized nested dissection and show that performing cholesky factorization on said ordering requires @xmath222 operations , where @xmath223 is the number of vertices @xcite .",
    "planar graphs satisfy a _ @xmath224-separator theorem _ , which states that the vertices of the graph @xmath68 can be partitioned into three sets @xmath225 , and @xmath6 such that there are no edges having one endpoint in @xmath20 and the other in @xmath226 , @xmath227 , and @xmath228 . _ nested dissection ordering _ is computed by partitioning @xmath229 into @xmath20,@xmath226 , and @xmath6 according to the separator theorem , number the unnumbered vertices in @xmath6 such that they are eliminated after the unnumbered vertices in @xmath20 and @xmath226 , and then recursively number the unnumbered vertices in @xmath230 and @xmath231 .",
    "the recursion stops when the number of vertices under consideration is less than @xmath232 , at which point , the unnumbered vertices are numbered arbitrarily .",
    "it is shown in lipton et al .",
    "that , for a given @xmath225 and @xmath6 in any level of the recursion , no vertex in @xmath20 is adjacent to any vertex in @xmath226 in the chordal extension graph @xmath107 .",
    "therefore , any maximal clique of @xmath107 can contain at most the vertices in the separator @xmath6 of each recursion hierarchy and additional @xmath232 vertices from the lowest level of recursion .",
    "since each recursion reduces the number of vertices to at most @xmath233 , where @xmath234 is the number of vertices in consideration of the current level , and the separator has at most @xmath235 vertices , the number of vertices in any maximal clique is at most @xmath236 .",
    "the number of maximal cliques is no greater than @xmath223 .",
    "therefore , straightforward calculation of @xmath204 requires @xmath237 operations , which is greater than that of cholesky factorization by a factor of @xmath223 .",
    "as seen from the planar case , straightforward computation of @xmath204 can be significantly more expensive than cholesky factorization .",
    "another common case that suffers from the same problem is when @xmath105 is a banded matrix .",
    "a _ banded matrix _ with _ bandwidth @xmath238 _ satisfies the property that the entry @xmath239 if @xmath240 .",
    "performing cholesky factorization on such a matrix takes @xmath241 operations . to analyze time complexity of @xmath204 computation , first notice that @xmath117 is a perfect elimination ordering and that the sequence of maximal cliques @xmath242 , where @xmath243 @xmath244 , satisfies the running intersection property ( [ running ] ) .",
    "therefore , straightforward computation of @xmath204 requires @xmath245 operations , which is greater than @xmath241 operations of cholesky factorization",
    ".    however , it is possible to reduce the complexity of computing @xmath204 to @xmath241 operations in the banded matrix case by using the following idea .",
    "the determinant of each ( positive definite ) submatrix @xmath111 and @xmath211 is usually computed from the product of diagonal entries of its cholesky factor . if a set @xmath246 ( resp .",
    "@xmath247 ) shares many members with another set @xmath248 ( resp .",
    "@xmath249 ) , @xmath250 , the cholesky factor of a symmetric permutation of @xmath251 ( resp .",
    "@xmath252 ) can be constructed from the cholesky factor of @xmath111 ( resp .",
    "@xmath211 ) or _ vice versa _ , which is more efficient than computing cholesky factor of @xmath251 ( resp .",
    "@xmath252 ) from scratch . in the banded matrix case , any two adjacent cliques @xmath246 and @xmath253 @xmath254 share the same @xmath255 elements @xmath256 .",
    "the same can be said about adjacent @xmath247 s .",
    "observe that @xmath257 @xmath254 and @xmath258 .",
    "therefore , the two adjacent @xmath247 and @xmath259 @xmath260 share @xmath261 elements .",
    "the process of updating a cholesky factor is as follow .",
    "let @xmath119 be the cholesky factor of @xmath111 .",
    "remove the first row of @xmath119 , which corresponds to the @xmath262th row / column of @xmath105 , and let @xmath263 be the resulting @xmath264 submatrix .",
    "we then transform @xmath265 to a @xmath266 upper triangular matrix @xmath267 by performing givens rotations to zero out the @xmath255 entries below the main diagonal of @xmath265 .",
    "notice that the columns of @xmath267 corresponds to the @xmath268th , @xmath269th,  ,@xmath270th columns of @xmath105 .",
    "therefore , @xmath271 is exactly the first @xmath255 rows of the cholesky factor @xmath272 of @xmath273 .",
    "the final row of @xmath272 , which corresponds to the @xmath274th column of @xmath105 , can be computed straightforwardly given the other rows of @xmath272 and @xmath273 .",
    "the same technique can be repeated to construct the cholesky factor of @xmath275 from @xmath272 and so on .",
    "this technique computes @xmath272 in @xmath276 operations ( as opposed to @xmath277 operations if @xmath272 is computed from scratch ) and hence reduces the total time to compute @xmath204 to @xmath241 operations , which is the same order as the complexity of cholesky factorization .",
    "also note that , incidentally , @xmath271 is the cholesky factor of @xmath211 .",
    "this coincidence does not always occur in general case .",
    "this cholesky updating process is not limited to the banded matrix case . in general , given the cholesky factor @xmath119 of @xmath111 ( resp .",
    "@xmath211 ) , the cholesky factor of a symmetric permutation of @xmath251 ( resp .",
    "@xmath252 ) can be constructed by removing the rows of @xmath119 corresponding to @xmath278 ( resp .",
    "@xmath279 ) , performing givens rotation to transform its transpose into an upper triangular matrix , and then appending the rows corresponding to @xmath280 ( resp .",
    "@xmath281 ) . the resulting matrix may not be the cholesky factor of @xmath251 ( resp .",
    "@xmath252 ) but rather of some symmetric permutation of it because the rows corresponding to @xmath280 ( resp .",
    "@xmath281 ) are always appended to the bottom . since permuting a matrix symmetrically",
    "does not affect its determinant , the resulting cholesky factor can be used for determinant computation as is .",
    "roughly speaking , the above technique is more efficient the smaller @xmath282 and @xmath283 are .",
    "larger @xmath282 _ usually _ implies more givens rotations while larger @xmath283 implies more computation of the entries of the appending rows .",
    "however , if the rows to be removed are the bottom rows of @xmath119 , no givens rotations are required ( as the resulting matrix remains lower triangular ) .",
    "therefore , large @xmath282 does not imply many givens rotations in this case .",
    "hence , we are able to compute @xmath204  optimally \" ( in the sense of within the same order of complexity as performing cholesky factorization ) in two special cases : block diagonal and banded matrices",
    ". however , there are cases where it seems @xmath204 can not be computed  optimally \" using cholesky updating scheme , for example , the planar graph case .",
    "it is this reason that prevents us from using the dual algorithm described in section [ section_dualnewton ] to compute the primal projected newton direction as it requires evaluation of @xmath284 in each iteration of the conjugate gradient and evaluating @xmath284 is generally at least as expensive as evaluating @xmath204 .",
    "as mentioned in section [ section_primalnewton ] , computing @xmath43 in step ( ii ) of dpd requires the knowledge of @xmath285 .",
    "in addition , steps ( iii ) , ( iv ) , and ( ix ) also call for @xmath176 and @xmath286 in the formula for @xmath64 and in the steepest descent direction , respectively . from ( [ logmaxdet ] ) , the matrix @xmath174 is seen to be @xmath287 recall that @xmath288 and @xmath289 are completely dense . for this reason ,",
    "using automatic differentiation would not yield a more efficient first - derivative computing algorithm than simply computing @xmath290 and @xmath291 conventionally ( by finding their inverses ) and piecing them together according to ( [ logmaxdetderiv ] ) .",
    "the same is true with the product of the second derivative and an arbitrary matrix @xmath292 recall that @xmath293 .",
    "therefore , our algorithm computes @xmath294 by the simple algorithm described above .",
    "the computation of @xmath295 is also handled similarly : by computing the product of the second derivative of each dense submatrix and @xmath296 conventionally and then piecing them together .",
    "the cholesky updating technique as described in section [ section_maxdet ] can also be applied to the computation of @xmath294 and @xmath295 .",
    "both of these computations involves computing the cholesky factor of each clique in order to compute its inverse .",
    "hence , the same technique can be applied to reduce the computation time required to find the cholesky factors .",
    "we give estimates of time and space complexities of our algorithm in this section .",
    "let @xmath297 and @xmath298 denote the time and space complexity of cholesky factorizing a matrix with sparsity pattern @xmath97 , respectively",
    ". steps ( i ) and ( v ) do not require any computation .",
    "step ( ii ) involves a conjugate gradient to solve ( [ lag2 ] ) .",
    "each iteration of the conjugate gradient requires one evaluation of the hessian - vector product @xmath192 , which is @xmath297 and @xmath298 .",
    "the conjugate gradient takes at most @xmath223 iterations to converge .",
    "step ( iii ) requires one evaluation of @xmath286 .",
    "step ( iv ) requires one evaluation of @xmath176 and @xmath286 each .",
    "step ( vi ) involves a conjugate gradient that takes one evaluation of the hessian - vector product @xmath198 and therefore requires @xmath297 and @xmath298 per conjugate gradient iteration .",
    "the conjugate gradient also takes at most @xmath223 iterations to converge .",
    "step ( vii ) requires one evaluation of the entries in @xmath97 of @xmath164 , which is @xmath297 and @xmath298 . step ( viii )",
    "calls for one evaluation of the entries in @xmath97 of @xmath299 and @xmath164 each .",
    "finally , step ( ix ) requires one evaluation of @xmath204 per iteration of steepest descent .",
    "a few steps in the algorithm , namely steps ( iii ) , ( iv ) , and ( ix ) , require evaluation of either @xmath204 or one of its derivatives . generally , evaluating @xmath204 or its derivative",
    "is more expensive than @xmath297 .",
    "fortunately , the computational results in section [ section_exp ] show that only a constant number of steepest descent iterations are needed to find good step size , and steps ( iii ) and ( iv ) only require at most two evaluations of such quantities",
    ". space complexity of an evaluation of @xmath204 , on the other hand , is still @xmath298 as evaluating @xmath204 reduces to computing the cholesky factor of each of the maximal cliques without having to store the cholesky factor of more than one clique at a time . for the case where @xmath67 is planar and @xmath97 is its chordal extension when the vertices @xmath229 are ordered in nested dissection ordering , computing @xmath204 ( and , consequently , each iteration of the steepest descent method ) requires @xmath15 operations and @xmath16 space .",
    "notice that @xmath15 operations for these steps are acceptable as each of the conjugate gradient takes @xmath300 operations per iteration and at most @xmath223 iterations to converge , resulting also in @xmath15 operations for steps ( ii ) and ( vi ) .    as the last remark for this section",
    ", we note that we suspect that this estimate of @xmath15 for planar case may not be tight . for the special case that @xmath68 is a grid graph",
    ", it is not hard to show that computing @xmath204 requires only @xmath222 operations , which is the same order as performing cholesky factorization .",
    "we implemented and tested our algorithm by using it to solve various instances of the problem of finding maximum cut ( max - cut ) .",
    "the procedure of using sdp to solve max - cut is proposed by goemans and williamson in 1995 @xcite .",
    "readers are referred to goemans and williamson s paper for the details of the procedure . in max - cut , the input graph whose maximum cut is sought is exactly the aggregated sparsity pattern @xmath99 of the resulting sdp program .    given the aggregated sparsity pattern @xmath99 , we find its chordal extension by ordering the vertices of @xmath99 according to the symmetric minimum degree ordering @xcite , perform symbolic cholesky factorization on the reordered matrix , and use the resulting cholesky factor as the chordal extension .",
    "the primal partial variable is initialized to the identity matrix .",
    "the dual variable is initialized to @xmath301 after @xmath6 has been reordered according to the minimum degree ordering . after the algorithm finds an iterate",
    "whose duality gap is less than @xmath302 , it continues for 3 additional iterations and then terminates .",
    "each conjugate gradient runs until @xmath303 is less than @xmath304 times the 2-norm of the constant term of the system that the algorithm is trying to solve .",
    "finally , step ( ix ) of the algorithm is implemented using the method of steepest descent starting from four initial points @xmath305 , @xmath306 , @xmath307 , and @xmath308 separately ( refer to chapter 6.5.2 of @xcite for the explanation of the method of steepest descent ) .",
    "we do not perform any line searches in the steepest descent ; we simply take the step size to be identically one and take the step as long as the new point decreases the potential .",
    "line searches are ignored because , according to our testing , performing line searches does not generally improve computation time of the algorithm .",
    "the additional evaluation of @xmath204 in each step of the line searches appears to be too expensive compared to the extra decrease in potential resulted from them .",
    "the test instances were generated by adding edges to the graph randomly until the chosen number of edges were met .",
    "the number of main iterations reported in column 5 of table [ table_main ] is the number of times the algorithm repeats step ( i ) to ( x ) before it finds an optimal solution is found .",
    ".summary of results of the algorithm on random instances . from left to right , the columns are the number of vertices , the number of edges , the number of trials run , the average cpu time in seconds , the number of main iterations , the average number of conjugate gradient iterations required to compute the search direction @xmath63 for one point , the average number of conjugate gradient iterations required to compute @xmath130 for one point , and the average iterations to minimize potential along the four directions for one starting point ( step ( ix ) in the algorithm ) . [",
    "cols=\">,>,>,>,>,>,>,>\",options=\"header \" , ]     table [ table_4vs2 ] shows that using all four directions make the algorithm find the optimal solution in shorter time in all test cases .",
    "the reason toward this result is that all of the quantities involved in computation of the two uncommon directions are also required to compute the other two projected newton directions .",
    "therefore , computation of the additional two unusual directions is relatively cheap compared to the reduction in potential they induce .    finally , we tested the computation of @xmath204 with cholesky updating scheme on banded matrices to verify its @xmath241 , or more precisely , @xmath309 complexity .",
    "we began by fixing the bandwidth @xmath238 to be 3 and varying @xmath223 from 6 to 40 , repeated 500 times for each @xmath223 .",
    "figure [ figure_band_vary_n ] shows the plot of the average cpu time to compute @xmath204 for banded matrices with bandwidth 3 of various size against the number of vertices , and it confirms the linear dependency on @xmath223 of the complexity .",
    "next , we fixed @xmath310 to 10 and varying @xmath238 from 1 to 40 , repeated 50 times each .",
    "the plot of the average cpu time against the square of the bandwidth for this experiment is shown in figure [ figure_band_vary_p ] .",
    "the plot agrees that the complexity of @xmath204 is proportional to @xmath311 in the banded case .     in the case that @xmath105 is a banded matrix .",
    "bandwidth is fixed to 3 while varying the number of vertices.,scaledwidth=90.0% ]     in the case that @xmath105 is a banded matrix .",
    "the quantity @xmath310 is fixed to 10 while varying the bandwidth.,scaledwidth=90.0% ]",
    "we showed an implementation of a sdp solver that exploits sparsity in the data matrices for both primal and dual variables .",
    "our algorithm is based on the primal - dual potential reduction method of nesterov and nemirovskii and uses partial primal matrix variable as proposed by fukuda et al .",
    "two of the search directions are projected newton directions that can be found by solving linear systems involving the gradient and the hessian of the logarithm of the determinant of a matrix with respect to a vector .",
    "we observed that the idea from reverse mode of automatic differentiation can be applied to compute the mentioned gradient and the product of the hessian and an arbitrary vector efficiently , which is in the same order as computing determinant of a sparse matrix . using this observation , we solve the linear system for the search directions by conjugate gradient , which requires one evaluation of the product of the hessian and a vector in each iteration in exchange for not having to factorize the hessian matrix . for the primal case",
    ", we propose a way to compute one of the primal search directions without requiring the determinant of the positive definite completion in each iteration of the conjugate gradient because the determinant of such completion is generally more expensive than performing cholesky factorization .",
    "this determinant is still required in the potential minimization to find step sizes as well as in the course of computing one other search direction .",
    "we described a technique to reduce the complexity of computing the logarithm of the determinant of a positive definite matrix completion by reusing the cholesky factors when there are many overlaps of maximal cliques .",
    "this technique reduces the complexity to that of performing cholesky factorization in the banded matrix case but still can not achieve the same complexity as the cholesky factorization in general .",
    "fortunately , only a few number of evaluations of the determinant of such completion is required per one sdp iterate .",
    "the other two non - newton directions can be computed efficiently since they require the same quantities that are already computed in the process of finding the former projected newton directions .",
    "we then tested our algorithm on random instances of the max - cut problems . from the results ,",
    "the conjugate gradients do not require too many iterations to converge for the algorithm to be impractical .",
    "there are questions unanswered in this paper that can help improve the algorithm described here .",
    "for example , can we compute the logarithm of the determinant of a positive definite matrix completion more efficiently , perhaps by another derivation different from ( [ cliquefactor ] ) ?",
    "the other issue is regarding the stability .",
    "how can we incorporate preconditioning to mitigate the ill - conditioning of the linear system problems ?",
    "regardless , our algorithm should prove efficient in the applications where the data matrices are sparse ."
  ],
  "abstract_text": [
    "<S> in this paper , we show a way to exploit sparsity in the problem data in a primal - dual potential reduction method for solving a class of semidefinite programs . </S>",
    "<S> when the problem data is sparse , the dual variable is also sparse , but the primal one is not . to avoid working with the dense primal variable , we apply fukuda et al.s theory of partial matrix completion and work with partial matrices instead . </S>",
    "<S> the other place in the algorithm where sparsity should be exploited is in the computation of the search direction , where the gradient and the hessian - matrix product of the primal and dual barrier functions must be computed in every iteration . by using an idea from automatic differentiation in backward mode , </S>",
    "<S> both the gradient and the hessian - matrix product can be computed in time proportional to the time needed to compute the barrier functions of sparse variables itself . moreover , </S>",
    "<S> the high space complexity that is normally associated with the use of automatic differentiation in backward mode can be avoided in this case . </S>",
    "<S> in addition , we suggest a technique to efficiently compute the determinant of the positive definite matrix completion that is required to compute primal search directions . </S>",
    "<S> the method of obtaining one of the primal search directions that minimizes the number of the evaluations of the determinant of the positive definite completion is also proposed . </S>",
    "<S> we then implement the algorithm and test it on the problem of finding the maximum cut of a graph .    </S>",
    "<S> [ section ] [ section ] </S>"
  ]
}