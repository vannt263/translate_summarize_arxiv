{
  "article_text": [
    "sphere organized an activity recognition competition in conjunction with ecml - pkdd and drivendata .",
    "the goal was to recognize activities - postures and movements - from sensor data collected from participants .",
    "our solution reached second prize .",
    "hopefully , this paper will contain sufficient information to reproduce our results , and we will try to make it transparent when our choices were time - driven rather than following proper scientific method .    the paper is organized as follow : we first introduce the challenge s goal and its data .",
    "then , we describe our approach to create a train set suitable for the machine learning paradigm .",
    "the next sections deal respectively with the feature engineering , the machine learning models and the post - processing we used in the competition .",
    "the goal of this challenge was to predict , on a second - by - second basis , a person s activity based on sensor data .",
    "it was modeled as a multi - class classification problem .",
    "the target variable could take 20 different values , representing the individual s activities , postures and transitions : ascend stairs , descend stairs , jump , walk with load , walk , bending , kneeling , lying , sitting , squatting , standing , stand - to - bend , kneel - to - stand , lie - to - sit , sit - to - lie , sit - to - stand , stand - to - kneel , stand - to - sit , bend - to - stand and turn .",
    "several annotators have manually defined the ground truth for the target variable .",
    "for instance , if `` jump '' is given a value of 0.05 at a given second , this should be interpreted as meaning that on average the annotators marked 5% of that second as arising from jumping .",
    "for this contest , the sphere team equipped a house with three sensor modalities .",
    "participants wore a tri - axial accelerometer on their dominant wrist .",
    "the device wirelessly transmits the value of acceleration to several receivers positioned within the house .",
    "this device gives two valuable pieces of information .",
    "first , the value of acceleration , in three directions .",
    "second , the signal power that was recorded by each receiver ( in units of dbm ) - this data will be informative for indoor localization .",
    "three cameras were used in the living room , hallway and kitchen .",
    "automatic detection of humans was performed . in order to preserve the anonymity of the participants ,",
    "the raw video data are not shared . instead , the coordinates of the 2d bounding box , 2d centre of mass , 3d bounding box and 3d centre of mass are provided .",
    "the values of passive ( pir ) sensors positioned within the house are given .",
    "+ in order to generate the train data , 10 participants successively performed a script of daily - life actions in this house .",
    "hence , the train data consists of 10 continuous sequences of monitoring .",
    "each sequence was recorded on a second - by - second basis and lasts approximately 30 minutes .",
    "+ the test data was generated by 10 other participants who followed the same script of daily - life actions : these 10 test sequences of monitoring were also recorded on a second - by - second basis and have a similar duration .",
    "however , instead of supplying 10 continuous test sequences of 30 minutes of monitoring , the sphere team randomly split these 10 long sequences into 800 smaller subsequences . to do so , they iteratively sampled a subsequence duration and a number of seconds to drop between two subsequences .",
    "the subsequence duration was chosen to follow a uniform distribution between 10 and 30 seconds .",
    "the gap length follows a similar distribution .",
    "these subsequences were finally permuted so that it would be difficult to reconstitute the whole 30-minute test sequences .",
    "this was probably done to force the inference of test sequences to be independent of the daily - life action script .",
    "the competition entrants model would thus have to generalize to other scripts and participants , which would make them useful in real - life situations .",
    "submissions to the competition are evaluated with the brier score defined as :    @xmath1    where @xmath2 is the number of test sequences , @xmath3 is the number of classes , @xmath4 is the predicted probability of instance @xmath5 being from class @xmath6 , @xmath7 is the proportion of annotators that labeled instance @xmath5 as arising from class @xmath6 , and @xmath8 is the weight for each class .",
    "lower brier scores indicate better performance , and optimal performance is achieved with a brier score of 0 .",
    "class weights place more weight on the classes that are less frequent .",
    "the first step was to change the structure of the train set to have its distribution follow that of the test set more closely .",
    "therefore , we randomly split the 10 train sequences of 30 minutes into 800 smaller subsequences of 10 to 30 seconds , to follow the test set creation methodology .    by doing this random splitting several times , with different random seeds ,",
    "it is possible to generate several train sets .",
    "then , we could follow a bagging approach : create one model per train set and average their predictions .",
    "this approach showed good results in cross - validation , but due to time constraints it was not part of our final model .",
    "+ the second operation was to optimize a hard target . in order to have an easier integration with existing python machine learning libraries ( such as scikit - learn and xgboost ) , we converted our probabilistic ( soft ) target into a hard target , by keeping for each line the target label with the highest probability .",
    "a good cross - validation strategy is crucial to have a faithful estimation of the performance of our models on the leaderboard and to avoid overfitting . in the competition ,",
    "train and test sets were generated using two distinct groups of participants .",
    "since it was crucial for our model to generalize to unknown people , we split up the train data into :    * a train subset : data generated by all individuals but number 6 and 10 * a validation subset : data generated by individuals 6 and 10 +    we observed for each model a rather constant gap between our evaluation and the score obtained on the public leaderboard .",
    "so every improvement in our local cross - validation score led to a similar improvement on public leaderboard .",
    "note that this cross - validation strategy might not be optimal .",
    "it might even cause overfitting if individuals 6 and 10 turn out to be more similar to the individuals involved in public leaderboard , than to those involved in the private one .",
    "the raw train and test datasets contained 119 features .",
    "however , many of these features are highly correlated or are at a level of granularity too refined .",
    "for instance , each camera gives the x , y and z - coordinates of the individual s centre of mass .",
    "but the camera records at 25 frames per second . for every second",
    ", we kept the mean , median , min , max and standard deviation of these 25 coordinate values .",
    "hence we got 5 features that describe , for every second , the coordinate values of the individual s centre of mass . similarly , the accelerometers sample at 20 hz .",
    "so , for a given second of monitoring , we keep the mean , median , min , max and standard deviation of the 5 acceleration values generated by the accelerometer .",
    "first , we extracted basic features : speeds , accelerations , derivatives of acceleration , second derivatives of acceleration and rotations .",
    "we noted that the accelerometer was fixed on the individual s dominant wrist .",
    "figure [ fig : acc ] clearly highlights two distributions of y - accelerations : one for right - handed individuals and another one for left - handed ones . to correct this bias , we multiplied accelerometer x and y data by -1 for left - handed individuals .",
    "+        [ fig : acc ]      the previous features exploit the current value of sensors data .",
    "but they do not exploit the past or future sensors data . in order to take into account the time series component of the problem",
    ", we added lagged variables : values of existing features 1 to 10 seconds before .",
    "note how important it is to make train and test sets look alike . in the test set ,",
    "lagged values are always empty for the first line of each subsequence .",
    "whereas in the train set , if we had nt split the 30-minute sequences into subsequences , lagged values would hardly be empty .",
    "we thus avoided a covariate shift on lagged variables .",
    "+ experimentally , adding lags helped our model perform well , so we also added leads .",
    "that is we added new variables giving values of features 1 to 10 seconds after .",
    "one may wonder whether adding leads makes sense in real - life applications .",
    "should we really wait a couple of seconds before sending help to an individual , in order to add leads to our model and make sure that the individual actually has a problem ? yet , given the context of the challenge , we decided to exploit this test set artifact .",
    "+ when looking at random forest or gradient boosting trees feature importance , we noted that lead variables were as important as lagged ones .",
    "moreover and quite naturally , the importance of the 1 second lag / lead variables was greater than the importance of 2 second ones , etc .",
    "the room variable indicates the room where the individual is located .",
    "intuitively , this variable should be very useful to predict activity : for instance , when someone is in the toilets , he is probably not jumping nor lying down .",
    "unfortunately , this room variable is available in the train set , but missing in the test set .",
    "we here propose a technique , that we call _ stack transferring _ , to propagate this information .",
    "it consists in three steps .",
    "on the train set , we replace the exact values of the room variable by out - of - folds predictions of the room variable . in other words ,",
    "use 9 folds of the train set - corresponding to 9 participants out of 10 - to predict the room variable on the remaining fold . by doing so 10 times",
    ", we can predict the room variable on all the train set .",
    "this generates out - of - folds predictions of the room variable on the train set .",
    "we can now update the room variable on the train set by dropping the exact values of the room variable and keeping its out - of - fold predictions .",
    "the room variable being missing on the test set , we can add it as follows . in step 1 ,",
    "we have trained 10 models that predict room variable .",
    "we simply apply them to the test set and average these 10 predictions .",
    "now , the room variable should be available on the test set .",
    "now that we have updated the room variable on the train set , and that we have predictions of the room variable on the test set , we can add this variable to the model .",
    "it should improve the activity prediction .",
    "notice that the individuals from train and test sets were asked to perform the same list of actions in the same order .",
    "therefore , the room variable had the same distribution on train and test sets .",
    "this is a necessary condition for stack transferring to perform well . + eventually , feature engineering increased the number of variables from 119 to 2700 .",
    "table [ tab : feat ] shows the top 15 features by importance for a random forest trained on the engineered train set .",
    "5 of them come from our feature engineering .    .top",
    "15 most important features for our level - one random forest learners .",
    "features colored in green come from feature engineering . [",
    "cols=\"^,^\",options=\"header \" , ]     [ tab : feat ]",
    "it is in general a good idea to start with a simple model that does not need much tuning - for instance a random forest - while doing feature engineering .",
    "they are easy to implement and able to handle large amounts of variables , so they give valuable feedback on the quality of our work .",
    "feature engineering diminished our random forest s error - rate from 22% to 16.4% , ranking us @xmath9 of the competition .",
    "+ when performance seemed to reach a plateau even when we were adding new features , we tried other models that require more tuning .",
    "we then went on for the machine learning blockbuster , xgboost .",
    "we grid - searched its parameters - max depth , min child weight , column sample by tree , subsample - and derived the optimal number of estimators thanks to an early stopping on users 6 and 10 .",
    "optimizing xgboost typically took one hour on our 12 cores computer , which was fast enough to explore a great number of feature combinations .",
    "+ the xgboost classifier can optimize its predictions for a given loss function .",
    "this loss function can be chosen among several pre - implemented loss functions .",
    "but the metric of the challenge - brier score - is not one of them .",
    "so , we chose a random pre - implemented loss function - logloss .",
    "it is not an optimal solution , because minimizing logloss should not necessarily lead to minimizing the brier score .",
    "however , this already performed very well : our error rate reached 14.6% and ranked us top 5 .",
    "we could then try to customize the xgboost code to make it optimize the brier score loss function instead of logloss .",
    "our goal was to make the xgboost classifier optimize its predictions for the metric of the challenge - the brier score .",
    "xgboost provides a python api to customize softmax loss functions , by defining their gradient and hessian .",
    "the first step was to define the softmax brier score loss function :    ( ) = _ n=1^n _ c=1^c w_c ( _ n , c ( ) - y_n , c)^2    where @xmath10 and @xmath11 are respectively equal to @xmath12    we can then implement the loss gradient and hessian based on the following expressions . notice that xgboost does not work with the exact hessian but with its diagonal approximation .",
    "( ) = _ n=1^n _ n , c_0 ( )    & ( ) = _ n=1^n &    unfortunately , the xgboost python api only allows this easy customization of loss function when the target is a hard target . in the sphere challenge ,",
    "the target is probabilistic .",
    "an easy way to deal with this issue was to convert the probabilistic ( soft ) target into a hard target , by keeping for every line the label that has the highest probability .",
    "this inevitably generated an approximation in the metric optimized by xgboost .",
    "we managed to minimize this approximation by duplicating lines on our train dataset .",
    "for instance , for a given line , if label a has a probability of 0.7 , label b of 0.1 and label c of 0.2 , then we would create k new lines : @xmath13 lines that would have label a as hard target , @xmath14 line would have label b as hard target and @xmath15 lines would have label c as hard target . by doing so , our xgboost would optimize the following approximate softmax brier score :    _ approx ( ) = _ n=1^n _ c=1^c w_c(y_n , c^2- 2y_n , c & & + + )    which is quite close to the exact softmax brier score : @xmath16    we refer to k as a resolution parameter , that governs the approximation in the brier score metric made by xgboost when dealing with a hard target instead of a probabilistic one .",
    "higher values of k reduce this approximation .",
    "we have implemented this method with @xmath17 , meaning that our train dataset consisted of 100,000 lines and 2700 features : but xgboost training time was too long .",
    "+ therefore , the only solution was to fork the c++ xgboost source code to make it accept customized loss functions even when the target is probabilistic .",
    "this was much trickier , but unsurprisingly it gave slightly better results than the traditional xgboost .",
    "customized xgboost decreased our cv score from 0.1817 to 0.1814 .",
    "once we had trained 10 individual models - including linear regressions , naive bayes classifiers , random forests , extra - trees and xgboost models - , we opted for ensemble learning methods .",
    "a grid - searched xgboost combined the predictions of our individual models and leveraged their strengths .",
    "it turned out to be very efficient : it reduced our error rate to 12.9% and ranked us number 1 at that point .",
    "the previous approaches consider each prediction independently .",
    "however , it seems very unlikely that a person lying on a bed can be jumping the next second .",
    "this means that there are chances that transitions from one activity to another follow different probabilities .",
    "this mathematical property is known as the markov chain property .",
    "a great way to take advantage of this underlying structure is to implement hidden markov models .      yet , given the deadline , we did not have time to implement hmm models .",
    "we rather opted for a post - processing that smooths predictions over time .",
    "the idea is to make a weighted average between the activity predictions of a given second and the activity predictions of the last two seconds and of the future two seconds .",
    "we optimized the coefficients of this weighted average .",
    "post - processing gave tremendous cross - validation results , with an error rate around 11% .",
    "however , we did not have time to submit it in our final model .",
    "in this paper , we presented our solution to sphere challenge as well as several techniques that may have worked if we had more time .",
    "our final solution is based on a rich pre - processing and cutting - edge machine learning methods .    after recreating a train set similar to the test set",
    ", we perform feature engineering . to our knowledge ,",
    "what we call `` stack transferring '' - the idea of using predictions of a variable known in the train set but not in the test set as features - is new .",
    "the final model is based on the stacking of weak learners through a grid searched xgboost algorithm .",
    "+ our solution won the second prize of the challenge on the private leader - board , though we were ranked first on the public one .",
    "we hope that this work can modestly contribute to finding better way to detect old people fall for a quicker intervention .",
    "we thank dataiku for allocating time and servers for our team .",
    "we also thank drivendata , ecml - pkdd and sphere for organizing and hosting this contest and supplying these valuable datasets ."
  ],
  "abstract_text": [
    "<S> our team won the second prize@xmath0 of the safe aging with sphere challenge organized by sphere , in conjunction with ecml - pkdd and driven data . </S>",
    "<S> the goal of the competition was to recognize activities performed by humans , using sensor data . </S>",
    "<S> this paper presents our solution . </S>",
    "<S> it is based on a rich pre - processing and state of the art machine learning methods . from the raw train data </S>",
    "<S> , we generate a synthetic train set with the same statistical characteristics as the test set . </S>",
    "<S> we then perform feature engineering . </S>",
    "<S> the machine learning modeling part is based on stacking weak learners through a grid searched xgboost algorithm . </S>",
    "<S> finally , we use post - processing to smooth our predictions over time . + </S>"
  ]
}