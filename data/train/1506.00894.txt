{
  "article_text": [
    "computers are physical systems and information has to be stored and transmitted using physical devices .",
    "this trivially sounding statement has led to very important insights as soon as one starts to ask for the fundamental physical limits of computation .",
    "the most known statement is probably landauer s principle : erasing a data set with information content ( i.e. , entropy ) @xmath0 causes a minimum heat dissipation of @xmath1 if @xmath2 is the inverse temperature of the surrounding environment .",
    "this was first formulated by landauer in 1961  @xcite . more generally , it was argued by bennett  @xcite and others @xcite , that each logically irreversible operation ( as , e.g. , information erasure ) must be accompanied with a corresponding heat dissipation whereas _",
    "each logically reversible operation can be implemented  at least in principle  in an energetically neutral way . _",
    "the heat flow of such computers , if operated slowly enough , is then equal to the change in their shannon entropy ( times @xmath3 ) demonstrating the usefulness of the shannon entropy to describe thermodynamic processes .",
    "although the question about the relation between logical and thermodynamical reversibility sounds like a purely academic question , it is indeed of practical importance because one of the limitations of todays computers lies in the heat which they produce during computation and which is hard to be drained off quickly enough .",
    "in addition , it is well - known that the thermodynamics of computation can be successfully applied to resolve the famous maxwell demon paradox  @xcite .    today",
    ", it seems that most physicists accept the thermodynamics of computation as a well established field and also feynman notes ( page 160 in  @xcite ) : `` i see nothing wrong with his [ bennett s ] arguments .",
    "[ ... ] i concluded that there was no minimum energy [ consumption of computers ] . '' indeed , however , criticism was raised against bennett s exorcism of maxwell s demon and the thermodynamics of computation already some time ago  @xcite , which was subsequently defended by bub and bennett again  @xcite , but criticism and controversies still prevail for different reasons , mainly ( but not only ) on the philosophical side @xcite .",
    "an important class of physical models used to illustrate the thermodynamics of computation are inspired by biochemical processes as , e.g. , dna replication  @xcite . indeed , copying a dna strand can be regarded as a simple computational task where the dna strand represents a certain input signal , which is manipulated by enzymes to produce an identical copy of the input .",
    "energetic barriers in the computational path , i.e. , barriers between two logical states of the computation , can be overcome by the random thermal motion of the molecules involved and a bias in chemical potentials can be used to drive the computation in a desired direction . for a small enough chemical bias the average dissipation of energy per step can be made arbitrarily small and thus , one usually concludes that computation can be carried out thermodynamically reversibly .    in a more general frame , systems which use the random thermal motion of its components to perform a computation",
    "are usually called _",
    "brownian computers_. however , in addition to the arguments presented above , a detailed and general mathematical treatment of such computers seems to be missing in the literature .",
    "indeed , the authors of refs .",
    "@xcite based their reasoning largely on ingenious arguments instead of detailed calculations . if one finds more detailed ( yet not very general calculations ) in the literature",
    "@xcite , then they seem to contradict the statement that brownian computers are thermodynamically reversible .    in the present contribution we will therefore treat the subject of brownian computers in general , i.e. , without having any specific computational task or physical problem in mind",
    ". we will start by considering an arbitrary turing machine ( tm ) , which is known to be a model for a general purpose computer .",
    "this means that for each computable function ( or algorithm ) there exists a tm which can implement it .",
    "moreover , it is even possible to construct a special tm , called a universal tm , which is able to simulate any other tm and hence , tms are said to be computationally universal ( an introduction to tms can be found in refs .",
    "@xcite ) . based on the ideas of bennett",
    "we will then show how to construct a logically reversible tm and in addition , we show how to model this tm by a continuous - time markov process , i.e. , by a markovian master equation ( me ) .",
    "the resulting model is then able to compute in a stationary regime where it transforms a string of incoming symbols ( the inputs of the computation ) into a corresponding string of outgoing symbols ( simply called the outputs ) .",
    "the big advantage of a description in terms of a me is that its thermodynamic behaviour is well understood since many years  @xcite , and also their stochastic behaviour can be treated within a consistent thermodynamic formalism , which is known as stochastic thermodynamics  @xcite .",
    "indeed , there has been a large interest recently in using small autonomous machines describable by , e.g. , a markovian me , to address questions of information processing as , e.g. , sensing , feedback or adaptation , in a thermodynamic context .",
    "such machines were studied for abstract models  @xcite , more general settings  @xcite , in a biochemical context  @xcite or in the field of artificial nanostructures  @xcite .",
    "furthermore , we have reached the realm where experiments are performed at the landauer limit  @xcite . however , to the best of our knowledge , there is no reference which has treated the question of how to describe a _ general _",
    "computer within a me framework and which has worked out its thermodynamic consequences in detail .",
    "we will find out that the entropy production rate of a brownian computer can be made arbitrarily small while the total entropy production grows logarithmically with the number of computational steps , thereby resolving a part of the controversy between bennett , norton and others .",
    "_ outline : _ because the treatment of tms is no standard subject taught in physics , we will give an introduction to it in sec .",
    "[ sec tms ] to make the paper as self - contained as possible .",
    "then , in sec .",
    "[ sec stochastic tm ] we will first show how to build a tm in a logically reversible way before we demonstrate how to map it to a me .",
    "because the details of a logical reversible tm are a bit technical but not of major importance for a general understanding of the rest of the paper , we will shift them to appendix  [ app logical reversible tm ] . finally , after we have discussed the structure of the me in sec .  [ sec stochastic tm ] , we discuss the thermodynamics of brownian computation in sec .",
    "[ sec thermodynamics brownian computation ] . a last section is then devoted to a summary of our results and an outlook on interesting future work .",
    "simple sketch of a tm : the machine , specified by a state @xmath4 , has access to an infinite tape , which is divided into equal squares . the machine scans with its head always only one square on the tape , on which it finds a symbol @xmath5 ( which might be also the blank symbol @xmath6 ) written .",
    ", scaledwidth=25.0% ]    a tm @xmath7 is an idealized machine to model or simulate problems in computer science ( see fig .",
    "[ fig tm sketch ] ) . in the standard treatment @xmath7 has two parts : first , we have the machine itself , which can be in some state @xmath8 where @xmath9 denotes the finite set of internal machine states .",
    "second , the machine has access to an external storage medium ( usually called the _ tape _ ) , which is divided into equal _ squares _ and each square contains either a special symbol @xmath10 ( where @xmath11 is again a finite set ) or it contains a blank @xmath6 . the machine has a _ head _ with which it is coupled to one and only one square of the tape at each point of time .    at each time step",
    ", the machine in state @xmath4 reads the symbol @xmath5 written on the square and subsequently it changes its own state to @xmath12 , writes a new symbol @xmath13 on the square ( which can be the same as the old one ) and either shifts the tape one square to the left or to the right or stays where it is .",
    "mathematically , these rules can be defined by three functions @xmath14 and @xmath0 : @xmath15 where @xmath16 encodes the direction of movement of the tape with @xmath17 meaning move the tape right , do not move it , move it left .",
    "these three maps ( together with an agreement about the initial state , see below ) _ completely define _ the action of @xmath7 .",
    "often one writes these maps in terms of a so - called quintuple @xmath18 or simply as @xmath19    a computation of @xmath7 is then defined as follows : the machine starts initially in a special state r @xmath20 ( we use r for `` ready '' ) and scans by convention the first blank symbol to the left of a finite number of input symbols @xmath21 written on the tape .",
    "note that we demand that there is no blank symbol in between the input symbols and that the input string is finite .",
    "then , @xmath7 starts to move right and reads the first input symbol @xmath22 .",
    "it then proceeds according to the rules above [ eq . ( [ eq tm standard map ] ) ] . after some time",
    "the tm might be done with the computation .",
    "we then assume that it shifts to the first blank symbol to the right of the remaining string of symbols @xmath23 written on the tape and then changes to a special final state h ( h for `` halt '' ) .",
    "the string @xmath24 is called the output or the _ result _ of the computation , which is again finite but not necessarily of the same length as the input ( i.e. , @xmath25 is possible ) . in short ,",
    "we will also write a computation as @xmath26 or @xmath27 .",
    "hence , we see that the idea of a tm is very simple : given @xmath28 , i.e. , a tm @xmath7 with input @xmath29 in standard format as above , it follows the rules ( [ eq tm standard map ] ) until it is done with the computation . especially , we note that there is only a _",
    "finite _ set of quintuples or rules ( [ eq tm standard map ] ) because @xmath9 and @xmath11 were assumed to be finite sets . introducing @xmath30 and @xmath31 ( with `` @xmath32 '' denoting the cardinality of a set )",
    ", we see that any tm is completely specified by @xmath33 many quintuples .",
    "note that we also need a rule for the machine if it scans a blank symbol , hence the factor @xmath34 . in view of these facts",
    "it seems very remarkable that , first of all , tms are capable of universal computation and , second , that they can show an incredibly complex behaviour .",
    "the first property is related to the church - turing thesis  which has to be taken for granted though  which states that every intuitvely computable function can be computed by a tm .",
    "the second property is reflected , for instance , in the fact that it is _ impossible _ to design a tm @xmath35 , which tells us for an arbitrary given tm @xmath7 and input @xmath29 whether @xmath28 will halt or not .",
    "this is the famous halting problem .",
    "thus , it might be that the computation defined above never reaches the state h and goes on forever . in this case",
    ", the computation simply has no result .",
    "thus , the incredible power of tms ( namely their computational universality ) has a serious drawback ( namely their in general unpredictable behaviour ) .",
    "a much more detailed account of tms can be found in refs .",
    "sketch of the general setup , which allows us to analyze any abstract computational process in terms of thermodynamic quantities .",
    "the upper tape corresponds to the output tape , which is initially blank , and the lower tape corresponds to the input tape . note that we will always assume that blank tapes are for free , i.e.",
    ", the machine can have as many blank space on which it can write as it requires .",
    "furthermore , the machine itself has access to two additional internal tapes ( not sketched ) , see sec .",
    "[ sec logical reversible tm ] .",
    ", scaledwidth=45.0% ]    examining the thermodynamics of computation can be done in many different ways . here ,",
    "as stressed in the introduction , we want to capture three main features : first , we want to look at a general computational problem and not one specific task ; second , we are interested in a logically reversible computer ; and third , we want to model the computation stochastically , i.e. , as a brownian computer .",
    "the general thermodynamic picture is hence as sketched in fig .",
    "[ fig td comp ] .",
    "some machine ( with a very complex interior in general , see below ) is coupled to a thermal reservoir at inverse temperature @xmath2 and a work reservoir .",
    "the work reservoir can be used to drive the computation in a certain direction whereas the thermal reservoir is equipped with a well - defined notion of heat and entropy .",
    "the task of our machine is to compute .",
    "it therefore receives input signals and transforms them to output signals corresponding to the result of the computation .",
    "logical reversibility demands that we use two separate tapes for the inputs and outputs ( henceforth called the input and output tapes , respectively ) .",
    "in fact , if we do not keep the initial inputs but simply overwrite them with the output ( as the tm from sec .",
    "[ sec tms ] would do ) , our machine will be in general irreversible . and @xmath6 , which are given as their binary equivalent on the input string .",
    "clearly , knowing only the result @xmath36 of the computation , i.e. , @xmath37 , does not allow us to infer the values of @xmath38 and @xmath6 and hence , our machine would be logically irreversible . ]    more specifically , a logically reversible computer is in principle able to unambiguously retrace its _ computational path _",
    "( i.e. , the sequence of logical states visited so far ) back to the initial state .",
    "in fact , most tms as introduced in sec .",
    "[ sec tms ] are logically irreversible , for instance , already due to the fact that the machines usually do not remember from which direction they were coming from . but even if it would remember this ( for instance by writing the direction @xmath39 on an additional tape ) , the machine might still be logically irreversible .",
    "consider for example that there exists a pair of states and symbols @xmath40 and @xmath41 such that @xmath42 , @xmath43 and @xmath44 .",
    "then , given the state @xmath45 [ or even @xmath46 , the machine has two possible predecessor states and it can not know from which it was coming .",
    "hence , it is logically irreversible .",
    "this situation is sometimes called the merging of two computational paths , see fig .  [ fig comp paths ]",
    ".    however , even a logically reversible computer still proceeds deterministically step by step along its computational path .",
    "this deterministic behaviour unambigously defines a _ computational direction _ , see fig .",
    "[ fig comp paths ] again ( we remark that the computational direction does not necessarily coincide with the `` physical '' direction of the movement of the tape , which can  as we have seen  be shifted either way ) .",
    "in contrast , we also want to look at a stochastic machine , which makes random transitions in both directions , i.e. , it is also allowed to jump back to a previous computational state . in order to assure that we can control the direction of computation _ on average _ , we demand that we can change the potential energy externally , for instance , by using a work source or by adjusting chemical potentials appropriately . in the next section we will present the main ideas of how to obtain a logically reversible tm from an irreversible tm as discussed in sec .  [ sec tms ] sparing the mathematical details to the appendix .",
    "then , given that we have a logically reversible tm , we will show how to associate a me to it in sec .",
    "[ sec stochastic tm subsec ] .",
    "a computational path is defined by the way the machine proceeds from state to state through a high - dimensional state space ( each state of the machine including the tapes is symbolized by a black circle ) . here",
    ", we sketched two different paths ( solid red and dashed blue lines ) . for standard machines ( which compute only from left to right ) each path",
    "is uniquely determined by the input signal string and the initial machine state . for a logically irreversible machine ,",
    "however , it might happen that two different paths merge at some point ( as shown on top ) making it impossible to find the inverse of a state in general ( that is to say there is a unique way to go from left to right in the diagram but not from right to left ) .",
    "in contrast , this can not happen for logically reversible machines ( as shown in the middle ) , but this feature comes at the cost of introducing additional states and tapes making the state space even larger . finally , whereas the standard logically reversible tm still proceeds deterministically from left to right , a stochastic tm jumps randomly according to some rates [ e.g. , @xmath47 and @xmath48 as used in eqs .",
    "( [ eq me one step process ] ) and ( [ eq help 1 ] ) ] and hence , it can move both ways ( bottom ) .",
    ", scaledwidth=45.0% ]      the procedure to make a computation of a tm logically reversible was explained in detail in a famous publication by bennett  @xcite .",
    "he explicitly showed  given an arbitrary tm @xmath7 as described by eq .",
    "( [ eq tm standard map ] )  how to construct a machine @xmath49 which is computationally equivalent to @xmath7 but always logically reversible . to accomplish this , bennett introduced a finite number of new machine states and two additional tapes , which record the previous computational steps taken .",
    "furthermore , the computation is broken up into three _ stages _ ( each stage in general consists of many single computational steps ) and in total , the logically reversible tm needs approximately four times as many steps as the irreversible one  @xcite . before we proceed , we remark that the construction given by bennett is , of course , not unique ( as he discusses as well ) but seems to be very convenient .",
    "it is also noteworthy that tms acting on @xmath50 tapes are not more powerful ( in terms of what they can compute ) than a tm described by eq .",
    "( [ eq tm standard map ] ) because one can be mapped onto the other  @xcite .",
    "in addition to the treatment of bennett , who considered only a single computation @xmath26 , we want to construct a machine which continuously processes a stream of incoming input strings of the form @xmath51 .",
    "thus , we imagine an infinite input tape with different input strings @xmath52 separated by blank symbols to mark the beginning and the end of each single input string .",
    "the output tape then contains the results of the different computations , i.e. , it looks like @xmath53 . in this picture",
    "our machine resembles the devices from refs .",
    "@xcite in which also an external tape is manipulated but mainly to extract work and not for computational purposes though .    [",
    "cols=\"<,<,<,<,<,<\",options=\"header \" , ]     our logically reversible tm thus will have in total four tapes and one computational cycle proceeds in five stages .",
    "the four tapes are called the input , working , history and output tape . whereas the input and output tape are supplied externally ( see fig .",
    "[ fig td comp ] ) , the working and history tape belong to the machine itself . by this",
    "we especially want to emphasize that we require them to be blank at the end of the computation such that they are ready for usage again .",
    "more specifically , one computational cycle consists of the following five stages ( also see table  [ table ] ) :    1 .   _",
    "copy input onto working tape : _ a new input arrives at the machine on the input tape and the machine copies this input onto its working tape leaving it there in standard format at the end of the first stage .",
    "compute : _ in this stage the actual computation is performed which finally maps the input to the output . furthermore , a history tape keeps track of the intermediate steps such that the computer would be able to retrace each step .",
    "copy output to output tape : _ if the computation halts , the output on the working tape is copied to the output tape and the working tape is reset to its position as at the end of stage 2 .",
    "retrace computation : _ the computer retraces all its computation such that the output on the working tape becomes the input and the history is blank again .",
    "this stage is the inverse of stage 2 .",
    "_ erase working tape : _ we erase the working tape with the help of the input tape such that the working tape is blank again .",
    "note that this erasure step is logically reversible because we have an identical copy of the input on the input tape .",
    "finally , we use an additional symbol ( @xmath54 ) to mark that we already performed a computation for the current input and the machine moves on to the next input on the input string .",
    "stage 2 , 3 and 4 were already treated by bennett in ref .",
    "in addition , we require stages 1 and 5 because we want that our machine works continuously and not only once .",
    "the reader who is curious about the details of each step is refered to appendix  [ app logical reversible tm ] . otherwise , instead of one big machine @xmath7 doing a computation in five stages",
    ", it might also help to imagine five small machines @xmath55 .",
    "our big machine @xmath7 is then nothing else than a composition or concatenation of these small machines , i.e. , @xmath56 ( similar to the composition of different functions ) .",
    "the action of @xmath7 on the state of the four tapes ( in the order of the input , working , history and output tape , respectively ) can be written as @xmath57 eq . ( [ eq computational cycle ] ) can be regarded as a short form of table  [ table ] . here , @xmath58 denotes a blank tape and @xmath59 denotes the history tape at the end of stage 2 . clearly ,",
    "if all the machines @xmath55 are logically reversible , then also @xmath7 is .      in this section",
    "we will show how to use a continuous - time markov process to model any logically reversible tm , which we will simply call a stochastic tm .",
    "we start , however , by repeating what a me is and what we need for a consistent thermodynamic interpretation .    a continuous - time markov process describing the dynamics of a system @xmath60 corresponds to a set of states @xmath61 and an associated probability @xmath62 to be in a state @xmath63 , which changes according to a markovian first order differential equation called the me  @xcite : @xmath64 here , the rate matrix @xmath65 has real - valued entries and fulfills @xmath66 for all @xmath67 .",
    "this guarantees that probability is conserved throughout the evolution , i.e. , @xmath68 for all @xmath69 .",
    "if we want to equip the me ( [ eq me generic ] ) with a thermodynamic interpretation  @xcite , we have to associate to each state @xmath70 an energy @xmath71 and the rate matrix has to additionally fulfill a property called _ local detailed balance _ , which states that @xmath72 = -\\beta(e_x - e_{x'})$ ] where @xmath2 is the inverse temperature of the environment to which the system @xmath60 has contact . note",
    "that this automatically implies that , if @xmath73 , then also @xmath74 , i.e. , for each transition @xmath75 the reversed one @xmath76 must also exist .",
    "this framework can also be extended to more general situations involving , e.g. , multiple environments at different temperatures  @xcite , but we do not need more than the things just mentioned .    now , associating a me to a logically reversible deterministic tm is in fact very easy .",
    "all we have to do is to change the ( unidirectional ) deterministic updating rules from appendix  [ app logical reversible tm ] into ( bidirectional ) probabilistic transition rules , i.e. , we allow for transitions in the computational forward direction as well as transitions which just undo the last computational step ( backward direction ) .",
    "it is worth pointing out that the underlying state space @xmath61 of the me is in general infinite , but this is not necessarily related to the size of the input tape .",
    "remember that  due to the halting problem  a computation might go on forever even if it only received a finite input .",
    "in fact , even if the computation halts , there is no general way to give a reasonable estimate of the size of @xmath61 in advance  @xcite .",
    "however , on the other hand , the structure of the me is very simple and this is related to the fact that we build the machine in a logically reversible way .",
    "in fact , each state @xmath63 has only two adjacent states , namely its logical predecessor and its logical successor state .",
    "hence , our me describes a simple one - step or birth - and - death process  @xcite or equivalently , according to schnakenberg  @xcite , we could say that the topology of the underlying network is trivial .",
    "in fact , if there were any branchings or loops in the underlying network , the computation would not be logically reversible anymore because then a state could have multiple predecessors or successors .",
    "hence , quite generally we can put the final me into the form @xmath77 here , of course , @xmath78 is a multi - index denoting the entire machine and tape configuration .",
    "let us discuss the general structure of the me a little further .",
    "first of all , it is important to note that only the current squares of the tapes can change stochastically whereas the rest of the tapes , which is not coupled to the machine , remains fixed .",
    "in fact , although there is an infinite number of possible different states , not all states @xmath63 are coupled with each other .",
    "which states are coupled to each other is determined by the rules from appendix  [ app logical reversible tm ] and by the input strings @xmath79 because they single out a unique computational path through the `` labyrinth '' of states in @xmath61 .",
    "in addition , the number of transition rules is always _ finite _ and fixed as expressed in appendix [ app logical reversible tm ] .",
    "this is true independently of the number of computations or the lengths of the input strings .",
    "although there seem to be quite a lot of rules , note that they suffice to build a _",
    "universal logically reversible computer_. of course , things become much easier if we relax some of the requirements . hence , in a more pictorial language we could say that the hardware ( i.e. , the set of transitions rules with the associated rates ) of our machine remains fixed , but the software ( i.e. , the inputs determining the computational path ) can change .    thus , if we focus only on the computation for one input string , i.e. , on the stages 2 to 4 , the full rate matrix @xmath80 in eq .",
    "( [ eq me generic ] ) decomposes into blocks for each input string @xmath29 , i.e. , it has the form @xmath81 and no transition between different blocks is allowed . here , we labeled the different input strings @xmath82 in some canonical way and note that each block can be infinitely large if the computation does not halt . for each @xmath83",
    "the me describes a simple one - step process and by rearranging the states appropriately we can write each block as a tridiagonal matrix    @xmath84    where @xmath47 ( @xmath85 ) denotes the forward ( backward ) rate at step @xmath50 in the me ( [ eq me one step process ] ) .",
    "hence , to conclude , although the state space @xmath61 is extremely large , the rate matrix is also extremely _ sparse _ , i.e. , it contains only a small number of non - zero elements ( in relation to the total number of elements ) .",
    "finally , we would need to associate a consistent energy landscape to our system and the rate of forward and backward transitions would then need to obey local detailed balance , which fixes the temperature of the environment .",
    "we will discuss this issue in the next section .",
    "so far we have shown that a stochastic , logically reversible tm can be modeled by a simple one - step process as given by the me ( [ eq me one step process ] ) . to interpret it thermodynamically we still need to associate a consistent energy landscape to it , which we could control externally via a work source or , alternatively , a bias in chemical potentials as it would be the case for biochemical processes .",
    "for the sake of simplicity , we will choose below a linear energy landscape along the computational path , i.e. , the difference in energy between a logical state and its successor state is taken to be the constant @xmath86 ( i.e. , for @xmath87 the computation proceeds on average in the forward direction along a chain of states with decreasing energies ) .",
    "this choice is in agreement with the one usually appearing in the literature  @xcite . before we proceed , however , we discuss and justify this choice in more detail .    first of all , in sec .",
    "[ sec effective fpe ] we will actually discuss the thermodynamics of our model on a coarse - grained level of description .",
    "that is to say we will be interested in the regime where the computer was running already for quite a long time such that the variance @xmath88 of the number of computational steps is large compared to unity where we defined @xmath89 . in this picture ,",
    "@xmath86 might denote just an average slope in the energy landscape , i.e. , we explicitly allow for spatial irregularities in the energy landscape as long as they are not too large .",
    "more specifially , if @xmath90 denotes the energy of state @xmath50 according to the me ( [ eq me one step process ] ) , we demand that @xmath91 holds for all @xmath50 and for @xmath92 of the order of the variance @xmath88 such that the energy landscape looks linear at the coarse - grained level .",
    "second , it is worth pointing out that in fact  except for the spatially allowed irregularities ",
    "no other energy landscape seems to be feasible for a general purpose computer .",
    "the reason for this is twofold : first , we are interested in a steady state regime , i.e. , the average dissipation _ per step _ should be independent of the number of computational steps performed so far .",
    "this demand rules out quadratic or exponential energy landscape .",
    "second , we are also still faced with the halting problem .",
    "this implies that we can not know in advance the number of computational steps we need for one computational cycle .",
    "thus , associating any particularly shaped energy landscapes like a sine or a hill ( as in  @xcite ) is unfeasible because we do not know , for instance , how to choose a senseful period for the sine .",
    "hence , we conclude : the only feasible energy landscape with which we can ensure to control the speed and direction of computation independently of the number of computational steps ( which we can not know in advance ) and which is translationally invariant on the state space @xmath78 of the me ( [ eq me one step process ] ) is an on average linear landscape .",
    "having agreed on the ( on average ) linear energy landscape we will choose the transition rates in eq .",
    "( [ eq me one step process ] ) as follows : @xmath93 here , @xmath94 is some rate setting the overall time - scale of our problem and we see that the rates fulfill local detailed balance , i.e. , @xmath95 = -\\beta ( e_n - e_{n+1 } ) = -\\beta\\epsilon$ ] where @xmath96 favors a computation in the forward direction .    in the limit where the mean @xmath97 and variance @xmath88 are large compared to one , we can approximate derivatives by @xmath98 then",
    ", the fokker - planck equation ( fpe ) corresponding to the me ( [ eq me one step process ] ) reads @xmath99 p_n(t).\\ ] ] this fpe describes the movement of an overdamped brownian particle in a constant force field and with a constant diffusion coefficient with @xmath100 denoting the position of the particle .",
    "it even admits an explicit solution : assuming that the machine has started initially at some fixed state @xmath101 , i.e. , @xmath102 , we obtain @xmath103 ^ 2}{4\\gamma\\cosh(\\beta\\epsilon/2)t}\\right\\}.   \\end{split}\\ ] ]      discussing the thermodynamic behaviour of eq .",
    "( [ eq fpe ] ) can be done using standard methods , see e.g. ref .",
    "we first of all compute the mean number of computational steps , which is @xmath104 and hence , the _ speed _ of computation becomes @xmath105 which  as expected  can be controlled by @xmath86 .",
    "especially , we see that we have @xmath106 for @xmath87 and vice versa .",
    "the variance of the distribution is @xmath107 and grows linearly with time .",
    "based on this we might ask the question when does the computation become approximately deterministic , i.e. , when does the mean dominate the standard deviation ? calculating their ratio yields @xmath108 if we want this quantity to be much larger than one , we obtain a condition for the minimum amount of time we have to wait until our machine computes almost in a deterministic fashion : @xmath109 here , we performed an expansion in @xmath110 at the end . thus , the closer we get to the reversible limit , i.e. , the smaller @xmath111 ( see below ) , the longer we have to wait until the computer starts to work reliably .    furthermore , we can explicitly calculate the shannon entropy of our distribution , which is @xmath112 using eq .",
    "( [ eq average n ] ) we can also write the shannon entropy as @xmath113,\\ ] ] i.e. , the shannon entropy scales with the average number @xmath114 of computational steps as @xmath115 .",
    "the rate at which entropy is produced is given by the change in shannon entropy plus @xmath2 times the heat flow dissipated into the environment  @xcite . since the latter is simply @xmath116 , we can write for the entropy production rate @xmath117 we now note that for @xmath118 the last term vanishes quadratically , i.e. , the heat dissipated can be made arbitrarily small in this limit . the first term , however , is independent of @xmath86 but vanishes for @xmath119 .",
    "hence , we have @xmath120 thus , a brownian computer can work thermodynamically reversibly ( i.e. , with zero entropy production rate ) in the steady state regime if the bias @xmath86 is small enough .",
    "this would confirm the conclusions from ref .",
    "@xcite .",
    "however , we can also confirm norton s perspective on the matter  @xcite .",
    "starting initially at @xmath121 we see that the total amount of entropy produced up to time @xmath69 is @xmath122 thus , even for @xmath123 the shannon entropy still grows logarithmically with the number of computational steps because the probability distribution of the machine spreads over the available phase space similarly to the free expansion of a one - molecule gas , which is a thermodynamically irreversible process .",
    "if we think in terms of thermodynamic cycles instead of a thermodynamic machine , which works in the stationary regime , we would have to dissipate an amount of entropy proportional to @xmath124 to reset the brownian computer to its initial zero entropy state .",
    "however , again , compared to the number of computational steps @xmath114 taken , the ratio @xmath125 becomes arbitrarily small for a large number of steps , i.e. , for a long computation .",
    "finally , we remark that a full treatment in terms of the me ( [ eq me one step process ] ) instead of the fpe ( [ eq fpe ] ) would provide very similar results .",
    "in fact , the first two cumulants ( mean and variance ) can be shown to coincide .",
    "let us summarize our main findings and discuss possible interesting open questions based on our findings .    we have started with an arbitrary tm as a model for a general purpose computer , which is , however , in general logically irreversible and free from any thermodynamic interpretation .",
    "we then discussed how a computation can be regarded as a thermodynamic process and we decided to investigate the thermodynamics of a logically reversible stochastic computer , which simulates the tm considered at the beginning .",
    "we explained in detail how to obtain a logically reversible computer out of an irreversible one and then used bidirectional probabilistic transition rules instead of unidirectional deterministic rules to model the stochastic or random motion of our computer .",
    "although the problem seems to be very complex , we have seen that the resulting me has the very simple structure of a one - step process , which describes how the computer follows a well - defined path in a large state space and eventually maps the input signals to output signals ( the results of the computation ) .",
    "we have argued that the only feasible energy landscape for such a computer is an approximately linear one , which not only simplified the thermodynamic discussion , but also allowed us to solve a corresponding fpe describing the drift and diffusion of the computer exactly .    we have then seen that our stochastic computer can work thermodynamically reversibly , i.e. , in a dissipation - free fashion , in a steady state regime and in this respect feynman was indeed right with his initially quoted statement .",
    "however , just because the entropy production rate can become arbitrary small , this does not imply that the overall integrated entropy production is zero .",
    "especially , if we think in terms of a computational cycle , in which we want to reset the computer to its initial state at the end , there is an unavoidable cost due to the increasing shannon entropy of the probability distribution during the computation .",
    "in fact , this additional cost is _ not _ independent of the number of computational steps but scales logarithmically with it and it seems that this effect has been only recognized by norton so far  @xcite . here",
    ", we have verified this result in a conceptually clean and general framework .",
    "furthermore , it is worth emphasizing that our computer works _ error - free _ at a _",
    "finite _ entropy production .",
    "in fact , by construction our model does only allow for temporary errors in the computation ( due to the fact that our stochastic machine can randomly hop back to its previous state ) , but in the long run each temporary error is corrected by the next step in the computational forward direction and there are no other sources of errors allowed .",
    "including errors ( for instance , random bit flips or  to avoid the halting problem and an infinitely long computation ",
    "one could decide to terminate the computation after a fixed number @xmath126 of steps ) in our scheme and investigating the thermodynamic cost to correct them might be an interesting project for the future .",
    "another interesting question is whether we can sensefully assign a notion of _ efficiency _ to our computer . from a purely physical point of view",
    "the machine we have considered is actually senseless because it describes only a simple ( and never - ending ) relaxation process .",
    "however , the machine is indeed `` working '' , i.e. , doing something `` useful '' for us , because it tells us the answer to many questions . but how can we quantify the usefulness of our machine ? having a rigorous notion of a thermodynamic efficiency for a computer would allow us to study question of , e.g. , efficiency at maximum power , which is an important question for the design of realistic machines , see , e.g. ,  @xcite . in this context , one can also ask the question whether a logically reversible computer is really desirable or whether a logically irreversible computer might indeed be able to work at a fundamentally better efficiency . at the end",
    ", it seems that biochemical processes in our body work very efficiently , but not necessarily in a logically reversible way .    finally , let us say a few words about the relation between the present work and the devices investigated in refs .",
    "@xcite ( also see feynman for a simple example of such a device who called them information - driven engines  @xcite ) . indeed , as in our case , these information - driven engines are coupled to an external tape or `` information reservoir '' .",
    "this additional reservoir can then be used to extract work from a single heat bath while simultaneously writing information on the tape ( i.e. , increasing its shannon entropy ) .",
    "this picture , however , does not carry over to our situation .",
    "in fact , the shannon entropies of the incoming and outgoing tapes are _",
    "equal _ because the input tape gets mapped to itself and the output tape is uniquely determined by the input . for a logically irreversible computer this does not need to be true as it can be already seen from the devices in refs .",
    "@xcite where it was also shown that they can be used as an information eraser .",
    "financial support by the dfg ( scha 1646/3 - 1 , sfb 910 , and grk 1558 ) is gratefully acknowledged .",
    "33 r. landauer , _ irreversibility and heat generation in the computing process _ , ibm j. res",
    ". dev . * 5 * , 183 ( 1961 ) . c. h. bennett , _ logical reversibility of computation _ , ibm j. res",
    "* 17 * , 525 ( 1973 ) . c. h. bennett , _ the thermodynamics of computation  a review _ , int .",
    "j. theor . phys . * 21 * , 905 ( 1982 ) . c. h. bennett ,",
    "_ notes on the history of reversible computation _ , ibm j. res",
    ". dev . * 32 * , 16 ( 1988 ) .",
    "r. w. keyes and r. landauer , _ minimal energy dissipation in logic _",
    ", ibm j. res .",
    "* 14 * , 152 ( 1970 ) .",
    "k. likharev , _ classical and quantum limitations on energy consumption in computation _ , int",
    "* 21 * , 311 ( 1982 ) . c. h. bennett and r. landauer , _ the fundamental physical limits of computation _ , scientific american * 253 * , 48 ( 1985 ) .",
    "r. p. feynman , _",
    "feynman lectures on computation _ ( addison - wesley publishing company , 1996 ) .",
    "h. s. leff and a. f. rex ( editors ) , _",
    "maxwell s demon 2 : entropy , classical and quantum information , computing _",
    "( iop publishing , bristol , 2003 ) .",
    "j. earman and j. d. norton , _ exorcist xiv : the wrath of maxwell s demon .",
    "part i. from maxwell to szilard _ , stud .",
    "mod . phys . * 29 * , 435 ( 1998 ) .",
    "j. earman and j. d. norton , _ exorcist xiv : the wrath of maxwell s demon .",
    "part ii . from szilard to landauer and beyond _ , stud .",
    ". mod . phys . * 30 * , 1 ( 1999 ) .",
    "o. r. shenker , _",
    "maxwell s demon and baron munchausen : free will as a perpetuum mobile _ , stud .",
    "phys . * 30 * , 347 ( 1999 ) .",
    "maxwell s demon and the thermodynamics of computation _ , stud .",
    "mod . phys . * 32 * , 569 ( 2001 ) . c. h. bennett , _ notes on landauer s principle , reversible computation , and maxwell s demon _ , stud .",
    "* 34 * , 501 ( 2003 ) . o. j. e. maroney , _ the ( absence of a ) relationship between thermodynamic and logical reversibility _ , stud .",
    "phys . * 36 * , 355 ( 2005 ) .",
    "j. d. norton , _ eaters of the lotus : landauer s principle and the return of maxwell s demon _ , stud .",
    "36 * , 375 ( 2005 ) .",
    "j. ladyman , s. presnell , a. j. short , and b. groisman , _ the connection between logical and thermodynamic irreversibility _ , stud .",
    "38 * , 58 ( 2007 ) .",
    "m. hemmo and o. r. shenker , _",
    "maxwell s demon _ ,",
    "j. philos . *",
    "107 * , 389 ( 2010 ) .",
    "j. d. norton , _ waiting for landauer _ , stud . hist .",
    "* 42 * , 184 ( 2011 ) .",
    "l. b. kish and c. g. granqvist , _ energy requirement of control : comments on szilard s engine and maxwell s demon _",
    ". lett . * 98 * , 68001 ( 2012 ) .",
    "j. d. norton , _ brownian computation is thermodynamically irreversible _ , found .",
    "phys . * 43 * , 1384 ( 2013 ) .",
    "j. d. norton , _ all shook up : fluctuations , maxwell s demon and the thermodynamics of computation _",
    ", entropy * 15 * , 4432 ( 2013 ) .",
    "kish , c. g. granqvist , s. p. khatri , and h. wen , _ demons : maxwell s demon , szilard s engine and landauer s erasure - dissipation _ , international journal of modern physics : conference series * 33 * ( 2014 ) .",
    "r. alicki , _ information is not physical _ , arxiv : 1402.2414 .",
    "r. alicki , _ breaking information - thermodynamics link _ , arxiv : 1406.5879 . j. d. norton , _ on brownian computation _ , int .",
    "ser . * 33 * , 1460366 ( 2014 ) .",
    "m. l. minsky , _ computation : finite and infinite machines _",
    "( prentice - hall , london , 1967 ) .",
    "j. schnakenberg , _ network theory of microscopic and macroscopic behavior of master equation systems _ , rev .",
    "* 48 * , 571 ( 1976 ) . n. g. van kampen , _",
    "stochastic processes in physics and chemistry _",
    "( north - holland publishing company , amsterdam , 3rd edition , 2007 ) .",
    "u. seifert , _",
    "stochastic thermodynamics , fluctuation theorems and molecular machines _ , rep .",
    "phys . * 75 * , 126001 ( 2012 ) .",
    "a. e. allahverdyan , d. janzing , and g. mahler , _ thermodynamic efficiency of information and heat flow _ , j. stat",
    "* p09011 * ( 2009 ) .",
    "d. mandal and c. jarzynski , _ work and information processing in a solvable model of maxwell s demon _ , proc .",
    "sci . * 109 * , 11641 ( 2012 ) .",
    "a. c. barato and u. seifert , _ an autonomous and reversible maxwell s demon _ , europhys .",
    "* 101 * , 60001 ( 2013 ) .",
    "d. mandal , h. t. quan , and c. jarzynski , _",
    "maxwell s refrigerator : an exactly solvable model _",
    "lett . * 111 * , 030602 ( 2013 ) .",
    "t. munakata and m. l. rosinberg , _ feedback cooling , measurement errors , and entropy production _ , j. stat .",
    "p06014 * ( 2013 ) .",
    "n. shiraishi , s. ito , k. kawaguchi , and t. sagawa , _ role of measurement - feedback separation in autonomous maxwell s demons _ , new . j. phys .",
    "* 17 * , 045012 ( 2015 ) .",
    "s. deffner and c. jarzynski , _ information processing and the second law of thermodynamics : an inclusive , hamiltonian approach _ ,",
    "phys . rev .",
    "x * 3 * , 041003 ( 2013 ) .",
    "d. hartich , a. c. barato , and u. seifert , _",
    "stochastic thermodynamics of bipartite systems : transfer entropy inequalities and a maxwell s demon interpretation _ , j. stat .",
    "p02016 * ( 2014 ) . j. m. horowitz and m. esposito , _ thermodynamics with continuous information flow _ ,",
    "x * 4 * , 031015 ( 2014 ) .",
    "a. c. barato and u. seifert , _",
    "stochastic thermodynamics with information reservoirs _ , phys .",
    "e * 90 * , 042150 ( 2014 ) .",
    "n. shiraishi and t. sagawa , _ fluctuation theorem for partially masked nonequilibrium dynamics _ phys .",
    "e * 91 * , 012130 ( 2015 ) .",
    "a. e. allahverdyan and q. a. wang , _ adaptive machine and its thermodynamic costs _ , phys .",
    "e * 87 * , 032139 ( 2013 ) .",
    "a. c. barato , d. hartich , and u. seifert , _ information - theoretic versus thermodynamic entropy production in autonomous sensory networks _ phys .",
    "e * 87 * , 042104 ( 2013 ) .",
    "a. c. barato , d. hartich , and u. seifert . ,",
    "_ efficiency of cellular information processing _",
    ", new j. phys . * 16 * , 103024 ( 2014 ) .",
    "p. sartori , l. granger , c. f. lee , and j. m. horowitz , _ thermodynamic costs of information processing in sensory adaptation _ , plos comput .",
    "biol . * 10*(12 ) , e1003974 ( 2014 ) .",
    "p. sartori and s. pigolotti , _ thermodynamics of error correction _",
    ", arxiv : 1504.06407 .",
    "p. strasberg , g. schaller , t. brandes , and m. esposito , _ thermodynamics of a physical model implementing a maxwell demon _ , phys .",
    "* 110 * , 040601 ( 2013 ) .",
    "p. strasberg , g. schaller , t. brandes , and c. jarzynski , _ the second laws for an information driven current through a spin valve _ ,",
    "e * 90 * , 062107 ( 2014 ) .",
    "a. brut , a. arakelyan , a. petrosyan , s. ciliberto , r. dillenschneider , and e. lutz , _ experimental verification of landauer s principle linking information and thermodynamics _ ,",
    "nature * 483 * , 187189 ( 2012 ) .",
    "y. jun , m. gavrilov , and j. bechhoefer , _ high - precision test of landauer s principle in a feedback trap _ , phys .",
    "* 113 * 190601 ( 2014 ) .",
    "j. hong , b. lambson , s. dhuey , and j. bokor , _ experimental verification of landauer s principle in erasure of nanomagnetic memory bits _",
    ", arxiv : 1411.6730 .",
    "j. p. p. silva , r. s. sarthour , a. m. souza , i. s. oliveira , j. goold , k. modi , d. o. soares - pinto , and l. c. cleri , _ experimental demonstration of information to energy conversion in a quantum system at the landauer limit _",
    ", arxiv : 1412.6490 .",
    "a. brut , a. petrosyan , and s. ciliberto , _ information and thermodynamics : experimental verification of landauer s erasure principle _ ,",
    "k. sekimoto , _ stochastic energetics _ ( lect . notes phys .",
    "799 , springer , berlin heidelberg , 2010 ) .",
    "f. l. curzon and b. ahlborn , _ efficiency of a carnot engine at maximum power output _ am .",
    "* 43 * , 22 ( 1975 ) . c. van den broeck , _ thermodynamic efficiency at maximum power _ phys .",
    "lett . * 95 * , 190602 ( 2005 ) . t. schmiedl and u. seifert , _ efficiency at maximum power : an analytically solvable model for stochastic heat engines _",
    ", europhys . lett . *",
    "81 * , 20003 ( 2008 ) .",
    "m. esposito , k. lindenberg , and c. van den broeck , _ universality of efficiency at maximum power _",
    "* 102 * , 130602 ( 2009 ) .",
    "m. esposito , k. lindenberg , and c. van den broeck , _ thermoelectric efficiency at maximum power in a quantum dot _ , europhys .",
    "lett . * 85 * , 60010 ( 2009 ) .",
    "we here provide the detailed rules for the machine behaviour at each stage of the computation . these rules are , of course , not unique . however , because we are not primarily interested in the speed or efficiency of our machine , but only in _ what _ it can do , possibly different implementations are unimportant for the present context .",
    "furthermore , note that at each stage the machine is only manipulating two tapes whereas the other two tapes remain fixed ( see table  [ table ] ) .",
    "we will therefore use the notation @xmath127 $ ] where @xmath128 denotes the internal machine state at stage @xmath129 , @xmath130 the symbol @xmath5 printed on square @xmath131 of the first tape of interest and @xmath132 the symbol @xmath69 printed on square @xmath50 of the second tape of interest .",
    "what are the tapes of interest will become clear in the treatment of each stage .",
    "note that the notation is different from the one used by bennett  @xcite .",
    "we want to copy the input on the input tape ( first tape of interest ) to the working tape ( second tape of interest ) .",
    "the input is given in the form @xmath133 and we assume that the machine scans initially the symbol at the far right ( i.e. , @xmath134 ) ( see also table  [ table ] ) . furthermore , the working tape is by construction initially completely blank ( that this is so can only be seen after the completion of all five stages , of course ) .",
    "the copy stage then proceeds as follows : @xmath135 \\rightarrow&~    [ q_1^{(1 ) } , ( s_m)_m , ( s_m)_n ]    \\\\                  \\rightarrow&~    [ q_0^{(1 ) } , ( s_{m-1})_{m-1 } , b_{n-1 } ]    \\\\                  \\rightarrow&~    [ q_1^{(1 ) } , ( s_{m-1})_{m-1 } , ( s_{m-1})_{n-1 } ]    \\\\                  & ~       \\vdots   \\\\                  \\rightarrow&~    [ q_1^{(1 ) } , ( s_1)_{m-(m-1 ) } , ( s_1)_{n-(m-1 ) } ]    \\\\                  \\rightarrow&~    [ q_0^{(1 ) } , b_{m - m } , b_{n - m } ]    \\\\                  \\rightarrow&~    [ \\text{r}^{(2)},b_{m - m } , b_{n - m } ] .",
    "\\end{split}\\ ] ] hence , we see that during the copy operation the machine changes between the two states @xmath136 and @xmath137 where the first is responsible for copying the symbol on the input tape to the working tape and the second is responsible for a shift of both tapes .",
    "this procedure goes on until it hits the first blank symbol to the left of the input string .",
    "the machine then changes to the `` ready '' state r@xmath138 , which is the special initial state for the second stage .",
    "note that  due to the fact that the copying procedure is unidirectional , i.e. , the machine moves the tape always in the same direction ",
    "each map has a clear logical inverse .",
    "furthermore , the positions @xmath131 and @xmath50 of the squares are in general arbitrary and can be chosen initially at will .",
    "this is the central stage of our computational cycle .",
    "if we would not bother about logical reversibility , this would be the only stage to execute .",
    "we thus have to explicitly think about how to make the map ( [ eq tm standard map ] ) logically reversible .",
    "the idea is the following  @xcite : first , because each standard ( i.e. , irreversible ) tm is defined by its @xmath33 many quintuples , we introduce a set @xmath139 with @xmath140 of additional machine states @xmath141 .",
    "then , to each map @xmath142 we can associate a special state @xmath143 , which _ uniquely _ labels each quintuple .",
    "second , we will make use of an additional tape called the history tape , which remembers the past @xmath143 such that the machine is able to uniquely retrace its computational path .    stage 2 is thus a bit more complicated .",
    "the corresponding maps are @xmath144&~~~\\rightarrow [ \\tilde q^{(2)}_{r^{(2)}b } , b_{n - m } , b_{m+1 } ]    \\\\                  & ~~~\\rightarrow [ q_1 , ( s_1)_{n - m+1 } , ( \\tilde q_{\\text{r}^{(2)}b})_{m+1 } ]     \\\\                  & ~~~ ~~~ ~~     \\vdots   \\\\    \\text{step } \\ell      & \\left\\{\\begin{array}{ll }                                       \\rightarrow     &    [ q_\\ell^{(2)},(s_\\ell)_{n'},(\\tilde q_{q_{\\ell-1}^{(2)}s_{\\ell-1}})_{m+\\ell } ] \\\\                                       \\rightarrow     &    [ \\tilde q^{(2)}_{q_{\\ell}^{(2)}s_{\\ell}},(s'_\\ell)_{n'},b_{m+\\ell+1 } ] \\\\                                       \\rightarrow     &    [ q'^{(2)}_{\\ell},(s_{\\ell+1})_{n'+d'},(\\tilde q_{q_{\\ell}^{(2)}s_{\\ell}})_{m+\\ell+1 } ]",
    "\\\\                      \\end{array}\\right .   \\\\",
    "& ~~~ ~~~ ~~     \\vdots   \\\\    \\text{step } \\nu       & ~~~\\rightarrow [ \\tilde q^{(2)}_{q_{\\nu}^{(2)}s_{\\nu}},(s'_{\\nu})_{n''-1},b_{m+\\nu+1 } ]   \\\\                  & ~~~\\rightarrow [ \\text{h}^{(2)},b_{n''},(\\tilde q_{q_\\nu^{(2)}s_\\nu})_{m+\\nu+1 } ] .",
    "\\end{split}\\ ] ] the most important step to understand is the one in the middle , which corresponds to the @xmath145th computational step of the ordinary irreversible tm defined by ( [ eq tm standard map ] ) .",
    "initially , the machine is in state @xmath146 and scans the symbol @xmath147 on the square @xmath148 of the working tape .",
    "the history tape contains the state @xmath149 , which uniquely labels the _ previous _ computational step .",
    "then , the machine changes its state to @xmath150 , writes the symbol @xmath151 on the square according to the function @xmath152 and shifts the history tape one square to the left , which contains a blank symbol .",
    "finally , we write @xmath153 to the history tape and change the machine state to @xmath154 .",
    "furthermore , we shift the working tape one square according to @xmath155 such that the machine now scans the new symbol @xmath156",
    ". then , the whole procedure can start again where  in order that we are able to apply map ( [ eq tm standard map ] )  we identify @xmath157 because the final state of the machine at the end of step @xmath145 is the initial state for step @xmath158 .    the first and last two lines of eq .",
    "( [ eq stage 2 explicit ] ) then simply describe the initial and final steps of the computation .",
    "initially , the machines starts in r@xmath138 and then shifts the working tape to the left such that it reads the first symbol @xmath22 and starts in the state @xmath159 . finally , if the machine halts , it reaches the state h and stops at the first blank to the right of the output of the computation .",
    "we now want to copy the output @xmath160 of the computation from stage 2 from the working tape ( first tape of interest ) to the output tape ( second tape of interest ) .",
    "this goes as follows @xmath161     \\rightarrow&~    [ q_0^{(3 ) } , b_{n '' } , b_m ]    \\\\                  \\rightarrow&~    [ q_1^{(3 ) } , ( s_{m'})_{n''-1 } , b_{m-1 } ]   \\\\                  \\rightarrow&~    [ q_0^{(3 ) } , ( s_{m'})_{n''-1 } , ( s_{m'})_{m-1 } ]    \\\\                  & ~       \\vdots   \\\\                  \\rightarrow&~    [ q_0^{(3 ) } , ( s_1)_{n''-m ' } , ( s_1)_{m - m ' } ]    \\\\                  \\rightarrow&~    [ q_1^{(3 ) } , b_{n''-m'-1 } , b_{m - m'-1 } ]   \\end{split}\\ ] ] and is very similar to ( [ eq stage 2 explicit ] ) .",
    "finally , however , to prepare the machine for the next stage , we want that it scans the output on the working tape at the very right again ( at the moment it scans the blank on the working tape to the left of the output ) . to accomplish this",
    "we use two more machine states : @xmath162    \\\\    & \\rightarrow [ q_2^{(3 ) } , b_{n''-m'-1 } , b_{m - m'-1 } ]    \\\\    & \\rightarrow [ q_3^{(3 ) } , ( s_1)_{n''-m ' } , b_{m - m'-2 } ] \\\\    & \\rightarrow",
    "[ q_3^{(3 ) } , ( s_2)_{n''-m'+1 } , b_{m - m'-3 } ]    \\\\    & ~~~~~ \\vdots     \\\\    & \\rightarrow [ q_3^{(3 ) } , ( s_{m'})_{n''-1 } , b_{m-2 m ' } ]     \\\\    & \\rightarrow [ q_3^{(3 ) } , b_{n '' } , b_{m-2m'-1 } ] .   \\end{split}\\ ] ] here , @xmath163 is an intermediate state and its sole purpose is to indicate that the copying procedure is over and the machine starts now only to shift the working tape without changing it .",
    "the state @xmath164 then actually accomplishes this task by shifting the working tape one step to the left while simultaneously shifting the output tape one step to the right until it reaches the first blank symbol to the right of the output on the working tape , which will indicate the start of stage 4 .      in this stage",
    "we will basically apply the inverse of stage 2 such that at the end the working tape contains the input again and the history tape is returned to its initial blank state .",
    "this goes as follows : @xmath165 \\\\    & \\rightarrow [ \\text{h}^{(4 ) } , b_{n '' } , ( \\tilde q_{q_\\nu^{(2)}s_\\nu})_{m+\\nu } ]     \\\\    & \\rightarrow [ \\tilde q^{(4)}_{q_\\nu^{(2)}s_\\nu } , ( s'_\\nu)_{n''-1},b_{m+\\nu } ] \\\\    & \\rightarrow [ q_\\nu^{(4 ) } , ( s'_{\\nu-1})_{n''-1},(\\tilde q_{q_{\\nu-1}^{(2)}s_{\\nu-1}})_{m+\\nu-1 } ] \\\\    & ~~~~~ \\vdots     \\\\    & \\rightarrow [ q'^{(4)}_\\ell , ( s_{\\ell+1})_{n'+d ' } , ( \\tilde q_{q_\\ell^{(2)}s_\\ell})_{m+\\ell+1 } ]    \\\\    & \\rightarrow [ \\tilde q^{(4)}_{q_\\ell^{(2)}s_\\ell } , ( s'_\\ell)_{n ' } , b_{m+\\ell+1 } ] \\\\    & \\rightarrow [ q_\\ell^{(4 ) } , ( s_\\ell)_{n ' } , ( \\tilde q_{q_{\\ell-1}^{(2)}s_{\\ell-1}})_{m+\\ell } ] \\\\    & ~~~~~ \\vdots     \\\\    & \\rightarrow [ q_1^{(4 ) } , ( s_1)_{n - m+1 } , ( \\tilde q_{\\text{r}^{(2)}b})_{m+1 } ]   \\\\    & \\rightarrow [ \\tilde q^{(4)}_{\\text{r}^{(2)}b } , b_{n - m } , b_{m+1 } ]     \\\\    & \\rightarrow [ \\text{r}^{(4 ) } , b_{n - m } , b_m ] .   \\end{split}\\ ] ] note that we are using the superscript @xmath166 on the _ internal _ machine states to explicitly distinguish them from the states of stage 2 indicating that we are truly in a different stage here .",
    "the last step consists in erasing the content on the working tape such that it is blank again and ready for the next computation .",
    "note that the `` erasure '' of the working tape does not actually erase any information because the working tape contains the same input @xmath133 as the input tape .",
    "as in stage 1 we choose the first tape of interest to be the input tape and the second tape of interest is the working tape .",
    "then , we actually only have to apply the inverse of stage 1 , i.e. , @xmath167     \\rightarrow&~    [ q_0^{(5)},b_{m - m } , b_{n - m } ]     \\\\                      \\rightarrow&~    [ q_1^{(5)},(s_1)_{m-(m-1 ) } , ( s_1)_{n-(m-1 ) } ]     \\\\                      \\rightarrow&~    [ q_0^{(5)},(s_1)_{m-(m-1 ) } , b_{n-(m-1 ) } ]     \\\\                      & ~       \\vdots",
    "\\\\                      \\rightarrow&~    [ q_1^{(5)},(s_m)_{m } , ( s_m)_{n } ]     \\\\                      \\rightarrow&~    [ q_0^{(5)},(s_m)_{m } , b_{n } ]     \\\\                      \\rightarrow&~    [ q_1^{(5)},b_{m+1 } , b_{n+1 } ] .",
    "\\end{split}\\ ] ] if we would postulate the final transition rule @xmath168 \\rightarrow [ q_0^{(1 ) } , ( s_m)_m , b_n]$ ] we would be exactly back at the initial state of stage 1 and the only change would be that we have printed the result of the computation on the output tape .",
    "however , if this were true , we would be doomed to repeat the same computation again , while in fact we want to compute with the _ next _ input @xmath169 on the input string . to achieve this we add the following rules : @xmath170   \\rightarrow&~    [ q_2^{(5 ) } , \\star_{m+1 } , b_{n+1 } ]    \\\\",
    "\\rightarrow&~    [ q_2^{(5 ) } , ( s_m)_{m } , b_{n+2 } ] \\\\                  \\rightarrow&~    [ q_2^{(5 ) } , ( s_{m-1})_{m-1 } , b_{n+3 } ]    \\\\                  & ~       \\vdots   \\\\",
    "\\rightarrow&~    [ q_2^{(5 ) } , ( s_{1})_{m - m } , b_{n+m+1 } ]    \\\\                  \\rightarrow&~    [ q_2^{(5 ) } , b_{m - m-1 } , b_{n+m+2 } ]    \\\\                  \\rightarrow&~    [ q_3^{(5 ) } , b_{m - m-1 } , b_{n+m+2 } ]    \\\\                  & ~       \\vdots   \\\\                  \\rightarrow&~    [ q_3^{(5 ) } , ( s'_{m'})_{m ' } , b_{\\hat n } ] \\\\",
    "\\rightarrow&~    [ q_0^{(1 ) } , ( s'_{m'})_{m ' } , b_{\\hat n } ] .   \\end{split}\\ ] ] here , we first of all marked the input @xmath79 with a @xmath54 to indicate that we have done already a computation for that input .",
    "we then used an additional state @xmath171 , which simply traverses the input string @xmath79 until it hits a blank symbol .",
    "the machine then changes to the state @xmath172 and goes further to the left until it hits the next _ non - blank _ symbol on the input string .",
    "this symbol then indicates the beginning of the next input @xmath169 such that  starting from stage 1 again  we can readily execute the next computational cycle .",
    "suppose that the irreversible tm from sec .",
    "[ sec tms ] has @xmath173 many internal states , @xmath174 many different symbols ( including the blank ) on the tape and hence , it has @xmath175 many quintuples . furthermore , suppose it was given an input of length @xmath176 and produced an output of length @xmath177 after @xmath178 computational steps in total",
    ".    then , our reversible machine has @xmath179 states from the first , second , ... , fifth stage of the computation ,",
    "i.e. , in total @xmath180 states .",
    "furthermore , it needs @xmath181 many computational steps .",
    "here , the @xmath70 denotes the number of unknown blank symbols separating the current input from the next input on the input tape ( see stage 5 ) .",
    "note that we need @xmath182 such that there is enough space for the symbol @xmath54 and to guarantee that all input strings ( including potentially the symbol @xmath54 ) are separated by at least one blank symbol from eachother ."
  ],
  "abstract_text": [
    "<S> in analogy to brownian computers we explicitly show how to construct stochastic models , which mimic the behaviour of a general purpose computer ( a turing machine ) . </S>",
    "<S> our models are discrete state systems obeying a markovian master equation , which are logically reversible and have a well - defined and consistent thermodynamic interpretation . the resulting master equation , which describes a simple one - step process on an enormously large state space , allows us to thoroughly investigate the thermodynamics of computation for this situation . especially , in the stationary regime we can well approximate the master equation by a simple fokker - planck equation in one dimension . </S>",
    "<S> we then show that the entropy production rate at steady state can be made arbitrarily small , but the total ( integrated ) entropy production is finite and grows logarithmically with the number of computational steps . </S>"
  ]
}