{
  "article_text": [
    "neural maps are a widely ranging class of neural vector quantizers which are commonly used e.g. in data visualization , feature extraction , principle component analysis , image processing , and classification tasks .",
    "a well studied approach is the neural gas network ( ng ) @xcite .",
    "an important advantage of the ng is the adaptation dynamics , which minimizes a potential , in contrast to the self - organizing map ( som ) @xcite frequently used in vector quantization problems .    in the present paper",
    "we consider a new control scheme for the _ magnification _ of the map bishop97a , claussen2002b , dersch95a , luttrell91a , ritter92a , villmann2000e .",
    "controlling the magnification factor is relevant for many applications in control theory or robotics , were ( neural ) vector quantizers are often used to determine the actual state of the system in a first step , which is an objective of the control task @xcite .",
    "for instance , in @xcite was demonstrated that the application of a magnification control scheme for the neural gas based classification system of position and movement state of a robot can reduce the crash probability .",
    "another area of application is information - theoretically optimal coding of high - dimensional data as occur in satellite remote sensing image analysis of hyperspectral images @xcite which is , in fact , the task of equiprobabilistic mapping @xcite .",
    "further applications can be found in medical visualization and classification tasks @xcite .",
    "generally , vector quantization according to an arbitrary @xmath0-norm can be related to the problem of magnification control as it is explained below .",
    "the ng maps data vectors @xmath1 from a ( possibly high - dimensional ) data manifold @xmath2**@xmath3**@xmath4 onto a set @xmath5 of neurons @xmath6 , formally written as @xmath7 .",
    "each neuron @xmath6 is associated with a pointer @xmath8**@xmath3**@xmath4 also called weight vector , or codebook vector .",
    "all weight vectors establish the set @xmath9 .",
    "the mapping description is a winner take all rule , i.e. a stimulus vector @xmath10 is mapped onto the neuron @xmath11 the pointer @xmath12 of which is closest to the actually presented stimulus vector @xmath1 , @xmath13the neuron @xmath14 is called _",
    "winner neuron_. the set @xmath15is called ( masked ) receptive field of the neuron @xmath6 .    during the adaptation process a sequence of data points @xmath16 is presented to the map with respect to the stimuli distribution @xmath17 .",
    "each time the currently most proximate neuron @xmath18 according to ( [ argmin ] ) is determined , and the pointer @xmath12 as well as all pointers @xmath19 of neurons in the neighborhood of @xmath12 are shifted towards @xmath1 , according to @xmath20the property of being in the neighborhood of @xmath21  is represented by a neighborhood function @xmath22 .",
    "the neighborhood function is defined as @xmath23where @xmath24 is defined as the number of pointers @xmath25 for which the relation @xmath26 is valid , i.e. @xmath27  is the winning rank @xcite . in particular , for the winning neuron @xmath18 we have @xmath28 .",
    "we remark that in contrast to the som the neighborhood function is evaluated in the input space .",
    "moreover , the adaptation rule for the weight vectors in average follows a potential dynamics @xcite .",
    "the _ magnification _ of the trained map reflects the relation between the data density @xmath17 and the density @xmath29 of the weight vectors @xcite .",
    "for the ng the relation @xmath30 with @xmath31 has been derived @xcite .",
    "the exponent @xmath32 is called _",
    "magnification factor_. for the ng it depends on the _ intrinsic _ dimensionality @xmath33 of the data which can be numerically determined by several methods bruske95a , camastra2003a , camastra2001a , grassberger83a , takens85a . for simplicity",
    "we further require that the ( embedding ) data dimension is the intrinsic one .    generally , the information transfer is not independent of the magnification of the map @xcite .",
    "it is known that for a vector quantizer ( or a neural map in our context ) with optimal information transfer the relation @xmath34 holds .",
    "otherwise , a vector quantizer which minimizes the mean distortion error @xmath35has the magnification factor @xmath36with @xmath37 * * @xmath3**@xmath4 , i.e. the magnification of a vector quantizer is directly related to the minimization of the description error according to a certain @xmath0-norm @xcite .",
    "hence , the ng minimizes the usual @xmath38 distortion error .",
    "we now address the question how to extend the ng to achieve an _ a priori _ chosen optimization goal , i.e. an _ a priori _ chosen magnification factor .",
    "for the som several methods exist to control the magnification of the map . the first approach to influence the magnification of a learning vector quantizer , proposed in @xcite",
    "is called the _ mechanism of conscience_. for this purpose a bias term is added in the winner rule ( [ argmin ] ) : @xmath39where @xmath40 is the actual winning probability of the neuron @xmath6 and @xmath41 is a balance factor .",
    "hence , the winner determination is influenced by this modification .",
    "the algorithm should converge such that the winning probabilities of all neurons are equalized .",
    "this is related to a maximization of the entropy and consequently the resulting magnification is equal to unity .",
    "however , as pointed out by @xcite , adding a conscience algorithm to the som does not equate to equiprobabilistic mapping , in general . only for _ very high dimensions _",
    ", a minimum distortion quantizer ( such as the conscience algorithm ) approaches an equiprobable quantizer ( @xcite - page 93 ) .",
    "further , an arbitrary magnification can not be achieved by this mechanism . moreover , numerical studies of the algorithm have shown instabilities @xcite . to control the magnification ,",
    "a local learning parameter was introduced @xcite into the usual som - learning scheme .",
    "the now localized learning allows in principle an arbitrary magnification .",
    "other authors proposed variants which lead more away from the original som by kernel methods @xcite or statistical approaches @xcite .    for the ng a solution of the magnification control problem can be realized by introducing an adaptive _ local learning _",
    "step size @xmath42 @xcite according to the above mentioned approach for som @xcite .",
    "then , the new _ localized _ learning rule reads as @xmath43with the local learning parameters @xmath44 depending on the stimulus density @xmath45 at the position of the weight vectors @xmath19 via @xmath46the brackets @xmath47 denote the average in time , and @xmath14 is the best  matching neuron with respect to ( [ argmin ] ) . note , that the local learning rate @xmath42 of the winning neuron is applied in the adaptation step ( [ trn_local_lernen ] ) for each neuron .",
    "this approach finally leads to the new magnification law @xmath48which is a modification of the old one .",
    "hence , the parameter @xmath49 plays the role of a control parameter .",
    "however , in real applications one has to estimate the generally unknown data distribution @xmath45 .",
    "usually this is done by estimation of the volume of the receptive fields and the firing rates @xcite .",
    "this may lead to numerical instabilities of the control mechanism villmann97n , van_hulle00a , villmann99j .",
    "therefore , an alternative control mechanism is demanded .",
    "recently , a new approach for magnification control of the som was introduced @xcite which avoids the @xmath45-estimation problem .",
    "the respective approach is a generalization of a modification of the usual som @xcite .",
    "it is called ( generalized ) winner relaxing som ( wrsom ) @xcite . in winner relaxing som an additional term occurs in weight vector update for the winning neuron , implementing a relaxing behavior .",
    "the relaxing force is a weighted sum of the difference between the weight vectors and the input according to their distance rank .",
    "the relaxing term was originally introduced in @xcite to obtain a learning dynamic for som according to an average reconstruction error including the effect of shifting voronoi borders .",
    "it was shown that the generalized winner relaxing mechanism applied in wrsom can be used for magnification control in som , too @xcite .",
    "thereby , the winner relaxing approach provides a magnification control scheme for som which is _ independent _ of the shape of the data distribution only depending on parameters of the winner relaxing term .",
    "we now transfer the generalized winner relaxing approach for som to the ng and consider its influence on the magnification . in complete analogy to the wrsom we add a general winner relaxing term @xmath50 to the usual ng - learning dynamic ( [ allg_lernen ] ) .",
    "then the weight update reads as @xmath51whereby the winner relaxing term is defined as @xmath52depending on the additional weighting parameters @xmath53 and @xmath54 .",
    "we refer to this algorithm as the _ winner relaxing ng _",
    "the original winner relaxing term described in @xcite is obtained for the special parameter choice @xmath55 .",
    "note , that the relaxing term only contributes to the winner weight vector update as in the original approach .",
    "we now derive a relation between the densities @xmath29 and @xmath45 in analogy to @xcite for the winner relaxing learning ( winner_relaxing_learning ) .",
    "the procedure is very similar as in martinetz93d , villmann2000e .",
    "the average change @xmath56 for the winner relaxing ng learning rule ( [ winner_relaxing_learning ] ) is @xmath57 we now consider the equilibrium state , i.e. @xmath58 .",
    "for this purpose , we first separate the integral ( [ average_change ] ) into @xmath59with @xmath60@xmath61and @xmath62the integral @xmath63 is the usual one according to the ng dynamics whereas @xmath64 , @xmath65 are related to the winner relaxing scheme . in the following we treat each integral in a separate manner .",
    "thereby we always assume a continuum approach , i.e. the index @xmath6 becomes continuous .",
    "hence , for a given input @xmath1 one can find an optimal @xmath12 fulfilling even @xmath66 @xcite .",
    "doing so , the @xmath67-integral vanishes in the ( first order ) continuum limit because the integration over @xmath68 only contributes for @xmath21 , but in this case @xmath69 holds .",
    "we now pay attention to the @xmath65-integral : the continuum assumption made above allows a turn over from sum @xmath70 to the integral form @xmath71 in ( [ i3_integral ] ) .",
    "the further treatment is in complete analogy to the derivation of the magnification in the usual ng @xcite .",
    "let @xmath72 be the difference vector@xmath73the winning rank @xmath74 only depends on @xmath72 , therefore we introduce the new variable @xmath75which can be assumed as monotonously increasing with @xmath76 .",
    "thus , the inverse @xmath77 exists and we can rewrite the @xmath65-integral ( [ i3_integral ] ) into @xmath78 d\\mathbf{% v}\\]]with the @xmath79jacobian ",
    "matrix @xmath80@xmath65 only contributes to @xmath81 for the winning weight ( realized by @xmath68 ) , i.e. , for @xmath82 which is equal to @xmath1 according to the continuum approach .",
    "hence , the integration over @xmath1 yields@xmath83if @xmath84 rapidly decreases to zero with increasing @xmath72 , we can replace the quantities @xmath85 , @xmath86 by the first terms of their respective taylor expansions around the point @xmath87 neglecting higher derivatives .",
    "we obtain @xmath88which corresponds to @xmath89with @xmath90 as the volume of a @xmath33dimensional unit sphere @xcite .",
    "further , @xmath91and , hence,@xmath92therefore , the integral in equation ( [ i_3_equation ] ) can be rewritten as@xmath93",
    "@xmath94 the integral terms in ( [ taylor_integral ] ) of odd order in @xmath95 vanish because of the rotational symmetry of @xmath96 . then",
    "( [ i_3_equation ] ) yields , neglecting terms in higher order in @xmath95 ,    @xmath97    with@xmath98    it remains to consider the @xmath63-integral . as mentioned above , it is identical to the averaged adaptation of the usual ng .",
    "hence , the treatment can be taken from there and we get @xmath99as an equivalent equation @xcite .",
    "taking together ( [ i_1_solution ] ) and ( [ i_3_solution ] ) , the stationary solution of ( [ winner_relaxing_learning ] ) is given by @xmath100this differential equation roughly has the same form as the one for the usual neural gas ( [ i_1_solution ] ) .",
    "its solution is given by @xmath101with the exponent @xmath102being the magnification factor .",
    "hence , the magnification factor of the wrng can be described also in terms of the magnification of the usual neural gas @xmath103note , that the parameter @xmath104 of the winner relaxing term @xmath105 does not influence the magnification .",
    "two direct observations can be immediately made : firstly , the magnification exponent appears to be independent of the additional diagonal term ( controlled by @xmath104 ) for the winner which is in agreement with the wrsom result @xcite .",
    "therefore @xmath106 again is the usual setting in wrng for magnification control .",
    "secondly , by adjusting @xmath107 appropriately , the magnification exponent can be adjusted , e.g. to the most interesting case of maximum mutual information linsker87a , zador82a .",
    "maximum mutual information , which corresponds to optimal information transfer , is obtained when magnification equals the unit @xcite .",
    "hence , we have for this case the optimum value @xmath108 if the same stability borders @xmath109 of the wrsom also are valid here , one can expect to increase the ng exponent by positive values of @xmath110 , or to lower the ng exponent by a factor @xmath111 for @xmath112 .",
    "in contrast to the winner enhancing som , where the relaxing term has to be inverted ( @xmath113 ) to increase the magnification exponent , for the neural gas positive values of @xmath107 are required to increase the magnification exponent .",
    "however , the magnification factor still remains dependent on the generally unknown ( intrinsic ) dimension of the data .",
    "if this dimension is known , the parameter @xmath107 can be set _ a priori _ to obtain a neural gas of maximal mutual information . in this approach",
    "it is not necessary to keep track of the local reconstruction errors and firing rate for each neuron to adjust a local learning rate .",
    "possibilities for estimating the intrinsic dimension are the well - known grassberger - procaccia - analysis @xcite or the neural network approach using again a ng @xcite .",
    "however , one has to be cautious when transferring the @xmath114 result obtained above ( which would require to increase the number of neurons as well ) to a realistic situation where a decrease of @xmath115 with time will be limited to a final finite value to avoid the stability problems found in @xcite .",
    "if the neighborhood length in som is kept small but fixed for the limit of fine discretization , the neighborhood function of the second but one winner will again be of order 1 ( as for the winner ) .",
    "for the ng however the neighborhood is defined by the rank list . as",
    "the winner is not present in the @xmath116 integral , all terms share the factor @xmath117 by @xmath118 which indicates that in the discretized algorithm @xmath110 has to be rescaled by @xmath119 to agree with the continuum theory .",
    "the maximum coefficient @xmath120 that contributes to the @xmath116 integral is given by the prefactor of the second but one winner , which is given by @xmath121 . ]",
    "a numerical study shows how the winner - relaxing mechanism is able to control the magnification for optimization of the mutual information of a map generated by the wrng . using a standard setup as in villmann97n of @xmath122 neurons and @xmath123 training steps with a probability density @xmath124 , with fixed @xmath125 and @xmath126 decaying from @xmath127 to @xmath128 , the entropy of the resulting map computed for an input dimension of @xmath129 , @xmath130 and @xmath131",
    "is plotted in fig .",
    "[ fig_entropy_results ] .",
    "thereby , the entropy is computed using the winning probabilty @xmath40 of the neurons:@xmath132    the entropy shows a dimension - dependent maximum approximately at @xmath133 .",
    "the scaling of the position of the entropy maximum with input dimension is in agreement with the continuum theory , as well as the prediction of the opposite sign of @xmath107 that has to be taken to increase mutual information .",
    "our numerical investigation indicates that the above discussed prefactor , in fact , has to be taken in account for finite @xmath134 and a finite number of neurons .",
    "we obtain , within a broad range around the optimal @xmath107 the entropy is close to the maximum @xmath135 given by information theory .    in a second numerical study",
    "we investigate the influence of the additional diagonal term ( controlled by @xmath104 ) for the winner .",
    "already for the wrsom the magnification exponent is independent of this diagonal term @xcite . in the respective derviation ( @xmath67-integral ( [ i2_integral ] ) ) only first order approximations were used .",
    "otherwise , @xmath64 may contribute in higher orders . to verify that the contribution of an additionally added diagonal term is marginal",
    ", the entropy was calculated both for @xmath106 and @xmath136 @xcite .",
    "however , no influence on the entropy was found for the choice @xmath137 instead of @xmath106 .",
    "( fig  [ fig_independence_result ] ) .",
    "more pronounced is the influence of the diagonal term on stability ; according to the larger prefactor no stable behavior has been found for @xmath138 , therefore @xmath106 is the recommended setting .",
    "we introduced a winner - relaxing term in neural gas algorithm to obtain a winner - relaxing neural gas with the possibility of magnification control .",
    "the winner relaxing scheme is adopted from winner - relaxing som .",
    "the new controlling scheme offers a method which is independent on the explicit knowledge of the generally unknown data distribution which is an advantage in comparison to the earlier presented neural gas with localized learning for magnification control .",
    "in particular , we avoid the difficult determination of the data probability density by estimation of the volume of the receptive fields of the neuron and the firing rate .",
    "numerical simulations show the abilities of the proposed algorithm .        c.  m. bishop , m.  svensen , and c.  k.  i. williams .",
    "magnification factors for the som and gtm algorithms . in _ proceedings of wsom97 ,",
    "workshop on self - organizing maps , espoo , finland , june 4 - 6 _ , ( helsinki university of technology , neural networks research centre , espoo , finland , 1997 ) , 333338 .",
    "r.  w. brause , an approximation network with maximal transinformation , in m.  marinaro and p.  g. morasso , editors , _ proc .",
    "icann94 , international conference on artificial neural networks _ ,",
    "volume  i , ( london , uk , 1994 .",
    "springer),701704 .",
    "j.  c. claussen and h.  g. schuster , asymptotic level density of the elastic net self - organizing feature map , in j.  dorronsoro , editor , _ proc . international conf . on artificial neural networks ( icann ) _",
    "( springer , berlin 2002 ) , lecture notes in computer science 2415 , 939944 .    j.  c. claussen and t.  villmann , magnification control in neural gas by winner relaxing learning : independence of a diagonal term , in o.  kaynak , editor , _ proc . international conference on artificial neural networks ( icann2003 ) _",
    "( istanbul , 2003 ) , 5861 .          m.  herrmann and t.  villmann , vector quantization by optimal neural gas , in w.  gerstener , a.  germond , m.  hasler , and j .- d .",
    "nicoud , editors , _ artificial neural networks  proceedings of international conference on artificial neural networks ( icann97 ) lausanne _ , ( springer , berlin 1997 ) , lecture notes in computer science 1327 , 625630 .",
    "r.  linsker , towards an organizing principle for a layered perceptual network , in d.  z. anderson , editor , _ neural information processing systems _ , ( amer .",
    "phys . , new york , ny , 1987 ) , pages 485494 .",
    "f.  takens . on the numerical determination of the dimension of an attractor , in b.  braaksma , h.  broer , and f.  takens , editors , _ dynamical systems and bifurcations _ , ( springer , berlin , 1985 ) , lecture notes in mathematics no .",
    "1125 , 99106 .",
    "t.  villmann and a.  heinze , application of magnification control for the neural gas network in a sensorimotor architecture for robot navigation , in h .-",
    "gro , k.  debes , and h .- j .",
    "bhme , editors , _ proceedings of selbstorganisation von adaptivem verfahren ( soave2000 ) ilmenau _ , fortschrittsberichte des vdi ( vdi - verlag dsseldorf , 2000 ) , 125134 ."
  ],
  "abstract_text": [
    "<S> an important goal in neural map learning , which can conveniently be accomplished by magnification control , is to achieve information optimal coding in the sense of information theory . in the present contribution </S>",
    "<S> we consider the winner relaxing approach for the neural gas network . </S>",
    "<S> originally , winner relaxing learning is a slight modification of the self - organizing map learning rule that allows for adjustment of the magnification behavior by an _ a priori _ chosen control parameter . </S>",
    "<S> we transfer this approach to the neural gas algorithm . </S>",
    "<S> the magnification exponent can be calculated analytically for arbitrary dimension from a continuum theory , and the entropy of the resulting map is studied numerically conf irming the theoretical prediction . </S>",
    "<S> the influence of a diagonal term , which can be added without impacting the magnification , is studied numerically . </S>",
    "<S> this approach to maps of maximal mutual information is interesting for applications as the winner relaxing term only adds computational cost of same order and is easy to implement . in particular , it is not necessary to estimate the generally unknown data probability density as in other magnification control approaches .     </S>",
    "<S> +    and    neural gas , self - organizing maps , magnification control , vector quantization </S>"
  ]
}