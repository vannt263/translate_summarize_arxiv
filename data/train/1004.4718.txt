{
  "article_text": [
    "data mining has been pursued since the 1990 s , and clustering is an important technique in data mining .",
    "clustering is finding the groups of objects having similar features , and it has been rigorously studied  @xcite , since it has a wide range of applications .",
    "examples of the applications are target marketing and recommendation services .",
    "the former is finding groups of customers having similar purchasing patterns and then establishing marketing strategies according to the patterns .",
    "the latter is presenting the products to the customer who is highly likely to purchase them according to his / her sales preferences .",
    "recently , transaction databases have become a new target of clustering  @xcite .",
    "a _ transaction _ is defined as a set of related items , and a _ transaction database _ is a database consisting of the transactions obtained in an application  @xcite . as an example , figure  [ fig01 ] shows four transactions in a transaction database in the application of search engine services . each transaction contains the search keywords issued in the same user s session .",
    "another example of transaction database is the product purchase records at a big retail market such as wal - mart . in that database ,",
    "a transaction is defined as a set of products purchased by a customer at a time .",
    "[ cols= \" < \" , ]     transaction databases have introduced a few technical challenges .",
    "first , the objects handled in previous clustering algorithms were represented as @xmath0-dimensional vectors  @xcite .",
    "that is , they were represented as the points in @xmath0-dimensional space and were processed based on the euclidean distance between them  @xcite .",
    "however , the transactions in transaction databases can not be represented as @xmath0-dimensional vectors ; they are called _ categorical data _  @xcite .",
    "second , the size of transaction databases is much larger than the dataset handled in previous algorithms  @xcite . while the size of dataset in previous algorithms is about several kbs to several mbs",
    ", transaction databases have sizes of several gbs up to several tbs .",
    "in this paper , we emphasize the need for data cleansing , which is a pre - processing step before clustering on transaction databases , and propose a new data cleansing method that improves clustering performance and quality .",
    "previous clustering algorithms did not consider data cleansing process .",
    "in fact , transaction databases , such as search keyword databases , contain a lot of noise .",
    "for example , there are meaningless search keywords such as ` tjdnfeorhddnjs ' that never appear more than once in the database .",
    "this sort of noise causes an increase of the number of useless clusters and the degradation of clustering performance and quality .",
    "a relevant idea is used in information retrieval and text mining .",
    "we explain the differences in detail in section [ sec2 ] .",
    "this paper is organized as follows . in section  [ sec2 ]",
    ", we briefly explain the related work on clustering transaction databases . in section  [ sec3 ] , we explain the need for data cleansing and propose a new data cleansing method . in section  [ sec4 ]",
    ", we evaluate our data cleansing method through experiments .",
    "finally , we conclude this paper in section  [ sec5 ] .",
    "most of previous clustering algorithms handled only data objects that can be represented as @xmath0-dimensional vectors .",
    "there are small number of clustering algorithms that handle categorical data or transaction databases , and the most representative one is the rock algorithm  @xcite .",
    "it was shown in  @xcite that we could only get unsatisfactory clustering result on categorical data based on the euclidean distance .",
    "therefore , rock adopted jaccard coefficient as a similarity measure between categorical data . however , since rock has the time complexity higher than @xmath1 , where @xmath2 is the number of objects , it can hardly be applied to large - scale transaction databases .",
    "efficient clustering algorithms on large - scale transaction databases have been proposed in  @xcite . a new notion of _",
    "large item _ has been proposed in  @xcite . for a pre - specified support @xmath3 and a transaction item @xmath4 ,",
    "if the ratio of clusters containing @xmath4 in a cluster @xmath5 is larger than @xmath6 , the item @xmath4 is defined as a large item in the cluster @xmath5 ; otherwise , it is defined as a _",
    "small item_. the clustering algorithm in  @xcite , which we call the _ large _ algorithm in this paper , is executed in the direction of maximizing the number of large items and simultaneously minimizing small items by trying to bring the same transaction items together in a cluster .",
    "the clope algorithm  @xcite , an improvement of large , is also a heuristic algorithm and maximizes clustering quality by iteration .",
    "the algorithm does not use the notion of large / small items ; it proposed a more efficient measure for computing clustering quality .",
    "clope algorithm was shown in  @xcite to have better clustering performance and quality than rock and large through a series of experiments .",
    "the problems of large and clope are as follows .",
    "the algorithms did not consider the effect of noise data and assumed that the number of result clusters @xmath7 is very small .",
    "however , in actual transaction databases , there contained a lot of noise data with very low frequencies , and the number of result clusters is fairly close to the number of transactions @xmath2 .",
    "as a matter of fact , @xmath7 should be highly variable depending on transactions in the database and items contained in the transactions .",
    "if @xmath7 is very small compared with @xmath2 , the average number of transactions in a cluster should be very high , and such large clusters should have little practical usefulness .",
    "large and clope have the time complexity of @xmath8 , which approaches @xmath1 as @xmath7 approaches @xmath2 .    in a broad sense , a text database or a document database",
    "can be regarded as a form of transaction database ; a term and a document correspond to an item and a transaction , respectively .",
    "however , these databases have a few essential differences from the transaction databases as the following .",
    "first , since the primary application of text databases is , given a query term , finding and ranking relevant documents , the relevance metrics and feature selection methods are defined between a term and a document  @xcite . however , in the transaction database , we use the similarity metrics defined between transactions since we are interested in the relationship between transactions . when clustering documents using relevance metrics such as tf - idf , we should compute the relevance value for each combination of a term and a document , and then we generate feature vectors for each of the documents  @xcite , which causes severe performance degradation .",
    "this preprocessing cost becomes larger when dealing with a larger size of text databases .",
    "however , when clustering transactions using inter - transaction similarity metrics , we do not need the preprocessing step of generating feature vectors , and the clustering performance is not severely affected by the size of transaction databases .",
    "this advantage of inter - transaction similarity metrics over term - document relevance metrics is more significant when dealing with a frequently updated database .",
    "when the database is updated , the entire feature vectors in text database should be re - generated , which is totally unnecessary in the transaction databases .",
    "second , most transaction databases do not allow duplicated items in a transaction , while any number of same terms can appear in a document in text databases .",
    "this causes some relevance metrics useless in transaction databases . for example , for an item @xmath9 and a transaction @xmath10 , the term frequency is @xmath11 , where @xmath12 is the cardinality of @xmath10 , i.e. , the number of items in @xmath10 , and the inverse document frequency is always identical .",
    "hence , the tf - idf value between @xmath9 and @xmath10 is dependent only on the cardinality of @xmath10 ; the transaction @xmath10 of smaller size is regarded to be more relevant to @xmath9 , which is nonsense .",
    "third , although removing some high frequency and low frequency terms is effective in text databases , the detailed procedure is very different from that in transaction databases . they should be very cautious when removing unnecessary terms in text databases ; the terms should not be removed only due to their frequencies , and it is true for both high frequency and low frequency terms .",
    "for example , in the world movie database , the term ` ponyo ' should not be removed only because it appears very rarely , since there should be a lot of people that are interested in the japanese animation `` ponyo on the cliff . '' removing unnecessary terms in a majority of text databases is controlled under human supervision , which means that it can hardly be fully automated",
    ". however , the transaction database has no such issue , and removing unnecessary items can be fully automated . in this paper",
    ", we propose a new fully automated data cleansing method with minimal parameter settings and show its effectiveness through experiments .",
    "in this section , we explain the need for data cleansing and propose a new data cleansing method that improves clustering performance and quality .",
    "our data cleansing method decides the usefulness of items according to their frequencies in transactions .",
    "figure  [ fig02 ] shows the item frequencies in two real - world transaction databases .",
    "the horizontal axis represents item frequencies , and the vertical axis represents the number of items . as shown in the figure ,",
    "there exist a lot of items whose frequencies are very small .",
    "the two transaction databases are explained in detail in section  [ sec4 ] .     + ( a ) aol database .",
    "+   + ( b ) keywords database .    transaction items with too low or too high frequencies have negative effects on clustering performance and quality .",
    "we explain the phenomenon with examples .",
    "we use the same similarity measure between transactions as rock as the following eq .",
    "( [ eq1 ] ) : @xmath13 where the denominator represents the number of whole items ( without duplication ) contained in transactions @xmath14 and @xmath15 , and the numerator represents the number of items commonly contained in @xmath14 and @xmath15 .",
    "first , we explain the effect of the items with too low frequencies .",
    "assume that similarity threshold @xmath6 between transactions is given as @xmath16 .",
    "consider three transactions @xmath17 , @xmath18 , and @xmath19 .",
    "then , for every transaction pair @xmath20 and @xmath21 @xmath22 , it holds that @xmath23 , and hence the transactions @xmath14 , @xmath15 , and @xmath24 does not form a cluster .",
    "however , by removing the items with very low frequencies ( i.e. , @xmath25 ) , @xmath14 , @xmath15 , and @xmath24 become @xmath26 , @xmath27 , and @xmath28 , respectively .",
    "since , for every transaction pair @xmath29 and @xmath30 , it holds that @xmath31 , three transactions @xmath32 , @xmath33 , and @xmath34 should form a useful cluster .",
    "in fact , we can easily find enormous number of such transactions as @xmath14 , @xmath15 , and @xmath24 in real - world transaction databases .",
    "the problem due to low frequency items can not be solved by adjusting or lowering the threshold @xmath6 , because the number of low frequency items is not constant across transactions and hence the threshold can not be fixed .",
    "second , we show an example where clustering quality is degraded due to the items with too high frequencies . consider four transactions @xmath35 , @xmath36 , @xmath37 , and @xmath38 . since , for every transaction pair @xmath20 and @xmath39 @xmath40 , it holds that @xmath31 , it is highly likely that the transactions @xmath14 , @xmath15 , @xmath24 , and @xmath41 should form a large useless cluster @xmath42 .",
    "however , by removing the items with very high frequencies ( i.e. , @xmath43 ) , @xmath14 , @xmath15 , @xmath24 , and @xmath41 become @xmath44 , @xmath45 , @xmath46 , and @xmath47 , respectively .",
    "the transactions @xmath32 , @xmath33 , @xmath34 , and @xmath48 naturally form two useful clusters @xmath49 and @xmath50 . similarly to",
    "low frequency items , there are enormous number of transactions such as @xmath14 , @xmath15 , @xmath24 , and @xmath41 in real - world transaction databases , and the problem due to high frequency items can not be solved by adjusting or raising the threshold @xmath6 .",
    "we assume that the item frequency shown in figure  [ fig02 ] should follow the lognormal or the exponential distribution  @xcite .",
    "based on this assumption , our data cleansing method performs as the following .",
    "first , in the transaction database , we count the number of transaction items for each item frequency ( a positive integer value ) .",
    "next , using the ( item frequency , count ) pairs , we estimate the parameters such as mean @xmath51 and standard deviation @xmath52 for the lognormal or the exponential distribution .",
    "finally , for a pre - specified parameter @xmath53 , we remove all the items whose frequencies are either less than @xmath54 or greater than @xmath55 . after removing such items",
    ", we also remove empty transactions whose items have been entirely removed . in most cases , @xmath53 should be 3 @xmath56 5 .    in the case of lognormal distribution ,",
    "the estimates for two parameters @xmath51 and @xmath52 are obtained using the following eq .",
    "( [ eq2 ] ) : @xmath57 where @xmath2 is the number of transaction items , and @xmath58 represents item frequency .",
    "if there are @xmath7 items whose frequencies are @xmath58 , then @xmath58 appears @xmath7 times in eq .",
    "( [ eq2 ] ) .    in the case of exponential distribution , we compute the estimates for two parameters @xmath51 and @xmath52 using the following eq .",
    "( [ eq3 ] ) : @xmath59 where the estimate @xmath60 is computed as the following : @xmath61    choosing which of two distributions for a specific transaction database is highly dependent on human expert s view . in our experiments , while choosing any of two distributions contributed to the improvement of clustering quality and performance , the lognormal distribution was more effective .",
    "moreover , improper selection of parameter @xmath53 value could result in worse clustering performance and quality .",
    "larger @xmath53 values were advantageous for the lognormal distribution , while smaller @xmath53 values were advantageous for the exponential distribution .",
    "our data cleansing method can improve the quality of _ incomplete _ clustering results .",
    "clope can not always achieve complete clustering ; actually , in most cases , its clustering results are incomplete . in such cases ,",
    "our method helps improve clustering quality as well as clustering performance .",
    "in this section , we evaluate our data cleansing method through a series of experiments . for our evaluation",
    ", we implemented clope  @xcite and executed it using real - world transaction databases .",
    "we compared clustering quality and performance between two cases : case  ( 1 ) using our data cleansing method and case  ( 2 ) without using it . in case  ( 1 ) , the target transaction databases are pre - processed by our data cleansing method and then clustered by clope , while , in case  ( 2 ) , the databases are directly clustered by clope .",
    "as explained in section  [ sec2 ] , clope is a heuristic algorithm that enhances clustering quality by iteration .",
    "the algorithm computes quality measure called _ profit _ of the intermediate clustering result at every iteration , and it stops when the profit does not increase any more . in our evaluation , we use the final profit as the clustering quality measure .",
    "clope receives _ repulsion _ @xmath62 as an input parameter .",
    "repulsion is a real value for controlling inter - cluster similarity ; higher repulsion implies tighter similarity .",
    "repulsion plays the analogous role of threshold @xmath6 parameter given to rock and large , and by adjusting repulsion , we can control the number and quality of clusters .",
    "it was justified experimentally in  @xcite that , by using the profit as a metric of clustering quality , clope was more effective than the previous algorithms . in the experiment",
    ", clope was run on the mushroom dataset which contains human classification information on poisonous and edible mushrooms .",
    "clope achieved the accuracy of 100% for the repulsion @xmath63 .",
    "we used two datasets for our evaluation : ( a )  aol search query database and ( b )  keyword registration database .",
    "the aol database consists of about 20 m queries issued by about 650k users from march 1 through may 31 , 2006 .",
    "the database is a list of records , and every record consists of five fields _ anonid _ , _ query _ , _ querytime _ , _ itemrank _ , and _",
    "clickurl_. the first three fields anonid , query , and querytime represent anonymous user i d , search keyword by the user , and timestamp when the query was issued , respectively .",
    "the fields itemrank and clickurl are optional , and they appear when the user clicked on any item in query result ; they represent the rank and url of the item clicked by the user , respectively .",
    "the keyword registration database is a transaction database ; each transaction consists of a url and a list of registered keywords .",
    "the same keyword can be registered by multiple urls .",
    "when a query on a certain keyword is issued , the urls that registered the keyword are shown in the query result .",
    "we transformed aol database into a transaction database in the form shown in figure  [ fig01 ] for clustering by clope .",
    "since a record in aol database has a query at one time , a user s search queries are spread into multiple records , which appear adjacently in the aol database .",
    "the queries by the same user are collected and a record ( transaction ) is formed in the transaction database .",
    "we used the user - id field ( anonid ) when transforming aol dataset into a transaction database . a transaction in the transaction database shown in figure  [ fig01 ]",
    "contains all the query terms of the same user - id .",
    "the query terms of the same user - id are collected into one transaction , and different transactions have different user - ids .",
    "hence , the inter - transaction similarity based on user - id becomes always zero .",
    "we believe that the recommender systems should undergo similar procedures .",
    "the settings for our evaluation are as follows .",
    "we used a pc equipped with intel core2quad q9550 2.83ghz cpu , 4 gb ram , and 600 gb hdd and implemented programs using gnu c++ 4.1.2 on centos",
    "linux 5.4 64bit edition with kernel 2.6.18 .",
    "we set repulsion for clope as @xmath64 , which is a largest value permitted by our system .",
    "we assumed that the number of transaction items follow the lognormal distribution and set @xmath65 .",
    "figure  [ exp1 ] shows the result of the first experiment using ( a )  aol database ; it compares clustering quality and performance between the cases  ( 1 ) and ( 2 ) . in case  ( 2 ) , for the number of transactions 50k , our program",
    "was terminated abnormally , which is most likely due to lack of main memory and swap space . as shown in the figure , clustering quality and performance",
    "was improved by applying our data cleansing method for every number of transactions .",
    "the improvement ratio of quality and performance reached up to 165% and 330% , respectively . in case",
    "( 1 ) , much smaller number @xmath7 of clusters were formed by clope under the same settings .",
    "for that reason , since clope has @xmath8 time complexity , we could gain the improvement of clustering performance .",
    "+ ( a ) clustering quality .",
    "+   + ( b ) clustering performance .",
    "we performed the second experiment using ( b )  keyword database with the same settings as the first experiment , and the result is shown in figure  [ exp2 ] . as in figure  [ exp1 ] , clustering quality and performance was also improved by applying our data cleansing method for every number of transactions .",
    "the improvement ratio of quality and performance reached up to 115% and 166% , respectively .",
    "+ ( a ) clustering quality .",
    "+   + ( b ) clustering performance .",
    "the third experiment was performed for two distributions and a few parameter @xmath53 values .",
    "we used ( a )  aol database used in the first experiment , and the number of transactions was set as 10k .",
    "the experiment result is shown in figure  [ exp3 ] . with the lognormal distribution , clustering quality and performance converge to a point for @xmath53 values larger than or equal to 4.0 .",
    "this means that there is no improvement in clustering quality and performance by our data cleansing method . with the exponential distribution ,",
    "smaller @xmath53 values were advantageous for improving clustering quality and performance .     + ( a ) clustering quality .",
    "+   + ( b ) clustering performance .",
    "in this paper , we emphasized the need for data cleansing as a pre - processing step before clustering large - scale transaction databases and proposed a new data cleansing method that improves clustering quality and performance . as the result of our evaluation on our data cleansing method through experiments , the clustering quality and performance",
    "were significantly improved by up to 165% and 330% , respectively .",
    "although our evaluation was performed by clope , we believe that other clustering algorithms such as rock and large should profit by applying our data cleansing method .    00 f. beil , m. ester , and x. xu , `` frequent term - based text clustering , '' in _ proc .",
    "acm sigkdd intl conf . on knowledge discovery and data mining ( kdd ) _ , edmonton , alberta , canada , pp .",
    "436 - 442 , july 2002 .",
    "z. feng , b. zhou , and j. shen , `` a parallel hierarchical clustering algorithm for pcs cluster system , '' _ neurocomputing _ , vol .",
    "70 , no . 4 - 6 ,",
    "809 - 818 , jan .",
    "s. guha , r. rastogi , and k. shim , `` rock : a robust clustering algorithm for categorical attributes , '' _ information systems _ , vol .",
    "345 - 366 , 2000 .",
    "j. han and m. kamber , _ data mining : concepts and techniques _ , 2nd edition , morgan kaufman , 2006 .",
    "g. karypis , e .- h .",
    "han , and v. kumar , `` chameleon : hierarchical clustering using dynamic modeling , '' _ ieee computer _ , vol .",
    "32 , no . 8 , pp .",
    "68 - 75 , aug . 1999 .",
    "r. kohavi , b. m. masand , m. spiliopoulou , and j. srivastava , `` web mining , '' _ data mining and knowledge discovery ( dmkd ) _ , vol . 6 , no .",
    "1 , pp . 5 - 8 , jan . 2002 .",
    "t. liu , s. liu , z. chen , and w .- y .",
    "ma , `` an evaluation on feature selection for text clustering , '' in _ proc . of the intl conf . on machine learning ( icml ) _ ,",
    "washington , dc , pp .",
    "488 - 495 , aug . 2003 .",
    "a. nanopoulos and y. manolopoulos , `` efficient similarity search for market basket data , '' _ vldb journal _ , vol .",
    "138 - 152 , 2002 .",
    "b. tang , m. a. shepherd , m. i. heywood , and x. luo , `` comparing dimension reduction techniques for document clustering , '' _ lecture notes in computer science ( lncs ) _ , vol . 3501 , pp .",
    "292 - 296 , may 2005 .",
    "k. s. trivedi , _ probability and statistics with reliability , queueing , and computer science applications _ , 2nd edition , wiley - interscience , oct",
    "k. wang , c. xu , and b. liu , `` clustering transactions using large items , '' in _ proc .",
    "intl conf . on information and knowledge management ( cikm )",
    "_ , kansas city , missouri , usa , pp .",
    "483 - 490 , nov . 1999 .",
    "y. yang , x. guan , and j. you , `` clope : a fast and effective clustering algorithm for transactional data , '' in _ proc .",
    "acm sigkdd intl conf . on knowledge discovery and data mining ( kdd ) _ , edmonton , alberta , canada , pp .",
    "682 - 687 , july 2002 ."
  ],
  "abstract_text": [
    "<S> in this paper , we emphasize the need for data cleansing when clustering large - scale transaction databases and propose a new data cleansing method that improves clustering quality and performance . </S>",
    "<S> we evaluate our data cleansing method through a series of experiments . as a result , </S>",
    "<S> the clustering quality and performance were significantly improved by up to 165% and 330% , respectively . </S>"
  ]
}