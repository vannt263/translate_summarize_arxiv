{
  "article_text": [
    "during the past decades , a large number of theoretical results have been obtained for supervised learning such as classification and regression @xcite . for unsupervised learning , however , relatively few theoretical results are available .",
    "a main difficulty is that the objective functions in unsupervised learning are usually non - convex and multi - modal , so the optimization algorithms usually can not find the global optima . as a result , it is generally difficult to obtain theoretical guarantees for the performances of the unsupervised learning algorithms .",
    "a simple and typical example of unsupervised learning is clustering or learning mixture models , and a typical algorithm for fitting the mixture models is the em algorithm @xcite , which is a statistical counterpart of the k - means algorithm for clustering .",
    "although the em algorithm is simple and interpretable , and is known to converge monotonically to a local mode of the observed - data log - likelihood , little is known about its theoretical performance in terms of correctly recovering the mixture components . as such , the em algorithm",
    "is often considered a heuristic algorithm .",
    "a major recent advance in the theoretical understanding of the em algorithm for fitting mixture models was made by dasgupta and shulman @xcite .",
    "they proposed a two - round variant of the em algorithm that consists of only two iterations of em : the first iteration is initialized from a number of randomly selected training examples as the centers of the gaussian distributions , and the second iteration is carried out after pruning the clusters learned from the first iteration .",
    "they showed that the two - round em can learn the mixture of gaussian distributions with near optimal precision with high probability if the gaussian distributions are well separated and if the dimensionality of the gaussian distributions is sufficiently high . here",
    "near optimal precision means that one can estimate the parameters of the gaussian distributions as if the memberships of the observations are known .         in this paper",
    ", we generalize the theory of dasgupta and shulman @xcite to learning mixture of bernoulli templates .",
    "each template is a binary vector , and it generates examples by independently switching its binary components with a certain probability . so the observed examples are also binary vectors .",
    "this setup is a version of the latent class model of @xcite restricted to binary data . in potential applications in computer vision ,",
    "a binary vector is a feature map of an image , where each binary component indicates whether a local feature or structure is present or absent within a certain cell of the image domain .",
    "[ fig:1 ] illustrates the basic idea by a synthetic example .",
    "the image domain is equally partitioned into squared cells ( in the example in fig .",
    "[ fig:1 ] , there are a total of @xmath0 cells in the image domain ) .",
    "there is an alphabet of sketch patterns that can appear in these cells ( fig .",
    "[ fig:1 ] shows an alphabet of 18 types of sketch patterns ) .",
    "each cell may contain one or more sketch patterns , so the binary vector for each image consists of @xmath1 binary components , each component indicates whether a certain sketch pattern is present or not within a certain cell .",
    "specifically , each component is a binary decision that can be made based on local edge detection , gabor filter responses @xcite , beamlet transformation @xcite or a pre - trained classifier .",
    "a gabor filter is a 2d linear filter that has a prefered orientation . along that orientation",
    "the gabor filter resembles a gaussian and along the perpendicular direction it resembles the derivative of a gaussian . it was shown in @xcite that the gabor filters are a good approximation of the receptive field profiles of orientation - sensitive neurons in a cat s visual cortex .",
    "the formulation is very general .",
    "one can design any alphabet of local features or patterns , and one can use any binary detector or classifier to decide the presence or absence of these features within each cell . the whole feature map is a composition of local image features and is in the form of a binary vector , usually high dimensional ( on the order of @xmath2 ) .",
    "a template itself is a binary vector that is subject to component - wise switching or bernoulli noise to account for the variations of the feature maps of individual images .",
    "the reason we focus on binary feature maps in this article is that they are easy to design and we do not need to make strong assumptions on their distributions such as gaussianity .        as another illustration , fig .",
    "[ fig : ab ] displays some examples of real images and their binary sketches based on a simple design of image features and binary decision rules .",
    "we partition the image domain into squared cells of equal size ( in these images , the cells are relatively small , ranging from @xmath3 pixels to @xmath4 pixels ) .",
    "we convolve the image with gabor filters @xcite at 8 orientations . within each cell , at each orientation , we pool a local maximum of the gabor filter responses ( in absolute values ) . if the local maximum is above a threshold , we then declare that there is a sketch within this cell at this orientation , and the sketch is depicted by a bar in the corresponding binary sketch image in fig .",
    "[ fig : ab ] .",
    "clearly the sketch image captures a lot of information in the corresponding original image .    now back to the issue of learning mixture models by em .",
    "we assume that there are @xmath5 bernoulli templates , and each observed example is a noisy observation of one of the @xmath5 template .",
    "the question we want to answer is : given a number of training examples that are noisy observations of the @xmath5 templates , can an em - type algorithm reliably recover these @xmath5 templates with high probability ?",
    "the reason we are interested in this question is that it will shed light on unsupervised learning of templates of objects ( or their parts ) from real images , which is a crucial task for object modeling and recognition in computer vision .",
    "many learning methods are based on fitting mixture models by em - type algorithms , including the active basis model @xcite . in the language of the and - or graph @xcite for object modeling , each template is an and - node , which is a composition of a number of sketches .",
    "the mixture of @xmath5 templates is an or - node , with each template being its child node .",
    "so the mixture of the templates is an or - and structure .",
    "the theoretical results in this paper will be useful for us to understand the learning of the or - and structure from training images .    to answer the above question",
    ", we shall generalize the theory of dasgupta and shulman @xcite to bernoulli distributions , and we shall show that the two - round em algorithm can learn mixtures of bernoulli templates with near optimal precision with high probability if the templates are sufficiently different and if the dimensions are sufficiently high .    generalizing",
    "the theory of @xcite from gaussian mixtures to the mixtures of bernoulli distributions is far from being straightforward .",
    "the sample space is no longer euclidean , and some results for gaussian distributions can not be translated directly into those for the bernoulli models .",
    "so we have to establish a theoretical foundation that is suitable for our purpose .",
    "for example , we will need bounds on the tails of the distribution of distances between a template @xmath6 and the mean of @xmath7 binary vectors obtained by perturbing @xmath6 by bernoulli noise .",
    "similar bounds for the gaussian case are easy to obtain because the moment generating function of @xmath8 is known when @xmath9 is an isotropic gaussian .",
    "the rest of the paper is organized as follows .",
    "section 2 describes the two - round em algorithm and states the main theorem .",
    "sections 3 to 4 present theoretical results that lead to the proof of the main theorem . section 5 illustrates the theoretical results by some experiments on synthetic and real examples .",
    "section 6 concludes with a discussion . in the text",
    ", we shall only state the theoretical results .",
    "the proofs can be found in the appendix .",
    "let @xmath6 be a template .",
    "it is an @xmath10-dimensional binary vector , i.e. , @xmath11 . in the example in fig . 1 , @xmath12 .",
    "let @xmath13 be the @xmath14-th component of @xmath6 , @xmath15 .",
    "an example @xmath16 generated by @xmath6 is a noisy version of @xmath6 , and we write @xmath17 .",
    "specifically , let @xmath18 be the @xmath14-th component of @xmath16 .",
    "then @xmath19 with probability @xmath20 , and @xmath21 with probability @xmath22 , i.e. , @xmath22 is the probability of switching a component of @xmath6 , and it defines the level of bernoulli noise .",
    "we assume that @xmath23 .",
    "we also assume that the components of @xmath16 are independent given @xmath6 .",
    "we call @xmath6 a bernoulli template because it is binary and is subject to bernoulli noise .",
    "let @xmath24 be @xmath5 bernoulli templates with mixture weights @xmath25 .",
    "we assume that @xmath5 is given .",
    "otherwise , @xmath5 can be determined by some model selection criteria such as bic @xcite .",
    "let @xmath26 be @xmath7 noisy observations of these @xmath5 templates , where the noise level is @xmath22 . the probability that @xmath27 is generated by @xmath28 is @xmath29 , and we let @xmath30 .",
    "we define @xmath31 to be the expectation of the examples generated by @xmath28 , i.e. , @xmath32 $ ] where @xmath33 .",
    "let @xmath34 be the set of examples coming from the template @xmath28 .",
    "for two @xmath10-dimensional vectors @xmath6 and @xmath35 , let @xmath36 be the @xmath37 distance between @xmath6 and @xmath35 .",
    "let @xmath38 be the separation between @xmath28 and @xmath39 , i.e. , @xmath40 .",
    "the mixture is called @xmath41-separated if @xmath42 .",
    "we shall show that if the separation @xmath41 is sufficiently large , then the two - round em algorithm will reliably recover @xmath24 .",
    "we use the notation @xmath43 to denote the estimated @xmath28 . in the two - round em , the first round initializes @xmath44 to be @xmath45 randomly selected training examples .",
    "the initial number of clusters , @xmath45 , is greater than the true number @xmath5 .",
    "specifically , we let @xmath46 , where @xmath47 is the confidence parameter , i.e. , with probability @xmath48 , the algorithm will succeed in recovering the mixture components . according to the coupon collector problem ,",
    "the @xmath45 examples cover all the @xmath5 clusters with high probability .",
    "we estimate the bernoulli noise level @xmath49 so that @xmath50 based on the statistics of distances between examples derived in prop .",
    "[ prop : eps ] .",
    "then we run one more iteration of em .",
    "examples @xmath51 , @xmath52 templates @xmath53 initialize @xmath54 as @xmath45 random training examples initialize @xmath55 and @xmath56 such that @xmath57 e - step : compute for each @xmath58 @xmath59 m - step : update , for @xmath60 , @xmath61 pruning : remove all @xmath62 with @xmath63 pruning : keep only @xmath5 templates @xmath62 far apart .",
    "let @xmath64 index the remaining @xmath5 templates .",
    "initialize @xmath65 and @xmath66 .",
    "e - step : compute , for @xmath64 , @xmath67 m - step : update , for @xmath64 , @xmath68    after the first iteration , we prune the clusters by a starvation scheme .",
    "the pruning process consists of two stages . in the first stage , we remove all the templates @xmath69 whose weights are below a threshold @xmath70 . in the second stage , we keep only @xmath5 templates that are far apart from each other through an inclusion process . specifically , we start the inclusion process by randomly picking a template . then in each subsequent step of the inclusion process , we add a template that is farthest away from the selected templates in terms of the minimum distance between the candidate template and the selected templates .",
    "we repeat this step until we get @xmath5 templates .",
    "we let @xmath64 to index the remaining @xmath5 templates .    after the pruning process",
    ", we run another iteration of em .",
    "the estimated templates from this second round em are already near optimal as we will show .",
    "to be more precise , algorithm 1 describes the two - round em . in step 9",
    "the templates @xmath71 are to be converted to binary by rounding to the nearest integer .      for the convenience of reference ,",
    "the following summarizes the notation used in this paper :    * @xmath10 is the dimension of bernoulli templates , which generate examples in @xmath72 .",
    "* @xmath7 is the number of observations .",
    "* @xmath5 is the true number of clusters .",
    "* @xmath73 is the level of noise * @xmath74 , * @xmath75 * @xmath76 : the minimum of the mixture weights . * @xmath28 is the @xmath77-th bernoulli template * @xmath34 is the set of examples coming from the template @xmath28 .",
    "* @xmath36 is the @xmath37 distance between @xmath78 and @xmath79 .",
    "* @xmath38 is the separation between the bernoulli templates , @xmath80 * @xmath81 * @xmath45 is the initial number of mixture components @xmath46 .",
    "the parameter @xmath47 is the confidence level in theorem [ thm : main ] .",
    "* @xmath82 is the threshold for pruning the clusters learned by the first round .",
    "* @xmath83 collects the templates that are initialized from examples in the @xmath77-th cluster @xmath34 and survive the pruning process after the first round of em , i.e. @xmath84      [ thm : main ] let @xmath7 examples be generated from a mixture of @xmath5 bernoulli templates under bernoulli noise of level @xmath22 and mixing weights @xmath85 for all @xmath77 .",
    "let @xmath86 .",
    "if the following conditions hold :    1 .",
    "the initial number of clusters is @xmath87 2 .",
    "the number of examples is @xmath88 .",
    "3 .   the separation is @xmath89 4 .",
    "the dimension is @xmath90    then with probability at least @xmath48 , the estimated templates after the round 2 of em satisfy : @xmath91    the above theorem states that with high probability , the estimated templates from the two - round em is nearly as accurate as if we knew the memberships of the examples .",
    "the proof follows the steps of the two - round em .",
    "we show that after the initialization , with high probability , the initial templates cover all the clusters and the estimated noise level @xmath49 is close to the true noise level @xmath22 . then after the first round , the estimated templates are likely to be close to the true templates of the same clusters . after the pruning process , we prove that it is very likely that exactly one template is kept for each cluster . finally after the second round",
    ", the estimated templates are proved to be near optimal .",
    "we shall first establish some basic facts about the bernoulli templates perturbed by bernoulli noise .",
    "they are concerned with the @xmath37 distances among templates and their examples .",
    "let @xmath92 be bernoulli templates with noise level @xmath22 .",
    "we have : [ thm : expbern ]    1 .   if @xmath17 then @xmath93=nq , var[d({\\mathbf{x}},{\\mathbf{p}})]=nq(1-q ) \\end{split}\\ ] ] 2 .   if @xmath17 and @xmath94 then @xmath95&=nq+d({\\mathbf{p}},{\\mathbf{y}})(1 - 2q)\\\\ var[d({\\mathbf{x}},{\\mathbf{y}})]&=nq(1-q ) \\end{split}\\ ] ] 3 .   if @xmath96 then @xmath95&=2nq(1-q)\\\\ var[d({\\mathbf{x}},{\\mathbf{y}})]&=2nq(1-q)(1 - 2q+2q^{2 } ) \\end{split}\\ ] ] 4 .",
    "if @xmath97 then @xmath95&=2nq(1-q)+d({\\mathbf{p}},{\\mathbf{q}})(1 - 2q)^{2}\\\\ var[d({\\mathbf{x}},{\\mathbf{y}})]&=2nq(1-q)(1 - 2q+2q^{2 } ) \\end{split}\\ ] ]    let @xmath92 be bernoulli templates with noise level @xmath22 .",
    "we have : [ thm : devbern ]    1 .   if @xmath17 and @xmath98 then @xmath99 2 .   if @xmath17 and @xmath100 then @xmath101 3 .   if @xmath102 and @xmath103 then for any @xmath104 @xmath105    prop .",
    "[ thm : devbern ] states that the @xmath37 distance between an example and its template is concentrated around @xmath106 , while the distance between two examples from two different templates is concentrated around @xmath107 .",
    "this leads to the following proposition .",
    "draw @xmath7 samples from a c - separated mixture of @xmath5 bernoulli templates with mixing weights at least @xmath76 .",
    "[ prop : eps ] let @xmath108 .",
    "then with probability at least @xmath109    1 .   for any @xmath110",
    "we have @xmath111 2 .   for any @xmath112 , @xmath113",
    ", we have @xmath114 3 .   for any @xmath115 we have @xmath116 4 .   each @xmath117 .    here",
    "we employ the notation that @xmath118 means @xmath119 .",
    "let @xmath120 where @xmath121 are bernoulli random variables with @xmath122=q$ ] .",
    "then [ lem : abszq ] @xmath123    * ( average of subsets ) * draw a set @xmath124 of @xmath7 examples randomly from template @xmath125 with noise level @xmath126 . then with probability at least @xmath48 for any subset of size at least @xmath127 there is no subset of @xmath124 of size at least @xmath128 whose average @xmath129 has [ prop : avg1 ] @xmath130    prop .",
    "[ prop : avg1 ] states that the sample average is unlikely to deviate too far from @xmath6 .    *",
    "( weighted averages ) * for any finite set of points @xmath131 and weights @xmath132,{\\mathbf{x}}\\in s$ ] there exists a subset @xmath133 such that [ prop : wtavg ]    1 .",
    "@xmath134 2 .",
    "@xmath135 where @xmath136    prop .",
    "[ prop : wtavg ] states that the weighted average can be bounded by unweighted average .",
    "this result is needed because the templates are estimated as the weighted averages in both rounds of the em algorithm and from prop .",
    "[ prop : avg1 ] and [ prop : wtavg ] we can bound on the distance to the template .",
    "in this section we state the results that hold for the estimated template parameters after each em iteration .",
    "we assume that the following technical conditions hold    1 .",
    "@xmath137 2 .",
    "@xmath138 3 .",
    "@xmath139    these conditions are a subset of the conditions of theorem [ thm : main ] that do nt depend on @xmath76 and @xmath47 .",
    "they will be referred to in the proofs of the statements of this section .",
    "we also assume that @xmath140 where condition c3 guarantees that @xmath141 .",
    "observe that condition @xmath142 imposes an upper bound on the noise level @xmath22 since @xmath143 . in our experiments",
    "this upper bound was between @xmath144 and @xmath145 .",
    "this section analyzes the initial estimates for the parameters before the first round of em .    with probability at least @xmath146",
    "we have [ prop : em0 ]    1 .   for each true template @xmath28 ,",
    "the number of @xmath147 coming from @xmath28 is at least 2 . 2 .",
    "for each true template @xmath28 , the number of @xmath147 coming from @xmath28 is at most @xmath148 3 .",
    "the noise estimate satisfies @xmath149    by initializing from more templates than the actual number of clusters , there is a high probability that the estimated templates cover all the clusters .",
    "suppose @xmath150 and @xmath151 , @xmath113 . in the cases",
    "when the conclusions of proposition [ prop : eps ] hold , for any @xmath115 the ratio between the probabilities @xmath152 and @xmath153 is [ prop : probs ] @xmath154    prop .",
    "[ prop : probs ] states that the first round of em will likely give higher weights to the templates representing the correct cluster than to a wrong cluster .    in the cases",
    "when the conclusions of proposition [ prop : eps ] hold , any non - starved estimate @xmath155 satisfies with probability @xmath156 [ prop : em1 ] @xmath157    so the estimated template of a cluster is very likely to be close to the true template of this cluster .",
    "we prove that with high probability the pruning step will keep exactly one template from each cluster .    in the cases when propositions [ prop : eps ] , [ prop : em0 ] and [ prop : em1 ] hold",
    ", the set @xmath83 obeys the following properties:[prop : pruning ]    1 .",
    "each @xmath83 is non - empty 2",
    ".   there exists @xmath158 such that for any @xmath159 and @xmath160 we have @xmath161 and @xmath162 .",
    "3 .   the pruning procedure finds exactly one member of each @xmath83 .      we permute the obtained templates @xmath62 so that @xmath163 .",
    "suppose @xmath163 and @xmath164 , @xmath113 . in the cases when propositions [ prop : eps ] , [ prop : em0 ] and [ prop : em1 ] hold , for any @xmath115 the ratio between the probabilities @xmath152 and @xmath153 is [ prop : probs2 ] @xmath165    [ thm : em2 ] suppose that @xmath166 , @xmath167 for all @xmath77 and that conditions @xmath168 hold . then with probability at least @xmath169 , the estimated templates",
    "after the round 2 of em satisfy : @xmath170    we are now ready to prove theorem [ thm : main ] .",
    "_ proof of theorem [ thm : main]_.    from @xmath171 , we get @xmath172 .",
    "also @xmath173 but @xmath174 so @xmath175    take @xmath176 ( because of c3 ) . from the dimension condition",
    "@xmath177 we get @xmath178 , so @xmath179    from the dimension condition @xmath180 we get @xmath181 .    from the condition on the number of examples",
    ", we get @xmath182 .    from theorem [ thm :",
    "em2 ] , putting all of the above inequalities together and taking @xmath183 , we obtain that theorem [ thm : main ] holds with probability at least @xmath48 .",
    "this section illustrates the theoretical results obtained in the previous sections by a simulation study as well as experiments on synthetic image sketches and real images",
    ".      in this section we conduct experiments showing that indeed , the true templates are found with high probability when the conditions of theorem [ thm : main ] hold",
    ".    we will work with a mixture of two templates , @xmath185 and @xmath186 where the number of 1 s is @xmath187 , to obtain a desired separation @xmath188 $ ] in dimension @xmath10 .",
    "we experiment with standard em for 2 , 10 and 20 iterations .",
    "the standard em starts from @xmath5 clusters , instead of @xmath45 clusters followed by pruning as in the two - step em .",
    "for the standard em we also assumed the noise level @xmath22 is a known parameter .",
    "all results are obtained from 100 runs .",
    "binary templates correctly @xmath189 of the time when @xmath190 .",
    "the first plot is for @xmath191 , with @xmath192 , and the second plot is for @xmath193 with @xmath194 .",
    "also shown is the domain theoretically guaranteed by theorem [ thm : main ] .",
    "each domain is above and to the right of the corresponding curve .",
    ", title=\"fig : \" ]   binary templates correctly @xmath189 of the time when @xmath190 .",
    "the first plot is for @xmath191 , with @xmath192 , and the second plot is for @xmath193 with @xmath194 .",
    "also shown is the domain theoretically guaranteed by theorem [ thm : main ] .",
    "each domain is above and to the right of the corresponding curve .",
    ", title=\"fig : \" ] -3.mm    fig .",
    "[ fig : plotdomainw ] and [ fig : plotdomain ] show the domains where the two - step em and the standard em find the templates @xmath195 with @xmath189 probability , thus @xmath196 .    in the two plots of fig .",
    "[ fig : plotdomainw ] , the horizontal axis is the minimum weight @xmath197 , and the vertical axis is the separation @xmath41 .",
    "the domain for each algorithm is the region above and to the right of the corresponding curve .",
    "two version of the two step em algorithm were evaluated : the two - step em , and 10-step version that does 9 em steps after the pruning step .",
    "five version of the original em were evaluated , with 2 or 10 iterations , and 1 , 5 or 10 random initializations ( and selecting from the 5 or 10 obtained results the largest likelihood one as the final result ) .",
    "the first plot is obtained at the noise level @xmath191 , while the second plot is for the noise level @xmath193 .",
    "we take the number of observations @xmath190 . for the first plot",
    "the dimension is @xmath198 and for the second plot , @xmath194 .",
    "one can see that for low noise , the two - step em works better than the original em .",
    "also displayed is the domain where the conditions of our theorem are satisfied .",
    "( top ) and @xmath199 ( bottom ) . in these examples @xmath200 .",
    "each domain is above and to the right of the corresponding curve .",
    ", title=\"fig : \" ]   ( top ) and @xmath199 ( bottom ) . in these examples @xmath200 .",
    "each domain is above and to the right of the corresponding curve .",
    ", title=\"fig : \" ]   ( top ) and @xmath199 ( bottom ) . in these examples @xmath200 .",
    "each domain is above and to the right of the corresponding curve .",
    ", title=\"fig : \" ]   ( top ) and @xmath199 ( bottom ) . in these examples @xmath200 .",
    "each domain is above and to the right of the corresponding curve .",
    ", title=\"fig : \" ] -3.mm    in the four plots from fig .",
    "[ fig : plotdomain ] the horizontal axis is the number @xmath7 of observations and the vertical axis is the dimension @xmath10 .",
    "the four plots show the domain where the two - step em algorithm finds the templates @xmath195 with @xmath189 probability for the levels of noise @xmath201 .",
    "the curves corresponding to conditions 2 - 4 of theorem [ thm : main ] and the technical conditions c1-c3 are also displayed , as well as the domain where all conditions of our theorem are satisfied .    from the experiments we observe that the domain where the templates are found with high probability is larger than the domain where the conditions of theorem [ thm : main ] hold .",
    "the largest discrepancy is in the dimensionality conditions , where the gap between theory and experiments is considerable .",
    "this gap could be substantially decreased if tighter bounds could be obtained for prop [ prop : avg1 ] and consequently for prop [ prop : em1 ] and theorem [ thm : em2 ] .",
    "+    -3 mm     and two mixture weights @xmath202 ( left ) and @xmath203 ( right).,title=\"fig : \" ]   and two mixture weights @xmath202 ( left ) and @xmath203 ( right).,title=\"fig : \" ] -3.mm      in this experiment we work with a mixture of two bernoulli templates , shown in the bottom row of fig .",
    "[ fig : dog_examples ] , in a space of dimension @xmath204 . by perturbing the entries with bernoulli noise of level @xmath22 we obtain images such as those shown in the top row of fig .",
    "[ fig : dog_examples ] .    fig .",
    "[ fig : plotdelta_em_and ] shows the success rate of finding the two templates exactly using the two - round em algorithm vs. the number of training examples .",
    "the experiments are run for two levels of noise @xmath205 and two mixture weights @xmath206 .",
    "also shown is the bound @xmath207 from condition 2 of theorem [ thm : main ] .",
    "the separation between the two templates is quite small @xmath208 , because the two templates share a lot of zero components .",
    "so the separation conditions fail in this case .",
    "since we are not in the conditions of the theorem 1 , the bound on the training examples is not expected to hold .",
    "we may achieve a better bound if we reduce the dimension @xmath10 while increasing @xmath41 by selecting those features that differentiate the templates . in any case , we see that in the given scenarios the two templates can be recovered with 100% certainty with the two - round em given sufficiently many examples .",
    "so theorem [ thm : main ] might hold under milder assumptions than ours .",
    "-4 mm    [ cols=\"<,<,<,<,<,<\",options=\"header \" , ]     -1 mm    currently we use a very simple sketch detector by thresholding the gabor filter responses at different orientations .",
    "we will design more sophisticated features and associated detectors in future work .",
    "this paper obtains theoretical guarantees on the performance of a two - round em algorithm for learning mixture of bernoulli templates , by generalizing the theory of @xcite . unlike the theoretical results for supervised learning , results on unsupervised learning such as clustering are relatively scarce .",
    "the results obtained in this paper can be useful for understanding the behavior of em - type algorithms for unsupervised learning .    in our future work",
    ", we shall improve the theoretical results by relaxing the conditions on the separation between the templates as well as the sample size .",
    "we shall also generalize bernoulli templates to more general statistical models for images , such as templates with dependent switching of the binary components , as well as other non - gaussian models such as exponential family models .",
    "the authors wish to acknowledge support from darpa msee grant fa 8650 - 11 - 1 - 7149 and nsf grant dms 1007889 .",
    "the authors thank maria pavlovskaia for her help with the experiment on synthetic sketches .",
    "also thanks to jianwen xie for assistance with the animal face experiments , and to prof .",
    "song - chun zhu for valuable suggestions .",
    "99    s. dasgupta and l.j .",
    "shulman . a two - round variant of em for gaussian mixtures . _ proceedings of 16th conference on uncertainty in artificial intelligence ( uai-2000 ) _ , 152 - 159 , 2000 .",
    "j. g. daugman .",
    "complete discrete 2-d gabor transforms by neural networks for image analysis and compression _ ieee trans . on acoustics , speech and signal processing _",
    ", * 36 * , 1169 - 1179 , 1988 .",
    "a. p. dempster , n. m. laird , and d. b. rubin .",
    "maximum likelihood from incomplete data via the em algorithm ( with discussion ) .",
    "_ journal of the royal statistical society , b _ , * 39 * , 1 - 38 , 1977 .    c. fraley and a. e. raftery .",
    "model - based clustering , discriminant analysis , and density estimation .",
    "_ journal of the american statistical association _ , * 97 * , 611 - 631 , 2002 .",
    "exploratory latent structure analysis using both identifiable and unidentifiable models .",
    "_ biometrika _ , * 61 * , 215231 , 1974 .",
    "x. huo and d. l. donoho .",
    "applications of beamlets to detection and extraction of lines , curves and objects in very noisy images . _ nonlinear signal and image processing _ , 2001 .",
    "g. e. schwarz .",
    "estimating the dimension of a model .",
    "_ annals of statistics _ , * 6 * , 461 - 464 , 1978 .",
    "z si and h gong and sc zhu and yn wu . learning active basis models by em - type algorithms . _ statistical science _",
    ", * 25 * , 458 - 475 , 2010    v. n. vapnik . .",
    "springer , 2000 .",
    "t. tuytelaars , c. h. lampert , m. b. blaschko , and w. buntine .",
    "unsupervised object discovery : a comparison .",
    "_ international journal of computer vision _ , * 88 * , 284 - 302 , 2010    s. c. zhu and d. b. mumford . a stochastic grammar of images . ,",
    "* 2 * , 259362 , 2006 .",
    "_ proof of prop .",
    "[ thm : expbern ] . _    \\1 .",
    "we have @xmath93=e[\\sum_{k=0}^n b_k]=\\sum_{k=0}^n e[b_k]=nq \\end{split}\\ ] ] and @xmath209=e[(\\sum_{k=0}^n b_k)^{2}]=e[\\sum_{i=0}^n b_i^{2}+\\sum_{i\\not = j } b_i b_j]\\\\ & = \\sum_{i=0}^n e[b_i]+\\sum_{i\\not = j } e[b_i b_j]=nq+n(n-1)q^{2}\\\\ var&(d({\\mathbf{x}},{\\mathbf{p}}))=e[d({\\mathbf{x}},{\\mathbf{p}})^{2}]-e[d({\\mathbf{x}},{\\mathbf{p}})]^{2}\\\\ & = n(n-1)q^{2}+nq - n^{2}q^{2}=nq(1-q ) \\end{split}\\ ] ] 2 .",
    "let @xmath210 .",
    "without loss of generality , let @xmath211 where @xmath212 and @xmath213 . observe that if two random variables are independent then @xmath214 .",
    "then @xmath95&=e[d({\\mathbf{u}},{\\mathbf{a}})+d({\\mathbf{z}},1-{\\mathbf{b}})]\\\\&=(n - d)q+(d - e[d({\\mathbf{z}},{\\mathbf{b}})])=(n - d)q+d - dq\\\\ var(d({\\mathbf{x}},{\\mathbf{y}}))&=var[d({\\mathbf{u}},{\\mathbf{a}})+d - d({\\mathbf{z}},{\\mathbf{b}})]\\\\ & = var[d({\\mathbf{u}},{\\mathbf{a}})]+var[d - d({\\mathbf{z}},{\\mathbf{b}})]\\\\ & = ( n - d)q(1-q)+dq(1-q)=nq(1-q ) \\end{split}\\ ] ] 3 . in the case when @xmath96 we have @xmath215=e_{\\mathbf{x}}[e_{\\mathbf{y}}[d({\\mathbf{x}},{\\mathbf{y}})]]=e_{\\mathbf{x}}[nq+d({\\mathbf{x}},{\\mathbf{p}})(1 - 2q)]\\\\ & = nq+nq(1 - 2q)=2nq(1-q)\\\\ var_{{\\mathbf{x}},{\\mathbf{y}}}&(d({\\mathbf{x}},{\\mathbf{y}}))=e_{{\\mathbf{x}},{\\mathbf{y}}}[d({\\mathbf{x}},{\\mathbf{y}})^{2}]-\\hspace{-0.5mm}(e_{{\\mathbf{x}},{\\mathbf{y}}}[d({\\mathbf{x}},{\\mathbf{y}})])^{2}\\\\ & = e_{\\mathbf{x}}(e_{\\mathbf{y}}[d({\\mathbf{x}},{\\mathbf{y}})^{2}])-e_{\\mathbf{x}}(e_{\\mathbf{y}}^{2}[d({\\mathbf{x}},{\\mathbf{y}})])\\\\&+e_{\\mathbf{x}}(e_{\\mathbf{y}}^{2}[d({\\mathbf{x}},{\\mathbf{y}})])-(e_{\\mathbf{x}}[e_{\\mathbf{y}}(d({\\mathbf{x}},{\\mathbf{y}}))])^{2}\\\\ & = e_{\\mathbf{x}}(var_{\\mathbf{y}}[d({\\mathbf{x}},{\\mathbf{y}})])+var_{\\mathbf{x}}[e_{\\mathbf{y}}(d({\\mathbf{x}},{\\mathbf{y}}))]\\\\ & = e_{\\mathbf{x}}(nq(1-q))+var_{\\mathbf{x}}[nq+d({\\mathbf{x}},{\\mathbf{p}})(1 - 2q)]\\\\ & = nq(1-q)+nq(1-q)(1 - 2q)^{2 } \\end{split}\\ ] ]    \\4 . in the case when @xmath102 we have @xmath216=e_{\\mathbf{x}}[e_{\\mathbf{y}}[d({\\mathbf{x}},{\\mathbf{y } } ) ] ] = e_{\\mathbf{x}}[nq+d({\\mathbf{x}},{\\mathbf{q}})(1 - 2q)]\\\\ & = nq+(nq+d({\\mathbf{p}},{\\mathbf{q}})(1 - 2q))(1 - 2q)\\\\ & = 2nq(1-q)+d({\\mathbf{p}},{\\mathbf{q}})(1 - 2q)^{2}\\\\ var_{{\\mathbf{x}},{\\mathbf{y}}}&(d({\\mathbf{x}},{\\mathbf{y}}))=e_{\\mathbf{x}}(var_{\\mathbf{y}}[d({\\mathbf{x}},{\\mathbf{y}})])+var_{\\mathbf{x}}[e_{\\mathbf{y}}(d({\\mathbf{x}},{\\mathbf{y}}))]\\\\ & = e_{\\mathbf{x}}(nq(1-q))+var_{\\mathbf{x}}[nq+d({\\mathbf{x}},{\\mathbf{q}})(1 - 2q)]\\\\&=nq(1-q)+nq(1-q)(1 - 2q)^{2}. \\box \\end{split}\\ ] ]      \\c ) let @xmath217 be indices of the @xmath218 common elements of @xmath6 and @xmath35 .",
    "let @xmath219 be the bernoulli event that the @xmath77-th element of @xmath16 and @xmath220 are different .",
    "then @xmath221 if @xmath222 and @xmath223 if @xmath224",
    ". observe that @xmath225 .",
    "thus by the chernoff inequality , since @xmath226=2nq(1-q)+d(1 - 2q)^{2}$ ] we get @xmath227    _ proof of prop .",
    "[ prop : eps ] . _",
    "a ) from point c ) of prop .",
    "[ thm : devbern ] with @xmath228 , we have @xmath229",
    "so for any two points @xmath110 we have @xmath230 .",
    "thus for all @xmath231 combinations of two points we have @xmath232 b ) similar to the proof of a ) , with @xmath233 .",
    "we obtain @xmath234 c ) from point b ) of prop .",
    "[ thm : devbern ] we have @xmath235 so for all @xmath7 points we have @xmath236          _ proof of prop .",
    "[ prop : avg1 ] .",
    "_ first , it is sufficient to prove it for subsets of size exactly @xmath128 , otherwise we increase @xmath128 . without loss of generality",
    ", we can assume @xmath248 .",
    "from lemma [ lem : abszq ] we have @xmath249 the number of @xmath128-point subsets of @xmath124 is @xmath250 , thus @xmath251 solving for @xmath252 we get @xmath253 therefore @xmath254 } \\leq \\delta . \\;\\box \\end{split}\\ ] ]      _ proof of prop . [ prop : em0 ] .",
    "_ let @xmath219 be the bernoulli event that a random sample from the mixture comes from the @xmath77-th true template @xmath28",
    ". then @xmath261=w_i$ ] .",
    "having @xmath45 random samples @xmath121 from the bernoulli event @xmath219 , then @xmath262 so @xmath263 .",
    "thus @xmath264 , so @xmath265 .",
    "\\3 . as there exist @xmath267 representing the same cluster , then @xmath268 ( from prop .",
    "[ prop : eps ] , a ) . also from prop .",
    "[ prop : eps ] , if the minimum is attained for two centers @xmath267 representing the same cluster , we are done .",
    "otherwise @xmath269 so both parts of the inequality are proved .",
    "@xmath184    _ proof of prop [ prop : probs ] .",
    "_ we have @xmath270 with @xmath271 . but from prop .",
    "[ prop : eps ] @xmath272 since we have the following condition @xmath273 obtained from @xmath140 .",
    "we also have since @xmath274 @xmath275 so @xmath276    _ proof of prop .",
    "[ prop : em1 ] .",
    "_ without loss of generality we can assume @xmath277 .",
    "@xmath278 from prop [ prop : probs ] , for any @xmath279 we have @xmath280",
    ". then @xmath281 from @xmath282 and conditions @xmath283 ( c2 ) and @xmath284 ( c1 ) .    from prop .",
    "[ prop : wtavg ] there exists @xmath285 with @xmath286 such that @xmath287 . from prop [ prop : avg1 ] , with probability @xmath156 @xmath288 then since @xmath289 we have @xmath290from condition @xmath291 ( c2 ) and @xmath292 ( which holds for @xmath293 ) .    for the second term , from prop [ prop : eps ] we have , for @xmath294 @xmath295 where since @xmath296 we have @xmath297 so @xmath298 using condition @xmath299 ( c1 ) .",
    "putting together and we get the result . @xmath184      let @xmath302 be such that @xmath303 and @xmath115 . for any @xmath240 such that @xmath304 we have from prop [ prop : probs ] @xmath305 .",
    "then @xmath306 and thus @xmath307 .",
    "but then @xmath308 but @xmath309 so there is a @xmath310 such that @xmath311 using condition @xmath284 ( c1 ) , thus @xmath83 is not empty .",
    "\\b ) pick any @xmath155 and @xmath312 for @xmath313 . then from proposition",
    "[ prop : em1 ] we have @xmath314 while using proposition [ prop : em1 ] and the triangle inequality we get @xmath315 from condition @xmath316 ( c3 ) , so we can take @xmath317 .",
    "\\c ) there are @xmath5 true clusters , exactly as many as selected templates .",
    "if two selected templates were from the same cluster , there should be a cluster that has no selected templates .",
    "but the two templates from the same cluster are at distance at most @xmath318 while the distance of a template from the unselected cluster has distance more than @xmath318 , we get a contradiction .",
    "@xmath184    _",
    "proof of prop .",
    "[ prop : probs2 ] .",
    "_ using the triangle inequality , prop .",
    "[ prop : eps ] and prop .",
    "[ prop : em1 ] we have @xmath319 and @xmath320 so @xmath321 where @xmath271 , and therefore @xmath322\\ln a)\\\\ & = \\exp(n[c_{ij}(1 - 2q)(1-\\epsilon_0)-2q-\\epsilon_0(q+\\sqrt{q } )   -4\\sqrt{\\frac{6ql}{n } }   ] \\ln a ) \\\\&\\geq   \\exp(nc_{ij}\\frac{1}{4}(1 - 2q)\\ln \\frac{1}{6\\sqrt{q } } ) \\end{split}\\ ] ] using the condition @xmath323 obtained from @xmath324 .",
    "@xmath184      proposition [ prop : eps ] holds with probability at least @xmath325@xmath326 .",
    "proposition [ prop : em0 ] holds with probability at least @xmath146 .",
    "proposition [ prop : em1 ] holds with probability at least @xmath156 for each of the @xmath5 clusters .",
    "all other propositions hold if these three propositions hold ."
  ],
  "abstract_text": [
    "<S> dasgupta and shulman @xcite showed that a two - round variant of the em algorithm can learn mixture of gaussian distributions with near optimal precision with high probability if the gaussian distributions are well separated and if the dimension is sufficiently high . in this paper , we generalize their theory to learning mixture of high - dimensional bernoulli templates . </S>",
    "<S> each template is a binary vector , and a template generates examples by randomly switching its binary components independently with a certain probability . in computer vision applications , </S>",
    "<S> a binary vector is a feature map of an image , where each binary component indicates whether a local feature or structure is present or absent within a certain cell of the image domain . </S>",
    "<S> a bernoulli template can be considered as a statistical model for images of objects ( or parts of objects ) from the same category . </S>",
    "<S> we show that the two - round em algorithm can learn mixture of bernoulli templates with near optimal precision with high probability , if the bernoulli templates are sufficiently different and if the number of features is sufficiently high . </S>",
    "<S> we illustrate the theoretical results by synthetic and real examples . </S>"
  ]
}