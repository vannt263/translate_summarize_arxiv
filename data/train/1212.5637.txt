{
  "article_text": [
    "a widespread approach to the solution of classification problems is representing datasets through a weighted graph where nodes are the data items and edge weights quantify the similarity between pairs of data items .",
    "this technique for coding input data has been applied to several domains , including web spam detection  @xcite , classification of genomic data  @xcite , face recognition  @xcite , and text categorization  @xcite . in many applications ,",
    "edge weights are computed through a complex data - modelling process and typically convey information that is relevant to the task of classifying the nodes .    in the sequential version of this problem ,",
    "nodes are presented in an arbitrary ( possibly adversarial ) order , and the learner must predict the binary label of each node before observing its true value . since real - world applications typically involve large datasets ( i.e. , large graphs ) , online learning methods play an important role because of their good scaling properties .",
    "an interesting special case of the online problem is the so - called transductive setting , where the entire graph structure ( including edge weights ) is known in advance .",
    "the transductive setting is interesting in that the learner has the chance of reconfiguring the graph before learning starts , so as to make the problem look easier .",
    "this data preprocessing can be viewed as a kind of regularization in the context of graph prediction .",
    "when the graph is unweighted ( i.e. , when all edges have the same common weight ) , it was found in previous works  @xcite that a key parameter to control the number of online prediction mistakes is the size of the cut induced by the unknown adversarial labeling of the nodes , i.e. , the number of edges in the graph whose endpoints are assigned disagreeing labels . however , while the number of mistakes is obviously bounded by the number of nodes , the cutsize scales with the number of edges .",
    "this naturally led to the idea of solving the prediction problem on a spanning tree of the graph  @xcite , whose number of edges is exactly equal to the number of nodes minus one .",
    "now , since the cutsize of the spanning tree is smaller than that of the original graph , the number of mistakes in predicting the nodes is more tightly controlled . in light of the previous discussion",
    ", we can also view the spanning tree as a `` maximally regularized '' version of the original graph .",
    "since a graph has up to exponentially many spanning trees , which one should be used to maximize the predictive performance ?",
    "this question can be answered by recalling the adversarial nature of the online setting , where the presentation of nodes and the assignment of labels to them are both arbitrary .",
    "this suggests to pick a tree at random among all spanning trees of the graph so as to prevent the adversary from concentrating the cutsize on the chosen tree  @xcite .",
    "kirchoff s equivalence between the effective resistance of an edge and its probability of being included in a random spanning tree allows to express the expected cutsize of a random spanning tree in a simple form .",
    "namely , as the sum of resistances over all edges in the cut of @xmath0 induced by the adversarial label assignment .",
    "although the results of @xcite yield a mistake bound for arbitrary unweighted graphs in terms of the cutsize of a random spanning tree , no general lower bounds are known for online unweighted graph prediction .",
    "the scenario gets even more uncertain in the case of weighted graphs , where the only previous papers we are aware of @xcite essentially contain only upper bounds . in this paper",
    "we fill this gap , and show that the expected cutsize of a random spanning tree of the graph delivers a convenient parametrization . ] that captures the hardness of the graph learning problem in the general weighted case .",
    "given any weighted graph , we prove that any online prediction algorithm must err on a number of nodes which is at least as big as the expected cutsize of the graph s random spanning tree ( which is defined in terms of the graph weights ) .",
    "moreover , we exhibit a simple randomized algorithm achieving in expectation the optimal mistake bound to within logarithmic factors .",
    "this bound applies to any sufficiently connected weighted graph whose weighted cutsize is not an overwhelming fraction of the total weight .",
    "following the ideas of  @xcite , our algorithm first extracts a random spanning tree of the original graph .",
    "then , it predicts all nodes of this tree using a generalization of the method proposed by @xcite .",
    "our tree prediction procedure is extremely efficient : it only requires _ constant _ amortized time per prediction and space _ linear in the number of nodes_. again , we would like to stress that computational efficiency is a central issue in practical applications where the involved datasets can be very large .",
    "in such contexts , learning algorithms whose computation time scales quadratically , or slower , in the number of data points should be considered impractical . as in  @xcite ,",
    "our algorithm first linearizes the tree , and then operates on the resulting line graph via a nearest neighbor rule .",
    "we show that , besides running time , this linearization step brings further benefits to the overall prediction process .",
    "in particular , similar to ( * ? ? ?",
    "* theorem  4.2 ) , the algorithm turns out to be resilient to perturbations of the labeling , a clearly desirable feature from a practical standpoint .    in order to provide convincing empirical evidence",
    ", we also present an experimental evaluation of our method compared to other algorithms recently proposed in the literature on graph prediction .",
    "in particular , we test our algorithm against the perceptron algorithm with laplacian kernel by @xcite , and against a version of the label propagation algorithm by @xcite .",
    "these two baselines can viewed as representatives of global ( perceptron ) and local ( label propagation ) learning methods on graphs .",
    "the experiments have been carried out on five medium - sized real - world datasets .",
    "the two tree - based algorithms ( ours and the perceptron algorithm ) have been tested using spanning trees generated in various ways , including committees of spanning trees aggregated by majority votes . in a nutshell",
    ", our experimental comparison shows that predictors based on our online algorithm compare well to all baselines while being very efficient in most cases .",
    "the paper is organized as follows .",
    "next , we recall preliminaries and introduce our basic notation . section  [ s : rel ] surveys related work in the literature . in section  [ s : lower ] we prove the general lower bound relating the mistakes of any prediction algorithm to the expected cutsize of a random spanning tree of the weighted graph . in the subsequent section , we present our prediction algorithm wta  ( weighted tree algorithm ) , along with a detailed mistake bound analysis restricted to weighted trees .",
    "this analysis is extended to weighted graphs in section  [ s : gen ] , where we provide an upper bound matching the lower bound up to log factors on any sufficiently connected graph . in section  [",
    "s : robust ] , we quantify the robustness of our algorithm to label perturbation . in section  [ s :",
    "compl ] , we provide the constant amortized time implementation of wta . based on this implementation , in section  [ s : exp ] we present the experimental results .",
    "section  [ s : concl ] is devoted to conclusive remarks .",
    "let @xmath1 be an undirected , connected , and weighted graph with @xmath2 nodes and positive edge weights @xmath3 for @xmath4 .",
    "a labeling of @xmath0 is any assignment @xmath5 of binary labels to its nodes .",
    "we use @xmath6 to denote the resulting labeled weighted graph .",
    "the online learning protocol for predicting @xmath6 can be defined as the following game between a ( possibly randomized ) learner and an adversary .",
    "the game is parameterized by the graph @xmath1 .",
    "preliminarly , and hidden to the learner , the adversary chooses a labeling @xmath7 of @xmath0 .",
    "then the nodes of @xmath0 are presented to the learner one by one , according to a permutation of @xmath8 , which is adaptively selected by the adversary .",
    "more precisely , at each time step @xmath9 the adversary chooses the next node @xmath10 in the permutation of @xmath8 , and presents it to the learner for the prediction of the associated label @xmath11",
    ". then @xmath11 is revealed , disclosing whether a mistake occurred .",
    "the learner s goal is to minimize the total number of prediction mistakes .",
    "note that while the adversarial choice of the permutation can depend on the algorithm s randomization , the choice of the labeling is oblivious to it .",
    "in other words , the learner uses randomization to fend off the adversarial choice of labels , whereas it is fully deterministic against the adversarial choice of the permutation .",
    "the requirement that the adversary is fully oblivious when choosing labels is then dictated by the fact that the randomized learners considered in this paper make all their random choices at the beginning of the prediction process ( i.e. , before seeing the labels ) .",
    "now , it is reasonable to expect that prediction performance degrades with the increase of `` randomness '' in the labeling . for this reason , our analysis of graph prediction algorithms bounds from above the number of prediction mistakes in terms of appropriate notions of graph label _ regularity_. a standard notion of label regularity is the cutsize of a labeled graph , defined as follows .",
    "a @xmath12-edge of a labeled graph @xmath6 is any edge @xmath13 such that @xmath14 .",
    "similarly , an edge @xmath13 is @xmath12-free if @xmath15 .",
    "let @xmath16 be the set of @xmath12-edges in @xmath6 .",
    "the quantity @xmath17 is the _ cutsize _ of @xmath6 , i.e. , the number of @xmath12-edges in @xmath18 ( independent of the edge weights ) .",
    "the _ weighted cutsize _ of @xmath6 is defined by @xmath19 for a fixed @xmath6 , we denote by @xmath20 the effective resistance between nodes @xmath21 and @xmath22 of @xmath0 . in the interpretation of the graph as an electric network , where the weights @xmath23 are the edge conductances , the effective resistance @xmath20 is the voltage between @xmath21 and @xmath22 when a unit current flow is maintained through them . for @xmath4 ,",
    "let also @xmath24 be the probability that @xmath13 belongs to a random spanning tree @xmath25 see , e.g. , the monograph of  @xcite .",
    "then we have @xmath26 where the expectation @xmath27 is over the random choice of spanning tree @xmath25 .",
    "observe the natural weight - scale independence properties of ( [ e : expected ] ) .",
    "a uniform rescaling of the edge weights @xmath23 can not have an influence on the probabilities @xmath28 , thereby making each product @xmath29 scale independent .",
    "in addition , since @xmath30 is equal to @xmath31 , irrespective of the edge weighting , we have @xmath32 .",
    "hence the ratio @xmath33 $ ] provides a _ density - independent _ measure of the cutsize in @xmath0 , and even allows to compare labelings on different graphs .",
    "now contrast @xmath34 to the more standard weighted cutsize measure @xmath35 .",
    "first , @xmath35 is clearly weight - scale dependent .",
    "second , it can be much larger than @xmath2 on dense graphs , even in the unweighted @xmath36 case .",
    "third , it strongly depends on the density of @xmath0 , which is generally related to @xmath37 .",
    "in fact , @xmath34 can be much smaller than @xmath35 when there are strongly connected regions in @xmath0 contributing prominently to the weighted cutsize . to see this ,",
    "consider the following scenario : if @xmath38 and @xmath23 is large , then @xmath13 gives a big contribution to @xmath35 .",
    "can be much larger than @xmath2 .",
    "] however , this does not necessarily happen with @xmath39 .",
    "in fact , if @xmath21 and @xmath22 are strongly connected ( i.e. , if there are many disjoint paths connecting them ) , then @xmath20 is very small and so are the terms @xmath29 in  ( [ e : expected ] ) .",
    "therefore , the effect of the large weight @xmath23 may often be compensated by the small probability of including @xmath13 in the random spanning tree .",
    "see figure [ f:1 ] for an example .",
    "a different way of taking into account graph connectivity is provided by the covering ball approach taken by @xcite see the next section .     a barbell graph .",
    "the weight of the two thick black edges is equal to @xmath40 , all the other edges have unit weight . if the two labels @xmath41 and @xmath42 are such that @xmath43 , then the contribution of the edges on the left clique @xmath44 to the cutsizes @xmath45 and @xmath46 must be large .",
    "however , since the probability of including each edge of @xmath44 in a random spanning tree @xmath25 is @xmath47 , @xmath44 s contribution to @xmath34 is @xmath48 times smaller than @xmath49 .",
    "if @xmath50 , then the contribution of edge ( 3,4 ) to @xmath46 is large . because this edge is a bridge , the probability of including it in @xmath25 is one , independent of @xmath51 .",
    "indeed , we have @xmath52 . if @xmath53 , then the contribution of the right clique @xmath54 to @xmath46 is large . on the other hand ,",
    "the probability of including edge @xmath55 in @xmath25 is equal to @xmath56 .",
    "hence , the contribution of @xmath55 to @xmath34 is small because the large weight of @xmath55 is offset by the fact that nodes @xmath57 and @xmath58 are strongly connected ( i.e. , there are many different paths among them ) . finally , note that @xmath59 holds for all edges @xmath13 in @xmath54 , implying ( similar to clique @xmath44 ) that @xmath54 s contribution to @xmath34 is @xmath48 times smaller than @xmath60 .",
    ", title=\"fig : \" ]",
    "with the above notation and preliminaries in hand , we now briefly survey the results in the existing literature which are most closely related to this paper .",
    "further comments are made at the end of section  [ s : gen ] .",
    "standard online linear learners , such as the perceptron algorithm , are applied to the general ( weighted ) graph prediction problem by embedding the @xmath2 vertices of the graph in @xmath61 through a map @xmath62 , where @xmath63 is the @xmath21-th vector in the canonical basis of @xmath61 , and @xmath64 is a positive definite @xmath65 matrix . the graph perceptron algorithm  @xcite uses @xmath66 , where @xmath67 is the ( weighted ) laplacian of @xmath0 and @xmath68 .",
    "the resulting mistake bound is of the form @xmath69 , where @xmath70 is the resistance diameter of @xmath0 .",
    "as expected , this bound is weight - scale independent , but the interplay between the two factors in it may lead to a vacuous result . at a given scale for the weights @xmath23 ,",
    "if @xmath0 is dense , then we may have @xmath71 while @xmath46 is of the order of @xmath72 .",
    "if @xmath0 is sparse , then @xmath73 but then @xmath74 may become as large as @xmath2 .",
    "the idea of using a spanning tree to reduce the cutsize of @xmath0 has been investigated by  @xcite , where the graph perceptron algorithm is applied to a spanning tree @xmath25 of @xmath0 .",
    "the resulting mistake bound is of the form @xmath75 , i.e. , the graph perceptron bound applied to tree @xmath25 . since @xmath76 this bound has a smaller cutsize than the previous one . on the other hand",
    ", @xmath77 can be much larger than @xmath74 because removing edges may increase the resistance .",
    "hence the two bounds are generally incomparable .",
    "@xcite suggest to apply the graph perceptron algorithm to the spanning tree @xmath25 with smallest geodesic diameter .",
    "the geodesic diameter of a weighted graph @xmath0 is defined by @xmath78 where the minimum is over all paths @xmath79 between @xmath21 and @xmath22 .",
    "the reason behind this choice of @xmath25 is that , for the spanning tree @xmath25 with smallest geodesic diameter , it holds that @xmath80 .",
    "however , one the one hand @xmath81 , so there is no guarantee that @xmath82 , and on the other hand the adversary may still concentrate all @xmath12-edges on the chosen tree @xmath25 , so there is no guarantee that @xmath83 remains small either .",
    "@xcite introduce a different technique showing its application to the case of unweighted graphs . after reducing the graph to a spanning tree @xmath25 ,",
    "the tree is linearized via a depth - first visit .",
    "this gives a line graph @xmath84 ( the so - called _ spine _ of @xmath0 ) such that @xmath85 . by running a nearest neighbor ( nn ) predictor on @xmath84 , @xcite",
    "prove a mistake bound of the form @xmath86 .",
    "as observed by  @xcite , similar techniques have been developed to solve low - congestion routing problems .",
    "another natural parametrization for the labels of a weighted graph that takes the graph structure into account is _ clusterability _ , i.e. , the extent to which the graph nodes can be covered by a few balls of small resistance diameter . with this inductive bias in mind",
    ", @xcite developed the pounce algorithm , which can be seen as a combination of graph perceptron and nn prediction .",
    "the number of mistakes has a bound of the form @xmath87 where @xmath88 is the smallest number of balls of resistance diameter @xmath89 it takes to cover the nodes of @xmath0 .",
    "note that the graph perceptron bound is recovered when @xmath90 .",
    "moreover , observe that , unlike graph perceptron s , bound ( [ e : pounce ] ) is never vacuous , as it holds uniformly for all covers of @xmath0 ( even the one made up of singletons , corresponding to @xmath91 ) .",
    "a further trick for the unweighted case proposed by  @xcite is to take advantage of both previous approaches ( graph perceptron and nn on line graphs ) by building a binary tree on @xmath0 .",
    "this `` support tree '' helps in keeping the diameter of @xmath0 as small as possible , e.g. , logarithmic in the number of nodes @xmath2 .",
    "the resulting prediction algorithm is again a combination of a perceptron - like algorithm and nn , and the corresponding number of mistakes is the minimum over two earlier bounds : a nn - based bound of the form @xmath92 and an unweighted version of bound ( [ e : pounce ] ) .    generally speaking , clusterability and resistance - weighted cutsize @xmath34",
    "exploit the graph structure in different ways .",
    "consider , for instance , a barbell graph made up of two @xmath93-cliques joined by @xmath94 unweighted @xmath12-edges with no endpoints in common ( hence @xmath95 ) .",
    "if @xmath93 is much larger than @xmath94 , then bound ( [ e : pounce ] ) scales linearly with @xmath94 ( the two balls in the cover correspond to the two @xmath93-cliques ) . on the other hand",
    ", @xmath34 tends to be constant : because @xmath93 is much larger than @xmath94 , the probability of including any @xmath12-edge in @xmath25 tends to @xmath96 , as @xmath93 increases and @xmath94 stays constant . on the other hand ,",
    "if @xmath94 gets close to @xmath93 the resistance diameter of the graph decreases , and ( [ e : pounce ] ) becomes a constant .",
    "in fact , one can show that when @xmath97 even @xmath34 is a constant , independent of @xmath93 .",
    "in particular , the probability that a @xmath12-edge is included in the random spanning tree @xmath25 is upper bounded by @xmath98 , i.e. , @xmath99 when @xmath93 grows large .- edge @xmath13 as the minimum , over all unit - strength flow functions with @xmath21 as source and @xmath22 as sink , of the squared flow values summed over all edges , see , e.g. , @xcite . ]    when the graph at hand has a large diameter , e.g. , an @xmath93-line graph connected to an @xmath93-clique ( this is sometimes called a  lollipop \" graph ) the gap between the covering - based bound ( [ e : pounce ] ) and @xmath34 is magnified . yet , it is fair to say that the bounds we are about to prove for our algorithm have an extra factor , beyond @xmath34 , which is logarithmic in @xmath93 .",
    "a similar logarithmic factor is achieved by the combined algorithm proposed in  @xcite .    an even more refined way of exploiting cluster structure and connectivity in graphs is contained in the paper of @xcite , where the authors provide a comprehensive study of the application of dual - norm techniques to the prediction of weighted graphs , again with the goal of obtaining logarithmic performance guarantees on large diameter graphs . in order to trade - off the contribution of cutsize @xmath100 and resistance diameter @xmath74 ,",
    "the authors develop a notion of @xmath101-norm resistance .",
    "the obtained bounds are dual norm versions of the covering ball bound ( [ e : pounce ] ) .",
    "roughly speaking , one can select the dual norm parameter of the algorithm to obtain a logarithmic contribution from the resistance diameter at the cost of squaring the contribution due to the cutsize .",
    "this quadratic term can be further reduced if the graph is well connected .",
    "for instance , in the unweighted barbell graph mentioned above , selecting the norm appropriately leads to a bound which is constant even when @xmath102 .",
    "further comments on the comparison between the results presented by @xcite and the ones in our paper are postponed to the end of section [ s : gen ] .",
    "departing from the online learning scenario , it is worth mentioning the significantly large literature on the general problem of learning the nodes of a graph in the train / test transductive setting : many algorithms have been proposed , including the label - consistent mincut approach of @xcite and a number of other `` energy minimization '' methods e.g . , the ones by  @xcite of which label propagation is an instance .",
    "see the work of  @xcite for a relatively recent survey on this subject .",
    "our graph prediction algorithm is based on a random spanning tree of the original graph .",
    "the problem of drawing a random spanning tree of an arbitrary graph has a long history see , e.g. , the recent monograph by  @xcite . in the unweighted case , a random spanning tree can be sampled with a random walk in expected time @xmath103 for `` most '' graphs , as shown by @xcite . using the beautiful algorithm of @xcite , the expected time reduces to @xmath104 see also the work of  @xcite .",
    "however , all known techniques take expected time @xmath105 on certain pathological graphs . in the weighted case",
    ", the above methods can take longer due to the hardness of reaching , via a random walk , portions of the graph which are connected only via light - weighted edges . to sidestep this issue , in our experiments we tested a viable fast approximation where",
    "weights are disregarded when building the spanning tree , and only used at prediction time . finally",
    ", the space complexity for generating a random spanning tree is always linear in the graph size .    to conclude this section",
    ", it is worth mentioning that , although we exploit random spanning trees to reduce the cutsize , similar approaches can also be used to approximate the cutsize of a weighted graph by sparsification see , e.g. , the work of  @xcite .",
    "however , because the resulting graphs are not as sparse as spanning trees , we do not currently see how to use those results .",
    "this section contains our general lower bound .",
    "we show that any prediction algorithm must err at least @xmath106 times on any weighted graph .",
    "[ th : lower ] let @xmath1 be a weighted undirected graph with @xmath2 nodes and weights @xmath3 for @xmath4 .",
    "then for all @xmath107 there exists a randomized labeling @xmath7 of @xmath0 such that for all ( deterministic or randomized ) algorithms @xmath108 , the expected number of prediction mistakes made by @xmath108 is at least @xmath109 , while @xmath110 .",
    "the adversarial strategy . numbers on edges are the probabilities @xmath28 of those edges being included in a random spanning tree for the weighted graph under consideration .",
    "numbers within nodes denote the weight of that node based on the @xmath28 see main text .",
    "we set the budget @xmath64 to 6 , hence the subset @xmath84 contains the 6 nodes having smallest weight .",
    "the adversary assigns a random label to each node in @xmath84 thus forcing @xmath111 mistakes in expectation .",
    "then , it labels all nodes in @xmath112 with a unique label , chosen in such a way as to minimize the cutsize consistent with the labels previously assigned to the nodes of @xmath84 .",
    ", title=\"fig : \" ]    the adversary uses the weighting @xmath113 induced by @xmath114 and defined by @xmath24 . by  ( 1 ) , @xmath28 is the probability that edge @xmath13 belongs to a random spanning tree @xmath25 of @xmath0 .",
    "let @xmath115 be the sum over the induced weights of all edges incident to node @xmath21 .",
    "we call @xmath116 the _ weight _ of node @xmath21 .",
    "let @xmath117 be the set of @xmath64 nodes @xmath21 in @xmath0 having the smallest weight @xmath116 .",
    "the adversary assigns a random label to each node @xmath118 .",
    "this guarantees that , no matter what , the algorithm @xmath108 will make on average @xmath109 mistakes on the nodes in @xmath84 .",
    "the labels of the remaining nodes in @xmath112 are set either all @xmath119 or all @xmath120 , depending on which one of the two choices yields the smaller @xmath121",
    ". see figure [ f:2 ] for an illustrative example .",
    "we now show that the weighted cutsize @xmath121 of this labeling @xmath7 is less than @xmath64 , _ independent of _ the labels of the nodes in @xmath84 .",
    "since the nodes in @xmath112 have all the same label , the @xmath12-edges induced by this labeling can only connect either two nodes in @xmath84 or one node in @xmath84 and one node in @xmath112 .",
    "hence @xmath122 can be written as @xmath123 where @xmath124 is the cutsize contribution within @xmath84 , and @xmath125 is the one from edges between @xmath84 and @xmath112 .",
    "we can now bound these two terms by combining the definition of @xmath84 with the equality @xmath126 as in the sequel .",
    "let @xmath127 from the very definition of @xmath128 and @xmath129 we have @xmath130 .",
    "moreover , from the way the labels of nodes in @xmath112 are selected , it follows that @xmath131 .",
    "finally , @xmath132 holds , since each edge connecting nodes in @xmath84 is counted twice in the sum @xmath133 . putting everything together we obtain @xmath134 the inequality following from the definition of @xmath84 . hence @xmath135 concluding the proof .",
    "we now describe the weighted tree algorithm ( @xmath136 ) for predicting the labels of a weighted tree . in section  [ s : gen ]",
    "we show how to apply wta  to the more general weighted graph prediction problem .",
    "wta  first transforms the tree into a line graph ( i.e. , a list ) , then runs a fast nearest neighbor method to predict the labels of each node in the line . though this technique is similar to that one used by  @xcite , the fact that the tree is weighted makes the analysis significantly more difficult , and the practical scope of our algorithm significantly wider .",
    "our experimental comparison in section [ s : exp ] confirms that exploiting the weight information is often beneficial in real - world graph prediction problem .     *",
    "top : * a weighted graph @xmath0 with 9 nodes .",
    "initially , wta  extracts a random spanning tree @xmath25 out of @xmath0 .",
    "the weights on the edges in @xmath25 are the same as those of @xmath0 . *",
    "middle : * the spanning tree @xmath25 is linearized through a depth - first traversal starting from an arbitrary node ( node 2 in this figure ) . for simplicity",
    ", we assume the traversal visits the siblings from left to right .",
    "as soon as a node is visited it gets stored in a line graph @xmath137 ( first line graph from top ) .",
    "backtracking steps produce duplicates in @xmath137 of some of the nodes in @xmath25 .",
    "for instance , node 7 is the first node to be duplicated when the visit backtracks from node 8 .",
    "the duplicated nodes are progressively eliminated from @xmath137 in the order of their insertion in @xmath137 .",
    "several iterations of this node elimination process are displayed from the top to the bottom , showing how @xmath137 is progressively shrunk to the final line @xmath138 ( bottom line ) .",
    "each line represents the elimination of a single duplicated node .",
    "the crossed nodes in each line are the nodes which are scheduled to be eliminated . each time a new node @xmath22 is eliminated ,",
    "its two adjacent nodes @xmath21 and @xmath94 are connected by the lighter of the two edges @xmath13 and @xmath139 .",
    "for instance : the left - most duplicated 7 is dropped by directly connecting the two adjacent nodes 8 and 1 by an edge with weight @xmath140 ; the right - most node 2 is eliminated by directly connecting node 6 to node 9 with an edge with weight @xmath140 , and so on .",
    "observe that this elimination procedure can be carried out _ in any order _ without changing the resulting list @xmath138 . *",
    "bottom : * we show wta s prediction on the line @xmath138 so obtained . in this figure ,",
    "the numbers above the edges denote the edge weights , the ones below are the resistors , i.e. , weight reciprocals .",
    "we are at time step @xmath141 where two labels have so far been revealed ( gray nodes ) .",
    "wta  predicts on the remaining nodes according to a nearest neighbor rule on @xmath138 , based on the resistance distance metric .",
    "all possible predictions made by wta  at this time step are shown .",
    ", title=\"fig : \" ]    given a labeled weighted tree @xmath142 , the algorithm initially creates a weighted line graph @xmath137 containing some duplicates of the nodes in @xmath25 .",
    "then , each duplicate node ( together with its incident edges ) is replaced by a single edge with a suitably chosen weight .",
    "this results in the final weighted line graph @xmath138 which is then used for prediction . in order to create @xmath138 from @xmath25 , @xmath136 performs the following _ tree linearization _ steps :    1 .",
    "an arbitrary node @xmath143 of @xmath25 is chosen , and a line @xmath137 containing only @xmath143 is created .",
    "2 .   starting from @xmath143 , a depth - first visit of @xmath25 is performed .",
    "each time an edge @xmath13 is traversed ( even in a backtracking step ) from @xmath21 to @xmath22 , the edge is appended to @xmath137 with its weight @xmath23 , and @xmath22 becomes the current terminal node of @xmath137 .",
    "note that backtracking steps can create in @xmath137 at most one duplicate of each edge in @xmath25 , while nodes in @xmath25 may be duplicated several times in @xmath137 .",
    "@xmath137 is traversed once , starting from terminal @xmath143 . during this traversal ,",
    "duplicate nodes are eliminated as soon as they are encountered .",
    "this works as follows .",
    "let @xmath22 be a duplicate node , and @xmath144 and @xmath145 be the two incident edges .",
    "the two edges are replaced by a new edge @xmath146 having weight @xmath147 . by the lightest edge among the eliminated ones in @xmath137 .",
    "] let @xmath138 be the resulting line .",
    "the analysis of section  [ ss : analysis ] shows that this choice of @xmath148 guarantees that the weighted cutsize of @xmath138 is smaller than twice the weighted cutsize of @xmath25 . once @xmath138 is created from @xmath25 , the algorithm predicts the label of each node @xmath10 using a nearest - neighbor rule operating on @xmath138 with a _ resistance distance _ metric",
    "that is , the prediction on @xmath10 is the label of @xmath149 , being @xmath150 the previously revealed node closest to @xmath10 , and @xmath151 is the sum of the resistors ( i.e. , reciprocals of edge weights ) along the unique path @xmath152 connecting node @xmath21 to node @xmath22 .",
    "figure [ f:3 ] gives an example of wta  at work .",
    "the following lemma gives a mistake bound on wta  run on any weighted line graph .",
    "given any labeled graph @xmath6 , we denote by @xmath153 the sum of resistors of @xmath12-free edges in @xmath0 , @xmath154 also , given any @xmath12-free edge subset @xmath155 , we define @xmath156 as the sum of the resistors of all @xmath12-free edges in @xmath157 , @xmath158 note that @xmath159 , since we drop some edges from the sum in the defining formula .",
    "finally , we use @xmath160 as shorthand for @xmath161 .",
    "the following lemma is the starting point of our theoretical investigation please see appendix  a for proofs .",
    "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * the mistake bound in theorem  [ ub_tree ] can be large ( or loose ) in two very particular cases : ( i ) the average weight of a @xmath162-edge or ( ii ) the average resistance diameter of a cluster are extremely large ( e.g. exponential o superexponential in @xmath2 , since they are both part of a logarithm argument ) .",
    "the first problem can be solved simply with the following observation : multiplying all edge weights by the same positive constant @xmath163 , the algorithm changes no prediction .",
    "hence , we can rescaling the value of @xmath164 by a suitable constant @xmath165 on condition that we divide , at the same time , the value of @xmath166 by the same constant @xmath163 . choosing for example a value for @xmath163 that makes the average weight of an edge of @xmath167 equal to @xmath168",
    ", if the avarage weight @xmath169 of a @xmath162-edge is still exponential in @xmath2 , it means that the weight of the @xmath162-edges is extremely large compare to the other weightsa and the labeling is very irregular ; this implies that the mispredictions of most labels can be considered , for such case , a very natural and justifiable behaviour for an algorithm driven by the edge weightning . for what concerns the second problem , it is possible to prove ( we omit the details for space limitations ) that we can eliminate from @xmath170 the contribution of any non-@xmath162-edge ( e.g. the ones exponentially light , that could be seen as a disconnection of @xmath25 ) simply on condition that we also count only one mistake more . in short",
    "the bound can be refined in the following way : @xmath171 where @xmath172 is any subset of non-@xmath162-edge of @xmath25 and @xmath173 is equal to @xmath174 . *",
    "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *    [ l : ub - l - to - t ] if wta  is run on a labeled weighted line graph @xmath175 , then the total number @xmath176 of mistakes satisfies @xmath177 for all subsets @xmath178 of @xmath179 .    note that the bound of lemma  [ l : ub - l - to - t ] implies that , for any @xmath180 , one can drop from the bound the contribution of any set of @xmath64 resistors in @xmath181 at the cost of adding @xmath64 extra mistakes .",
    "we now provide an upper bound on the number of mistakes made by wta  on any weighted tree @xmath182 in terms of the number of @xmath12-edges , the weighted cutsize , and @xmath166 .",
    "[ t : ub - tree ] if wta  is run on a labeled weighted tree @xmath142 , then the total number @xmath183 of mistakes satisfies @xmath184 for all subsets @xmath178 of @xmath179 .",
    "the logarithmic factor in the above bound shows that the algorithm takes advantage of labelings such that the weights of @xmath12-edges are small ( thus making @xmath164 small ) and the weights of @xmath12-free edges are high ( thus making @xmath166 small ) .",
    "this matches the intuition behind @xmath136 s nearest - neighbor rule according to which nodes that are close to each other are expected to have the same label . in particular , observe that the way the above quantities are combined makes the bound independent of rescaling of the edge weights .",
    "again , this has to be expected , since @xmath136 s prediction is scale insensitive . on the other hand",
    ", it may appear less natural that the mistake bound also depends linearly on the cutsize @xmath185 , _ independent of the edge weights_. the specialization to trees of our lower bound ( theorem  [ th : lower ] in section  [ s : lower ] ) implies that this linear dependence of mistakes on the unweighted cutsize is necessary whenever the adversarial labeling is chosen from a set of labelings with bounded @xmath185 .    *",
    "* * * * * * * * * * * * dropped from the workshop version * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *    in order to prove theorem  [ t : ub - tree ] , we will first prove the following lemma .",
    "[ ub_line ] if wta  is run on a weighted line graph @xmath175 , then the total number @xmath176 of mistakes satisfies @xmath186    by construction , @xmath176 is the number of mistakes made on the line @xmath138 obtained by linearizing @xmath25 .",
    "call cluster any sub - line of @xmath138 whose edges are all @xmath12-free .",
    "then @xmath138 contains exactly @xmath187 clusters , which we number consecutively starting from one of the two terminal nodes .",
    "consider the @xmath94-th cluster @xmath188 .",
    "let @xmath189 be the terminal node of @xmath188 connected to node @xmath190 of cluster @xmath191 .",
    "similarly , let @xmath192 be the other terminal node of @xmath188 connected to node @xmath193 of cluster @xmath194 .",
    "for simplicity of presentation , throughout this proof we ignore the special cases when @xmath191 or @xmath194 are missing .",
    "let @xmath195 be the first node of @xmath188 whose label is predicted by wta .",
    "after @xmath196 is revealed , the cluster splits into two edge - disjoint sub - lines , both having @xmath195 as terminal node : cluster @xmath197 , containing node @xmath189 , and cluster @xmath198 , containing node @xmath192 .",
    "whithout loss of generality , we now bound the mistakes on @xmath198 .",
    "the nearest neighbor prediction rule of wta  guarantees that the first mistake made on @xmath198 must occur on a node @xmath199 such that @xmath200 .",
    "after the revelation of @xmath201 , @xmath198 splits into a sub - line containing @xmath195 and a sub - line containing @xmath192 . henceforth , a mistake can only occur on a node @xmath202 such that @xmath203 , which belongs to the sub - line of @xmath198 containing @xmath192 . by iterating this argument",
    "we get that the total number of mistakes made on @xmath198 is bounded by @xmath204 . similarly , the total number of mistakes made on @xmath197 is bounded by @xmath205 .",
    "now , observe that @xmath206 and @xmath207 . since @xmath208 and @xmath209 , the total number of mistakes on @xmath188 is bounded by @xmath210 .",
    "hence , summing over clusters @xmath188 we obtain @xmath211 where in the second last step we used jensen s inequality , and in the last one the fact that @xmath212    the following lemma ( proof omitted ) establishes that one can drop from the mistake bound of lemma  [ ub_line ] , for any @xmath213 , the contribution of any set of @xmath64 resistors in @xmath214 at the cost of adding just @xmath64 extra mistakes .    * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * in order to conclude the proof , it is enough to refine the analysis for the mistakes that can occur on sublines @xmath215 and @xmath216 in the following way",
    ". without loss of generality , consider solely the subline @xmath215 .",
    "let @xmath13 be an edge included in @xmath215 .",
    "if @xmath13 was cutted , @xmath215 would split into two sublines ( eventually formed by a single node ) : @xmath217 , having @xmath21 as terminal node , and @xmath218 , having @xmath22 as terminal node .",
    "let @xmath219 and @xmath220 be the resistance diameters of @xmath217 and @xmath218 respectively .",
    "we can bound the number mistakes made on @xmath215 by summing the number of mistakes that can occur on @xmath217 and @xmath218 . without loss of generality ,",
    "let @xmath218 the subline having @xmath195 as terminal node . reusing the dichotomic method for bounding the number of mistakes that can occur on @xmath215 we can now state that , after the revelation of @xmath196 , the algorithm can make at most @xmath221 mistakes on @xmath217 and at most @xmath222 mistakes on @xmath218 .",
    "hence , the total number of mistakes @xmath223 that can occur on @xmath215 after the revelation of @xmath196 is bounded by @xmath224 according to the above analysis , in the last equation we get an upper bound of the number of mistakes that can occur on the subline obtained by @xmath215 removing edge @xmath13 ( so as to make @xmath21 coincident with @xmath22 ) .",
    "this implies that the number of mistakes made on @xmath215 after the revelation of @xmath196 is bounded by @xmath225 , where @xmath226 is equal to @xmath227 minus the resistance contribution of any set of @xmath228 edges included in @xmath215 .",
    "the proof is concluded applying this result in the analysis that we used for bounding the number of mistakes on @xmath138 summing over clusters @xmath188 .",
    "in order to solve the more general problem of predicting the labels of a weighted graph @xmath0 , one can first generate a spanning tree @xmath25 of @xmath0 and then run wta  directly on @xmath25 . in this case",
    ", it is possible to rephrase theorem  [ t : ub - tree ] in terms of the properties of @xmath0 .",
    "note that for each spanning tree @xmath25 of @xmath0 , @xmath229 and @xmath230 .",
    "specific choices of the spanning tree @xmath25 control in different ways the quantities in the mistake bound of theorem  [ t : ub - tree ] .",
    "for example , a minimum spanning tree tends to reduce the value of @xmath231 , betting on the fact that @xmath12-edges are light .",
    "the next theorem relies on _ random _ spanning trees .",
    "[ th : graph ] if wta  is run on a random spanning tree @xmath25 of a labeled weighted graph @xmath6 , then the total number @xmath232 of mistakes satisfies @xmath233\\bigl(1 + \\log \\left(1 + { w_{\\mathrm{max}}}^{\\phi } { { \\mathbb{e}}}\\bigl[r^w_t\\bigr ] \\bigr ) \\right)~,\\ ] ] where @xmath234 .    note that the mistake bound in  ( [ e : th_general_graphs ] ) is scale - invariant , since @xmath235=\\sum_{(i , j ) \\in e^{\\phi } } w_{i , j } r_{i , j}^w$ ] can not be affected by a uniform rescaling of the edge weights ( as we said in subsection  [ ss : not ] ) , and so is the product @xmath236=w^{\\phi}_{\\max}\\sum_{(i , j ) \\in e   \\setminus e^{\\phi } } r^w_{i , j}$ ] .",
    "we now compare the mistake bound  ( [ e : th_general_graphs ] ) to the lower bound stated in theorem  [ th : lower ] .",
    "in particular , we prove that @xmath136 is optimal ( up to @xmath237 factors ) on every weighted connected graph in which the @xmath12-edge weights are not `` superpolynomially overloaded '' w.r.t .  the @xmath12-free edge weights .",
    "in order to rule out pathological cases , when the weighted graph is nearly disconnected , we impose the following mild assumption on the graphs being considered .",
    "we say that a graph is _ polynomially connected _ if the ratio of any pair of effective resistances ( even those between nonadjacent nodes ) in the graph is polynomial in the total number of nodes @xmath2 .",
    "this definition essentially states that a weighted graph can be considered connected if no pair of nodes can be found which is substantially less connected than any other pair of nodes .",
    "again , as one would naturally expect , this definition is independent of uniform weight rescaling .",
    "the following corollary shows that if wta  is not optimal on a polynomially connected graph , then the labeling must be so irregular that the total weight of @xmath12-edges is an overwhelming fraction of the overall weight .",
    "[ cor : upper ] pick any polynomially connected weighted graph @xmath0 with @xmath2 nodes . if the ratio of the total weight of @xmath12-edges to the total weight of @xmath12-free edges is bounded by a polynomial in @xmath2 , then the total number of mistakes @xmath232 made by wta   when run on a random spanning tree @xmath25 of g satisfies @xmath238 \\log n$ ] .",
    "note that when the hypothesis of this corollary is not satisfied the bound of wta  is not necessarly vacuous .",
    "for example , @xmath239w^{\\phi}_{\\max } = n^{\\mathrm{polylog}(n)}$ ] implies an upper bound which is optimal up to @xmath240 factors . in particular",
    ", having a constant number of @xmath12-free edges with exponentially large resistance contradicts the assumption of polynomial connectivity , but it need not lead to a vacuous bound in theorem  [ th : graph ] .",
    "in fact , one can use lemma  [ l : ub - l - to - t ] to drop from the mistake bound of theorem  [ th : graph ] the contribution of any set of @xmath241 resistances in @xmath239=\\sum_{(i , j ) \\in e \\setminus e^{\\phi } } r^w_{i , j}$ ] at the cost of adding just @xmath241 extra mistakes .",
    "this could be seen as a robustness property of @xmath136 s bound against graphs that do not fully satisfy the connectedness assumption .",
    "we further elaborate on the robustness properties of wta  in section  [ s : robust ] . in the meanwhile , note how corollary  [ cor : upper ] compares to the expected mistake bound of algorithms like graph perceptron ( see section  [ s : rel ] ) on the same random spanning tree .",
    "this bound depends on the expectation of the product @xmath242 , where @xmath77 is the diameter of @xmath25 in the resistance distance metric .",
    "recall from the discussion in section  [ s : rel ] that these two factors are negatively correlated because @xmath164 depends linearly on the edge weights , while @xmath77 depends linearly on the reciprocal of these weights .",
    "moreover , for any given scale of the edge weights , @xmath77 can be linear in the number @xmath2 of nodes .",
    "another interesting comparison is to the covering ball bounds of  @xcite . consider the case when @xmath0 is an unweighted tree with diameter @xmath243 . whereas the dual norm approach of  @xcite gives a mistake bound of the form @xmath244 , our approach , as well as the one by  @xcite , yields @xmath245 .",
    "namely , the dependence on @xmath45 becomes linear rather than quadratic , but the diameter @xmath243 gets replaced by @xmath2 , the number of nodes in @xmath0 .",
    "replacing @xmath2 by @xmath243 seems to be a benefit brought by the covering ball approach . on unweighted trees",
    "is also achieved by the direct analysis of  @xcite . ] more generally , one can say that the covering ball approach seems to allow to replace the extra @xmath237 term contained in corollary  [ cor : upper ] by more refined structural parameters of the graph ( like its diameter @xmath243 ) , but it does so at the cost of squaring the dependence on the cutsize . a typical ( and unsurprising ) example where the dual - norm covering ball bounds are better",
    "then the one in corollary  [ cor : upper ] is when the labeled graph is well - clustered .",
    "one such example we already mentioned in section  [ s : rel ] : on the unweighted barbell graph made up of @xmath93-cliques connected by @xmath102 @xmath12-edges , the algorithm of  @xcite has a _ constant _ bound on the number of mistakes ( i.e. , independent of both @xmath93 and @xmath94 ) , the pounce algorithm has a _ linear _ bound in @xmath94 , while corollary [ cor : upper ] delivers a _ logarithmic _ bound in @xmath246 .",
    "yet , it is fair to point out that the bounds of  @xcite refer to computationally heavier algorithms than wta : pounce has a deterministic initialization step that computes the inverse laplacian matrix of the graph ( this is cubic in @xmath2 , or quadratic in the case of trees ) , the minimum @xmath247-seminorm interpolation algorithm of  @xcite has no initialization , but each step requires the solution of a constrained convex optimization problem ( whose time complexity was not quantified by the authors ) .",
    "further comments on the time complexity of our algorithm are given in section [ s : compl ] .",
    "* * * * * * * * * * * * * * * * * * * * * * * dropped from workshop submission * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *",
    "a direct implementation of @xmath136 operating on a tree @xmath25 with @xmath2 nodes would require running time @xmath248 over the @xmath2 prediction trials , and linear memory space .",
    "we now sketch how to implement @xmath136 in @xmath104 time , i.e. , in _ constant _ amortized time per trial .",
    "this is how we implemented wta  in the experiments reported in section  [ s : exp ] .",
    "once the given tree @xmath25 is linearized into an @xmath2-node line @xmath138 ( as described in section [ s : alg ] ) , @xmath138 is initially traversed from left to right .",
    "call @xmath249 the left - most terminal node of @xmath138 . during this traversal , the resistance distance @xmath250 is incrementally computed for each node @xmath21 in @xmath138 .",
    "this makes it possible to calculate @xmath251 in constant time for any pair of nodes , since @xmath252 . on top of line",
    "@xmath138 a complete binary tree @xmath253 is constructed having @xmath254 leaves .",
    "is a power of @xmath255 .",
    "if this is not the case , we could add dummy nodes to @xmath138 before building @xmath253 . ] the @xmath94-th leftmost leaf ( in the usual tree representation ) of @xmath253 is the @xmath94-th node in @xmath138 ( numbering the nodes of @xmath138 from left to right ) .",
    "the algorithm maintains this data - structure in such a way that at time @xmath256 : ( i ) the subsequence of leaves whose labels are revealed at time @xmath256 are connected through a ( bidirectional ) list @xmath257 , and ( ii ) all the ancestors in @xmath253 of the leaves of @xmath257 are marked .",
    "see figure [ f : fast - impl ] for an example",
    ".     constant amortized - time implementation of @xmath136 .",
    "the line @xmath138 has of @xmath258 nodes ( the adjacent squares at the bottom ) .",
    "shaded squares are the revealed nodes , connected through a dark grey doubly - linked list @xmath257 .",
    "the depicted tree @xmath253 has both unmarked ( white ) and marked ( shaded ) nodes .",
    "the arrows indicate the traversal operations performed by @xmath136 when predicting the label of node @xmath10 : the upward traversal stops as soon as a marked ancestor @xmath259 is found , and then a downward traversal begins .",
    "note that @xmath136 first descends to the left , and then keeps going right all the way down .",
    "once @xmath260 is determined , a single step within @xmath257 suffices to determine @xmath261 .",
    ", title=\"fig : \" ]    when @xmath136 is required to predict the label @xmath11 , the algorithm looks for the two closest leaves @xmath260 and @xmath261 oppositely located in @xmath138 with respect to @xmath10 .",
    "the above data structure supports this operation as follows .",
    "@xmath136 starts from @xmath10 and goes upwards in @xmath253 until the first marked ancestor @xmath259 of @xmath10 is reached . during this upward traversal , the algorithm marks each internal node of @xmath253 on the path connecting @xmath10 to @xmath259 .",
    "then , @xmath136 starts from @xmath259 and goes downwards in order to find the leaf @xmath262 closest to @xmath10 .",
    "note how the algorithm uses node marks for finding its way down : for instance , in figure [ f : fast - impl ] the algorithm goes left since @xmath259 was reached from below through the right child node , and then keeps right all the way down to @xmath260 .",
    "node @xmath261 ( if present ) is then identified via the links in @xmath257 .",
    "the two distances @xmath263 and @xmath264 are compared , and the closest node to @xmath10 within @xmath257 is then determined .",
    "finally , @xmath136 updates the links of @xmath257 by inserting @xmath10 between @xmath260 and @xmath261 .    in order to quantify the amortized time per trial",
    ", the key observation is that each internal node @xmath94 of @xmath253 gets visited only twice during _ upward _ traversals over the @xmath2 trials : the first visit takes place when @xmath94 gets marked for the first time ( just after one of the two children of @xmath94 is marked ) ; the second visit of @xmath94 occurs when a subsequent upwards visit also marks the other ( unmarked ) child of @xmath94 .",
    "once both of @xmath94 s children are marked , we are guaranteed that no further upward visits to @xmath94 will be performed , since any subsequent upward visit of @xmath253 will stop earlier than @xmath94 . because the nodes in @xmath253 are @xmath104 ,",
    "the other per - trial operations can be performed in constant time , and the preprocessing operations ( linearization of @xmath25 into @xmath138 , construction of @xmath253 , calculation of incremental distances ) are again @xmath104 , the total running time over @xmath2 trials is linear in @xmath2 , as anticipated .",
    "note , however , that the worst - case time per trial is @xmath265 .",
    "for instance , on the very first trial @xmath253 has to be traversed all the way up and down .    * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *",
    "in this section we show that wta  is tolerant to noise , i.e. , the number of mistakes made by wta   on most labeled graphs @xmath6 does not significantly change if a small number of labels are perturbed before running the algorithm .",
    "this is especially the case if the input graph @xmath0 is polynomially connected ( see section  [ s : gen ] for a definition ) .    as in previous sections , we start off from the case when the input graph is a tree , and then we extend the result to general graphs using random spanning trees .",
    "suppose that the labels @xmath7 in the tree @xmath142 used as input to the algorithm have actually been obtained from another labeling @xmath266 of @xmath25 through the perturbation ( flipping ) of some of its labels . as explained at the beginning of section  [ s : alg ]",
    ", wta  operates on a line graph @xmath138 obtained through the linearization process of the input tree @xmath25 .",
    "the following theorem shows that , whereas the cutsize differences @xmath267 and @xmath268 on tree @xmath25 can in principle be very large , the cutsize differences @xmath269 and @xmath270 on the line graph @xmath138 built by wta  are always small .    in order to quantify the above differences , we need a couple of ancillary definitions .",
    "given a labeled tree @xmath142 , define @xmath271 to be the sum of the weights of the @xmath64 heaviest edges in @xmath25 , @xmath272 if @xmath25 is unweighted we clearly have @xmath273 . moreover , given any two labelings @xmath7 and @xmath266 of @xmath25 s nodes , we let @xmath274 be the number of nodes for which the two labelings differ , i.e. , @xmath275    [ t : robust ] on any given labeled tree @xmath142 the tree linearization step of wta   generates a line graph @xmath138 such that :    1 .",
    "@xmath276  ; 2 .",
    "@xmath277  .    in order to highlight the consequences of wta s linearization step contained in theorem  [ t : robust ] , consider as a simple example an unweighted star graph @xmath142 where all labels are @xmath119 except for the central node @xmath278 whose label is @xmath120 .",
    "we have @xmath279 , but flipping the sign of @xmath280 we would obtain the star graph @xmath281 with @xmath282 . using theorem  [ t : robust ] ( item 2 ) we get @xmath283 . hence , on this star graph wta s linearization step generates a line graph with a constant number of @xmath12-edges even if the input tree @xmath25 has no @xmath12-free edges . because flipping the labels of a few nodes ( in this case the label of @xmath278 ) we obtain a tree with a much more regular labeling , the labels of those nodes can naturally be seen as corrupted by noise .",
    "the following theorem quantifies to what extent the mistake bound of wta  on trees can take advantage of the tolerance to label perturbation contained in theorem  [ t : robust ] . introducing shorthands for the right - hand side expressions in theorem  [ t : robust ] ,",
    "@xmath284 and @xmath285 we have the following robust version of theorem  [ t : ub - tree ] .",
    "[ t : ub - tree - r ] if wta  is run on a weighted and labeled tree @xmath142 , then the total number @xmath183 of mistakes satisfies @xmath286 for all subsets @xmath178 of @xmath179 .    as a simple consequence , we have the following corollary .",
    "[ c : ub - tree - r - poly ] if wta  is run on a weighted and polynomially connected labeled tree @xmath142 , then the total number @xmath183 of mistakes satisfies @xmath287    theorem  [ t : ub - tree - r ] combines the result of theorem  [ t : ub - tree ] with the robustness to label perturbation of wta s tree linearization procedure .",
    "comparing the two theorems , we see that the main advantage of the tree linearization lies in the mistake bound dependence on the logarithmic factors occurring in the formulas : theorem  [ t : ub - tree - r ] shows that , when @xmath288 , then the performance of wta  can be just linear in @xmath185 .",
    "theorem  [ t : ub - tree ] shows instead that the dependence on @xmath185 is in general superlinear even in cases when flipping few labels of @xmath7 makes the cutsize @xmath185 decrease in a substantial way . in many cases ,",
    "the tolerance to noise allows us to achieve even better results : corollary  [ c : ub - tree - r - poly ] states that , if @xmath25 is polynomially connected and there exists a labeling @xmath266 with small @xmath274 such that @xmath289 is much smaller than @xmath185 , then the performance of wtais about the same as if the algorithm were run on @xmath281 .",
    "in fact , from lemma  [ l : ub - l - to - t ] we know that when @xmath25 is polynomially connected the mistake bound of wta  mainly depends on the number of @xmath12-edges in @xmath175 , which can often be much smaller than those in @xmath142 . as a simple example ,",
    "let @xmath25 be an unweighted star graph with a labeling @xmath7 and @xmath290 be the difference between the number of @xmath119 and the number of @xmath120 in @xmath7 .",
    "then the mistake bound of @xmath136 is linear in @xmath291 irrespective of @xmath185 and , specifically , irrespective of the label assigned to the central node of the star , which can greatly affect the actual value of @xmath185 .",
    "we are now ready to extend the above results to the case when wta  operates on a general weighted graph @xmath292 via a uniformly generated random spanning tree @xmath25 .",
    "as before , we need some shorthand notation . define @xmath293 as @xmath294+\\delta({\\boldsymbol{y}},{\\boldsymbol{y}}')\\bigr)~,\\ ] ] where the expectation is over the random draw of a spanning tree @xmath25 of @xmath0 .",
    "the following are the robust versions of theorem  [ th : graph ] and corollary  [ cor : upper ] .",
    "[ th : graph_r ] if wta  is run on a random spanning tree @xmath25 of a labeled weighted graph @xmath6 , then the total number @xmath232 of mistakes satisfies @xmath295 \\bigr ) \\right)+{{\\mathbb{e}}}\\bigl[\\phi_t({\\boldsymbol{y}})\\bigr]~,\\ ] ] where @xmath234 .",
    "[ c : graph_pol_r ] if wta  is run on a random spanning tree @xmath25 of a labeled weighted graph @xmath6 and the ratio of the weights of each pair of edges of @xmath0 is polynomial in @xmath2 , then the total number @xmath232 of mistakes satisfies @xmath296    the relationship between theorem  [ th : graph_r ] and theorem  [ th : graph ] is similar to the one between theorem  [ t : ub - tree - r ] and theorem  [ t : ub - tree ]",
    ". when there exists a labeling @xmath266 such that @xmath274 is small and @xmath297 \\ll { { \\mathbb{e}}}\\bigl[\\phi_t({\\boldsymbol{y}})\\bigr]$ ] , then theorem  [ th : graph_r ] allows a linear dependence on @xmath235 $ ] .",
    "finally , corollary  [ c : graph_pol_r ] quantifies the advantages of wta s noise tolerance under a similar ( but stricter ) assumption as the one contained in corollary  [ cor : upper ] .",
    "as explained in section  [ s : alg ] , @xmath136 runs in two phases : ( i ) a random spanning tree is drawn ; ( ii ) the tree is linearized and labels are sequentially predicted . as discussed in subsection  [ ss : not ] , wilson s algorithm can draw a random spanning tree of `` most '' unweighted graphs in expected time @xmath104 .",
    "the analysis of running times on weighted graphs is significantly more complex , and outside the scope of this paper . a naive implementation of wta s second phase runs in time @xmath248 and requires linear memory space when operating on a tree with @xmath2 nodes .",
    "we now describe how to implement the second phase to run in time @xmath104 , i.e. , in _ constant _ amortized time per prediction step .",
    "once the given tree @xmath25 is linearized into an @xmath2-node line @xmath138 , we initially traverse @xmath138 from left to right .",
    "call @xmath249 the left - most terminal node of @xmath138 . during this traversal , the resistance distance @xmath250 is incrementally computed for each node @xmath21 in @xmath138 .",
    "this makes it possible to calculate @xmath251 in constant time for any pair of nodes , since @xmath298 for all @xmath299 . on top of @xmath138 , a complete binary tree @xmath253 with @xmath254 leaves",
    "is constructed .",
    "is a power of @xmath255 .",
    "if this is not the case , we could add dummy nodes to @xmath138 before building @xmath253 . ] the @xmath94-th leftmost leaf ( in the usual tree representation ) of @xmath253 is the @xmath94-th node in @xmath138 ( numbering the nodes of @xmath138 from left to right ) .",
    "the algorithm maintains this data - structure in such a way that at time @xmath256 : ( i ) the subsequence of leaves whose labels are revealed at time @xmath256 are connected through a ( bidirectional ) list @xmath257 , and ( ii ) all the ancestors in @xmath253 of the leaves of @xmath257 are marked .",
    "see figure  [ f : fast - impl ] .",
    "constant amortized time implementation of @xmath136 .",
    "the line @xmath138 has @xmath300 nodes ( the adjacent squares at the bottom ) .",
    "shaded squares are the revealed nodes , connected through a dark grey doubly - linked list @xmath257 .",
    "the depicted tree @xmath253 has both unmarked ( white ) and marked ( shaded ) nodes .",
    "the arrows indicate the traversal operations performed by @xmath136 when predicting the label of node @xmath10 : the upward traversal stops as soon as a marked ancestor @xmath259 is found , and then a downward traversal begins .",
    "note that @xmath136 first descends to the left , and then keeps going right all the way down .",
    "once @xmath260 is determined , a single step within @xmath257 suffices to determine @xmath261 .",
    ", title=\"fig : \" ]    when @xmath136 is required to predict the label @xmath11 , the algorithm looks for the two closest revealed leaves @xmath260 and @xmath261 oppositely located in @xmath138 with respect to @xmath10 .",
    "the above data structure supports this operation as follows .",
    "@xmath136 starts from @xmath10 and goes upwards in @xmath253 until the first marked ancestor @xmath259 of @xmath10 is reached . during this upward traversal , the algorithm marks each internal node of @xmath253 on the path connecting @xmath10 to @xmath259",
    "then , @xmath136 starts from @xmath259 and goes downwards in order to find the leaf @xmath262 closest to @xmath10 .",
    "note how the algorithm uses node marks for finding its way down : for instance , in figure  [ f : fast - impl ] the algorithm goes left since @xmath259 was reached from below through the right child node , and then keeps right all the way down to @xmath260 .",
    "node @xmath261 ( if present ) is then identified via the links in @xmath257 .",
    "the two distances @xmath263 and @xmath264 are compared , and the closest node to @xmath10 within @xmath257 is then determined .",
    "finally , @xmath136 updates the links of @xmath257 by inserting @xmath10 between @xmath260 and @xmath261 .    in order to quantify the amortized time per trial",
    ", the key observation is that each internal node @xmath94 of @xmath253 gets visited only twice during _ upward _ traversals over the @xmath2 trials : the first visit takes place when @xmath94 gets marked for the first time , the second visit of @xmath94 occurs when a subsequent upward visit also marks the other ( unmarked ) child of @xmath94 .",
    "once both of @xmath94 s children are marked , we are guaranteed that no further upward visits to @xmath94 will be performed .",
    "since the preprocessing operations take @xmath104 , this shows that the total running time over the @xmath2 trials is linear in @xmath2 , as anticipated .",
    "note , however , that the worst - case time per trial is @xmath265 .",
    "for instance , on the very first trial @xmath253 has to be traversed all the way up and down .",
    "this is the way we implemented wta  on the experiments described in the next section .",
    "we now present the results of an experimental comparison on a number of real - world weighted graphs from different domains : text categorization , optical character recognition , spam detection and bioinformatics .",
    "although our theoretical analysis is for the sequential prediction model , all experiments are carried out using a more standard train - test scenario .",
    "this makes it easy to compare wta  against popular non - sequential baselines , such as label propagation",
    ".        introduced by @xcite and here abbreviated as @xmath301 ( graph perceptron algorithm ) .",
    "this algorithm sequentially predicts the nodes of a weighted graph @xmath302 after mapping @xmath8 via the linear kernel based on @xmath303 , where @xmath67 is the laplacian matrix of @xmath0 .",
    "following @xcite , we run @xmath301 on a spanning tree @xmath25 of the original graph .",
    "this is because a careful computation of the laplacian pseudoinverse of a @xmath2-node tree takes time @xmath304 where @xmath93 is the number of training examples plus the number of test examples ( labels to predict ) , and @xmath243 is the tree diameter see the work of  @xcite for a proof of this fact .",
    "however , in most of our experiments @xmath305 , implying a running time of @xmath306 for @xmath301 . note that @xmath301 is a global approach , in that the graph topology affects , via the inverse laplacian , the prediction on all nodes",
    ".      introduced here and abbreviated as @xmath307 .",
    "since the common underlying assumption to graph prediction algorithms is that adjacent nodes are labeled similarly , a very intuitive and fast algorithm for predicting the label of a node @xmath21 is via a weighted majority vote on the available labels of the adjacent nodes .",
    "more precisely , wmv  predicts using the sign of @xmath308 where @xmath309 if node @xmath22 is not available in the training set . the overall time and space requirements are both of order @xmath310 , since we need to read ( at least once ) the weights of all edges .",
    "@xmath307 is also a local approach , in the sense that prediction at each node is only affected by the labels of adjacent nodes",
    ".      introduced by  @xcite and here abbreviated as @xmath311 .",
    "this is a batch transductive learning method based on solving a ( possibly sparse ) linear system of equations which requires @xmath312 time on an @xmath2-node graph with @xmath93 edges .",
    "this bad scalability prevented us from carrying out comparative experiments on larger graphs of @xmath313 or more nodes .",
    "note that wmv  can be viewed as a fast approximation of labprop .",
    "( @xmath314 ) .",
    "each spanning tree is taken with probability proportional to the product of its edge weights see , e.g. , ( * ? ? ?",
    "* chapter 4 ) .",
    "in addition , we also tested wta  combined with rst  generated by ignoring the edge weights ( which were then restored before running wta ) .",
    "this second approach gives a prediction algorithm whose total expected running time , including the generation of the spanning tree , is @xmath315 on most graphs .",
    "we abbreviate this spanning tree as @xmath316 ( non - weighted @xmath314 ) .",
    "( @xmath317 ) .",
    "this spanning tree is created via the following randomized depth - first visit : a root is selected at random , then each newly visited node is chosen with probability proportional to the weights of the edges connecting the current vertex with the adjacent nodes that have not been visited yet .",
    "this spanning tree is faster to generate than rst , and can be viewed as an approximate version of rst .",
    "( @xmath318 ) . the spanning tree minimizing the sum of the resistors of all edges .",
    "this is the tree whose laplacian best approximates the laplacian of @xmath0 according to the trace norm criterion see , e.g. , the paper of  @xcite .",
    "( @xmath319 ) .",
    "@xcite use the shortest path tree because it has a small diameter ( at most twice the diameter of @xmath0 ) .",
    "this allows them to better control the theoretical performance of @xmath301 .",
    "we generated several shortest path spanning trees by choosing the root node at random , and then took the one with minimum diameter .    in order to check whether the information carried by the edge weight has predictive value for a nearest neighbor rule like wta",
    ", we also performed a test by ignoring the edge weights during both the generation of the spanning tree and the running of wta s nearest neighbor rule .",
    "this is essentially the algorithm analyzed by  @xcite , and we denote it by @xmath320 ( non - weighted wta ) .",
    "we combined nwwta  with weighted and unweighted spanning trees .",
    "so , for instance , nwwta+rst  runs a 1-nn rule ( nwwta ) that does not take edge weights into account ( i.e. , pretending that all weights are unitary ) on a random spanning tree generated according to the actual edge weights . nwwta+nwrst  runs nwwta  on a random spanning tree that also disregars edge weights .    finally , in order to make the classifications based on rst s more robust with respect to the variance associated with the random generation of the spanning tree , we also tested committees of rst s .",
    "for example , k*wta+rst  denotes the classifier obtained by drawing @xmath64 rst s , running wta  on each one of them , and then aggegating the predictions of the @xmath64 resulting classifiers via a majority vote . for our experiments we chose @xmath321 .                        a large dataset ( 110,900 nodes and 1,836,136 edges ) of inter - host links created for the web spam challenge 2008  @xcite .",
    "this is a weighted graph with binary labels and a pre - defined train / test split : 3,897 training nodes and 1,993 test nodes ( the remaining ones being unlabeled ) .",
    "we created graphs from rcv1 and usps with as many nodes as the total number of examples @xmath323 in the datasets .",
    "that is , 10,000 nodes for rcv1 and 7291 + 2007 = 9298 for usps . following previous experimental settings  @xcite ,",
    "the graphs were constructed using @xmath94-nn based on the standard euclidean distance @xmath324 between node @xmath21 and node @xmath22 .",
    "the weight @xmath23 was set to @xmath325 , if @xmath22 is one of the @xmath94 nearest neighbors of @xmath21 , and 0 otherwise .",
    "to set @xmath326 , we first computed the average square distance between @xmath21 and its @xmath94 nearest neighbors ( call it @xmath327 ) , then we computed @xmath328 in the same way , and finally set @xmath329 .",
    "we generated two graphs for each dataset by running @xmath94-nn with @xmath330 ( rcv1 - 10 and usps-10 ) and @xmath331 ( rcv1 - 100 and usps-100 ) .",
    "the labels were set using the four most frequent categories in rcv1 and all 10 categories in usps .    in krogan and combined we only considered the biggest connected components of both datasets , obtaining 2,169 nodes and 6,102 edges for krogan , and 2,871 nodes and 6,407 edges for combined . in these graphs , each node belongs to one or more classes , each class representing a gene function .",
    "we selected the set of functional labels at depth one in the funcat classification scheme of the mips database  @xcite , resulting in seventeen classes per dataset .    in order to associate binary classification tasks with the six non - binary datasets / graphs ( rcv1 - 10 ,",
    "rcv1 - 100 , usps-10 , usps-100 , krogan , combined ) we binarized the corresponding multiclass problems via a standard one - vs - rest scheme .",
    "we thus obtained : four binary classification tasks for rcv1 - 10 and rcv1 - 100 , ten binary tasks for usps-10 and usps-100 , seventeen binary tasks for both krogan and combined . for a given a binary task and dataset , we tried different proportions of training set and test set sizes . in particular , we used training sets of size 5% , 10% , 25% and 50% . for",
    "any given size , the training sets were randomly selected .",
    "we report error rates and f - measures on the test set , after macro - averaging over the binary tasks .",
    "the results are contained in tables [ t : rcv1-k10][t : spamresults ] ( appendix [ app : tables ] ) and in figures [ f : charts][f : charts2 ] .",
    "specifically , tables [ t : rcv1-k10][t : combined ] contain results for all combinations of algorithms and train / test split for the first six datasets ( i.e. , all but webspam ) .",
    "the webspam dataset is very large , and requires us a lot of computational resources in order to run experiments on this graph .",
    "moreover , @xmath301 has always shown inferior accuracy performance than the corresponding version of wta  ( i.e. , the one using the same kind of spanning tree ) on all other datasets .",
    "hence we decided not to go on any further with the refined implementation of @xmath301 on trees we mentioned above . in table",
    "[ t : spamresults ] we only report test error results on the four algorithms wta , wmv , labprop , and wta  with a committee of seven ( nonweighted ) random spanning trees .      1 .",
    "we first generated ten random permutations of the node indices for each one of the six graphs / datasets ; 2 .   on each permutation we generated the training / test splits ; 3 .",
    "we computed @xmath318 and @xmath319 for each graph and made ( for @xmath136 , @xmath301 , @xmath307 , and @xmath311 ) one run per permutation on each of the 4 + 4 + 10 + 10 + 17 + 17 = 62 binary problems , averaging results over permutations and splits ; 4 .   for each graph",
    ", we generated ten random instances for each one of @xmath314 , @xmath316 , @xmath317 , and then operated as in step  2 , with a further averaging over the randomness in the tree generation .",
    "figure [ f : charts ] extracts from tables [ t : rcv1-k10][t : combined ] the error levels of the best spanning tree performers , and compared them to wmv  and labprop . for comparison purposes , we also displayed the error levels achieved by wta  operating on a committee of seventeen random spanning trees ( see below ) .",
    "figure [ f : charts2 ] ( left ) contains the error level on webspam reported in table  [ t : spamresults ] .",
    "finally , figure [ f : charts2 ] ( right ) is meant to emphasize the error rate differences between rst  and nwrst  run with wta .",
    "c c & +   macroaveraged test error rates on the first six datasets as a function of the training set size .",
    "the results are extracted from tables  [ t : rcv1-k10][t : combined ] in appendix b. only the best performing spanning tree ( i.e. , mst ) is shown for the algorithms that use spanning trees .",
    "these results are compared to wmv , labprop , and 17*wta+rst .",
    ", title=\"fig:\",width=264 ] &   macroaveraged test error rates on the first six datasets as a function of the training set size .",
    "the results are extracted from tables  [ t : rcv1-k10][t : combined ] in appendix b. only the best performing spanning tree ( i.e. , mst ) is shown for the algorithms that use spanning trees .",
    "these results are compared to wmv , labprop , and 17*wta+rst .",
    ", title=\"fig:\",width=264 ] +   macroaveraged test error rates on the first six datasets as a function of the training set size .",
    "the results are extracted from tables  [ t : rcv1-k10][t : combined ] in appendix b. only the best performing spanning tree ( i.e. , mst ) is shown for the algorithms that use spanning trees .",
    "these results are compared to wmv , labprop , and 17*wta+rst .",
    ", title=\"fig:\",width=264 ] &   macroaveraged test error rates on the first six datasets as a function of the training set size .",
    "the results are extracted from tables  [ t : rcv1-k10][t : combined ] in appendix b. only the best performing spanning tree ( i.e. , mst ) is shown for the algorithms that use spanning trees .",
    "these results are compared to wmv , labprop , and 17*wta+rst . ,",
    "title=\"fig:\",width=264 ] +   macroaveraged test error rates on the first six datasets as a function of the training set size .",
    "the results are extracted from tables  [ t : rcv1-k10][t : combined ] in appendix b. only the best performing spanning tree ( i.e. , mst ) is shown for the algorithms that use spanning trees .",
    "these results are compared to wmv , labprop , and 17*wta+rst .",
    ", title=\"fig:\",width=264 ] &   macroaveraged test error rates on the first six datasets as a function of the training set size .",
    "the results are extracted from tables  [ t : rcv1-k10][t : combined ] in appendix b. only the best performing spanning tree ( i.e. , mst ) is shown for the algorithms that use spanning trees .",
    "these results are compared to wmv , labprop , and 17*wta+rst .",
    ", title=\"fig:\",width=264 ] +        n.  alon , c.  avin , m.  kouck , g.  kozma , z.  lotker , and m.r .",
    "many random walks are faster than one . in _ proc .",
    "of the 20th annual acm symposium on parallel algorithms and architectures _ , pages 119128 .",
    "springer , 2008 .",
    "n.  cesa - bianchi , c.  gentile , f.  vitale , and g. zappella .",
    "random spanning trees and the prediction of weighted graphs . in _ proceedings of the 27th international conference on machine learning ( 27th icml ) _ , 2010 .",
    "h.  chang and d.y .",
    "graph laplacian kernels for object classification from a single example . in _ proc .",
    "ieee computer society conference on computer vision and pattern recognition _ ,",
    "pages 20112016 .",
    "ieee press , 2006 .",
    "a.  goldberg and x.  zhu .",
    "seeing stars when there are nt many stars : graph - based semi - supervised learning for sentiment categorization . in _ hlt - naacl 2006 workshop on textgraphs : graph - based algorithms for natural language processing _ , 2004 .",
    "g.  pandey , m.  steinbach , r.  gupta , t.  garg , and v.  kumar .",
    "association analysis - based transformations for protein interaction networks : a function prediction case study . in _ proc . of the 13th acm",
    "sigkdd international conference on knowledge discovery and data mining _ ,",
    "pages 540549 .",
    "acm press , 2007 .",
    "x.  zhu , z.  ghahramani , and j.  lafferty .",
    "semi - supervised learning using gaussian fields and harmonic functions . in _",
    "icml workshop on the continuum from labeled to unlabeled data in machine learning and data mining _ , 2003 ."
  ],
  "abstract_text": [
    "<S> we investigate the problem of sequentially predicting the binary labels on the nodes of an arbitrary weighted graph . </S>",
    "<S> we show that , under a suitable parametrization of the problem , the optimal number of prediction mistakes can be characterized ( up to logarithmic factors ) by the cutsize of a random spanning tree of the graph . </S>",
    "<S> the cutsize is induced by the unknown adversarial labeling of the graph nodes . in deriving our characterization </S>",
    "<S> , we obtain a simple randomized algorithm achieving in expectation the optimal mistake bound on any polynomially connected weighted graph . </S>",
    "<S> our algorithm draws a random spanning tree of the original graph and then predicts the nodes of this tree in constant expected amortized time and linear space . </S>",
    "<S> experiments on real - world datasets show that our method compares well to both global ( perceptron ) and local ( label propagation ) methods , while being generally faster in practice . </S>"
  ]
}