{
  "article_text": [
    "hyperspectral images consist of very high - dimensional pixel observations that allow reconstruction of the spectral profiles of objects imaged thanks to the acquisition of several hundred narrow spectral bands .",
    "the supervised classification of these pixels is a challenging task , which commonly arises in remote sensing imaging @xcite .",
    "structure of the hyperspectral imagery is seldom studied in a comprehensive manner , with most approaches focusing either on spatial information building on tools available for normal imagery or with a focus on spectral information without a principled way to make use of both .",
    "we propose a novel approach to classification based on kernel embeddings of distributions which utilizes both the spatial and spectral information in the data . while aimed at hyperspectral imaging , the method we propose is general and can be applied to other types of data .",
    "kernel methods and support vector machines have been employed in the hyperspectral imaging in @xcite the pixel data is lifted into a potentially infinite - dimensional feature space , called reproducing kernel hilbert space ( rkhs ) , where linear separating hyperplanes are sought .",
    "however , spectral information contained in pixels is often not sufficient for such task , and as we will see , including the local / spatial information available in the imagery is key to obtain good classification accuracy .",
    "our approach is to encode the spatial neighbourhood of each pixel as a random sample from a distribution associated to that pixel and to treat such distribution as an additional feature for classification . in order to add consistent spatial information",
    ", we also use the hadamard multiplication of two kernels .",
    "where one kernel is the kernel embeddings of distributions , and the other one is the linear kernel of spatial information similarly to @xcite .    in section [ sec : related_work ] , related work is reviewed .",
    "section [ sec : background ] provides the background on kernel embeddings of distributions , random features for fast approximations to kernel methods , and on mathematical morphology , which allow us to analyse and understand the geometrical structures of images .",
    "section [ sec : theory ] studies the consistency and convergence rate of the proposed method and experiments are given in section [ sec : experiments ] .",
    "many techniques aim to include the spatial information in the classification process . of particular interest",
    "are those combining feature space representations describing the spatial information with those describing the pixels .",
    "morphological feature spaces have been considered in several publications , with impressive results @xcite . on the other hand ,",
    "kernel methods have also been studied extensively , and more particularly the compositions of kernels @xcite , which allow building new feature space representations .",
    "we marry these approaches with a framework of @xcite , where instead of the usual feature map , sending each data point to the feature space , a whole distribution can be represented in the rkhs .",
    "this yields a framework for learning on distributions via their representations in this rkhs . in our approach",
    ", each pixel is associated to a distribution of its neighbours  effectively , a hyperspectral image is treated as a set of such distributions .",
    "this is similar to the approach to regression applied in @xcite to the multispectral imaging data .",
    "however , the authors of @xcite partition a multispectral image and classify the partitions - with a goal to obtain responses at the level of the _ groups of neighbouring pixels _ , which suffices when the goal is to predict an averaged quantity of an image area ( e.g. aerosol concentration as studied in @xcite ) and the pixel - level classification is not considered .",
    "another related line of work is that of @xcite , where they used the mean map on hyperspectral to perform a dimensionality reduction .",
    "let @xmath0 be a positive definite kernel . by moore - aronszajn theorem @xcite , there is a unique rkhs @xmath1 of real - valued functions on @xmath2 where @xmath3 , for all @xmath4 , implying that @xmath5 corresponds to an inner product between features and , in particular , @xmath6 .",
    "this means that @xmath7 can be viewed as a feature of @xmath8 . for many typical choices of kernels @xmath5 ,",
    "the rkhs @xmath1 is infinite - dimensional .",
    "now , let @xmath9 denote a random variable following a distribution @xmath10 .",
    "the _ mean map _ or the _ kernel embedding _",
    "@xcite of @xmath10 is defined as : @xmath11 = \\int_{{\\mathcal x}}k(\\cdot , x ) \\",
    "\\mathrm{d}\\mathcal{p}(x ) , \\end{aligned}\\ ] ] where the expectation is over @xmath1 . for _ characteristic _ kernels @xcite , which include gaussian rbf , matern family and many others",
    ", this embedding is injective on the space of all probability distributions ( i.e. captures information on all moments , akin to a characteristic function ) .",
    "further , if we are given two random variables , @xmath9 following the distribution @xmath10 , and @xmath12 following the distribution @xmath13 , the inner product between the corresponding embeddings is given as @xmath14,\\end{aligned}\\ ] ] which is sometimes referred to as a _",
    "mean map kernel_. for a random sample @xmath15 , drawn independently and identically distributed from @xmath10 , we can define the empirical mean map : @xmath16 and for random samples @xmath15 from @xmath10 and @xmath17 from @xmath13 , we obtain the empirical mean map kernel : @xmath18      the computational and storage requirements for kernel methods on large datasets can be prohibitive in practice due to the need to compute and store the kernel matrix . if we consider a dataset of @xmath19 @xmath20-dimensional observations , the storage requirements are @xmath21 and the calculation takes @xmath22 operations .",
    "a remedy developed by @xcite is to approximate translation - invariant kernels in an unbiased way using a random feature representation .",
    "namely , any translation - invariant positive definite kernel @xmath5 , such that @xmath23 , @xmath24 can be written as @xmath25 @xmath26 $ ] , where @xmath27 follows some distribution @xmath28 ( spectral measure of the kernel ) .",
    "thus , by sampling i.i.d .",
    "vectors @xmath29 from @xmath28 , we can approximate kernel @xmath5 by @xmath30 defined by : @xmath31 @xmath32 , so that the original feature map @xmath33 , potentially living in an infinite - dimensional space , is approximated by an explicit @xmath34dimensional feature vector : @xmath35 thus , the mean map and the mean map kernel can be estimated using these finite - dimensional representations . in this contribution , we will focus on gaussian rbf kernels for which the spectral measure @xmath28 is also gaussian .",
    "let us now turn our attention to a hyperspectral image @xmath36 . around each pixel location @xmath37 , we consider a square patch @xmath38 of size @xmath39 where we will treat the pixels as a random sample from a distribution @xmath40 specific to the location @xmath37 . instead of calculating the kernel between individual data points , we will calculate kernel between these distributions .",
    "an empirical mean map kernel is thus given simply by : @xmath41 where @xmath42 denotes the measurement vector at location @xmath43 and in the last line we employ a random feature approximation of @xmath5 .",
    "it should be noted that there may be outliers in a patch , which can damage the estimation of the mean .",
    "similarly to the work of @xcite , we proposed to use a weighted mean map , where the weights depend on spatial information .",
    "the kernels we obtain , called convolutional kernels , have also been used in @xcite .",
    "in contrast to @xcite , however , we will use random feature expansions to explicitly represent the feature space .",
    "the convolutional kernel is defined as : @xmath44 where @xmath45 represents a normalised version of @xmath36 , such that for all @xmath46 $ ] = @xmath47 .",
    "so we do a product of a kernel on the positions , another on the magnitudes , and a third one is an rbf kernel between spectra .",
    "this formula can be interpreted as a weighted mean map which is defined by : @xmath48 where @xmath49 is a positive definite kernel arising from the random feature space expansion .",
    "let us consider that the data are partitioned into sets following the same distribution , then the structure of our data is given by @xmath50 with @xmath51 , where @xmath52 are drawn from a joint meta distribution @xmath53 .",
    "we follow the notation of @xcite .",
    "let us denote @xmath54 a loss function .",
    "let us write the following expected risk function of the data for the svm problem : @xmath55 we can modify it to mean map embedding classification problem : @xmath56 we can also write the empirical risk function , for mean map embedding classification problem : @xmath57 finally we can also write the empirical risk function , for the empirical mean map embedding classification problem : @xmath58 then we would like to obtain an inequality between @xmath59 and @xmath60 . to do that , inspired by @xcite",
    ", we derive a inequality @xmath59 and @xmath61 : +    given that @xmath62 an arbitrary probability distribution with variance @xmath63 , a lipschitz continuous function @xmath64 with constant @xmath65 , an arbitrary loss function @xmath66 that is lipschitz continuous in the second argument with constant @xmath67 , it follows that : @xmath68    the proof of this theorem can be found on the supplementary materials",
    ". then we might use @xcite where we have an inequality between @xmath59 and @xmath69 :    let @xmath70 denote the loss class , let @xmath71 denote the rademacher complexity .",
    "let @xmath72 be a bound on the variance of the functions in @xmath73 .",
    "if the trace of the kernel is bounded , the loss function @xmath74 that is lipschitz continuous , for any @xmath75 , the following bound holds with probability at least @xmath76 @xmath77    given that @xmath64 is a lipschitz continuous function with constant @xmath65 , an arbitrary loss function @xmath66 that is lipschitz continuous with constant @xmath78 , it follows that : @xmath79    the proof of this theorem can be found on the supplementary materials .",
    "we also need the following theorem proved in @xcite    assume that @xmath80 for all @xmath81 with @xmath82 , and that @xmath5 is an universal kernel . then with probability at least @xmath83 :    @xmath84 - \\mu[x]| \\leq 2\\mathcal{r}_n(\\mathcal{h } , p ) + r\\sqrt{\\log\\left(1/\\delta\\right)/n}\\end{aligned}\\ ] ]    where @xmath85 denotes the rademacher average associated with @xmath86 and @xmath1 .    then by combining the previous theorems we easily have the following theorem .",
    "[ theorem1 ] given the conditions of the previous theorems .",
    "then with probability at least @xmath83 : @xmath87 \\\\ + 8 \\mathcal{r}_n(\\mathcal{g } ) + \\sigma(\\mathcal{g } ) \\sqrt{\\frac{8 \\log(2/\\delta)}{n}}+ \\frac{3 \\log(2/\\delta)}{n}\\end{aligned}\\ ] ]",
    "we evaluate the classification accuracy of the proposed approach using two standard datasets : the aviris indian pines , and the rosis university of pavia .",
    "the first data set is an image of dimension @xmath88 pixels , with @xmath89 spectral bands and its geometrical resolution is of 3.7 m. the training set is composed of @xmath90 pixels , and the image is composed of @xmath91 classes .",
    "the dimensions of the second data set are @xmath92 pixels , with @xmath93 spectral bands and its geometrical resolution is of 1.3 m. the training set is composed of @xmath94 pixels and a testing set of @xmath95 pixels , and the image is composed of @xmath96 classes .",
    "there is a commonly used testing set for pavia data , and we report performance on this testing set . in the first data set",
    "there is no testing set so we generate 20 monte - carlo simulations , selecting randomly 5 pixels per class , then aggregate the result of the classification .",
    "we used the morphological profile ( mp ) feature @xcite space which is commonly used in pixel classifiation and is described in the supplementary material .",
    "we also use the product of the two kernels where one is the mp kernel and the other is the kmm kernels .",
    "this kind of techinique has been previously explored in @xcite .",
    "in contrast to the previous work , which approximate this product of kernels thanks to addition of kernels , we can do the real multiplication since we work with finite dimension hilbert spaces .",
    "the results of classification are reported in table [ table1 ] ( indian pine ) , and table [ table2 ] ( pavia ) .",
    "the classification algorithm used is the c - svm @xcite were the parameter @xmath97 was selected with @xmath98fold cross - validation on a grid @xmath99,with @xmath100 $ ] .",
    "the results on table [ table1 ] and [ table2 ] show us that kernel mean map can perform as well as state of art results on these images .",
    "the size of scale @xmath39 seems to be important , this is linked with the theorem [ theorem1 ] , where we see that increasing the size of the scale increase in a way the size of the training set .",
    "an explanation of the parameters of evaluation can be found in @xcite ( pages 166 - 167 ) and also in the supplementary materials .",
    ".overall accuracy , kappa statistic , and average accuracy obtained for different kernels , applied on the aviris indian pines hyperspectral data set .",
    "i have run 20 monte carlo simulations .",
    "i have selected on the training set just 5 samples per class . [ cols= \" < , < , < , < , < \" , ]",
    "in this article we developed a new method for pixel classification in hyperspectral imaging .",
    "the method uses spatial information encoded through distributions of the neighbourhood around each pixel .",
    "even with very simple kernel choices ( gaussian rbf applied directly to raw data ) the obtained results are comparable with state - of - the - art .",
    "we establish convergence rates for , and prove that we are two - stage consistent .",
    "further improvements are possible by using different feature spaces employing suitable representations of the individual pixels . we believe that we have established an interesting research direction where local distributions are treated as additional features for a supervised learning task , which is of particular interest in hyperspectral imaging where it is difficult to combine spatial and spectral information in a principled way and our approach can be viewed as a step in that direction .",
    "10    gustavo camps - valls , devis tuia , lorenzo bruzzone , and jon atli  benediktsson , `` advances in hyperspectral image classification : earth monitoring with statistical learning methods , '' , vol .",
    "1 , pp . 4554 , 2014 .    j  anthony gualtieri and robert  f cromp , `` support vector machines for hyperspectral remote sensing classification , '' in _ the 27th aipr workshop : advances in computer - assisted recognition_. international society for optics and photonics , 1999 , pp",
    ". 221232 .",
    "m.  fauvel , j.a .",
    "benediktsson , j.  chanussot , and j.r .",
    "sveinsson , `` spectral and spatial classification of hyperspectral data using svms and morphological profiles , '' , vol .",
    "46 , no . 11 , pp . 38043814 , nov 2008 .",
    "mathieu fauvel , yuliya tarabalka , jon  atli benediktsson , jocelyn chanussot , and james  c tilton , `` advances in spectral - spatial classification of hyperspectral images , '' , vol .",
    "3 , pp . 652675 , 2013 .",
    "leyuan fang , shutao li , wuhui duan , jinchang ren , and jn  atli benediktsson , `` classification of hyperspectral images by exploiting spectral  spatial information of superpixel via multiple kernels , '' , vol .",
    "12 , pp . 66636674 , 2015 .",
    "jun li , prashanth reddy  marpu , antonio plaza , jos  m bioucas - dias , and jon atli  benediktsson , `` generalized composite kernel framework for hyperspectral image classification , '' , vol . 51 , no .",
    "9 , pp . 48164829 , 2013 .",
    "gustavo camps - valls , luis gomez - chova , jordi muoz - mar , joan vila - francs , and javier calpe - maravilla , `` composite kernels for hyperspectral image classification , '' , vol .",
    "1 , pp . 9397 , 2006 .",
    "mathieu fauvel , , ph.d .",
    "thesis , institut national polytechnique de grenoble - inpg ; universit dislande , 2007 .",
    "m.  dalla  mura , j.a .",
    "benediktsson , b.  waske , and l.  bruzzone , `` morphological attribute profiles for the analysis of very high resolution images , '' , vol .",
    "10 , pp . 37473762 , oct 2010 .",
    "martino pesaresi and j.a .",
    "benediktsson , `` a new approach for the morphological segmentation of high - resolution satellite imagery , '' , vol .",
    "309320 , feb 2001 .",
    "sebastien lefevre , laetitia chapel , and francois merciol , `` hyperspectral image classification from multiscale description with constrained connectivity and metric learning , '' june 2014 , pp .",
    "jun li , xin huang , paolo gamba , jos  m bioucas - dias , liangpei zhang , jon atli  benediktsson , and antonio plaza , `` multiple feature learning for hyperspectral image classification , '' , vol .",
    "3 , pp . 15921606 , 2015 .",
    "fauvel mathieu , chanussot jocelyn , benediktsson jn  atli , et  al .",
    ", `` kernel principal component analysis for the classification of hyperspectral remote sensing data over urban areas , '' , vol .",
    "2009 , 2009 .",
    "a.  smola , a.  gretton , l.  song , and b.  schlkopf , `` a hilbert space embedding for distributions , '' in _ alt _ , 2007 ,",
    "krikamol muandet , kenji fukumizu , francesco dinuzzo , and bernhard schlkopf , `` learning from distributions via support measure machines , '' in _ advances in neural information processing systems _ , 2012 , pp .",
    "zoltn szab , arthur gretton , barnabs pczos , and bharath sriperumbudur , `` two - stage sampled learning theory on distributions , '' , 2014 .",
    "junier  b oliva , dougal  j sutherland , barnabs pczos , and jeff schneider , `` deep mean maps , '' , 2015 .",
    "michele volpi and devis tuia , `` spatially aware supervised nonlinear dimensionality reduction for hyperspectral data , '' june 2014 , pp .",
    "a.  berlinet and c.  thomas - agnan , , kluwer , 2004 .",
    "b.  sriperumbudur , a.  gretton , k.  fukumizu , g.  lanckriet , and b.  schlkopf , `` hilbert space embeddings and metrics on probability measures , '' , vol . 11 , pp . 15171561 , 2010 .",
    "b.  sriperumbudur , k.  fukumizu , and g.  lanckriet , `` universality , characteristic kernels and rkhs embedding of measures , '' , vol .",
    "12 , pp . 23892410 , 2011 .",
    "a.  rahimi and b.  recht , `` random features for large - scale kernel machines , '' in _ advances in neural information processing systems _ , 2007 , pp",
    ". 11771184 .",
    "seth  r flaxman , yu - xiang wang , and alexander  j smola , `` who supported obama in 2012 ?",
    ": ecological inference through distribution regression , '' in _ proceedings of the 21th acm sigkdd international conference on knowledge discovery and data mining_. acm , 2015 , pp .",
    "289298 .",
    "julien mairal , piotr koniusz , zaid harchaoui , and cordelia schmid , `` convolutional kernel networks , '' in _ advances in neural information processing systems _ , 2014 , pp .",
    "26272635 .",
    "corinna cortes , marius kloft , and mehryar mohri , `` learning kernels using local rademacher complexity , '' in _ advances in neural information processing systems _ , 2013 , pp .",
    "27602768 .",
    "chih - chung chang and chih - jen lin , `` libsvm : a library for support vector machines , '' , vol .",
    "27:127:27 , 2011 , software available at http://www.csie.ntu.edu.tw/  cjlin / libsvm .",
    "jean serra , , academic press , inc . ,",
    "orlando , fl , usa , 1983 .",
    "mathematical morphology operators are non - linear image processes based on the spatial structure of the image .",
    "let @xmath101 be a grey scale image which can be represented by a function .",
    "two basic operators in morphology are the grey - level erosion and the grey - level dilatation whose definition are respectively given by @xcite : @xmath102 where @xmath103 is a structuring function , which introduces the effect of the operators by the geometry of its support as well as the penalizations .",
    "we consider for simplicity uniform structuring functions which are formalised by their support set or shape @xmath104 , called structuring element . by concatenation of these two basic morphological operators",
    "it is possible to obtain more evolved operators such as the opening and the closing @xcite :      these operators remove from @xmath101 all the bright ( opening ) or dark ( closing ) structures where the structuring element @xmath104 can not fit .",
    "however they also modify the value of pixels when @xmath104 fit .",
    "thus to avoid these artefacts it has been proposed in @xcite to use geodesic opening and closing . then by considering a set @xmath106 , @xmath107 , of indexed geodesic openings , and a set @xmath108 , @xmath107 , of indexed geodesic closings where typically , the index @xmath109 is associated to the size of the structuring element .",
    "then thanks to the granulometry axiomatic @xcite we obtain a scale space representation of an image , which allows an image structures decomposition . then the moprhological profile ( mp ) of a grey scale image @xmath101",
    "is defined as @xmath110 dimension vector :      to be able to use the mp on hyperspectral images we first reduce the dimension of the data thanks to pca , and then project the data on a @xmath112 dimensional space which is of smaller dimension than the original space . so a hyperspectral image is represented by @xmath112 grey scale images , then on each of these images we calculate the mp and we concatenate them . hence , the spatial feature space is of dimension @xmath113 .",
    "the formula for the convolutional kernel mean map is defined in equation ( [ kcn1 ] ) by : @xmath114 this equation can be rewritten as : @xmath115 where @xmath116 corresponds to the position of the pixel , whereas @xmath117 is a pixel value and so a spectrum of dimension @xmath20 . for the following formula",
    "we write @xmath118 and @xmath119 respectively the first and the second spatial coordinate , and @xmath120 the @xmath109-th coordinate of the vector @xmath42 .",
    "finally @xmath121 is defined by : @xmath122 then we use the random feature trick on this new vector .      given that @xmath62 an arbitrary probability distribution with variance @xmath63 , a lipschitz continuous function @xmath64 with constant @xmath65 , an arbitrary loss function @xmath66 that is lipschitz continuous in the second argument with constant @xmath67 , it follows that : @xmath68    @xmath123 $ ] + @xmath124",
    "since @xmath54 is lipschitz continuous we have : @xmath125 .",
    "+ thanks to the cauchy - schwarz inequality we have : + @xmath126 since @xmath101 is lipschitz continuous we have : + @xmath127        @xmath129 $ ] + @xmath130 since @xmath54 is lipschitz continuous we have : @xmath131 . +",
    "thanks to the cauchy - schwarz inequality we have : + @xmath132 since @xmath101 is lipschitz continuous we have : + @xmath133      [ [ assessing - the - accuracy - of - the - classification - on - hyperspectral - data ] ] assessing the accuracy of the classification on hyperspectral data ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~    most of the time , when one does a classification on hyperspectral data , we do not face a binary classification .",
    "hence we have to handle a more tricky classification .",
    "to evaluate this classification we use what is called a confusion matrix .",
    "each column of the matrix represents the number of occurrences of an estimated class , while each row represents the number of occurrences of a real class .",
    "if we write @xmath97 the confusion matrix of a classification then @xmath134 is the number of pixels of class @xmath109 assign to the class @xmath135 by the classifier .",
    "let us write @xmath136 the number of class ."
  ],
  "abstract_text": [
    "<S> we propose a novel approach for pixel classification in hyperspectral images , leveraging on both the spatial and spectral information in the data . </S>",
    "<S> the introduced method relies on a recently proposed framework for learning on distributions  by representing them with mean elements in reproducing kernel hilbert spaces ( rkhs ) and formulating a classification algorithm therein . </S>",
    "<S> in particular , we associate each pixel to an empirical distribution of its neighbouring pixels , a judicious representation of which in an rkhs , in conjunction with the spectral information contained in the pixel itself , give a new explicit set of features that can be fed into a suite of standard classification techniques  </S>",
    "<S> we opt for a well established framework of support vector machines ( svm ) . </S>",
    "<S> furthermore , the computational complexity is reduced via random fourier features formalism . </S>",
    "<S> we study the consistency and the convergence rates of the proposed method and the experiments demonstrate strong performance on hyperspectral data with gains in comparison to the state - of - the - art results . </S>",
    "<S> + * keywords : * hyperspectral images , pixelwise classification , kernel methods .    </S>",
    "<S> [ theorem]theorem 3 [ theorem]theorem 1 [ theorem]definition [ theorem]proposition    [ 1][proof]*#1 . *       ' '' '' </S>"
  ]
}