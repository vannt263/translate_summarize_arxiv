{
  "article_text": [
    "the paper chooses to assess the validity of the model based on the marginal likelihood @xmath25 instead of the predictive @xmath26 .",
    "while this has the advantage of  using the data once \" , it suffers from a strong impact of the prior modelling and of not conditioning on the observed data @xmath9 .",
    "a more appropriate ( if still ad - hoc ) procedure is to relate the observed statistics @xmath27 with statistics simulated from @xmath26 , as in , e.g. , @xcite .",
    "it may be argued that checking the prior adequacy is a good thing , but having no way to distinguish between prior and sampling model inadequacy is a difficulty , as seen in the poisson example .    * example*for the location family , @xmath15 , the joint posterior distribution of @xmath28 is @xmath29 and",
    "therefore the difference @xmath30 is not identifiable from the data , solely from the prior(s).@xmath14    note that , from an abc perspective , using @xmath26 instead of @xmath25 does not imply a considerable increase in computing time .",
    "however , computing the bayes factor ( and therefore the evidence ) using the acceptance rate of the abc algorithm is even faster .",
    "moreover , it provides a different answer .",
    "* example*for the poisson @xmath10 model , if we take as an example an exponential @xmath31 prior @xmath32 , the evidence associated with the model is @xmath33 while the quantitative assessment of @xcite is @xmath34 with @xmath35 the numerical comparison of both functions of @xmath9 in figure [ fig : pval ] shows a much slower decrease in @xmath9 for the @xmath36-value than for the evidence , not to mention a frankly puzzling non - monotonicity of the @xmath36-value .",
    "@xmath14     comparison of the decreasing rates of the evidence _",
    "( blue ) _ and of the @xmath36-value _ ( black ) _ derived from @xcite for a poisson model . ]",
    "while the approach by @xcite provides an informal assessment that can be derived in an abc setting , the bayesian foundations of the method may be questioned .",
    "the core of the bayesian approach is to incorporate all aspects of uncertainty and all aspects of decision consequences into a single inferential machine that provides the  optimal \" solution . in the current case , while the consequences of rejecting the current model are not discussed , they would most likely include the construction of another model . in the first graph in the paper ,",
    "several models are contrasted and this leads us to wonder about the gain compared with using the bayes factor , which can be directly derived from the abc simulation as well since the ( accepted or rejected ) proposed values are simulated from @xmath37 .",
    "* example*for the poisson @xmath6 model , running abc with no approximation ( since this is a finite setting ) produces an exact evaluation of the evidence.@xmath14    we also note that the non - parametric evaluation at the basis of the @xmath38 algorithm of @xcite can equally be used for approximating the true marginal density @xmath25 . the smooth version of @xmath38 presented in section s1.5 , eqn .",
    "[ s8 ] , is however far from being a density estimate of @xmath2 since it based on a single realisation from @xmath5 .",
    "it should rather be construed as a ( further ) smoothed version of its smooth abc counterpart and this suggests integreting over @xmath39 as well .",
    "unless some group structure can be exploited to avoid the repetition of simulations @xmath40 , the non - parametric estimator [ s9 ] can not be used as a practical device because either @xmath41 is small , in which case the non - parametric approximation is poor , or @xmath41 is large , in which case producing the @xmath42 s for every value of @xmath1 is too time - consuming . obviously , using moderate @xmath41 is always feasible from a computational point of view and it can also be argued that the approximation of @xmath43 by @xmath44 is not of major interest , since the former is only an approximation to the true target .",
    "( in a vaguely connected way , the rejection sampler of subsection s1.8 does seem an approximation to exact rejection - sampling , in that the choice of the upper bound @xmath45 over the samples simulated in step 1 of the algorithm does not produce a true upper bound . )",
    "the error term @xmath0 is defined as part of the model , based on the marginal , with the additional input of a prior distribution @xmath46 . since @xcite analyse this error based on the product of two densities , @xmath47 , this product is not properly defined from a probabilistic point of view . the authors choose to call @xmath2 a  likelihood \" by a fiducial argument , but this is ( strictly speaking ) not [ proportional to ] a density in @xmath9 .",
    "obviously , simulating from the density that is proportional to @xmath48 is entirely possible as long as this function integrates in @xmath28 against the dominating measure , but it suffers from an undefined probabilistic background in that , for instance , it is not invariant under reparameterisation in @xmath0 : changing @xmath0 to @xmath49 introduces the squared jacobian @xmath50 in the  density \" .",
    "we acknowledge that most abc strategies can be seen as using a formal  prior+likelihood \" representation of the distribution of @xmath0 , since @xmath51 but this formal perspective does not turn @xmath0 into a  true \" parameter and @xmath11 into its prior . for instance , non parametric @xmath11 s may be based on the observations or on additional simulations .",
    "the denomination of  likelihood \" is thus debatable in that @xmath48 can not always be turned into a density on @xmath9 ( or even on a statistic @xmath27 ) .",
    "* example*for the poisson @xmath6 model , @xmath2 is the translated poisson distribution @xmath52 , truncated to positive values .",
    "while this is indeed a distribution on @xmath9 , conditional on @xmath28 , it can not be used as the original poisson distribution , because of the unidentifiability of @xmath0.@xmath14    we also think that comparing models via the (  posterior \" ) distributions of the errors @xmath0 does not provide a coherent setup in that this approach does not incorporate the model complexity penalisation that is at the heart of the bayesian model comparison tools like the bayes factor .",
    "first , a more complex ( e.g. , with more parameters ) model will most likely have a more dispersed distribution on @xmath0 .",
    "second , returning to the first argument of that nore , the choice of the prior @xmath46 ( and of the error @xmath0 itself ) is model dependent ( as stressed in the paper via the notation @xmath53 ) and the comparison thus reflects possibly mostly the prior modelling instead of the data assessment , as shown , again , by the location parameter example . using the same band of rejection for",
    "all models as in figure 1 of @xcite thus does not seem possible nor recommendable on a general basis .",
    "this work was partially supported by the agence nationale de la recherche ( anr , 212 , rue de bercy 75012 paris ) through the 2005 project anr-05-blan-0196 - 01 misgepop and the 2009 project anr-08-blan-0218 bigmc ( for c.p.r . ) .",
    "we are grateful to oliver ratmann for clarifying several points about his paper ."
  ],
  "abstract_text": [
    "<S> the new perspectives on bayesian model criticisms presented in @xcite are challenging standard approaches to bayesian model choice . </S>",
    "<S> we discuss here some issues arising from the approach , including prior influence , model assessment and criticism , and the meaning of error .    </S>",
    "<S> * keywords : * approximate bayesian computation , bayesian statistics , bayesian model choice , bayesian model criticism , bayesian model comparison , computational statistics .    in @xcite , the perception of the approximation error in the abc algorithm @xcite </S>",
    "<S> is radically modified , moving from a computational parameter that is calibrated by the user when balancing precision and computing time into a genuine parameter @xmath0 about which inferences can be made in the same manner as for the original parameter @xmath1 . as stressed in section s2 of @xcite , </S>",
    "<S> this is indeed a change of perception rather than a modification of the abc method in that the target in @xmath1 remains the same . </S>",
    "<S> ( this should not be construed as a criticism in that the unification of most abc representations proposed in section 2 is immensely valuable . ) </S>",
    "<S> although the derivation of the distribution @xmath2 is somewhat convoluted in section s1 , we note here that it is simply the distribution of the error @xmath3 when @xmath4 , i.e.  a projection of @xmath5 in probabilistic terms .    </S>",
    "<S> * example*for a poisson @xmath6 model , a natural divergence is the difference @xmath7 which is distributed as a translated poisson @xmath8 when conditional on @xmath9 and which is marginaly distributed as the difference of two iid @xmath10 variables . since @xmath0 thus is an integer valued variable , the supplementary prior @xmath11 should reflect this feature . </S>",
    "<S> a natural solution is @xmath12 since the series @xmath13 is converging , even though using a proper prior @xmath11 does not appear to be a necessary condition in @xcite.@xmath14    the change of perception in @xcite is based on the underlying assumption that the data is informative about the error term @xmath0 , which is not necessarily the case , as shown by the previous and following examples .    </S>",
    "<S> * example*for a location family , @xmath15 , if we take @xmath7 , the posterior distribution of @xmath0 is @xmath16 and therefore a mostly flat prior @xmath17 with a large support produces a posterior @xmath18 identical to @xmath19 for most values of @xmath9 . </S>",
    "<S> conversely , a highly concentrated prior @xmath19 hardly modifies the posterior @xmath20.@xmath14    * example*for the binomial model @xmath21 , assuming a uniform prior @xmath22 , we can consider @xmath7 , in which case @xmath0 is supported on @xmath23 . if we use a uniform prior on @xmath0 as well , @xmath24 and therefore the ( bayesian ) model brings no information about @xmath0.@xmath14    obviously , this example is not directly incriminating against the method of @xcite , in that it only considers a single statistic , instead of several as in @xcite ( which distinguishes this paper from the remainder of the literature , where @xmath0 is a single number ) . </S>"
  ]
}