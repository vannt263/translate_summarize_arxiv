{
  "article_text": [
    "information measures of random variables are used in several fields .",
    "the shannon entropy @xcite is one of the famous measures of uncertainty for a given random variable . on the studies of information measures , inequalities for information measures",
    "are commonly used in many applications . as an instance , fano s inequality @xcite gives the tight upper bound of the conditional shannon entropy with a fixed error probability .",
    "then , note that the _ tight _ means the existence of the distribution which attains the equality of the bound .",
    "later , the reverse of fano s inequality , i.e. , the tight lower bound of the conditional shannon entropy with a fixed error probability , are established @xcite . on the other hand , harremos and",
    "topse @xcite derived the exact range between the shannon entropy and the index of coincidence ( or the simpson index ) for all @xmath1-ary probability vectors , @xmath4 . in the above studies , note that the error probability and the index of coincidence are closely related to @xmath5-norm and @xmath6-norm , respectively .",
    "similarly , several axiomatic definitions of the entropies @xcite are also related to the @xmath0-norm .",
    "furthermore , the @xmath0-norm are also related to some diversity indices , such as the index of coincidence .    in this study",
    ", we examine extremal relations between the shannon entropy and the @xmath0-norm for @xmath1-ary probability vectors , @xmath2 .",
    "more precisely , we establish the tight bounds of @xmath0-norm with a fixed shannon entropy in theorem [ th : extremes ] . similarly , we also derive the tight bounds of the shannon entropy with a fixed @xmath0-norm in theorem [ th : extremes2 ] . directly extending theorem [ th : extremes ] to corollary [ cor : extremes ]",
    ", we can obtain the tight bounds of several information measures which are determined by the @xmath0-norm with a fixed shannon entropy , as shown in table [ table : extremes ] .",
    "in particular , we illustrate the exact feasible regions between the shannon entropy and the rnyi entropy in fig .",
    "[ fig : renyi ] by using and . in section [ subsect : focusing ] , we consider applications of corollary [ cor : extremes ] for a particular class of discrete memoryless channels , defined in definition [ def : focusing ] , which is called _ uniformly focusing _",
    "@xcite or _ uniform from the output _ @xcite .",
    "let the set of all @xmath1-ary probability vectors be denoted by @xmath7 for an integer @xmath2 . for @xmath8 , let @xmath9 } \\ge p_{[2 ] } \\ge \\dots \\ge p_{[n]}\\end{aligned}\\ ] ] denote the components of @xmath10 in decreasing order , and let @xmath11 } , p_{[2 ] } , \\dots , p_{[n ] } ) \\label{def : rearrangement}\\end{aligned}\\ ] ] denote the decreasing rearrangement of @xmath10 .",
    "in particular , we define the following two @xmath1-ary probability vectors : ( i ) an @xmath1-ary deterministic distribution @xmath12 is defined by @xmath13 and @xmath14 for @xmath15 and ( ii ) the @xmath1-ary equiprobable distribution @xmath16 is defined by @xmath17 for @xmath18 .    for an @xmath1-ary random variable @xmath19 , we define the shannon entropy @xcite as @xmath20 where @xmath21 denotes the natural logarithm and assume that @xmath22 .",
    "moreover , we define the @xmath0-norm of @xmath23 as @xmath24 for @xmath25 .",
    "note that @xmath26 for @xmath23 . on the works of extending shannon entropy ,",
    "the @xmath0-norm is appear in the several information measures .",
    "as an instance , rnyi @xcite generalized the shannon entropy axiomatically to the rnyi entropy of order @xmath27 , defined as @xmath28 for @xmath19 .",
    "note that it is usually defined that @xmath29 since @xmath30 by lhpital s rule .",
    "in other axiomatic definitions of entropies @xcite , we can also define them by using the @xmath0-norm , as with .    in this study",
    ", we analyze relations between @xmath31 and @xmath32 to examine relationships between the shannon entropy and several information measures .",
    "note that @xmath31 and @xmath32 are invariant for any permutation of the indices of @xmath23 ; that is , @xmath33 for any @xmath23 .",
    "hence , we only consider @xmath34 for @xmath23 in the analyses of the study . since @xmath35 for any @xmath23",
    ", we have no interest in the case @xmath36 ; hence , we omit the case @xmath36 in this study . furthermore , since @xmath37 the cases @xmath38 and",
    "@xmath39 are trivial ; thus , we also omit these cases in the analyses of this study .      for a fixed @xmath2 , let the @xmath1-ary distribution @xmath42 be defined by @xmath43 for @xmath44 $ ] , and let the @xmath1-ary distribution is similar to the definition of ( * ? ? ?",
    "* eq . ( 26 ) ) . ]",
    "@xmath45 be defined by @xmath46 for @xmath47 $ ] , where @xmath48 denotes the floor function .",
    "note that @xmath49 for @xmath50 $ ] . in this subsection",
    ", we examine the properties of the shannon entropies and the @xmath0-norms for @xmath40 and @xmath41 . for simplicity , we define @xmath51 then , we first show the monotonicity of @xmath52 with respect to @xmath53 $ ] in the following lemma .",
    "[ lem : hv ] @xmath52 is strictly increasing for @xmath53 $ ] .",
    "it is easy to see that @xmath54 then , the first - order derivative of @xmath52 with respect to @xmath55 is @xmath56 since @xmath57 for @xmath58 , it follows from that @xmath59 for @xmath58 .",
    "note that @xmath52 is continuous for @xmath53 $ ] since @xmath60 and @xmath61 by the assumption @xmath22 .",
    "therefore , @xmath52 is strictly increasing for @xmath53 $ ] .",
    "lemma [ lem : hv ] implies the existence of the inverse function of @xmath52 for @xmath53 $ ] .",
    "we second show the monotonicity of @xmath62 with respect to @xmath47 $ ] as follows :    [ lem : hw ] @xmath62 is strictly decreasing for @xmath47 $ ] .    for an integer",
    "@xmath63 $ ] , assume that @xmath64 $ ]",
    ". then , note that @xmath65 .",
    "it is easy to see that @xmath66 where ( a ) follows by the assumption @xmath22 .",
    "then , the first - order derivative of @xmath62 with respect to @xmath55 is @xmath67 since @xmath68 for @xmath69 , it follows from that @xmath70 for @xmath69 . on the other hand",
    ", we observe that @xmath71 for an integer @xmath72 $ ] and @xmath73 for an integer @xmath63 $ ] .",
    "note that @xmath74 from and the assumption @xmath22 .",
    "hence , for any integer @xmath75 $ ] , we get that @xmath76 which imply that @xmath62 is continuous for @xmath47 $ ] .",
    "therefore , @xmath62 is strictly decreasing for @xmath47 $ ] .    as with lemma [ lem : hv ] , lemma [",
    "lem : hw ] also implies the existence of the inverse function of @xmath62 for @xmath47 $ ] . since @xmath77 , @xmath78 , @xmath79 , and @xmath80",
    ", we can denote the inverse functions of @xmath52 and @xmath62 with respect to @xmath55 as follows : we denote by @xmath81 \\to [ 0 , \\frac{1}{n}]$ ] the inverse function of @xmath52 for @xmath53 $ ] .",
    "moreover , we also denote by @xmath82 \\to [ \\frac{1}{n } , 1]$ ] the inverse function of @xmath62 for @xmath47 $ ] .",
    "now , we provide the monotonicity of @xmath83 with respect to @xmath52 in the following lemma .",
    "[ lem : mono_v ] for any fixed @xmath2 and any fixed @xmath84 , if @xmath53 $ ] , the following monotonicity hold :    * if @xmath85 , then @xmath83 is strictly decreasing for @xmath86 $ ] and * if @xmath87 , then @xmath83 is strictly increasing for @xmath86 $ ] .",
    "the proof of lemma [ lem : mono_v ] is given in a similar manner with ( * ? ? ?",
    "* appendix i ) . by the chain rule of the derivation and the inverse function theorem",
    ", we have @xmath88 direct calculation shows @xmath89 substituting and into , we obtain @xmath90 we now define the sign function as @xmath91 since @xmath92 for @xmath58 , we observe that @xmath93 for @xmath58 and @xmath94 ; and therefore , we have @xmath95 for @xmath58 and @xmath94 , which implies lemma [ lem : mono_v ] .",
    "it follows from lemmas [ lem : hv ] and [ lem : mono_v ] that , for each @xmath27 , @xmath83 is bijective for @xmath53 $ ] .",
    "similarly , we also show the monotonicity of @xmath96 with respect to @xmath62 in the following lemma .",
    "[ lem : mono_w ] for any fixed @xmath2 and any fixed @xmath27 , if @xmath47 $ ] , the following monotonicity hold :    * if @xmath85 , then @xmath96 is strictly decreasing for @xmath97 $ ] and * if @xmath87 , then @xmath96 is strictly increasing for @xmath97 $ ] .    since @xmath98 for",
    "@xmath50 $ ] , we can obtain immediately from that @xmath99 for @xmath100 . since @xmath101 for @xmath100",
    ", we observe that @xmath102 for @xmath100 and @xmath94 ; and therefore , we have @xmath103 for @xmath100 and @xmath94 , as with .",
    "hence , for @xmath94 , we have that    * if @xmath85 , then @xmath96 is strictly decreasing for @xmath104 $ ] and * if @xmath87 , then @xmath96 is strictly increasing for @xmath104 $ ] .",
    "finally , since @xmath105 and @xmath106 for any integer @xmath75 $ ] , any @xmath64 $ ] , and any @xmath107 , we can obtain that    * if @xmath85 , then @xmath96 is strictly decreasing for @xmath108 $ ] and * if @xmath87 , then @xmath96 is strictly increasing for @xmath108 $ ]    for any integer @xmath63 $ ] and any @xmath27 .",
    "this completes the proof of lemma [ lem : mono_w ] .",
    "it also follows from lemmas [ lem : hw ] and [ lem : mono_w ] that , for each @xmath27 , @xmath96 is also bijective for @xmath47 $ ] .",
    "0    [ def : inversen ] we denote by @xmath109 \\to [ 0 , \\frac{1}{n}]$ ] the inverse function of @xmath83 for @xmath53 $ ] .",
    "moreover , we also denote by @xmath110 \\to [ \\frac{1}{n } , 1]$ ] the inverse function of @xmath96 for @xmath47 $ ] .",
    "in section [ subsect : extremes ] , we examine the extremal relations between the shannon entropy and the @xmath0-norm , as shown in theorems [ th : extremes ] and [ th : extremes2 ] .",
    "then , we can identify the exact feasible region of @xmath111 for any @xmath2 and any @xmath27 . extending theorems [ th : extremes ] and [",
    "th : extremes2 ] to corollary [ cor : extremes ] , we can obtain the tight bounds between the shannon entropy and several information measures which are determined by the @xmath0-norm , as shown in table [ table : extremes ] . in section [ subsect : focusing ] , we apply the results of section [ subsect : extremes ] to uniformly focusing channels of definition [ def : focusing ] .      let the @xmath112-logarithm function @xcite be denoted by @xmath113 for @xmath114 and @xmath115 ; besides , since @xmath116 by lhpital s rule , it is defined that @xmath117 . for the @xmath112-logarithm function , we can see the following lemma .",
    "[ lem : frac_qlog ] for @xmath118 and @xmath119 @xmath120 , we observe that @xmath121 with equality if and only if @xmath122 .",
    "for @xmath119 @xmath120 , we consider the monotonicity of @xmath123 with respect to @xmath112 .",
    "direct calculation shows @xmath124 then , we can see that @xmath125 where    * the equality ( a ) follows from the fact that @xmath126 for @xmath127 and @xmath128 , * the equality ( b ) follows from the fact that @xmath129 for @xmath130 and @xmath131 , and * the equality ( c ) follows by the change of variables : @xmath132 and @xmath133 .",
    "then , it can be easily seen that @xmath134 thus , to check the sign of @xmath135 , we now examine the function @xmath136 .",
    "we readily see that @xmath137 for @xmath138 .",
    "we calculate the second order derivative of @xmath136 with respect to @xmath139 as follows : @xmath140 hence , we observe that @xmath141 for @xmath142 , which implies that    * if @xmath143 , then @xmath136 is strictly concave in @xmath142 and * if @xmath144 , then @xmath136 is strictly convex in @xmath142 .    therefore , it follows from that    * it @xmath143 , then @xmath145 and * it @xmath144 , then @xmath146    since @xmath147 and @xmath148 , note that    * if @xmath85 , then @xmath149 for @xmath150 and * if @xmath87 , then @xmath151 for @xmath150 .    hence , we obtain @xmath152 for @xmath130 and @xmath150 . concluding the above analyses , we have @xmath153 for @xmath154 , where the last equality follows from and .",
    "note that @xmath155 for @xmath156 , which implies that @xmath123 is continuous at @xmath36 . therefore , we have that , if @xmath157 , then @xmath123 is strictly increasing for @xmath130 , which implies lemma [ lem : frac_qlog ] .",
    "the following two lemmas have important roles in the proving theorem [ th : extremes ] .",
    "[ lem : vector_v ] for any @xmath2 and any @xmath23 , there exists @xmath53 $ ] such that @xmath158 and @xmath159 for all @xmath25 .",
    "if @xmath160 , then it can be easily seen that @xmath161 for any @xmath162 and some @xmath163 $ ] ; therefore , the lemma obviously holds when @xmath160 . moreover , since @xmath164 the lemma obviously holds if @xmath165 .",
    "thus , we omit the cases @xmath160 and @xmath165 in the analyses and consider @xmath166 for @xmath167 . for a fixed @xmath4 and a constant @xmath168 , we assume for @xmath166 that @xmath169 for that @xmath10 , let @xmath170 be the index such that @xmath171 } > p_{[k+1 ] } = p_{[n]}$ ] ; namely , the index @xmath172 is chosen to satisfy the following inequalities : @xmath9 } \\ge p_{[2 ] } \\ge \\dots \\ge p_{[k-1 ] } \\ge p_{[k ] } \\ge p_{[k+1 ] } = p_{[k+2 ] } = \\dots = p_{[n ] } \\qquad ( p_{[k-1 ] } > p_{[k+1 ] } ) .",
    "\\label{eq : equal_k+1_to_n}\\end{aligned}\\ ] ] since @xmath173 , we observe that @xmath174 } } \\left ( \\sum_{i=1}^{n } p_{i } \\right ) & = \\frac { \\mathrm{d } } { \\mathrm{d } p_{[k ] } } ( 1 ) \\\\ & \\iff & \\frac { \\mathrm{d } } { \\mathrm{d } p_{[k ] } } \\left ( \\sum_{i=1}^{n } p_{[i ] } \\right ) & = 0 \\\\ & \\iff & \\frac { \\mathrm{d } p_{[k ] } } { \\mathrm{d } p_{[k ] } } + \\sum_{i = 1 : i \\neq k}^{n } \\frac { \\mathrm{d } p_{[i ] } } { \\mathrm{d } p_{[k ] } } & = 0 \\\\ & \\iff & 1 + \\sum_{i = 1 : i \\neq k}^{n } \\frac",
    "{ \\mathrm{d } p_{[i ] } } { \\mathrm{d } p_{[k ] } } & = 0 \\\\ & \\iff & \\sum_{i = 1 : i \\neq k}^{n } \\frac { \\mathrm{d } p_{[i ] } } { \\mathrm{d } p_{[k ] } } & = - 1 .",
    "\\label{eq : total_diff_prob}\\end{aligned}\\ ] ] in this proof , we further assume that @xmath175 } } { \\mathrm{d } p_{[k ] } } = 0 \\label{eq : hypo1}\\end{aligned}\\ ] ] for @xmath176 and @xmath177 } } { \\mathrm{d } p_{[k ] } } = \\frac { \\mathrm{d } p_{[n ] } } { \\mathrm{d } p_{[k ] } } \\label{eq : hypo2}\\end{aligned}\\ ] ] for @xmath178 . by constraints and , we get @xmath179 } } { \\mathrm{d } p_{[k ] } } & = - 1 \\\\ & \\iff & \\sum_{i = 1}^{k-1 } \\frac { \\mathrm{d } p_{[i ] } } { \\mathrm{d } p_{[k ] } } + \\sum_{j = k+1}^{n } \\frac { \\mathrm{d } p_{[j ] } } { \\mathrm{d } p_{[k ] } } & = - 1 \\\\ & \\overset{\\eqref{eq : hypo1}}{\\iff } & \\frac { \\mathrm{d } p_{[1 ] } } { \\mathrm{d } p_{[k ] } } + \\sum_{j = k+1}^{n } \\frac { \\mathrm{d } p_{[j ] } } { \\mathrm{d } p_{[k ] } } & = -1 \\\\ & \\overset{\\eqref{eq : hypo2}}{\\iff } & \\frac { \\mathrm{d } p_{[1 ] } } { \\mathrm{d } p_{[k ] } } + ( n - k ) \\frac { \\mathrm{d } p_{[n ] } } { \\mathrm{d } p_{[k ] } } & = -1 \\\\ & \\iff & \\frac { \\mathrm{d } p_{[1 ] } } { \\mathrm{d } p_{[k ] } } & = - 1 - ( n - k ) \\frac { \\mathrm{d } p_{[n ] } } { \\mathrm{d } p_{[k ] } } .",
    "\\label{eq : total_prob_hypo}\\end{aligned}\\ ] ] moreover , since @xmath180 , we observe that @xmath181 } } \\left ( - \\sum_{i = 1}^{n } p_{i } \\ln p_{i } \\right ) & = \\frac { \\mathrm{d } } { \\mathrm{d } p_{[k ] } } ( a ) \\\\ & \\iff & \\frac { \\mathrm{d } } { \\mathrm{d } p_{[k ] } } \\left ( - \\sum_{i = 1}^{n } p_{[i ] } \\ln p_{[i ] } \\right ) & = 0 \\\\ & \\iff & - \\sum_{i = 1}^{n } \\frac { \\mathrm{d } } { \\mathrm{d } p_{[k ] } } ( p_{[i ] } \\ln p_{[i ] } ) & = 0 \\\\ & \\iff & - \\frac { \\mathrm{d } } { \\mathrm{d } p_{[k ] } } ( p_{[k ] } \\ln p_{[k ] } ) - \\sum_{i = 1 : i \\neq k}^{n } \\frac { \\mathrm{d } } { \\mathrm{d } p_{[k ] } } ( p_{[i ] } \\ln p_{[i ] } ) & = 0 \\\\ & \\iff & - ( \\ln p_{[k ] } + 1 ) - \\sum_{i = 1 : i \\neq k}^{n } \\frac { \\mathrm{d } } { \\mathrm{d } p_{[k ] } } ( p_{[i ] } \\ln p_{[i ] } ) & = 0 \\\\ & \\iff & - \\sum_{i = 1 : i \\neq k}^{n } \\frac { \\mathrm{d } } { \\mathrm{d } p_{[k ] } } ( p_{[i ] } \\ln p_{[i ] } ) & = \\ln p_{[k ] } + 1 \\\\ & \\overset{\\text{(a)}}{\\iff } & - \\sum_{i = 1 : i \\neq k}^{n } \\left ( \\frac { \\mathrm{d } p_{[i ] } } { \\mathrm{d } p_{[k ] } } \\right ) \\left ( \\frac { \\mathrm{d } } { \\mathrm{d } p_{[i ] } } ( p_{[i ] } \\ln p_{[i ] } ) \\right ) & = \\ln p_{[k ] } + 1 \\\\ & \\iff & - \\sum_{i =",
    "1 : i \\neq k}^{n } \\left ( \\frac { \\mathrm{d } p_{[i ] } } { \\mathrm{d } p_{[k ] } } \\right ) ( \\ln p_{[i ] } + 1 ) & = \\ln p_{[k ] } + 1 \\label{eq : diff1_h_halfway } \\\\ & \\iff & - \\sum_{i = 1}^{k-1 } \\left ( \\frac { \\mathrm{d } p_{[i ] } } { \\mathrm{d } p_{[k ] } } \\right ) ( \\ln p_{[i ] } + 1 ) - \\sum_{j = k+1}^{n } \\left ( \\frac { \\mathrm{d } p_{[j ] } } { \\mathrm{d } p_{[k ] } } \\right ) ( \\ln p_{[j ] } + 1 ) & = \\ln p_{[k ] } + 1 \\\\ & \\overset{\\eqref{eq : equal_k+1_to_n}}{\\iff } & - \\sum_{i = 1}^{k-1 } \\left ( \\frac { \\mathrm{d } p_{[i ] } } { \\mathrm{d } p_{[k ] } } \\right ) ( \\ln p_{[i ] } + 1 ) - ( \\ln p_{[n ] } + 1 ) \\sum_{j = k+1}^{n } \\left ( \\frac { \\mathrm{d } p_{[j ] } } { \\mathrm{d } p_{[k ] } } \\right ) & = \\ln p_{[k ] } + 1 \\\\ & \\overset{\\eqref{eq : hypo1}}{\\iff } & - \\left ( \\frac { \\mathrm{d } p_{[1 ] } } { \\mathrm{d } p_{[k ] } } \\right ) ( \\ln p_{[1 ] } + 1 ) - ( \\ln p_{[n ] } + 1 ) \\sum_{j = k+1}^{n } \\left ( \\frac { \\mathrm{d } p_{[j ] } } { \\mathrm{d } p_{[k ] } } \\right ) & = \\ln p_{[k ] } + 1 \\\\ & \\overset{\\eqref{eq : hypo2}}{\\iff } & - \\left ( \\frac { \\mathrm{d } p_{[1 ] } } { \\mathrm{d } p_{[k ] } } \\right ) ( \\ln p_{[1 ] } + 1 ) - ( \\ln p_{[n ] } + 1 ) ( n - k ) \\left ( \\frac { \\mathrm{d } p_{[n ] } } { \\mathrm{d } p_{[k ] } } \\right ) & = \\ln p_{[k ] } + 1 \\\\ & \\overset{\\eqref{eq : total_prob_hypo}}{\\iff } & - \\left ( - 1 - ( n - k ) \\frac { \\mathrm{d } p_{[n ] } } { \\mathrm{d } p_{[k ] } } \\right ) ( \\ln p_{[1 ] } + 1 ) - ( n - k ) \\left ( \\frac { \\mathrm{d } p_{[n ] } } { \\mathrm{d } p_{[k ] } } \\right ) ( \\ln p_{[n ] } + 1 ) & = \\ln p_{[k ] } + 1 \\\\ & \\iff & \\ ! \\ ! \\ ! \\ ! \\ ! ( \\ln p_{[1 ] } + 1 ) + ( n - k ) \\left ( \\frac { \\mathrm{d } p_{[n ] } } { \\mathrm{d } p_{[k ] } } \\right ) ( \\ln p_{[1 ] } + 1 ) - ( n - k ) \\left ( \\frac { \\mathrm{d } p_{[n ] } } { \\mathrm{d } p_{[k ] } } \\right ) ( \\ln p_{[n ] } + 1 ) & = \\ln p_{[k ] } + 1 \\\\ & \\iff & ( n - k ) \\left ( \\frac { \\mathrm{d } p_{[n ] } } { \\mathrm{d } p_{[k ] } } \\right ) ( \\ln p_{[1 ] } + 1 ) - ( n - k ) \\left ( \\frac { \\mathrm{d } p_{[n ] } } { \\mathrm{d } p_{[k ] } } \\right ) ( \\ln p_{[n ] } + 1 ) & = \\ln p_{[k ] } - \\ln p_{[1 ] } \\\\ & \\iff & ( n - k ) \\left ( \\frac { \\mathrm{d } p_{[n ] } } { \\mathrm{d } p_{[k ] } } \\right ) ( \\ln p_{[1 ] } - \\ln p_{[n ] } ) & = \\ln p_{[k ] } - \\ln p_{[1 ] } \\\\ & \\iff & ( n - k ) \\left ( \\frac { \\mathrm{d } p_{[n ] } } { \\mathrm{d } p_{[k ] } } \\right ) & = \\frac { \\ln p_{[k ] } - \\ln p_{[1 ] } } { \\ln p_{[1 ] } - \\ln p_{[n ] } } \\\\",
    "& \\iff & \\frac { \\mathrm{d } p_{[n ] } } { \\mathrm{d } p_{[k ] } } & = - \\frac { 1 } { n - k } \\left ( \\frac { \\ln p_{[1 ] } - \\ln p_{[k ] } } { \\ln p_{[1 ] } - \\ln p_{[n ] } } \\right ) \\ !",
    "\\label{eq : total_entropy_hypo}\\end{aligned}\\ ] ] where the equivalence ( a ) follows by the chain rule .",
    "we now check the sign of the right - hand side of .",
    "if @xmath182 } > p_{[k ] } \\ge p_{[n ] } > 0 $ ] , then @xmath183 } - \\ln p_{[k ] } } { \\ln p_{[1 ] } - \\ln p_{[n ] } } < 1 \\label{eq : cond1}\\end{aligned}\\ ] ] since @xmath184 } > \\ln p_{[k ] } > \\ln p_{[n ] } $ ] ; therefore , we get from that @xmath185 } } { \\mathrm{d } p_{[k ] } } < 0 \\label{eq : sign_dndk_1}\\end{aligned}\\ ] ] for @xmath182 } > p_{[k ] } > p_{[n ] } > 0 $ ] .",
    "note that @xmath186 .",
    "moreover , if @xmath182 } = p_{[k ] } > p_{[n ] } > 0 $ ] , then @xmath187 } } { \\mathrm{d } p_{[k ] } } & = - \\frac { 1 } { n - k } \\left ( \\frac { \\ln p_{[1 ] } - \\ln p_{[k ] } } { \\ln p_{[1 ] } - \\ln p_{[n ] } } \\right ) \\\\ & = - \\frac { 1 } { n - k } \\left ( \\frac { 0 } { \\ln p_{[1 ] } - \\ln p_{[n ] } } \\right ) \\\\ & = 0 .",
    "\\label{eq : sign_dndk_2}\\end{aligned}\\ ] ] furthermore , if @xmath182 } > p_{[k ] } = p_{[n ] } > 0 $ ] , then @xmath187 } } { \\mathrm{d } p_{[k ] } } & = - \\frac { 1 } { n - k } \\left ( \\frac { \\ln p_{[1 ] } - \\ln p_{[k ] } } { \\ln p_{[1 ] } - \\ln p_{[n ] } } \\right ) \\\\ & = - \\frac { 1 } { n - k } .",
    "\\label{eq : sign_dndk_3}\\end{aligned}\\ ] ] combining , , and , we get under the constraints , , , and that @xmath188 } } { \\mathrm{d } p_{[k ] } } \\right ) = \\begin{cases } 0 & \\mathrm{if } \\ p_{[1 ] } = p_{[k ] } , \\\\",
    "-1 & \\mathrm{otherwise } \\end{cases } \\label{eq : sign_dndk}\\end{aligned}\\ ] ] for @xmath182 } \\ge p_{[k ] } \\ge p_{[n ] } > 0 \\ ( p_{[1 ] } > p_{[n]})$ ] .",
    "note for the constraint that @xmath189 } , p_{[k+2 ] } , \\dots , p_{[n ] } ) \\to ( 0^{+ } , 0^{+ } , \\dots , 0^{+ } ) } h ( p_{[1 ] } , p_{[2 ] } , \\dots , p_{[n ] } ) = h ( p_{[1 ] } , p_{[2 ] } , \\dots , p_{[k ] } , 0 , 0 , \\dots , 0 ) \\end{aligned}\\ ] ] since @xmath190 by the assumption @xmath22 .",
    "thus , it follows from that , for all @xmath191 , @xmath192}$ ] is strictly decreasing for @xmath193}$ ] under the constraints , , , and .",
    "similarly , we check the sign of the right - hand side of : @xmath194 } } { \\mathrm{d } p_{[k ] } } & = - 1 - ( n - k ) \\frac { \\mathrm{d } p_{[n ] } } { \\mathrm{d } p_{[k ] } } .\\end{aligned}\\ ] ] by , , and , we can see that @xmath195 } } { \\mathrm{d } p_{[k ] } } < 0\\end{aligned}\\ ] ] for @xmath182 } \\ge p_{[k ] } > p_{[n ] } > 0 $ ] and @xmath194 } } { \\mathrm{d } p_{[k ] } } & = 0\\end{aligned}\\ ] ] for @xmath182 } > p_{[k ] } = p_{[n ] } > 0 $ ] ; therefore , we also get under the constraints , , , and that @xmath196 } } { \\mathrm{d } p_{[k ] } } \\right ) = \\begin{cases } 0 & \\mathrm{if } \\ p_{[k ] } = p_{[n ] } , \\\\",
    "-1 & \\mathrm{otherwise } \\end{cases } \\label{eq : sign_d1dk}\\end{aligned}\\ ] ] for @xmath182 } \\ge p_{[k ] } \\ge p_{[n ] } > 0 \\ ( p_{[1 ] } > p_{[n]})$ ] .",
    "as with , it follows from that @xmath197}$ ] is strictly decreasing for @xmath193}$ ] under the constraints , , , and .",
    "on the other hand , for a fixed @xmath128 , we have @xmath198 } } & = \\frac { \\mathrm{d } } { \\mathrm{d } p_{[k ] } } \\left ( \\sum_{i=1}^{n } p_{i}^{\\alpha } \\right)^{\\frac{1}{\\alpha } } \\\\ & = \\frac{1}{\\alpha } \\left ( \\sum_{i=1}^{n } p_{i}^{\\alpha } \\right)^{\\frac{1}{\\alpha } - 1 } \\left ( \\frac { \\mathrm{d } } { \\mathrm{d } p_{[k ] } } \\sum_{i=1}^{n } p_{i}^{\\alpha } \\right ) \\\\ & = \\frac{1}{\\alpha } \\left ( \\sum_{i=1}^{n } p_{i}^{\\alpha } \\right)^{\\frac{1}{\\alpha } - 1 } \\left ( \\frac { \\mathrm{d } } { \\mathrm{d } p_{[k ] } } \\sum_{i=1}^{n } p_{[i]}^{\\alpha } \\right ) \\\\ & = \\frac{1}{\\alpha } \\left ( \\sum_{i=1}^{n } p_{i}^{\\alpha } \\right)^{\\frac{1}{\\alpha } - 1 } \\left ( \\sum_{i=1}^{n } \\frac { \\mathrm{d } } { \\mathrm{d } p_{[k ] } } ( p_{[i]}^{\\alpha } ) \\right ) \\\\ & = \\frac{1}{\\alpha } \\left ( \\sum_{i=1}^{n } p_{i}^{\\alpha } \\right)^{\\frac{1}{\\alpha } - 1 } \\left ( \\frac { \\mathrm{d } } { \\mathrm{d } p_{[k ] } } ( p_{[k]}^{\\alpha } ) + \\sum_{i=1 : i \\neq k}^{n } \\frac { \\mathrm{d } } { \\mathrm{d } p_{[k ] } } ( p_{[i]}^{\\alpha } ) \\right ) \\\\ & = \\frac{1}{\\alpha } \\left ( \\sum_{i=1}^{n } p_{i}^{\\alpha } \\right)^{\\frac{1}{\\alpha } - 1 } \\left ( \\alpha \\ , p_{[k]}^{\\alpha-1 } + \\sum_{i=1 : i \\neq k}^{n } \\frac { \\mathrm{d } } { \\mathrm{d } p_{[k ] } } ( p_{[i]}^{\\alpha } ) \\right ) \\label{eq : diff_norm_pk_halfway } \\\\ & = \\frac{1}{\\alpha } \\left ( \\sum_{i=1}^{n } p_{i}^{\\alpha } \\right)^{\\frac{1}{\\alpha } - 1 } \\left ( \\alpha \\ , p_{[k]}^{\\alpha-1 } + \\sum_{i=1 : i \\neq k}^{n } \\left ( \\frac { \\mathrm{d } p_{[i ] } } { \\mathrm{d } p_{[k ] } } \\right ) \\left ( \\frac { \\mathrm{d } } { \\mathrm{d } p_{[i ] } } ( p_{[i]}^{\\alpha } ) \\right ) \\right ) \\\\ & = \\frac{1}{\\alpha } \\left ( \\sum_{i=1}^{n } p_{i}^{\\alpha } \\right)^{\\frac{1}{\\alpha } - 1 } \\left ( \\alpha \\ , p_{[k]}^{\\alpha-1 } + \\sum_{i=1 : i \\neq k}^{n } \\left ( \\frac { \\mathrm{d } p_{[i ] } } { \\mathrm{d } p_{[k ] } } \\right ) ( \\alpha \\ , p_{[i]}^{\\alpha-1 } ) \\right ) \\\\ & = \\left ( \\sum_{i=1}^{n } p_{i}^{\\alpha } \\right)^{\\frac{1}{\\alpha } - 1 } \\left ( p_{[k]}^{\\alpha-1 } + \\sum_{i=1 : i \\neq k}^{n } \\left ( \\frac { \\mathrm{d } p_{[i ] } } { \\mathrm{d } p_{[k ] } } \\right ) ( p_{[i]}^{\\alpha-1 } ) \\right ) \\\\ & = \\left ( \\sum_{i=1}^{n } p_{i}^{\\alpha } \\right)^{\\frac{1}{\\alpha } - 1 } \\left ( p_{[k]}^{\\alpha-1 } + \\sum_{i=1}^{k-1 } \\left ( \\frac { \\mathrm{d } p_{[i ] } } { \\mathrm{d } p_{[k ] } } \\right ) ( p_{[i]}^{\\alpha-1 } ) + \\sum_{j = k+1}^{n } \\left ( \\frac { \\mathrm{d } p_{[j ] } } { \\mathrm{d } p_{[k ] } } \\right ) ( p_{[j]}^{\\alpha-1 } ) \\right ) \\\\ & \\overset{\\eqref{eq : equal_k+1_to_n}}{= } \\left ( \\sum_{i=1}^{n } p_{i}^{\\alpha } \\right)^{\\frac{1}{\\alpha } - 1 } \\left ( p_{[k]}^{\\alpha-1 } + \\sum_{i=1}^{k-1 } \\left ( \\frac { \\mathrm{d } p_{[i ] } } { \\mathrm{d } p_{[k ] } } \\right ) ( p_{[i]}^{\\alpha-1 } ) + ( p_{[n]}^{\\alpha-1 } ) \\sum_{j = k+1}^{n } \\left ( \\frac { \\mathrm{d } p_{[j ] } } { \\mathrm{d } p_{[k ] } } \\right ) \\right ) \\\\ & \\overset{\\eqref{eq : hypo1}}{= } \\left ( \\sum_{i=1}^{n } p_{i}^{\\alpha } \\right)^{\\frac{1}{\\alpha } - 1 } \\left ( p_{[k]}^{\\alpha-1 } + \\left ( \\frac { \\mathrm{d } p_{[1 ] } } { \\mathrm{d } p_{[k ] } } \\right ) ( p_{[1]}^{\\alpha-1 } ) + ( p_{[n]}^{\\alpha-1 } ) \\sum_{j = k+1}^{n } \\left ( \\frac { \\mathrm{d } p_{[j ] } } { \\mathrm{d } p_{[k ] } }",
    "\\right ) \\right ) \\\\ & \\overset{\\eqref{eq : hypo2}}{= } \\left ( \\sum_{i=1}^{n } p_{i}^{\\alpha } \\right)^{\\frac{1}{\\alpha } - 1 } \\left ( p_{[k]}^{\\alpha-1 } + \\left ( \\frac { \\mathrm{d } p_{[1 ] } } { \\mathrm{d } p_{[k ] } } \\right ) ( p_{[1]}^{\\alpha-1 } ) + ( p_{[n]}^{\\alpha-1 } ) ( n - k ) \\left ( \\frac { \\mathrm{d } p_{[n ] } } { \\mathrm{d } p_{[k ] } } \\right ) \\right ) \\\\ & \\overset{\\eqref{eq : total_prob_hypo}}{= } \\left ( \\sum_{i=1}^{n } p_{i}^{\\alpha } \\right)^{\\frac{1}{\\alpha } - 1 } \\left ( p_{[k]}^{\\alpha-1 } + \\left ( - 1 - ( n - k ) \\frac { \\mathrm{d } p_{[n ] } } { \\mathrm{d } p_{[k ] } } \\right ) ( p_{[1]}^{\\alpha-1 } ) + ( n - k ) \\left ( \\frac { \\mathrm{d } p_{[n ] } } { \\mathrm{d } p_{[k ] } } \\right ) ( p_{[n]}^{\\alpha-1 } ) \\right ) \\\\ & = \\left ( \\sum_{i=1}^{n } p_{i}^{\\alpha } \\right)^{\\frac{1}{\\alpha } - 1 } \\left ( p_{[k]}^{\\alpha-1 } - p_{[1]}^{\\alpha-1 } - ( n - k ) \\left ( \\frac { \\mathrm{d } p_{[n ] } } { \\mathrm{d } p_{[k ] } } \\right ) ( p_{[1]}^{\\alpha-1 } ) + ( n - k ) \\left ( \\frac { \\mathrm{d } p_{[n ] } } { \\mathrm{d } p_{[k ] } } \\right ) ( p_{[n]}^{\\alpha-1 } ) \\right ) \\\\ & = \\left ( \\sum_{i=1}^{n } p_{i}^{\\alpha } \\right)^{\\frac{1}{\\alpha } - 1 } \\left ( ( p_{[k]}^{\\alpha-1 } - p_{[1]}^{\\alpha-1 } ) + ( n - k ) \\left ( \\frac { \\mathrm{d } p_{[n ] } } { \\mathrm{d } p_{[k ] } } \\right ) ( p_{[n]}^{\\alpha-1 } - p_{[1]}^{\\alpha-1 } ) \\right ) \\\\ & \\overset{\\eqref{eq : total_entropy_hypo}}{= } \\left ( \\sum_{i=1}^{n } p_{i}^{\\alpha } \\right)^{\\frac{1}{\\alpha } - 1 } \\left ( ( p_{[k]}^{\\alpha-1 } - p_{[1]}^{\\alpha-1 } ) + ( n - k ) \\left ( - \\frac { 1 } {",
    "n - k } \\left ( \\frac { \\ln p_{[1 ] } - \\ln p_{[k ] } } { \\ln p_{[1 ] } - \\ln p_{[n ] } } \\right ) \\right ) ( p_{[n]}^{\\alpha-1 } - p_{[1]}^{\\alpha-1 } ) \\right ) \\\\ & = \\left ( \\sum_{i=1}^{n } p_{i}^{\\alpha } \\right)^{\\frac{1}{\\alpha } - 1 } \\left ( ( p_{[k]}^{\\alpha-1 } - p_{[1]}^{\\alpha-1 } ) - \\left ( \\frac { \\ln p_{[1 ] } - \\ln p_{[k ] } } { \\ln p_{[1 ] } - \\ln p_{[n ] } } \\right ) ( p_{[n]}^{\\alpha-1 } - p_{[1]}^{\\alpha-1 } ) \\right ) \\\\ & = \\left ( \\sum_{i=1}^{n } p_{i}^{\\alpha } \\right)^{\\frac{1}{\\alpha } - 1 } \\left ( p_{[n]}^{\\alpha-1 } - p_{[1]}^{\\alpha-1 } \\right ) \\left ( \\frac { p_{[k]}^{\\alpha-1 } - p_{[1]}^{\\alpha-1 } } { p_{[n]}^{\\alpha-1 } - p_{[1]}^{\\alpha-1 } } - \\frac { \\ln p_{[1 ] } - \\ln p_{[k ] } } { \\ln p_{[1 ] } - \\ln p_{[n ] } } \\right ) \\\\ & = \\left ( \\sum_{i=1}^{n } p_{i}^{\\alpha } \\right)^{\\frac{1}{\\alpha } - 1 } \\left ( p_{[n]}^{\\alpha-1 } - p_{[1]}^{\\alpha-1 } \\right ) \\left ( \\frac { p_{[1]}^{\\alpha-1 } \\left ( \\left ( \\frac { p_{[k ] } } { p_{[1 ] } } \\right)^{\\alpha-1 } - 1 \\right ) } { p_{[1]}^{\\alpha-1 } \\left ( \\left ( \\frac { p_{[n ] } } { p_{[1 ] } } \\right)^{\\alpha-1 } - 1 \\right ) } - \\frac { \\ln p_{[1 ] } - \\ln p_{[k ] } } { \\ln p_{[1 ] } - \\ln p_{[n ] } } \\right ) \\\\ & = \\left ( \\sum_{i=1}^{n } p_{i}^{\\alpha } \\right)^{\\frac{1}{\\alpha } - 1 } \\left ( p_{[n]}^{\\alpha-1 } - p_{[1]}^{\\alpha-1 } \\right ) \\left ( \\frac { \\left ( \\frac { p_{[k ] } } { p_{[1 ] } } \\right)^{\\alpha-1 } - 1 } { \\left ( \\frac { p_{[n ] } } { p_{[1 ] } } \\right)^{\\alpha-1 } - 1 } - \\frac { \\ln p_{[1 ] } - \\ln p_{[k ] } } { \\ln p_{[1 ] } - \\ln p_{[n ] } } \\right ) \\\\ & = \\left ( \\sum_{i=1}^{n } p_{i}^{\\alpha } \\right)^{\\frac{1}{\\alpha } - 1 } \\left ( p_{[n]}^{\\alpha-1 } - p_{[1]}^{\\alpha-1 } \\right ) \\left ( \\frac { \\left ( \\frac { p_{[1 ] } } { p_{[k ] } } \\right)^{1 - \\alpha } - 1 } { \\left ( \\frac { p_{[1 ] } } { p_{[n ] } } \\right)^{1 - \\alpha } - 1 } - \\frac { \\ln p_{[1 ] } - \\ln p_{[k ] } } { \\ln p_{[1 ] } - \\ln p_{[n ] } } \\right ) \\\\ & = \\left ( \\sum_{i=1}^{n } p_{i}^{\\alpha } \\right)^{\\frac{1}{\\alpha } - 1 } \\left ( p_{[n]}^{\\alpha-1 } - p_{[1]}^{\\alpha-1 } \\right ) \\left ( \\frac { \\ln_{\\alpha }",
    "\\frac { p_{[1 ] } } { p_{[k ] } } } { \\ln_{\\alpha } \\frac { p_{[1 ] } } { p_{[n ] } } } - \\frac { \\ln \\frac { p_{[1 ] } } { p_{[k ] } } } { \\ln \\frac { p_{[1 ] } } { p_{[n ] } } } \\right ) .\\end{aligned}\\ ] ] hence , we can see that @xmath199 } } \\right ) & = { \\operatorname{sgn}}\\ !",
    "\\left ( \\left ( \\sum_{i=1}^{n } p_{i}^{\\alpha } \\right)^{\\frac{1}{\\alpha } - 1 } \\left ( p_{[n]}^{\\alpha-1 } - p_{[1]}^{\\alpha-1 } \\right ) \\left ( \\frac { \\ln_{\\alpha } \\frac { p_{[1 ] } } { p_{[k ] } } } { \\ln_{\\alpha } \\frac { p_{[1 ] } } { p_{[n ] } } } - \\frac { \\ln \\frac { p_{[1 ] } } { p_{[k ] } } } { \\ln \\frac { p_{[1 ] } } { p_{[n ] } } } \\right ) \\right ) \\\\ & = \\underbrace { { \\operatorname{sgn}}\\ ! \\left ( \\left ( \\sum_{i=1}^{n } p_{i}^{\\alpha } \\right)^{\\frac{1}{\\alpha } - 1 } \\right ) } _ { = 1 } \\cdot \\ , { \\operatorname{sgn}}\\ ! \\left ( p_{[n]}^{\\alpha-1 } - p_{[1]}^{\\alpha-1 } \\right ) \\cdot { \\operatorname{sgn}}\\ ! \\left ( \\frac { \\ln_{\\alpha } \\frac { p_{[1 ] } } { p_{[k ] } } } { \\ln_{\\alpha } \\frac { p_{[1 ] } } { p_{[n ] } } } - \\frac { \\ln \\frac { p_{[1 ] } } { p_{[k ] } } } { \\ln \\frac { p_{[1 ] } } { p_{[n ] } } } \\right ) \\\\ & = { \\operatorname{sgn}}\\ ! \\left ( p_{[n]}^{\\alpha-1 } - p_{[1]}^{\\alpha-1 } \\right ) \\cdot { \\operatorname{sgn}}\\ ! \\left ( \\frac { \\ln_{\\alpha } \\frac { p_{[1 ] } } { p_{[k ] } } } { \\ln_{\\alpha } \\frac { p_{[1 ] } } { p_{[n ] } } } - \\frac { \\ln \\frac { p_{[1 ] } } { p_{[k ] } } } { \\ln \\frac { p_{[1 ] } } { p_{[n ] } } } \\right ) \\label{eq : diff1_norm_pk_1}\\end{aligned}\\ ] ] for @xmath94 . since @xmath200 , i.e. , @xmath197 } > p_{[n]}$ ] , we readily see that @xmath201}^{\\alpha-1 } - p_{[1]}^{\\alpha-1 } \\right ) & = \\begin{cases } 1 & \\mathrm{if } \\ \\alpha < 1 , \\\\ 0 & \\mathrm{if }",
    "\\ \\alpha = 1 , \\\\",
    "-1 & \\mathrm{if } \\ \\alpha > 1 .",
    "\\end{cases } \\label{eq : sign_pn - p1}\\end{aligned}\\ ] ] moreover , for @xmath202 } } { p_{[k ] } } \\le \\frac { p_{[1 ] } } { p_{[n ] } } \\",
    "( \\frac { p_{[1 ] } } { p_{[n ] } } \\neq 1)$ ] , we observe from lemma [ lem : frac_qlog ] that @xmath203 } } { p_{[k ] } } } { \\ln_{\\alpha } \\frac { p_{[1 ] } } { p_{[n ] } } } - \\frac { \\ln \\frac { p_{[1 ] } } { p_{[k ] } } } { \\ln \\frac { p_{[1 ] } } { p_{[n ] } } } \\right ) & = \\begin{cases } 1 & \\mathrm{if } \\",
    "\\alpha > 1 \\ \\mathrm{and }",
    "\\ p_{[1 ] } > p_{[k ] } > p_{[n ] } , \\\\ 0 & \\mathrm{if } \\",
    "\\alpha = 1 \\ \\mathrm{or } \\ p_{[1 ] } = p_{[k ] } \\ \\mathrm{or } \\ p_{[k ] } = p_{[n ] } , \\\\",
    "-1 & \\mathrm{if } \\",
    "\\alpha < 1 \\ \\mathrm{and } \\ p_{[1 ] } > p_{[k ] } > p_{[n ] } .",
    "\\end{cases } \\label{eq : sign_f(p1pkpn)}\\end{aligned}\\ ] ] therefore , under the constraints , , , and , we have @xmath199 } } \\right ) & \\overset{\\eqref{eq : diff1_norm_pk_1}}{= } { \\operatorname{sgn}}\\ ! \\left ( p_{[n]}^{\\alpha-1 } - p_{[1]}^{\\alpha-1 } \\right ) \\cdot { \\operatorname{sgn}}\\ ! \\left ( \\frac { \\ln_{\\alpha } \\frac { p_{[1 ] } } { p_{[k ] } } } { \\ln_{\\alpha } \\frac { p_{[1 ] } } { p_{[n ] } } } - \\frac { \\ln \\frac { p_{[1 ] } } { p_{[k ] } } } { \\ln \\frac { p_{[1 ] } } { p_{[n ] } } } \\right ) \\\\ & = \\begin{cases } 0 & \\mathrm{if } \\ \\alpha = 1 \\ \\mathrm{or } \\ p_{[1 ] } = p_{[k ] } \\ \\mathrm{or } \\ p_{[k ] } = p_{[n ] } , \\\\",
    "-1 & \\mathrm{if } \\",
    "\\alpha \\neq 1 \\ \\mathrm{and } \\ p_{[1 ] } > p_{[k ] } > p_{[n ] } \\end{cases } \\label{eq : sign_diff_norm}\\end{aligned}\\ ] ] for @xmath94 , where the last equality follows from and .",
    "hence , we have that @xmath32 with a fixed @xmath204 is strictly decreasing for @xmath193}$ ] under the constraints , , , and .    using the above results",
    ", we now prove this lemma .",
    "if @xmath193 } = p_{[k+1]}$ ] , then we reset the index @xmath205 to @xmath206 ; namely , we now choose the index @xmath170 to satisfy the following inequalities : @xmath9 } \\ge p_{[2 ] } \\ge \\dots \\ge p_{[k-1 ] } \\ge p_{[k ] } > p_{[k+1 ] } = p_{[k+2 ] } = \\dots = p_{[n ] } \\ge 0 .",
    "\\label{eq : choose_k}\\end{aligned}\\ ] ] then , we consider to decrease @xmath193}$ ] under the constraints of , , , and .",
    "it follows from that @xmath197}$ ] is strictly increased by according to decreasing @xmath193}$ ] . hence , if @xmath193}$ ] is decreased , then the condition @xmath197 } > p_{[2]}$ ] must be held .",
    "similarly , it follows from and that , for all @xmath191 , @xmath192}$ ] is also strictly increased by according to decreasing @xmath193}$ ] .",
    "hence , if @xmath193}$ ] is decreased , then the condition @xmath207 } = p_{[k+2 ] } = \\dots = p_{[n ] } > 0 $ ] must be held .",
    "let @xmath208 denote the probability vector that made from @xmath10 by continuing the above operation until to satisfy @xmath193 } = p_{[k+1]}$ ] under the conditions of , , , , and .",
    "namely , the probability vector @xmath209 satisfies the following inequalities : @xmath210 } > q_{[2 ] } \\ge q_{[3 ] } \\ge \\dots \\ge q_{[k-1 ] } > q_{[k ] } = q_{[k+1 ] } = \\dots = q_{[n ] } > 0 .\\end{aligned}\\ ] ] since @xmath209 is made from @xmath10 under the constraint , note that @xmath211 moreover , it follows from that @xmath32 with a fixed @xmath107 is also strictly increased by according to decreasing @xmath193}$ ] ; that is , we observe that @xmath212 for @xmath107 .",
    "repeating these operation until to satisfy @xmath213 and @xmath193 } = p_{[n]}$ ] , we have that @xmath214 for all @xmath107 and some @xmath53 $ ] . that completes the proof of lemma [ lem : vector_v ] .",
    "[ lem : vector_w ] for any @xmath2 and any @xmath23 , there exists @xmath47 $ ] such that @xmath215 and @xmath216 for all @xmath25 .",
    "this proof is similar to the proof of lemma [ lem : vector_v ] . if @xmath160 , then it can be easily seen that @xmath217 for any @xmath162 and some @xmath218 $ ] ; therefore , the lemma obviously holds when @xmath160 .",
    "moreover , since @xmath219 the lemma obviously holds if @xmath165 . furthermore , if @xmath220 for an integer @xmath221 , then the lemma also obviously holds .",
    "thus , we omit the cases @xmath222 , @xmath165 , and @xmath220 in the analyses . for a fixed @xmath4 and a constant @xmath168 , we assume for @xmath166 that @xmath223 for that @xmath10 , let @xmath224 be the indices such that @xmath197 } = p_{[k-1 ] } > p_{[k+1]}$ ] and @xmath225 } > p_{[l+1 ] } = 0 $ ] ; namely , the indices @xmath226 are chosen to satisfy the following inequalities : @xmath9 } = \\dots = p_{[k-1 ] } \\ge p_{[k ] } \\ge p_{[k+1 ] } \\ge \\dots \\ge p_{[l-1 ] } \\ge p_{[l ] } > p_{[l+1 ] } = \\dots = p_{[n ] } = 0 \\quad ( p_{[k-1 ] } > p_{[k+1 ] } ) .",
    "\\label{eq : equal_1_to_k-1}\\end{aligned}\\ ] ] since @xmath173 , we observe as with that @xmath227 } } { \\mathrm{d } p_{[k ] } } = - 1 .",
    "\\label{eq : total_diff_prob_w}\\end{aligned}\\ ] ] in this proof , we further assume that @xmath175 } } { \\mathrm{d } p_{[k ] } } = \\frac { \\mathrm{d } p_{[1 ] } } { \\mathrm{d } p_{[k ] } } \\label{eq : hypo1_w}\\end{aligned}\\ ] ] for @xmath176 , @xmath177 } } { \\mathrm{d } p_{[k ] } } = 1 \\label{eq : hypo2_w}\\end{aligned}\\ ] ] for @xmath228 , and @xmath229 } } { \\mathrm{d } p_{[k ] } } = 0 \\label{eq : hypo3_w}\\end{aligned}\\ ] ] for @xmath230 .",
    "note that implies that , for all @xmath228 , the increase / decrease rate of @xmath192}$ ] is equivalent to the increase / decrease rate of @xmath193}$ ] . by constraints , , and , we get @xmath231 } } { \\mathrm{d } p_{[k ] } } & = - 1 \\\\ & \\iff & \\sum_{i = 1}^{k-1 } \\frac { \\mathrm{d } p_{[i ] } } { \\mathrm{d } p_{[k ] } } + \\sum_{j = k+1}^{l-1 } \\frac { \\mathrm{d } p_{[j ] } } { \\mathrm{d } p_{[k ] } } + \\frac { \\mathrm{d } p_{[l ] } } { \\mathrm{d } p_{[k ] } } + \\sum_{m = l+1}^{n } \\frac { \\mathrm{d } p_{[m ] } } { \\mathrm{d } p_{[l ] } } & = - 1 \\\\ & \\overset{\\eqref{eq : hypo1_w}}{\\iff } & ( k-1 ) \\frac { \\mathrm{d } p_{[1 ] } } { \\mathrm{d } p_{[k ] } } + \\sum_{j = k+1}^{l-1 } \\frac { \\mathrm{d } p_{[j ] } } { \\mathrm{d } p_{[k ] } } + \\frac { \\mathrm{d } p_{[l ] } } { \\mathrm{d } p_{[k ] } } + \\sum_{m = l+1}^{n } \\frac { \\mathrm{d } p_{[m ] } } { \\mathrm{d } p_{[l ] } } & = - 1 \\\\ &",
    "\\overset{\\eqref{eq : hypo2_w}}{\\iff } & ( k-1 ) \\frac { \\mathrm{d } p_{[1 ] } } { \\mathrm{d } p_{[k ] } } + ( l - k - 1 ) + \\frac { \\mathrm{d } p_{[l ] } } { \\mathrm{d } p_{[k ] } } + \\sum_{m = l+1}^{n } \\frac { \\mathrm{d } p_{[m ] } } { \\mathrm{d } p_{[l ] } } & = - 1 \\\\ & \\overset{\\eqref{eq : hypo3_w}}{\\iff } & ( k-1 ) \\frac { \\mathrm{d } p_{[1 ] } } { \\mathrm{d } p_{[k ] } } + ( l - k - 1 ) + \\frac { \\mathrm{d } p_{[l ] } } { \\mathrm{d } p_{[k ] } } & = - 1 \\\\ & \\iff & ( k-1 ) \\frac { \\mathrm{d } p_{[1 ] } } { \\mathrm{d } p_{[k ] } } + \\frac { \\mathrm{d } p_{[l ] } } { \\mathrm{d } p_{[k ] } } & = - ( l - k ) \\\\ & \\iff & ( k-1 ) \\frac { \\mathrm{d } p_{[1 ] } } { \\mathrm{d } p_{[k ] } }   & = - ( l - k ) - \\frac { \\mathrm{d } p_{[l ] } } { \\mathrm{d } p_{[k ] } } \\\\ & \\iff & \\frac { \\mathrm{d } p_{[1 ] } } { \\mathrm{d } p_{[k ] } } & = - \\frac{1}{k-1 } \\left ( ( l - k ) +   \\frac { \\mathrm{d } p_{[l ] } } { \\mathrm{d } p_{[k ] } } \\right ) , \\label{eq : total_prob_hypo_w}\\end{aligned}\\ ] ] where note in that @xmath232 .",
    "moreover , since @xmath180 , we observe that @xmath233 } } { \\mathrm{d } p_{[k ] } } \\right ) ( \\ln p_{[i ] } + 1 ) & = \\ln p_{[k ] } + 1 \\\\ & \\iff & - \\sum_{i = 1 : i \\neq k}^{l } \\left ( \\frac { \\mathrm{d } p_{[i ] } } { \\mathrm{d } p_{[k ] } } \\right ) ( \\ln p_{[i ] } + 1 ) - \\sum_{m = l+1}^{n } \\left ( \\frac { \\mathrm{d } p_{[m ] } } { \\mathrm{d } p_{[k ] } } \\right ) ( \\ln p_{[m ] } + 1 ) & = \\ln p_{[k ] } + 1 \\\\ & \\overset{\\text{(a)}}{\\iff } & - \\sum_{i = 1 : i \\neq k}^{l } \\left ( \\frac { \\mathrm{d } p_{[i ] } } { \\mathrm{d } p_{[k ] } } \\right ) ( \\ln p_{[i ] } + 1 ) & = \\ln p_{[k ] } + 1 \\\\ & \\iff & - \\sum_{i = 1}^{k-1 } \\left ( \\frac { \\mathrm{d } p_{[i ] } } { \\mathrm{d } p_{[k ] } } \\right ) ( \\ln p_{[i ] } + 1 ) - \\sum_{j = k+1}^{l-1 } \\left ( \\frac { \\mathrm{d } p_{[j ] } } { \\mathrm{d } p_{[k ] } } \\right ) ( \\ln p_{[j ] } + 1 ) - \\left ( \\frac { \\mathrm{d } p_{[l ] } } { \\mathrm{d } p_{[k ] } } \\right ) ( \\ln p_{[l ] } + 1 ) & = \\ln p_{[k ] } + 1 \\\\ & \\overset{\\eqref{eq : equal_1_to_k-1}}{\\iff } & - ( \\ln p_{[1 ] } + 1 ) \\sum_{i = 1}^{k-1 } \\left ( \\frac { \\mathrm{d } p_{[i ] } } { \\mathrm{d } p_{[k ] } } \\right ) - \\sum_{j = k+1}^{l-1 } \\left ( \\frac { \\mathrm{d } p_{[j ] } } { \\mathrm{d } p_{[k ] } } \\right ) ( \\ln p_{[j ] } + 1 ) - \\left ( \\frac { \\mathrm{d } p_{[l ] } } { \\mathrm{d } p_{[k ] } } \\right ) ( \\ln p_{[l ] } + 1 ) & = \\ln p_{[k ] } + 1 \\\\ & \\overset{\\eqref{eq : hypo2_w}}{\\iff } & - ( \\ln p_{[1 ] } + 1 ) \\sum_{i = 1}^{k-1 } \\left ( \\frac { \\mathrm{d } p_{[i ] } } { \\mathrm{d } p_{[k ] } } \\right ) - \\sum_{j = k+1}^{l-1 } ( \\ln p_{[j ] } + 1 ) - \\left ( \\frac { \\mathrm{d } p_{[l ] } } { \\mathrm{d } p_{[k ] } } \\right ) ( \\ln p_{[l ] } + 1 ) & = \\ln p_{[k ] } + 1 \\\\ & \\overset{\\eqref{eq : hypo1_w}}{\\iff } & - ( k-1 ) ( \\ln p_{[1 ] } + 1 ) \\left ( \\frac { \\mathrm{d } p_{[1 ] } } { \\mathrm{d } p_{[k ] } } \\right ) - \\sum_{j = k+1}^{l-1 } ( \\ln p_{[j ] } + 1 ) - \\left ( \\frac { \\mathrm{d } p_{[l ] } } { \\mathrm{d } p_{[k ] } } \\right ) ( \\ln p_{[l ] } + 1 ) & = \\ln p_{[k ] } + 1 \\\\ & \\overset{\\eqref{eq : total_prob_hypo_w}}{\\iff } & ( \\ln p_{[1 ] } + 1 ) \\left ( ( l - k ) +   \\frac { \\mathrm{d } p_{[l ] } } { \\mathrm{d } p_{[k ] } } \\right ) - \\sum_{j = k+1}^{l-1 } ( \\ln p_{[j ] } + 1 ) - \\left ( \\frac { \\mathrm{d } p_{[l ] } } { \\mathrm{d } p_{[k ] } } \\right ) ( \\ln p_{[l ] } + 1 ) & = \\ln p_{[k ] } + 1 \\\\ & \\iff & ( l - k ) ( \\ln p_{[1 ] } + 1 ) - \\sum_{j = k+1}^{l-1 } ( \\ln p_{[j ] } + 1 ) + \\left ( \\frac { \\mathrm{d } p_{[l ] } } { \\mathrm{d } p_{[k ] } } \\right ) ( \\ln p_{[1 ] } - \\ln p_{[l ] } ) & = \\ln p_{[k ] } + 1 \\\\ & \\iff & ( l - k ) ( \\ln p_{[1 ] } + 1 ) - \\sum_{j = k}^{l-1 } ( \\ln p_{[j ] } + 1 ) + \\left ( \\frac { \\mathrm{d } p_{[l ] } } { \\mathrm{d } p_{[k ] } } \\right ) ( \\ln p_{[1 ] } - \\ln p_{[l ] } ) & = 0 \\\\ & \\iff & \\sum_{j = k}^{l-1 } ( \\ln p_{[1 ] } - \\ln p_{[j ] } ) + \\left ( \\frac { \\mathrm{d } p_{[l ] } } { \\mathrm{d } p_{[k ] } } \\right ) ( \\ln p_{[1 ] } - \\ln p_{[l ] } ) & = 0 , \\label{eq : total_entropy_hypo_w_halfway}\\end{aligned}\\ ] ] where ( a ) follows from the fact that @xmath234 } } { \\mathrm{d } p_{[k ] } } \\right ) ( \\ln p_{[m ] } + 1 ) = 0 $ ] for @xmath230 since @xmath235 } } { \\mathrm{d } p_{[k ] } } = 0 $ ] ( see eq . ) , @xmath236 } = 0 $ ] ( see eq . ) , and @xmath22 . hence , under the constraints , , , , and , we observe that @xmath237 } } { \\mathrm{d } p_{[k ] } } = - \\frac { \\sum_{j = k}^{l-1 } ( \\ln p_{[1 ] } - \\ln p_{[j ] } ) } { \\ln p_{[1 ] } - \\ln p_{[l ] } } .",
    "\\label{eq : total_entropy_hypo_w}\\end{aligned}\\ ] ] we now check the sign of the right - hand side of . note that @xmath238 } - \\ln p_{[l-1 ] } } { \\ln p_{[1 ] } - \\ln p_{[l ] } } \\right ) \\le \\frac { \\mathrm{d } p_{[l ] } } { \\mathrm{d } p_{[k ] } } \\le - ( l - k ) \\left ( \\frac { \\ln p_{[1 ] } - \\ln p_{[k ] } } { \\ln p_{[1 ] } - \\ln p_{[l ] } } \\right ) \\label{ineq : total_entropy_hypo_w}\\end{aligned}\\ ] ] since @xmath239 } \\ge \\ln p_{[j ] } \\ge \\ln p_{[l-1]}$ ] for all @xmath240 .",
    "if @xmath182 } > p_{[k ] } \\ge p_{[l ] } > 0 $ ] , then @xmath241 } - \\ln p_{[k ] } } { \\ln p_{[1 ] } - \\ln p_{[l ] } } > 0\\end{aligned}\\ ] ] since @xmath184 } > \\ln p_{[k ] } \\ge \\ln p_{[l ] } $ ] ; therefore , we get for the upper bound of that @xmath238 } - \\ln p_{[k ] } } { \\ln p_{[1 ] } - \\ln p_{[l ] } } \\right ) < 0 \\label{eq : sign_dndk_1_w}\\end{aligned}\\ ] ] for @xmath182 } > p_{[k ] } \\ge p_{[l ] } > 0 $ ] , where note that @xmath242 . moreover ,",
    "if @xmath182 } = p_{[k ] } > p_{[l ] } > 0 $ ] , then @xmath238 } - \\ln p_{[k ] } } { \\ln p_{[1 ] } - \\ln p_{[l ] } } \\right ) & = - ( l - k ) \\left ( \\frac { 0 } { \\ln p_{[1 ] } - \\ln p_{[l ] } } \\right ) \\\\ & = 0 .",
    "\\label{eq : sign_dndk_2_w}\\end{aligned}\\ ] ] combining and , we see that the upper bound of is always nonpositive for @xmath182 } \\ge p_{[k ] } \\ge p_{[l ] } > 0 \\ ( p_{[1 ] } > p_{[l]})$ ] ; that is",
    ", we observe under the constraints , , , , and that @xmath243 } } { \\mathrm{d } p_{[k ] } } \\right ) & \\overset{\\eqref{ineq : total_entropy_hypo_w}}{\\le } { \\operatorname{sgn}}\\ ! \\left ( - ( l - k ) \\left ( \\frac { \\ln p_{[1 ] } - \\ln p_{[k ] } } { \\ln p_{[1 ] } - \\ln p_{[l ] } } \\right ) \\right ) \\\\ & = \\begin{cases } 0 & \\mathrm{if } \\ p_{[1 ] } = p_{[k ] } , \\\\",
    "-1 & \\mathrm{otherwise } \\end{cases } \\label{eq : sign_dndk_w}\\end{aligned}\\ ] ] for @xmath182 } \\ge p_{[k ] } \\ge p_{[l ] } > 0 \\ ( p_{[1 ] } > p_{[l]})$ ] .",
    "note for the constraint that @xmath244 } \\to 0^{+ } } h ( p_{[1 ] } , p_{[2 ] } , \\dots , p_{[l-1 ] } , p_{[l ] } , 0 , 0 , \\dots , 0 ) = h ( p_{[1 ] } , p_{[2 ] } , \\dots , p_{[l-1 ] } , 0 , 0 , \\dots , 0 ) \\end{aligned}\\ ] ] since @xmath190 by the assumption @xmath22 .",
    "thus , it follows from that @xmath225}$ ] is strictly decreasing for @xmath193}$ ] under the constraints , , , , and .",
    "similarly , we check the sign of the right - hand side of . substituting the lower bound of into the right - hand side of",
    ", we observe that @xmath194 } } { \\mathrm{d } p_{[k ] } } \\le - \\frac{l - k}{k-1 } \\left ( 1 - \\frac { \\ln p_{[1 ] } - \\ln p_{[l-1 ] } } { \\ln p_{[1 ] } - \\ln p_{[l ] } } \\right ) .",
    "\\label{eq : total_entropy_hypo_w_ub}\\end{aligned}\\ ] ] if @xmath182 } \\ge p_{[l-1 ] } > p_{[l ] } > 0 $ ] , then @xmath241 } - \\ln p_{[l-1 ] } } { \\ln p_{[1 ] } - \\ln p_{[l ] } } < 1\\end{aligned}\\ ] ] since @xmath184 } \\ge \\ln p_{[l-1 ] } > \\ln p_{[l ] } $ ] ; therefore , we get for the upper bound of that @xmath245 } - \\ln p_{[l-1 ] } } { \\ln p_{[1 ] } - \\ln p_{[l ] } } \\right ) < 0 \\label{eq : sign_dndk_1_w2}\\end{aligned}\\ ] ] for @xmath182 } \\ge p_{[l-1 ] } > p_{[l ] } > 0 $ ] , where note that @xmath246 .",
    "moreover , if @xmath182 } = p_{[l-1 ] } > p_{[l ] } > 0 $ ] , then @xmath245 } - \\ln p_{[l-1 ] } } { \\ln p_{[1 ] } - \\ln p_{[l ] } } \\right ) & = - \\frac{l - k}{k-1 } \\left ( 1 - \\frac { \\ln p_{[1 ] } - \\ln p_{[l ] } } { \\ln p_{[1 ] } - \\ln p_{[l ] } } \\right ) \\\\ & = - \\frac{l - k}{k-1 } ( 1 - 1 ) \\\\ & = 0 .",
    "\\label{eq : sign_dndk_1_w3}\\end{aligned}\\ ] ] it follows from and that the upper bound of is always nonpositive for @xmath182 } \\ge p_{[l-1 ] } \\ge p_{[l ] } > 0 \\ ( p_{[1 ] } > p_{[l]})$ ] ; that is , we observe under the constraints , , , , and that @xmath196 } } { \\mathrm{d } p_{[k ] } } \\right ) & \\overset{\\eqref{eq : total_entropy_hypo_w_ub}}{\\le } { \\operatorname{sgn}}\\ ! \\left ( - \\frac{l - k}{k-1 } \\left ( 1 - \\frac { \\ln p_{[1 ] } - \\ln p_{[l-1 ] } } { \\ln p_{[1 ] } - \\ln p_{[l ] } } \\right ) \\right ) \\\\ & = \\begin{cases } 0 & \\mathrm{if } \\ p_{[l-1 ] } = p_{[l ] } , \\\\",
    "-1 & \\mathrm{otherwise } \\end{cases } \\label{eq : sign_d1dk_w}\\end{aligned}\\ ] ] for @xmath182 } \\ge p_{[l-1 ] } \\ge p_{[l ] } > 0 \\ ( p_{[1 ] } > p_{[l]})$ ] . as with , it follows from that , for all @xmath247 , @xmath248}$ ] is strictly decreasing for @xmath193}$ ] under the constraints , , , , and .    on the other hand , for a fixed @xmath107",
    ", we have @xmath198 } } & \\overset{\\eqref{eq : diff_norm_pk_halfway}}{= } \\frac{1}{\\alpha } \\left ( \\sum_{i=1}^{n } p_{i}^{\\alpha } \\right)^{\\frac{1}{\\alpha } - 1 } \\left ( \\alpha \\ , p_{[k]}^{\\alpha-1 } + \\sum_{i=1 : i \\neq k}^{n } \\frac { \\mathrm{d } } { \\mathrm{d } p_{[k ] } } ( p_{[i]}^{\\alpha } ) \\right ) \\\\ & = \\frac{1}{\\alpha } \\left ( \\sum_{i=1}^{n } p_{i}^{\\alpha } \\right)^{\\frac{1}{\\alpha } - 1 } \\left ( \\alpha \\ , p_{[k]}^{\\alpha-1 } + \\sum_{i=1 : i \\neq k}^{l } \\frac { \\mathrm{d } } { \\mathrm{d } p_{[k ] } } ( p_{[i]}^{\\alpha } ) + \\sum_{m = l+1}^{n } \\frac { \\mathrm{d } } { \\mathrm{d } p_{[k ] } } ( p_{[m]}^{\\alpha } ) \\right ) \\\\ & \\overset{\\text{(a)}}{= } \\frac{1}{\\alpha } \\left ( \\sum_{i=1}^{n } p_{i}^{\\alpha } \\right)^{\\frac{1}{\\alpha } - 1 } \\left ( \\alpha \\ , p_{[k]}^{\\alpha-1 } + \\sum_{i=1 : i \\neq k}^{l } \\frac { \\mathrm{d } } { \\mathrm{d } p_{[k ] } } ( p_{[i]}^{\\alpha } ) \\right ) \\\\ & = \\frac{1}{\\alpha } \\left ( \\sum_{i=1}^{n } p_{i}^{\\alpha } \\right)^{\\frac{1}{\\alpha } - 1 } \\left ( \\alpha \\ , p_{[k]}^{\\alpha-1 } + \\sum_{i=1 : i \\neq k}^{l } \\left ( \\frac { \\mathrm{d } p_{[i ] } } { \\mathrm{d } p_{[k ] } } \\right ) \\left ( \\frac { \\mathrm{d } } { \\mathrm{d } p_{[i ] } } ( p_{[i]}^{\\alpha } ) \\right ) \\right ) \\\\ & = \\frac{1}{\\alpha } \\left ( \\sum_{i=1}^{n } p_{i}^{\\alpha } \\right)^{\\frac{1}{\\alpha } - 1 } \\left ( \\alpha \\ , p_{[k]}^{\\alpha-1 } + \\sum_{i=1 : i \\neq k}^{l } \\left ( \\frac { \\mathrm{d } p_{[i ] } } { \\mathrm{d } p_{[k ] } } \\right ) ( \\alpha \\ , p_{[i]}^{\\alpha-1 } ) \\right ) \\\\ & = \\left ( \\sum_{i=1}^{n } p_{i}^{\\alpha } \\right)^{\\frac{1}{\\alpha } - 1 } \\left ( p_{[k]}^{\\alpha-1 } + \\sum_{i=1 : i \\neq k}^{l } \\left ( \\frac { \\mathrm{d } p_{[i ] } } { \\mathrm{d } p_{[k ] } } \\right ) p_{[i]}^{\\alpha-1 } \\right ) \\\\ & = \\left ( \\sum_{i=1}^{n } p_{i}^{\\alpha } \\right)^{\\frac{1}{\\alpha } - 1 } \\left ( p_{[k]}^{\\alpha-1 } + \\sum_{i=1}^{k-1 } \\left ( \\frac { \\mathrm{d } p_{[i ] } } { \\mathrm{d } p_{[k ] } } \\right ) p_{[i]}^{\\alpha-1 } + \\sum_{j = k+1}^{l-1 } \\left ( \\frac { \\mathrm{d } p_{[j ] } } { \\mathrm{d } p_{[k ] } } \\right ) p_{[j]}^{\\alpha-1 } + \\left ( \\frac { \\mathrm{d } p_{[l ] } } { \\mathrm{d } p_{[k ] } } \\right ) p_{[l]}^{\\alpha-1 } \\right ) \\\\ & \\overset{\\eqref{eq : equal_1_to_k-1}}{= } \\left ( \\sum_{i=1}^{n } p_{i}^{\\alpha } \\right)^{\\frac{1}{\\alpha } - 1 } \\left ( p_{[k]}^{\\alpha-1 } + p_{[1]}^{\\alpha-1 } \\sum_{i=1}^{k-1 } \\left ( \\frac { \\mathrm{d } p_{[i ] } } { \\mathrm{d } p_{[k ] } } \\right ) + \\sum_{j = k+1}^{l-1 } \\left ( \\frac { \\mathrm{d } p_{[j ] } } { \\mathrm{d } p_{[k ] } } \\right ) p_{[j]}^{\\alpha-1 } + \\left ( \\frac { \\mathrm{d } p_{[l ] } } { \\mathrm{d } p_{[k ] } } \\right ) p_{[l]}^{\\alpha-1 } \\right ) \\\\ & \\overset{\\eqref{eq : hypo1_w}}{= } \\left ( \\sum_{i=1}^{n } p_{i}^{\\alpha } \\right)^{\\frac{1}{\\alpha } - 1 } \\left ( p_{[k]}^{\\alpha-1 } + p_{[1]}^{\\alpha-1 } ( k-1 ) \\left ( \\frac { \\mathrm{d } p_{[1 ] } } { \\mathrm{d } p_{[k ] } } \\right ) + \\sum_{j = k+1}^{l-1 } \\left ( \\frac { \\mathrm{d } p_{[j ] } } { \\mathrm{d } p_{[k ] } } \\right ) p_{[j]}^{\\alpha-1 } + \\left ( \\frac { \\mathrm{d } p_{[l ] } } { \\mathrm{d } p_{[k ] } } \\right ) p_{[l]}^{\\alpha-1 } \\right ) \\\\ & \\overset{\\eqref{eq : hypo2_w}}{= } \\left ( \\sum_{i=1}^{n } p_{i}^{\\alpha } \\right)^{\\frac{1}{\\alpha } - 1 } \\left ( p_{[k]}^{\\alpha-1 } + p_{[1]}^{\\alpha-1 } ( k-1 ) \\left ( \\frac { \\mathrm{d } p_{[1 ] } } { \\mathrm{d } p_{[k ] } } \\right ) + \\sum_{j = k+1}^{l-1 } p_{[j]}^{\\alpha-1 } + \\left ( \\frac { \\mathrm{d } p_{[l ] } } { \\mathrm{d } p_{[k ] } } \\right ) p_{[l]}^{\\alpha-1 } \\right ) \\\\ & \\overset{\\eqref{eq : total_prob_hypo_w}}{= } \\left ( \\sum_{i=1}^{n } p_{i}^{\\alpha } \\right)^{\\frac{1}{\\alpha } - 1 } \\left ( p_{[k]}^{\\alpha-1 } - p_{[1]}^{\\alpha-1 } \\left ( ( l - k ) + \\frac { \\mathrm{d } p_{[l ] } } { \\mathrm{d } p_{[k ] } } \\right ) + \\sum_{j = k+1}^{l-1 } p_{[j]}^{\\alpha-1 } + \\left ( \\frac { \\mathrm{d } p_{[l ] } } { \\mathrm{d } p_{[k ] } } \\right ) p_{[l]}^{\\alpha-1 } \\right ) \\\\ & = \\left ( \\sum_{i=1}^{n } p_{i}^{\\alpha } \\right)^{\\frac{1}{\\alpha } - 1 } \\left ( \\sum_{j = k}^{l-1 } p_{[j]}^{\\alpha-1 } - p_{[1]}^{\\alpha-1 } ( l - k )   + \\left ( \\frac { \\mathrm{d } p_{[l ] } } { \\mathrm{d } p_{[k ] } } \\right ) ( p_{[l]}^{\\alpha-1 } - p_{[1]}^{\\alpha-1 } ) \\right ) \\\\ & = \\left ( \\sum_{i=1}^{n } p_{i}^{\\alpha } \\right)^{\\frac{1}{\\alpha } - 1 } \\left ( \\sum_{j = k}^{l-1 } ( p_{[j]}^{\\alpha-1 } - p_{[1]}^{\\alpha-1 } )   + \\left ( \\frac { \\mathrm{d } p_{[l ] } } { \\mathrm{d } p_{[k ] } } \\right ) ( p_{[l]}^{\\alpha-1 } - p_{[1]}^{\\alpha-1 } ) \\right ) \\\\ & \\overset{\\eqref{eq : total_entropy_hypo_w}}{= } \\left ( \\sum_{i=1}^{n } p_{i}^{\\alpha } \\right)^{\\frac{1}{\\alpha } - 1 } \\left ( \\sum_{j = k}^{l-1 } ( p_{[j]}^{\\alpha-1 } - p_{[1]}^{\\alpha-1 } )   + \\left ( - \\frac { \\sum_{j = k}^{l-1 } ( \\ln p_{[1 ] } - \\ln p_{[j ] } ) } { \\ln p_{[1 ] } - \\ln p_{[l ] } } \\right ) ( p_{[l]}^{\\alpha-1 } - p_{[1]}^{\\alpha-1 } ) \\right ) \\\\ & = \\left ( \\sum_{i=1}^{n } p_{i}^{\\alpha } \\right)^{\\frac{1}{\\alpha } - 1 } \\left ( \\vphantom{\\sum } p_{[l]}^{\\alpha-1 } - p_{[1]}^{\\alpha-1 } \\right ) \\left ( \\frac { \\sum_{j = k}^{l-1 } ( p_{[j]}^{\\alpha-1 } - p_{[1]}^{\\alpha-1 } ) } { p_{[l]}^{\\alpha-1 } - p_{[1]}^{\\alpha-1 } } - \\frac { \\sum_{j = k}^{l-1 } ( \\ln p_{[1 ] } - \\ln p_{[j ] } ) } { \\ln p_{[1 ] } - \\ln p_{[l ] } } \\right ) \\\\ & = \\left ( \\sum_{i=1}^{n } p_{i}^{\\alpha } \\right)^{\\frac{1}{\\alpha } - 1 } \\left ( \\vphantom{\\sum } p_{[l]}^{\\alpha-1 } - p_{[1]}^{\\alpha-1 } \\right ) \\sum_{j = k}^{l-1 } \\left ( \\frac { p_{[j]}^{\\alpha-1 } - p_{[1]}^{\\alpha-1 } } { p_{[l]}^{\\alpha-1 } - p_{[1]}^{\\alpha-1 } } - \\frac { \\ln p_{[1 ] } - \\ln p_{[j ] } } { \\ln p_{[1 ] } - \\ln p_{[l ] } } \\right ) \\\\ & = \\left ( \\sum_{i=1}^{n } p_{i}^{\\alpha } \\right)^{\\frac{1}{\\alpha } - 1 } \\left ( \\vphantom{\\sum } p_{[l]}^{\\alpha-1 } - p_{[1]}^{\\alpha-1 } \\right ) \\sum_{j = k}^{l-1 } \\left ( \\frac { \\left ( \\frac { p_{[1 ] } } { p_{[j ] } } \\right)^{1-\\alpha } - 1 } { \\left ( \\frac { p_{[1 ] } } { p_{[l ] } } \\right)^{1-\\alpha } - 1 } - \\frac { \\ln p_{[1 ] } - \\ln p_{[j ] } } { \\ln p_{[1 ] } - \\ln p_{[l ] } } \\right ) \\\\ & = \\left ( \\sum_{i=1}^{n } p_{i}^{\\alpha } \\right)^{\\frac{1}{\\alpha } - 1 } \\left ( \\vphantom{\\sum } p_{[l]}^{\\alpha-1 } - p_{[1]}^{\\alpha-1 } \\right ) \\sum_{j = k}^{l-1 } \\left ( \\frac { \\ln_{\\alpha } \\frac { p_{[1 ] } } { p_{[j ] } } } { \\ln_{\\alpha } \\frac { p_{[1 ] } } { p_{[l ] } } } - \\frac { \\ln \\frac { p_{[1 ] } } { p_{[j ] } } } { \\ln \\frac { p_{[1 ] } } { p_{[l ] } } } \\right ) \\label{eq : diff_norm_pk_w}\\end{aligned}\\ ] ] where ( a ) holds since the constraint implies that @xmath236}$ ] is constant for @xmath193}$ ] . hence , we can see that @xmath199 } } \\right ) & = { \\operatorname{sgn}}\\ ! \\left ( \\left ( \\sum_{i=1}^{n } p_{i}^{\\alpha } \\right)^{\\frac{1}{\\alpha } - 1 } \\left ( \\vphantom{\\sum } p_{[l]}^{\\alpha-1 } - p_{[1]}^{\\alpha-1 } \\right ) \\sum_{j = k}^{l-1 } \\left ( \\frac { \\ln_{\\alpha } \\frac { p_{[1 ] } } { p_{[j ] } } } { \\ln_{\\alpha } \\frac { p_{[1 ] } } { p_{[l ] } } } - \\frac { \\ln \\frac { p_{[1 ] } } { p_{[j ] } } } { \\ln \\frac { p_{[1 ] } } { p_{[l ] } } } \\right ) \\right ) \\\\ & = \\underbrace { { \\operatorname{sgn}}\\ ! \\left ( \\left ( \\sum_{i=1}^{n } p_{i}^{\\alpha } \\right)^{\\frac{1}{\\alpha } - 1 } \\right ) } _ { = 1 } \\cdot \\ , { \\operatorname{sgn}}\\ ! \\left ( \\vphantom{\\sum } p_{[l]}^{\\alpha-1 } - p_{[1]}^{\\alpha-1 } \\right ) \\cdot \\ , { \\operatorname{sgn}}\\ ! \\left ( \\sum_{j = k}^{l-1 } \\left ( \\frac { \\ln_{\\alpha } \\frac { p_{[1 ] } } { p_{[j ] } } } { \\ln_{\\alpha } \\frac { p_{[1 ] } } { p_{[l ] } } } - \\frac { \\ln \\frac { p_{[1 ] } } { p_{[j ] } } } { \\ln \\frac { p_{[1 ] } } { p_{[l ] } } } \\right ) \\right ) \\\\ & = { \\operatorname{sgn}}\\ ! \\left ( \\vphantom{\\sum } p_{[l]}^{\\alpha-1 } - p_{[1]}^{\\alpha-1 } \\right ) \\cdot \\ , { \\operatorname{sgn}}\\ ! \\left ( \\sum_{j = k}^{l-1 } \\left ( \\frac { \\ln_{\\alpha } \\frac { p_{[1 ] } } { p_{[j ] } } } { \\ln_{\\alpha } \\frac { p_{[1 ] } } { p_{[l ] } } } - \\frac { \\ln \\frac { p_{[1 ] } } { p_{[j ] } } } { \\ln \\frac { p_{[1 ] } } { p_{[l ] } } } \\right ) \\right ) \\label{eq : diff1_norm_pk_1_w}\\end{aligned}\\ ] ] for @xmath27 . as with , we readily see that @xmath249}^{\\alpha-1 } - p_{[1]}^{\\alpha-1 } \\right ) & = \\begin{cases } 1 & \\mathrm{if } \\ \\alpha < 1 , \\\\ 0 & \\mathrm{if } \\ \\alpha = 1 , \\\\",
    "-1 & \\mathrm{if } \\",
    "\\alpha > 1 \\end{cases } \\label{eq : sign_pl - p1_w}\\end{aligned}\\ ] ] for @xmath197 } > p_{[l ] } > 0 $ ] . moreover , since @xmath202 } } { p_{[j ] } } \\le \\frac { p_{[1 ] } } { p_{[l ] } } \\",
    "( \\frac { p_{[1 ] } } { p_{[l ] } } \\neq 1)$ ] for @xmath240 , we observe from lemma [ lem : frac_qlog ] that @xmath203 } } { p_{[j ] } } } { \\ln_{\\alpha } \\frac { p_{[1 ] } } { p_{[l ] } } } - \\frac { \\ln \\frac { p_{[1 ] } } { p_{[j ] } } } { \\ln \\frac { p_{[1 ] } } { p_{[l ] } } } \\right ) & = \\begin{cases } 1 & \\mathrm{if } \\ \\alpha > 1 \\ \\mathrm{and }",
    "\\ p_{[1 ] } > p_{[j ] } > p_{[l ] } , \\\\ 0 & \\mathrm{if } \\",
    "\\alpha = 1 \\ \\mathrm{or } \\ p_{[1 ] } = p_{[j ] } \\ \\mathrm{or } \\ p_{[j ] } = p_{[l ] } , \\\\",
    "-1 & \\mathrm{if } \\",
    "\\alpha < 1 \\ \\mathrm{and } \\ p_{[1 ] } > p_{[j ] } > p_{[l ] } .",
    "\\end{cases}\\end{aligned}\\ ] ] for @xmath240 ; and therefore , we have @xmath250 } } { p_{[j ] } } } { \\ln_{\\alpha } \\frac { p_{[1 ] } } { p_{[l ] } } } - \\frac { \\ln \\frac { p_{[1 ] } } { p_{[j ] } } } { \\ln \\frac { p_{[1 ] } } { p_{[l ] } } } \\right ) \\right ) & = \\begin{cases } 1 & \\mathrm{if } \\",
    "\\alpha > 1 \\ \\mathrm{and } \\",
    "( p_{[1 ] } > p_{[k ] } \\ge p_{[l ] } \\ \\mathrm{or } \\ p_{[1 ] } \\ge p_{[k ] } > p_{[l ] } ) , \\\\ 0 & \\mathrm{if } \\",
    "\\alpha = 1 \\ \\mathrm{or } \\ ( p_{[1 ] } = p_{[k ] } \\ \\mathrm{and } \\ p_{[k+1 ] } = p_{[l ] } )   \\ \\mathrm{or } \\ p_{[k ] } = p_{[l ] } , \\\\",
    "-1 & \\mathrm{if } \\",
    "\\alpha < 1 \\ \\mathrm{and } \\ ( p_{[1 ] } > p_{[k ] } \\ge p_{[l ] } \\ \\mathrm{or } \\ p_{[1 ] } \\ge p_{[k ] } > p_{[l ] } ) \\end{cases } \\label{eq : sign_f(p1pkpn)_w}\\end{aligned}\\ ] ] for @xmath23 under the constraint . therefore , under the constraints , , , , and",
    ", we obtain @xmath199 } } \\right ) & \\overset{\\eqref{eq : diff1_norm_pk_1_w}}{= } { \\operatorname{sgn}}\\ ! \\left ( \\vphantom{\\sum } p_{[l]}^{\\alpha-1 } - p_{[1]}^{\\alpha-1 } \\right ) \\cdot \\ , { \\operatorname{sgn}}\\ ! \\left ( \\sum_{j = k}^{l-1 } \\left ( \\frac { \\ln_{\\alpha } \\frac { p_{[1 ] } } { p_{[j ] } } } { \\ln_{\\alpha } \\frac { p_{[1 ] } } { p_{[l ] } } } - \\frac { \\ln \\frac { p_{[1 ] } } { p_{[j ] } } } { \\ln \\frac { p_{[1 ] } } { p_{[l ] } } } \\right ) \\right ) \\\\ & = \\begin{cases } 0 & \\mathrm{if } \\ \\alpha = 1 \\ \\mathrm{or } \\ ( p_{[1 ] } = p_{[k ] } \\ \\mathrm{and } \\ p_{[k+1 ] } = p_{[l ] } )   \\ \\mathrm{or } \\ p_{[k ] } = p_{[l ] } , \\\\",
    "-1 & \\mathrm{if } \\",
    "\\alpha \\neq 1 \\ \\mathrm{and } \\",
    "( p_{[1 ] } > p_{[k ] } \\ge p_{[l ] } \\ \\mathrm{or } \\ p_{[1 ] } \\ge p_{[k ] } > p_{[l ] } ) \\end{cases } \\label{eq : sign_diff_norm_w}\\end{aligned}\\ ] ] for @xmath107 , where the last equality follows from and .",
    "hence , we have that @xmath32 with a fixed @xmath107 is strictly decreasing for @xmath193}$ ] under the constraints , , , , and .    using the above results",
    ", we now prove this lemma .",
    "note that , if @xmath171 } = p_{[k]}$ ] and @xmath251 , then @xmath252 for some @xmath47 $ ] .",
    "if @xmath171 } = p_{[k]}$ ] and @xmath253 , then we reset the index @xmath254 to @xmath255 ; namely ; we now choose the indices @xmath224 to satisfy the following inequalities : @xmath9 } = p_{[2 ] } = \\dots = p_{[k-1 ] } > p_{[k ] } \\ge p_{[k+1 ] } \\ge \\dots \\ge p_{[l-1 ] } \\ge p_{[l ] } > p_{[l+1 ] } = p_{[l+2 ] } = \\dots = p_{[n ] } = 0 .",
    "\\label{eq : choose_k2}\\end{aligned}\\ ] ] then , we consider to increase @xmath193}$ ] under the constraints of , , , , and .",
    "note that the constraint implies that , for all @xmath228 , @xmath192}$ ] is strictly increased with the same speed of increasing @xmath193}$ ] .",
    "it follows from and that , for all @xmath247 , @xmath248}$ ] is strictly decreased by according to increasing @xmath193}$ ] .",
    "hence , if @xmath193}$ ] is decreased , then there is a possibility that @xmath197 } = \\dots = p_{[k-1 ] } = p_{[k]}$ ] .",
    "similarly , it follows from that @xmath225}$ ] is also strictly decreased by according to increasing @xmath193}$ ] .",
    "hence , if @xmath193}$ ] is decreased , then there is a possibility that @xmath225 } = p_{[l+1 ] } = \\dots = p_{[n ] } = 0 $ ] .",
    "let @xmath208 denotes the probability vector that made from @xmath10 by continuing the above operation until to satisfy @xmath197 } = p_{[k]}$ ] or @xmath225 } = 0 $ ] under the conditions of , , , and .",
    "namely , the probability vector @xmath209 satisfies either @xmath210 } = q_{[2 ] } = \\dots = q_{[k-1 ] } = q_{[k ] } \\ge q_{[k+1 ] } \\ge q_{[k+2 ] } \\ge \\dots \\ge q_{[l-1 ] } > q_{[l ] } \\ge q_{[l+1 ] } = q_{[l+2 ] } = \\dots = q_{[n ] } = 0 \\label{ineq : q_w_1}\\end{aligned}\\ ] ] or @xmath210 } = q_{[2 ] } = \\dots = q_{[k-1 ] } \\ge q_{[k ] } \\ge q_{[k+1 ] } \\ge q_{[k+2 ] } \\ge \\dots \\ge q_{[l-1 ] } > q_{[l ] } = q_{[l+1 ] } = q_{[l+2 ] } = \\dots = q_{[n ] } = 0 .",
    "\\label{ineq : q_w_2}\\end{aligned}\\ ] ] note that there is a possibility that both of and hold ; that is , @xmath210 } = q_{[2 ] } = \\dots = q_{[k-1 ] } = q_{[k ] } \\ge q_{[k+1 ] } \\ge q_{[k+2 ] } \\ge \\dots \\ge q_{[l-1 ] } > q_{[l ] } = q_{[l+1 ] } = q_{[l+2 ] } = \\dots = q_{[n ] } = 0\\end{aligned}\\ ] ] holds .",
    "since @xmath209 is made under the constraint , note that @xmath211 moreover , it follows from that @xmath32 with a fixed @xmath107 is also strictly decreased by according to increasing @xmath193}$ ] ; therefore , we observe that @xmath256 for @xmath107 . repeating these operation until to satisfy @xmath251 and @xmath197 } = p_{[k ] } > p_{[l ] } \\ge p_{[l-1 ] } = p_{[n ] } = 0 $ ] , we have that @xmath257 for all @xmath107 and some @xmath47 $ ] . that completes the proof of lemma [ lem : vector_w ] .",
    "lemmas [ lem : vector_v ] and [ lem : vector_w ] are derived by using lemma [ lem : frac_qlog ] .",
    "lemmas [ lem : vector_v ] and [ lem : vector_w ] imply that the distributions @xmath40 and @xmath41 have extremal properties in the sense of a relation between the shannon entropy and the @xmath0-norm .",
    "then , we can derive tight bounds of @xmath0-norms with a fixed shannon entropy as follows :    [ th : extremes ] let @xmath258 and @xmath259 for @xmath23 .",
    "then , we observe that @xmath260 for any @xmath2 , any @xmath23 , and any @xmath25 .",
    "it follows from lemmas [ lem : vector_v ] and [ lem : vector_w ] that , for any @xmath2 and any @xmath23 , there exist @xmath53 $ ] and @xmath261 $ ] such that @xmath262 for all @xmath263 .",
    "then , we now consider @xmath264 such that @xmath265 for @xmath263 .",
    "it also follows from lemmas [ lem : vector_v ] and [ lem : vector_w ] that there exist @xmath266 $ ] and @xmath267 $ ] such that @xmath268 for @xmath263 .",
    "note from and that @xmath269 note that it follows from lemmas [ lem : hv ] and [ lem : hw ] that @xmath52 and @xmath270 are both bijective functions of @xmath53 $ ] and @xmath261 $ ] , respectively .",
    "therefore , we get @xmath271 which imply that , for @xmath209 and @xmath272 , the following equalities must be held : @xmath273 that completes the proof of theorem [ th : extremes ] .",
    "note that the distributions @xmath274 and @xmath275 denote @xmath276 and @xmath277 , respectively , such that @xmath278 for a given @xmath23 .",
    "theorem [ th : extremes ] shows that , among all @xmath1-ary probability vectors with a fixed shannon entropy , the distributions @xmath40 and @xmath41 take the maximum and the minimum @xmath0-norm , respectively .",
    "thus , the bounds of theorem [ th : extremes ] are tight in the sense of the existences of the distributions @xmath40 and @xmath41 which attain both equalities of the bounds . in other words ,",
    "theorem [ th : extremes ] implies that the boundaries of @xmath279 , defined in , can be attained by @xmath40 and @xmath41 .",
    "we illustrate the graphs of the boundaries of @xmath279 in fig .",
    "[ fig : region_p6_half ] .",
    "note that @xmath280 for any @xmath162 and any @xmath25 since @xmath281 for @xmath163 $ ] .",
    "therefore , theorem [ th : extremes ] becomes meaningful for @xmath4 .",
    "on the other hand , the following theorem shows that , among all @xmath1-ary probability vectors with a fixed @xmath0-norm , the distributions @xmath40 and @xmath41 also take the extreme values of the shannon entropy .",
    "[ th : extremes2 ] let @xmath53 $ ] and @xmath261 $ ] be chosen to satisfy @xmath282 for a fixed @xmath27 .",
    "then , we observe that @xmath283 for any @xmath2 and any @xmath23 .    from theorem [ th : extremes ] , for a fixed @xmath2 , we consider @xmath23 , @xmath53 $ ] , and @xmath261 $ ] such that @xmath262 for @xmath107 .",
    "note that @xmath55 and @xmath284 are uniquely determined for a given @xmath23 .",
    "it follows from lemmas [ lem : hv ] and [ lem : hw ] that @xmath86 $ ] and @xmath285 $ ] are strictly increasing for @xmath53 $ ] and strictly decreasing for @xmath261 $ ] , respectively .",
    "moreover , it follows from lemmas [ lem : mono_v ] and [ lem : mono_w ] that , if @xmath286 , then @xmath83 and @xmath287 are strictly increasing for @xmath53 $ ] and strictly decreasing for @xmath261 $ ] , respectively .",
    "therefore , decreasing both @xmath53 $ ] and @xmath261 $ ] , we can obtain @xmath266 $ ] and @xmath267 $ ] such that @xmath288 for a fixed @xmath286 .",
    "on the other hand , it follows from lemmas [ lem : mono_v ] and [ lem : mono_w ] that , if @xmath289 , then @xmath83 and @xmath287 are strictly decreasing for @xmath53 $ ] and strictly increasing for @xmath47 $ ] , respectively .",
    "therefore , increasing both @xmath53 $ ] and @xmath261 $ ] , we can obtain @xmath266 $ ] and @xmath267 $ ] such that @xmath290 for a fixed @xmath289 .",
    "finally , we note that the strict monotonicity of lemmas [ lem : mono_v ] and [ lem : mono_w ] prove the uniquenesses of the values @xmath266 $ ] and @xmath267 $ ] .",
    "in fact , it follows from lemmas [ lem : hv ] , [ lem : hw ] , [ lem : mono_v ] , and [ lem : mono_w ] that , for a fixed @xmath2 and a fixed @xmath107 , @xmath83 and @xmath287 are both bijective function of @xmath53 $ ] and @xmath261 $ ] , respectively . that completes the proof of theorem [ th : extremes2 ] .    in theorem",
    "[ th : extremes2 ] , note that the values @xmath53 $ ] and @xmath261 $ ] are uniquely determined by the value of @xmath32 .",
    "theorems [ th : extremes ] and [ th : extremes2 ] show that extremality between the shannon entropy and the @xmath0-norm can be attained by the distributions @xmath40 and @xmath41 .    following a same manner with (",
    "* theorem 2 ) , we extend the bounds of theorem [ th : extremes ] from the @xmath0-norm to several information measures , which are related to @xmath0-norm , as follows :    [ cor : extremes ] let @xmath291 be a strictly monotonic function .",
    "then , we observe that : ( i ) if @xmath291 is strictly increasing , then @xmath292 and ( ii ) if @xmath291 is strictly decreasing , then @xmath293 for any @xmath2 , any @xmath23 , and any @xmath25 .",
    "since any strictly increasing function @xmath291 satisfies @xmath294 for @xmath295 , it is easy to see that from of theorem [ th : extremes ] .",
    "similarly , since any strictly decreasing function @xmath291 satisfies @xmath296 for @xmath295 , it is also easy to see that from of theorem [ th : extremes ] .",
    "therefore , we can obtain tight bounds of several information measures , which are determined by @xmath0-norm , with a fixed shannon entropy .",
    "as an instance , we introduce the application of corollary [ cor : extremes ] to the rnyi entropy as follows : let @xmath297 .",
    "then , we readily see that @xmath298 .",
    "it can be easily seen that @xmath299 is strictly increasing for @xmath300 when @xmath286 and strictly decreasing for @xmath300 when @xmath301 .",
    "hence , it follows from corollary [ cor : extremes ] that @xmath302 for any @xmath2 and any @xmath23 .",
    "moreover , if @xmath53 $ ] and @xmath261 $ ] are chosen to satisfy @xmath303 for a fixed @xmath27 , then and hold for any @xmath2 and any @xmath23 from theorem [ th : extremes2 ] .",
    "these bounds between the shannon entropy and the rnyi entropy imply the boundary of the region @xmath304 for any @xmath2 and any @xmath27 .",
    "we illustrate the boundaries of @xmath305 in fig .",
    "[ fig : renyi ] .",
    "similarly , we can apply corollary [ cor : extremes ] to several entropies as shown in table [ table : extremes ] , and we illustrate these exact feasible region in figs . [",
    "fig : renyi][fig : r ] .    [ cols=\"^,<,^,^\",options=\"header \" , ]     harremos and topse @xcite showed that the exact region of @xmath306 for @xmath4 , where @xmath307 denotes the index of coincidence .",
    "then , we can see that corollary [ cor : extremes ] contains its result by @xmath308 .      in this subsection",
    ", we consider applications of corollary [ cor : extremes ] for a particular class of discrete memoryless channels ( dmcs ) , i.e. , uniformly focusing channels @xcite .",
    "let the rnyi divergence @xcite of order @xmath27 is denoted by @xmath309 for @xmath310 . since @xmath311 by lhpital s rule , we write @xmath312 , where @xmath313 denotes the relative entropy . since @xmath314 for @xmath25 , we can obtain corollary [ cor : renyidiv ] from and .",
    "[ cor : renyidiv ] if @xmath315 , then @xmath316 for any @xmath2 and any @xmath23",
    ". moreover , if @xmath85 , then @xmath317 for any @xmath2 and any @xmath23 .    since @xmath318",
    ", we note that corollary [ cor : renyidiv ] shows the tight bounds of rnyi divergence from a uniform distribution with a fixed relative entropy from a uniform distribution .",
    "namely , corollary [ cor : renyidiv ] implies the boundary of @xmath319 for any @xmath2 and any @xmath27 .",
    "we illustrate boundaries of its region in fig .",
    "[ fig : renyidiv ] .",
    "we now define dmcs as follows : let the discrete random variables @xmath320 and @xmath321 denote the input and output of a dmc , respectively , where @xmath322 and @xmath323 denote the finite input and output alphabets , respectively .",
    "let @xmath324 denote the transition probability of a dmc @xmath325 for @xmath326 .",
    "then , we define the following three classes of dmcs .",
    "[ def : dispersive ] a channel @xmath325 is said to be _ uniformly dispersive @xcite _ or _",
    "uniform from the input @xcite _ if there exists a permutation @xmath327 for each @xmath328 such that @xmath329 for all @xmath330 .",
    "[ def : focusing ] a channel @xmath325 is said to be _ uniformly focusing @xcite _ or _",
    "uniform from the output @xcite _ if there exists a permutation @xmath331 for each @xmath332 such that @xmath333 for all @xmath334 .",
    "[ def : strongly ] a channel is said to be _ strongly symmetric @xcite _ or _",
    "doubly uniform @xcite _ if it is both uniformly dispersive and uniformly focusing .    for a uniformly dispersive channel @xmath325 , it is known that @xmath335 for any @xmath328 ( see ( * ? ? ?",
    "* eq . ( 5.18 ) ) or ( * ? ? ?",
    "* lemma 4.1 ) ) , where the conditional shannon entropy @xcite of @xmath336 is defined by @xmath337\\end{aligned}\\ ] ] and @xmath338 $ ] denotes the expected value of the random variable .",
    "moreover , let the conditional rnyi entropy @xcite of order @xmath27 be denoted by @xmath339\\end{aligned}\\ ] ] for @xmath336 . by convention , we write @xmath340 . as with uniformly focusing channels , for uniformly focusing channels",
    ", we can provide the following lemma .",
    "[ lem : focusing ] if a channel @xmath325 is uniformly focusing and the input @xmath341 follows a uniform distribution , then @xmath342 for any @xmath332 and any @xmath25 .    consider a uniformly focusing channel @xmath325 .",
    "assume that the input @xmath341 follows a uniform distribution , i.e. , @xmath343 for all @xmath328 .",
    "note from @xcite or ( * ? ? ?",
    "i , lemma 4.2 ) that , if the input @xmath341 follows a uniform distribution , then the output @xmath344 also follows a uniform distribution , i.e. , @xmath345 for all @xmath332 . then , since the a posteriori probability of @xmath325 is written as @xmath346 for @xmath326 by bayes rule and the fraction @xmath347 is constant for @xmath326 , it follows from definition [ def : focusing ] that there exists a permutation @xmath331 for each @xmath332 such that @xmath348 for all @xmath334 .",
    "hence , we get @xmath349 for any @xmath350 . similarly , we also get @xmath351 & = \\sum_{y \\in \\mathcal{y } } p_{y } ( y ) \\|",
    "p_{x|y}(\\cdot \\mid y ) \\|_{\\alpha } \\\\ & = \\sum_{y \\in \\mathcal{y } } p_{y } ( y ) \\left ( \\sum_{x \\in \\mathcal{x } } p_{x|y}(x \\mid y)^{\\alpha } \\right)^{\\frac{1}{\\alpha } } \\\\ & = \\sum_{y \\in \\mathcal{y } } p_{y } ( y ) \\left ( \\sum_{x \\in \\mathcal{x } } p_{x|y}(\\pi_{y } ( x ) \\mid y)^{\\alpha } \\right)^{\\frac{1}{\\alpha } } \\\\ & \\overset{\\eqref{eq : a_posteriori_pi}}{= } \\left ( \\sum_{y \\in \\mathcal{y } } p_{y } ( y ) \\right ) \\left ( \\sum_{x \\in \\mathcal{x } } p_{x|y}(\\pi_{y^{\\prime } } ( x ) \\mid y^{\\prime})^{\\alpha } \\right)^{\\frac{1}{\\alpha } } \\\\ & = \\left ( \\sum_{x \\in \\mathcal{x } } p_{x|y}(\\pi_{y^{\\prime } } ( x ) \\mid y^{\\prime})^{\\alpha } \\right)^{\\frac{1}{\\alpha } } \\\\ & = \\left ( \\sum_{x \\in \\mathcal{x } } p_{x|y}(x \\mid y^{\\prime})^{\\alpha } \\right)^{\\frac{1}{\\alpha } } \\\\ & = \\| p_{x|y}(\\cdot \\mid y^{\\prime } ) \\|_{\\alpha } \\label{eq : cond_n_focusing}\\end{aligned}\\ ] ] for any @xmath328 and any @xmath25 . since @xmath352 $ ] for @xmath27 and @xmath353 , eqs . and",
    "imply lemma [ lem : focusing ] .",
    "therefore , it follows from lemma [ lem : focusing ] that the results of corollary [ cor : extremes ] can be applied to uniformly focusing channels @xmath325 if the input @xmath341 follows a uniform distribution , as with and . for a channel @xmath325 ,",
    "let the mutual information of order @xmath25 @xcite between @xmath341 and @xmath344 be denoted by @xmath354 for @xmath25 .",
    "note that @xmath355 denotes the ( ordinary ) mutual information between @xmath341 and @xmath344 . in this paragraph , we assume that a channel @xmath325 is uniformly focusing and the input @xmath341 follows a uniform distribution . since @xmath356 for @xmath25 , it follows from lemma [ lem : focusing ] that @xmath357 for any @xmath332 and any @xmath25 , where @xmath358 denotes the cardinality of the finite set .",
    "therefore , it follows that the tight bounds of @xmath359 with a fixed @xmath360 are equivalent to the bounds of corollary [ cor : renyidiv ] under the hypotheses .    furthermore , we consider gallager s @xmath3 function @xcite of a channel @xmath325 , defined by @xmath361 for @xmath362 .",
    "then , we can obtain the following theorem .",
    "[ th : e0_focusing ] for a uniformly focusing channel @xmath325 , let @xmath363 where @xmath364 , @xmath365 , and @xmath366 .",
    "if the input @xmath341 follows a uniform distribution , then we observe that @xmath367 for any @xmath362 .",
    "we can see from ( * ? ? ?",
    "* eq . ( 16 ) ) that @xmath368 where @xmath369 denotes the tilted distribution .",
    "we can see from that the @xmath3 function is closely related to the mutual information of order @xmath112 .",
    "note that , if the distribution @xmath370 is a uniform distribution , then its tilted distribution @xmath371 is also a uniform distribution for any @xmath25 .",
    "thus , if a channel @xmath325 is uniformly focusing and the input @xmath341 follows a uniform distribution , then it follows from and that @xmath372 for any @xmath362 and any @xmath332 . hence , noting the relations @xmath373 the @xmath3 function can also be evaluate as with corollary [ cor : renyidiv ] .",
    "note that the distributions @xmath374 and @xmath375 denote @xmath276 and @xmath277 , respectively , such that @xmath376 for a given channel @xmath325 . since @xmath377 under a uniform input distribution , theorem [ th : e0_focusing ] shows bounds of the @xmath3 function with a fixed mutual information .",
    "note that , since and are defined by the @xmath378 and @xmath379 , respectively , there exist two strongly symmetric channels which attain each equality of the bounds .",
    "namely , theorem [ th : e0_focusing ] provides tight bounds .",
    "we illustrate graphical representations of theorem [ th : e0_focusing ] in fig .",
    "[ fig : e0_focusing ] , as with figs .",
    "[ fig : region_p6_half ] and [ fig : renyi ] .",
    "theorem [ th : e0_focusing ] is a generalization of ( * ? ? ?",
    "* theorem 2 ) from ternary - input strongly symmetric channels to @xmath1-ary input uniformly focusing channels under a uniform input distribution .",
    "finally , we consider the hypothesis of a uniform input distribution .",
    "if a channel @xmath325 is symmetric , then the mutual information of order @xmath112 is maximized by a uniform input distribution for @xmath25 .",
    "therefore , since a strongly symmetric channel is symmetric , the hypothesis is optimal if the channel @xmath325 is strongly symmetric .",
    "in this study , we established the tight bounds of the @xmath0-norm with a fixed shannon entropy in theorem [ th : extremes ] , and vise versa in theorem [ th : extremes2 ] .",
    "previously , the tight bounds of the shannon entropy with a fixed error probability were derived @xcite .",
    "since the error probability is closely related to the @xmath5-norm , this study is a generalization of previous studies @xcite .",
    "note that the set of all @xmath1-ary probability vectors , which are sorted in decreasing order , with a fixed @xmath0-norm is convex set .",
    "the previous works @xcite used the concavity of the shannon entropy in probability vectors to examine the shannon entropy with a fixed @xmath0-norm . however , since @xmath32 is strictly concave in @xmath23 when @xmath286 and is strictly convex in @xmath23 when @xmath301 , the concavity of the shannon entropy in probability vectors turns out to be hard - to - use when the @xmath0-norm is fixed . in this study",
    ", we derived theorems [ th : extremes ] and [ th : extremes2 ] by using elementary calculus without using the concavity of the shannon entropy . 0 as application , we extend the bounds of theorem [ th : extremes ] from the @xmath0-norm to several information measures , which are determined by the @xmath0-norm , in corollary [ cor : extremes ] . as instances",
    ", we showed some applications of corollary [ cor : extremes ] in table [ table : extremes ] ; in particular , we illustrated the boundary of @xmath305 in fig .",
    "[ fig : renyi ] .",
    "in addition , we can apply corollary [ cor : extremes ] to several diversity indices , such as the index of coincidence .",
    "moreover , we presented further applications of corollary [ cor : extremes ] to uniformly focusing channels , defined in definition [ def : focusing ] , in section [ subsect : focusing ] .",
    "this study was partially supported by the ministry of education , science , sports and culture , grant - in - aid for scientific research ( c ) 26420352 .",
    "v. a. kovalevsky , `` the problem of character recognition from the point of view of mathematical statistics , '' _ character readers and pattern recognition .",
    "_ new york : spartan , pp . 330 , 1968 .",
    "( russian edition in 1965 ) .",
    "j. l. massey , _ applied digital information theory i and ii .",
    "_ lecture notes , signal and information processing laboratory , eth zurich , 19951996 . [ online ] .",
    "available at http://www.isiweb.ee.ethz.ch / archive / massey_scr/.      s. arimoto , `` information measures and capacity of order @xmath112 for discrete memoryless channels , '' in _ topics in information theory , 2nd colloq .",
    "j. bolyai _ ,",
    "keszthely , hungary , vol .",
    "16 , pp . 4152 , 1977 .",
    "y. sakai and k. iwata , `` feasible regions of symmetric capacity and gallager s @xmath3 function for ternary - input discrete memoryless channels , '' _ proc .",
    "inf . theory _ ( isit2015 ) , hong kong , pp . 8185 , june 2015 ."
  ],
  "abstract_text": [
    "<S> the paper examines relationships between the shannon entropy and the @xmath0-norm for @xmath1-ary probability vectors , @xmath2 . </S>",
    "<S> more precisely , we investigate the tight bounds of the @xmath0-norm with a fixed shannon entropy , and vice versa . as applications of the results , </S>",
    "<S> we derive the tight bounds between the shannon entropy and several information measures which are determined by the @xmath0-norm . </S>",
    "<S> moreover , we apply these results to uniformly focusing channels . </S>",
    "<S> then , we show the tight bounds of gallager s @xmath3 functions with a fixed mutual information under a uniform input distribution . </S>"
  ]
}