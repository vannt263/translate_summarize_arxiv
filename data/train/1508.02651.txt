{
  "article_text": [
    "state - space models are used in a variety of applications to describe phenomena in which the transmitted signals or acquired observations are noisy , corrupted , filtered , or in general altered due to processing , transmission , conversion or capture .",
    "the recovery of the original signal , the study of its properties and the estimation of the underlying parameters is important in applications , including signal processing , computer vision , neural networks , target tracking , statistics and financial modeling ( @xcite , @xcite , @xcite ) .",
    "long - memory state - space models are models in which the unobserved process exhibits long - range dependence .",
    "intuitively , this means that observations that are far apart in time are strongly correlated .",
    "this type of non - markovian models is used to describe phenomena in a variety of disciplines , such as hydrology ( noisy rainfall data or water accumulation in river gauges ) , traffic networks ( ethernet network data ) , computer vision ( image processing data ) and financial modeling .",
    "our interest in long - memory state - space models emerged when studying long - memory and/or rough stochastic volatility models in finance . more specifically , in this class of models the observations are the stock prices and are modeled by a markovian process , such as a geometric brownian motion in continuous time , while the volatility process is described by a long - range dependent process ( e.g. @xcite , @xcite , @xcite , @xcite ) or a rough process ( e.g. @xcite , @xcite ) . under this framework ,",
    "filtering of the unobserved volatility and parameter estimation are essential in derivatives pricing and hedging .    in the simpler context",
    "in which the state - space model is markovian and gaussian , kalman and extended kalman filters are used to linear and nonlinear smoothing , filtering and estimation and are very popular in terms of applications because of their recursive nature , which makes their implementation very efficient .",
    "the kalman filtering and , in general , bayesian - type methods are focused on updating the state equation using information that arrives sequentially . however , in more complex problems , where the state - space model is non - linear and non - gaussian , the posterior density is intractable , the kalman approach looses its recursive nature and one has to result in discrete numerical approximations in a mixture modeling type of framework .",
    "sequential monte carlo ( smc ) methods , also known as particle filters or recursive monte carlo filters , in their modern form , arose from the seminal work of gordon , salmond and smith ( 1993 ) , @xcite .",
    "smc methods are iterative algorithms that are based on the dynamic evolution and update of discrete sets of sampled state vectors , that are referred to as particles , that are associated with properly selected weights .",
    "the power and popularity of smc techniques relies on their generality , since they can be applied in a variety of frameworks , including cases where the state - space is constrained , their flexibility , as they allow for computation of all kinds of moments , quantiles , and their precision , which depends only on the number of particles used .    in the literature",
    ", there are two classes of approaches in smc : the sequential importance sampling / resampling ( sisr ) and the auxiliary particle filters ( apf ) .",
    "the idea of sis was present in the literature since 1970s and tracks back to the work of handschin and mayne ( 1969 ) , @xcite , and handschin ( 1970 ) , @xcite .",
    "however , this was not linked to filtering yet for two main reasons : the computational complexity of the sis algorithm and the problem known as sample impoverishment .",
    "the latter one refers to the weight degeneracy that happens as the number of iterations increases .",
    "this problem has been addressed in the literature , initially by gordon et al .",
    "( 1993 ) , @xcite , by the introduction of one additional re - sampling step , resulting in the so - called bootstrap filter .",
    "therefore , the sisr algorithm relies on a proposal distribution dependent on all the paths that were previously sampled , rather than the terminal state , and re - samples weights regularly according to an importance distribution .",
    "the role of the importance distribution is to reduce the variance and the re - sampling of the weights is necessary in order to reduce the accumulation of variance ( specially on the weights ) over time .",
    "the sisr approach in smc has been studied by several authors including doucet et al .",
    "( 2000 ) , @xcite , liu ( 2001 ) , @xcite and del moral ( 2004 ) , @xcite .",
    "the apf was originally introduced by pitt and shephard ( 1999 ) , @xcite , and employs an approximation of the predictive likelihood through an auxiliary variable that is weighted properly in an additional step .",
    "since it was initially introduced , there have been several improvements of this approach , mainly to reduce the variance of the proposed estimators , including but not limited to works by carpenter et al .",
    "( 1999 ) , @xcite , pitt and shephard ( 2001 ) , @xcite .",
    "for an introduction to the smc literature , there are several reviews and tutorials , including , the edited volume by doucet , freitas and gordon ( 2001 ) , the works of capp et al .",
    "( 2007 ) , @xcite , doucet and johansen ( 2009 ) , @xcite and kantas et al . (",
    "2010 ) , @xcite .    the study of the asymptotic properties of smc methods and the asymptotic properties of the filter as the number of particle increases is a quite hard problem .",
    "a form of consistency of the filter , when the number of particles tends to infinity is a common result in the majority of the literature , while central limit theorem type of results are fewer .",
    "one can refer to the works of del moral and guionnet ( 1999 , 2001 ) , @xcite , @xcite , to name a few . to the best of our knowledge , the most general result in the literature is due to chopin ( 2004 ) , @xcite . in his paper , chopin , derives a quite general central limit theorem that applies to most sequential monte carlo techniques in the literature , including the sisr and apf approaches , since the result does not depend on model assumptions , other than certain moment conditions .    in this article ,",
    "our goal is _ dual _ : first , we generalize the classical smc framework and sisr algorithm in order to incorporate the case that the unobserved process exhibits long - range dependence , which also encompasses processes with medium or short range dependence .",
    "more specifically , we extend the sisr algorithm to allow for dependent states , and we prove that the asymptotic properties of the particle filter are preserved even in the long - range dependent framework .    secondly , in the context of non - markovian state - space models , we address the problem of parameter estimation . in the literature , there are two main approaches : offline versus online methods , otherwise classified as bayesian versus maximum likelihood .",
    "the offline approach relies on a bayesian argument and consists of two steps : the approximation of the density of the parameter given the data using monte carlo and then the sampling of the parameter via an sis scheme .",
    "one of the problems is that smc looses its appealing sequential nature .",
    "a detailed review and comparison of these two methodologies can be found in the recent article by kantas et al .",
    "( 2014 ) , @xcite .",
    "our approach is based on the ideas of online estimation initially introduced by liu and west ( 2001 ) .",
    "generally speaking , the approach relies on augmenting the unobserved state by considering the parameter as an unobserved state .",
    "however , our approach is novel in two ways : first , we attack the parameter estimation problem in the case of long - range dependence in the model , by appropriately reformulating the original setup , and secondly we show that the estimators provided are consistent and asymptotically normal .    the rest of the paper is organized as follows : in section 2 , we introduce the mathematical formulation of the problem . in section 3.1 , we generalize the sisr algorithm in the long - range dependent case . in section 3.2",
    ", we introduce the sisr method with parameter learning and we present the theoretical results for the proposed parameter estimators . in section 4 , we study the performance of both methods using simulated data . in section 5 , we apply our method in estimating the unobserved volatility of a discrete - time stochastic volatility model with long - range dependence for s & p 500 data .",
    "finally , we summarize our results in section 6 .",
    "consider a state - space model in which the state vector is denoted by @xmath0 and the observations @xmath1 are obtained sequentially in time . in addition , we assume that the state vector depends on an unknown , but fixed , parameter vector that we denote by @xmath2 . in the sequel , we use the notation @xmath3 or @xmath4 for the random variables and @xmath5 or @xmath6 for the corresponding realized values .    unlike other models in the literature",
    ", we do not assume that the state vector @xmath0 is a markovian process .",
    "instead , we consider the case in which the unobserved process is not necessarily markovian , with particular interest in the long - range dependent case . formally , long - range dependence or long - memory is defined as follows :    for a stationary process",
    "@xmath0 , there exists a parameter ( hurst index ) @xmath7 , such that @xmath8 where @xmath9 is the autocorrelation function of the process .    when @xmath10 , then the process is markovian , so this is a generalization of the models that are treated in the smc literature .",
    "equivalently , long - range dependence implies that the autocorrelation function @xmath11 of a long - range dependent process is non - summable , that is @xmath12 .",
    "if the auto - correlation function is summable , then the process has what is called medium memory , in which case @xmath13 .    formally , at time @xmath14 , the state - space model is specified by the _ observation equation _ that is determined by the observation density @xmath15 and the _ state equation _ given by the conditional density @xmath16 where @xmath17 is an unknown vector of parameters , and @xmath18 is open .",
    "we assume that the observations @xmath19 are conditionally independent given @xmath0 and that the long - range dependent process @xmath0 has known initial density @xmath20 .    in this article ,",
    "our goal is to use simulation for online filtering . in other words ,",
    "we want to learn about the current state @xmath21 given available information up to time @xmath14 , which reduces to estimating the probability distribution function @xmath22 where @xmath19 are the observations up to time @xmath14 .",
    "however , since we assume that the parameter @xmath2 is unknown , at the same time , we also want to estimate @xmath2 .",
    "for an arbitrary sequence @xmath23 , we use the following notation @xmath24 . let us assume first that the parameter @xmath2 is known .",
    "we wish to recursively estimate the sequence of posterior distributions @xmath25 using sequential monte carlo techniques : @xmath26 consider @xmath27 at time @xmath14 to be the importance distribution that imputes @xmath21 ( with @xmath28 ) .",
    "the importance distribution @xmath29 is generally up to the user to choose , but it is clear from propositions [ p : clt_longrange_noparameter ] and [ p : clt_longrange_withparameter ] that its choice affects the variance of the estimator .",
    "ideally , one wants to choose an importance distribution @xmath29 that is proportional to @xmath30 in order to minimize the variance . a more detailed exposition of viable choices of @xmath29 can be found in @xcite . by adapting the algorithm proposed by johansen and doucet , @xcite ,",
    "a generic sequential importance sampling / re - sampling algorithm is as follows : + * at time @xmath31 *    1 .   _",
    "sampling : _ for @xmath32 , sample @xmath33 .",
    "re - sampling : _ for @xmath32 , set @xmath34 normalize @xmath35 , such that @xmath36 and re - sample @xmath37 where @xmath38 is the classical delta - dirac function .",
    "* at time @xmath14 , @xmath39 ( step @xmath40 ) *    1 .",
    "_ sampling : _ for @xmath32 , set @xmath41 and sample @xmath42 2 .",
    "_ re - sampling : _ for @xmath32 , set + @xmath43 + normalize @xmath44 such that @xmath45 and re - sample @xmath46    * output * the filtering distribution @xmath47 is approximated by @xmath48    notice that @xmath49 approximates @xmath50",
    ". +    1 .",
    "the reason why we choose to include a re - sampling step is because plain sis provides us with importance weights whose variance increases , as the number of particles increases , and in the non - markovian cases , this is no different .",
    "2 .   we also need to stress the fact that in the non - markovian case , re - sampling also leads to what is called sample impoverishment , which means that the number of distinct particles reduces after each step .",
    "let @xmath51 be an appropriate test function as in @xcite and assume that we want estimate @xmath52 the sisr algorithm provides us with the estimator @xmath53    so , it is natural to quantify the performance of the algorithm by studying the convergence of @xmath54 to @xmath55 as @xmath56 . in the markovian case , this result",
    "is completely covered by the results of @xcite .",
    "following @xcite we define the set of appropriate test functions under which a central limit theorem can be established , appropriately formulated for our case of interest : @xmath57    the long range dependence is not explicitly mentioned there , even though the statements and proofs of @xcite immediately extend to the present long range dependence case , providing us with the following proposition .",
    "[ p : clt_longrange_noparameter ] let us assume that there exists @xmath58 such that for every @xmath59 @xmath60 and consider @xmath61 .",
    "then , we get @xmath62 as @xmath56 , where at time @xmath31 @xmath63 and for @xmath64 @xmath65    notice that in contrast to the markovian case , in the formula for @xmath66 the conditional distributions on the @xmath67algebra @xmath68 depend on the whole history @xmath69 up to time @xmath70 . the details of the proof are omitted since it is a straightforward extension of the proof of proposition 2.2.1 of @xcite .      in this article , apart from filtering for the unobserved states we also want to estimate the unknown parameter vector @xmath2 on which the state vector depends .",
    "our approach will be to consider @xmath2 as an additional state and thus our goal will be to estimate the posterior distribution @xmath71 given by @xmath72 where @xmath73 is a prior density for the parameter vector @xmath2 .",
    "if the parameter is known , then the density is degenerate and reduces to .",
    "therefore , the additional difficulty here is that we need to compute or approximate the theoretical density function @xmath74 .",
    "one approach in the literature ( @xcite , @xcite , @xcite , @xcite ) , is to consider that @xmath2 is not fixed and assume that it artificially evolves in time , for example @xmath75 where @xmath76 is an artificial white noise with decreasing variance .",
    "then , at each time @xmath14 , @xmath74 will be updated inside the sisr algorithm in order to incorporate the additional information that is obtained .    as it was discussed in liu and west , @xcite",
    ", this approach leads to artificial variance inflation , since the parameter is not truly random .",
    "however , we use a kernel density estimate with shrinkage correction in order to control this artificial over - dispersion .    more specifically , standing at time @xmath14",
    ", we approximate @xmath73 by a set of samples @xmath77 and weights @xmath78 using a discrete monte carlo .",
    "the index @xmath14 in @xmath2 is in parenthesis to indicate that @xmath2 does not evolve in time , but that its value is drawn from the posterior density @xmath74 at time @xmath14 .",
    "then , the smooth kernel density with shrinkage correction will be of the form @xmath79 where @xmath80 denotes a multivariate normal density with mean @xmath81 and variance @xmath82 .",
    "so , essentially , @xmath83 is approximated by a mixture of normals with mean mean @xmath84 and variance @xmath85 , weighted by sample weights @xmath86 .",
    "the kernel location is specified by @xmath87 where @xmath88 and @xmath89 denotes the average over all parameter samples . regarding @xmath90 ,",
    "a typical choice would be a decreasing function of the sample size , but if one wants to control the loss of information then @xmath91 , where @xmath38 is a discount factor typically around 0.950.99 and @xmath92 becomes @xmath93 .",
    "therefore , the sisr algorithm is adjusted in order to incorporate the update of @xmath2 .",
    "the key idea of our approach that also allows us to establish asymptotic consistency and normality of the estimators , is to re - formulate the weights so that they represent the joint posterior @xmath94 and update @xmath2 along with the state vector @xmath3 :    * at time @xmath31 *    1 .   _",
    "sampling _ + for @xmath32 , sample @xmath33 and @xmath95 .",
    "2 .   _ re - sampling _ + for @xmath32 , set @xmath96 normalize @xmath35 , such that @xmath36 and re - sample @xmath97    * at time @xmath98 ( step @xmath40 ) *    1 .",
    "_ sampling _",
    "+ for @xmath32 , set @xmath99 where @xmath100 , sample @xmath101 and @xmath102 where @xmath103 .",
    "_ re - sampling _ + for @xmath32 , set @xmath104 and normalize @xmath105 , such that @xmath45 .",
    "+ for @xmath32 , re - sample @xmath106 and set @xmath107    * output * the filtering distribution @xmath47 is approximated by @xmath48 and the estimator for @xmath2 is @xmath108 .",
    "we also record the approximation for the combined distribution @xmath109 which is approximated by @xmath110      let us now study the convergence properties of this algorithm .",
    "we are seeking for a result similar to proposition [ p : clt_longrange_noparameter ] .",
    "let @xmath111 be an appropriate test function .",
    "notice now that the sisr algorithm provides us with the estimator @xmath112    it is relatively straightforward to see that @xmath54 is estimating @xmath113 where @xmath114 is the value that is drawn from the posterior density @xmath73 at time @xmath14 .",
    "so , it is natural to quantify the performance of the algorithm by studying the convergence of @xmath54 to @xmath55 as @xmath56 .",
    "following the proof of the central limit theorem results of @xcite for the markovian case , without the parameter estimation aspect , the following result is derived .",
    "[ p : clt_longrange_withparameter ] let us assume that there exists @xmath58 such that for every @xmath59 @xmath115 and consider @xmath61 .",
    "then , we get @xmath62 as @xmath56 , where at time @xmath31 @xmath116 and for @xmath64 @xmath117    the proof follows from proposition a.1.1 of @xcite after making the adequate identifications .",
    "indeed , for a general sequential importance sampling algorithm with weights @xmath118 , proposition a.1.1 of @xcite implies that the formula for the variance in question is given by @xmath119-\\bar{\\phi}_{t}\\right)\\right ]   + \\text{var}_{\\rho_{t}}\\left[w_{t}(\\phi_{t}-\\bar{\\phi}_{t})\\right].\\ ] ]    in our case we have @xmath120 and the weights take the form @xmath121 plugging these expressions in the formula for @xmath122 one immediately recovers the form of @xmath66 , completing the proof of the proposition .",
    "essentially , @xmath2 is viewed as an augmented state variable .",
    "proposition [ p : clt_longrange_withparameter ] quantifies the convergence of the filter , but it does not discuss the statistical properties of @xmath123 .",
    "let us recall that @xmath124 where @xmath125    by inspecting the algorithm it becomes clear that the convergence properties of @xmath123 as @xmath56 is described by a statement very similar to that of proposition [ p : clt_longrange_withparameter ] after making the choice @xmath126 .",
    "the first part of proposition [ p : clt_parameter ] shows that at time @xmath14 , the estimator for @xmath2 , @xmath127 converges to @xmath128 as @xmath56 .",
    "so , it is natural to ask whether @xmath108 is a consistent estimator of @xmath2 as @xmath129 .",
    "this is addressed in the second part of proposition [ p : clt_parameter ] . in particular , the answer is positive and it comes from doob s consistency theorem , see for example theorem 10.10 in @xcite .",
    "we recall here that the sequence of of posterior measures @xmath130 is called consistent under @xmath2 , if under @xmath131-probability , it converges in distribution to the measure @xmath132 that is degenerate at @xmath2 in probability .",
    "in particular , we have the following result .    [",
    "p : clt_parameter ] let us assume that there exists @xmath58 such that for every @xmath59 @xmath115 and consider the function identity @xmath133 , assuming that it belongs to the set @xmath134 in ( [ eq : admissibletestfunctions ] ) .",
    "let us define the mean of the posterior distribution @xmath135 @xmath136 then , we get @xmath137 as @xmath56 .",
    "moreover , if the model @xmath138 is identifiable , i.e. , if @xmath139 for @xmath140 , then the posterior mean @xmath108 consistently estimates the true parameter value @xmath2 , as @xmath129 .",
    "the asymptotic variance @xmath141 is defined as follows . at time @xmath31 @xmath142 and for @xmath64 @xmath143    this proposition is essentially a special case of proposition [ p : clt_longrange_withparameter ] with @xmath126 .",
    "we notice that @xmath144 which is the mean of the posterior distribution @xmath135 , as claimed .",
    "then , by doob s consistency theorem , see for example theorem 10.10 in @xcite , we have that if the model @xmath138 is identifiable , i.e. , if @xmath139 for @xmath140 , then for every prior probability measure @xmath145 on @xmath146 the sequence of posterior measures @xmath130 is consistent for @xmath147almost every @xmath2 .",
    "this concludes the proof of the proposition .",
    "the _ fractional arima ( autoregressive integrated moving average ) _ process was proposed by box and jenkins , @xcite , in 1970 and has been very popular in applied time series .",
    "a fractional arima(@xmath148 ) process is formally defined as follows ( due to granger and joyeux , @xcite ) :    let @xmath149 and @xmath150 be polynomials of orders @xmath151 and @xmath29 respectively and @xmath152 a stationary process such that @xmath153 @xmath154 and and @xmath155 is a sequence of iid variables with mean @xmath156 and variance @xmath157 .",
    "then , the process @xmath158 is called a fractional arima(@xmath159 ) process .",
    "in contrast to the classical arima(@xmath148 ) process , where the parameter @xmath160 is an integer , in the fractional case @xmath160 is a real valued parameter with values between @xmath161 .",
    "it is called the fractional integration parameter and is related to the hurst index , @xmath162 in , via @xmath163 .",
    "@xmath164 denotes the lag or backshift operator , and @xmath165 where the sum is taken over an infinite number of indices .",
    "the fractional arima process is long - range dependent when @xmath166 , while the upper bound on @xmath160 is needed to ensure that the process is stationary .",
    "more details regarding these models can be found in beran @xcite .    in our framework",
    ", we consider a state - space model in which the unobserved process is modeled by a fractional arima@xmath167 process .",
    "specifically , the state - space model is defined as follows @xmath168 where @xmath169 and @xmath170 are two independent iid sequences of gaussian random variables and @xmath171 is a known function .",
    "+      we apply our algorithm to simulated data from an arima(1 , 0.3 , 0 ) model with parameter @xmath172 .",
    "that is @xmath173 the simulated model is shown in figure [ figsim1](a ) and the estimated filter using the sisr algorithm is depicted in figure [ figsim1](b ) .    [",
    "cols=\"^,^ \" , ]",
    "to summarize , in this article we extended the standard sisr algorithm to incorporate the case that the observations are long - range dependent .",
    "our findings show that the results are very close to the case that the observations are independent or markov . however , the main drawback of this method is the computational time that is required to perform the iterations .",
    "since we need to take into account , and technically speaking to store all past values of the trajectory , this increases the computational time and complexity of the method .",
    "in addition , by naturally extending existing results in the literature , we proved that the filter converges to the true distribution of the unobserved process .",
    "our second outcome , was the development of an sisr algorithm that along with the estimation of the unobserved distribution of the hidden process , it also estimated unknown model parameters .",
    "our approach was dynamic , in the sense that the parameter was regarded as `` time - varying '' and thus the parameter estimators were updated at every step of the algorithm .",
    "we also showed that the proposed estimators for the unknown parameter are consistent and asymptotically normal and we corroborated these results with a simulation study .",
    "there are quite a few open problems that we would like to investigate in the future .",
    "the first one is to study ways to improve the computational efficiency of the algorithm . in our approach",
    ", we used all the history of the trajectory to run the algorithm , which severely affected the computational efficiency , but it would be interesting to investigate if a  window \" approach would provide us with a reasonable estimator for the filter and/or the parameter , and possibly quantify the loss that one might have by doing so in terms of accuracy .",
    "in addition , one question that we did not address in this paper , is what happens with the long memory parameter in practice . in our approach",
    ", we assumed that @xmath160 ( or equivalently @xmath162 ) is known ( given or estimated from the data ) .",
    "however , it is an open question how one would consistently estimate the memory parameter in the scenario that the long - range dependent process is hidden .",
    "kantas , n. , doucet , a. , singh , s. s. , maciejowski , j. m. , chopin , n. , an overview of sequential monte carlo methods for parameter estimation in general state - space models . _ submitted to statistical science _ , 2014 .",
    "tanizaki , h. , nonlinear and non - gaussian state - space modeling with monte carlo techniques : a survey and comparative study , in _ handbook of statistics 21 .",
    "stochastic processes : modelling and simulation _ ,",
    "shanbhag , d. n. and rao , c. r. , eds .",
    "elsevier , 2003 , 871929"
  ],
  "abstract_text": [
    "<S> we consider a state - space model that is specified up to an unknown vector of parameters and in which the unobserved state process is non - markovian . </S>",
    "<S> our goal is to estimate both the state process and the parameter vector . for this , we propose a sequential monte carlo method that is based on smoothing the sample points of model parameters . </S>",
    "<S> following a dynamic approach , we also estimate the unobserved parameters of the model . </S>",
    "<S> we establish a central limit theorem for the state and parameter filter and we study asymptotic properties ( consistency and asymptotic normality ) for the filter . </S>",
    "<S> we illustrate our results with a simulation study and we apply our method to estimating the volatility of a long - range dependent model for s & p 500 data . </S>"
  ]
}