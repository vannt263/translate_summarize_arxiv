{
  "article_text": [
    "n - body codes are one of the most important tools of theoretical cosmology @xcite because they offer the possibility of simulating most of the gravitational processes driving the formation of the large scale structure of the universe ( hereafter lss ) @xcite@xcite@xcite .",
    "these simulations are often used to check cosmological models and to constrain the free parameters of these models which can not be fixed either theoretically or observationally .",
    "+ the typical mass scale for gravitational instability , the jeans mass , has a value of @xmath12 solar masses ( 1 solar mass @xmath13 ) at the recombination epoch , and it gives the approximate size of the first objects forming by gravitational collapse at that epoch . on the other hand , the largest structure we see in our universe today , the `` supercluster '' of galaxies , has a mass in excess of @xmath14 solar masses .",
    "moreover , the gravitational force has a truly long - range character , which makes it impossible to introduce reasonable upper cutoffs in the mass range .",
    "for all these reasons , one would like to be able to perform simulations spanning more than 12 orders of magnitude in mass , but present - day state - of - the - art software and hardware technology does not allow simulations with more than @xmath15 bodies . for these reasons ,",
    "the quest for increasingly efficient algorithms is still in progress .",
    "however , the importance of making n - body simulations is clear to several authors @xcite@xcite@xcite . during the past years n - body codes",
    "have been much improved and applied successfully to various problems in galaxy dynamics , galaxy formation , and cosmological large structure formation .",
    "nevertheless , the computational expense has remained prohibitive for @xmath16 , even using tree - based algorithms on the most powerful computers .",
    "+ the situation is even worse for other n - body algorithms .",
    "the n - body direct evolution method scales as @xmath17 , which makes it impossible to run simulations with more than @xmath18 particles . to overcome this difficulty , and when high accuracy is required , alternative numerical methods based on hierarchical force - computation algorithms are widely used .",
    "the recent effort has addressed the production of new software and algorithms for the new generation of high - performance computer systems .",
    "the ultimate target is an implementation of the tree n - body algorithm to run simulations with higher accuracy and particle number , decreasing the cost of the simulation in terms of cpu time and increasing performance in terms of number of particles / second elaborated when running on mpp systems .",
    "+ among the tree algorithms designed to compute the gravitational force in n - body systems , one of the most used and powerful in modern cosmology is that by barnes and hut ( bh ) @xcite .",
    "the bh octal - tree recursive method is inherently adaptive and allows one to achieve a higher mass resolution even if parallel implementation of this algorithm @xcite@xcite suffers from a serious drawback : it can easily run into imbalance as soon as the configuration evolves , causing performance degradation . in this paper",
    "we present a modified version of the bh algorithm in which we have introduced an enhanced grouping strategy .",
    "we will show how this feature allows an increase in performance when we consider n - body simulation with a large number of particles ( @xmath19 ) .",
    "the code we present incorporates fully periodic boundary conditions using the ewald method , without the use of fast fourier transform techniques @xcite .",
    "+ in section 2 we give a brief description of our n - body parallel code , based on the bh tree algorithm , and the dynamic load balance ( dlb ) policy adopted . in section 3 we describe our enhanced grouping strategy . in section 4",
    "we show the results of our tests and in section 5 we report our conclusions .",
    "since the publication of the monograph by hockney and eastwood in 1981 @xcite , a new class of particle simulation methods @xcite@xcite@xcite@xcite@xcite has emerged as an alternative to particle - particle ( pp ) @xcite@xcite@xcite particle - mesh ( pm ) ( for a review of this method see @xcite ) and particle - particle - particle - mesh ( @xmath20 ) @xcite methods .",
    "these new methods are characterized by the particles being arranged into a hierarchy of clusters , which span the full range of length scales from the minimum interparticle spacing up to the diameter of the entire system .",
    "these methods are usually known as _ tree methods _ or _ tree codes _ because of the data structures used . with these new methods ,",
    "the short - range force on a particle @xmath5 is calculated as a direct sum over nearby particles .",
    "remote bodies are organized into groups which become progressively larger with the distance from the particle ; then a multipole expansion of the potential of each cluster about its center of mass is performed .",
    "the long - range contribution to the acceleration is given by the sum of the particle - cluster interactions .",
    "the bh algorithm works using a hierarchy of cubes arranged in an octal - tree structure ; that is , each node in the tree has eight siblings and each node represents a physical volume of the space .",
    "the total mass of all particles within a given volume and their centers of mass are stored at the corresponding node .",
    "thus , the system is first surrounded by a single cell ( cube ) encompassing all the particles . this main cell ( called _ root",
    "is subdivided into eight subcells of equal volume , each containing its own subset of particles . each subcell in turn is subdivided into eight new subcells and so on .",
    "this procedure is repeated until each cell at the lowest level contains only one particle .",
    "the force on any given @xmath5 is then the sum of the forces by the nearby particles plus the force by the distant cells whose mass distributions are approximated by multipole series truncated typically at the quadrupole order @xcite .",
    "the criterion for determining whether a cell is sufficiently distant for a multipole force evaluation ( that is , for approximating the cell as a multipole ) is based on an opening angle parameter @xmath21 given by @xmath22 where @xmath23 is the size of the cell and @xmath24 is the distance of @xmath5 from the center of mass of the cell .",
    "smaller values of @xmath21 lead to more cell opening and more accurate forces ( for @xmath25 we have an error lower than 1% on the accelerations @xcite ) .",
    "the equations of the dynamics are solved using the leapfrog integrator .",
    "+      in our parallel implementation of the bh tree algorithm , using the pghpf / craft ( an implementation of high performance fortran by the portland group ) programming environment for the cray t3e system , we have exploited both the _ data sharing _ and the _ work sharing _ programming models .",
    "the flexibility of the pghpf / craft environment allows one to mix these two modes in order to gain the maximum efficiency and speed - up .",
    "we can distinguish two main phases in our code structure : the tree_formation ( tf ) and the force_compute ( fc ) .",
    "a data distribution in contiguous blocks    !",
    "distribute particle_attribute(block , * )    and alternatively , a fine grain distribution    ! hpf$ distribute tree_attribute(cyclic , * )    were adopted to distribute the particle data properties and the tree data properties .",
    "hpf$ distribute directive of the pghpf / craft compiler allows us to consider an array like particle_attribute ( or tree_attribute ) as a unique large array , accessible from all the processors , the array being physically distributed in the local memory of all the processors .",
    "we used two different sets of initial conditions , namely uniform and clustered distributions having 2 million particles each , and they were carried out using from 16 to 128 pes .",
    "our results show that the higher code performances are obtained using a fine grain tree data distribution and a coarse grain bodies data distribution .",
    "a detailed description can be found in @xcite .",
    "the static array distribution , fixed as described above , allows each pe to cooperate during the tf phase by using principally the do independent structure of pghpf that is a synchronous mechanism , and then to execute the fc phase in asynchronous mode . to minimize the communication overhead",
    ", each pe executes the fc phase mainly on the local residing bodies .",
    "the block distribution arranges bodies with the nearest logical number ( near in the space ) in the same pe local memory , or in the nearest pes . using the above mentioned data distribution",
    ", each pe has a block of closed bodies in the local memory ( @xmath26 , where n$pes is the number of processors used for the simulation);in an initial condition with a uniform distribution , the pes having extreme numeration in the pool of available pes have a lower load at each time - step .",
    "the load imbalance is enhanced when a clustered situation occurs during the system evolution .",
    "the pes having bodies in clustered regions have a greater workload since the load of the fc phase increases as the mass density grows .",
    "the technique we follow to perform a load redistribution among the pes is to assign each pe to execute this phase _ only _ for a fixed portion of the bodies residing in the local memory @xmath27 given by + @xmath28 where @xmath29 const .",
    "( @xmath30 ) .    the fc phase for all the remaining bodies + @xmath31 is executed by all the pes that have concluded the fc phase for the assigned @xmath27 bodies .",
    "no correlation is considered between the pe memory location of the body belonging to the @xmath32 set and the pe that computes the fc phase on it .",
    "the results imply that it is possible to fix a @xmath33 value that allows the best code performances .",
    "data already presented in @xcite show that it is convenient to fix the @xmath33 value near @xmath34 , which is the value that maximizes the load balance for n - body simulations of the lss both in uniform and clustered situation .",
    "our work and data sharing - parallel tree ( wdsh - pt ) code is principally aimed at running lss cosmological simulations with a number of particles as high as possible using supercomputers such as cray t3e systems . in order to increase the code efficiency ,",
    "we adopt initially the grouping method proposed by barnes @xcite and introduce a modified implementation of his grouping policy yielding very high gains in the code performances with the same accuracy .",
    "to compute the force on a body , the bh algorithm needs to build an interaction list ( @xmath35 ) for each particle @xmath10 .",
    "starting from the root cell , a tree inspection is done and the opening angle parameter @xmath36 is used to evaluate whether a cell must be opened or closed as mentioned above . if a cell has dimension @xmath23 and distance @xmath24 from the particle @xmath10 so that eq .",
    "( [ eq : un ] ) is verified , the cell is closed , it is added to the @xmath35 , and its subcells are not investigated further . otherwise the cell is opened and its subcells are investigated in the same way .",
    "bodies belonging to an opened cell are added to the @xmath35 .",
    "+ next , the force on the body is computed using the monopole and quadrupole momenta for all the cells in the list . +    [ [ bh - timing ] ]",
    "bh timing + + + + + + + + +    the tree inspection phase represents a sizeable task to compute the force because the cell opening criterion is applied many times for each particle .",
    "the cpu time @xmath37 to compute the force in a time - step for all the @xmath38 particles is    @xmath39    where @xmath40 is the average time to build an @xmath35 and @xmath41 is the average time to compute the force on each particle using the interaction list .",
    "+    [ [ b90-timing ] ] b90 timing + + + + + + + + + +    the basic idea of b90 was to build a unique interaction list that allows the force for a _ group _ of particles inside a region ; i.e. , a cell @xmath2 of the tree _",
    "( grouping cell ) _ , to be computed reducing the number of tree inspections to build the @xmath42 .",
    "b90 builds an il that applies everywhere within @xmath2 and reuses this il for each particle @xmath43 in turn . in this way it is possible to reduce the tree inspection phase .",
    "the cpu time @xmath44 for b90 may be written as    @xmath45    where    * @xmath46 is the number of grouping cells ( assuming that each body is inside a group region ) ; * @xmath47 is the average time to build an interaction list for a group ; *   @xmath48 is the average time to compute the force on a particle using the list formed for the group .    in the following paragraphs",
    "we will compare the @xmath44 time with the @xmath37 time considering the generic case @xmath49 .",
    "we notice that different values of @xmath36 give similar results , as shown by the accompanying figures .",
    "[ [ b90-opening - criterion ] ] b90 opening criterion + + + + + + + + + + + + + + + + + + + + +    the original bh algorithm adopts an opening criterion @xmath50 , based on the distance between the position of the @xmath10 particle and the center of mass of the remote cells , the @xmath35 length ( @xmath51 ) being proportional to @xmath52 . in order to have the same accuracy as the original algorithm , the interaction list of the grouping cell is formed using eq .",
    "( [ eq : un ] ) but now the @xmath24 term is computed in terms of the distance from the center of mass of an inspected cell and the edge of the grouping cell , as shown in fig . 1 ( @xmath53 is used instead of @xmath54 ) .",
    "this implies that the @xmath35 formed using the grouping cell contains more elements than the @xmath35 formed by applying the original bh algorithm .",
    "+    [ [ b90-interaction - list - increment ] ] b90 interaction list increment + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the b90 adopts an opening criterion ( @xmath55 ) based on the distance between the edge of @xmath2 and the center of mass of the remote cells . in this case the @xmath56 length will be proportional to @xmath57",
    ". moreover , the b90 criterion uses @xmath55 numerically equal to @xmath50 when using the original bh algorithm .",
    "we found experimentally the relation between @xmath55 and @xmath50 , using 2 million particles in a uniform distribution ( see fig .",
    "2 ) : this relation agrees with data in salmon @xcite .",
    "a typical value used as opening criterion to run simulations for the lss is @xmath58 .",
    "consequently we consider @xmath59 , which , in terms of @xmath35 length increment , corresponds to running a simulation with @xmath60",
    ". +    [ [ b90-timing - vs - bh - timing ] ] b90 timing vs bh timing + + + + + + + + + + + + + + + + + + + + + + +    figs .",
    "3 and 4 show the relationship @xmath61 and @xmath62 with @xmath36 ranging between @xmath63 and @xmath64 . considering @xmath65 and @xmath66 with @xmath67 , eq .",
    "( [ eq : du ] ) may be rewritten using the above relations as follows :    @xmath68    where @xmath69 , and @xmath70 is the average number of particles in a grouping cell .",
    "+ for a large number of systems and in particular for our wdsh - pt code running on the t3e system , @xmath41 ranges from @xmath71 to @xmath72 at @xmath49 . considering these figures ,",
    "we obtain from eqs .",
    "( [ eq : tre ] ) and ( [ eq : tgl1 ] ) , respectively ,    @xmath73    and    @xmath74    and then @xmath75 . +",
    "a real gain could be obtained using b90 if the cpu time spent to form the interaction list were longer than the phase to compute the force on the particle .",
    "the results reported in @xcite demonstrate that if @xmath76 the code performance is between two and three times faster of the bh algorithm , and a good choice for the @xmath77 value is about 32 @xcite .",
    "we will now describe the modification we introduce in the 1999 version of our wdshpt code ( wd99 ) to increase performance even if @xmath78 .",
    "the basic idea is to assign the same @xmath56 to each particle within a cell @xmath2 , containing a maximum of @xmath3 particles .",
    "+ we will not use the b90 criterion to build the interaction list .",
    "instead , we will use the same @xmath50 criterion used in the original bh algorithm .",
    "this criterion is applied to a hypothetical particle placed in the center of mass of the @xmath2 , hereafter vb ( virtual body ) ( fig .",
    "moreover , we consider the @xmath56 as formed by two parts given by @xmath79 @xmath80 and @xmath81 being two subsets of the interaction list . an element is included in one of the two subsets , using the following sphere criterion for all the elements that satisfy eq .",
    "( [ eq : un ] ) .",
    "@xmath82 +   +   + @xmath83 + @xmath84 +   + @xmath81 +   +    moreover all @xmath43 are included in @xmath81 .    using the two subsets it is possible to compute the force @xmath6 on a particle @xmath4 as the sum of two components , @xmath85    where @xmath8 is a force component due to the elements listed in @xmath84 and @xmath9 is the force component due to the elements in @xmath81 .",
    "we assume the component @xmath8 to be the same for each particle @xmath43 and compute it considering the gravitational interaction between the vb and only the elements listed in @xmath84 , while the @xmath9 component is computed separately for each @xmath10 particle by the direct interaction with the elements listed in @xmath81 .",
    "moreover , @xmath9 contains a restricted number of elements in comparison with the @xmath8 list , so we expect a net gain in performance even if @xmath86 .",
    "the gain that is possible depends on several parameters ( @xmath77 , the size of the @xmath2 and the _ sphere radius _ ) , whose ranges of variation are constrained by the maximum allowed value of the overall error of the method , as we will describe in the following sections .      before showing the performance of our wd99 procedure in an n - body simulation of the large scale structure of the universe ,",
    "it is important that we perform an error analysis of the procedure itself .",
    "considering that the cumulative error , when simulations for the lss studies are run using the original bh algorithm , is lower than @xmath87 @xcite , fixing the opening criterion @xmath88 , we will give some constraint concerning the size of @xmath2 , the @xmath77 value , and the _ sphere radius _ needed to have negligible cumulative error .",
    "the following sub - sections discuss the two main sources of error .",
    "the first error source is that wd99 uses the sphere criterion and the vb to create an interaction list @xmath89 and wd99 applies the @xmath90 to all bodies @xmath43 .",
    "this approximation could introduce an error in the force value on the @xmath10 particle if the @xmath91 , created using the original bh algorithm , and the @xmath90 have a difference in the elements greater then @xmath87 . as we found with our tests , in order to decrease this difference it is necessary to limit the size of @xmath2 that is equivalent to fixing a _ critical level _ of the tree structure : cells above the critical level can not form a grouping cell .",
    "the user has to fix the critical level considering the density of local bodies in the box where bodies are arranged : the critical level must be chosen in such a way as to make the difference between @xmath91 and @xmath90 negligible ( no more than @xmath87 of the elements ) .",
    "it seems reasonable for a lss simulation in a 50 mpc box with more than 2 million particles to fix the critical level as the sixth level of the tree .",
    "the next section shows in detail the obtained results . in any case",
    ", the cumulative error is very small considering the increase in accuracy compared with the original bh algorithm , due to the inclusion of all @xmath43 in the interaction list .",
    "the second error source is due to the assignment of @xmath8 , computed for the vb , to each @xmath43 .",
    "we found that the sphere criterion allows us to reduce this error to values much lower than 0.01% for @xmath92 if the dimension of the @xmath2 cell is fixed with the critical level as mentioned above , and the _ sphere radius _ is three times the radius of the sphere enclosing the @xmath2 cell ( fig .",
    "+ another important constraint to be fixed is the value of @xmath77 .",
    "all the elements @xmath43 are listed in the @xmath81 list and there is a direct body - body interaction among the @xmath93 ( @xmath94 ) elements forming the group .",
    "this introduces a term @xmath95 in the algorithm complexity . in order to avoid a decrease of the code efficiency and to maintain a good code accuracy , as with the original bh algorithm",
    ", it seems reasonable , running lss simulation with more than 1 million particles , to maintain @xmath96 .",
    "we adopted , in our runs , a safe value @xmath97 even if we obtained good results with @xmath98 .",
    "+ in the next section we show the errors obtained using the above - mentioned constraints when applying wd99 to lss simulation with both uniform and clustered distributions .",
    "we carried out many tests to estimate the error introduced in the wd99 and obtained increased performances using several values of @xmath77 .",
    "therefore , this section is subdivided as follows : first we test whether our algorithm increases the average length of the interaction list , then we measure the resulting percentage error , and we conclude with an overall performance analysis . as a test case we ran a simulation using 2 million particles for lss in a cubic region of 50 mpc , starting from a homogeneous initial condition ( redshift @xmath99 ) and reaching a clustered configuration ( redshift @xmath100 ) .",
    "we used an opening angle parameter @xmath36 ranging from @xmath101 to @xmath64 .",
    "our tests were executed on a cray t3e system and the results will be shown in the following sections .      the aim of this first test is to verify that the wd99 algorithm does not introduce a significant computational cost when the force for a generic particle is computed .",
    "this measurement is substantially performed on the average length of the @xmath35 we form adopting our code .",
    "6 reports the result we obtain when the simulation evolves at redshift @xmath99 .",
    "tests were executed for several values of redshift , but the differences between bh and our algorithm was computed only at the end of the run .",
    "the curves were obtained by fixing @xmath98 and varying the critical level from 5 to 8 . in all cases",
    "the differences we obtained are negligible , which means that the computed @xmath35 for the vb ( with the adopted sphere criterion ) is about equal to the @xmath35 we obtain for a generic particle with the original bh algorithm .",
    "the first important result is that wd99 does not produce any increment in the il length and consequently @xmath102 .",
    "we carry out this measurement in two phases .",
    "first we run a single time - step of the 2-million - particle simulation at redshift @xmath99 .",
    "we compare the values we obtain running the bh original algorithm and the wd99 . as a reference case",
    ", we adopt the critical level equal to 6 .",
    "a similar comparison is made at @xmath103 and the bh and wd99 histograms of the forces of each component are compared .",
    "the comparison shows a negligible difference in the force distribution in a single time - step , at least an order of magnitude less than the error of the original bh algorithm .",
    "+ the second measurement is made analysing an entire system evolution .",
    "we start with the initial condition of 2 million particles with redshift @xmath99 , @xmath104 , @xmath67 , and particle mass about @xmath105 solar masses .",
    "the system evolution is carried out up to redshift @xmath100 .",
    "the evolution is executed with the original bh algorithm and with the wd99 code . as reference case",
    ", we adopt the critical level equal to 6 .",
    "we measure the absolute error @xmath106 in the position of particles and in the velocities of particles in the mean square sense ,    @xmath107    and    @xmath108    the study of the final evolution is described in the next sub - section . here",
    "we give the measured value at the end of the simulation : @xmath109 and @xmath110 .",
    "similar values are measured when running simulations with more than 2 million particles and with a critical level equal to 6 .",
    "the obtained @xmath106 values lead us to conclude that the wd99 procedure does not introduce significant errors in comparison with the bh algorithm .",
    "the final stages obtained running simulations with the bh algorithm and the wd99 code are very similar .",
    "the two - point correlation function is defined as    @xmath111    @xmath112 being the number of pairs of particles with separations between @xmath113 and @xmath114 , @xmath115 the volume considered , @xmath116 the particle number taken as centres , and @xmath117 the mean particle density .",
    "we calculate this function at redshift @xmath103 for wd99 and bh algorithms .",
    "the values we obtain are perfectly equal , and the substructures we form ( number and size ) are identical .      to conclude our wd99 description , we report the performances measured for the wd99 code ( fig .",
    "7 ) ( including the boundary periodic conditions using the ewald method @xcite ) and the performances of the original bh algorithm .    the measured performances lead us to the conclusion that when the system evolution is clustered ( @xmath103 ) the wd99 does not decrease the performance as the bh algorithm .",
    "this effect is due to the nature of the wd99 algorithm , which has a structure that increases the efficiency when clusters of particles are well closed .",
    "this important effect allows us to run simulations with very clustered systems , obtaining very good performance and negligible errors . moreover , the efficiency of the wd99 increases by a factor of up to five at the redshift @xmath103 .",
    "the gain is enhanced when bigger simulations are run : a recent simulation with 16 million particles performed on the cray t3e system using wd99 showed an increase in performance by a factor of 7 at the redshift @xmath103 .",
    "we note that the gain obtained , in comparison with that obtained by the original bh algorithm , is greater using a lower critical level ( 5 or 6 ) .",
    "the gain is incremented using a sphere criterion with _ sphere radius _ lower than the value we consider , having only a small increment in the global error .",
    "the code wd99 is mainly used for lss studies , but it could be tested and used for other applications where accuracy not higher than 1% is necessary . considering the high performances we obtained , the wd99 method may be very successfully applied when clustered configurations such as galaxies or clusters of galaxies have to be studied .",
    "the new approach could be applied also to other fields of physics where collisionless systems are to be simulated , as in plasma and hydrodynamic studies .",
    "+ the code is written in fortran 90 with pghpf / craft , but the latest version ( written in f90 and c languages ) uses the one - side communication library shmem , allowing it to run on the origin 2000 systems .",
    "a new version will be implemented using dynamical array allocation , and we are studying the implementation of the parallel out - of - core @xcite , moving data in the disk . this version will be developed for a cc - numa machine with mpi-2 . we plan to have a freely available version of wd99 in october 2000 .",
    "all the tests were carried out using the cray t3e 1200/256 machine at the cineca ( casalecchio di reno ( bo ) , italy ) , a 256 pe system , using the financial support of the italian consortium cnaa ( consorzio nazionale per lastronomia e lastrofisica ) .",
    "we thank dr .",
    "g. erbacci of cineca and dr .",
    "a. f. lanza of catania astrophysical observatory for their useful help .",
    "aarseth , _ astrophys .",
    "space sci . _ * 14 * , 118 ( 1971 ) .",
    "appel , _ siam j. sci .",
    "comput . _ * 6 * , 85 ( 1985 ) .",
    "j. barnes , in _ use of supercomputers in stellar dynamics _ , edited by p. hut and s. mcmillan ( springer - verlag , berlin , 1986 ) , p. 175 .",
    "j. barnes , _ j. comput .",
    "_ * 87 * , 161 ( 1990 ) .",
    "barnes and p. hut , _ nature _ * 324 * , 446 ( 1986 ) .",
    "barnes and p. hut , _ astrophys .",
    "_ * 70 * , 389 ( 1989 ) .",
    "u. becciani , v. antonuccio - delogu and a. pagliaro , _ comput .",
    "commun . _ * 99 * , 1 ( 1996 ) .",
    "u. becciani , r. ansaloni , v. antonuccio - delogu , g. erbacci , m. gambera and a pagliaro , _ comput .",
    "commun . _ * 106 * , 1 ( 1997 ) .",
    "e. bertschinger and j.m .",
    "gelb , _ comput .",
    "_ * 5 * , 164 ( 1991 ) .",
    "birdsall and a.b .",
    "langdon , _ plasma physics via computer simulation _ ( mcgraw - hill international , new york , 1985 ) .",
    "j. dubinski , m.sc .",
    "thesis ( university of toronto , 1988 ) j. dubinski , _ new astronomy _ * 133 * , 1 ( 1996 ) . j.w . eastwood and r.w .",
    "hockney , _",
    "j. comput .",
    "_ * 16 * , 342 ( 1974 ) .",
    "m. hnon , _ ann .",
    "dastrophys . _",
    "* 27 * , 83 ( 1964 ) .",
    "l. hernquist , _ astrophys .",
    "j. suppl . _ * 64 * , 715 ( 1987 ) .",
    "l. hernquist , f.r .",
    "bouchet and y. suto _ astrophys . j. suppl . _ * 75 * , 231 ( 1991 )",
    ". r.w . hockney and j.w.eastwood ,",
    "_ computer simulation using particles _ ( mcgraw - hill international , new york , 1981 ) .",
    "jernigan , in _ iau symposium 113 , dynamics of star cluster _ , edited by j. goodman and p. hut ( reidel , dordrecht , 1985 ) , p. 275 .",
    "a. kravtsov , a.a .",
    "klypin and a.m. khokhlov , _ astrophys .",
    "j. suppl . _",
    "* 111 * , 73 ( 1997 ) . b. kuhlman , a.l .",
    "melott and s.f .",
    "shandarin , _ astrophys .",
    "j. lett . _",
    "* 470 * , l41 ( 1996 ) .",
    "d. porter , ph.d .",
    "thesis , ( university of california , berkeley , 1985 ) .",
    "press , _ in use of supercomputers in stellar dynamics _ , edited by p. hut and s. mcmillan ( springer - verlag , berlin 1986 ) , p. 184 .",
    "j. salmon , ph.d .",
    "thesis ( california institute of technology , 1990 ) . j. salmon and m.s .",
    "warren , in _ proc . of the eight conf . on parallel processing for scientific computing _",
    "( soc . for industr .",
    "& appl . math .",
    "philadelphia ) , 1997 ) .",
    "s. von hoerner , _ z. astrophys . _",
    "* 50 * , 184 ( 1960 ) ."
  ],
  "abstract_text": [
    "<S> n - body codes for performing simulations of the origin and evolution of the large scale structure of the universe have improved significantly over the past decade in terms of both the resolution achieved and the reduction of the cpu time . </S>",
    "<S> however , state - of - the - art n - body codes hardly allow one to deal with particle numbers larger than a few @xmath0 , even on the largest parallel systems . in order to allow simulations with larger resolution </S>",
    "<S> , we have first reconsidered the grouping strategy as described in j. barnes ( 1990 , _ j. comput . </S>",
    "<S> phys . _ * 87 * , 161 ) ( hereafter b90 ) and applied it with some modifications to our wdsh - pt ( work and data sharing - parallel tree ) code ( u. becciani et al . </S>",
    "<S> , 1996 , _ comput . </S>",
    "<S> phys . </S>",
    "<S> comm . _ * 99*,1 ) . in the first part of this paper we will give a short description of the code adopting the algorithm of j. e. barnes and p. hut ( 1986 , _ nature _ , * 324 * , 446 ) and in particular the memory and work distribution strategy applied to describe the _ data distribution _ on a cc - numa machine like the cray - t3e system . in very large simulations </S>",
    "<S> ( typically @xmath1 ) , due to network contention and the formation of clusters of galaxies , an uneven load easily verifies . to remedy this </S>",
    "<S> , we have devised an automatic work redistribution mechanism which provided a good dynamic load balance without adding significant overhead . in the second part of the paper </S>",
    "<S> we describe the modification to the barnes grouping strategy we have devised to improve the performance of the wdsh - pt code . </S>",
    "<S> we will use the property that nearby particles have similar interaction lists . </S>",
    "<S> this idea has been checked in b90 , where an interaction list is built which applies everywhere within a cell @xmath2 containing a small number of particles @xmath3 . </S>",
    "<S> b90 reuses this interaction list for each particle @xmath4 in the cell in turn . </S>",
    "<S> we will assume each particle @xmath5 to have the same interaction list . </S>",
    "<S> we consider that the agent force @xmath6 on a particle @xmath5 can be decomposed into two terms @xmath7 . the first term @xmath8 is the same for each particle in the cell and is generated by the interaction between a hypothetical particle placed in the center of mass of the @xmath2 and the farther cells contained in the interaction list . </S>",
    "<S> @xmath9 is different for each particle @xmath10 and is generated by the interaction between @xmath5 and the elements near @xmath2 . </S>",
    "<S> thus it has been possible to reduce the cpu time and increase the code performance . </S>",
    "<S> this enable us to run simulations with a large number of particles ( @xmath11 ) in nonprohibitive cpu times .    </S>",
    "<S> 0.5 cm -2 cm </S>"
  ]
}