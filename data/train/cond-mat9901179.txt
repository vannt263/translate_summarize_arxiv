{
  "article_text": [
    ""
  ],
  "abstract_text": [
    "<S> we investigate layered neural networks with differentiable activation function and student vectors without normalization constraint by means of equilibrium statistical physics . </S>",
    "<S> we consider the learning of perfectly realizable rules and find that the length of student vectors becomes infinite , unless a proper weight decay term is added to the energy . </S>",
    "<S> then , the system undergoes a first order phase transition between states with very long student vectors and states where the lengths are comparable to those of the teacher vectors . additionally in both configurations there is a phase transition between a specialized and an unspecialized phase . </S>",
    "<S> an anti - specialized phase with long student vectors exists in networks with a small number of hidden units .    </S>",
    "<S> statistical physics has been applied successfully to the investigation of equilibrium states of neural networks . </S>",
    "<S> @xcite the by now standard analysis of off - line training from a fixed training set is based on the interpretation of training as a stochastic process which leads to a well - defined thermal equilibrium . </S>",
    "<S> investigations of perceptrons @xcite or committee machines @xcite have widely improved understanding of learning in neural networks . </S>",
    "<S> meanwhile these studies are being extended to the more application relevant scenario of networks with continuous activation function and output . </S>",
    "<S> @xcite    the soft - committee machine is a two - layered neural network which consists of a layer of @xmath0 hidden units , all of which are connected with the entire @xmath1-dimensional input @xmath2 . </S>",
    "<S> the total output @xmath3 ist proportional to the sum of outputs of all hidden units : @xmath4 where the weights of the @xmath5-th hidden unit are represented by the @xmath1-dimensional vector @xmath6 . </S>",
    "<S> we investigate learning of a perfectly matching rule parametrized by a teacher network of the same architecture with output @xmath7 and orthogonal vectors @xmath8 , which we assume to have the length @xmath9 . </S>",
    "<S> the transfer function @xmath10 is taken to be a sigmoidal function , e.g. the error function . </S>",
    "<S> networks of this type have been studied in the limit of high temperature @xcite , the annealed approximation @xcite , and by means of the replica formalism @xcite . </S>",
    "<S> all these studies imposed the simplifying condition that the order parameters @xmath11 are restricted to the value 1 for @xmath12 , so the length of the student vectors is fixed to that of the teacher vectors . </S>",
    "<S> this system shows a phase transition between an unspecialized configuration , where the student - teacher overlaps @xmath13 are identical for all @xmath14 and a specialized configuration where @xmath15 for @xmath16 . </S>",
    "<S> however , constraining the student lengths implies significant _ a priori _ knowledge of the rule which is not available in practical applications . </S>",
    "<S> so , in this paper we want to obtain first results for soft committee machines which determine student lengths in the course of learning .    </S>",
    "<S> learning is guided by the minimization of the training error @xmath17 where @xmath18 is the number of examples used for training . </S>",
    "<S> after training , the success of learning can be quantified by an average of the quadratic error measure over the distribution of possible inputs , the so - called generalization error : @xmath19 following the standard statistical physics approach , we consider a gibbs ensemble , which is characterized by the partition function @xmath20 with a formal temperature @xmath21 which controls the thermal average of energy in the equilibrium . </S>",
    "<S> the extensive energy @xmath22 is a function of the training error , the standard choice being @xmath23 . </S>",
    "<S> typical equilibrium properties are calculated from the associated quenched free energy @xmath24 where the average is performed over the random set of training examples . </S>",
    "<S> the evaluation of @xmath25 in general requires the rather involved replica formalism . to obtain first results we consider the simplifying _ high - temperature limit _ </S>",
    "<S> @xmath26 @xcite . </S>",
    "<S> the calculation of equilibrium states is guided by minimization of @xmath27 . here </S>",
    "<S> @xmath28 ist the rescaled number of examples , which we assume to be @xmath29 and @xmath30 the entropy per degree of freedom with order parameters held fixed . </S>",
    "<S> the latter is given by @xmath31 where @xmath32 is the @xmath33-matrix of all cross- and self - overlaps of student and teacher vectors . </S>",
    "<S> equation [ entropy ] is of quite general validity and can be derived by means of a saddle point integration from the definition of the entropy . in @xcite a simpler derivation is presented .    here </S>",
    "<S> we assume the components of all examples to be independent random numbers with mean zero and unit variance . </S>",
    "<S> then , in the thermodynamic limit @xmath34 the generalization error can be calculated analytically , if we choose the activation function @xmath35 @xcite which is very similar to the more popular hyperbolic tangent , so the basic features of the model should not be altered : @xmath36 } \\label{generalization}\\ ] ]    in the following , we will first investigate the simplest case @xmath37 , i.e. a network consisting of one single unit to show the basic principles . </S>",
    "<S> then we will study networks with arbitrary @xmath0 and finally investigate the limit @xmath38 of very large networks .    in the @xmath39 case equations [ entropy ] and [ generalization ] read </S>",
    "<S> : @xmath40 trying to minimize @xmath41 , we find that @xmath42 remains of order 1 for arbitrary @xmath43 , @xmath44 while @xmath30 becomes infinite for @xmath45 , yielding @xmath46 . </S>",
    "<S> this means that in thermal equilibrium the length of the student vector increases to infinity , while its overlap with the teacher becomes irrelevant . </S>",
    "<S> of course , this is not the desired result of training . </S>",
    "<S> the method of choice to avoid this behavior , is to `` punish '' configurations with large @xmath44 with an additional energy called `` weight decay '' . </S>",
    "<S> this is a method of _ regularization _ which is widely used in practice in order to improve the generalization ability of feedforward neural networks @xcite . </S>",
    "<S> so we introduce @xmath47 @xcite and obtain @xmath48 with @xmath49 which has to be minimized w.r.t . </S>",
    "<S> @xmath43 and @xmath44 .    </S>",
    "<S> ( 14,4.5)(0,0 ) ( 0,-1.5 ) ( 3.5,1.5 ) ( 7,-1.5 ) ( 10.5,1.0 ) ( 6.5,0)@xmath50 ( 13.5,0)@xmath50 ( 0,4.5)@xmath42 ( 7,4.5)@xmath42 ( 6,2.0)@xmath50 ( 3.5,4.5)@xmath44 ( 13,1.5)@xmath50 ( 10.5,4)@xmath51    in figure [ kaeins ] we show @xmath42 as a function of the rescaled number of examples , @xmath50 for @xmath52 . for small @xmath50 </S>",
    "<S> the network is in a state with large @xmath44 ( and @xmath42 ) . </S>",
    "<S> for @xmath53 a second state with small @xmath44 and small @xmath42 exists , which becomes globally stable at @xmath54 . at @xmath55 the state with large </S>",
    "<S> @xmath44 becomes even locally unstable . </S>",
    "<S> we remark that this phase transition is solely due to the differentiable nature of the activation function , which causes the energy to depend on the length of the student vector , and does not occur in the simple perceptron . </S>",
    "<S> it was also not found for the simpler linear unit with @xmath56 @xcite , where the training error is more sensitive to a mismatched @xmath44 than in the case of a bounded , saturating transfer function . </S>",
    "<S> the phase transition disappears for @xmath57 .    </S>",
    "<S> we have performed continuous monte - carlo simulations of a metropolis - like learning process of the single unit . </S>",
    "<S> the results shown in figure [ kaeins ] confirm our theoretical results .    in order to extend our analysis to networks with @xmath58 </S>",
    "<S> we assume the network configuration to be _ site - symmetric _ with respect to the hidden units so the order parameters fulfill the conditions @xmath59 and @xmath60 . </S>",
    "<S> this assumption reflects the symmetry of the rule yet allows for specialization of the student , as student overlaps with teacher vectors can yield different values for @xmath61 and @xmath16 . </S>",
    "<S> now generalization error and entropy read : @xmath62 - \\frac{2}{\\pi }   { \\sin^{-1}}\\left ( \\frac{r}{\\sqrt{2(1 + q ) } } \\right)}\\ ] ] @xmath63 + \\frac{k - 1}{2 } \\ln \\left [ q - c - ( r - s)^2 \\right]}\\ ] ] the weight decay term introduced for the single unit generalizes naturally to @xmath64 , so the free energy becomes @xmath65 in a site symmetric state . </S>",
    "<S> numerical minimization leads to the results shown in figure [ kaeins ] and [ kazwei ] for @xmath66 and @xmath67 .    </S>",
    "<S> ( 14,4.5)(0,0 ) ( 0,-1.5 ) ( 3.5,0.5 ) ( 7,-1.5 ) ( 6.5,0)@xmath50 ( 13.5,0)@xmath50 ( 0,4.5)@xmath42 ( 7,4.5)@xmath42 ( 6,1.5)@xmath50 ( 3.5,4)@xmath51    in addition to the first order phase transition already observed at the single unit , which connects states with different lengths of student vectors , we observe transitions between phases which are characterized by the parameter @xmath68 indicating specialization features . as both transitions are due to independent mechanisms , namely on the one hand a change of student vector _ lengths _ and on the other hand an alteration of their _ directions _ , specialized ( @xmath69 ) and unspecialized ( @xmath70 ) phases can exist both in the large-@xmath44 configuration and in the small-@xmath44 regime . </S>",
    "<S> indeed for @xmath71 first order transitions between specialized and unspecialized phases can be observed in both configurations . additionally , there is a second order phase transition between the unspecialized large @xmath44 phase and an anti - specialized phase ( @xmath72 ) with large @xmath44 at @xmath54 . </S>",
    "<S> the @xmath66 system shows a second order transition in the large-@xmath44 regime , while an unspecialized configuration with small @xmath44 can not be observed . </S>",
    "<S> this difference in behavior results from the higher degree of symmetry in the @xmath66 system , where the free energy is invariant under exchange of @xmath43 and @xmath73 . </S>",
    "<S> consequently there is no physical difference between specialized and anti - specialized configurations in the @xmath66 system .    to study the behaviour of very large networks ( @xmath38 ) scaling assumptions of order parameters have to be made . </S>",
    "<S> supposing @xmath74 to be @xmath75 , the output of the student will be @xmath76 and thus on a different scale as the teacher output . </S>",
    "<S> so we assume the hidden unit overlaps to be @xmath77 , writing @xmath78 and further introduce @xmath79 , while @xmath51 and @xmath44 remain @xmath75 . inserting this and performing @xmath80 </S>",
    "<S> we find that the condition @xmath81 can be fulfilled only if @xmath82 is assumed to be @xmath77 . </S>",
    "<S> so we substitute @xmath83 before performing the limit @xmath38 . </S>",
    "<S> the corresponding generalization error is shown in figure [ kazwei ] as a function of @xmath50 . for small @xmath50 , </S>",
    "<S> the network is in an unspecialized phase with large @xmath44 . at @xmath84 a locally stable , unspecialized configuration with small </S>",
    "<S> @xmath44 appears , which is globally stable between @xmath85 and @xmath86 , where the specialized small @xmath44 configuration becomes globally stable . </S>",
    "<S> however , the unspecialized configuration remains locally stable . </S>",
    "<S> additionally , at @xmath87 the specialized large @xmath44 phase appears , the free energy of which is smaller than that of the unspecialized large @xmath44 phase for @xmath88 . </S>",
    "<S> anti - specialized configurations do not exist in the limit @xmath89 . </S>",
    "<S> we expect them to be a characteristic feature of systems with small @xmath71 .    in summary , </S>",
    "<S> we have shown by means of statistical physics that learning an unknown rule without a priori knowledge in the form of normalized student vectors leads to a much more complicated behaviour than learning with normalized students . </S>",
    "<S> the number of phases in which the system can exist increases . further , student lengths tend to infinity unless the network weights are regularized by means of a proper weight decay .    </S>",
    "<S> further investigations will extend research to finite temperatures by applying the replica formalism and study the relevance of our results for practical training processes .    </S>",
    "<S> we thank w kinzel and a freking for stimulating discussions and a critical reading of the manuscript . </S>"
  ]
}