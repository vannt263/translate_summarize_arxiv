{
  "article_text": [
    "model selection has always had a place in empirical economics , whether or not it is formally acknowledged .",
    "a key problem in modern empirical work is that researchers face datasets with large numbers of variables , sometimes more than observations .",
    "a complementary problem is that economic theory and prior knowledge may mandate controlling for certain variables , but are generally silent regarding functional form .",
    "these two problems force researchers to search for a model that is simultaneously parsimonious and adequately flexible .",
    "many formal methods are computationally infeasible with a large number of variables .",
    "a typical response to this challenge is to iteratively search over a small set of alternative specifications , guided only by the researcher s taste and intuition .",
    "but no matter the approach used , subsequent inference almost never takes accounts for this `` specification search '' and the resulting confidence intervals are not robust to model selection mistakes , and hence are unreliable in empirical work .",
    "this problem is particularly important in estimating average treatment effects under selection on observables , because in this framework using the right covariates is crucial for identification and correct inference . in this context",
    ", we provide an easy - to - implement and objective method for covariate selection and post - selection inference on average treatment effects .",
    "we establish four main results for multivalued treatments effects with arbitrary heterogeneity in observables and heteroskedasticity .",
    "first , we show that a doubly - robust estimator is robust to model selection errors .",
    "these estimators were initially developed for robustness to parametric misspecification , but are now known to be robust to selection . by taking explicit account of the model selection stage and its inherent selection errors , we derive precise conditions required for any model selector to deliver confidence intervals for average treatment effects that are uniformly valid over a large class of data - generating processes .",
    "second , we show that a simple refitting procedure allows researchers to augment variables chosen according economic theory with data - driven selection to deliver flexible inference that remains uniformly valid .",
    "third , we prove that our estimator is asymptotically linear , and standard conditions imposed in the program evaluation literature , semiparametrically efficient bound .",
    "fourth , we derive new results for multinomial ( and binary ) logistic regression , the most widely used model for treatment assignment .",
    "inference following model selection is notoriously difficult . in a sequence of papers ,",
    "leeb and ptscher @xcite have shown that inference relying too heavily on model selection can not be made uniformly valid .",
    "loosely speaking , uniform validity of a confidence interval captures the idea that the interval should have the same quality ( coverage ) for many data - generating processes .",
    "this theoretical property is practically important because it implies greater reliability in applications .",
    "our proposed methods for post model selection inference build upon the path - breaking recent work of .    the crucial insight that leads to uniform",
    "inference is to change the goal of model selection away from perfect _ covariate _ selection ( the oracle property ) and to high - quality approximation of the underlying _ functions_. this fundamental shift in focus allows us to circumvent , without contradicting , the impossibility results of leeb and ptscher .",
    "valid post - selection inference has attracted considerable attention during the preparation of this paper : in contexts and with methods quite different from ours , contributions have been made by , , , , , and , among others .",
    "our approach , based on the doubly - robust estimator , has several key features .",
    "the name `` doubly - robust '' reflects that it is robust to misspecification of either the treatment equation ( propensity score ) or the outcome equation , a property obtained by combining inverse probability weighting and regression imputation .",
    "first , we show that this robustness extends to model selection , enabling us to allow for selection errors in both equations without impacting inference .",
    "second , we capture arbitrary treatment effect heterogeneity ( dependence of the effect on an individual s observed characteristics ) , which is crucial in empirical work . with such heterogeneity , the average treatment effect and the treatment on the treated differ , and hence we present results for both .",
    "third , the doubly - robust estimator also stems from the semiparametric efficient moment conditions , and hence we obtain the semiparametric efficiency bound , even under heteroskedasticity , under standard additional conditions .",
    "thus , result that sparse estimators have large confidence sets is also circumvented .",
    "taking all these features together enables us to obtain uniform inference over such a large class of treatment effects models .    in recent independent work , ,",
    "propose a similar approach .",
    "their main focus is inference on the linear part of a partially linear model , which motivates an estimator quite different from ours , but it will recover the average treatment effect in the special case of a binary treatment where the effect is constant across observables . however , their section 5 , developed independently from our work , considers heterogeneous effects and proposes an estimator based on the efficient influence function , similar to the present study .",
    "there are two broad differences in our approaches .",
    "first , we allow for multivalued treatments , which offers a larger set of estimands and can thus enhance the understanding of program impacts . in this context",
    "we propose a group lasso based approach that naturally exploits the already - present structure of treatment effects data to improve model selection by pooling information across treatment levels .",
    "this is particularly natural in the multivalued case , but even in the binary case there is still a grouped structure in the outcome regressions , though not in treatment assignment ( i.e. , in propensity score estimation ) .",
    "second , although in both cases the doubly - robust estimator is used for average treatment effects . ]",
    "( following a quite different model selection step ) , we show that this estimator has two benefits : ( i ) it may require weaker conditions on the first stage ( see assumption [ first stage ] ) ; and ( ii ) it does not require using variables selected for the treatment equation in the outcome model , and vice versa ( `` post double selection '' ) , and indeed , doing may require additional assumptions ( see assumption [ ate union ] ) .",
    "our analysis is conducted under selection on observables , which has a long tradition and remains quite popular in empirical economics .",
    "covariates play three crucial roles in this framework .",
    "first , using more observed covariates as proxies , and more flexibly , may help account for unobserved confounding and hence increase the plausibility of unconfoundedness .",
    "second , some observed variables may not be part of the causal mechanism under study , and should be excluded .",
    "third , the efficient conditioning set are those variables that drive the outcome , not necessarily those important for treatment assignment .",
    "this reasoning mandates contradicting goals for practitioners : a large , rich set of controls on the one hand , and parsimony on the other .",
    "our approach is a formal , theory - driven attempt to reconcile this contradiction .",
    "a special feature of our analysis is that we match the empirical realities of large data sets by considering selection from amongst ( possibly ) more covariates than observations , so - called _ high - dimensional _ data .",
    "the goal of variable selection is to find a small model that is nonetheless sufficiently flexible to capture unknown features of the data - generating process required for inference .",
    "if a small model can perfectly capture the unknown feature it is said to be _",
    "exactly sparse_. more realistic is _ approximate sparsity _",
    ", when the bias from using a small model is well - controlled , but nonzero .",
    "sparsity is a natural framework for thinking about model selection .",
    "indeed , any time only a few of the available variables are used , a sparsity assumption has effectively been made .",
    "it is common empirical practice to report results from several small models , but for these results to be valid one must assume these specifications give high - quality , sparse representations of the unknown features .",
    "the alternative we provide involves selecting a sparse , yet flexible , model from among a large set of variables .",
    "results may then be compared with more traditional methods .    with the aim of mimicking common empirical practice",
    "we estimate the propensity score with multinomial logistic regression , coupled with group lasso selection @xcite .",
    "our results are stated in the language of treatment effects , but apply to general data structures and are of independent interest in the high - dimensional literature .",
    "much of the literature has focused on linear models ( see for a survey ) , while prior studies of nonlinear models often assume exact sparsity or present limited results .",
    "furthermore , these studies often use high - level conditions that can be hard to verify .",
    "in contrast , we obtain sharp results for logistic regression under the same simple and intuitive conditions used for linear modeling by exploiting mathematical techniques of self - concordant functions put forth by .",
    "we also provide extensions to prior work on linear models needed to apply them in treatment effect estimation .",
    "finally , we offer numerical evidence on the finite sample performance of our procedure . in a small simulation study",
    "we find that our procedure delivers very accurate coverage of confidence intervals even for models where covariate selection is difficult , either because of a low signal - to - noise ratio or lack of sparsity , thus highlighting the uniform validity of inference .",
    "we also apply our method to the widely - used national supported work demonstration data @xcite and find very accurate estimates and tight confidence intervals ( see table [ table - lalonde ] ) .",
    "the paper proceeds as follows .",
    "section [ sec - overview ] gives short , self - contained overview .",
    "section [ sec - notation ] collects notation .",
    "section [ sec - model ] describes the treatment effect models .",
    "sparse models are discussed in section [ sec - sparse ] , which shows how several commonly used models fit in this framework .",
    "section [ sec - ate ] presents our estimation method and complete results on treatment effect inference .",
    "theoretical results for the group lasso are in section [ sec - grplasso ] .",
    "section [ sec - data ] presents the numerical evidence and section [ sec - conclusion ] concludes .",
    "the main proofs are presented in the appendix , while the remainder are available in a supplement .",
    "here we give an overview of the paper , including treatment effect inference ( section [ sec - overview ate ] ) , our new results for the group lasso ( section [ sec - overview grplasso ] ) , and notation used throughout ( section [ sec - notation ] ) .",
    "we consider a multivalued treatment , with status indicated by @xmath0 .",
    "interest lies in mean effects of the treatment on a scalar outcome @xmath1 .",
    "let @xmath2 be the ( latent ) potential outcomes : @xmath3 is the outcome a unit would have under @xmath4 and is only observed for units with @xmath4 ; that is , @xmath5 .",
    "many interesting parameters combine means of potential outcomes , and having multivalued treatments allows for a wider range of estimands .",
    "define the mean of one potential outcome as @xmath6 $ ] . to fix ideas",
    ", @xmath7 is the average treatment effect in the binary case ( @xmath8 ) .",
    "sections [ sec - model ] and [ sec - ate ] consider more general average effects , including effects on treated groups . for simplicity , in this section",
    "we focus on a single @xmath9 .",
    "we use the selection on observables framework to identify @xmath9 . for a vector of covariates @xmath10 , define the generalized propensity score and conditional outcome regressions as @xmath11     \\qquad \\text { and } \\qquad   { \\mu_{t}}(x ) = { \\mathbb{e } } [ y \\vert { d}= { t } , x = x].\\ ] ] for identification it is sufficient to assume that @xmath12 = { \\mathbb{e}}[y({t } ) \\vert x]$ ] ( mean independence ) and @xmath13 is bounded away from zero ( overlap ) for all treatment levels .",
    "broadly , these two assumptions imply that units from one treatment group are good proxies for other treatments and that there are always such proxies available ( see section [ sec - model ] ) .",
    "suppose we have an i.i.d .",
    "sample @xmath14 from @xmath15 .",
    "then , for model - selection - based estimators @xmath16 and @xmath17 , we estimate @xmath9 with @xmath18 this doubly - robust estimator combines regression imputation and inverse probability weighting , and remains consistent if either the model @xmath19 or @xmath20 is misspecified .",
    "following widespread empirical practice , we estimate @xmath16 with multinomial logistic regression and @xmath17 linearly ( see section [ sec - grplasso ] ) . the choice of covariates in @xmath16 and @xmath17 impacts consistency , efficiency , and finite sample performance .",
    "covariate selection based on ad hoc , iterative searches is common in empirical evaluations , but is not formal , objective , or replicable .",
    "balancing tests are also commonly used in this context , but have the additional drawback of assuming the same covariates are important for outcomes and treatment assignment , and more generally do not weight the covariates by their importance for bias .    on the other hand",
    ", our proposed procedure gives practitioners an easy to implement , fully objective tool to perform data - driven covariate selection and treatment effect inference , with replicable results .",
    "importantly , we do not preclude the addition of variables known to be important from economic theory or prior knowledge .",
    "our procedure is intended to supplement these variables with a flexible set of controls , guarding against misspecification or overfitting .",
    "the following theorem is an example of the more general results presented in section [ sec - ate theory ] , wherein we also define @xmath21 and @xmath22 .",
    "[ thm - ate overview ] consider a sequence @xmath23 of data - generating processes that obey , for each @xmath24 , assumptions [ ignorability ] and [ dgp asmpts ] below .",
    "we require two conditions on the first stage :    1 .",
    "@xmath25 and @xmath26 ; 2 .",
    "@xmath27^{1/2 } \\bigl[{\\sum_{i=1}^n}{\\ensuremath{{{\\rm 1\\hspace*{-0.84ex }     \\rule{0.06ex}{1.37ex}\\hspace*{0.9ex}}}\\!}}\\{{d_i}= { t}\\}({\\hat{\\mu}_{t}}(x_i ) - { \\mu_{t}}(x_i))^2/n\\bigr]^{1/2 } = o_{p_n}(n^{-1/2})$ ] .    under these conditions , @xmath28 and @xmath29 .",
    "for each @xmath24 , let @xmath30 be the set of data - generating processes satisfying assumption [ ignorability ] and [ dgp asmpts ] and conditions ( i ) and ( ii ) .",
    "then @xmath31 - ( 1 - \\alpha ) \\right| \\to 0,\\ ] ] where @xmath32 .",
    "this result establishes the uniform validity of an asymptotic confidence interval for @xmath9 , overcoming all the post model selection inference challenges : robustness to model selection errors , selecting a model that is small but flexible enough to capture the features of the underlying data generating process , and still retaining efficiency under standard conditions ( see section [ sec - efficiency ] ) .",
    "intuitively , this is similar to ( but distinct from ) overcoming pretesting bias in other contexts . also , although our discussion is in terms of covariate selection in high - dimensional , sparse models , the inference result is generic for any first stage estimator .",
    "the two conditions placed on the first stage are analogous to the commonly - used , high - level requirement in semiparametrics that first stage components converge faster than @xmath33 .",
    "however exploiting features of the doubly - robust estimator yields weaker conditions .",
    "the first is a mild consistency requirement .",
    "the second requires a rate on the product of errors and is thus easier to satisfy if one function is easier to estimate , e.g.  more smooth or more sparse . in model selection ,",
    "the rates for the first stage depend on the sample size , the number of covariates considered , and the sparsity level .",
    "importantly , the rate will depend on the total number of covariates only logarithmically , allowing for a large number .",
    "we propose to use the group lasso and prove that these estimators satisfy ( i ) and ( ii ) .",
    "we propose refitting following group lasso selection , and show that it meets all requirements on the model selector .",
    "the group lasso is well - suited to program evaluation applications because covariates are penalized according to their overall contribution in all treatment groups .",
    "this has two consequences .",
    "first , information from all treatments is pooled when doing selection , and hence a weaker signal may be extracted , which improves the selection properties .",
    "second , the selected variables are common to all treatment levels . from a practical point of view",
    "this is desirable , as interest rarely lies in a single @xmath9 , but rather a collection , and substantial commonality is expected in the variables important for different treatment levels .",
    "we consider high - dimensional , sparse models for @xmath19 and @xmath20 .",
    "these are defined by a @xmath34-dimensional vector @xmath35 based on the original variables @xmath10 .",
    "the @xmath35 may consist of any combination of the original variables , interactions , flexible parametric transformations , and/or nonparametric series terms ( such as splines or polynomials ) .",
    "a model is approximately sparse if there are @xmath36 of these terms that yield a good approximation ( @xmath37 is allowed ) . to build intuition ,",
    "suppose that @xmath20 obeys a @xmath34-dimensional linear model .",
    "then the sparsity assumption is that there is an @xmath38-dimensional submodel with sufficiently small specification bias . in the nonparametric case ,",
    "sparsity is weaker than ( but analogous to ) the familiar assumption that a small set of basis functions can approximate the unknown objects well . in practice researchers",
    "employ a hybrid of these approaches , which is covered by our results .",
    "section [ sec - sparse ] gives more detail and examples .",
    "we form @xmath39 and @xmath40 in two steps ( complete details in section [ sec - grplasso ] ) .",
    "first , the group lasso is applied separately to multinomial logistic and least squares regression to select covariates from @xmath35 .",
    "we then estimate @xmath19 and @xmath20 by refitting unpenalized models using the selected variables , possibly augmented with controls suggested by prior work or economic theory .",
    "it is not desirable for a model selector to discard theory and prior work , and our procedure explicitly avoids this .",
    "we also allow for using logistic - selected variables in the linear model refitting and vice versa , but this is not necessary for uniformity nor efficiency .",
    "our main results give precise bounds for the number of covariates selected and the estimation error , both for the penalized and unpenalized estimates .",
    "section [ sec - grplasso ] results gives nonasymptotic bounds , with exact constants .",
    "such results are complex and so we give the following intuitive , asymptotic result ( the notation @xmath41 is defined in section [ sec - notation ] ) .",
    "[ thm - grplasso overview ] suppose the biases from the best @xmath42- and @xmath43-term approximations to @xmath19 and @xmath20 are order @xmath44 and @xmath45 , respectively . then under the assumptions in section [ sec - grplasso ] , and @xmath46 described therein , with high probability we have :    1 .",
    "@xmath47 and 2 .",
    "@xmath48 .",
    "these two results for our proposed group lasso estimators can be directly used to verify the high - level conditions in theorem [ thm - ate overview ] above . specifically , if @xmath49 , conditions ( i ) and ( ii ) of theorem [ thm - ate overview ] are met ( requiring @xmath50 , up to @xmath51 factors , as found in other results in the literature ) .",
    "further , it is clear how the doubly - robust estimator can help : if one function is more smooth or more sparse , @xmath42 or @xmath43 will be lower , easing the restriction .",
    "section [ sec - grplasso results ] gives further results : showing that the number of variables selected is the same order as the sparsity level , and provides bounds on the logistic and linear coefficients directly .",
    "both these results are important for certain steps in treatment effect estimation that are nt reflected in the simple statement of theorem [ thm - ate overview ] .",
    "these results appear to be entirely new for the multinomial logistic regression , for any version of the lasso . from a practical point of view , these results provide formal justification for using multinomial logistic regression , coupled with group lasso selection and post - selection refitting .",
    "we collect here notation to be used for the rest of the paper .",
    "the population data generating process ( dgp ) is denoted by @xmath52 and is defined by the joint law of the random variables @xmath53 . for a given @xmath24 ,",
    "@xmath54 constitute @xmath24 draws from @xmath52 .",
    "in general , the dgp may vary with @xmath24 , along with features such as parameters , distributions , an so forth , as discussed in section [ sec - vary with n ] .",
    "this is generally suppressed for notational clarity .",
    "we further adopt the following conventions .",
    "treatments . : :    define the treatment sets    @xmath55    and    @xmath56 .",
    "no order is assumed in the treatments .",
    "for each unit @xmath57 ,    @xmath58 indicates treatment assignment , and define    @xmath59 .",
    "let @xmath60 be the number of    individuals with treatment @xmath61 and define    @xmath62    and    @xmath63 .",
    "further define    @xmath64 .",
    "vectors . : :    define @xmath65 .",
    "for a    doubly - indexed collection of scalars    @xmath66 ,    define    @xmath67    as the vector that collects over all @xmath61 for fixed    @xmath68 ;    @xmath69 collects over    @xmath70 for fixed @xmath61 ; and    @xmath71    the concatenation of all @xmath72 . for    simplicity , we write @xmath73 for    @xmath72 .",
    "when considering the    multinomial logistic model , @xmath61 will vary only over    @xmath74 but the notation will be    maintained .",
    "for a set @xmath75 , let    @xmath76 be the    vector of @xmath77 for fixed    @xmath61 and similarly let    @xmath78 . norms . :",
    ":    single bars will be either absolute value or cardinality of a set , and    will be clear from the context . for a vector @xmath79 ,",
    "let    @xmath80 and @xmath81 denote the    @xmath82 and @xmath83 norms , respectively . for    the group lasso , define the mixed    @xmath83/@xmath82 norm as    @xmath84 .",
    "it will always be the case that the ( `` outer '' ) @xmath82    norm is over the covariates and the ( `` inner '' ) @xmath83    norm is over the treatments ( in our application ) . when discussing the    multinomial logistic model ,",
    "treatments will be restricted to    @xmath74 with no change in notation .",
    "data - generating processes .",
    ": :    the dgp for a fixed @xmath24 will be denoted by    @xmath52 .",
    "the set of all such @xmath52 considered is    @xmath30 . for sequences ,",
    "@xmath85 .",
    "expectations and probabilities will be understood to be taken against    @xmath52 , though notationally suppressed .",
    "for asymptotic    arguments dependence on @xmath24 is explicit , so that    @xmath86 and @xmath87 have    their usual meaning with the understanding that the measure    @xmath52 is used for each @xmath24 .",
    "+    the empirical expectation will be denoted    @xmath88 = { \\sum_{i=1}^n}w_i / n$ ] . also , define    @xmath89 = { \\sum_{i \\in { \\mathbb{i}_{t}}}}w_i / n_{t}= { \\sum_{i=1}^n}{{d_i}^{{t}}}w_i / n_{t}$ ]    for observations with treatment @xmath61 .    for",
    "a set of scalars @xmath90 , let @xmath91^{-1}$ ] denote the multinomial logit function .",
    "in this section we formally define the treatment effects model and the parameters of interest . recall that @xmath0 indicates treatment status , @xmath92 are the ( latent ) potential outcomes , and @xmath3",
    "is only observed for units with @xmath4 ; that is , @xmath93 .",
    "the building blocks of many general estimands are the averages @xmath94 ,    \\quad    { t}\\in { \\overline{{\\mathbb{n}}}_{\\mathcal{t } } } ,       \\qquad \\text { and } \\qquad       { \\mu_{{t},{t}'}}= { \\mathbb{e}}[y({t } ) \\vert d = { t } ' ] ,     \\quad    { t } , { t } ' \\in { \\overline{{\\mathbb{n}}}_{\\mathcal{t}}}\\times { \\overline{{\\mathbb{n}}}_{\\mathcal{t}}}.\\ ] ] in the binary case , the average treatment effect is given by @xmath7 , whereas the treatment on the treated is @xmath95 .",
    "having a multivalued treatment allows for a much larger range of interesting estimands . to fix ideas , we keep as running examples two leading cases from the literature",
    "first , the so - called dose - response function : the @xmath96-vector @xmath97 .",
    "second , define @xmath98 as the @xmath99-vector with element @xmath61 given by @xmath100 .",
    "this gives the effect of each treatment relative to the baseline @xmath101 , only for those who received that treatment .",
    "these vectors are by no means the only interesting estimands constructed from @xmath9 and @xmath102 ; many others are discussed by , , and others .",
    "the following two conditions are sufficient to identify @xmath9 and @xmath102 .",
    "[ ignorability ] for all @xmath103 and almost surely @xmath10 , @xmath52 obeys :    1 .",
    "( mean independence ) @xmath104 = { \\mathbb{e}}[y({t } ) \\vert x = x]$ ] , and [ mean inde ] 2 .",
    "( overlap ) @xmath105 ) \\geq p_{\\min } > 0 $ ] for all @xmath103 .",
    "[ overlap ]    this assumption is a form of `` ignorability '' coined by .",
    "this model allows arbitrary treatment effect heterogeneity in observables , but not unobservables .",
    "this assumption is standard in the program evaluation literature , and its plausibility has been discussed at length , so we omit a general discussion ( see , e.g. , , , and references therein ) .",
    "however , in the context of model selection , three remarks are warranted .",
    "first , in place of assumption [ mean inde ] , it is more common to instead assume full conditional independence : @xmath106 .",
    "however , as observed by , the weaker mean independence is sufficient . for our purposes ,",
    "the `` gap '' between the two assumptions is important .",
    "suppose full independence holds only conditional on a set of variables strictly larger than the variables entering the mean functions ( e.g.  the excess variables affect higher moments ) .",
    "in this case , because mean independence is still sufficient , we need not aim to select the larger set of covariates .",
    "our results of course hold under full independence , which is important for the efficiency discussed in section [ sec - efficiency ] below .",
    "second , the covariate set may , in general , include instruments for treatment status , but they are not known to be such .",
    "this is standard in the literature , but generally left implicit in discussions of the ignorability assumption .",
    "if instruments are present , and selected for estimation , efficiency suffers but unbiasedness is not harmed .",
    "efficiency bounds in this context typically ( implicitly ) assume there are no instruments in @xmath10 . note that assumption [ overlap ] rules out perfect predictors .",
    "section [ sec - efficiency ] offers further discussion .",
    "finally , the main drawback of assumption [ mean inde ] is that it does not give identification of average effects on transformations of @xmath3 .",
    "however , we are expressly interested in model selection on the mean function of the level of @xmath3 , and hence assumption [ mean inde ] is more natural . to operationalize model selection , structure must be placed on @xmath107 $ ] , and hence functional form conditions tied to mean independence are not limiting per se .",
    "if the parameter of interest is changed , say to @xmath108 $ ] , and a sparsity assumption is made for @xmath109 $ ] , then our method applies .",
    "assumption [ ignorability ] yields identification of @xmath9 and @xmath102 using either inverse weighting or regression , and double robustness follows from combining the two strategies .",
    "recall the notation @xmath110 $ ] and @xmath111 $ ] .",
    "applying assumption [ ignorability ] we find that @xmath112 = { \\mathbb{e}}\\left [   \\frac{{\\ensuremath{{{\\rm 1\\hspace*{-0.84ex }     \\rule{0.06ex}{1.37ex}\\hspace*{0.9ex}}}\\!}}\\{{d}= { t}\\ } y}{p_{t}(x ) } + { \\mu_{t}}(x ) - \\frac{{\\ensuremath{{{\\rm 1\\hspace*{-0.84ex }     \\rule{0.06ex}{1.37ex}\\hspace*{0.9ex}}}\\!}}\\{{d}= { t}\\ } { \\mu_{t}}(x)}{p_{t}(x ) }   -   { \\mu_{t}}\\right ]   =   0\\ ] ] and @xmath113        \\\\       = { \\mathbb{e}}\\left [ \\frac{{\\ensuremath{{{\\rm 1\\hspace*{-0.84ex }     \\rule{0.06ex}{1.37ex}\\hspace*{0.9ex}}}\\!}}\\{{d}= { t}'\\ }   { \\mu_{t}}(x)}{p_{{t } ' } } +   \\frac { p_{{t}'}(x)}{p_{{t } ' } } \\frac { { \\ensuremath{{{\\rm 1\\hspace*{-0.84ex }     \\rule{0.06ex}{1.37ex}\\hspace*{0.9ex}}}\\!}}\\{{d}= { t}\\ } ( y - { \\mu_{t}}(x ) ) } { p_{t}(x ) }   -   { \\mu_{{t},{t}'}}\\right ]   =   0,\\end{gathered}\\ ] ] where @xmath114 $ ] .",
    "the moment condition holds if either @xmath19 or @xmath20 is misspecified . for @xmath102 , if @xmath20 is misspecified , both @xmath13 and @xmath115 must be correctly specified , while if @xmath20 is correct , both propensity scores may be misspecified .",
    "it is important to note that the forms of @xmath116 and @xmath117 are fixed , so the function itself does not depend on the sample size even if its arguments do .",
    "our estimator is a plug - in version of this moment condition .",
    "[ remark - tot ] identification of @xmath118 does not require assumption [ ignorability ] .",
    "@xmath3 is fully observed for the sub - population of interest and so a simple average will deliver @xmath119 / p_{t}$ ] . note that reduces to this when @xmath120 . for @xmath98",
    "this means we must only estimate the function @xmath121 for @xmath122 .",
    "intuitively , we must use comparison group observations to proxy for treated units , but not the other way around .",
    "thus , for certain parameters of interest , assumption [ ignorability ] can be weakened to hold only for the comparison group .",
    "however , we cover generic estimands , without necessarily specifying a comparison group , and so we maintain assumption [ ignorability ] for simplicity , rather than keeping track of hosts of special cases .",
    "[ remark - eif ] the functions @xmath116 and @xmath117 are the efficient influence functions . thus , our estimators have the interpretation of being plug - in versions of these influence functions . indeed , as discussed section [ sec - efficiency ]",
    ", our estimators will be asymptotically linear with this influence function .",
    "we now formalize approximate sparsity .",
    "let @xmath123 and @xmath124 be @xmath34-dimensional transformations of the covariates @xmath10 , with @xmath125 allowed .",
    "these transformations are specific to the outcome and treatment models , but may overlap . they do not vary with @xmath61 , nor depend on the dgp .",
    "some examples are given below in section [ sec - examples ] .",
    "for the multinomial logistic model it is convenient to work with the log - odds ratio . we take @xmath126 and write @xmath127 similarly , write the outcome regressions as @xmath128 the terms @xmath129 and @xmath130 are bias terms arising from the parametric specification . as discussed below ,",
    "these encompass the usual nonparametric bias as well .",
    "approximate sparsity requires that only a small number of the @xmath35 are needed to make the bias small .",
    "define @xmath131 and @xmath132 , so that these sets capture all variables important for treatment and outcomes , respectively .",
    "we assume that there are some @xmath133 and @xmath134 , such that for @xmath135 and @xmath136 , the biases @xmath137 and @xmath138 are sufficiently small .",
    "this is made precise by defining the bounds : @xmath139^{1/2 } \\leq { b_{s}}^d          \\quad   \\text { and } \\quad         { \\mathbb{e}_n}[b_{t}^y(x_i)^2]^{1/2 } \\vee { \\mathbb{e}_{n,{t}}}[b_{t}^y(x_i)^2]^{1/2 } \\leq { b_{s}}^y.\\ ] ] note that the former bias bound is placed directly on the propensity score because it is the ultimate object of interest , rather than on the linearization of the log - odds .",
    "while a great deal of overlap is expected , in practice it is likely that a few covariates will be more or less important for different treatments , and so we do not require that the supports of @xmath140 or @xmath141 are constant over @xmath61 , nor that @xmath142 overlaps with @xmath143 . instead , it may be better to think of @xmath144 and @xmath145 as the `` common nonsupports '' of the treatment and outcome equations .",
    "when it is clear from the context we will abbreviate both @xmath124 and @xmath123 by @xmath35 ( and their realizations by @xmath146 ) and refer to them generically as `` covariates '' , and further write @xmath38 for either @xmath42 or @xmath43 .",
    "we assume @xmath147 = 1 $ ] without loss of generality ( see remark [ remark - penalty ] ) .",
    "to concretize the sparse model idea , we now discuss how several models commonly used in practice fit into this framework .",
    "these include parametric and nonparametric models for @xmath19 and @xmath20 , and hybrids of these",
    ". a common theme to all examples will be comparison to the _ oracle _ model : the model that knows the true support in advance .",
    "our uniform inference results include all these examples as special cases because , loosely speaking , we obtain uniformity over dgps where @xmath19 and @xmath20 have sparse representations .",
    "we aim for an accessible discussion of each model , and defer technicalities to the literature @xcite .",
    "assume models and hold with @xmath148 and @xmath149 . let @xmath150 .",
    "all covariates are used in all modeling .",
    "if dimension is fixed this is the textbook parametric model , see for example .",
    "alternatively , the dimension can be diverging , but more slowly than @xmath24 .",
    "we are not aware of any work which covers this case explicitly , though for the first stage , cover linear and logistic regression , and their results easily extend to multinomial logistic models .",
    "the vast majority of treatment effect studies adopt this model ( with dimension fixed ) , taking the set of covariates as given . in our framework , this is equivalent to the researcher having prior knowledge of which covariates are important and which are not . such knowledge no doubt plays an important role , but it can not cover all situations or all variables . furthermore , as more data become available , the researcher does not increase the complexity of their model .",
    "[ eg - parametric ] retain the exact parametric structure of the prior example , but let @xmath151 be possibly larger than @xmath24 , and assume that @xmath143 and @xmath142 are unknown sets of cardinality less than @xmath24 .",
    "model selection must be performed .",
    "often , researchers ( implicitly ) rely on the _ oracle property _ , that @xmath143 and @xmath142 can be found with probability approaching one , and conduct inference conditioning on this event .",
    "this approach can not be made uniformly valid and has poor finite sample properties , as shown by leeb and ptscher @xcite .",
    "[ eg - approx sparse ] again suppose a purely parametric model , so that @xmath149 and @xmath151 , possibly greater than @xmath24 .",
    "suppose that there exist coefficients @xmath152 and @xmath153 such that @xmath154   = { x_{d}^ * } ' \\gamma_{t}^0 $ ] and @xmath155 exactly , but instead of any coefficients being precisely zero , suppose they may be ordered such that @xmath156 and @xmath157 , with @xmath158 and @xmath158 at least one . then , there exist @xmath42 and @xmath43 that are @xmath159 such that equations and , and other conditions needed , are satisfied for @xmath160 for @xmath161 and @xmath162 for @xmath163 and the rest truncated to zero . that is @xmath142 and @xmath143 collect the largest coefficients and @xmath164 , and similarly for @xmath138 .",
    "[ eg - semiparametric ] assume @xmath19 and @xmath20 are unknown functions that can be well - approximated by a linear combination of @xmath42 and @xmath43 basis functions , respectively ( e.g. are sufficiently smooth ) . in and ,",
    "@xmath165 and @xmath166 are the coefficients of these approximations , while @xmath167 and @xmath138 are the usual nonparametric biases . @xmath168 and @xmath169 are series terms used in the approximation .",
    "standard semiparametric analyses , such as , , or , can be viewed in this context as oracle models that know in advance which terms yield the best approximation , typically assumed to be the first terms .",
    "instead , we only require that some @xmath42 ( or @xmath43 ) of a set of @xmath34 series terms give good approximations .",
    "this allows for greater flexibility in applications , where there is no knowledge of which series terms to use , and the researcher may want to mix terms from different bases .    [ eg - mixed ]",
    "partition @xmath170 .",
    "suppose that the true log - odds function satisfies @xmath171",
    "= x_1'\\gamma_{{t}}^1 + h_{t}(x_2 ) + b_{t}^1(x)$ ] , where @xmath172 is a specification bias and @xmath173 is a smooth unknown function . for a set of basis functions @xmath174 , there will exist coefficients @xmath175 such that @xmath176 and so @xmath177 we require that some collection of variables and series terms give a good , sparse approximation , without placing explicit conditions on how many of either .",
    "implicitly , one will restrict the other .",
    "for example , if the dimension of the parametric part is large , then we require that @xmath173 can be more easily approximated .",
    "we treat @xmath20 the same .",
    "this example is closest to actual practice , where some variables ( e.g. dummies ) enter in a known way and should not be considered part of a nonparametric object , while other covariates must be considered flexibly .",
    "it is important to note that misspecification of the type guarded against by double robustness can arise in any type of model . in parametric cases ,",
    "this is most often functional form misspecification . while this type of misspecification does not occur in nonparametrics , others are possible , such as shape restrictions or separability assumptions being incorrect , or omitting relevant variables .",
    "none of these errors disappear asymptotically , and all of them are guarded against by use of the doubly - robust estimator .      much of the dgp , including parameters and distributions , is allowed to depend on @xmath24 .",
    "perhaps the most salient features that do not depend on @xmath24 are the set of treatments and the functions @xmath178 and @xmath179 .",
    "it is likely that our results can be extended to accommodate a growing number of treatments , but that is beyond the scope of our study . in the models and , @xmath35 , @xmath165 , and @xmath166",
    "must depend on @xmath24 by construction .",
    "our results on estimation of these models are nonasymptotic . for treatment effect inference ,",
    "we use triangular array asymptotics to retain the dependence on @xmath24 of the dgp .",
    "the interpretation of the results does , and should , change depending on what is assumed about the dgp . to illustrate ,",
    "let us return to examples [ eg - parametric ] and [ eg - semiparametric ] .",
    "first , consider the simple parametric models of example [ eg - parametric ] . in this case , @xmath180 = { \\mathbb{e } } [ x']{\\beta^*_{t}}$ ] , which depends on @xmath24 by construction , as the dimension is diverging .",
    "it may seem unnatural that the parameter to be estimated depends on @xmath24 , as we typically think of `` true '' parameters being features of a ( large ) fixed study population .",
    "however , with a diverging number of covariates , there is no fixed dgp . indeed ,",
    "if we estimate @xmath181 based upon @xmath182 observations , and then proceed to gather @xmath183 _ more _ observations , when we re - estimate our target is now @xmath184 .",
    "one possible resolution is as follows .",
    "first , the parameter of interest is @xmath185 $ ] , which is defined without reference to covariates",
    ". we can view each successive @xmath24-dependent @xmath9 as an approximation of @xmath186 based upon @xmath187 covariates .",
    "note well that in our thought experiment , @xmath188 , and so additional variables should have been collected for all @xmath189 samples .",
    "contrast this with the semiparametric model in example [ eg - semiparametric ] .",
    "it is common to assume the population dgp is fixed over @xmath24 .",
    "the treatment effects may be constructed in terms of the underlying variables , e.g.  @xmath185 = { \\mathbb{e}}[{\\mathbb{e}}[y({t } ) \\vert x]]$ ] , with @xmath35 serving only the purpose of aiding in approximating the regression functions .",
    "model selection is performed on series terms , not underlying variables , to estimate the coefficients @xmath165 and @xmath166 .",
    "if @xmath190 { \\beta^*_{t}}+ { \\mathbb{e}}[b_{t}^y]$ ] does not depend on @xmath24 , the bias term , by definition , exactly compensates for the @xmath24-dependence in @xmath191 { \\beta^*_{t}}$ ] .",
    "we emphasize that our inference results allow for general @xmath24-dependence in the dgp , and interpretation by the econometrician must take careful account of any conceptual assumptions .",
    "in this section we present results on uniformly valid treatment effect inference .",
    "we first present the estimators and conditions required for a generic first stage to yield uniform inference .",
    "although our focus is on model selection and sparsity , our results are more general , showcasing the benefits of doubly robust estimation for any model in section [ sec - sparse ] where assumption [ first stage ] below ( which does not refer to selection or sparsity ) can be satisfied .      the moment functions @xmath116 and @xmath117 of equations and have fixed and known form , and so for estimators @xmath39 and @xmath40 , we can define @xmath192 and @xmath193 where @xmath194 . by combining these estimators",
    "appropriately we can construct estimators @xmath195 and @xmath196 for the dose - response function @xmath197 and the vector @xmath98 , respectively , and any other estimand . notice that when @xmath198 @xmath199 is an average over the appropriate subpopulation : @xmath200 $ ] .",
    "although in this section we allow for generic estimates @xmath39 and @xmath40 , it is important to distinguish between estimates based upon selected sets that have no `` additional randomness '' and those that do .",
    "model selection based estimation will naturally have two steps : first data - driven selection and then refitting to ameliorate the shrinkage bias and allow the researcher to augment the selected variables .",
    "let @xmath201 and @xmath202 be the selected sets and @xmath203 and @xmath204 be the final sets of variables used in the refitting .",
    "we will say that these contain no `` additional randomness '' if the added variables ( i.e. @xmath205 , for @xmath1 or @xmath206 ) are nonrandomly selected , such as from economic theory or prior knowledge . on the other hand , the added variables may be selected from a random process beyond that",
    "included in @xmath207 .",
    "the leading example would be using logistic - selected variables in the regressions or vice versa .",
    "then the variables used in @xmath17 depend not only on the randomness of @xmath202 , but also on that of @xmath201 , and hence on @xmath208 .",
    "additional conditions are required for the estimators with additional randomness .",
    "the choice of method is in part dependent on the assumptions of the underlying model .",
    "to illustrate , first , return to example [ eg - parametric ] , where we have a purely parametric model with @xmath209 .",
    "the researcher may want to set @xmath210 , in order to have a better chance that @xmath211 .",
    "the set @xmath203 now contains additional randomness due to @xmath202 .",
    "conversely , consider example [ eg - semiparametric ] .",
    "it is natural to include `` low - order '' basis functions for each underlying covariate , say linear and quadratic polynomials .",
    "thus , the researcher may want to include these in @xmath212 , whether or not selected by the group lasso .",
    "however , there is no reason that the series terms useful for approximating the functions @xmath20 would be useful for @xmath19 , or vice versa , and no additional randomness is injected .",
    "we now state the sufficient conditions used for treatment effect estimation and inference . for exposition",
    ", we present these in three groups : those concerning the underlying dgp , requirements of @xmath39 and @xmath40 in the `` no additional randomness '' case , and finally the additional conditions to allow for `` additionally random '' selected sets .",
    "begin with conditions on the dgp .",
    "let @xmath213 and impose the following conditions .",
    "[ dgp asmpts ] for each @xmath24 , the following are true for @xmath52 .    1 .",
    "@xmath54 is an i.i.d",
    ". sample from @xmath214 .",
    "[ iid ] 2 .",
    "the covariates @xmath35 have bounded support , with @xmath215 , uniformly in @xmath24 .",
    "transformations may depend on @xmath24 but not the underlying data generating process .",
    "[ bounded ] 3 .",
    "@xmath216 \\leq \\mathcal{u}^4 $ ] , uniformly in @xmath24 .",
    "[ fourth moments ] 4 .",
    "@xmath217 \\wedge { \\mathbb{e } } [ { x_j^*}^2 ( { \\ensuremath{{{\\rm 1\\hspace*{-0.84ex }     \\rule{0.06ex}{1.37ex}\\hspace*{0.9ex}}}\\!}}\\{{d}= { t}\\ } - p_{t}(x))^2]$ ] is bounded away from zero , uniformly in @xmath24 .",
    "[ positive variance ] 5 .   for some @xmath218 : @xmath219 $ ] and @xmath220 $ ] are bounded , uniformly in @xmath24 .",
    "[ ate moments ]    the conditions of assumption [ dgp asmpts ] are mild and intuitive and are not unique to high - dimensional models or model selection . assumption [ iid ]",
    "restricts attention to cross - sectional applications .",
    "the condition of bounded covariates is unlikely to be a limitation in practice .",
    "any @xmath35 that are underlying variables will naturally be bounded in applications .",
    "this condition is automatically satisfied for most common choices of basis functions employed in nonparametric estimation .",
    "the rest are moment conditions on the potential outcome models , including allowing the errors to be heteroskedastic and non - gaussian . formalizing the requirement of uniform bounds in @xmath24",
    "is needed when doing array asymptotics .",
    "we now give precise conditions on the model selector sufficient for uniformly valid inference .",
    "[ first stage ]   the estimators @xmath39 and @xmath40 obey the following for a sequence @xmath23 , uniformly in @xmath103 .    1 .   @xmath221   = o_{p_n}(1)$ ] and @xmath222   = o_{p_n}(1)$ ] , [ consistent ] 2 .",
    "@xmath223^{1/2 }   { \\mathbb{e}_n } [ ( \\hat{p}_{t}(x_i ) - p_{t}(x_i))^2]^{1/2 }     = o_{p_n}(n^{-1/2})$ ] .",
    "[ ate rates ]    these two collectively play the same role as the commonly - used , high - level requirement in semiparametrics that each first - step component separately converge at @xmath33 at least .",
    "indeed , employ just such a condition for each component .",
    "however , by making use of the doubly - robust property we have the weaker conditions shown .",
    "condition , allowing the nonparametric portion to converge at a slower rate , at any rate , or in some cases be inconsistent ; examples include , , , , cattaneo , crump , and jansson @xcite , and , among others . ]",
    "the first is a mild consistency requirement .",
    "the second requires an explicit rate on the product of errors , and hence if one function is relatively easy to estimate assumption [ ate rates ] can be satisfied even if the other does not converge at @xmath33 .",
    "this formalizes the benefit of doubly - robust estimation in general . in high - dimensional ,",
    "sparse modeling specifically the rates for the first stage depend on the sample size , the number of covariates considered , and the sparsity level .",
    "thus , if one function requires fewer covariates to estimate , i.e. smaller @xmath34 or @xmath38 , then greater complexity can be allowed for in the other ( capturing , in particular , their relative smoothness ) .",
    "the so - called `` additional - randomness '' estimators are more specific to the ( approximately ) sparse model context , and so we now codify the sparsity requirements of section [ sec - sparse ] and then give the additional conditions required for these estimators .",
    "[ sparsity ]   for each @xmath24 , @xmath52 obeys , , and , with @xmath224 and @xmath225 .    [",
    "ate union ]   for a sequence @xmath23 , @xmath226 and the estimators @xmath19 and @xmath40 obey the following , uniformly @xmath103 : @xmath227 \\right| = o_{p_n}(n^{-1/2 } )         \\quad \\text{and } \\quad                \\left\\| { \\hat{\\gamma}_{t}}- { \\gamma^*_{t}}\\right\\|_1    \\vee    \\| { \\hat{\\beta}_{t}}- { \\beta^*_{t}}\\|_1 = o_{p_n}(\\log(p \\vee n)^{-1/2}).\\ ] ]    these conditions are needed because we apply bounds for self - normalized sums @xcite . were the first to use these techniques in high - dimensional , sparse models .",
    "the first condition is high - level , but can be verified with conditions on the errors and a bound for estimation . for the former , assume that @xmath228 for some @xmath229 .",
    "a larger @xmath230 eases the restriction in assumption [ ate union ] but at the expense of stronger conditions on the noise distribution .",
    "for example , if @xmath231 are assumed gaussian , @xmath230 can be taken to be any ( large ) positive number .",
    "[ remark - lpm ] our results cover use of a linear probability model for @xmath19 , instead of the multinomial logistic form .",
    "all we require is a sufficiently high - quality approximation of the unknown function , and hence if assumptions [ first stage ] , and [ ate union ] if appropriate , can be slightly weakened in this case due to the linear link function . ]",
    "are met then uniform inference is possible using a linear probability model .",
    "our group lasso results ( theorems [ thm - ols ] and [ thm - post ols ] ) can be used directly to verify these conditions . in the same vein",
    ", multinomial logistic regression can be used to estimate @xmath20 if the outcome @xmath1 is discretely valued .",
    "we now come to our main results on inference on average treatment effects .",
    "most of our discussion will concern @xmath9 and @xmath197 ; similar points apply to results for @xmath102 and @xmath98 .",
    "our first result formalizes consistency of our estimates under misspecification .",
    "[ thm - ate consistency ] consider a sequence @xmath23 of data - generating processes .",
    "suppose that for some @xmath232 and @xmath233 , @xmath234   = o_{p_n}(1)$ ] and @xmath235    = o_{p_n}(1)$ ] .",
    "let assumptions [ ignorability ] and [ dgp asmpts ] hold for each @xmath24 , with the regularity conditions also holding for @xmath232 and @xmath233 .",
    "if @xmath236 or @xmath237 , then @xmath238 .",
    "this theorem formalizes the double - robustness property of our estimators : the propensity score or regression may be misspecified if the limiting objects are well - behaved .",
    "compare to assumption [ consistent ] .",
    "the nearly identical result for @xmath102 is omitted to save space .",
    "we now turn to our main inference results .",
    "first we demonstrate a bahadur representation of a generic @xmath239 or @xmath240 .",
    "these are shown to be equivalent to a sample average of the moment functions @xmath116 and @xmath241 , respectively , after proper centering and scaling , evaluated at the true @xmath242 and @xmath121 .",
    "using these results , asymptotic normality can be obtained for general estimands .",
    "we state explicit results for the leading examples @xmath197 and @xmath98 .    before giving the results for @xmath197 , we need an asymptotic variance formula .",
    "let the conditional variance of the potential outcomes be @xmath243 $ ] .",
    "define the @xmath244-square matrix @xmath245 with elements @xmath246 = { \\ensuremath{{{\\rm 1\\hspace*{-0.84ex }     \\rule{0.06ex}{1.37ex}\\hspace*{0.9ex}}}\\!}}\\{{t}= { t}'\\ } { \\mathbb{e}}\\left [   \\frac{\\sigma_{t}^2(x)}{p_{t}(x)}\\right ] + { \\mathbb{e}}\\left[({\\mu_{t}}(x ) - { \\mu_{t}})(\\mu_{{t}'}(x ) - \\mu_{{t}'})\\right ] \\equiv v_{{\\bm{\\mu}}}^w({t } ) + v_{{\\bm{\\mu}}}^b({t},{t}').\\ ] ] straightforward plug - in estimators for these two components are given by @xmath247     \\qquad \\text { and } \\qquad   \\hat{v}_{{\\bm{\\mu}}}^b({t},{t } ' ) = { \\mathbb{e}_n}\\left [ ( { \\hat{\\mu}_{t}}(x_i ) - { \\hat{\\mu}_{t } } ) ( \\hat{\\mu}_{{t}'}(x_i ) - \\hat{\\mu}_{{t } ' } ) \\right].\\ ] ]    our first result gives the asymptotic behavior of @xmath239 and @xmath195 for a sequence of dgps .",
    "[ thm - ate ] consider a sequence @xmath23 of data - generating processes that obey assumptions [ ignorability ] , [ dgp asmpts ] , and [ first stage ] for each @xmath24 .",
    "if @xmath17 and @xmath16 do not have additional randomness in the estimated supports , we have :    1 .",
    "[ thm - ate linearity ] @xmath248 ; 2 .",
    "[ thm - ate normality ] @xmath249 ; and 3 .",
    "[ thm - ate variance ] @xmath250 and @xmath251 .    if , in addition , assumptions [ sparsity ] and [ ate union ] hold , then the same is true when the supports contain additional randomness .",
    "theorem [ thm - ate ] itself may appear standard , but what is nonstandard is that the model selection step of the estimation has been explicitly accounted for .",
    "this immediately gives the following uniform inference results .",
    "[ thm - ate uniform ] let @xmath30 be the set of data - generating processes satisfying the conditions of theorem [ thm - ate ] for a given @xmath24 .",
    "for a fixed , twice uniformly continuously differentiable function @xmath252 with gradient @xmath253 such that @xmath254 is bounded away from zero , we have : @xmath255 - ( 1 - \\alpha ) \\right| \\to 0,\\ ] ] where @xmath32 .",
    "corollary [ thm - ate uniform ] shows that these procedures are uniformly valid over the class of dgps we consider , and hence will be reliable in applications . the crucial insight that leads to uniform",
    "inference is to change the goal of model selection away from perfect _ covariate _ selection ( the oracle property ) and to high - quality approximation of the underlying _ functions _ ( here @xmath256 and @xmath257 ) .",
    "this fundamental shift in focus allows us to avoid the uniformity problems demonstrated by leeb and ptscher .",
    "assumption [ first stage ] formalizes exactly the quality of approximation needed .",
    "such an approximation can be found for any element in @xmath30 , and hence inference is uniformly valid over that class .",
    "this method of proving uniformity follows and , and is distinct from the approach of .",
    "results for the treatment effects on the treated are similar .",
    "the variance formula for @xmath98 is slightly more cumbersome .",
    "define the @xmath99-square matrix @xmath258 with elements @xmath259   & =   { \\ensuremath{{{\\rm 1\\hspace*{-0.84ex }     \\rule{0.06ex}{1.37ex}\\hspace*{0.9ex}}}\\!}}\\{{t}= { t}'\\ } { \\mathbb{e}}\\left [ \\frac{p_{t}(x)}{p_{t}^2}\\left[\\sigma_t^2(x ) + \\left({\\mu_{t}}(x ) - \\mu_0(x ) - \\mu_{{t},{t } } + \\mu_{0,{t}}\\right)^2 \\right ] \\right ] +   { \\mathbb{e}}\\left[\\frac{p_{t}(x ) p_{{t}'}(x ) } { p_{t}p_{{t } ' } p_0(x)}\\sigma_0 ^ 2(x ) \\right ]         \\\\          & \\equiv    v_{{\\bm{\\tau}}}^w({t } ) + v_{{\\bm{\\tau}}}^b({t},{t } ' ) .",
    "\\end{aligned}\\ ] ] straightforward plug - in estimators for these two components are given by @xmath260 \\right ] \\text { and } \\hat{v}_{{\\bm{\\tau}}}^b({t},{t } ' ) = { \\mathbb{e}_n}\\left [ \\frac{\\hat{p}_{t}(x_i ) \\hat{p}_{{t}'}(x_i ) } { \\hat{p}_{t}\\hat{p}_{{t } ' } \\hat{p}_0(x_i)^2 } { d_i}^0(y_i - \\hat{\\mu}_0(x_i ) ) ^2 \\right].\\ ] ] note that we need nt estimate @xmath20 and @xmath261 , due to the simplification in remark [ remark - tot ] .",
    "with this notation , we have the following results .",
    "proofs are so similar to those for theorem [ thm - ate ] and corollary [ thm - ate uniform ] that we omit them .",
    "[ thm - tot ] consider a sequence @xmath23 of data - generating processes that obey assumptions [ ignorability ] , [ dgp asmpts ] , and [ first stage ] for each @xmath24 .",
    "then under @xmath52 , as @xmath262 , if @xmath17 and @xmath16 do not have additional randomness in the estimated supports :    1 .",
    "[ thm - tot linearity ] @xmath263 ; 2 .",
    "[ thm - tot normality ] @xmath264 ; and 3 .",
    "[ thm - tot variance ] @xmath265 and @xmath266 .    if , in addition , assumptions [ sparsity ] and [ ate union ] hold , then the same is true when the supports contain additional randomness .",
    "[ thm - tot uniform ] let @xmath30 be the set of data - generating processes satisfying the conditions of theorem [ thm - tot ] for a given @xmath24 . for a fixed , twice uniformly continuously differentiable function @xmath267 with gradient @xmath253 such that @xmath268 is bounded away from zero , we have : @xmath269 - ( 1 - \\alpha ) \\right| \\to 0,\\ ] ] where @xmath32 .",
    "the prior theoretical results are aimed at delivering robust inference . in this section ,",
    "we briefly discuss the efficiency of our estimator .",
    "we consider two efficiency criteria : semiparametric efficiency and oracle efficiency . to put each criterion on sound conceptual footing",
    ", we separate discussion and restrict each to the most appropriate set of models .    for semiparametric efficiency ,",
    "assume that @xmath19 and @xmath270 are nonparametric objects , as in example [ eg - semiparametric ] . recall that @xmath10 are fixed - dimension variables and the dgp does not vary with @xmath24 . if we `` upgrade '' the mean independence of assumption [ mean inde ] to full independence , namely @xmath271 , then theorems [ thm - ate ] and theorem [ thm - tot ] immediately yield asymptotic linearity and semiparametric efficiency , attaining or bounds .",
    "this requires there be no ( known ) instruments for treatment status in @xmath10 , as implicitly assumed in those works , otherwise the bound may change @xcite .    turning to oracle efficiency",
    ", an alternative to our robust approach is to prove that the true supports can be found with probability approaching one ( the oracle property ) , then conduct inference conditioning on this event .",
    "this approach can not be made uniformly valid , but may be of interest in the exactly sparse models of example [ eg - parametric ] ( there is no `` true '' support in approximately sparse models ) , because discovering the true support is equivalent to finding the variables in the causal mechanism @xcite , if one exists .",
    "this may be interesting in its own right , or for future applications by way of hypothesis generation .",
    "the post oracle selection estimator is made efficient by using only the variables important for @xmath272 $ ] .",
    "this amounts to entirely removing the instrumental variables indexed by @xmath273 , whose inclusion would , in general , reduce efficiency , though not increase bias .",
    "further , @xmath274 are excluded from propensity score estimation .",
    "perfect selection requires two strong conditions : ( i ) an orthogonality condition on the gram matrices that restricts the correlation between the variables in and out of the true support @xcite , and ( ii ) a _ beta - min _",
    "condition bounding the nonzero coefficients away from zero .",
    "intuitively , highly correlated variables can not be distinguished , nor can coefficients sufficiently close to zero be found with certainty . both bounds",
    "may depend on @xmath24 , and in particular the lower bound on the coefficients may vanish at an appropriate rate . under such conditions , it is straightforward to show that @xmath143 and @xmath142 can be found with probability approaching one .",
    "we now give details for group lasso model selection and estimation , and make the refitting precise",
    ". section [ sec - lambda ] discusses penalty choices and implementation .",
    "restricted and sparse eigenvalues , key quantities in our bounds , are discussed in section [ sec - eigenvalues ] .",
    "our main nonasymptotic results are stated in section [ sec - grplasso results ] .",
    "these results are of interest more generally in the literature on high - dimensional sparse models finally , section [ sec - rates ] gives asymptotic rates and verifies the conditions of section [ sec - ate ] .",
    "we first select covariates by applying the group lasso penalty to the multinomial logistic loss ( for the propensity scores ) and to least squares loss ( to estimate the outcome regression ) .",
    "the loss functions are defined as @xmath275         \\qquad \\text { and } \\qquad       { \\mathcal{e}}({\\beta_{{\\bm{\\cdot } } , { \\bm{\\cdot } } } } ) = { \\sum_{{t}\\in { \\overline{{\\mathbb{n}}}_{\\mathcal{t } } } }   } { \\mathbb{e}_{n,{t } } } [ ( y_i - { x_i^ * } ' { \\beta_{t}})^2].\\ ] ] then , the group lasso estimates for the propensity score coefficients , denoted @xmath276 , and the regression coefficients , @xmath277 , respectively solve @xmath278 where @xmath279 and @xmath280 are the penalty parameters discussed below and @xmath281 is the mixed @xmath83/@xmath82 norm .    to ameliorate the downward bias induced by the penalty and to allow for researcher - added variables , we refit unpenalized models . and @xmath166 .",
    "there is no relation to specification biases @xmath137 and @xmath138 .",
    "] let @xmath282 and @xmath283 be the selected covariates and @xmath203 and @xmath204 those used in refitting . and",
    "@xmath284 do not vary much over @xmath61 , the group lasso is known to have better properties than the ordinary lasso in terms of selection and convergence . give a sharp bound on the overlap necessary to yield improvements , while , , and also demonstrate advantages of the group lasso approach .",
    "these works show , among other things , that the group lasso advantage increases with large @xmath99 , and with the group structure , may perform better with smaller samples .",
    "we defer to the works cited for a formal discussion . ]",
    "we require @xmath285 and @xmath286 for @xmath206 and @xmath1 ( we will prove that @xmath287 in both cases ) . the refitting estimators",
    "solve @xmath288    [ remark - penalty ] the group lasso penalty can be weighted in two ways .",
    "first , one may weight the @xmath83 portion , as in @xmath289 , where @xmath290 is the design matrix for covariate @xmath68 , across all the treatments .",
    "other weight matrices are possible , but with this choice , the estimate is invariant to within group ( treatment ) reparameterizations , and is thus scale invariant for each covariate .",
    "we therefore assume @xmath147 = 1 $ ] without loss of generality .",
    "second , the @xmath82 norm can be weighted to give a penalty of the form @xmath291 .",
    "two common choices for @xmath292 are the number of variables in group @xmath68 or an adaptive penalty from a pilot estimate .",
    "our groups are equally sized , and although adaptive procedures may improve oracle properties @xcite , our goal is not perfect selection .",
    "we must specify choices of @xmath279 and @xmath280 for programs . from a theoretical point of view ,",
    "these must be chosen so that the penalty dominates the noise , which is captured by the magnitude of the score in the dual of the @xmath293 norm , with high probability .",
    "to acheive this , we set @xmath294 for some @xmath295 and @xmath296 . with these choices",
    ", @xmath297 \\|_2 $ ] and @xmath298 \\|_2 $ ] with probability @xmath299 for a small ( and shrinking ) @xmath300 . in generic terms",
    ", @xmath301 is of the form @xmath302 , where @xmath303 is an upper bound on the true score and @xmath304 is a rate that depends on @xmath24 and @xmath34",
    ". appears instead of @xmath305 .",
    "no error bound appears in @xmath279 because the errors are bounded by one .",
    "the multiple @xmath306 for @xmath280 , instead of 2 , can be traced to the quadratic loss .",
    "these forms are determined at heart by the maximal inequality of . ]",
    "the specific rate chosen serves to balances the rate of convergence against the concentration effect : a smaller @xmath304 would increase the rate of convergence , but at the expensive of lowering the concentration probability @xmath299 .",
    "in the appendix we show that ( for appropriate @xmath307 and @xmath24 or @xmath305 ) the concentration probability is given by @xmath308    there are two practical methods to make these choices for feasible for implementation . when @xmath39 and @xmath40 are used to estimate average treatment effects , the decreased sensitivity of the final estimate to the first stage , thanks to the doubly - robust estimator , in turn results in less sensitivity to the choice of penalty ( through the sparsity ) .",
    "the first option is an iterative procedure to estimate the unknown @xmath309 and @xmath310 in @xmath280 and @xmath279 , as employed by ( validity of this procedure may be established along the same lines as in that study ) .",
    "we use @xmath311 for @xmath309 and estimate @xmath310 by iteration : given an initial estimate @xmath312 , set @xmath313^{1/4}$ ] , where @xmath314 , @xmath315 , is based on eqn .  . in implementation we found 10 iterations more than sufficient , and",
    "based the initial estimate on ridge regression ( with penalty chosen by cross validation ) .",
    "a second option is to select @xmath280 and @xmath279 directly by cross - validation .",
    "this has the appealing feature that the precise forms of eqn .",
    "need not be characterized and estimated .",
    "if interest lies in the underlying functions @xmath19 and @xmath20 , cross validation is appropriate as it minimizes a relevant loss function .",
    "formal results establishing the validity of cross - validation are not available , but it performs well in practice .      the local behavior of optimizations and is captured by their respective hessians , which involve the second moment matrix of the covariates .",
    "the eigenvalues of such matrices will be explicit in our bounds .",
    "we are interested in finite sample bounds , and so we will only discuss the empirical gram matrices ( see remark [ remark - gram ] ) .",
    "define @xmath316     \\qquad \\text { and } \\qquad   q_{t}= { \\mathbb{e}_{n,{t}}}[{x_i^*}{x_i^*}'].\\ ] ] in high - dimensional data , both are singular , and so we use restricted eigenvalues and sparse eigenvalues @xcite .    for the multinomial logistic regression , the minimal restricted eigenvalue",
    "is defined by @xmath317 for least squares estimation we instead use @xmath318 note that @xmath319 appears for @xmath320 , whereas the @xmath321 are used in @xmath322 .",
    "the restricted set , or cone constraint , requires the magnitude of @xmath323 off the true support be small relative to the true support , measured in the group lasso norm .",
    "is traceable to the nonlinear model .",
    "] we will show that @xmath324 and @xmath325 obey the respective constraints .",
    "in contrast , the refitting errors @xmath326 and @xmath327 from may not obey the cone constraint , but are sparse by construction .",
    "this motivates the use of sparse eigenvalues . for a set @xmath75 and a @xmath328 matrix @xmath329 ,",
    "define @xmath330 finally , it will be useful to define a bound on @xmath331 over all subsets of a certain size . to this end , for any integer @xmath332 , define @xmath333 .",
    "we take these quantities to be primitive , and defer discussion to the literature . for example , see , , , , and . in particular , show that the group lasso may need fewer observations to satisfy conditions on sparse eigenvalues .",
    "[ remark - gram ] often , invertibility of @xmath319 and @xmath321 relies on their convergence to nonsingular population counterparts . case . ] some of the papers cited above verify conditions on the restricted and sparse eigenvalues by just this approach .",
    "our theorems can be restated in this way by conditioning on the event that @xmath319 and @xmath321 are close to their counterparts in the appropriate sense , and adjusting the probability with which the conclusions hold .",
    "we instead take bounds to be infinite if the minimum eigenvalues are zero .",
    "we now have the necessary notation and assumptions to state our theoretical results on group lasso estimation , beginning with multinomial logistic regression , followed by a terse treatment of linear models .",
    "corollary [ thm - grplasso overview ] is a special case of the results in this section , see section [ sec - rates ] .",
    "our first result is a nonasymptotic bound on the group lasso estimates from .",
    "[ thm - mlogit ] suppose assumptions [ overlap ] , [ iid ] , [ bounded ] , [ fourth moments ] , and [ sparsity ] hold .",
    "define @xmath334 and @xmath335 for @xmath336 . then with probability @xmath299 @xmath337 ^{1/2 } \\leq r_{\\mathcal{m}}+ { b_{s}}^d,\\ ] ] @xmath338 and @xmath339 where @xmath340    this theorem is new to the literature , to the best of our knowledge .",
    "much of the detail involves capturing the finite sample behavior of the hessian and gram matrices .",
    "we discuss the features of this result in the following remarks .    *",
    "the hessian of @xmath341 is @xmath342 $ ] for a @xmath99-square matrix @xmath343 that depends on the coefficients and @xmath344 through the estimated probabilities @xmath345 .",
    "the error @xmath346 depends on how well - controlled is this matrix .",
    "the factors @xmath347 , @xmath348 , and @xmath349 capture the behavior of @xmath343 and @xmath350 accounts for the rest . under overlap",
    ", the true probabilities are bounded below by @xmath347 , and hence @xmath351 captures the nonsingularity of the population version of @xmath343 . to get to this point",
    "requires two steps .",
    "first , the sparse parametric representations @xmath352 must also be bounded away from zero , leading to the factor of @xmath348 .",
    "this is essentially a bias condition , which in the asymptotic case holds trivially : @xmath348 may be chosen arbitrarily close to one as @xmath353 .",
    "second , @xmath349 controls the neighborhood in which @xmath354 is also bounded away from zero .",
    "intuitively ( and asymptotically ) , the estimate will be in a small ( shrinking ) neighborhood of the @xmath352 . in asymptotics @xmath349",
    "may be chosen arbitrarily close to 2 , which stems from the factor of 1/2 in a quadratic expansion of @xmath355 . a lower bound on @xmath349 is required in finite samples to ensure that @xmath354 is positive , and hence the two - term expansion is valid .",
    "this is analogous to `` restricted nonlinear impact coefficient '' approach , also used by with a central difference that @xmath349 is captured in our bound directly .",
    "* the maximal sparse eigenvalues are crucial to the bound on @xmath356 . in many prior results ,",
    "the latter is bounded using the largest eigenvalue of @xmath319 itself , i.e. @xmath357 . adapting the technique of to the present case , we are able to find a tighter bound , which yields sparsity proportional to @xmath38 under weaker conditions .",
    "this is crucial for refitting .",
    "* for the linear model the constants in the group lasso bounds can offset the ( logarithmic ) suboptimality in rate @xcite , and this may be true here as well .",
    "this is application dependent however .",
    "the error bounds for post - selection estimation are more complex and depend in part on the good properties of the initial group lasso fit .",
    "the following theorem gives our results .",
    "[ thm - post mlogit ] suppose the conditions of theorem [ thm - mlogit ] hold .",
    "to save notation , let @xmath358 and @xmath359 . then for @xmath360 define @xmath361 and @xmath362^{1/2 } \\right\\}.\\ ] ] then with probability @xmath299 , @xmath363 ^{1/2 } \\leq    r_{\\mathcal{m } } ''",
    "+ { b_{s}}^d,\\ ] ] and @xmath364    we explicitly capture the dependence on the loss function @xmath341 and the impact of the initial group lasso fit . it is not readily discernible if these bounds improve upon the group lasso estimates .",
    "this will in general depend on the dgp , the selection success of the initial fit , and any added variables . in this result",
    ", further lower bounds on @xmath349 are required to handle the sparse eigenvalues , compared to the restricted version in theorem [ thm - mlogit ] .",
    "the role played by @xmath349 is the same in both cases , as with the other factors .",
    "it is worth noting that , despite the complexity of multinomial logistic regression , the conditions for theorems [ thm - mlogit ] and [ thm - post mlogit ] are simple and intuitive , and match those used for linear models .",
    "we now give our results for group lasso estimation of the conditional outcome regressions .",
    "in computing @xmath121 for @xmath365 we are performing out of sample prediction , which slightly complicates the bounds .",
    "our first result is on the initial group lasso fit .",
    "[ thm - ols ] suppose assumptions [ overlap ] , [ iid ] , [ bounded ] , [ fourth moments ] , and [ sparsity ] hold .",
    "define @xmath366 then with probability @xmath299 @xmath367 ^{1/2 } \\leq \\left ( \\frac{\\overline{\\phi}\\{q , \\tilde{s}^y \\cup s_y^ * \\ } } { \\underline{\\phi}\\{q_{t } , \\tilde{s}^y \\cup s_y^ * \\ } } \\right)^{1/2 } r_{\\mathcal{e}}+ { b_{s}}^y,\\ ] ] @xmath368 and @xmath369 where @xmath370    this theorem generalizes to the nonparametric , approximately sparse case , improves the sparsity bound , and gives out of sample prediction ( imputation ) results .",
    "the analogous generalization for within sample prediction loss ( e.g.  multi - task learning ) , @xmath371 ^{1/2}$ ] , may be found in the supplement .    for refitting",
    ", we are predicting for the entire sample and so we utilize the general results given by for post - selection estimation of least squares .",
    "the following result is a direct implication of their lemma 7 and our theorem [ thm - ols ] .",
    "[ thm - post ols ] suppose @xmath226 in addition to the conditions of theorem [ thm - ols ] .",
    "then @xmath372^{1/2 }   \\leq a_1   \\sqrt { \\frac { s ( { \\mathcal{t}}\\wedge \\log(s { \\mathcal{t } } ) ) } { n \\underline{\\phi}\\{q , s_y^*\\ } } }   +   a_2 \\sqrt { \\frac { |\\hat{s}_y \\setminus s_y^*| \\log(p{\\mathcal{t } } ) } { n \\underline{\\phi } \\{q , s_y^{fp}\\ } } }   +    a_3 \\sqrt { { \\mathbb{e}_n}[({x_i^*}'{\\tilde{\\beta}_{t}}- { \\mu_{t}}(x_i))^2 ] } \\ ] ] and @xmath373\\right)^{1/2 } , \\ ] ] for absolute constants @xmath374 , k=1 , 2 , 3 , 4 that do not depend on @xmath24 nor the dgp .    as above",
    ", the performance of the refitting procedure depends in part on the success of the initial group lasso fit .",
    "indeed , the middle term is dropped if the true support union is found .",
    "the constants @xmath374 , k=1 , 2 , 3 , 4 are not given explicitly but are known to be absolute bounds @xcite under assumption [ dgp asmpts ] .",
    "this result is less precise than theorems [ thm - mlogit ] and [ thm - post mlogit ] , but sufficient to verify assumptions [ first stage ] and [ ate union ] .",
    "this section briefly presents asymptotic rates of convergence for the group lasso estimates , and uses these results to verify assumptions [ first stage ] and [ ate union ] in section [ sec - ate ] . for simplicity , we only state results for the post - selection estimators that we recommend in practice . in reducing the finite sample results of theorems [ thm - post mlogit ] and [ thm - post ols ] to rates of convergence we aim to retain the dependence on @xmath24 , @xmath34 , @xmath38 , and the bias .",
    "note that the number of treatments is fixed , and the overlap assumption ensures that all @xmath375 .",
    "further , the various ( restricted and sparse ) eigenvalues are commonly taken to be bounded ( or bounded away from zero ) in asymptotic analyses .",
    "this accounts for the remaining factors in the displayed bounds .    for multinomial logistic regression ,",
    "we obtain the following result .",
    "[ thm - mlogit rates ] suppose the conditions of theorem [ thm - post mlogit ] hold and further that ( i ) @xmath376 , ( ii ) @xmath320 is bounded away from zero , and ( iii ) @xmath377 is bounded away from zero and @xmath378 is bounded , uniformly in @xmath379",
    ". then    1 .",
    "@xmath380 , 2 .",
    "@xmath381 = o_{p_n } \\left(n^{-1 } s_d \\log(p \\vee n)^{3/2 + \\delta_{d } }    + ( { b_{s}}^d)^2 \\right)$],vand 3 .   @xmath382 .",
    "similarly , we have the following for the linear models .",
    "[ thm - ols rates ] suppose the conditions of theorem [ thm - post ols ] hold and further that ( i ) @xmath383 , ( ii ) @xmath322 is bounded away from zero , and ( iii ) uniformly in @xmath384 , @xmath385 is bounded away from zero and @xmath386 is bounded uniformly in @xmath387",
    ". then    1 .",
    "@xmath388 , 2 .",
    "@xmath389 = o_{p_n } \\left (   n^{-1 } s_y \\log(p \\vee n)^{3/2 + \\delta_y }   + ( { b_{s}}^y)^2   \\right)$ ] , and 3 .   @xmath390 .    it is now straightforward to verify the requirements of section [ sec - ate ] . assumption [ ate rates ] requires @xmath391 under the common assumption that @xmath392 , we require @xmath393 .",
    "both this , and the display above , clearly show how the sparsity and smoothness of the two functions interact due to the double robustness .",
    "assumption [ ate union ] can be verified similarly .",
    "these rates of convergence ( i.e. part 2 of each corollary ) are optimal up to factor @xmath394 . at heart",
    ", this loss appears to stem from the maximal inequality used to establish the concentration probability of . in practice , this is unlikely to be a limitation . as mentioned above , the use of group lasso can yield improvements in the constants if the data obey a grouped sparsity pattern , as is expected for treatment effects data , and may even yield improvements in the detection of the sparse signal , further offsetting the suboptimal @xmath51 factor ( see for example or ) .",
    "alternative methods could , in principle , yield a rate improvement .",
    "chief among these would be lasso - penalized linear probability models ( see also remark [ remark - lpm ] ) or separate logistic regressions .",
    "the group lasso approach adopted here reflects common practice , and so it may be preferred . in any case , the @xmath51 factors do not impact the treatment effect inference .",
    "to illustrate the uniform validity of our inference procedure we conducted a small monte carlo exercise to study how our estimator behaves as the propensity score and regression functions change , and the model selection problem becomes more or less difficult .",
    "for simplicity we focus on the average effect of a binary treatment .",
    "we generated 1000 observations @xmath395 from the models in example [ eg - approx sparse ] , using both @xmath396 and @xmath397 .",
    "the covariates include an intercept , with the remainder drawn from @xmath398 , with covariance @xmath399 = 2^{-|j_1 - j_2| } , 2 \\leq j_1 , j_2 \\leq p$ ] .",
    "errors are standard normal",
    ". the crucial aspects of the dgp are the coefficient vectors @xmath400 , @xmath401 , and @xmath402 .",
    "we consider a range of models , defined by positive scalars @xmath403 , @xmath404 , @xmath405 , and @xmath158 , as follows : @xmath406 with @xmath407 .",
    "the multipliers @xmath403 and @xmath404 affect the signal - to - noise ratio ( the variance is fixed ) , but not the sparsity . for very smaller values distinguishing the large and small coefficients is more difficult for a given sample size . the exponents @xmath405 and @xmath158 control the sparsity , where a sparse representation is not possible for small values .",
    "figure [ fig - plot - main ] shows the empirical coverage rates of 95% confidence intervals for @xmath7 for different dgps , for @xmath396 and @xmath408 .",
    "panels ( a ) and ( c ) show coverage as the multipliers @xmath403 and @xmath404 range over 0.01 ( weak signal ) to 1 ( strong ) , with @xmath409 . panels ( b ) and",
    "( d ) vary the sparsity exponents @xmath405 and @xmath158 over 1/8 ( not sparse ) to 4 ( very sparse ) , with @xmath410 . of 1000 observations total , the ( mean ) size of the comparison group declines from roughly 500 to 300 as @xmath404 increases and 450 to 300 as @xmath158 increases , over their given ranges .",
    "coverage is accurate over all signal strengths , and breaks down only when neither @xmath121 nor @xmath242 is sparse , which is exactly when assumption [ ate rates ] ( or condition ( ii ) of theorem [ thm - ate overview ] ) can not be satisfied .",
    "note that coverage accuracy is retained when only one function is sparse , showcasing the double - robustness property .",
    "the penalty parameters @xmath279 and @xmath280 are chosen using the iterative procedure described in section [ sec - lambda ] , with @xmath411 and @xmath412 throughout figure [ fig - plot - main ] .",
    "different dgps exhibit more or less sensitivity to these values .",
    "results using penalties chosen via 10-fold cross - validation appear in figure [ fig - plot - cross ] , which also exhibits excellent coverage across all sparse designs . are omitted .",
    "see the supplement for limited versions . this will be explored for future software development . ]            to illustrate the role that model selection can play in a real - world application , we revisit the national supported work ( nsw ) demonstration .",
    "the nsw has been analyzed numerous times since .",
    "our aim is a simple study of model selection , not a comprehensive or conclusive evaluation of the nsw .",
    "as such , we focus on the subsample used by and the panel study of income dynamics ( psid ) comparison sample , taking as given their data definitions , sample selection , and trimming rules . detailed discussion of these choices , and the nsw program may be found in dehejia and wahba @xcite ( hereafter dw99 and dw02 ) and , and references therein .",
    "briefly , the outcome of interest is earnings following a job training program .",
    "the dataset includes a treatment indicator , post - treatment earnings ( 1978 ) , two years of pre - treatment earnings ( 1974 and 1975 ) , as well as age , education , a marital status , and indicators for black and hispanic .",
    "thus , @xmath10 consists of seven variables .",
    "our goal is to highlight the role model selection has in inference , and hence we will be interested in comparing specifications .",
    "we will keep the estimator fixed : all estimates will be based on the doubly - robust estimator with standard errors from section [ sec - ate theory ] .",
    "we will consider the following :    1 .   * no selection * : @xmath10 , ( earn1974)@xmath415 , ( earn1975)@xmath415 , ( age)@xmath415 , and ( educ)@xmath415 ; 2 .   * informally selected : * the above , plus @xmath416\\{educ@xmath417hs } , @xmath416\\{earn1974=0 } , @xmath416\\{earn1975=0 } , and ( @xmath416\\{earn1974=0}@xmath418hispanic ) .",
    "this specification was selected by dw02 using an informal balance test .",
    "* group lasso selection : * @xmath10 , @xmath416\\{educ@xmath417hs } , @xmath416\\{earn1974=0 } , @xmath416\\{earn1975=0 } , all possible first - order interactions , and all polynomials up to order five of the continuous covariates ( age , educ , earn1974 , earn1975 ) .    for specifications 1 and 2 ,",
    "the same covariates are in the outcome and treatment models .",
    "all specifications include an intercept and we include education and pre - treatment income in the refitting step following model selection .",
    "we follow dw99 and dw02 and trim comparisons with estimated propensity score larger ( smaller ) than the maximum ( minimum ) in the treated sample .",
    "table [ table - lalonde ] presents results from these three specifications , and includes the experimental arm of the nsw .",
    "the group lasso based estimate performs very well : the point estimate is accurate and the interval is tight . selecting from 171 possible covariates allows for a great deal of flexibility , but the sparsity of the estimate keeps the variance well - controlled .",
    "the no - selection point estimate is accurate , but fails to yield significance , while the specification of dw02 yields a significant , but overly high estimate and wide confidence interval .",
    "the benefits of explicit model selection are clear ."
  ],
  "abstract_text": [
    "<S> this paper concerns robust inference on average treatment effects following model selection . in the selection on observables framework </S>",
    "<S> , we show how to construct confidence intervals based on a doubly - robust estimator that are robust to model selection errors and prove that they are valid uniformly over a large class of treatment effect models . </S>",
    "<S> the class allows for multivalued treatments with heterogeneous effects ( in observables ) , general heteroskedasticity , and selection amongst ( possibly ) more covariates than observations . </S>",
    "<S> our estimator attains the semiparametric efficiency bound under appropriate conditions . </S>",
    "<S> precise conditions are given for any model selector to yield these results , and we show how to combine data - driven selection with economic theory . for implementation , we give a specific proposal for selection based on the group lasso , which is particularly well - suited to treatment effects data , and derive new results for high - dimensional , sparse multinomial logistic regression . </S>",
    "<S> a simulation study shows our estimator performs very well in finite samples over a wide range of models . </S>",
    "<S> revisiting the national supported work demonstration data , our method yields accurate estimates and tight confidence intervals .    </S>",
    "<S> * keywords : * high - dimensional sparse model , heterogeneous treatment effects , uniform inference , model selection , doubly - robust estimator , unconfoundedness . </S>"
  ]
}