{
  "article_text": [
    "modern cosmology , the study of the large scale structure and evolution of our universe @xcite , has advanced to the point where we can now answer some very fundamental questions about the distribution of matter within our universe .",
    "ever since einstein postulated the theory of general relativity and , together with de sitter @xcite , showed how it could be applied to the universe as a whole , generations of physicists have pondered on the question of what is the overall geometry of our universe . within the past few years observations of the relic microwave radiation from the `` big bang ''",
    "@xcite have shown that the universe exhibits a geometry quite unlike that expected from theoretical prejudices alone .",
    "although on the largest scales the distribution of matter within our universe is both homogeneous and isotropic , on smaller scales  less than 1/20th the size of our visible universe  it is highly inhomogeneous .",
    "even though the matter distribution of the universe was exceptionally smooth 300,000 years after the creation event @xcite , over billions of years the ubiquitous attraction of the gravitational force amplifies the minute fluctuations in the early matter distribution into the structure we see today .",
    "moreover , the current best theories of structure formation suggest that the matter distribution we observe is formed in a ` hierarchical clustering ' manner with the small structures merging to form larger ones and so forth @xcite . this growth of structure is accelerated by an unseen massive ` dark matter ' component in our universe . although dark matter can not be observed directly , there is sufficient evidence within observations to conclusively infer its existence .",
    "modifications to newton s equations , to change gravitational accelerations on large scales , have had limited success , and can not presently be cast in a form compatible with general relativity @xcite .",
    "understanding the distribution of matter within our local universe can tell us much about the cosmic structure formation process . while on the very largest scales gravity is the dominant force , on smaller scales gas pressure forces , from the gaseous inter - galactic ( igm ) and inter - stellar mediums ( ism ) ,",
    "can play a significant role . in clusters of galaxies , for example , hydrodynamic forces produced by the igm lead to a distribution of gas that is held close to hydrostatic equilibrium .",
    "indeed , understanding the interaction between the ism and the stars that condense out of it , is currently one of the hottest research areas in cosmology @xcite .",
    "since if we can understand this process we are much closer to being able to infer how the galaxies we observe relate to the underlying distribution of dark matter that dominates the evolution of structure .",
    "although we are yet to absolutely determine the relation between galaxies and dark matter , measuring the distribution of galaxies is the only way of infering the distribution of all matter ( visible or not ) .",
    "measurements of the speed of recession of local galaxies , led @xcite to form the distance - redshift relation now know as ` hubble s law ' , which has become a bedrock for the development of cosmological theory .",
    "although modern surveys of galaxies use an updated , and more accurate , form of the distance - redshift relation to uncover the spatial distribution of galaxies , the principles involved remain the same as those used by hubble .    aided by",
    "highly automated observing and computer driven data analysis , a new generation of high quality galaxy redshift surveys is mapping our local universe with exquisite precision .",
    "the 2 degree field @xcite and sloan digital sky survey @xcite provide astronomers with a survey of the local universe out to a redshift of @xmath1 , and contain over 200,000 and one million ( when complete ) redshifts respectively . in figure [ 2df ]",
    "we show the distribution of galaxies for the 2df survey to give an visual impression of the type of inhomogeneity observed .",
    "traditionally , one of the primary goals of analysis of redshift surveys is the calculation of the two point auto - correlation function ( 2-pt cf ) .",
    "the large sample volumes provided by 2df and the sdss have allowed the 2-pt cf to be calculated with great accuracy . while the initial conditions produced by the `` big bang '' are widely believed to exhibit gaussian statistics ( kolb and turner , 1990 ) , the formation of structure by gravitational instability introduces non - gaussian features into the statistics of the matter distribution .",
    "hence , the 2-pt cf can not be a complete descriptor of the underlying matter distribution at late times .",
    "astronomers were aware of this issue comparatively early in the development of the field , and the theoretical basis for calculating higher order statistics was developed through the 1970 s ( see @xcite for a detailed summary ) .",
    "early attempts to measure higher order moments of the mass distribution , via the counts - in - cells method ( again see @xcite ) , suffered from inadequate sample size . because higher order moments tend to be progressively dominated by the most dense regions in a given sample , ensuring that adequate sampling has been performed is of utmost importance . ensuring low sample variance is also necessary , and given one sample the only way to check this is to analyse sub - samples , which rapidly depletes the available information .    from a theoretical perspective , higher order statistics are interesting in relation to gravitational perturbation theory and the evolution of non - linear gravitational clustering .",
    "analyses examining the accuracy of numerical simulation methods often rely upon higher order statistics .",
    "this is especially important in the study of gravitational clustering in ` scale free ' universes @xcite .",
    "the development of fast , parallel , statistical algorithms is vital to progress in this arena . while the development of parallel simulation algorithms has advanced forward rapidly ( thacker et al . , 2003 )",
    "development of parallel analysis tools has lagged behind .",
    "this is partially due to the fact that the benefits of developing a parallel analysis code can be shorted lived because the required analyses can change rapidly ( much faster than the simulation algorithms themselves ) .",
    "the rapid development times available on shared memory parallel machines make them an ideal complement to large distributed memory machines which most simulations are now run on .",
    "although throughout this paper we discuss the application of our new method to cosmology , it can be applied equally well to the statistics of any point process .",
    "indeed the terms ` particle ' and ` point ' are often used interchangeably .",
    "the method can also be modified to apply to different dimensions , although in 2 dimensions the gains are expected to be less significant due to the reduced amount of work in the counts - in - cells method .",
    "the layout of this paper is as follows : in section [ sect : stats ] , we quickly review the statistics we wish to calculate .",
    "this is followed by an explicit description of our new algorithm , and an examination of its performance .",
    "next we present a brief case study on applying our algorithm to cosmology and conclude with a brief summary .",
    "due to space limitations a full discussion of the counts - in - cells method , and how it is related to higher order moments , is beyond the scope of this paper .",
    "however an excellent discussion of counts - in - cells and statistical measurement processes may be found in @xcite . for completeness",
    ", we briefly summarize the statistics we are interested in measuring .",
    "the 2-pt cf , @xmath2 , measures the radial excess / deficit over poisson noise for a point process .",
    "it is defined in terms of the joint probability , @xmath3 , of finding objects in volume elements @xmath4 and @xmath5 separated by a radial distance @xmath6 , viz , @xmath7 where @xmath8 is the average number density of the point process .",
    "the fourier transform pair of the 2-pt cf is the power spectrum , @xmath9 , @xmath10 which is used to describe the statistics of the initial density field in cosmology .",
    "the joint probability idea can be generalized to n - pt processes , for example , the reduced 3-pt cf is defined by ; @xmath11 @xmath12 where @xmath13,@xmath14 and @xmath15 are defined by the triangle described by the three points under consideration .",
    "for cosmology , the assumptions of homogeneity and isotropy require that @xmath16 be a symmetric function of these three lengths .",
    "higher order correlation functions follow in a logical manner .    using the counts - in - cells method",
    ", it can be shown that the second central moment @xmath17 , where n is the count of points within spheres of radius @xmath18 ( and volume @xmath19 ) , is given by @xmath20 the third central moment @xmath21 , is given by @xmath22 both these equations show how integrals over the correlation functions enter in to calculations of the central moments .",
    "relationships for the higher order moments can be constructed , but rapidly become lengthy to calculate ( fry and peebles , 1978 ) .",
    "the final definition we require is one that relates higher order cumulants to the variance . to aid our discussion",
    "we introduce the following notation : the over - density of a point process relative to the mean density , @xmath23 , is given by @xmath24 where @xmath25 is the local deviation from the average density .",
    "although this is most usually recognized as a continuum description , it also provides a useful construct for our discussion of point processes .",
    "for example , since the local density of particles in the counts - in - cells method is given by @xmath26 , @xmath27 . from this definition of @xmath28",
    "the @xmath8-th order connected moments of the point process define the ` @xmath29 ' statistics via the following definition statistics are motivated by the assumption that , given the 2-pt cf , @xmath30 , the @xmath8-pt correlation functions scale as @xmath31 , see balian and schaeffer ( 1989 ) . ] :    @xmath32    the @xmath29 statistics play a central role in analysis of redshift surveys . to date , up to @xmath33 has been calculated by researchers @xcite .",
    "while the counts - in - cells method is conceptually beautiful in its relation to the @xmath29 statistics , it is computationally strenuous to calculate .",
    "as the radius of the sampling sphere becomes larger , on average the work to calculate the count within the sphere will grow at a cubic rate . in reality",
    "the situation can be potentially worse , since inefficiencies in particle book - keeping can appear ( having to search far down tree - nodes , or equivalently searching through very dense cells in a grid code ) . to counter this problem one can use a hierarchical ( tree ) storage of counts in cells on a grid , as discussed in @xcite .",
    "this greatly improves calculation time , since the summation over particles within cells is much reduced at large radii . using this method",
    "it has been reported that @xmath34 samples from a data set with 47 million particles can be generated in 8 cpu hours .",
    "the basis of our alternative ` smooth field algorithm ' is that each counts - in - cells value is a discrete sample of the local density field smoothed over the scale of the sample sphere . in the continuum limit of an infinite number of particles , defining the density @xmath28 , the sampled value @xmath35 can be written as an integral over the spherical top - hat function @xmath36 @xmath37 of radius @xmath38 and the raw density field @xmath28 , to give , @xmath39 where @xmath19 is the volume of the periodic sample region and @xmath40 the volume of the sample sphere ( a 3 dimensional top - hat ) . via the convolution theorem , the fourier transform of @xmath41 , namely , @xmath42 is given by @xmath43 thus we can quickly calculate the _ entire _",
    "@xmath44 field by fourier methods .",
    "the discrete calculation of counts can be expressed in almost the same way , except that the continuous density field is replaced by a discrete sum of three dimensional dirac delta functions , @xmath45 , @xmath46 where @xmath47 is the number of particles in the simulation , and @xmath48 gives the position of particle @xmath49 . in the counts - in - cells",
    "method the integral over the volume is replaced by a summation within the given search volume @xmath40 .    to connect these two approaches",
    "all that is needed is a smoothing function that will convert a discrete set of points to a continuous density field .",
    "we require a smoothing function , @xmath50 , which can be summed over the particle positions to reproduce a smooth field @xmath28 .",
    "provided we can do this , we can use fourier methods to precalculate all of the required @xmath41 values and greatly reduce the amount of work . in practice",
    "it will be necessary to define a discrete density on a grid , and then use an interpolation process to provide a continuum limit .",
    "the smoothing idea has been studied in great depth ( see @xcite for explicit details ) and there exists a series of computationally efficient smoothing strategies that have good fourier space properties , as well as having well defined interpolation function pairs . the most common smoothing function ( ` assignment function ' )",
    "mechanisms are ` cic ' ( cloud - in - cell ) , and ` tsc ' ( triangular shaped cloud ) .",
    "cloud - in - cell interpolation provides a continuous piece - wise linear density field , while tsc has a continuous value and first derivative .",
    "the only potential issue of difficulty is that sampling a continuous periodic variable at discrete points means that the fourier domain is finite and periodic and thus has the possibility of being polluted by aliased information ( with images separated by @xmath51 where l is the size of the period ) . in practice ,",
    "the higher order assignment functions have a sufficiently sharp cut - off in fourier space that this is not a significant problem .    having established that we can convert our discrete set of points into a continuous density defined by a grid of values and an interpolation function , we must decide upon the size of grid to be used .",
    "the initial configuration of points ( corresponding to a low amplitude power spectrum ) is such that the majority of neighbouring particles have separations close to the mean inter - particle separation @xmath52 .",
    "therefore , for this configuration we use a grid defined such that @xmath53 .",
    "this is beneficial on two counts : firstly , the grid requires a comparatively small amount of memory to store than the particle data , and secondly , it captures almost all the density information stored in the particle distribution ( since most particles are separated by sizes close to the grid spacing ) .    to summarize , the steps in the sfa are as follows :    1 .",
    "use an assignment function , @xmath50 , to smooth the mass ( @xmath54 ) associated with each of the particles on to a grid .",
    "this creates the grid representation of the density field , @xmath55 : @xmath56 2 .",
    "fourier transform the density field @xmath55 to form @xmath57 3 .",
    "multiply by @xmath58 , the product of the fourier transform of the real space top - hat filter ( @xmath59 ) and the inverse of the assignment function filter , which includes an alias sum out to two images 4 .",
    "fourier transform the resulting field back to real space 5 .",
    "calculate @xmath60 at all sampling positions using the interpolation function pair to the original assignment function @xmath50 6 .",
    "calculate desired statistics    in this paper we have used a 3rd order polynomial assignment function ( ` pqs ' , see hockney and eastwood , 1988 ) which is defined ( in 1-dimension ) by ; @xmath61 and the 3-dimensional function is defined @xmath62 .",
    "note that @xmath63 is not an isotropic function , which in this case is beneficial for speed , since it is unnecessary to calculate a square root .",
    "it also simplifies calculating the fourier transform of the assignment function since all the dimensions are now separable . note that @xmath64 has a comparatively wide smoothing profile , and",
    "therefore its fourier transform is a strongly peaked function with good band - limiting properties .",
    "this is advantageous for dealing with the aliasing problem mentioned earlier .",
    "indeed , the fourier transform of @xmath64 is : @xmath65 which has a @xmath66 suppression of power .",
    "this is sufficiently sharp to ensure that only the first and second images need be accounted for in @xmath58 ( the green s function associated with top - hat filtering and the assignment process ) .",
    "before proceeding to parallelize the algorithm , it is instructive to compare the speed of the serial algorithm as compared to the counts - in - cells method . in figure [ comp ] we show the time to calculate @xmath67 samples on @xmath68 points as a function of the sample radius .",
    "a ( logarithmic ) least - squares fit showed that the time for the standard counts - in - cells method ( version 1 ) grows as @xmath69 , which is slightly lower than the expected value of @xmath70 .",
    "for the second counts - in - cells algorithm we developed , which is optimized by storing a list of counts in the chaining cells used to control particle book - keeping in the code , the dependence with radius was found to be @xmath71 .",
    "this is understood from the perspective that most of the work in each sample has already been performed in the summation within chaining cells and that the work for each sample thus becomes dependent on sorting over the cells at the surface of the sample area , which is proportional to @xmath71 .",
    "however comparison of both these methods to the sfa shows they are far slower in comparison . because the entire @xmath41 field is precalculated ( modulo the interpolation process to non - grid positions ) in the sfa method ,",
    "the time to calculate the samples is constant as a function of radius , and is exceptionally fast .",
    "based up the data presented in figure [ comp ] , we initially estimated being able to calculate @xmath34 sample points on a @xmath72 data set in less than 2 cpu hours , which is over 4x faster than the results reported for tree - optimized counts - in - cells methods @xcite .",
    "we have recently confirmed this result using our parallel code , which took 6.5 minutes on 32 processors to calculate @xmath34 samples on a @xmath72 particle data set produced for a project being conducted at the pittsburgh supercomputing center .",
    "typically when calculating statistics , the value of the sampling radius ( equivalently the top - hat radius ) is varied so that the entire sampling process must be repeated many times .",
    "thus the most obvious method of parallelization is to create several different grids for each smoothing radius and process them in parallel .",
    "however , available memory considerations may well make this impractical .",
    "instead , it is better to parallelize each calculation for each radius .",
    "this is non - trivial as the following algorithmic steps must be parallelized :    1",
    ".   calculation of green s function 2 .",
    "forward fft of density grid to @xmath73-space 3 .",
    "multiplication of density grid by green s function 4 .",
    "reverse fft to real space 5 .",
    "sum over sample points    the first four items have all been parallelized previously for our main simulation code ( see thacker et al . , 1998 ) .",
    "the final step , while appearing to be somewhat straightforward , must be approached with care ( as we shall demonstrate ) .",
    "the obvious issues which need to addressed are ( 1 ) ensuring each thread has a different random seed for sample positions and ( 2 ) that the sum reduction of the final values across threads is performed . in practice ,",
    "both of these issues can be dealt with in very straightforward ways using the openmp shared memory programming standard .",
    "sum reductions can be controlled via the reduction primitive while different random seeds can be set using an array of initial values .",
    "parallelization in this environment turned out to be straightforward .",
    "tests on a 32 processor hp gs320 ( 1 ghz alpha ev6/7 processors ) at the canadian institute for theoretical astrophysics ( cita ) , showed reasonable speed - up ( see figure [ times ] ) , but comparatively poor efficiency ( 22% ) when 32 processors were used .",
    "there is also a noticeable step in the speed - up at 4 to 8 processors .",
    "this step is caused by memory for a job being moved to a second memory domain , or ` resource affinity domain ' ( rad ) , within the machine .",
    "the 32 processor machine has 8 rads in total , connected via a cross - bar , with 4 processors belonging to each rad .",
    "latency to remote rads is significantly higher than to local rads , which explains the increased execution time .",
    "additionally , as the amount of traffic on the cross - bar between the rads increases , latencies are known to increase by very large factors ( up to 3000 nanoseconds , cvetanovic , 2003 ) .",
    "this is a serious bottleneck in the gs320 design which has been removed in the latest gs1280 machine . ultimately , to improve performance on the gs320 ,",
    "it is necessary to increase the locality of the sampling technique to reflect the locality of memory within the machine , and avoid sending data across the cross - bar .",
    "note that using a block decomposition of data across the rads means that locality is only really necessary in one axis direction .",
    "therefore , we adopted the following strategy to improve performance :    1 .",
    "block decomposition of the @xmath41 grid across rads 2 .",
    "pre - calculate the list of random positions in the z - axis 3 .",
    "parallel sort the list of random positions in increasing z value 4 .",
    "parallelize over the list of z positions , calculating x and y values randomly    the resulting sample still exhibits poisson noise statistics and is therefore valid for our purposes .",
    "however , the sample points are now local in the z direction , which greatly reduces the possibility of remote access due to the block assignment of data .",
    "the scaling improvement for this method is shown in figure [ times ] .",
    "the improvement is striking .",
    "we achieved a 1.2x increase in performance for the single processor result alone , while at 32 processors we have achieved a 4.8@xmath74 improvement in speed - up and a tripling of the parallel efficiency ( 82% ) .",
    "note that the speed - up is still not perfect for the improved version .",
    "this may be a bandwidth issue since the interpolation at each sampling point requires 64 grid values , which breaks down into 16 cache lines , with only 8 floating point calculations performed for all the data in each cache line .",
    "note that it is unlikely that using the next lowest level of interpolation ( tsc ) would help .",
    "tsc requires 27 points grid points per sample , which is 9 cache lines , with 6 floating point calculations per cache - line .",
    "thus the overall ratio of calculation to memory fetches is actually reduced .",
    "the initial conditions for cosmological structure are prescribed by initial density , temperature and velocity fields .",
    "although there is debate over whether evolution in the early universe ( such as magnetic fields ) may induce a non - gaussian signal in the initial conditions @xcite , most researchers believe that the density field is gaussian process , and the velocity may be derived directly from it . in the absence of non - gaussian features , the density field , which is usually discussed in terms of the linear over - density @xmath75 , is completely described by its continuous power spectrum @xmath76 , where a is a normalization constant .",
    "this initially smooth field evolves under gravity to produce the locally inhomogeneous and biased distribution of galaxies we observe today ( see figure [ dstn ] , which compare particles positions from initial to final outputs ) .",
    "early evolution , when @xmath77 , is in the linear regime and can be described by perturbation theory .",
    "as the over - density values approach and later exceed unity , it is necessary to use simulations to calculate the non - linear evolution .",
    "thus , ideally , the initial conditions for simulations should correspond to the latest time that can be followed accurately by perturbation theory .",
    "@xcite has developed an algorithm for the fast calculation of the particle positions required for cosmological simulations via 2nd order lagrangian perturbation theory ( 2lpt ) .",
    "we have recently implemented this algorithm in parallel using openmp .",
    "although 2lpt requires more computation , it has significant advantages over the standard 1st order technique ( known as the zeldovich ( 1968 ) approximation ) as higher order moments exhibit far less transient deviations at the beginning of the simulation .",
    "further , one should in principle be able to follow the initial evolution to slightly later epochs using 2lpt and therefore begin simulations at a slightly later time . in practice ,",
    "the transient deviation issue is most significant .    in general",
    ", the more negative the spectral index the faster the initial transients die away .",
    "this is helpful , since most simulations are conducted with an effective spectral index , @xmath8 , of between -1.5 to -3 ( depending on the size of the simulation volume ) .",
    "also , although we have focused solely on particle position statistics in this paper , it is worth noting that a similar analysis can be applied to velocity fields defined on the point process .",
    "analysis of the transients in the velocity divergence field , @xmath78 , shows an even greater improvement when using the 2lpt method @xcite .    to test whether our new 2lpt code was reproducing the correct results we have compared the measured @xmath79 statistics for our 2lpt initial conditions versus those produces with the zeldovich approximation ( 1st order ) . at the initial expansion factor of @xmath80 ,",
    "the za predicts the following value for @xmath79 @xcite ; @xmath81 while 2lpt predicts ; @xmath82 thus after performing the 2nd order correction the value of @xmath79 should increase by 6/7 . in figure [ s3 ] we show the calculated values of @xmath79 for two sets of initial conditions , one created using the za and the other with the additional 2lpt correction . both the sfa measured values of @xmath79 are high for this particular set of phases ( as compared to the theoretical prediction ) , but we have confirmed that alternative random seeds can produce similar results .",
    "indeed we have found the values of @xmath79 are quite dependent upon the phases of the fourier waves used , and achieving a value that is asymptotic to the theoretical value is extremely difficult .",
    "we are currently investigating this phenomenon in more detail .",
    "however , a brief visual inspection of figure [ s3 ] provides evidence that the residual , @xmath83 , between the za and 2lpt results is close to @xmath84 .",
    "analysis of the set of residuals between the two lines gives @xmath85 ( @xmath86 deviation ) , confirming that our code is accurately reproducing the difference in @xmath79 values .",
    "we have presented a new fast algorithm for rapid calculation of one point cumulants for point processes .",
    "our algorithm is based upon a smoothed field approach , which reproduces the underlying statistical properties of the point processes field from which it is derived .",
    "the method is significantly faster than counts - in - cells methods because the overhead of evaluating the number of particles in a given sphere has been removed .",
    "we are able to calculate @xmath34 sample points on a @xmath72 data set in less than 2 cpu hours , which is over 4x faster than the results reported for tree - optimized counts - in - cells methods @xcite .",
    "we also note that while tree methods also lead to very large speed ups , they are still subject to noise from the point process for low amplitude signals .",
    "we are currently applying this new technique to examine the evolution of high order moments in cosmological density fields at low amplitude levels and will present our findings elsewhere ( thacker , couchman and scoccimarro in prep ) .",
    "we also anticipate making the codes described in this paper publically available in the near future .",
    "rjt is partially supported by a cita national fellowship .",
    "hmpc acknowledges the support of nserc and the ciar .",
    "rjt would like to thank evan scannapieco and lars bildsten for hosting him at u. c. santa barbara where part of this research was conducted .",
    "this research utilized cita and sharcnet computing facilities .",
    "bennett , c.l .",
    "_ et al_. ( 2003 ) first - year wilkinson microwave anisotropy probe ( wmap ) observations : preliminary maps and basic results , _ the astrophysical journal : supplements _ , vol .",
    "1 , pp.127 .",
    "thacker , r.j . and couchman , h.m.p ( 2001 ) ` star formation , supernova feedback , and the angular momentum problem in numerical cold dark matter cosmogony : halfway there ? ' , _ the astrophysical journal _ , vol .",
    "555 , no .  1 ,",
    "pp.l17-l20 .",
    "thacker , r.j . ,",
    "pringle , g. , couchman , h.m.p . and booth , s. ` hydra - mpi : an adaptive particle - particle , particle - mesh code for conducting cosmological simulations on massively parallel architectures ' , _ high performance computing systems and applications 2003 _ nrc research press .",
    "fry , j.n .",
    "and peebles , p.j.e .",
    "( 1978 ) ` statistical analysis of catalogs of extragalactic objects .",
    "ix - the four - point galaxy correlation function ' , _ the astrophysical journal _ , vol .",
    "221 , no .  1",
    ", pp.1933 .",
    "szapudi , i. , meiksin a. and nichol , r.c .",
    "( 1996 ) ` higher order statistics from the edinburgh / durham southern galaxy catalogue survey .",
    "i. counts in cells ' , _ the astrophysical journal _ , vol .",
    "473 , no .  2 , pp.1521"
  ],
  "abstract_text": [
    "<S> higher order cumulants of point processes , such as skew and kurtosis , require significant computational effort to calculate . </S>",
    "<S> the traditional counts - in - cells method implicitly requires a large amount of computation since , for each sampling sphere , a count of particles is necessary . </S>",
    "<S> although alternative methods based on tree algorithms can reduce execution time considerably , such methods still suffer from shot noise when measuring moments on low amplitude signals . </S>",
    "<S> we present a novel method for calculating higher order moments that is based upon first top - hat filtering the point process data on to a grid . after correcting for the smoothing process , we are able to sample this grid using an interpolation technique to calculate the statistics of interest </S>",
    "<S> . the filtering technique also suppresses noise and allows us to calculate skew and kurtosis when the point process is highly homogeneous . </S>",
    "<S> the algorithm can be implemented efficiently in a shared memory parallel environment provided a data - local random sampling technique is used . </S>",
    "<S> the local sampling technique allows us to obtain close to optimal speed - up for the sampling process on the alphaserver gs320 numa architecture .    </S>",
    "<S> 3mp@xmath0 m 3map@xmath0 m </S>"
  ]
}