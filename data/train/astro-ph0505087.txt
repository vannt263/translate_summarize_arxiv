{
  "article_text": [
    "cosmological n - body simulations are the main tool used to study the dynamics of collisionless dark matter and its role in the formation of cosmic structure .",
    "they first became widely used 20 years ago after it was realized that the gravitational potentials of galaxies are dominated by dark matter .",
    "at the same time , theories of the early universe were developed for dark matter fluctuations so that galaxy formation became an initial value problem .",
    "although many of the most pressing issues of galaxy formation require simulation of gas dynamics as well as gravity , there is still an important role for gravitational n - body simulations in cosmology .",
    "dark matter halos host galaxies and therefore gravitational n - body simulations provide the framework upon which one adds gas dynamics and other physics .",
    "moreover , many questions of structure formation can be addressed with n - body simulations as a good first approximation : the shapes and radial mass profiles of dark matter halos , the rate of merging and its role in halo formation , the effect of dark matter caustics on ultra - small scale structure , etc .    in a cosmological n - body simulation ,",
    "the matter is discretized into particles that feel only the force of gravity .",
    "a subvolume of the universe is sampled in a rectangular ( not necessarily cubic ) volume with periodic boundary conditions . in principle , one simply uses newton s laws to evolve the particles from their initial state of near - perfect hubble expansion .",
    "gravity takes care of the rest .    in practice ,",
    "cosmological n - body simulation is difficult because of the vast dynamic range required to adequately model the physics .",
    "gravity knows no scales and the cosmological initial fluctuations have power on all scales .",
    "after numerical accuracy and speed , dynamic range is the primary goal of the computational cosmologist .",
    "one would like to simulate as many particles as possible ( at least @xmath2 to sample galaxies well within a supercluster - sized volume ) , with as great spatial resolution as possible ( at least @xmath3 per dimension ) , for as long as possible ( @xmath4 to @xmath3 timesteps to follow the formation and evolution of structure up to the present day ) .",
    "a single computer is insufficient to achieve the maximum possible dynamic range .",
    "one should use many computers cooperating to solve the problem using the technique of parallelization . in a parallel n - body simulation",
    ", the computation and memory are distributed among multiple _ processes _ running on different _ nodes _ ( computers ) .",
    "unfortunately , ordinary compilers can not effectively parallelize a cosmological n - body simulation code .",
    "a programmer must write special code instructing the computers how to divide up the work and specifying the communication between processes .",
    "a parallel code is considered successful if it produces load - balanced and scalable simulations .",
    "a simulation is _ load balanced _ when the distribution of the effective workloads among the nodes is uniform .",
    "_ scalability _ for a given problem means that the wall clock time spent by the computer cluster doing simulations scales inversely with the number of nodes used .",
    "ideally , of course , the code should also be _ efficient _ : as much as possible , the wall clock time should be entirely devoted to computation .    at present , there are two main algorithms used for cosmological n - body codes : tree and p@xmath0 m ( see bertschinger 1998 for review ) .",
    "the current parallel tree code implementations include treesph @xcite ) , hot @xcite , gadget @xcite , and gasoline @xcite .",
    "tree codes have the advantage of relatively easy parallelization and computing costs that scale as @xmath5 where @xmath6 is the number of particles .",
    "however , they have relatively large memory requirements .    the p@xmath0 m ( particle - particle / particle - mesh ) method was introduced to cosmology by @xcite and is described in detail in @xcite ( see also bertschinger & gelb 1991 ) . for moderate clustering strengths ,",
    "p@xmath0 m is faster than the tree code but it becomes slower when clustering is strong .",
    "this is because p@xmath0 m is a hybrid approach that splits the gravitational force field of each particle into a long - range part computed quickly on a mesh plus a short - range contribution computed by direct summation over close pairs .",
    "when clustering is weak , the computation time scales as @xmath7 where @xmath8 is the number of grid ( mesh ) points , while when clustering is strong the computation time increases in proportion to @xmath9 .",
    "the scaling can be restored to @xmath5 using adaptive methods @xcite .",
    "currently there exist several parallel implementations of the p@xmath0 m algorithm , including the version of @xcite for the ( now defunct ) connection machine cm-5 and the hydra code of @xcite .",
    "the hydra code uses shared memory communications for the cray t3e .",
    "there is a need for a message - passing based version of p@xmath0 m ( and its adaptive extension ) to run on beowulf clusters .",
    "this need motivates the present work .",
    "the difficulty of parallelizing adaptive p@xmath0 m has led a number of groups to use other techniques to add short - range forces to the particle - mesh ( pm ) algorithm .",
    "the tree and pm algorithms have been combined by @xcite and @xcite while @xcite use a two - level adaptive mesh refinement of the pm force calculation .",
    "the flash code @xcite has been extended to incorporate pm forces with multi - level adaptive mesh refinement .    when the matter distribution becomes strongly clustered , parallel codes based on pm and p@xmath0 m face severe challenges to remain load - balanced .    in general",
    ", p@xmath0 m and pm - based parallel codes suffer complications when the matter becomes very clustered as happens at the late stages of structure formation .",
    "most of the existing codes use a static one - dimensional slab domain decomposition , which is to say that the simulation volume is divided into slices and each process works on the same slice throughout , even when the particle distribution becomes strongly inhomogeneous .",
    "the gotpm code uses dynamic domain decomposition , with the slices changing in thickness as the simulation proceeds , resulting in superior load balancing .",
    "however , even this code will break down at very strong clustering because it also uses a one - dimensional slab domain decomposition .",
    "the flash code uses a more sophisticated domain decomposition similar in some respects to the method introduced in the current paper .",
    "the motivation of the current work is to produce a publicly available code that will load balance and scale effectively for all stages of clustering on any number of nodes in a beowulf cluster .",
    "this paper introduces a new , scalable and load - balanced approach to the parallelization technique for the p@xmath0 m force calculation .",
    "we achieve this by using dynamic domain decomposition based on a space - filling hilbert curve and by optimizing data storage and communication in ways that we describe .",
    "this paper is the first of two describing our parallelization of an adaptive p@xmath0 m algorithm .",
    "the current paper describes the domain decomposition and other issues associated with parallel p@xmath0 m .",
    "the second paper will describe the adaptive refinement method used to speed up the short - range force calculation .",
    "the outline of this paper is as follows .",
    "the serial p@xmath0 m algorithm ( based on gelb & bertschinger 1994 and ferrell & bertschinger 1994 ) that underlies our parallelization is summarized in  [ sec_serial ] .",
    "section [ sec_par ] discusses domain decomposition methods starting with the widely - implemented static one - dimensional slab decomposition method .",
    "we then introduce the space - filling hilbert curve and describe its use to achieve a flexible three - dimensional decomposition .",
    "section [ sec_loadbal ] presents our algorithm for dynamically changing the domain decomposition so as to achieve load balance .",
    "section [ sec_layout ] presents our techniques for organizing the particle data so as to minimize efficiency in memory usage , cache memory access , and interprocessor communications . in  [ sec_hcforce ] we describe the algorithms used to parallelize the pm and pp force calculations .",
    "section [ sec_test ] presents code tests emphasizing load balance and scalability .",
    "conclusions are presented in  [ sec_concl ] .",
    "an appendix presents an overview of the code and frequently appearing symbols , and another appendix briefly describe the routines used to map the hilbert curve onto a three - dimensional mesh and vice versa .",
    "in this section we summarize our serial cosmological c implementation  based on an earlier serial fortran implementation of p@xmath0 m by one of the authors .",
    "we discuss in detail the code units and aspects of the force calculation that are necessary for understanding the parallelization issues covered in the later sections .      given the pairwise force @xmath10 between two particles of masses @xmath11 and @xmath12 and separation @xmath13 , we define the interparticle force law profile @xmath14 . for a system of many particles ,",
    "the gravitational acceleration of particle @xmath15 is @xmath16 .",
    "the required interparticle force law profile depends on the shape of the simulation particles . for point particles one uses the inverse square force law profile @xmath17 .",
    "the inverse square force law is not used for simulation of dark matter particles in order to avoid the formation of unphysical tight binaries , which happens as a result of two - body relaxation @xcite . for cold dark matter simulations many authors use the @xcite force law @xmath18 where @xmath19 is the plummer softening length .",
    "we take the plummer softening length to be constant in comoving coordinates . with plummer softening",
    "the particles have effective size @xmath19 . in a p@xmath0 m code , @xmath19 is usually set to a fraction of the pm - mesh spacing .    in a p@xmath0 m code ,",
    "the desired ( e.g. , plummer ) force law is approximated by the sum of a long - range ( particle - mesh or pm ) force evaluated using a grid and a short - range ( particle - particle or pp ) force evaluated by direct summation over close pairs .",
    "the pm force @xmath20 varies slightly depending on the locations of the particles relative to the grid ( see appendix a of ferrell & bertschinger 1994 ) .",
    "the average pm force law @xmath21 can be tabulated by a set of monte - carlo pm - force simulations each having one massive particle surrounded by randomly placed test particles @xcite . in practice",
    ", the mean pm force differs from the inverse square law by less than 1% for pair separations greater than a few pm grid spacings . for smaller separations , a correction ( the pp force ) must be applied .",
    "the total force is given by @xmath22 strictly speaking , the p@xmath0 m force is not translationally invariant and therefore depends on the positions of both particles .",
    "the p@xmath0 m force differs from the exact desired interparticle force profile @xmath23 by @xmath24 .    at large separations , both the pm - force and the required force reduce to the inverse square law ( modified on the scale of the simulation volume by periodic boundary conditions ) .",
    "the pp - force can therefore be set to zero at @xmath25 for some @xmath26 .",
    "the pp - correction is applied only for separations @xmath27 . the pm - force on the other hand",
    "is mainly contributed by remote particles .",
    "the equation of motion of particles in a robertson - walker universe is @xmath28 where  @xmath29 is the comoving position and @xmath30 is comoving ( conformal ) time .",
    "the potential @xmath31 satisfies the poisson equation @xmath32 where @xmath33 is the excess of the proper density over the background uniform density .",
    "the equations take a simpler and dimensionless form in a special set of units that we adopt .",
    "the coordinates , energy and time in our code are brought to this form .",
    "let us denote by tildes variables expressed in code units .",
    "then for the units of time , position , velocity and energy ( or potential ) , we write @xmath34 , @xmath35 , @xmath36 and @xmath37 or @xmath38 , where  @xmath39 is the expansion factor of the universe , @xmath40 is the proper velocity , @xmath41 is the hubble constant and  @xmath42 is the cell spacing of the pm density mesh in our code ( see sec .",
    "[ sec_pm ] ) expressed in comoving mpc . in these units ,",
    "the equation of motion ( [ eq_motion_cosm ] ) reduces to @xmath43 we choose units of mass so that the poisson equation takes the following form in dimensionless variables : @xmath44 where @xmath45 is the proper mean matter density .",
    "particle masses are made dimensionless by @xmath46 $ ] .",
    "the dimensionless total mass of all the particles is @xmath47 where @xmath48 is the total number of pm mesh points .",
    "periodic boundary conditions are assumed in each dimension so that a finite volume simulation represents a small portion of a universe that is homogeneous on larger scales .    as a check on overall code accuracy , we monitor global energy conservation by integrating the layzer - irvine equation , which in code units takes the form @xmath49 where @xmath50 are the dimensionless gravitational and kinetic energies in the simulation .",
    "note that in a robertson - walker background , the hamiltonian is time - dependent and so the energy is not conserved @xcite .",
    "however , we can integrate equation ( [ eq_energy ] ) to get a quantity that should remain constant as the simulation progresses , @xmath51      a particle is represented in both our serial and parallel codes by a structure , defined as @xmath52 the size of the structure is 44 bytes on 32 bit machines .",
    "the structure contains three positions , the mass , accelerations and velocities of the particles along the three spatial cartesian directions , all made dimensionless by the choice of units described above . in addition , the integer @xmath53 is used to tag particles .",
    "this number can be arbitrary and is not used anywhere in force calculations or particle propagation . in the serial code ,",
    "the particles are stored in memory simply as an array with base pointer @xmath54 and end pointer @xmath55 where @xmath56 is the total number of particles in the simulation volume .",
    "to scan all the particles , e.g. for their imaging , we loop over all the pointers @xmath57 within the range @xmath58 .",
    "the particle masses are not required to be equal to each other in general .",
    "all the particles in the code are positioned within the simulation box of size @xmath59 , where @xmath60 labels the spatial dimension .",
    "( we allow for unequal lengths with @xmath61 equalling a ratio of small integers . )",
    "periodic boundary conditions are applied to bring particles that move outside back into the simulation volume .",
    "we currently use a drift - kick - drift ( dkd ) leapfrog integrator scheme @xcite to integrate the equations of motion ( [ eq_motionxv ] ) for the particles : @xmath62      { \\rm force \\ ; calculation\\ ; } { \\bf g}_{n+1/2}\\\\[3pt ]      { \\bf v}_{n+1 } = { \\bf v}_n+{\\bf g}_{n+1/2}\\ , \\delta t\\\\[1pt ]      { \\bf x}_{n+1 } = { \\bf x}_{n+1/2}+\\frac{1}{2}{{\\bf v}_{n+1}\\ , \\delta t}\\ ,    \\end{array}\\ ] ] where the subscripts denote timesteps .",
    "@xcite discuss the accuracy and stability of this scheme . note that the p@xmath0 m force calculation is needed only once each timestep",
    "all integrators have advantages and limitations . for our problem , which can be expressed as a continuum hamiltonian time evolution , the leapfrog integrator of equation ( [ eq_leapfrog ] ) is a good choice , since with a constant timestep it is _",
    "symplectic_. a symplectic integrator preserves the poincar integral invariants and follows the time evolution under a discrete hamiltonian that is close to the continuum hamiltonian of interest .",
    "the difference between the discrete and continuum hamiltonian or the discrete integrator error is itself a hamiltonian . when the error is a hamiltonian , and is sufficiently small , according to the kam theorem @xcite the difference between the hamiltonian paths evolved by the two hamiltonians is a set of finite measure .",
    "therefore most of the structure of the hamiltonian flow evolved by the continuum hamiltonian will be preserved when evolved by the discrete hamiltonian with the symplectic integrator .",
    "most of the stable orbits in the continuum hamiltonian system will remain stable under the discrete hamiltonian evolution and vice versa .",
    "higher order symplectic integrators for hamiltonian evolution can be constructed using the method of @xcite , which requires more force evaluations per timestep . in general , a @xmath6-th order symplectic integrator requires at least  @xmath63 force evaluations per complete timestep .    in a cosmological simulation ,",
    "particles become more clustered with time .",
    "it is not practical therefore to have a fixed value of the timestep for the whole simulation .",
    "currently we advance equation ( [ eq_leapfrog ] ) with the same value of timestep @xmath64 for all the particles but allow it to change with time .",
    "the choice for @xmath64 is based on the current particle data and therefore depends on the phase space variables .",
    "consequently , equation ( [ eq_leapfrog ] ) is no longer an exact symplectic map .",
    "nevertheless , it remains in practice well - behaved provided the timestep varies sufficiently slowly .    the timestep must at least satisfy the leapfrog stability criterion @xmath65 given by equation ( 4 - 42 ) of @xcite .",
    "this stability requirement is essentially equivalent to the constraint that the global timestep must be small enough not to exceed the local dynamical time at any point within the simulation box , @xmath66 where @xmath67 is a dimensionless constant .",
    "the density is somewhat expensive to obtain , but given the particle accelerations and using the approximation @xmath68 , we have now , expressed in code units , @xmath69 where @xmath70 is the dimensionless plummer softening length ( see   [ sec_force ] ) , @xmath67 is a free parameter in the code usually set to a small value such as @xmath71 , and @xmath72 is the maximum acceleration of a particle in the simulation box in code units .",
    "this criterion is conservative in assuring that the orbits of all particles are well sampled .    to further improve the integration technique",
    ", one may consider adaptive integrators , with individual particle timesteps changing according to the local dynamical time at the given position within the simulation volume @xcite . on the other hand",
    ", one may consider higher order symplectic integrators , which would require more force evaluations per timestep .",
    "some of the non - symplectic higher order integrators , such as runge - kutta , are known not to preserve the hamiltonian flow structure even with fixed timesteps .",
    "for example , it can be shown by integration of kepler orbit that the popular fourth order runge - kutta integrator yields a divergent orbit very quickly .",
    "on the other hand , for second order integrators , the dkd scheme shows stable orbits with errors behaving as small perturbations as expected on the basis of kam theorem . in this paper",
    "we adopt the leapfrog integrator with variable timestep ( set to the same value for all particles ) , leaving the implementation of a higher order symplectic integrator and individual particle timesteps for future work .",
    "the particle  mesh ( pm ) force is the long - range force that can be computed using fast fourier transforms ( fft ) . in our code",
    ", we use the fftw fourier transform implementation @xcite and the pm algorithm of @xcite . for large total number of particles @xmath56 in the simulation box ,",
    "the pm force computation is faster than the direct summation method , requiring only @xmath73 operations in total ( @xmath74 is the number of pm grid points ) , as opposed to @xmath75 .    the rectangular pm - density mesh is allocated for the whole simulation volume in the serial code .",
    "this grid is to be filled with the density values interpolated from the particles nearby .",
    "@xcite discuss a number of methods for the density interpolation with increasing smoothness , ranging from nearest grid point ( ngp ) to cloud - in - cell ( cic ) to the triangular - shaped cloud ( tsc ) method .",
    "the highest of accuracy of these is given by the tsc interpolation scheme and that is the scheme we have implemented . as shown by @xcite , an interpolation of the mass value from a particle at position @xmath76 to a grid point at position @xmath77 within the pm mesh and vice versa takes place if and only if @xmath78 where the absolute value is taken with the proper account for the boundary conditions , and @xmath79 is the window function domain locality length , specific to the interpolation scheme used , e.g. @xmath80 , @xmath81 and @xmath82 . in our code",
    "we have @xmath83 .",
    "centered on the grid point .",
    "right : force interpolation from grid points to particles pm step 5 . to get the force on a given particle ( open circle ) , force values are used from all of the surrounding grid points .",
    "]    there are several steps involved for one pm force calculation :    1 .",
    "density interpolation : masses of particles are interpolated to a rectangular density mesh of grid points using a forward tsc interpolation scheme as illustrated by the left figure [ fg_grid ] .",
    "details are given on pp .",
    "142146 of @xcite and equation ( a.16 ) of @xcite .",
    "the mesh density is fourier transformed to the complex domain .",
    "the force is computed in the complex domain using a pretabulated green s function given by equation ( a.14 ) of @xcite .",
    "the mesh force field is inversely fourier transformed to return to the real domain .",
    "force interpolation : forces are interpolated from the force mesh to particles using a backward tsc interpolation scheme , as shown in the right figure [ fg_grid ] .",
    "this step is opposite to step 1 .    in step 5 ,",
    "information flows in exactly the opposite direction as step 1 .",
    "only the same grid points satisfying equation ( [ eq_int ] ) that acquired their density values from the particles in step 1 are used for the interpolation of the forces to only the same particles in step 5 .",
    "if an exchange of the information between a grid point and a particle ever occurs , _ it has to be both ways_. this point will be very useful when we discuss density and force grid messages for the parallel code in  [ sec_pm_denfor ] .",
    "the timing of the pm force evaluation scales as @xmath84 where the first term is due to the density and force interpolation and the second is due to the fast fourier transform .",
    "the coefficients @xmath85 and @xmath86 do not depend on @xmath56 and @xmath48 .",
    "the coefficient @xmath85 depends on the interpolation scheme used . for the tsc interpolation scheme in @xmath87 dimensions , the density is always interpolated from a particle to the @xmath88 nearby grid points satisfying the condition ( [ eq_int ] ) . during the force interpolation ,",
    "the inverse occurs three times : once for each of the three spatial dimensions .",
    "the factor of  @xmath89 therefore enters into an expression for  @xmath85 when tsc interpolation is used .",
    "the coefficient  @xmath86 is independent of  @xmath85 and is given by the existing benchmarks for the fftw implementation @xcite .      in order to calculate the short range force",
    ", we must first find all the pairs separated by less than @xmath90 .",
    "this is accomplished using a fast linked - list sorting procedure @xcite . at the start of a simulation",
    "the whole simulation volume is partitioned into rectangular _ chaining mesh _",
    "cells whose spacings in dimension @xmath15 are constrained by @xmath91 given this constraint , for any particle in any chaining mesh cell , only the particles within the same or one of the adjacent chaining mesh cells need be included in the short range force calculation , since the pp force is zero for separations greater than @xmath90 . choosing the smallest possible value satisfying equation ( [ eq_dxcm ] ) , this leads to @xmath92\\ , \\ ] ] where the square brackets signify taking the integer part . at the start of the run ,",
    "we sort all the particles into chaining mesh cells occupying the 3d volume and form linked lists of particles belonging to each cell .",
    "each chaining mesh cell then contains the root of the linked list to all the particles within that cell",
    ".     constitute the 4 cells needed for pp force calculation in two dimensions , @xmath93 . ]    in order to apply a short range force correction to a particle @xmath94 within the simulation volume , we access particles contained within the same cell as well as the particles within the @xmath95 surrounding chaining mesh cells . since the short range correction procedure is applied for each pair of particles within the simulation volume , we need to traverse only half of the surrounding cells , as illustrated in figure [ fg_bpp ] . for a given chaining mesh cell @xmath96 , let @xmath97 be the number of particles within the cell and @xmath98 be the set of the @xmath99 surrounding cells used for the short range force calculation . the number of floating point operations needed in order to apply the short range force correction for every particle within the simulation volume scales as @xmath100 \\ .\\ ] ] the pp force calculation takes a lot of time when particles are highly clustered because of the quadratic dependence on numbers .",
    "@xmath101 p@xmath0 m simulation , in bytes .",
    "+ particle array & @xmath102 & 1,441,792 + particle linked list & @xmath103 & 262,144 + chaining mesh & @xmath104 & 5,324 + green s function & @xmath105 & 69,632 + density and force meshes & @xmath106 & 278,528 + total & @xmath107 & 2,057,420 +    the total memory requirement for the serial code consists of several significant parts listed in table [ tb_sermem ] , where the variables @xmath108 and  @xmath56 are defined in table [ tb_cvars ] of appendix [ sec_overview ] . using the serial n - body code with an average of @xmath94 particles per pm gridpoint ,",
    "the total memory requirement for a p@xmath0 m code is @xmath109 the maximum amount of memory available for dynamic allocation for a 32-bit machine in unix is 2 gb . in practice",
    "the amount of memory available for our application is about 30% smaller . for a simulation having one particle per density mesh cell with a cubic grid , @xmath110 bytes .",
    "the maximum problem size for such a simulation with the upper limit on total memory of @xmath111 gb is @xmath112 .",
    "this severe limitation on problem size is avoided using the parallel code described in the rest of this paper .",
    "in order to perform simulations with more than @xmath113 particles and gridpoints , we distribute the computation to multiple processors of a parallel computer . we are using the single program , multiple data ( spmd ) model in which one program runs on multiple processors which perform computations on different subsets of the data .",
    "the first decision to be made is how to distribute the data and computation .",
    "the computational volume is divided into parts called domains and the memory and computation associated with each domain is assigned to a different parallel process .",
    "the problem of domain decomposition is to decide how to partition the computational volume into domains .",
    "as we will see , there are a number of considerations that enter this decision .",
    "this section first describes the simplest method , one - dimensional static domain decomposition , which is well suited for spatially homogeneous problems but not for strongly clustered n - body simulations .",
    "we then introduce the hilbert curve method of dynamic domain decomposition used in our parallel code .",
    "we use the word _ process _ to refer to one of the instances of our parallel program being applied to the data in its domain .",
    "a process may correspond to one cpu ( or one virtual cpu , in the case of hyperthreading ) or there may be multiple processes on one cpu .      in a static slab domain decomposition",
    ", the volume is divided by fixed planes with equal spacing .",
    "this it the method used , for example , in the fftw fast fourier transform @xcite .",
    "it is well suited for problems in which the computation is uniformly distributed over volume .",
    "a variation on this method is to use a two - dimensional lattice of columns instead of a one - dimensional lattice of slabs .",
    "several groups have implemented static domain decomposition in parallel n - body codes based on pm or p@xmath0 m ( see  [ sec_intro ] ) . as a first step",
    ", we developed our own implementation  of the static slab domain decomposition particle - mesh n - body code .        a static slab or any other static particle domain decomposition is a good strategy when the number density distribution of particles across the simulation box is nearly uniform and each slab contains approximately the same number of particles to process each timestep",
    "however , gravitational instability destroys the spatial uniformity leading to serious inefficiency .",
    "as particle clusters grow , the memory and computational resources of the processes containing the largest clusters ( e.g. processes 1 , 2 , 3 , 8 , and 9 in figure [ fg_virgo_sl ] ) grow quickly .",
    "other processes finish their work and have to wait idle .",
    "worse , the heavily loaded processes may run out of memory causing paging to disk .",
    "the inevitable result is that the computation becomes unbalanced and the code grinds to a halt ( see the timing results in  [ sec_test ] for a @xmath114 test run ) .",
    "the same problem will arise in any gravitational n - body code that uses static domain decomposition .",
    "such a situation , when the performance of the cluster degrades as a result of hugely varying workloads , is called work  _ load imbalance_. in the remainder of this section we introduce an alternative method of dynamic domain decomposition that solves the load imbalance problem for strongly clustered systems .",
    "as we have seen from the slab domain decomposition example in the previous section , it is important for an n - body code to load balance .",
    "we solve the load balancing problem by the implementation of dynamic particle domain decomposition defined by a hilbert space - filling curve as suggested by @xcite .",
    "domain decomposition methods based on morton ordering ( a different space - filling curve ) have been used by @xcite and @xcite .    , but now superimposed by a hilbert curve .",
    "the hilbert curve is divided into 12 colored segments separated by cross - hatched bars and labelled by the circled numbers . during the run",
    "the partitions will move along the hilbert curve so that each process will have approximately the same amount of work to do . in a real simulation",
    "the hilbert curve is divided into much finer segments . ]",
    "the hilbert curve ( hc ) is a fractal invented by the german mathematician in 1891 and is one of the possible space - filling curves that completely fill a cubic rectangular volume .",
    "a unique hc is defined for any positive integer @xmath115 ( the _ hc order _ ) , and dimensionality @xmath116 , for which the hc will fill each cell of a @xmath116-dimensional cube of length @xmath117 . for @xmath118 , examples are given in figures [ fg_virgo ] ( with @xmath119 ) and [ fg_hc8 ] ( with @xmath120 ) .",
    "the hc provides a bijective ( one - to - one ) mapping between the index @xmath121 along the curve ( the _ hc index _ ) and the cell within the volume . in our code",
    "the mapping was provided by the hilbert curve implementation of @xcite ( see appendix [ sec_moore ] ) .",
    "the real simulation volume and the space filling curve we use are in fact three - dimensional but the two - dimensional case is used in the figures throughout the paper in order to simplify the presentation .",
    "the main idea of hilbert curve domain decomposition is to take a three - dimensional volume with inhomogeneous workload and to convert it into a one - dimensional curve that is easily partitioned into approximately equal workloads .",
    "the key advantage compared with slab decomposition is that the hilbert curve method breaks up the problem into @xmath122 chunks of work with @xmath87 instead of @xmath123 . with much finer granularity",
    "it is possible to load balance extremely inhomogeneous problems .",
    "in addition , the hilbert curve minimizes communication between processes , as we show below .",
    "the hilbert curve has the following properties:@xmath124  _ compactness _ : it tends to fill the space very compactly .",
    "a  set of cells defined by a continuous section of a hc tends to be quasi - spherical , having small surface to volume ratio .",
    "one can approximate the surface to volume ratio of any continuous segment of @xmath125 cells along the three - dimensional hilbert curve with @xmath126 which decreases with the increasing @xmath125 .",
    "this approximation is crude at small @xmath125 . the maximum possible ratio @xmath127 is reached for @xmath128 , since one volume cell is surrounded by 26 adjacent surface cells .",
    "@xmath129  _ locality _ : the successive cells along the curve are mapped next to each other within the mesh ; @xmath130  _ self - similarity _ :  the curve is self - similar on different scales .",
    "it can therefore be extended to arbitrarily large size .",
    "figure [ fg_virgo ] demonstrates the bijective mapping of @xmath131 cells in a two - dimensional computational volume onto the indexed hilbert space - filling curve .",
    "the curve visits each cell of the simulation volume exactly once . by connecting the two ends of the curve ,",
    "the curve has the topology of a circle . by introducing @xmath132 partitions along the circle ( the _ partitioning state _ ) each being ascribed to one of the @xmath132 processes in the parallel code , we specify the particle domain decomposition of the whole simulation volume into @xmath132 _ local region_s , each consisting of the cells along the curve between two adjacent partitions and being assigned to one of the @xmath132 processes .",
    "let us denote the local region of process @xmath15 defined by the partitioning state and the hilbert curve by @xmath133 .",
    "as we see , the space - filling curve provides an easy way of bookkeeping for decomposition , since the local domains of each process are completely specified by the hilbert curve setup and the @xmath132 numbers that specify the partitioning state .",
    "the surface to volume ratio of local domains defined by the continuous segments of the hilbert curve is small due to the compactness property of the hilbert curve .",
    "this is the primary reason for choosing a hilbert curve as the space filling curve for our domain decomposition .",
    "the small surface to volume ratio significantly speeds up the reassignment of particles crossing the @xmath134  boundaries (   [ sec_adv ] ) and the pp - force computation (   [ sec_hcpp ] ) . in the @xmath135 run presented in  [ sec_long ] ,",
    "the surface to volume ratio was on average @xmath136 for the domains of voids .",
    "in the hydra code @xcite , using a static @xmath137 two - dimensional cyclic domain decomposition , the surface to volume ratio is @xmath138 , leading to more than five times as much communication cost for the particle advancement and the pp - calculation in comparison to our algorithm .      at the beginning of a simulation",
    "we set up the hilbert curve completely using the functions of @xcite with an appropriate choice of the hc mesh parameters .",
    "only one parameter , the hilbert curve order @xmath139 , is needed to completely specify the geometry of a hilbert curve filling an entire  @xmath116-dimensional cube of volume @xmath140 , which we will call  _ the complete hc - mesh_. adding more parameters  the  _ hc mesh cell spacings _",
    "@xmath141 , the curve starting point in the simulation volume , and the curve orientation  completely determines the hilbert curve within the simulation volume .    while the real n - body simulation volume and the space filling curve are three dimensional , two dimensional examples are used in figures throughout this paper solely to simplify the presentation .",
    "we use the hilbert curve in our code only to specify the domain decomposition for particle storage and computation .",
    "the domain decomposition does not affect any physical values computed .",
    "the choice of the hilbert curve order @xmath139 in our code is made based solely on the parallel code performance considerations . from the point of view of improving the resolution for particle domain decomposition , higher @xmath139 is preferred . on the other hand",
    "each local region cell costs additional memory , favoring lower @xmath139 . for a p@xmath0 m simulation , in order to simplify the force calculation , we choose the hc mesh cells to coincide with the pp chaining mesh cells : @xmath142     fills a @xmath143 simulation box ( _ dashed line _ ) . by connecting the ends of the hilbert curve , the resulting curve has a circular topology .",
    "the number of processes is @xmath144 , so there are 3 partitions along the circle indicated by the cross - hatched bars .",
    "we have @xmath145 , @xmath146 and @xmath147 , @xmath148 . ]",
    "while the complete hc - mesh is a cube of length @xmath117 cells , the chaining mesh length does not have to be a power of two .",
    "therefore we choose the hc order @xmath115 to be the smallest integer satisfying @xmath149 from equations ( [ eq_ppsimple ] ) and ( [ eq_order ] ) , the complete chaining mesh is just a subset of the complete hc mesh .",
    "if @xmath150 for all  @xmath151 , as in figure [ fg_virgo ] , the curve completely fits the simulation volume and the two coincide .",
    "if @xmath152 for some @xmath15 , the complete hilbert curve mesh covers an extra space outside the chaining mesh of the simulation volume as in figure [ fg_hc8 ] , containing the chaining mesh as a subset .",
    "we will refer to this submesh as the _ simulation volume hc mesh _ or simply as the _ hilbert curve mesh _ where the context is clear .",
    "since the cells of the hc outside the simulation volume are irrelevant , they do not take memory and their hc indices are irrelevant too .",
    "let us introduce a _ raw hc index _ along the curve . for a hc mesh cell @xmath153 which belongs to the simulation volume",
    ", we define the _ raw hc index _ , @xmath154 , as the number of hc cells that the curve spent within the simulation volume since its starting point ( hc index @xmath155 ) .",
    "in other words , while the hc index is incremented each cell along the curve , the hc raw index is incremented only at the cells along the curve that belong to the simulation volume .",
    "the mapping between the hc index @xmath156 and the hc raw index @xmath157 is specified completely by the _ table of hc entries_. each entry contains the hc index of an entry point @xmath158 of the curve into the simulation volume and the number of consecutive hc cells that the hc spends within the simulation volume @xmath159 before the next exit .",
    "let @xmath160 be the number of entries in the hc table , and let @xmath161 if the hc mesh fits the simulation box exactly [ @xmath162 for each @xmath151 ] . because the hilbert curve visits all the cells in the simulation box , we have @xmath164 we denote the mapping of a cell @xmath153 in the simulation box into its hc raw index @xmath157 by @xmath165 .",
    "@xmath166    figure [ fg_hc8 ] gives an example of @xmath167 simulation volume mapped by an @xmath168   hilbert curve ( @xmath169 ) in two dimensions ( @xmath118 ) .",
    "table [ tb_hcraw ] lists all the cells of the complete hc mesh @xmath170 along with the raw index of those of them that belong to simulation volume hc mesh .",
    "the hc table of @xmath171 entries is @xmath172 the simulation volume contains @xmath173 cells , in agreement with equations ( [ domvh ] ) and ( [ expldohib ] ) .    the space locality of the hc as a curve filling the simulation volume is lost if @xmath174 .",
    "once the curve exits the simulation volume , the next entry back into the simulation volume may be far away ( see fig .",
    "[ fg_hc8 ] ) .",
    "the resulting @xmath134 may therefore consist of several disjoint parts , each having a surface to volume ratio given by equation ( [ eq_sv ] ) . since the surface to volume ratio of a segment of hc decreases with increasing number of cells in the segment , taken together those subsegments have bigger surface to volume ratio than one big segment of the hc of same volume",
    ". a smaller value of surface - to - volume ratio reduces the communication cost of pp - force calculation by approximately the same factor ( see   [ sec_hcpp ] ) .      to completely specify local regions @xmath133 of each process",
    "@xmath175 , we introduce @xmath132 partitions along the curve .",
    "a bottom partition of the process @xmath15 is set by the raw hc index @xmath176 ( also denoted @xmath177 ) of the cell directly above the partition along the hc . in figure",
    "[ fg_hc8 ] , for example , the entire domain is divided between three worker processes by the three partitions with indices @xmath178 .    in general , a _ partitioning state _ and therefore all local regions @xmath133 , @xmath179 are completely specified by a set of @xmath132 numbers @xmath180 , @xmath181 , where @xmath182 is the spacing between the partitions @xmath15 and @xmath183 .",
    "this implies @xmath184\\mod ( \\donhc)\\ .\\ ] ] we will denote a partitioning state symbolically by @xmath185 . for the example in figure ( [ fg_hc8 ] ) , we have @xmath147 and @xmath186 .",
    "one should always keep in mind the circular topology of the domain decomposition data structures .",
    "the set of hilbert curve indices is a circle with length @xmath187 .",
    "the set of the hilbert curve raw indices is a circle with length @xmath188 .",
    "the set of partitions is again a circle of length @xmath132 .",
    "having introduced the hilbert curve , we next consider how to use it , that is , how to choose the partitioning state each timestep .",
    "we wish to do this so as to balance the workloads of all processes so as to maximize the parallel efficiency .",
    "this section discusses details of our dynamic domain decomposition algorithm .",
    "_ repartitioning _ is the run - time ( dynamic ) change of particle domain decomposition in order to solve the load balancing problem .",
    "repartitioning is performed by shifting the hc raw indices @xmath177 ( i.e.  the cross - hatched bars on fig .",
    "[ fg_hc8 ] ) to minimize the load imbalance by minimizing the resulting expected maximum work load per process .    in a discrete time evolution problem like ours ,",
    "the simulation is synchronized among the processes each timestep , meaning that the amount of time spent by a cluster of computers on a given timestep is given by the maximum amount of wall clock time spent by any process in the cluster doing its share of the problem .",
    "we define the _ workload _ of a process as the wall clock time that it takes for the process to complete one timestep , including the communication waiting time .",
    "the amount of wall clock time spent by a process depends on the structure of the workload assignment .",
    "wall clock time is the number of elementary operations ( clock cycles ) a processor performs for a given parallel process divided by the cpu frequency . during some of those cycles",
    "the processor may be idle or working on other tasks ; we call those computationally useless periods waiting time and distinguish them from cpu time . because the different parallel processes must be synchronized ( at several points ) each timestep , the workload of each process is given by the wall clock time , and may be decomposed as follows : @xmath189 wall clock time is measured using the system call ntp_gettime ( ) .    ideally , we would like to eliminate the waiting time so that at all times all cpus are doing useful work .",
    "the waiting time has a very complex and non - local structure as it depends on communication and other factors unrelated to the computations done by one process .",
    "( for example , on multiprocessor nodes , different processes compete for memory access . ) in our treatment , we balance only the cpu time of different processes . because the wall clock times of all processes are forced to be the same by synchronization , if the cpu time is balanced then there will be no waiting time aside from the minimal amount required for communication and memory access .",
    "the cpu time of a process may be divided into two parts : one that can be attributed entirely to the content of individual hc cells ( e.g. particle data ) and all the rest ( e.g. fft ) .",
    "the dominant hc cell - specific and cpu - intensive portions of the p@xmath0 m code are the pm - density and force interpolation and the pp - force pair summation .",
    "they execute at 100% cpu usage ( as they involve no interprocess communication ) .",
    "all the contributions are summed to define the p@xmath0 m _ instantaneous cpu workload _ at timestep @xmath190 for an hc cell at timestep @xmath125 as @xmath191      \\tilde{w}^{\\rm pp}(\\nstep)&=&\\mbox{pp pair summation wall clock time}\\\\[4pt ]      \\tilde{w}^{\\rm p3m}(\\nstep)&=&\\tilde{w}^{\\rm pm}(\\nstep)+\\tilde{w}^{\\rm pp}(\\nstep)\\ .\\\\",
    "\\end{array}\\ ] ] we use wall clock time to measure the cpu workload for these portions of the computation because there is ( ideally ) no waiting time .    given a set of local _ cell workloads _",
    "@xmath192 ( which may differ from @xmath193 ) for all the cells @xmath194 local to each process , we define the _ cpu workload of process @xmath15 _ as @xmath195 ( note that we use lower case @xmath192 for the workload of a single hc cell and upper case @xmath196 for the total workload of all hc cells assigned to one process . )",
    "we use a subscript hc because the total cpu time of p@xmath0 m is dominated by the hc cell - specific pm and pp computations and only these portions of the code need be included in the workload .",
    "the other significant cost , the fft , is automatically load - balanced by fftw .",
    "note that @xmath197 depends on the local domains and other factors hence it may be varied by repartitioning as discussed below .",
    "the _ load imbalance _ is defined as a function of the set of all cpu workload @xmath198 on each process as @xmath199 giving the fraction of time that any processes are waiting instead of computing .",
    "the quantity @xmath200 is the average of @xmath198 over processes @xmath15 . in practice",
    ", we use @xmath201 for the workload @xmath198 .    the cell workload defined by equation ( [ workload_p3 m ] ) ideally should be proportional to the number of floating point operations needed to compute the relevant parts of the force calculation",
    ". however , the measured cell workload ( wall clock time ) is affected by other factors . for example , there are frequent , unpredictable runtime changes in the efficiency of cpu cache memory management .",
    "( most cpus have a speed much greater than the memory bandwidth . ) in addition , there may be multiple processes running on one ( single- or multi - processor ) computing node and their competition for system resources affects wall clock time .",
    "in addition , if some cpus in the cluster are slower than others , the workload measurement for the same cell will be higher when measured by the slower processes .",
    "the result of these complications can be large fluctuations in the cell workload measurements that are not repeatable from one timestep to another and therefore interfere with our attempts to load balance .",
    "we represent these complications by noting that the instantaneous cell workload defined by equation ( [ workload_p3 m ] ) depends on several factors : @xmath202    to reduce our sensitivity to unpredictable cpu fluctuations , we introduce _ effective cell workloads _ as @xmath203 where @xmath204 is a constant parameter and @xmath190 is the timestep . the effective cell workload is a time average with clipping to eliminate large fluctuations .",
    "it is slightly more accurate than the instantaneous workload for predicting the workload of the next timestep .",
    "a series of tests with large simulations showed that the optimal value parameter is @xmath205 .",
    "the _ instantaneous _ and _ effective load imbalance _ are defined by equation ( [ eq_imbaltar ] ) using equations ( [ workload_p3 m ] ) and ( [ wk_cellrob ] ) respectively for the cell workloads .",
    "the instantaneous load imbalance represents the fraction of time that the parallel processes spend idle , while the effective load imbalance is an estimate of the same fraction in the absence of cpu fluctuations .",
    "each timestep @xmath125 , we compute the values of instantaneous @xmath206 and effective @xmath207 load imbalance .",
    "we perform repartitioning each time when the value of the effective load imbalance exceeds the maximum tolerance value .",
    "the _ target partitioning state _",
    "@xmath208 ( see   [ sec_partit ] ) should be chosen so as to minimize the expected value of the instantaneous load imbalance during the force evaluation next timestep . aside from the target partitioning state , that value also depends on the unknown cell workloads at the next timestep . to find the optimal partitioning state ,",
    "one may estimate the cell workload in the next timestep very well using its latest measured value @xmath209    as illustrated by equation ( [ wk_cellarg ] ) , the cell workload during the next timestep is a function of the unknown particle positions at the next timestep .",
    "however , since particles do not move far in one timestep compared to the size of a hc cell , we can ignore this dependence for now .",
    "the other two arguments factors determining the cell workload are due mainly to the effectiveness of cpu cache memory management , which depends on the memory layout and is hard to predict .",
    "the main change in the memory layout during the next timestep is a different partitioning state which means different local regions . by introducing the technique described in ",
    "[ sec_adv ] , we eliminate the dependence of the second argument in equation ( [ wk_cellarg ] ) on local region assignment .",
    "the third argument of equation ( [ wk_cellarg ] ) can not be eliminated and is the main cause of inaccuracy of equation ( [ eq_wknext ] ) , as demonstrated in   [ sec_long ] and [ sec_sclb ] using test simulations .",
    "the _ residual load imbalance _ is defined as the minimum possible load imbalance , computed with equations ( [ wk_pc ] ) and ( [ eq_imbaltar ] ) allowing for arbitrary repartitioning , based on the effective cell workloads of the current timestep : @xmath210 we seek to find the partitioning state that minimizes @xmath211 , called the _ target partitioning state_. with this choice of partitioning , @xmath212 will become an estimate for the effective load imbalance of the next timestep .    even in the absence of cpu fluctuations",
    ", the residual load imbalance can not be reduced to zero because of the granularity of the workload distribution across hc cells . for an extremely clustered matter",
    "distribution , the workload @xmath213 of the densest hc cell within the simulation volume may be greater than the average workload of all processes , @xmath214 .",
    "( this requires extreme inhomogeneity because most processes have thousands or even millions of hc cells associated with them , while the slowest to finish may have only one hc cell . )",
    "the granularity of the hc method requires that each process have at least one hc cell . in this case , the residual load imbalance is bounded by @xmath215 in this regime there is no point in extending the problem to a larger number of processes , since the wall clock time will be given by that of the process holding the cell @xmath213 (   [ sec_sclb ] ) .",
    "in general , the n - body problem is scalable only up to a number of processes given by @xmath216 improved load balance can be achieved by further subdividing the computation of short - range forces using an adaptive mesh refinement technique , as we will demonstrate in a later paper .      as discussed in  [ sec_partit ] the local regions at any given time",
    "are completely specified by the current partitioning state @xmath217 .",
    "the target partitioning state is given by a primed set @xmath218 .",
    "the target partitioning state can be reached from the initial one by a sequence of sets of @xmath132 non - overlapping _ elementary partition shifts _",
    "@xmath219 along the circle indexed with the hc raw indices , so that @xmath220 it is efficient to perform each set of the elementary partition shifts in two stages : first by moving simultaneously all the even partitions followed by the movement of all the odd ones . this way , during each of the two stages",
    ", the entire process group will decouple into pairs of adjacent processes each involved with an elementary partition shift exchanging particles with the other process in the pair .",
    "given the initial and target partitioning states , each partition can be moved from its starting to its target state in one of two possible directions along the circle .",
    "we define a parametric isomorphic linear mapping @xmath221 that takes the initial partitioning state @xmath217 into the target one @xmath208 as the parameter @xmath222 goes from zero to one : @xmath223\\ , \\\\[5pt ]      r_n^i(\\alpha)\\equiv\\dorn^i + \\alpha \\left[\\ ; \\dorn^{\\prime\\,i}-\\dorn^i        \\;\\right]\\ , \\\\[5pt ]    \\end{array}\\ ] ] where @xmath224 is the total number of hc cells and the partition @xmath225 is treated so as to ensure a circular topology .",
    "it follows that @xmath226 the initial and target partition state starting indices are given by @xmath227 and @xmath228 , respectively .",
    "the direction of movement of the individual partitions along the circle in our code is given by differentiating equation ( [ eq_rbalpha ] ) with respect to @xmath222 .",
    "the target partitioning state is reached from the initial one by the sequence of maximal non - overlapping elementary partition shifts in the directions specified by the above procedure until the target partitioning state is achieved .",
    "all of the partition - dependent data are adjusted to reflect the change of partitioning state .",
    "the corresponding particle sends and receives are performed and the relevant cell data are exchanged .",
    "in addition , the irregular particle domains are reallocated for each process participating in any of the resulting elementary partition shifts .    in order to avoid paging one",
    "needs to impose a total memory constraint for repartitioning . since the memory associated with particles dominates the problem , while doing repartitioning we check whether the reallocation of the particle array on the receiving processes succeeds . if it does not , we divide the requested number of cells @xmath229 by two and try the repartitioning again .",
    "this procedure guarantees that we satisfy the memory limit on each process .",
    "another practical consideration arises when using a cluster with multi - processor or multi - process nodes . as a result of hilbert curve domain decomposition",
    "the memory loads and cache usage of sequential processes are correlated .",
    "these correlations can make it more difficult to achieve load balance .",
    "one should therefore avoid assigning sequential processes to the same computational node .      in this section ,",
    "we show how to find the target partitioning state @xmath208 that minimizes load imbalance ( eq.[eq_imbaltar ] ) , given the current hc cell workloads and the current partitioning state @xmath217 . as discussed in  [ sec_repartit ] , we assume that the current cell workloads are an adequate predictor of those at the next timestep , equation ( [ eq_wknext ] ) .",
    "the optimal target partitioning state depends on the workloads of every hc cell on every process , @xmath230 for @xmath231 .",
    "this information can be represented as a one - dimensional _ continuous total workload bar _ of length @xmath232 equalling the total work summed over all cells . for each hc cell",
    "we mark the bar with vertical dashes at positions @xmath233 which gives the cumulative workload of cells up to the one with raw index @xmath234 .",
    "figure [ fg_partit ] illustrates this with continuous total workload bars @xmath235 and @xmath236 .",
    "the horizontal spacings between the adjacent dashes ( the white stripes ) represent the cell workloads of each cell : @xmath237 .",
    "each white stripe is due to the cell workload associated with one cell .",
    "a single dash however may be an overlap of thousands of very close dashes showing up as one due to the limited resolution of the figure .",
    "in a large n - body simulation , the total number of hc cells is huge .",
    "for example , in the simulation described in",
    " [ sec_lcdm ] , @xmath238 , which requires  @xmath239 to hold the values of the workloads .",
    "this memory requirement grows with the volume of the simulation box and if the mesh is large enough the problem of finding the optimal partitioning state is impossible to process serially ( i.e. on one of the cluster nodes ) .     and @xmath236 ) and discrete ( @xmath240 , @xmath241 , and @xmath242 ) workload bars , as described in the text .",
    "this example is for a simulation on @xmath243 processes , with hc - mesh of size @xmath244 and @xmath245 . @xmath235 and @xmath236 are the continuous total workload bars before and after repartitioning , respectively .",
    "the filled triangles give the locations of the initial ( @xmath235 ) and target ( @xmath236 ) partitions .",
    "a bin @xmath246 along the bar @xmath240 is filled if and only if the number of dashes in the same interval of bar @xmath235 is non - zero .",
    "the discrete partitioning states are marked by the filled rectangles above the filled bins of bars @xmath240@xmath242 .",
    "the solution in the discrete space marked on bar @xmath242 is obtained by first repartitioning @xmath247 [ holding @xmath248 fixed ] and then shifting @xmath249 .",
    "finally , the continuous target partitioning state @xmath250 marked on bar @xmath236 follows from @xmath242 .",
    "note that the topology of each bar is a circle formed by connecting its ends . ]    to solve this problem we compress the cell workload data by discretizing it .",
    "the total workload bar is divided into @xmath251 segments per process , or @xmath252 segments in total .",
    "the continuous total workload array @xmath253 is replaced the much smaller array @xmath246 with @xmath254 .",
    "figure [ fg_partit ] illustrates this with the bars @xmath240 , @xmath241 , and @xmath242 .",
    "each array member @xmath246 is assigned to the subinterval @xmath255 of the total workload bar , where  @xmath256 .",
    "the value @xmath246 is defined as the number of cell boundaries ( the dashes ) within the corresponding subinterval of the total workload bar .",
    "the non - zero members @xmath257 correspond to the filled rectangles of bars @xmath240@xmath242 in figure  [ fg_partit ] .",
    "suppose we start from the initial partitioning state @xmath185 marked by triangles above @xmath235 in figure [ fg_partit ] .",
    "we define a _ discrete partitioning state _",
    "@xmath258 in the discrete workload space by @xmath259 $ ] , @xmath260 , where the square brackets signify taking the integer part ; @xmath261 is the spacing between the consecutive @xmath262 along the binned bar of length @xmath263 .",
    "we define the workloads in the discretized problem as @xmath264 . following equation ( [ eq_imbaltar ] ) , the load imbalance of a discrete partitioning state is defined by @xmath265 = 1-\\frac{\\langle \\hat{r}_n \\rangle }      { \\max \\hat{r}_n}\\ .\\ ] ] the residual load imbalance is redefined in the discrete space as [ cf .",
    "( [ eq_resimbal ] ) ] @xmath266 = \\min\\limits_{\\{\\hat{r}'_b ,    \\hat{r}'_n\\}:\\;b(\\hat{r}'_b ) > 0 } \\mathcal{\\hat{l}}[\\hat{r}'_n ] \\    .\\ ] ] the problem of load balancing is posed in the discrete space as finding the discrete target partitioning state @xmath267 that will minimize the load imbalance . we discuss how this is done in the next subsection .",
    "once the discrete target partitioning state @xmath267 is found , the continuous target partitioning state @xmath208 is also found by setting @xmath268 , where @xmath269 is the raw hc index of any cell such that @xmath270 $ ] .",
    "there are , in general , many hc indices that will accomplish this . for example , in figure [ fg_partit ] , the final triangles for bar @xmath236 may be placed at any dash lying beneath the rectangles above bar @xmath242 .",
    "the choice is arbitrary and this freedom in setting the target partitioning state will result in negligible differences in the residual load imbalance @xmath271 . in practice , we set the partition at the first hc cell that lies in the desired interval .",
    "there are two practical approaches to solving the discrete target partitioning state problem of equation ( [ eq_probdisimbal ] ) .    in the _ cumulative repartitioning _",
    "approach we keep the zeroth partition fixed while setting the other ones as close as possible to being equally spaced along the discrete workload bar , subject to the constraints @xmath272 .",
    "it is evident that the resulting target partitioning state is a function of only the initial position of the zeroth partition @xmath273 and the discrete workload array @xmath274 .",
    "the cumulative approach alone is not satisfactory for optimizing the discrete load imbalance equation ( [ eq_disimbal ] ) when the cell workloads of some of the hc cells far exceed the discretization load @xmath275 .",
    "indeed this problem is illustrated in figure [ fg_partit ] .",
    "the initial discrete partitioning state is given by @xmath276 as shown by the rectangles above the workload bar @xmath240 .",
    "applying the cumulative approach using the above rule , we have @xmath277 , and @xmath278 , yielding load imbalance @xmath279 , which is relatively poor .",
    "( the superscript cml is used for partitions found with cumulative repartitioning . )",
    "this approach uses only the position of the zeroth partition and the discrete cumulative workload array .",
    "it is insensitive to differences in the adjacent workloads , e.g. @xmath261 and @xmath280 .    in the _ circular cyclic correction repartitioning _ approach ( denoted by superscript cc ) ,",
    "we start from a partition @xmath15 and shift it to the bin @xmath281 such that it is the closest possible distance to the bin in the middle of the two adjacent partitions , @xmath282/2 $ ] . after the correction of the partition @xmath15",
    "is done , we move on to the next partition @xmath183 , applying the same technique but using the already corrected value for the position of partition @xmath15 .",
    "we then continue applying the same scheme for all the other partitions in cycles along the circle @xmath283 until the resulting shifts for all partitions @xmath283 become zero .",
    "the resulting positions of the partitions will define the target state in the circular cyclic correction repartitioning approach .",
    "this approach if used alone is not satisfactory just as for the cumulative partitioning approach above , however the nature of the problem is completely different .",
    "if a large variation in workload @xmath261 develops across a large range of indices @xmath15 ( e.g. between @xmath15 and @xmath284 ) , this variation will not be suppressed by the circular cyclic correction scheme since only the adjacent partitions @xmath285 and @xmath286 are used for correction of any given partition @xmath262 . on the other hand ,",
    "all the local fluctuations in workload will be suppressed very effectively .    as we see , the cumulative repartitioning approach and the cyclic circular partitioning approaches smooth the large scale and small scale ( in terms of the range of indices ) workload fluctuations respectively .",
    "applying the two approaches in sequence works well to provide a nearly optimal solution for the discrete workload . in the example of figure [ fg_partit ]",
    ", the bar @xmath242 shows the result of applying the circular partition correction approach to the output of the cumulative approach ( bar @xmath241 ) obtained from the initial discrete partitioning state ( bar @xmath240 ) . as follows from the bar @xmath242 of figure [ fg_partit ] ,",
    "the resulting target partitioning state is @xmath287 and @xmath288 .",
    "the resulting discrete load imbalance is @xmath289 is 3.4 times smaller than the load imbalance obtained using only the cumulative method .",
    "our experiments show that the combination of the two approaches results in a good approximation to the load - balanced target partitioning state . the residual load imbalance is generally limited not by our ability to find the optimal solution but instead by the cpu time fluctuations due to variations in cache usage .",
    "in a serial code , the array of particle structures ( [ eq_part ] ) is static , that is , it remains fixed length with unchanging particle labels . in a parallel code with domain decomposition",
    ", particles may move from one process to another .",
    "this not only requires interprocessor communication , it also complicates the storage of particle data .",
    "this section discusses our solutions to these problems .",
    "the particle data are stored as a single local particle array of pointer @xmath290 on each process .",
    "a slightly larger range @xmath291 is allocated to avoid reallocation every timestep .",
    "in addition to the particle array , we have a linked list that tells which particles lie in each hc cell . for each hc cell",
    "there is a pointer ( the root ) that ( if it is non - null ) points into the particle array to the first particle in that hc cell . a complete list of particles within a given local hc region @xmath133 is obtained by dereferencing the appropriate linked list root and then following the linked list from one particle to the next , as illustrated in figure [ fg_linked ] .",
    "the linked list also has a root  that points to disabled particles .",
    "there are several challenges associated with this simple linked list method of particle access .",
    "first , one must transfer particles between processes .",
    "second , hc cells are themselves exchanged between processes as a result of repartitioning .",
    "third , one must optimize the traversal of the linked lists to optimize code performance .",
    "finally , one must specify which hc cells are associated with a given process .",
    "we discuss these issues in the remainder of this section .     of fig .",
    "[ fg_hc8 ] .",
    "the hc cells associated with this process are @xmath292 .",
    "the particle arrays are the horizontal bars ( with disabled particles corresponding to gaps in the array opened up when particles moved to other processes ) .",
    "the linked list is given by the arrows going from one particle to another ; the solid ( dashed ) arrows give the linked list for the active ( disabled ) particles .",
    "the linked list roots are the pointers  ( for disabled particles ) and @xmath292 ( for active particles ) beneath the particle array bars .",
    "each linked list begins at a root and ends with the null pointer .",
    "the particle array is allocated slightly more storage ( ) than needed ( ) .",
    "a ) the particle array and linked list before sorting .",
    "b ) the same particles and the linked list after sorting . ]    during each position advancement equation ( [ eq_leapfrog ] ) , twice every timestep some particles move across the boundary of their local particle domain . as a result",
    ", such a particle is sent from a process @xmath15 to another process  @xmath96 whose local region @xmath293 it entered .",
    "particles may cross the boundary of any pair of domains .",
    "the associated communication cost scales linearly with the @xmath134  surface area .",
    "the hilbert curve domain decomposition minimizes this cost because of the low surface to volume ratio (   [ sec_hc ] ) .",
    "when a particle @xmath57 moves outside the local region @xmath133 , it leaves a gap in the local particle array .",
    "we set the particle mass to @xmath294 and call this particle array member a disabled particle .",
    "all the disabled particles on each process form a separate linked list with root .",
    "the particles entering @xmath133 from other processes replace the disabled particles or are added to the end of the particle array .",
    "as a particle initially in process @xmath15 crosses a boundary to another process , the i d of the target process @xmath96 should be immediately found in order to send this particle to the new process .",
    "dividing the new particle coordinates by the hc mesh spacing gives the new hilbert curve mesh cell coordinates @xmath295 .",
    "the target process i d can then be found calling moore s function for the new hc index @xmath296 . by using the current hilbert curve partitioning ,",
    "one finds the i d of the target process @xmath96 from @xmath156 .",
    "once all particles to move have been identified , the particles are transferred between processes .    as we show in appendix [ sec_moore ] ,",
    "moore s function calls are relatively expensive . to avoid having this cost each time a particle crosses the boundary , we allocate an extra one layer of hc cells surrounding the boundary of @xmath133 , as shown in figure [ fg_pp ] , and we mark the surrounding cells with the ids of the appropriate processes @xmath96 by calling moore s function for each of them exactly once . by doing this once , we avoid calling moore s functions in the future",
    ". however we still have to call the function for the very small fraction of the boundary - crossing particles that went further than one boundary layer cell in one timestep .",
    "the extra layer of hc cells surrounding the local region is also used with the particle - particle force computation as described in  [ sec_hcpp ] .",
    "we maintain the particle linked list throughout the simulation instead of reforming it each timestep .",
    "as particles cross from one hc cell to another  even if they are in the same local region @xmath133  the linked list is updated to reflect these changes .",
    "the particle array is reallocated whenever the fraction of disabled particles exceeds a few percent ( the exact value is a parameter set by the user ) , or the amount of particles exceeds the boundary of the pre - allocated particle array @xmath297 .    cells containing all the particles assigned to process @xmath15 .",
    "information about the layer of boundary cells ( all gray and white cells outside the local region ) is also stored by process @xmath15 .",
    "this information is used both when particles are transferred between processes and during the short - range ( particle - particle ) force computation . in the latter case ,",
    "the particle data for the shaded cells is used to compute forces on particles in cells a and b as discussed in   [ sec_hcpp ] .",
    "]    in addition to the pointer to the root of the linked list that contains all the particles within each hc cell , each cell of the local region contains other structure members : the process number the cell belongs to , the current and previous timestep cell workloads required by equation ( [ wk_cellrob ] ) , the number of particles in this cell , etc",
    ". we will refer to this structure as the _ hc cell structure _ and the array of structures for all hc cells the _ hc cell array_. one member of this array has size 16 bytes .",
    "when repartitioning occurs , we send and receive the relevant hc cell array members and the particles they contain to the appropriate processes .",
    "some program components , such as particle position advancement , require access to the complete particle list on each process .",
    "all local particles can be accessed using the particle array and filtering out the passive particle array members as follows : @xmath298 we found that because of cache memory efficiencies , it is up to ten times faster to use a simple array to access every local particle than it is to dereference the three - dimensional linked list roots for each of the local cells of @xmath133 .",
    "the reason for such difference is that simple array members are sequential in the machine memory , while the successive linked list members are not , and the cpu cache memory is more effectively used when data are accessed sequentially in an array .",
    "the improvement in efficiency is especially important in the particle - particle calculation because each particle is accessed many times during one force computation .",
    "here we introduce a fast sorting technique that places the particle data belonging to the same hc cell sequentially within the segments of the particle array , ordered by increasing hc - cell raw index .",
    "this sorting procedure is performed each timestep before the force computation .",
    "every timestep , before a force calculation , we follow all the  @xmath134 cells in the order of their raw hc index , and concatenate their linked lists , resulting in just one linked list of all the particles in the local particle array . then , using the unnecessary acceleration g0 and g1 members of the particle structure as pointers , we form an extended linked list replacing the old one . the result is a new linked list which can be traversed both forward ( using g1 ) and backward ( using g0 ) .",
    "then , starting from the first particle of a simple array of particles , we swap it with the first particle in the extended linked list while the forward and backward pointers of the immediately adjacent within the extended linked list particles being updated .",
    "we then proceed to the next particle in the simple array and in the linked list doing the same , until we have sorted the entire particle list .",
    "the result of this sorting is illustrated by figure [ fg_linked]b .",
    "in addition to optimizing the cpu cache memory usage , the above sorting technique eliminates the need to allocate an additional buffer for sending and receiving particles while repartitioning , because all the particles to be moved as the result of repartitioning will occupy contiguous segments in the simple particle array .",
    "when the sorting is completed the original linked list is unnecessary and is deallocated in order to be formed again directly using the sorted particle array , before the particle advancement and repartitioning take place .    to transfer particles between processes we use a modification of mpi_alltoallv that assures",
    "no failure will occur if insufficient memory has been pre - allocated for the send and receive buffers .",
    "this achieved by using mpi_alltoall to exchange the numbers of particles to be sent and received and then using as many mpi_alltoall and mpi_alltoallv calls as necessary to avoid overflowing the available memory of each processor .",
    "as mentioned above , during particle exchange and force computation one needs frequent access to a cell s particle list and other cell data , given the indices @xmath153 of the cell in the hc mesh .",
    "the most obvious method is to call moore s function @xmath299 to get the global hc index and then use our table of hc entries (   [ sec_hci ] ) to convert @xmath156 into the raw index @xmath234 .",
    "the raw index then gives the root to the particle linked list as shown in figure [ fg_linked ] .",
    "this method is unsatisfactory because of the expense of calling moore s function many times during the force evaluation .",
    "( dark gray ) assigned to one process and the rectangular array that includes it ( light gray combined with dark gray ) .",
    "the ragged array ( middle gray combined with dark gray ) requires much less storage but only the ragged array with gaps ( dark gray ) corresponds exactly to @xmath134 .",
    "four cells belonging to @xmath134 are randomly selected and labelled b , c , d and e. ]    another simple method of allocation for the @xmath134  cells would be a _",
    "@xmath116-dimensional rectangular array _ of cells holding the frequently used roots of the linked lists to the particles contained in this cell and the total number of particles within it .",
    "the access to a hc cell given its coordinates @xmath153 in this case is given by dereferencing the array @xmath300[c_1][c_2]$ ] in the case of @xmath87 , where @xmath85 is an array of hc cell raw indices ( or pointers to hc cells ) updated after each repartitioning .",
    "the problem here , illustrated in figure [ fg_lrbad ] , is that many of the entries of @xmath85 are wasted because the hc local regions are not rectangular parallelpipeds .",
    "this can be improved by adjusting the bounds of the array indices @xmath301 to the extremal values for cells in the local region .",
    "the result is a simple _ @xmath116-dimensional ragged array _ , also illustrated in figure [ fg_lrbad ] .",
    "the optimal method of local region hc cell allocation and access is to add one more dimension to the array of hc cell pointers @xmath85 used in a simple ragged array .",
    "the extra dimension accounts for variable number of disjoint parts in the last dimension .",
    "this method allocates the minimal storage needed beyond the number of hc cells in @xmath133 .",
    "we call this a _ d - dimensional ragged array with gaps_. the hc cell is then obtained by dereferencing the @xmath302-dimensional array @xmath85 .",
    "to access a cell with coordinates @xmath303 using a @xmath116-dimensional ragged array with gaps , we use @xmath304[\\,c_1]\\ldots[c_{d-2}][\\mathcal{m}][\\,c_{d-1}]$ ] , where @xmath305 is the integer function equal to the number of the completed contiguous intervals in the @xmath306-ordered set of all the hc cells in the local region having coordinates @xmath307 and having @xmath308-th coordinate less than @xmath306 .",
    "for example , in the case @xmath118 of figure [ fg_lrbad ] , access to the cells b , c , d , and e is given by @xmath309[0][26]$ ] , @xmath310[0][12]$ ] , @xmath311[0][9]$ ] and @xmath312[1][23]$ ] .",
    "the disadvantages of the other methods considered above do not apply now : the @xmath302-array dereference call is exponentially faster than the function call , and the space allocated exactly equals the required number of @xmath134 cells . for @xmath87 , the function evaluation",
    "@xmath313 takes a time that grows only logarithmically with the number of disjoint parts along the last dimension for a give @xmath314 and @xmath315 .",
    "in this section , we present an efficient method for parallel pm and pp computation of forces for particles within the hc local regions . by using the techniques developed in   [ sec_loadbal ] and [ sec_layout ] , we have made our algorithms load balanced and efficient .",
    "the pm force calculation requires communication between two different data structures with completely different distributions across the processes .",
    "the particles on one process are organized into irregularly - shaped hc local regions .",
    "the density and force meshes , on the other hand , have a one - dimensional slab decomposition based on fftw .",
    "the parallel computation is an spmd implementation of the five pm steps presented in  [ sec_pm ] .    , the small circles are the discrete set of pm density gridpoints @xmath316 .",
    "the filled circles are the pm gridpoints within a fftw slab @xmath96 , @xmath317 .",
    "the gray filled region is the hc local region @xmath133 .",
    "the set of all circles within the dashed line is @xmath318 ; the set of filled circles within the dashed line is @xmath319 . extending this",
    "last set slightly gives the continuous set within the solid line , @xmath320 .",
    "( [ eq_lmes ] ) gives the intersection of this last set with the gray region@xmath133 . ]",
    "we define a few concepts that will be needed in order to describe and implement the data exchange between the two different data structures during the parallel pm force calculation .",
    "the various sets used in the calculation are illustrated in figure [ fg_lg ] .",
    "the fftw parallel fast fourier transform implementation @xcite allows one to compute forward and inverse fourier transforms of the complete three dimensional array of @xmath321 mesh points distributed among the processes  @xmath96 in the form of slabs of @xmath322 grid points , where @xmath323 , each slab starting at the position  @xmath324 along the 0-th dimension .",
    "we will call these slabs the _ density _ or _ force mesh slabs _ ( depending on the context ) and denote them by @xmath325 .",
    "the geometry of the slab  @xmath325 is calculated once and for all at the start of the run by calling the fftw fourier transform plan initialization routine .",
    "let us denote the complete discrete set of all density mesh gridpoints needed for a complete fourier transform by @xmath326 , and the complete continuous set of all positions within the whole simulation volume by @xmath327 .",
    "we have @xmath328 here , @xmath15 labels the process holding the hc local region while @xmath96 labels the process holding a given density / force mesh slab .    for a continuous set of positions @xmath329 ,",
    "let us define @xmath330 to be the minimal complete subset of the density grid points @xmath331 such that equation ( [ eq_int ] ) is satisfied for any position vector @xmath332 . by this definition , if all the local particles are contained within @xmath333 , after the density assignment of step 1 of the pm force calculation , the only non - zero pm - density grid points of @xmath326 are in fact within a subset @xmath334 .",
    "for a discrete subset @xmath335 of the density gridpoints , let us define @xmath336 to be the minimal complete continuous set of points @xmath337 such that equation ( [ eq_int ] ) is satisfied for any @xmath338 .",
    "now , if all the grid points local to a process are within a subset @xmath339 of all the particles in the simulation volume @xmath327 , only the particles of the subset @xmath336 may acquire any non - zero force contribution from those gridpoints during _ step  5 _ of the pm - force calculation .      as we discussed in  [ sec_pm ] , step 1 of the pm force calculation involves filling the density grid points in @xmath340 using the particles distributed in the volumes @xmath341 .",
    "steps 24 involve working only with @xmath325 and are straightforward since they do not require any interprocessor communication aside from the parallel fft . during step 5",
    "the information flows in the exactly opposite direction , therefore an algorithm for step 1 applies to step 5 as well with the direction of the information flow reversed .",
    "the problem remaining now is for step 1 of the pm force calculation to decide how to fill the local density grids @xmath325 from the particles distributed within the local regions @xmath133 . to solve this problem we considered a number of approaches described briefly below , but only the last one is implemented in our code and is effective over the entire range of clustering .",
    "[ [ a - sending - particles . ] ] _ a ) sending particles . _ + + + + + + + + + + + + + + + + + + + + + + +    under this method , each pair @xmath342 of processes sends the appropriate portion of the particle data from process @xmath15 to process @xmath96 to fill the density mesh @xmath325 of slab @xmath96 . for each pair of processes",
    "the set of the density gridpoints @xmath343 on process @xmath96 will be updated with the particles brought from the volume @xmath344 within the hc local region of process @xmath15 .",
    "this method is very efficient for the pairs where the particle sender processes @xmath15 have low particle number density , thus reducing the number of particles to be sent and the communication cost .",
    "[ [ b - sending - grid - points . ] ] _ b ) sending grid points . _",
    "+ + + + + + + + + + + + + + + + + + + + + + + + +    under this method , each pair @xmath342 of the processes fills the portion ( [ eq_gmes ] ) of the grid points using the local particles within ( [ eq_lmes ] ) , then sends the filled gridpoints to process @xmath96 .",
    "this method performs poorly when the particle number density is low on the sender process , because most of the density values in the message are zero .",
    "this method is very efficient for the pairs where the particle sender processes @xmath15 have a high particle number density : each gridpoint of the sender process contains the contributions from many particles .",
    "[ [ c - combined - particle - and - grid - point - send . ] ] _ c ) combined particle and grid point send . _",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    method _ a ) _ is effective with low particle number density while method _ b ) _ is effective with high particle number density on the particle sender process .",
    "the idea of the combined particle and grid point send method is to choose for each pair of processes the approach that requires sending the least data .",
    "[ [ d - sending - compressed - grid - points . ] ] _ d ) sending compressed grid points . _ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    this approach optimizes the communication cost in both the extreme cases of low and high number density of the particles on the sender process @xmath15 . the idea behind this method is to use the approach _",
    "b ) _ above and apply _ sparse compression _ to the gridpoint messages ( [ eq_gmes ] ) .",
    "as we know , the grid point approach performs poorly when the particle number density is low on the sender process . using sparse compression as we explain in the following subsection",
    "significantly alleviates this problem by reducing the message size for the underdense regions @xmath133 .",
    "is sent to process @xmath96 using an mpi function call .",
    "a compressed force message is constructed on process @xmath96 using the template given by the density message .",
    "the forces are sent back to process @xmath15 and expanded .",
    "the bracketed values in the bottom array can be ignored because there are no particles nearby the relevant grid points . ]      in a cosmological simulation , the overdense regions have small hc local regions with every grid point having many nearby particles so that the force and density messages are small . on the other hand , low - density regions have large hc local regions with many pm grid points but the density and force messages are made small by the compression method illustrated in figure [ fg_sparse ] .    during step 1 of the pm computation ,",
    "if a number of binary zeros are encountered in the grid message , they all are substituted by a pair of numbers before sending packets : the first number is a delimiter ( an illegal density or force value such as flt_max ) and the second number is an integer giving the number of zeros to follow in the original uncompressed message .",
    "this technique is called _ run - length encoding_. the resulting compression factor is unlimited and depends on how frequent and contiguous the zero values are positioned in the grid message . the receiver process @xmath96 simply uncompresses the message by filling the gridpoints within @xmath345 .    during step 5 , the force values",
    "are sent from process @xmath96 to @xmath15 three times ( once for each of the three dimensions ) .",
    "the force array message is identical in the size to the density message that was sent during step 1 for each pair @xmath342 of processes .",
    "we compressed the density values in step 1 using run - length encoding of zero value densities .",
    "in the force message the technique runs into a difficulty because the gravitational forces are long range forces by nature and their values are nowhere equal to zero .",
    "if we do not compress the force values , there is no advantage in choosing the compressed gridpoint approach , since the force messages would have the same length as the uncompressed density messages .    by using packet information obtained while receiving the density array",
    ", we can compress the forces using exactly the same pattern formed by the packets of the density message , as shown in figure [ fg_sparse ] .",
    "the receiving process will decompress the force and obtain exactly the initial force array excluding the values of force at the array members which were skipped in the density assignment ( the square bracketed force values in fig .",
    "[ fg_sparse ] ) .",
    "this loss of information is however completely irrelevant for interpolation of the force values to the particles in _ step 5 _ because the square bracketed force values in the force array belong to grid points which earlier acquired absolutely no density values from the surrounding particles , which means that for that grid point and for any particle within @xmath133 , the gridpoint has no nearby particles [ the condition ( [ eq_int ] ) is not satisfied ] .",
    "thus the force values at that grid point will not be interpolated to any particles during step 5 .",
    "the idea of sparse array compression is not implemented in the hydra code @xcite .",
    "once implemented it will significantly reduce their communication and memory costs .",
    "equation ( [ eq_gmes ] ) gives the minimal set of density grid points on process @xmath96 needing to be filled with values from particles on process @xmath15 .",
    "this set is impractical to work with because of its irregular shape . for a practical implementation",
    "we embed this region within a rectangular submesh of @xmath326 during steps 1 and 5 of the pm force computation , as follows .    for",
    "a continuous set of positions inside the simulation volume @xmath329 , let us define @xmath346 to be the minimal rectangular subset of density grid points such that @xmath347 . for grid points with @xmath346 but",
    "outside @xmath330 we set the density values to zero .",
    "it follows at once that if we use @xmath348 instead of equation ( [ eq_gmes ] ) for the definition of pm grid point messages , we will have the rectangular mesh @xmath349 for interpolation of density for particles within @xmath133 , and this still give the correct result .",
    "however , since the extent of the local region @xmath133 inside the simulation box is not limited , neither is the extent of @xmath349 . for example , when @xmath133 consists of just two cells with the coordinates @xmath350 and @xmath351 , it is easy to see from the definition that @xmath349 encloses the whole simulation density mesh @xmath326 as a subset and this is too much memory space for allocation on a process .    to avoid this problem",
    ", we dissect the local region @xmath133 uniformly into @xmath352 slices @xmath353 along the 0-th dimension so that the extent of each slice along the 0-th dimension will not exceed @xmath354 . using the previous equation we have , now summed for all the receiving processes  @xmath355 @xmath356 for each slice @xmath357 of the hc local region @xmath133",
    ", the density is interpolated onto the rectangular mesh @xmath358 which is small enough to be allocated since its extent in the 0-th dimension is limited by roughly @xmath354 grid points .",
    "then , the messages under the inner sum of equation ( [ eq_pmmsg ] ) are sent to processes @xmath359 .",
    "the procedure is repeated for each slice @xmath357 .    in the code",
    "presented in this paper we use the blocking mpi routines for pm message communication , which requires synchronization between each pair of processes exchanging the message . in order to reduce waiting time",
    ", mpi allows bi - directional blocking communication using mpi_sendrecv . in the above equation",
    "the process @xmath15 is described as the _ sender _ of the pm - grid messages obtained by interpolation from the particles within @xmath133 to the processes @xmath96 in order to update their fftw - slabs @xmath325 .",
    "note however , that the same process @xmath15 also behaves as a _ receiver _ of the pm grid messages from the other processes @xmath96 in order to update the fftw - slab @xmath360 .",
    "the set of the received messages is obtained by simply swapping the indices @xmath15 and @xmath96 in the above equation .",
    "adding the two together we have for the set of gridpoints participating in the communication on process @xmath15 in both directions @xmath361 =    \\sum_{k=0}^{n_k^{ij}-1 }    \\sum_{j=0}^{\\donpc-1 }    \\left[\\ ; \\gsl^j \\cap \\rfun(m_h^{ik}\\ , ) +",
    "\\gsl^i \\cap \\rfun(m_h^{jk}\\ , )    \\right]\\ , \\ ] ] where @xmath362 and the @xmath357 is defined to be an empty set for @xmath363 .    in order to access particles in a given slice @xmath357 of the local region we use the particle access technique described  [ sec_voids ] .",
    "the sorting technique described in  [ sec_adv ] speeds up the density and force interpolation .",
    "the timing of the interpolation for each hc cell gives the pm part of the hc - cell workloads in equation ( [ workload_p3 m ] ) .",
    "the above procedure is used for both density and force interpolation in the pm force calculation . in the current implementation",
    ", the mpi messages are blocking , which means additional waiting time . in a subsequent paper",
    "we describe the implementation of non - blocking communication resulting in a significant speedup of the pm calculation .      the particle - particle ( pp ) force calculation increments forces acting on each of the particles in a pair if the particles are closer than @xmath90 .",
    "the method of particle access developed in  [ sec_voids ] allows one to access all the particles within a given hc cell . from equation ( [ eq_ppsimple ] ) , hc cells are coincident with the chaining mesh cells needed for the pp force calculation . to see how the communication and computation work , consider the example of figure [ fg_pp ] . to compute the pp force for a particle @xmath57 within chaining mesh cell @xmath85 ,",
    "the particle data in the surrounding cells @xmath364 are required .",
    "the particle data within the cell @xmath365 are locally available .",
    "however one needs to bring the positions and masses from the other processes to get the particle data for the boundary layer cells @xmath366 .",
    "once the particle data from the boundary layer cells are gathered , the pp force calculation may be performed by pair summation , after which the resulting forces for the particles within @xmath366 are sent back to their processes where the pp forces of the original particles are incremented .",
    "the same algorithm applies to any other cell within @xmath134 , for example the cell @xmath86 of figure [ fg_pp ] , for which the particle data for @xmath367 and @xmath368 are available locally while the particle data for cells @xmath369 and @xmath370 must be brought to the local process from the others .",
    "because of its pairwise nature only half the surrounding cells are needed for the pp force calculation for each hc cell . in total , the particle data for the non - local cells shaded in figure [ fg_pp ] are required for the pp force calculation for each particle within @xmath134 .",
    "the amount of communication needed for a complete pp force calculation is proportional to the number of particles in the cells required to be brought from the other processes through the boundary layer cells .",
    "if the pp pair summation step is started synchronously on all processes , it will finish at approximately the same time on all processes if the load imbalance is low . otherwise , the processes that complete the pp force computation first will have to wait for the remaining processes to finish their pair summation . since the pair summation is the most time - consuming step of p@xmath0 m ,",
    "it is crucial that the procedure be load - balanced .",
    "this is accomplished using the methods of  [ sec_loadbal ] .",
    "the cpu time of the pair summation step is used in the cell workload calculation of equation ( [ workload_p3 m ] ) .",
    "the particle access time in the pair summation loop is minimized by pre - sorting the particles as described in  [ sec_adv ] .",
    "in early versions of our code , the memory often exceeded the available resources causing the code to crash . by implementing runtime tracking of memory usage",
    "we were able to identify the problems and optimize the memory requirements .",
    "memory usage was reduced largely in three ways : the irregular shaped local domain memory technique of  [ sec_voids ] , the elimination of particle buffer allocation while repartitioning , and memory balancing when necessary during repartitioning as described in  [ sec_rep ] .",
    ".[tb_parmem ] dominant memory requirements of the parallel  code . here",
    "@xmath371 is the number of particles and @xmath372 is the thickness of the pm slab , both on process @xmath15 .",
    "[ cols= \" < , < , < , < \" , ]     we tested the scalability of  using two problem sizes ( @xmath373 and @xmath374 @xmath375cdm in a 200 mpc box with plummer softening length @xmath376 mpc , evolved to redshift zero , taking 634 and 657 timesteps , respectively ) and a range of numbers of computing nodes as shown in table [ tb_sclb ] .",
    "each computing node has two cpus .",
    "the runs have either two processes per node ( one per cpu , runs 4a , b ) or four processes per node ( two per cpu , using intel hyperthreading ) . for perfect scalability",
    ", the times in the last column would be equal for simulations of the same grid size @xmath48 .    from table [ tb_sclb ]",
    "we may draw several conclusions .",
    "first ,  does not scale perfectly like an embarrassingly parallel application . on the other hand , increasing the number of processes up to 80 leads to a steadily decreasing wall clock time .",
    "comparing runs 3a and 3 g , we see that for up to 48 processes , the wall clock time scales as @xmath377 .",
    "hyperthreading also gives a significant speedup .",
    "comparing runs 3f and 4b , which have the same total number of processes but different numbers of compute nodes , we see that hyperthreading improves the code performance by a factor 1.62 .",
    "we also see that the code scales reasonably well as the problem size is increased .",
    "comparing runs 3f and 5b , the wall clock time is proportional to @xmath378 where @xmath6 is the number of particles . when the wall clock time is dominated by pp pair summation , we expect scaling as @xmath9 .",
    ", left ) and 5 ( @xmath379 , right ) .",
    "the individual runs are labelled .",
    ", title=\"fig : \" ] , left ) and 5 ( @xmath379 , right ) .",
    "the individual runs are labelled .",
    ", title=\"fig : \" ]    the most significant deviations from perfect scalability arise with the largest numbers of processes , in particular runs 3h , 3i , and 5d .",
    "these arise from load imbalance , as shown in figure [ fg_sclb - imbal ] .",
    "a significant increase in load imbalance shows up after timestep 500 in runs 3 and timestep 600 in runs 4 due to the formation of a dense dark matter clump .",
    "when the number of processes is sufficiently large , this leads to one or a few hc cells beginning to take as much time for pp pairwise summation as the average time for the other processes . according to equation ( [ eq_resimbalcon ] ) ,",
    "the result is a growing residual load imbalance .",
    "scalability breaks down beyond a certain number of processes , given by equation ( [ eq_npclim ] ) .",
    "once the performance saturates , the instantaneous and residual load imbalance match because it is no longer possible to improve the load balancing by rearrangement of the partitioning .",
    "although the performance of  is limited by the pp pair summation and not by the pm force computation , it is worth recalling that , because the current code uses blocking sends and receives to pass data between the particle and grid structures , the pm time also scales imperfectly .",
    "when we implement adaptive p@xmath0 m , the pp time will decrease significantly so that the pm time becomes a significant fraction of the total wall clock time . to improve the parallel scaling",
    ", it will be important to implement non - blocking communication for the pm particle / grid messages .",
    "parallelizing a gravitational n - body code involves considerably more work than simply computing different sections of an array on different processors .",
    "the extreme clustering that develops as a result of gravitational instability creates significant challenges .",
    "a successful parallelization strategy requires careful consideration of cpu load balancing , memory management , communication cost , and scalability .",
    "the first decision that must be made in parallelizing any algorithm is how to divide up the problem to run on multiple processes . in the present context",
    "this means choosing a method of domain decomposition . because p@xmath0 m is a hybrid algorithm combining elements of three - dimensional rectangular meshes and one - dimensional particle lists , we chose a hybrid method of domain decomposition .",
    "a regular mesh , distributed among the processes by a simple slab domain decomposition , is used to obtain the pm force from the mesh density .",
    "a one - dimensional structure  the hilbert curve  is introduced to handle the distribution of particles across the processes and to load balance the work done on particles .    implementing hilbert curve domain decomposition in a particle code is the major innovation of our work . to take full advantage of it we had to employ a number of advanced techniques .",
    "first , in  [ sec_loadbal ] we devised a discrete algorithm to find the nearly optimal partitioning of the hilbert curve so as to achieve load balance , the desirable state in which all processors have the same amount of work to do .",
    "this is a much greater challenge in a hybrid code than in a purely mesh - based code such as a hydrodynamic solver or a gridless particle code such as the tree code .",
    "we then made the domain decomposition dynamic by repartitioning the hilbert curve every timestep , allowing us to dynamically maintain approximate load balance even when the particle clustering became strong .    in  [ sec_voids ]",
    "we presented a fast method for finding the position of a cell along the hilbert curve given its three - dimensional location .",
    "this procedure allows us to access arbitrary cells in a general irregular domain by a lookup table much faster than using the special - purpose hilbert curve function of @xcite .    in  [ sec_compress ] we introduced run - length encoding to greatly reduce the communication cost for transferring information between the particle and mesh structures required during the pm force computation .    in  [ sec_adv ] we optimized the particle distribution within each process so as to improve the cache performance critical for efficient pair summation in the pp force calculation .    by choosing the domain decomposition method appropriate for each data structure , and by implementing these additional innovations , we achieved good load balance and scalability even under extreme clustering .",
    "the techniques we introduced for effective parallelization should be applicable to a broad range of other computational problems in astrophysics including smooth - particle hydrodynamics and radiative transfer",
    ".    tests of our algorithm in ",
    "[ sec_test ] showed that we achieved our goals of scalability and load balance , with two caveats mentioned at the end .",
    "in figure [ fg_wall ] we demonstrated the importance of using a dynamic three - dimensional domain decomposition method instead of a static one - dimensional slab decomposition .",
    "the latter method is unable to handle extreme spatial inhomogeneity .",
    "next , we performed a long @xmath1 @xmath375cdm simulation ( performed on only 20 dual - processor computing nodes ) to thoroughly test the load balancing algorithm .",
    "the average load imbalance for this simulation run with 80 processes was only 12% , meaning that 12% of the total wall clock time of all the cpus was wasted .",
    "while not perfect , this is very good performance for the p@xmath0 m algorithm .",
    "the largest cause of load imbalance over most of the simulation was our inability to predict the total cpu time of the next timestep on each process because of variations in cache memory usage .    finally , we tested the limits of scalability by performing the set of runs in table [ tb_sclb ] . for up to 48 processes",
    "the code performed with very good parallel speedup  the wall clock time scaled as @xmath377 for @xmath132 processes , as compared with @xmath380 for perfect scalability .",
    "our tests revealed two limitations to scalability that will be addressed in a later paper presenting an adaptive p@xmath0 m algorithm .",
    "first , the current code uses blocking communication for sending data between the particle and grid structures in the pm force calculation .",
    "in other words , some processes sit idle waiting for others to complete their communications requests .",
    "this inefficiency , while small when pp forces are expensive to compute , will become more important when adaptive mesh refinement reduces the pp cost .",
    "the solution is to restructure the communication to work with non - blocking sends and receives .    finally , we observed our code to become inefficient when a handful of hilbert curve cells ( out of millions in the entire simulation ) begin to dominate the computation of pp forces .",
    "because a non - adaptive code does not allow refinement of one cell , a single process must handle these extremely clustered cells even if the other processes have to wait idly while it finishes .",
    "the solution to this problem is simply to use adaptive refinement . in a later paper",
    "we present an algorithm for scalable adaptive p@xmath0 m building upon the techniques introduced in the current paper .",
    "once this paper is accepted for publication , the simulation codes presented here will be made publicly available at http://antares.mit.edu/.    a. shirokov would like to thank paul shapiro and mike warren for useful discussions and serhii zhak for helpful comments on hardware issues .",
    "this work was supported by nsf grant ast-0407050 .",
    "figure [ fg_block ] presents a block diagram of our parallel hilbert curve domain decomposition code .",
    "the code may run on any number of processes @xmath132 ( this is not restricted to being a power of 2 ) .",
    "the code is written in ansi c with mpi calls .",
    "excluding fftw , it consists of about 33,000 lines of code .",
    "this appendix gives an overview of the code guiding the reader to the relevant parts of the main paper .",
    "m code . ]",
    "the code begins by loading particle data from one or more files . at the beginning of a simulation ,",
    "these files contain the initial conditions .",
    "a simulation may also be started using particle data that have already been evolved .",
    "the particle data may be either in one file on the cluster server or they may be in multiple files , one stored on each cluster compute node .    the next step is to initialize the hilbert curve for domain decomposition based on the particle distribution , as described in  [ sec_hci ] . the  code stores particle data ( e.g. positions and other variables as described in  [ ser_pa ] ) differently than mesh data ( e.g. density ) .",
    "mesh - based data are stored on a regular pm mesh which is divided by planes into a set of thick slabs , one for each parallel process .",
    "particle data are organized into larger cells called hilbert curve ( hc ) cells .",
    "( these cells have a size just slightly larger than the cutoff radius for the particle - particle or pp short - range force . )",
    "the cells are then connected like beads on a necklace by a closed one - dimensional curve called a hilbert curve .",
    "the hilbert curve initialization step computes and stores the information needed to determine the location of every bead on the necklace , that is , it associates a one - dimensional address with each hc cell .",
    "once the hilbert curve is initialized , the hilbert curve is cut into a series of segments , each segment ( called a hc local region ) containing a set of hc cells and their associated particles .",
    "each parallel process owns one of the local regions .",
    "the particles are thus sent from the process on which they were initially loaded to the process where they belong .",
    "when restarting a run on the same nodes , the particles are already on the correct processes . when starting a new simulation , the partitions are set with equal spacing along the hilbert curve and the particles are sent to the appropriate processes .",
    "this method of assigning particles to processes based on their position along a one - dimensional curve of discrete segments is called hilbert curve domain decomposition and it is explained in  [ sec_par ] .",
    "the organization of particles within a process is described in ",
    "[ sec_layout ] .",
    "after these initialization steps the code integrates the equations of motion given in ",
    "[ sec_eom ] using a leapfrog scheme presented in  [ sec_leapfrog ] .",
    "first the positions are advanced one - half timestep , and if they cross hc local region boundaries they are moved to the correct process .",
    "next , gravitational forces are computed .",
    "most of the work done by the code is spent computing forces .",
    "the interparticle forces are split into a long - range particle - mesh part computed on the mesh and interpolated to the particles , plus a short - range particle - particle correction , as described in   [ sec_pm ] , [ sec_pp ] , and [ sec_hcforce ] .",
    "most of the communication between processes occurs during these steps .",
    "if the particle - mesh green s function has not yet been computed , it is computed just before the first pm calculation .",
    "the green s function is essentially the discrete fourier transform of @xmath381 , modified by an anti - aliasing filter to reduce anisotropy on scales of the pm mesh spacing .",
    "after the particle - mesh forces are computed , they are incremented by the particle - particle forces ( the most time - consuming part of p@xmath0 m ) . after the forces are computed , velocities and then positions are advanced to the end of the timestep .",
    "once more , particles that cross hc local region boundaries are transferred to the correct process .",
    "after the particles have moved , the cuts along the hilbert curve are moved so as to change the segment lengths and thereby change the domain decomposition .",
    "this step is called repartitioning .",
    "its purpose is to ensure that , as much as possible , each process takes the same amount of time to perform its work as every other process , so that processes do not sit idle waiting for others to finish their work .",
    "( certain operations , like the fft , must be globally synchronized . )",
    "when this ideal situation is met , the code is said to be load balanced .",
    "repartitioning is performed every timestep to optimize load balance , as explained in  [ sec_loadbal ] .    at the end of the integration step ,",
    "the code generally loops back to advance another step .",
    "periodically the code also outputs the particle data , usually writing in parallel to local hard drives attached to each compute node .",
    "table [ tb_cvars ] presents a list of frequently used symbols and variables in the code .     description +   + @xmath382 & & the simulation box size in comoving mpc , @xmath383 .",
    "+ @xmath56 & & the total number of particles in the simulation volume + @xmath384 & dx & the pm mesh spacing , same in all dimensions + @xmath385 & epsilon & plummer softening length , in units of @xmath384 + @xmath67 & etat & time integration parameter , usually @xmath386 + @xmath90 & cr.max & pp - force length , in units of @xmath384 , typically @xmath387 + @xmath388 & n0 .. n2 & the size of the simulation box , in units of pm cells + @xmath389 & ncm0 .. ncm2 & the size of the simulation box in chaining mesh cells + @xmath48 & ngrid & @xmath390 , the total number of pm grid points + @xmath391 & cr.len0..cr.len2 & chaining - mesh grid spacing along three dimensions + & , & starting and finishing pointers of particle array  @xmath290 + & & pointer to the end of the preallocated particle array , equals pa_f in the serial code . in the parallel code  @xmath392 .",
    "+   + & & starting index of fftw slab @xmath15 for the fft plan + & & thickness of fftw slab @xmath15 . the whole slab on process @xmath15 has size @xmath393 $ ] , where @xmath394 + @xmath395 & hc_n0  hc_n2 & the size of the simulation box in hilbert curve ( hc ) mesh cells , per dimension + @xmath224 & & @xmath396 , the total number of hc mesh cells .",
    "+ @xmath397 & & the number of particles local to process",
    "@xmath15 + @xmath132 & wk.nproc & number of worker processes , those containing particle data + @xmath153 & & coordinates of a cell in the hilbert curve mesh + @xmath121 & & a hilbert curve index + @xmath157 & & a raw hilbert curve index + @xmath398 & & mapping between the hc index @xmath156 and the cell s coordinates + @xmath399 & & the inverse of the above mapping + @xmath165 & & mapping between the hc raw index @xmath156 and the cell s coordinates + @xmath139 & & hc order : the number of cells in the hc mesh is @xmath122 , @xmath87 + @xmath400 & & hc mesh spacing along dimension @xmath15 , in units of @xmath384 + @xmath133 & & hc local region of process @xmath283 + & hc_stg & 3-d ragged array with gaps of the cells of the @xmath133 + @xmath160 & & the number of entries of the hc into the simulation volume + @xmath401 & & the hc index of the @xmath402-th entry of the curve into the simulation volume @xmath403 , and the number of the hc cells that follow contiguously inside the simulation volume along the curve + @xmath404 & & the hc index of the bottom partition and number of cells on process @xmath15 + @xmath405 & & same , with the raw index +",
    "working with a hilbert ( space - filling ) curve requires a mapping from hc index @xmath121 to hc cell position @xmath406 and vice versa .",
    "@xcite implemented c functions that accomplish these mappings .",
    "the most straightforward implementation of the hilbert curve is too slow , since a hilbert curve is defined recursively by its self similarity .",
    "moore s implementation is based on a much faster non - recursive algorithm of @xcite .",
    "a one - to - one correspondence between a cell and the hc index is given by the following functions of moore s implementation : @xmath407 the hilbert curve index @xmath121 is of type long long unsigned and @xmath408 a vector of three integer indices giving the spatial coordinates of the cell .",
    "these two functions are inverse to each other .",
    "they are implemented for any spatial dimension @xmath116 .",
    "for example for @xmath118 in figure [ fg_virgo ] , a function @xmath409 will return the position of the curve s starting point , and the function @xmath410 returns the position of the next cell along the curve .",
    "we verified that the resulting curve indeed provides a one - to - one mapping between the cell and its hc index preserving space locality for all hc mesh sizes up to @xmath411 .",
    "table [ tb_moore ] shows the average measured cpu time to make one call to the hc function  on a 2.4 ghz intel xeon processor . the time shown is compared with the average times to make other simple arithmetic operations or memory references .",
    "it is surprising how fast the implementation is : it takes just two minutes to make @xmath114 hilbert curve function calls on a single processor .",
    "however , in comparison with a simple arithmetic operation or triple array dereferencing , it is very slow : an average  function call is about 120 times slower than a triple array dereferencing for the @xmath114 hc mesh ; the function call time increases linearly with the increase of the hilbert curve order @xmath139 as @xmath412 sec .",
    "we should therefore avoid using the hc implementation function calls when it is possible to use memory dereferencing instead . as we discuss in   [ sec_adv ] and [ sec_hcpm ] we successfully avoid multiple calls to hilbert_c2i  during the force calculation and the particle advancement by proper organization of memory usage .",
    "call , @xmath413 sec + nothing ( bare triple for loop ) & 7.75 + inline multiplication ( innermost integer index squared ) & 12.8 + arithmetic function call ( innermost integer index squared ) & 18.57 + triple array dereferencing & 16.29 +  function call ( @xmath414 ) & 1056",
    ". +  function call ( @xmath414 ) & 920 .",
    "+        bennett , c.l . , et al .",
    "2003 , , 148 , 1 bertschinger , e. 1991 , in after the first three minutes , ed .",
    "s. holt , v. trimble , & c. bennett ( new york : aip ) , 297 bertschinger , e. 1995 , cosmics software release ( astro - ph/9506070 ) bertschinger , e. 1996 , in cosmology and large scale structure , proc .",
    "les houches summer school , session lx , ed .",
    "r. schaeffer , j. silk , m. spiro , and j. zinn - justin ( amsterdam : elsevier science ) , 273            butz , a.r .",
    "1971 , ieee trans .  comp .",
    ", 20 , 424 couchman , h.m.p .",
    "1991 , , 368 , l23 dav , r. , dubinski , d.r . & hernquist , l. 1997 , , 2 , 277 dubinski , j. , kim , j. , park , c. , & humble , r. 2004 , , 9 , 111 efstathiou , g. & eastwood , j.w . 1981 , , 194 , 503                moore , d. 1994 , hilbert curve implementation at http://www.caam.rice.edu/%7edougm/twiddle/hilbert/ pilkington , j. & baden , s. 1996 , ieee trans .  par .  dist .",
    "systems , 7 , 288 plummer , h. c. 1911 , , 71 , 460 quinn , t. , katz , n. , stadel , j. & lake , g. 1997 , preprint ( astro - ph/9710043 ) ruth , r. d. 1983 , ieee trans .  nucl .",
    ", 30 , 2669 salmon , j. & warren , m. 1994 , j.  comp .",
    "phys . , 111 , 136"
  ],
  "abstract_text": [
    "<S> we present a parallel implementation of the particle - particle / particle - mesh ( p@xmath0 m ) algorithm for distributed memory clusters . </S>",
    "<S> the  ( gravitational cosmology ) code uses a hybrid method for both computation and domain decomposition . </S>",
    "<S> long - range forces are computed using a fourier transform gravity solver on a regular mesh ; the mesh is distributed across parallel processes using a static one - dimensional slab domain decomposition . </S>",
    "<S> short - range forces are computed by direct summation of close pairs ; particles are distributed using a dynamic domain decomposition based on a space - filling hilbert curve . </S>",
    "<S> a nearly - optimal method was devised to dynamically repartition the particle distribution so as to maintain load balance even for extremely inhomogeneous mass distributions . </S>",
    "<S> tests using @xmath1 simulations on a 40-processor beowulf cluster showed good load balance and scalability up to 80 processes . </S>",
    "<S> we discuss the limits on scalability imposed by communication and extreme clustering and suggest how they may be removed by extending our algorithm to include adaptive mesh refinement .    </S>"
  ]
}