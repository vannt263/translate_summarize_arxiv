{
  "article_text": [
    "physical and technological processes frequently exhibit intrinsic stochasticity . the main mathematical framework to describe and reason about such systems",
    "is provided by the theory of continuous time ( markovian ) stochastic processes .",
    "such processes have been well studied in chemical physics for several decades as models of chemical reactions at very low concentrations ( * ? ? ?",
    "more recently , the theory has found novel and diverse areas of application including systems biology at the single cell level @xcite , ecology @xcite and performance modelling in computer systems @xcite , to name but a few .",
    "the popularity of the approach has been greatly enhanced by the availability of efficient and accurate simulation algorithms @xcite , which permit a numerical solution of medium - sized systems within a reasonable time frame .    as with most of science",
    ", many of the application domains of continuous time stochastic processes are becoming increasingly data - rich , creating a critical demand for inference algorithms which can use data to calibrate the models and analyse the uncertainty in the predictions .",
    "this raises new challenges and opportunities for statistics and machine learning , and has motivated the development of several algorithms for efficient inference in these systems . in this paper",
    ", we focus on the bayesian approach , and formulate the inverse problem in terms of obtaining an approximation to a posterior distribution over the stochastic process , given observations of the system and using existing scientific information to build a prior model of the process .",
    "the data scenario which has attracted most attention within the bayesian framework is the discretely ( partially ) observed case . in this scenario ,",
    "the experiment returns noisy observations of the state ( or some components ) of the system at a precise set of time points . to proceed with bayesian inference over the trajectories / parameters of the process",
    ", one needs to approximate the likelihood function , i.e. the conditional probability of the observations given the parameters .",
    "this is challenging , as likelihood computations are generally analytically intractable for continuous time processes , and has motivated the development of several approximation strategies based on sampling , variational approximations or system approximations ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "an alternative data scenario which has received much less attention consists of qualitative observations of the trajectory s behaviour .",
    "these are not uncommon : for example , in a biochemical experiment , we may observe that a protein level is above a certain detection threshold over a certain interval of time , without being able to precisely quantify its abundance at any time . in a computer science application",
    ", observations may be error logs which report whether the system s trajectory has violated e.g. some safety threshold over its run .",
    "this type of observations can not be localised in time , but it is concerned with global properties of the system s trajectories : we term them continuous time constraints .",
    "evaluating the probability of a system satisfying some specific trajectory constraints is a highly non - trivial problem ; in computer science , this is called the _ model checking _ problem ( not to be confused with the problem of model checking in statistics , i.e. assessing the statistical fit to a data set ) .",
    "evaluating this probability as a function of parameters , providing a likelihood for bayesian inference , is even more challenging .",
    "recent work , based on gaussian process emulation and optimisation @xcite , has provided a practical solution for maximum likelihood parameter identification for small and medium scale systems ; however , gaussian process optimisation can only provide a point estimate of the parameters , and does not provide a posterior measure over the space of trajectories .    in this paper , we present a flexible approximate scheme for posterior inference in a wide class of stochastic processes from both discrete time observations an continuous time observations / trajectory constraints .",
    "the method can be seen as an extension to continuous time of the expectation - propagation ( ep ) approximate inference algorithm @xcite .",
    "the algorithm was already presented in @xcite for latent linear diffusion processes ; in this paper , we extend that work in several ways .",
    "we extend the approach to a wider class of processes , including markov jump processes ( mjp ) , by applying moment closure of the corresponding chemical langevin equation ( cle ) .",
    "furthermore , we present a novel derivation of the approach based on optimisation of a variational free energy @xcite .",
    "we demonstrate the approach on new numerical examples , demonstrating the effectiveness and efficiency of the approach on realistic models .",
    "in this paper we consider bayesian models for diffusion processes that are observed in discrete and continuous time , where the continuous time observation model can be represented in a specific time integral form .",
    "this class of models for continuous time observations can represent a wide range of phenomena such as continuously observed state space models or path constraints .",
    "we consider a diffusion process @xmath0 with known dynamics defined on the time interval @xmath1 $ ] .",
    "the process @xmath2 is defined through the stochastic differential equation ( sde ) @xmath3 where @xmath4 is the standard wiener process @xcite and @xmath5 and @xmath6 are vector and matrix valued functions respectively with @xmath7 being positive semi - definite for all  @xmath8 $ ] .",
    "the functions @xmath9 and @xmath6 are referred to as _ drift _ and _ diffusion _ functions , respectively .",
    "alternatively , the process @xmath2 can also be defined through the markovian transition probabilities satisfying the fokker - planck equation @xmath10           + \\frac{1}{2 } { \\sum\\limits_{ij}}\\partial_{x_i}\\partial_{x_j}[b_{ij}({\\bm{x}}_{t } , t , { \\bm{\\theta } } ) p({\\bm{x}}_{t } \\vert { \\bm{x}}_{s})].\\end{aligned}\\ ] ] even though the process does not possess a formulation through density functions ( with respect to the lebesgue measure ) , we use the alias @xmath11 to denote the  probability density \" of the path / trajectory @xmath0 in order to be able to symbolically represent and manipulate the process ( or its variables ) in the bayesian formalism .",
    "alternatively , we also use this notation as a reference to a process .",
    "we assume that the process can be observed ( noisily ) both at discrete time points and for continuous time intervals ; we use the @xmath12 to denote the discrete time observations at times @xmath13 $ ] and @xmath14 to denote the continuous time observations for all @xmath8 $ ] .",
    "we also use @xmath15 to shorten notation where necessary . in this paper we consider models where the observation likelihood admits the general formulation @xmath16 the term @xmath17 is the conditional probability of the discrete observation @xmath12 given the state of the process at time @xmath18 , while the function @xmath19 is the negative log - likelihood of the continuous time observations ( also referred to as loss function ) .",
    "we choose this class of models because they are sufficiently expressive to model a wide range of phenomena and because the markovian structure of the probabilistic model is preserved , thus making bayesian inference accessible .",
    "for example , the well known linear dynamical systems with the continuous time observation model @xmath20 can be be formulated as follows : ( 1 ) we choose a linear drift @xmath21 and a time - only dependent diffusion @xmath22 ( 2 ) we choose the continuous time loss as the quadratic function @xmath23 - { \\bm{x}}^{t } [ { \\bm{c}}_{t}^{t}{\\bm{r}}_{t}^{-1 } { \\bm{c}}_t]{\\bm{x}}_t/2 $ ] .",
    "another class of models where such time integral representations are useful are temporal log gaussian cox process models ( e.g. * ? ? ? * ; * ? ? ? * ) .",
    "bayesian inference in diffusion process models is generally understood as the inference of the posterior process @xmath24 and model evidence @xmath25 clearly , the existence of a closed form specification of the posterior process as , say , defined by a new pair of drift and diffusion functions , would be desirable .",
    "however , this is only possible in a few special cases such as ornstein - uhlenbeck / gaussian - markov processes with gaussian discrete time observations and quadratic loss functions .",
    "for this reason we aim to approximate the time marginals @xmath26 ( referred to as marginals hereafter ) and the model evidence .    in this paper",
    "we address the case where the intractability is mainly due to the likelihood terms ; intractability in the prior ( e.g. due to nonlinear drift/ diffusion terms ) can be handled as long as efficient methods for approximating marginal moments exist ( see section [ secmc ] ) .",
    "we address this problem in two steps .",
    "first , we approximate the likelihood terms by terms that have exponential forms such as @xmath27 second , we propose approximations to the posterior marginals @xmath26 and the model evidence @xmath28 given by . here",
    ", the choice of the function @xmath29 typically follows from the model and the approximation method .",
    "for example , when the prior @xmath30 is gaussian - markov or when we can obtain good gaussian approximations to @xmath31 we choose @xmath29 to be linear and quadratic @xmath32 , thus corresponding to a gaussian . in some cases , however , the resulting computations can still be intractable and a factorising gaussian corresponding to @xmath33 is chosen to make computations tractable . throughout this paper",
    "we consider @xmath29 to correspond to a multivariate gaussian , however , to simplify notation and the emphasise that the results hold for any suitably chosen @xmath29 or any restricted class of gaussian , we opt for this exponential form representation .",
    "in this section we introduce an _ expectation propagation _ ( ep ) based method to approximate the marginals  @xmath34 . in its original derivation @xcite , expectation propagation is an algorithm for approximating an intractable distribution .",
    "one assumes that the distribution is written as a product of certain terms , where typically one term is the prior and the other terms correspond to the likelihoods of individual observations . in the approximating distribution ,",
    "terms are replaced by tractable likelihood proxies .",
    "the ep algorithm aims at achieving consistency between the moments of the approximating distribution and a set of auxiliary distributions .",
    "each auxiliary distribution is obtained by replacing a _ single _ likelihood proxy by its original counterpart . for many problems",
    "auxiliary distributions are still tractable .    ep has been applied to dynamical models @xcite in discrete time . however , a generalisation to continuous time processes is not straightforward .",
    "it is not immediately clear what an addition or removing of a single likelihood term at a given time means for the case of continuous time observations / constraints",
    ".    we will show in the following that by first applying ep to a time discretised process and by taking a subsequent continuous time limit , we obtain a well defined algorithm , which treats discrete and continuous time likelihoods in different ways",
    ".    our derivation of ep for stochastic processes will not follow the original derivation of the ep algorithm in @xcite but we will use an equivalent free energy approximation instead .",
    "we thereby follow a line of arguments similar to @xcite where the fixed points of the ep algorithm were derived as the stationary points of a specific free energy function .",
    "this strategy is similar to the derivation of belief propagation algorithms from a bethe free energy ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "our approach will proivde us with an approximation to the log partition function @xmath35 which normalises the posterior distribution and also approximations to marginal posterior moments @xmath36 where @xmath29 typically corresponds to moments up to a certain order .",
    "however , our approach is sufficiently general to accommodate other choices of @xmath29 as detailed below .",
    "note that the integral in is over the path @xmath37",
    ". * notation . * since we focus on approximating @xmath38 and the moments @xmath39 for a fixed @xmath40 , we omit @xmath40 from our notation in the following .",
    "moreover , we use the shorthand notation @xmath41 and use @xmath42 as an alternative notation for vector inner products .",
    "we refer to the exponential family of distributions defined by the sufficient statistic @xmath29 as the family @xmath43 of distributions having the form @xmath44 with @xmath45 .",
    "we refer to the parameters @xmath46 as canonical parameters .",
    "the first two moments corresponding to @xmath47 can be computed as @xmath48 and @xmath49 . to avoid inconsistent notation",
    ", we use @xmath50 to denote the log partition function in  . in the following",
    "we assume that for any distribution @xmath51 for which @xmath52 exists , there exists a unique canonical parameter vector @xmath46 such that @xmath53 .",
    "this canonical parameter can be defined formally as @xmath54,\\end{aligned}\\ ] ] where kl denotes the kullback - leibler divergence @xmath55 = { \\left\\langle \\log(s_1({\\bm{z}})/s_2({\\bm{z } } ) \\right\\rangle}_{s_1({\\bm{z}})}$ ] . throughout this paper",
    "we assume that given @xmath52 , we can compute @xmath46 efficiently ; we denote this by @xmath56 $ ] and refer to this by as  moment - to - canonical parameter transformation \" .      in order to approximate @xmath57 in",
    ", we first introduce an auxiliary approximating process @xmath58 where the likelihoods are replaced by simpler likelihood proxies .",
    "we assume that the partition function @xmath59 and the marginal moments @xmath60 of the process in are computationally tractable or can be approximated with reasonable accuracy .",
    "we will present a suitable approximation method in section  [ secmc ] . here",
    "the parameter @xmath61 is a variational parameter to be optimised later . using the process and its partition function",
    ", we can represent as @xmath62 rewriting this using we obtain @xmath63 this yields an expression of @xmath57 as the sum of a tractable log  partition function @xmath64 and a correction term accounting for the  error \" resulting from replacing the likelihoods by simpler proxies . a popular approach to simplify",
    "the correction would be to use jensen s inequality in and move the expectation from inside to the outside of the logarithm",
    ". this would give the approximating bound @xmath65 this approximation would be followed by an optimisation of the resulting lower bound with respect to the variational parameters @xmath66 . for recent applications to inference in diffusion processes ,",
    "see @xcite .",
    "the ep approximation to the expectation term in proceeds in a different way .",
    "to this end , we define a set of exponential family marginals @xmath67\\}$ ] with @xmath68 that satisfy the marginal moment matching constraints @xmath69.\\end{aligned}\\ ] ] note that for a gaussian approximation to the posterior process , these would simply be the gaussian marginal densities . following a similar route as in @xcite in their derivation of ep",
    ", our goal will be to approximate the intractable average over the process @xmath70 by a distribution which _ factorises in time _ and is given by the product of the densities @xmath71 .",
    "hence , we retain the information about marginal statistics , but loose the dependencies between different time points . this type of approximation  approximating the expectation of a product with the product of expectations given certain moment constraints  is used in a variety of successful approximation methods such as expectation propagation in latent gaussian models @xcite or belief propagation in pairwise graphical models ( e.g. * ? ? ?",
    "the approximation we propose here can be viewed as the extension of this approach to continuous time ( infinite dimensional ) models .",
    "of course , it is not trivial to define such a factorising density ( corresponding to a delta  correlated process ) directly in continuous time . hence , it will be useful to introduce a discretisation in time into slices of size @xmath72 first and then proceed to the limit @xmath73 .",
    "we use the euler discretisation @xmath74 and @xmath75 and approximate the expectation of a product , see , as @xmath76 taking the limit @xmath73 results in the approximation @xmath77 where the first two terms follow from @xmath78 \\backsimeq   -\\delta t { \\left\\langle u({\\bm{x}}_{t } , t ) \\right\\rangle}_{q_{{\\bm{\\eta}}_{t}}}$ ] . a  comparison with shows that the first two terms in corresponding to continuous time likelihoods would equal their counterparts in the variational bound if the densities @xmath71 are the correct marginals of the process @xmath79 .",
    "this is the case for a gaussian posterior approximation .",
    "however , the second term is different .    by introducing the variables @xmath80 , @xmath81 and applying to we",
    "obtain the approximation @xmath82 where we require the marginal moment matching constraints in to hold .",
    "this approximation contains the two sets of parameters @xmath83 and @xmath66 . following similar arguments as given in @xcite to derive ep for gaussian latent variable models",
    ", we will now argue that it makes sense to optimise the approximation by computing the stationary points of @xmath84 with respect to the variation of these parameters .",
    "in fact , we can show that variation w.r.t .",
    "@xmath85 leads to the moment matching condition . since the exact partition function does not depend on @xmath66",
    "it also makes sense to set the variation w.r.t .",
    "@xmath66 to zero , thereby making the approximation least sensitive w.r.t .",
    "to variation of @xmath66 .",
    "using straightforward calculus one can show that the differentials of w.r.t . @xmath86 and @xmath87",
    "are given by @xmath88 where we use @xmath89 to denote the distribution @xmath90 the variations of w.r.t .",
    "@xmath91 and @xmath92 are @xmath93 where we make us of the @xmath94 property of the exponential family distributions . note that from @xmath95 we recover the marginal moment matching constraints postulated in and @xmath96 is guaranteed to hold when setting @xmath97 and @xmath98 .      except @xmath99 , all other stationary conditions corresponding to and can be viewed as moment matching conditions . since @xmath100 is from the exponential family , @xmath101 and @xmath102",
    "can be expressed in terms of canonical parameters as @xmath103 .    \\quad",
    "\\text{and }   \\quad   { \\bm{\\lambda}}_{i }   + { \\bm{\\eta}}_i= \\text{project}[q_{{\\bm{\\lambda}}}({\\bm{x}}_{t_i } ) ; { \\bm{f } } ] .",
    "\\end{aligned}\\ ] ] we then use to define the fixed point updates @xmath104 - { \\bm{\\eta}}_i      \\quad      \\text{and }      \\quad      { \\bm{\\eta}}_{i}^{new } = \\text{project}[q_{{\\bm{\\lambda}}}({\\bm{x}}_{t_i } ) ; { \\bm{f } } ] - { \\bm{\\lambda}}_i .      \\quad",
    "\\end{aligned}\\ ] ] similarly , from , we obtain updates @xmath105^{-1 } \\partial_{\\eta_t } \\big\\langle   u({\\bm{x}}_{t } , t ) \\big\\rangle _ { q_{{\\bm{{\\bm{\\eta}}}}_{t } } }       \\quad      \\text{and }      \\quad      { \\bm{\\eta}}_{t}^{new } = \\text{project}[q_{{\\bm{\\lambda}}}({\\bm{x}}_{t_i } ) ; { \\bm{f } } ] .",
    "\\quad    \\end{aligned}\\ ] ] readers familiar with the expectation propagation frameworks proposed in @xcite , @xcite and @xcite can identify @xmath87 as the canonical parameters of the term approximations .",
    "the distributions @xmath89 are the tilted distributions and @xmath86 are the parameters of the so - called cavity distributions . the updates in for the discrete time likelihood proxies correspond to expectation propagation updates . the updates in correspond to non - conjugate variational updates @xcite .",
    "a similar fixed point iteration for latent gaussian - markov models has been derived in @xcite by applying expectation propagation to the euler discretisation of the posterior process .      in order to run the fixed point iteration",
    "we need to ( approximately ) compute the canonical parameters corresponding to the updates in and .",
    "since we use exponential family distributions , this simplifies to ( approximately ) computing @xmath106 , @xmath107 , and @xmath108 and computing the corresponding canonical parameters .",
    "we further assume that good numerical approximation for @xmath107 and @xmath106 exist and the computational bottleneck of the proposed method is the computation of @xmath109 .",
    "if the assumptions for @xmath107 and @xmath106 do not hold ( e.g. @xmath89 is a complicated multivariate density because of @xmath100 ) , we can relax the problem by choosing a restricted family of approximations @xmath110 corresponding to  weaker \" sufficient statistics , say , @xmath111 ^ 2/2\\})$ ] .",
    "now we introduce approximations to @xmath109 . due to the markovian nature of the process @xmath79",
    ", the exact marginals @xmath112 can be expressed in terms of the distributions @xmath113 and the conditional likelihoods @xmath114 of @xmath115 as @xmath116 where we define @xmath117 and @xmath118 generally , there are two ways to compute :    * we independently compute @xmath119 and @xmath120 by combining the solutions of the forward and backward fokker - planck equations  corresponding to prior  with iterative bayesian updates corresponding to the likelihood proxies in and .",
    "we then multiply these quantities to obtain the marginals in . * instead of computing @xmath121",
    "we compute @xmath122 directly by making use of the _ smoothing equation _ for @xmath60 @xcite .",
    "the smoothing equation depends on @xmath66 only through @xmath123 which are computed as in ( i ) .    in the following we use the latter approach .",
    "we do this because the approximation method we introduce in the next section is not well defined for conditional likelihoods like @xmath124 .",
    "when the prior process @xmath125 is linear , that is , @xmath126 , and @xmath29 is linear and quadratic , the computations result in solving the kalman - bucy forward and backward equations ( e.g. * ? ? ?",
    "these are a set odes for the mean and covariance of the corresponding gaussian distributions for which efficient numerical methods exist . however , when the prior @xmath125 is non - linear we have to resort to approximations as the computational cost of solving the forward / backward fokker - plank equations is generally excessive .",
    "most approximations proposed in the literature assume a parametric form for @xmath123 and @xmath127 and derive odes for the parameters of the corresponding approximations .",
    "for example , there is a variety of different approximation methods using ( multivariate ) gaussian approximations presented in @xcite and @xcite .",
    "as mentioned above , to compute @xmath119 we need to solve a non - linear fokker planck equation for which generally no analytic solutions are known .",
    "however , note that in our approach we only require the moments @xmath128 and we hence aim at approximating these directly . if we multiply with @xmath129 and take the expectation , we obtain the following odes for the moments @xmath130 of the solution @xmath131 of : @xmath132 the moments @xmath128 fulfil similar odes which additionally take the measurements @xmath133 and @xmath134 into account and which are given in appendix  b. unfortunately , the odes in are not closed ( equations for the moments of order @xmath135 depend on higher order moments ) , leading to an infinite hierarchy of odes @xcite . nonetheless , there are well established moment - closure methods to approximate the solutions of these odes .",
    "one popular class of moment - closure approximations breaks this infinite hierarchy by expressing moments above a certain order as functions of lower order moments @xcite .",
    "one is thus left with a finite system of coupled odes for which efficient numerical integration schemes exist . in this article",
    "we use the _ normal _ or _ cumulant - neglect _ moment closure which sets all cumulants above a certain order to zero . setting all cumulant above order two to zero corresponds to choosing a multivariate gaussian distribution as approximation @xcite .",
    "we then combine the resulting equations with iterative bayesian updates corresponding to @xmath133 and @xmath134 to approximate the moments of @xmath136 .",
    "we denote these moments as @xmath137 and the corresponding canonical parameters as @xmath138 .",
    "more details are given in appendix  [ secalg ] .",
    "as detailed above , in order to approximate @xmath60 we can either approximate @xmath120 or approximate @xmath60 directly . to compute @xmath120 we would need to solve a non - linear backward fokker - planck equation . since @xmath120 is not a distribution , we can not approximate it using moment closure . however , we can use moment closure on the moment smoothing equations proposed in @xcite and @xcite to directly approximate the moments of @xmath60 instead .",
    "these equations compute the marginals moments @xmath128 by using the ( exact ) @xmath119 .",
    "they read as @xmath139 similar to , this corresponds to an infinite cascade of coupled odes .",
    "we solve these approximately by substituting @xmath140as obtained in the previous section  into and applying a corresponding moment closure .",
    "we denote the resulting moments by @xmath141 , the canonical parameters by @xmath142 , and the approximation  by  @xmath143 .",
    "overall , we have introduced two levels of approximations : ( i ) an approximation of using independence assumptions ( section  [ secvfe ] ) ; ( ii ) moment closure to approximate the moments required by the optimisation problem resulting from ( i ) ( section  [ secmc ] ) .",
    "the first level of approximations reduces the inference problem to moment computations , while the second level performs these moment computations approximately by conveniently combining moment closures and iterative bayesian updates within an exponential family of distributions . to derive an algorithm , one first has to decide what family of exponential distributions ( choice of @xmath29 ) is best for the model and data at hand .",
    "given @xmath29 , the form of the moment - closure equations for and can be derived .",
    "it is important to point out that one can do moment closure for a wider set of moments than the ones given by @xmath29 , be it for computational or accuracy reasons .",
    "for example , one can opt for a linear and quadratic @xmath29 but compute moments up to @xmath144 order for better accuracy in approximating @xmath108 . in appendix  [ secalg ]",
    "we provide a detailed description of the algorithm we used to implement the ( approximate ) fixed point iteration in and .",
    "the computational complexity of the algorithm scales linearly w.r.t .",
    "time ( solving odes ) while the scaling w.r.t .",
    "the dimensionality of @xmath145 can vary according to @xmath29 . for the linear and quadratic @xmath29 ( gaussian approximations )",
    "we consider in this paper the computational complexity of solving the odes resulting from the approach presented in section  [ secmc ] scales cubically w.r.t .",
    "the dimensionality of @xmath145 ( matrix multiplications ) .",
    "this computational complexity is similar to the complexity of the method presented in @xcite .",
    "[ subsec_diff ]    we next aim at extending the applicability of the ep method developed in the previous section to markov jump processes ( mjps ) .",
    "mjps are used in many scientific disciplines ranging from queueing theory to epidemiology and systems biology .",
    "they constitute a convenient framework to model stochastic dynamics in discrete valued processes .",
    "typically these are systems in which a set of species interact via various stochastic rules .",
    "the latter are implemented as transitions or `` jumps '' between the discrete states of the system .",
    "the state of a @xmath146-dimensional mjp is given by the vector @xmath147 where each @xmath148 is typically a non - negative integer number .",
    "in this paper we consider mjps that have a finite set of possible transitions @xmath149 , and whose rates @xmath150 depend only on the current state @xmath151 of the system . here , @xmath152 is the @xmath153th column vector of the stoichiometric matrix @xmath154 characterising the transitions .",
    "the single - time marginal distribution @xmath155 of the process with initial state @xmath156 is known to fulfil the master equation @xcite @xmath157 the state component @xmath148 could for instance denote the molecule number of the @xmath158th species in a chemical reaction system . in this case",
    "the transitions correspond to chemical reactions between species and is called the _ chemical master equation _ @xcite .",
    "other types of systems that can be described by a master equation of the type in are for instance prey - predator @xcite or epidemic systems @xcite . for all but the most simple systems , analytic solutions to the master equation in are not available , and one has to rely on either stochastic simulations or analytic approximations .",
    "we discuss next a popular method that approximates an mjp by a non - linear diffusion process . in the chemical reaction context",
    "the equation defining the diffusion process is often called the _ chemical langevin equation _ @xcite .",
    "the approximating non - linear diffusion process is of the same form as the diffusion processes considered in the previous sections defined in .",
    "the inference method proposed in this paper can hence be readily applied to mjps by combining it with the diffusion approximation presented here .",
    "the diffusion equation approximating an mjp described by is a diffusion process with drift and diffusion given by @xcite @xmath159 here @xmath145 is the continuous real - valued pendant to @xmath151 , @xmath160 , where @xmath161 is the rate function of the @xmath158th reaction , and @xmath162 is the diagonal matrix with @xmath163 on the diagonal . in section  [ results ]",
    "we apply the proposed ep method to a lotka - volterra system modeled as a mjp by combining it with the diffusion approximation presented here .",
    "in @xcite we propose an expectation propagation method for diffusion process models where the prior process is gaussian  markov . in this paper , we extend this method to models with non - nonlinear prior processes . here",
    "we use expectation propagation only to approximate the likelihood terms .",
    "we avoid approximating the prior process by using moment - closure approximations on the process resulting from the prior and the likelihood approximations .",
    "when we choose a gaussian ",
    "markov prior process , the method proposed in this paper is identical to the one proposed in @xcite .",
    "in @xcite the authors present a variational approach to approximate non - linear processes with time - only dependent diffusion terms by orstein - uhlenbeck / gaussian - markov processes . to our knowledge",
    "the extension of the approach in @xcite to prior processes with state - dependent diffusion terms is not straightforward since a gaussian - markov approximation to the posterior process would lead to an ill - defined variational objective .",
    "the approach presented in this paper provides a convenient way to avoid this problem .",
    "we only obtain approximations of the posterior marginals instead of a process approximation @xcite , however , we can address inference problems where the diffusion terms are state dependent . in recent work , @xcite proposed an alternative variational approach based on an approximating process with fixed marginal laws .",
    "this extends the gaussian approximation of @xcite to cater for cases where gaussian marginals are not appropriate , e.g. in stochastic reaction networks where concentrations are constrained positive .",
    "the constraint on the marginals however considerably limits the flexibility of their algorithm , and requires a considerable amount of user input ; furthermore , it is unclear how accurate the approximation is in general .",
    "there have been may application of ep in various discrete time models , early works include @xcite , @xcite and @xcite . in these papers the joint distribution of the variables ( markov chain ) is approximated by using a factorising approximation in ep .",
    "note that this is not identical to variational mean - field @xcite . as mentioned in section  [ secvfe ]",
    "their formulation is not straightforward to extend to continuous time and the derivation we present here is a possible way to go around the problem . in @xcite",
    "the authors develop an ep algorithm for continuous time bayesian networks .",
    "their algorithm can be viewed as a generalisation of belief propagation @xcite where each variable in belief propagation corresponds to the path of a single variable in the bayesian network .",
    "the problems they address , like computing the marginal distribution of the whole path of a group of variables ( marginalisation over paths ) , are not directly related to the ones we address in this paper .",
    "their work is similar in sprit to @xcite and @xcite ( see below ) .",
    "inference in markov jump processes from discrete time observations is a well studied problem , with several available algorithms employing sampling , variational or system approximations ( * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "the extension of our proposed method to mjps ( section  [ subsec_diff ] ) can be viewed as an alternative way to do inference for such models , with the additional capability of performing inference from continuous time observations .",
    "@xcite and @xcite propose a continuous time extension of the popular unscented transformation in @xcite to obtain gaussian state space approximations in sde models with time - only dependent diffusion terms and both non - linear / non - gaussian discrete and continuous time observation . in @xcite the authors compare these approaches to the variational method in @xcite which they then use to improve on their smoothing estimates .    in a recent work @xcite present a mean - field variational approximation where they approximate the posterior process with a set of independent univariate gaussian processes ( factorised approximation ) .",
    "the considered model has polynomial drift terms and state - independent diffusion terms and the observations are at discrete time - points . due to a clever parameterisations ( piecewise polynomials ) of the mean and the variance function of the variational approximation",
    "the dimensionality of the state can scale to thousands .",
    ".,scaledwidth=100.0% ]    as an example , we consider a classical benchmark problem , the lotka - volterra system . this system is a popular model describing the nonlinear interactions between a prey and a predator population . the prey @xmath164 and predator @xmath165 interact via the reactions @xmath166 where the first reaction corresponds to a birth process of @xmath164 , the second to the reproduction of @xmath164 , the third to reproduction of @xmath165 by consumption of one @xmath164 , and the fourth to a death process of @xmath165 .",
    "the corresponding rate function @xmath167 and stoichiometric @xmath154 matrix can be written as @xmath168 where @xmath169 and @xmath170 are the number counts of species @xmath164 and @xmath165 , respectively .",
    "depending on the parameters , single realisations of the process show oscillatory behaviour .",
    "we choose a fixed parameter set for which this is the case , namely @xmath171 .    to our knowledge",
    ", no analytic solutions are known for the master equation of this system . to perform approximate inference , we first approximate the master equation by its diffusion approximation defined in and .",
    "this allows us to apply the fixed point iteration procedure described in section  [ secvfe ] to perform approximate inference for non - gaussian likelihoods and continuous time constraints .",
    "the data is generated by simulating the mjp by means of the stochastic simulation algorithm @xcite .",
    "i all scenarios presented in this section , the ep algorithm converged to a accuracy of 0.01 ( corresponding to @xmath172 in the appendix  [ secalg ] ) after a few iteration , typically 10 - 20 .",
    "we have chosen @xmath172 because further iterations resulted in no significant changes in the relevant performance measures ( see below ) .",
    "we first consider discrete time measurements of the system assuming log - normal measurement noise with a variance of @xmath173 .",
    "the panels of figure  [ fig_lv1 ] show the sampled data and the inferred approximate state space marginals .",
    "the left panel shows the results of the assumed density filtering ( adf ) method @xcite shown in algorithm  [ algadf ] of appendix  [ secalg ] while the right panel shows the results obtained by the expectation propagation ( ep ) method developed in this paper .",
    "the detailed procedure is given in algorithm  [ algep ] in appendix  [ secalg ] .",
    "next we consider again discrete log - normal measurements with variance @xmath173 and additionally impose a continuous time constraint with loss function @xmath174 figure [ fig_lv2 ] shows the results obtained by the ep algorithm , without ( left panel ) and with ( right panel ) the continuous time constraint taken into account .",
    "the constraint was chosen to limit the process close to its originally sampled path in the regions highlighted on the panel ( grey area ) .",
    "we observe that the constraints significantly reduce the variance of the approximate posterior state space marginals in the corresponding regions .",
    "to assess whether ep improves on adf , we conducted the following experiment . for several values of the observation noise variance we sampled 40 process paths / trajectories and observation values .",
    "we then measured and averaged the rmse between the true path and the inferred results using adf combined with the corresponding smoothing ( adf - s)this corresponds to the first step of ep  and the ep algorithm .",
    "the latter was iterated until convergence ( a few iterations ) .",
    "we computed the rmse at the data locations ( rmse observations ) as well as over the whole sampled path ( rmse path ) . in this way we assessed both the training and the predictive performance of the approximation .",
    "the results are shown in table  [ rms ] .",
    "we can observe that ep does indeed improve on adf - s for ( almost ) all parameter settings .",
    "moreover , it seems that the predictive performance of ep compared to adf - s is slightly increasing with the increase in the observation noise variance .",
    "this is to be expected , since a larger observation noise variance corresponds to stronger non - gaussianity of the posterior .    .comparing rmse of adf - s and ep .",
    "the table shows the average root mean square error ( rmse ) resulting from the mean of the approximate state - space marginals .",
    "the rmse were obtained by averaging over 40 process / data samples for each noise variance value .",
    "we find that the ep method gives slightly more accurate results than adf .",
    "see main text in section  [ results ] for further details . [",
    "cols=\"^,^,^,^,^ \" , ]      and continuous time constraints for the lotka - volterra system defined in .",
    "the panels show the ep result with ( right panel ) and without ( left panel ) continuous time constraints taken into account .",
    ", scaledwidth=100.0% ]",
    "in this paper , we have derived a novel approximate solution to the bayesian inference problem for continuous time stochastic processes of diffusion type .",
    "this approach can be generalised via a langevin approximation to markov jump processes , providing therefore a practical solution to bayesian inference in a wide class of models .",
    "a distinctive feature of our approach is that it can handle both discrete - time observations and trajectory constraints encoded as a continuous - time loss function .",
    "the resulting approach is therefore highly flexible .",
    "numerical experiments on a classical benchmark , the lotka - volterra system , show both good accuracy and computational performance .",
    "the method we presented is based on a self - consistent algorithm to optimise a variational free energy associated with the inference problem .",
    "this formulation is closely related to other variational approaches for inference in continuous - time processes @xcite , however , it is distinct from others in that we do not seek to reconstruct an approximating process , but focus on computing accurate approximations to the marginal moments of the posterior process .",
    "a major advantage of this moment - based approach is that it still leads to a well - defined algorithm even in the case of state - dependent diffusion processes , when a gaussian variational approach can not be deployed .",
    "in section 5 we compared our ep method with adf , and we here give a detailed description of both algorithms .",
    "we present two algorithms : ( i ) the ep algorithm corresponding to the fixed point iteration in section  [ secvfe ] and ( ii ) an assumed density filtering ( adf ) algorithm @xcite and the adf - s algorithm that performs an extra smoothing step after adf .",
    "adf - s a can be viewed as one single step of the  ep .    before presenting the algorithms , we provide details of how moment closure and iterative bayesian updates are combined to approximate the moments @xmath108 .",
    "let the moment closures for the filtering and smoothing equation of @xmath108 result in @xmath175 we add the contribution of the bayesian updates in the forward computation by @xmath176 ) { \\bm{\\lambda}}_t dt ,       \\label{eqnalgfilter1 }      \\\\",
    "d\\hat{{\\bm{\\mu}}}_{t_i + } ^{\\rm fw } & = \\partial \\log z_{f}(\\text{project}[\\hat{{\\bm{\\mu}}}_{t_i}^{\\rm fw } ; { \\bm{f } } ] + { \\bm{\\lambda}}_i ) ,       \\label{eqnalgfilter2}\\end{aligned}\\ ] ] where we use @xmath177 $ ] to denote the moment - to - canonical parameter transformation .",
    "we use @xmath178 as a function to denote the canonical - to - moment transformation , that is , @xmath179 ) $ ] .",
    "the form of the second term in the r.h.s . of follows from the continuous time bayesian updates by applying a taylor expansion to @xmath180 + dt { \\bm{\\lambda}}_t)$ ] .",
    "smoothing is performed by solving the second equation in backward in time .",
    "the measurements do not have to be incorporated explicitly here , they implicitly enter via the solution for  @xmath181 .",
    "now that the approximation of @xmath108 is formally fixed , we turn our attention to formulating the algorithm corresponding to the fixed point iteration defined in section  [ secvfe ] .",
    "algorithm  [ algep ] shows an implementation of this iteration .",
    "we initialise @xmath87 by choosing a good approximation to @xmath182 and @xmath183 .",
    "when this is not possible we simply set @xmath184 and @xmath185 . in each step of the algorithm",
    "we proceed as follows : ( i ) approximate the moments @xmath108 by solving - and the smoothing equation in , ( ii ) compute @xmath86 and the cavity distribution @xmath89 and ( iii ) update @xmath87 and @xmath85 . in many cases",
    "@xmath186 can be expressed as a function of of @xmath187 .",
    "therefore , we choose to compute the differentials w.r.t .",
    "@xmath187 instead of @xmath188 .",
    "we perform damped updates of @xmath87 and @xmath85 and terminate the iteration when the absolute value of the change in the updates falls below a specified threshold .",
    "algorithm  [ algadf ] shows an implementation of the adf algorithm . in adf a single forward step is performed . here the iterative bayesian updates for the likelihood proxies are performed such that @xmath85 and @xmath87 represent the current estimates computed using @xmath189 .",
    "for the continuous time likelihoods this can be viewed as substituting the update of @xmath85 in into line 4 of the algorithm . for the discrete time",
    "likelihoods the update can be viewed as choosing @xmath184 when performing the first and only update .",
    "choose a convenient @xmath46 such that @xmath190 is reasonably flat initialise @xmath191 $ ] initialise @xmath192 $ ] compute @xmath193 $ ] and @xmath194 as described in sections  [ secfw ]  and  [ secbw ] compute @xmath195 compute @xmath196 - { \\bm{\\eta}}_i $ ] for all @xmath158 compute @xmath197 for all @xmath8 $ ] update @xmath198 , @xmath199 break approximate @xmath57 as described in sections  [ secvfe ] and [ secmc ]     let @xmath200 be the ode resulting from the moment closure of solve @xmath201 )               \\partial_{\\hat{{\\bm{\\mu}}}_t^{\\rm fw } } \\big\\langle   u({\\bm{x}}_{t } , t ) \\big\\rangle _ { q_{\\hat{{\\bm{\\eta}}}_{t}^{\\rm fw } } }   dt$ ] on @xmath202 $ ] compute @xmath203 $ ] and @xmath204 compute @xmath205 ( for adf - s compute @xmath187 as described in sections  [ secfw ]  and  [ secbw ] )"
  ],
  "abstract_text": [
    "<S> we consider the inverse problem of reconstructing the posterior measure over the trajectories of a diffusion process from discrete time observations and continuous time constraints . </S>",
    "<S> we cast the problem in a bayesian framework and derive approximations to the posterior distributions of single time marginals using variational approximate inference . </S>",
    "<S> we then show how the approximation can be extended to a wide class of discrete - state markov jump processes by making use of the chemical langevin equation . </S>",
    "<S> our empirical results show that the proposed method is computationally efficient and provides good approximations for these classes of inverse problems . </S>"
  ]
}