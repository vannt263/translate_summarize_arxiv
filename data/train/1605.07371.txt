{
  "article_text": [
    "a probabilistic model over a discrete state space is classified as energy - based if it can be written in the form @xmath0 where the _ energy _ @xmath1 is a computationally tractable function of the system s configuration  @xmath2 , @xmath3 is a set of parameters to be learned from data , and @xmath4 is a normalization constant also known as the _ partition function_. many methods for learning energy - based models exist ( e.g. @xcite ) which makes them useful in a wide variety of fields , most recently as data analysis tools in neuroscience @xcite",
    ".    a popular way of parametrizing the energy function is by decomposing it into a sum of _ potentials _ representing interactions among different groups of variables , i.e. @xmath5 the resulting models , also termed gibbs random fields @xcite , are easy to interpret but , even for moderate  @xmath6 , learning the potential functions from data is intractable unless we can a priori set most of them to zero , or we know how parameters of multiple potentials relate to each other . a common assumption is to consider single- and two - variable potentials only , but many tasks require an efficient parametrization of higher - order interactions .",
    "a powerful way of modeling higher - order dependencies is to assume that they are mediated through hidden variables coupled to the observed system .",
    "many hidden variable models are , however , notoriously hard to learn ( e.g. boltzmann machines @xcite ) , and their distributions over observed variables can not be represented with tractable energy functions . an important exception is the restricted boltzmann machine ( rbm ) @xcite which is simple to learn even when the dimension of the data is large , and which has proven effective in many applications @xcite .",
    "this paper considers a new alternative for modeling higher - order interactions .",
    "we generalize any model of the form to @xmath7 where @xmath8 is an arbitrary strictly increasing and twice differentiable function which needs to be learned from data together with the parameters @xmath3 .",
    "while this defines a new energy - based model with an energy function @xmath9 , we will keep referring to @xmath1 as the energy function .",
    "this terminology reflects our interpretation that @xmath1 should parametrize local interactions between small groups of variables , e.g. low - order terms in eq . , while the function @xmath8 globally couples the whole system .",
    "we will formalize this intuition in section [ why_it_works ] . since setting @xmath10 recovers",
    ", we will refer to @xmath8 simply as _",
    "the nonlinearity_.    generalized energy - based models have been previously studied in the physics literature on nonextensive statistical mechanics @xcite but , to our knowledge , they have never been considered as data - driven generative models .",
    "if @xmath11 is a continuous rather than a discrete vector , then models are related to elliptically symmetric distributions @xcite .",
    "we wish to make as few prior assumptions about the shape of the nonlinearity @xmath8 as possible . we restrict ourselves to the class of strictly monotone twice differentiable functions for which @xmath12 is square - integrable .",
    "it is proved in @xcite that any such function can be represented in terms of a square - integrable function @xmath13 and two constants @xmath14 and @xmath15 as @xmath16 where @xmath17 is arbitrary and sets the constants to @xmath18 , @xmath19 .",
    "is a solution to the differential equation @xmath20 , and so @xmath13 is a measure of the local curvature of @xmath8 . in particular",
    ", @xmath8 is a linear function on any interval on which @xmath21 .",
    "the advantage of writing the nonlinearity in the form is that we can parametrize it by expanding @xmath13 in an arbitrary basis without imposing any constraints on the coefficients of the basis vectors .",
    "this will allow us to use unconstrained optimization techniques during learning .",
    "we will use piecewise constant functions to parametrize @xmath13 .",
    "let @xmath22 $ ] be an interval containing the range of energies @xmath1 which we expect to encounter during learning .",
    "we divide the interval @xmath22 $ ] into @xmath23 non - overlapping bins of the same width with indicator functions @xmath24 , i.e. @xmath25 if @xmath26 is in the @xmath27th bin , otherwise @xmath28 , and we set @xmath29 .",
    "the integrals in eq .   can be carried out analytically for this choice of @xmath13 yielding an exact expression for @xmath8 as a function of @xmath30 and @xmath31 , as well as for its gradient with respect to these parameters ( see appendix a )",
    ".    the range @xmath22 $ ] and the number of bins @xmath23 are metaparameters which potentially depend on the number of samples in the training set , and which should be chosen by cross - validation .",
    "any off - the - shelf method for learning energy - based models is suitable for learning the parameters of the nonlinearity and the energy function simultaneously .",
    "the nature of these two sets of parameters is , however , expected to be very different , and an algorithm that takes this fact into account explicitly would likely be useful . as a step in this direction",
    ", we present an approximation to the likelihood function of the models which can be used to efficiently learn the nonlinearity when the parameters of the energy function are fixed .",
    "let @xmath32 count the number of states which map to the same energy @xmath33 .",
    "the probability distribution of @xmath1 when @xmath11 is distributed according to is @xmath34 given data @xmath35 , let @xmath36 be the data distribution of the energy , and let @xmath37 be the image of @xmath1 . the average log - likelihood of the data can be rewritten as @xmath38 where the first line is a standard expression for energy based models , and the second line follows by substituting the logarithm of .",
    "eq . has a simple interpretation . the last term , which is the only one depending on @xmath8 , is the average log - likelihood of the samples @xmath39 under the model @xmath40 , and so , for any @xmath3 , the purpose of the nonlinearity is to reproduce the data probability distribution of the energy .",
    "our restriction that @xmath8 is a twice differentiable increasing function can be seen as a way of regularizing learning .",
    "if @xmath8 was arbitrary , then , for any @xmath3 , an upper bound on the likelihood is attained when @xmath41 . according to",
    ", this can be satisfied with any function @xmath8 ( potentially infinite at some points ) such that for all @xmath42 @xmath43 energy functions often assign distinct energies to distinct states in which case the choice leads to a model which exactly reproduces the empirical distribution of data , and hence overfits .",
    "the partition function can be rewritten as @xmath44 the size of @xmath37 is generically exponential in the dimension of @xmath11 , making the above sum intractable for large systems . to make it tractable",
    ", we will use a standard trick from statistical mechanics , and approximate the sum over distinct energies by smoothing @xmath45 .",
    "let @xmath46 $ ] be an interval which contains the set @xmath47 , and which is divided into @xmath48 non - overlapping bins  @xmath49 $ ] of width @xmath50 .",
    "let @xmath51 be the energy in the middle of the @xmath27th bin .",
    "the partition function can be approximated as @xmath52 where @xmath53 is the _ density of states _ , and @xmath54 as @xmath55 since @xmath8 is differentiable .",
    "the approximation @xmath56 is useful for two reasons .",
    "first , in most problems , this approximation becomes very good already when the number of bins @xmath48 is still much smaller than the size of @xmath37 .",
    "second , an efficient monte carlo method , the wang and landau algorithm @xcite , exists for estimating the density of states for any fixed value of @xmath3 .",
    "the approximation yields the following approximation of the average log - likelihood @xmath57 which can be maximized with respect to the parameters of @xmath8 using any gradient - based optimization technique ( after specifying the metaparameters @xmath58 , @xmath59 , and @xmath48 ) .",
    "many learning algorithms represent information about the model @xmath60 as a finite list of samples from the model .",
    "this representation is necessarily bad at capturing the low probability regions of @xmath40 . according to",
    ", this means that any such algorithm is expected to be inefficient for learning @xmath8 in these regions . on the other hand ,",
    "the density of states @xmath61 estimated with the wang and landau algorithm carries information about the low probability regions of @xmath40 with the same precision as about the high probability regions , and so algorithms based on maximizing should be efficient at learning the nonlinearity .",
    "the approximation can not be used to learn the parameters @xmath3 of the energy function , and so the above algorithm has to be supplemented with other techniques .",
    "a major question in neuroscience is how populations of neurons , rather than individual cells , respond to inputs from the environment .",
    "it is well documented that single neuron responses are correlated with each other but the precise structure of the underlying redundancy , and its functional role in information processing is only beginning to be unraveled @xcite .    the response of a population of @xmath6 neurons during a short time window can be represented as a binary vector @xmath62 by assigning a @xmath63 to every neuron which elicited a spike .",
    "we pool all population responses recorded during an experiment , and we ask what probabilistic model would generate these samples .",
    "this question was first asked in a seminal paper @xcite which showed that , for small networks of retinal neurons ( @xmath64 ) , the fully visible boltzmann machine , or a _ pairwise model _ , @xmath65 is a good description of the data .",
    "later it was realized that for large networks ( @xmath66 ) pairwise models can not accurately capture probability distributions of data statistics which average across the whole population such as the total population activity  @xmath67 .",
    "this issue was solved by the introduction of the so - called _ k - pairwise models _",
    "@xcite , @xmath68    here we look at the performance of two additional models .",
    "a _ semiparametric pairwise model _ , which is a generalization of the pairwise model , and a restricted boltzmann machine with @xmath69 hidden units which is known to be an excellent model of higher order dependencies .",
    "the philosophy of our comparison is not to find a state - of - the art model , but rather to contrast several different models of comparable complexity ( measured as the number of parameters ) in order to demonstrate that the addition of a simple nonlinearity to an energy - based model can result in a significantly better fit to data .",
    "we analyze a simultaneous recording from @xmath70 neurons in a salamander s retina which is presented with @xmath71 repetitions of a @xmath72 second natural movie .",
    "the data was collected as part of @xcite , and it contains a total of @xmath73 population responses .    our goal is to compare the performance , measured as the out - of - sample log - likelihood per sample per neuron , of the above models across several subnetworks of different sizes . to this end , we use our data to construct @xmath74 smaller datasets as follows .",
    "we randomly select @xmath75 neurons from the total of @xmath70 as the first dataset",
    ". then we augment this dataset with @xmath76 additional neurons to yield the second dataset , and we keep repeating this process until we have a dataset of @xmath77 neurons .",
    "this whole process is repeated @xmath78 times , resulting in @xmath78 datasets for each of the @xmath79 different network sizes . only the datasets with @xmath75 and @xmath80 neurons",
    "do not have significant overlaps which is a limitation set by the currently available experiments .",
    "for each dataset , we set aside responses corresponding to randomly selected @xmath80 ( out of 297 ) repetitions of the movie , and use these as test data .",
    "our results are summarized in figure [ fig1 ] .",
    "we see that both semiparametric pairwise models and rbms with @xmath69 hidden units substantially outperform the previously considered k - pairwise models .",
    "in particular , the improvement increases as we go to larger networks .",
    "rbms are consistently better than semiparametric pairwise models , but , interestingly , this gap does not seem to scale with the network size .",
    "the inferred nonlinearity @xmath8 for semiparametric pairwise models does not vary much across subnetworks of the same size .",
    "this is not surprising for large networks since there is a substantial overlap between the datasets , but it is nontrivial for smaller networks .",
    "the average inferred nonlinearities are shown in figure [ fig2]a .",
    "semiparametric pairwise models have one free parameter since the transformation @xmath81 does not change their probability distribution .",
    "therefore , to make the comparison of @xmath8 across different datasets fair , we fix @xmath82 by requiring @xmath83 .",
    "furthermore , the nonlinearities in figure [ fig2]a are normalized by @xmath6 because we expect the probabilities in a system of dimension @xmath6 to scale as @xmath84 .",
    "the inferred nonlinearities are approximately concave , and their curvature increases with the network size .",
    "[ fig_nonlinearity ] , top is @xmath85 ) .",
    "b. inferred probability densities of the latent variable for one sequence of subnetworks.,title=\"fig : \" ]      training was done by a variation of persistent contrastive divergence @xcite which performs an approximate gradient ascent on the log - likelihood of any energy - based model .",
    "given an initial guess of the parameters @xmath86 , and a list of @xmath87 samples drawn from @xmath88 , the algorithm can be summarized as    t:=1 l _",
    "t = _ t-1 + ( [ _ e(;_t-1)]__t-1 - [ _ e(;_t-1 ) ] _ ) _ t = ^n(_t-1,_t )    where @xmath89 is the number of iterations , @xmath90 is the learning rate , @xmath91_{\\text{list}}$ ] denotes an average over the list of states , and @xmath92 represents @xmath93 applications ( i.e. @xmath93 neurons are switched ) of the gibbs sampling transition operator .    in the case of semiparametric pairwise models ,",
    "persistent contrastive divergence was used as a part of an alternating maximization algorithm in which we learned the nonlinearity by maximizing the approximate likelihood while keeping the couplings @xmath94 fixed , and then learned the couplings using persistent contrastive divergence with the nonlinearity fixed .",
    "details of the learning algorithms for all models are described in appendix b.    the most interesting metaparameter is the number of bins @xmath23 necessary to model the nonlinearity @xmath8 .",
    "we settled on @xmath95 but we observed that decreasing it to @xmath96 would not significantly change the training likelihood .",
    "however , @xmath95 yielded more consistent nonlinearities over different subgroups .      estimating likelihood of our data is easy because the state @xmath97 occurs with high probability ( @xmath98 ) , and all the inferred models retain this property",
    "therefore , for any energy - based model , we estimated @xmath99 by drawing @xmath100 samples using gibbs sampling , calculated the partition function as @xmath101 , and used it to calculate the likelihood .",
    "the models do not have any explicit regularization .",
    "we tried to add a smoothed version of l1 regularization on coupling matrices but we did not see any improvement in generalization using a cross - validation on one of the training datasets .",
    "certain amount of regularization is due to sampling noise in the estimates of likelihood gradients , helping us to avoid overfitting .",
    "perhaps surprisingly , the addition of a simple nonlinearity to the pairwise energy function @xmath102 significantly improves the fit to data .",
    "here we give heuristic arguments that this should be expected whenever the underlying system is globally coupled .",
    "let @xmath103 be a sequence of positive probabilistic models ( @xmath11 is of dimension @xmath6 ) , and suppose that @xmath103 can be ( asymptotically ) factorized into subsystems statistically independent of each other whose number is proportional to @xmath6 .",
    "then @xmath104 is an average of independent random variables , and we expect its standard deviation @xmath105 to vanish in the @xmath106 limit . alternatively ,",
    "if @xmath107 , then the system can not be decomposed into independent subsystems , and there must be some mechanism globally coupling the system together .",
    "it has been argued that many natural systems @xcite including luminance in natural images @xcite , amino acid sequences of proteins @xcite , and neural activity such as the one studied in sec .",
    "[ neurons ] @xcite belong to the class of models whose log - probabilities per dimension have large variance even though their dimensionality is big .",
    "therefore , models of such systems should reflect the prior expectation that there is a mechanism which couples the whole system together . in our case , this mechanism is the nonlinearity @xmath8 .",
    "recent work attributes the strong coupling observed in many systems to the presence of latent variables ( @xcite ) .",
    "we can rewrite the model in terms of a latent variable considered in @xcite if we assume that @xmath108 is a totally monotone function , i.e. that it is continuous for @xmath109 ( we assume , without loss of generality , that @xmath110 ) , infinitely differentiable for @xmath111 , and that @xmath112 for @xmath113 .",
    "theorem @xcite then asserts that we can rewrite the model as @xmath114 where @xmath115 is a probability density ( possibly containing delta functions ) .",
    "suppose that the energy function has the form",
    ". then we can interpret as a latent variable @xmath116 being coupled to every group of interacting variables , and hence inducing a coupling between the whole system whose strength depends on the size of the fluctuations of @xmath116 .",
    "while the class of positive , twice differentiable , and decreasing functions that we consider is more general than the class of totally monotone functions , we can find the maximum likelihood densities @xmath115 which correspond to the pairwise energy functions inferred in section [ neurons ] using the semiparametric pairwise model .",
    "we model @xmath115 as a histogram , and maximize the likelihood under the model by estimating @xmath117 using the approximation .",
    "the maximum likelihood densities are shown in figure [ fig2]b for one particular sequence of networks of increasing size .",
    "the units of the latent variables are arbitrary , and set by the scale of @xmath94 which we normalize so that @xmath83 .",
    "the bimodal structure of the latent variables is observed across all datasets .",
    "we do not observe a significant decrease in likelihood by replacing the nonlinearity @xmath8 with the integral form .",
    "therefore , at least for the data in sec .",
    "[ neurons ] , the nonlinearity can be interpreted as a latent variable globally coupling the system .",
    "suppose that the true system @xmath118 which we want to model with satisfies @xmath107 .",
    "then we also expect that energy functions @xmath119 which accurately model the system satisfy @xmath120 . in the limit @xmath106 , the approximation of the likelihood becomes exact when @xmath8 is differentiable ( @xmath121 has to be appropriately scaled with @xmath6 ) , and",
    "arguments which led to can be reformulated in this limit to yield @xmath122 where @xmath123 is the density of states , and @xmath124 is now the probability density of @xmath119 under the true model . in statistical mechanics , the first term @xmath125",
    "is termed the _ microcanonical entropy _ , and is expected to scale linearly with @xmath6 . on the other hand , because @xmath120 , we expect the second term to scale at most as @xmath126 .",
    "thus we make the prediction that if the underlying system can not be decomposed into independent subsystems , then the maximum likelihood nonlinearity satisfies @xmath127 up to an arbitrary constant .    in figure",
    "[ fig3]a we show a scatter plot of the inferred nonlinearity for one sequence of subnetworks in sec .",
    "[ neurons ] vs the microcanonical entropy estimated using the wang and landau algorithm . while the convergence is slow , the plot suggests that these functions approach each other as the network size increases . to demonstrate this prediction on yet another system",
    ", we used the approach in @xcite to fit the semiparametric pairwise model with @xmath128 to @xmath129 patches of pixel log - luminances in natural scenes from the database @xcite .",
    "this model has an analytically tractable density of states which makes the inference simple .",
    "figure [ fig3]b shows the relationship between the inferred nonlinearity and the microcanonical entropy for collections of patches which increase in size , confirming our prediction that the nonlinearity should be given by the microcanonical entropy .",
    "we presented a tractable extension of any energy - based model which can be interpreted as augmenting the original model with a latent variable .",
    "as demonstrated on the retinal activity data , this extension can yield a substantially better fit to data even though the number of additional parameters is negligible compared to the number of parameters of the original model . in light of our results",
    ", we hypothesize that combing a nonlinearity with the energy function of a restricted boltzmann machine might yield a model of retinal activity which is not only accurate , but also simple as measured by the number of parameters .",
    "simplicity is an important factor in neuroscience because of experimental limitations on the number of samples .",
    "we plan to pursue this hypothesis in future work .",
    "our models are expected to be useful whenever the underlying system can not be decomposed into independent components .",
    "this phenomenon has been observed in many natural systems , and the origins of this global coupling , and especially its analogy to physical systems at critical points , have been hotly debated .",
    "our models effectively incorporate the prior expectations of a global coupling in a simple nonlinearity , making them superior to models based on gibbs random fields which might need a large number of parameters to capture the same dependency structure .",
    "we thank david schwab , elad schneidman , and william bialek for helpful discussions .",
    "this work was supported in part by hfsp program grant rgp0065/2012 and austrian science fund ( fwf ) grant p25651 .",
    "let @xmath130 where @xmath24 are the indicator functions defined in the main text .",
    "for @xmath131 , we have @xmath132 . for @xmath133 we have @xmath134 , where @xmath135 - 1 } \\exp\\left(\\delta \\sum_{j=1}^{i-1 } \\beta_j \\right ) \\frac{\\exp(\\delta \\beta_i)-1}{\\beta_i}+ \\exp \\left(\\delta \\sum_{j=1}^{[e]-1 } \\beta_j \\right )   \\frac{\\exp(\\beta_{[e]}(e - ( [ e]-1)\\delta ) ) -1}{\\beta_{[e]}}. \\end{aligned}\\ ] ] we define @xmath136 $ ] as the number of the bin that contains @xmath26 . if @xmath137 , then we define @xmath136 = q + 1 $ ] , and @xmath138",
    ".    the gradient is @xmath139 @xmath140 @xmath141 if @xmath142 $ ] , then @xmath143 if @xmath144 $ ] , then @xmath145 - 1 } \\beta_j \\right)\\frac{\\exp ( \\delta \\beta_{[e]})\\delta \\beta_{[e ] } - \\exp ( \\delta \\beta_{[e ] } ) + 1}{\\beta_{[e]}^2}.\\ ] ] if @xmath146 $ ] , then @xmath147 - 1 } \\exp\\left(\\delta \\sum_{j=1}^{i-1 } \\beta_j \\right ) \\frac{\\exp(\\delta \\beta_i)-1}{\\beta_i } \\\\      &",
    "\\qquad + \\delta \\exp\\left(\\delta \\sum_{j=1}^{[e]-1 } \\beta_j \\right ) \\frac{\\exp(\\beta_{[e]})(e - ( [ e]-1)\\delta)-1}{\\beta_{[e]}}. \\end{aligned}\\ ] ]",
    "pairwise models , k - pairwise models , and rbms were all trained using persistent contrastive divergence with @xmath148 , @xmath149 , and with initial parameters drawn from a normal distribution with @xmath150 mean and @xmath151 standard deviation .",
    "we iterated the algorithm three times , first with @xmath152 , then with @xmath153 , and finally with @xmath154 ( the last step had @xmath155 for k - pairwise models , and also for rbms when @xmath85 ) .",
    "we initialized the coupling matrix @xmath156 as the one learned using a pairwise model .",
    "the @xmath17 and @xmath157 metaparameters of @xmath8 were set to the minimum and maximum energy @xmath158 observed in the training set .",
    "we set @xmath159 , and we initialized the parameters @xmath30 and @xmath31 of the nonlinearity by maximizing the approximate likelihood with @xmath160 fixed .",
    "the metaparameters @xmath161 and @xmath162 for the approximate likelihood were set to the minimum , and twice the maximum of @xmath163 over the training set .",
    "@xmath48 was set between @xmath164 and @xmath165 .",
    "the density of states was estimated with a variation of the algorithm described in @xcite with accuracy @xmath166 .    starting with these initial parameters , we ran two iterations of persistent contrastive divergence with @xmath167 , simultaneously learning the coupling matrix and the nonlinearity .",
    "the first iteration had @xmath152 , and the second one @xmath153 . in order for the learning to be stable",
    ", we had to choose different learning rates for the coupling matrix ( @xmath148 ) , and for @xmath30 and @xmath31 ( @xmath168 ) .",
    "for the last step , we adjusted the metaparameters of the nonlinearity so that @xmath17 and @xmath157 are the minimum and maximum observed energies with the current @xmath94 , and @xmath95 .",
    "we maximized the approximate likelihood to infer the nonlinearity with these new metaparameters .",
    "then we fixed @xmath8 , and ran a persistent contrastive divergence ( @xmath169 ) learning @xmath94 .",
    "finally we maximized the approximate likelihood with fixed @xmath94 to get the final nonlinearity ."
  ],
  "abstract_text": [
    "<S> probabilistic models can be defined by an energy function , where the probability of each state is proportional to the exponential of the state s negative energy . </S>",
    "<S> this paper considers a generalization of energy - based models in which the probability of a state is proportional to an arbitrary positive , strictly decreasing , and twice differentiable function of the state s energy . </S>",
    "<S> the precise shape of the nonlinear map from energies to unnormalized probabilities has to be learned from data together with the parameters of the energy function . as a case study </S>",
    "<S> we show that the above generalization of a fully visible boltzmann machine yields an accurate model of neural activity of retinal ganglion cells . </S>",
    "<S> we attribute this success to the model s ability to easily capture distributions whose probabilities span a large dynamic range , a possible consequence of latent variables that globally couple the system . </S>",
    "<S> similar features have recently been observed in many datasets , suggesting that our new method has wide applicability . </S>"
  ]
}