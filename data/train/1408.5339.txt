{
  "article_text": [
    "monotone trajectories describing the evolution of state(s ) over time appear widely in scientific studies .",
    "the most widely studied are probably growth of organisms such as humans or plants ( milani , 2000 ; erickson , 1976 ; silk and erickson , 1979 ) .",
    "there are many parametric models for describing the features of growth curves , particularly in human growth ( hauspie et al . , 1980 ; milani , 2000 ) .",
    "most of these works focus on modeling the trajectories themselves or modeling the rate of change , i.e. , the derivative of the trajectories",
    ". other examples of monotone trajectories appear in population dynamics under negligible resource constraints ( turchin , 2003 ) , in dose - response analysis in pharmacokinetics ( kelly and rice , 1990 ) , in auction price dynamics in e - commerce ( jank and shmueli , 2006 ; wang et al .",
    ", 2008 ; liu and mller , 2009 ) , and in analysis of trajectories of aircrafts after take - off ( nicol , 2013 ) .",
    "some of these works are looking at function estimation with monotonic constraints and some of them are taking a functional data analysis approach .",
    "in contrast , our goal here is to estimate the functional relationship between the rate of change and the state , i.e. , the dynamics of the trajectory , through a nonparametric model .",
    "many systems such as growth of organisms or economic activity of a country / region are intrinsically dynamic in nature ( cf .",
    "ljung and glad , 1994 ) .",
    "a dynamics model provides a mechanistic description of the system rather than a purely phenomenological one .",
    "moreover , due to insufficient scientific knowledge , quite often there is a need for nonparametric modeling of the dynamical system .",
    "in addition , nonparametric fits can be used to develop measures of goodness - of - fit for hypothesized parametric models .",
    "there is a large literature in modeling continuous time smooth dynamical systems through systems of parametric differential equations ( see , e.g. , perthame , 2007 , strogatz , 2001 ) .",
    "these methods have been used to model hiv dynamics ( wu , ding and degruttola , 1998 ; wu and ding , 1999 ; xia , 2003 ; chen and wu , 2008a , 2008b ) , the dynamic behavior of gene regulation networks ( gardner _ et al . _ , 2003 ; cao and zhao , 2008 ) , etc .",
    "approaches for fitting a parametric dynamics model include the maximum likelihood or nonlinear least squares . a recent approach proposed by ramsay _",
    "( 2007 ) and cao _ et al . _",
    "( 2008 ) for parametric ordinary differential equations is based on the idea of balancing the model fit and the goodness of fit of the trajectories simultaneously .",
    "another popular approach to fit dynamics models is a two - stage procedure ( chen and wu , 2008a , 2008b ; , varah , 1982 ) , where the trajectories and their derivatives are first estimated nonparametrically and then the dynamics is fitted by regressing the fitted derivatives to the fitted trajectories . the two - stage approach can be easily adapted to estimate a nonparametric dynamics model .",
    "however , their performance is unsatisfactory due the difficulty of resolving the bias - variance trade - off in a data dependent way .",
    "brunel ( 2008 ) gives a comprehensive theoretical analysis of such an approach .",
    "very recently , hall and ma ( 2014 ) proposed a one - step estimation procedure that mitigates some of the inefficiencies of two - stage estimators .",
    "however , this approach does not seem to extend naturally to estimate nonparametric dynamical systems .",
    "there is also an extensive literature on the nonparametric estimation of monotone functions , e.g. , brunk ( 1970 ) , wright and wegman ( 1980 ) , mammen ( 1991 ) , ramsay ( 1988 , 1998 ) .",
    "however , most methods in this field are not concerned with the estimation of the gradient function , except for ramsay ( 1998 ) where the unknown function is modeled in terms of a second order differential equation and a smoothed estimate of its gradient is obtained as a byproduct .",
    "a key observation of estimating the dynamics of monotone trajectories is that for any smooth monotone trajectory , its dynamics can be described by a first order autonomous differential equation .",
    "specifically , if @xmath1 is positive , strictly monotone and differentiable on a finite time interval , then we can express @xmath2\\ ] ] where @xmath3 is the gradient function . in this paper",
    ", we estimate the unknown gradient function @xmath4 nonparametrically from discrete noisy observations of @xmath5 .",
    "specifically , we model the gradient function by a basis representation where the number of basis functions grow with the sample size . we adopt a nonlinear least squares framework for model fitting .",
    "we then carry out a detailed theoretical analysis and derive the rate of convergence of the proposed estimator .",
    "we now highlight the major contributions of this work . although there is a large literature on linear nonparametric inverse problems ( cavalier _ et al . _ , 2004 ; cavalier , 2008 ; donoho , 1995 ; johnstone _ et al . _ ,",
    "2004 ) , especially on the nonparametric estimation of the derivative of a curve ( gasser and mller , 1984 ; mller et al . , 1987",
    "; fan and gijbels , 1996 ) , there is little theoretical development on nonlinear nonparametric inverse problems .",
    "thus , our work makes a new contribution to this important area . in this paper",
    ", we first quantify the degree of ill - posedness of the estimation of the gradient function @xmath4 as the number of basis functions grow to infinity .",
    "we then use this result to show that if @xmath4 is @xmath6 times differentiable then the @xmath0-risk of the proposed estimator has the same optimal rate of convergence , viz . , @xmath7 , as that of the estimator of the derivative of a trajectory assuming that the latter is @xmath8 times differentiable . in section [ sec : discussion ] , we show that the optimal rate of the proposed estimator is indeed the minimax rate for estimation of @xmath4 under @xmath0 loss if the class of estimators is restricted to be uniformly lipschitz . in the rest of the paper , unless otherwise specified , the phrase `` optimal rate '' refers to the best rate of convergence of the proposed estimator .    among the few instances of nonparametric modeling of the gradient function known to us , xue , miao and wu ( 2010 ) dealt with a related but different problem of estimating a parametric ode with time - varying parameters , where the latter are modeled as unknown smooth functions of time . in a work most closely related to ours , wu et al .",
    "( 2014 ) proposed a sparse additive model for describing the dynamics of a multivariate state vector and developed a combination of two - stage smoothing and sparse penalization for fitting the model .",
    "their model can be seen as a multi - dimensional generalization of the autonomous ode model studied here . in their paper , while deriving the risk bounds , it is assumed that whenever the gradient function @xmath4 is @xmath6 times differentiable , the state @xmath5 is at least @xmath9 times differentiable .",
    "however , due to the representation @xmath3 , it follows that @xmath4 is @xmath6 times differentiable if and only if @xmath5 is @xmath8 times differentiable .",
    "therefore , at least for the one - dimensional state variable case , the assumptions made in wu et al .",
    "( 2014 ) are not satisfied in reality if @xmath6 indeed denotes the maximal order of smoothness of @xmath4 .",
    "this indicates that the rate of convergence their estimator of @xmath4 is not optimal for the current problem .",
    "it is also instructive to note that , due to the assumption about the additional degree of smoothness of the state variable , wu et al .",
    "( 2014 ) did not encounter the technical challenge posed by the ill - posedness of the problem .",
    "the rest of the paper is organized as follows . in section [ sec :",
    "model ] , we briefly describe the model and the estimation procedure .",
    "we present the main theoretical results in section [ sec : theory ] and outline the main steps of the proof in section [ sec : proofs ] .",
    "we present a simulation study in section [ sec : simulation ] and an application to the berkeley growth data in section [ sec : real ] .",
    "we discuss the optimality of the estimation of @xmath4 in section [ sec : discussion ] .",
    "some proof details are provided in the appendix ( section [ sec : appendix ] ) .",
    "some derivations and graphical summaries are provided in the supplementary material ( sm ) .",
    "the class of models studied in this paper is of the form : @xmath10,\\ ] ] where @xmath4 is an unknown smooth function which is assumed to be positive on the range of @xmath11\\}$ ] .",
    "therefore , the sample trajectory @xmath1 is a strictly increasing function of time @xmath12 .",
    "the observations are @xmath13 where @xmath14 are observation times .",
    "the noise terms @xmath15 s are assumed to be i.i.d . with mean 0 and variance @xmath16 .",
    "our goal is to estimate the gradient function @xmath4 based on the observed data @xmath17s .",
    "we propose to approximate @xmath4 through a basis representation : @xmath18 where @xmath19 is a set of linearly independent compactly supported smooth functions . henceforth , we use @xmath20 to denote @xmath21 .      in practice , the initial value @xmath22 and the right boundary @xmath36 may not be observed or may be observed with noise .",
    "the choice of the endpoints of the combined support of the basis functions then becomes a delicate matter .",
    "this is because evaluation of the trajectory is an initial value problem , so error in @xmath37 propagates throughout the time domain .",
    "we discuss this in more details in the appendix ( particularly , see figure [ fig : trajectory_envelop ] ) .    in the following ,",
    "we propose a modified estimation procedure when @xmath37 and @xmath38 are unknown .",
    "the basic idea is to first estimate the trajectory at the endpoints of a slightly smaller time interval @xmath39 $ ] for a small positive constant @xmath40 , and then estimate the gradient function using data falling within this time interval . throughout the paper , @xmath40",
    "is treated as a fixed quantity . in practice",
    ", we may select @xmath40 to be the time point such that about 5% of the data fall in the intervals @xmath41 $ ] and @xmath42 $ ] .",
    "too small a value of @xmath40 may cause distortions of the estimated @xmath4 at the boundaries .",
    "we first obtain nonparametric estimates of @xmath43 and @xmath44 , denoted by @xmath45 and @xmath46 , respectively .",
    "we then define @xmath47 and @xmath48 , where @xmath49 is a small positive number satisfying @xmath50 which implies that @xmath51 as @xmath52 goes to infinity . at the same time",
    ", @xmath49 should be large enough so that @xmath53 which ensures that @xmath54 and @xmath55 as @xmath52 goes to infinity .",
    "for some technical considerations , to be utilized later , we also want @xmath56 . in practice , we may select @xmath49 to be @xmath57 where @xmath58 is the length of the smallest support among the basis functions @xmath59 . for more details on how to obtain @xmath60 , @xmath61 ,",
    "see lemma [ lem : consistency_x_j_hat ] in section [ sec : theory ] .",
    "in addition , we also assume that @xmath60 , @xmath61 are estimated from a sample independent from that used in estimating @xmath34 .",
    "this can be easily achieved in practice by sub - sampling of the measurements .",
    "this assumption enables us to prove the consistency result ( in section [ sec : theory ] ) conditionally on @xmath60 , @xmath61 and treating them as nonrandom sequences converging to @xmath62 , @xmath61 .",
    "in this section , we discuss the consistency of the estimator @xmath71 defined by the loss function ( [ eq : loss_modified ] ) .",
    "the asymptotic framework is that the number of basis functions @xmath35 goes to infinity together with the number of measurements @xmath52 .",
    "the consistency of the estimator @xmath68 over @xmath72 $ ] is formulated in terms of the @xmath0-loss as : @xmath73 in theorem [ thm : optimal_rate ] we derive a bound on the rate of convergence of @xmath68 in terms of the @xmath0-loss as @xmath74 that depends upon the degree of smoothness of @xmath4 . specifically , the optimal rate is @xmath75 for @xmath76 .",
    "the following assumptions are made on the model .",
    "* @xmath77 , and @xmath78 on @xmath79 for some integer @xmath80 , where @xmath79 is an open interval containing @xmath25 $ ] . * the collection of basis functions @xmath81 satisfies : * * @xmath21 s have unit @xmath0 norm ; * * the combined support of @xmath82 is @xmath83 $ ] and for every @xmath84 , the length of the support of @xmath21 is @xmath85 ; * * @xmath86 for all @xmath84 ; * * @xmath87 , for @xmath88 ; * * the gram matrix @xmath89 is such that there exist constants @xmath90 , not depending on @xmath35 such that @xmath91 for all @xmath35 ; * * for every @xmath35 , there is a @xmath92 such that @xmath93}|x_g(t ) - x(t;\\bs\\beta^*)| = o(m^{-(p+1)})$ ] and @xmath94 } |g^{(j)}(u ) - g_{\\bs\\beta^*}^{(j)}(u)| = o(m^{-p+j})$ ] for @xmath88 , where @xmath95 and @xmath96 with @xmath66 as in ( [ eq : initial_beta_delta ] ) . * time points @xmath97 are realizations of @xmath98 , where @xmath99 s are i.i.d . from a continuous distribution @xmath100 supported on @xmath101 $ ] with a density @xmath102 satisfying @xmath103 for some @xmath104 .",
    "* the noise @xmath15 s are i.i.d .",
    "sub - gaussian random variables ( cf .",
    "vershynin , 2010 ) with mean 0 and variance @xmath105 .",
    "we give brief explanations of these assumptions . *",
    "a1 * ensures sufficient smoothness of the solution paths of the differential equation ( [ eq : basic ] ) . also by * a1 *",
    ", @xmath106 is @xmath8 times continuously differentiable on @xmath79 .",
    "assumptions ( i ) to ( v ) of * a2 * are satisfied by b - spline basis , rescaled to have unit norm , of order @xmath107 , with equally spaced knots .",
    "define @xmath108 which is used in determining the rates of convergence of the estimator .",
    "thus , by making use of * a4 * , we get the following results with respect to the estimates of @xmath109 and @xmath110 ( cf . fan and gijbels , 1996 ) .    [",
    "lem : consistency_x_j_hat ] suppose that * a1 * and * a4 * hold . consider using a kernel of sufficient degree of smoothness to obtain estimates @xmath60 for @xmath62 , @xmath111 , through local polynomial method with bandwidth of order @xmath112 .",
    "define @xmath113 . then @xmath114 and",
    "given @xmath115 , there exists @xmath116 such that @xmath117 with probability at least @xmath118 , where @xmath119 is as in ( [ eq : xi_n ] ) .",
    "if @xmath120 ( as in theorem [ thm : consistency ] ) and @xmath121 for some @xmath122 , then we have @xmath123 as @xmath124 .",
    "this ensures that @xmath125 $ ] is within the interval @xmath25 $ ] a.s .",
    "for large enough @xmath52 and hence the properties of the function @xmath4 hold on @xmath126 .",
    "in addition , @xmath126 contains the interval @xmath72 $ ] . therefore condition ( ii ) in * a2 * ensures that the combined support of the basis functions covers the range of the data used in estimating @xmath4 .    conditions ( i ) to ( v ) of * a2 * are satisfied by many classes of basis functions , including normalized b - spline basis of order @xmath107 with equally spaced knots in the interval @xmath127 $ ] .",
    "we show in appendix b that if the b - splines basis of order @xmath128 with equally spaced knots in @xmath127 $ ] , then ( vi ) of * a2 * is also satisfied . condition ( vi ) of * a2 * ensures that a solution @xmath30 of ( [ eq : initial_general ] ) on @xmath129 $ ] exists for all @xmath130 sufficiently close to @xmath131 .",
    "this allows us to apply the perturbation theory of differential equations to bound the fluctuations of the sample paths when we perturb the parameter @xmath34 .",
    "assumption * a3 * on the randomness of the sample points allows us to work with the random variables @xmath132 defined as @xmath99 conditional on @xmath133 $ ] with conditional density @xmath134 given by @xmath135 .",
    "the properties of @xmath102 ensure that @xmath134 satisfies the same property on @xmath136 $ ] with possibly modified values of the constants @xmath137 and @xmath138 .",
    "it should be noted that the key derivations leading to the consistency of @xmath68 are conditional on @xmath139 and therefore @xmath140 is only a convenient assumption for describing the regularity of the time points .",
    "the asymptotic results ( theorems [ thm : consistency ] and [ thm : optimal_rate ] ) hold if instead of being randomly distributed , the time points form a fixed regular grid , say , with equal spacing .      as mentioned earlier , the estimation of @xmath141 is a nonlinear inverse problem since @xmath142 is not directly observable .",
    "in addition , this is also an ill - posed estimation problem .",
    "let @xmath143 be the partial derivative of @xmath144 with respect to @xmath130 , where @xmath145 is the solution of ( [ eq : initial_beta_delta ] ) with @xmath146 .",
    "let @xmath147 be as in * a2*. define @xmath148 where the expectation is with respect to the distribution of @xmath149 .",
    "clearly @xmath150 is a positive semi - definite matrix .",
    "it becomes clear from the analysis carried out later that the degree of ill - posedness of the estimation problem is determined by the size of the operator norm of the matrix @xmath151 as a function of @xmath35 .",
    "the following proposition gives a precise quantification of the degree of ill - posedness .",
    "the situation here is in contrast with standard nonparametric function estimation problems where the corresponding matrix is well - conditioned .",
    "[ prop : kappa ] assume that assumptions * a1 * to * a3 * hold with @xmath80 .",
    "assume further that ( a ) @xmath152 ( a.s . ) and",
    "( b ) @xmath153 . then ( a.s . )",
    "@xmath154    by lemma [ lem : consistency_x_j_hat ] and the discussion that follows , under the condition of theorem [ thm : consistency ] , ( a ) and ( b ) of proposition [ prop : kappa ] hold .",
    "we now state the main result on the consistency of the estimate @xmath68 .",
    "[ thm : consistency ] suppose that the observed data @xmath155 follow the model described by equations ( [ eq : basic ] ) and ( [ eq : data_model ] ) and that assumptions * a1**a4 * are satisfied with @xmath156 .",
    "suppose further that the sequence @xmath35 is such that @xmath157 for some @xmath158 , @xmath121 , and @xmath119 be as defined in lemma [ lem : consistency_x_j_hat ] .",
    "let @xmath159 for some @xmath160 ( sufficiently small ) and @xmath161 for some @xmath162 .",
    "then as @xmath163 , with probability tending to one , there exists a local minimum @xmath164 of the objective function @xmath165 ( defined through ( [ eq : loss_modified ] ) ) , which is also a global minimum within radius @xmath166 of @xmath167 ( note that , @xmath168 by ( [ eq : m_alpha_optimal ] ) ) such that , with @xmath169 , @xmath170    the proof of theorem [ thm : consistency ] is given in section [ sec : proofs ] .",
    "[ rem : consistency ] assuming @xmath171 to be a constant , if @xmath35 is chosen to be of the order @xmath172 , then @xmath173 in ( [ eq : consistency ] ) simplifies to @xmath174 , which is within a factor of @xmath175 of the optimal rate in terms of the @xmath0-loss for estimating @xmath142 based on the data @xmath155 given by ( [ eq : basic ] ) when @xmath176)$ ] .",
    "the fact that an estimator of @xmath4 can attain this rate can be anticipated from the representation of @xmath4 as @xmath3 . for @xmath76",
    ", we can improve the rate of convergence of @xmath68 slightly further , by dropping the factor of @xmath175 , as stated in the following result .",
    "[ thm : optimal_rate ] suppose that the conditions of theorem [ thm : consistency ] are satisfied with @xmath76 and , further , the sequence @xmath35 satisfies the condition that @xmath177 for some @xmath178 .",
    "let @xmath68 be as in theorem [ thm : consistency ] .",
    "then , @xmath179 with the optimal rate given by @xmath180 , which is obtained when @xmath181 for some @xmath178 .",
    "proof of theorem [ thm : optimal_rate ] is given in section [ subsec : optimal_rate ] of sm .",
    "we can also derive an approximate expression for the asymptotic variance of @xmath182 . using a consistent root @xmath164",
    ", we can use the equation @xmath183 . using the asymptotic representation of @xmath184 used in the proof of theorem [ thm : optimal_rate ] ( see section [ subsec : optimal_rate ] of sm ) , and ignoring higher order terms and the contribution of the model bias , and finally evaluating the expressions at @xmath164 instead of @xmath167 ( which is unknown )",
    ", we have @xmath185^{-1}.\\ ] ] here the estimated noise variance @xmath186 can be computed as the mean squared error @xmath187 . the expression ( [ eq : approx_asymptotic_variance ] )",
    "allows us to obtain an approximate asymptotic variance for @xmath188 by @xmath189 , for any given @xmath190 , where @xmath191 .      in theorem [ thm : consistency ]",
    "we prove the rate of convergence for a local minimizer , which is a global minimizer within a radius of @xmath192 of @xmath167 for a suitable range of values of @xmath35 .",
    "therefore , we need an initial estimate which resides within this domain . in the following ,",
    "we describe one way of obtaining such an initial estimate , through a two - stage approach , which is similar in spirit to the approaches by chen and wu ( 2008a , 2008b ) .",
    "suppose that we first estimate @xmath1 and @xmath142 by local polynomial smoothing and denote these estimates by @xmath193 and @xmath194 .",
    "then , we fit the regression model @xmath195 by ordinary least squares , where @xmath196 .",
    "we refer to the resulting estimator @xmath197 as the two - stage estimator of @xmath34 : @xmath198}(t_j)]^{-1 } ( \\sum_{j=1}^n \\hat x'(t_j ) \\bs\\phi(\\hat x(t_j ) ) \\mathbf{1}_{[\\delta,1-\\delta]}(t_j)).\\ ] ] since @xmath1 is @xmath8 times continuously differentiable and @xmath142 is @xmath6-times continuously differentiable ( by * a1 * ) , and @xmath199 is sub - gaussian , with the optimal choice of bandwidths , we have @xmath200}(t_j ) & = & o((\\sigma_\\varepsilon^2/n)^{(p+1)/(2p+3)}\\sqrt{\\log n } ) \\label{eq : rate_local_polynomial}\\\\ \\max_{1\\leq",
    "j \\leq n } |\\hat x'(t_j ) - x'(t_j)| \\mathbf{1}_{[\\delta,1-\\delta]}(t_j ) & = & o((\\sigma_\\varepsilon^2/n)^{p/(2p+3)}\\sqrt{\\log n } ) \\label{eq : rate_local_polynomial_deriv}\\end{aligned}\\ ] ] with probability tending to 1 .",
    "we state the following result about the rate of convergence of the two - stage estimator . the proof is given in section [ subsec : proofs_two_stage ] of sm .",
    "[ prop : two_stage_regression ] suppose that @xmath201 and * a1**a4 * hold and that the two - stage estimate of @xmath4 is given by @xmath202 where @xmath197 is defined in ( [ eq : beta_tilde_two_stage ] ) .",
    "then , supposing that @xmath203 , with probability tending to 1 , @xmath204 where @xmath205    when @xmath206 , the optimal value of @xmath207 is of the order @xmath208 is obtained when @xmath209 . it can be checked that for all @xmath80 , this rate is slower than the optimal @xmath210 for the nonlinear regression - based estimator @xmath68 derived in theorem [ thm : consistency ] .",
    "however , the rate of convergence of this estimator is faster than @xmath211 if @xmath212 .",
    "so , for these range of @xmath35 , which includes @xmath213 , the two - stage estimator resides within the ball of radius @xmath214 around @xmath167 , over which @xmath68 , the optimizer of ( [ eq : loss_modified ] ) , is a global optimum .",
    "in this section , we outline the main steps of the proof . some technical details are deferred to the appendix .    the main idea behind the proof of theorem [ thm : consistency ] is to obtain a lower bound on the difference @xmath215 which is proportional @xmath216 when @xmath34 lies in an annular region around @xmath167 .",
    "the outer radius of the annular region depends on the degree of ill - conditioning of the problem , as quantified by proposition [ prop : kappa ] , and the smoothness of the function @xmath4 and the approximating bases , as indicated in condition * a2*. this lower bound then naturally leads to the conclusion about the existence and rate of convergence of a local minimizer @xmath68 .      for convenience of notations",
    ", we define @xmath217 to be the sample path @xmath218 .",
    "since @xmath219 is given by ( [ eq : tilde_x_beta_closed ] ) in the appendix , @xmath220 in order to prove proposition [ prop : kappa ] , it suffices to find a lower bound on @xmath221^{2}\\tilde f_t(t ) dt\\ ] ] where @xmath222 with @xmath223 . by * a3 * , without loss of generality , we can take the density @xmath224 to be uniform on @xmath136 $ ] .",
    "we make use of the following result known as halperin - pitt inequality ( mitrinovic _ et al .",
    "_ , 1991 ) .",
    "[ lemma : halperin_pitt ] if @xmath225 is locally absolutely continuous and @xmath226 is in @xmath227)$ ] , then for any @xmath228 the following inequality holds with @xmath229 , @xmath230    now defining @xmath231 we have , @xmath232 x_*'(t)\\\\ & = & \\left[\\frac{g_{\\mathbf{b}}'(x_*(t))}{g_{\\bs{\\beta}^*}(x_*(t ) ) } - \\frac{g_{\\mathbf{b}}(x_*(t))g_{\\bs{\\beta}^*}'(x_*(t))}{g_{\\bs{\\beta}^*}^2(x_*(t))}\\right ] g_{\\bs{\\beta}^*}(x_*(t)).\\end{aligned}\\ ] ]    by ( vi ) of * a2 * , we have @xmath93 } |x_g(t ) - x_*(t)| = o(m^{-(p+1)})$ ] and hence @xmath233 hence , using the facts that the coordinates of @xmath234 are @xmath235 and the coordinates of @xmath236 are @xmath237 , and all these functions are supported on intervals of length @xmath85 , we deduce that , @xmath238 an application of lemma [ lemma : halperin_pitt ] with @xmath239 and @xmath240 yields @xmath241 take @xmath242 for some @xmath243 , then by ( [ eq : r_prime_int ] ) , @xmath244 for constants @xmath245 dependent on @xmath246 .",
    "next , we write @xmath247 where @xmath248 which is bounded below by a positive constant on the interval @xmath249 $ ] .    observe that by ( [ eq : x_star_boundary ] ) , the combined support of @xmath63 , viz .",
    ", @xmath127 $ ] , contains ( for sufficiently large @xmath35 ) the interval @xmath250 $ ] . also , @xmath251 and @xmath252 } h(v)\\right)~ \\mathbf{b}^t [ \\int_{x_{0,m}}^{x_{1,m } } \\bs\\phi(v ) ( \\bs\\phi(v))^t dv - o(1)]\\mathbf{b } ~\\geq~ k_3,\\end{aligned}\\ ] ] for some constant @xmath253 , for sufficiently large @xmath35 .",
    "thus , by appropriate choice of @xmath254 , we have @xmath255 for some constant @xmath256 , which yields ( [ eq : g_star_condition ] ) .        from ( [ eq : diff_l_delta_beta ] ) and ( [ eq : diff_l_l_tilde_delta_beta ] ) , we deduce that @xmath271 where @xmath272}(t_j ) \\\\ & & \\hskip-.2 in -\\frac{2}{n } \\sum_{j=1}^n ( x_g(t_j ) - x(t_j;\\bs\\beta^*))(x(t_j;\\bs\\beta^*;\\widehat x_0 ) - x(t_j;\\bs\\beta^ * ) ) \\mathbf{1}_{[\\delta,1-\\delta]}(t_j )   \\nonumber\\\\ u_{4n}(\\bs\\beta,\\bs\\beta^ * ) & \\hskip-.1in= & \\hskip-.1in\\frac{2}{n } \\sum_{j=1}^n ( x(t_j;\\bs\\beta ) - x(t_j;\\bs\\beta^*))(x(t_j;\\bs\\beta;\\widehat x_0)-x(t_j;\\bs\\beta ) ) \\mathbf{1}_{[\\delta,1-\\delta]}(t_j).\\end{aligned}\\ ] ] using the fact that @xmath273 can be expressed as @xmath274,\\ ] ] provided @xmath275 for @xmath276 $ ] , we have , for all @xmath260 , @xmath277 } \\sup_{t \\in [ \\delta,1-\\delta ] } |x(t;\\bs\\beta , a_0 ) - x(t;\\bs\\beta , x_{0,\\delta})| \\leq c_1\\xi_n\\ ] ] for some @xmath278 . here , we have used the fact that for @xmath129 $ ] , and @xmath279 $ ] , @xmath280 and that @xmath281}| g_{\\bs\\beta}(x ) - g_{\\bs\\beta^*}(x)| = o(\\bar\\alpha_n m^{1/2 } ) = o(m^{-3/2}),\\ ] ] so that , by using ( [ eq : x_diff_bound ] ) , and the fact that @xmath282 , @xmath283 } x(1-\\delta;\\bs\\beta , a_0 ) ]   \\subset [ x_{0,m},x_{1,m}]\\ ] ] for large enough @xmath35 and @xmath52 .",
    "we now bound individual terms in the expansion ( [ eq : diff_l_tilde_delta_beta_expand ] ) .",
    "first , we have the following lower bound on @xmath284 , the proof of which is given in appendix c.    [ lemma : gamma_beta_beta_star_bound ] let @xmath284 be as defined in ( [ eq : gamma_beta_beta_star_def ] ) .",
    "then given @xmath115 , there exist constants @xmath285 and @xmath286 independent of @xmath287 such that @xmath288 uniformly in @xmath260 with probability at least @xmath118 .    since @xmath289 , and the constant @xmath290",
    "can be chosen to be small enough so that we can conclude from ( [ eq : gamma_beta_beta_star_bound ] ) that given @xmath115 , there exists @xmath291 such that @xmath292 next , by cauchy - schwarz inequality , we have @xmath293 ) , we have @xmath294 and hence @xmath295 and @xmath296}(t_j ) } { \\sigma_\\varepsilon \\sqrt{\\sum_{j=1}^n ( x(t_j;\\bs\\beta ) - x(t_j;\\bs\\beta^*))^2\\mathbf{1}_{[\\delta,1-\\delta]}(t_j)}}~,\\ ] ] and setting @xmath297 being zero if the denominator is zero , we have @xmath298 be a @xmath299-net for @xmath300 .",
    "then @xmath301 . then , by using lemma [ lem : subgaussian_hoeffding ] in sm , and ( [ eq : gamma_beta_beta_star_bound_refined ] ) , we conclude that given @xmath115 , there exist constants @xmath302 , and a set @xmath303 with @xmath304 , such that for all @xmath305 , @xmath306 for some constant @xmath307 .",
    "thus , taking @xmath40 to be sufficiently small , say , @xmath308 for @xmath309 large enough , and using the smoothness of the process @xmath297 as a function of @xmath34 , we can show that given any @xmath115 , there exists @xmath310 , such that for all @xmath305 , @xmath311 very similarly , defining @xmath312}(t_j ) } { \\sigma_\\varepsilon \\sqrt{\\sum_{j=1}^n ( x(t_j;\\bs\\beta;\\widehat x_0)-x(t_j;\\bs\\beta))^2\\mathbf{1}_{[\\delta,1-\\delta]}(t_j)}},\\ ] ] expressing @xmath313 , and using ( [ eq : v_1n_bound ] ) , we have , for any given @xmath115 , there exists @xmath314 and a set @xmath315 with @xmath316 , such that for all @xmath317 , @xmath318 finally , by * a2 * we have the bound @xmath319 } |x_g(t ) - x(t;\\bs\\beta^*)|^2 \\leq c_2 m^{-2(p+1)}\\ ] ] for some @xmath320 .    combining ( [ eq : u_2n_bound])([eq : d_n_star_bound ] )",
    ", we claim that , given @xmath115 , there exist constants @xmath321 , @xmath322 , and constants @xmath323 , @xmath324 , not depending on @xmath287 , such that uniformly on @xmath300 @xmath325 with probability at least @xmath326 .    from ( [ eq : l_tilde_beta_diff_bound ] ) and ( [ eq : gamma_beta_beta_star_bound_refined ] ) , and a careful choice of the constant @xmath327 in the definition ( [ eq : alpha_n_def ] ) of @xmath210 , and with @xmath35 as in [ eq : m_alpha_optimal ] , we conclude that for any @xmath115 , there exists @xmath328 such that , uniformly in @xmath260 , @xmath329 with probability at least @xmath326 . from this , we can conclude that with probability at least @xmath326 there exists a local minimum @xmath164 of @xmath165 , which is also a global minimum within radius @xmath166 of @xmath167 and which satisfies @xmath330 .",
    "in this section , we conduct a simulation study to examine the finite sample performance of the proposed estimation procedure , as well as to compare it with the two - stage estimator described in section [ subsec : two_stage_regerssion ] .    in the simulation , the true gradient function @xmath4 is represented by @xmath331 b - spline functions with knots at @xmath332 and respective coefficients @xmath333 ( shown by the blue curve in figure [ fig : simu ] ) .",
    "we set the initial value @xmath334 in equation ( [ eq : basic ] ) to generate the true trajectory @xmath335 .",
    "we then simulate @xmath336 independent data sets according to equation ( [ eq : data_model ] ) .",
    "specifically , for each data set , we first randomly choose an integer @xmath52 from @xmath337 . then @xmath52 observation times @xmath338",
    "are uniformly sampled from @xmath101 $ ] .",
    "finally , the @xmath17 s are generated according to equation ( [ eq : data_model ] ) with added noise @xmath339 .",
    "the observed data from one such replicate is shown in figure [ fig : obs_trajectory ] in sm together with the true trajectory @xmath335 .",
    "we fit the proposed estimator @xmath340 with @xmath35 b - spline basis functions with equally spaced knots on @xmath341 $ ] .",
    "we consider @xmath342 and choose @xmath35 by an approximate leave - one - out cv score criterion similar to that used in paul et al .",
    "( 2011 ) . out of the @xmath336 replicates , @xmath343 times the model with @xmath344 ( the true model ) is chosen and @xmath345 times the model with @xmath346 is chosen .",
    "we also consider the two - stage estimator , where in the first stage , the sample trajectory @xmath335 and its derivative @xmath347 are estimated by applying local linear and local quadratic smoothing with gaussian kernel , respectively , to the observed data @xmath348 .",
    "the bandwidths are chosen by cross - validation . in the second stage ,",
    "a quadratic smoothing of @xmath349 versus @xmath350 is performed to get an estimate of @xmath141 .",
    "figure [ fig : simu ] shows the estimated gradient functions ( red curves ) of these @xmath336 independent replicates overlayed on the true gradient function ( blue curve ) .",
    "it can be seen from this figure that , the proposed estimator shows little bias .",
    "its sampling variability is somewhat larger on the left side of the observed @xmath190 domain than on the right side of the observed @xmath190 domain .",
    "it performs much better than the two - stage estimator which shows both high bias and high variance .",
    "indeed , the bias of the two - stage estimator would not go away even when in the second stage the true model is used to estimate @xmath4 ( through a least - squares regression of @xmath349 versus @xmath350 ) .",
    "figure [ fig : simu_traj ] shows the estimated trajectories ( red curves ) of these @xmath336 independent replicates overlayed on the true trajectory ( blue curve ) . in the left panel of the figure ,",
    "the estimated trajectories are solved from equation ( [ eq : basic ] ) using the 4th - order runge - kutta method with @xmath4 being the proposed estimator @xmath340 . in the right panel of the figure ,",
    "the trajectories are estimated by applying local linear smoothing of the observed data ( which are then used in the two - stage fitting for @xmath141 ) .",
    "the estimated trajectories from the proposed procedure follow the true trajectory very well with little bias , whereas the estimator from the first - stage smoothing of the two - stage procedure shows more bias and more variability .",
    "figure [ fig : simu_traj_deriv ] in sm shows the estimated derivative of the trajectory .",
    "again , the proposed procedure gives a much better estimate of @xmath351 than the presmoothing estimate ( by local quadratic smoothing ) used in the two - stage procedure .",
    "in this section , we apply the proposed model to the berkeley growth data ( tuddenham and snyder , 1954 ) .",
    "although in the literature , there are many studies of growth curves ( hauspie et al . , 1980 ; milani , 2000 ) , most of them try to model either the growth trajectories ( i.e. , @xmath335 ) or the rate of growth ( i.e. , @xmath347 ) . on the contrary",
    ", our goal is to estimate the gradient function , i.e. , the functional relationship between @xmath347 and @xmath335 which provides insights of the growth dynamics , such as at what height the growth rate tends to be the highest .    specifically , we fit the proposed model to each of the @xmath352 female subjects in this data set . for each girl ,",
    "her heights were measured at @xmath353 time points from @xmath354 year old to @xmath355 years old .",
    "we use @xmath35 b - spline basis functions with equally spaced knots .",
    "we consider @xmath356 and for each subject we choose the `` best '' @xmath35 using an approximate leave - one - out cv score . in @xmath357 out of @xmath352 subjects , the model with @xmath358 is chosen , and for the rest @xmath359 subjects , the model with @xmath360 is chosen .",
    "figure [ fig : fit_gradient ] shows the fitted gradient functions for these @xmath352 subjects . from this figure",
    ", we can see that , most girls experienced two growth spurs , one at the birth ( when their heights are shortest ) and another when they were around either @xmath361 cm tall or @xmath362 cm tall",
    ". moreover figure [ fig : fit_gradient_se ] in sm shows the fitted gradient functions with the two - standard - error bands ( by equation ( [ eq : approx_asymptotic_variance ] ) ) for @xmath363 girls .",
    "figure [ fig : fit_trajectory ] in sm shows the observed ( red dots ) and fitted ( black curve ) growth trajectories for these @xmath363 girls .",
    "it can be seen that , the fitted trajectories fit the observed data very well .",
    "in this paper we have proposed an estimation procedure for nonparametrically estimating the unknown gradient function of a first order autonomous differential equation over a finite domain , when the trajectories are strictly monotone . in this section ,",
    "we discuss the asymptotic rate optimality of the proposed estimator .",
    "we show that , if the estimators of the gradient function @xmath4 are restricted to a class of uniformly lipschitz function , the optimal rate for estimation of @xmath4 , i.e. , of the order @xmath364 , is the same as the optimal rate for estimation of the derivative of @xmath365 based on model ( [ eq : data_model ] ) in terms of the @xmath0 loss .",
    "we conjecture that the lipschitz requirement on the estimator of @xmath4 is not necessary and the minimax rate for estimation of @xmath4 is indeed of the order @xmath364 .    in order to make this statement precise , we first specify the function class for @xmath4 as @xmath366 where @xmath367 and @xmath368 are constants . define the class of uniformly lipschitz functions @xmath369 where @xmath370 depends on ( at least as large as ) @xmath138 in ( [ eq : g_class ] ) . if @xmath371 , then we have @xmath372)$ ] and @xmath373)$ ] . in addition , we assume the observation model ( [ eq : data_model ] ) with the noise @xmath374 .",
    "let @xmath40 be as in section [ sec : model ] . by the condition @xmath375 , we know that there exist @xmath376 such that @xmath377 for all @xmath129 $ ] , for all @xmath371 .",
    "define , @xmath378 .",
    "then there are constants @xmath379 such that for any given estimator @xmath380 of @xmath4 , @xmath381 observe that @xmath382 .    on the other hand , since @xmath372)$",
    "] , there exists an estimator @xmath383 with the property that , given @xmath384 , there exists constant @xmath385 such that @xmath386 for all @xmath387 .",
    "we define the estimator @xmath388 for @xmath389 .",
    "then , by triangle inequality , @xmath390 where , in the last step we have used the fact that @xmath380 .    since @xmath373)$ ] , the minimax rate of estimation of @xmath389 in terms of the @xmath0 loss @xmath391 is of the order @xmath364 .",
    "this can be derived directly for @xmath4 restricted to @xmath392 by only slightly modifying the arguments in stone ( 1982 ) .",
    "combining this fact with ( [ eq : g_hat_error_bound ] ) , ( [ eq : x_g_minimax ] ) and ( [ eq : x_deiv_estimate_bound ] ) , we obtain that there exists @xmath393 , such that @xmath394 in other words , as long as @xmath68 is uniformly lipschitz , the rate @xmath364 is a lower bound on the rate for estimating @xmath4 in terms of the @xmath0-loss .",
    "we note that , the requirement @xmath380 can be relaxed by only requiring that this holds with probability approaching one as @xmath124 .",
    "the latter is satisfied by the estimator we proposed .",
    "thus , combining with theorem [ thm : optimal_rate ] , we deduce that the optimal rate of estimation of @xmath4 is @xmath364 for @xmath76 .",
    "in this section , we provide technical details for the proofs of the main results .",
    "specifically , in appendix a , we present results on perturbation analysis of differential equations that are central to controlling the bias in the estimates . in appendix b",
    ", we verify that condition ( vi ) of * a2 * is satisfied by a b - spline basis of sufficiently high order . in appendix c , we prove lemma [ lemma : gamma_beta_beta_star_bound ] .",
    "further technical details are given in the supplementary material .        since @xmath335 satisfies the ode @xmath395,\\ ] ] differentiating with respect to @xmath34 we obtain the the linear differential equations : @xmath396 for @xmath397 , where @xmath398 .",
    "the hessian of @xmath335 with respect to @xmath130 is given by the matrix @xmath399 , where @xmath400 , which satisfies the system of odes , for @xmath401 : @xmath402 , ~~{x}^{\\beta_r,\\beta_{r'}}(0)=0 .",
    "\\end{aligned}\\ ] ] with @xmath403 and @xmath404 denoting @xmath405 , we also have @xmath406    note that ( [ eq : tilde_x_beta ] ) , ( [ eq : tilde_x_beta_hessian ] ) and ( [ eq : tilde_x_a ] ) are linear differential equations .",
    "if the function @xmath407 is positive on the domain then the gradients of the trajectories can be solved explicitly as follows .",
    "@xmath408 @xmath409ds \\nonumber\\\\ & & ~+ g_{\\bs{\\beta}}(x(t ) ) \\int_0^t   \\frac{1}{g_{\\bs{\\beta}}(x(s ) ) } x^{\\beta_r}(s)x^{\\beta_{r'}}(s ) g_{\\bs{\\beta}}''(x(s ) ) ds .   \\end{aligned}\\ ] ] and",
    "@xmath410.\\ ] ]    now we summarize approximations of various relevant quantities . the following result on the perturbation of the solution path in an initial value problem due to a perturbation in the gradient function",
    "is derived from deuflhard and bornemann ( 2002 ) .",
    "[ prop : perturb ] consider the initial value problem : @xmath411 where @xmath412 . on the augmented phase space @xmath413 ,",
    "say , let the mappings @xmath225 and @xmath414 be continuous and continuously differentiable with respect to the state variable .",
    "assume that for @xmath415 , the initial value problem ( [ eq : ode_general ] ) , and the perturbed problem @xmath416 have the solutions @xmath190 and @xmath417 , respectively . if @xmath225 is such that @xmath418 for a function @xmath419 bounded on @xmath420 $ ] , and @xmath421 for some nonnegative function @xmath422 on @xmath420 $ ] , then @xmath423.\\ ] ]    we use the above result to compute bounds for the trajectories and their derivatives corresponding to the different values of the parameter @xmath34 in a neighborhood of the point @xmath167 . in order to keep the exposition simple",
    ", we assume that @xmath424 for @xmath425 and @xmath426 for @xmath427 with a differentiability requirement at the points @xmath428 and @xmath429 .",
    "our aim is to show that the range of the trajectories @xmath430 is contained in the set @xmath125 $ ] , for all @xmath129 $ ] and for all @xmath431 .",
    "let @xmath432 .",
    "also , let @xmath433 .",
    "as in the proof of proposition [ prop : kappa ] , we can easily show that @xmath72 \\subset [ x_{0,m},x_{1,m}]$ ] for sufficiently large @xmath35 .",
    "on the other hand , by using the perturbation bound given by proposition [ prop : perturb ] progressively over small subintervals of the interval @xmath136 $ ] , it can be shown that @xmath434}|x(t;\\bs\\beta , x_{0,\\delta } ) - x_g(t;x_{0,\\delta})| \\leq c_1 \\gamma_n + c_2 \\xi_n,\\ ] ] for appropriate positive constants @xmath435 that depend on the value of @xmath4 and @xmath436 on the interval @xmath25 $ ] .",
    "now , using lemma [ lem : consistency_x_j_hat ] , the condition on @xmath210 as given in theorem [ thm : consistency ] , and the definitions of @xmath60 , @xmath62 and @xmath437 , for @xmath61 , we conclude that for large enough @xmath35 , the range of @xmath430 is contained in @xmath126 for all @xmath438 $ ] and for all @xmath439 .",
    "the scenario is depicted in figure [ fig : trajectory_envelop ] , where the dashed curves indicate the envelop of the trajectories @xmath430 , while the solid curve indicates the trajectory @xmath263 .",
    "next , we provide bounds for trajectories and their derivatives . in the following , @xmath440 is used to denote the @xmath441-norm over @xmath442 $ ] .",
    "first , by * a2 * we have the following : @xmath443 where @xmath444 and @xmath445 denote the @xmath446-th derivative of @xmath4 and @xmath447 , respectively .",
    "next , again from * a2 * , for @xmath35 large enough , solutions @xmath448\\}$ ] exist for all @xmath130 such that @xmath449 .",
    "this also implies that the solutions @xmath450 and @xmath451 exist on @xmath136 $ ] for all @xmath130 such that @xmath449 , since they follow linear differential equations where the coefficient functions depend on @xmath452 .",
    "moreover , by _",
    "gronwall s lemma _",
    "( deuflhard and bornemann , 2002 ) , ( [ eq : g_beta_estimates ] ) and the fact that @xmath453 for @xmath88 ( again by * a2 * ) .    hence , if @xmath454 , then using proposition [ prop : perturb ] , the fact that @xmath455 for @xmath88 , and the expressions for the odes for the partial derivatives , we obtain ( almost surely ) : @xmath456 the same technique can be used to prove the following : @xmath457 whenever @xmath458 .    to illustrate the key arguments , we prove ( [ eq : x_beta_bound ] ) and ( [ eq : x_diff_beta_bound ] ) .",
    "first , ( [ eq : x_beta_bound ] ) follows by ( [ eq : tilde_x_beta_closed ] ) , and the fact that @xmath459 and is supported on an interval of length @xmath85 . in fact it holds for all @xmath34 such that @xmath458 . next , note that the function @xmath460 is lipschitz with lipschitz constant @xmath237 and is supported on an interval of length @xmath85 . since ( [ eq : tilde_x_beta ] ) is a linear differential equation , using proposition [ prop : perturb ] with @xmath461 given by @xmath462 + \\phi_{r}(x(t;\\bs{\\beta } ) ) - \\phi_{r}(x(t;\\bs{\\beta}^*))\\end{aligned}\\ ] ] we obtain ( [ eq : x_diff_beta_bound ] ) by using ( [ eq : x_diff_bound ] ) and the following facts : @xmath463 } |x^{\\beta_r}(t;\\bs\\beta)| = o(m^{-1/2})$ ] for all @xmath464 ; @xmath465 ; @xmath466 ; and @xmath467 .      in this subsection",
    ", we verify that the condition ( vi ) of * a2 * is satisfied if @xmath63 is a normalized b - spline basis with equally spaced knots on @xmath127 $ ] and of order @xmath468 . in particular , we show that the rate of approximation of @xmath1 by @xmath469 with a carefully chosen @xmath470 satisfies the requirement that @xmath93 } |x(t ) - x(t;\\bs\\beta^*)| = o(m^{-(p+1)})$ ] and the conditions @xmath471 } |g^{(j)}(x ) - g_{\\bs\\beta^*}^{(j)}(x)| = o(m^{-p+j})$ ] for @xmath88 . the result is proved through the following lemmas proved in sm .",
    "[ lemma : path_bound_refined ] suppose that @xmath63 has combined support @xmath472 = [ x(\\delta),x(1-\\delta)]$ ] and satisfies ( ii)(v ) of * a2 * and @xmath167 furthermore has the property that @xmath473 }",
    "\\left|\\int_{x_{0,\\delta}}^{x } \\frac{g(u ) - g_{\\bs\\beta^*}(u)}{g(u ) } du   \\right| = a_m\\ ] ] such that @xmath474 , uniformly in @xmath35 , for some @xmath475 $ ] and some @xmath476 .",
    "then , if @xmath477 , there exists @xmath122 such that @xmath478 } |x(t)-x(t;\\bs\\beta^*)|   \\leq c a_m.\\ ] ]    [ lemma : integral_g_spline_approx ] suppose that * a1 * holds with @xmath479 .",
    "let @xmath63 denotes the normalized b - spline basis of order @xmath480 with equally spaced knots on the interval @xmath127 $ ] .",
    "then there exists a @xmath481 such that @xmath482 satisfies @xmath483 } \\left|\\int_{x_{0,\\delta}}^{x } \\frac{g(u ) - g_{\\bs\\beta^*}(u)}{g(u ) } du   \\right| = o(m^{-(p+1)}).\\ ] ]      by a taylor expansion we have , for @xmath484 , @xmath485 where @xmath486 for all @xmath446 . from this",
    ", it follows that , for all @xmath260 , @xmath487}(t_j)\\right ] ( \\bs\\beta - \\bs\\beta^ * ) \\nonumber\\\\ & & - 3 \\parallel \\bs\\beta - \\bs\\beta^*\\parallel^2 \\frac{1}{n } \\sum_{j=1}^n \\parallel x^{\\bs\\beta}(t_j;\\tilde{\\bs\\beta}(t_j))- x^{\\bs\\beta}(t_j;\\bs\\beta^*)\\parallel^2 \\mathbf{1}_{[\\delta,1-\\delta]}(t_j),\\end{aligned}\\ ] ] where we have used @xmath488 .",
    "using proposition [ prop : kappa ] and lemma [ lem : g_gamma_quad_bound ] ( stated below ) we conclude , given @xmath115 , there exists @xmath489 such that , @xmath490}(t_j ) \\right ] ( \\bs\\beta - \\bs\\beta^ * ) \\\\ & \\geq & c_{10}(\\eta ) \\frac{1}{m^2 } \\parallel \\bs\\beta - \\bs\\beta^*\\parallel^2\\end{aligned}\\ ] ] for all @xmath260 , with probability at least @xmath118 .",
    "now , another application of the mean value theorem yields that for @xmath491 $ ] , @xmath492 where @xmath493 denotes the frobenius norm , and @xmath494 for all @xmath495 and @xmath496 .",
    "now , using ( [ eq : x_beta_beta_bound ] ) and ( [ eq : x_diff_beta_beta_bound ] ) , and combining the last three displays , we get ( [ eq : gamma_beta_beta_star_bound ] ) .    [",
    "lem : g_gamma_quad_bound ] suppose that * a1**a4 * hold .",
    "let @xmath497}(t_j).\\ ] ] then , given @xmath115 , there exists constants @xmath498 such that , with probability @xmath118 , uniformly in @xmath499 , @xmath500      let @xmath501 .",
    "define @xmath502 .",
    "notice that @xmath503}(t_j ) ] = \\mathbb{e}_{\\tilde t}[\\mathbf{v}_j\\mathbf{v}_j^t ] = g_*,\\ ] ] where the first expectation is with respect to the distribution of @xmath504 and the second with respect to that of @xmath149 .",
    "hence , we can write @xmath505 where @xmath506}(t_j)}{f_t(1-\\delta)-f_t(\\delta ) } - \\mathbb{e}_{\\tilde t}[\\mathbf{v}_j\\mathbf{v}_j^t]\\right)\\bs\\gamma.\\ ] ] note that , the random variables @xmath507 have zero conditional mean , are uniformly bounded , and are independent .",
    "moreover , the functions @xmath507 are differentiable functions of @xmath508 . then , since by ( [ eq : x_beta_bound ] ) , @xmath507 s are uniformly bounded by some @xmath509 , @xmath510 & \\leq & k_1 \\sum_{j=1}^{n}\\mathbb{e}|u_j(\\bs\\gamma)| ~\\leq~ 2k_1 n \\bs\\gamma^t g _ * \\bs\\gamma .\\end{aligned}\\ ] ] thus , by bernstein s inequality , for every @xmath511 and @xmath512 , @xmath513 on the other hand , by ( [ eq : g_star_condition ] ) , @xmath514 for some @xmath178 . by this , and the condition that @xmath515 , it is easy to see that @xmath516 .",
    "thus , using an entropy argument as in the proof of ( [ eq : z_beta_sup_bound ] ) , we conclude that given @xmath115 there exists @xmath517 such that @xmath518 recalling the definition of @xmath519 , and again using the fact that @xmath514 and @xmath515 , ( [ eq : g_gamma_quad_bound ] ) follows from ( [ eq : sum_u_bound ] ) .",
    "cavalier , l. , golubev , g. k. , lepskii , o. and tsybakov , a. b. ( 2004 ) .",
    "block thresholding and sharp adaptive estimation in severely ill - posed inverse problems .",
    "_ theory probability application _ , * 48 * , 426-446 .",
    "chen , j. and wu , h. ( 2008b ) .",
    "efficient local estimation for time - varying coefficients in deterministic dynamic models with applications to hiv-1 dynamics . _ journal of american statistical association _ * 103 * , 369384 .",
    "hauspie , r. c. , wachholder , a. , baron , g. , cantraine , f. , susanne , c. and graffar , m. ( 1980 ) . a comparative study of the fit of four different functions to longitudinal data of growth in height of belgian girls . _ annals of human biology _ , * 7*(4 ) , 347358 .",
    "liu , b. and mller , h .-",
    "( 2009 ) . estimating derivatives for samples of sparsely observed functions , with application to online auction dynamics .",
    "_ journal of the american statistical association _ ,",
    "* 104 * , 704716 .",
    "miao , h. , dykes , c. , demeter , l. m. and wu , h. ( 2009 ) .",
    "differential equation modeling of hiv viral fitness experiments : model identification , model selection , and multimodel inference .",
    "_ biometrics _ , * 65 * , 292300 .",
    "nicol , f. ( 2013 ) . functional principal component analysis of aircraft trajectories . _ isiatm 2013 , 2nd international conference on interdisciplinary science for innovative air traffic management , toulouse , france .",
    "_          poyton , a. a. , varziri , m. s. , mcauley , k. b. , mclellan , p. j. and ramsay , j. o. ( 2006 ) .",
    "parameter estimation in continuous dynamic models using principal differential analysis .",
    "_ computers & chemical engineering _ * 30 * , 698708 .",
    "qi , x. and zhao , h. ( 2010 ) .",
    "asymptotic efficiency and finite - sample properties of the generalized profiling estimation of parameters in ordinary differential equations .",
    "_ annals of statistics _ , * 38 * , 435481 .",
    "ramsay , j. o. , hooker , g. , campbell , d. and cao , j. ( 2007 ) .",
    "parameter estimation for differential equations : a generalized smoothing approach .",
    "_ journal of the royal statistical society , series b _ * 69 * , 741796 .",
    "wu , h. , lu , t. , xue , h. and liang , h. ( 2014 ) .",
    "sparse additive ordinary differential equations for dynamic gener regulatory network modeling .",
    "_ journal of the american statistical association _",
    ", * 109 * , 700716 .",
    "xue , h. , miao , h. and wu , h. ( 2010 ) .",
    "sieve estimation of constant and time - varying coefficients in nonlinear ordinary differential equation models by considering both numerical error and measurement error .",
    "_ annals of statistics _ , * 38 * , 23512387 ."
  ],
  "abstract_text": [
    "<S> we study a class of nonlinear nonparametric inverse problems . specifically </S>",
    "<S> , we propose a nonparametric estimator of the dynamics of a monotonically increasing trajectory defined on a finite time interval . under suitable regularity conditions , </S>",
    "<S> we prove consistency of the proposed estimator and show that in terms of @xmath0-loss , the optimal rate of convergence for the proposed estimator is the same as that for the estimation of the derivative of a trajectory . </S>",
    "<S> this is a new contribution to the area of nonlinear nonparametric inverse problems . </S>",
    "<S> we conduct a simulation study to examine the finite sample behavior of the proposed estimator and apply it to the berkeley growth data .    </S>",
    "<S> .1 in        .1 in    _ department of statistics , university of california , davis _    .1 in * keywords : * autonomous differential equation ; nonlinear inverse problem ; monotone trajectory ; nonparametric estimation ; perturbation theory ; spline </S>"
  ]
}