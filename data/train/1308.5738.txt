{
  "article_text": [
    "suppose we monitor @xmath0 independent data streams in a system , and at a given time @xmath1 we observe one real - valued observation @xmath2 from the @xmath3-th data stream for each @xmath4 without loss of generality , the @xmath5 s can be thought of as the residuals from some baseline spatio - temporal model instead of raw observations , and thus it is reasonable to assume that @xmath6 are initially independent and identically distributed ( i.i.d . ) with a known density @xmath7 for all @xmath8 at some unknown time @xmath9 the data streams are assumed to be affected simultaneously by an occurring event in the sense that for the k - th data stream , @xmath10 are i.i.d . with another density @xmath11 here",
    "we consider the parametric model in which the forms of the post - change densities are known but the post - change parameters @xmath12 s are unknown .",
    "two different scenarios will be studied : one is consensus detection where the unknown post - change parameters @xmath12 s can be treated as random deviations from a common ( unknown ) post - change value @xmath13 and the other is parallel detection in which only some ( unknown ) but not all @xmath14 s are different from the initial value @xmath15 in either scenario , the objective is to utilize the observed streaming data to raise an alarm as quickly as possible after the event occurs while controlling the system - wise false alarm rate .",
    "such a problem is often called _ sequential change - point detection _ , and has many important applications in practice such as surveillance , monitoring , and quality control .",
    "one specific motivating example is to monitor the outbreak of gastrointestinal disease in @xcite by observing multiple daily syndromic counts streams of visiting emergency departments of hospitals .",
    "note that our problem is simply monitoring a possible change in distribution for the @xmath0-dimensional data vector @xmath16 over time @xmath17 if all post - change parameters @xmath12 s were completely specified and the affected data streams were also known , then the problem is well understood .",
    "this is because both pre - change and post - change distributions of @xmath16 are known , and thus many efficient classical detection schemes for one - dimensional data stream ( see @xcite and @xcite ) , which are often based on the likelihood ratios of pre - change and post - change distributions , can be easily adapted for this case , as the density of one - dimensional data can easily be replaced by the density of the @xmath0-dimensional data @xmath18 for instance , the well - known shiryaev - roberts procedure ( @xcite and @xcite ) can be defined in our context as follows .",
    "let @xmath19 be the likelihood ratio statistic of all observations up to time @xmath20 in the problem of testing @xmath21 _ no change _ against @xmath22 _ a change occurs at time @xmath23 _ i.e. , @xmath24 then shiryaev - roberts procedure raises an alarm at time @xmath25 for some pre - specified threshold @xmath26    on the other hand , the problem becomes much more challenging if the post - change parameters @xmath12 s or the subset of affected data streams are unknown , which is often the case in practice . to develop efficient detection schemes , lorden and pollak @xcite proposed to replace unknown @xmath14 s in ( [ eqn001 ] ) with their estimators from the observed past data , thereby leading shiryaev - roberts - robbins - siegmund ( srrs ) detection scheme , as similar ideas have been used in @xcite for hypothesis testing problems .",
    "in order to estimate the post - change parameter @xmath12 s in the term @xmath27 of the likelihood ratio @xmath28 in ( [ eqn001 ] ) , it is important to note that this term corresponds to the scenario at time @xmath29 when one wants to test the alternative hypothesis that a change occurs at time @xmath30 hence , it is often ( but not always ) suggested to estimate @xmath12 in the term @xmath27 by using the @xmath31 observations @xmath32 which are i.i.d . with density @xmath33 under the alterative hypothesis .",
    "note that this estimation approach does not use the current observation @xmath34 in the estimation of @xmath35 and such an idea is similar to one - step - ahead forecasting . while this approach might lose some efficiency in estimating @xmath35 it has the benefit of only using @xmath34 once for testing the alternative hypothesis of a possible change at time @xmath36 so that the martingale structure of likelihood ratio @xmath37 in ( [ eqn001 ] ) is kept and thus the corresponding theoretical analysis is tractable .    to be more specific , denote by @xmath38 any reasonable estimator of @xmath12 from @xmath39 such as the method of moments estimator ( mom ) or maximum likelihood estimator ( mle ) .",
    "then the likelihood ratio @xmath19 in ( [ eqn001 ] ) can be estimated by @xmath40 and the srrs scheme proposed in @xcite is to raise an alarm at first time @xmath41 it was shown in @xcite that the srrs scheme in ( [ new_lr ] ) and ( [ eqn003 ] ) holds the first - order asymptotic optimality properties ( under some suitable criteria ) when @xmath38 in ( [ new_lr ] ) is the mom or mle estimator from @xmath42    in this paper , we propose to investigate the srrs scheme when the @xmath38 s in ( [ new_lr ] ) are the shrinkage estimators of @xmath12 s in the context of monitoring @xmath43 independent data streams .",
    "our main motivation is that in ( [ new_lr ] ) , one essentially faces the problem of simultaneously estimating @xmath0 unknown parameters @xmath12 s from the @xmath0-dimensional data , @xmath39 for @xmath44 since the pioneering work in @xcite , @xcite , it is well - known that shrinkage can improve the performance when analyzing high - dimensional data in the modern off - line statistical research such as point estimation and wavelet , see @xcite for the review and more references .",
    "the purpose of this paper is to demonstrate the usefulness of shrinkage in the context of sequential change - point detection or online monitoring of multiple or high - dimensional data streams .",
    "we should acknowledge that the shrinkage idea has been applied to the change - point detection problem in other contexts .",
    "for the off - line version , shrinkage is used in @xcite for estimating the location of change - point , and in @xcite for applying wavelet into nonparametric estimation for post - change density function . for the online version ,",
    "shrinkage was only implicitly used to develop the global monitoring schemes .",
    "for example , lai and shan @xcite studies the problem of detecting changes in state - space regression models , and xie and siegmund @xcite considers a semi - bayesian approach for monitoring multiple independent data streams .",
    "both papers proposed schemes that have shrinkage flavors on the detection statistics , but they are still using the mom or mle to estimate the post - change parameters . in this paper ,",
    "our application of shrinkage is different from these existing research as we apply shrinkage explicitly to estimate the post - change parameters simultaneously .",
    "the remaining of this paper is as follows . in section 2",
    ", we will review two specific shrinkage estimators in point estimation that will be used later in our paper : linear shrinkage and hard - thresholding .",
    "for ease of understanding , we first apply linear shrinkage in sections 3 and 4 for consensus detection of online monitoring changes in the means for @xmath0 independent normally distributed data streams : section 3 focuses on the simpler one - sided sequential hypothesis testing problem , and section 4 considers the sequential change - detection problem . in section 5 , we apply the hard - thresholding rule for parallel detection when the changes occur only to a subset of normally distributed data streams , and it turns out that the main ideas are similar , but it involves more technique details . in sections 6 and 7 , we further demonstrate the usefulness of shrinkage post - change estimations in other two contexts : ( i ) when monitoring data streams with other distributions such as poisson ; and ( ii ) when monitoring large - scale data streams and computationally simple schemes are desirable .",
    "numerical simulation results will be reported separately in different sections under different scenarios .",
    "the proofs of theorems [ cor : e tau b optimality p dim ] and [ thm : threshold ] , and proposition [ thm : meas q mun approx mu min gt 0 ] are included in the appendices .",
    "in this section we will review some well - known facts of shrinkage in ( offline ) point estimation for high - dimensional data , so that it is easier to understand how to apply the shrinkage idea in the context of online monitoring of @xmath0 data streams .",
    "suppose that there are @xmath45 independent random variables , say , @xmath46 and assume that @xmath47 follows normal distribution with unknown mean @xmath14 and known variance @xmath48 for @xmath49 suppose we are interested in estimating the @xmath0-dimensional mean vector @xmath50 and want to find a good estimator @xmath51 under the mean squared error criterion @xmath52    it is trivial to see that the mom or mle estimate of @xmath12 is @xmath53 for @xmath54 since each @xmath12 corresponds to only one normal variable @xmath55 to find a better estimator ( than mom or mle ) , one possibility is to consider the linear shrinkage estimators ( see @xcite ) of @xmath14 s defined by @xmath56 where @xmath57 is the shrinkage factor , and the @xmath58 s are some pre - specified real - valued constants . since @xmath59 it is easy to see that for the shrinkage estimator in ( [ eq : mujs ] ) , we have @xmath60 ^ 2 \\cr & = & c^2 p \\sigma^2 + ( 1-c)^2 \\sum_{k=1}^{p } ( \\omega_{k } - \\mu_{k})^2 ,   \\label{oracle_est}\\end{aligned}\\ ] ] as @xmath61 and @xmath62 in the context of point estimation , it is natural to find @xmath63 that minimizes the @xmath64 , and it is easy to see that such an optimal @xmath63 is given by @xmath65 which is often called the _ oracle shrinkage factor _ since it would need an _ oracle _ that tells us the value of the true means @xmath12 s .    while @xmath66 in ( [ oracle_point ] ) is infeasible in practice , it shed new light on simultaneous point estimation .",
    "for instance , since @xmath67 one can estimate the denominator in ( [ oracle_point ] ) by @xmath68 if we consider a general case by letting the numerator in ( [ oracle_point ] ) be any pre - specified values , then we have the shrinkage estimator ( [ eq : mujs ] ) with the shrinkage factor @xmath69 when @xmath70 this reduces to the mom or mle estimator .",
    "when @xmath71 the corresponding shrinkage estimator is the well - known james - stein estimator @xmath72 which was shown in @xcite to have a surprising property : @xmath73 has a uniformly smaller mean squared error than the mom or mle regardless of the true values of @xmath12 s when @xmath74 indeed , for the shrinkage factor @xmath75 the corresponding shrinkage estimator @xmath76 in ( [ eq : mujs ] ) satisfies @xmath77 \\sigma^2 { { \\bf e}}(\\frac{1}{\\sum_{k=1}^{p } ( \\omega_{k } - y_{k})^2}),\\ ] ] which is minimized at @xmath78 regardless of the true values of @xmath12 s .",
    "observe that the shrinkage estimator @xmath79 in ( [ eq : mujs ] ) has the common shrinkage factor @xmath63 for all @xmath8 while this works well when the true @xmath12 s have similar values , it might not be desirable in the sparsity scenario when only a ( small ) subset of @xmath14 s are not equal to @xmath80 in such scenario , it makes more sense to shrink some non - significant mom or mle estimators of @xmath14 s directly to @xmath80 this leads to another family of shrinkage estimators that are the projections of @xmath14 s : @xmath81 where the shrinkage factors @xmath82 s are either @xmath83 or @xmath84 now @xmath85 if @xmath86 and @xmath87 if @xmath88 hence the optimal choices of @xmath82 s are @xmath89 for @xmath54 and the corresponding optimal projection estimator has @xmath90 since @xmath91 involves the unknown @xmath92 it is natural to replace it by its estimator @xmath93 if we further extend the threshold @xmath94 in @xmath91 to any real - valued constant @xmath95 then we have the hard - thresholding estimator @xmath96 whose optimality properties were established in @xcite in the context of point estimation .",
    "due to the mathematical simplicity , let us first investigate the usefulness of linear shrinkage of the form ( [ eq : mujs ] ) in one - sided sequential probability ratio test ( sprt ) involving normal distributions , so that it is easier to understand the key ideas in the next section when we extend to the sequential change - point detection problem .",
    "assume we are monitoring @xmath0 independent data streams , and the observations from @xmath3-th data stream , @xmath97 are i.i.d . with density @xmath98",
    "suppose we are interested in testing @xmath99 for all @xmath100 against @xmath101 for at least one ( and likely more than one ) @xmath8 in the open - ended hypothesis testing problem , we want to take as many observations as possible ( even infinitely many ) when @xmath102 is true , but we want to take as few observations as possible when @xmath103 is true .",
    "in such a problem , a statistical hypothesis testing procedure can be defined as a stopping time @xmath104 where @xmath105 only depends on the observations up to time @xmath17 also the decision @xmath105 means that we stop at time @xmath20 to reject @xmath102 and claim that @xmath103 is true , whereas @xmath106 implies that we take infinitely many observations and declare that @xmath102 is true .",
    "denote by @xmath107 and @xmath108 the probability measure and expectation of @xmath109 @xmath110@xmath111 when @xmath112 is the true mean vector . to emphasize the null hypothesis when all @xmath113 we denote the same by @xmath114 and",
    "@xmath115 the one - sided hypothesis testing problem can then be formally formulated as finding a stopping time @xmath116 that asymptotically minimizes @xmath117 for all possible non - zero vectors @xmath118 , subject to the constraint on the type i error @xmath119 where @xmath120 is a pre - specified constant .    when @xmath121 is completely specified , the optimal solution is the well - known one - sided sprt proposed by @xcite .",
    "let @xmath122 be the likelihood ratio statistic based on first @xmath20 observations in testing @xmath102 against @xmath103 : @xmath123 the ( one - sided ) sprt is defined as a stopping time @xmath124 and one rejects @xmath102 at time @xmath125 whenever @xmath126    when @xmath121 is unknown , robbins and siegmund @xcite investigate such a problem for @xmath127 and their proposed procedure can be extended to the case @xmath128 in a straightforward way as follows . at time @xmath36",
    "let @xmath129 be an ( mom or mle ) estimator of @xmath14 based on @xmath130 , and then @xmath122 in ( [ eq : sprtlambda ] ) can be estimated by @xmath131 this leads to a new stopping time @xmath132 whose asymptotic properties were investigated in @xcite for @xmath133 when the @xmath129 is the mom or mle estimator of @xmath134    it is of great interest on how to find a good estimator @xmath129 of @xmath14 for @xmath44 for each individual @xmath135 the ( mom or ) mle estimate of @xmath14 is @xmath136 which can be thought of as @xmath137 in the context of point estimation in section 2 .",
    "however , since we are interested in making a global decision on which hypothesis is true , we actually face the problem of estimating all @xmath0 parameters @xmath12 s simultaneously .",
    "to highlight our main ideas , let us focus on consensus detection when the @xmath12 s have similar values under @xmath138 by ( [ eq : mujs ] ) , it is natural to consider the stopping time @xmath139 in ( [ eq : rands ] ) with the linear shrinkage estimators @xmath140 for @xmath141 and @xmath142 where @xmath143 $ ] is a shrinkage factor . here and below we assume @xmath144 and at least one of @xmath145 for online monitoring , @xmath58 can be chosen as the smallest meaningful difference to be detected for the @xmath3-th data stream , e.g. , one wants to reject @xmath102 with high probability only when @xmath146 for some @xmath8 in many statistical subfields such as the sample size calculation , the values of @xmath58 s are often pre - specified and chosen to be of practical significance .    in order to demonstrate the most significant impact of shrinkage ,",
    "let us first consider the alternative hypothesis @xmath103 and focus on the average sample number ( asn ) of @xmath147 in ( [ eq : rands ] ) with @xmath129 defined in ( [ for : mu estimate ] ) .",
    "to do so , define @xmath148 for @xmath44 then under the alternative hypothesis @xmath103 when the true probability measure is @xmath149 @xmath129 converges to @xmath150 almost surely as @xmath29 goes to @xmath151 let @xmath152 and for any @xmath153 it is useful to define the information number @xmath154 now two special cases of @xmath155 are related to the first- and second- order asymptotic expansions of @xmath156 the first one is when all @xmath157 @xmath158 [ c \\mu_{k } + ( 1-c ) \\omega_{k } - 2 \\mu_{k } ]   \\cr & = & \\frac{1}{2}[\\|\\mu \\|^2 - ( 1-c)^2 \\|\\mu - \\omega\\|^2],\\end{aligned}\\ ] ] where @xmath159 and @xmath160 the second case of @xmath155 in ( [ eqn05sec3-info ] ) is to consider its taylor series expansion of @xmath161 in the neighborhood of @xmath162 and for normal distributions , such expansion has a nice form .",
    "specifically , when @xmath163 @xmath164 now we need to consider the special case of ( [ eqn07sec3-info ] ) when @xmath165 for large @xmath166 recall that as @xmath29 goes to @xmath167 @xmath129 in ( [ for : mu estimate ] ) converges to @xmath168 and thus @xmath169 should be in the neighborhood of @xmath83 for large @xmath36 implying that ( [ eqn07sec3-info ] ) is applicable to these @xmath169 s .",
    "it is evident from ( [ for : mu estimate ] ) that @xmath170 and @xmath171 hence , by ( [ eqn07sec3-info ] ) , for @xmath172 @xmath173 the coefficient @xmath174 turns out to be the coefficient of the second - order term in the asymptotic expression of @xmath156 the main result can be summarized as follows .",
    "[ cor : e tau b optimality p dim ] assume that @xmath175 defined in ( [ eqn06sec3-info ] ) is positive , and consider the stopping time @xmath147 in ( [ eq : rands ] ) with @xmath129 defined in ( [ for : mu estimate ] ) . as @xmath176 @xmath177",
    "the detailed proof for theorem [ cor : e tau b optimality p dim ] is included in the appendix [ pf : thm1 ] .",
    "here we add several remarks to better understand the theorem .",
    "let us first discuss the condition @xmath178 this is necessary for the theorem , as it makes sure that @xmath147 in ( [ eq : rands ] ) with @xmath129 defined in ( [ for : mu estimate ] ) will stop with probability @xmath179 to reject @xmath102 when @xmath103 is indeed true .",
    "a sufficient condition to @xmath180 ( if @xmath181 ) is when @xmath57 and the true means @xmath182 for all @xmath3 and @xmath183 for some @xmath8    second , consider the first - order approximation of @xmath184 as @xmath185 goes to @xmath151 it is obvious from ( [ eq : asn ] ) that minimizing @xmath184 is asymptotically equivalent ( up to first - order ) to maximizing @xmath186 which is equivalent to minimizing @xmath187 hence , the first - order asymptotic optimality of @xmath147 can be attained at two difference cases : ( i ) @xmath188 for all @xmath189 or ( ii ) @xmath190 note that case ( i ) implies that all estimator @xmath129 s would have been shrunk to their true value @xmath12 , whereas case ( ii ) is exactly the procedure in @xcite , which is asymptotically optimal up to first - order .",
    "third , it is important to observe that when @xmath0 is large but @xmath185 is only moderately large , say comparable to @xmath191 then the second term @xmath192 in the numerator of ( [ eq : asn ] ) might be comparable to or even larger than the first term @xmath185 for large @xmath191 and thus a smaller value of @xmath63 might be able to significantly reduce this second term for the asn .",
    "the price we paid is to slightly increase @xmath193 the coefficient of the first - order term .",
    "indeed , the shrinkage factor @xmath63 can not be too small , due to the third term in the numerator of ( [ eq : asn ] ) and the role of @xmath63 in the denominator of ( [ eq : asn ] ) .",
    "last , similar to the oracle shrinkage factor in point estimation in section 2 , theorem [ cor : e tau b optimality p dim ] provides a natural way to find the oracle shrinkage factor asymptotically for online monitoring , and we will discuss in more details at section 4.2 .",
    "let us now consider the null hypothesis @xmath102 and derive the asymptotic properties of @xmath194 for the stopping time @xmath147 in ( [ eq : rands ] ) with @xmath129 defined in ( [ for : mu estimate ] ) .",
    "following @xcite , a standard approach is to use the change of measure technique .",
    "let @xmath195 be a probability measure on @xmath196 such that for each @xmath197 @xmath198 where @xmath199 is defined in ( [ for : mu estimate ] ) .",
    "in other words , under the new probability measure @xmath200 @xmath201 where the @xmath202 s are i.i.d . @xmath203 distributed . + an important step is to understand the asymptotic behavior of @xmath199 under the new probability measure @xmath195 as @xmath204 .",
    "the case of @xmath205 is discussed in @xcite , and as mentioned in remark # 4 on page 1442 of @xcite , @xmath206 converges to a random variable with distribution @xmath207 however , the case @xmath208 is very different , and the result is summarized in the following proposition .",
    "[ thm : meas q mun approx mu min gt 0 ] assume @xmath209 then the sequence @xmath206 converges almost surely to the real number @xmath58 under measure @xmath195 for all @xmath210    proposition [ thm : meas q mun approx mu min gt 0 ] , which is proved in appendix [ pf : lemma asy mu ] , shows another interesting effect of shrinkage : the convergence limits of @xmath206 when @xmath211 are different from those in the case of @xmath212 as in the former case @xmath213 always shrink to the pre - specified constants @xmath58 s and do not depend on the shrinkage factor @xmath63 as long as @xmath214    the following theorem establishes the properties of type i error of the stopping time @xmath147 in ( [ eq : rands ] ) with @xmath129 defined in ( [ for : mu estimate ] ) under the null hypothesis @xmath215    [ thm : hypo gamma c ] assume @xmath216 then the one - sided sprt @xmath147 defined by ( [ eq : rands ] ) and ( [ for : mu estimate ] ) satisfies @xmath217 moreover , as @xmath176 @xmath218 where the over - shoot factor @xmath219 and @xmath220 @xmath221 is cdf of @xmath222 , and @xmath223 is a function that often appears in the overshoot analysis in renewal theory : @xmath224    for the stopping time @xmath139 in ( [ eq : rands ] ) , we first need to show that @xmath225 for @xmath226 the case of @xmath205 is proved in lemma 2 of @xcite , and the key observation is that the conclusion holds if @xmath227 @xmath228 @xmath229 a.s . under probability measure @xmath230 by proposition [ thm : meas q mun approx mu min gt 0 ] , when @xmath231 @xmath232 a.s . ,",
    "and thus @xmath233 @xmath228 @xmath229 under @xmath195 as long as @xmath234 for some @xmath8 hence @xmath225 for @xmath226    by the standard change - of - measure argument , @xmath235 relation ( [ eqn_0010sec03 ] ) follows directly from the above relation and the definition of @xmath147 that @xmath236 relation ( [ eq : type 1 expr ] ) is proved in theorem 1 of @xcite , and the key idea is a renewal - theoretical analysis of @xmath237 under @xmath238 a high - level rough sketch is as follows .",
    "let us consider another stopping time @xmath239 for some constant @xmath240 that is sufficiently large but relatively smaller than @xmath241 on the one hand , @xmath242 by definition . on the other hand , we can choose @xmath63 large enough as compared to the overshoot so that @xmath243 with a high probability",
    ". in such a case , let @xmath244 for @xmath54 then for @xmath245 @xmath246 and the increments of @xmath247 acts like i.i.d .",
    "random variables : @xmath248 which can be reduced to a one - dimensional random walk with increments of the form @xmath249 with @xmath250 thus the standard linear renewal theory can be applied to show the corresponding ( conditional on @xmath251 ) overshoot factor is @xmath252 meanwhile , if @xmath199 converges a.s . to a random variable whose distribution is @xmath221 under @xmath195 , then the @xmath253 s have a distribution @xmath254 combining them together yields the formula of @xmath255 in ( [ eq : gamma const ] ) , see theorem 1 of @xcite for the detailed proof for @xmath256 the proof for @xmath257 is identical , except that proposition [ thm : meas q mun approx mu min gt 0 ] now shows that @xmath199 converges to a real number @xmath258 i.e. , the probability measure @xmath259 will degenerate to dirac measure @xmath260 which defined only at a single atom @xmath261 and thus ( [ eq : gamma const ] ) also holds for @xmath214    it is useful to point out that the proof of ( [ eqn_0010sec03 ] ) and ( [ eq : type 1 expr ] ) does not require the normal distributions , though the characterization of the over - shoot factor in ( [ eq : gamma const ] ) does depend on the normal assumption and the linear shrinkage estimators .",
    "theorem [ thm : hypo gamma c ] is useful to answer the following question : how to choose the threshold @xmath185 for the stopping time @xmath147 in ( [ eq : rands ] ) with @xmath129 defined in ( [ for : mu estimate ] ) so that it satisfies the type - i error constraint in ( [ st : hypothesis ] ) .",
    "it is clear from theorem [ thm : hypo gamma c ] that a conservative choice is @xmath262 whereas a more accurate choice is @xmath263 with @xmath264 being the overshoot factor defined in ( [ eq : gamma const ] ) .",
    "now we consider the sequential change - point detection problem .",
    "assume we are monitoring @xmath0 independent normally distributed data streams .",
    "for each stream @xmath265 observations @xmath266 ,",
    "@xmath267 , @xmath268 are i.i.d . normally distributed with initial mean @xmath83 and variance @xmath84 at some unknown time @xmath9",
    "the means of the observations @xmath269 change from @xmath83 to another mean @xmath270 in this section we focus on the consensus detection in which all post - change means @xmath12 s are non - negative and have similar ( but not necessarily identical ) values .",
    "let @xmath112 . denote by @xmath271 and @xmath272 the probability measure and expectation of @xmath273 , @xmath110@xmath111 when the change happens at time @xmath274 and @xmath112 is the true mean vector . denote by @xmath114 and @xmath275 when no change happens , i.e. @xmath276 for each stream @xmath3 and each time @xmath277    a standard formulation in the sequential change - point detection problem is to asymptotically minimize the  worst - case \" detection delay proposed in @xcite @xmath278 for all possible post - change mean vectors @xmath279 s subject to the constraint on the average run length ( arl ) to false alarm @xmath280 here @xmath281 denotes all information up to time @xmath282 and the constraint @xmath283 in ( [ eq : change detect cond ] ) is pre - specified .",
    "as mentioned in the introduction , lorden and pollak @xcite proposed the srrs scheme @xmath284 in ( [ eqn003 ] ) with @xmath38 in ( [ new_lr ] ) being the mom or mle of @xmath285 which is @xmath286 following the shrinkage estimator in ( [ eq : mujs ] ) and ( [ for : mu estimate ] ) , in the context of consensus detection , we propose to consider the srrs scheme @xmath284 in ( [ eqn003 ] ) with @xmath38 in ( [ new_lr ] ) chosen as the linear shrinkage estimator of @xmath12 defined by @xmath287 in other words , @xmath38 in ( [ for : change detect mu esti ] ) shrinks the mle estimator towards @xmath95 the minimum change we want to detect . + below we will discuss the properties of our proposed srrs scheme @xmath284 with shrinkage estimators ( [ for : change detect mu esti ] ) in the three subsections : asymptotic properties , the choice of shrinkage factor @xmath63 in ( [ for : change detect mu esti ] ) , and numerical simulation results .",
    "[ thm : detection change ] assume the shrinkage factor @xmath226 consider the srrs scheme @xmath284 in ( [ new_lr ] ) and ( [ eqn003 ] ) with @xmath38 defined in ( [ for : change detect mu esti ] ) .",
    "+ ( i ) for all @xmath288 @xmath289 ( ii ) assume @xmath178 then as @xmath290 , the detection delay @xmath291 where @xmath175 is defined in ( [ eqn06sec3-info ] ) .",
    "\\(i ) can be proved along the same line as in @xcite .",
    "the key is that as long as @xmath38 only depends on @xmath285 shrinkage or not , the plug - in likelihood ratio @xmath292 in ( [ new_lr ] ) can be used to construct a martingale @xmath293 under @xmath294 hence @xmath295 for the proposed stopping time @xmath296 and part ( i ) follows at once from this and the fact that @xmath297 by the definition of @xmath298    the proof of ( ii ) takes advantage of theorem [ cor : e tau b optimality p dim ] for the one - sided sprt @xmath147 in ( [ eq : rands ] ) and the close relationship between hypothesis testing problem and change - point detection problem . as shown in @xcite , under our context , @xmath299 as the most difficult time to detect a change is when the true change occurs at the time @xmath300 note that @xmath301 defined in ( [ eqn003 ] ) is greater than @xmath302 defined in ( [ eq : rands ] ) and thus @xmath303 with @xmath304 for any given threshold @xmath305 part ( ii ) of the theorem follows at once from these facts and theorem [ cor : e tau b optimality p dim ] .",
    "we add a couple of comments for theorem [ thm : detection change ] .",
    "first , part ( i ) provides a lower bound for the arl to the false alarm . in order for the srrs scheme @xmath284 in ( [ new_lr ] ) and ( [ eqn003 ] ) to satisfy the constraint ( [ eq : change detect cond ] ) , a conservative choice is to set @xmath306 for any shrinkage factor @xmath307 in particular , when @xmath205 or @xmath308 the srrs scheme @xmath284 with @xmath309 is the first - order asymptotic optimal for fixed @xmath0 in the sense of minimizing the detection delay @xmath310 up to the first - order subject to the constraint ( [ eq : change detect cond ] ) when the constraint @xmath311 goes to @xmath151    second , it is well known that there is a natural correlation between one - sided hypothesis testing problem and sequential change - point detection problem , see @xcite and @xcite .",
    "lorden and pollak @xcite pointed out that for @xmath212 the srrs - scheme @xmath312 satisfies @xmath313 where the  overshoot correction constant \" @xmath255 is defined in theorem [ thm : hypo gamma c ] for hypothesis testing and @xmath255 can be evaluated in ( [ eq : gamma const ] ) for normal distributions .",
    "it is not difficult to show that equation ( [ conj : arl ] ) also holds for our proposed scheme for any @xmath257 as well , where @xmath314 which has the same value as in theorem [ thm : hypo gamma c ] .",
    "hence a choice of @xmath315 will yield our proposed scheme satisfying the arl to false alarm constraint ( [ eq : change detect cond ] ) .",
    "this turns out to be consistent with simulation result in the next subsection 4.3 . unfortunately , in the context of parallel detection with hard - thresholding , it is unclear how to characterize the overshoot factor @xmath255 in ( [ conj : arl ] ) , and thus we presented ( [ conj : arl ] ) only as a remark here so that the theorems are similar for both consensus detection and parallel detection .",
    "+      now let us discuss how to choose the shrinkage parameter @xmath63 in ( [ for : change detect mu esti ] ) , and let us begin with the case when @xmath63 is a constant .",
    "a trivial choice is @xmath316 , as the corresponding stopping time @xmath312 is just the shiryaev - roberts procedure @xmath317 in ( [ for : change rule ] ) and ( [ eqn001 ] ) with @xmath318 being the known post - change parameter for all @xmath189 .",
    "another obvious choice is @xmath212 which is the original srrs scheme proposed in @xcite . or one can try other choices @xmath319 such as @xmath320 or @xmath321    however , what is the optimal choice of the shrinkage factor @xmath322 it is natural to consider the oracle shrinkage factor that minimizes the detection delay @xmath310 subject to the constraint ( [ eq : change detect cond ] ) if an oracle tells us the value of the true means @xmath12 s . unfortunately , there is no closed form as in @xmath66 in ( [ oracle_point ] ) for point estimation .",
    "nevertheless , it inspires us to approximate the oracle shrinkage factor via two different approaches .",
    "the first one is a more theoretical approach based on theorem [ thm : detection change ] , and one can find @xmath63 that minimizes the upper bound on detection delay ( or the dominant term ) .",
    "that is , set @xmath323 , then @xmath324 that minimizes @xmath325\\}}{(1/2)(\\|\\mu\\|^2 - ( 1-c)^2 ( \\|\\mu - \\omega\\|^2)}. \\ ] ] the other approach is a numerical approach to search @xmath66 directly via monte carlo simulations of the detection delays .",
    "for each fixed combination of post - change parameters , we compare the detection delays of the srrs scheme with different shrinkage factors , say @xmath63 changes from @xmath326 to @xmath327 with step size @xmath328 i.e. , @xmath329 for each shrinkage factor @xmath330 we first use monte carlo numerical simulations to determine @xmath331 to satisfy the constraint ( [ eq : change detect cond ] ) and then simulate the corresponding detection delay @xmath332 then we define the optimal shrinkage factor @xmath333 as @xmath63 that leads to the smallest detection delay @xmath334    for the purpose of comparison , below we also propose adaptive shrinkage estimators @xmath335 in ( [ for : change detect mu esti ] ) that is inspired by james - stein estimator and the corresponding shrinkage factor @xmath63 which depends on observations ( though our theoretical results are not applicable to these schemes with non - constant shrinkage factors ) .",
    "note that for the shrinkage estimators @xmath335 in ( [ for : change detect mu esti ] ) , the corresponding mle or mom estimator is @xmath336 which is normally distributed with mean @xmath12 and variance @xmath337 if @xmath338 are i.i.d .",
    "@xmath339 thus , the james - stein estimator of @xmath12 considers the shrinkage factor @xmath340 ^ 2},\\end{aligned}\\ ] ] which are different for different pairs @xmath341 and lead to adaptive shrinkage estimators @xmath335 in ( [ for : change detect mu esti ] ) .",
    "+      in this subsection , we run numerical simulations to verify our theoretical results .",
    "assume we observe @xmath342 independent normal data streams , say observing @xmath343 over time @xmath17 initially the data vector @xmath343 are i.i.d .",
    "normally distributed with mean @xmath344 and identity covariance matrix .",
    "when a change happens at some time @xmath274 ( in all simulations we set @xmath345 ) , the mean of data vector changes to @xmath346 we want to raise an alarm as soon as possible after a change occurs , under the arl to false alarm constraint in ( [ eq : change detect cond ] ) with bound @xmath347    since the detection delay depends on the post - change parameters , we consider four different combinations of the post - change mean parameters to evaluate our proposed scheme . to reflect consensus detection",
    ", we randomly generate post - change mean parameters from a normal distribution with mean @xmath348 and variance @xmath349 in our numerical simulation , we chose @xmath350 and obtained three specific combinations of the three - dimension post - change mean parameters @xmath351 @xmath352 @xmath353 @xmath354 in addition , we also consider the forth combination : @xmath355 in our study , we set the smallest meaningful difference to be detected as @xmath356 for all @xmath357    for the purpose of comparison , we consider the following seven different srrs schemes @xmath284 in ( [ new_lr ] ) and ( [ eqn003 ] ) with @xmath38 defined in ( [ for : change detect mu esti ] ) , depending on the choices of shrinkage factors @xmath63 s ( the first six cases are constant @xmath63 s ) .",
    "* @xmath212 which is the srrs scheme proposed in @xcite and will be treated as the baseline scheme ; * @xmath358 * @xmath359 * @xmath360 * @xmath361 defined by minimizing ( [ for : optcthm ] ) ; * @xmath333 based on numerical simulation to search @xmath66 as mentioned in section 4.2 ; * adaptive shrinkage factor @xmath362 in ( [ met : adp c ] ) .    for each of these seven schemes",
    "@xmath363 we first run simulation to search @xmath364 such that @xmath365 and then simulate the detection delay @xmath366 for each of the four post - change mean combinations @xmath367 s .",
    "the results are summarized in table [ tab : samplesizecmp ] , and all results are based on @xmath368 replications .       from table [",
    "tab : compute simple ] , when the post - change means are mis - specified , i.e. , the true post - change mean is @xmath369 not @xmath370 then the scheme @xmath371 in ( [ cs_t ] ) is significantly better than both  max \" and  sum \" schemes .",
    "even if the true post - change mean is indeed correctly specified ( i.e. , @xmath372 ) , the hard - thresholding based scheme @xmath371 in ( [ cs_t ] ) is still better than both  max \" and  sum \" schemes when @xmath373 or @xmath374 out of @xmath375 data streams are affected .",
    "of course , the scheme @xmath371 in ( [ cs_t ] ) does not perform that well when @xmath376 data streams are affected and the true post - change means are @xmath377 this is understandable , since it is non - trivial to develop a scalable scheme that can detect all kinds of post - changes quickly .",
    "we hope that this paper , particularly the idea of shrinkage , will shed new light on developing more and better scalable schemes in the context of online monitoring large - scale data streams .",
    "the proof of theorem [ cor : e tau b optimality p dim ] is along the same line as in that of theorem 3 in @xcite which can be thought of dealing with a special case of the stopping time @xmath147 in ( [ eq : rands ] ) with @xmath129 defined in ( [ for : mu estimate ] ) when the shrinkage factor is @xmath378 and the dimension is @xmath379 below we will highlight the main difference with a general shrinkage factor @xmath63 and the dimension @xmath380    let @xmath381 to simplify our notation .",
    "we first consider the necessary modification regarding to a general shrinkage factor @xmath307 under @xmath149 it is clear that as @xmath20 goes to @xmath167 @xmath382 for @xmath44 let @xmath383 and a key step is to relate @xmath384 in ( [ eq : * * ] ) to the likelihood ratio @xmath385 when the post change parameters are @xmath386 for all @xmath8    note that @xmath387 when @xmath388 this inspires us to consider @xmath389 for @xmath390 and @xmath391 then under @xmath149 @xmath392 is a random walk with i.i.d .",
    "increments that have finite variance and mean @xmath175 in ( [ eqn06sec3-info ] ) .",
    "next , the standard estimates show that @xmath393 ( here @xmath394 may depend on @xmath0 ) . by wald s equation",
    ", we have @xmath395 as mentioned in theorem 3 of @xcite , we can apply the martingale optional sampling theorem to @xmath396 where @xmath155 is defined in ( [ eqn07sec3-info ] ) for any @xmath397 when @xmath398 i.e. , @xmath399 in ( [ for : mu estimate ] ) for all @xmath135 , define @xmath400 then we have @xmath401 and @xmath402 for @xmath141 ( and @xmath403 for @xmath404 ) .",
    "hence , from ( [ eqn07sec3-info ] ) , @xmath405 where the summation can be estimated as in theorem 3 of @xcite by @xmath406 where @xmath407 the largest integer @xmath408 combining the above results yields @xmath409 then @xmath117 can be found by solving the equation of the form @xmath410 for large @xmath411 and possibly large @xmath412 taking logarithms of both sides yields @xmath413 where we use the fact that @xmath414 for @xmath415 and @xmath416 for large @xmath417 plugging this relation back to @xmath410 yields that @xmath418 using the above arguments to derive @xmath117 and absorbing the insignificant terms to the @xmath419 term , we have @xmath420 which becomes ( [ eq : asn ] ) as @xmath185 goes to @xmath151 thus the theorem is proved .      to prove proposition [ thm : meas q mun approx mu min gt 0 ]",
    ", it suffices to prove it for a fixed @xmath135 which corresponds to the case of @xmath133 when there is only one data stream , as different data streams are independent . to simplify our notation , we drop the subscript @xmath135 and focus on a single sequence of random variables @xmath421 the shrinkage estimator of @xmath279 becomes @xmath422 for @xmath390 and @xmath423 the probability measure @xmath195 in proposition [ thm : meas q mun approx mu min gt 0 ] is then defined in such a way that the conditional distribution of @xmath424 given @xmath425 is @xmath426 where @xmath427 is defined in ( [ eqn : 002 ] ) .",
    "proposition [ thm : meas q mun approx mu min gt 0 ] states that for @xmath428 the sequence @xmath429 converges to @xmath430 a.s . under the probability measure",
    "@xmath230    first , we claim that it suffices to prove lemma [ thm : meas q mun approx ] below which deals a special case when @xmath431 to see this , for general @xmath432 define @xmath433 and @xmath434 by lemma [ thm : meas q mun approx ] below , under the probability measure @xmath200 @xmath435 a.s . and thus @xmath436 a.s .",
    "when @xmath437 thus proposition [ thm : meas q mun approx mu min gt 0 ] holds for general @xmath438        note that @xmath441 for all @xmath442 where the @xmath443 s are i.i.d .",
    "@xmath444 the essential idea is to re - write @xmath445 s in terms of @xmath446 s so that we can avoid difficult conditional probabilities . since @xmath447 @xmath448 in ( [ eqn : 002 ] ) can be rewritten as @xmath449\\\\ & = & \\frac{n-1+c}{n}\\hat{\\mu}_n + \\frac{c}{n}z_n = \\cdots = \\\\ & = & a_{n0 } \\hat{\\mu}_1 + \\sum_{i=1}^{n-1}a_{ni } z_i + a_{nn } z_n\\end{aligned}\\ ] ] where @xmath450    from this new representation of @xmath451 it is clear that @xmath452 and @xmath453 as @xmath423 thus lemma [ thm : meas q mun approx ] holds under the following sufficient condition : @xmath454 note that this is also true for any arbitrary initial value of @xmath455 when @xmath231 since we have @xmath456 goes to @xmath83 as @xmath437    it remains to prove ( [ eq : 008 ] ) for @xmath457 this requires us to approximate @xmath458 and @xmath459 via some classical calculus arguments for sufficiently large @xmath17 let us fix ( sufficiently large ) @xmath1 and by abuse the notation , we write @xmath460 for @xmath458 in ( [ eq : 003 ] ) .",
    "note that @xmath461 it is not difficult to show that @xmath462 for @xmath463 and thus when @xmath440 we have @xmath464 thus @xmath465    observe that @xmath466 when @xmath440 and let us first approximate the largest value @xmath467 by the well - known fact that @xmath468 converges to the euler s constant @xmath469 ( here we do not use the traditional notation @xmath470 which unfortunately has already been used for the overshoot in ( [ eq : gamma const ] ) in theorem [ thm : hypo gamma c ] ) , we have @xmath471 and thus @xmath472 where @xmath394 depends on @xmath63 but not on @xmath17 next , while it is nontrivial to get a good estimate of @xmath460 for all @xmath473 it is easy to do so when @xmath474 is large , say , @xmath475 for some fixed @xmath476 @xmath477 and thus @xmath478 where @xmath394 does not depend on @xmath17    to prove ( [ eq : 008 ] ) , a simple approach is to split @xmath479 $ ] into two subintervals : @xmath480 $ ] and @xmath481 $ ] for some @xmath482 since the @xmath460 s are decreasing , we have @xmath483 which converges to @xmath83 as @xmath20 goes to @xmath229 as long as @xmath484 and @xmath485 if we let @xmath486 then this simply approach was able to prove ( [ eq : 008 ] ) for @xmath487 or a better choice is @xmath488 which can prove ( [ eq : 008 ] ) for @xmath489    it is natural to see whether there is any benefit if we extend the simple approach to more than two subintervals .",
    "let us consider the case of three subintervals .",
    "recall that when @xmath490 the above simple arguments of two subintervals work fine on the subinterval @xmath491 $ ] but lead a poor estimate on the subinterval @xmath492.$ ] hence one can further split @xmath493 $ ] into three intervals : @xmath494,$ ] @xmath495 $ ] and @xmath491 $ ] for some @xmath496 then @xmath497 which converges to @xmath83 as long as @xmath498 for some @xmath499 in particular , let @xmath500 then the arguments with three intervals was able to prove ( [ eq : 008 ] ) for @xmath501    to prove ( [ eq : 008 ] ) for any fixed @xmath440 let @xmath502 the smallest integer that @xmath503 and define the @xmath504 exponents @xmath505 s as @xmath506 @xmath507 for @xmath508 and @xmath509 in other words , @xmath510 for @xmath511 then we split the interval @xmath479 $ ] into @xmath512 subintervals , @xmath513 $ ] for @xmath511 by an abuse of notation , let @xmath514 when @xmath515 then @xmath516 for @xmath517 and @xmath518 which converges to @xmath83 as @xmath519 since all @xmath394 terms do not depend on @xmath1 and our choice of @xmath512 and the definition of @xmath505 s make sure that @xmath520 and @xmath521 for all @xmath522 this shows that ( [ eq : 008 ] ) indeed holds for any given @xmath440 completing the proof of lemma [ thm : meas q mun approx ] .",
    "the proof of theorem [ thm : threshold ] is same as that of theorem [ thm : detection change ] if relation ( [ eqn4sec5-info ] ) holds .",
    "since @xmath512 is fixed and @xmath29 goes to @xmath167 it suffices to prove ( [ eqn4sec5-info ] ) for @xmath523 as @xmath29 goes to @xmath151      now for fixed @xmath529 @xmath530 with @xmath531 also @xmath532 by abuse of notation , we need to investigate @xmath533 and @xmath534 for @xmath535 when @xmath536 for fixed @xmath279 and @xmath430 as @xmath48 goes to @xmath80 following the traditional notation , let @xmath537 and denote by @xmath538 and @xmath539 for the pdf and cdf of @xmath444 also define @xmath540 as theorem [ thm : threshold ] assumes that @xmath541    it is natural to consider two different scenarios , depending on whether @xmath542 or not .",
    "let us first consider the easier scenario when @xmath543 then @xmath544 @xmath545 and so @xmath546 where we use the fact @xmath547 by the well - known fact that @xmath548 for all @xmath549 it is clear that @xmath550 as @xmath94 goes to @xmath80 in addition , @xmath551 where the third equation is from integration by parts for @xmath552 thus , when @xmath553 are fixed , both @xmath533 and @xmath534 are negligible ( less than order @xmath554 ) as @xmath94 goes to @xmath555 as @xmath556 goes to @xmath83 in an exponentially rate as @xmath94 goes to @xmath80    the other scenario is when @xmath557 in this scenario , @xmath558 @xmath559 thus @xmath560 which is again negligible .",
    "here we use the fact @xmath561 in the second equation and the symmetric properties of @xmath538 in the third equation .",
    "in addition , @xmath562   \\\\ & = & \\mu^2 { { \\bf p } } ( z > \\lambda ) + \\sigma^2 [ 1 -   \\int_{\\lambda}^{\\infty } z^2 \\phi(z ) d z ] \\\\ & = & ( \\mu^2 - \\sigma^2 ) { { \\bf p } } ( z > \\lambda )   -   \\sigma^2   \\lambda \\phi(\\lambda ) + \\sigma^2,\\end{aligned}\\ ] ] whose dominant term is @xmath48 as @xmath94 goes to @xmath83 when @xmath563 however , when @xmath564 the dominant terms are @xmath565 with @xmath566 being a constant term .    now applying the above results back to our context of @xmath169 and @xmath567 under our assumption that @xmath568 for all @xmath135 we have @xmath569 as @xmath29 goes to @xmath167 as only those @xmath3 in @xmath570 makes a contribution of @xmath571 to the summation @xmath572 and all other terms are negligible .",
    "this proves ( [ eqn4sec5-info ] ) , and thus the theorem holds .",
    "+                                              tartakovsky , a. g. , rozovskiia , b.l . ,",
    "blazeka , r.b . and kim , h. ( 2006 ) .",
    "detection of intrusions in information systems by sequential change - point methods ( with discussion ) . _",
    "methodol . _ * 3 * 252 - 340 ."
  ],
  "abstract_text": [
    "<S> the sequential change - point detection problem is considered when we are monitoring multiple independent data streams but the post - change distributions involve unknown parameters . </S>",
    "<S> one monitoring scheme is the srrs scheme proposed by lorden and pollak ( 2005 ) that estimates the post - change parameters by the method of moments ( mom ) or maximum likelihood estimators ( mle ) of past observations and then uses the shiryaev - roberts - type procedure to raise a global alarm . </S>",
    "<S> however , it is well - known from the off - line point estimation literature that  shrinkage \" often leads to better performances compared to mom or mle in the multi - dimensional scenario , see james and stein ( 1961 ) , donoho and johnstone ( 1994 ) . here </S>",
    "<S> we propose to adopt two kinds of shrinkage estimators in the srrs scheme for online monitoring : linear shrinkage for consensus detection and hard thresholding for parallel detection . our theoretical analysis and numerical simulations demonstrate the usefulness of shrinkage in the sequential or online monitoring setting . </S>",
    "<S> moreover , the srrs scheme and the shrinkage post - change estimators are also illustrated to be flexible and can be modified to develop a computationally simple scheme when monitoring large - scale data streams . </S>"
  ]
}