{
  "article_text": [
    "entropy is a measure of disorder or randomness , and assumes its maximal value when a system can be in a number of states randomly with equal probability , and is minimally zero when the system is in a given state , with no uncertainty in its description .",
    "apart from this common feature shared by all definitions of entropy at two ends of the scale , variations are possible in particularizing the functional form in between @xcite .",
    "they lead to different forms of the probability distributions for states with different energies or some other conserved attribute .",
    "some turn up as extensive , where the entropy of a combination of systems is simply the sum of the entropies of the systems , as in the classical case of the shannon form , while others can be defined to be not so .",
    "renyi entropy @xcite is different from shannon , yet extensive , and hence the shannon form is not unique with respect to the property of extensivity .",
    "tsallis entropy @xcite has attracted a lot of attention in recent years , not only on account of its conceptual and theoretical novelty , but also because it can be shown in specific physical cases @xcite to be the relevant form where nonextensivity is expected on account of the interaction of the combined subsystems . in the proper limiting case it reduces to the standard shannon entropy , indicating the consistency of the concept .    in this paper , however , we shall introduce entropy from a new perspective , which too will bear semblance to the normal form in the limit .",
    "we shall first present the rationale for this new definition and compare it briefly with the forms already being used .",
    "then we shall find the form of the probability distribution for this entropy , which we shall henceforth call s - entropy , as it will be seen to be related to the concept of rescaling of the phase space .",
    "let us consider a register of only one letter .",
    "let @xmath0 be the set of probabilities for each of the @xmath1 letters @xmath2 that can occupy this position .",
    "we are here using the language of information theory , as used for example , in the shannon coding theorem , though it is trivially extensible to states @xmath3 of a single state of an ensemble where the individual systems can be in any @xmath1 states with probabilities @xmath4 .",
    "let us now consider a small deformation of the register to a new size so that it can accommodate @xmath5 letters .",
    "the probability that the whole new phase space is occupied by the letter @xmath4 is now @xmath6 by the corresponding and operation and hence the probability that the new deformed cell is occupied by any of the pure letters @xmath2 is    @xmath7    for @xmath8 this would give a shortfall from the original total probability of unity for @xmath9 .",
    "it is obvious that the shortfall , which we denote by    @xmath10    represents the total probability that the mixed cell has a mixture of @xmath2 and some other @xmath11 fractionally , since the total probability that the cell is occupied by one or more ( fractional included ) letters must be unity .",
    "hence the mixing probability @xmath12 is actually a measure of the disorder introduced by increasing the cell scale from unity to @xmath13 .",
    "the introduction of fractional values of cell numbers can be taken in the same spirit as defining the fractal ( hausdorff ) dimensions of curves , and in complex systems there have been studies of diffusion @xcite and percolation in complex systems with effectively fractional dimensions for fluids where the special geometric constraints translate into a change in the dimension of the corresponding space to an apparently nonintuitive fractional dimension . in coding theories for optimal transmission of information @xcite , we come across huffmann coding , where the optimum alphabet size may be formally a fraction , though for practical purposes it may be changed to the nearest higher integer . in probabilistic optimization , we may therefore consider a fractional size of the registrar , or equivalently , an integral number of cells in the registrar with fractional sized cells to accommodate a given amount of information .",
    "probabilistic optimization in place of the deterministic parameterization of classical shannon information theory @xcite becomes inevitable in quantum computing contexts , and hence our use of the fractional cell sizes may be a classical precursor of the inevitable departure from stringent shannon - type concepts .",
    "for an alphabet of @xmath14letters we define the entropy from the information content of the registrar by    @xmath15    so that the entropy indicates an effective change in the mixing probability due to an infinitesimal change in the cell - size of the registrar .",
    "this leads to    @xmath16    in other words    @xmath17    * we have some material at the other place .",
    "* this differential form is analogous to but different from the tsallis form    @xmath18    where there is an apparent singularity at @xmath9 which is the shannon limit .",
    "the difference between the tsallis expression and ours becomes clearer if we express entropy as the expectation value of the ( generalized or ordinary ) logarithm .",
    "@xmath19    where the generalized q - logarithm is defined as    @xmath20    the expectation value is defined in terms of the simple probability distribution @xmath21    in our case we define the expectation value with respect to the deformed probability corresponding to the extended cell , while keeping the usual logarithm    @xmath22    with    @xmath23    in the limit @xmath24 @xmath25 approaches the normal logarithm , and hence tsallis entropy coincides with shannon entropy and also as @xmath26 we too get the normal shannon entropy .",
    "the renyi entropy is defined by    @xmath27    like shannon entropy this one is also extensive , i.e. simply additive for two subsystems for any value of @xmath28 . to get shannon entropy",
    "uniquely one needs @xcite a slightly different formulation of the extensivity axiom    @xmath29    where @xmath30 is the entropy of subsystem @xmath31 given subsystem @xmath32 is in state @xmath3 .",
    "the @xmath4 can be obtained in terms of the energy of the states , or possibly also other criteria in the usual way by maximizing the entropy with constraints    @xmath33    and    @xmath34    .",
    "the solution of the optimization equation gives for energy @xmath35 the probability @xmath4    @xmath36    where @xmath37    and w(z ) is the lambert function defined by    @xmath38    here @xmath39 and @xmath40 are constants coming from the lagrange s multipliers for the two constraints and are related to the overall normalization and to the relative scale of energy , i.e. to temperature ( @xmath41 ) as in the shannon case where we get the gibbs expression for @xmath4 . in the tsallis case",
    "@xmath4 has the well - known value    @xmath42    which is easily seen to reduce to shannon form for @xmath43 .    after some algebra it can be shown that this form reduces to the shannon form for @xmath24 .",
    "the nonextensivity of tsallis entropy is seen easily by expanding    @xmath44    for renyi entropy we have the simple additive relation    @xmath45    in case of the new entropy    @xmath46    where the @xmath47 are the mixing probability of states for subsystem @xmath39 as defined in eq.2 .",
    "in fig.1 we show the variation of the probability function for different @xmath48 at different @xmath28 values .",
    "comparison of the pdf for the new entropy for values of @xmath49 and @xmath50 .",
    "the solid line is for @xmath9 , i.e. the gibbs exponential distribution and the lines are in the order of @xmath28,width=302 ]    we note that the pdf drops increasingly rapidly for higher values of @xmath28 , and is quite different in shape and in magnitude at high energy values from the gibbs exponential distribution .",
    "a variation of even @xmath51 from the standard value of @xmath9 can cause a quite discernible change in the pdf and should be observable in experimental contexts fairly easily . at @xmath52 ,",
    "the shape is almost linear .    in fig.2 and fig.3",
    "we show the comparison of tsallis pdf and the pdf for the new entropy for the same values of @xmath28 , 1.1 in the former and 1.3 in the latter .",
    "we notice that for larger @xmath28 values the new entropy gives much stiffer probability functions departing substantially from the tsallis pdf s .",
    "comparison of pdf s for tsallis nonextensive entropy and the new entropy presented here , for @xmath53,width=302 ]    the same as fig . 2 , but for a higher @xmath52,width=302 ]",
    "we see that the new entropy presented here based on the simple concept of the amount of mixing of states freedom introduced per unit cell of phase space leads to a nonextensive form different from any of the presently studied entropies .",
    "it leads to a complicated , but still integrable form of the pdf which departs substantially from tsallis entropy .",
    "this entropy is also nonextensive in a fashion different from tsallis entropy , though like tsallis it too becomes extensive trivially in the limit @xmath24 , as expected .",
    "it would now be interesting to find a physical situation where such an entropy arises from first principles , though like some initial phenomenological studies of tsallis entropy it can be also used as a parametrization scheme with @xmath28 as a parameter to fit experimental data .",
    "the stiffness of any data may point to its preferability to tsallis - type entropies .",
    "landsberg , `` entropies galore '' , _ braz .",
    "j. phys . _",
    "* 29 * , 46 ( 1999 ) a.  renyi , _ probability theory _ ( north - holland , ams terdam , 1970 ) c.  tsallis,_j .",
    "phys . _ , * 52 * , 479(1988 ) p.  grigolini , c. tsallis and b.j .",
    "west,_chaos , fractals and solitons_,*13 * , 367 ( 2001 ) a.r .",
    "plastino and a. plastino , _ j. phys .",
    "* 27 * , 5707 ( 1994 ) c.  beck , `` nonextensive statistical mechanics and particle spectra '' , hep - ph/0004225 ( 2000 ) o.  sotolongo - costa et al .",
    ", `` a nonextensive approach to dna breaking by ionizing radiation '' , cond - mat/0201289 ( 2002 ) c.  wolf , `` equation of state for photons admitting tsallis statistics '' , _ fizika b _ * 11 * , 1 ( 2002 ) m.  buiatti , p. grigolini and a. montagnini , _ phys .",
    "lett . _ * 82 * , 3383 ( 1999 ) m.a .",
    "nielsen and m. chuang , _ quantum computation and quantum information _",
    "( cambridge u.p . ,",
    "ny , 2000 ) a.i .",
    "kinchin , _ mathematical foundationsof information theory _ , ( dover publications , new york , 1957 )"
  ],
  "abstract_text": [
    "<S> we propose a new way of defining entropy of a system , which gives a general form which may be nonextensive as tsallis entropy , but is linearly dependent on component entropies , like renyi entropy , which is extensive . </S>",
    "<S> this entropy has a conceptually novel but simple origin and is mathematically easy to define by a very simple expression , though the probability distribution resulting from optimizing it gives rather complex , which is compared numerically with the other entropies . </S>",
    "<S> it may , therefore , appear as the right candidate in a physical situation where the probability distribution does not suit any of the previously defined forms . </S>"
  ]
}