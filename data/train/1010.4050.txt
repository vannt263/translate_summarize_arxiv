{
  "article_text": [
    "matrix completion amounts to estimate all the entries in a matrix @xmath0 from a partial and potentially noisy observation @xmath1 where @xmath2 is a binary matrix with 1/0 entries masking a portion of @xmath3 through the element - wise multiplication @xmath4 , and @xmath5 an additive noise . with matrix rows , columns , and entry values assigned to various attributes , matrix completion may have numerous applications .",
    "for example , when the rows and the columns of @xmath3 are attributed to users and items such as movies and books , and an entry at position @xmath6 records a score given by the @xmath7-th user to the @xmath8-th item , matrix completion predicts users scores on the items they have not yet rated , based on the available scores recorded in @xmath9 , so that personalized item recommendation system becomes possible .",
    "this is a classical problem in collaborative filtering .    to solve an ill - posed matrix completion problem",
    ", one must rely on some prior information , or in other words , some data model .",
    "the most popular family of approaches in the literature assumes that the matrix @xmath3 follows approximately a low rank model , and calculates the matrix completion with a matrix factorization  @xcite .",
    "theoretical results regarding the completion of low - rank matrices have been recently obtained as well , e.g. , @xcite and references therein .",
    "more elaborative probabilistic models and some refinement have been further studied on top of matrix factorization , leading to state - of - the - art results  @xcite .    in image processing , assuming that local image patches follow gaussian mixture models ( gmm ) , yu , sapiro and mallat have recently reported excellent results in a number of inverse problems  @xcite . in particular , for inpainting , which is an analogue to matrix completion where the data is an image , the maximum a posteriori expectation - maximization ( map - em ) algorithm for solving the gmm leads to state - of - the - art results , with a computational complexity considerably reduced with respect to the previous state - of - the - art approaches based on sparse models  @xcite , ( which are analogous to the low - rank model assumption for matrices ) .    in this paper",
    ", we investigate gaussian modeling ( a particular case of gmm with only a single gaussian distribution ) for matrix completion .",
    "subparts of the matrix , typically rows or columns , are regarded as a collection of signals that are assumed to follow a gaussian distribution .",
    "an efficient map - em algorithm is introduced to estimate both the gaussian parameters ( mean and covariance ) and the signals .",
    "we show through numerical experiments that the fast map - em algorithm , based on the gaussian model which is the simplest probabilistic model one can imagine , leads to results in the same ballpark as the state - of - the - art in movie rating prediction , at significantly lower computational cost .",
    "recent theoretical results  @xcite further support the consideration of gaussian models for the recovery of missing data .",
    "section  [ sec : model : algo ] introduces the gaussian model and the map - em algorithm . after presenting the numerical experiments in section  [ sec : numeric ]",
    ", section  [ sec : conclusion ] concludes with some discussions .",
    "similar to the local patch decomposition often applied in image processing  @xcite , let us consider each subpart of the matrix @xmath0 , the @xmath10-th row @xmath11 for example , as a signal .",
    "let @xmath12 denote the @xmath10-th row in the binary matrix @xmath13 , and let @xmath14 be the number of non - zero entries in @xmath15 .",
    "let @xmath16 denote a masking operator which maps from @xmath17 to @xmath18 extracting entries of @xmath19 corresponding to the non - zero entries of @xmath15 , i.e. , all but the @xmath20-th the entries in the @xmath21-th row of @xmath22 are zero , with @xmath20 the index of the @xmath21-th non - zero entry in @xmath15 , @xmath23 .",
    "let @xmath24 and @xmath25 be respectively the _ sub - vector _ of the @xmath10-th row of @xmath9 and @xmath26 , where the entries of @xmath15 are non - zero . with this notation , we can rewrite   in a more general linear model @xmath27 for all the signals @xmath19 , @xmath28 .   in the reduced dimension",
    "@xmath29 leads to a calculation in dimension @xmath29 instead of @xmath30 , which is considerably faster if @xmath31 . ]",
    "note that @xmath19 can also be columns , or 2d sub - matrices of @xmath3 rendered in 1d .",
    "the gaussian model assumes that each signal @xmath19 is drawn from a gaussian distribution , with a probability density function @xmath32 where @xmath33 and @xmath34 are the unknown covariance and mean parameters of the gaussian distribution .",
    "the noise @xmath35 is assumed to follow a gaussian distribution with zero mean and covariance @xmath36 , here assumed to be known ( or calibrated ) .    estimating the matrix @xmath3 from the partial observation @xmath9 can thus be casted into the following problem :    * estimate the gaussian parameters @xmath37 , from the observation @xmath38 . * estimate @xmath19 from @xmath39 , @xmath28 , using the gaussian distribution @xmath40 .",
    "since this is a non - convex problem , we present an efficient maximum a posteriori expectation - maximization ( map - em ) algorithm that calculates a local - minimum solution  @xcite .      following a simple initialization , addressed in section  [ subsec : initialization ] ,",
    "the map - em algorithm is an iterative procedure that alternates between two steps :    1 .",
    "e - step : signal estimation . assuming the estimates @xmath41 are known ( following the previous m - step ) , for each @xmath10 one computes the maximum a posteriori ( map ) estimate @xmath42 of @xmath19 .",
    "m - step : model estimation . assuming the signal estimates @xmath42 , @xmath43 ,",
    "are known ( following the previous e - step ) , one estimates ( updates ) the gaussian model parameters @xmath44 .",
    "it is well known that under the gaussian models assumed in section  [ subsec : gaussian : model ] , the map estimate that maximizes the log a - posteriori probability @xmath45is a linear estimator and is optimal in the sense that it minimizes the mean square error ( mse ) , i.e. , @xmath46   =   \\min_{g}e_{{\\mathbf{f}}_i , { \\mathbf{w}}_i }   [ \\|{\\mathbf{f}}_i- g({{\\mathbf{y}}}_i)\\|_2 ^ 2 ] , \\nonumber   $ ] as well as the mean absolute error ( mae ) , i.e. , @xmath47   =   \\min_{g } e_{{\\mathbf{f}}_i , { \\mathbf{w}}_i }   [ \\|{\\mathbf{f}}_i - g({{\\mathbf{y}}}_i)\\|_1 ] , \\nonumber $ ] where @xmath48 is any mapping from @xmath49  @xcite . the second equality of   follows from the bayes rule , the third follows from the gaussian models @xmath50 and @xmath51 , and the last is obtained by deriving the third line with respect to @xmath19 .",
    "the close - form map estimate   can be calculated fast .",
    "observe that @xmath16 is a sparse extraction matrix , each row containing only one non - zero entry with value @xmath52 , whose index corresponds to the non - zero entry in @xmath15 .",
    "therefore , the multiplication operations that involve @xmath22 or @xmath53 can be realized by extracting the appropriate rows or columns at zero computational cost .",
    "the complexity of   is therefore dominated by the matrix inversion . as @xmath54 is positive - definite , @xmath55 can be implemented with @xmath56 flops through a cholesky factorization  @xcite .    in a typical case where",
    "@xmath19 is the @xmath10-th row of the matrix @xmath3 , @xmath28 , to estimate @xmath3 the total complexity of the e - step is therefore dominated by @xmath57 flops . for typical rating prediction datasets that are highly sparse , among a large number of items @xmath30 , most users have rated more or less only a small number @xmath58 of items , where @xmath59 is large .",
    "the total complexity of the e - step is thus dominated by @xmath60 flops .",
    "the parameters of the model are estimated / updated with the maximum likelihood estimate , @xmath61 with the gaussian model @xmath50 , it is well known that the resulting estimate is the empirical one , @xmath62    the empirical covariance estimate may be improved through regularization when there is lack of data ( let us take an example of standard rating prediction , where there are @xmath63 items and @xmath64 users , the dimension of the covariance matrix @xmath65 is @xmath66 ) .",
    "a simple and standard eigenvalue - based regularization is used here , @xmath67 where @xmath68 is a small constant .",
    "the regularization also guarantees that the estimate @xmath69 of the covariance matrix is full - rank , so that   is always well defined .    to estimate @xmath3 , the computational complexity of the m - step",
    "is dominated by the calculation of the empirical covariance estimate requiring @xmath70 flops , which is negligible with respect to the e - step .    as the map - em algorithm iterates , the map probability of the observed signals @xmath71 increases .",
    "this can be observed by interpreting the e- and m - steps as a coordinate descent optimization  @xcite . in the experiments ,",
    "the algorithm converges within 10 iterations .",
    "the map - em algorithm is initialized with an initial guess of @xmath19 , @xmath43 .",
    "the experiments show that the result is insensitive to the initialization for movie rating prediction . in the numerical experiments ,",
    "all the unknown entries are initialized to 3 for datasets containing ratings ranging from 1 to 5 or 6 .      in a typical case where the matrix row decomposition in   is considered , the overall computational complexity of the map - em to estimate an @xmath72 matrix",
    "is dominated by @xmath73 , with @xmath74 the number of iterations ( typically @xmath75 ) and @xmath76 the available data ratio , with @xmath59 typically large ( @xmath77 for the standard movie ratings data ) .",
    "the algorithm is thus very fast . as each row @xmath19",
    "is treated as a signal and the signals can be estimated in sequence , the memory requirement is dominated by @xmath78 ( to store the covariance matrix @xmath33 ) .",
    "the experimental protocols strictly follow those described in the literature  @xcite .",
    "the proposed method is evaluated on two movie rating prediction benchmark datasets , the _ eachmovie _ dataset and the _ 1 m movielens _ dataset .",
    "two test protocols , the so - called `` weak generalization '' and `` strong generalization , ''  @xcite are applied .    * * weak generalization * measures the ability of a method to generalize to other items rated by the _",
    "users used for training the method .",
    "one known rating is randomly held out from each user s rating set to form a test set , the rest known ratings form a training set . the method is trained using the data in the training set , and its performance is evaluated over the test set . * * strong generalization * measures the ability of the method to generalize to some items rated by _ novel _ users that have _ not _ been used for training .",
    "the set of users is first divided into training users and test users .",
    "learning is performed with all available ratings from the training set . to test the method ,",
    "the ratings of each test users are further split into an observed set and a held - out set .",
    "the method is shown the observed ratings , and is evaluated by predicting the held - out ratings .",
    "the eachmovie dataset contains 2.8 million ratings in the range @xmath79 for 1,648 movies ( columns ) and 74,424 users ( rows ) . following the standard procedure  @xcite ,",
    "users with fewer than 20 ratings and movies with less than 2 ratings are removed .",
    "this leaves us 36,656 users , 1,621 movies , and 2.5 million ratings ( available data ratio @xmath80 ) .",
    "we randomly select 30,000 users for the weak generalization , and 5,000 users for the strong generalization .",
    "the 1 m movielens dataset contains 1 million ratings in the range @xmath81 for 3,900 movies ( columns ) and 6,040 users ( rows ) . the same filtering leaves us 6,040 users , 3,592 movies , and 1 million ratings ( available data ratio @xmath82 ) .",
    "we randomly select 5,000 users for the weak generalization , and 1,000 users for the strong generalization .",
    "each experiment is run 3 times and the average reported .",
    "the performance of the method is measured by the standard normalized mean absolute error ( nmae ) , computed by normalizing the mean absolute error by a factor for which random guessing produces a score of 1 .",
    "the factor is 1.944 for eachmovie , and 1.6 for movielens .",
    "in contrast to most exiting algorithms in the literature , the proposed method , thanks to its simplicity , enjoys the advantage of having very few intuitive parameters .",
    "the covariance regularization parameter @xmath68 in   is set equal to @xmath83 ( whose square root is one order of magnitude smaller than the maximum rating ) , the results being insensitive to this value as shown by the experiments .",
    "the number of iterations of the map - em algorithm is fixed at @xmath84 , beyond which the convergence of the algorithm is always observed .",
    "the noise @xmath35 in   is neglected , i.e. , @xmath85 is set to @xmath86 , as the movie rating datasets mainly involve missing data , the noise being implicit and assumed negligible .",
    "the experiments show that considering row @xmath11 of the matrix @xmath0 as signals leads to slightly better results than taking columns or 2d sub - matrices .",
    "this means that each user is a signal , whose dimension is the number of movies .    as in previous works  @xcite , a post - processing that projects the estimated rating to an integer within the rating range",
    "is performed .",
    "a matlab code of the proposed algorithm is available at http://www.cmap.polytechnique.fr/~yu/research/mc/demo.html[_http://www.cmap.polytechnique.fr/@xmath87yu/research/mc/demo.html_ ] .",
    "the results of the proposed method are compared with the best published ones including user rating profile ( urp )  @xcite , attitude  @xcite , maximum margin matrix factorization ( mmmf )  @xcite , ensemble of mmmf ( e - mmmf )  @xcite , item proximity based collaborative filtering ( ipcf )  @xcite , gaussian process latent variable models ( gplvm )  @xcite , mixed membership matrix factorization ( m@xmath88f )  @xcite , and nonparametric bayesian matrix completion ( nbmc )  @xcite . for each of these methods ,",
    "more than one results produced with different configurations are often reported , among which we systematically cite the best one .",
    "all these methods are significantly more complex than the one here proposed",
    ".    tables  [ tab : each : movie ] and  [ tab:1m : movielens ] presents the results of various methods for both weak and strong generalizations on the two datasets .",
    "nbmc generates most often the best results , followed closely by the proposed method referred to as gm ( gaussian model ) and gplvm , all of them outperforming the other methods .",
    "the results produced by the proposed gm , with a by far simpler model and faster algorithm , is in the same ballpark as those of nbmc and gplvm , the difference with nmae being smaller than about 0.005 , marginal in the rating range that goes from 1 to 5 or 6 .",
    ".nmaes generated by different methods for eachmovie database .",
    "the smallest nmae is in boldface . [",
    "cols=\"^,^,^\",options=\"header \" , ]",
    "we have shown that a gaussian model and a map - em algorithm provide a simple and computational efficient solution for matrix completion , leading to results in the same ballpark as state - of - the - art ones for movie rating prediction .",
    "future work may go in several directions .",
    "on the one hand , the proposed conceptually simple and computationally efficient method may provide a good baseline for further refinement , for example by incorporating user and item bias or meta information  @xcite . on the other hand ,",
    "gaussian mixture models ( gmm ) that have been shown to bring dramatic improvements over single gaussian models in image inpainting  @xcite , are expected to better capture different characteristics of various categories of movies ( comedy , action , etc . ) and classes of users ( age , gender , etc . ) .",
    "however , no significant improvement by gmm over gaussian model has yet been observed for movie rating prediction .",
    "this needs to be further investigated , and such improvement might come from proper grouping and initialization .",
    "* acknowledgments : * work partially supported by nsf , onr , nga , aro , and nsseff . we thank s. mallat for co - developing the proposed framework , and m. zhou and l. carin for their comments and help with the data ."
  ],
  "abstract_text": [
    "<S> a general framework based on gaussian models and a map - em algorithm is introduced in this paper for solving matrix / table completion problems . </S>",
    "<S> the numerical experiments with the standard and challenging movie ratings data show that the proposed approach , based on probably one of the simplest probabilistic models , leads to the results in the same ballpark as the state - of - the - art , at a lower computational cost .    </S>",
    "<S> matrix completion , inverse problems , collaborative filtering , gaussian mixture models , map estimation , em algorithm </S>"
  ]
}