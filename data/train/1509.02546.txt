{
  "article_text": [
    "advances in measurement techniques and statistical inference methods allow us to characterize the connectivity properties of large biological systems such as neural and gene regulatory networks @xcite . in many cases connectivity",
    "is shown to be well modeled by a combination of random and deterministic components .",
    "for example , in neural networks , the location of neurons in anatomical or functional space , as well as their cell - type identity influences the likelihood that two neurons are connected @xcite .    for these reasons",
    "it has become increasingly popular to study the spectral properties of structured but random connectivity matrices using a range of techniques from mathematics and physics @xcite . in most cases ,",
    "the spectrum of the random matrix of interest is studied independently of the dynamics of the biological network it implies .",
    "therefore , these results can be used only to make statements about the dynamics of a linear system where knowing the eigenvalues and eigenvectors is sufficient to characterize the dynamics .    here",
    "we study the dynamics of nonlinear random recurrent networks with a continuous synapse - specific gain function that can depend on the pre- and post - synaptic neurons locations in an anatomical or functional space .",
    "these networks become spontaneously active at a critical point that is derived here , directly related to the boundary of the spectrum of a new random matrix model . given the gain function",
    "we predict analytically the network s leading principal components in the space of individual neurons autocorrelation functions .    in the context of analysis of single and multi - unit recordings",
    "our results offer a mechanism for relating structured recurrent connectivity to functional properties of individual neurons in the network ; and suggest a natural reduced space where the system s trajectories can be fit by a simple state - space model .",
    "recently we showed how a certain type of mesoscopic structure can be introduced into the class of random recurrent network models by drawing synaptic weights from a finite number of cell - type - dependent probability distributions @xcite .",
    "in contrast to networks with a single cell - type @xcite , these networks can sustain multiple dynamic global modes .    here",
    "these results are further generalized to networks where the synaptic weight between neurons  @xmath1 is drawn at random from a distribution with mean 0 and variance @xmath2 , where @xmath3 is the size of the network .",
    "the smoothness conditions satisfied by the gain function @xmath0 are stated below .",
    "this allows us to treat , for example , networks with continuous spatial modulation of the synaptic gain .",
    "the solution to the network s system of mean - field equations that we derive offers a new view - point on how functional properties of single neurons can in fact be a network phenomenon .",
    "consider a general synapse - specific gain function @xmath4 that depends on normalized neuron indices @xmath5 , where @xmath6 .",
    "we assume that there is some length scale @xmath7 below which @xmath0 has no discontinuities .",
    "that is , we let @xmath8 ^ 2\\to\\mathbb{r}_+$ ] be a uniformly bounded , continuous function everywhere on the unit square except possibly on a measure zero set @xmath9 .",
    "the function @xmath0 may depend on @xmath3 in such a way that its lipschitz constant @xmath10 , with @xmath11 and @xmath12 .",
    "every point where @xmath0 does not satisfy the above smoothness conditions must be on the boundary between squares of side @xmath13 where it does .",
    "the network connectivity matrix is then @xmath14 with elements @xmath15 where @xmath16 is a random matrix with elements drawn at random from a distribution with mean 0 , variance @xmath17 and finite fourth moment . in the simulations we use a gaussian distribution unless noted otherwise .    in this paper",
    "we analyze the the eigenvalue spectrum of the connectivity matrix @xmath18 and the corresponding dynamics of the neural network .",
    "note that by requiring that @xmath0 is bounded and differentiable on the unit square outside of @xmath9 we allow the synaptic gain function to be a combination of discrete modulation ( e.g. cell - type dependent connectivity for distinct cell - types , as in @xcite ) and of continuous modulation ( e.g. networks with heterogeneous and possibly correlated in- and out - degree distributions , as in @xcite ) .    when @xmath0 can be written as an outer product of two vectors ( i.e. @xmath19 ) , the model discussed here coincides with that studied by wei and by ahmadian et al .",
    "@xcite .",
    "the spectral density of @xmath18 is circularly symmetric in the complex plane , and is supported by a disk centered at the origin with radius @xmath20 with @xmath21\\right\\},\\ ] ] where @xmath22 is a deterministic matrix with elements @xmath23_{ij } = \\frac{1}{n}g^2(z_i , z_j)$ ] .",
    "note that @xmath24 is the perron - frobenious eigenvalue of a non - negative matrix , so indeed @xmath25 .",
    "for general synapse - specific gain function @xmath0 it has not been possible so far to obtain an explicit formula for @xmath24 .",
    "however , we have been able to derive explicit analytic formulae in three cases of biological significance .",
    "first , in section [ sec : example ] we discuss the case where @xmath26 is a circulant matrix such that @xmath27 with @xmath28 and show that @xmath29 .",
    "this special case is important for large neural networks where connectivity often varies smoothly as a function of neuron s index . in section [ sec : degdist ]",
    "we derive the support of the bulk spectrum and the outliers of a random connectivity matrix with heterogeneous joint in- and out - degree distribution .",
    "finally , in section [ sec : ecology ] we discuss a third example pertinent to large scale models of ecosystems .",
    "these systems are often modeled using @xmath0 that has a triangular structure and again there is an analytic formula for @xmath24 in this case .",
    "given the connectivity matrix @xmath18 defined in eq .",
    "( [ eq : j ] ) , the dynamics of neural network model with @xmath3 neurons is described by @xmath30 where @xmath31 $ ] .",
    "the @xmath32 variables can be thought of as the membrane potential of each neuron , and the @xmath33 variables as the deviation of the firing rates from their average values .",
    "using a modified version of dynamic mean field theory we show that in the limit @xmath34 this system undergoes a phase transition , where @xmath35 is the coordinate that describes this transition and @xmath36 is the critical point . below the critical point ( @xmath37 )",
    ", the neural network has a single stable fixed point at @xmath38 . above the critical point",
    "the system is chaotic .",
    "we analyze the dynamics above the critical point in more detail and find a direct link between the network structure ( @xmath0 ) and its functional properties . to that end",
    "we define @xmath3 dimensional autocorrelation vectors @xmath39 where @xmath40 denotes average over the ensemble of matrices @xmath18 and time .",
    "these vectors are restricted to the potentially low dimensional subspace spanned by the right eigenvectors of @xmath26 with corresponding eigenvalues that have real part greater than @xmath41 .",
    "thus , although the network dynamics are chaotic , they are confined to a low dimensional space , which has been suggested as a mechanism that could make computation in the network more robust @xcite .",
    "we begin by recalling our recent results for a function @xmath0 that has block structure .",
    "we defined a @xmath42 matrix with elements @xmath43 and partitioned the indices @xmath44 into @xmath45 groups , with the @xmath46-th partition have a fraction @xmath47 neurons .",
    "the synaptic gain function was then defined by @xmath48 , where @xmath49 is the partition index of the @xmath50-th neuron .",
    "defining @xmath51 allows us to write formally @xmath52\\right.\\right\\}$ ] . with these definitions ,",
    "we rewrite eq .",
    "( [ eq : dyn0 ] ) in a form that emphasizes the separate contributions from each group to a neuron : @xmath53    in @xcite we used the dynamic mean field approach @xcite to study the network behavior in the @xmath54 limit . averaging eq .",
    "( [ eq : dyn ] ) over the ensemble from which @xmath18 is drawn implies that neurons that belong to the same group are statistically identical .",
    "therefore , to represent the network behavior it is enough to look at the activities @xmath55 of @xmath45 representative neurons and their inputs @xmath56 .    the stochastic mean field variables @xmath57 and @xmath58 will approximate the activities and inputs in the full @xmath3 dimensional network provided that they satisfy the dynamic equation @xmath59 and provided that @xmath56 is drawn from a gaussian distribution with moments satisfying the following conditions .",
    "first , the mean @xmath60 for all @xmath61 .",
    "second , the correlations of @xmath62 should match the input correlations in the full network , averaged separately over each group . using eq .",
    "( [ eq : mfield ] ) and the property @xmath63 we get the self - consistency conditions : @xmath64 where @xmath65 denotes averages over @xmath66 and @xmath67 in addition to average over realizations of @xmath18 .",
    "the average firing rate correlation vector is denoted by @xmath68 .",
    "its components ( using the variables of the full network ) are @xmath69\\phi[x_i(t+\\tau)]\\right\\rangle,\\ ] ] translating to @xmath70\\phi[\\xi_d(t+\\tau)]\\right\\rangle\\ ] ] using the mean field variables .",
    "importantly , the covariance matrix @xmath71 with elements @xmath72 is diagonal , justifying the definition of the vector @xmath73 . with this",
    "in hand we rewrite eq .",
    "( [ eq : mf_constraints ] ) in matrix form as @xmath74 where @xmath75 is a constant matrix reflecting the network connectivity structure : @xmath76 .    a trivial solution to this equation is @xmath77 which corresponds to the silent network state : @xmath78 .",
    "recall that in the network with a girko matrix as its connectivity matrix ( @xmath79 ) , the matrix @xmath80 is a scalar and eq .",
    "( [ eq : matrix_form ] ) reduces to @xmath81 . in this case",
    "the silent solution is stable only when @xmath82 . for @xmath83",
    "the autocorrelations of @xmath62 are non - zero which leads to chaotic dynamics in the @xmath3 dimensional system @xcite .    when @xmath84 , eq .",
    "( [ eq : matrix_form ] ) can be projected on the eigenvectors of @xmath85 leading to @xmath45 consistency conditions , each equivalent to the single group case .",
    "each projection has an effective scalar given by the eigenvalue in place of @xmath86 in the @xmath79 case .",
    "hence , the trivial solution will be stable if all eigenvalues of @xmath85 have real part @xmath87 .",
    "this is guaranteed if @xmath24 , the largest eigenvalue of @xmath85 , is @xmath87 . if @xmath88 the projection of eq .",
    "( [ eq : matrix_form ] ) on the leading eigenvector of @xmath85 gives a scalar self - consistency equation analogous to the @xmath79 case for which the trivial solution is unstable .",
    "as we know from the analysis of the @xmath79 case , this leads to chaotic dynamics in the full network .",
    "therefore @xmath89 is the critical point of the @xmath84 network .",
    "furthermore , the fact that in the @xmath79 case the presence of the destabilized fixed point at @xmath38 corresponds to a finite mass of the spectral density of @xmath18 with real part @xmath90 @xcite allowed us to read the radius of the support of the connectivity matrix with @xmath84 and identify it as @xmath20 @xcite .      the vector dynamic mean field theory we developed in @xcite relies on having an infinite number of neurons in each partition with the same statistics .",
    "the natural choice is therefore to have the size of each group of neurons be linear in the system size : @xmath91 .",
    "this scaling imposes two limitations if one wishes to compare the results to the dynamics of more realistic networks .",
    "it requires knowledge of the cell - type - identity of each neuron in the recording , which often is not available ; and it confines the statements we are able to make about the dynamics to quantities that are averaged over neurons that belong to the same cell - type .    to lift the requirement of block structured variances ( i.e. now @xmath92 ) , we can do the following .",
    "let @xmath93 be a weakly monotonic function of @xmath3 such that @xmath94 recall that @xmath95 and that the lipschitz constant of @xmath0 scales as @xmath96 , implying that @xmath97 .",
    "a natural choice is @xmath98 with @xmath99 , but as long as @xmath100 the specific scaling behavior will not matter in our analysis . for convenience we will suppress the @xmath3 dependence when possible .",
    "let @xmath101 and let @xmath102\\right.\\right\\}.\\label{eq : mui}\\ ] ] furthermore , define @xmath103 with elements @xmath104 in other words , @xmath105 is an @xmath106 matrix with @xmath107 equally sized square blocks .",
    "the value of elements in each block is the value of the function @xmath0 in the middle of that block .",
    "these definitions allow us to bridge the gap between the block and the continuous cases for the following reasons .",
    "consider the random connectivity matrix with elements @xmath108 and the network that has @xmath109 as its connectivity .",
    "first , since @xmath110 as @xmath34 , the number of neurons in each group goes to infinity , and we may use the vector dynamic mean field theory as before , but in a @xmath111 dimensional space ( rather than @xmath45 which was @xmath112 ) .",
    "the critical point is now given in terms of the largest eigenvalue of an @xmath106 matrix @xmath113 with elements @xmath114 where @xmath115 .",
    "second , recall that the partitioning of the matrix @xmath105 depends on @xmath3 and the function @xmath0 is assumed to be smooth outside of a set with measure zero @xmath9 .",
    "these properties will allow us to show ( see appendix [ sec : kn ] ) that as @xmath34 we have @xmath116_{ij},\\ ] ] meaning that by studying the system with connectivity structure @xmath105 in the limit @xmath34 we are in fact obtaining results for the generalized connectivity matrix with a smooth synaptic gain function @xmath0 .",
    "in @xcite we used random matrix theory techniques to derive , for the case of block - structured @xmath18 , an implicit equation that the full spectral density of @xmath18 satisfies .",
    "the circular symmetry of the spectrum for that case is obvious because the equations ( see eq . 3.6 in @xcite ) depend on the complex variable @xmath117 only through @xmath118 .",
    "similar implicit equations , with integrals instead of sums , can be written for the continuous case .",
    "rigorous mathematical analysis of the spectral density implied by such equations is beyond the scope of this paper and will be presented elsewhere .",
    "nevertheless , the integral equations still depend on @xmath118 , supporting the circular symmetry of the spectrum .",
    "to study the spontaneous dynamics above the critical point we recall again the analogous result for a matrix with block structure . the @xmath45 dimensional average autocorrelation vectors @xmath119 ( see definition below ) are restricted to a @xmath120 dimensional subspace , where @xmath120 is the number of eigenvalues of @xmath85 with real part @xmath90 ( i.e. the algebraic multiplicity of these eigenvalues ) .",
    "this result is obtained by projecting eq .",
    "( [ eq : matrix_form ] ) on the schur basis vectors of @xmath85 @xcite .",
    "the definitions of the @xmath121 component of these vectors are @xmath122 and the @xmath120 dimensional subspace is @xmath123 where @xmath124 are the right eigenvectors of @xmath85 in descending order of the real part of their corresponding eigenvalue ( see examples in fig . [",
    "fig : spectrum ] ) .",
    "an equivalent statement is that , independent of the lag @xmath125 , projections of the vectors @xmath119 on any vector in the orthogonal complement subspace @xmath126 are approximately @xmath127 .",
    "note that for asymmetric ( but diagonalizable ) @xmath85 , @xmath126 is spanned by the left rather than the right eigenvectors of @xmath85 : @xmath128      we can repeat the analysis of @xcite for a network with connectivity @xmath129 that has @xmath107 blocks , and for each @xmath130 obtain the subspace @xmath131 that the @xmath111 dimensional autocorrelation vectors @xmath132 are restricted to .",
    "these vectors have components @xmath133    now when we take the limit @xmath34 the dimensionality of the autocorrelation vectors @xmath132 becomes infinite as well , but the subspace @xmath131 may be of finite dimension @xmath134 , where @xmath134 is the algebraic multiplicity of eigenvalues of @xmath113 with real part greater than 1 ( see section [ sec : example ] for an example ) .",
    "we have shown that for @xmath0 that satisfies the smoothness conditions , studying the network with connectivity @xmath135 is equivalent to studying the network with connectivity @xmath109 in the limit @xmath54 .",
    "therefore , in that limit , the individual neuron autocorrelation functions @xmath136 ( eq . [ eq : aci ] ) are restricted to the subspace spanned by the right eigenvectors of @xmath137 corresponding to eigenvalues with real part @xmath90 .",
    "this in fact is equivalent to , given the network structure @xmath0 , predicting analytically the leading principal components in the @xmath3 dimensional space of individual neuron autocorrelation functions ( see fig .",
    "[ fig : ac ] ) .",
    "note that traditionally principal component analysis is performed in the @xmath3 dimensional space of neuron firing rates rather than autocorrelation functions .",
    "numerical analysis performed in @xcite suggests that the system s trajectories , when considered in the space spanned by the vectors @xmath32 or @xmath138 ( individual neuron activations / firing - rates ) , occupy a space of dimension that is extensive in the system size @xmath3 .",
    "however , when considered in the space of individual neuron autocorrelation functions , the dimension of trajectories is intensive in @xmath3 and usually finite . in the subspace",
    "we derive here the information about the relative phases between neurons is lost , but the amplitude and frequency information is preserved .",
    "section [ sec : conc ] includes further discussion of the consequences of our predictions and how they may be applied .          for a finite system it is evident from numerical simulations that the @xmath3 dimensional vector of autocorrelation functions `` leaks '' : it has non - zero projections on inactive modes - eigenvectors of @xmath26 with corresponding eigenvalue which is @xmath87 ( see fig .",
    "[ fig : ac ] ) . here",
    "we study the magnitude of this effect , and specifically its dependence on @xmath3 and on the model s structure function @xmath0 . for simplicity",
    ", we will study the projections of the autocorrelation vector @xmath139 at lag @xmath140 .",
    "let @xmath141 where @xmath142 is an @xmath143 matrix with columns equal to orthogonalized eigenvectors of @xmath26 ( i.e. the schur basis vectors ) with corresponding eigenvalue less than 1 , see eqs .",
    "( [ eq : um ] ) and ( [ eq : umperp ] ) . here",
    "@xmath40 denotes averaging over an ensemble of connectivity matrices ( with the same structure @xmath0 and same size @xmath3 ) .",
    "consider the homogeneous network ( i.e. constant @xmath144 ) .",
    "now @xmath142 contains all the vectors in @xmath145 perpendicular to the dc mode @xmath146 $ ] .",
    "thus , @xmath147 is simply the variance over the neural population of the individual neuron autocorrelation functions at lag @xmath140 .",
    "we can now use the mean - field approximation to determine the @xmath3 dependence of @xmath147 . for @xmath148 , the elements of the vector @xmath149 follow a scaled @xmath150 distribution @xmath151 where @xmath152 and @xmath153 is the standard chi - squared distribution with @xmath3 degrees of freedom .",
    "thus , in this limit , @xmath154 the autocorrelation function is in general a single neuron property .",
    "therefore , their variation about the mean is uncorrelated across neurons independent of the network s structure : @xmath155 .",
    "thus , we can use the notation @xmath156 .",
    "similarly , in the case with @xmath45 partitions , @xmath157 finally , for @xmath158 partitions , @xmath159/n.\\label{eq : varck}\\end{aligned}\\ ] ] at this stage , eq .",
    "( [ eq : varck ] ) remains ambiguous because the function @xmath158 is not a property of the neural network model .",
    "rather , it is a construction we use to show that in the limit @xmath54 we are able to characterize the dynamics using the vector dynamic mean field approach .",
    "therefore , for finite @xmath3 we now wish to estimate an appropriate value of @xmath160 $ ] .",
    "this can be done by noting that the network with block structured connectivity is a special case of the one with a continuous structure function . for that special case",
    "we know that @xmath161=d$ ] .",
    "since @xmath0 is smooth , for sufficiently large @xmath162 , we can assume that in each block @xmath0 is linear in both variables @xmath163 and @xmath164 : @xmath165 here @xmath166 is the first derivative of @xmath0 with respect to the first variable , evaluated in the middle of the @xmath167 block .",
    "the only expression for @xmath161 $ ] that depends on first derivatives of @xmath0 and agrees with the homogeneous and block cases is @xmath168 & \\approx & 1 + \\iint \\left[\\left|g^{(1,0)}(x , y)\\right|+\\left|g^{(0,1)}(x , y)\\right|\\right]dxdy\\nonumber \\\\           & \\approx & 1 + \\iint \\|\\nabla g\\|dxdy.\\end{aligned}\\ ] ]    we are unable to test this prediction quantitatively , because we do not know the dependence of the function @xmath169 on the structure @xmath0 .",
    "we are able to show however that the dependence on @xmath3 is the same as for the block models , which is confirmed by numerical simulations ( compare solid purple , orange and red lines in fig .",
    "[ fig : ac]f ) . in the cases where @xmath0 depends on @xmath3 ,",
    "the value of @xmath161 $ ] will also depend on @xmath3 , such that the scaling of the `` leak '' may no longer be @xmath170 .",
    "when the matrix @xmath4 is circulant such that @xmath27 with @xmath171 the eigenvalues and eigenvectors of @xmath172 are given in closed form by integrals of the function @xmath173 and the fourier modes with increasing frequency .",
    "in particular , the largest eigenvalue @xmath174 corresponds to the zero frequency eigenvector @xmath175 $ ] . to show this ,",
    "consider the @xmath176 eigenvalue of the circulant matrix @xmath26 : @xmath177 so in the limit @xmath34 , @xmath178 as desired .",
    "as an example we study a network with ring structure that will be defined by @xmath179 , such that neurons that are closer are more strongly connected .",
    "this definition leads to the following form for the critical coordinate along which the network undergoes a phase transition @xmath180 interestingly , as @xmath181 increases continuously , additional discrete modes with increasing frequency over the network s spatial coordinate become active by crossing the critical point @xmath182 .",
    "when modes with sufficiently high spatial frequency have been introduced , nearby neurons may have distinct firing properties .",
    "in contrast to the ring network discussed above , the connectivity in real networks often depends on multiple factors",
    ". these could be the spatial coordinates of the cell body or the location in a functional space ( e.g. the frequency that each particular neuron is sensitive to ) .",
    "therefore we would like to consider a network where the function @xmath0 depends on the distance between neurons embedded in a multidimensional space .",
    "this problem was recently addressed by muir and mrsic - flogel @xcite by studying the spectrum of a specific type of euclidean random matrix . in their model , neurons were randomly and uniformly distributed in a space of arbitrary dimension , and the connectivity was a deterministic function of their distance . while their approach resolves the issue of the spectral properties of the random matrix when connectivity depends on distance in more than one dimension , the dynamics these matrices imply remain unknown .    to study the spectrum and the dynamics jointly , we define a network where neurons positions form a square @xmath183 grid ( with @xmath184 ) on the @xmath185 \\times [ 0,1]$ ] torus ( see fig .",
    "[ fig : torus]a ) : @xmath186 the positions of the neurons on the torus are schematized in fig .",
    "[ fig : torus]a .",
    "an analogue parameterization for @xmath0 to the one we used in the ring example which respects the toroidal geometry reads @xmath187\\big[\\cos\\big(2\\pi\\sqrt{n}z_{ij}\\big)+1\\big].\\label{eq : torusg}\\ ] ] note that now @xmath0 depends on @xmath3 , but it is bounded and its lipschitz constant scales as @xmath188 , so it satisfies the smoothness conditions .",
    "[ fig : torus]b shows the spectrum of @xmath189 and the corresponding eigenvectors , plotted on a torus .",
    "because there are non - uniform modes that are active ( two through five ) , then each neuron has a different participation in the vector of autocorrelation functions . in fig .",
    "[ fig : torus]c , d we show for a network with a range of @xmath3 values that indeed the vector of autocorrelation functions is restricted to the predicted subspace in contrast to the firing rate vector .",
    "the gain function analyzed here depends on a euclidean distance on the torus .",
    "other metrics , for example a city - block norm , can be treated similarly .",
    "overall these results provide a mechanism whereby continuous and non - fine tuned connectivity that depends on a single or multiple factors can lead to a few active dynamic modes in the network .",
    "importantly , the modes maintained by the network inherit their structure from the deterministic part of the connectivity .",
    "here we will use our general result to compute the spectrum of a random connectivity matrix with specified in- and out - degree distributions .",
    "realistic connectivity matrices found in many biological systems have degree distributions which are far from the binomial distribution that would be expected for standard erds - rnyi networks @xcite . specifically , they often exhibit correlation between the in- and out - degrees , clustering , community structures and possibly heavy - tailed degree distributions @xcite .",
    "we consider a connectivity matrix appropriate for a neural network model . since each element of this matrix will have a non - zero mean ,",
    "our current theory can not make statements about the dynamics .",
    "nevertheless the spectrum of the connectivity matrix is important on its own as a step towards understanding the behavior of random networks with general and possibly correlated degree distributions .",
    "consider a network with @xmath190 excitatory and @xmath191 inhibitory neurons ( @xmath192 ) .",
    "each inhibitory neuron has incoming and outgoing connections with probability @xmath193 to and from every other neuron in the network . within the excitatory subnetwork ,",
    "degree distributions are heterogeneous .",
    "specifically , @xmath194 are the average excitatory in- and out - degree sequences that are drawn from a joint degree distribution that could be correlated .",
    "we assume that @xmath195 , where @xmath196 is the mean connectivity , and that the marginals of the degree distribution are equal .",
    "define @xmath197 to be the @xmath190 dimensional vectors @xmath198 and @xmath199 .",
    "the matrix @xmath200 defines the probability of connections given the fixed normalized degree sequences and @xmath193 : @xmath201 the random adjacency matrix is then @xmath202 .",
    "note that because the adjacency matrix is random , @xmath203 and @xmath204 are the _ average _ in- and out - degree sequences .",
    "the connectivity matrix is then @xmath205 with @xmath206 where @xmath207 is the ratio of the synaptic weight of inhibitory to excitatory synapses .    to leading order",
    ", the distribution of eigenvalues of @xmath18 will depend only on the mean and variance of its elements , which are summarized in the deterministic matrices @xmath208 ( means ) and @xmath209 ( variances ) with elements @xmath210_{ij } & = & p_{ij}\\left(1-p_{ij}\\right)w_{ij}^2\\end{aligned}\\ ] ]    we will show that @xmath211 ( generically for large @xmath3 and non - fine tuned parameters @xmath212 ) . in @xcite ,",
    "tao studied the spectrum of the sum of a random matrix with independent and identically distributed elements and a low - rank perturbation .",
    "the outlying eigenvalues of such a matrix fluctuate around the non - zero eigenvalues of the low - rank perturbation provided that they are outside of the bulk spectrum originating from the random part .",
    "a modification of the arguments in @xcite can be used to show that the same is true for the sum of a random matrix with independent but not identically distributed elements and a low - rank perturbation .    combining these",
    ", we expect that if the non - zero eigenvalues of @xmath208 are outside of the bulk that originates from the random part , the spectrum of the matrix @xmath18 ( with non - zero means ) will be approximately a composition of the bulk and outliers that can be computed separately and that the approximation will become exact as @xmath34 .",
    "this is verified through numerical calculations ( fig .",
    "[ fig : degdist ] ) .    viewing the normalized degree sequences @xmath197 as deterministic variables",
    "we define @xmath213    given the parameters @xmath214 , we show in appendix [ sec : cp ] that @xmath215 ( generically for large @xmath3 and non - fine tuned parameters @xmath216 ) and its characteristic polynomial is @xmath217 with    @xmath218\\nonumber\\\\ a_3 & = & n_iw_0 ^ 2p_0\\left(1-p_0\\right)\\left\\ { \\mathcal{r}-\\mathcal{zt}+p_0\\left(1-p_0\\right)\\left[\\mathcal{s}^2-\\mathcal{u}^2-n_e\\left(\\mathcal{t}-\\mathcal{z}\\right)\\right]\\right\\ } \\nonumber\\\\ a_4 & = & n_iw_0 ^ 2p_0 ^ 2\\left(1-p_0\\right)^2\\left[n_e\\left(\\mathcal{zt}-\\mathcal{r}\\right)-\\mathcal{z}\\mathcal{s}^2-\\mathcal{u}^2\\mathcal{t}+\\mathcal{suv}\\right],\\label{eq : cpg}\\end{aligned}\\ ] ]    and @xmath219 for @xmath220 .",
    "therefore , using our results , the radius of the bulk spectrum of @xmath18 is equal to the square - root of the largest solution to @xmath221 .",
    "furthermore we show that the non - zero eigenvalues of @xmath208 are equal to the roots of the polynomial @xmath222 , with @xmath223\\nonumber\\\\ b_3 & = & n_iw_0p_0 ^ 2\\left[n_e\\mathcal{t}-\\mathcal{s}^2\\right],\\label{eq : cpq}\\end{aligned}\\ ] ] and @xmath224 for @xmath225 , such that the outlying eigenvalues of @xmath18 are approximated by the roots of @xmath226 that lie outside of the bulk .    , where @xmath227 and @xmath228 are the form and scale parameters respectively of the @xmath229 distribution from which the in- and out - degree sequences are randomly drawn .",
    "the average correlation @xmath230 between the in- and out - degree sequences was varied between 0 and 1 .",
    "for the values @xmath231 ( left ) and @xmath232 ( right ) we drew 25 degree sequences and based on them drew the connectivity matrix according to the prescription outlined in section [ sec : degdist ] .",
    "the eigenvalues of each matrix were computed numerically and are shown in black .",
    "for each value of @xmath230 we computed the average functions @xmath233 etc . and the roots of the characteristic polynomials @xmath234 and @xmath235 ( see appendices [ sec : cp ] and [ sec : gamma ] for derivation ) .",
    "the predictions for the support of the bulk ( red ) and the outliers ( orange ) are in agreement with the numerical calculation .",
    "inset : as a function of @xmath230 , there is a positive outlier that exits the disk to the right . ]",
    "if the degree sequences are not specified , but only the joint in- and out - degree distribution they are drawn from , the random matrix @xmath18 will be constructed in two steps : first @xmath236 and @xmath237 are drawn from their joint in- and out - degree distribution , and then the elements of @xmath18 are drawn using the prescription outlined above . in such cases , one can in principle compute the averages @xmath238 , etc .",
    ", in terms of the moments of the joint degree distribution , and substitute these averages into the formulae we give assuming the degree sequences are fixed .",
    "we have carried out that calculation ( appendix [ sec : gamma ] ) for @xmath229 degree distributions with form parameter @xmath227 , scale parameter @xmath228 and arbitrary correlation @xmath230 of the in- and out - degree sequences ( see fig .",
    "[ fig : degdist ] ) .",
    "we find that , for fixed marginals , the radius of the bulk spectrum depends extremely weakly on the correlation of the in- and out - degree sequences ( see red line in inset to fig .",
    "[ fig : degdist ] ) .",
    "the matrix @xmath208 however has a real , positive eigenvalue that for typical examples increases monotonically with the correlation , such that for some value it exits the bulk to the right ( see fig .",
    "[ fig : degdist ] ) .",
    "work by roxin @xcite , schmeltzer et al .",
    "@xcite and unpublished work by landau and sompolinsky @xcite has shown that the broadness and correlation of the joint degree distribution can lead to qualitative changes in the behavior of a spiking network .",
    "further work is required to investigate whether and why these changes can be explained by the spectrum of the connectivity matrix derived here .",
    "the past few years have seen a resurgence of interest in the use of methods from random matrix theory to study the stability of ecosystems @xcite . while the original work by robert may assumed a random unstructured connectivity pattern between species @xcite , experimental data shows marked departures from random connectivity @xcite .",
    "this includes hierarchical organization within ecosystems where larger species have asymmetric effect on smaller species , larger variance in the number of partners for a given species @xcite , and fewer cycles involving three or more interacting species than would be expected from an erds - rnyi graph @xcite .",
    "a popular model for food web structure is the cascade model @xcite , where species are rank - ordered , and each species can exclusively prey upon lower - ranked species .",
    "the differential effects between predators and prey in the cascade model can be described using connectivity matrices with different statistics for entries above and below the diagonal @xcite : @xmath239 with @xmath240 where @xmath241 is the heaviside step function .",
    "we use the convention @xmath242 . here",
    ", @xmath18 describes the interactions between different species in the ecosystem . for @xmath243 and sufficiently larger than @xmath244 ,",
    "the entries above ( below ) the diagonal are positive ( negative ) , so the matrix describes a perfectly hierarchical food web , where the top - ranked species consumes all the other species , the second species consumes all the species but the first , and so on .",
    "we will focus on the random part of the matrix ( i.e. we set @xmath245 ) .",
    "the spectrum of the sum of the deterministic and random parts remains a problem for future study .",
    "note that since the deterministic part has full rank , one can not apply simple perturbation methods .    according to our analysis ,",
    "the support of the spectrum of @xmath18 is a disk with radius @xmath246 , @xmath247 $ ] , and @xmath248 following the derivation in @xcite we will show that @xmath249 .    the characteristic polynomial @xmath250 is simplified by subtracting the @xmath251 column from the @xmath50-th column for @xmath252 giving @xmath253 where we have defined @xmath254 and @xmath255 .",
    "this simplifies to the recursion relation @xmath256 . taking into account that @xmath257 , this recursion relation can be solved , giving : @xmath258.\\end{aligned}\\ ] ] setting the characteristic polynomial @xmath259 to 0 leads to the equation @xmath260 which has multiple roots @xmath261 we are interested in the largest among the @xmath3 roots , which is real and positive .",
    "taking into account the dependence of @xmath262 and @xmath263 on @xmath3 , we find that : @xmath264 = \\frac{g_a^2-g_b^2}{\\log\\left(\\frac{g_a^2}{g_b^2}\\right)},\\ ] ] as desired .",
    "interestingly , for all values of @xmath244 the spectral radius of @xmath18 is smaller than the radius of the network if the predator - prey structure did not exist .",
    "the latter is equal to @xmath265 .",
    "this suggests that the hierarchical structure of the interaction network serves to stabilize the ecosystem regardless of how dominant the predators are over the prey .",
    "note however that in this model there are no correlations . in @xcite , it was shown numerically that correlations ( i.e. the expectation value of @xmath266 ) can dramatically change the stability of the network , compared with one that has no correlations .",
    "we studied jointly the spectrum of a new random matrix model and the dynamics of the neural network model it implies .",
    "we found that , as a function of the deterministic structure of the network ( given by @xmath0 ) , the network becomes spontaneously active at a critical point .    identifying a space where the dynamics of a neural network can be described efficiently and robustly is one of the challenges of modern neuroscience @xcite . in our model , above the critical point , the deterministic dynamics of the entire network are well approximated by a potentially low dimensional probability distribution , with dimension equal to the number of eigenvalues of a deterministic matrix that have real part greater than 1 .",
    "two limitations of using the results of our previous studies @xcite to interpret multi - unit recordings are that it requires knowing the cell - type identity of each neuron in the network and it only provides a prediction for quantities averaged over all neurons of a specific type .    here these are remedied .",
    "first , while some information about the connectivity structure is still required , this could be in the form of global spatial symmetries ( `` rules '' ) present in the network , such as the connectivity rule we used in the ring model .",
    "second , our analysis provides a prediction for single neuron quantities , namely the participation of every neuron in the network in the global active dynamic modes .",
    "existence of discrete network modules with no apparent fine - tuned connectivity has been shown to exist in networks of grid - cells in mammalian medial entorhinal cortex @xcite .",
    "these cells fire when the animal s position is on the vertices of a hexagonal lattice , and are thought to be important for spatial navigation .",
    "interestingly , when characterizing the firing properties of many such cells in a single animal one finds that the the lattice spacing of all cells belongs ( approximately ) to a discrete set that forms a geometric series @xcite .",
    "much work has been devoted to trying to understand how such a code could be used efficiently to represent the animal s location ( see for example @xcite ) and how such a code could be generated @xcite",
    ".    however , we are not aware of a model that explains how multiple modules ( sub - networks with distinct grid spacing ) could be generated without fine - tuned connectivity , which is not observed experimentally . in our model , continuous changes to a connectivity parameter can introduce additional discrete and spatially periodic modes into the network represented by finer and finer lattices .",
    "we are not arguing that the random network we are studying here could serve as a model of grid - cell networks , as there are many missing details that can not be accounted for by our model .",
    "nevertheless our analysis uncovers a mechanism by which a low - dimensional , spatially structured dynamics could arise as a result of random connectivity .",
    "the authors would like to thank nicolas brunel , stefano allesina , alex roxin and james heys for discussions .",
    "ja was supported by nsf crcns iis-1430296 .",
    "mv acknowledges a doctoral grant by fundaci `` la caixa '' and a travel grant by fundaci ferran sunyer i balaguer .",
    "tos was supported by grants no .",
    "r01ey019493 and no .",
    "p30 ey019005 and nsf career award ( iis 1254123 ) and by the salk innovations program .",
    "here we will show that the difference between the piecewise estimate @xmath105 and the continuous synaptic gain function @xmath0 goes to @xmath127 as @xmath34 .",
    "we assumed that the unit square can be tiled by square subsets of area @xmath268 where @xmath0 is bounded , differentiable , and its first derivative is bounded in each subset .",
    "note that the with lipschitz constant of @xmath0 can depend on @xmath3 , but @xmath13 can not .    for @xmath269 , recall our definitions for @xmath105 and @xmath270 ( eqs .",
    "[ eq : mui ] , [ eq : piecewise ] ) and define @xmath271\\times\\left ( k^{-1 } ( \\mu_j-1 ) , k^{-1 } \\mu_j\\right]$ ] . also recall our assumption each point is either inside a square with side @xmath13 within which there are no discontinuities or on the border of such a subset .",
    "thus , for @xmath272 we can assume that every constant region of @xmath105 is contained within a single square subset .",
    "we would like to show that for all @xmath1 @xmath273 since @xmath13 is independent of @xmath3 , we only have to show that eq .",
    "( [ eq : limn ] ) is true within a subset where @xmath0 satisfies the smoothness conditions .    using our definitions and the fact that @xmath0 has lipschitz constant @xmath274 ,    @xmath275^{\\frac{1}{2 } } =   c^0_l\\frac{n^{\\beta}}{2k}\\end{aligned}\\ ] ]",
    "so finally , @xmath276",
    "here we compute directly the characteristic polynomials of @xmath26 and @xmath208 ( eqs .",
    "[ eq : cpg ] , [ eq : cpq ] ) using the minor expansion formula .",
    "recall that @xmath192 , and let @xmath277 be the @xmath278 matrix with elements taken from the intersection of @xmath279 specific rows and columns of @xmath172 .",
    "the notation @xmath280 will indicate that exactly @xmath281 and @xmath282 of these rows and columns correspond to excitatory and inhibitory neurons , respectively .    for convenience we will use @xmath283 and @xmath284 .",
    "we would like to write an expression for the characteristic polynomial of @xmath285 using the sums over its diagonal minors @xmath286 where @xmath287 for @xmath288 and @xmath289 .",
    "the notation @xmath290 means a sum over all combinations of @xmath291 such that @xmath292 ( i.e. the so - called @xmath279-row diagonal minors of @xmath209 ) .",
    "we will compute @xmath293 explicitly and show that @xmath219 for @xmath220 .",
    "we begin by noting that the determinant of the @xmath294 matrix @xmath295 is 0 because the middle matrix is the sum of two rank 1 matrices .      by definition , @xmath289 .      the second coefficient",
    ", @xmath297 is simply the trace @xmath298 where in the second row we used the functions of the degree sequences ( eq . [ eq : degseqfunctions ] ) .",
    "the third coefficient @xmath299 is the sum of @xmath300 row diagonal minors .",
    "there are three types of diagonal minors , only two of which are non - zero @xmath301\\nonumber\\\\ \\det \\mathcal{g}_{0,2}^{(2 ) } & = & \\det\\left(\\begin{array}{cc } w & w\\\\ w & w \\end{array}\\right)=0\\end{aligned}\\ ] ] carrying out the summation over possible combinations @xmath302= n_iw\\left[\\mathcal{t}-\\mathcal{z}-v n_e\\right]\\end{aligned}\\ ] ] putting these together we get @xmath303.\\ ] ]      the fourth coefficient @xmath304 is the sum of all @xmath305 row diagonal minors .",
    "now there are four types of minors , only one of which is non - zero @xmath306\\nonumber\\\\ \\det \\mathcal{g}_{1,2}^{(2 ) } & = & \\det \\mathcal{g}_{0,3}^{(2)}=0\\qquad\\text{(repeated columns of inhibitory neurons)}\\end{aligned}\\ ] ] carrying out the sum @xmath307 + vw n_i\\frac{1}{2}\\sum_{i=1}^{n_e}\\sum_{j=1}^{n_e}\\left[x_jy_i\\left(1-x_jy_i\\right)+x_iy_j\\left(1-x_iy_j\\right)-x_iy_i\\left(1-x_iy_i\\right)-x_jy_j\\left(1-x_jy_j\\right)\\right]\\nonumber\\\\   & = & w n_i\\left\\ { \\mathcal{r}-\\mathcal{zt}+v\\left[\\mathcal{s}^2-\\mathcal{u}^2-n_e\\left(\\mathcal{t}-\\mathcal{z}\\right)\\right]\\right\\ } \\end{aligned}\\ ] ]      the last non - zero coefficient is @xmath308 , the sum of all @xmath309 row diagonal minors . here",
    "there are five types , only one of which is non - zero : @xmath310 carrying out the sum we get @xmath311\\end{aligned}\\ ] ]      now we show that @xmath219 for @xmath220 . a diagonal minor representing a subnetwork of five neurons or more",
    "can have @xmath313 , @xmath314 , or @xmath315 . if @xmath316 the diagonal minor is zero because of repeated columns .",
    "if @xmath314 , then @xmath317 . here , the determinant is a weighted sum of @xmath318 row diagonal minors of the form @xmath319 which is zero for @xmath320 .",
    "lastly if @xmath313 then again we have a sum of terms of the form @xmath321 which are zero as discussed above .",
    "using a similar approach we will compute the characteristic polynomial of @xmath208 and show that generically @xmath322 . using the sums over diagonal minors of @xmath323 @xmath324 where @xmath325 for @xmath288 and where @xmath326 is a @xmath278 matrix with elements taken from the intersection of @xmath279 rows and columns of @xmath208 .",
    "again , @xmath327 will indicate that @xmath281 and @xmath282 rows and columns correspond to excitatory and inhibitory neurons , respectively .      by definition",
    "we have @xmath329 .",
    "the second term is the trace @xmath331      the third coefficient is the sum over @xmath300 row diagonal minors @xmath333 carrying out the summation , we get @xmath334      the fourth and last non - zero coefficient is the sum over @xmath305 row diagonal minors @xmath336 carrying out the sum @xmath337      now we show that @xmath224 for @xmath225 .",
    "a minor representing a subnetwork of four neurons or more can have @xmath313 , @xmath314 , or @xmath316 . if @xmath316 the minor is zero because of repeated columns . if @xmath314 , then @xmath320 . here , the determinant is a sum of @xmath318 row diagonal minors of the form @xmath339 which is zero for @xmath340 .",
    "lastly if @xmath313 then again we have a sum of terms of the form @xmath341 which are zero as discussed above .",
    "we choose a specific parameterization where the marginals of the joint in- and out - degree distribution are @xmath229 with form parameter @xmath227 , scale parameter @xmath228 and have average correlation @xmath230 .",
    "owing to the properties of sums of random variables that follow a @xmath229 distribution , we can write the random in- and out - degree sequences as @xmath342 where @xmath343 . in this appendix @xmath40 will denote averages over the joint in- and out - degree distribution .",
    "the moments of the @xmath229 distribution imply that , for this parametrization , @xmath344 for all @xmath343 . here , since elements of @xmath345 and @xmath345 are ( separately ) independent and identically distributed we will suppress the subscript @xmath50 and superscripts @xmath346 when possible , and let @xmath347 , @xmath348 etc .                      to compute @xmath359",
    "we first derive an expression for @xmath360 . using the independence of @xmath361 : @xmath362+\\nonumber \\\\   &   & \\qquad 2\\kappa^3\\left[1 + 2\\rho\\right]+\\kappa^4\\big\\}.\\end{aligned}\\ ] ]",
    "now we can write @xmath363+\\nonumber \\\\   &   & \\qquad 2\\kappa\\left[1 + 2\\rho\\right]+\\kappa^2\\big\\ } \\end{aligned}\\ ] ]            37ifxundefined [ 1 ] ifx#1 ifnum [ 1 ] # 1firstoftwo secondoftwo ifx [ 1 ] # 1firstoftwo secondoftwo `` `` # 1''''@noop [ 0]secondoftwosanitize@url [ 0 ] ",
    "+ 12$12  & 12#1212_12%12@startlink[1]@endlink[0]@bib@innerbibempty http://www.nature.com/nrm/journal/v9/n10/abs/nrm2503.html [ * * ,   ( ) ] http://www.nature.com/nature/journal/v473/n7345/abs/nature09880.html [ * * ,   ( ) ] http://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.0030068 [ * * ,   ( ) ] http://www.nature.com/neuro/journal/v8/n11/abs/nn1565.html [ * * ,   ( ) ] http://dx.doi.org/10.1103/physrevlett.60.1895 [ * * ,   ( ) ] http://journals.aps.org/prl/abstract/10.1103/physrevlett.97.188104 [ * * ,   ( ) ] http://journals.aps.org/pre/abstract/10.1103/physreve.85.066116 [ * * ,   ( ) ] http://link.springer.com/article/10.1007/s00440-011-0397-9 [ * * ,   ( ) ] http://arxiv.org/abs/1411.2688 [ ( ) ] http://link.aps.org/doi/10.1103/physreve.91.012820 [ * * ,   ( ) ] http://link.aps.org/doi/10.1103/physrevlett.114.088101 [ * * ,   ( ) ] http://link.aps.org/doi/10.1103/physreve.91.042808 [ * * ,   ( ) ] http://journals.aps.org/prl/abstract/10.1103/physrevlett.61.259 [ * * ,   ( ) ] http://dx.doi.org/10.3389/fncom.2011.00008 [ * * ( ) ] http://www.ncbi.nlm.nih.gov/pmc/articles/pmc4482728/ [ * * ,   ( ) ] http://dx.doi.org/10.1162/neco_a_00409 [ * * ,   ( ) ] http://dx.doi.org/10.1109/tsmc.1972.4309193 [ ( ) ] http://journals.aps.org/prb/pdf/10.1103/physrevb.25.6860 [ * * ,   ( ) ] http://journals.aps.org/prl/abstract/10.1103/physrevlett.69.3717 [ * * ,   ( ) ] in http://papers.nips.cc/paper/4051-inferring-stimulus-selectivity-from-the-spatial-structure-of-neural-network-dynamics[__ ]  ( ) http://dx.doi.org/10.1126/science.286.5439.509 [ * * ,   ( ) ] http://www.sciencedirect.com/science/article/pii/s037015730500462x [ * * ,   ( ) ] in  @noop _ _",
    "( ) http://dx.doi.org/10.1007/s10144-014-0471-0 [ * * ,   ( ) ] http://dx.doi.org/10.1007/s10144-015-0482-5 [ * * ,   ( ) ] http://dx.doi.org/10.1007/s10144-014-0476-8 [ * * ,   ( ) ] http://www.nature.com/nature/journal/v238/n5364/abs/238413a0.html [ * * ,   ( ) ] http://dx.doi.org/10.1016/0040-5809(90)90027-s [ * * ,   ( ) ] http://dx.doi.org/10.1073/pnas.192407699 [ * * ,   ( ) ] http://dx.doi.org/10.1126/science.1156269 [ * * ,   ( ) ] http://www.jstor.org/stable/35980 [ * * ,   ( ) ] http://dx.doi.org/10.1038/ncomms8842 [ * * ( ) ] http://www.sciencedirect.com/science/article/pii/s0959438815000768 ;    http://arxiv.org/pdf/1503.08779 [ * * , ( ) ] http://www.nature.com/nature/journal/v492/n7427/full/nature11649.html [ * * ,   ( ) ] http://biorxiv.org/content/biorxiv/suppl/2015/06/21/021204.dc2/021204-1.pdf [ ( ) ] http://dx.doi.org/10.7554/elife.08362 [ ( ) ] http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1000291 [ * * ,   ( ) ]"
  ],
  "abstract_text": [
    "<S> using a generalized random recurrent neural network model , and by extending our recently developed mean - field approach [ j. aljadeff , m. stern , t. sharpee , phys . </S>",
    "<S> rev . </S>",
    "<S> lett . * 114 * , 088101 ( 2015 ) ] , we study the relationship between the network connectivity structure and its low dimensional dynamics . </S>",
    "<S> each connection in the network is a random number with mean 0 and variance that depends on pre- and post - synaptic neurons through a sufficiently smooth function @xmath0 of their identities . </S>",
    "<S> we find that these networks undergo a phase transition from a silent to a chaotic state at a critical point we derive as a function of @xmath0 . above the critical point , although unit activation levels are chaotic , their autocorrelation functions are restricted to a low dimensional subspace . </S>",
    "<S> this provides a direct link between the network s structure and some of its functional characteristics . </S>",
    "<S> we discuss example applications of the general results to neuroscience where we derive the support of the spectrum of connectivity matrices with heterogeneous and possibly correlated degree distributions , and to ecology where we study the stability of the cascade model for food web structure . </S>"
  ]
}