{
  "article_text": [
    "model reduction of controlled dynamical systems has been a long standing , and as yet , unsettled challenge in control theory .",
    "the benefits are clear : a low dimensional approximation of a high dimensional system can be manipulated with a simpler controller , and can be simulated at lower computational cost . a complex",
    ", high dimensional system may even be replaced by a simpler model all together leading to significant cost savings , as in circuit design , while the `` important variables '' of a system might shed light on underlying physical or biological processes .",
    "reduction of linear dynamical systems has been treated with some success to date . as we describe in more detail below , model reduction in the linear case proceeds by reducing the dimension of the system with an eye towards preserving its essential input - output behavior , a notion directly related to `` balancing '' observability and controllability of the system . the nonlinear picture , however , is considerably more involved .    in this paper",
    "we propose a scheme for balanced model - order reduction of general , nonlinear control systems .",
    "a key , and to our knowledge , novel point of departure from the literature on nonlinear model reduction is that our approach marries approximation and dimensionality reduction methods known to the machine learning and statistics communities with existing ideas in linear and nonlinear control . in particular , we apply a method similar to kernel pca as well as function learning in reproducing kernel hilbert spaces ( rkhs ) to the problem of balanced model reduction . working in rkhs provides a convenient , general functional - analytical framework for theoretical understanding as well as a ready source of existing results and error estimates .",
    "the approach presented here is also strongly empirical , in that observability and controllability , and in some cases the dynamics of the nonlinear system are estimated from simulated or measured trajectories .",
    "this emphasis on the empirical makes our approach broadly applicable , as the method can be applied without having to tailor anything to the particular form of the dynamics .    the approach we propose begins by constructing empirical estimates of the observability and controllability gramians in a high ( or possibly infinite ) dimensional feature space .",
    "the gramians are simultaneously diagonalized in order to identify directions which , in the feature space , are both the most observable and the most controllable .",
    "the assumption that a nonlinear system behaves linearly when lifted to a feature space is far more reasonable than assuming linearity in the original space , and then carrying out the linear theory hoping for the best .",
    "working in the high dimensional feature space allows one to perform linear operations on a representation of the system s state and output which can capture strong nonlinearities .",
    "therefore a system which is not model reducible using existing methods , may become reducible when mapped into such a nonlinear feature space .",
    "this situation closely parallels the problem of linear separability in data classification : a dataset which is not linearly separable might be easily separated when mapped into a nonlinear feature space .",
    "the decision boundary is linear in this feature space , but is nonlinear in the original data space .",
    "nonlinear reduction of the state space already opens the door to the design of simpler controllers , but is only half of the picture .",
    "one would also like to be able to write a closed , reduced dynamical system whose input - output behavior closely captures that of the original system .",
    "this problem is the focus of the second half of our paper , where we again exploit helpful properties of rkhs in order to provide such a closed system .",
    "the paper is organized as follows . in the next section we provide the relevant background for model reduction and balancing .",
    "we then adapt and extend balancing techniques described in the background to the current rkhs setting in section  [ sec : empirical_gramians ] .",
    "section  [ sec : closed_sys ] then proposes a method for determining a closed , reduced nonlinear control system in light of the reduction map described in  section  [ sec : empirical_gramians ] .",
    "finally , section  [ sec : expts ] provides experiments illustrating an application of the proposed methods to a specific nonlinear system .",
    "several approaches have been proposed for the reduction of linear control systems in view of control , but few exist for finite or infinite - dimensional controlled nonlinear dynamical systems . for linear systems",
    "the pioneering `` input- output balancing '' approach proposed by b.c .",
    "moore observes that the important states are the ones that are both easy to reach and that generate a lot of energy at the output . if a large amount of energy is required to reach a certain state but the same state yields a small output energy , the state is unimportant for the input - output behavior of the system .",
    "the goal is then to find the states that are _ both _",
    "the most controllable and the most observable .",
    "one way to determine such states is to find a change of coordinates where the controllability and observability gramians ( which can be viewed as a measure of the controllability and the observability of the system ) are equal and diagonal . states that are difficult to reach and that do nt significantly affect the output are then ignored or truncated . a system expressed in the coordinates where each state is equally controllable and observable",
    "is called its _",
    "balanced realization_.    a proposal for generalizing this approach to nonlinear control systems was advanced by j. scherpen  @xcite , where suitably defined controllability and observability energy functions reduce to gramians in the linear case .",
    "in general , to find the balanced realization of a system one needs to solve a set of hamilton - jacobi and lyapunov equations ( as we will discuss below ) .",
    "moore  @xcite proposed an alternative , data - based approach for balancing in the linear case .",
    "this method uses samples of the impulse response of a linear system to construct empirical controllability and observability gramians which are then balanced and truncated using principal components analysis ( pca , or pod ) .",
    "this data - driven strategy was then extended to nonlinear control systems with a stable linear approximation by lall et al .",
    "@xcite , by effectively applying moore s method to a nonlinear system by way of the galerkin projection .",
    "despite the fact that the balancing theory underpinning their approach assumes a linear system , lall and colleagues were able to effectively reduce some nonlinear systems .",
    "phillips et al .",
    "@xcite has also studied reduction of nonlinear circuit models in the case of linear but unbalanced coordinate transformations and found that approximation using a polynomial rkhs could afford computational advantages .",
    "gray and verriest mention in  @xcite that studying algebraically defined gramian operators in rkhs may provide advantageous approximation properties , though the idea is not further explored .",
    "finally , coifman et al .",
    "@xcite discuss reduction of an uncontrolled stochastic langevin system .",
    "there , eigenfunctions of a combinatorial laplacian , built from samples of trajectories , provide a set of reduction coordinates but does not provide a reduced system .",
    "this method is related to kernel principal components ( kpca ) using a gaussian kernel , however reduction in this study is carried out on a simplified linear system outside the context of control .    in the following section we review balancing of linear and nonlinear systems as introduced in  @xcite and  @xcite .",
    "consider a linear control system @xmath0 where @xmath1 is controllable , @xmath2 is observable and @xmath3 is hurwitz .",
    "we define the controllability and the observability gramians as , respectively , @xmath4 these two matrices can be viewed as a measure of the controllability and the observability of the system  @xcite .",
    "for instance , consider the past energy  @xcite , @xmath5 , defined as the minimal energy required to reach @xmath6 from @xmath7 in infinite time @xmath8 and the future energy  @xcite , @xmath9 , defined as the output energy generated by releasing the system from its initial state @xmath10 , and zero input @xmath11 for @xmath12 , i.e. @xmath13 for @xmath10 and @xmath14 . in the linear case , it can be shown that @xmath15 and @xmath16 the columns of @xmath17 span the controllable subspace while the nullspace of @xmath18 coincides with the unobservable subspace .",
    "as such , @xmath17 and @xmath18 ( or their estimates ) are the key ingredients in many model reduction techniques .",
    "it is also well known that @xmath17 and @xmath18 satisfy the lyapunov equations  @xcite @xmath19 several methods have been developed to solve these equations directly  @xcite .",
    "the idea behind balancing is to find a representation where the system s observable and controllable subspaces are aligned so that reduction , if possible , consists of eliminating uncontrollable states which are also the least observable .",
    "more formally , we would like to find a new coordinate system such that @xmath20 where @xmath21 .",
    "if @xmath1 is controllable and @xmath2 is observable , then there exists a transformation such that the state space expressed in the transformed coordinates @xmath22 is balanced and @xmath23 .",
    "typically one looks for a gap in the singular values @xmath24 for guidance as to where truncation should occur . if we see that there is a @xmath25 such that @xmath26 , then the states most responsible for governing the input - output relationship of the system are @xmath27 while @xmath28 are assumed to make negligible contributions .",
    "if @xmath29 is unstable then the controllability and observability quantities defined in   are undefined since the integrals will be unbounded .",
    "there may , however , still exist solutions to the lyapunov equations   when @xmath29 is unstable , and these solutions will be unique if and only if @xmath30 . in this case balancing may be carried out as usual by finding ( if possible ) a transformation @xmath31 such that @xmath32 where @xmath33 is again diagonal and positive semidefinite  @xcite .",
    "other approaches to balancing unstable linear systems exist ( see  @xcite for the method of lqg balancing for example ) .",
    "although several methods also exist for computing @xmath31  @xcite , it is common to simply compute the cholesky decomposition of @xmath18 so that @xmath34 , and form the svd @xmath35 of @xmath36",
    ". then @xmath31 is given by @xmath37 .",
    "we also note that the problem of finding the coordinate change @xmath31 can be seen as an optimization problem  @xcite of the form @xmath38 . \\nonumber\\ ] ]      in the nonlinear case , the energy functions @xmath39 and @xmath40 in ( [ l_c ] ) and ( [ l_o ] ) are obtained by solving both a lyapunov and a hamilton - jacobi equation . here",
    "we follow the development of scherpen  @xcite .",
    "consider the nonlinear system @xmath41 with @xmath42 , @xmath43 , @xmath44 , @xmath45 , @xmath46 for @xmath47 , and @xmath48 .",
    "moreover , assume the following hypothesis .",
    "+ _ assumption a : _ the linearization of   around the origin is controllable , observable and @xmath49 is asymptotically stable .",
    "[ thm : scherp1]@xcite if the origin is an asymptotically stable equilibrium of @xmath50 on a neighborhood @xmath51 of the origin , then for all @xmath52 , @xmath53 is the unique smooth solution of @xmath54 under the assumption that ( [ lo_hjb ] ) has a smooth solution on @xmath51 .",
    "furthermore for all @xmath52 , @xmath55 is the unique smooth solution of @xmath56 under the assumption that ( [ lc_hjb ] ) has a smooth solution @xmath57 on @xmath51 and that the origin is an asymptotically stable equilibrium of @xmath58 on @xmath51 .    with the controllability and",
    "the observability functions on hand , the input - normal / output - diagonal realization of system   can be computed by way of a coordinate transformation .",
    "more precisely ,    [ theorem_scherpen]@xcite consider system   under assumption a and the assumptions in theorem  [ thm : scherp1 ] .",
    "then , there exists a neighborhood @xmath51 of the origin and coordinate transformation @xmath59 on @xmath51 converting the energy functions into the form @xmath60 @xmath61 where @xmath62 .",
    "the functions @xmath63 are called _ hankel singular value functions_.    analogous to the linear case , the system s states can be sorted in order of importance by sorting the singular value functions , and reduction proceeds by removing the least important states .    in the above framework for balancing of nonlinear systems , one needs to solve ( or numerically evaluate ) the pdes ( [ lo_hjb ] ) , ( [ lc_hjb ] ) and compute the coordinate change @xmath59 , however there are no systematic methods or tools for solving these problems .",
    "various approximate solutions based on taylor series expansions have been proposed  @xcite .",
    "newman and krishnaprasad  @xcite introduce a statistical approximation based on exciting the system with white gaussian noise and then computing the balancing transformation using an algorithm from differential topology .",
    "as mentioned earlier , an essentially linear empirical approach was proposed in  @xcite . in this paper",
    ", we combine aspects of both data - driven approaches and analytic approaches by carrying out balancing in a suitable rkhs .",
    "we consider a general nonlinear system of the form @xmath64 with @xmath42 , @xmath43 , @xmath65 , @xmath66 , and @xmath48 .",
    "let @xmath67 be the reachable set from the initial condition @xmath68 .",
    "_ hypothesis h : _ the system ( [ eqn : nlsys ] ) is zero - state observable , its linearization around the origin is controllable , and the origin of @xmath69 is asymptotically stable .",
    "we treat the problem of estimating the observability and controllability gramians as one of estimating an integral operator from data in a reproducing kernel hilbert space ( rkhs )  @xcite . _",
    "our approach hinges on the key modeling assumption that the nonlinear dynamical system is linear in an appropriate high ( or possibly infinite ) dimensional lifted feature space_. covariance operators in this feature space and their empirical estimates are the objects of primary importance and contain the information needed to perform model reduction .",
    "in particular , the ( linear ) observability and controllability gramians are estimated and diagonalized in the feature space , but capture nonlinearities in the original state space .",
    "the reduction approach we propose adapts ideas from kernel pca ( kpca )  @xcite and is driven by a set of simulated or sampled system trajectories , extending and generalizing the work of moore  @xcite and lall et al .",
    "@xcite .    in the development below we lift state vectors of the system ( [ eqn : nlsys ] ) into a reproducing kernel hilbert space  @xcite .",
    "in this section , we give a brief overview of reproducing kernel hilbert spaces as used in statistical learning theory .",
    "the discussion here borrows heavily from  @xcite .",
    "early work developing the theory of rkhs was undertaken by n. aronszajn  @xcite .",
    "let @xmath70 be a hilbert space of functions on a set @xmath71",
    ". denote by @xmath72 the inner product on @xmath70 and let @xmath73 be the norm in @xmath70 , for @xmath74 and @xmath75 .",
    "we say that @xmath70 is a reproducing kernel hilbert space ( rkhs ) if there exists @xmath76 such that    * @xmath77 has the reproducing property , i.e. @xmath78 , @xmath79 .",
    "* @xmath77 spans @xmath70 , i.e. @xmath80 .    @xmath77 will be called a reproducing kernel of @xmath70 .",
    "@xmath81 will denote the rkhs @xmath70 with reproducing kernel @xmath77 .",
    "( mercer kernel map ) a function @xmath76 is called a mercer kernel if it is continuous , symmetric and positive definite .",
    "the important properties of reproducing kernels are summarized in the following proposition    [ prop1 ] if @xmath77 is a reproducing kernel of a hilbert space @xmath70 , then    * @xmath82 is unique .",
    "* @xmath83 , @xmath84 ( symmetry ) . * @xmath85 for @xmath86 and @xmath87 ( positive definitness ) . * @xmath88 .",
    "* let @xmath89 .",
    "the following kernels , defined on a compact domain @xmath90 , are mercer kernels : @xmath91 ( linear ) , @xmath92 ( polynomial ) , @xmath93 ( gaussian ) .",
    "[ thm1 ] let @xmath76 be a symmetric and positive definite function .",
    "then , there exists a hilbert space of functions @xmath70 defined on @xmath71 admitting @xmath77 as a reproducing kernel .",
    "conversely , let @xmath70 be a hilbert space of functions @xmath94 satisfying @xmath95 then , @xmath70 has a reproducing kernel @xmath77 .",
    "[ thm3 ] every sequence of functions @xmath96 which converges strongly to a function @xmath74 in @xmath81 , converges also in the pointwise sense , that is , @xmath97 , for any point @xmath98 .",
    "further , this convergence is uniform on every subset of @xmath99 on which @xmath100 is bounded .",
    "[ thm4 ] let @xmath82 be a positive definite kernel on a compact domain or a manifold @xmath99 .",
    "then there exists a hilbert space @xmath101 and a function @xmath102 such that @xmath103 @xmath104 is called a feature map , and @xmath101 a feature space defined as @xmath105 if @xmath77 is a mercer kernel , for @xmath106 and @xmath107 is a borel measure on @xmath99 . ] .    * in theorem [ thm4 ] , and using property [ iv .",
    "] in proposition [ prop1 ] , we can take @xmath108 in which case @xmath109  the `` feature space '' is the rkhs .",
    "* the fact that mercer kernels are positive definite and symmetric reminds us of similar properties of gramians and covariance matrices .",
    "this is an essential fact that we are going to use in the following . * in practice , we choose a mercer kernel , such as the ones in [ v. ] in proposition [ prop1 ] , and theorem [ thm1 ] guarantees the existence of a hilbert space admitting such a function as a reproducing kernel .",
    "rkhs play an important in learning theory whose objective is to find an unknown function @xmath110 from random samples @xmath111 .",
    "for instance , assume that the random probability measure that governs the random samples is @xmath112 and is defined on @xmath113 .",
    "let @xmath99 be a compact subset of @xmath114 and @xmath115 .",
    "if we define the least square error of @xmath74 as @xmath116 then the function that minimzes the error is the regression function @xmath117 @xmath118 where @xmath119 is the conditional probability measure on @xmath120 .",
    "since @xmath112 is unknown , neither @xmath117 nor @xmath121 is computable .",
    "we only have the samples @xmath122 .",
    "the error @xmath117 is approximated by the empirical error @xmath123 by @xmath124 for @xmath125 , @xmath126 plays the role of a regularizing parameter . in learning theory ,",
    "the minimization is taken over functions from a hypothesis space often taken to be a ball of a rkhs @xmath127 associated to mercer kernel @xmath77 , and the function @xmath128 that minimizes the empirical error @xmath129 is @xmath130 where the coefficients @xmath131 is solved by the linear system @xmath132 and @xmath128 is taken as an approximation of the regression function @xmath117 . hence , minimizing over the ( possibly infinite dimensional ) hilbert space , reduces to minimizing over @xmath133 .",
    "the series ( [ learning1 ] ) converges absolutely and uniformly to @xmath74 .",
    "we call _ learning _ the process of approximating the unknown function @xmath74 from random samples on @xmath134 .    in the following ,",
    "we assume that the kernels @xmath77 are continuous and bounded by @xmath135      following  @xcite , we estimate the controllability gramian by exciting each coordinate of the input with impulses while setting @xmath136 .",
    "one can also further excite using rotations of impulses as suggested in  @xcite , however for simplicity we consider only the original signals proposed in  @xcite .",
    "let @xmath137 be the @xmath138-th excitation signal , and let @xmath139 be the corresponding response of the system .",
    "form the matrix @xmath140 \\in { \\mathbb{r}}^{n\\times m}$ ] , so that @xmath141 is seen as a data matrix with column observations given by the respective responses @xmath139 .",
    "then @xmath142 is given by @xmath143 we can approximate this integral by sampling the matrix function @xmath141 within a finite time interval @xmath144 $ ] assuming the regular partition @xmath145 .",
    "this leads to the empirical controllability gramian @xmath146    as described in  @xcite , the observability gramian is estimated by fixing @xmath147 , setting @xmath148 for @xmath149 , and measuring the corresponding system output responses @xmath150 .",
    "as before , assemble the responses into a matrix @xmath151\\in { \\mathbb{r}}^{p\\times n}$ ] .",
    "the observability gramian @xmath152 and its empirical counterpart @xmath153 are given by @xmath154 and @xmath155 where @xmath156 .",
    "the matrix @xmath157 can be thought of as a data matrix with column observations @xmath158 so that @xmath159 corresponds to the response at time @xmath160 of the single output coordinate @xmath161 to each of the ( separate ) initial conditions @xmath162 .",
    "this convention will lead to greater clarity in the steps that follow .",
    "kernel pca  @xcite will be a helpful starting point for understanding the approach to balanced reduction introduced in this paper .",
    "we briefly review the relevant background here .",
    "kernel pca ( kpca ) generalizes linear pca by carrying out pca in a high dimensional feature space defined by a feature map @xmath163 . taking the feature map @xmath164 and given the set of data @xmath165",
    ", we can consider pca in the feature space by simply working with the covariance of the mapped vectors , @xmath166 where @xmath167 denotes the tensor product between two vectors in @xmath168 .",
    "we will assume the data are centered in the feature space so that @xmath169 . if not , data may be centered according to the prescription in  @xcite .",
    "the principal subspaces are computed by diagonalizing @xmath170 , however as is shown in  @xcite , one can equivalently form the matrix @xmath171 of kernel products @xmath172 for @xmath173 , and solve the eigenproblem @xmath174 if @xmath175 then we have that @xmath176 where @xmath177 , and the non - zero eigenvalues of @xmath77 and @xmath170 coincide .    the eigenvectors @xmath178 of @xmath77 are then normalized so that the eigenvectors @xmath179 of @xmath170 have unit norm in the feature space , leading to the condition @xmath180 . assuming this normalization convention ,",
    "sort the eigenvectors according to the magnitudes of the corresponding eigenvalues in descending order , and form the matrix @xmath181 , 1\\leq q\\leq \\min(n , n).\\ ] ]    similarly , form the matrix @xmath182 , 1\\leq q\\leq n$ ] of sorted eigenvectors of @xmath170 .",
    "the first @xmath183 principal components of a vector @xmath184 in the feature space are then given by @xmath185 .",
    "it can be shown however ( see  @xcite ) that principal components in the feature space can be computed in the original space with kernels using the map @xmath186 where @xmath187 .",
    "the method we propose consists , in essence , of collecting samples and then performing a process similar to `` simultaneous principal components analysis '' on the controllability and observability gramian estimates in the ( same ) rkhs . as mentioned above ,",
    "given a choice of the kernel @xmath77 defining a rkhs @xmath168 , principal components in the feature space can be computed implicitly in the original input space using @xmath77 .",
    "it is worth emphasizing however that we will be co - diagonalizing _ two _ gramians in the feature space by way of a _ non - orthogonal _ transformation ; the process bears a resemblance to ( k)pca , and yet is distinct .",
    "indeed the favorable properties associated with an orthonormal basis are no longer available , the quantities we will in practice diagonalize are different , and the issue of data - centering must be considered with some additional care .    first note that the controllability gramian @xmath188 can be viewed as the sample covariance of a collection of @xmath189 vectors , scaled by @xmath31 @xmath190 and",
    "the observability gramian can be similarly viewed as the sample covariance of a collection of @xmath191 vectors @xmath192 where the @xmath193 are defined in equation  .",
    "we can thus consider three quantities of interest :    * the _ controllability kernel matrix _",
    "@xmath194 of kernel products @xmath195 for @xmath196 where we have re - indexed the set of vectors @xmath197 to use a single linear index . * the _ observability kernel matrix _ @xmath198 , @xmath199 for @xmath200 , where we have again re - indexed the set @xmath201 for simplicity . *",
    "the _ hankel kernel matrix _",
    "@xmath202 , @xmath203 for @xmath204 , @xmath205 .",
    "we have chosen the suggestive terminology `` hankel kernel matrix '' above because the square - roots of the nonzero eigenvalues of the matrix @xmath206 are the empirical hankel singular values of the system mapped into feature space , where we assume the system behaves linearly .",
    "this assertion will be proved immediately below .",
    "note that ordinarily , @xmath207 and @xmath208 will be rank deficient .    before proceeding we",
    "consider the issue of data centering in feature space .",
    "pca and kernel pca assume that the data have been centered in order to make the problem translation invariant . in the setting considered here , we have two distinct sets of data : the observability samples and the controllability samples . a reasonable centering convention centers the data in each of these datasets separately .",
    "let @xmath209 denote the matrix whose columns are the observability samples mapped into feature space by @xmath104 , and let @xmath210 be the matrix similarly built from the feature space representation of the controllability samples . then @xmath211 and @xmath212 .",
    "assume for the moment that there are @xmath213 observability data samples and @xmath214 controllability samples , and let @xmath215 denote the length @xmath214 , @xmath213 vectors of all ones , respectively .",
    "we can define centered versions of the feature space data matrices @xmath216 as @xmath217 where @xmath218 and @xmath219 .",
    "we will need two centered quantities in the development below .",
    "the first centered quantity we consider is the centered version of @xmath220 , namely @xmath221 .",
    "although one can not compute @xmath222 explicitly from the data , we can compute @xmath223 by observing that @xmath224 the second quantity we ll need a centered version of the _ empirical observability feature map _",
    "@xmath225 where @xmath226 is the state variable and the observability samples @xmath227 are again indexed by a single variable as in equation  .",
    "centering follows reasoning similar to that of the hankel kernel matrix immediately above : @xmath228 note : _ throughout the remainder of this paper we will drop the special notation @xmath229 and assume that @xmath230 are centered appropriately . _    with the quantities defined above , we can co - diagonalize the empirical gramians ( balancing ) and reduce the dimensionality of the state variable ( truncation ) in feature space by carrying out calculations in the original data space .",
    "as the system is assumed to behave linearly in the feature space , the order of the model can be reduced by discarding small hankel values @xmath231 , and projecting onto the subspace associated with the first @xmath232 largest eigenvalues .",
    "the following key result describes this process :    [ thm : redmap_thm ] balanced reduction in feature space can be accomplished by applying the state - space reduction map @xmath233 given by @xmath234 where @xmath235 if @xmath236 , and @xmath237 is the empirical observability feature map .",
    "we assume the data have been centered in feature space .",
    "let @xmath210 be a matrix with columns @xmath238 , so that @xmath239 is the feature space controllability gramian counterpart to equation  .",
    "similarly , let @xmath209 be a matrix with columns @xmath240 , so that @xmath241 is the feature space observability gramian counterpart to equation  . since by definition @xmath242",
    ", we also have that @xmath243 and @xmath244 . in general",
    "the gramians @xmath245 are infinite dimensional whereas the kernel matrices @xmath246 are necessarily of finite dimension .",
    "we now carry out linear balancing on @xmath247 in the feature space ( rkhs ) .",
    "first , take the svd of @xmath248 so that @xmath249 the last equality in equation   follows since @xmath99 is symmetric and therefore @xmath250 is too .",
    "the linear balancing transformation is then given by @xmath251 , and one can readily verify that @xmath252 .",
    "here , inverses should be interpreted as peudo - inverses when appropriate when the number of data points is less than the dimension of the rkhs . ] . from equations  - ,",
    "we see that @xmath253 and thus @xmath254 .",
    "we can project an arbitrary mapped data point @xmath255 onto the ( balanced ) `` principal '' subspace of dimension @xmath183 spanned by the first @xmath183 rows of @xmath213 by computing @xmath256 where @xmath257 is the empirical observability feature map , recalling that @xmath258 is the matrix formed by taking the top @xmath183 eigenvectors of @xmath206 by equation  .",
    "we note that square roots of the non - zero eigenvalues of @xmath206 are exactly the hankel singular values of the system mapped into the feature space , under the assumption of linearity in the feature space .",
    "this can be seen by noting that @xmath259 , where @xmath260 refers to the non - zero eigenvalues of its argument .    in section  [ sec : closed_sys ] below we show how to use the nonlinear reduction map   to realize a closed , reduced order system which can approximate the original system to a high degree of accuracy .",
    "given the nonlinear state space reduction map @xmath233 , a remaining challenge is to construct a corresponding ( reduced ) dynamical system on the reduced state space which well approximates the input - output behavior of the original system on the original state space . setting @xmath261 and applying the chain rule , @xmath262 where @xmath263 refers to an appropriate notion ( to be defined ) of the inverse of @xmath264 .",
    "however we are faced with the difficulty that the map @xmath264 is not in general injective ( even if @xmath265 ) , and moreover one can not guarantee that an arbitrary point in the rkhs has a non - empty preimage under @xmath104  @xcite .",
    "we propose an approximation scheme to get around this difficulty : the dynamics @xmath74 will be approximated by an element of an rkhs _ defined on the reduced state space_. when @xmath74 is assumed to be known explicitly it can be approximated to a high degree of accuracy .",
    "an approximate , least - squares notion of @xmath263 will be given to first or second order via a taylor series expansion , but only where it is strictly needed  and at the last possible moment  so that a first or second order approximation will not be as crude as one might suppose .",
    "we will also consider , as an alternative , a direct approximation of @xmath266 which takes into account further properties of the reproducing kernel as well as the fact that the jacobian is to be evaluated at @xmath267 in particular . in both cases",
    ", the important ability of the map @xmath264 to capture strong nonlinearities will not be significantly diminished .",
    "the vector - valued map @xmath268 can be approximated by a composing a set of @xmath269 regression functions ( one for each coordinate ) @xmath270 in an rkhs , with the reduction map @xmath264 .",
    "it is reasonable to expect that this approximation will be better than directly computing @xmath271 using , for instance , a taylor expansion approximation for @xmath263 which may ignore important nonlinearities at a stage where crude approximations must be avoided .",
    "let @xmath272 denote a reduced state variable , and concatenate the input examples @xmath273 so that @xmath274 , and @xmath275 is a set of input - output training pairs describing the @xmath138-th coordinate of the map @xmath276 .",
    "the training examples should characterize `` typical '' behaviors of the system , and can even re - use those trajectories simulated in response to impulses for estimating the gramians above",
    ". we will seek the function @xmath277 which minimizes @xmath278 where @xmath279 here is a regularization parameter .",
    "we have chosen the square loss , however other suitable loss functions may be used .",
    "it can be shown  @xcite that in this case @xmath280 takes the form @xmath281 , where @xmath282 defines the rkhs @xmath283 ( and is unrelated to @xmath77 used to estimate the gramians ) .",
    "note that although our notation takes the rkhs for each coordinate function to be the same , in general this need not be true : different kernels may be chosen for each function . here",
    "the @xmath284 comprise a set of coefficients learned using the regularized least squares ( rls ) algorithm .",
    "the kernel family and any hyper - parameters can be chosen by cross - validation .",
    "for notational convenience we will further define the vector - valued empirical feature map @xmath285 for @xmath286 . in this notation @xmath287 where @xmath288 . a broad class of systems seen in the literature  @xcite",
    "are also characterized by separable dynamics of the form @xmath289 . in this case",
    "one need only estimate the functions @xmath74 and @xmath290 from examples @xmath291 and @xmath292 .",
    "we turn to approximating the component @xmath293 appearing in equation  .",
    "a simple solution is to compute a low - order taylor expansion of @xmath264 and then invert it using the moore - penrose pseudoinverse to obtain the approximation .",
    "for example , consider the first order expansion @xmath294 .",
    "then we can approximate @xmath295 ( in the first - order , least - norm sense ) as @xmath296 we may start with @xmath297 , but periodically update the expansion in different regions of the dynamics if desired . a good expansion point could be the estimated preimage of @xmath298 returned by the algorithm proposed in  @xcite . if @xmath299 is the centered version of the length @xmath213 vector @xmath237 defined by  , then @xmath300 where @xmath301 is the length @xmath213 vector of all ones . an example calculation of @xmath302 in the case of a polynomial kernel",
    "is given in the section immediately below .",
    "for certain choices of the kernel @xmath77 defining the gramian feature space @xmath168 , one can exploit the fact that @xmath303 and its derivative bear a special relationship , and potentially improve the estimate for @xmath266 .",
    "perhaps the most commonly used off - the - shelf kernel families are the polynomial and gaussian families .",
    "for any two kernels with hyperparameters @xmath304 and @xmath183 ( respectively ) in one of these classes , we have that @xmath305 .",
    "we ll consider the polynomial kernel of degree @xmath306 , @xmath307 in particular ; the gaussian case can be derived using similar reasoning . for a polynomial kernel we have that @xmath308 recalling that @xmath309 and @xmath310 ,",
    "if @xmath264 were invertible then we would have @xmath311 the map @xmath264 is not injective however , and in addition the fibers of @xmath104 may be potentially empty , so we must settle for an approximation . it is reasonable then to _ define _ @xmath312 as the solution to the convex optimization problem @xmath313 where @xmath314 is defined as in equation  .",
    "if a point @xmath315 has a pre - image in @xmath114 this definition is consistent with composing @xmath104 with the formal definition @xmath316 and noting that in this case @xmath317 .",
    "furthermore , a trajectory @xmath298 of the closed dynamical system on the reduced statespace need not ( and may not ) have a counterpart in the original statespace by virtue of the way in which @xmath263 is used in our formulation of the reduction map and corresponding reduced dynamical system .",
    "one will recognize that the solution @xmath318 to   is just the moore - penrose pseudoinverse @xmath319 .",
    "inserting this solution into the feature map representation of a kernel @xmath77 gives the following definition for @xmath320 : @xmath321 where the final equality follows applying equations  - and @xmath322 is defined as in theorem  [ thm : redmap_thm ] .",
    "substituting into the derivative for a polynomial kernel @xmath323 gives @xmath324 which immediately gives an expression for @xmath266 .",
    "note that this approximation is global in the sense that the @xmath325 matrix inverse @xmath326 need only be computed once for fixed @xmath29 but many different @xmath327 . ]",
    "; no updating is required during simulation of the closed system .",
    "given an estimate @xmath328 of @xmath329 in the rkhs @xmath283 and a notion of @xmath293 from above , we can write down a closed dynamical system on the reduced statespace .",
    "we have @xmath330 where @xmath331 is a matrix with the vectors @xmath332 as its rows , and @xmath333 is the jacobian of the empirical feature map defined in equation  . here",
    "the expression @xmath334 should be interpreted as notation for either of the jacobian approximations suggested in section  [ sec : jacobian_app ] .",
    "equation   is seen to give a closed nonlinear control system expressed solely in terms of the reduced variable : @xmath335    where the map @xmath336 modeling the output function @xmath337 is estimated as described immediately below .",
    "although the `` true '' reduced system does not actually exist due to non - injectivity of the feature map @xmath104 , in many situations one can expect that the above system will capture the essential input - output behavior of the original system .",
    "we leave a precise analysis of the error in the approximations appearing in   to future work .",
    "analogous to the case of the dynamics @xmath74 , we are faced with two possibilities for approximating @xmath338 .",
    "we can apply a crude taylor series approximation to estimate @xmath263 and therefore @xmath339 , or as in section  [ sec : f_rkhs ] , we can estimate a map @xmath340 from the reduced state space to the output space directly , using rkhs methods",
    ". given samples @xmath341 , each coordinate function @xmath342 is given in the familiar form @xmath343 , where @xmath344 is the kernel chosen to define the rkhs , and may be different for each coordinate .",
    "it should be noted that just given the state space reduction map @xmath264 , one can immediately compare the output of the system defined by @xmath345 to the original system without defining a closed dynamics as above .",
    "in fact with @xmath264 and @xmath346 one can design a simpler controller which takes as input the reduced state variable @xmath347 , but controls the original system .      in this section ,",
    "we show that a linearization of the reduced order system preserves the important structural properties of a linearization of the full order system .",
    "we leave the study of the structural properties of the reduced nonlinear system for future work .",
    "we first note that the approach introduced in this paper reduces to moore s approach  @xcite if the system is linear and one adopts the linear kernel @xmath348 . for instance , consider the linear control system ( [ linsys ] ) .",
    "if we take @xmath349 , then the empirical controllability and observability gramians in feature space are exactly the gramians  - , as introduced in moore s work  @xcite .",
    "the feature space is the original data space , so balancing in the rkhs as explained above reduces to moore s notion of balancing . in this case one obtains reduced order system dynamics via a galerkin projection rather than by attempting to statistically estimate the ( linear ) dynamics and output functions .",
    "the following brief proposition shows that the reduced order system obtained using the methods proposed here preserves important properties of the original system .",
    "if the linearization of the full order system ( [ eqn : nlsys ] ) is controllable / observable / hurwitz then the linearization of the reduced order system ( [ reduced_system ] ) is controllable / observable / hurwitz .    from ( [ eqn : exact_closed ] ) and ( [ eqn : approx_xr ] ) , the dynamics of the reduced order system is given by @xmath350 and the output is given by @xmath351 where @xmath352 . because our approach reduces to moore s approach in the linear case , then using moore s results in  @xcite we may conclude that if the linearization of the full order system is asymptotically stable , controllable , and observable ( which is the case under assumption h ) , then @xmath353 is hurwitz , @xmath354 is controllable and @xmath355 is observable .      to summarize , the approach we have proposed proceeds as follows    1 .",
    "given a nonlinear control system ( [ sigma ] ) , let @xmath137 be the @xmath138-th excitation signal for @xmath356 , and let @xmath357 be the corresponding response of the system . run the system and sample the trajectories at times @xmath358 to generate a collection of @xmath189 vectors @xmath359 .",
    "2 .   fixing @xmath147 and setting @xmath148 for @xmath149 ( separately ) , measure the corresponding system output responses @xmath360 .",
    "as before , sample the responses at times @xmath358 and save the collection of @xmath191 vectors @xmath361 defined as @xmath362 3 .",
    "choose a kernel @xmath77 defining a rkhs @xmath168 , and form the hankel kernel matrix @xmath202 , @xmath363 where we have re - indexed the sets @xmath364 to use single indices .",
    "4 .   compute the eigendecomposition @xmath236 assuming @xmath220 has been centered according to equation  .",
    "the order of the model is reduced by discarding small eigenvalues @xmath231 , and projecting onto the subspace associated with the first @xmath232 largest eigenvalues .",
    "this leads to the state - space reduction map @xmath233 given by @xmath365 where @xmath235 and @xmath237 is the centered empirical observability feature map given by equation  .",
    ".   from input / output pairs or simulated / measured trajectories , learn approximations of the dynamics and output function defined on the reduced state space using , for instance , the method described in section ",
    "[ sec : defins ] ( equations ( [ learning1])-([learning2 ] ) . the rkhs used to approximate these functions need not be the same as the rkhs in which balanced truncation was carried out .",
    "approximate the jacobian contribution as described in section ",
    "[ sec : jacobian_app ] .",
    "combine the approximations to determine an expression for a closed , reduced , nonlinear dynamical system as described in sections  .",
    "[ reduced_model ] and  .",
    "[ output ] .     0.5 cm",
    "we demonstrate an application of our method on two examples appearing in  @xcite ( examples 3.1 .",
    "and 3.2 , pgs .",
    "52 - 54 ) .",
    "consider the nonlinear system @xmath366 it can be shown that this system has the same input - output relationship as the system @xmath367 by rearranging terms so that @xmath368 defining the new variables @xmath369 and @xmath370 , the system can then be re - written @xmath371 it can be seen that the variable @xmath372 may be truncated because it does nt appear in the expression of the output and thus does nt affect @xmath373 .      we will also consider a 7-dimensional nonlinear system with one dimensional input and output : @xmath374      for both systems impulse and initial - condition responses of the system were simulated as described above , and 800 samples equally spaced in the time interval @xmath375 $ ] were sampled to build the hankel kernel matrix @xmath220 given by the third degree polynomial kernel @xmath376 . for the 2-d system we retained one component , and for the 7-d system we retained two for the sake of variety .",
    "thus the reduction map @xmath264 was defined by taking the top one or two eigenvectors ( scaled columns of @xmath31 ) corresponding to the largest hankel singular values , giving a reduced state space of dimension one or two for the 2-d and 7-d systems , respectively .",
    "next , a map from the reduced variable @xmath347 to @xmath377 was estimated following section  [ sec : f_rkhs ] . the same procedure was followed in both experiments .",
    "the control input was chosen to be a 10hz square wave with peaks at @xmath378 at 50% duty cycle , and 1000 samples from the simulated system in the interval @xmath375 $ ] were mapped down using @xmath264 and then used to solve the rls regression problems , one for each state variable , again using a third degree polynomial kernel .",
    "all initial conditions were set to zero .",
    "the desired outputs ( dependent variable examples ) used to learn @xmath379 were taken to be the true function @xmath74 evaluated at the samples from the simulated state trajectory .",
    "we also added a bias dimension of 1 s to the data to account for an offset , and used a fast leave - one - out cross - validation ( loocv ) computation  @xcite to select the optimal regularization parameter .",
    "two remarks are in order .",
    "the above dynamics can in fact be represented explicitly and exactly in a 3rd degree polynomial rkhs ; only monomials up to degree 3 appear in the dynamics .",
    "second , the control input is decoupled from the state .",
    "both of these facts can be used to obtain an improved reduced model , however we did not make use of these special properties and instead applied the simplest version of the techniques described above which assume no special structure .",
    "we followed a similar process to learn the output function @xmath380 for both systems .",
    "here we used a 10hz square wave control input ( peaks at @xmath381 , 50% duty cycle ) , zero initial conditions and 700 samples in the interval @xmath375 $ ] . for this function",
    "the gaussian kernel @xmath382 was used to demonstrate that our method does not rely on any particular match between the form of the dynamics and the type of kernel .",
    "the scale hyperparameter @xmath383 was chosen to be the reciprocal of the average squared - distance between the training examples .",
    "we again used loocv to select the rls regularization parameter .    finally , closed systems were simulated as described above using @xmath384 and a control input different from those used to learn the dynamics and output functions : @xmath385 where @xmath386 denotes the square wave function .",
    "this input is shown at the top of both simulation summaries in figure  [ fig : sims ] .",
    "the taylor series approximation for @xmath264 was done once , about @xmath6 , and was not updated further .",
    "figure  [ fig : hankel_values ] shows the top seven hankel singular values in the feature space for the two problems on a log scale .",
    "one can see that , for both systems , even a single component ought to capture most of the system s behavior .",
    "further simulations of the 7-d system with a single component showed only a small amount of additional error beyond that the two component system , as one would expect from the decay of that system s hankel values .",
    "the simulated outputs @xmath387 of the closed reduced systems as well as the output @xmath388 of the original system are plotted in figure  [ fig : sims ] ( left , 2-d system ; right , 7-d system ) .",
    "one can see that , even for a significantly different input , the reduced systems closely capture the original systems .",
    "the main source of error is seen to be over- and under - shoot near the square wave transients .",
    "this error can be further reduced by simulating the system for different sorts of inputs ( and/or frequencies ) and including the collected samples in the training sets used to learn @xmath389 and @xmath346 .",
    "indeed , we have had some success driving example systems with random uniform input in some cases .",
    "finally , for illustrative purposes we show examples of the controllability and observability kernel matrices @xmath390 and @xmath391 for the 7-d system in figure  [ fig : kernmats ] .",
    "we have introduced a new , empirical model reduction method for nonlinear control systems .",
    "the method assumes that the nonlinear system is approximately linear in a high dimensional feature space , and carries out linear balanced truncation in that space .",
    "this leads to a nonlinear reduction map , which we suggest can be combined with representations of the dynamics and output functions by elements of an rkhs to give a closed reduced order dynamical system which captures the input - output characteristics of the original system .",
    "we then demonstrated an application of our technique to a pair of nonlinear systems and simulated the original and reduced models for comparison , showing that the approach proposed here can yield good low - order nonlinear reductions of strongly nonlinear control systems .",
    "we believe that techniques well known to the machine learning and statistics communities can offer much to control and dynamical systems research , and many further directions remain , including computing error estimates , reduction of unstable systems , structure preserving systems , stochastically perturbed systems , and finding easily verifiable conditions of model reducibility of nonlinear systems .",
    "bouvrie , j. and b. hamzi ( 2010 ) .",
    "balanced reduction of nonlinear control systems in reproducing kernel hilbert space , proc .",
    "48th annual allerton conference on communication , control , and computing , 2010 , pp .",
    "294  301 .",
    "coifman , r. r. , i. g. kevrekidis , s. lafon , m. maggioni and b. nadler ( 2008 ) .",
    "diffusion maps , reduction coordinates , and low dimensional representation of stochastic systems , _ multiscale model .",
    "_ , 7(2):842 - 864 .",
    "jonckheere , e.a . , and l. m. silverman ( 1983 ) .",
    "_ a new set of invariants for linear systems - application to reduced order compensator design_. ieee transactions on automatic control , * ac-28 * , 10 , 953 - 964 .",
    "kenney , c. and g. hewer ( 1987 ) .",
    "_ necessary and sufficient conditions for balancing unstable systems _ , ieee transactions on automatic control , * 32 * , 2 , pp . 157 - 160 .",
    "krener , a. j. ( 2007 ) .",
    "the important state coordinates of a nonlinear system . in _",
    "`` advances in control theory and applications '' _ , c. bonivento , a. isidori , l. marconi , c. rossi , editors , pp .",
    "161 - 170 .",
    "krener , a. j. ( 2008 ) . reduced order modeling of nonlinear control systems . in _",
    "`` analysis and design of nonlinear control systems '' _ , a. astolfi and l. marconi , editors , pp .",
    "springer .",
    "lall , s. , j. marsden , and s. glavaski ( 2002 ) . a subspace approach to balanced truncation for model reduction of nonlinear control systems , _ international journal on robust and nonlinear control _ , * 12 * , 5 , pp .",
    "519 - 535 .",
    "mika , s. , b. schlkopf , a. smola , k. r. mller , m. scholz , and g. rtsch ( 1998 ) .",
    "kernel pca and de - noising in feature spaces , in _ proc .",
    "advances in neural information processing systems ( nips ) 11 _ , pp . 536542 , mit press .",
    "newman , a.j . , and p. s. krishnaprasad ( 2000 ) .",
    "computing balanced realizations for nonlinear systems , _ proc .",
    "of the math .",
    "theory of networks and systems ( mtns)_. nilsson , o. ( 2009 ) .",
    "_ on modeling and nonlinear model reduction in automotive systems _ , ph.d .",
    "thesis , lund university ."
  ],
  "abstract_text": [
    "<S> we introduce a data - driven order reduction method for nonlinear control systems , drawing on recent progress in machine learning and statistical dimensionality reduction . </S>",
    "<S> the method rests on the assumption that the nonlinear system behaves linearly when lifted into a high ( or infinite ) dimensional feature space where balanced truncation may be carried out implicitly . </S>",
    "<S> this leads to a nonlinear reduction map which can be combined with a representation of the system belonging to a reproducing kernel hilbert space to give a closed , reduced order dynamical system which captures the essential input - output characteristics of the original model . </S>",
    "<S> empirical simulations illustrating the approach are also provided . </S>"
  ]
}