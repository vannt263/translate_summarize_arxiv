{
  "article_text": [
    "estimation of covariance matrix and its inverse is an important problem in many areas of statistical analysis . among many interesting examples",
    "are principal component analysis , linear / quadratic discriminant analysis , and graphical models .",
    "stable and accurate covariance estimation is becoming increasingly more important in the high dimensional setting where the dimension @xmath2 can be much larger than the sample size @xmath1 . in this setting classical methods and results based on fixed @xmath2 and large @xmath1 are no longer applicable .",
    "an additional challenge in the high dimensional setting is the computational costs .",
    "it is important that estimation procedures are computationally effective so that they can be used in high dimensional applications .",
    "let @xmath6 be a @xmath2-variate random vector with covariance matrix @xmath7 and precision matrix @xmath8 .",
    "given an independent and identically distributed random sample @xmath9 from the distribution of @xmath10 , the most natural estimator of @xmath7 is perhaps @xmath11 where @xmath12 .",
    "however , @xmath13 is singular if @xmath14 , and thus is unstable for estimating @xmath15 , not to mention that one can not use its inverse to estimate the precision matrix @xmath16 . in order to estimate the covariance matrix @xmath7 consistently ,",
    "special structures are usually imposed and various estimators have been introduced under these assumptions . when the variables exhibit a certain ordering structure , which is often the case for time",
    "series data , bickel and levina ( 2008a ) proved that banding the sample covariance matrix leads to a consistent estimator .",
    "cai , zhang and zhou ( 2010 ) established the minimax rate of convergence and introduced a rate - optimal tapering estimator . el karoui ( 2008 ) and bickel and levina ( 2008b ) proposed thresholding of the sample covariance matrix for estimating a class of sparse covariance matrices and obtained rates of convergence for the thresholding estimators .",
    "estimation of the precision matrix @xmath17 is more involved due to the lack of a natural pivotal estimator like @xmath13 . assuming certain ordering structures , methods based on banding the cholesky factor of the inverse",
    "have been proposed and studied .",
    "see , e.g. , wu and pourahmadi ( 2003 ) , huang et al .",
    "( 2006 ) , bickel and levina ( 2008b ) . penalized likelihood methods have also been introduced for estimating sparse precision matrices . in particular , the @xmath0 penalized normal likelihood estimator and its variants , which shall be called @xmath0-mle type estimators , were considered in several papers ; see , for example , yuan and lin ( 2007 ) , friedman et al.(2008 ) , daspremont et al .",
    "( 2008 ) , and rothman et al .",
    "convergence rate under the frobenius norm loss was given in rothman et al .",
    "( 2008 ) . yuan ( 2009 ) derived the rates of convergence for subgaussian distributions . under more restrictive conditions such as mutual incoherence or irrepresentable conditions , ravikumar et al .",
    "( 2008 ) obtained the rates of convergence in the elementwise @xmath5 norm and spectral norm .",
    "nonconvex penalties , usually computationally more demanding , have also been considered under the same normal likelihood model .",
    "for example , lam and fan ( 2009 ) and fan et al .  ( 2009 ) considered penalizing the normal likelihood with the nonconvex scad penalty .",
    "the main goal is to ameliorate the bias problem due to @xmath18 penalization .",
    "a closely related problem is the recovery of the support of the precision matrix , which is strongly connected to the selection of graphical models . to be more specific ,",
    "let @xmath19 be a graph representing conditional independence relations between components of @xmath10 .",
    "the vertex set @xmath20 has @xmath2 components @xmath21 and the edge set @xmath22 consists of ordered pairs @xmath23 , where @xmath24 if there is an edge between @xmath25 and @xmath26 . the edge between @xmath25 and @xmath26",
    "is excluded from @xmath22 if and only if @xmath25 and @xmath26 are independent given @xmath27 . if @xmath28 , then the conditional independence between @xmath25 and @xmath26 given other variables is equivalent to @xmath29 , where we set @xmath30 .",
    "hence , for gaussian distributions , recovering the structure of the graph @xmath31 is equivalent to the estimation of the support of the precision matrix ( lauritzen ( 1996 ) ) . a recent paper by liu et al .",
    "( 2009 ) showed that for a class of non - gaussian distribution called nonparanormal distribution , the problem of estimating the graph can also be reduced to the estimation of the precision matrix . in an important paper , meinshausen and bhlmann ( 2006 ) demonstrated convincingly a neighborhood selection approach to recover the support of @xmath16 in a row by row fashion .",
    "yuan ( 2009 ) replaced the lasso selection by a dantzig type modification , where first the ratios between the off - diagonal elements @xmath32 and the corresponding diagonal element @xmath33 were estimated for each row @xmath34 and then the diagonal entries @xmath33 were obtained given the estimated ratios .",
    "convergence rates under the matrix @xmath0 norm and spectral norm losses were established .    in the present paper ,",
    "we study estimation of the precision matrix @xmath16 for both sparse and non - sparse matrices , without restricting to a specific sparsity pattern .",
    "in addition , graphical model selection is also considered .",
    "a new method of constrained @xmath0-minimization for inverse matrix estimation ( clime ) is introduced .",
    "rates of convergence in spectral norm as well as elementwise @xmath5 norm and frobenius norm are established under weaker assumptions , and are shown to be faster than those given for the @xmath0-mle estimators when the population distribution has polynomial - type tails .",
    "a matrix is called @xmath3-sparse if there are at most @xmath3 non - zero elements on each row .",
    "it is shown that when @xmath17 is @xmath3-sparse and @xmath10 has either exponential - type or polynomial - type tails , the error between our estimator @xmath35 and @xmath17 satisfies @xmath36 and @xmath37 , where @xmath38 and @xmath39 are the spectral norm and elementwise @xmath40 norm respectively .",
    "properties of the clime estimator for estimating banded precision matrices are also discussed .",
    "the clime method can also be adopted for the selection of graphical models , with an additional thresholding step .",
    "the elementwise @xmath41 norm result is instrumental for graphical model selection .",
    "in addition to its desirable theoretical properties , the clime estimator is computationally very attractive for high dimensional data .",
    "it can be obtained one column at a time by solving a linear program , and the resulting matrix estimator is formed by combining the vector solutions ( after a simple symmetrization ) .",
    "no outer iterations are needed and the algorithm is easily scalable .",
    "an r package of our method has been developed and is publicly available on the web .",
    "numerical performance of the estimator is investigated using both simulated and real data .",
    "in particular , the procedure is applied to analyze a breast cancer dataset .",
    "results show that the procedure performs favorably in comparison to existing methods .",
    "the rest of the paper is organized as follows . in section [ sec :",
    "estimation ] , after basic notations and definitions are introduced , we present the clime estimator .",
    "theoretical properties including the rates of convergence are established in section [ sec : rate ] .",
    "graphical model selection is discussed in section [ sec : consistency ] .",
    "numerical performance of the clime estimator is considered in section [ sec : simu ] through simulation studies and a real data analysis .",
    "further discussions on the connections and differences of our results with other related work are given in section [ sec : conclusion ] .",
    "the proofs of the main results are given in section [ sec : proof ] .",
    "in compressed sensing and high dimensional linear regression literature , it is now well understood that constrained @xmath0 minimization provides an effective way for reconstructing a sparse signal .",
    "see , for example , donoho et al .  ( 2006 ) and cands and tao ( 2007 ) . a particularly simple and elementary analysis of constrained @xmath0 minimization methods is given in cai , wang and xu ( 2010 ) .",
    "in this section , we introduce a method of constrained @xmath0 minimization for inverse covariance matrix estimation .",
    "we begin with basic notations and definitions . throughout , for a vector @xmath42 , define @xmath43 and @xmath44 . for a matrix @xmath45",
    ", we define the elementwise @xmath40 norm @xmath46 , the spectral norm @xmath47 , the matrix @xmath0 norm @xmath48 , the frobenius norm @xmath49 , and the elementwise @xmath0 norm @xmath50 .",
    "@xmath51 denotes a @xmath52 identity matrix .",
    "for any two index sets @xmath53 and @xmath54 and matrix @xmath55 , we use @xmath56 to denote the @xmath57 matrix with rows and columns of @xmath55 indexed by @xmath53 and @xmath54 respectively .",
    "the notation @xmath58 means that @xmath55 is positive definite .",
    "we now define our clime estimator .",
    "let @xmath59 be the solution set of the following optimization problem : @xmath60 where @xmath61 is a tuning parameter . in ( [ c1 ] )",
    ", we do not impose the symmetry condition on @xmath62 and as a result the solution is not symmetric in general .",
    "the final clime estimator of @xmath17 is obtained by symmetrizing @xmath63 as follows .",
    "write @xmath64 .",
    "the clime estimator @xmath35 of @xmath17 is defined as @xmath65 in other words , between @xmath66 and @xmath67 , we take the one with smaller magnitude .",
    "it is clear that @xmath35 is a symmetric matrix .",
    "moreover , theorem [ thn-2 ] shows that it is positive definite with high probability .    the convex program ( [ c1 ] )",
    "can be further decomposed into @xmath2 vector minimization problems .",
    "let @xmath68 be a standard unit vector in @xmath69 with @xmath70 in the @xmath34-th coordinate and @xmath71 in all other coordinates .",
    "for @xmath72 , let @xmath73 be the solution of the following convex optimization problem @xmath74 where @xmath75 is a vector in @xmath69 .",
    "the following lemma shows that solving the optimization problem ( [ c1 ] ) is equivalent to solving the @xmath2 optimization problems ( [ o1 ] ) .",
    "that is , @xmath76 .",
    "this simple observation is useful both for implementation and technical analysis .",
    "[ le1 ] let @xmath59 be the solution set of ( [ c1 ] ) and let @xmath77 where @xmath78 are solutions to ( [ o1 ] ) for @xmath79",
    ". then @xmath80 .    to illustrate the motivation of ( [ c1 ] ) ,",
    "let us recall the method based on @xmath18 regularized log - determinant program ( cf .",
    "daspremont et al .",
    "( 2008 ) , friedman et al .",
    "( 2008 ) , banerjee et al .",
    "( 2008 ) ) as follows , which shall be called glasso after the algorithm that efficiently computes the solution , @xmath81 the solution @xmath82 satisfies @xmath83 where @xmath84 is an element of the subdifferential @xmath85 .",
    "this leads us to consider the optimization problem : @xmath86 however , the feasible set in ( [ c - c1 ] ) is very complicated . by multiplying the constraint with @xmath62 , such a relaxation of ( [ c - c1 ] ) leads to the convex optimization problem ( [ c1 ] ) , which can be easily solved . figure  [ fig : feasible ] illustrates the solution for recovering a @xmath87 by @xmath87 precision matrix @xmath88 $ ] , and we only consider the plane @xmath89 vs @xmath90 for simplicity .",
    "the point where the feasible polygon meets the dashed diamond is the clime solution @xmath35 .",
    "note that the log - likelihood function as in glasso is a smooth curve as compared to the polygon constraint in clime .",
    "constrained feasible set ( shaded polygon ) and the elementwise @xmath0 norm objective ( dashed diamond near the origin ) from clime .",
    "the log - likelihood function as in glasso is shown by the dotted line.,scaledwidth=70.0% ]",
    "in this section we investigate the theoretical properties of the clime estimator and establish the rates of convergence under different norms . write @xmath91 , @xmath92 and @xmath93 .",
    "it is conventional to divide the technical analysis into two cases according to the moment conditions on @xmath10 .    *",
    "( exponential - type tails ) * suppose that there exists some @xmath94 such that @xmath95 and @xmath96 where @xmath97 is a bounded constant .    * ( c2 ) .",
    "( polynomial - type tails ) * suppose that for some @xmath98 , @xmath99 , and for some @xmath100 @xmath101    for @xmath0-mle type estimators , it is typical that the convergence rates in the case of polynomial - type tails are much slower than those in the case of exponential - type tails .",
    "see , e.g. , ravikumar et al .",
    "we shall show that our clime estimator attains the same rates of convergence under either of the two moment conditions , and significantly outperforms @xmath0-mle type estimators in the case of polynomial - type tails .",
    "we begin by considering the uniformity class of matrices : @xmath102 for @xmath103 , where @xmath104 .",
    "similar parameter spaces have been used in bickel and levina ( 2008b ) for estimating the covariance matrix @xmath7 . note that in the special case of @xmath105 , @xmath106 is a class of @xmath107-sparse matrices .",
    "let @xmath108}^{2}=:\\max_{ij}\\theta_{ij}.\\end{aligned}\\ ] ] the quantity @xmath109 is related to the variance of @xmath110 , and the maximum value @xmath111 captures the overall variability of @xmath13 .",
    "it is easy to see that under either ( c1 ) or ( c2 ) @xmath111 is a bounded constant depending only on @xmath112 .",
    "the following theorem gives the rates of convergence for the clime estimator @xmath35 under the spectral norm loss .",
    "[ thn-2 ] suppose that @xmath113 .",
    "assume ( c1 ) holds .",
    "let @xmath114 , where @xmath115 and @xmath116 .",
    "then @xmath117 with probability greater than @xmath118 , where @xmath119 .    ( ii ) .",
    "assume ( c2 ) holds .",
    "let @xmath120 , where @xmath121 . then @xmath122 with probability greater than @xmath123 , where @xmath124 .",
    "when @xmath125 does not depend on @xmath126 , the rates in theorem [ thn-2 ] are the same as those for estimating @xmath7 in bickel and levina ( 2008b ) . in the polynomial - type tails case and when @xmath105 , the rate in ( [ d2 ] ) is significantly better than the rate @xmath127 for the @xmath0-mle estimator obtained in ravikumar et al .",
    "( 2008 ) .",
    "it would be of great interest to get the convergence rates for @xmath128 .",
    "however , it is even difficult to prove the existence of the expectation of @xmath129 as we are dealing with the inverse matrix .",
    "we modify the estimator @xmath35 to ensure the existence of such expectation and the same rates are established .",
    "let @xmath130 be the solution set of the following optimization problem : @xmath131 where @xmath132 with @xmath133 .",
    "write @xmath134 .",
    "define the symmetrized estimator @xmath135 as in ( [ me1 ] ) by @xmath136 clearly @xmath137 is a feasible point , and thus we have @xmath138 .",
    "the expectation @xmath139 is then well - defined .",
    "the other motivation to replace @xmath140 with @xmath141 comes from our implementation , which computes ( [ c1 ] ) by the primal dual interior point method .",
    "one usually needs to specify a feasible initialization .",
    "when @xmath142 , it is hard to find an initial value for ( [ c1 ] ) . for ( [ re - c1 ] ) , we can simply set the initial value to @xmath137 .",
    "[ thn-3 ] suppose that @xmath113 and ( c1 ) holds .",
    "let @xmath114 with @xmath143 being defined in theorem [ thn-2 ] ( i ) and @xmath144 being sufficiently large .",
    "let @xmath145 if @xmath146 for some @xmath147 , then we have @xmath148    * remark : * it is not necessary to restrict @xmath149 .",
    "in fact , from the proof we can see that theorem [ thn-3 ] still holds for @xmath150 with any @xmath151 .",
    "when the variables of @xmath10 are ordered , better rates can be obtained .",
    "similar as in bickel and levina ( 2008a ) , we consider the following class of precision matrices : @xmath152 for @xmath153 .",
    "suppose the modified cholesky factor of @xmath17 is @xmath154 , with the unique lower triangular matrix @xmath53 and diagonal matrix @xmath155 .",
    "to estimate @xmath17 , bickel and levina ( 2008a ) used the banding method and assumed @xmath156 .",
    "it is easy to see that @xmath156 implies @xmath157 for some constant @xmath158 . rather than assuming @xmath156",
    ", we use a more general assumption that @xmath159 .",
    "[ th2 ] let @xmath159 and @xmath160 with sufficiently large @xmath161 .",
    "( i ) . if ( c1 ) or ( c2 ) holds , then with probability greater than @xmath123 , @xmath162    ( ii ) .",
    "suppose that @xmath146 for some @xmath147 .",
    "if ( c1 ) holds and @xmath163 , then @xmath164    theorem [ th2 ] shows that our estimator has the same rate as that in bickel and levina ( 2008a ) by banding the cholesky factor of the precision matrix for the ordered variables .",
    "we have so far focused on the performance of the estimator under the spectral norm loss .",
    "rates of convergence can also be obtained under the elementwise @xmath40 norm and the frobenius norm .",
    "[ cr1 ] ( i ) . under the conditions of theorem [ thn-2 ] ( i ) ,",
    "we have @xmath165 with probability greater than @xmath118 .",
    "( ii ) . under the conditions of theorem [ thn-2 ] ( ii ) , we have @xmath166 with probability greater than @xmath123 .",
    "the rate in theorem [ cr1 ] ( ii ) is significantly faster than the one obtained by ravikumar et al .",
    "( 2008 ) ; see section 3.3 for more detailed discussions . a similar rate to ours was obtained by lam and fan ( 2009 ) under the frobenius norm .",
    "the elementwise @xmath41 norm result will lead to the model selection consistency result to be shown in the next section .",
    "we now give the rates for @xmath167 under expectation .",
    "[ n - cr1 ] under the conditions of theorem [ thn-3 ] , we have @xmath168    the proofs of theorems 1 - 5 rely on the following more general theorem .",
    "[ th1 ] suppose that @xmath113 and @xmath169 . if @xmath170 , then we have @xmath171 @xmath172 and @xmath173 where @xmath174 and @xmath175 .",
    "we compare our results to those of ravikumar et al .",
    "( 2008 ) , wherein the authors estimated @xmath17 by solving the following @xmath18 regularized log - determinant program : @xmath176 where @xmath177 . to obtain the rates of convergence in the elementwise @xmath5 norm and the spectral norm",
    ", they imposed the following condition :    * irrepresentable condition in ravikumar et al .",
    "( 2008 ) * there exists some @xmath178 $ ] such that @xmath179 where @xmath180 , @xmath181 is the support of @xmath17 and @xmath182 .",
    "the above assumption is particularly strong . under this assumption ,",
    "it was shown in ravikumar et al .",
    "( 2008 ) that @xmath183 estimates the zero elements of @xmath17 exactly by zero with high probability .",
    "in fact , a similar condition to for lasso with the covariance matrix @xmath7 taking the place of the matrix @xmath184 is sufficient and nearly necessary for recovering the support using the ordinary lasso ; see for example meinshausen and bhlmann ( 2006 ) .",
    "suppose that @xmath17 is @xmath107-sparse and consider subgaussian random variables @xmath185 with the parameter @xmath186 . in addition to , ravikumar et al.(2008 ) assumed that the sample size @xmath1 satisfies the bound @xmath187 where @xmath188 .",
    "under the aforementioned conditions , they showed that with probability greater than @xmath189 , @xmath190 where @xmath191_{ss})^{-1}\\|_{l_{1}}$ ] .",
    "note that their constant depends on quantities @xmath192 and @xmath193 , while our constant depends on @xmath125 , the bound of @xmath194 .",
    "they required ( [ c7 ] ) , while we only need @xmath195 .",
    "another substantial difference is that the irrepresentable condition ( [ ravicond ] ) is not needed for our results .",
    "we next compare our result to that of ravikumar et al .",
    "( 2008 ) under the case of polynomial - type tails .",
    "suppose ( c2 ) holds .",
    "corollary 2 in ravikumar et al .",
    "( 2008 ) shows that if @xmath196 for some @xmath197 , then with probability greater than @xmath189 , @xmath198 theorem [ cr1 ] shows our estimator still enjoys the order of @xmath199 in the case of polynomial - type tails .",
    "moreover , when @xmath200 , the range @xmath201 in our theorem is wider than their range @xmath196 with @xmath197 .",
    "it is worth noting that instead of the sparse precision matrices , our estimator allows for a wider class of matrices .",
    "for example , the estimator is still consistent for the model which is not truly sparse but has many small entries .",
    "as mentioned in the introduction , graphical model selection is an important problem .",
    "the constrained @xmath0 minimization procedure introduced in section [ sec : estimation ] for estimating @xmath17 can be modified to recover the support of @xmath17 .",
    "we introduce an additional thresholding step based on @xmath35 . more specifically , define a threshold estimator @xmath202 with @xmath203 where @xmath204 is a tuning parameter and @xmath61",
    "is given in theorem [ thn-2 ] .",
    "define @xmath205 and @xmath206 from the elementwise @xmath41 results established in theorem [ cr1 ] , with high probability , the resulting elements in @xmath35 shall exceed the threshold level if the corresponding element in @xmath16 is large in magnitude . on the contrary",
    ", the elements of @xmath35 outside the support of @xmath16 will remain below the threshold level with high probability .",
    "therefore , we have the following theorem on the threshold estimator @xmath207 .",
    "[ thm : supp ] suppose that ( c1 ) or ( c2 ) holds and @xmath208 . if @xmath209 , then with probability greater than @xmath123 , we have @xmath210 .",
    "the threshold estimator @xmath207 not only recovers the sparsity pattern of @xmath17 , but also recovers the signs of the nonzero elements .",
    "this property is called sign consistency in some literature .",
    "the condition @xmath209 is needed to ensure that nonzero elements are correctly retained . from theorem [ cr1 ] , we see that , if @xmath125 does not depend on @xmath126 , then @xmath211 is of order @xmath199 which is the same order as in ravikumar et al.(2008 ) for exponential - type tails , but weaker than their assumption @xmath212 for polynomial - type tails .    based on meinshausen and bhlmann ( 2006 ) , zhou et al .",
    "( 2009 ) applied adaptive lasso to covariance selection in gaussian graphical models . for @xmath213",
    ", they regress @xmath25 versus the other variables @xmath214 : @xmath215 , where @xmath216 is a normally distributed random variables with mean zero and the underlying coefficients can be shown to be @xmath217",
    ". then they use the adaptive lasso to recover the support of @xmath218 , which is identical to the support of @xmath17 . a main assumption in their paper",
    "is the restricted eigenvalue assumption on @xmath7 which is weaker than the irrepresentable condition .",
    "their method can recover the support of @xmath17 but is unable to estimate the elements in @xmath17 . without imposing the unnecessary irrepresentable condition , the additional advantage of our method is that it not only recovers the support of @xmath17 but also provides consistency results under the elementwise @xmath40 norm and the spectral norm .",
    "in this section we turn to the numerical performance of our clime estimator .",
    "the procedure is easy to implement .",
    "an r package of our method has been developed and is available on the web at + ` http://stat.wharton.upenn.edu/~tcai/paper/html/precision-matrix.html ` .",
    "+ the goal of this section is to first investigate the numerical performance of the estimator through simulation studies and then apply our method to the analysis of a breast cancer dataset .",
    "the proposed estimator @xmath35 can be obtained in a column by column fashion as illustrated in lemma 1 .",
    "hence we will focus on the numerical implementation of solutions to the optimization problem : @xmath219 we consider relaxation of the above , which is equivalent to the following linear programming problem : @xmath220 the same linear relaxation was considered in cands and tao ( 2007 ) , and was shown there to be very efficient for the dantzig selector problem in regression . to solve",
    ", we follow the primal dual interior method approach , for example see boyd and vandenberghe ( 2004 ) .",
    "the resulting algorithm has comparable numerical performance as other numerical procedures , for example glasso .",
    "note that we only need sweep through the @xmath2 columns once but glasso does need to have an extra outer layer of iterations to loop through the @xmath2 columns several times by cyclical coordinate descent .",
    "once @xmath221 is obtained by combining the @xmath222 s for each column , we symmetrize @xmath221 by setting the entry @xmath223 to be the smaller one in magnitude of two entries @xmath224 and @xmath225 , for all @xmath226 , as in .",
    "similar to many iterative methods , our method also requires a proper initialization within the feasible set .",
    "the initializing @xmath227 however can not be simply replaced by the solution of the linear system @xmath228 for each @xmath34 when @xmath14 , since @xmath13 is singular .",
    "the remedy is to add a small positive constant @xmath229 ( e.g. @xmath230 ) to all the diagonal entries of the matrix @xmath13 , that is we use the @xmath229-perturbed matrix @xmath231 to replace the @xmath13 in .",
    "such a perturbation does not noticeably affect the computational accuracy of the final solution in our numerical experiments .",
    "the resulting solution @xmath135 in the perturbed problem is shown to have all the theoretical properties in sections 3 and 4 , and even better the convergence rate of the spectral norm under expectation is also established there for @xmath135 .    in the context of high dimensional linear regression",
    ", a second stage refitting procedure was considered in cands and tao ( 2007 ) to correct the biases introduced by the @xmath0 norm penalization .",
    "their refitting procedure seeks the best coefficient vector , giving the maximum likelihood , which has the same support as the original dantzig selector .",
    "inspired by this two - stage procedure , we propose a similar two - stage procedure to further improve the numerical performance of the clime estimator by refitting as @xmath232 where @xmath233 and @xmath234 . here",
    "the estimator @xmath235 minimizes the bregman divergence among all symmetric positive definite matrices under the constraint .",
    "we shall call @xmath235 refitted clime hereafter .",
    "the bounds under the three norms in section  [ sec : rate ] and the support recovery @xmath236 can also be established .",
    "for example , the frobenius loss bound can be easily derived from the same approach used in rothman et al .",
    "( 2008 ) and fan et al.(2009 ) .",
    "other theoretical properties are more involved and we leave this to future work .",
    "we now compare the numerical performance of the clime estimator @xmath237 , the refitted clime estimator , the graphical lasso @xmath82 and the scad @xmath238 from fan et al .",
    "( 2009 ) which is defined as @xmath239 where the scad function @xmath240 is proposed by fan ( 1997 ) .",
    "we use recommended choice @xmath241 by fan and li ( 2001 ) throughout and set all @xmath242 to be the same for all @xmath223 entries for simplicity .",
    "this setting for @xmath243 and @xmath242 is the same as that of fan et al .",
    "see fan et al .",
    "( 2009 ) for further details on @xmath238 .",
    "note that @xmath82 has the equivalent performance as the spice estimator by rothman et al .",
    "( 2008 ) according to their study .",
    "we consider three models as follows :    * model 1 .",
    "@xmath244 . * model 2 .",
    "the second model comes from rothman et al .",
    "we let @xmath245 , where each off - diagonal entry in @xmath246 is generated independently and equals to 0.5 with probability 0.1 or 0 with probability 0.9 .",
    "@xmath247 is chosen such that the conditional number ( the ratio of maximal and minimal singular values of a matrix ) is equal to @xmath2 .",
    "finally , the matrix is standardized to have unit diagonals .",
    "* model 3 . in this model",
    ", we consider a non - sparse matrix and let @xmath17 have all off - diagonal elements @xmath248 and the diagonal elements @xmath70 .",
    "the first model has a banded structure , and the values of the entries decay as they move away from the diagonal .",
    "the second is an example of a sparse matrix without any special sparsity patterns .",
    "the third serves as a dense matrix example .    for each model",
    ", we generate a training sample of size @xmath249 from a multivariate normal distribution with mean zero and covariance matrix @xmath15 , and an independent sample of size @xmath250 from the same distribution for validating the tuning parameter @xmath242 . using the training data , a series of estimators with @xmath251 different values of @xmath242",
    "are computed , and the one with the smallest likelihood loss on the validation sample is used , where the likelihood loss is defined by @xmath252 the glasso and scad estimators are computed on the same training and testing data using the same cross validation scheme .",
    "we consider different values of @xmath253 and replicate @xmath250 times .",
    "the estimation quality is first measured by the following matrix norms : the operator norm , the matrix @xmath0 norm and the frobenius norm .",
    "table [ tb : simu ] reports the averages and standard errors of these losses .     & & & + @xmath2 & @xmath237 & @xmath254 & @xmath238 & @xmath237 & @xmath82 & @xmath238 & @xmath237 & @xmath82 & @xmath238 + 30 & @xmath255&@xmath256&@xmath257&@xmath258&@xmath259&@xmath260&@xmath261&@xmath262&@xmath263 + 60 & @xmath264&@xmath265&@xmath266&@xmath267&@xmath268&@xmath269&@xmath270&@xmath271&@xmath272 + 90 & @xmath273&@xmath274&@xmath275&@xmath276&@xmath277&@xmath278&@xmath279&@xmath280&@xmath281 + 120 & @xmath282&@xmath283&@xmath284&@xmath285&@xmath286&@xmath287&@xmath288&@xmath289&@xmath290 + 200 & @xmath291&@xmath292&@xmath293&@xmath294&@xmath295&@xmath296&@xmath297&@xmath298&@xmath299 +   +   + & & & + @xmath2 & @xmath237 & @xmath254 & @xmath238 & @xmath237 & @xmath82 & @xmath238 & @xmath237 & @xmath82 & @xmath238 + 30 & @xmath300&@xmath301&@xmath300&@xmath302&@xmath303&@xmath304&@xmath305&@xmath306&@xmath307 + 60 & @xmath308&@xmath309&@xmath310&@xmath311&@xmath312&@xmath313&@xmath314&@xmath315&@xmath316 + 90 & @xmath317&@xmath318&@xmath319&@xmath320&@xmath321&@xmath322&@xmath323&@xmath323&@xmath324 + 120 & @xmath325&@xmath326&@xmath327&@xmath328&@xmath329&@xmath330&@xmath331&@xmath332&@xmath333 + 200 & @xmath309&@xmath334&@xmath335&@xmath336&@xmath337&@xmath338&@xmath339&@xmath340&@xmath341 +   +   + & & & + @xmath2 & @xmath237 & @xmath254 & @xmath238 & @xmath237 & @xmath82 & @xmath238 & @xmath237 & @xmath82 & @xmath238 + 30 & @xmath342&@xmath343&@xmath344&@xmath345&@xmath346&@xmath347&@xmath262&@xmath348&@xmath349 + 60 & @xmath350&@xmath351&@xmath352&@xmath329&@xmath353&@xmath310&@xmath271&@xmath271&@xmath272 + 90 & @xmath354&@xmath355&@xmath356&@xmath357&@xmath358&@xmath359&@xmath360&@xmath361&@xmath362 + 120 & @xmath363&@xmath364&@xmath365&@xmath366&@xmath367&@xmath368&@xmath369&@xmath370&@xmath371 + 200 & @xmath372&@xmath373&@xmath374&@xmath375&@xmath376&@xmath377&@xmath297&@xmath298&@xmath378 +    [ tb : simu ]    we see that clime nearly uniformly outperforms glasso .",
    "the improvement tends to be slightly more significant for sparse models when @xmath2 is large , but overall the improvement is not dramatic . among the three methods ,",
    "scad is computationally most costly , but numerically it has the best performance among the three when @xmath379 and is comparable to clime when @xmath2 is large .",
    "note that scad employs a nonconvex penalty to correct the bias while clime currently optimizes the convex @xmath0 norm objective efficiently .",
    "a more comparable procedure that also corrects the bias is our two - stage refitted clime , denoted by @xmath380 .",
    "table  [ tab : gclime ] illustrates the improvement from bias correction , and we only list the spectral norm loss for reasons of space .",
    "it is clear that our refitted clime estimator has comparable or better performance than scad , and our refitted clime is especially favorable when @xmath2 is large .",
    ".comparison of average(se ) operator norm losses from model 1 and 2 over @xmath250 replications . [ cols=\"^,^,^,^,^ \" , ]     [ tb : pcr ]    it is clear that clime significantly outperforms on the sensitivity and is comparable with other two methods on the specificity .",
    "the overall classification performance measured by mcc overwhelmingly favors our method clime , which shows an @xmath381 improvement over the best alternative methods .",
    "clime also produced the most sparse matrix than all other alternatives , which is usually favorable for interpretation purposes on real data sets .",
    "this paper develops a new constrained @xmath0 minimization method for estimating high dimensional precision matrices .",
    "both the method and the analysis are relatively simple and straightforward , and may be extended to other related problems .",
    "moreover , the method and the results are not restricted to a specific sparsity pattern .",
    "thus the estimator can be used to recover a wide class of matrices in theory as well as in applications .",
    "in particular , when applying our method to covariance selection in gaussian graphical models , the theoretical results can be established without assuming the irrepresentable condition in ravikumar et al .",
    "( 2008 ) , which is very stringent and hard to check in practice .",
    "several papers , such as yuan and lin ( 2007 ) , rothman et al .",
    "( 2008 ) and ravikumar et al .",
    "( 2008 ) , estimate the precision matrix by solving the optimization problem ( [ lass ] ) with @xmath0 penalty only on the off diagonal entries , which is slightly different from our starting point ( [ glass ] ) presented here .",
    "one can also similarly considered the following optimization problem @xmath382 analogous results can also be established for the above estimator .",
    "we omit them in this paper , due to high resemblance in proof techniques and conclusions .",
    "there are several possible extensions for our method .",
    "for example , zhou et al .",
    "( 2008 ) considered the time varying undirected graphs and estimated @xmath383 by glasso .",
    "it would be very interesting to study the estimation of @xmath383 by our method .",
    "ravikumar and wainwright ( 2009 ) considered high - dimensional ising model selection using @xmath18-regularized logistic regression .",
    "it would be interesting to apply our method to their setting as well .",
    "another important subject is to investigate the theoretical property of the tuning parameter selected by cross - validation method , though from our experiments clime is not very sensitive to the choice of the tuning parameter .",
    "an example of such results on cross validation can be found in bickel and levina ( 2008b ) on thresholding .",
    "after this paper was submitted , it came to our attention that zhang ( 2010 ) proposed a precision matrix estimator , called gmacs , which is the solution of the following optimization problem : @xmath384 the objective function here is different from that of clime , and this basic version can not be solved column by column and is not as easy to implement . zhang ( 2010 ) considers only the gaussian case and @xmath385 balls , whereas we consider subgaussian and polynomial - tail distributions and more general @xmath386 balls .",
    "also , the gmacs estimator requires an additional thresholding step in order for the rates to hold over @xmath385 balls .",
    "in contrast , clime does not need an additional thresholding step and the rates hold over general @xmath386 balls .",
    "* proof of lemma [ le1 ] . * write @xmath387 , where @xmath388 .",
    "the constraint @xmath389 is equivalent to @xmath390 thus we have @xmath391 since @xmath392 , by the definitions of @xmath59 , we have @xmath393 by([c2 ] ) and ( [ c3 ] ) , we have @xmath394 . on the other hand , if @xmath395 , then there exists an @xmath34 such that @xmath396 .",
    "hence by ( [ c2 ] ) we have @xmath397 .",
    "this is in conflict with ( [ c3 ] ) .",
    "+ the main results all rely on theorem [ th1 ] , which upper bounds the elementwise @xmath41 norm .",
    "we will prove it first .",
    "* proof of theorem [ th1 ] .",
    "* let @xmath398 be a solution of ( [ o1 ] ) by replacing @xmath140 with @xmath141 .",
    "note that lemma [ le1 ] still holds for @xmath399 and @xmath400 with @xmath401 . for notation briefness",
    ", we only prove the theorem for @xmath402 .",
    "the proof is exactly the same for general @xmath133 . by the condition in theorem [ th1 ] , @xmath403 then we have @xmath404 where we used the inequality @xmath405 for matrices @xmath406 of appropriate sizes . by the definition of @xmath407",
    ", we can see that @xmath408 for @xmath72 . by lemma [ le1 ] ,",
    "@xmath409 we have @xmath410 therefore by ( [ c4])-([a9 ] ) , @xmath411 it follows that @xmath412 this establishes ( [ t2 ] ) by the definition in .",
    "we next prove ( [ t1 ] ) .",
    "let @xmath413 and define @xmath414 by the definition ( [ me1 ] ) of @xmath35 , we have @xmath415 which implies that @xmath416 .",
    "this follows that @xmath417 .",
    "so we only need to upper bound @xmath418 .",
    "we have @xmath419 where we used the following inequality : for any @xmath420 , we have @xmath421 this completes the proof of ( [ t1 ] ) .",
    "finally , ( [ t3 ] ) follows from ( [ t4 ] ) , ( [ t2 ] ) and the inequality @xmath422 for any @xmath52 matrix . + * proof of theorems [ thn-2 ] ( i ) and [ cr1 ] ( i ) . * by theorem [ th1 ] , we only need to prove @xmath423 with probability greater than @xmath118 under ( c1 ) . without loss of generality , we assume @xmath424 .",
    "let @xmath425 and @xmath426 .",
    "then we have @xmath427 .",
    "let @xmath428 .",
    "using the inequality @xmath429 for any @xmath430 and letting @xmath431 , by basic calculations , we can get @xmath432 hence we have @xmath433 by the simple inequality @xmath434 for @xmath435 , we have @xmath436 for all @xmath437 .",
    "let @xmath438 and @xmath439 .",
    "as above , we can show that @xmath440 by ( [ c5 ] ) , ( [ c6 ] ) and the inequality @xmath441 , we see that ( [ c9 ] ) holds .",
    "+ * proof of theorems [ thn-2 ] ( ii ) and [ cr1 ] ( ii ) .",
    "* let @xmath442 since @xmath443 , we have by ( c2 ) , @xmath444 by bernstein s inequality ( cf . bennett ( 1962 ) ) and some elementary calculations ,",
    "@xmath445 so we have @xmath446 using the same truncation argument and bernstein s inequality , we can show that @xmath447 hence @xmath448 combining ( [ c13 ] ) and ( [ c12 ] ) , we have @xmath449 with probability greater than @xmath123 . the proof is completed by and theorem [ th1 ] . + * proof of theorems [ thn-3 ] and [ n - cr1 ] . *",
    "since @xmath450 is a feasible point , we have by ( [ a10 ] ) , @xmath451 by ( [ c9 ] ) , theorem [ th1 ] , the fact @xmath146 and since @xmath144 is large enough , we have @xmath452 this proves theorem [ thn-3 ] . the proof of theorem [ n - cr1 ] is similar . + * proof of theorem [ th2 ] .",
    "* let @xmath453 be an integer satisfying @xmath454 .",
    "define @xmath455 by the proof of theorem [ th1 ] , we can show that @xmath456 .",
    "since @xmath457 , we have @xmath458 . by theorem [ cr1 ] , @xmath459 with probability greater than @xmath123 .",
    "theorem [ th2 ] ( i ) is proved by taking @xmath460 $ ] .",
    "the proof of theorem [ th2 ] ( ii ) is similar as that of theorem [ thn-3 ] .",
    "we would like to thank the associate editor and two referees for their very helpful comments which have led to a better presentation of the paper .",
    "hess , k. r. , anderson , k. , symmans , w. f. , valero , v. , ibrahim , n. , mejia , j. a. , booser , d. , theriault , r. l. , buzdar , a. u. , dempsey , p. j. , rouzier , r. , sneige , n. , ross , j. s. , vidaurre , t. , gmez , h. l. , hortobagyi , g. n. and pusztai , l. ( 2006 ) .",
    "pharmacogenomic predictor of sensitivity to preoperative chemotherapy with paclitaxel and fluorouracil , doxorubicin , and cyclophosphamide in breast cancer .",
    "_ journal of clinical oncology _ 24 : 4236 - 44 .",
    "ravikumar , p. , wainwright , m. , raskutti , g. and yu , b. ( 2008 ) . high - dimensional covariance estimation by minimizing @xmath461-penalized log - determinant divergence .",
    "technical report 797 , uc berkeley , statistics department , nov . 2008 .",
    "( submitted ) .",
    "zhou , s. , lafferty , j. and wasserman , l. ( 2008 ) .",
    "time varying undirected graphs . to appear in machine learning journal ( invited ) , special issue for the 21st annual conference on learning theory ( colt 2008 ) ."
  ],
  "abstract_text": [
    "<S> a constrained @xmath0 minimization method is proposed for estimating a sparse inverse covariance matrix based on a sample of @xmath1 iid @xmath2-variate random variables . </S>",
    "<S> the resulting estimator is shown to enjoy a number of desirable properties . </S>",
    "<S> in particular , it is shown that the rate of convergence between the estimator and the true @xmath3-sparse precision matrix under the spectral norm is @xmath4 when the population distribution has either exponential - type tails or polynomial - type tails . </S>",
    "<S> convergence rates under the elementwise @xmath5 norm and frobenius norm are also presented . </S>",
    "<S> in addition , graphical model selection is considered . </S>",
    "<S> the procedure is easily implementable by linear programming . </S>",
    "<S> numerical performance of the estimator is investigated using both simulated and real data . </S>",
    "<S> in particular , the procedure is applied to analyze a breast cancer dataset . </S>",
    "<S> the procedure performs favorably in comparison to existing methods .    </S>",
    "<S> * keywords : * constrained @xmath0 minimization , covariance matrix , frobenius norm , gaussian graphical model , rate of convergence , precision matrix , spectral norm . </S>"
  ]
}