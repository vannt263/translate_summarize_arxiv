{
  "article_text": [
    "this note has been inspired by our reading of jog s and patil s interesting work on elastic stability @xcite which is full of new ideas and insights , notably the inspiring introduction of the _ true - stress - true - strain _ monotonicity condition ( c.f .",
    "@xcite ) @xmath0      & \\sigma(\\log v ) = { \\frac{1}{\\det v}}\\cdot\\tau(\\log v ) = e^{-\\operatorname{tr}(\\log v)}\\cdot\\underbrace{\\partial_{\\log v}\\ , w(\\log v)}_{=:\\tau(\\log v)}\\,,\\nonumber\\end{aligned}\\ ] ] where @xmath1 is the _ cauchy - stress _ ( or _ true stress _ ) tensor considered as a function of the _ logarithmic _ ( or _ true _ ) _ strain _ @xmath2 , @xmath3 is the _ left biot - stretch _ tensor and @xmath4 denotes the canonical inner product on @xmath5 . inequality , which can also be stated as @xmath6 was originally used by jog and patil @xcite to characterize material instabilities in elastic materials . while inequality is not fulfilled by the stress response induced by the _ _ isotropic hencky energy__@xcite @xmath7 ^ 2\\ ] ] with the shear modulus @xmath8 and the bulk modulus @xmath9 , the energy function @xmath10 ^ 2}\\ , , \\qquad k>\\frac38\\,,\\ ; { \\hat{k}}>\\frac18\\ , , \\label{eq : tstsenergy}\\ ] ] which approximates the hencky energy for sufficiently small strains , satisfies on all of @xmath11 ( * ? ? ?",
    "* corollary 4.1 ) ; here , @xmath12 is the trace of @xmath13 , @xmath3 is the left biot - stretch tensor , @xmath14 denotes the frobenius matrix norm , @xmath15 is the deviatoric part of @xmath16 and @xmath17 are the two lam constants . furthermore ,",
    "another variant recently introduced as the _ exponentiated hencky energy _",
    "@xcite @xmath18 ^ 2}\\ ] ] with dimensionless parameters @xmath19 and @xmath20 fulfils on the conical `` elastic domain '' @xmath21 for a given yield stress @xmath22 ( * ? ? ?",
    "* remark 4.1 ) . for other well - known energy functions like neo - hooke ,",
    "mooney - rivlin or the ogden energy , is not satisfied . until it was not even clear whether there exists an isotropic hyperelastic formulation satisfying at all .",
    "we believe that the true - stress - true - strain monotonicity condition has the potential to greatly advance the subject of constitutive requirements in nonlinear elasticity .",
    "therefore , we find it apt to shed some light on different notions of monotonicity and their interconnections which arise in nonlinear elasticity in general as well as in computations for checking inequality in particular .",
    "since many of the stress tensors in nonlinear elasticity are symmetric , we consider in the following matrix functions mapping a convex subset of @xmath23 to the set @xmath23 of symmetric matrices .",
    "of particular interest is the monotonicity of the principal matrix logarithm @xmath24 on the set @xmath11 of positive definite matrices .",
    "let @xmath25 be a finite - dimensional hilbert space with the inner product @xmath26 and let @xmath27 be a convex open subset of @xmath25 .",
    "a function @xmath28 is called _ monotone _ ( or _ hilbert space monotone _ ) on @xmath29 if @xmath30 for all @xmath31 , and it is called _ strictly monotone _ if @xmath32 for all @xmath33 . it is well known that a function @xmath34 is monotone on @xmath29 if and only if @xmath35.h,\\,h \\rangle}\\geq 0\\ ] ] for all @xmath36 , i.e.  if and only if the frchet derivative @xmath37 $ ] is positive semi - definite for all @xmath38 , and it is strictly monotone on @xmath29 if @xmath35.h,\\,h \\rangle } > 0 \\label{eq : strongstrictpositivedefiniteness}\\ ] ] for all @xmath36 , i.e.  if @xmath37 $ ] is positive definite for all @xmath38 . note , however , that is not a necessary condition for strict monotonicity .",
    "the following lemma shows that for a continuously differentiable function @xmath39 on a convex set whose derivative @xmath40 is self - adjoint and invertible everywhere , the positive definiteness of @xmath40 in a single point is sufficient for @xmath39 to be strictly monotone everywhere .",
    "[ lemma : posdefsinglepoint ] let @xmath27 be a convex open subset of @xmath25 , and let @xmath34 satisfy @xmath41\\text { is \\emph{positive definite},}\\\\ & \\text{ii ) } & & \\quad\\forall\\:a\\in m : \\quad & & df[a]\\text { is \\emph{invertible } and \\emph{self - adjoint}.}\\end{aligned}\\ ] ] then @xmath42.h,\\ , h \\rangle } > 0 $ ] for all @xmath43 and thus @xmath39 is strictly monotone on @xmath29 .",
    "assume that @xmath39 is not monotone on @xmath29 .",
    "then there exists @xmath44 such that @xmath45 $ ] is not positive semi - definite .",
    "since @xmath39 is continuously differentiable , the function @xmath46)\\ ] ] mapping @xmath47 to the smallest eigenvalue of @xmath37 $ ] is continuous on @xmath29 ; note that the mapping of a matrix to its smallest eigenvalue is continuous on a set of self - adjoint tensors .",
    "+ the set @xmath29 is convex ( and thus connected ) by assumption , hence we can choose a curve @xmath48;m)$ ] with @xmath49 , @xmath50 and obtain @xmath51 ) \\:>\\ : 0\\,,{\\nonumber \\\\}\\varphi(\\gamma(1 ) ) & \\:=\\ : \\lambda_{\\min}(df[a_1 ] ) \\:\\leq\\ : 0\\,.\\end{aligned}\\ ] ] thus there exists @xmath52 $ ] with @xmath53 , according to the intermediate value theorem .",
    "but then @xmath54 is an eigenvalue of @xmath55 $ ] and hence @xmath55 $ ] is not invertible , contradicting ii ) .",
    "note that while the proof requires @xmath29 to be convex in order to show monotonicity , connectedness of @xmath29 is sufficient to show that @xmath40 is positive definite everywhere .    in the one - dimensional case ,",
    "lemma [ lemma : posdefsinglepoint ] simply states the fact that for a continuously differentiable function @xmath39 on @xmath56 it follows from @xmath57 everywhere and @xmath58 for some @xmath59 that @xmath60 everywhere on @xmath56 .",
    "in this section we consider a _ primary matrix function _",
    "@xmath39 on the set @xmath23 of symmetric matrices .",
    "such a function is defined as follows : let @xmath61 be an open interval in @xmath56 and let @xmath62 .",
    "we denote and @xmath63 .",
    "] by @xmath64 the set of symmetric matrices with no eigenvalues outside @xmath61 : @xmath65 where @xmath66 is the ordered vector of the @xmath67 ( not necessarily distinct ) eigenvalues of @xmath29 .",
    "then the primary matrix function @xmath68 is defined by @xmath69 where @xmath70 , @xmath71 , is any orthogonal diagonalization of @xmath47 .",
    "furthermore we denote by @xmath72 the canonical inner product on @xmath23 .",
    "for now we assume that @xmath73 , where @xmath74 is the set of analytic functions on @xmath56 .",
    "the more general case will be considered later on . + for readability reasons all lemmas , propositions and proofs will be stated for the case @xmath75 and , correspondingly , @xmath76 . the restriction to the set of positive definite matrices ( or even , for some open interval @xmath77 , the ( convex ) set @xmath64 of symmetric matrices @xmath47 with @xmath78 ) allows for nearly identical proofs .",
    "the following lemma is stated in @xcite in a more general form .",
    "the proof given there is based on the expansion of @xmath39 into a matrix power series : observe for example that , for @xmath79 , @xmath80.h = ah+ha\\ ] ] and hence @xmath35.h,\\ , { \\widetilde h}\\rangle } = { \\langle ah+ha,\\ , { \\widetilde h}\\rangle } = { \\langle ah,\\ , { \\widetilde h}\\rangle } + { \\langle ha,\\ , { \\widetilde h}\\rangle } = { \\langle h,\\ , a{\\widetilde h}+{\\widetilde h}a \\rangle } = { \\langle h,\\ , df[a].{\\widetilde h}\\rangle}\\ ] ] for @xmath81 , thus @xmath37 $ ] is self - adjoint with respect to the canonical inner product on @xmath5 .",
    "similarly , the derivative of @xmath82 is self - adjoint for all @xmath83 , from which one can show that the derivative of an analytic matrix function @xmath84 is self - adjoint as well .",
    "[ lemma : primaryselfadjoint ] let @xmath85 .",
    "then the derivative of @xmath76 is self - adjoint with respect to the canonical inner product on @xmath23 : @xmath35.h,\\,{\\widetilde h}\\rangle } = { \\langle h,\\,df[a].{\\widetilde h}\\rangle } \\quad \\forall a , h,{\\widetilde h}\\in{\\operatorname{sym}(n)}\\,.\\ ] ]    we use an integral formula given in ( * ? ? ?",
    "* ( 6.6.2 ) ) : @xmath86.h = { \\frac{1}{2\\pi i}}\\int_\\gamma f(z ) ( z { { 1\\!\\!\\!\\:1 } } -a){^{-1}}\\,h\\ , ( z { { 1\\!\\!\\!\\:1 } } -a){^{-1}}\\,{\\mathrm{dz}}\\ ] ] for @xmath87 , where @xmath88 is a closed curve in @xmath89 such that every eigenvalue of @xmath47 has winding number @xmath90 .",
    "+ for @xmath91 we compute @xmath92.h,\\,{\\widetilde h}\\rangle } & = { \\langle { \\frac{1}{2\\pi i}}\\int_\\gamma f(z)\\,(z { { 1\\!\\!\\!\\:1 } } -a){^{-1}}\\,h\\ , ( z { { 1\\!\\!\\!\\:1 } } -a){^{-1}}\\ , { \\mathrm{dz}},\\ : { \\widetilde h}\\rangle}{\\nonumber \\\\}&= { \\frac{1}{2\\pi i}}\\int_\\gamma f(z)\\,{{\\langle ( z { { 1\\!\\!\\!\\:1 } } -a){^{-1}}\\,h\\ , ( z { { 1\\!\\!\\!\\:1 } } -a){^{-1}},\\ : { \\widetilde h}\\rangle}_{\\mathbb{c}}}\\,{\\mathrm{dz}}{\\nonumber \\\\}&= { \\frac{1}{2\\pi i}}\\int_\\gamma f(z)\\,{{\\langle h,\\ : ( z { { 1\\!\\!\\!\\:1 } } -a)^{- * } \\,{\\widetilde h}\\ , ( z { { 1\\!\\!\\!\\:1 } } -a)^{- * } \\rangle}_{\\mathbb{c}}}\\,{\\mathrm{dz}}{\\nonumber \\\\}&= { \\frac{1}{2\\pi i}}\\int_\\gamma f(z)\\,{{\\langle \\big((z { { 1\\!\\!\\!\\:1 } } -a)^{- * } \\,{\\widetilde h}\\ , ( z { { 1\\!\\!\\!\\:1 } } -a)^{-*}\\big)^*,\\:h^ * \\rangle}_{\\mathbb{c}}}\\,{\\mathrm{dz}}{\\nonumber \\\\}&= { \\frac{1}{2\\pi i}}\\int_\\gamma f(z)\\,{{\\langle \\big((z { { 1\\!\\!\\!\\:1 } } -a)^{- * } \\,{\\widetilde h}\\ , ( z { { 1\\!\\!\\!\\:1 } } -a)^{-*}\\big)^*,\\:h^ * \\rangle}_{\\mathbb{c}}}\\,{\\mathrm{dz}}\\label{eq : integralone}\\\\      & = { \\frac{1}{2\\pi i}}\\int_\\gamma f(z)\\,{{\\langle ( z { { 1\\!\\!\\!\\:1 } } -a)^{-t } \\,{\\widetilde h}\\ , ( z { { 1\\!\\!\\!\\:1 } } -a)^{-t},\\:h \\rangle}_{\\mathbb{c}}}\\,{\\mathrm{dz}}\\label{eq : integraltwo}\\\\      & = { \\frac{1}{2\\pi i}}\\int_\\gamma f(z)\\,{{\\langle ( z { { 1\\!\\!\\!\\:1 } } -a){^{-1}}\\,{\\widetilde h}\\ , ( z { { 1\\!\\!\\!\\:1 } } -a){^{-1}},\\:h \\rangle}_{\\mathbb{c}}}\\,{\\mathrm{dz}}\\label{eq : integralthree}\\\\      & = { { \\langle { \\frac{1}{2\\pi i}}\\int_\\gamma f(z)\\,(z { { 1\\!\\!\\!\\:1 } } -a){^{-1}}\\,{\\widetilde h}\\ , ( z { { 1\\!\\!\\!\\:1 } } -a){^{-1}}\\,{\\mathrm{dz}},\\:h \\rangle}_{\\mathbb{c } } } = { \\langle df[a].{\\widetilde h},\\:h \\rangle } = { \\langle h,\\:df[a].{\\widetilde h}\\rangle}\\,,\\nonumber\\end{aligned}\\ ] ] where equality of and holds due to @xmath93 and @xmath94 being real and symmetric while the symmetry of @xmath95 implies .",
    "this lemma can now be used to obtain some interesting properties of primary matrix functions and their derivatives .",
    "[ prop : specmontohilmonana ] let @xmath85 with @xmath96 for all @xmath97 .",
    "then the primary matrix function @xmath76 is hilbert space monotone .",
    "according to lemma [ lemma : primaryselfadjoint ] , the derivative @xmath37 $ ] is self - adjoint with respect to the inner product @xmath26 for every @xmath87 .",
    "thus we can apply lemma [ lemma : posdefsinglepoint ] if we show that @xmath98 $ ] is invertible everywhere and positive definite in one point .",
    "+ let @xmath99 .",
    "then the derivative of @xmath39 at @xmath100 is @xmath101.h & = \\lim_{t\\to 0}\\:{\\frac{1}{t}}\\:[f ( { { 1\\!\\!\\!\\:1 } } + t\\,h)-f ( { { 1\\!\\!\\!\\:1 } } ) ] \\\\      & = \\lim_{t\\to 0}\\:{\\frac{1}{t}}\\:q^t[f ( { { 1\\!\\!\\!\\:1 } } + \\operatorname{diag}(th_1,\\dotsc , th_n))-f(1 ) { { 1\\!\\!\\!\\:1 } } ] q\\\\      & = \\lim_{t\\to 0}\\:{\\frac{1}{t}}\\:q^t[f(\\operatorname{diag}(1+th_1,\\dotsc,1+th_n))-\\operatorname{diag}(f(1),\\dotsc , f(1))]q\\\\      & = \\lim_{t\\to 0}\\:{\\frac{1}{t}}\\:q^t[\\operatorname{diag}(f(1+th_1)-f(1),\\dotsc , f(1+th_n)-f(1))]q\\\\      & = q^t\\operatorname{diag}(h_1f'(1),\\dotsc , h_nf'(1 ) ) \\;=\\ ; f'(1)\\,h\\,,\\end{aligned}\\ ] ] thus @xmath102.h,\\,h \\rangle } = f'(1)\\,{\\langle h,\\,h \\rangle }   = f'(1)\\,{\\vert h \\vert}^2 \\:>\\:0\\end{aligned}\\ ] ] because @xmath60 and @xmath103 by assumption .",
    "+ to see that @xmath37 $ ] is invertible for every @xmath87 we simply note that @xmath39 is invertible on @xmath56 and the differentiable primary matrix function @xmath104 is the inverse of @xmath39 on @xmath23 .",
    "then for all @xmath87 , the linear mapping @xmath37 $ ] must be invertible as well .",
    "the next lemma shows that every analytic primary matrix function can be represented as the gradient field ( differentiated with respect to @xmath105 ) of an isotropic energy function satisfying the _ valanis - landel hypothesis _",
    "@xcite of additive separation '' @xcite .",
    "] : @xmath106 where @xmath107 is the @xmath108-th eigenvalue of @xmath105 .",
    "this might be considered the `` hidden assumption '' underlying the theory of primary matrix functions .",
    "[ prop : potentialana ] let @xmath85 .",
    "then for any @xmath109 with @xmath110 , the function @xmath111 is a _ potential _ of @xmath76 , i.e. @xmath112 = f(a)\\ ] ] or , more precisely , @xmath112.h = { \\langle f(a),\\,h \\rangle}\\ ] ] for all @xmath113 .",
    "let @xmath113 . since @xmath114.h$ ] is the partial derivative of @xmath115 in direction @xmath93 at the point @xmath47 we find @xmath112.h = \\lim_{t\\to 0 } \\:{\\frac{1}{t}}\\ : [ w(a+th)-w(a)]\\,.\\ ] ] we choose @xmath71 such that @xmath116 , where @xmath117 , @xmath118 denoting the eigenvalues of @xmath47 , and compute @xmath119.h & = \\lim_{t\\to 0 } \\:{\\frac{1}{t}}\\ : [ w(a+th)-w(a ) ] = \\lim_{t\\to 0 } \\:{\\frac{1}{t}}\\:[\\operatorname{tr}f(a+th ) -",
    "\\operatorname{tr}f(a)]\\\\      & = \\lim_{t\\to 0 } \\:{\\frac{1}{t}}\\:{\\langle f(a+th)-f(a),\\ , { { 1\\!\\!\\!\\:1 } } \\rangle } = { \\langle \\lim_{t\\to 0 } \\:{\\frac{1}{t}}\\:[f(a+th)-f(a)],\\ , { { 1\\!\\!\\!\\:1 } } \\rangle } = { \\langle df[a].h,\\ , { { 1\\!\\!\\!\\:1 } } \\rangle}\\ , .",
    "\\ ] ] according to lemma [ lemma : primaryselfadjoint ] , the total derivative @xmath98 $ ] is self - adjoint with respect to @xmath26 and thus @xmath120.h,\\ , { { 1\\!\\!\\!\\:1 } } \\rangle } = { \\langle h,\\,df[a ] . { { 1\\!\\!\\!\\:1 } } \\rangle}\\,.\\ ] ] we find @xmath121 .",
    "{ { 1\\!\\!\\!\\:1 } } & = \\lim_{t\\to 0 } \\:{\\frac{1}{t}}\\ : [ f(a+t\\ , { { 1\\!\\!\\!\\:1 } } ) -f(a ) ] \\:=\\ : \\lim_{t\\to 0 } \\:{\\frac{1}{t}}\\ : [ f(q^tdq+t\\ , { { 1\\!\\!\\!\\:1 } } ) -f(q^tdq)]\\\\      & = \\lim_{t\\to 0 } \\:{\\frac{1}{t}}\\ : [ f(q^t(d+t\\ , { { 1\\!\\!\\!\\:1 } } ) q)-q^tf(d)q]\\\\      & = \\lim_{t\\to 0 } \\:{\\frac{1}{t}}\\ : [ q^tf(d+t\\ , { { 1\\!\\!\\!\\:1 } } ) q - q^tf(d)q]\\\\      & = \\lim_{t\\to 0 } \\:{\\frac{1}{t}}\\ : [ q^tf({\\operatorname{diag}(\\lambda_1,\\dotsc,\\lambda_n)}+t\\ , { { 1\\!\\!\\!\\:1 } } ) q - q^tf({\\operatorname{diag}(\\lambda_1,\\dotsc,\\lambda_n)})q]\\\\      & = \\lim_{t\\to 0 } \\:{\\frac{1}{t}}\\ : q^t[f(\\operatorname{diag}(\\lambda_1+t,\\dotsc,\\lambda_n+t))-f({\\operatorname{diag}(\\lambda_1,\\dotsc,\\lambda_n)})]q\\\\      & = \\lim_{t\\to 0 } \\:{\\frac{1}{t}}\\ : q^t\\big[\\operatorname{diag}\\big(f(\\lambda_1+t)-f(\\lambda_1),\\dotsc , f(\\lambda_n+t)-f(\\lambda_n)\\big)\\big]q\\\\      & = q^t\\big[\\operatorname{diag}\\big(\\lim_{t\\to 0 } \\:{\\frac{1}{t}}\\ : f(\\lambda_1+t)-f(\\lambda_1),\\dotsc,\\lim_{t\\to 0 } \\:{\\frac{1}{t}}\\ : f(\\lambda_n+t)-f(\\lambda_n)\\big)\\big]q\\\\      & = q^t\\big[\\operatorname{diag}\\big(f'(\\lambda_1),\\dotsc , f'(\\lambda_n)\\big)\\big]q = q^tf(d)\\,q \\:=\\ : f(a)\\end{aligned}\\ ] ] and therefore @xmath112.h = { \\langle df[a].h,\\ , { { 1\\!\\!\\!\\:1 } } \\rangle } = { \\langle h,\\,df[a ] .",
    "{ { 1\\!\\!\\!\\:1 } } \\rangle } = { \\langle h,\\,f(a ) \\rangle } = { \\langle f(a),\\,h \\rangle}\\,.\\qedhere\\ ] ]    using the fact that @xmath40 is self - adjoint everywhere , we can also obtain the potential directly by using ( * ? ? ?",
    "* lemma 3.28 ) . for @xmath116 , @xmath122",
    ", @xmath71 we find @xmath123 with @xmath124",
    ".      in this section we no longer require the function @xmath39 to be analytic . while the results are almost identical to those of the previous subsection , the more general proofs require them to be stated in a different order .",
    "+ the first proposition shows that every continuously differentiable primary matrix function can be represented as the gradient field ( differentiated with respect to @xmath105 ) of an isotropic energy function satisfying the _ valanis - landel hypothesis_.    [ prop : primarypotential ] let @xmath62 . then for @xmath125 with @xmath110 , the function @xmath126 is a _ potential _ of @xmath68 , i.e. @xmath112 = f(a)\\ ] ] or , more precisely , @xmath112.h = { \\langle f(a),\\,h \\rangle}\\ ] ] for all @xmath127 .",
    "this is a corollary to theorem 1.1 in @xcite , where it is shown that any _ spectral function _ of the form @xmath128 with a symmetric function @xmath129 is differentiable with @xmath112 = q^t\\operatorname{diag}({\\nabla}g(\\lambda))q\\ ] ] for all @xmath130 with @xmath131 . by putting @xmath132",
    "we find @xmath133 , and since @xmath110 we obtain @xmath112 = q^t\\operatorname{diag}(f(\\lambda_1),\\dotsc , f(\\lambda_n))q = f(a)\\,.\\qedhere\\ ] ]    the next lemma is due to brown et al .",
    "* theorem 2.1 ) .",
    "the proof can be found there .",
    "[ lemma : primarydifferentiability ] let @xmath62 .",
    "then the primary matrix function @xmath68 is continuously differentiable on @xmath64 .    according to proposition [ prop : primarypotential ] every primary matrix function @xmath39 on @xmath64 corresponding to @xmath62",
    "has a potential .",
    "thus the derivative of @xmath39 on @xmath64 , which exists due to [ lemma : primarydifferentiability ] , is self adjoint according to schwarz theorem .",
    "[ prop : primaryselfadjoint ] let @xmath62 .",
    "then the primary matrix function @xmath68 is differentiable on @xmath64 and its derivative @xmath40 is self - adjoint with respect to the canonical inner product on @xmath23 : @xmath35.h,\\,{\\widetilde h}\\rangle } = { \\langle h,\\,df[a].{\\widetilde h}\\rangle } \\quad \\forall a\\in s_i,\\;h,{\\widetilde h}\\in{\\operatorname{sym}(n)}\\,.\\qedhere\\ ] ]    proposition [ prop : primarypotential ] can also be used to show how the monotonicity of @xmath39 on @xmath77 relates to the hilbert space monotonicity of the primary matrix function @xmath39 on @xmath134 .",
    "[ prop : specmontohilmon ] let @xmath62 .",
    "then the primary matrix function @xmath68 is hilbert space monotone if and only if @xmath39 is monotone on @xmath61 .",
    "choose an antiderivative @xmath125 of @xmath39 and define @xmath135 according to proposition [ prop : primarypotential ] , @xmath115 is a potential of @xmath39 on @xmath64 , i.e.  @xmath114=f(a)$ ] for all @xmath130 .",
    "but then @xmath136 is monotone on @xmath64 if and only if @xmath115 is convex on @xmath64 .",
    "according to an extension of the chandler davis theorem ( * ? ? ?",
    "* corollary 2 ) , this is the case if and only if the function @xmath137 is convex on @xmath138 , which in turn is the case if and only if @xmath139 is monotone on @xmath61 .",
    "it is possible to give a proof based on the eigenvalue formula given in ( * ? ? ?",
    "* theorem 2.1 )",
    ". this might be useful to distinguish monotonicity and strict monotonicity as well as positive definiteness and positive semi - definiteness of @xmath40 .    a very similar result is also given by norris in @xcite and ( * ? ? ?",
    "* lemma 4.1 ) , where it is shown that @xmath37 $ ] is self - adjoint and positive definite for all @xmath140 if the function @xmath141 has the following properties : @xmath142 norris calls these functions _ strain measures _ , based on a definition by hill given in @xcite and @xcite , although norris requires the derivative @xmath143 to be strictly positive , whereas hill admits functions which are simply monotone on @xmath144 as well .",
    "returning to the principal logarithm @xmath24 on @xmath11 and its inverse , the matrix exponential @xmath145 on @xmath23 , we find that proposition [ prop : specmontohilmon ] immediately shows that @xmath24 and @xmath145 are monotone .",
    "furthermore , both functions are diffeomorphisms , hence their derivatives @xmath146 $ ] and @xmath147 $ ] for @xmath148 are invertible as well . since the monotonicity implies that @xmath146 $ ] and @xmath147 $ ] are positive semi - definite , they are therefore positive definite , thus @xmath24 and @xmath145 are strictly monotone as well .",
    "+ for these two functions we can also compute some of the aforementioned properties directly : using a representation of @xmath149 given in ( * ? ? ?",
    "10.2 ) , we find @xmath150.h,\\,{\\widetilde h}\\rangle } & = { \\langle \\int_0 ^ 1 \\exp(sa)\\,h\\,\\exp((1-s)a)\\,{\\mathrm{ds}},\\,{\\widetilde h}\\rangle}\\\\      & = \\int_0 ^ 1 { \\langle \\exp(sa)\\,h\\,\\exp((1-s)a),\\,{\\widetilde h}\\rangle } \\,{\\mathrm{ds}}= \\int_0 ^ 1 { \\langle h,\\,\\exp(sa)^t\\,{\\widetilde h}\\,\\exp((1-s)a)^t \\rangle } \\,{\\mathrm{ds}}\\\\      & = \\int_0 ^ 1 { \\langle h,\\,\\exp(sa)\\,{\\widetilde h}\\,\\exp((1-s)a ) \\rangle } \\,{\\mathrm{ds}}\\:=\\ : { \\langle h,\\,d\\exp[a].{\\widetilde h}\\rangle}\\end{aligned}\\ ] ] for @xmath81 , showing that @xmath151 $ ] is self - adjoint , as well as @xmath152 showing that @xmath151 $ ] is positive semi - definite . + for the matrix logarithm and @xmath153 we use , again.h$ ] in a direction @xmath93 for commuting @xmath47 and @xmath93 as well as some properties of derivatives of primary matrix functions in arbitrary directions can be found in much earlier works by h. richter @xcite ; however , richter did not give the more general formula used here .",
    "] , a representation formula given in ( * ? ? ?",
    "11.2 ) to find @xmath154.h,\\,{\\widetilde h}\\rangle } & = { \\langle \\int_0 ^ 1 ( t(a- { { 1\\!\\!\\!\\:1 } } ) + { { 1\\!\\!\\!\\:1 } } ) { ^{-1}}\\,h\\,(t(a- { { 1\\!\\!\\!\\:1 } } ) + { { 1\\!\\!\\!\\:1 } } ) { ^{-1}}\\,{\\mathrm{ds}},\\,{\\widetilde h}\\rangle}\\\\      & = \\int_0 ^ 1 { \\langle ( t(a- { { 1\\!\\!\\!\\:1 } } ) + { { 1\\!\\!\\!\\:1 } } ) { ^{-1}}\\,h\\,(t(a- { { 1\\!\\!\\!\\:1 } } ) + { { 1\\!\\!\\!\\:1 } } ) { ^{-1}},\\,{\\widetilde h}\\rangle}\\,{\\mathrm{ds}}\\\\      & = \\int_0 ^ 1 { \\langle h,\\,(t(a- { { 1\\!\\!\\!\\:1 } } ) + { { 1\\!\\!\\!\\:1 } } ) ^{-t}\\,{\\widetilde h}\\,(t(a- { { 1\\!\\!\\!\\:1 } } ) + { { 1\\!\\!\\!\\:1 } } ) ^{-t } \\rangle } \\,{\\mathrm{ds}}\\\\      & = \\int_0 ^ 1 { \\langle h,\\,(t(a- { { 1\\!\\!\\!\\:1 } } ) + { { 1\\!\\!\\!\\:1 } } ) { ^{-1}}\\,{\\widetilde h}\\,(t(a- { { 1\\!\\!\\!\\:1 } } ) + { { 1\\!\\!\\!\\:1 } } ) { ^{-1}}\\rangle } \\,{\\mathrm{ds}}\\;=\\ ; { \\langle h,\\,d\\log[a].{\\widetilde h}\\rangle}\\,,\\end{aligned}\\ ] ] showing that @xmath155 $ ] is self - adjoint , as well as @xmath156 showing that @xmath155 $ ] is positive semi - definite",
    ". + note , however , that the matrix exponential is not monotone on @xmath157 or @xmath158 : for @xmath159 we compute @xmath160      & = { \\langle { \\begin{pmatrix } \\cos(\\alpha)&-\\sin(\\alpha)\\\\\\sin(\\alpha)&\\cos(\\alpha ) \\end{pmatrix } } - { \\begin{pmatrix } \\cos(-\\alpha)&-\\sin(-\\alpha)\\\\\\sin(-\\alpha)&\\cos(-\\alpha ) \\end{pmatrix}},\\ : { \\begin{pmatrix } 0&-2\\alpha\\\\2\\alpha&0 \\end{pmatrix } } \\rangle}\\\\[2 mm ]      & = { \\langle { \\begin{pmatrix } 0&-2\\sin(\\alpha)\\\\2\\sin(\\alpha)&0 \\end{pmatrix}},\\ : { \\begin{pmatrix } 0&-2\\alpha\\\\2\\alpha&0 \\end{pmatrix } } \\rangle } \\:=\\ : 8\\,\\alpha\\,\\sin(\\alpha)\\end{aligned}\\ ] ] and thus , for @xmath161 , @xmath162      we consider the _ hencky constitutive model _",
    ", induced by the isotropic hencky energy function @xmath7 ^ 2\\,.\\ ] ] in this constitutive model , the _ kirchhoff stress _",
    "@xmath163 corresponding to the stretch @xmath164 is given by @xmath165 if @xmath166 , this relation reduces to @xmath167 , thus the mapping @xmath168 is strictly monotone on @xmath11 in this special case ( also called the _",
    "lateral contraction free _ case ) .",
    "however , this monotonicity does not hold for arbitrary choices of @xmath169 .",
    "moreover , the mapping @xmath170 of the _ true strain tensor _",
    "@xmath2 to the kirchhoff stress @xmath163 is monotone ( a property also called _",
    "hill s inequality _",
    "@xcite ) , while the _ cauchy stress response _ @xmath171 as well as the mapping @xmath172 are not monotone , thus the hencky model does not satisfy the true - stress - true - strain monotonicity condition .",
    "we may distinguish three types of ( strict ) monotonicity :    * the _ _ hilbert space monotonicity__@xmath173 * the _ _ operator monotonicity__@xmath174 * the _ spectral monotonicity _ ( or monotonicity of @xmath39 on @xmath56)@xmath175    furthermore we consider the following condition on @xmath39 : @xmath176    _ only _ the following implications hold : @xmath177     + @xmath178 : for given @xmath179 choose @xmath180 . since @xmath181 is positive definite , implies that @xmath182 is positive definite as well .",
    "@xmath184 : let @xmath39 satisfy condition . then , with @xmath185 we find @xmath186 for @xmath187 with @xmath188 , choose @xmath189 . then @xmath190 and thus @xmath191 .",
    "+ @xmath192 : proposition [ prop : specmontohilmon ] + @xmath193 : this implication is trivial ; simply choose @xmath180 .",
    "+ to see that the operator monotonicity is not implied by the other conditions , consider the function @xmath194 .",
    "while this function is monotone in the sense of and , it is not operator monotone ( * ? ? ?",
    "* example v.1.2 ) .",
    "if @xmath39 is not a primary matrix function given through a scalar function on the spectrum ( and is therefore not well defined ) , then the only generally true implications are @xmath195 to see that operator monotonicity does not imply hilbert space monotonicity in this general case , consider the function @xmath196 for @xmath197 we find @xmath198.h \\:=\\ : { { \\frac{\\mathrm{d}}{\\mathrm{dt}}}}{\\left.{\\det(c+th)}\\right|_{t=0 } } \\cdot { { 1\\!\\!\\!\\:1 } } \\:=\\ : \\underbrace{\\det(c)}_{>0\\vphantom{{\\operatorname{psym}(n)}}}\\operatorname{tr}(\\,\\underbrace{h\\vphantom{()}}_{\\mathclap{\\in{\\operatorname{psym}(n)}}}\\overbrace{c{^{-1}}}^{\\mathclap{\\in{\\operatorname{psym}(n ) } } } ) \\cdot { { 1\\!\\!\\!\\:1 } } \\,.\\ ] ] since @xmath199 for @xmath200 ( c.f .",
    "@xcite ) we find @xmath201 .",
    "thus @xmath202 is a positive definite matrix : @xmath198.h = \\det(c)\\operatorname{tr}(hc{^{-1 } } ) \\ , { { 1\\!\\!\\!\\:1 } } \\in { \\operatorname{psym}(n)}\\,,\\ ] ] hence @xmath203 is operator monotone .",
    "however @xmath203 is not hilbert space monotone : in the case @xmath204 , with @xmath205 and @xmath206 we find @xmath207 for arbitrary dimensions",
    "@xmath208 the same follows for @xmath209 note also that @xmath210.h$ ] is generally not self - adjoint : for @xmath91 we find @xmath211.h,\\,{\\widetilde h}\\rangle } = { \\langle { \\langle \\operatorname{cof}c,\\ , h \\rangle}\\cdot { { 1\\!\\!\\!\\:1 } } , \\ : { \\widetilde h}\\rangle } =   { \\langle \\operatorname{cof}c,\\ , h \\rangle }",
    "{ \\langle \\operatorname{cof}c,\\ , { \\widetilde h}\\rangle } \\operatorname{tr}h\\,.\\ ] ] thus does not admit a potential .",
    "returning to our original motivation , namely the true - stress - true - strain inequality , we consider the equation @xmath212 based on the chain rule . to see how the positive definiteness of two of these terms imply the positive definiteness of the third we need the following lemma .",
    "[ lemma : selfadjointproductpositivedefinite ] let @xmath164 be a finite - dimensional hilbert space and let @xmath213 with    * @xmath47 and @xmath214 are self - adjoint and positive definite , * @xmath215 is self - adjoint .",
    "then @xmath215 is positive definite .",
    "since @xmath47 , @xmath214 and @xmath215 are self - adjoint , we find @xmath216 hence @xmath47 and @xmath214 commute .",
    "therefore @xmath47 and @xmath214 are simultaneously diagonalizable : we can choose an orthonormal basis such that the corresponding matrices @xmath217 and @xmath218 representing @xmath47 and @xmath214 are diagonal . since @xmath47 and @xmath214 are positive definite , all diagonal entries of @xmath217 and @xmath218 are positive .",
    "the matrix @xmath219 representing @xmath220 in the same basis is given by @xmath221 and is therefore a diagonal matrix with positive diagonal entries as well , thus @xmath220 is positive definite .",
    "+ note that in the case of @xmath222 we can simply choose @xmath71 such that @xmath223 , @xmath224 with diagonal matrices @xmath225 .",
    "then @xmath226 , and since the diagonal entries of @xmath227 and @xmath228 are positive , so are the diagonal entries of @xmath229 .",
    "we consider the derivatives @xmath230 on @xmath23 and make the following assumptions on the functions @xmath1 and @xmath231 : @xmath232 furthermore , we know from the previous sections that @xmath233 $ ] and its inverse @xmath234 @xmath235)$ ] are self - adjoint and positive definite . + then according to lemma [ lemma : selfadjointproductpositivedefinite ] the following holds : @xmath236 this follows directly from @xmath237 . note that and hold if @xmath238 for a primary matrix function @xmath231 induced by a monotone function @xmath141 with @xmath96 for all @xmath239 .",
    "to apply lemma [ lemma : selfadjointproductpositivedefinite ] , all of the involved matrices @xmath47 , @xmath214 and @xmath220 must be self - adjoint . while the term `` positive definite '' usually implies the symmetry by definition , we will now consider matrices @xmath47 which are `` positive definite '' in the sense that @xmath240 for all @xmath241 , which is the case if and only if the symmetric part @xmath242 of @xmath47 is positive definite .",
    "+ we will show that the lemma does not generally hold if only one of the considered matrices is symmetric .",
    "let @xmath243 for @xmath244 $ ] .",
    "then @xmath245 and we find :    * @xmath246\\to{{\\mathbb{r}^{n\\times n}}}$ ] is continuous , * @xmath47 is invertible , symmetric and positive definite , * @xmath247 is invertible and `` positive definite '' ( i.e.  @xmath248 is positive definite ) for all @xmath244 $ ] and * @xmath249 is invertible for all @xmath244 $ ] .    however , while @xmath250 is obviously positive definite , the matrix @xmath251 is not since @xmath252 which implies that @xmath253 is not positive definite .",
    "+ this shows not only that the lemma does not hold for non - symmetric matrices , but also that a `` positive definite '' ( non - symmetric ) matrix can be continuously deformed into a non - positive matrix without losing invertibility along the way .    in ( *",
    "* eq . ( 50 ) ) , jog and patil argue that a tensor valued function @xmath47 `` loses positive definiteness '' if and only if @xmath214 `` loses positive definiteness '' , which is deduced from the fact that @xmath254 for some symmetric positive definite @xmath255 . since @xmath47 and @xmath214 are not symmetric in general , the authors define `` losing positive definiteness '' as the loss of invertibility . while for this definition the stated equivalence is correct , it should be carefully noted that since invertibility of a gradient is not a sufficient condition for monotonicity , this result can not be applied to show the monotonicity of a function with gradient @xmath47 . in particular ,",
    "if @xmath256 is not symmetric we can not simply combine lemma [ lemma : posdefsinglepoint ] and lemma [ lemma : selfadjointproductpositivedefinite ] to conclude that @xmath257 is positive definite .",
    "we thank prof . chandrashekhar s. jog ( indian institute of science ) for interesting discussions on the topic of constitutive inequalities as well as prof .",
    "karl - hermann neeb ( university of erlangen ) and prof .",
    "nicholas higham ( university of manchester ) for their helpful remarks .",
    "consider the first order approximation @xmath258.h + \\underbrace{\\dots}_{\\mathclap{\\substack{\\text{higher order}\\\\\\text{terms}}}}\\ ] ] of the determinant function at a _ diagonal _ matrix @xmath259 .",
    "first we assume that @xmath260 is an _ off - diagonal _ matrix of the form @xmath261 with @xmath262 .",
    "we compute @xmath263 since @xmath47 is diagonal by assumption , the column vector @xmath264 has the form @xmath265 and @xmath266 has the form @xmath267 . therefore the vectors @xmath264 and @xmath268 as well as @xmath266 and @xmath269 are linearly dependent .",
    "thus reduces to @xmath270 the term @xmath271 is quadratic in @xmath272 , thus the linear approximation is simply@xmath273.h = 0\\,.\\ ] ] for a matrix @xmath260 of the form . through similar computations ,",
    "it is easy to show that @xmath274.h = 0 $ ] for any off - diagonal @xmath260 .    to find the derivative @xmath274 . { { 1\\!\\!\\!\\:1 } } $ ] we compute @xmath275\\,,\\end{aligned}\\ ] ] thus @xmath273 .",
    "{ { 1\\!\\!\\!\\:1 } } = { { \\frac{\\mathrm{d}}{\\mathrm{dh}}}}\\det(a+h { { 1\\!\\!\\!\\:1 } } ) = \\sum_{i=1}^n \\prod_{\\substack{j=1\\\\j\\neq i}}^n a_j\\ , .",
    "\\label{eq : detderivativeinunitydirection}\\ ] ] for @xmath276 we obtain @xmath273 . { { 1\\!\\!\\!\\:1 } } = a_1 a_2 + a_2 a_3 + a_1 a_3\\,.\\ ] ] furthermore , if we assume that @xmath54 is a simple eigenvalue of @xmath47 ( which is the case for matrices of the form @xmath277 where @xmath278 is a diagonal matrix with simple eigenvalues ; such matrices appeared in equation ) , then can be written as @xmath273 .",
    "{ { 1\\!\\!\\!\\:1 } } = \\prod_{\\substack{j=1\\\\j\\neq k}}^n a_j \\neq 0\\,,\\ ] ] where @xmath54 is the @xmath279-th eigenvalue of @xmath47 .",
    "we directly compute : @xmath283,\\,h \\rangle } + \\dots\\\\      \\rightarrow \\smash{\\overbrace{w(q^txq)}^{=w(x ) } } + { \\langle dw[q^txq],\\,q^thq \\rangle } + \\dots & = w(x ) + { \\langle dw[x],\\,h \\rangle } + \\dots\\\\      \\rightarrow { \\langle dw[q^txq],\\,q^thq \\rangle } & = { \\langle dw[x],\\,h \\rangle}\\\\      \\rightarrow { \\langle qdw[q^txq]q^t,\\,h \\rangle } & = { \\langle dw[x],\\,h \\rangle}\\,.\\end{aligned}\\ ] ] since this holds for all @xmath260 , we obtain @xmath284q^t = dw[x]\\ ] ] and thus @xmath282 = q^tdw[x]q\\ , . \\qedhere\\ ] ]      we could also try to prove proposition [ prop : primarypotential ] for the more general case of non - analytic functions by directly computing the derivative of the function @xmath285 unfortunately , while the derivative of @xmath115 at a point @xmath87 in directions @xmath93 can be explicitly computed if @xmath47 and @xmath93 commute , it is difficult to do so for arbitrary choices of @xmath260 .",
    "+ one possible approach is to assume that the function @xmath286 mapping a matrix @xmath287 to its ( ordered ) eigenvalues @xmath288 is differentiable in a neighbourhood of @xmath87 .",
    "for example , this is the case if all eigenvalues of @xmath47 are simple @xcite .",
    "the basic idea is to write @xmath289 with @xmath290 . then @xmath112 = d\\psi[\\lambda(a ) ] \\cdot d\\lambda[a]\\ , .",
    "\\label{eq : potentialeigenvaluederivative}\\ ] ] it is therefore useful to compute the derivative @xmath291 $ ] of the eigenvalue function . since lemma [ lemma : isotropicfunctionderivative ] implies @xmath292 = q^t\\,d\\lambda[a]\\,q\\,,\\ ] ] the derivative of @xmath293 at @xmath47",
    "is determined by the derivative at the diagonal matrix corresponding to @xmath47 .",
    "we will therefore assume w.l.o.g .",
    "that @xmath47 is already a diagonal matrix .",
    "+ the eigenvalues @xmath118 of @xmath47 are characterized by @xmath294 let @xmath260 .",
    "we compute the first order approximation of : @xmath295\\cdot { { 1\\!\\!\\!\\:1 } } + \\dots ] ) = 0{\\nonumber \\\\}&\\rightarrow\\ : \\det([a-\\lambda_i(a ) { { 1\\!\\!\\!\\:1 } } ] + h-[d\\lambda_i(a).h]\\cdot { { 1\\!\\!\\!\\:1 } } + \\dots ) = 0{\\nonumber \\\\}&\\rightarrow\\ : \\underbrace{\\det[a-\\lambda_i(a ) { { 1\\!\\!\\!\\:1 } } ] } _ { = 0 } + { \\langle \\operatorname{cof}[a-\\lambda_i(a ) { { 1\\!\\!\\!\\:1 } } ] \\,,\\ : h-[d\\lambda_i(a).h]\\cdot { { 1\\!\\!\\!\\:1 } } + \\dots \\rangle } + \\dots = 0 \\label{eq : diagonalminussimpleeigenvalueexample}\\end{aligned}\\ ] ] by ignoring higher order terms we obtain @xmath296\\,,\\ : h-[d\\lambda_i(a).h]\\cdot { { 1\\!\\!\\!\\:1 } } \\rangle } = 0 \\label{eq : ignoredhigherorderterms}\\ ] ] recall that @xmath47 is diagonal by assumption .",
    "since @xmath47 commutes with diagonal matrices @xmath93 ( and thus the derivative @xmath114.h$ ] could be computed by more direct means ) , we are only interested in cases where the symmetric matrix @xmath93 is _ off - diagonal _ , i.e.  @xmath297 for @xmath298 .",
    "but then @xmath299}_{\\text{diagonal}},\\ : \\underbrace{h\\vphantom{[]}}_{\\mathclap{\\text{off - diagonal}}}\\ , \\rangle } = 0\\,,\\ ] ] thus reduces to @xmath296\\,,\\ : [ d\\lambda_i(a).h]\\cdot { { 1\\!\\!\\!\\:1 } } \\rangle } = 0\\,,\\ ] ] which we can also write as @xmath300\\right ) = 0\\,.\\ ] ] to conclude that @xmath301 it remains to show that @xmath302\\right ) \\neq 0 $ ] . assuming that the diagonal entries of @xmath47 are ordered we write @xmath303 and find @xmath304 and thus @xmath305 = { \\begin{pmatrix } \\prod\\limits_{k\\neq1}(\\lambda_k-\\lambda_i ) & & 0\\\\ & \\ddots & \\\\ 0 & & \\prod\\limits_{k\\neq n}(\\lambda_k-\\lambda_i ) \\end{pmatrix}}\\,.\\ ] ] we compute the trace : @xmath306\\right ) & = \\sum_{j=1}^n \\ , \\prod\\limits_{\\substack{k=1\\\\k\\neq j}}^n ( \\lambda_j-\\lambda_i ) = \\prod\\limits_{\\substack{k=1\\\\k\\neq i}}^n ( \\lambda_j-\\lambda_i)\\,,\\end{aligned}\\ ] ] where the second equality holds due to the fact that the product is zero if it contains the factor @xmath307 .",
    "hence this term is nonzero if and only if all eigenvalues of @xmath47 are simple , in which case we can conclude that @xmath308 for all off - diagonal @xmath260 .",
    "let @xmath309 , @xmath310 with @xmath110 and let @xmath87 such that all eigenvalues of @xmath47 are simple .",
    "then the function @xmath311 is differentiable at @xmath47 with @xmath112 = f(a ) = q^t \\operatorname{diag}(f(\\lambda_1),\\dotsc , f(\\lambda_n ) ) \\,q\\,,\\ ] ] where @xmath312 is the spectral decomposition of @xmath47 .    according to lemma [ lemma : isotropicfunctionderivative ] , @xmath313 = q^tdw[x]q$ ] ,",
    "hence we find @xmath112 = q^t dw[\\operatorname{diag}(\\lambda_1,\\dotsc,\\lambda_n ) ] \\,q\\,.\\ ] ] therefore it remains to show that @xmath314.h = { \\langle \\operatorname{diag}(f(\\lambda_1),\\dotsc , f(\\lambda_n)),\\ : h \\rangle } \\label{eq : simplecorollaryequationtoshow}\\ ] ] for all @xmath260 and pairwise different @xmath315 .",
    "we first consider the case of diagonal matrices @xmath316 .",
    "writing @xmath317 we find @xmath318 thus @xmath319.{h^{\\mathrm{diag}}}&= \\lim_{t\\to 0 } \\:{\\frac{1}{t } } ( w({a^{\\mathrm{diag}}}+t{h^{\\mathrm{diag}}})-w({a^{\\mathrm{diag}}}))\\\\      & = \\lim_{t\\to 0 } \\:{\\frac{1}{t } } \\sum_{i=1}^n f(\\lambda_i+th_i ) - f(\\lambda_i ) = \\sum_{i=1}^n f'(\\lambda_i)\\,h_i{\\nonumber \\\\}&= { \\langle \\operatorname{diag}(f(\\lambda_1),\\dotsc , f(\\lambda_n)),\\ : \\operatorname{diag}(h_1,\\dotsc , h_n ) \\rangle } = { \\langle f({a^{\\mathrm{diag}}}),\\:{h^{\\mathrm{diag}}}\\rangle}\\,.\\nonumber\\end{aligned}\\ ] ] now let @xmath320 be a symmetric off - diagonal matrix , i.e.  @xmath321 for @xmath298 . using equation : @xmath112 = d\\psi[\\lambda(a ) ] \\cdot d\\lambda[a]\\,,\\ ] ] as well as the result of the previous considerations for diagonal @xmath47 and off - diagonal @xmath322 : @xmath323.{h^{\\mathrm{off}}}= 0\\,,\\ ] ] we conclude @xmath314.{h^{\\mathrm{off}}}= 0\\,.\\ ] ] finally , for arbitrary @xmath260 , we can write @xmath324 with a diagonal matrix @xmath325 and a symmetric off - diagonal matrix @xmath322 . then @xmath319.h & = dw[{a^{\\mathrm{diag}}}].{h^{\\mathrm{diag}}}+ dw[{a^{\\mathrm{diag}}}].{h^{\\mathrm{off}}}= dw[{a^{\\mathrm{diag}}}].{h^{\\mathrm{diag}}}{\\nonumber \\\\}&= { \\langle f({a^{\\mathrm{diag}}}),\\:{h^{\\mathrm{diag}}}\\rangle } = { \\langle f({a^{\\mathrm{diag}}}),\\:h \\rangle}\\,,\\end{aligned}\\ ] ] showing and concluding the proof ."
  ],
  "abstract_text": [
    "<S> this note contains some observations on primary matrix functions and different notions of monotonicity with relevance towards constitutive relations in nonlinear elasticity . </S>",
    "<S> focussing on primary matrix functions on the set of symmetric matrices , we discuss and compare different criteria for monotonicity . </S>",
    "<S> the demonstrated results are particularly applicable to computations involving the _ true - stress - true - strain _ monotonicity condition , a constitutive inequality recently introduced in an arch .  </S>",
    "<S> appl .  </S>",
    "<S> mech .  </S>",
    "<S> article by c.s . jog and k.d . </S>",
    "<S> patil . </S>",
    "<S> we also clarify a statement by jog and patil from the same article which could be misinterpreted . </S>"
  ]
}