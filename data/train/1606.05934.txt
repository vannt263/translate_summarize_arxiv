{
  "article_text": [
    "extreme learning machine ( elm for short ) was originally developed based on single hidden - layer feed forward neural networks ( slfns ) @xcite . compared with the conventional learning machines ,",
    "it is of extremely fast learning capacity and good generalization capability .",
    "thus , elm , with its variants @xcite , has been widely applied in many fields @xcite .",
    "the results indicate that elm produces comparable or better classification accuracies with reduced training time and implementation complexity compared to artificial neural networks methods and support vector machine methods .",
    "unfortunately , as a black - box method , elm fails to measure up to the task of time series data classification by itself .",
    "a possible solution to this issue is to improve the interpretability of elm by feature selection .",
    "if a set of selected features improve the classification accuracy much more than original feature sets , it is reasonable to interpret the results by them .",
    "nevertheless , selection the most representative and interpretative feature can improve the interpretability of elm and make elm more adapt to time series classification . in the context of feature selection in time series data analysis , most of the current methods adopt such a framework that ranks subsequences according to their individual discriminative power to the target class and then selects top - k ranked subsequences@xcite .",
    "these methods have some common drawbacks : ( 1 ) the selected features are not the most representative and interpretative , ( 2 ) many redundant features are selected , and ( 3 ) the number of selected feature is arbitrarily specified by a parameter k .    in this paper , a novel method is proposed to improve elm representative and interpretative ability by extraction diversified top - k shapelets features.shapelets was introduced as a primitive for time series data mining @xcite and was utilized in classifying time series data @xcite .",
    "the original shapelet based classifier embeds the shapelet discovery algorithm in a decision tree , and uses information gain to assess the quality of candidates .",
    "shapelets transformed classification methods @xcite were proposed to separate the processing of shapelets selection and classification .",
    "the k shapelets are selected in an offline manner , and which can not only improve the affectivity and efficiency of classification , but also introduce a common feature attraction method which can be used in all typical time series classification algorithms .",
    "nevertheless , shapelets based classification methods have been widely discussed and used in many real applications @xcite .",
    "the most challenge is that there are large quantities of redundant shapelets in candidates which decreasing the accuracy of classification and the parameter k is hard to determine .",
    "some works @xcite detect this problem and use clustering or pruning methods to remove the redundant , but still exist redundant shapelets , also , the k value is determined from experiments .",
    "this paper make the following contributions : first , in order to get rid of the similar and redundant shapelets in candidate set , two conceptions including similar shapelets and diversified top - k shapelets are presented .",
    "based on these conceptions , a method of construction diversify shapelets graph is proposed .",
    "second , a diversified top - k shapelets query method is presented to find top - k representative shapeletes of each class .",
    "third , we propose an diversified top - k shapelets transformed elm algorithm which can automatically determine the parameter k and transform data using the determined k shapelets .",
    "the experimental results show that the proposed approach significantly improves the interpretability and performance of elm .",
    "extreme learning machine ( elm ) is a generalized single hidden - layer feedforward network . in elm ,",
    "the hidden layer node parameters are mathematically calculated instead of being iteratively tuned ; thus , it provides good generalization performance at thousands of times faster speed than traditional popular learning algorithms for feedforward neural networks .",
    "suppose there are @xmath0 arbitrary distinct training instances @xmath1 , where @xmath2^t } \\in { { \\rm{r}}^n}$ ] , and @xmath3^t } \\in { { \\rm{r}}^m}$],standard slfns with @xmath4 hidden nodes and activation function @xmath5 are mathematically modeled as @xmath6 where @xmath7^t}$ ] is the weight vector connecting the @xmath8th hidden node and the input nodes , @xmath9^t}$ ] is the weight vector connecting the @xmath8th hidden nodes and the output nodes , and @xmath10 is the threshold of the @xmath8th hidden node.if a slfn with @xmath11 hidden nodes with activation function @xmath5 can approximate these @xmath0 samples with zero error , it then implies that there exist @xmath12 , @xmath13 and @xmath14 , such that : @xmath15 the above @xmath16 equations can be written compactly as @xmath17 where @xmath18 @xmath19_{\\tilde n \\times m } $ and $ \\operatorname{t }   = { \\left [ \\begin{gathered } t_1^t \\hfill \\\\",
    "\\ldots \\hfill \\\\",
    "\\ldots \\hfill \\\\",
    "t_n^t \\hfill \\\\ \\end{gathered }   \\right]_{n \\times m } } $ } \\ ] ]    h is named as hidden layer output matrix of the network , where with respect to inputs @xmath20 and its @xmath21th row represents the output vector of the hidden layer with respect to input @xmath22 .",
    "elm differs from other training algorithms in that the hidden node parameters @xmath13 and @xmath14 are not tuned during training , but are instead assigned with random values according to any continuous samplings distribution .",
    "eq . then becomes a linear system and the output weight @xmath23 are estimates as eq . .",
    "@xmath24 where @xmath25 is the moore - penrose generalized inverse of the hidden layer output matrix h.",
    "in this section , we discuss three parts of our work ( 1 ) construction the diversity graph of shapelets candidates , ( 2 ) querying diversified top - k shapelets , and ( 3 ) transforming the data based on diversified top - k shapelets and applying in elm .",
    "the following contents will discuss above three contribution separately .    before removing the redundant shapelets",
    ", we firstly need to get the shapelets candidates set .",
    "the original shapelets extraction algoritm is time consuming and complexity is @xmath26 , @xmath27 is the number of time series in the data set , @xmath28 is the length of each time series . in order to improve the efficiency of shapelets based classification method , we follow the method proposed in @xcite , which transformed the data sets through sax method and decreased the time complexity to @xmath29 .",
    "considerable works have focused on the diversified top - k query , but they almost applying on a typical circumstance . in our work , we use the diversity graph@xcite to find a general method to extract diversified top - k shapelets .",
    "given @xmath30 is a shapelets candidate sets , @xmath31 , and @xmath27 is the number of @xmath30 .",
    "the question is how to measure the similarity of two shapelets and how to define the diversified top - k shapelets .",
    "so we first give the two definitions .    *",
    "definition 1 : similar shapelets .",
    "* given two shapelets @xmath32 and @xmath33 which represent the same class , @xmath34 and @xmath35 is the number of shapelets candidates .",
    "the optimal split point of @xmath32 and @xmath33 are @xmath36 and @xmath37 , the split threshold are @xmath38 and @xmath39 .",
    "we say @xmath32 and @xmath33 are similar shapelets when they satisfy @xmath40 .",
    "we denote the similar shapelets as @xmath41 .",
    "* definition 2 : diversified top - k shapelets . * given a shapelets candidates set @xmath31 , and an integer k where @xmath42 .",
    "the diversified top - k shapelets query results of @xmath43 , denoted as divtopk(@xmath43 ) , is a list of results that satisfy the following three conditions .",
    "\\1 ) divtopk(@xmath43 ) @xmath44 i , @xmath45divtopk(@xmath43)@xmath46    \\2 ) for any two results @xmath47 and @xmath48 and @xmath49 , if @xmath41 , then @xmath50divtopk(@xmath43 ) .",
    "\\3 ) @xmath51 is maximized .",
    "* input : * shapelets candidates allshapelets + * output : * diverisity shapelets graph    graph = @xmath52 sort(allshapelets ) graph.add(allshapelets[i ] ) graph[j].add(graph[k ] ) graph[k].add(graph[j ] ) * return * graph    we give a diversity shapelet graph example of chlorineconcentration dataset as in fig .",
    "1.there are ten shapelets candidates as shown in fig1-a and the diversity graph of these ten candidates from algorithm 1 are shown in fig1-b.the black , red and green subsequence are the top-3 shapelets and can get the best classification accuracy.next section",
    "we will explain how to get the diversified top - k shapelets on the diversify graph .",
    "example of diversity shapelets graph , title=\"fig : \" ] +",
    "( a ) shapelets candidates    example of diversity shapelets graph , title=\"fig : \" ] + ( b ) diversity graph      traditional top - k query only returns the objects with largest k score , however , diversified top - k query concerns not only the score value but also the similarity of each object and remove all the redundant objects from results . according to @xcite ,",
    "find top - k results falling into two categories : incremental manner and bounding manner .",
    "we noticed that the bounding manner first satisfied the k value , but in each step it may not add the largest score vertex . in our problem , we must maintain the largest information gain shapelets in order to have the best classification accuracy .",
    "so we calculate the diversified top - k shapelets via an incremental manner .",
    "the detailed search procedure is shown as in algorithm2 .",
    "* input : * diverisity graph , k value + * output : * k number of shapelets    kshapelets =  , n = @xmath45v(graph)@xmath45 kshapelets.add(v1 ) while(@xmath45 kshapelets @xmath45@xmath53 k ) kshapelets.add(vi ) * return * kshapelets      after getting diversified top - k shapelets , we can use these shapelets to transform data before elm classification . for each instance of data @xmath54 , the subsequence distance is computed between @xmath54 and @xmath55 , @xmath55 is a shapelet in top - k shapelets.the resulting @xmath56 distances are used to form a new instance of transformed data , where each attribute corresponds to the distance from each shapelet to the original time series .    in order to get the best classification accuracy and also to get rid of the independence on the parameter k , we set k in an interval of [ 1 , @xmath57 ] where @xmath57 is an empirical optimal value , according to our experiments(see section 4.1 ) , which is set to 9,then we use the elm to learning training data and evaluate each diversified top - k shapelets candidate .",
    "the k value with the largest prediction accuracy is selected .",
    "when using data split into training data and testing data , the shapelets extraction and k determination is carried out only on the training data to avoid bias .",
    "the optimal diversified top - k shapelets are then used to transform each instance of the testing data.the details are as in following algorithm 3 .",
    "* input : * @xmath57 value + * output : * elm classification results    kshapelets = divtopkshapelet(graph , k ) output =  transformed =  dist = subsequencedist(ts , s ) transformed.add(dist ) output.add(transformed ) using elm evaluate output kshapelets = the output with highest accuracy on elm using kshapelets transform testing data * return * elm classification results",
    "to evaluate our proposed methods , we selected 15 data sets from the ucr time series repository ( listed in table 1).we use a simple train / test split and all reported results are testing accuracy.all shapelets candidates seletction , top - k diversified shapelets extraction and classifier construction is done on the training set .",
    "all experiments are implemented in java within the weka framework .",
    "there are two parameters _ min _ and _ max _ in the procedure of shapelets candidates generation .",
    "the two parameters determine the length of shapelets candidates which can influence finding the best representative shapelets .",
    "followed @xcite , we set min - length and max - length of subsequences to generate shapeless are m/11 and m/2 separately , m is the length of each time series .",
    "first , in order to explain how the k value influences accuracy of classification , we test the average accuracy of six classifier on fifteen data sets with the varying k value . as shown in fig .",
    "2 , with the increasing of k value , average classification accuracy first increases and then becomes stable when k is 9 .",
    "accordingly , we set the @xmath57 value as 9 and use this value in the following experiments .",
    "accuracy varying with k , title=\"fig : \" ] +      in this section , we want to get a visual overlook at what was the optimal shapelets indeed . because reference@xcite has verified that shapeletselection can remove more redundant shapelets than other similar methods , we only compared the optimal shapelets sets between * divtopkshapelets * and shapeletselection when the two algorithms all have the best classification , as shown in fig .",
    "the optimal shapelets sets were acquired from shapeletselection when k=8 ( fig.3-a ) , and from divtopkshapelet when k=2(fig.3-b ) .",
    "optimal shapelets sets , title=\"fig : \" ] + ( a ) optimal 8 shapelets of shapeletselection with the best accuracy    optimal shapelets sets , title=\"fig : \" ] + ( b ) optimal 2 shapelets of divtopkshapelet with the best accuracy      in this section , we select six traditional time series classification algorithms including c4.5 , 1nn , naive bays(nab ) , bayesiannetwork(ban ) , randomforest(raf ) and rotationforest(rof ) to compare the accuracy with our proposed methods .",
    "firstly , we directly use selected classification algorithms to classify the datasets .secondly , we use * divtopkshapelets*(set k=9)to extract optimal shapeletes sets and transform data , then classify transformed data sets with six selected classification algorithms.results are presented in table 1 , the column captions with classifier name plus s(see c4.5(s ) ) means * divtopkshapelets * transformed classification results . from the table",
    "we can see that compared to traditional classification algorithms , * divtopkshapelets * transformed classification methods can improve accuracy of 9 out of 15 datasets .",
    "for all six classification algorithms , average accuracy are improved . especially for naivebays , * divtopkshapelets * improves 13 data sets accuracy .",
    "\\(a ) +    .[tab:1]accuracy comparison with traditional classification algorithm(the method / s with the highest accuracy in each database are shown in bold ) [ cols= \" < , > , > , > , > , > , > \" , ]",
    "in this paper , we proposed a novel method to adapt elm to time series classification by extraction diversified top - k shapelets .",
    "our work includes three parts : ( 1 ) we introduce two conceptions of similar shapelets and diversified top - k shapelets , based on these conceptions , a method of construction diversity shapelets graph is presented , ( 2 ) we propose a diversified top - k shapelets extraction method , named as * divtopkshaplete * , to find out all of the most representative and interpretative features of each class , and ( 3 ) we put forward a shapelets transformed elm algorithm , named as * divshapelm * , which automatically determine k value and get the diversified top - k shapelets to improve performance of elm .",
    "the experiments results show that * divshapelm * can improve the efficiency and interpretative of elm .",
    "also , we experimentally verify that * divtopkshaplete * is an excellent feature extraction method which can improve the accuracy of traditional time series classification algorithms .",
    "yuhai zhao , guoren wang , ying yin , yuan li and zhanghui wang , improving elm - based microarray data classification by diversified sequence features selection . _",
    "neural comput&applic _ , 27(1 ) : 155 - 166,2016 ."
  ],
  "abstract_text": [
    "<S> elm ( extreme learning machine ) is a single hidden layer feed - forward network , where the weights between input and hidden layer are initialized randomly . </S>",
    "<S> elm is efficient due to its utilization of the analytical approach to compute weights between hidden and output layer . </S>",
    "<S> however , elm still fails to output the semantic classification outcome . to address such limitation , </S>",
    "<S> in this paper , we propose a diversified top - k shapelets transform framework , where the shapelets are the subsequences i.e. , the best representative and interpretative features of each class . as we identified , the most challenge problems are how to extract the best k shapelets in original candidate sets and how to automatically determine the k value . </S>",
    "<S> specifically , we first define the similar shapelets and diversified top - k shapelets to construct diversity shapelets graph . </S>",
    "<S> then , a novel diversity graph based top - k shapelets extraction algorithm named as * divtopkshapelets *  is proposed to search top - k diversified shapelets . </S>",
    "<S> finally , we propose a shapelets transformed elm algorithm named as * divshapelm * to automatically determine the k value , which is further utilized for time series classification . the experimental results over public data sets demonstrate that the proposed approach significantly outperforms traditional elm algorithm in terms of effectiveness and efficiency .    </S>",
    "<S> * keywords : * extreme learning machine , shapelets transformed classification , diversified query , feature extraction </S>"
  ]
}