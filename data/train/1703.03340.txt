{
  "article_text": [
    "compressed sensing ( cs ) @xcite states that most of the signals of scientific interest can be approximated very accurately using a smaller number of measurements , compared to the dimension of the signal . for that , the signal needs to be sparse or have a sparse representation in terms of proper sparsifying bases .",
    "this observation has a huge impact in signal processing , machine learning , and statistics .",
    "mathematically speaking , the goal of the cs problem is to recover the signal @xmath0^t$ ] with length @xmath1 from its undersampled random projections , also referred to as measurements . @xmath2",
    "random projections are generated using a measurement matrix @xmath3 from the linear measurement process , @xmath4 where @xmath5^t$ ] represents the measurement vector and @xmath6 denotes the corrupting noise .",
    "signal @xmath7 is said to be @xmath8-sparse if it has at most @xmath8 non - zero entries in a proper basis .",
    "the sparsity of @xmath7 can be exploited to find a unique solution of the underdetermined system equation with high probability from @xmath9 measurements @xcite .    in this paper",
    ", we consider the problem of reconstructing a correlated time series of such compressible vectors from their noisy undersampled measurement .",
    "particularly , we are interested in approximating the time series @xmath10 from the measurement time series @xmath11 . in many real - world applications ,",
    "the signal of interest has a substantial correlation in time .",
    "the main idea is to incorporate the knowledge from the previous estimates of the signal to achieve a more accurate estimation of the signal at the current time step .",
    "moreover , in many applications , different parts of the signal have different recovery requirements .",
    "thus , different coefficients of the signal have different _ importance levels_. for instance , in video processing , it is desired to recover the salient area more accurately . moreover , if the signal is sparse in canonical basis , we are interested in reconstructing the large coefficients with less error . _ non - uniform _ acquisition and recovery of signal is desirable in many applications such as image processing @xcite , camera sensor networks @xcite , wireless sensor networks @xcite , collaborative vector estimation @xcite , component analysis @xcite , and internet of things @xcite .    in this work ,",
    "we propose an adaptive framework to design a non - uniform measurement matrix , which contrast with _ dynamic cs algorithms _",
    "@xcite focusing only on the recovery step .",
    "our method is also distinct from the _ adaptive cs _ @xcite methods that are concerned with reconstructing signals , which are static over time . here ,",
    "similar to adaptive cs , the main idea is to concentrate the sensing energy on the more important coefficients , by designing a proper measurement matrix .",
    "however , due to dynamic nature of the problem , the algorithm should not make firm decisions about the location of more important coefficients .",
    "hence , soft importance level information is advantageous . to infer the importance level of each coefficient at each time step ,",
    "a generative model is imposed on the coefficients and the parameters of the model are updated in an online fashion .",
    "figure [ fig : block_diagram ] shows the overall architecture of the proposed method . at each time step , after reconstructing the signal , by using a conventional cs recovery algorithm , the importance levels of the coefficients are inferred mathematically .",
    "the importance levels are further employed to design the measurement matrix for the next time step .",
    "the rest of this paper is organized as follows . in section [ sec : system_model ] , the system model is presented .",
    "then , the generative model of the proposed bayesian framework is introduced in section [ sec : bayesian ] . in section",
    "[ sec : matrix_design ] , the inferred importance level information are used to design the measurement matrix for sensing .",
    "finally , section [ sec : results ] presents the simulation results and section [ sec : conclusions ] draws conclusions .",
    "we consider recovery of a vector - valued time series @xmath10 from the linear measurements given by @xmath12 where @xmath13 represents the noise and is modeled as an additive white gaussian noise ( awgn ) with @xmath14 .",
    "it is assumed that the signal of interest @xmath15 is compressible and contains coefficients with different importance levels , which are not known a priori . in many scenarios ,",
    "it is desirable to have non - uniform recovery performance on different parts of signal .",
    "more important coefficients may correspond to support of a sparse vector or the salient area in a video frame .",
    "we also assume that , at each time step , using the estimation of the signal @xmath16 , more important coefficients are tagged using a possibly erroneous algorithm .",
    "the variable @xmath17 marks the detected _ region of interest _ ( roi ) in the signal at time @xmath18 .",
    "specifically , @xmath19 , if the @xmath20 coefficient of the signal is detected to be in the roi , and @xmath21 otherwise .",
    "however , due to sensing failure , error in recovery of @xmath16 , and/or misdetection of the roi , @xmath17 may contain erroneous elements .    as mentioned earlier , the signal of interest often exhibits substantial temporal correlation . here",
    ", we assume that roi , and therefore the support of non - zero entries in @xmath17 , changes slowly in time .",
    "our goal is to employ the temporal correlation to infer reliable importance level information and employ the importance levels to design a non - uniform measurement matrix .",
    "to extract reliable information from possibly faulty roi data @xmath17 , we propose to employ bayesian inference . in bayesian framework , the goal is to infer the probability distribution of hidden variables given the observations .",
    "the hidden variables are often the parameters that are desired to be estimated .",
    "specifically , in our model , the following hidden variables are introduced :    1 .",
    "coefficient - specific reliability @xmath22 , which is either @xmath23 or @xmath24 and describes the reliability of roi data of the @xmath20 coefficient .",
    "overall reliability @xmath25 $ ] , denoting the overall trustworthiness of the roi detection algorithm . for small values of @xmath26 ,",
    "the algorithm is more prone to reporting faulty data .",
    "a generally reliable algorithm will report trustworthy measurements on most of the coefficients .",
    "importance level for each coefficient @xmath27 $ ] , describing the probability that coefficient @xmath28 is in roi .    as mentioned earlier , in the proposed generative model , @xmath29 is the observed variable .",
    "if @xmath30 , the @xmath20 coefficient is detected to be in roi , and @xmath31 otherwise . in this model , coefficient - specific reliability and overall reliability model the faulty data . without them , all the observations would be assumed to be trustworthy , which is not the case in real - world scenarios .",
    "figure [ fig : graph_model ] illustrates the graphical representation of the proposed generative model .",
    "the arrows in the graph represent the dependency among the variables .",
    "hence , the observed roi data depends on the actual importance level of the coefficients and the reliability of algorithm in detecting the roi coefficients .",
    "the goal of the inference algorithm is to obtain the probability distribution of the overall reliability , coefficient - specific reliability , and the importance levels , given the roi data . at each time",
    "step @xmath18 , the proposed model can be formulated as follows , for @xmath32 : @xmath33    the observed variable @xmath34 is modeled with summation of two bernoulli distributions .",
    "this means that if the roi data for @xmath20 coefficient is reliable , i.e. @xmath35 , @xmath29 will be sampled from a bernoulli distribution with true parameter for importance level , i.e. @xmath36 .",
    "otherwise , it will be sampled from @xmath37 and will be more probable to report faulty data . since @xmath36 is used as the parameter of a bernoulli distribution , it is the natural choice to model it with a beta distribution .",
    "this is due to the fact that the conjugate prior for bernoulli distribution is beta distribution .    similarly , the variable representing the overall reliability , i.e. @xmath26 , is modeled with a beta distribution",
    "this is because the coefficient - specific reliability variables are sampled from @xmath38 .",
    "this means that if the roi detection is reliable in general , roi data on most of the coefficients will be reliable .",
    "this prior links the performance of the algorithm on different coefficients and reduces the chance of overfitting the coefficient - specific reliability .    as mentioned earlier",
    ", the goal of the inference algorithm is to obtain the distribution of hidden variables , given the observations , i.e. @xmath39 . for compactness of notation , we set @xmath40 , @xmath41 , and @xmath42 . at each time step , after receiving the roi data , the distribution of hidden variables are inferred by exploiting the data and the prior belief , represented by the prior distribution @xmath43 . for that , we need to specify the joint distribution of the observation and the hidden variables .",
    "specifically , using the model formulated in ( [ eq : gen_model ] ) , we have : @xmath44    however , due to obvious practical reasons and to limit the history of the inference , the inference is performed using a few of recent observations . for that , a sliding window of length @xmath45 is utilized and the parameters of the posterior distributions are inferred using only the last @xmath45 observations .    to infer the importance level of the coefficients as well as the reliability of the roi data , we need to find the posterior distribution given the roi data , i.e. , @xmath46 .",
    "however , directly obtaining the posterior distributions is not computationally feasible and results in explosive number of probability factors growing exponentially with number of coefficients . to handle the intractable integrals of the inference procedure , _ variational inference _",
    "is often employed @xcite .    in variational inference",
    ", the posterior distribution is assumed to be fully factorized over all the hidden variables . in other words ,",
    "the posterior distribution is being approximated by a family of distributions , for which the inference procedure is tractable . for our model ,",
    "the fully factorized approximation of the posterior distribution , also referred to as the variational distribution , is defined as : @xmath47 where @xmath48 , @xmath49 , @xmath50 , @xmath51 , and @xmath52 are the parameters of the factorized distributions . by introducing the variable @xmath52",
    ", we are seeking the best approximate of @xmath46 among all the distributions @xmath53 , by factorizing the distribution over _ disjoint _ groups of hidden variables .",
    "@xmath54 , @xmath55 , and @xmath26 .",
    "it is worthwhile to mention that we make no further assumption about the distributions and their functional forms .",
    "specifically , we aim to find the best set of distributions and parameters that maximizes the lower bound of log likelihood of the observations @xcite .",
    "the lower bound of log - likelihood of the observations can be written as ( * ? ? ?",
    "* chapter 10 ) : @xmath56 where the expected value is with respect to variational distribution .",
    "hence , the problem boils down to maximizing @xmath57 to find the best variational distributions . since the lower bound is concave with respect to each of the factorized distributions , i.e. , @xmath58,@xmath59,and @xmath60 , we can determine the best approximate distributions by maximizing @xmath57 with respect to one factor at a time @xcite .",
    "thus , at each step , the lower bound is maximized over one factor , keeping all the other distributions .",
    "this procedure is repeated until convergence .    for simplicity of notation ,",
    "let us denote the whole set of hidden variables with @xmath61 . in ( [ eq : variational_dist ] ) , @xmath62 is divided into disjoint groups @xmath63 , where each @xmath64 is representing one of the hidden variables in @xmath62 . by maximizing the lower bound @xmath65 , the variational distribution of each partition @xmath66",
    "is given by ( * ? ? ?",
    "* chapter 10 ) : @xmath67 where @xmath68 is the expectation with respect to distributions @xmath69 .",
    "then by plugging in @xmath70 from ( [ eq : joint_dist ] ) and employing the exponential form of the distributions , the variational distributions can be obtained .",
    "the constant value is determined by normalizing the distribution . using ( [ eq : general_update_rule ] )",
    ", we can derive closed form expressions for parameters of the variational distributions . at each time step , after receiving the new observation vector , @xmath17 , the distribution of the hidden variables are updated using the derived update rules . then",
    ", the updated distributions are used to concentrate the sensing energy on the more important coefficients of the signal .",
    "in this section , the distributions of the importance levels are exploited to design the measurement matrix at each time step @xmath71 .",
    "the idea is to employ the information extracted from the previous measurements and focus the sensing energy on the roi coefficients .    in conventional compressive sensing methods ,",
    "the sensing energy is distributed uniformly among the coefficients of the signal .",
    "in many standard methods , it is assumed that the column of the measurement matrix are scaled to be of unit norm .",
    "thus , the total amount of sensing energy is @xmath72 . in this work",
    ", we also assume that the available sensing energy is @xmath1 . a constraint on the available sensing energy is necessary for any practical implementation .",
    "also , without the constraint , the issue of noise would be irrelevant .    in adaptive sensing procedures @xcite",
    ", no energy is allocated to the coefficients that are not likely to be in support of the signal , i.e. , roi .",
    "however , in our problem , since we are dealing with time - varying signals , such hard decisions should be avoided .",
    "the key aspect of the proposed method is the allocation of sensing energy across the coefficients of the signal . in section [ sec : bayesian ]",
    ", a bayesian framework is introduced to obtain the distribution of the importance of each coefficient .",
    "specifically , the norm of the @xmath20 column of the measurement matrix @xmath73 is given as : @xmath74 where @xmath75 is the expected value of the importance level of the @xmath20 coefficient of the signal , i.e. @xmath76 . and @xmath77 is a constant to ensure that the energy constraint is met .",
    "specifically , for @xmath78 , we will have @xmath72 .",
    "thus , at each time step the estimate of the signal is used to update the distribution of the hidden variables .",
    "then , the inferred importance levels are exploited to tune the energy allocated to each coefficient of the signal .",
    "in this section , a series of numerical experiments are presented to highlight the performance gain of the ancs .",
    "the primary performance metric used in our studies is time averaged normalized mse ( tnmse ) , which is defined as @xmath79 where @xmath80 is the number of time slots of the signal , @xmath81 is the @xmath82-norm of a vector , and @xmath16 is the estimate of @xmath15 at time @xmath18 .",
    "the parameters of the algorithm are set as follows . since",
    ", no prior information is assumed on the importance levels of the coefficients , the parameters are initialized as @xmath83 .",
    "this choice of parameters results in a uniform distribution for the importance levels . to initialize @xmath84 and @xmath85 ,",
    "it is reasonable to assume that at least half of the measurements are reliable . in our numerical experiments , we initialized @xmath86 and @xmath87 , which means on average @xmath88 of the measurements are trustworthy .",
    "the maximum number of iterations for the inference algorithm is set to @xmath89 , with possibility of early termination if @xmath90 at @xmath91 iteration .",
    "moreover , a window length of @xmath92 is used .    in all the simulations to construct the measurement matrices , elements of the matrix",
    "were drawn from an i.i.d zero mean gaussian distribution .",
    "for uniform sampling , the columns of the matrices are scaled to have unit norm . on the other hand , for ancs , ( [ eq : col_norm ] )",
    "is used to realize the non - uniform distribution of energy among the columns .",
    "the total sensing energy of all the methods is assumed to be the same , i.e. @xmath93 .    as a performance benchmark and to quantify the performance improvement obtained by the ancs , we exploit the proposed method as the sampling step of an @xmath94 minimization recovery algorithm . specifically , the estimate of the signal is obtained by solving an @xmath94 minimization problem , given by : @xmath95 where @xmath96 and @xmath97 is set to be equal to @xmath98 . to solve the problem , cvx @xcite , which is a toolbox for specifying and solving convex problems , is used .      for the first experiment , the performance gain of ancs",
    "is quantified for the signals that are sparse in canonical basis . to model the temporal correlation , both in amplitude and support of the signal",
    ", the signal is assumed to be outcome of two random processes .",
    "specifically , a binary vector @xmath99^t$ ] describes the support of the signal at time @xmath18 .",
    "@xmath100 indicates the coefficients in the support and @xmath101 denotes the zero coefficients .",
    "coefficients of @xmath102 are assumed to be independent and a markov chain process is defined for each of the coefficients .",
    "the markov chain processes are described by @xmath103 and @xmath104 .",
    "thus , @xmath105 is related to the sparsity level of signal .",
    "furthermore , a second process models the amplitude of the large coefficients .",
    "we employ an independent gauss - markov process for each of the coefficients of the signal .",
    "amplitude of the @xmath20 coefficient evolves over time as : @xmath106 here , @xmath107 is a constant between @xmath23 and @xmath24 and controls the degree of correlation . for @xmath108",
    ", the amplitude would be an uncorrelated gaussian random process .",
    "@xmath109 is the amount of variation among two consecutive time steps and is modeled with @xmath110 .",
    "thus the mean of the process is assumed to be @xmath23 . at each time step , the coefficients of the signal are constructed as @xmath111    the simulation parameters are set as follows , unless otherwise is stated .",
    "we assume that the signal of interest is of length @xmath112 with sparsity level of @xmath113 .",
    "the variance of noise , i.e. , @xmath114 , is set to have a signal - to - noise ratio ( snr ) of @xmath115 db .",
    "other model parameters are set as @xmath116 , @xmath117 , @xmath118 , and @xmath119 .    to detect the roi , i.e. , support of the signal , after determining the estimate of the signal @xmath16",
    ", a simple thresholding is performed .",
    "specifically , @xmath34 is set to @xmath24 , if @xmath120 .",
    "figure [ fig : importanceovertime ] shows the evolution of the inferred importance levels , i.e. , @xmath121 , over time for @xmath122 . in other words , this figure illustrates how the sensing energy is distributed among the coefficients at each time step . as it is clear , at the first time step , @xmath123 , indicating unbiased estimate of importance levels when no further information is available .",
    "however , as more measurements are received , uncertainty decreases and the support of the signal is revealed .",
    "it is also worthwhile to point out that an error in the roi detection procedure can potentially impact up to @xmath92 time slots .",
    "error propagation , as well as computational complexity , are the main reasons that choosing large values for @xmath45 should be avoided .    .",
    "the total sensing energy is the same for all the methods .",
    "@xmath112 , snr @xmath124 db , and @xmath119 , and @xmath125 .",
    ", width=249 ]    to study the performance of ancs for different levels of temporal correlation , figure [ fig : tnmsevsp ] illustrates the tnmse of ancs for different values of @xmath126 .",
    "the results are averaged over substantially large number of monte - carlo trials . here ,",
    "ancs is employed also as the sampling step of support - aware mmse , as well as the @xmath94 minimization recovery method .",
    "sa - mmse calculates the minimum mean square error estimate of the signal when the support of the sparse signal is known .",
    "the actual support of the signal , @xmath127 , @xmath114 , and @xmath107 are provided as the inputs of the sa - mmse algorithm .",
    "the performance of sa - mmse is an indicator of lowest mse achievable by a recovery algorithm .    for small values of @xmath126 ,",
    "the signal is nearly static over time .",
    "thus , the method is able to detect the support accurately and the tnmse is decreased significantly . furthermore , since the signal is sparse in canonical basis and the support of the signal is set to be the roi , overall recovery error is the same as the recovery error of the roi coefficients .",
    "it is due to the fact that whole energy of the signal is concentrated in the roi . as it can be noticed in the figure , for @xmath128 , ancs can enhance the performance of the @xmath94 minimization algorithm substantially .",
    "as @xmath126 increases , the support of the signal changes over time and the observations of previous time steps become less informative about the signal and the performance gain of ancs decreases . however , for values of @xmath129 , nonuniform recovery of the signal is still achieved .    , snr @xmath124 db , and @xmath119 . , width=249 ]",
    "figure [ fig : errorvsm ] compares the performance of different recovery algorithms with uniform sampling and ancs as the sampling step for different number of measurements .",
    "as it is clear , ancs can decrease the tnmse up to @xmath130 db , compared to @xmath94 minimization recovery , and can reduce the required number of measurements .",
    "as an example , to achieve a tnmse of @xmath131 db , ancs employs about @xmath132 of the measurements required by uniform sampling , highlighting one of the major benefits of ancs : for a sparse signal in canonical basis , ancs is able to reduce the recovery error and number of required measurements substantially .    to highlight the performance gain achieved by ancs in low snr regimes , figure [ fig : errorvssnr ] depicts tnmse of different methods versus snr .",
    "it is easy to notice that the performance of ancs is very close to sa - mmse with uniform sampling , which is mse - optimal .",
    "this is one of the main benefits of adaptive cs . as mentioned in section [ sec : intro ] , it is known that adaptive cs provides the opportunity to detect and estimate signals at lower snrs .",
    "furthermore , performance of sa - mmse algorithm in figure [ fig : errorvsm ] and figure [ fig : errorvssnr ] illustrates that ancs is able reduce the lower bound of recovery error by up to @xmath133 db .     minimization and sa - mmse with and without ancs as the sampling step in terms of tnmse ( in db ) .",
    "of different methods for @xmath112 , @xmath117 , and @xmath125 . , width=249 ]      in this series of experiments , the performance gain achieved by ancs",
    "is evaluated for signals that are not sparse in canonical basis , but has a sparse representation in some proper domain . in our numerical experiments",
    ", we employed dct domain as the sparsifying basis .    to generate the sparse signal in dct domain",
    ", the same procedure explained in section [ subsec : results_canonical ] is exploited .",
    "specifically , let @xmath134 represent the sparse representation of the signal of interest , @xmath15 , in dct domain .",
    "@xmath135 denotes the dct transform matrix . to generate a time correlated signal , elements of @xmath136 are constructed as @xmath137 , where @xmath138 and @xmath139 are outcome of two random processes described in section [ subsec : results_canonical ] . to reconstruct the signal , we use @xmath140 , where @xmath141 and @xmath142 .",
    "furthermore , to model the variation of roi over time , a new set of binary markov processes is employed .",
    "this means that the probability of a coefficient being in the roi is independent from its location and its value .",
    "to describe this markov process , for simplicity , we use the same set of parameters as the random process corresponding to the support of the signal , i.e. , @xmath105 and @xmath126 .",
    "hence , the rates of change for support of @xmath136 and the roi in @xmath15 are assumed to be the same .",
    "it is also assumed that the roi detection algorithm may report erroneous observations to ancs .    in figure",
    "[ fig : dctvs_m ] , we evaluate the performance of ancs versus the number of measurements @xmath2 for fault rate of @xmath143 .",
    "this experiment also shows that the proposed ancs is able to decrease the error of roi coefficients up to @xmath144-@xmath145 db for different number of measurements .",
    "this benefit comes at the cost of losing performance on total recovery error .",
    "interestingly , for smaller values of @xmath2 , this benefit comes at almost no cost and without losing any performance for non - roi entries .     of ancs .",
    "@xmath112 , snr @xmath124 db , and @xmath119 , and @xmath125 .",
    ", width=249 ]    finally , as it was expected , figure [ fig : dct_tnmsevsmicmodel ] illustrates that as ancs receives more faulty data from the roi detection algorithm , its performance becomes more similar to conventional cs with uniform sampling .",
    "this is because the faulty data prevents the inference algorithm from gaining certainty on the location of roi coefficients .",
    "however , even for fault rates of as much as @xmath132 , non - uniform recovery of the signal is achieved .    , snr @xmath124 db , and @xmath119 , and @xmath125 . , width=249 ]",
    "this work presented _ adaptive non - uniform compressive sampling _ ( ancs ) for time - varying sparse signals .",
    "the main idea is to employ the observations of previous time slots to infer the region of interest ( roi ) in the signal and concentrate the sensing energy on the corresponding coefficients . for that",
    ", we presented a bayesian framework , by modeling the overall and coefficient - specific reliability of the roi detection algorithm .",
    "the results show that the proposed framework is able to achieve the desired non - uniform recovery and can decrease the error in roi significantly for signals that are sparse or have a sparse representation in a proper basis .",
    "the results also illustrated that the proposed method is particularly advantageous for signals that are sparse in canonical basis .",
    "for such signals , ancs results in substantial improvement in accuracy of estimation .",
    "this material is based upon work supported by the national science foundation under grant no . eccs-1418710 ."
  ],
  "abstract_text": [
    "<S> in this paper , adaptive non - uniform compressive sampling ( ancs ) of time - varying signals , which are sparse in a proper basis , is introduced . </S>",
    "<S> ancs employs the measurements of previous time steps to distribute the sensing energy among coefficients more intelligently . to this aim </S>",
    "<S> , a bayesian inference method is proposed that does not require any prior knowledge of importance levels of coefficients or sparsity of the signal . </S>",
    "<S> our numerical simulations show that ancs is able to achieve the desired non - uniform recovery of the signal . moreover , </S>",
    "<S> if the signal is sparse in canonical basis , ancs can reduce the number of required measurements significantly .    </S>",
    "<S> compressive sensing , sequential measurements , adaptive sensing , time - varying sparse signals , bayesian inference , non - uniform sampling </S>"
  ]
}