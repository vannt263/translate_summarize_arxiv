{
  "article_text": [
    "algorithmic information theory ( ait ) is a the of individual objects , using , and concerns itself with the relationship between computation , information , and randomness .",
    "the information content or complexity of an object can be measured by the length of its shortest description .",
    "for instance the string `` 0101010101010101010101010101010101010101010101010101010101010101 '' has the short description `` 32 repetitions of 01'' , while `` 1100100001100001110111101110110011111010010000100101011110010110 '' presumable has no simple description other than writing down the string itself .",
    "more formally , the ( ac ) of a string @xmath0 is defined as the length of the shortest program that computes or outputs @xmath0 , where the program is run on some fixed universal computer .",
    "a closely related notion is the probability that a universal computer outputs some string @xmath0 when fed with a program chosen at random .",
    "this ( ap ) is key in addressing the old philosophical problem of in a formal way .",
    "the major drawback of ac and ap are their incomputability .",
    "time - bounded `` levin '' complexity penalizes a slow program by adding the logarithm of its running time to its length .",
    "this leads to computable variants of ac and ap , and ( us ) that solves all inversion problems in optimal ( apart from some huge multiplicative constant ) time .    ac and ap also allow a formal and rigorous definition of randomness of individual strings that does not depend on physical or philosophical intuitions about nondeterminism or likelihood . roughly , a string is ( ar ) if it is incompressible in the sense that its algorithmic complexity is equal to its length .",
    "ac , ap , us , and ar are the core subdisciplines of ait , but ait spans into many other areas .",
    "it serves as the foundation of the ( mdl ) principle , can simplify proofs in computational complexity theory , has been used to define a universal similarity metric between objects , solves the maxwell demon problem , and many others .",
    "formalizes the concept of simplicity and complexity .",
    "intuitively , a string is simple if it can be described in a few words , like `` the string of one million ones '' , and is complex if there is no such short description , like for a random string whose shortest description is specifying it bit by bit .",
    "typically one is only interested in descriptions or _ codes _ that are effective in the sense that decoders are on some computer .",
    "the universal @xmath1 is the standard abstract model of a general - purpose computer in theoretical computer science .",
    "we say that program @xmath2 is a description of string @xmath0 if @xmath2 run on @xmath1 outputs @xmath0 , and write @xmath3 .",
    "the length of the shortest description is denoted by @xmath4 where @xmath5 is the length of @xmath2 measured in bits .",
    "one can show that this definition is nearly independent of the choice of @xmath1 in the sense that @xmath6 changes by at most an additive constant independent of @xmath0 .",
    "the statement and proof of this invariance theorem in @xcite is often regarded as the birth of algorithmic information theory .",
    "this can be termed kolmogorov s thesis : the intuitive notion of ` shortest effective code ' in its widest sense is captured by the formal notion of kolmogorov complexity , and no formal mechanism can yield an essentially shorter code .",
    "note that the shortest code is one for which there is a general decompressor : the kolmogorov complexity establishes the ultimate limits to how short a file can be compressed by a general purpose compressor .",
    "there are many variants , mainly for technical reasons : the historically first `` plain '' complexity , the now more important `` prefix '' complexity , and many others .",
    "most of them coincide within an additive term logarithmic in the length of the string .    in this article",
    "we use @xmath7 for the prefix complexity variant .",
    "a has a separate input tape which it reads from left - to - right without backing up , a separate worktape on which the computation takes place , and a separate output tape on which the output is written .",
    "we define a as the initial segment of the input that is scanned at the time when the machine halts , and the is the string that has been written to the separate output tape at that time . the conditional prefix complexity @xmath8 is the length of the shortest binary program @xmath9 on a universal prefix turing machine @xmath1 with output @xmath0 and input @xmath10 @xcite . for non - string objects",
    "( like numbers @xmath11 , pairs of strings @xmath12 , or computable functions @xmath13 ) one can specify some default coding @xmath14 and define @xmath15 .",
    "the most important properties are :    * that @xmath7 is approximable from above in the limit but not computable , * the upper bounds @xmath16 and @xmath17 , * kraft s inequality implies @xmath18 , * the lower bound @xmath19 for `` most '' @xmath0 and @xmath20 for @xmath21 , * extra information bounds @xmath22 , * subadditivity @xmath23 , * symmetry of information @xmath24 , * information non - increase @xmath25 for computable functions @xmath13 , * and coding relative to a probability distribution ( mdl ) + @xmath26 for computable probability distributions @xmath27 ,    where all ( in)equalities hold within an additive constant .",
    "furthermore , it shares many properties with shannon s entropy ( information measure ) , but @xmath7 has many advantages .",
    "the properties above allow us to draw a schematic graph of @xmath7 as depicted in figure [ figk ] .    _ schematic graph of prefix kolmogorov complexity @xmath6 with string @xmath0 interpreted as integer .",
    "@xmath28 for ` most ' @xmath0 and @xmath29 for all @xmath0 for suitable constant @xmath30._,scaledwidth=55.0% ]",
    "solomonoff ( 1964 ) considered the probability that a universal computer outputs some string when fed with a program chosen at random . this algorithmic `` solomonoff '' probability ( ap )",
    "is key in addressing the old philosophical problem of induction in a formal way .",
    "it is based on    * ( choose the simplest model consistent with the data ) , * epicurus principle of multiple explanations ( keep all explanations consistent with the data ) , * bayes s rule ( transform the a priori distribution to a posterior distribution according to the evidence , experimentally obtained data ) , * ( universal ) turing machines ( to compute , quantify and assign codes to all quantities of interest ) , and * algorithmic complexity ( to define what simplicity / complexity means ) .",
    "occam s razor ( appropriately interpreted and in compromise with epicurus principle of indifference ) tells us to assign high / low a priori plausibility to simple / complex strings @xmath0 . using @xmath7 as the complexity measure",
    ", one could choose any monotone decreasing function of @xmath7 , e.g.  @xmath31 .",
    "the precise definition of ( ap ) , also called universal a priori probability , @xmath32 is the probability that the output of a ( so - called monotone ) universal turing machine @xmath1 starts with @xmath0 when provided with fair coin flips on the input tape .",
    "formally , @xmath33 can be defined as @xmath34 where the sum is over all ( so - called minimal , not necessarily halting , denoted by * ) programs @xmath2 for which @xmath1 outputs a string starting with @xmath0 . since the shortest programs @xmath2 dominate the sum , @xmath32 is roughly @xmath31 .",
    "@xmath33 has similar remarkable properties as @xmath7 .",
    "additionally , the predictive distribution @xmath35 converges rapidly to 1 on ( hence predicts ) any computable sequence @xmath36 .",
    "it can also be shown that @xmath33 leads to excellent predictions and decisions in general stochastic environments .",
    "if married with sequential , it leads to an optimal reinforcement learning agent embedded in an arbitrary unknown environment @xcite , and a formal definition and test of intelligence .",
    "a formally related quantity is the probability that @xmath1 halts when provided with fair coin flips on the input tape ( i.e.  that a random computer program will eventually halt ) .",
    "this halting probability , also known as chaitin s constant @xmath37 , or ` the number of wisdom ' has numerous remarkable mathematical properties , and can be used for instance to quantify goedel s incompleteness theorem .",
    "consider a problem to solve for which we have two potential algorithms @xmath38 and @xmath39 , for instance breadth versus depth first search in a finite ( game ) tree .",
    "much has been written about which algorithm is better under which circumstances .",
    "consider the following alternative very simple solution to the problem : a meta - algorithm @xmath40 runs @xmath38 and @xmath39 in parallel and waits for the first algorithm to halt with the answer . since @xmath40 emulates @xmath38 and @xmath39 with half - speed ,",
    "the running time of @xmath40 is the minimum of @xmath41time@xmath42 and @xmath41time@xmath43 , i.e.  @xmath40 is as fast as the faster of the two , apart from a factor of 2 .",
    "small factors like 2 are often minor compared to potentially much larger difference in running time of @xmath38 and @xmath39 .",
    "( us ) extends this idea from two algorithms to _ all _ algorithms .",
    "first , since there are infinitely many algorithms , computation time has to be assigned non - uniformly .",
    "the optimal way is that @xmath40 devotes a time fraction of @xmath44 to each ( prefix ) program @xmath2 .",
    "second , since not all programs solve the problem ( some never halt , some just print `` hello world '' , etc . ) @xmath40 has to verify whether the output is really a solution , and if not discard it and continue .",
    "how does this fit into ait ?",
    "a problem of ac @xmath7 is its incomputability .",
    "time - bounded `` levin '' complexity penalizes a slow program by adding the logarithm of its running time to its length : @xmath45 it is easy to see that @xmath46 is just the logarithm of the running time ( without verification ) of @xmath40 , and is therefore computable .",
    "while universal search is nice in theory , it is not applicable in this form due to huge hidden multiplicative constants in the running time .",
    "another restriction is that verification needs to be fast .",
    "hutter @xcite developed a more general asymptotically fastest algorithm , which removes the multiplicative constant and necessity of verification , unfortunately at the expense of an even larger additive constant .",
    "schmidhuber @xcite developed the first practical variants of @xmath40 by carefully choosing the programming language ( @xmath1 ) , allocating time in @xmath40 adaptively , designing training sequences of increasing complexity , reusing subroutines from earlier simpler problems , and various other `` tricks '' .",
    "he also defined the speed prior , which is to @xmath47 what ap is to ac .",
    "the mathematical formalization of the concept of probability or chance has a long intertwined history .",
    "the ( now ) standard axioms of probability , learned by all students , are due to kolmogorov ( 1933 ) .    while mathematically convincing , the semantics is far from clear .",
    "frequentists interpret probabilities as limits of observed relatives frequencies , objectivists think of them as real aspects of the world , subjectivists regard them as one s degree of belief ( often elicited from betting ratios ) , while cournot only assigns meaning to events of high probability , namely as happening for sure in our world .",
    "none of these approaches answers the question of whether some _ specific individual _ object or observation , like the binary strings above , is random .",
    "kolmogorov s axioms do not allow one to ask such questions .",
    "von mises ( 1919 ) , with refinements to his approach by wald ( 1937 ) , and church ( 1940 ) attempted to formalize the intuitive notion of one string looking more random than another ( see the example in the introduction ) with partial success .",
    "for instance , if the relative frequency of 1s in an infinite sequence does not converge to 1/2 it is clearly non - random , but the reverse is not true : for instance `` 0101010101 ... '' is not random , since the pair `` 01 '' occurs too often .",
    "pseudo - random sequences , like the digits of @xmath48 , cause the most difficulties .",
    "unfortunately no sequence can satisfy  all  randomness tests .",
    "the mises - wald - church approach seemed satisfactory untill ville ( 1939 ) showed that some sequences are random according to their definition and yet lack certain properties that are universally agreed to be satisfied by random sequences .",
    "for example , the relative frequency of  1 s in increasingly long initial segments should infinitely often switch from above 1/2 to below 1/2 and vice versa .    martin - loef ( 1966 ) , rather than give a definition and check whether it satisfied all requirements , took the approach to formalize the notion of all effectively testable requirements in the form of tests for randomness .",
    "the tests are constructive ( namely all and only lower semi - computable ) ones , which are typically all one ever cares about . since the tests are constructed from turing machines , they can be effectively enumerated according to the effective enumeration of the turing machines they derive from . since the set of sequences satisfying a test ( having the randomness property the test verifies ) has measure one , and there are only countably many tests , the set of sequences satisfying  all  such tests also has measure one .",
    "these are the ones called algorithmic random|algorithmically `` martin - loef '' random ( ar ) .",
    "the theory is developed for both finite strings and infinite sequences . in the latter case",
    "the notion of test is more complicated and we speak of sequential tests .    for infinite sequences one can show that these are exactly the sequences which are incompressible in the sense that the algorithmic prefix complexity of every initial segment is at least equal to their length",
    ". more precisely , the infinite sequence @xmath49 an important result due to g.j .",
    "chaitin and c. schnorr .",
    "this notion makes intuitive sense : a string can be compressed ",
    "iff  there are some regularities in the string ",
    "the string is non - random .",
    "* ml - random sequences can not be effectively constructed .",
    "yet we can give a natural example : the , @xmath37 is a real number between 0 and 1 , and the sequence of bits in its binary expansion is an infinite ml - random sequence . *",
    "randomness of other objects than strings and sequences can also be defined .",
    "* coupling the theory of ar with recursion theory ( downey and hirschfeldt 2007 ) , we find a hierarchy of notions of randomness , at least if we leave the realm of computability according to turing .",
    "many variants can be obtained depending on the precise definition of `` constructive '' .",
    "in particular `` relative randomness '' based on ( halting ) oracle machines leads to a rich field connected to recursion theory . * finally , the crude binary separation of random versus non - random strings can be refined , roughly by considering strings with @xmath50 for some @xmath51 .",
    "if strings are interpreted as ( the expansion of ) real numbers , this leads to the notion of constructive or effective hausdorff ( fractal ) dimension .",
    "despite the incomputability of its core concepts , ait has many , often unexpected , applications .",
    "ait helps to tackle many philosophical problems in the sense that it allows one to formalize and quantify many intuitive but vague concepts of great importance as we have seen above , and hence allows one to talk about them in a meaningful and rigorous way , thus leading to a deeper understanding than without ait .",
    "most importantly , ac formalizes and quantifies the concepts of simplicity and complexity in an essentially unique way .",
    "a core scientific paradigm is occam s razor , usually interpreted as `` among two models that describe the data equally well , the simpler one should be preferred . ''",
    "using ac to quantify `` simple '' allowed solomonoff and others to develop their universal theories of induction and action , in the field of .",
    "ait is also useful in the foundations of thermodynamic and its second theorem about entropy increase , and in particular for solving the problem of .    by ( often crudely ) approximating the `` ideal '' concepts ,",
    "ait has been applied to various problems of practical interest , e.g.  in linguistics and genetics .",
    "the principle idea is to replace the universal turing machine @xmath1 by more limited `` turing '' machines , often adapted to the problem at hand .",
    "the major problem is that the approximation accuracy is hard to assess and most theorems in ait break down .",
    "the universal similarity metric by vitanyi and others is probably the greatest practical success of ait : a reasonable definition for the similarity between two objects is how difficult it is to transform them into each other .",
    "more formally one could define the similarity between strings @xmath0 and @xmath10 as the length of the shortest program that computes @xmath0 from @xmath10 ( which is @xmath52 ) .",
    "symmetrization and normalization leads to the universal similarity metric .",
    "finally , approximating @xmath7 by standard compressors like lempel - ziv ( zip ) or bzip(2 ) leads to the normalized compression distance , which has been used to fully automatically reconstruct language and phytogenetic trees , and many other clustering problems .    see for details and references .    in science itself , ait can constructivize other fields : for instance , statements in shannon information theory and classical probability theory necessarily only hold in expectation or with high probability .",
    "theorems are typically of the form `` there exists a set of measure x for which y holds '' , i.e.  they are useful for ( large ) samples .",
    "ar on the other hand can construct high - probability sets , and results hold for individual observations / strings . and",
    "real numbers also have constructive counterparts .",
    "naturally , ait concepts have also been exploited in theoretical computer science itself : ait , via the incompressibility method , has resolved many open problems in computational complexity theory and mathematics , simplified many proofs , and is important in understanding ( dissipationless ) reversible computing .",
    "it has found applications in statistics , cognitive sciences , biology , physics , and economics .",
    "ait can also serve as an umbrella theory for other more practical fields , e.g. , in machine learning , the minimum description length ( mdl ) principle can be regarded as a downscaled practical version of ac .",
    "@xcite suggested to define the information content of an object as the length of the shortest program computing a representation of it .",
    "@xcite invented the closely related universal a priori probability distribution and used it for time series forecasting . together with @xcite ,",
    "this initiated the field of algorithmic information theory in the 1960s .",
    "and others significantly contributed to the field in the 1970s ( see e.g.  @xcite ) .",
    "in particular the prefix complexity and time - bounded complexity are ( mainly ) due to him .",
    "li and vitanyi @xcite is the standard ait textbook .",
    "the book by calude @xcite focusses on ac and ar , hutter @xcite on ap and us , and downey and hirschfeldt @xcite on ar .",
    "the at http://www.hutter1.net/ait.htm contains further references , a list of active researchers , a mailing list , a list of ait events , and more .",
    "there is still no generally agreed upon notation and nomenclature in the field .",
    "one reason is that researchers of different background ( mathematicians , logicians , and computer scientists ) moved into this field .",
    "another is that many definitions are named after their inventors , but if there are many inventors or one definition is a minor variant of another , things become difficult .",
    "this article uses descriptive naming with contributors in quotation marks .",
    "not even the name of the whole field is generally agreed upon .",
    "_ algorithmic information theory _ , coined by gregory chaitin , seems most appropriate , since it is descriptive and impersonal , but the field is also often referred to by the more narrow and personal term _ kolmogorov complexity_.",
    "the ait field may be subdivided into about 4 separate subfields : ac , ap , us , and ar . the fifth item below refers to applications .    *",
    "( ac ) * * philosophical considerations * * properties of ac * * plain ( kolmogorov ) complexity * * prefix complexity * * resource bounded complexity * * other complexity variants * ( ap ) * * occam s razor and epicurus principle * * discrete algorithmic probability * * continuous algorithmic probability = a priori semimeasure * * universal sequence prediction * * the halting probability = chaitin s omega = the number of wisdom * ( us ) * * levin search * * levin complexity and speed prior * * adaptive levin search * * fastest algorithms for general problems * * optimal ordered problem solver * * goedel machines * ( ar ) / recursion theory * * recursion theory * * effective real numbers * * randomness of reals * * van mises - wald - church randomness * * martin - loef randomness * * more randomness concepts and relative randomness * * effective hausdorff dimension * * * minimum description / message length * * machine learning * * artificial intelligence * * computational complexity * * the incompressibility method * * ( shannon ) information theory * * reversible computing * * universal similarity metric * * thermodynamics * * entropy and maxwell demon * * compression in nature"
  ],
  "abstract_text": [
    "<S> this article is a brief guide to the field of algorithmic information theory ( ait ) , its underlying philosophy , and the most important concepts . </S>",
    "<S> ait arises by mixing information theory and computation theory to obtain an objective and absolute notion of information in an individual object , and in so doing gives rise to an objective and robust notion of randomness of individual objects . </S>",
    "<S> this is in contrast to classical information theory that is based on random variables and communication , and has no bearing on information and randomness of individual objects . </S>",
    "<S> after a brief overview , the major subfields , applications , history , and a map of the field are presented . </S>"
  ]
}