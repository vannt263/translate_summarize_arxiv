{
  "article_text": [
    "in this article we are interested in inferring a particaular class of posterior distributions in bayesian statistics . the scenario is when the likelihood can not be evaluated point - wise , nor do we have access to a positive unbiased estimate of it ( it is assumed we can simulate from the associated distribution , although this is not always required ) . in such a case ,",
    "it is not possible to draw inference from the true posterior , even using numerical techniques such as markov chain monte carlo ( mcmc ) or sequential monte carlo ( smc ) .",
    "the common response in bayesian statistics , is to adopt an approximation of the posterior using the notion of approximate bayesian computation ( abc ) ; see @xcite for a review .",
    "abc approximations of posteriors are based upon defining a probability distribution on an extended state - space , with the additional random variables lying on the data - space and usually distributed according the true likelihood .",
    "the closeness of the abc posterior distribution is controlled by a tolerance parameter @xmath8 and for some abc approximations ( but not all ) the approximation is exact as @xmath9 .",
    "abc has been considered in a wealth of articles and model contexts ; see for instance @xcite for a non - exhaustive list . in many cases of practical interest ,",
    "the abc posterior is not available exactly , and one must resort to numerical approximations , for instance using mcmc or smc ; see for instance @xcite and the references therein .",
    "we consider using monte carlo to approximate expectations w.r.t .",
    "the abc posterior .",
    "multilevel monte carlo @xcite ( see also @xcite ) methods are such that one sets an error threshold for a target expectation and then attains an estimator with the prescribed error utilizing an optimal allocation of monte carlo resources .",
    "the idea assumes that one has a collection of approximations associated to a probability law , but the probability of interest is intractable , even using monte carlo methods .",
    "for instance , it could be a probability associated to a time - discretization of a stochastic differential equation and the collection of approximations are finer and finer time - discretizations .",
    "implicitly , one is assuming that the cost associated to direct sampling of the approximations increase with accuracy .",
    "the idea is then to rewrite the expectation w.r.t .  the most accurate approximation and then use a telescoping sum of expecatations w.r.t .  the sequence of approximations .",
    "given one can appropriately sample the sequence of approximations , it can be shown for certain models that for a given level of mean square error , mlmc has a lower cost than i.i.d .",
    "sampling from the most accurate approximation .",
    "see @xcite for a recent overview and the method is described in more detail in section [ sec : ml_abc ] .",
    "the connection between abc and mlmc thus becomes clear ; one can consider a sequence of abc approximations for @xmath10 and then leverage upon using the mlmc approach .",
    "there are , however , several barriers to conducting such an approach .",
    "the first is associated to an appropriate sampling of the sequence ; the ideas of mlmc rely upon independent sampling .",
    "this issue is easily addressed , as there exist many approaches in the literature for dependent sampling of the sequence ; see for instance @xcite . the second and more",
    "challenging , is that the advantage of the mlmc method relies on an appropriate _ coupled _ sampling from the sequence of approximations .",
    "constructing such a coupling is non - trivial for general abc problems .",
    "we adopt an approach which replaces coupling with importance sampling .",
    "this paper presents an adaptation of the mlsmc method of @xcite for abc problems .",
    "we show that , under assumptions , the use of mlsmc is such that for a given level of mean square error , this method for abc has a lower cost than i.i.d .",
    "sampling from the most accurate abc approximation .",
    "several numerical examples are presented . before our ideas",
    "are developed , we note that the mlmc method is inherently biased , in that there is approximation error , but this error can be removed by using the ideas in @xcite ( see also @xcite ) .",
    "this idea is cleverly utilized in @xcite to perform abc with no ` @xmath11 ' error and hence is related to the mlabc method in this paper .",
    "however , it is well - known in the ml literature that in certain contexts the variance / cost of the debiasing method blows up , whereas , this is not the case for mlmc ; see @xcite .",
    "this article is structured as follows . in section",
    "[ sec : ml_abc ] the idea of mlmc for abc is introduced and developed .",
    "it is noted that in its standard form , it is not straightforward to apply in many contexts where abc is typically used . in section [ sec : mlsmc_abc ]",
    "the idea is extended to using mlsmc .",
    "some theoretical results are considered , showing under some assumptions that for a given level of mean square error , the mlsmc method for abc has a lower cost than i.i.d .",
    "sampling from the most accurate abc approximation .",
    "numerical results are given in section [ sec : numerics ] .",
    "the article is concluded in section [ sec : summary ] with a discussion of extensions .",
    "the appendix houses a proofs of propositions in the article .",
    "consider data @xmath12 , associated to finite - dimensional parameter @xmath13 .",
    "define the posterior : @xmath14 we suppose that @xmath15 is unavailable numerically , even up - to a non - negative unbiased estimator .",
    "we consider approximate bayesian computation ( abc ) .",
    "let @xmath16 ( with associated sigma - algebra @xmath17 ) and define for @xmath10 , @xmath18 : @xmath19 where @xmath20 is a user - defined non - negative function that is typically maximized when @xmath21 and concentrates on this maximum as @xmath22 .",
    "set @xmath23 and @xmath24 .",
    "let @xmath25 with @xmath26 bounded and measurable . set @xmath27 then we know that by the standard multilevel ( ml ) identity @xcite : @xmath28(\\varphi).\\ ] ] let @xmath29 be given .",
    "it is known that if one can sample the coupling @xmath30 it is possible to reduce the computational effort to achieve a given mean square error ( mse ) of @xmath31 , relative to i.i.d .",
    "sampling from @xmath32 , when approximating @xmath33 .",
    "although that is not verified for the abc context , we show that it is possible , with the following simple argument .",
    "let @xmath34 be distributed from some coupling of @xmath30 , @xmath35 .",
    "suppose that ( call the following bullet points ( a ) ) :    * @xmath36 , for some @xmath37 .",
    "* @xmath38=\\mathcal{o}(\\epsilon_l^{\\beta})$ ] , for some @xmath39 .",
    "* the cost of sampling from @xmath30 is @xmath40 , for some @xmath41 .",
    "then supposing that @xmath42 is distributed according to @xmath43 , one can approximate @xmath44 by @xmath45(\\varphi)\\ ] ] where for @xmath35 @xmath46 and @xmath47 are the empirical measures of @xmath48 independently sampled values @xmath49 from the coupling @xmath30 , independently for each @xmath50 and @xmath51 is the empirical measure of @xmath52 independent samples from @xmath43 ( independent of all other random variables ) . then the mse is @xmath53(\\varphi ) - \\eta_{\\infty}(\\varphi))^2 ]   = \\ ] ]",
    "@xmath54 + \\sum_{l=1}^l \\frac{1}{n_l}\\mathbb{v}\\textrm{ar}_{(\\eta_l,\\eta_{l-1})}[\\varphi(x_l)-\\varphi(y_l)].\\ ] ] setting @xmath55 for some fixed integer @xmath56 if we want the mse to be @xmath31 we can make the bias and variance this order .",
    "so we want @xmath57 so @xmath58 .",
    "now we require @xmath59 and at the same time , we seek to minimize the cost of doing so @xmath60 .",
    "this constrained optimization problem is easily solved with lagrange multipliers ( e.g.  @xcite ) yielding that @xmath61 where @xmath62 . under this choice",
    "@xmath63 this yields a cost of @xmath64 .",
    "the cost of i.i.d .",
    "sampling from @xmath32 to achieve a mse of @xmath31 is @xmath65 . if @xmath66 then the mlmc method certainly outperforms i.i.d .",
    "sampling from @xmath32 .",
    "the worst scenario is when @xmath67 . in this case",
    "it is sufficient to set @xmath68 to make the variance @xmath31 , and then the number of samples on the finest level is given by @xmath69 whereas the total algorithmic cost is @xmath70 , where @xmath71 . in this case",
    ", one can choose the largest value for the bias , @xmath72 , so that @xmath73 and the total cost , @xmath74 , is dominated by this single sample .",
    "we remark that when debiasing this procedure and @xmath67 using @xcite the variance / cost blows up .",
    "the issue with this construction , ignoring verifying ( a ) , is that in an abc context , it is challenging to construct the coupling and even if one can , seldom can one achieve i.i.d .",
    "sampling from the couples .",
    "the approach in @xcite is to by - pass the issue of coupling , by using importance sampling and then to use sequential monte carlo ( smc ) @xcite samplers to provide the appropriate simulation .",
    "set @xmath75 .",
    "then @xcite show that @xmath76 @xcite show how such an identity can be approximated as we now describe .",
    "it is remarked that much of the below information is in @xcite and is necessarily recalled here .",
    "we will apply an smc sampler to obtain a collection of samples ( particles ) that sequentially approximate @xmath77 .",
    "we consider the case when one initializes the population of particles by sampling i.i.d",
    ".  from @xmath43 , then at every step resamples and applies a mcmc kernel to mutate the particles .",
    "we denote by @xmath78 , with @xmath79 , the samples after mutation ; one resamples @xmath80 according to the weights @xmath81 , for indices @xmath82 .",
    "let @xmath83 denote a sequence of mcmc kernels , with the property @xmath84 .",
    "these kernels are used at stages @xmath85 of the smc sampler . for @xmath86 , @xmath87 , we have the following estimator of @xmath88 $ ] : @xmath89 we define @xmath90 the joint probability distribution for the smc algorithm is @xmath91 if one considers one more step in the above procedure , that would deliver samples @xmath92 , a standard smc sampler estimate of the quantity of interest in ( [ eq : ml_approx ] ) is @xmath93 ; the earlier samples are discarded .",
    "an smc approximation of @xmath94    ( * ? ? ?",
    "* theorem 1 ) shows that the mse of the mlsmc method is upper - bounded by @xmath95 @xmath96 where @xmath97 is the supremum norm and @xmath98 , @xmath99 are constants that do not depend upon @xmath100 .",
    "@xcite use the following assumptions , which we will consider in the analysis of mlsmc in the abc context .",
    "note that these assumptions have been weakened in @xcite .",
    "[ hyp : a ] there exist @xmath101 such that @xmath102    [ hyp : b ] there exists a @xmath103 such that for any @xmath104 , @xmath105 , @xmath106 : @xmath107    one can see , in ( a[hyp : b ] ) , that the mcmc kernel must mix uniformly well w.r.t .  the level indicator .",
    "if the mcmc kernel cost is @xmath108 ( i.e.  independent of @xmath11 ) then one can iterate to ( e.g. ) @xmath40 at a given level @xmath50 .",
    "that is , as one expects the complexity of the posterior to increase as @xmath11 falls , one must put in more effort to efficiently sample the posterior and achieve a uniform mixing rate . in other situations ,",
    "the cost of the mcmc step may directly depend upon @xmath109 , in order for the mixing rate to be uniform in @xmath50 .      in order to understand the utility of applying mlsmc for abc",
    ", we must understand the mse and in particular , terms such as @xmath110 we show that under fairly general assumptions , that this expression can be controlled in terms of @xmath111 .",
    "it is supposed that @xmath112 and @xmath113 ( for some @xmath114 be given ) are compact and we take : @xmath115 this is a quite general context , as we do not assume anything more about @xmath116 and @xmath117 .",
    "it is supposed that @xmath118 , which is reasonable ( e.g.  @xmath119 ) .    in this scenario , it is straightforward to show that for any @xmath120 @xmath121 for any fixed @xmath122 where @xmath123 do not depend on @xmath50 ; this verifies ( a[hyp : a ] ) .",
    "we have the following result , the proof of which , is in the appendix :    [ prop : norm_control ] let @xmath114 be given .",
    "then there exists a @xmath124 such that for any @xmath35 : @xmath125    suppose that the cost of one independent sample from @xmath126 is @xmath127 and that our mcmc kernel also costs the same .",
    "given @xmath29 , and supposing the bias of @xmath128 , @xmath119 @xmath58 the procedure for finding the optimal @xmath129 to minimize the cost @xmath130 so that the variance is @xmath31 is as in @xcite . the idea there is to just consider the term @xmath131 in the variance part of the bound .",
    "the constrained optimization is then as in @xcite .",
    "we then check that the additional term in is @xmath31 or smaller . therefore ,",
    "setting , @xmath132 , the variance part of is @xmath133.\\ ] ] as shown in ( * ? ? ? * section 3.3 ) if @xmath134 then the additional term is @xmath31 .",
    "so therefore , the conclusion is as in section [ sec : ml_abc ] ( the cost is the same as discussed there ) : for a given level of mse , the mlsmc method for abc has a lower cost than i.i.d .  sampling from @xmath32 .",
    "the main issue is to determine the bias , which often needs to be model specific ; we give an example where this is possible .",
    "we consider a state - space model .",
    "let @xmath135 and @xmath136 , where we suppose @xmath137 are compact subsets of a power of the real - line . in a state - space model",
    ", we can write : @xmath138 where @xmath139 is the joint density of the random variables @xmath140 , @xmath141 is a probability density on @xmath142 , for any @xmath143 , @xmath144 ( resp .",
    "@xmath145 ) is a probability density on @xmath146 ( resp  @xmath142 ) .",
    "we are interested in the posterior : @xmath147 if @xmath148 and @xmath149 are intractable in some way , but can be sampled ( although this is not a requirement - see @xcite and the references therein ) , then an abc approximation is : @xmath150 with @xmath151 .",
    "let @xmath152 be bounded and measurable and @xmath153 .",
    "then , under the assumptions in @xcite @xmath154(\\varphi)| \\leq c\\|\\vartheta\\|_{\\infty}\\epsilon_l\\ ] ] where @xmath155 depends linearly on @xmath156 , so that the bias assumption of ( a ) is satisfied with @xmath157 for additive functionals .",
    "suppose one uses a single site gibbs sampler as the mcmc kernel .",
    "let @xmath35 and for a vector @xmath158 set @xmath159 be all the elements except the @xmath160 , @xmath161 , then for each @xmath162 sampling is performed from @xmath163 with the case @xmath164 @xmath165 it is simple to show that ( a[hyp : b ] ) is satisfied ( the constants depend upon @xmath156 ) .",
    "that is , that writing the density of the kernels as @xmath166 it can be shown that @xmath167 for any fixed @xmath168 and @xmath155 is independent of @xmath50 .",
    "moreover , if one samples from the full conditionals using rejection sampling with proposal when @xmath162 ( resp .",
    "@xmath164 ) @xmath169 ( resp .",
    "@xmath170 ) , we have the following result , whose proof is in the appendix :    [ prop : cost ] the expected cost of one iteration of the above gibbs sampler is @xmath171 .    in this example for a given level of mse , the mlsmc method for abc",
    "has a lower cost than i.i.d .",
    "sampling from @xmath32 as the associated ( exact independent ) rejection sampling cost is @xmath172 and the cost of sampling @xmath43 is @xmath108 .",
    "we now consider some simulations in the context of the example in section [ sec : example ] . in this case ,",
    "@xmath173 and we take : @xmath174 where @xmath175 is a gaussian distribution of mean @xmath141 and variance @xmath176 , with @xmath177 and both @xmath178 given constants . the abc approximation is taken as in section [ sec : example ] equation with kernel as in . in this scenario , there is of course no reason to use abc methods , however , one can compute the exact value of ( for instance ) @xmath179 $ ] exactly , which allows us to compute accurate mses .",
    "the data are generated from the model .",
    "we will compare the mlsmc method of this article to an smc sampler ( such as in @xcite with no adaptation ) that has approximately the same computational cost .",
    "by smc sampler , we simply mean that the number of samples used at each time step of the smc algorithm is the same and only the samples which approximate @xmath32 are used to estimate expectations w.r.t .  this distribution .",
    "we will consider the estimate of @xmath180 as noted above , if @xmath181 then one knows this value exactly .",
    "we set @xmath182 and @xmath183 and consider the cases @xmath184 .",
    "the mcmc kernel adopted is a single - site metropolis - hastings kernel with proposals as in section [ sec : example ] .",
    "that is , for @xmath162 ( resp .",
    "@xmath164 ) @xmath169 ( resp .",
    "@xmath170 ) . in the mlsmc sampler , we set @xmath185 with @xmath186 variable across examples - 6 different values are run",
    ". the smc sampler is run so that the computational run - time is almost the same .",
    "we repeat our simulations 10 times .",
    "the results are given in figures [ fig : t10]-[fig : t25 ] .",
    "figures [ fig : t10]-[fig : t25 ] show that for the scenario under study , the mlsmc sampler out - performs the standard smc sampler approach , as is also shown in @xcite .",
    "even though some of the mathematical assumptions that are made in ( * ? ? ?",
    "* theorem 1 ) are violated , the predicted improvement at almost no extra coding effort is seen in practice .",
    ".,scaledwidth=70.0%,height=226 ]    .,scaledwidth=70.0%,height=226 ]      we consider the stochastic volatility model ( svm ) given by @xmath187 where @xmath188 are the mean - corrected returns and @xmath189 denotes a stable distribution with location parameter @xmath190 , scale parameter @xmath191 , asymmetry parameter @xmath192 and skewness parameter @xmath193 .",
    "we set @xmath194 and @xmath195 as in @xcite . to guarantee stationarity of the latent log - volatility @xmath196",
    ", we assume that @xmath197 .",
    "we assign priors @xmath198 , @xmath199 on @xmath200 and @xmath201 .",
    "note @xmath202 is an inverse gamma distribution with mean @xmath203 and infinite variance .",
    "the abc approximation is taken as in section [ sec : example ] equation with kernel as in .",
    "we use the daily index of the s&p 500 index between 1 january 20112014 february 2013 ( 533 data points ) .",
    "the dataset can be obtained from http://ichart.finance.yahoo.com .",
    "we first estimate the value of @xmath204 using the mlsmc algorithm with @xmath205 , and then we compare the mlsmc sampler with the smc sampler with @xmath206 as in examples for the linear gaussian state - space model .",
    "we again set @xmath207 and @xmath208 .",
    "for the mcmc kernel , we adapt a single - site metropolis - hastings kernel with proposals as in @xcite .",
    "the results , when estimating the same functional as for the linear gaussian model , can be found in figure [ fig : svm ] .",
    "the figure shows as for the previous example that the mlsmc procedure is out - performing using smc , in the sense that the mse for a given cost is lower for the former approach .",
    "in this article we have considered the development of the mlmc method in the context of abc .",
    "several extensions of this work are possible .",
    "the first is that , it is well - known that the sequence of @xmath11 can be set on the fly , using an adaptive smc method .",
    "it is then of interest to see if mlsmc has a benefit from a theoretical perspective ( see e.g.  @xcite for an analysis of adaptive smc ) .",
    "the second is the consideration of the possible improvement of mlsmc when the summary statistics of abc are not sufficient , as they have been in this paper .",
    "aj , sj , dn & cs were all supported by grant number r-069 - 000 - 074 - 646 , operations research cluster funding , nus .",
    "we give the proof in the case @xmath209 ; the general case is the same , except with some minor complications in notations .",
    "we have @xmath210 we will deal with the two expressions on the r.h.s .  of separately . throughout @xmath155",
    "is a constant that does not depend on a level index @xmath50 but whose value may change upon appearance .",
    "we will show that @xmath211 and that @xmath212 .",
    "we start with the first task .",
    "we have @xmath213.\\ ] ] where we have set @xmath214 .",
    "then elementary calculations yield @xmath215 now as @xmath216 and as @xmath217 @xmath218 we have @xmath219 now @xmath220 now @xmath221 so that @xmath222 we now will show that @xmath223 is lower bounded uniformly in @xmath50 which will show that @xmath212 . @xmath224 then @xmath225 as @xmath226 and @xmath227 .",
    "so we have that @xmath228 and @xmath229 combining and yields @xmath230      clearly @xmath231 we first deal with the numerator on the r.h.s . :",
    "@xmath232   f(u|\\theta)\\pi(\\theta)d(\\theta , u)\\\\ & = & \\int_e   \\big[\\frac{\\epsilon_{l-1}^2\\epsilon_{l}^2(\\epsilon_{l}^2-\\epsilon_{l-1}^2)}{(c+\\epsilon_{l-1}^2)(c+\\epsilon_{l}^2)}\\big ]   f(u|\\theta)\\pi(\\theta)d(\\theta , u).\\end{aligned}\\ ] ] as @xmath233 we have @xmath234 therefore one has @xmath235 using almost the same calculation as for showing @xmath228 , we have @xmath236 and so @xmath237        we will show that the expected cost of sampling a given full - conditional is @xmath238 . throughout @xmath155 is a constant that does not depend on a level index @xmath50 nor @xmath239 but whose value may change upon appearance .    it is easily shown that @xmath240 and thus that the probability of accepting , in the rejection scheme is @xmath241 the expected number of simulations is then the inverse . clearly @xmath242 and as @xmath243 is compact @xmath244 so that the expected number of simulations to sample the full conditional is at most @xmath245 which completes the proof ."
  ],
  "abstract_text": [
    "<S> in the following article we consider approximate bayesian computation ( abc ) inference . we introduce a method for numerically approximating abc posteriors using the multilevel monte carlo ( mlmc ) . </S>",
    "<S> a sequential monte carlo version of the approach is developed and it is shown under some assumptions that for a given level of mean square error , this method for abc has a lower cost than i.i.d </S>",
    "<S> .  sampling from the most accurate abc approximation . </S>",
    "<S> several numerical examples are given . </S>",
    "<S> + * key words : * approximate bayesian computation , multilevel monte carlo , sequential monte carlo .    * multilevel monte carlo in approximate bayesian computation *    by ajay jasr@xmath0 , seongil j@xmath1 , david not@xmath2 , christine shoemake@xmath3 & raul tempon@xmath4    @xmath5department of statistics & applied probability & operations research cluster , national university of singapore , singapore , 117546 , sg . </S>",
    "<S> + e-mail:`staja@nus.edu.sg , joseongil@gmail.com , standj@nus.edu.sg `    @xmath6department of civil & environmental engineering & operations research cluster , national university of singapore , singapore , 119260 , sg . </S>",
    "<S> + e-mail:`shoemaker@nus.edu.sg `    @xmath7center for uncertainty quantification in computational science & engineering , king abdullah university of science and technology , thuwal , 23955 - 6900 , ksa . </S>",
    "<S> + e-mail:`raul.tempone@kaust.edu.sa ` </S>"
  ]
}