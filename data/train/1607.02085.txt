{
  "article_text": [
    "classification is a basic machine learning task .",
    "conventional classification algorithms operate on numerical vectors . over the past decade ,",
    "such algorithms have been extended for classifying data with more complex structure , e.g. time series data  @xcite . in many real - world applications ,",
    "time series data can be irregularly and/or sparsely sampled .",
    "this poses a challenge for time series classification . on the other hand ,",
    "the data - generating processes in such applications could be well understood and mechanistic models accounting for the data structure could have been developed in the form of dynamical systems . using such mechanistic models in time",
    "series classification would allow for natural incorporation of the domain experts knowledge . in this setting , time series data can be seen as partial observations of the underlying dynamical system and the machine learning task becomes classification of partially observed dynamical systems . in this work ,",
    "we formulate and validate a general framework for such classification tasks .",
    "@xcite distinguish two major conventional approaches to time series classification , in particular , feature - based and distance - based approaches .",
    "feature based approaches construct discriminative features on the time series data . these can be local patterns ( i.e. short subsequences )  @xcite , or global ones resulting from time - frequency and wavelet analysis  @xcite .",
    "distance - based methods classify time series based on a distance ( e.g. euclidean ) between time series pairs .",
    "this approach is not directly applicable for the time series of variable length . to circumvent this problem ,",
    "`` dynamical time wrapping '' ( dtw ) methods have been developed . in dtw",
    "two time series are aligned according some criteria so that a distance can be calculated  @xcite . however , such approaches are not applicable for classifying irregularly and sparsely sampled time series .",
    "more importantly , they do not utilise the available experts knowledge about the underlying processes . alternatively",
    ", model - based approaches have also been adopted for time series classification , e.g. hidden markov model ( hmm)-based approaches for biological sequence classification  @xcite . in those approaches ,",
    "a prototypical time series model is constructed for each time series class .",
    "for example , if the prototypical model is probabilistic , the class label for a new time series is given by the model with the highest likelihood for that time series ( or the highest posterior probability , if class priors are available ) .",
    "however , a single model may not adequately represent all time series in the given class . from this point of view , it is more desirable to represent time series by individual models . in this setting ,",
    "the classifier employed classifies individual models ( that stand for individual time series ) and thus operates in the model space .",
    "we refer to this approach as  _ learning in the model space _  ( lims ) and",
    "have adopted it for classifying partially observed dynamical systems .    in most of lims methods for time series classification ,",
    "given a time series , a point estimate of model parameter is used to represent that time series .",
    "such estimates could be used directly as feature vectors . in this case ,",
    "any vector - based classifier could be employed for the task .",
    "for example , @xcite employ dynamic causal model ( dcm )  @xcite to represent individual fmri data from each participant .",
    "the maximum - a - posterior estimates of model parameter were then used as feature vectors for classifying dcms . in  @xcite , a reservoir computation model was used as a generic non - parametric model to represent non - linear time series data .",
    "high dimensional dynamical reservoir was fixed and individual time series were represented by the corresponding read - out mappings from the generic dynamic reservoir .",
    "the estimated read - out parameters were then used as feature vector for time series classification . in both approaches ,",
    "their respective parameter space is considered as a linear metric space and its global metric tensor can be learned in a supervised manner , so as to improve the classification performance .",
    "other lims approaches use directly model distances ( e.g. geodesic on the model manifold ) instead of global metric in the parameter space .",
    "such approaches treat the parameter space as a non - linear metric space and learn metric on the underlying manifold .",
    "such non - linear structure could be induced by the intrinsic properties of the underlying processes , or by the constraints imposed on the models ( e.g. stability of autoregressive ( ar ) models ) . to compute geodesic distances",
    ", one can first reconstruct the underlying metric tensor field in the parameter space .",
    "@xcite and  @xcite propose a general framework based on pullback metric to learn discriminative metric tensors in the space of linear dynamical systems ( lds ) and hidden markov models ( hmm ) , respectively",
    ". the manifold structure in the parameter space is induced by stability constraints on the lds parameters , or by normalisation constraints on the hmm parameters .",
    "yet another class of lims approaches is formulated in the framework of kernel machines .",
    "although the employed kernels do nt fully recover the underlying metric tensor field , they still define useful distance functions that account for the underlying non - linear structure in the parameter space .",
    "typically , the kernels used have been developed to operate on probability distributions / measures , for example , kernels based on ( information - theoretic ) divergence functions between two distributions  ( e.g. kl divergence , * ? ? ?",
    "in particular , @xcite used kl - kernels on vector auto - regressive ( var ) models to classify dynamic textures in video sequence analysis .",
    "@xcite proposed the probability product kernel ( ppk ) , which can be seen as a dot product in the function space of two probability distributions .",
    "bhattacharyya kernels , a special case of ppk , are related to the hellinger distance between two functions . in  @xcite ppk kernels",
    "were used to classify both lds and hmm . computation of kl and ppk kernels is analytically tractable only for simple classes of dynamical systems , such lds and hmm . in general , their computation could be very expensive , since it can involve infinite - dimensional integral over all possible state trajectories ; binet - cauchy kernels could be seen as a counterpart of ppk kernel for deterministic dynamical systems  @xcite .",
    "in contrast to ppk , binet - cauchy kernels are defined as a dot product in the trajectory space .",
    "for deterministic systems , their trajectories are completely determined by their model parameters and the initial states .    finally , we mention two kernels used in the literature for model - based time series classification that fall outside the lims framework since no individual models are inferred from individual time series . fisher kernel proposed by @xcite uses a single fixed time series model .",
    "each time series is then represented by a tangent vector in the tangent space of that model .",
    "ar kernel proposed by @xcite is a marginalisation kernel applied to ar models  @xcite .",
    "each time series is represented by a ( infinite - dimensional ) `` profile vector '' - ar likelihood for a set of model parameters , given that time series .",
    "the kernel between two time series is the dot product of the two corresponding profile functions , weighted by a prior distribution over the ar parameters .    in this paper",
    ", we present a general framework for classifying partially observed dynamical systems based on lims",
    ". one key ingredient of this framework is that given a class of parameterised dynamical system models , we represent each partially observed dynamical system ( i.e. each time series ) by a posterior distribution over models . in contrast to all model - based approaches surveyed above ,",
    "our approach takes into account the model uncertainty around each individual model .",
    "this is of particular relevance for the sparsely sampled time series as it could give rise to a considerable amount of uncertainty around the inferred model . to classify those posterior distributions",
    ", one could employ any classifier that operates on probability distributions , for example , the classifier based on probability product kernel .",
    "we also use a distributional kernel induced by the kernel mean embedding ( kme )  @xcite .",
    "this embedding maps each distribution onto the hilbert space induced by a chosen kernel  @xcite . note that the ppk kernels here are defined on two distributions over model parameter whereas the ppk kernels in  @xcite are defined on two prior measures over system trajectories .",
    "recall that the latter encodes information about the intrinsic structure in the model space . in our approach , however , this information is encoded in the posterior distributions .",
    "this means that in both cases .",
    "the classifiers do utilise the intrinsic structure in the model space for classification .",
    "the rest of this paper is organised as follows .",
    "we first formulate our framework in section  [ sect : framework ] .",
    "section  [ sect : implementation ] presents an implementation of this framework by means of kernel logistic regression ( klr ) . in  section  [",
    "sect : relatedworks ] we further establish connections between our classifier and two other related state - of - the - art classifiers .",
    "section  [ sect : testbeds ] introduces two classes of dynamical systems used to validate our framework and the experiments are detailed in section  [ sect : experiments ] . finally , section  [ sect : conclusion ] summarises key research findings .",
    "first , a classification task is formulated as follows : suppose we have @xmath5 examples in the form of @xmath5 labelled univariate or multivariate time series , denoted by @xmath6 where @xmath7 denotes the @xmath8-th time series and @xmath9 represents its binary label . as we do not assume that all time series are collected on a fixed , regular time grid , each time series @xmath7 is accompanied with a sequence of observation times @xmath10 at which the observations @xmath11 are collected .",
    "hence the @xmath8-th time series is jointly represented by @xmath12 with @xmath13 and @xmath14 .",
    "note that the length of time series @xmath15 can vary across examples .",
    "however , the dimensionality @xmath16 of the observed time series is assumed to be fixed .",
    "the task is to predict a label for a new time series @xmath17 of length @xmath18 . due to variability of observation times and length of the training time series , direct application of a vector - based classifier",
    "would not be suitable .",
    "note that if the training time series were long enough and `` suitably '' sampled , one could represent each time series through e.g. a vector of fourier or wavelet coefficients .",
    "however , we do not wish to impose any such restrictions and in particular , we are interested in cases of short , sparsely and irregularly sampled time series .",
    "we propose to represent time series by a set of individual time series models from a given model class .",
    "in particular , since the observed time series can be noisy , short and irregularly sampled , each time series will be represented as the posterior distribution over the models , given the time series itself and model prior .      in our work , a dynamical system approach is adopted to model time series . in other words , we consider a given time series as a ( possibly partial ) observation of some underlying dynamical system from a parametric class of dynamical systems . in the following ,",
    "we first introduce mathematical representation of the class of dynamical systems considered in this work .",
    "next , a model accounting for partial observations is formulated . following this ,",
    "we introduce a bayesian approach for representing partially observed dynamical systems .",
    "a continuous - time deterministic dynamical system can be mathematically represented as a multivariate ordinary differential equation ( ode ) : @xmath19 where @xmath20 denotes @xmath21-dimensional state vector at time @xmath22 .",
    "the mapping @xmath23 specifies the dynamics of this system by defining the functional relation between state @xmath24 and drift @xmath25 at time @xmath22 .",
    "this mapping is parameterised by @xmath26 .",
    "note that model parameter @xmath26 includes the initial state @xmath27 , unless @xmath27 is assumed to be known .",
    "a stochastic dynamical system can be considered as an ode driven by a multivariate random process parameterized by covariance matrix @xmath28 .",
    "each component of this process is a standard univariate brownian motion scaled by square root of the corresponding diagonal term of @xmath28 .",
    "its covariance structure at @xmath22 is specified by the non - diagonal terms .",
    "it is equivalent to adding gaussian noise to the drift .",
    "mathematically , this system can be represented by a multivariate stochastic differential equation ( sde ) : @xmath29 where the vector @xmath30 collects the @xmath21 independent standard brownian motions .",
    "a sde s initial condition is specified by a probability distribution over @xmath27 , which is often assumed to be a gaussian distribution with mean @xmath31 and covariance matrix @xmath32 . as in odes ,",
    "the initial condition specification is part of the model parameters @xmath26 .",
    "all model parameters are collected in vector @xmath33 , i.e. for odes @xmath34 and for sdes @xmath35 .",
    "observations @xmath36 , @xmath37 from the underlying dynamical system s trajectory @xmath24 are obtained through a measurement function @xmath38 : @xmath39 where @xmath40 denotes observation noise at time @xmath41 . in general",
    ", @xmath38 can be a parametric function with unknown parameters .",
    "frequently , @xmath38 represents a set of indicator functions which specify a subset of state variables that are directly observed . for clarity in formulating our general framework",
    ", we assume @xmath38 to be an identity function .",
    "observation noise @xmath42 is often assumed to be i.i.d .",
    "gaussian noise with zero mean and error covariance matrix @xmath43 .",
    "@xmath43 can be determined form prior knowledge or learned from the data .    in",
    "the _ learning in the model space _ ( lims ) framework , the observed time series are represented through models parametrized via @xmath33 . given a time series @xmath44 , a maximum likelihood ( ml ) estimate of @xmath33",
    "can be obtained by maximizing the likelihood function @xmath45 for an ode system and @xmath46 \\label{sde_marginal_likelihood}\\ ] ] for an sde system .",
    "however , this approach ignores uncertainty around the model estimate . in cases where only noisy and/or sparse data are available ,",
    "any point estimate of the model parameter is not a sufficient representation of the partially observed dynamical system . instead",
    ", the posterior distribution of @xmath33 should be used , @xmath47 where @xmath48 is the prior over @xmath33 . in most cases ,",
    "computation of the normalizing term is analytically not tractable and the posterior has to be approximated by using a finite grid in the parameter space or by sampling / variational methods  @xcite .",
    "formulation of a classifier for partially observed dynamical systems in the lims framework depends on the way the underlying systems are represented .",
    "we consider two different options :    1 .",
    "representation through data @xmath49 .",
    "the resulting classifier operates directly and solely on the data .",
    "a probabilistic classifier of this kind is formulated by defining the conditional probability @xmath50 that is used to predict label @xmath51 in a probabilistic manner .",
    "note that this classifier completely ignores the underlying model and thus is of disadvantage if the underlying model structure is known ; 2 .",
    "representation via posterior distributions over models , @xmath52 .",
    "thus , the counterpart of @xmath53 is @xmath54 .",
    "the resulting classifier actually operates in the space of posterior distributions rather than in the model or data space .",
    "such posterior distributions not only encode intrinsic information about the underlying dynamical system but also quantitatively represent the uncertainty that arises due to finite number of ( possibly irregularly sampled ) observations and observation noise .",
    "the posterior @xmath55 is shaped by the metric structure in the model space .",
    "to define our classifier , we first consider the classifier that operates on data .",
    "recall that data @xmath56 is assumed to be sampled from a hidden trajectory @xmath24 generated by model instance @xmath57 ( a model from model class @xmath23 with an unknown model parameter @xmath33 ) . to take this additional knowledge into account",
    ", we express @xmath53 as @xmath58 where the hidden trajectory @xmath24 and unknown model parameter @xmath33 are both marginalised out .",
    "the density @xmath59 is defined with respect to the standard brownian motion and @xmath60 represents path integral over trajectories .",
    "the above formulation implies a classifier @xmath61 which utilises the model instance @xmath33 , the trajectory @xmath24 generated by @xmath57 , and noisy observations @xmath17 assumed to be sampled from @xmath24 .",
    "given @xmath57 , @xmath24 is either specified deterministically ( in the case of odes ) , or is driven by a standard brownian motion ( in the case of sdes ) . assuming that no additional relevant information for the classification task could be extracted from observation noise or observation times ( the noise and observation times processes are not conditional on the class label )",
    ", all the relevant information in @xmath62 for the class label prediction can be collapsed into the model @xmath33 .",
    "consequently , we replace @xmath61 with @xmath63 .",
    "( [ marginalisation ] ) now reads : @xmath64 \\nonumber \\\\                    & = &   q(c | \\pi ) .",
    "\\label{our_classifier}\\end{aligned}\\ ] ] note that the classifier @xmath65 operates on posterior distributions @xmath66 , but is formulated based on classifier @xmath63 operating in the model space .    in the following ,",
    "we define the theoretical risk for @xmath65 .",
    "generally , theoretical risk for a classifier is defined through a joint distribution over the input / label spaces and a loss function quantifying the cost of miss - classification . in our case ,",
    "the joint distribution of @xmath67 is written as @xmath68 , where @xmath69 denotes a distribution over distributions ( random measure ) .",
    "the loss function we employ is the negative log - likelihood , @xmath70 .",
    "the theoretical risk of @xmath65 can be written as @xmath71      \\bigg ] .",
    "\\label{eq : risk}\\ ] ]    it is difficult to formulate @xmath69 as a parametric generative model . for the classifier @xmath65 , however , based on ( [ our_classifier ] )",
    ", we have @xmath72 .",
    "the theoretical risk for @xmath53 is given by @xmath73   \\big]\\ ] ] where @xmath74 a parametric formulation of the above theoretical risk is obtained by adopting _",
    "( i ) _ a parametric noise model for @xmath75 ; _ ( ii ) _ a parametric dynamical noise model @xmath76 ; _ ( iii ) _ a prior for the covariance of the observational noise @xmath77 ; _ ( iv ) _ a point process for @xmath78 in the observation window since it is defined with respect to the standard poisson process . ] and _ ( v ) _ an appropriate model for @xmath79 .",
    "for partially observed non - linear dynamical systems the computation of posterior distributions is analytically not tractable .",
    "therefore , the expectation over @xmath33 w.r.t .",
    "@xmath55 in eq.[our_classifier ] can only be computed via approximation .",
    "there exist two principled approximation strategies that have the required convergence properties : approximation by sampling and finite - grid approximation .    in the first approach ( approximation by sampling ) ,",
    "the posterior distribution is approximated by @xmath80 where @xmath81 , ... , @xmath82 are @xmath83 parameter vectors which are independently sampled from @xmath55 .",
    "accordingly , the classifier defined in eq .",
    "[ our_classifier ] is approximated by @xmath84 as the posterior distribution is only known up to normalising constant , mcmc algorithms are the most efficient sampling method .    in the second approach ( finite - grid approximation )",
    ", one could first compute the unnormalised posterior density ( that is , the product of normalised prior and likelihood densities ) over a finite grid approximating the parameter space and then normalise those values into a multinomial distribution approximating the posterior density .",
    "we denote this grid and the multinomial posterior probabilities on the grid by @xmath85 and @xmath86 respectively .",
    "the resulting approximate classifier is given by @xmath87 for sde , however , the marginal likelihood for each parameter vector on the grid is analytically not tractable and thus the likelihood is not normalised . to solve this problem at low computational cost",
    ", we employ the variational gaussian process approximation method for computing the approximate marginal likelihood  @xcite .      in the following , we first briefly introduce kernel logistic regression ( klr ) as a ( binary ) classifier for vectors ( e.g. model parameter @xmath33 ) .",
    "we then present an extension of klr for distributions so that the classifier can be directly applied to posteriors @xmath55 .",
    "a binary klr classifier operating on @xmath33s is defined via @xmath88 where @xmath89 denotes a sigmoid function is defined as @xmath90 . ] , @xmath91 is @xmath92-dimensional classifier parameter , and @xmath93 represents a ( non - linear ) mapping of @xmath94-dimensional model parameter vector @xmath33 to @xmath92-dimensional feature space : @xmath95^{\\intercal},\\ ] ] where @xmath96 represents a kernel function operating on the the space of model parameter vectors and @xmath97 denotes the set of model parameters for constructing this feature map . in this work , we adopt a gaussian kernel , @xmath98 where @xmath99 is the euclidean norm and @xmath100 is a scale parameter .    for learning the classifier parameter @xmath91 , a training set of @xmath5 labelled model parameters @xmath101 , @xmath102 , would be used to obtain the maximum likelihood estimate ( mle ) of @xmath91 : @xmath103 this is equivalent to minimizing the cross entropy error @xmath104 for a gradient - based minimisation of @xmath105 w.r.t .",
    "@xmath91 , the gradient is computed as @xmath106    for a classifier that operates on the posterior distributions and a training set given as @xmath107 the classifier parameter is obtained by minimizing the cross entropy error @xmath108 the approximate cross - entropy error is computed by @xmath109 where @xmath110 denotes the normalised posterior weight on the @xmath111-th grid point for the @xmath8-th posterior .",
    "the corresponding gradient is given by @xmath112\\ ] ] where @xmath113 , @xmath114 note that @xmath115 is a @xmath92-dimensional vector whose @xmath116-th component is given by @xmath117 .",
    "the two grids on the parameter space ,",
    "in literature , most distributional classifiers combine an existing kernel - based classfier , such as svm , with a kernel that is defined on the space of distributions .",
    "an example of such a kernel is the so - called probability product kernel  @xcite , @xmath118 where @xmath119 and @xmath120 are two distributions over a metric space @xmath121 and @xmath122 is a tempering parameter . in recent literature ,",
    "another kernel on distributions has been introduced based on hilbert space embedding  @xcite . given a universal kernel @xmath123 , there exists an injective mapping from distribution space @xmath124 to feature space , @xmath125 this mapping is called kernel mean embedding ( kme ) .",
    "as the embedding is bijective , no information encoded in the probability distribution is lost through the mapping .",
    "the mapping in turn defines a kernel on probability distributions , @xmath126 : @xmath127    we compare these two distributional classifiers ( one based on @xmath128 , the other one based on @xmath129 ) with our classifier in terms of their predictive class distributions , given a test input ( distribution ) @xmath66 :    * probabilistic classifier based on probability product kernel ( ppk ) : @xmath130   { \\mathop{}\\!\\mathrm{d}}{\\boldsymbol\\theta } \\bigg )   \\label{eq : f_ppk } \\\\   & = &   \\zeta \\bigg (    \\mathbb{e}_{\\pi }   \\bigg [ \\upsilon_{\\mbox{\\tiny ppk}}(\\boldsymbol\\theta ; \\v ) \\bigg ]    \\bigg ) , \\label{eq : c_ppk } \\ ] ] where @xmath131 denotes the function to be learnt ( by adjusting the free parameter @xmath132 ) for classifying distributions ; * probabilistic classifier based on kernel mean embedding ( kme ) : @xmath133    \\cdot k(\\boldsymbol\\theta , \\boldsymbol\\eta ) { \\mathop{}\\!\\mathrm{d}}{\\boldsymbol\\eta } \\bigg ]   { \\mathop{}\\!\\mathrm{d}}{\\boldsymbol\\theta } \\bigg ) \\nonumber \\\\ & = & \\zeta \\bigg (   \\int_{\\boldsymbol\\theta \\in { \\vartheta } } \\pi(\\boldsymbol\\theta ) \\cdot   \\bigg [     \\sum_{i = 1}^l v_i \\cdot   \\int_{\\boldsymbol\\eta \\in { \\vartheta } }",
    "\\pi_i(\\boldsymbol\\eta ) \\cdot k(\\boldsymbol\\theta , \\boldsymbol\\eta ) { \\mathop{}\\!\\mathrm{d}}{\\boldsymbol\\eta } \\bigg ]   { \\mathop{}\\!\\mathrm{d}}{\\boldsymbol\\theta } \\bigg ) \\\\ & = & \\zeta \\bigg (   \\int_{\\boldsymbol\\theta \\in { \\vartheta } } \\pi(\\boldsymbol\\theta ) \\cdot   \\underbrace { \\bigg [     \\sum_{i = 1}^l v_i \\cdot   \\tilde \\pi_i(\\boldsymbol\\theta ) \\bigg]}_{\\upsilon_{\\mbox{\\tiny kme}}(\\boldsymbol\\theta ; \\v ) } { \\mathop{}\\!\\mathrm{d}}{\\boldsymbol\\theta } \\bigg ) \\label{eq : f_mke } \\\\ & = & \\zeta \\bigg (    \\mathbb{e}_{\\pi }   \\bigg [ \\upsilon_{\\mbox{\\tiny kme}}(\\boldsymbol\\theta ; \\v ) \\bigg ]    \\bigg ) \\label{eq : c_mke}\\end{aligned}\\ ] ] where @xmath134 are kernel - smoothed posteriors @xmath135 and @xmath136 is the function to be learnt ; * probabilistic classifier proposed in this work ( eq .  [ our_classifier ] ):",
    "@xmath137   \\label{eq : c_kr}\\end{aligned}\\ ] ] where @xmath138 is learnt by adjusting the free parameter @xmath91 .    to see a deeper connection between the three classifiers above ,",
    "consider first the usual setting of kernel logistic regression , @xmath139 this can be interpreted as follows : the model imposes a smooth field ( natural parameter of bernoulli distribution ) @xmath140 over the inputs @xmath33 .",
    "the field assigns to each input a real number that expresses the ` strength ' with which that particular input wants to belong to class + 1 .",
    "pushing the field through the link function @xmath141 creates a new field @xmath142 over the inputs , assigning to each @xmath33 the probability with which it belongs to class + 1 .    in case",
    "our inputs are not individual models @xmath33 , but ( posterior ) distributions @xmath66 over the models , the classifier ( [ eq : kr ] ) can be generalized in two ways :    1 .",
    "use the posterior distribution @xmath66 to average over individual natural parameters @xmath140 to create the overall mean natural parameter @xmath143 $ ] .",
    "this can then be passed through the link function @xmath141 to calculate the class + 1 probability for @xmath66 , @xmath144)$ ] .",
    "this scenario can be described as forming an ( infinite ) ensemble to form the overall opinion about the strength of @xmath66 belonging to class + 1 and only then turning it into the class probability .",
    "this option is taken by the classifiers based on probability product kernel and kernel mean embedding , ( [ eq : c_ppk ] ) and ( [ eq : c_mke ] ) , respectively 2 .",
    "use the posterior distribution @xmath66 to average over individual class probabilities @xmath145 to form the overall class probability @xmath146 $ ] of @xmath66 .",
    "this corresponds to creating an ensemble of probabilistic classifiers @xmath147 acting on individual models @xmath33 , as done by the proposed classifier ( see ( [ eq : c_kr ] ) ) .",
    "one can view the latter approach @xmath148 $ ] as a regularization of the former one @xmath144)$ ] .",
    "loosely speaking , when collecting ensemble votes to form an opinion about the probability of class + 1 given @xmath66 , @xmath146 $ ] ignores the ( potentially huge ) differences between individual natural parameters @xmath149 giving negligible differences in the probabilities @xmath150 because of the saturation regions at both extremes of the link function @xmath141 .",
    "this effectively collapses input regions of models @xmath33 with high positive field values into a single high class probability region .",
    "analogously , regions of models @xmath33 with low negative values will be identified into a low class probability region .",
    "another point of view is to compare the models for the field @xmath140 utilized in the three classifiers . in all cases the fields",
    "are modelled as linear combinations of basis functions .",
    "because kernels of the classifiers based on probability product kernel and kernel mean embedding operate on full distributions , the basis functions for modelling the field @xmath140 are the ( possibly tempered ) training posterior distributions @xmath151 or their kernel - smoothed versions @xmath134 , respectively ( see ( [ eq : f_ppk ] ) and ( [ eq : f_mke ] ) . in contrast",
    ", the proposed classifier ( [ eq : c_kr ] ) models the field @xmath140 in a less constrained framework of kernel regression as a linear combination of kernel basis functions @xmath152 ( see ( [ eq : f_kr ] ) ) .",
    "in particular , no assumption is made that the field should lie in the span of the training distributions @xmath151 or their smoothed versions @xmath134 .",
    "in this work , we validate our general framework using two example dynamical systems : gonadotropin - releasing hormone signalling model ( gnrh )  @xcite and stochastic double - well systems ( sdw )  @xcite .",
    "gnrh is an example of ordinary differential equation ( ode ) systems and sdw is an example of stochastic differential equation ( sde ) .",
    "gnrh is also an example of biological pathway / compartment model .      , @xmath153 , and @xmath154 ) .",
    "the signalling pathway of these models is highlighted by the flow of red , blue and green arrows , respectively .",
    "each model comprises of gnrh signal as the driving input , gsu as the measurable output , and one to three compartments along its signalling pathway .",
    "right panel : two classes of gnrh signalling models : ( 1 ) class of normal subjects with bell - shaped frequency - response relationship ( blue diamonds ) and ( 2 ) class of abnormal subjects with simple frequency dependency of response ( red disks ) .",
    "these two classes are separated by two straight lines in the log - log parameter space . , width=226,height=226 ]    , @xmath153 , and @xmath154 ) . the signalling pathway of these models is highlighted by the flow of red , blue and green arrows , respectively .",
    "each model comprises of gnrh signal as the driving input , gsu as the measurable output , and one to three compartments along its signalling pathway .",
    "right panel : two classes of gnrh signalling models : ( 1 ) class of normal subjects with bell - shaped frequency - response relationship ( blue diamonds ) and ( 2 ) class of abnormal subjects with simple frequency dependency of response ( red disks ) .",
    "these two classes are separated by two straight lines in the log - log parameter space .",
    ", width=188,height=226 ]    mathematically , gnrh signalling model is an ode system with 11 state variables .",
    "these state - variables include concentrations of gonadotropin releasing hormones ( [ gnrh ] ) and gonadotropin hormones ( [ gsu ] ) as the driving input and measurable output , respectively , of this model . the remaining state variables can be grouped into three compartments along the signalling pathway : ( 1 ) c1 for gnrh binding process ; ( 2 ) c2 for extracellular signal regulated kinase ( erk ) activation ; and ( 3 ) c3 for transcription factor ( tf ) activation .",
    "we refer to this model as @xmath155 and consider it as the full model in a hierarchy of three nested gnrh signalling models .",
    "this hierarchy is schematically illustrated in figure  [ gnrh_model ] .",
    "we highlight the signalling pathway in _",
    "m1 _ by red arrows . by removing the compartment c2 from the pathway ,",
    "we obtain a two - compartment model denoted by _",
    "m2_. when we further remove c3 from the pathway , _",
    "m2 _ is reduced to _ m3 _ in which gnrh signals directly modulate stimulation of transcriptional activation .",
    "the pathways of _ m2 _ and _ m3 _ are highlighted in figure  [ gnrh_model ] by blue and green arrows , respectively .",
    "gnrh signal is the chemical signal which stimulates the reproductive endocrine system .",
    "this signal is modeled by @xmath156}}{{\\mathop{}\\!\\mathrm{d}}t } = - \\mbox{[gnrh ] } + p_{\\mbox{\\tiny \\bf gnrh } } \\cdot \\big\\ { h\\big(t \\",
    "\\mbox{\\bf \\footnotesize mod } \\",
    "f^{-1 } \\big ) -   h\\big ( ( t \\",
    "\\footnotesize mod } \\",
    "f^{-1})-t_p\\big ) \\big\\ } , \\label{gnrh_input}\\ ] ] where @xmath157 is the gnrh pulse magnitude , @xmath158 is the pulse frequency and @xmath159 is the pulse duration . in this work",
    ", we set @xmath157 to be a constant ( i.e. @xmath157 = 0.1 ) and treat both @xmath158 and @xmath159 as model parameters .    the amount of @xmath160 and @xmath161 , denoted by @xmath162 $ ] and @xmath163 $ ] , are two state variables in c3 which modulate the dynamics of gsu expression as follows : @xmath164}}{{\\mathop{}\\!\\mathrm{d}}t } = { k_{\\mbox{complex } } } \\cdot \\left ( \\frac { \\frac { { [ \\mbox{tf}_1]}}{{k_{d_{\\mbox{\\tiny tf}_1 } } } } \\cdot \\frac { { [ \\mbox{tf}_{2 } ] } } { { k_{d_{\\mbox{\\tiny tf}_2 } } } } \\cdot { [ dna_{\\mbox{tot}}]}^2 } { \\left (   1 + \\frac { { [ \\mbox{tf}_1 ] } } { { k_{d_{\\mbox{\\tiny tf}_1 } } } } + \\frac { { [ \\mbox{tf}_{2 } ] } } { { k_{d_{\\mbox{\\tiny tf}_2 } } } } \\right)^2 } \\right ) -   d_{[\\mbox{gsu } ] } \\cdot { [ \\mbox{gsu } ] } \\label{gnrh_output}\\ ] ] where @xmath165 and @xmath166 are the dissociation constants of @xmath162 $ ] and @xmath167 $ ] , respectively .",
    "they are both considered as model parameters .",
    "the remaining model parameters are set values reported in the literature  @xcite . in summary",
    ", the gnrh signalling model has one observable and four free model parameters .",
    "the observable is gsu and the model parameters are : gnrh pulse frequency @xmath158 , gnrh pulse duration @xmath159 , the dissociation constant of @xmath162 $ ] , @xmath165 , and the dissociation constant of @xmath167 $ ] , @xmath166 .",
    "it is widely accepted that the reproductive system is controlled via gnrh pulse frequency .",
    "this frequency varies under different physiological conditions , affecting the transcription of gsu and secretion of reproductive hormones that are crucial for the physiology of the reproductive system .",
    "gnrh frequency decoding mechanisms vary under normal and pathological conditions , but two main possibilities exist : * ( 1 ) * increasing pulse frequency simply increases output ( gsu ) until a maximal response is maintained with continuous stimulation ( see _ figure 6 panel a _ in  @xcite ) ; and * ( 2 ) * pulsatile stimuli may elicit maximal responses at sub - maximal frequencies , generating bell - shaped frequency - response relationship ( see _ figure 6 panel b _ in  @xcite ) . in this work ,",
    "we utilise these two mechanisms to define two classes of subjects : `` abnormal '' ( mechanism ( 1 ) ) and `` normal '' ( mechanism ( 2 ) ) subjects ) .",
    "as these two classes differ in how they respond to a change in pulse frequency , it is not sufficient to represent individual subjects by a single gnrh mode .",
    "instead , every subject needs to be represented by an ensemble of gnrh models with different frequencies that adequately cover the entire permissible range . in this work ,",
    "we define such an ensemble with six different pulse frequencies : @xmath168 , @xmath169 , @xmath170 , @xmath171 , @xmath172 , and @xmath173 . for a given model setting",
    "@xmath174 we thus have an ensemble of 6 models @xmath175 , @xmath176 .",
    "further , the measurable output of this ensemble model is @xmath177_1 , ... , [ \\mbox{gsu}]_6)^{\\intercal}$ ] where the flow @xmath178_i$ ] is the output of the @xmath179th ensemble member .    it has been shown that the frequency - response behaviour of gnrh models is determined by @xmath165 and @xmath166 , but not by @xmath159 .",
    "the right panel of figure  [ gnrh_model ] shows that in the space of ( @xmath180 , @xmath181 ) , there exist three linearly separated domains in which only one of two frequency - response behaviours ( linear or bell - shaped ) is observed .",
    "the domain in the middle represents the normal subjects , whereas both remaining domains represent the abnormal subjects .       for four example stochastic double - well systems with ( @xmath16 , @xmath182 , @xmath183 ) = ( 1.0 , 1.0 , 0.1 ) ( red solid curve ) , ( @xmath16 , @xmath182 , @xmath183 ) = ( 1.3 , 1.5 , -0.1 ) ( blue solid curve ) , ( @xmath16 , @xmath182 , @xmath183 ) = ( 1.0 , 1.5 , 0 ) ( red dotted curve ) , and ( @xmath16 , @xmath182 , @xmath183 ) = ( 1.2 , 1.5 , 0 ) ( blue dotted curve ) .",
    "right panel : the same as in in right panel but for stochastic multi - well systems .",
    ", width=245,height=245 ]     for four example stochastic double - well systems with ( @xmath16 , @xmath182 , @xmath183 ) = ( 1.0 , 1.0 , 0.1 ) ( red solid curve ) , ( @xmath16 , @xmath182 , @xmath183 ) = ( 1.3 , 1.5 , -0.1 ) ( blue solid curve ) , ( @xmath16 , @xmath182 , @xmath183 ) = ( 1.0 , 1.5 , 0 ) ( red dotted curve ) , and ( @xmath16 , @xmath182 , @xmath183 ) = ( 1.2 , 1.5 , 0 ) ( blue dotted curve ) .",
    "right panel : the same as in in right panel but for stochastic multi - well systems . ,",
    "width=245,height=245 ]    stochastic double - well ( sdw ) system is mathematically defined as @xmath184 where @xmath185 represents the univariate standard brownian motion and @xmath186 collects the three model parameters , namely the well location parameter @xmath16 , well asymmetry parameter @xmath183 and standard deviation @xmath182 of the dynamical noise .",
    "[ gdw ] shows that the drift term @xmath187 is not explicitly time - dependent .",
    "therefore , the underlying dynamics is governed by the potential @xmath188 with @xmath189 .",
    "moreover , the equilibrium probability distribution of its state @xmath190 is given by @xmath191  @xcite .",
    "the potential corresponding to eq .",
    "[ gdw ] is given by @xmath192 the equilibrium probability distribution of two example sdws is shown in the left panel of fig .",
    "[ fig : pgdw ] .",
    "we can see that there exist two meta - stable states located at @xmath190 = @xmath16 and @xmath190 = @xmath193 .",
    "the larger is the dynamical noise variance , @xmath194 , the more frequent are the transitions from one meta - stable state to the other",
    ". figure  [ fig : pgdw ] shows that the peak probability for @xmath182 = 1.0 ( red solid curve ) is larger than that for @xmath182 = 1.5 ( blue solid curve ) . for positive well asymmetry parameter @xmath183 , the transition from @xmath190 = @xmath193 to @xmath190",
    "= @xmath16 is more likely than the transition in the opposite direction . as a result ,",
    "the equilibrium probability at @xmath190 = @xmath16 is higher than that at @xmath190 = @xmath193 ( see red solid curve in figure  [ fig : pgdw ] ) .",
    "analogously , the equilibrium probability at @xmath190 = @xmath193 is higher than that at @xmath190 = @xmath16 for negative well asymmetry parameter ( see blue solid curve in figure  [ fig : pgdw ] ) .",
    "the dynamics of double - well systems is dominated by switching between the two wells .",
    "we also study more complex multi - well systems where the potential has more than two wells .",
    "an example of such a multi - well system dominated by an overall two - well structure ( wells in positive range of @xmath190 are generally deeper than those in the negative range ( or vice - versa ) ) is given below ( see also right panel of figure  [ fig : pgdw ] ) : @xmath195 where @xmath196 denotes the perturbed potential .",
    "( red vs. blue solid curves ) .",
    "the range of each sub - panel s vertical axis is scaled to [ @xmath1972.5 , @xmath1982.5 ] .",
    "the inter - sample interval ( @xmath199 ) is 0.5 , and the variance @xmath200 of gaussian distributed observation noise is 0.04 .",
    "right panel : the same as in left panel but for @xmath200 = 0.36.,width=264,height=396 ]     ( red vs. blue solid curves ) .",
    "the range of each sub - panel s vertical axis is scaled to [ @xmath1972.5 , @xmath1982.5 ] .",
    "the inter - sample interval ( @xmath199 ) is 0.5 , and the variance @xmath200 of gaussian distributed observation noise is 0.04 .",
    "right panel : the same as in left panel but for @xmath200 = 0.36.,width=264,height=396 ]    in this work , we formed two classes of sdws through two class - conditional gaussian distributions in the parameter space as follows : ( @xmath201 + @xmath202 , @xmath203 + @xmath204 , @xmath205 ) for _ class 1 _ and ( @xmath206 + @xmath202 , @xmath207 + @xmath204 , @xmath208 ) for _ class 0 _ , where ( @xmath201 , @xmath203 , @xmath205 ) and ( @xmath206 , @xmath207 , @xmath208 ) denote the class - conditional prototypical model parameter ; @xmath202 and @xmath204 are gaussian - distributed zero - mean random variables with standard deviations 0.1/3 and 0.05/3 , respectively .",
    "an example of such two classes of sdws is defined by ( @xmath201 , @xmath203 , @xmath205 ) = ( 1.3 , 1.5 , @xmath1970.1 ) and ( @xmath206 , @xmath207 , @xmath208 ) = ( 1.0 , 1.0 , 0.1 ) corresponding to the blue and red solid curves in the left panel of figure  [ fig : pgdw ] , respectively .",
    "it is more likely for the trajectories from _ class 0 _ to stay above , rather than below , the horizontal line with @xmath209 .",
    "the opposite holds for _ class 1_. this is because the asymmetry parameters of these two classes take their values with opposite signs . as a result",
    ", the classification task can be well accomplished by a classifier based on simple features directly extracted from the signal - in this case the overall trajectory mean .",
    "figure  [ pgdw_cluster_trajectory_new ] illustrates a contrasting task in which two classes of sdws are defined by ( @xmath201 , @xmath203 , @xmath205 ) = ( 1.2 , 1.5 , 0 ) and ( @xmath206 , @xmath207 , @xmath208 ) = ( 1.0 , 1.5 , 0 ) ( see the blue and red dotted curves , respectively , in the left panel of figure  [ fig : pgdw ] ) .",
    "as the mean asymmetry parameter is set to zero for both classes , the overall trajectory mean fluctuates around zero across the trajectories in each of these two classes .",
    "we thus hypothesise that in such cases , the proposed classification lims framework will be superior to classification based on direct signal based features .",
    "in the experiments we evaluate performance of the three classifiers , namely the proposed classifier ( lims ) and two well - established distributional classifiers based on probability product kernel ( ppk ) and kernel mean embedding ( kme ) , on two classes of dynamical systems , one representant of ode ( gnrh , section [ sec : gnrh ] ) , the other of sde ( sdw , section [ sec : sdw ] ) . for a fair comparison",
    "all three classifiers were implemented in the framework of kernel logistic regression .",
    "our study addressed two important issues for classifying partially observed dynamical systems ( pods ) :    1 .   _ the influence of model uncertainty on classification in the model space . _",
    "+ model uncertainty arises when the underlying system is not completely observed .",
    "it is represented through posterior distribution over the underlying dynamical systems inferred from the partial observations .",
    "it is natural to expect that the posterior over possible models , given the observations , is a better ( model space ) representation of the observed time series than a single model , e.g. map point estimate .",
    "it is also natural to expect that the classification performance will increase with reducing model uncertainty .",
    "we compare the lims , ppk , and kme classifiers in terms of capability to deal with increased levels of model uncertainty quantified through entropy of the posterior distributions .",
    "we also use the level of observation noise @xmath210 , or the number of observations @xmath111 as surrogate uncertainty measures .",
    "performance degradation when the model class used to represent the observed time series through posterior distributions over it is a reduced sub - model class of the true model class generating the training and test data . _",
    "+ there can be several reasons for the inferential model to be different from the underlying data generating model .",
    "for example , in real - world applications , it is inevitable that there is a gap between the real - world and the mathematical model developed to account for it . alternatively , while the given mathematical model can be considered adequate , it is too complex and computationally expensive to simulate . to circumvent this problem ,",
    "a reduced model could be used to represent time series , as long as it captures characteristics relevant for the given classification task .",
    "we compare the classification performance between different inferential models ranging from the full , multiple - compartment pathway ode model to the trivial single compartment model .",
    "analogous experiments were performed in the sde case - sdw models representing data generated by stochastic multi - well systems .      in this section",
    "we discuss a number of practical issues related to testing the lims , kme and ppk classifiers :    * _ does the input of a distributional classifier need to be normalised ? _",
    "+ for the task of classifying pods , the actual input is the posterior distribution over parameter vectors . in our setting , it includes a set of posterior probabilities defined on a grid of parameter vectors . for ppk classifiers , only those probabilities are used and thus there is no need for normalisation . for the other two classifiers , however , we use parameter vectors ( on the grid ) together with the corresponding posterior probabilities . moreover , the parameter vectors are involved in the classification via a spherical kernel function that is defined on the product of two parameter grids .",
    "therefore , we normalise the parameter grid to vary in each dimension from 0 to 1 .",
    "of course , the original parameter values associated with grid points will be preserved . * _ how to initialise the classifier s parameters for gradient - based training ? _",
    "+ we implement all three classifiers in the klr framework .",
    "hence , the ppk - based classifier parameter effectively weights the training examples , whereas in the case of lims and kme , the parameter puts weights on the model grid . in this work , all elements of the parameter vectors are initialised by drawing from gaussian distribution with zero mean and unit variance .",
    "the parameters are then optimized through gradient descent as explained in section  [ sec : klr ] .",
    "this procedure is repeated @xmath211 times , resulting in @xmath211 classifiers combined in flat ensemble outputting the average of the @xmath211 predictive class probabilities ( given a test input ) .",
    "we set @xmath212 .",
    "* for the binary classification tasks in this work , we first generate the training and hold - out test sets with balanced class distribution , each containing 200 observation time series .",
    "both classes from the training set are randomly sub - sampled ( without replacement ) to 45 time series ( out of 100 ) , yielding a training batch of 90 time series .",
    "this is repeated @xmath213 times .",
    "we then report the mean ( @xmath214std .",
    "deviation ) classification performance on the test set across the @xmath215 runs .      to conduct experiments with the classification task defined in section  [ sec : gnrh ] , we generate two independent sets of gnrh models for training and testing ( 200 labelled models each ) . to that end we randomly sample 400 parameter vectors @xmath216 of the gnrh model and @xmath166 since their permissible range extends over several magnitudes .",
    "each of the three model parameters are sampled from the corresponding gaussian distribution truncated to the permissible range . for each parameter ,",
    "the mean and standard deviation of the untruncated gaussian are set to the mid - point and radius , respectively of the permissible range ( see table  [ gnrh_model_par ] ) .",
    "the parameter vectors",
    "are then labelled as _",
    "class 0 _ ( normal conditions ) or _ class 1 _ ( abnormal conditions ) as described in section  [ sec : gnrh ] ( see the right panel of figure  [ gnrh_model ] ) .",
    ".the truncated gaussian distributions of three gnrh model parameters ( i.e. @xmath180 , @xmath181 and @xmath159 ) used for generating the training and testing set of gnrh models . [ gnrh_model_par ] [ cols=\"^,<,<,<,<\",options=\"header \" , ]     .",
    "in this paper , we have presented a general learning in the model space ( lims ) framework for classifying partially observed dynamical systems .",
    "the key ingredient of this framework is the use of posterior distributions over models to represent the individual observation sets , taking into account in a principled manner the uncertainty due to both the generative ( observational and/or dynamic noise ) and observation ( sampling in time ) processes .",
    "this is in contrast to the existing learning in the model space classification approaches that use model point estimates to represent data items .",
    "another key ingredient of our approach is a new distributional classifier for classifying posterior distributions over dynamical systems .",
    "we evaluated this classifier on two testbeds , namely a biological pathway model and a stochastic double - well system .",
    "empirically the classifier clearly outperforms the classifier based on probability product kernel ( ppk ) - a state - of - the - art kernel method for classifying distributions . moreover , its performance is comparable with a recent distributional classification method based on kernel mean embedding .",
    "we derived a deep connection linking those three seemingly diverse approaches to distributional classification and provided a plausible explanation concerning superiority of the proposed classifier over the ppk classifier .",
    "the experiments show a clear relation between model uncertainty and classification performance .",
    "as expected , the performance drops with increasing model uncertainty .",
    "principled treatment of model uncertainty in the learning in the model space approach is crucial in situations characterized by non - negligible observational noise and/or limited observation times . to illustrate this point",
    "further we also trained a baseline classifier that , given the observed time series , completely ignores the model uncertainty and instead of posterior distribution only employs the map point estimate of the model parameter . as all the other classifiers , the baseline classifier ( referred to as map ) is also implemented in the klr framework .",
    "we compared the three posterior based classifiers with the map classifier using both testbeds .",
    "the comparison follows the philosophy of comparing baseline classifier ( bklr ) with the distributional classifiers in the sdw experiment ( columns 68 in table  [ signrank_dw_1 ] ) . in particular , in the gnrh experiment , we tested three hypotheses ( distributional classifier outperforms map ) at nine uncertainty levels ( see column 1 in table  [ signrank ] ) .",
    "both lims and kme classifiers outperform ( in the mean ) the map classifier in all , except for one , uncertainty levels . for lims and kme ,",
    "this superiority is statistically significant ( @xmath217 @xmath218 0.05 ) in all cases except for the lowest and the two lowest uncertainty levels , respectively .",
    "this is to be expected , as at low uncertainty levels the posterior over the models can be reasonably approximated by the map model estimate .",
    "in contrast , ppk classifier outperforms the map classifier only at 4 uncertainty levels , with statistical significance obtained only at the three highest uncertainty levels . in the sdw experiment ,",
    "the tests were performed at 15 uncertainty levels ( see column 12 in table  [ signrank_dw_1 ] ) .",
    "the lims , kme and ppk classifiers outperform the map classifier at all ( 15 ) , 11 and 7 uncertainty levels , with statistical significance obtained at 9 , 4 and 4 uncertainty levels , respectively .",
    "crucially , we showed that the classifier performance would not be impaired when the model class used for inferring posterior distributions is much more simple than the observation - generating model class , provided the reduced complexity inferential model class captures the essential characteristics needed for the given classification task .",
    "this finding is potentially very significant for real - world applications .",
    "although mechanistic models encode expert domain knowledge and are of huge importance in forward modelling ( e.g. assessing response to drug at certain dosage ) , such models may be too complex for the inferential ( inverse - task ) purposes .",
    "fortunately , much reduced model alternatives can be used in the learning in the model space framework if , as explained above , they already encode features important for the classification task .",
    "a semi - automated task - driven model simplification for learning in the model space framework is a matter of our future research .",
    "this work was supported by the epsrc grant `` personalised medicine through learning in the model space '' ( grant number ep / l000296/1 ) .",
    "kt - a gratefully acknowledges the financial support of the epsrc via grant ep / n014391/1 .      c.  c. aggarwal . on effective classification of strings with wavelets . in _ kdd02 : proceedings of the fifth acm sigkdd international conference on knowledge discovery and data mining _ ,",
    "pages 163172 , 2002 .    c.  archambeau , m.  opper , y.  shen , d.  cornford , and j.  shawe - taylor .",
    "variational inference for diffusion processes . in _ advances in neural information processing systems",
    "20 _ , pages 1724 . mit press , 2008 .",
    "k.  h. brodersen , t.  m. schofield , a.p .",
    "leff , c.  s. ong , e.  i lomakina , j.  m. buhmann , and k.  e. stephan .",
    "generative embedding for model - based classification of fmri data . _",
    "plos computational biology _ , 7:0 e1002079 , 2011 .",
    "a.  b. chan and n.  vasconcelos .",
    "probabilistic kernels for the classification of auto - regressive visual processes . in _",
    "cvpr05 : proceedings of ieee computer society conference on computer vision and pattern recognition 2005 _ , pages 846851 , 2005 .",
    "a.  b. chan and n.  vasconcelos",
    ". classifying video with kernel dynamic textures . in _",
    "cvpr07 : proceedings of ieee computer society conference on computer vision and pattern recognition 2007 _ , pages 16 , 2007 .",
    "h.  chen , f.  tang , p.  tino , and x.  yao .",
    "model - based kernel for efficient time series analysis . in _",
    "kdd13 : proceedings of the fifth acm sigkdd international conference on knowledge discovery and data mining _ , pages 392400 , 2013 .",
    "h.  chen , f.  tang , p.  tino , and x.  yao . model metric co - learning for time series classification . in _",
    "ijcai2015 : proceedings of the 24th international joint conference on artificial intelligence _ , pages 33873394 , 2015 .",
    "f.  cuzzolin .",
    "manifold learning for multi - dimensional auto - regressive dynamical models . in l.",
    "wang , g .. zhao , l.  cheng , and m.  pietikainen , editors , _ machine learning for vision - based motion analysis _ , pages 5574 .",
    "springer , 2011 .",
    "f.  dondelinger , m.  filippone , s.  rogers , and d.  husmeier .",
    "ode parameter inference using adaptive gradient matching with gaussian processes",
    ". _ journal of machine learning research - workshop & conference proceedings _ , 31:0 216228 , 2013 .",
    "n.  lesh , m.  j. zaki , and m.  ogihara .",
    "mining features for sequence classification . in _",
    "kdd99 : proceedings of the fifth acm sigkdd international conference on knowledge discovery and data mining _ ,",
    "pages 342346 , 1999 .",
    "p.  j. moreno , p.  ho , and n.  vasconcelos . a kullback - leibler divergence based kernel for svm classification in multimedia applications . in s.",
    "thrun , l.  k. saul , and b.  schlkopf , editors , _ advances in neural information processing systems 16 _ , pages 13851392 . mit press , 2004 .",
    "k.  muandet , k.  fukumizu , f.  dinuzzo , and b.  schlkopf .",
    "learning from distributions via support measure machines . in _ advances in neural information processing systems 25 _ ,",
    "pages 1018 . mit press , 2012 .            k.  trapeva - atanasova , p.  mina , c.  j. caunt , s.  p.armstrong , and c.  a. mcardle .",
    "decoding gnrh neurohormone pulse frequency by convergent signaling modules . _ journal of the royal society interface _",
    ", 9:0 170182 , 2012 ."
  ],
  "abstract_text": [
    "<S> we present a general framework for classifying partially observed dynamical systems based on the idea of learning in the model space . </S>",
    "<S> in contrast to the existing approaches using model point estimates to represent individual data items , we employ posterior distributions over models , thus taking into account in a principled manner the uncertainty due to both the generative ( observational and/or dynamic noise ) and observation ( sampling in time ) processes . </S>",
    "<S> we evaluate the framework on two testbeds - a biological pathway model and a stochastic double - well system . </S>",
    "<S> crucially , we show that the classifier performance is not impaired when the model class used for inferring posterior distributions is much more simple than the observation - generating model class , provided the reduced complexity inferential model class captures the essential characteristics needed for the given classification task .    </S>",
    "<S> a classification framework for partially observed dynamical systems     + * yuan shen@xmath0 , peter tino@xmath0 , krasimira tsaneva - atanasova@xmath1 * + @xmath0school of computer science + the university of birmingham + birmingham , united kingdom + email : \\{y.shen.2@xmath2pxt}@cs.bham.ac.uk + @xmath3department of mathematics , + college of engineering , mathematics and physical sciences , + university of exeter , + exeter ex4 4qf , uk + k.tsaneva-atanasova@exeter.ac.uk + @xmath4department of mathematics , + epsrc centre for predictive modelling in healthcare + university of exeter , + exeter ex4 4qf , uk + k.tsaneva-atanasova@exeter.ac.uk + </S>"
  ]
}