{
  "article_text": [
    "state - of - the - art machine learning algorithms are able to solve many problems sufficiently well . however , both theoretical and experimental studies have shown that in order to achieve solutions of reasonable quality they need an access to extensive amounts of training data .",
    "in contrast , humans are known to be able to learn concepts from just a few examples .",
    "a possible explanation may lie in the fact that humans are able to reuse the knowledge they have gained from previously learned tasks for solving a new one , while traditional machine learning algorithms solve tasks in isolation .",
    "this observation motivates an alternative , transfer learning approach .",
    "it is based on idea of transferring information between related learning tasks in order to improve performance .",
    "there are various formal frameworks for transfer learning , modeling different learning scenarios . in this work we focus on two of them : the multi - task and the lifelong settings . in the multi - task scenario ,",
    "the learner faces a fixed set of learning tasks simultaneously and its goal is to perform well on all of them . in the lifelong learning setting ,",
    "the learner encounters a stream of tasks and its goal is to perform well on new , yet unobserved tasks .    for any transfer learning scenario to make sense ( that is , to benefit from the multiplicity of tasks ) , there must be some kind of relatedness between the tasks .",
    "a common way to model such task relationships is through the assumption that there exists some data representation under which learning each of the tasks is relatively easy .",
    "the corresponding transfer learning methods aim at learning such a representation .    in this work we focus on the case of large - margin learning of kernels .",
    "we consider sets of tasks and families of kernels and analyze the sample complexity of finding a kernel in a kernel family that allows low expected error on average over the set of tasks ( in the multi - task scenario ) , or in expectation with respect to some unknown task - generating probability distribution ( in the lifelong scenario ) .",
    "we provide generalization bounds for empirical risk minimization learners for both settings . under the assumption that the considered kernel family has finite pseudodimension",
    ", we show that by learning several tasks simultaneously the learner is guaranteed to have low estimation error with fewer training samples per task ( compared to solving them independently ) . in particular ,",
    "if there exists a kernel with low approximation error for all tasks , then , as the number of observed tasks grows , the problem of learning any specific task with respect to a family of kernels converges to learning when the learner knows a good kernel in advance - the multiplicity of tasks relieves the overhead associated with learning a kernel .",
    "our assumption on finite pseudodimension of the kernel family is satisfied in many practical cases , like families of gaussian kernels with a learned covariance matrix , and linear and convex combinations of a finite set of kernels ( see  @xcite ) .",
    "we also show that this is the case for families of all sparse combinations of kernels from a large  dictionary \" of kernels .      * multi - task and lifelong learning . * a method for learning a common feature representation for linear predictors in the multi - task scenario was proposed in  @xcite .",
    "a similar idea was also used by  @xcite and extended to the lifelong scenario by  @xcite . a natural extension of representation",
    "learning approach was proposed for kernel methods in  @xcite , where the authors described a method for learning a kernel that is shared between tasks as a combination of some base kernels using maximum entropy discrimination approach .",
    "a similar approach , with additional constraints on sparsity of kernel combinations , was used by  @xcite .",
    "these ideas were later generalized to the case , when related tasks may use slightly different kernel combinations  @xcite , and successfully used in practical applications  @xcite .    despite intuitive attractiveness of the possibility of automatically learning a suitable feature representation compared to learning with a fixed , perhaps high - dimensional or just irrelevant set of features , relatively little",
    "is known about its theoretical justifications . a seminal systematic theoretical study of the multi - task",
    "/ lifelong learning settings was done by baxter in  @xcite . there",
    "the author provided sample complexity bounds for both scenarios under the assumption that the tasks share a common optimal hypothesis class .",
    "the possible advantages of these approaches according to baxter s results depend on the behavior of complexity terms , which , however , due to the generality of the formulation , often can not be inferred easily given a particular setting . therefore ,",
    "studying more specific scenarios by using more intuitive complexity measures may lead to better understanding of the possible benefits of the multi - task / lifelong settings , even if , in some sense , they can be viewed as particular cases of baxter s result . along that line ,",
    "maurer in  @xcite proved that learning a common low - dimensional representation in the case of lifelong learning of linear least - squares regression tasks is beneficial .",
    "* multiple kernel learning . *",
    "the problem of multiple kernel learning in the single - task scenario has been theoretically analyzed using different techniques . by using covering numbers , srebro et",
    "al in  @xcite have shown generalization bounds with additive dependence on the pseudodimension of the kernel family .",
    "another bound with multiplicative dependence on the pseudodimension was presented in  @xcite , where the authors used rademacher chaos complexity measure .",
    "both results have a form @xmath3 , where @xmath4 is the pseudodimension of the kernel family and @xmath5 is the sample size . by carefully analyzing the growth rate of the rademacher complexity in the case of the linear combinations of finitely many kernels with @xmath6 constraint on the weights , cortes et al in  @xcite have improved the above results . in particular , in the case of @xmath7 constraints ,",
    "the bound from  @xcite has a form @xmath8 , where @xmath9 in the total number of kernels , while the bound from  @xcite is @xmath10 .",
    "the fast rate analysis of the linear combinations of kernels using local rademacher complexities was performed by kloft et al in  @xcite .    in this work",
    "we utilize techniques from  @xcite .",
    "it allows us to formulate results that hold for any kernel family with finite pseudodimension and not only for the case of linear combinations , though at the price of potentially suboptimal dependence on the number of kernels in the latter case . moreover , additive dependence on the pseudodimension is especially appealing for the analysis of the multi - task and lifelong scenarios , as it allows obtaining bounds where that additional complexity term vanishes as the number of tasks grows and therefore these bounds clearly show possible advantages of transfer learning .",
    "we start by describing the formal set up and preliminaries in section  [ sec : setup],[sec : cov_numbers ] and providing a list of known kernel families with finite pseudodimensions , including our new result for sparse linear combinations , in  [ sec : pseudodim ] . in section",
    "[ sec : mt ] we provide the proof of the generalization bound for the multi - task case and extend it to the lifelong setting in section  [ sec : ll ] .",
    "we conclude by discussion in section  [ sec : conc ] .",
    "throughout the paper we denote the input space by @xmath11 and the output space by @xmath12 .",
    "we assume that the learner ( both in the multi - task and the lifelong learning scenarios ) has an access to @xmath13 tasks represented by the corresponding training sets @xmath14 , where each @xmath15 consists of @xmath5 i.i.d .",
    "samples from some unknown task - specific data distribution @xmath16 over @xmath17 .",
    "in addition we assume that the learner is given a family @xmath18 of kernel functions[multiblock footnote omitted ] defined on @xmath19 and uses the corresponding set of linear predictors for learning . formally , for every kernel @xmath20",
    "we define @xmath21 to be such set : @xmath22{\\mbox{\\normalfont\\tiny",
    "def}}}{=}}}\\left\\{h : x\\mapsto\\langle w,\\phi(x)\\rangle \\ ; |\\ ; \\|w\\|\\leq1 , k(x , x')=\\langle\\phi(x),\\phi(x')\\rangle\\right\\}\\ ] ] and @xmath23 to be the union of them : @xmath24 .    in the multi - task scenario",
    "the data distributions @xmath25 are assumed to be fixed and the goal of the learner is to identify a kernel @xmath20 that performs well on all of them .",
    "therefore we would like to bound the difference between the expected error rate over the tasks : @xmath26 and the corresponding empirical margin error rate : @xmath27 alternatively the learner may be interested in identifying a particular predictor for every task .",
    "if we define @xmath28 and @xmath29 , then it means finding some @xmath30 with low generalization error : @xmath31 based on its empirical margin performance : @xmath32 however , due to the following inequality , it is enough to bound the probability of large estimation error for the second case and a bound for the first one will follow immediately : @xmath33    for the lifelong learning scenario we adopt the notion of task environment proposed in  @xcite and assume that there exists a set of possible data distributions ( i.e. tasks ) @xmath34 and that the observed tasks are sampled from it i.i.d .",
    "according to some unknown distribution @xmath35 .",
    "the goal of the learner is to find a kernel @xmath20 that would work well on future , yet unobserved tasks from the environment @xmath36 .",
    "therefore we would like to bound the probability of large deviations between the expected error rate on new tasks , given by : @xmath37 and the corresponding empirical margin error rate @xmath38 .    in order to obtain the generalization bounds in both cases we employ the technique of covering numbers .      in this subsection",
    "we describe the types of covering numbers we will need and establish their connections to pseudodimensions of kernel families .",
    "a subset @xmath39 is called an @xmath40-cover of @xmath41 with respect to a distance measure @xmath4 , if for every @xmath42 there exists a @xmath43 such that @xmath44 .",
    "the covering number @xmath45 is the size of the smallest @xmath40-cover of @xmath41 .    to derive bounds for the multi - task setting we will use covers of @xmath46 with respect to @xmath47 metric associated with a sample @xmath48 : @xmath49 the corresponding uniform covering number @xmath50 is given by considering all possible samples @xmath48 : @xmath51    in contrast , for the lifelong learning scenario we will need covers of the kernel family @xmath18 with respect to a probability distribution . for any probability distribution @xmath52 over @xmath53 ,",
    "we denote its projection on @xmath11 by @xmath54 and define the following distance between the kernels : @xmath55 similarly , for any set of @xmath13 distributions @xmath56 we define : @xmath57 the minimal size of the corresponding @xmath40-cover of a set of kernels @xmath18 we will denote by @xmath58 and the corresponding uniform covering number by by @xmath59 .    in order to make the guarantees given by the generalization bounds , that we provide ,",
    "more intuitively appealing we state them using a natural measure of complexity of kernel families , namely , pseudodimension  @xcite :    the class @xmath18 pseudo - shatters the set of @xmath13 pairs of points@xmath60 if there exist thresholds @xmath61 such that for any @xmath62 there exists @xmath20 such that @xmath63 .",
    "the pseudodimension @xmath64 is the largest @xmath13 such that there exists a set of @xmath13 pairs pseudo - shattered by @xmath18 .    to do so we develop upper bounds on the covering numbers we use in terms of the pseudodimension of the kernel family @xmath18 .",
    "first , we prove the result for @xmath50 that will be used in the multi - task setting :    for any set @xmath18 of kernels bounded by @xmath65(@xmath66 for all @xmath20 and all @xmath67 ) with pseudodimension @xmath68 the following inequality holds : @xmath69 [ lemma : n(nm ) ]    in order to prove this result , we first introduce some additional notation . for a sample @xmath70",
    "we define @xmath71 distance between two functions : @xmath72 then the corresponding uniform covering number is : @xmath73 we also define @xmath71 distance between kernels with respect to a sample @xmath74 with the corresponding uniform covering number : @xmath75 in contrast , in  @xcite the distance between two kernels is defined based on a single sample @xmath76 of size @xmath5 : @xmath77 and the corresponding covering number is @xmath78 .",
    "note that this definition is in strong relation with ours : @xmath79 , and therefore , by lemma 3 in  @xcite : @xmath80 for any kernel family @xmath18 bounded by @xmath65 with pseudodimension @xmath68 .",
    "now we can prove lemma  [ lemma : n(nm ) ] :    fix @xmath81 .",
    "define @xmath82 and @xmath83 .",
    "let @xmath84 be an @xmath85-net of @xmath18 with respect to @xmath86 .",
    "for every @xmath87 and every @xmath88 let @xmath89 be an @xmath90-net of @xmath91with respect to @xmath92 .",
    "now fix some @xmath93 .",
    "then there exists a kernel @xmath94 such that @xmath95 .",
    "therefore there exists a kernel @xmath87 such that @xmath96 for every @xmath97 . by lemma 1 in  @xcite @xmath98 for some unit norm vector @xmath99 for every @xmath97 .",
    "therefore for @xmath100{\\mbox{\\normalfont\\tiny def}}}{=}}}\\widetilde{k}_{\\mathbf{x}_i}^{1/2}w_i\\in\\mathcal{f}_{\\widetilde{k}}$ ] we obtain that : @xmath101 in addition , for every @xmath102 there exists @xmath103 such that @xmath104 . finally ,",
    "if we define @xmath105 , we obtain : @xmath106 the above shows that @xmath107 is an @xmath40-net of @xmath46 with respect to @xmath108 .",
    "now the statement follows from   and the fact that for any @xmath21 with bounded by @xmath65 kernel @xmath94(@xcite ) : @xmath109    analogously we develop an upper bound on the covering number @xmath110 , which we will use for the lifelong learning scenario :    there exists a constant @xmath111 such that for any kernel family @xmath18 bounded by @xmath65 with pseudodimension @xmath68 : @xmath112 [ lemma : ll_cov_num ]    the proof of this result is based on the following lemma that connects sample - based and distribution - based covers of kernel families ( for the proof see appendix  [ ap : lemmas ] ) :    for any probability distribution @xmath52 over @xmath53 and any @xmath65-bounded set of kernels @xmath18 with pseudo - dimension @xmath68 there exists a sample @xmath108 of size @xmath113 for some constant @xmath114 , such that for every @xmath115 if @xmath116 , then @xmath117 ( where @xmath118 is the same as @xmath119 , but all expectations over @xmath52 are substituted by empirical averages over @xmath108 ) .",
    "[ lemma : distr_to_sample ]    fix some set of probability distributions @xmath120 . for",
    "every @xmath16 denote a sample described by lemma  [ lemma : distr_to_sample ] by @xmath121 .",
    "let @xmath122 be an @xmath123-cover of @xmath18 with respect to @xmath118 , where @xmath124 and @xmath113 .",
    "then the following chain of inequalities holds : @xmath125 consequently , by lemma 3 in  @xcite : @xmath126 .",
    "it is left to show that @xmath122 is an @xmath40-cover of @xmath18 with respect to @xmath127 . by definition , for every @xmath20",
    "there exists @xmath128 such that @xmath129 .",
    "therefore for every @xmath88 : @xmath130 consequently , by lemma  [ lemma : distr_to_sample ] , @xmath131 for all @xmath88 .      in @xcite",
    "the authors have shown the upper bounds on the pseudodimensions of some families of kernels :    * convex or linear combinations of @xmath9 kernels have pseudodimension at most @xmath9 * gaussian families with learned covariance matrix in @xmath132 have @xmath133 * gaussian families with learned low - rank covariance have @xmath134 , where @xmath9 is the maximum rank of the covariance matrix    here we extend their analysis to the case of sparse combinations of kernels .",
    "let @xmath135 be @xmath136 kernels and let @xmath137\\leq k\\}$ ] .",
    "then : @xmath138    for every kernel @xmath94 define a function @xmath139 : @xmath140 and denote a set of such functions for all @xmath20 by @xmath141",
    ". then @xmath142 .",
    "for every index set @xmath143 define @xmath144 to be a set of all linear combinations of @xmath145 . then : @xmath146 and @xmath147 .",
    "moreover , there are @xmath148 of possible sets of indices @xmath97 .",
    "therefore @xmath141 can also be seen as a union of at most @xmath149 sets with vc - dimension at most @xmath9 .",
    "vc - dimension of a union of @xmath150 classes of vc - dimension at most @xmath4 is at most @xmath151 .",
    "the statement of the lemma is obtained by setting @xmath152 and @xmath153 .",
    "we start with formulating the result using covering number @xmath50 :    for any @xmath154 , if @xmath155 , we have that : @xmath156 [ thm : mt_upbound ]    we utilize the standard 3-steps procedure ( see theorem 10.1 in  @xcite ) . if we denote : @xmath157 then according to the symmetrization argument @xmath158 .",
    "therefore , instead of bounding the probability of @xmath35 , we can bound the probability of @xmath159 .",
    "next , we define @xmath160 to be a set of permutations",
    "@xmath161 on the set @xmath162 such that @xmath163 for every @xmath164 and @xmath165",
    ". then @xmath166 .",
    "now we proceed with the last step - reduction to a finite class .",
    "fix @xmath167 and the corresponding @xmath168 .",
    "let @xmath169 be a @xmath170-cover of @xmath46 with respect to @xmath171 and fix @xmath172",
    ". by definition there exists @xmath30 such that @xmath173 , where @xmath174",
    ". we can rewrite it as : @xmath175 if we denote by @xmath176 the function in the cover @xmath169 corresponding to @xmath177 , then the following inequalities hold : @xmath178 by combining them with the previous inequality we obtain that : @xmath179 now , if we define the following indicator : @xmath180 , then : @xmath181 where @xmath182 are independent random variables uniformly distributed over @xmath183",
    ". then @xmath184 are @xmath185 independent random variables that take values between @xmath186 and @xmath187 and have zero mean .",
    "therefore by hoeffding s inequality : @xmath188 by noting that @xmath189 , we conclude the proof of theorem  [ thm : mt_upbound ] .    by using the same technique as for proving theorem  [ thm : mt_upbound ]",
    ", we can obtain a lower bound on the difference between the empirical error rate @xmath190 and the expected error rate with double margin : @xmath191    for any @xmath154 , if @xmath155 , the following holds : @xmath192 [ thm : mt_lowbound ]    now , by combining theorems  [ thm : mt_upbound ] ,  [ thm : mt_lowbound ] and lemma  [ lemma : n(nm ) ] we can state the final result for the multi - task scenario in terms of pseudodimensions :    for any probability distributions @xmath25 over @xmath193 , any kernel family @xmath18 , bounded by @xmath65 with pseudodimension @xmath194 , and any fixed @xmath195 , for any @xmath154 , if @xmath155 , then , for a sample @xmath196 generated by @xmath197 : @xmath198 where    @xmath199    [ thm : mtmkl ]    * discussion : * the most significant implications of this result are for the case where there exists some kernel @xmath200 that has low approximation error for each of the tasks @xmath16 ( this is what makes the tasks `` related '' and , therefore , the multi - task approach advantageous ) .",
    "in such a case , the kernel that minimizes the average error over the set of tasks is a useful kernel for each of these tasks .    1 .",
    "maybe the first point to note about the above generalization result is that as the number of tasks ( @xmath13 ) grows , while the number of examples per task ( @xmath5 ) remains constant , the error bound behaves like the bound needed to learn with respect to a single kernel . that is",
    ", if a learner wishes to learn some specific task @xmath16 , and all the learner knows is that in the big family of kernels @xmath201 , _ there exists _ some useful kernel @xmath94 for @xmath16 that is also good on average over the other tasks , then the training samples from the other tasks allow the learner of @xmath16 to learn as if he had access to a specific good kernel @xmath94 .",
    "another worthwhile consequence of the above theorem is that it shows the usefulness of an empirical risk minimization approach .",
    "namely , + let @xmath202 be a minimizer , over @xmath46 , of the empirical @xmath203-margin loss , @xmath190 .",
    "then for any @xmath204 ( and in particular for a minimizer over @xmath46 of the true @xmath205-loss @xmath206 ) : @xmath207 [ cor : mt ] + the result is implied by the following chain of inequalities : @xmath208 + where @xmath209 and @xmath210 follow from the above theorem and @xmath211 follows from the definition of an empirical risk minimizer .",
    "in this section we generalize the results of the previous section to the case of lifelong learning in two steps .",
    "first , note that by using the same arguments as for proving theorem  [ thm : mt_upbound ] we can obtain a bound on the difference between @xmath212 and : @xmath213 therefore the only thing that is left is a bound on the difference between @xmath214 and @xmath215 .",
    "we will use the following notation : @xmath216 and proceed in a way analogous to the proof of theorem  [ thm : mt_upbound ] .",
    "first , if we define : @xmath217 then according to the symmetrization argument @xmath158 .",
    "now , if we define @xmath218 to be a set of permutations @xmath161 on a set @xmath219 , such that @xmath220 for all @xmath88 , we obtain that @xmath221 , if @xmath222 .",
    "so , the only thing that is left is reduction to a finite class .",
    "fix @xmath196 and denote by @xmath223 a set of kernels , such that for every @xmath20 there exists a @xmath128 such that : @xmath224 then , if @xmath21 is such that @xmath225 , then the corresponding @xmath226 satisfies @xmath227",
    ". therefore : @xmath228 are independent random variables uniformly distributed over @xmath229 . as in the previous section , @xmath230",
    "are @xmath13 independent random variables that take values between @xmath186 and @xmath187 and have zero mean .",
    "therefore by applying hoeffding s inequality we obtain : @xmath231 to conclude the proof we need to understand how @xmath232 behaves .",
    "for that we prove the following lemma :    for any set of probability distributions @xmath233 there exists @xmath122 that satisfies condition of equation   and @xmath234 .",
    "[ lemma : tildek ]    fix a set of distributions @xmath233 and denote by @xmath122 an @xmath235-cover of @xmath18 with respect to @xmath127 . then @xmath236 . by definition of a cover for any kernel @xmath20",
    "there exists @xmath128 such that @xmath237 .",
    "equivalently , it means that for every @xmath20 there exists @xmath128 such that the following two conditions hold for every @xmath238 : @xmath239 fix some @xmath94 and the corresponding kernel @xmath226 from the cover and take any @xmath16 . by markov s inequality applied to the first condition we obtain that for every @xmath240 there exists a @xmath241 such that @xmath242",
    ". then @xmath243 . by applying the same argument to the second condition we conclude that for every @xmath244 there exists a @xmath245 such that @xmath242",
    ". then @xmath246 .",
    "by definition of infinum @xmath212 for every @xmath247 there exists @xmath240 such that @xmath248 . by above construction for such @xmath249",
    "there exists @xmath244 such that @xmath250 . by combining these inequalities",
    "we obtain that for every @xmath251 @xmath252 , or , equivalently , @xmath253 .",
    "analogously we can get that @xmath254 .",
    "so , we obtain condition  .    by combining the above lemma with   we obtain the following result ( the second inequality can be obtain in a similar manner ) :    for any @xmath154 , if @xmath222 , the following holds : @xmath255 [ thm : ll_upbound ]    note that by exactly following the proof of theorem  [ thm : mt_upbound ] one can obtain that : @xmath256 therefore , by combining the above result with its equivalent in the opposite direction with theorem  [ thm : ll_upbound ] and lemmas  [ lemma : n(nm ) ] and  [ lemma : ll_cov_num ] we obtain the final result for the lifelong kernel learning :    for any task environment , any kernel family @xmath18 , bounded by @xmath65 with pseudodimension @xmath68 , any fixed @xmath195 and any @xmath154 , if @xmath257 and @xmath258 , then : @xmath259 where @xmath260 [ thm : llmkl ]    * discussion : * as for the multi - task case , the most significant implications of this result are for the case where there exists some kernel @xmath200 that has low approximation error for all tasks in the environment . in such a case , the kernel that minimizes the average error over the set of observed tasks is a useful kernel for all the tasks .    1 .   first",
    ", note , that the only difference between theorem  [ thm : llmkl ] and theorem  [ thm : mtmkl ] is the presence of the second term .",
    "this additional complexity comes from the fact that for the lifelong learner we are bounding the expected error on new , yet unobserved tasks .",
    "therefore we have to pay additionally for not knowing exactly what these new tasks are going to be",
    "second , the behavior of the above result is similar to that of theorem  [ thm : mtmkl ] in the limit of infinitely many observed tasks ( @xmath261 ) . in this case , the second term vanishes , because by observing large enough amount of tasks the learner gets the full knowledge about the task environment .",
    "the first term behaves exactly the same as the one in theorem  [ thm : mtmkl ] : its part that depends on @xmath68 vanishes and therefore it converges to the complexity of learning one task as if the learner would know a good kernel in advance .",
    "this theorem also shows the usefulness of an empirical risk minimization approach as we can obtain a corollary of exactly the same form as corollary  [ cor : mt ] .",
    "multi - task and lifelong learning have been a topic of significant interest of research in recent years and attempts for solving these problems in different directions have been made .",
    "methods of learning kernels in these scenarios have been shown to lead to effective algorithms and became popular in applications . in this work ,",
    "we have established sample complexity error bounds that justify this approach .",
    "our results show that , under mild conditions on the used family of kernels , by solving multiple tasks jointly the learner can `` spread out '' the overhead associated with learning a kernel and as the number of observed tasks grows , the complexity converges to that of learning when a good kernel was known in advance .",
    "this work constitutes a step forward better understanding of the conditions under which multi - task / lifelong learning is beneficial .",
    "this work was in parts funded by the european research council under the european union s seventh framework programme ( fp7/2007 - 2013)/erc grant agreement no 308036 .    4 m. kloft and g. blanchard : on the convergence rate of lp - norm multiple kernel learning .",
    "journal of machine learning research ( 2012 )    c. cortes , m. mohri and a. rostamizadeh : generalization bounds for learning kernels . in proceedings of the international conference on machine learning ( 2010 )    y. ying and c. campbell : generalization bounds for learning the kernel . in proceedings of the workshop on computational learning theory ( 2009 )    n. srebro and s. ben - david : learning bounds for support vector machines with learned kernels . in proceedings of the workshop on computational learning theory ( 2006 )    p.l .",
    "bartlett , s. r. kulkarni and s.e .",
    "posner : covering numbers for real - valued function classes .",
    "ieee transactions on information theory vol .",
    "1721 - 1724 ( 1997 )    j. baxter : a model of inductive bias learning .",
    "journal of artificial intelligence research vol .",
    "149 - 198 ( 2000 )    t. evgeniou and m. pontil : regularized multi  task learning . in proceedings of the international conference on knowledge discovery and data mining ( 2004 )    m. anthony and p. l. bartlett : neural network learning : theoretical foundations . cambridge university press ( 1999 )",
    "a. argyriou , t. evgeniou and m. pontil : convex multi - task feature learning . machine learning vol .",
    "73 ( 2008 )    a. kumar and h. daum iii : learning task grouping and overlap in multi - task learning . in proceedings of the international conference on machine learning ( 2012 )    e. eaton and p. l. ruvolo : ella : an efficient lifelong learning algorithm . in proceedings of the international conference on machine learning ( 2013 )    t. jebara : multi - task feature and kernel selection for svms . in proceedings of the international conference on machine learning ( 2004 )    t. jebara",
    ": multitask sparsity via maximum entropy discrimination .",
    "journal of machine learning research ( 2011 )    m. gnen , m. kandemir and s. kaski : multitask learning using regularized multiple kernel learning . in proceedings of the international conference on neural information processing ( 2011 )    c.h . lampert and m.b .",
    "blaschko : a multiple kernel learning approach to joint multi - class object detection . in proceedings of the 30th dagm symposium ( 2008 )    w. samek , a. binder and m. kawanabe : multi - task learning via non - sparse multiple kernel learning .",
    "computer analysis of images and patterns ( 2011 )    a. rakotomamonjy , r. flamary , g. gasso and s. canu : lp - lq penalty for sparse linear and sparse multiple kernel multi - task learning .",
    "ieee transactions on neural networks ( 2011 )    y. zhou , r. jin and s. c. h. hoi : exclusive lasso for multi - task feature selection . in proceedings of the conference on uncertainty in artificial intelligence ( 2010 )    a. maurer : transfer bounds for linear feature learning .",
    "machine learning vol . 75 ( 2009 )",
    "define @xmath262 : g(x)=\\frac{|h(x)-h'(x)|}{\\sqrt{b } } \\;\\text{for some}\\ ; h , h'\\in\\cup\\mathcal{f}_k\\right\\}$ ] . then ( using lemma 2 and 3 in  @xcite and theorem 1 in  @xcite ) : @xmath263 for big enough @xmath5 @xmath264 is less than 1 , which means that there is a sample @xmath265 such that for all kernels @xmath115 we have @xmath266 . more precisely , @xmath5 should be bigger than @xmath267 for some constant @xmath114 ."
  ],
  "abstract_text": [
    "<S> we consider a problem of learning kernels for use in svm classification in the multi - task and lifelong scenarios and provide generalization bounds on the error of a large margin classifier . </S>",
    "<S> our results show that , under mild conditions on the family of kernels used for learning , solving several related tasks simultaneously is beneficial over single task learning . </S>",
    "<S> in particular , as the number of observed tasks grows , assuming that in the considered family of kernels there exists one that yields low approximation error on all tasks , the overhead associated with learning such a kernel vanishes and the complexity converges to that of learning when this good kernel is given to the learner .    </S>",
    "<S> * there is a mistake in the conference version of the manuscript : in theorem 4 on the right hand side there should be @xmath0 instead of @xmath1 . </S>",
    "<S> this results in the additional constant @xmath2 in theorem 5 . + * </S>"
  ]
}