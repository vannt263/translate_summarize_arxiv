{
  "article_text": [
    "in a classic paper on the stochasticity of gene expression , elowitz _ et al . _",
    "@xcite describe a clever two - reporter expression assay designed to tease apart `` intrinsic '' and `` extrinsic '' noise from the overall variability in gene expression .",
    "the idea is as follows : two identically regulated reporter genes ( cyan fluorescent protein and yellow fluorescent protein ) are inserted into individual _",
    "_ cells allowing for comparable expression measurements within and between cells .",
    "if @xmath0 cells are assayed , this leads to expression measurements @xmath1 and @xmath2 where the pair @xmath3 represent the expression measurements for the cyan and yellow reporters in the @xmath4th cell .",
    "the goal of the experiment is to measure the variance in gene expression from the pairs @xmath3 ( denoted by @xmath5 ) and to ascribe it to two different sources : first , variability due to the different states of cells ( `` extrinsic noise '' , denoted by @xmath6 ) , and second , inherent variability that exists even when the state of cells is fixed ( `` intrinsic noise '' , denoted by @xmath7 ) . in @xcite , formulas were provided for estimating @xmath8 and @xmath5 ( hereafter referred to as the elss estimates ) that were later interpreted in terms of the `` law of total variance '' in @xcite :    @xmath9    where @xmath10 and @xmath11 .",
    "although the work of @xcite sheds light on the statistical basis of the elss estimators , it does not address questions about their statistical properties , such as bias and accuracy . to analyze these aspects of the estimators we introduce a hierarchical model that provides a formal model for the experiments of @xcite .    in the rest of the paper , we focus on the numerators of ( [ eqn : noise.int.elowitz],[eqn : noise.ext.elowitz],[eqn : noise.tot.elowitz ] ) .",
    "they are the key components of the formulas and can be viewed as estimators of true variances .",
    "we note that lower case letters such as @xmath12 and @xmath13 denote observations not only in the elss formulas but throughout our paper ; we reserve uppercase letters for random variables .",
    "a hierarchical model for expression of the two reporters in a cell emerges naturally from the assumption that reporter expression , conditioned on the same cellular environment , is represented by independent and identically distributed random variables . to allow each cell to be different from the others , we introduce",
    "independent identically distributed random variables @xmath14 for @xmath15 that represent the environments of cells ( as in @xcite ) .",
    "we posit that the cellular conditional random variables associated to the two reporters have the same distribution @xmath16 with mean @xmath17 and variance @xmath18 , both parameters being unique to the @xmath4-th cell : @xmath19 thinking of a two reporter experiment as `` random '' , in the sense that the states of cells @xmath20 are random , across cells we have @xmath21 where @xmath22 is the distribution of all the @xmath17s , with mean @xmath23 and variance @xmath24 , and @xmath25 that of all the @xmath26s , with mean @xmath27 and variance @xmath28 .",
    "in other words , both the mean and variance of reporter expression level is cell specific and the random variable @xmath26 and its mean @xmath29 represent the  within - cell \" variation as distinguished from the parameter @xmath24 which represents the  between - cell \" variability in the anova setting .    for any @xmath4 , the mean of @xmath30 or @xmath31 is @xmath23 , according to the following calculation : @xmath32 = e_{z_i}[e[c_i|z_i ] ] = e[m_i ] = \\mu .",
    "\\label{eqn : meanlaw}\\end{aligned}\\ ] ] the total variance in @xmath30 ( or @xmath31 ) can be calculated using the `` law of total variance '' : @xmath33 = e_{z_i}[var[c_i|z_i ] ] + var_{z_i}[e[c_i|z_i ] ] .",
    "\\label{eqn : tvlaw}\\ ] ] using the notation of the hierarchical model described above , and dropping the subscripts for expectation because they are clear by context , we have , for any @xmath4 , @xmath34 & = \\sigmasq \\;\\;\\;\\ ; & \\text{(within - cell variability ; intrinsic noise ) } , \\label{eq : intrnoise}\\\\ var[e[c_i|z_i ] ] & = \\sigmasqmu \\;\\;\\;\\ ; & \\text{(between - cell variability ; extrinsic noise ) } \\label{eq : extnoise}.\\end{aligned}\\ ] ] with this notation equation ( [ eqn : tvlaw ] ) becomes @xmath35 = e[var[c_i|z_i ] ] + var[e[c_i|z_i ] ] = \\sigmasq + \\sigmasqmu \\;\\;\\;\\;\\;\\;\\;\\;\\ ; & \\text{(total noise ) } \\label{eq : totnoise}.\\end{aligned}\\ ] ] this means that the marginal ( unconditional ) distributions of @xmath30 and @xmath31 are identical : @xmath36    in the next sections , we will derive the estimators for intrinsic and extrinsic noise , and examine the bias and mean squared error ( mse ) of each estimator . specifically , for any estimator @xmath37 , the mse of @xmath37 with respect to the true parameter @xmath38 is calculated as follows : @xmath39 & = e [ s - e[s ] + e[s ] - \\tau]^2 \\\\                & = e \\bigg [ ( s - e[s])^2 + ( e[s]-\\tau)^2 + 2(s - e[s])(e[s]-\\tau ) \\bigg ] \\\\                & = e[s - e[s]]^2 + e[e[s]-\\tau]^2   \\\\                & = var[s ] + ( e[s ] - \\tau)^2,\\end{aligned}\\ ] ] where @xmath40-\\tau$ ] is the bias of @xmath37 .",
    "starting with the law of total variance , the within - cell variability @xmath41 $ ] for cell @xmath4 can be written as : @xmath34 & = var[c_i ] - var[e[c_i|z_i ] ] \\notag\\\\ & = { \\frac{1}{2}}[var[c_i ] + var[y_i ] ] - cov[c_i , y_i ] \\notag\\\\ & = { \\frac{1}{2}}[var[c_i ] - 2cov[c_i , y_i ] + var[y_i ] ] \\notag\\\\ & = { \\frac{1}{2}}var[c_i - y_i ] .",
    "\\notag\\\\ & = { \\frac{1}{2}}\\left ( e[c_i - y_i]^2 - ( e[c_i - y_i])^2\\right ) \\notag \\ ] ]    this leads to the following unbiased estimator for the intrinsic noise : @xmath42 ^ 2 \\\\                & = \\frac{1}{2(n-1 ) } \\sumi ( c_i - y_i)^2 - \\frac{n}{2(n-1 ) } ( \\cbar - \\ybar)^2.\\end{aligned}\\ ] ]    to find the estimator that minimizes the mse , we consider estimators of the following general form @xmath43 assuming normality of the distribution @xmath22 ( i.e. , cell - specific means @xmath17 follow a normal distribution ) , as well as @xmath44 and @xmath45 , the mse is given by @xmath46 ^ 2 & = var [ \\sint ] + ( e [ \\sint ] - \\sigmasq)^2 \\\\      & = \\frac{1}{2a^2 } \\bigg [ ( 2n^2 + \\frac{6}{n } - 7)\\sigma^4 + 2(\\frac{2}{n}-1)\\sigmasq \\sigmasqmu + \\frac{1}{n } \\sigma^4_\\mu \\bigg ]   - 2(n-1)\\sigma^4 \\frac{1}{a } + \\sigma^4 . \\end{aligned}\\ ] ] the value of @xmath47 that minimizes this expression is @xmath48 see appendices [ sec : app.moments ] and [ sec : app.mse.intrinsic ] for the complete derivation .",
    "the analysis above can be simplified with an additional assumption , namely that @xmath49 . in some experiments",
    "this may be a natural assumption to make , whereas in others the condition is likely to be violated ; we comment on this in more detail in the discussion . here",
    "we proceed to note that assuming that @xmath49 , the estimator ( [ eqn : intrinsic.general ] ) simplifies to @xmath50 the unbiased estimator with this form is easily derived by observing that @xmath51 & = \\frac{1}{2a } \\sumi e[c_i - y_i]^2   = \\frac{1}{2a } \\sumi var [ c_i - y_i ] \\\\          & = \\frac{n}{2a } ( 2\\sigmasq + 2\\sigmasqmu - 2\\sigmasqmu ) = \\frac{n}{a } \\sigmasq.\\end{aligned}\\ ] ] thus , in order for @xmath52 to be unbiased the parameter @xmath47 must be equal to @xmath0 .",
    "the resulting formula is the elss formula in ( [ eqn : noise.int.elowitz ] ) .",
    "this makes clear that the assumption @xmath49 underlies the derivation of the elss intrinsic noise estimator .    in order to study",
    "the mean squared error and derive an estimator that minimizes it , we again assume normality of @xmath22 . the mse of @xmath53 is then given by @xmath54 ^ 2 & = var[\\sinttilde ] + ( e[\\sinttilde ] - \\sigmasq)^2 \\\\                  & = \\frac{n}{a^2 } ( 3\\epsilon + 2\\sigma^4 ) + ( \\frac{n}{a } \\sigmasq - \\sigmasq)^2.\\end{aligned}\\ ] ] assuming again that @xmath44 and @xmath45 , the mse simplifies to @xmath54 ^ 2 & = \\frac{2n}{a^2}\\sigma^4 + \\sigma^4 \\left ( \\left(\\frac{n}{a } \\right)^2 - \\frac{2n}{a } + 1 \\right ) \\\\                                & = \\frac{n\\sigma^4(n+2)}{a^2 } - \\frac{2n\\sigma^4}{a } + \\sigma^4,\\end{aligned}\\ ] ] which is minimized when @xmath55 ( see appendices [ sec : app.moments ] and [ sec : app.var.int ] for the complete derivation ) .",
    "to examine estimators for extrinsic noise , we again start with the law of total variance , this time noting that the within - cell variability @xmath56 $ ] can be written as :    @xmath57    & = &   e[e[c_i|z_i]^2]-e[e[c_i|z_i]]^2   \\notag\\\\ & =   & e[e[c_i|z_i]e[y_i|z_i]]-e[e[c_i|z_i]]^2 \\notag\\\\ & =   & e[e[c_iy_i|z_i]]-e[e[c_i|z_i]e[e[y_i|z_i ] ] \\notag\\\\ & = &   e[c_iy_i ] - e[c_i]e[y_i ] \\notag\\\\ & = &   cov[c_i , y_i ] .",
    "\\label{eqn : cov}\\end{aligned}\\ ] ]    this connection between the extrinsic noise , the law of total variance and the covariance of @xmath30 and @xmath31 was noted by hilfinger and paulsson in @xcite .",
    "formula ( [ eqn : cov ] ) leads to the following unbiased estimator for the extrinsic noise , as it is an unbiased estimator estimator for the covariance : @xmath58 we note that the elss estimator ( [ eqn : noise.ext.elowitz ] ) uses the scalar @xmath59 , which unlike the case of the intrinsic noise estimator ( [ eqn : noise.int.elowitz ] ) leads to a biased estimator in this case .    in order to find the estimator that minimizes the mse , we consider the following general estimator : @xmath60 we again assume that @xmath17 is normal and that @xmath44 and @xmath45 .",
    "the mse of @xmath61 is @xmath62 ^ 2 & = \\frac{n-1}{b^2 } ( \\sigma^2 + \\sigma_\\mu^2)^2 + \\frac{(n-1)^2}{nb^2 } \\sigma_\\mu^4 + \\bigg(\\frac{n-1}{b } \\sigmasqmu - \\sigmasqmu \\bigg)^2 \\\\      & = ( n-1)(\\sigma^2 + \\sigma_\\mu^2)^2\\frac{1}{b^2 } + ( n-1)^2 \\bigg ( 1+\\frac{1}{n } \\bigg ) \\sigma_\\mu^4 \\frac{1}{b^2 } - 2(n-1)\\sigma_\\mu^4 \\frac{1}{b } + \\sigma_\\mu^4 \\\\      & = \\left ( ( n-1)(\\sigma^2 + \\sigma_\\mu^2)^2 + ( n-1)^2 \\left ( 1+\\frac{1}{n } \\right ) \\sigma_\\mu^4 \\right)\\frac{1}{b^2 } - 2(n-1)\\sigma_\\mu^4 \\frac{1}{b } + \\sigma_\\mu^4,\\end{aligned}\\ ] ] which is minimized when @xmath63 @xmath64    it is interesting to note that ( [ eq : bforext ] ) comprises two parts : the first , @xmath65 converges to @xmath66 as @xmath67 , while the second , @xmath68 is equal to @xmath69 where @xmath70 is the correlation between vectors @xmath71 and @xmath72",
    ". see appendices [ sec : app.moments ] and [ sec : app.var.ext ] for more details .",
    "figure 3a of @xcite shows a scatterplot of data @xmath3 for an experiment and suggests thinking of intrinsic and extrinsic noise geometrically in terms of projection of the points onto a pair of orthogonal lines . while this geometric interpretation of noise agrees exactly with the elss intrinsic noise formula , the interpretation of extrinsic noise is more subtle .",
    "here we complete the picture .",
    ", which is perpendicular to @xmath73 . in other words , it is the average of the squared lengths @xmath74 .",
    "the red point is the projection of point @xmath75 onto the line @xmath73 .",
    "the green point is the centroid .",
    "see the main text for additional detail .",
    "the extrinsic noise , or the between - cell variability , is the sample covariance between @xmath12 and @xmath13 .",
    "the colored triangles around the blue point illustrate the geometric interpretation of the sample covariance : it is the average ( signed ) area of triangles formed by pairs of data points : green triangles in q1 and q3 ( some not shown ) represent a positive contribution to the covariance , whereas the magenta triangles in q2 and q4 a negative contribution .",
    "since most data points lie in the 1st ( q1 ) and 3rd ( q3 ) quadrants relative to the blue point , most of the contribution involving the blue point is positive .",
    "similarly , since most pairs of data points can be connected by a positively signed line , their positive contribution will result in a positive covariance . in @xcite",
    "the direction along the line @xmath73 is labeled extrinsic , which makes sense in terms of the intuition for positive sample covariance .",
    "however we have placed that label `` extrinsic '' in quotes because the extrinsic noise estimator corresponding directly to the sample variance for points projected onto the line @xmath73 ( in analogy with intrinsic noise ) is heavily biased and not usable in practice . ]    to understand the intuition behind figure 3a in @xcite , we have redrawn it in a format that highlights the math ( fig  [ fig : noise ] ) .",
    "the projection of a point @xmath3 onto the line @xmath73 is the point @xmath76 , shown as the red point in fig .",
    "[ fig : noise ] .",
    "the intrinsic noise , as estimated by the unbiased estimator ( [ eqn : noise.int.elowitz ] ) is then the mean squared distance from the origin to the points projected onto the line @xmath77 .    the elss estimate for the extrinsic noise is the sample covariance .",
    "intuitively , it indicates how the measurements of one reporter track that of the other across cells .",
    "the geometric meaning of the sample covariance in fig .",
    "[ fig : noise ] is based on an alternative formulation of sample covariance @xcite : @xmath78 this formulation of the sample covariance has the interpretation of being an average of the signed area of triangles associated to pairs of points , and is very different from what might be considered at first glance an appropriate anology to intrinsic noise , namely the sample variance along the line @xmath73 .",
    "the estimate corresponding to the sample variance of the projected points along the line @xmath73 , using as a mean the projected centroid @xmath79 which is shown as the green point in fig .",
    "[ fig : noise ] , turns out to be biased by an amount equal to the total noise .",
    "using @xmath80 the bias is @xmath81 - \\sigmasqmu & = \\frac{1}{2 } var [ c_i + y_i ] - \\sigmasqmu\\\\ & = \\frac{1}{2 } \\left(var[c_i ] + var[y_i ] + 2 cov [ c_i , y_i ] \\right ) -\\sigmasqmu \\\\ & = \\frac{1}{2 } \\left ( 2(\\sigmasq + \\sigmasqmu ) + 2\\sigmasqmu\\right ) -\\sigmasqmu = \\sigmasq + \\sigmasqmu\\end{aligned}\\ ] ] which is the true total noise .",
    "the above calculation also shows that if the intrinsic and extrinsic noise are both estimated as variances along the projections to the lines @xmath77 and @xmath73 respectively , then the total noise will be overestimated by a factor of two .    in summary ,",
    "the caption to figure 3a in @xcite is completely accurate in stating that `` spread of points perpendicular to the diagonal line on which cfp and yfp intensities are equal corresponds to intrinsic noise , whereas spread parallel to this line is increased by extrinsic noise . ''",
    "however the geometric interpretation of covariance makes precise exactly _ how _ an increase in extrinsic noise relates to the spread of points in the direction of the line @xmath73 .",
    "our hierarchical model , as well as the anova interpretation , is consistent with the model in elowitz _",
    "_ @xcite ; both models assume that within each cell there are two distributions for the expression of the two reporter genes and that they have the same true mean and true variance . with the normality assumption",
    ", this means that the two reporters have identical distributions .",
    "measured the single - color distributions of strains that contained lac - repressible promoter pairs , which verified that this was a reasonable assumption in the case of cyan fluorescent protein ( cfp ) and yellow fluorescent protein ( yfp ) in their experiment .",
    "other studies have adapted this system and used other reporter combinations that may have markedly different distributions .",
    "for example , yang _",
    "@xcite used cfp and mcherry with vastly different ranges of intensity values : whereas cfp varied from 0 to 6000 ( arbitrary units ; i.e. , a.u . ) , mcherry could vary from 0 to 9000 ( a.u . )",
    "; see fig .",
    "3a from their paper .",
    "in contrast , another study @xcite normalized the two reporters used in their experiment ( zsgreen and mcherry ) to have the same mean . however , the variances , or more generally , the two distributions , also need to be the same . since the decomposition of the total noise depends on the assumption that both reporters in the same cellular environment have similar variance ( see ( [ eqn : model.ci ] ) and ( [ eqn : model.yi ] ) ) , we recommend that in general a quantile normalization which normalizes the reporter measurements to identical distributions be performed before the calculations of noise components . such a normalization procedure is standard in many settings requiring similar assumptions",
    "we have derived the estimators that are optimal for minimizing bias or the mse ( summarized in table  [ table : estimators ] ) .",
    "the elss estimator in ( [ eqn : noise.int.elowitz ] ) is in fact a special case of the general estimator under the assumption that @xmath82 , and is appropriate for data that are normalized to have the same sample mean ( i.e. , @xmath83 ) . in @xcite , the intensities of the two reporters were normalized to have mean 1 . in the case where the assumption of equal reporter means does not hold , the general estimator is more suitable .",
    "similar to the estimators for the intrinsic noise , we derived two estimators for extrinsic noise , optimized for bias and for mse respectively ( table  [ table : estimators ] ) .",
    "the sample size @xmath0 is the leading term in the denominator of all the optimal ( in either the bias or mse sense ) intrinsic and extrinsic noise estimators . as a result",
    ", the unbiased estimator has the same form as the min - mse estimator for large @xmath0 ( table  [ table : estimators ] ) . for extrinsic noise ,",
    "the general estimates converge to the elss estimate ( table  [ table : estimators ] ) . for intrinsic noise , assuming @xmath83 , the elss estimate is optimal for bias and mse for large @xmath0 and optimal for bias at small @xmath0 .",
    "indeed , in @xcite , typical values for @xmath0 are greater than 100 , making the elss formulas suitable for the analyses performed ( with the assumption of equal mean satisfied ) .",
    "however , our derivations indicate that the two types of noise can be estimated using fewer cells .    as a general rule we recommend computing the inverse squared correlation between the @xmath12 and @xmath13 values and applying a correction if it is comparable ( up to a small factor ) to the sample size .",
    "c|p1.6in|p3.0in|p2.1 in & & large @xmath0 + & minimizing bias ( unbiased ) & minimizing mse & +   +   + general & @xmath84 $ ] & @xmath85 $ ] , where @xmath86 & @xmath87 $ ] + assuming @xmath82 & @xmath88 & @xmath89 & @xmath88 + & ( elss estimator ) & & ( elss estimator ) +   +   + general & @xmath90 & @xmath91 & @xmath92 + & & & ( elss estimator ) +    [ table : estimators ]      we have seen that the proportion of the between - cell variability to total variability is the correlation @xmath70 .",
    "this leads to a simple approach for estimating the relative magnitude of the two types of noise : one can compute the sample correlation of the expression of the two reporters , @xmath93 , and the ratio of extrinsic to intrinsic noise is then estimated by @xmath94 $ ] .",
    "for example , in elowitz et al @xcite , the sample correlation @xmath95 is roughly 0.7 , which implies that about 70% of the total noise is extrinsic noise and the ratio of extrinsic to intrinsic noise is 2.33 .",
    "this project began as a result of discussion during a journal club meeting of jonathan pritchard s group that a.f .",
    "was attending .",
    "we thank michael elowitz and peter swain for facilitating reanalysis of the data from @xcite .",
    "was partially supported by k99hg007368 ( nih / nhgri ) .",
    "l.p . was partially supported by nih grants r01 hg006129 and r01 dk094699",
    ".    9    clive g. bowsher and peter s. swain . identifying sources of variation and the flow of information in biochemical networks .",
    "_ proceedings of the national academy of sciences _ , 109 , 20 , e1320-e1328 , 2012 .",
    "michael b. elowitz , arnold j. levine , eric d. siggia and peter s. swain .",
    "stochastic gene expression in a single cell .",
    "_ science _",
    ", 297 , 1183 - 1186 , 2002 .",
    "kevin hayes . a geometrical interpretation of an alternative formula for the sample covariance . _ the american statistician _ , 65 , 2 , 110 - 112 , 2011 .",
    "peter m. heffernan .",
    "new measures of spread and a simpler formula for the normal distribution .",
    "_ the american statistician _ , 42 , 2 , 100 - 102 , 1988 .",
    "andreas hilfinger and johan paulsson .",
    "separating intrinsic from extrinsic fluctuations in dynamic biological systems . _",
    "proceedings of the national academy of sciences _ , 108 , 29 , 12167 - 12172 , 2011 .",
    "jrn m. schmiedel , sandy l. klemm , yannan zheng , apratim sahay , nils blthgen , debora s. marks , alexander van oudenaarden .",
    "microrna control of protein expression noise .",
    "_ science _ , 348 , 128 - 132 , 2015 .",
    "sora yang , seunghyeon kim , yu rim lim , cheolhee kim , hyeong jeon an , ji - hyun kim , jaeyoung sung and nam ki lee .",
    "contribution of rna polymerase concentration variation to protein expression noise . _ nature communications _ , 5 , 4761 , 2014",
    "assuming that @xmath96 , we have @xmath97 ^ 3 & = 0 ; \\\\",
    "e[m_i - \\mu]^4 & = 3\\sigma_{\\mu}^4.\\end{aligned}\\ ] ] we can compute the third and fourth moments of @xmath17 as follows : @xmath97 ^ 3 & = e[m_i^2 + \\mu^2 - 2m_i \\mu)(m_i - \\mu ] \\\\                 & = e[m_i^3 - 2m_i^2 \\mu + m_i \\mu^2 - m_i^2 \\mu - \\mu^3 + 2m_i \\mu^2 ] \\\\                 & = e[m_i^3 - 3m_i^2 \\mu + 3m_i \\mu^2 - \\mu^3 ]   \\\\                 & = e[m_i^3 ] - 3\\mu ( \\sigma^2_\\mu + \\mu^2 ) + 3\\mu^3 - \\mu^3 \\\\                 & = e[m_i^3 ] - 3\\mu \\sigma^2_\\mu - \\mu^3,\\end{aligned}\\ ] ] which gives @xmath98= 3\\mu \\sigma^2_\\mu + \\mu^3.\\end{aligned}\\ ] ] @xmath97 ^ 4 & = e[m^2_i - 2m_i \\mu + \\mu^2]^2   \\\\                 & = e[m^4_i + \\mu^4 + 4m^2_i \\mu^2 + 2m^2_i \\mu^2 - 4m^3_i \\mu - 4m_i \\mu^3 ]   \\\\                 & = e[m^4_i + \\mu^4 + 6m^2_i \\mu^2 - 4m^3_i \\mu - 4m_i \\mu^3 ] \\\\                 & = e[m^4_i ] + \\mu^4 + 6\\mu^2 ( \\sigma^2_\\mu + \\mu^2 )",
    "- 4\\mu(3\\mu \\sigma^2_\\mu + \\mu^3 ) - 4\\mu^4 \\\\                 & = e[m^4_i ] + \\mu^4 + 6\\mu^2 \\sigma^2_\\mu + 6\\mu^4 - 12\\mu^2 \\sigma^2_\\mu - 4\\mu^4 - 4\\mu^4 \\\\                 & = e[m^4_i ] - 6\\mu^2 \\sigma^2_\\mu - \\mu^4,\\end{aligned}\\ ] ] which gives @xmath99 = 3\\sigma^4_\\mu + 6\\mu^2 \\sigma^2_\\mu + \\mu^4.\\end{aligned}\\ ] ] for the random variable @xmath30 , since @xmath100 , such that @xmath101 & = \\sigmasq ; \\\\",
    "\\var[\\sigmasqi ] & = \\epsilon,\\end{aligned}\\ ] ] we have @xmath102 & = e[e[c^4_i| z_i ] ] \\\\           & = e[3\\sigma^4_i + 6m^2_i \\sigma^2_i + m^4_i ] \\\\           & = 3(\\epsilon + \\sigma^4 ) + 6(\\sigmasqmu + \\mu^2 ) \\sigmasq + 3\\sigma^4_\\mu + 6\\mu^2 \\sigma^2_\\mu + \\mu^4 \\\\           & = 3\\epsilon + 3\\sigma^4 + 6\\sigmasqmu \\sigmasq + 6\\mu^2 \\sigmasq",
    "+ 3\\sigma^4_\\mu + 6\\mu^2 \\sigma^2_\\mu + \\mu^4.\\end{aligned}\\ ] ] further assuming that @xmath44 , i.e. , the means are all 0 , and that @xmath45 , which means that the variability is the same across cells , we have @xmath98 & = 0 \\\\ e[m_i^4 ] & = 3\\sigma^4_\\mu;\\end{aligned}\\ ] ] and @xmath103 & = 0 \\\\ e[c^4_i ] & = 3 ( \\sigmasq + \\sigmasqmu)^2.\\end{aligned}\\ ] ]",
    "the general form of the estimator for intrinsic noise is @xmath104 thus @xmath105 = \\frac{1}{4a^2 } \\left ( var \\left[\\sum(c_i - y_i)^2 ] + n^2 var[(\\bigcbar - \\bigybar)^2\\right ] - 2n cov \\bigg[\\sum(c_i - y_i)^2 , ( \\bigcbar - \\bigybar)^2 \\bigg ] \\right).\\end{aligned}\\ ] ] below we will assume normality , as well as @xmath44 and @xmath45 , to facilitate the derivation .    first , we note that @xmath106 & = var [ \\bigcbar^2 - 2\\bigcbar \\bigybar + \\bigybar^2 ] \\\\      & = var[\\bigcbar^2 ] + 4var [ \\bigcbar \\bigybar ] + var [ \\bigybar^2]- 4 cov [ \\bigcbar^2 , \\bigcbar \\bigybar ] - 4 cov [ \\bigybar^2 , \\bigcbar \\bigybar ] + 2 cov [ \\bigcbar^2 , \\bigybar^2].\\end{aligned}\\ ] ] @xmath107&= var \\bigg[\\frac{c_1 + \\dots + c_n}{n } \\cdot \\frac{c_1 + \\dots + c_n}{n}\\bigg ] \\\\      & = \\frac{1}{n^4 } var \\left[\\sum c^2_k + \\sum_{i\\neq j } c_i c_j\\right ] \\\\      & = \\frac{1}{n^4 } \\left ( var \\sum c^2_k + var [ \\sum_{i \\neq j } c_i c_j ] + 2 cov [ \\sum c^2_k , \\sum_{i \\neq j } c_i c_j ] \\right ) \\\\      & = \\frac{1}{n^4 } \\left ( 2n(\\sigmasq + \\sigmasqmu)^2 + n(n-1)(\\sigmasq + \\sigmasqmu)^2 + 0 \\right ) \\\\      & = \\frac{n+1}{n^3 } ( \\sigmasq + \\sigmasqmu)^2.\\end{aligned}\\ ] ] this is because @xmath108 & = \\sum_{i \\neq j } var [ c_i c_j ] \\\\      & = \\sum_{i \\neq j } \\left ( ec^2_i c^2_j - ( ec_i c_j)^2 \\right ) \\\\      & = \\sum_{i \\neq j } \\left ( ( \\sigmasq + \\sigmasqmu)^2 - 0 \\right ) \\\\      & = n(n-1 ) ( \\sigmasq + \\sigmasqmu)^2.\\end{aligned}\\ ] ]    additionally , @xmath109 & = \\frac{1}{n^2 } var [ n\\bigcbar \\bigybar ] \\\\      & = \\frac{1}{n^2 } \\left ( ( \\sigmasq + \\sigmasqmu)^2 + \\frac{\\sigma^4_\\mu}{n } \\right ) \\\\      & = \\frac{1}{n^2 } ( \\sigmasq + \\sigmasqmu)^2 + \\frac{\\sigma^4_\\mu}{n^3}.\\end{aligned}\\ ] ]    @xmath110&= \\frac{1}{n^4 } cov \\left[\\sum c^2_k + \\sum_{i \\neq j } c_i c_j , \\sum c_l y_l + \\sum_{m \\neq r } c_m c_r\\right ] \\\\      & = \\frac{1}{n^4 } \\bigg ( cov \\left[\\sum c^2_k , \\sum c_l y_l\\right ] + cov \\left[\\sum c^2_k , \\sum_{m \\neq r } c_m c_r\\right ] \\\\      & \\;\\;\\;\\ ; + cov \\left[\\sum_{i \\neq j } c_i c_j , \\sum c_l y_l\\right ] + cov \\left[\\sum_{i \\neq j } c_i c_j , \\sum_{m \\neq r } c_m c_r\\right ] \\bigg).\\end{aligned}\\ ] ]    @xmath111 & = cov \\left[\\sum c^2_k , \\sum c_k y_k\\right ] \\\\      & = \\sum ( e[c^3_k y_k ] - e[c^2_k ] e[c_k y_k ] ) \\\\      & = \\sum \\bigg [ 3\\sigmasq \\sigmasqmu + 3 \\sigma^4_\\mu - ( \\sigmasq + \\sigmasqmu ) \\sigmasqmu \\bigg ] \\\\      & = 2n \\sigmasqmu ( \\sigmasq + \\sigmasqmu).\\end{aligned}\\ ] ]    for @xmath112 $ ] , since @xmath113 = e[c^3_i y_j ] - e[c^2_i ] e[c_i y_j ] = 0\\end{aligned}\\ ] ] and @xmath114 = e[c^2_ic_j y_k ] - e[c^2_i ] e[c_j y_k ] = 0,\\end{aligned}\\ ] ] we have @xmath115 = 0.\\end{aligned}\\ ] ] for @xmath116 $ ] , since @xmath117 = e [ c^2_i y_i c_j]- e [ c_i c_j ] e [ c_i y_i ] = 0\\end{aligned}\\ ] ] and @xmath118 = e [ c_k c_l c_i",
    "y_i ] - e [ c_k c_l ] e [ c_i y_i ] = 0,\\end{aligned}\\ ] ] we have @xmath119 = 0.\\end{aligned}\\ ] ] additionally , @xmath120 & = \\sum_{i , j , m , r } cov [ c_i c_j , c_m c_r ] \\\\      & = \\sum_{i\\neq j } cov [ c_i c_j , c_i c_j ] \\\\      & = \\sum_{i \\neq j } var [ c_i c_j ] \\\\      & = n(n-1 ) ( \\sigmasq + \\sigmasqmu)^2.\\end{aligned}\\ ] ] therefore , @xmath110 = \\frac{2}{n^3 } \\sigmasqmu ( \\sigmasq + \\sigmasqmu ) + \\frac{n-1}{n^3 } ( \\sigmasq + \\sigmasqmu)^2.\\end{aligned}\\ ] ]    furthermore , @xmath121 & = \\frac{1}{n^4 } cov \\left[\\sum c^2_k + \\sum_{i \\neq j } c_i c_j , \\sum y^2_l + \\sum_{m \\neq r } y_m y_r\\right ] \\\\      & = \\frac{1}{n^4 } \\bigg ( cov \\left[\\sum c^2_k , \\sum y^2_l\\right ] + cov \\left[\\sum c^2_k , \\sum_{m \\neq r } y_m y_r\\right ] \\\\      & \\;\\;\\;\\ ; + cov\\left[\\sum y^2_l , \\sum_{i \\neq j } c_i c_j\\right ] + cov \\left[\\sum_{i \\neq j } c_i c_j , \\sum_{m \\neq r } y_m y_r\\right ] \\bigg).\\end{aligned}\\ ] ] in the expression above , @xmath122 = 2n \\sigma^4_\\mu;\\end{aligned}\\ ] ] @xmath123 =   cov \\left[\\sum y^2_l , \\sum_{i \\neq j } c_i c_j\\right ] = 0;\\end{aligned}\\ ] ] @xmath124 & = \\sum_{i \\neq j } cov [ c_i c_j , y_i y_j ] \\\\      & = \\sum_{i \\neq j } ( e [ c_i c_j y_i y_j ] - e [ c_i c_j ] e [ y_i y_j ] ) \\\\      & = \\sum_{i \\neq j } ( e[c_i y_i ] e [ c_j y_j ] - 0 ) \\\\      & = n(n-1 ) \\sigma^4_\\mu.\\end{aligned}\\ ] ] then we have @xmath121 & = \\frac{1}{n^4 } \\bigg ( 2n \\sigma^4_\\mu + n(n-1 ) \\sigma^4_\\mu \\bigg ) \\\\      & = \\frac{n+1}{n^3 } \\sigma^4_\\mu.\\end{aligned}\\ ] ]    putting the terms together , we have @xmath125 ^ 2 & = var [ \\bigcbar^2 ] + 4var [ \\bigcbar \\bigybar ] + var [ \\bigybar^2 ]           - 4 cov [ \\bigcbar^2 , \\bigcbar \\bigybar ] - 4 cov [ \\bigybar^2 , \\bigcbar \\bigybar ] + 2 cov [ \\bigcbar^2 , \\bigybar^2 ] \\\\           & = \\frac{2(n+1)}{n^3 } ( \\sigmasq + \\sigmasqmu)^2 + \\frac{4}{n^2 } ( \\sigmasq + \\sigmasqmu)^2 + \\frac{4\\sigma^4_\\mu}{n^3 } - \\frac{16}{n^3 } \\sigmasqmu ( \\sigmasq + \\sigmasqmu ) - \\frac{8(n-1)}{n^3 } ( \\sigmasq + \\sigmasqmu)^2 \\\\           & \\;\\;\\;\\ ; + \\frac{2(n+1)}{n^3 } \\sigma^4_\\mu \\\\           & = \\frac{2}{n^3 } \\bigg ( ( 6-n ) ( \\sigmasq + \\sigmasqmu)^2 - 8\\sigmasqmu ( \\sigmasq + \\sigmasqmu ) + ( n+3 ) \\sigma^4_\\mu \\bigg ) \\\\           & = \\frac{2}{n^3 } \\left ( ( 6-n)\\sigma^4 + ( 4 - 2n ) \\sigmasq \\sigmasqmu + \\sigma^4_\\mu \\right).\\end{aligned}\\ ] ]    next , we note that @xmath126 & = \\sum cov \\bigg [ ( c_i - y_i)^2 , ( \\bigcbar - \\bigybar)^2 \\bigg ] \\\\      & = \\sum \\bigg ( e[(c^2_i - 2 c_i y_i + y^2_i)(\\bigcbar^2 - 2\\bigcbar \\bigybar + \\bigybar^2 ) ] \\\\      & \\;\\;\\;\\ ; - e[(c^2_i - 2 c_i y_i + y^2_i ) ] e[(\\bigcbar^2 - 2\\bigcbar \\bigybar + \\bigybar^2 ) ] \\bigg),\\end{aligned}\\ ] ] where @xmath127 \\\\ & = e\\bigg [ c^2_i \\bigcbar^2 - 2 c_i y_i \\bigcbar^2 + y^2_i \\bigcbar^2 - 2 c^2_i \\bigcbar \\bigybar + 4 c_i y_i \\bigcbar \\bigybar - 2 y^2_i \\bigcbar \\bigybar + c^2_i \\bigybar^2 - 2 c_i y_i \\bigybar^2 + y^2_i \\bigybar^2 \\bigg],\\end{aligned}\\ ] ] and @xmath128   = 2(\\sigmasq + \\sigmasqmu ) - 2 \\sigmasqmu = 2\\sigmasq,\\end{aligned}\\ ] ] @xmath129 = \\frac{2}{n } ( \\sigmasq + \\sigmasqmu ) - \\frac{2}{n } \\sigmasqmu = \\frac{2\\sigmasq}{n}.\\end{aligned}\\ ] ]    @xmath130 & = \\frac{1}{n^2 } e [ c^2_i \\left(\\sum c^2_k + \\sum_{i \\neq j } c_i c_j\\right ) ] \\\\      & = \\frac{1}{n^2 } \\bigg ( e[c^4_i ] + \\sum_{k \\neq i } e[c^2_i ] e[c^2_k ] + \\sum_{i \\neq j } e [ c^2_k c_i c_j ] \\bigg ) \\\\      & = \\frac{1}{n^2 } \\bigg [ 3(\\sigmasq + \\sigmasqmu)^2 + ( n-1 ) ( \\sigmasq + \\sigmasqmu)^2 + 0 \\bigg ] \\\\      & = \\frac{n+2}{n^2 } ( \\sigmasq + \\sigmasqmu)^2.\\end{aligned}\\ ] ]    @xmath131 & = e \\bigg [ c_i y_i \\frac{\\sum c^2_j + \\sum_{k \\neq l } c_k c_l}{n^2 } \\bigg ] \\\\      & = \\frac{1}{n^2 } \\bigg ( e [ c_i y_i c^2_i ] + \\sum_{j \\neq i } e [ c_i y_i c^2_j ] + \\sum_{k \\neq l } e [ c_i y_i c_k c_l ] \\bigg ) \\\\      & = \\frac{1}{n^2 } \\bigg ( 3(\\sigmasq \\sigmasqmu + \\sigma^4_\\mu ) + ( n-1)(\\sigmasq \\sigmasqmu + \\sigma^4_\\mu ) + 0 \\bigg ) \\\\      & = \\frac{n+2}{n^2 } \\sigmasqmu ( \\sigmasq + \\sigmasqmu).\\end{aligned}\\ ] ]    @xmath132 & = e",
    "\\bigg [ y^2_i \\frac{\\sum c^2_j + \\sum_{k \\neq l } c_k c_l}{n^2 } \\bigg ] \\\\      & = \\frac{1}{n^2 } \\bigg ( e [ y^2_i c^2_i ] + \\sum_{j \\neq i } e [ y^2_i c^2_j ] + \\sum_{k \\neq l } e[y^2_i c_k c_l ] \\bigg ) \\\\      & = \\frac{1}{n^2 } \\bigg ( ( \\sigmasq + \\sigmasqmu)^2 + 2 \\sigma^4_\\mu + ( n-1)(\\sigmasq + \\sigmasqmu)^2 + 0 \\bigg ) \\\\      & = \\frac{1}{n^2 } \\bigg ( n(\\sigmasq + \\sigmasqmu)^2 + 2\\sigma^4_\\mu \\bigg).\\end{aligned}\\ ] ]    @xmath133 & = \\frac{1}{n^2 } \\bigg ( e[c^2_i c_i y_i ] + \\sum_{j \\neq i } e [ c^2_i c_j y_j ] + \\sum_{k \\neq l } e [ c^2_i c_k y_l ] \\bigg ) \\\\      & = \\frac{1}{n^2 } \\bigg ( 3(\\sigmasq \\sigmasqmu + \\sigma^4_\\mu ) + ( n-1)(\\sigmasq \\sigmasqmu + \\sigma^4_\\mu ) + 0 \\bigg ) \\\\      & = \\frac{n+2}{n^2 } \\sigmasqmu ( \\sigmasq + \\sigmasqmu).\\end{aligned}\\ ] ]    @xmath134&= \\frac{1}{n^2 } \\bigg ( e[c^2_i y^2_i   ] + \\sum_{j \\neq i } e [ c_i y_i c_j y_j ] + \\sum_{k \\neq l } e [ c_i y_i c_k y_l ] \\bigg ) \\\\      & = \\frac{1}{n^2 } \\bigg ( ( \\sigmasq + \\sigmasqmu)^2 + 2\\sigma^4_\\mu+ ( n-1 ) \\sigma^4_\\mu + 0 \\bigg ) \\\\      & = \\frac{1}{n^2 } \\bigg ( ( \\sigmasq + \\sigmasqmu)^2 + ( n+1 ) \\sigma^4_\\mu \\bigg).\\end{aligned}\\ ] ]    additionally , @xmath135 & = e[c^2_i \\bigcbar \\bigybar ] = \\frac{n+2}{n^2 } \\sigmasqmu ( \\sigmasq + \\sigmasqmu ) ; \\\\ e [ c^2_i \\bigybar^2 ] & = e[y^2_i \\bigcbar^2 ] = \\frac{1}{n^2 } \\bigg ( n(\\sigmasq + \\sigmasqmu)^2 + 2\\sigma^4_\\mu \\bigg ) ; \\\\",
    "e[c_i y_i \\bigybar^2 ] & = e[c_i y_i \\bigcbar^2 ] = \\frac{n+2}{n^2 } \\sigmasqmu ( \\sigmasq + \\sigmasqmu ) ; \\\\ e [ y^2_i \\bigybar^2 ] & = e [ c^2_i \\bigcbar^2 ] = \\frac{n+2}{n^2 } ( \\sigmasq + \\sigmasqmu)^2.\\end{aligned}\\ ] ] therefore , @xmath136\\\\ & = e\\bigg[c^2_i \\bigcbar^2 - 2 c_i y_i \\bigcbar^2 + y^2_i \\bigcbar^2 - 2 c^2_i \\bigcbar \\bigybar + 4 c_i y_i \\bigcbar \\bigybar - 2 y^2_i \\bigcbar \\bigybar + c^2_i \\bigybar^2 - 2 c_i y_i \\bigybar^2 + y^2_i \\bigybar^2 \\bigg ] \\\\ & = \\frac{2(n+2)}{n^2 } ( \\sigmasq + \\sigmasqmu)^2 - \\frac{4(n+2)}{n^2 } \\sigmasqmu ( \\sigmasq + \\sigmasqmu ) + \\frac{2}{n^2 } \\bigg ( n(\\sigmasq + \\sigmasqmu)^2 + 2\\sigma^4_\\mu \\bigg ) \\\\ & \\;\\;\\;\\ ; - \\frac{4(n+2)}{n^2 } \\sigmasqmu ( \\sigmasq + \\sigmasqmu ) + \\frac{4}{n^2 } \\bigg ( ( \\sigmasq + \\sigmasqmu)^2 + ( n+1 ) \\sigma^4_\\mu \\bigg ) \\\\ & = \\frac{4(n+2 ) \\sigma^4}{n^2}.\\end{aligned}\\ ] ]    so we have @xmath137 & = \\sum cov \\bigg [ ( c_i - y_i)^2 , ( \\bigcbar - \\bigybar)^2 \\bigg ] \\\\      & = \\sum \\bigg ( e(c^2_i - 2 c_i y_i + y^2_i ) ( \\bigcbar^2 - 2\\bigcbar \\bigybar + \\bigybar^2 ) \\\\      & \\;\\;\\;\\ ; - e(c^2_i - 2 c_i y_i + y^2_i ) e(\\bigcbar^2 - 2\\bigcbar \\bigybar + \\bigybar^2 ) \\bigg ) \\\\      & = n \\bigg ( \\frac{4(n+2 ) \\sigma^4}{n^2 } - 2\\sigmasq \\frac{2\\sigmasq}{n } \\bigg ) \\\\      & = \\frac{8 \\sigma^4}{n}.\\end{aligned}\\ ] ]    the variance of the estimator is then @xmath138 & = \\frac{1}{4a^2 } \\bigg ( var \\left[\\sum(c_i - y_i)^2 \\right]+ n^2 var[\\bigcbar - \\bigybar]^2 - 2n cov \\bigg[\\sum(c_i - y_i)^2 , ( \\bigcbar - \\bigybar)^2 \\bigg ] \\bigg ) \\\\      & = \\frac{1}{4a^2 } \\bigg ( 8n \\sigma^4 + \\frac{2}{n } \\bigg ( ( 6-n)\\sigma^4 + ( 4 - 2n ) \\sigmasq \\sigmasqmu + \\sigma^4_\\mu \\bigg ) - 16 \\sigma^4 \\bigg ) \\\\      &",
    "= \\frac{1}{2a^2 } \\bigg ( 4n \\sigma^4 + \\frac{1}{n } \\bigg ( ( 6-n)\\sigma^4 + ( 4 - 2n ) \\sigmasq \\sigmasqmu + \\sigma^4_\\mu \\bigg ) - 8 \\sigma^4 \\bigg).\\end{aligned}\\ ] ]    the expectation of the estimator is @xmath139 = \\frac{1}{2a } \\bigg ( \\sum e[c_i - y_i]^2 - n e[\\bigcbar - \\bigybar]^2 \\bigg),\\end{aligned}\\ ] ] where @xmath140 & = var [ c_i - y_i ] \\\\      & = var [ c_i ] + var [ y_i ] - 2 cov [ c_i , y_i ] \\\\      & = 2(\\sigmasq + \\sigmasqmu ) - 2 \\sigmasqmu = 2 \\sigmasq,\\end{aligned}\\ ] ] and @xmath141 & = var [ \\bigcbar - \\bigybar ] \\\\      & = var [ \\bigcbar ] + var [ \\bigybar ] - 2 cov [ \\bigcbar , \\bigybar ] \\\\      & = \\frac{2}{n } ( \\sigmasq + \\sigmasqmu ) - \\frac{2}{n } \\sigmasqmu = \\frac{2 \\sigmasq}{n}.\\end{aligned}\\ ] ] hence , @xmath142 = \\frac{1}{2a } ( 2n\\sigmasq - 2\\sigmasq ) = \\frac{n-1}{a } \\sigmasq.\\end{aligned}\\ ] ] the mse of the estimator is then @xmath143 & = var [ s ] + ( e [ s ] - \\sigmasq)^2 \\\\      & = \\frac{1}{2a^2 } \\bigg ( 4n \\sigma^4 + \\frac{1}{n } \\bigg ( ( 6-n)\\sigma^4 + ( 4 - 2n ) \\sigmasq \\sigmasqmu + \\sigma^4_\\mu \\bigg ) - 8 \\sigma^4 \\bigg ) \\\\      & \\;\\;\\;\\ ; + \\bigg ( \\frac{n-1}{a } - 1 \\bigg)^2 \\sigma^4 \\\\      & = \\frac{1}{2a^2 } \\bigg ( 4n \\sigma^4 + \\frac{1}{n } \\bigg ( ( 6-n)\\sigma^4 + ( 4 - 2n ) \\sigmasq \\sigmasqmu + \\sigma^4_\\mu \\bigg ) - 8 \\sigma^4 + 2(n-1)^2 \\sigma^4 \\bigg ) \\\\      & \\;\\;\\;\\ ; - 2(n-1)\\sigma^4 \\frac{1}{a } + \\sigma^4 \\\\      & = \\frac{1}{2a^2 } \\bigg ( ( 2n^2 + \\frac{6}{n } - 7)\\sigma^4 + 2(\\frac{2}{n}-1)\\sigmasq \\sigmasqmu + \\frac{1}{n } \\sigma^4_\\mu \\bigg )   - 2(n-1)\\sigma^4 \\frac{1}{a } + \\sigma^4 . \\end{aligned}\\ ] ] the value of @xmath47 that minimizes this mse is @xmath48",
    "@xmath145 & = \\frac{1}{4a^2 } var \\bigg [ \\sumi ( c_i - y_i)^2 \\bigg ] \\\\           & = \\frac{1}{4a^2 } var \\bigg [ \\sumi \\bigg(c_i^2 + y_i^2 - 2c_i y_i \\bigg ) \\bigg ] \\\\           & = \\frac{1}{4a^2 } var \\bigg [ \\sumi c_i^2 + \\sumi y_i^2 - 2 \\sumi c_i , y_i \\bigg ]   \\\\           & = \\frac{1}{4a^2 } \\bigg ( var \\left[\\sumi c_i^2\\right ] + var \\left[\\sumi y_i^2\\right ] + 4 var\\left [ \\sumi c_i y_i\\right ] + 2cov \\left[\\sumi c_i^2 , \\sumi y_i^2\\right ] \\\\           & \\;\\;\\;\\ ; - 4 cov \\left[\\sumi c_i^2 , \\sumi",
    "c_i y_i\\right ] - 4 cov \\left[\\sumi y_i^2 , \\sumi c_i y_i\\right ] \\bigg).\\end{aligned}\\ ] ]    the individual terms can be computed as follows : @xmath146 & = \\sumi var [ c_i^2 ] \\\\                   & = \\sumi \\bigg(e[c_i^4 ] - ( e [ c_i^2])^2 \\bigg ) \\\\                   & = \\sumi \\bigg ( e[c_i^4 ] - ( var [ c_i ] + ( e[c_i])^2)^2 \\bigg ) \\\\                   & = \\sumi \\bigg ( e[c_i^4 ] - ( \\sigmasq + \\sigmasqmu + \\mu^2)^2 \\bigg ) \\\\                   & = n ec_1 ^ 4 - n(\\sigmasq + \\sigmasqmu + \\mu^2)^2.\\end{aligned}\\ ] ] assuming normality , we have @xmath146 & = n\\bigg ( 3\\epsilon + 3\\sigma^4 + 6\\sigmasqmu \\sigmasq + 6\\mu^2 \\sigmasq + 3\\sigma^4_\\mu + 6\\mu^2 \\sigma^2_\\mu + \\mu^4 - ( \\sigmasq + \\sigmasqmu + \\mu^2)^2 \\bigg ) \\\\          & = n(3\\epsilon + 2\\sigma^4 + 2\\sigma_\\mu^4 + 4\\sigmasq \\sigmasqmu + 4\\mu^2 \\sigmasq + 4\\mu^2 \\sigmasqmu).\\end{aligned}\\ ] ] assuming additionally that @xmath44 and @xmath45 , we have @xmath147 = 2n(\\sigmasq + \\sigmasqmu)^2.\\end{aligned}\\ ] ] since @xmath30 and @xmath31 are symmetrically defined , we have @xmath148 = var \\left[\\sumi c_i^2 \\right ] . \\end{aligned}\\ ] ] next , @xmath149 & = \\sumi var [ c_i y_i ] \\\\                      & = \\sumi \\bigg ( e[c_i^2 y_i^2 ] - ( e[c_i y_i])^2   \\bigg)\\end{aligned}\\ ] ] where @xmath150 ^ 2 & = e\\bigg [ e [ c_i^2 y_i^2 | z_i \\bigg ] ] \\\\                 & = e [ e[c_i^2 | z_i ) e(y_i^2 | z_i ]   ] \\\\                 & = e[\\sigma_i^2 + m_i^2]^2 \\\\                 & = e[\\sigma_i^4 + m_i^4 + 2\\sigma_i^2 m_i^2 ] \\\\                 & = var [ \\sigma_i^2 ] + ( e[\\sigma_i^2])^2 + e[m^4_i ] + 2 e[\\sigma_i^2 ] e[m_i^2 ]   \\\\                 & = \\epsilon + \\sigma^4 + e[m^4_i]+ 2 \\sigma^2 ( \\sigmasqmu + \\mu^2);\\end{aligned}\\ ] ] and @xmath150 & = cov [ c_i , y_i]+ e[c_i ] e[y_i ] \\\\                & = \\sigmasqmu + \\mu^2.\\end{aligned}\\ ] ] therefore , @xmath149 & = \\sumi \\bigg ( \\epsilon + \\sigma^4 + em^4_i + 2 \\sigma^2 ( \\sigmasqmu + \\mu^2 ) -   ( \\sigmasqmu + \\mu^2)^2 \\bigg).\\end{aligned}\\ ] ] assuming normality , we have @xmath150 ^ 2 & = \\epsilon + \\sigma^4 + 3\\sigma^4_\\mu + 6\\mu^2 \\sigma^2_\\mu + \\mu^4 + 2\\sigmasq \\sigmasqmu + 2\\sigmasq \\mu^2 ; \\\\ e[c_i y_i ] & = \\sigmasqmu + \\mu^2 ; \\\\ var\\left [ \\sumi c_i y_i \\right ] & = n ( \\epsilon + \\sigma^4 + 2\\sigma^4_\\mu + 2\\sigmasq \\sigmasqmu + 2\\mu^2 \\sigmasq + 4\\mu^2 \\sigma^2_\\mu).\\end{aligned}\\ ] ] assuming additionally that @xmath44 and @xmath45 , we have @xmath150 ^ 2 & = ( \\sigmasq + \\sigmasqmu)^2 + 2\\sigma_\\mu^4 ; \\\\ e[c_i y_i ] & = \\sigmasqmu ; \\\\ var \\left [ \\sumi c_i y_i \\right ] & = n\\bigg [ ( \\sigmasq + \\sigmasqmu)^2 + \\sigma_\\mu^4 \\bigg].\\end{aligned}\\ ] ]    the covariance terms are computed as follows:@xmath151 =   \\sumi cov [ c_i^2 , y_i^2 ] = \\sumi ( e[c_i^2 y_i^2 ] - e[c_i^2 ] e[y_i^2]).\\end{aligned}\\ ] ] assuming normality , we have @xmath151 & = n\\bigg ( \\epsilon + \\sigma^4 + 3\\sigma^4_\\mu + 6\\mu^2 \\sigma^2_\\mu + \\mu^4 + 2\\sigmasq \\sigmasqmu + 2\\sigmasq \\mu^2 - ( \\sigmasq + \\sigmasqmu + \\mu^2)^2 \\bigg ) \\\\      & = n(\\epsilon + 2\\sigma_\\mu^4 + 4\\mu^2 \\sigmasqmu).\\end{aligned}\\ ] ] assuming additionally that @xmath44 and @xmath45 , we have @xmath151 = 2n\\sigma_\\mu^4.\\end{aligned}\\ ] ]    finally , since @xmath30 and @xmath31 are symmetrically defined , we have @xmath152 & = cov \\left[\\sumi y_i^2 , \\sumi c_i y_i\\right ] \\\\                             & = \\sumi cov [ c_i^2 , c_i y_i ] \\\\                             & = \\sumi \\bigg ( e[c^3_i y_i ] - e[c^2_i ] e[c_i y_i ] \\bigg),\\end{aligned}\\ ] ] where @xmath153 = e\\bigg [ e[c^3_i y_i | z_i ]   \\bigg ] = e\\bigg [ e[c^3_i | z_i ] e[y_i | z_i ] \\bigg].\\end{aligned}\\ ] ] assuming normality , we have @xmath153 & = e\\bigg [ ( 3m_i \\sigma^2_i + m^3_i ) m_i \\bigg ]   \\\\             & = e[3m^2_i \\sigma^2_i + m^4_i]\\\\             & = 3 e[m^2_i ] e[\\sigma^2_i ] + e[m^4_i ] \\\\             & = 3 ( \\sigmasqmu + \\mu^2 ) \\sigmasq + 3\\sigma^4_\\mu + 6\\mu^2 \\sigma^2_\\mu + \\mu^4",
    "\\\\             & = \\mu^4 + 3\\sigma_\\mu^4 + 3\\sigmasq \\sigmasqmu + 3\\mu^2 \\sigmasq + 6\\mu^2\\sigmasqmu ; \\\\",
    "e[c^2_i ] & = \\sigmasq + \\sigmasqmu + \\mu^2 ; \\\\ e[c_i y_i ] & = \\sigmasqmu + \\mu^2 ; \\end{aligned}\\ ] ] and therefore ,",
    "@xmath152&= n\\bigg(\\mu^4 + 3\\sigma_\\mu^4 + 3\\sigmasq \\sigmasqmu + 3\\mu^2 \\sigmasq + 6\\mu^2\\sigmasqmu - ( \\sigmasq + \\sigmasqmu + \\mu^2)(\\sigmasqmu + \\mu^2 ) \\bigg ) \\\\      & = n\\bigg(\\mu^4 + 3\\sigma_\\mu^4 + 3\\sigmasq \\sigmasqmu + 3\\mu^2 \\sigmasq + 6\\mu^2\\sigmasqmu - ( \\mu^4 + \\sigma_\\mu^4 + \\sigmasq \\sigmasqmu + \\mu^2 \\sigmasq + 2\\mu^2",
    "\\sigmasqmu ) \\bigg )   \\\\      & = 2n(\\sigma_\\mu^4 + \\sigmasq \\sigmasqmu + \\mu^2 \\sigmasq + 2\\mu^2 \\sigmasqmu).\\end{aligned}\\ ] ] assuming additionally that @xmath44 and @xmath45 , we have @xmath153 & = 3\\sigmasq \\sigmasqmu + 3\\sigma_\\mu^4 ; \\\\ e[c^2_i ] & = \\sigmasq + \\sigmasqmu ; \\\\ e[c_i y_i ] & = \\sigmasqmu ; \\\\ cov \\left[\\sumi c_i^2 , \\sumi",
    "c_i y_i\\right ] & = 2n\\sigmasqmu ( \\sigmasq + \\sigmasqmu).\\end{aligned}\\ ] ] putting the terms together , we derive the variance as follows , assuming that @xmath17 follows a normal distribution , @xmath145 & = \\frac{1}{4a^2 } \\bigg\\ { 2n(3\\epsilon + 2\\sigma^4 + 2\\sigma_\\mu^4 + 4\\sigmasq \\sigmasqmu + 4\\mu^2 \\sigmasq + 4\\mu^2",
    "\\sigmasqmu ) \\\\              & \\;\\;\\;\\ ; + 4n ( \\epsilon + \\sigma^4 + 2\\sigma^4_\\mu + 2\\sigmasq \\sigmasqmu + 2\\mu^2 \\sigmasq + 4\\mu^2",
    "\\sigma^2_\\mu ) + 2n(\\epsilon + 2\\sigma_\\mu^4 + 4\\mu^2 \\sigmasqmu ) \\\\              & \\;\\;\\;\\ ; - 16n(\\sigma_\\mu^4 + \\sigmasq \\sigmasqmu + \\mu^2 \\sigmasq + 2\\mu^2 \\sigmasqmu)\\bigg\\ } \\\\              &",
    "= \\frac{n}{a^2 } ( 3\\epsilon + 2\\sigma^4).\\end{aligned}\\ ] ] assuming additionally that @xmath44 and @xmath45 , we have @xmath154 = \\frac{2n}{a^2}\\sigma^4.\\end{aligned}\\ ] ]",
    "@xmath156 & = var \\bigg [ \\frac{1}{a } ( \\sumi c_i y_i - n \\bigcbar \\bigybar ) \\bigg ] \\\\             & = \\frac{1}{a^2 } var \\bigg[\\sumi c_i y_i - n \\bigcbar \\bigybar \\bigg ] \\\\             & = \\frac{1}{a^2 } \\bigg ( var\\left [ \\sumi c_i y_i \\right]+ var [ n \\bigcbar \\bigybar ] - 2 cov \\bigg[\\sumi c_i y_i , n",
    "\\bigcbar \\bigybar \\bigg ]   \\bigg ) . \\\\\\end{aligned}\\ ] ]    here , @xmath149 & = \\sumi \\bigg ( \\epsilon + \\sigma^4 + e[m^4_i ] + 2 \\sigma^2 ( \\sigmasqmu + \\mu^2 ) -   ( \\sigmasqmu + \\mu^2)^2 \\bigg).\\end{aligned}\\ ] ] also , @xmath157 & = n^2 var \\bigg [ \\frac{c_1 + \\cdots + c_n}{n } \\cdot \\frac{y_1 + \\cdots + y_n}{n}\\bigg ] \\\\                         & = \\frac{n^2}{n^4 } var \\bigg[\\sum_k c_k y_k + \\sum_{i\\neq j } c_i y_j \\bigg ] \\\\                         & = \\frac{1}{n^2 } \\bigg ( var   \\left [ \\sum_k c_k y_k \\right ] + var \\left [ \\sum_{i\\neq j } c_i y_j   \\right]+ 2cov \\bigg [ \\sum_k c_k y_k , \\sum_{i\\neq j } c_i y_j \\bigg ] \\bigg).\\end{aligned}\\ ] ] assuming normality on @xmath17 and assuming that @xmath44 and @xmath45 ( constant variance across cells ) , we have @xmath158 & = n(\\sigma^4 + 3\\sigma_\\mu^4 + 2\\sigmasq \\sigmasqmu - \\sigma_\\mu^4 ) \\\\                            & = n(\\sigmasq + \\sigmasqmu)^2 + n\\sigma_\\mu^4.\\end{aligned}\\ ] ] also , @xmath159 & = \\sum_{i\\neq j } var [ c_i y_j ] + 2\\sum_{i = k\\ ; \\text{or } \\ ; j = l } cov [ c_i y_j , c_k y_l ] + 2\\sum_{i\\neq k \\ ; \\text{and } \\ ; j \\neq l } cov [ c_i y_j , c_k y_l].\\end{aligned}\\ ] ] under the assumptions made above , we have @xmath160 & = e[c_i^2 y_j^2 ] - ( e[c_i y_j])^2 \\\\                 & = e[c_i^2 ] e[y_j^2 ] - ( e[c_i ] e[y_j])^2 \\\\                 & = ( \\sigmasq + \\sigmasqmu)^2.\\end{aligned}\\ ] ] if @xmath161 , @xmath162 & = e[c_i y_j c_k y_l ] - e[c_i y_j ] e[c_k y_l ] \\\\                              & = e[c_i^2 ] e[y_j ] e[y_l ] - ( e[c_i])^2 e[y_j ] e[y_l ] \\\\                              & = 0.\\end{aligned}\\ ] ] similarly , we can derive that the covariance is 0 for other cases where @xmath163 or where @xmath164 and @xmath165 . hence , @xmath159 = n(n-1)(\\sigmasq + \\sigmasqmu)^2.\\end{aligned}\\ ] ] additionally , under the normality assumption and with @xmath44 and @xmath45 , @xmath166 = 0.\\end{aligned}\\ ] ] therefore , @xmath157 & = \\frac{1}{n^2 } \\bigg ( n(\\sigmasq + \\sigmasqmu)^2 + n\\sigma_\\mu^4 + n(n-1)(\\sigmasq + \\sigmasqmu)^2 \\bigg ) \\\\                                 & =   \\frac{1}{n^2 } \\bigg ( n^2 ( \\sigma^2 + \\sigma_\\mu^2)^2 + n\\sigma_\\mu^4 \\bigg ) \\\\                                 & = ( \\sigma^2 + \\sigma_\\mu^2)^2 + \\frac{\\sigma_\\mu^4}{n}.\\end{aligned}\\ ] ] furthermore , @xmath167 & = \\frac{1}{n } cov \\left[\\sumi c_i y_i , \\sum_k c_k y_k + \\sum_{i\\neq j } c_i y_j\\right ]   \\\\      & = \\frac{1}{n } \\bigg ( cov \\bigg[\\sumi c_i y_i , \\sum_k c_k y_k\\bigg ] + cov \\bigg[\\sumi c_i y_i , \\sum_{i\\neq j } c_i y_j\\bigg ] \\bigg ) \\\\      & = \\frac{1}{n } \\bigg ( var \\left [ \\sumi c_i y_i \\right ]   \\bigg ) \\\\      & = ( \\sigmasq + \\sigmasqmu)^2 + \\sigma_\\mu^4.\\end{aligned}\\ ] ] @xmath168 & = \\frac{1}{a^2 } \\bigg ( n(\\sigmasq + \\sigmasqmu)^2 + n\\sigma_\\mu^4 + ( \\sigma^2 + \\sigma_\\mu^2)^2 + \\frac{\\sigma_\\mu^4}{n } - 2(\\sigmasq + \\sigmasqmu)^2 - 2\\sigma_\\mu^4 \\bigg ) \\\\              & = \\frac{n-1}{a^2 } ( \\sigma^2 + \\sigma_\\mu^2)^2 + \\frac{(n-1)^2}{na^2 } \\sigma_\\mu^4.\\end{aligned}\\ ] ]"
  ],
  "abstract_text": [
    "<S> gene expression is stochastic and displays variation ( `` noise '' ) both within and between cells . </S>",
    "<S> intracellular ( intrinsic ) variance can be distinguished from extracellular ( extrinsic ) variance by applying the law of total variance to data from two - reporter assays that probe expression of identical gene pairs in single - cells . </S>",
    "<S> we examine established formulas for the estimation of intrinsic and extrinsic noise and provide interpretations of them in terms of a hierarchical model . </S>",
    "<S> this allows us to derive corrections that minimize the mean squared error , an objective that may be important when sample sizes are small . </S>",
    "<S> the statistical framework also highlights the need for quantile normalization , and provides justification for the use of the sample correlation between the two reporter expression levels to estimate the percent contribution of extrinsic noise to the total noise . </S>",
    "<S> finally , we provide a geometric interpretation of these results that clarifies the current interpretation . </S>"
  ]
}