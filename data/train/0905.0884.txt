{
  "article_text": [
    "various estimation procedures based on @xmath1 penalization ( exemplified by the dantzig procedure in @xcite and the lasso procedure in @xcite ) have extensively been studied recently .",
    "these procedures are computationally efficient as shown in @xcite , and thus are adapted to high - dimensional data .",
    "they have been widely used in regression models , but only the lasso estimator has been studied in the density model ( see @xcite ) . although we will mostly consider the dantzig estimator in the density model for which no result exists so far , we recall some of the classical results obtained in different settings by procedures based on @xmath1 penalization .",
    "the dantzig selector has been introduced by cands and tao @xcite in the linear regression model .",
    "more precisely , given @xmath2 where @xmath3 , @xmath4 is a @xmath5 by @xmath6 matrix , @xmath7 is the noise vector and @xmath8 is the unknown regression parameter to estimate , the dantzig estimator is defined by @xmath9 where @xmath10 is the sup - norm in @xmath11 , @xmath12 is the @xmath0 norm in @xmath11 , and @xmath13 is a regularization parameter .",
    "a natural companion of this estimator is the lasso procedure or more precisely its relaxed form @xmath14 where @xmath13 plays exactly the exact same role as for the dantzig estimator .",
    "this @xmath0 penalized method is also called _ basis pursuit _ in signal processing ( see @xcite ) .",
    "cands and tao @xcite have obtained a bound for the @xmath15 risk of the estimator @xmath16 , with large probability , under a global condition on the matrix @xmath4 ( the restricted isometry property ) and a sparsity assumption on @xmath17 , even for @xmath18 .",
    "bickel et al .",
    "@xcite have obtained oracle inequalities and bounds of the @xmath19 loss for both estimators under weaker assumptions .",
    "actually , bickel et al .",
    "@xcite deal with the non parametric regression framework in which one observes @xmath20 where @xmath21 is an unknown function while @xmath22 are known design points and @xmath23 is a noise vector .",
    "there is no intrinsic matrix @xmath4 in this problem but for any dictionary of functions @xmath24 one can search @xmath21 as a weighted sum @xmath25 of elements of @xmath26 @xmath27 and introduce the matrix @xmath28 , which summarizes the information on the dictionary and on the design . notice that if there exists @xmath17 such that @xmath29 then the model can be rewritten exactly as the classical linear model",
    ". however , if it is not the case and if a model bias exists , the dantzig and lasso procedures can be after all applied under similar assumptions on @xmath4 .",
    "oracle inequalities are obtained for which approximation theory plays an important role in @xcite .",
    "let us also mention that in various settings , under various assumptions on the matrix @xmath4 ( or more precisely on the associated gram matrix @xmath30 ) , properties of these estimators have been established for subset selection ( see @xcite ) and for prediction ( see @xcite ) .",
    "we consider in this paper the density estimation framework already studied for the lasso estimate by bunea et al @xcite and van de geer @xcite .",
    "namely , our goal is to estimate @xmath31 , an unknown density function , by using the observations of an @xmath5-sample of variables @xmath32 of density @xmath33 . as in the non parametric regression setting , we introduce a dictionary of functions @xmath34 , and search again estimates of @xmath31 as linear combinations @xmath25 of the dictionary functions .",
    "we rely on the gram matrix associated with @xmath26 and on the empirical scalar products of @xmath31 with @xmath35 @xmath36 the dantzig estimate @xmath37 is then obtained by minimizing @xmath38 over the set of parameters @xmath39 satisfying the adaptive dantzig constraint : @xmath40 , @xmath41 is the scalar product of @xmath25 with @xmath35 , @xmath42 @xmath43 is a sharp estimate of the variance of @xmath44 and @xmath45 is a constant to be chosen .",
    "section [ def ] gives precise definitions and heuristics for using this constraint .",
    "we just mention here that @xmath46 comes from sharp concentration inequalities to give tight constraints .",
    "our idea is that if @xmath33 can be decomposed on @xmath26 as @xmath47 then we force the set of feasible parameters @xmath48 to contain @xmath49 with large probability and to be as small as possible .",
    "significant improvements in practice are expected .",
    "our goals in this paper are mainly twofold .",
    "first , we aim at establishing sharp oracle inequalities under very mild assumptions on the dictionary .",
    "our starting point is that most of the papers in the literature assume that the functions of the dictionary are bounded by a constant independent of @xmath6 and @xmath5 , which constitutes a strong limitation , in particular for dictionaries based on histograms or wavelets ( see for instance @xcite , @xcite , @xcite , @xcite , @xcite or @xcite ) .",
    "such assumptions on the functions of @xmath26 will not be considered in our paper .",
    "likewise , our methodology does not rely on the knowledge of @xmath50 that can even be infinite ( as noticed by birg @xcite for the study of the integrated @xmath51-risk , most of the papers in the literature typically assume that the sup - norm of the unknown density is finite with a known or estimated bound for this quantity ) .",
    "finally , let us mention that , in contrast with what bunea et al @xcite did , we obtain oracle inequalities with leading constant 1 , and furthermore these are established under much weaker assumptions on the dictionary than in @xcite .    the second goal of this paper deals with the problem of calibrating the so - called _ dantzig constant _ @xmath52 : how should this constant be chosen to obtain good results in both theory and practice ?",
    "most of the time , for lasso - type estimators , the regularization parameter is of the form @xmath53 with @xmath54 a positive constant ( see @xcite , @xcite , @xcite , @xcite , @xcite , @xcite or @xcite for instance ) .",
    "these results are obtained with large probability that depends on the tuning coefficient @xmath54 . in practice",
    ", it is not simple to calibrate the constant @xmath54 .",
    "unfortunately , most of the time , the theoretical choice of the regularization parameter is not suitable for practical issues .",
    "this fact is true for lasso - type estimates but also for many algorithms for which the regularization parameter provided by the theory is often too conservative for practical purposes ( see @xcite who clearly explains and illustrates this point for their thresholding procedure ) .",
    "so , one of the main goals of this paper is to fill the gap between the optimal parameter choice provided by theoretical results on the one hand and by a simulation study on the other hand .",
    "only a few papers are devoted to this problem . in the model selection",
    "setting , the issue of calibration has been addressed by birg and massart @xcite who considered @xmath55-penalized estimators in a gaussian homoscedastic regression framework and showed that there exists a minimal penalty in the sense that taking smaller penalties leads to inconsistent estimation procedures .",
    "arlot and massart @xcite generalized these results for non - gaussian or heteroscedastic data and reynaud - bouret and rivoirard @xcite addressed this question for thresholding rules in the poisson intensity framework .",
    "now , let us describe our results . by using the previous data - driven dantzig constraint ,",
    "oracle inequalities are derived under local conditions on the dictionary that are valid under classical assumptions on the structure of the dictionary .",
    "we extensively discuss these assumptions and we show their own interest in the context of the paper .",
    "each term of these oracle inequalities is easily interpretable .",
    "classical results are recovered when we further assume : @xmath56 where @xmath57 is a constant .",
    "this assumption is very mild and , unlike in classical works , allows to consider dictionaries based on wavelets .",
    "then , relying on our dantzig estimate , we build an adaptive lasso procedure whose oracle performances are similar .",
    "this illustrates the closeness between lasso and dantzig - type estimates .",
    "our results are proved for @xmath58 .",
    "for the theoretical calibration issue , we study the performance of our procedure when @xmath59 .",
    "we show that in a simple framework , estimation of the straightforward signal @xmath60}$ ] can not be performed at a convenient rate of convergence when @xmath59 .",
    "this result proves that the assumption @xmath61 is thus not too conservative .",
    "finally , a simulation study illustrates how dictionary - based methods outperform classical ones .",
    "more precisely , we show that our dantzig and lasso procedures with @xmath61 , but close to 1 , outperform classical ones , such as simple histogram procedures , wavelet thresholding or dantzig procedures based on the knowledge of @xmath50 and less tight dantzig constraints .",
    "section [ def ] introduces the density estimator of @xmath33 whose theoretical performances are studied in section [ results ] .",
    "section [ sec : conn - betw - dantz ] studies the lasso estimate proposed in this paper .",
    "the calibration issue is studied in section [ calib ] and numerical experiments are performed in section [ numerical ] .",
    "finally , section [ proofs ] is devoted to the proofs of our results .",
    "as said in introduction , our goal is to build an estimate of @xmath31 as a linear combination of functions of @xmath34 , where we assume without any loss of generality that , for any @xmath62 , @xmath63 : @xmath64 for this purpose , we naturally rely on natural estimates of the @xmath51-scalar products between @xmath33 and the @xmath65 s .",
    "so , for @xmath66 , we set @xmath67 and we consider its empirical counterpart @xmath68 that is an unbiased estimate of @xmath69 . the variance of this estimate is @xmath70 where @xmath71 note also that for any @xmath39 and any @xmath62 , the @xmath51-scalar product between @xmath25 and @xmath65 can be easily computed : @xmath72 where @xmath73 is the gram matrix associated to the dictionary @xmath26 defined for any @xmath74 by @xmath75 any reasonable choice of @xmath39 should ensure that the coefficients @xmath41 are close to @xmath76 for all @xmath62 . therefore ,",
    "using cands and tao s approach , we define the dantzig constraint : @xmath77 by @xmath78 with @xmath79 where for @xmath80 and @xmath66 , @xmath81 with @xmath82 and @xmath83 note that @xmath84 depends on the data , so the constraint ( [ constraint ] ) will be referred as the _ adaptive dantzig constraint _ in the sequel .",
    "we now justify the introduction of the density estimate @xmath85 .",
    "the definition of @xmath86 is based on the following heuristics .",
    "given @xmath62 , when there exists a constant @xmath87 such that @xmath88 for @xmath89 in the support of @xmath35 satisfying @xmath90 , then , with large probability , the deterministic term of ( [ etam ] ) is negligible with respect to the random one . in this case",
    ", the random term is the main one and we asymptotically derive @xmath91 having in mind that @xmath92 is a convenient estimate for @xmath93 ( see the proof of theorem [ concentrationth ] ) , the shape of the right hand term of the formula ( [ approxseuil ] ) looks like the bound proposed by cands and tao @xcite to define the dantzig constraint in the linear model .",
    "actually , the deterministic term of ( [ etam ] ) allows to get sharp concentration inequalities .",
    "as often done in the literature , instead of estimating @xmath93 , we could use the inequality @xmath94 and we could replace @xmath95 with @xmath50 in the definition of the @xmath84 . but",
    "this requires a strong assumption : @xmath33 is bounded and @xmath50 is known . in our paper",
    ", @xmath93 is estimated , which allows not to impose these conditions .",
    "more precisely , we slightly overestimate @xmath96 to control large deviation terms and this is the reason why we introduce @xmath95 instead of using @xmath97 , an unbiased estimate of @xmath98 . finally , @xmath45 is a constant that has to to be suitably calibrated and plays a capital role in practice .",
    "the following result justifies previous heuristics by showing that , if @xmath61 , with high probability , the quantity @xmath99 is smaller than @xmath84 for all @xmath62 .",
    "the parameter @xmath84 with @xmath45 close to @xmath100 can be viewed as the `` smallest '' quantity that ensures this property .",
    "[ concentrationth ] let us assume that @xmath6 satisfies @xmath101 for @xmath102 .",
    "let @xmath58 .",
    "then , for any @xmath103 , there exists a constant @xmath104 depending on @xmath105 @xmath106 and @xmath45 such that @xmath107 in addition , there exists a constant @xmath108 depending on @xmath106 and @xmath45 such that @xmath109 where , for @xmath66 , @xmath110 and @xmath111    this result is proved in section [ proofconcentrationth ] .",
    "the first part is a sharp concentration inequality proved by using bernstein type controls .",
    "the second part of the theorem proves that , up to constants depending on @xmath45 , @xmath84 is of order @xmath112 with high probability .",
    "note that the assumption @xmath58 is essential to obtain probabilities going to 0 .",
    "finally , let @xmath113 such that @xmath114 where @xmath115 is the projection on the space spanned by @xmath26 .",
    "we have @xmath116",
    "so , theorem [ concentrationth ] proves that @xmath49 satisfies the adaptive dantzig constraint ( [ constraint ] ) with probability larger than @xmath117 for any @xmath103 .",
    "actually , we force the set of parameters @xmath48 satisfying the adaptive dantzig constraint to contain @xmath49 with large probability and to be as small as possible .",
    "therefore , @xmath78 is a good candidate among sparse estimates linearly decomposed on @xmath26 for estimating @xmath31 .",
    "we mention that assumption ( [ mn ] ) can be relaxed and we can take @xmath118 provided the definition of @xmath84 is modified .",
    "in the sequel , we will denote @xmath119 to simplify the notations , but the dantzig estimator @xmath85 still depends on @xmath45 .",
    "moreover , we assume that ( [ mn ] ) is true and we denote the vector @xmath120 considered with the dantzig constant @xmath61 .",
    "let us state the main result of this paper .",
    "for any @xmath121 , we set @xmath122 and define @xmath123 the vector which has the same coordinates as @xmath48 on @xmath124 and zero coordinates on @xmath125 .",
    "we introduce a local assumption indexed by a subset @xmath126 .    * * local assumption * given @xmath127 , for some constants @xmath128 and @xmath129 depending on @xmath126 , we have for any @xmath39 , @xmath130    we obtain the following oracle type inequality without any assumption on @xmath31 .",
    "[ oracleadmiloc ] let @xmath131 be fixed .",
    "we suppose that holds .",
    "then , with probability at least @xmath132 , we have for any @xmath133 , @xmath134 with @xmath135    let us comment each term of the right hand side of ( [ eq2loc ] ) .",
    "the first term is an approximation term which measures the closeness between @xmath31 and @xmath25 .",
    "this term can vanish if @xmath33 can be decomposed on the dictionary .",
    "the second term is a price to pay when either @xmath39 is not supported by the subset @xmath136 considered or it does not satisfy the condition @xmath137 which holds as soon as @xmath39 satisfy the adaptive dantzig constraint .",
    "finally , the last term , which does not depend on @xmath39 , can be viewed as a variance term corresponding to the estimation on the subset @xmath136 .",
    "indeed , remember that @xmath84 relies on an estimate of the variance of @xmath44 .",
    "furthermore , we have with high probability : @xmath138 so , if @xmath33 is bounded then , @xmath139 and if there exists a constant @xmath57 such that for any @xmath62 , @xmath140 ( which is true for instance for a bounded dictionary ) , then @xmath141 ( where @xmath142 is a constant depending on @xmath45 and @xmath57 ) and tends to 0 when @xmath5 goes to @xmath143 .",
    "we obtain thus the following result .",
    "let @xmath131 be fixed .",
    "we suppose that holds .",
    "if ( [ conddicob ] ) is satisfied then , with probability at least @xmath132 , we have for any @xmath133 , for any @xmath39 that satisfies the adaptive dantzig constraint @xmath144 where @xmath145 is an absolute constant and @xmath146 depends on @xmath57 and @xmath45 .",
    "the parameter @xmath147 calibrates the weights given for the bias and variance terms . remark that if @xmath148 and if holds with @xmath149 , under ( [ conddicob ] ) , the proof of theorem  [ oracleadmiloc ] yields the more classical inequality @xmath150 where @xmath151 , with at least the same probability @xmath132 .    assumption   is local , in the sense that the constants @xmath152 and @xmath153 ( or their mere existence ) may highly depend on the subset @xmath136 . for a given @xmath39 ,",
    "the best choice for @xmath126 in inequalities   and   depends thus on the interaction between these constants and the value of @xmath39 itself .",
    "note that the assumptions of theorem  [ oracleadmiloc ] are reasonable as the next section gives conditions for which assumption   holds simultaneously with the same constant @xmath154 and @xmath155 for all subsets @xmath126 of the same size .",
    "as usual , when @xmath156 , properties of the dantzig estimate can be derived from assumptions on the structure of the dictionary @xmath26 . for @xmath157",
    ", we denote @xmath158 these quantities correspond to the `` restricted '' eigenvalues of the gram matrix @xmath73 . assuming that @xmath159 and @xmath160 are close to 1 means that every set of columns of @xmath73 with cardinality less than @xmath161 behaves like an orthonormal system .",
    "we also consider the restricted correlations @xmath162 small values of @xmath163 mean that two disjoint sets of columns of @xmath73 with cardinality less than @xmath161 and @xmath164 span nearly orthogonal spaces .",
    "we will use one of the following assumptions considered in @xcite .    * * assumption 1 * for some integer @xmath165 , we have @xmath166 oracle inequalities of the dantzig selector were established under this assumption in the parametric linear model by cands and tao in @xcite .",
    "it was also considered by bunea , ritov and tsybakov @xcite for non - parametric regression and for the lasso estimate .",
    "the next assumption , proposed in @xcite , constitutes an alternative to assumption 1 . *",
    "* assumption 2 * for some integers @xmath167 and @xmath161 such that @xmath168 we have @xmath169 if assumption 2 is true for @xmath167 and @xmath161 such that @xmath170 , then assumption 2 means that @xmath159 can not decrease at a rate faster than @xmath171 and this condition is related to the `` incoherent designs '' condition stated in @xcite .    in the sequel",
    ", we set , under assumption 1 , @xmath172 and under assumption 2 , @xmath173 now , to apply theorem [ oracleadmiloc ] , we need to check ( [ condlocal ] ) for some some subset @xmath126 of @xmath174 . either assumption  1 or assumption  2 implies this assumption .",
    "indeed , we have the following result .    [ lema0 ]",
    "let @xmath167 and @xmath161 two integers satisfying ( [ sl ] ) .",
    "we suppose that ( [ ass1 ] ) or ( [ ass2 ] ) is true .",
    "let @xmath175 of size @xmath176 and @xmath177 , then we have @xmath178 with @xmath179 and @xmath180 under ( [ ass1 ] ) ( respectively @xmath181 and @xmath182 under ( [ ass2 ] ) .",
    "if ( [ ass1 ] ) and ( [ ass2 ] ) are both satisfied , @xmath183 and @xmath184 .    proposition [ lema0 ] proves that theorem [ oracleadmiloc ] can be applied under assumptions 1 or 2 . in addition",
    ", the constants @xmath152 and @xmath153 only depend on @xmath185 . from theorem [ oracleadmiloc ]",
    ", we deduce the following result .",
    "[ oracleadmi ] let @xmath167 and @xmath161 two integers satisfying ( [ sl ] ) .",
    "we suppose that ( [ ass1 ] ) or ( [ ass2 ] ) is true .",
    "then , with probability at least @xmath132 , we have for any @xmath133 , @xmath186 where @xmath187    remark that the best subset @xmath126 of cardinal @xmath167 in theorem  [ oracleadmi ] can be easily chosen for a given @xmath39 : it is given by the set of the @xmath167 largest coordinates of @xmath39 .",
    "this was not necessarily the case in theorem  [ oracleadmiloc ] for which a different subset may give a better local condition and then may provide a smaller bound . if we further assume the mild assumption ( [ conddicob ] ) on the sup norm of the dictionary introduced in the previous section , we deduce the following result .",
    "let @xmath167 and @xmath161 two integers satisfying ( [ sl ] ) .",
    "we suppose that ( [ ass1 ] ) or ( [ ass2 ] ) is true . if ( [ conddicob ] ) is satisfied , with probability at least @xmath132 , we have for any @xmath133 , any @xmath39 that satisfies the adaptive dantzig constraint and for the best subset @xmath126 of cardinal @xmath167 ( that corresponds to the @xmath167 largest coordinates of @xmath39 in absolute value ) , @xmath188 where @xmath145 is an absolute constant and @xmath146 depends on @xmath57 and @xmath45 .",
    "note that , when @xmath39 is @xmath167-sparse so that @xmath189 , the oracle inequality ( [ eq2ter ] ) corresponds to the classical oracle inequality obtained in parametric frameworks ( see @xcite or @xcite for instance ) or in non - parametric settings .",
    "see , for instance @xcite , @xcite , @xcite , @xcite , @xcite or @xcite but in these works , the functions of the dictionary are assumed to be bounded by a constant independent of @xmath6 and @xmath5 .",
    "so , the adaptive dantzig estimate requires weaker conditions since under ( [ conddicob ] ) , @xmath190 can go to @xmath143 when @xmath5 grows .",
    "this point is capital for practical purposes , in particular when wavelet bases are considered .",
    "we show in this section the strong connections between lasso and dantzig estimates , which has already been illustrated in @xcite for non - parametric regression models . by choosing convenient random weights depending on @xmath191 for @xmath0-minimization ,",
    "the lasso estimate satisfies the adaptive dantzig constraint .",
    "more precisely , we consider the lasso estimator given by the solution of the following minimization problem @xmath192 where @xmath193 note that @xmath194 is the quantity minimized in unbiased estimation of the risk . for simplifications , we write @xmath195 .",
    "we denote @xmath196 .",
    "as said in introduction , classical lasso estimates are defined as the minimizer of expressions of the form @xmath197 where @xmath13 is proportional to @xmath198 .",
    "so , @xmath199 appears as a data - driven version of classical lasso estimates .",
    "the first order condition for the minimization of the expression given in ( [ lasso ] ) corresponds exactly to the adaptive dantzig constraint and thus theorem  [ oracleadmi ] always applies to @xmath199 . working along the lines of the proof of theorem  [ oracleadmi ] ( replace @xmath25 by @xmath200 and @xmath200 by @xmath201 in ( [ rela1 ] ) and ( [ maj2 ] ) ) , one can prove a slightly stronger result .",
    "[ equivalence ] let us assume that assumptions of theorem [ oracleadmi ] are true .",
    "let @xmath175 of size @xmath176 .",
    "then , with probability at least @xmath132 , we have for any @xmath133 , @xmath202    to extend this theoretical result , numerical performances of the dantzig and lasso estimates will be compared in section [ numerical ] .",
    "in this section , we consider the problem of calibrating previous estimates . in particular , we prove that the sufficient condition @xmath58 is `` almost '' a necessary condition since we derive a special and very simple framework in which lasso and dantzig estimates can not achieve the optimal rate if @xmath203 ( `` almost '' means that the case @xmath204 remains an open question ) .",
    "let us describe this simple framework .",
    "the dictionary @xmath26 considered in this section is the orthonormal haar system : @xmath205 with @xmath206}$ ] , @xmath207 , and for @xmath208 @xmath209 , @xmath210}-1_{[(k+0.5)/2^j,(k+1)/2^j]}\\right).\\ ] ] in this case , @xmath211 . in this",
    "setting , since functions of @xmath26 are orthonormal , the gram matrix @xmath73 is the identity .",
    "thus , the lasso and dantzig estimates both correspond to the soft thresholding rule : @xmath212 now , our goal is to estimate @xmath213}$ ] by using @xmath200 depending on @xmath45 and to show the influence of this constant . unlike previous results stated in probability , we consider the expectation of the @xmath51-risk :    [ lower ] on the one hand , if @xmath58 , there exists a constant @xmath142 such that @xmath214 on the other hand , if @xmath203 , there exists a constant @xmath215 and @xmath102 such that @xmath216    this result shows that choosing @xmath59 is a bad choice in our setting . indeed , in this case , the lasso and dantzig estimates can not estimate a very simple signal ( @xmath60}$ ] ) at a convenient rate of convergence .",
    "a small simulation study is carried out to strengthen this theoretical asymptotic result . performing our estimation procedure 100 times ,",
    "we compute the average risk @xmath217 for several values of the dantzig constant @xmath45 and several values of @xmath5 .",
    "this computation is summarized in figure  [ fig : penalmini ] which displays the logarithm of @xmath217 for @xmath218 with , from top to bottom , @xmath219 on a grid of @xmath45 s around @xmath100 . to discuss our results ,",
    "we denote by @xmath220 the best @xmath45 : @xmath221 we note that @xmath222 for all values of @xmath5 , with @xmath220 getting closer to @xmath100 as @xmath5 increases .",
    "taking @xmath45 too small strongly deteriorates the performance while a value close to @xmath100 ensures a risk withing a factor @xmath223 of the optimal risk .",
    "the assumption @xmath61 giving a theoretical control on the quadratic error is thus not too conservative . following these results , we set @xmath224 in our numerical experiments in the next subsection .      in this section",
    ", we present our numerical experiments with the dantzig density estimator and their results .",
    "we test our estimator with a collection of 6 dictionaries , 4 densities described below and for 2 sample sizes .",
    "we compare our procedure with the adaptive lasso introduced in section  [ sec : conn - betw - dantz ] and with a non adaptive dantzig estimator .",
    "we also consider a two - step estimation procedure , proposed by cands and tao  @xcite , which improves the numerical results .    the numerical scheme for a given dictionary @xmath34 and a sample @xmath225 is the following .    1",
    "compute @xmath226 for all @xmath62 , 2 .",
    "compute @xmath227 , 3 .",
    "compute @xmath84 as defined in by @xmath42 with @xmath228 and @xmath229 .",
    "4 .   compute the coefficients @xmath230 of the dantzig estimate , @xmath231 such that @xmath39 satisfies the dantzig constraint ( [ constraint ] ) @xmath232 .",
    "note that we have implicitly assumed that the gram matrix @xmath73 used in the definition of the dantzig constraint has been precomputed .    for the lasso estimator",
    ", the dantzig minimization of step 4 is replaced by the lasso minimization   @xmath233 which is solved using the lars algorithm .",
    "the non adaptive dantzig estimate is obtained by replacing @xmath234 in step @xmath235 by @xmath236 the two - step procedure of cands and tao adds a least - square step between step 4 and step 5 .",
    "more precisely , let @xmath237 be the support of the estimate @xmath238 .",
    "this defines a subset of the dictionary on which the density is regressed @xmath239 where @xmath240 is the submatrix of @xmath73 corresponding to the subset chosen .",
    "the values of @xmath241 outside @xmath237 are set to 0 and @xmath242 is set accordingly .",
    "we describe now the dictionaries we consider .",
    "we focus numerically on densities defined on the interval @xmath243 $ ] so we use dictionaries adapted to this setting .",
    "the first four are orthonormal systems , which are used as a benchmark , while the last two are `` real '' dictionaries .",
    "more precisely , our dictionaries are    * the fourier basis with @xmath244 elements ( denoted `` fou '' ) , * the histogram collection with the classical number @xmath245 of bins ( denoted `` hist '' ) , * the haar wavelet basis with maximal resolution @xmath246 and thus @xmath247 elements ( denoted `` haar '' ) , * the more regular daubechies 6 wavelet basis with maximal resolution @xmath248 and thus @xmath247 elements ( denoted `` wav '' ) , * the dictionary made of the union of the fourier basis and the histogram collection and thus comprising @xmath249 elements .",
    "( denoted `` mix '' ) , * the dictionary which is the union of the fourier basis , the histogram collection and the haar wavelets of resolution greater than @xmath250 comprising @xmath251 elements ( denoted `` mix2 '' ) .",
    "the orthonormal families we have chosen are often used by practitioners .",
    "our dictionaries combine very different orthonormal families , sine and cosine with bins or haar wavelets , which ensures a sufficiently incoherent design .",
    "we test the estimators of the following 4 functions shown in figure  [ fig : plots ] ( with their dantzig and dantzig+least square estimates with the `` mix2 '' dictionary ) :    * a very spiky density @xmath252 * a mix of gaussian and laplacian type densities @xmath253 * a mix of uniform densities on subintervals @xmath254 * a mix of a density easily described in the fourier domain and a uniform density on a subinterval @xmath255    boxplots of figures  [ fig : boxplots500 ] and [ fig : boxplots2000 ] summarize our numerical experiments for @xmath256 and @xmath257 and @xmath258 repetitions of the procedures .",
    "the left column deals with the comparison between dantzig and lasso , the center column shows the effectiveness of our data driven constraint and the right column illustrates the improvement of the two - step method",
    ". as expected , dantzig and lasso estimators are strictly equivalent when the dictionary is orthonormal and very close otherwise . for both algorithms and most of the densities",
    ", the best solution appears to be the `` mix2 '' dictionary , except for the density @xmath259 where the haar wavelets are better for @xmath256 .",
    "this shows that the dictionary approach yields an improvement over the classical basis approach .",
    "one observes also that the `` mix '' dictionary is better than the best of its constituent , namely the fourier basis and the histogram family , which corroborates our theoretical results .",
    "the adaptive constraints are much tighter than their non adaptive counterparts and yield to much better numerical results .",
    "our last series of experiments shows the significant improvement obtained with the least square step . as hinted by cands and tao @xcite ,",
    "this can be explained by the bias common to @xmath0 methods which is partially removed by this final least square adjustment .",
    "studying directly the performance of this estimator is a challenging task",
    ".    14.75 cm    [ cols=\"^,^,^,^ \" , ]",
    "to prove the first part of theorem [ concentrationth ] , we fix @xmath260 and we set for any @xmath261 , @xmath262 that satisfies almost surely @xmath263 then , we apply bernstein s inequality ( see @xcite on pages 24 and 26 ) with the variables @xmath264 and @xmath265 : for any @xmath266 , @xmath267 now , let us decompose @xmath97 in two terms : @xmath268 with @xmath269 let us first focus on @xmath270 that is the main term of @xmath97 by applying again bernstein s inequality with @xmath271 which satisfies @xmath272 one has that for any @xmath266 @xmath273 with @xmath274 ^ 2\\right).\\ ] ] but we have @xmath275 - 2{\\ensuremath{\\sigma}}^2_{0,m}{\\ensuremath{\\mathbb{e}}}\\left[({\\ensuremath{\\varphi}}_m(x_i)-\\beta_{0,m})^2\\right]\\right)\\\\ & = \\frac{1}{n}\\left({\\ensuremath{\\mathbb{e}}}\\left[({\\ensuremath{\\varphi}}_m(x_i)-\\beta_{0,m})^4\\right]-{\\ensuremath{\\sigma}}^4_{0,m}\\right)\\\\ & \\leq\\frac{{\\ensuremath{\\sigma}}^2_{0,m}}{n}\\left({\\ensuremath{\\vert\\!\\vert { \\ensuremath{\\varphi}}_m \\vert\\!\\vert}}_{\\infty}+|\\beta_{0,m}|\\right)^2\\\\ & \\leq \\frac{4{\\ensuremath{\\sigma}}^2_{0,m}}{n}{\\ensuremath{\\vert\\!\\vert { \\ensuremath{\\varphi}}_m \\vert\\!\\vert}}_\\infty^2 .\\end{aligned}\\ ] ] finally , with for any @xmath266 @xmath276 we have @xmath277 the term @xmath278 is a degenerate u - statistics that satisfies for any @xmath266 @xmath279 with for any @xmath266 @xmath280 where @xmath4 , @xmath281 , @xmath142 , @xmath282 and @xmath283 are constants not depending on @xmath284 that satisfy @xmath285 ( see @xcite ) .",
    "then , we have for any @xmath266 , @xmath286 now , we take @xmath284 that satisfies @xmath287 and @xmath288 therefore , for any @xmath289 , we have for @xmath5 large enough , @xmath290 so , for @xmath5 large enough , @xmath291 where @xmath292 . using inequalities ( [ concsn ] ) and ( [ concustats ] )",
    ", we obtain @xmath293 now , using ( [ majou ] ) , for any @xmath294 , we have for @xmath5 large enough , @xmath295 therefore , @xmath296 now , let us set @xmath297 and consider the polynomial @xmath298 with roots @xmath299 .",
    "so , we have @xmath300 it yields @xmath301 which means that for any @xmath302 , we have for @xmath5 large enough , @xmath303 finally , we can claim that for any @xmath304 , we have for @xmath5 large enough , @xmath305 now , we take @xmath306 . under assumptions of theorem [ concentrationth ] ,",
    "conditions ( [ condu1 ] ) and ( [ condu2 ] ) are satisfied .",
    "the previous concentration inequality means that @xmath307 now , using ( [ conccoeff ] ) , we have for @xmath5 large enough , @xmath308 then , the first part of theorem [ concentrationth ] is proved : for any @xmath103 , @xmath309 where @xmath310 is a constant that depends on @xmath105 @xmath106 and @xmath45 .",
    "for the second part of the result , we apply again bernstein s inequality with @xmath311 which satisfies @xmath312 one has that for any @xmath266 @xmath313 with @xmath274 ^ 2\\right)\\leq\\frac{4{\\ensuremath{\\sigma}}^2_{0,m}}{n}{\\ensuremath{\\vert\\!\\vert { \\ensuremath{\\varphi}}_m \\vert\\!\\vert}}_\\infty^2.\\ ] ] so , for any @xmath266 , @xmath314 now , for any @xmath315 , for any @xmath266 , @xmath316 using ( [ concustats ] ) , with @xmath317 @xmath318 using ( [ majou ] ) , @xmath319 since @xmath42 with @xmath320 we have for any @xmath321 , @xmath322 finally , with @xmath306 , with probability larger than @xmath323 , @xmath324 and @xmath325 finally , with @xmath326 , @xmath327 , for @xmath5 large enough , @xmath328 note that @xmath329 .    for the last part , starting from ( [ e2 ] ) with @xmath330 and @xmath331 , we have for @xmath5 large enough and with probability larger than @xmath323 , @xmath332 so , for @xmath5 large enough , @xmath333 and @xmath334      let @xmath335 and set @xmath336 .",
    "we have @xmath337 we have @xmath338 .",
    "moreover , with probability at least @xmath132 , we have @xmath339\\right|\\label{maj2}\\\\ \\le & { \\ensuremath{\\vert\\!\\vert \\delta \\vert\\!\\vert}_{\\ell_1 } } 2{\\ensuremath{\\vert\\!\\vert \\eta_{\\gamma } \\vert\\!\\vert}_{\\ell_\\infty}}\\nonumber,\\end{aligned}\\ ] ] where the last line is a consequence of the definition of the dantzig estimator and of theorem [ concentrationth ] .",
    "then , we have @xmath340 we use then the following lemma :        note that if @xmath39 satisfies the dantzig condition then by definition of @xmath347 : @xmath348 . using the previous lemma , we have : @xmath349 using now @xmath350 , so that @xmath351 as soon as @xmath39 satisfies the dantzig condition , we obtain @xmath352 and thus @xmath353 we deduce thus @xmath354 and then since @xmath355 we have @xmath356 which is the result of the theorem .",
    "to prove proposition [ lema0 ] , we establish lemmas [ lema3bis ] and [ lema4 ] . in the sequel",
    ", we consider two integers @xmath167 and @xmath161 such that @xmath165 , @xmath357 and @xmath358 .",
    "we first recall assumptions  1 and 2 .",
    "assumption  1 is stated in a more general form , which allows to unify the statement of the subsequent results .",
    "[ lema3bis ] let @xmath131 with cardinality @xmath361 and @xmath362 .",
    "we denote by @xmath363 the subset of @xmath174 corresponding to the @xmath161 largest coordinates of @xmath364 ( in absolute value ) outside @xmath136 and we set @xmath365 .",
    "we denote by @xmath366 the projector on the linear space spanned by @xmath367 .",
    "we have : @xmath368 with @xmath369    for @xmath370 , we denote by @xmath371 the indices corresponding to the coordinates of @xmath364 outside @xmath126 whose absolute values are between the @xmath372th and the @xmath373th largest ones ( in absolute value ) .",
    "note that this definition is consistent with the definition of @xmath374 . using this notation ,",
    "we have @xmath375 since @xmath376 has @xmath377 elements , we have @xmath378 note that @xmath379 for some vector @xmath380 . since , @xmath381 one obtains that @xmath382 moreover , using that @xmath371 has less than @xmath161 elements , we obtain that @xmath383 now using that @xmath384 , we obtain @xmath385 and finally @xmath386          the dictionary considered here is the haar dictionary @xmath392 and is double - indexed . as a consequence , in the following , the quantity @xmath393 , @xmath394 , @xmath395 @xmath396 , @xmath397 and @xmath398 are defined as in ( [ betazero ] ) , ( [ empirestim ] ) , ( [ sigmazero ] ) , ( [ etam ] ) , ( [ sigmatilde ] ) and ( [ sigmachapeau ] ) where @xmath65 is replaced by @xmath399 .",
    "note that , since @xmath60}$ ] , we have , for @xmath400 , @xmath401 and for any @xmath402 , @xmath403 if @xmath404 and @xmath405 otherwise .    the proof of ( [ majorisk ] ) is provided by using the oracle inequality satisfied by hard thresholding given by theorem 1 of @xcite and the rough control of the soft thresholding estimate by the hard one : @xmath406 ) , we establish the following lemma .",
    "then , we use the following inequality . for @xmath402 that satisfies ( [ condj ] ) , we have for @xmath413 , @xmath414 so , if @xmath415 and @xmath416 are such that @xmath417 , then applying lemma [ lem : calibration ] , inequality ( [ minorisk ] ) is proved for any @xmath106 such that @xmath418",
    ". +    [ proof of lemma  [ lem : calibration ] ] let @xmath402 that satisfies ( [ condj ] ) and @xmath419 .",
    "we have @xmath420",
    "so , for any @xmath421 , @xmath422 now , @xmath423 furthermore , we have @xmath424 where @xmath425 and @xmath426 are defined as in ( [ sn ] ) with @xmath35 replaced by @xmath399 .",
    "this implies that @xmath427 using ( [ concustats ] ) , with probability larger than @xmath428 , we have @xmath429 and , since @xmath403 @xmath430 where @xmath57 , @xmath145 , @xmath146 , @xmath431 , @xmath432 and @xmath433 are universal constants .",
    "finally , with probability larger than @xmath428 , we obtain that @xmath434 so , since @xmath59 , there exists @xmath435 , only depending on @xmath416 such that with probability larger than @xmath428 , @xmath436 we set @xmath437 so @xmath438 then , we have @xmath439 with @xmath440 we consider @xmath402 such that @xmath441 in particular , we have @xmath442 now , we can write @xmath443 that implies that @xmath444 now , we consider a bounded sequence @xmath445 such that for any @xmath5 , @xmath446 and such that @xmath447 is an integer with @xmath448 and @xmath449 is the largest integer smaller or equal to @xmath450 .",
    "we have @xmath451 since @xmath452 now , set @xmath453 that are positive for @xmath5 large enough . if @xmath454 and @xmath455 then we have @xmath456 .",
    "finally , we obtain that @xmath457\\nonumber\\\\ & \\geq v_{nj}(\\log n)^{-2{\\ensuremath{\\alpha}}}\\left[\\frac{n!}{l_{nj}!m_{nj}!(n - l_{nj}-m_{nj})!}p_j^{l_{nj}+m_{nj}}(1 - 2p_j)^{n-(l_{nj}+m_{nj})}-\\frac{6}{n^2}\\right]\\nonumber\\\\ & \\geq v_{nj}(\\log n)^{-2{\\ensuremath{\\alpha}}}\\times\\left[\\frac{n!}{l_{nj}!m_{nj}!(n-2\\tilde\\mu_{nj})!}p_j^{2\\tilde\\mu_{nj}}(1 - 2p_j)^{n-2\\tilde\\mu_{nj}}-\\frac{6}{n^2}\\right],\\label{terms}\\end{aligned}\\ ] ] where @xmath458 now , let us study each term of ( [ terms ] ) .",
    "we have @xmath459 @xmath460 and @xmath461 then , using the stirling relation , @xmath462 , we deduce that @xmath463 it remains to evaluate @xmath464 : @xmath465 if we set @xmath466 then @xmath467 @xmath468 and using that @xmath469 we obtain that @xmath470 similarly , we obtain that @xmath471 that implies that @xmath472 since @xmath473 we have , for @xmath5 large enough , @xmath474 and @xmath475 finally , we have @xmath476 since @xmath421 , we conclude that there exists @xmath102 such that @xmath477(1+o_n(1 ) ) \\\\ & \\geq \\frac{v_{nj}(\\log n)^{-2{\\ensuremath{\\alpha}}}}{2\\pi\\tilde\\mu_{nj } } \\left[\\exp\\left(-({\\ensuremath{\\gamma}}+2{\\ensuremath{\\varepsilon}})\\log n-2\\right)-\\frac{6}{n^2}\\right](1+o_n(1))\\\\ & \\geq\\frac{2{\\ensuremath{\\gamma}}(1+{\\ensuremath{\\varepsilon}})e^{-2}}{\\pi}(\\log n)^{1 - 2{\\ensuremath{\\alpha}}}n^{-({\\ensuremath{\\gamma}}+2{\\ensuremath{\\varepsilon}})}(1+o_n(1))\\end{aligned}\\ ] ] and lemma  [ lem : calibration ] is proved .",
    "bunea f. , tsybakov a.b . and wegkamp m.h .",
    "( 2006 ) _ aggregation and sparsity via @xmath0 penalized least squares _ , proceedings of 19th annual conference on learning theory ( colt 2006 ) , lecture notes in artificial intelligence v.4005 ( lugosi , g. and simon , h.u.,eds . ) , springer - verlag , berlin - heidelberg ."
  ],
  "abstract_text": [
    "<S> this paper deals with the problem of density estimation . </S>",
    "<S> we aim at building an estimate of an unknown density as a linear combination of functions of a dictionary . </S>",
    "<S> inspired by cands and tao s approach , we propose an @xmath0-minimization under an adaptive dantzig constraint coming from sharp concentration inequalities . </S>",
    "<S> this allows to consider a wide class of dictionaries . under local or global coherence assumptions , </S>",
    "<S> oracle inequalities are derived . </S>",
    "<S> these theoretical results are also proved to be valid for the natural lasso estimate associated with our dantzig procedure . </S>",
    "<S> then , the issue of calibrating these procedures is studied from both theoretical and practical points of view . </S>",
    "<S> finally , a numerical study shows the significant improvement obtained by our procedures when compared with other classical procedures .    </S>",
    "<S> * keywords * : calibration , concentration inequalities , dantzig estimate , density estimation , dictionary , lasso estimate , oracle inequalities , sparsity . + </S>",
    "<S> * ams subject classification * : 62g07 , 62g05 , 62g20 </S>"
  ]
}