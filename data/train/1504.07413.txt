{
  "article_text": [
    "with the coming era of massive data , large scale tensors have important applications in science and engineering . how to store and analyze these tensors ? this is a pressing and challenging problem . in the literature , there are two strategies for manipulating large scale tensors . the first one is to exploit their structures such as sparsity @xcite . for example , we consider an online store ( e.g. amazon.com ) where users may review various products @xcite .",
    "then , a third order tensor with modes : users , items , and words could be formed naturally and it is sparse .",
    "the other one is to use distributed and parallel computation @xcite .",
    "this technique could deal with large scale dense tensors , but it depends on a supercomputer .",
    "recently , researchers applied these two strategies simultaneously for large scale tensors @xcite .    in this paper , we consider a class of large scale dense tensors with a special hankel structure .",
    "hankel tensors appear in many engineering problems such as signal processing @xcite , automatic control @xcite , and geophysics @xcite .",
    "for instance , in nuclear magnetic resonance spectroscopy @xcite , a hankel matrix was formed to analyze the time - domain signals , which is important for brain tumour detection .",
    "papy et al .",
    "@xcite improved this method by using a high order hankel tensor to replace the hankel matrix .",
    "ding et al .",
    "@xcite proposed a fast computational framework for products of a hankel tensor and vectors . on the mathematical properties , luque and thibon @xcite explored the hankel hyperdeterminants .",
    "qi @xcite and xu @xcite studied the spectra of hankel tensors and gave some upper bounds and lower bounds for the smallest and the largest eigenvalues . in @xcite ,",
    "qi raised a question : can we construct some efficient algorithms for the largest and the smallest h- and z - eigenvalues of a hankel tensor ?",
    "numerous applications of the eigenvalues of higher order tensors have been found in science and engineering , such as automatic control @xcite , medical imaging @xcite , quantum information @xcite , and spectral graph theory @xcite .",
    "for example , in magnetic resonance imaging @xcite , the principal z - eigenvalues of an even order tensor associated to the fiber orientation distribution of a voxel in white matter of human brain denote volume factions of several nerve fibers in this voxel , and the corresponding z - eigenvectors express the orientations of these nerve fibers .",
    "the smallest eigenvalue of tensors reflects the stability of a nonlinear multivariate autonomous system in automatic control @xcite . for",
    "a given even order symmetric tensor , it is positive semidefinite if and only if its smallest h- or z - eigenvalue is nonnegative @xcite .",
    "the conception of eigenvalues of higher order tensors was defined independently by qi @xcite and lim @xcite in 2005 .",
    "unfortunately , it is an np - hard problem to compute eigenvalues of a tensor even though the involved tensor is symmetric @xcite . for two and three dimensional symmetric tensors , qi et al .",
    "@xcite proposed a direct method to compute all of its z - eigenvalues .",
    "it was pointed out in @xcite that the polynomial system solver , nsolve in _ mathematica _ , could be used to compute all of the eigenvalues of lower order and low dimensional tensors .",
    "we note that the mathematical software _",
    "maple _ has a similar command solve which is also applicable for the polynomial systems of eigenvalues of tensors .    for general symmetric tensors , kolda and mayo",
    "@xcite proposed a shifted symmetric higher order power method to compute its z - eigenpairs .",
    "recently , they @xcite extended the shifted power method to generalized eigenpairs of tensors and gave an adaptive shift .",
    "based on the nonlinear optimization model with a compact unit spherical constraint , the power methods @xcite project the gradient of the objective at the current iterate onto the unit sphere at each iteration .",
    "its computation is very simple but may not converge @xcite .",
    "kolda and mayo @xcite introduced a shift to force the objective to be ( locally ) concave / convex .",
    "then the power method produces increasing / decreasing steps for computing maximal / minimal eigenvalues .",
    "the sequence of objectives converges to eigenvalues since the feasible region is compact .",
    "the convergence of the sequence of iterates to eigenvectors is established under the assumption that the tensor has finitely many real eigenvectors .",
    "the linear convergence rate is estimated by a fixed - point analysis .    inspired by the power method ,",
    "various optimization methods have been established .",
    "han @xcite proposed an unconstrained optimization model , which is indeed a quadratic penalty function of the constrained optimization for generalized eigenvalues of symmetric tensors .",
    "hao et al .",
    "@xcite employed a subspace projection method for z - eigenvalues of symmetric tensors .",
    "restricted by a unit spherical constraint , this method minimizes the objective in a big circle of @xmath1 dimensional unit sphere at each iteration .",
    "since the objective is a homogeneous polynomial , the minimization of the subproblem has a closed - form solution .",
    "additionally , hao et al .",
    "@xcite gave a trust region method to calculate z - eigenvalues of symmetric tensors .",
    "the sequence of iterates generated by this method converges to a second order critical point and enjoys a locally quadratic convergence rate .",
    "since nonlinear optimization methods may produce a local minimizer , some convex optimization models have been studied .",
    "@xcite addressed a sequential semi - definite programming method to compute the extremal z - eigenvalues of tensors .",
    "a sophisticated jacobian semi - definite relaxation method was explored by cui et al .",
    "a remarkable feature of this method is the ability to compute all of the real eigenvalues of symmetric tensors .",
    "recently , chen et al .",
    "@xcite proposed homotopy continuation methods to compute all of the complex eigenvalues of tensors .",
    "when the order or the dimension of a tensor grows larger , the cpu times of these methods become longer and longer .    in some applications @xcite , the scales of hankel tensors",
    "can be quite huge .",
    "this highly restricted the applications of the above mentioned methods in this case . how to compute the smallest and the largest eigenvalues of a hankel tensor ?",
    "can we propose a method to compute the smallest and the largest eigenvalues of a relatively large hankel tensor , say @xmath3 dimension ?",
    "this is one of the motivations of this paper .",
    "owing to the multi - linearity of tensors , we model the problem of eigenvalues of hankel tensors as a nonlinear optimization problem with a unit spherical constraint .",
    "our algorithm is an inexact steepest descent method on the unit sphere . to preserve iterates on the unit sphere",
    ", we employ the cayley transform to generate an orthogonal matrix such that the new iterate is this orthogonal matrix times the current iterate . by the sherman - morrison - woodbury formula , the product of the orthogonal matrix and a vector has a closed - form solution .",
    "so the subproblem is straightforward .",
    "a curvilinear search is employed to guarantee the convergence .",
    "then , we prove that every accumulation point of the sequence of iterates is an eigenvector of the involved hankel tensor , and its objective is the corresponding eigenvalue . furthermore , using the kurdyka - ojasiewicz property of the eigen - problem of tensors , we prove that the sequence of iterates converges without an assumption of second order sufficient condition . under mild conditions ,",
    "we show that the sequence of iterates has a linear or a sublinear convergence rate .",
    "numerical experiments show that this strategy is successful .",
    "the outline of this paper is drawn as follows .",
    "we introduce a fast computational framework for products of a well - structured hankel tensor and vectors in section 2 .",
    "the computational cost is cheap . in section 3 ,",
    "we show the techniques of using the cayley transform to construct an effective curvilinear search algorithm .",
    "the convergence of objective and iterates are analyzed in section 4 .",
    "the kurdyka - ojasiewicz property is applied to analyze an inexact line search method .",
    "numerical experiments in section 5 address that the new method is efficient and promising .",
    "finally , we conclude the paper with section 6 .",
    "suppose @xmath4 is an @xmath0th order @xmath1 dimensional real symmetric tensor @xmath5 where all of the entries are real and invariant under any index permutation .",
    "two products of the tensor @xmath4 and a column vector @xmath6 used in this paper are defined as follows .",
    "* @xmath7 is a scalar @xmath8 * @xmath9 is a column vector @xmath10    when the tensor @xmath4 is dense , the computations of produces @xmath7 and @xmath9 require @xmath11 operations , since the tensor @xmath4 has @xmath12 entries and we must visit all of them in the process of calculation .",
    "when the tensor is symmetric , the computational cost for these products is about @xmath13 @xcite .",
    "obviously , they are expensive . in this section",
    ", we will study a special tensor , the hankel tensor , whose elements are completely determined by a generating vector .",
    "so there exists a fast algorithm to compute products of a hankel tensor and vectors .",
    "let us give the definitions of two structured tensors .",
    "an @xmath0th order @xmath1 dimensional tensor @xmath14 is called a hankel tensor if its entries satisfy @xmath15 the vector @xmath16 with length @xmath17 is called the generating vector of the hankel tensor @xmath14 .",
    "an @xmath0th order @xmath18 dimensional tensor @xmath19 is called an anti - circulant tensor if its entries satisfy @xmath20    it is easy to see that @xmath14 is a sub - tensor of @xmath19 . since for the same generating vector @xmath21 we have @xmath22 for example , a third order two dimensional hankel tensor with a generating vector @xmath23 is @xmath24.\\ ] ] it is a sub - tensor of an anti - circulant tensor with the same order and a larger dimension @xmath25.\\ ] ]    as discovered in ( * ? ? ?",
    "* theorem 3.1 ) , the @xmath0th order @xmath18 dimensional anti - circulant tensor @xmath19 could be diagonalized by the @xmath18-by-@xmath18 fourier matrix @xmath26 , i.e. , @xmath27 , where @xmath28 is a diagonal tensor whose diagonal entries are @xmath29 .",
    "it is well - known that the computations involving the fourier matrix and its inverse times a vector are indeed the fast ( inverse ) fourier transform fft and ifft , respectively .",
    "the computational cost is about @xmath30 multiplications , which is significantly smaller than @xmath31 for a dense matrix times a vector when the dimension @xmath18 is large .",
    "now , we are ready to show how to compute the products introduced in the beginning of this section , when the involved tensor has a hankel structure . for any @xmath6 , we define another vector @xmath32 such that @xmath33,\\ ] ] where @xmath34 and @xmath35 is a zero vector with length @xmath36 .",
    "then , we have @xmath37 to obtain @xmath38 , we first compute @xmath39 then , the entries of vector @xmath38 is the leading @xmath1 entries of @xmath40 . here , @xmath41 denotes the hadamard product such that @xmath42 .",
    "three matrices @xmath43 , @xmath44 and @xmath45 have the same size .",
    "furthermore , we define @xmath46 as the hadamard product of @xmath47 copies of @xmath43 .",
    "since the computations of @xmath48 and @xmath38 require @xmath49 and @xmath50 fft / iffts , the computational cost is about @xmath2 and obviously cheap .",
    "another advantage of this approach is that we do not need to store and deal with the tremendous hankel tensor explicitly .",
    "it is sufficient to keep and work with the compact generating vector of that hankel tensor .",
    "we consider the generalized eigenvalue @xcite of an @xmath0th order @xmath1 dimensional hankel tensor @xmath14 @xmath51 where @xmath0 is even , @xmath52 is an @xmath0th order @xmath1 dimensional symmetric tensor and it is positive definite . if there is a scalar @xmath53 and a real vector @xmath54 satisfying this system , we call @xmath53 a generalized eigenvalue and @xmath54 its associated generalized eigenvector .",
    "particularly , we find the following definitions from the literature , where the computation on the tensor @xmath52 is straightforward .    *",
    "qi @xcite called a real scalar @xmath53 a z - eigenvalue of a tensor @xmath14 and a real vector @xmath54 its associated z - eigenvector if they satisfy @xmath55 this definition means that the tensor @xmath52 is an identity tensor @xmath56 such that @xmath57 . *",
    "if @xmath58 , where @xmath59 the real scalar @xmath53 is called an h - eigenvalue and the real vector @xmath54 is its associated h - eigenvector @xcite .",
    "obviously , we have @xmath60 for @xmath61 .",
    "to compute a generalized eigenvalue and its associated eigenvector , we consider the following optimization model with a spherical constraint @xmath62 where @xmath63 denotes the euclidean norm or its induced matrix norm .",
    "the denominator of the objective is positive since the tensor @xmath52 is positive definite . by some calculations ,",
    "we get its gradient and hessian , which are formally presented in the following lemma .",
    "[ lm3 - 1 ] suppose that the objective is defined as in ( [ eq3 - 02 ] ) .",
    "then , its gradient is @xmath64 and its hessian is @xmath65 where @xmath66 .",
    "let @xmath67 be the spherical feasible region .",
    "suppose the current iterate is @xmath68 and the gradient at @xmath54 is @xmath69 . because @xmath70 the gradient @xmath69 of @xmath68 is located in the tangent plane of @xmath71 at @xmath54 .",
    "[ lm3 - 2 ] suppose @xmath72 , where @xmath68 and @xmath73 is a small number .",
    "denote @xmath74 .",
    "then , we have @xmath75 moreover , if the gradient @xmath69 at @xmath54 vanishes , then @xmath76 is a generalized eigenvalue and @xmath54 is its associated generalized eigenvector .    *",
    "proof * recalling the definition of gradient ( [ eq3 - 03 ] ) , we have @xmath77 since the tensor @xmath52 is positive definite and the vector @xmath54 belongs to a compact set @xmath71 , @xmath78 has a finite upper bound .",
    "thus , the first assertion is valid .    if @xmath79 , we immediately know that @xmath76 is a generalized eigenvalue and @xmath54 is its associated generalized eigenvector .    next , we construct the curvilinear search path using the cayley transform @xcite .",
    "cayley transform is an effective method which could preserve the orthogonal constraints .",
    "it has various applications in the inverse eigenvalue problem @xcite , @xmath80-harmonic flow @xcite , and matrix optimization @xcite .",
    "suppose the current iterate is @xmath81 and the next iterate is @xmath82 . to preserve the spherical constraint @xmath83",
    ", we choose the next iterate @xmath82 such that @xmath84 where @xmath85 is an orthogonal matrix , whose eigenvalues do not contain @xmath86 . using the cayley transform ,",
    "the matrix @xmath87 is orthogonal if and only if the matrix @xmath88 is skew - symmetric .",
    "now , our task is to select a suitable skew - symmetric matrix @xmath89 such that @xmath90 . for simplicity , we take the matrix @xmath89 as @xmath91 where @xmath92 are two undetermined vectors . from ( [ eq3 - 07 ] ) and ( [ eq3 - 08 ] )",
    ", we have @xmath93 then , by ( [ eq3 - 09 ] ) , it yields that @xmath94({{\\bf x}}_k+{{\\bf x}}_{k+1}).\\ ] ] for convenience , we choose @xmath95 here , @xmath96 is a positive parameter , which serves as a step size , so that we have some freedom to choose the next iterate . according to this selection and ( [ eq3 - 06 ] ) ,",
    "we obtain @xmath97 since @xmath86 is not an eigenvalue of the orthogonal matrix @xmath98 , we have @xmath99 for @xmath100 .",
    "therefore , the conclusion @xmath101 holds for any positive step size @xmath96 .",
    "we summarize the iterative process in the following theorem .",
    "suppose that the new iterate @xmath82 is generated by ( [ eq3 - 07 ] ) , ( [ eq3 - 08 ] ) , ( [ eq3 - 09 ] ) , and ( [ eq3 - 10 ] ) . then , the following assertions hold .",
    "* the iterative scheme is @xmath102 * the progress made by @xmath82 is @xmath103      whereafter , we devote to choose a suitable step size @xmath96 by an inexact curvilinear search . at the beginning , we give a useful theorem .",
    "[ th3 - 5 ] suppose that the new iterate @xmath105 is generated by ( [ eq3 - 11 ] ) .",
    "then , we have @xmath106    * proof * by some calculations , we get @xmath107 hence , @xmath108 . furthermore , @xmath109 .",
    "therefore , we obtain @xmath110 the proof is completed .",
    "give the generating vector @xmath21 of a hankel tensor @xmath14 , the symmetric tensor @xmath52 , an initial unit iterate @xmath111 , parameters @xmath112 $ ] , @xmath113 , @xmath114 , and @xmath115 .",
    "compute @xmath116 and @xmath117 by the fast computational framework introduces in section 2 .",
    "calculate @xmath118 , @xmath119 , @xmath120 and @xmath121 by ( [ eq3 - 03 ] ) .",
    "choose the smallest nonnegative integer @xmath18 and determine @xmath122 such that @xmath123 where @xmath105 is calculated by ( [ eq3 - 11 ] ) .",
    "update the iterate @xmath124 .",
    "choose an initial step size @xmath125 $ ] for the next iteration .",
    "@xmath126    according to theorem [ th3 - 5 ] , for any constant @xmath127 , there exists a positive scalar @xmath128 such that for all @xmath129 $ ] , @xmath130 hence , the curvilinear search process is well - defined .",
    "now , we present a curvilinear search algorithm ( acsa ) formally in algorithm [ alg ] for the smallest generalized eigenvalue and its associated eigenvector of a hankel tensor .",
    "if our aim is to compute the largest generalized eigenvalue and its associated eigenvector of a hankel tensor , we only need to change respectively ( [ eq3 - 11 ] ) and ( [ suf - dec ] ) used in steps 5 and 6 of the acsa algorithm to @xmath131 and @xmath132    when the z - eigenvalue of a hankel tensor is considered , we have @xmath133 and the objective @xmath134 is a polynomial . then , we could compute the global minimizer of the step size @xmath135 ( the exact line search ) in each iteration as @xcite",
    ". however , we use a cheaper inexact line search here .",
    "the initial step size of the next iteration follows dai s strategy @xcite @xmath136 which is the geometric mean of barzilai - borwein step sizes @xcite .",
    "since the optimization model ( [ eq3 - 02 ] ) has a nice algebraic nature , we will use the kurdyka - ojasiewicz property @xcite to analyze the convergence of the proposed acsa algorithm .",
    "before we start , we give some basic convergence results .",
    "if the acsa algorithm terminates finitely , there exists a positive integer @xmath47 such that @xmath137 . according to lemma [ lm3 - 2 ] , @xmath138 is a generalized eigenvalue and @xmath139 is its associated generalized eigenvector .",
    "next , we assume that acsa generates an infinitely sequence of iterates .",
    "suppose that the even order symmetric tensor @xmath52 is positive definite .",
    "then , all the functions , gradients , and hessians of the objective ( [ eq3 - 02 ] ) at feasible points are bounded .",
    "that is to say , there is a positive constant @xmath140 such that for all @xmath68 @xmath141    * proof * since the spherical feasible region @xmath71 is compact , the denominator @xmath78 of the objective is positive and bounds away from zero .",
    "recalling lemma [ lm3 - 1 ] , we get this theorem immediately .",
    "suppose that the infinite sequence @xmath142 is generated by acsa .",
    "then , the sequence @xmath142 is monotonously decreasing .",
    "and there exists a @xmath143 such that @xmath144    * proof * since @xmath145 which is bounded and monotonously decreasing , the infinite sequence @xmath142 must converge to a unique @xmath143 .",
    "this theorem means that the sequence of generalized eigenvalues converges . to show the convergence of iterates",
    ", we first prove that the step sizes bound away from zero .",
    "suppose that the step size @xmath135 is generated by acsa .",
    "then , for all iterations @xmath47 , we get @xmath146    * proof * let @xmath147 . according to the curvilinear search process of acsa ,",
    "it is sufficient to prove that the inequality ( [ suf - dec ] ) holds if @xmath148 $ ] .    from the iterative formula ( [ eq3 - 11 ] ) and the equality ( [ eq3 - 06 ] ) , we get @xmath149 hence , @xmath150    from the mean value theorem , ( [ eq3 - 11 ] ) , ( [ eq3 - 06 ] ) , and ( [ eq4 - 07 ] ) , we have @xmath151 it is easy to show that for all @xmath152 $ ] @xmath153 therefore , we have @xmath154 the proof is completed .    [ th4 - 04 ]",
    "suppose that the infinite sequence @xmath155 is generated by acsa .",
    "then , the sequence @xmath155 has an accumulation point at least .",
    "and we have @xmath156 that is to say , every accumulation point of @xmath155 is a generalized eigenvector whose associated generalized eigenvalue is @xmath143 .",
    "* proof * since the sequence of objectives @xmath157 is monotonously decreasing and bounded , by ( [ suf - dec ] ) and ( [ eq4 - 06 ] ) , we have @xmath158 it yields that @xmath159 thus , the limit ( [ eq4 - 05 ] ) holds .",
    "let @xmath160 be an accumulation point of @xmath155 .",
    "then @xmath160 belongs to the compact set @xmath71 and @xmath161 . according to lemma [ lm3 - 2 ]",
    ", @xmath160 is a generalized eigenvector whose associated eigenvalue is @xmath162 .      in this subsection",
    ", we will prove that the iterates @xmath155 generated by acsa converge without an assumption of the second - order sufficient condition .",
    "the key tool of our analysis is the kurdyka - ojasiewicz property .",
    "this property was first discovered by s. ojasiewicz @xcite in 1963 for real - analytic functions .",
    "bolte et al .",
    "@xcite extended this property to nonsmooth subanalytic functions .",
    "whereafter , the kurdyka - ojasiewicz property was widely applied to analyze regularized algorithms for nonconvex optimization @xcite .",
    "significantly , it seems to be new to use the kurdyka - ojasiewicz property to analyze an inexact line search algorithm , e.g. , acsa proposed in section 3 .",
    "we now write down the kurdyka - ojasiewicz property ( * ? ? ?",
    "* theorem 3.1 ) for completeness .",
    "[ th4 - 05 ] suppose that @xmath163 is a critical point of @xmath134 .",
    "then there is a neighborhood @xmath164 of @xmath163 , an exponent @xmath165 , and a constant @xmath166 such that for all @xmath167 , the following inequality holds @xmath168 here , we define @xmath169 .    [ lm4 - 06 ] suppose that @xmath163 is one of the accumulation points of @xmath155 . for the convenience of using the kurdyka - ojasiewicz property , we assume that the initial iterate @xmath111 satisfies @xmath170 where @xmath171 then , we have the following two assertions : @xmath172 and @xmath173    * proof * we prove ( [ eq4 - 10 ] ) by the induction .",
    "first , it is easy to see that @xmath174 .",
    "next , we assume that there is an integer @xmath175 such that @xmath176 hence , the kl property ( [ eq4 - 09 ] ) holds in these iterates .",
    "finally , we now prove that @xmath177 .    for the convenience of presentation",
    ", we define a scalar function @xmath178 obviously , @xmath179 is a concave function and its derivative is @xmath180 if @xmath181 .",
    "then , for any @xmath182 , we have @xmath183\\qquad }   & \\geq & \\frac{1}{\\|{{\\bf g}}({{\\bf x}}_k)\\|}(f({{\\bf x}}_k)-f({{\\bf x}}_{k+1 } ) ) \\\\      \\text{[since ( \\ref{suf - dec})]\\qquad }   & \\geq & \\frac{1}{\\|{{\\bf g}}({{\\bf x}}_k)\\|}\\eta\\alpha_k\\|{{\\bf g}}({{\\bf x}}_k)\\|^2 \\\\        & \\geq & \\frac{\\eta\\alpha_k\\|{{\\bf g}}({{\\bf x}}_k)\\|}{\\sqrt{1+\\alpha_k^2\\|{{\\bf g}}({{\\bf x}}_k)\\|^2 } } \\\\",
    "\\text{[because of ( \\ref{eq4 - 07})]\\qquad }   & \\geq & \\frac{\\eta}{2}\\|{{\\bf x}}_{k+1}-{{\\bf x}}_k\\| .",
    "\\end{aligned}\\ ] ] it yields that @xmath184 so , we get @xmath185 thus , @xmath177 and ( [ eq4 - 10 ] ) holds",
    ".    moreover , let @xmath186 in ( [ eq4 - 12 ] ) .",
    "we obtain ( [ eq4 - 11 ] ) .",
    "suppose that the infinite sequence of iterates @xmath155 is generated by acsa .",
    "then , the total sequence @xmath155 has a finite length , i.e. , @xmath187 and hence the total sequence @xmath155 converges to a unique critical point .",
    "* proof * since the domain of @xmath134 is compact , the infinite sequence @xmath155 generated by acsa must have an accumulation point @xmath163 . according to theorem [ th4 - 04 ]",
    ", @xmath163 is a critical point .",
    "hence , there exists an index @xmath188 , which could be viewed as an initial iteration when we use lemma [ lm4 - 06 ] , such that @xmath189 . from lemma",
    "[ lm4 - 06 ] , we have @xmath190 .",
    "therefore , the total sequence @xmath155 has a finite length and converges to a unique critical point .",
    "there exists a positive constant @xmath191 such that @xmath192    * proof * since @xmath193 and ( [ eq4 - 07 ] ) , we have @xmath194 let @xmath195 .",
    "we get this lemma .",
    "[ th4 - 06 ] suppose that @xmath163 is the critical point of the infinite sequence of iterates @xmath155 generated by acsa .",
    "then , we have the following estimations .    *",
    "if @xmath196 $ ] , there exists a @xmath197 and @xmath198 such that @xmath199 * if @xmath200 , there exists a @xmath197 such that @xmath201    * proof * without loss of generality , we assume that @xmath174 . for convenience of following analysis , we define @xmath202 then , we have @xmath203 }   & \\leq & \\frac{2c_1}{\\eta(1-\\theta)}|f({{\\bf x}}_k)-f({{\\bf x}}_*)|^{1-\\theta } \\nonumber\\\\      & = & \\frac{2c_1}{\\eta(1-\\theta)}\\left(|f({{\\bf x}}_k)-f({{\\bf x}}_*)|^{\\theta}\\right)^{\\frac{1-\\theta}{\\theta } } \\nonumber\\\\      \\text { [ kl property ] } & \\leq &          \\frac{2c_1}{\\eta(1-\\theta)}\\left(c_1\\|{{\\bf g}}({{\\bf x}}_k)\\|\\right)^{\\frac{1-\\theta}{\\theta } } \\nonumber\\\\      \\text { [ for ( \\ref{eq4 - 13 } ) ] } & \\leq &          \\frac{2c_1}{\\eta(1-\\theta)}\\left(c_1c_2^{-1}\\|{{\\bf x}}_k-{{\\bf x}}_{k+1}\\|\\right)^{\\frac{1-\\theta}{\\theta } } \\nonumber\\\\      & = & \\frac{2c_1^{\\frac{1}{\\theta}}c_2^{-\\frac{1-\\theta}{\\theta}}}{\\eta(1-\\theta ) }          \\left(\\delta_k-\\delta_{k+1}\\right)^{\\frac{1-\\theta}{\\theta } } \\nonumber\\\\      & \\equiv & c_3\\left(\\delta_k-\\delta_{k+1}\\right)^{\\frac{1-\\theta}{\\theta } } ,   \\label{eq4 - 14 }    \\end{aligned}\\ ] ] where @xmath204 is a positive constant .    if @xmath205 , we have @xmath206 . when the iteration @xmath47 is large enough , the inequality ( [ eq4 - 14 ] ) implies that @xmath207 that is @xmath208 hence , recalling @xmath209 , we obtain the estimation if we take @xmath210 .",
    "otherwise , we consider the case @xmath200 .",
    "let @xmath211 .",
    "obviously , @xmath212 is monotonously decreasing .",
    "then , the inequality ( [ eq4 - 14 ] ) could be rewritten as @xmath213 denote @xmath214 since @xmath200 .",
    "then , we get @xmath215 it yields that for all @xmath216 , @xmath217^{\\frac{1}{\\nu } } \\leq \\gamma k^{\\frac{1}{\\nu}},\\ ] ] where the last inequality holds when the iteration @xmath47 is sufficiently large .",
    "we remark that if the hessian @xmath218 at the critical point @xmath163 is positive definite , the key parameter @xmath219 in the kurdyka - ojasiewicz property is @xmath220 . under theorem [ th4 - 06 ] , the sequence of iterates generated by acsa has a linear convergence rate . in this viewpoint ,",
    "the kurdyka - ojasiewicz property is weaker than the second order sufficient condition of @xmath163 being a minimizer .",
    "to show the efficiency of the proposed acsa algorithm , we perform some numerical experiments .",
    "the parameters used in acsa are @xmath221 we terminate the algorithm if the objectives satisfy @xmath222 or the number of iterations exceeds @xmath223 .",
    "the codes are written in matlab r2012a and run in a desktop computer with intel core e8500 cpu at 3.17ghz and 4 gb memory running windows 7 .",
    "we will compare the following four algorithms in this section .",
    "* an adaptive shifted power method @xcite ( power m. ) is implemented as eig_sshopm and eig_geap in tensor toolbox 2.6 for z- and h - eigenvalues of even order symmetric tensors . * an unconstrained optimization approach @xcite ( han s uoa ) is solved by fminunc in matlab with settings : gradobj : on , largescale : off , tolx:1.e-10 , tolfun:1.e-8 , maxiter:10000 , display : off . * for general symmetric tensors without considering a hankel structure , we implement acsa as acsa - general . * the acsa algorithm ( acsa - hankel ) is proposed in section 3 for hankel tensors .",
    "first , we examine some small tensors , whose z- and h - eigenvalues could be computed exactly .",
    "[ exm-02 ] a hankel tensor @xmath4 whose entries are defined as @xmath224 its generating vector is @xmath225 .    if @xmath226 and @xmath227 , there are five z - eigenvalues which are listed as follows @xcite @xmath228    .computed z - eigenvalues of the hankel tensor in example [ exm-02 ] . [ cols=\"^,^,^,^,^\",options=\"header \" ,",
    "]     we illustrate by numerical experiments to show whether these bounds are tight ?",
    "first , for the dimension varying from ten to one million , we calculate the theoretical upper bounds of the largest z - eigenvalues of corresponding fourth order and sixth order hilbert tensors .",
    "then , for each hilbert tensor , we choose ten initial points and employ the acsa algorithm equipped with a fast computational framework for products of a hankel tensor and vectors to compute the largest z - eigenvalues .",
    "these results are shown in the left sub - figure of figure [ fig - hilbert - zeigen ] .",
    "the right sub - figure of figure [ fig - hilbert - zeigen ] shows the corresponding cpu times for acsa - hankel .",
    "we can see that the theoretical upper bounds for the largest z - eigenvalues of the hilbert tensors are almost tight up to a constant multiple .",
    "similar results for the largest h - eigenvalues and their theoretical upper bounds of hilbert tensors are illustrated in figure [ fig - hilbert - heigen ] .",
    "we proposed an inexact steepest descent method processing on a unit sphere for generalized eigenvalues and associated eigenvectors of hankel tensors .",
    "owing to the fast computation framework for the products of a hankel tensor and vectors , the new algorithm is fast and efficient as shown by some preliminary numerical experiments .",
    "since the hankel structure is well - exploited , the new method could deal with some large scale hankel tensors , whose dimension is up to one million in a desktop computer .",
    "the first author thanks mr . weiyang ding and dr .",
    "ziyan luo for the discussion on numerical experiments .",
    "h. attouch , j. bolte , p. redont and a. soubeyran , `` proximal alternating minimization and projection methods for nonconvex problems : an approach based on the kurdyka - ojasiewicz inequality '' , _ math . oper . res . *",
    "35 * _ ( 2010 ) 438 - 457 .",
    "y. dai , `` a positive bb - like stepsize and an extension for symmetric linear systems '' , in _ workshop on optimization for modern computation _ , beijing , china , ( 2014 ) , `` http://bicmr.pku.edu.cn/conference/opt-2014/slides/yuhong-dai.pdf '' .",
    "u. kang , e. papalexakis , a. harpale , and c. faloutsos , `` gigatensor : scaling tensor analysis up by 100 times - algorithms and discoveries '' , in _ proceedings of the 18th acm sigkdd international conference on knowledge discovery and data mining _ , ( 2012 ) 316 - 324 .",
    "lim , `` singular values and eigenvalues of tensors : a variational approach '' , in _ proceedings of the ieee international workshop on computational advances in multi - sensor adaptive processing ( camsap05 ) , * 1 * _ ( 2005 ) 129 - 132 .",
    "s. ojasiewicz , `` une proprit topologique",
    "des sous - ensembles analytiques rels '' , _ les quations aux drives partielles _ , ditions du centre national de la recherche scientifique , paris , 87 - 89 , ( 1963 ) ."
  ],
  "abstract_text": [
    "<S> large scale tensors , including large scale hankel tensors , have many applications in science and engineering . in this paper , we propose an inexact curvilinear search optimization method to compute z- and h - eigenvalues of @xmath0th order @xmath1 dimensional hankel tensors , where @xmath1 is large . owing to the fast fourier transform , the computational cost of each iteration of the new method is about @xmath2 . using the cayley transform </S>",
    "<S> , we obtain an effective curvilinear search scheme . then </S>",
    "<S> , we show that every limiting point of iterates generated by the new algorithm is an eigen - pair of hankel tensors . without the assumption of a second - order sufficient condition , we analyze the linear convergence rate of iterate sequence by the kurdyka - ojasiewicz property . </S>",
    "<S> finally , numerical experiments for hankel tensors , whose dimension may up to one million , are reported to show the efficiency of the proposed curvilinear search method .    </S>",
    "<S> * key words : * cayley transform , curvilinear search , eigenvalue , fast fourier transform , hankel tensor , kurdyka - ojasiewicz property , large scale tensor .    * ams subject classifications ( 2010 ) : * 15a18 , 15a69 , 65f15 , 65k05 , 90c52 . </S>"
  ]
}