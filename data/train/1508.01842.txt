{
  "article_text": [
    "the _ sparse representation _ problem involves solving the system of linear equations @xmath0 where @xmath1 is assumed to be @xmath2-sparse ; i.e. @xmath3 is allowed to have ( at most ) @xmath2 non - zero entries .",
    "the matrix @xmath4 is typically referred to as the _ dictionary _ with @xmath5 elements or _",
    "atoms_. it is well - known that @xmath3 can be uniquely identified if @xmath6 satisfies the so called _ _ spark condition _",
    "_ columns of @xmath6 are linearly independent . ] .",
    "meanwhile , there exist tractable and efficient convex relaxations of the combinatorial problem of finding the ( unique ) @xmath2-sparse solution of @xmath7 with provable recovery guarantees @xcite .    a related problem is _ dictionary learning _ or _",
    "sparse coding _ @xcite which can be expressed as a sparse factorization @xcite of the data matrix @xmath8 ( where both @xmath6 and @xmath9 are assumed unknown ) given that each column of @xmath10 is @xmath2-sparse and @xmath6 satisfies the spark condition as before .",
    "a crucial question is how many data samples ( @xmath11 ) are needed to _ uniquely _ identify @xmath6 and @xmath10 from @xmath12 ? unfortunately , the existing lower bound is ( at best ) exponential @xmath13 assuming an equal number of data samples over each @xmath2-sparse support pattern in @xmath10 @xcite .    in this paper",
    ", we address a more challenging problem .",
    "in particular , we are interested in the above sparse matrix factorization problem @xmath8 ( with both sparsity and spark conditions ) when only @xmath14 random linear measurements from each column of @xmath12 is available .",
    "we would like to find lower bounds for @xmath11 for the ( partially observed ) matrix factorization to be unique .",
    "this problem can also be seen as recovering both the dictionary @xmath6 and the sparse coefficients @xmath10 from compressive measurements of data .",
    "for this reason , this problem has been termed _ blind compressed sensing _ ( bcs ) before @xcite , although the end - goal of bcs is the recovery of @xmath12 .",
    "we start by establishing that the uniqueness of the learned dictionary over random data measurements is a sufficient condition for the success of bcs .",
    "perfect recovery conditions for bcs are derived under two different scenarios . in the first scenario ,",
    "fewer random linear measurements are available from each data sample .",
    "it is stated that having access to a large number of data samples compensates for the inadequacy of sample - wise measurements . meanwhile , in the second scenario , it is assumed that slightly more random linear measurements are available over each data sample and the measurements are partly fixed and partly varying over the data .",
    "this measurement scheme results in a significant reduction in the required number of data samples for perfect recovery .",
    "finally , we address the computational aspects of bcs based on the recent non - iterative dictionary learning algorithms with provable convergence guarantees to the generating dictionary .",
    "bcs was initially proposed in @xcite where it was assumed that , for a given random gaussian sampling matrix @xmath15 ( @xmath16 ) , @xmath17 is observed .",
    "the conclusion was that , assuming the factorization @xmath8 is unique , @xmath18 factorization would also be unique with a high probability when @xmath6 is an orthonormal basis",
    ". however , it would be impossible to recover @xmath6 from @xmath19 when @xmath14 .",
    "it was suggested that structural constraints be imposed over the space of admissible dictionaries to make the inverse problem well - posed .",
    "some of these structures were sparse bases under known dictionaries , finite set of bases and orthogonal block - diagonal bases @xcite .",
    "while these results can be useful in many applications , some of which are mentioned in @xcite , they do not generalize to unconstrained overcomplete dictionaries .",
    "subsequently , there has been a line of empirical work on showing that dictionary learning from compressive data  a sufficient step for bcs  can be successful given that a different sampling matrix is employed for each data sample is no longer valid which is possibly a reason for the lack of a theoretical extension of bcs to this case . ]",
    "( i.e. each column of @xmath12 ) .",
    "for example , @xcite uses a modified k - svd to train both the dictionary and the sparse coefficients from the incomplete data .",
    "meanwhile , @xcite use generic gradient descent optimization approaches for dictionary learning when only random projections of data are available .",
    "the empirical success of dictionary learning with partial as well as compressive or projected data triggers more theoretical interest in finding the uniqueness bounds of the unconstrained bcs problem .",
    "finally , we must mention the theoretical results presented in the pre - print @xcite on bcs with overcomplete dictionaries while @xmath10 is assumed to lie in a structured union of disjoint subspaces @xcite .",
    "it is also proposed that the results of this work extend to the generic sparse coding model if the ` one - block sparsity ' assumption is relaxed .",
    "we argue that the main theoretical result in this pre - print is incomplete and technically flawed as briefly explained here . in the proof of theorem 1 of @xcite",
    ", it is proposed that ( with adjustment of notation ) _ `` assignment [ of @xmath12 s columns to rank-@xmath20 disjoint subsets ] can be done by the ( admittedly impractical ) procedure of testing the rank of all possible @xmath21 matrices constructed by concatenating subsets of @xmath22 column vectors , as assumed in @xcite''_. however , it is ignored that the entries of @xmath12 are missing at random and the rank of an incomplete matrix can not be measured . as it becomes more clear later , the main challenge in the uniqueness analysis of unconstrained bcs is in addressing this particular issue .",
    "two strategies to tackle this issue that are presented in this paper are : 1 ) increasing the number of data samples and 2 ) designing and employing measurement schemes that preserve the low - rank structure of @xmath12 s sub - matrices .",
    "this paper is organized as follows . in section [ sec : problem ] , we provide the formal problem definition for bcs .",
    "our main results are presented in section [ sec : main ] .",
    "we present the proofs in section [ sec : proofs ] .",
    "practical aspects of bcs are treated in section [ sec : algorithm ] where we explain how provable dictionary learning algorithms , such as @xcite , can be utilized for bcs . finally , we conclude the paper and present future directions in section [ sec : conclusion ] .",
    "our general convention throughout this paper is to use capital letters for matrices and small letters for vectors and scalars . for a matrix @xmath9",
    ", @xmath23 denotes its entry on row @xmath24 and column @xmath25 , @xmath26 denotes its @xmath24th column and @xmath27 denotes its column - major vectorized format .",
    "the inner product between two matrices @xmath6 and @xmath28 ( of the same sizes ) is defined as @xmath29 .",
    "let @xmath30 denote the smallest number of @xmath6 s columns that are linearly dependent .",
    "@xmath6 is @xmath31-coherent if @xmath32 we have @xmath33 .",
    "finally , let @xmath34\\coloneqq\\{1,2,\\dots , m\\}$ ] and let @xmath35\\choose k}$ ] denote the set of all subsets of @xmath34 $ ] of size @xmath2 .",
    "construct the data matrix @xmath36 by concatenating @xmath11 signal vectors @xmath37 ( for @xmath25 from 1 to @xmath11 ) . throughout this paper , we make the following assumptions about the sampling operator and the data sparsity .",
    "it must be noted that the following assumptions over the sparse coding of @xmath12 are minimal among existing sparse coding assumptions for provable uniqueness guarantees ; see e.g. @xcite .",
    "suppose @xmath38 linear measurements are taken from each signal @xmath37 as in @xmath39 where @xmath40 is referred to as the sampling matrix .",
    "we could also represent the measurements as a linear projection of the signal onto the row - space of the sampling matrix can be computed from @xmath41 using the relationship @xmath42 .",
    "therefore , @xmath41 and @xmath43 carry the same amount of information about @xmath44 given the sampling matrix @xmath45 .",
    "] : @xmath46    we will use @xmath47^t\\in\\mathbb{r}^{pn}$ ] to denote the observations and @xmath48 to denote the projected matrix that is a concatenation of all @xmath41 . specifically ,",
    "when entries of each @xmath45 are drawn independently from a random gaussian distribution with mean zero and variance @xmath49 , we use the notations @xmath50 and @xmath51 .",
    "assume @xmath8 where @xmath4 denotes the dictionary ( @xmath52 in the overcomplete setting ) and @xmath9 is a sparse matrix with exactly @xmath2 non - zero entries per column and @xmath53 .",
    "additionally , assume that each column of @xmath10 is randomly drawn by first selecting its support @xmath54\\choose k}$ ] uniformly at random and then filling the support entries with random i.i.d .",
    "values uniformly drawn from a bounded interval , e.g. @xmath55\\subset\\mathbb{r}$ ] .",
    "we denote by @xmath56 the set of feasible @xmath12 under the described sparse coding model .",
    "note that the assumption @xmath53 is necessary to ensure a unique @xmath10 even when @xmath6 is known and fixed .    as noted and proved in @xcite , when @xmath57 , with probability one , no subset of @xmath2 ( or less ) columns of @xmath12 is linearly dependent . also with probability one ,",
    "if a subset of @xmath58 columns of @xmath12 are linearly dependent , then all of the @xmath58 columns must have the same support .",
    "given the above definitions , we can now formally express the problem definition for bcs :    recover @xmath57 from @xmath59 given @xmath60 , @xmath61 and @xmath2 .",
    "our results throughout this paper are mainly developed for the class of gaussian measurements @xmath62 .",
    "however , it is not difficult to extend these results to the larger class of continuous sub - gaussian distributions for @xmath60 .",
    "to start with , assume that there are exactly @xmath63 columns in @xmath10 for each support pattern @xmath64 where @xmath65\\choose k}$ ] .",
    "for better understanding and without loss of generality , one can assume that the data samples are ordered according to the following sketch for @xmath10 :    @xmath66 \\noalign{\\vspace{-2\\normalbaselineskip } } \\multicolumn{7}{c}{n=\\ell|\\mathcal{s}|\\mbox { samples}}\\\\ \\multicolumn{7}{c}{$\\downbracefill$}\\\\ \\multicolumn{3}{c}{\\mbox{group } \\#1\\;\\;\\ ; } & \\multicolumn{1}{c } { } & \\multicolumn{3}{c}{\\mbox{group } \\#|\\mathcal{s}|\\;\\;\\;}\\\\ \\multicolumn{3}{c}{\\mbox{support : } s^{1 } } & \\multicolumn{1}{c } { } & \\multicolumn{3}{c}{\\mbox{support : } s^{|\\mathcal{s}| } } \\\\ x_1 & \\dots & x_\\ell & \\dots & x_{n-\\ell + 1 } & \\dots & x_n \\\\",
    "\\multicolumn{3}{c}{$\\upbracefill$ } & \\multicolumn{1}{c}{}&\\multicolumn{3}{c}{$\\upbracefill$ } \\\\",
    "\\multicolumn{3}{c}{\\ell\\mbox { samples } } & \\multicolumn{1}{c } { } & \\multicolumn{3}{c}{\\ell\\mbox { samples } } \\\\ \\noalign{\\vspace{-2\\normalbaselineskip } } \\end{pmatrix }   \\vspace{2\\normalbaselineskip } \\ ] ] the best known bound for @xmath63 , for the factorization @xmath8 to be unique ( with probability one ) under the specified random sparse coding model , is @xmath67 .",
    "this results in an exponential sample complexity @xmath13 .",
    "specifically , it is said that ` @xmath8 factorization is unique ' if there exist a diagonal matrix @xmath68 and a permutation matrix @xmath69 such that for any other feasible factorization @xmath70 , we have @xmath71 .",
    "clearly , this ambiguity makes it more challenging to prove the uniqueness of the dictionary learning problem .",
    "meanwhile , authors in @xcite propose a strategy for handling the permutation and scaling ambiguity which is reviewed in lemma [ lem : apd ] .    through the following lemma",
    ", we can establish that the uniqueness of the learned dictionary is a sufficient condition for the success of bcs ( proof is provided in appendix ) .",
    "suppose for every pair @xmath72 that satisfy @xmath73 with @xmath74 , @xmath71 for some diagonal matrix @xmath75 and permutation matrix @xmath69 .",
    "then @xmath76 with probability one .",
    "[ lem : forgot ]    briefly speaking , existing uniqueness results exploit the fact that the rank of each group of columns in the above sketch is bounded above by @xmath2 .",
    "this makes it possible to uniquely identify groups of samples that share the same support pattern .",
    "meanwhile , when only @xmath59 is available , it might not be possible to uniquely identify these groups .",
    "nevertheless , it is noted in @xcite that @xmath77 ensures uniqueness without the need for grouping , at the cost of significantly increasing the required number of data samples ( compared to @xmath67 ) .    in our initial bcs uniqueness result",
    ", we use the pigeon - hole strategy of @xcite which results in a less practical bound @xmath78 even when @xmath12 is completely observed . ] . yet , it is interesting to explore the implications of a finite @xmath11 that ensures a successful bcs for the general sparse coding model .",
    "the cs theory requires the complete knowledge of @xmath6 to uniquely recover @xmath10 and @xmath12 from @xmath59 .",
    "meanwhile , our results assert that @xmath6 , @xmath10 and @xmath12 can be uniquely identified from @xmath59 given a large but finite number of samples @xmath11 .",
    "necessary proofs for the results of this section are presented in the following section .",
    "assume @xmath74 and there are exactly @xmath63 columns in @xmath10 for each @xmath64",
    ". then @xmath57 can be perfectly recovered from @xmath50 with probability one given that @xmath79 .",
    "[ lem : gell-2k ]    with probability at least @xmath80 , @xmath57 can be perfectly recovered from @xmath50 given that @xmath74 and @xmath81 .",
    "[ cor : gn-2k ]    aside from the intellectual implications of theorem [ lem : gell-2k ] and corollary [ cor : gn-2k ] discussed above , the stated bounds for @xmath63 and @xmath11 are clearly not very practical . to reduce these bounds while guaranteeing the success of bcs",
    ", we introduce a _ hybrid measurement _ scheme that we explain below .      *",
    "( hybrid gaussian measurement ) * in a hybrid measurement scheme , @xmath82 $ ] where @xmath83 stands for the fixed part of sampling matrix and @xmath84 stands for the varying part of the sampling matrix .",
    "the total number of measurements per column is @xmath85 . in a hybrid gaussian measurement scheme , @xmath86 and @xmath87 through @xmath88",
    "are assumed to be drawn independently from an i.i.d .",
    "zero - mean gaussian distribution with variance @xmath49 .",
    "the observations corresponding to @xmath86 and @xmath89 s are denoted by @xmath90 and @xmath91 respectively .    as mentioned earlier ,",
    "the hybrid measurement scheme was designed to reduce the required number of data samples for perfect bcs recovery . in particular , as formalized in lemma [ lem : rank - check ] , the fixed part of the measurements is designed to retain the low - rank structure of each @xmath2-dimensional subspace associated with a particular @xmath64 . meanwhile ,",
    "the varying part of the measurements is essential for the uniqueness of the learned dictionary .",
    "assume @xmath92 and there are exactly @xmath63 columns in @xmath10 for each @xmath64",
    ". then @xmath57 can be perfectly recovered from hybrid gaussian measurements with probability one given that @xmath93 .",
    "[ cor : gell-4k ]    similar to the statement of corollary [ cor : gn-2k ] , it can be stated that bcs with hybrid gaussian measurement succeeds with probability at least @xmath80 given that @xmath94 .",
    "the proof follows the proof of corollary [ cor : gn-2k ] .",
    "although we mainly follow the stochastic approach of @xcite in this paper , we could also employ the deterministic approach of @xcite to arrive at the uniqueness bound in theorem [ cor : gell-4k ] . in @xcite , an algorithm ( which is not necessarily practical )",
    "is proposed to uniquely recover @xmath6 and @xmath10 from @xmath12 .",
    "this algorithm starts by finding subsets of size @xmath63 of @xmath12 s columns that are linearly dependent by testing the rank of every subset .",
    "dismissing the degenerate possibilities are dismissed by adding extra assumptions in the deterministic sparse coding model .",
    "meanwhile , as pointed out in @xcite , such degenerate instances of @xmath10 would have a probability measure of zero in a random sparse coding model ] , these detected subsets would correspond to samples with the same support pattern in @xmath10 . under the assumptions in theorem",
    "[ cor : gell-4k ] , it is possible to test whether @xmath63 columns in @xmath12 are linearly dependent ( with probability one ) , as a consequence of lemma [ lem : rank - check ] in the following section .    until now , our goal was to show that @xmath6 ( and subsequently @xmath10 ) is unique given only cs measurements .",
    "as we mentioned before , uniqueness of @xmath6 is a _ sufficient _ condition for the success of bcs .",
    "consider the scenario where not all support patterns @xmath64 are realized in @xmath10 or for some there is not enough samples to guarantee recovery .",
    "for such scenarios , we present the following theorem .",
    "assume @xmath92 and let @xmath95 where @xmath96 and @xmath97 denotes the set of indices of columns of @xmath10 with support @xmath98 .",
    "then , under hybrid gaussian measurement , @xmath99 for all @xmath100 can be perfectly recovered with probability one .",
    "[ the : latest ]",
    "the following crucial lemma from @xcite handles the permutation ambiguity of sparse coding .",
    "assume @xmath53 for @xmath4 and let @xmath65\\choose k}$ ] . if there exists a mapping @xmath101 such that @xmath102 then there exist a permutation matrix @xmath69 and a diagonal matrix @xmath75 such that @xmath71 .",
    "[ lem : apd ]    the following lemma from random matrix theory , along with lemma [ lem : apd ] , are the main ingredients of our first main result ( proof is provided in the appendix ) .",
    "assume @xmath103 are rank-@xmath2 matrices and @xmath104 is a gaussian measurement operator with @xmath105 .",
    "if @xmath106 , then @xmath107 with probability one",
    ". [ lem : manifold ]    assume @xmath108 is an alternate factorization that satisfies @xmath109 and @xmath73 .",
    "we will prove @xmath71 for some diagonal @xmath75 and some permutation matrix @xmath69 using lemma [ lem : apd ] .",
    "consider a particular support pattern @xmath64 and let @xmath110 $ ] denote the set of indices of @xmath10 s columns that have the sparsity pattern @xmath98 . by definition , @xmath111 where @xmath112 .",
    "due to the pigeon - hole principle , there must be at least @xmath113 columns within @xmath114 that share some particular support pattern @xmath115 .",
    "in other words , if @xmath116 denotes the set of indices of @xmath117 s columns that have the support pattern @xmath118 , then @xmath119 . for simplicity , denote @xmath120 .",
    "clearly , @xmath121 ( because @xmath122 ) , and we have @xmath123    according to lemma [ lem : manifold ] , if @xmath124 or equivalently @xmath125 , then @xmath126 with probability one .",
    "meanwhile , since @xmath127 , @xmath126 necessitates that @xmath128 finally , since @xmath6 satisfies the spark condition , it is not difficult to see that @xmath129 is a bijective map . to explain more , assume there exists some @xmath130 such that @xmath131 combining with ( [ eq : map2 ] ) we arrive at @xmath132 which contradicts the spark condition for @xmath6 for @xmath130 .",
    "therefore , @xmath133 must be injective . now",
    ", since @xmath134 is a finite set and @xmath133 is an injective mapping from @xmath134 to itself , it must also be surjective and , thus , bijective .    in order to have at least @xmath63 columns in @xmath10 for each support @xmath64 in the random sparse coding model @xmath56",
    ", we must have more than just @xmath135 data samples . the following result from @xcite",
    "quantifies the number of required data samples to ensure at least @xmath63 columns per each @xmath64 with a tunable probability of success .    for a randomly generated @xmath10 with @xmath136 and @xmath137 $ ] , with probability at least @xmath80 , there are at least @xmath138 columns for each support pattern @xmath64 .",
    "[ lem : beta ]    proof is fairly trivial . according to lemma [ lem : beta ] ,",
    "we need @xmath139 samples to guarantee that with probability at least @xmath80 there are at least @xmath63 samples in @xmath10 for each support pattern @xmath64 . in theorem [ lem :",
    "gell-2k ] we established that @xmath79 guarantees the success of bcs under gaussian sampling .",
    "therefore , @xmath140 guarantees the desired uniqueness .    in order to prove the results for the hybrid measurement scheme",
    ", we present the following lemma which is proved in the appendix .",
    "assume @xmath83 is drawn from an i.i.d .",
    "zero - mean gaussian distribution ( with @xmath141 ) .",
    "let @xmath142 denote the columns of @xmath12 indexed by the set @xmath143 .",
    "if @xmath144 , then @xmath145 with probability one",
    ". [ lem : rank - check ]    assume @xmath108 is an alternate factorization that satisfies @xmath109 , @xmath146 and @xmath147 .",
    "also assume @xmath148 and @xmath149 .",
    "consider a particular support pattern @xmath115 and let @xmath150 $ ] denote the set of indices of @xmath117 s columns that have the same sparsity pattern @xmath118 .",
    "clearly , @xmath151 therefore , if @xmath152 , then @xmath153 and according to lemma [ lem : rank - check ] : @xmath154 with probability one .",
    "hence , @xmath155 . again using lemma [ lem : rank - check ] with @xmath152 , @xmath156 with probability one . therefore",
    ", all the columns in @xmath157 must have the same support , namely @xmath98 .",
    "note that since @xmath158 , @xmath159 .",
    "meanwhile , @xmath160 necessitates that @xmath161 for every @xmath115 .",
    "therefore , @xmath162 .",
    "now , given @xmath163 according to lemma [ lem : manifold ] , if @xmath164 , then @xmath126 with probability one . meanwhile , since @xmath165 , @xmath126 necessitates that @xmath166 finally , since @xmath6 satisfies the spark condition , @xmath129 is a bijective map and @xmath71 for some diagonal @xmath75 and permutation matrix @xmath69 according to lemma [ lem : apd ] .",
    "recall that for every @xmath100 we have @xmath167 .",
    "assume @xmath148 and @xmath149 as before .",
    "having @xmath168 allows testing whether a subset of @xmath58 columns of @xmath12 are linearly dependent ( have a rank of @xmath2 ) with probability one .",
    "therefore , by doing an exhaustive search among every sub - matrix @xmath169 with @xmath170\\choose k+1}$ ] , we are able to find subsets of @xmath97 ( of size @xmath58 ) if @xmath171 .",
    "moreover , we can combine and complete these subsets to uniquely identify every rank-@xmath2 sub - matrix @xmath99 with @xmath171 .",
    "now , among these sub - matrices , those with @xmath172 can be recovered perfectly ( with probability one ) since , for any rank-@xmath2 matrices @xmath99 and @xmath173 , @xmath174 with @xmath175 or @xmath176 implies @xmath177 according to lemma [ lem : manifold ] .",
    "recall that in the dictionary learning ( dl ) problem , the data matrix @xmath178 is given where @xmath179 and the task is to factorize @xmath180 such that @xmath181 for some permutation matrix @xmath69 and diagonal matrix @xmath75 .",
    "unfortunately , the corresponding optimization problem is non - convex ( even with @xmath182 relaxation ) .",
    "the majority of existing dl algorithms are based on the iterative scheme of starting from an initial state @xmath183 and alternating between updating @xmath184 while keeping @xmath185 fixed and updating @xmath186 while keeping @xmath184 fixed , each corresponding to a convex problem .",
    "it has been recently shown that if the initial dictionary @xmath187 is sufficiently close @xcite . ] to @xmath188 for some @xmath69 and @xmath75 , then the iterative algorithm converges to @xmath188 under certain incoherency assumptions about @xmath189 @xcite .",
    "similar guarantees have been derived for the well - known k - svd algorithm @xcite .",
    "furthermore , dl from incomplete or corrupt data has also been tackled in several studies . in particular , dl from compressive measurements has been addressed in @xcite where different iterative dl algorithms are modified to accommodate the compressive measurements . in some cases ,",
    "these modifications have been justified by showing that the output of each iteration does not significantly deviate from the reference output based on the complete data . however , to best of our knowledge , there are no convergence guarantees to @xmath188 for these iterative algorithms . as we mentioned before , a successful dl from compressive measurements is a sufficient condition for a successful bcs . in this section ,",
    "we plan to investigate the utility of a recently proposed ( non - iterative ) dl algorithm @xcite with guarantees for the approximate recovery of @xmath188 for an incoherent @xmath189 .",
    "one would hope that @xmath188 can be approximated from @xmath12 with fewer data samples than is required for the exact identification of @xmath188 which was the topic of previous sections .",
    "below , we review the main result of @xcite and analyze the performance of their dl algorithm if only hybrid gaussian measurements were available .",
    "recall that in our bcs measurement scheme , @xmath190 fixed and @xmath191 varying linear measurements are taken from each sample for a total of @xmath192 linear measurements ( per sample ) . before presenting their result , we need to introduce some new notation as well as modifications to the sparse coding model to reflect the model used in @xcite .",
    "in particular , let @xmath193 denote the random vector of sparse coefficients where its distribution class @xmath194 is defined below .",
    "hence , each @xmath195 denotes an outcome of @xmath196 .",
    "also , let @xmath197 denote the random variable associated with the @xmath24th entry of @xmath196 .",
    "( distribution class @xmath194 ) the distribution is in class @xmath194 if @xmath24 ) @xmath198\\cup [ 1,c]$ ] and @xmath199=0 $ ] .",
    "@xmath200 ) conditioned on any subset of coordinates in @xmath196 being non - zero , the values of @xmath197 are independent of each other .",
    "distribution has _ bounded @xmath63-wise moments _ if the probability that @xmath196 is non - zero in any subset @xmath98 of @xmath63 coordinates is at most @xmath201 times @xmath202 $ ] where @xmath203 .",
    "similar to @xcite , in the rest of paper we will assume @xmath204 .",
    "derived results generalize to the case @xmath205 by loosing constant factors in guarantees .",
    "two dictionaries @xmath206 are column - wise @xmath207-close , if there exists a permutation @xmath133 and @xmath208 such that @xmath209\\colon \\|a_i-\\theta_i b_{\\pi(i)}\\|_2\\leq \\epsilon$ ] .",
    "when talking about two dictionaries @xmath6 and @xmath28 that are @xmath207-close , we always assume the columns are ordered and scaled correctly so that @xmath210 .",
    "there is a polynomial time algorithm to learn a @xmath31-coherent dictionary @xmath6 from random samples . with high probability ,",
    "the algorithm returns a dictionary @xmath211 that is column - wise @xmath207-close to @xmath6 given random samples of the form @xmath212 , where @xmath196 is drawn from a distribution in class @xmath194 .",
    "specifically , if @xmath213 and the distribution has bounded @xmath63-wise moments , @xmath214 is a constant only depending on @xmath63 , then the algorithm requires @xmath215 samples and runs in time @xmath216 . [",
    "the : arora ]    this algorithm , which has fundamental similarities with a concurrent work @xcite , consists of two main stages : @xmath24 ) _ data clustering _ :",
    "the connection graph is built where each node corresponds to a column of @xmath12 and an edge between @xmath217 and @xmath44 implies their supports @xmath218 and @xmath219 have a non - empty intersection .",
    "then , an overlapping clustering procedure is performed over the connection graph to find overlapping maximal cliques ( with missing edges ) .",
    "@xmath200 ) _ dictionary recovery _ : every cluster in the connection graph represents the set of samples associated with a single dictionary atom . after finding these clusters in the connection graph ,",
    "each atom is approximated by the principal eigenvector of the covariance matrix for the data samples in its corresponding cluster .",
    "there are two challenges in extending the above result to the bcs framework : @xmath24 ) during generation of the connection graph from data and @xmath200 ) during computation of the principal eigenvector of the data covariance matrix .",
    "we address these challenges separately in the following subsections .      for building the connection graph , we use the fixed part of the hybrid measurements , i.e. @xmath220 with @xmath83 drawn from a gaussian distribution .",
    "computation of the connection graph in @xcite relies on the following lemma .",
    "suppose @xmath221 for large enough @xmath222 ( depending on @xmath223 in the definition of @xmath194 ) .",
    "then , if @xmath218 and @xmath219 are disjoint , with high probability @xmath224 .    without going into the details of the clustering algorithm of @xcite , we study the conditions under which the connection graph does not change when only @xmath190 linear measurements from each data sample is given .",
    "let @xmath225 be @xmath226-coherent .",
    "it is not hard to see from the above lemma that if @xmath227 , then with high probability for disjoint @xmath218 and @xmath219 , @xmath228 .",
    "to establish a relationship between @xmath226 , @xmath31 and @xmath190 , we use the following result from @xcite .",
    "let @xmath229 with @xmath230 .",
    "assume @xmath231 is a random matrix with independent @xmath232 entries .",
    "then , for all @xmath233 @xmath234\\leq 2\\exp ( -n\\frac{t^2}{c_1+c_2t})\\ ] ] with @xmath235 and @xmath236 .",
    "[ lem : holger ]    assume @xmath83 has i.i.d entries from @xmath237 .",
    "let @xmath6 be @xmath31-coherent and @xmath238 be @xmath226-coherent .",
    "then , @xmath239\\leq 2\\exp ( -p_f\\frac{t^2}{c_1+c_2t})\\ ] ] with @xmath240 and @xmath241 specified in lemma [ lem : holger ] .",
    "[ cor : mu ]    note that the variance of @xmath86 s entries does not have an effect on @xmath226 due to the normalization in the definition of the coherency and we could assume @xmath86 s entries have variance @xmath49 as before .",
    "we exploit lemma [ lem : holger ] by replacing @xmath242 and @xmath243 and @xmath244 .",
    "proof is complete by noticing that @xmath245\\leq\\mathbb{p}[|\\mu_f-\\mu|\\geq t]$ ]    based on corollary [ cor : mu ] , it can be deduced that with high probability @xmath246 .",
    "therefore , replacing @xmath247 in the original theorem [ the : arora ] with @xmath248 introduces slightly stronger sparsity requirement for the success of the algorithm .      at this stage , we only exploit the varying part of the measurements @xmath249 and use @xmath250 in place of @xmath191 for simplicity .",
    "let @xmath251 be the @xmath61 discovered overlapping clusters from the previous stage and define the empirical covariance matrix @xmath252 for the cluster @xmath24 .",
    "the svd approach ) selective averaging and @xmath200 ) the svd - based approach .",
    "we selected to work with the svd approach due to its more abstract and versatile nature .",
    "] of @xcite estimates @xmath253 by @xmath254 which is the principal eigenvector of @xmath255 .",
    "let @xmath256 denote the empirical covariance matrix resulting from the compressive measurements where @xmath257 as before .",
    "similarly , let @xmath258 denote the principal eigenvector of @xmath259 .",
    "our goal in this section is to show that @xmath260 is bounded by a small constant for finite @xmath11 and approaches zero for large @xmath11 . for this purpose",
    ", we use the recent results from the area of _ subspace learning _ , specifically , subspace learning from compressive measurements @xcite . a critical factor in estimation accuracy of the principle eigenvector of a _ perturbed _ covariance matrix is the _ eigengap _ between the principal and the second eigenvalues of the original covariance matrix .",
    "this is a well - known result from the works of chandler davis and william kahan known as the davis - kahan sine theorem @xcite .",
    "consider the following notation .",
    "let @xmath261 and @xmath262 denote projection operators onto the principal @xmath2-dimensional subspaces of @xmath263 and @xmath264 respectively ( i.e. the projection onto the top-@xmath2 eigenvectors ) .",
    "let @xmath265 denote the spectral norm of the difference between @xmath261 and @xmath262 .",
    "define the eigengap @xmath266 as the distance between the @xmath2th and @xmath58st largest eigenvalues of @xmath263 .",
    "suppose @xmath263 is computed from at least @xmath63 data samples ( @xmath267 for all @xmath24 ) .",
    "moreover , assume the data samples have bounded @xmath268 norms , i.e. @xmath269\\colon\\|y_j\\|_2 ^ 2\\leq \\eta$ ] for some positive @xmath270 .    with probability",
    "at least @xmath271 @xmath272 so that one can achieve @xmath273 provided that @xmath274 [ lem : akshay ]    below , we present a customization of lemma [ lem : akshay ] for the @xmath268 error of the principal eigenvector estimator .",
    "let @xmath254 and @xmath258 represent the principal eigenvectors of @xmath255 and @xmath259 respectively .",
    "with probability at least @xmath271 for all @xmath275 $ ] @xmath276 [ lem : temp ]    clearly , @xmath277 and @xmath278 . as we mentioned in the definition of @xmath207-closeness",
    ", @xmath279 is implicit in the error expression @xmath280 requiring that @xmath281 and consequently @xmath282 .",
    "also note that , by definition , for any @xmath283 @xmath284 now let @xmath285 . then @xmath286 therefore @xmath287 and the rest follows from lemma [ lem : akshay ] .    to obtain",
    "a lower - bound for the eigengap @xmath288 we need to review some of the intermediate results in @xcite .",
    "in fact , we compute a lower - bound for @xmath289 of @xmath290 which serves as a close approximation of @xmath288 when the number of data samples @xmath63 is large . for every @xmath275 $ ] , let @xmath291 be the distribution conditioned on @xmath292 .",
    "let @xmath293 for any unit - norm @xmath294 and let @xmath295 =   1+\\sum_{j\\neq i } \\langle a_i , a_j \\rangle ^2 \\mathbb{e}_{\\gamma_i}[\\mathcal{x}_j^2]\\ ] ] denote the _ projected variance _ of @xmath291 on the direction @xmath296 .",
    "it is shown @xcite that generally @xmath297 \\leq   \\alpha^2 r_i^2 + 2\\alpha\\sqrt{1-\\alpha^2}\\zeta + ( 1-\\alpha^2)\\zeta^2\\ ] ] where @xmath298 .",
    "the principal eigenvector of @xmath299 can be computed by finding the unit - norm @xmath294 that maximizes @xmath300 $ ] .",
    "meanwhile , it has been established that for @xmath296 , @xmath300 = r_i^2 $ ] .",
    "therefore , the range of @xmath301 for the principal eigenvector must satisfy the inequality ( for @xmath302 ) @xmath303 it is not difficult to show this range is @xmath304\\ ] ] now , for the second eigenvector and eigenvalue pair we must find a unit - norm @xmath305 that satisfies @xmath306 and maximizes @xmath307 $ ] .",
    "define @xmath308 .",
    "it can be shown that @xmath309\\ ] ] note that the first and the second largest eigenvalues correspond to projected variances of @xmath291 on the directions of @xmath294 and @xmath305 , respectively .",
    "therefore , based on the derived ranges for @xmath301 and @xmath310 , we are able to find the following lower - bound for @xmath289 : @xmath311 note that @xmath312 becomes very small as the problem size ( @xmath313 , @xmath61 , @xmath11 ) becomes large , resulting in @xmath314 . therefore , given a sufficient number of samples , it can be guaranteed that @xmath258 is an accurate estimation of @xmath254 and , in turn , an accurate estimation of @xmath253 even when only @xmath14 measurements per sample is available . once the dictionary has been approximated to within a close distance from the optimal dictionary @xmath315 , iterative algorithms such as @xcite can assure convergence to a local optimum and therefore perfect recovery as suggested in @xcite .",
    "finally , perfect recovery of the dictionary results in perfect recovery of @xmath10 and @xmath12 given the cs bounds for the number of measurements @xcite which are generally weaker than the stated bounds for the recovery of the dictionary .",
    "in this work , we studied the conditions for perfect recovery of both the dictionary and the sparse coefficients from linear measurements of the data .",
    "the first part of this work brings together some of the recent theories about the uniqueness of dictionary learning and the blind compressed sensing problem .",
    "moreover , we described a ` hybrid ' random measurement scheme that reduces the theoretical bounds for the minimum number of data samples to guarantee a unique dictionary and thus perfect recovery for blind compressed sensing . in the second part , we discussed the algorithmic aspects of dictionary learning under random linear measurements .",
    "it was shown that a polynomial - time algorithm can assure convergence to the generative dictionary given a sufficient number of data samples with high probability .",
    "it would be interesting to explore dictionary learning and blind compressed sensing for non - gaussian random measurements .",
    "in particular , when the data matrix is partially observed ( i.e. an incomplete matrix ) , data recovery becomes a matrix completion problem where the elements of the data matrix are assumed to lie in a union of interconnected rank-@xmath2 subspaces .",
    "this is a subject of future work .",
    "let @xmath316 . note that @xmath317 .",
    "thus , @xmath318 . our goal is to show @xmath319 and thus @xmath320 . to prove @xmath319 , we must show that for every @xmath321 $ ] , @xmath322 results in @xmath323 with probability one . for simplicity , we omit the sample index @xmath25 in the rest of the proof .",
    "let @xmath98 and @xmath324 respectively denote the sets of non - zero indices of @xmath3 and @xmath325 where @xmath326 . rewrite @xmath327 as @xmath328 .",
    "note that @xmath329 is supported on @xmath330 where @xmath331 .",
    "therefore , we must show that , with probability one , @xmath332\\choose 2k}\\colon rank(\\phi a_t)=|t|\\ ] ] necessitating @xmath333 or @xmath334 . since @xmath53 ,",
    "every @xmath335 columns of @xmath6 are linearly independent and we are able to perform a gram - schmidt orthogonalization on @xmath336 to get @xmath337 where @xmath338 is orthonormal ( @xmath339 ) and @xmath340 is a full - rank square matrix .",
    "hence , @xmath341 is distributed according to i.i.d .",
    "gaussian and is full - rank with probability one @xcite .",
    "we conclude the proof by noticing that @xmath342 since @xmath340 is a full - rank square matrix",
    ".          then @xmath348 .",
    "specifically , under the gaussian measurement scheme for bcs , we have : @xmath349\\in\\mathbb{r}^{p n\\times d n}\\ ] ] where non - zero entries of @xmath350 are i.i.d .",
    "gaussian with mean zero and variance @xmath49 .",
    "the following result from @xcite gives the required number of linear measurements to guarantee ( with probability one ) that a rank-@xmath351 matrix does not fall into the null - space of the measurement operator .",
    "let @xmath352 be a @xmath353-dimensional continuously differentiable manifold over the set of @xmath354 real matrices .",
    "suppose we take @xmath355 linear measurements from @xmath356 .",
    "assume there exists a constant @xmath357 such that @xmath358 for every @xmath12 with @xmath359 .",
    "further assume that for each @xmath360 that the random variables @xmath361 are independent .",
    "then with probability one , @xmath362 .",
    "[ lem : r11 ]    a careful inspection of the derivation of the above theorem in @xcite reveals that this result can be extended to include the manifolds over the set of rectangular matrices @xmath178 .",
    "specifically , for the manifold over rank-@xmath351 @xmath363 matrices we have ( see @xcite for example ) @xmath364 .",
    "let @xmath352 denote the manifold over the set of rank-@xmath351 @xmath363 matrices and let @xmath367 denote the manifold over the set of rank-@xmath368 @xmath363 matrices . also let @xmath343 with @xmath369 . then , for any @xmath370 , @xmath366 implies @xmath107 with probability one .",
    "clearly , @xmath371 implies @xmath372 for any @xmath373 over the set of rank-@xmath374 @xmath363 matrices with @xmath375 . also note that @xmath376 , thus @xmath377 .",
    "now , since @xmath378 and @xmath379 ( with probability one , according to lemma [ lem : r11 ] ) , we must have @xmath380 or @xmath107 with probability one .",
    "it only remains to show that @xmath104 satisfies the requirements of lemma [ lem : r11 ] .",
    "as noted in @xcite , the requirement @xmath381 requires that the densities of @xmath382 do not spike at the origin ; a sufficient condition for this to hold for every @xmath12 with @xmath359 is that each @xmath383 has i.i.d .",
    "entries with a continuous density .",
    "note that non - zero entries of @xmath384 are i.i.d .",
    "gaussian and cover every column in @xmath12 . therefore",
    ", none of the entries of @xmath385 would spike at the origin or equivalently there exists @xmath386 so that @xmath387 with @xmath388 given that the vector @xmath389 is drawn from a continuous distribution .",
    "let @xmath390 and @xmath391 .",
    "perform a gram - schmidt orthogonalization on @xmath169 to obtain @xmath392 where @xmath393 has orthogonal columns and @xmath394 is full - rank ; hence , given @xmath395 , we have @xmath396 .",
    "note that , since @xmath397 is orthogonal and @xmath86 is i.i.d .",
    "gaussian , @xmath398 is also i.i.d .",
    "gaussian . hence , with probability one , @xmath398 is full - rank @xcite and @xmath399 . to conclude the proof ,",
    "note that when @xmath400 , necessarily we have @xmath401 .",
    "e. candes , j. romberg and t. tao , `` robust uncertainty principles : exact signal reconstruction from highly incomplete frequency information , '' _ ieee transactions on information theory _ , vol .",
    "2 , pp . 489-509 , 2006 ."
  ],
  "abstract_text": [
    "<S> blind compressed sensing ( bcs ) is an extension of compressed sensing ( cs ) where the optimal sparsifying dictionary is assumed to be unknown and subject to estimation ( in addition to the cs sparse coefficients ) . since the emergence of bcs , dictionary learning , a.k.a . </S>",
    "<S> sparse coding , has been studied as a matrix factorization problem where its sample complexity , uniqueness and identifiability have been addressed thoroughly . however , in spite of the strong connections between bcs and sparse coding , recent results from the sparse coding problem area have not been exploited within the context of bcs . in particular , </S>",
    "<S> prior bcs efforts have focused on learning constrained and complete dictionaries that limit the scope and utility of these efforts . in this paper </S>",
    "<S> , we develop new theoretical bounds for perfect recovery for the general _ unconstrained _ bcs problem . </S>",
    "<S> these unconstrained bcs bounds cover the case of overcomplete dictionaries , and hence , they go well beyond the existing bcs theory . </S>",
    "<S> our perfect recovery results integrate the combinatorial theories of sparse coding with some of the recent results from low - rank matrix recovery . </S>",
    "<S> in particular , we propose an efficient cs measurement scheme that results in practical recovery bounds for bcs . </S>",
    "<S> moreover , we discuss the performance of bcs under polynomial - time sparse coding algorithms . </S>"
  ]
}