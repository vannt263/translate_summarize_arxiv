{
  "article_text": [
    "the theory of compressed sensing ( cs ) shows that many signals of interest can be reconstructed from a few linear , and typically random , observations  @xcite .",
    "interestingly , this reconstruction is made possible if the number of observations ( or _ measurements _ ) is adjusted to the intrinsic complexity of the signal , _",
    "e.g. _ , its sparsity for vectors or its low - rankness for matrices .",
    "thus , this principle is a generalization of the shannon - nyquist sampling theorem , where the sampling rate is set by the bandwidth of the signal .",
    "however , a significant aspect of cs systems is the effect of _ quantization _ on the acquired observations , in particular for the purpose of compression and transmission  @xcite .",
    "this quantization is a non - linear transformation that both distorts the cs observations and increases , especially at low bit rates , the reconstruction error of cs reconstruction procedures .",
    "this work focuses on minimizing the impact of ( scalar ) quantization during the reconstruction of a signal from its quantized compressive observations .",
    "while more efficient quantization procedures exist in the literature ( _ e.g. _ , @xmath2  @xcite , universal  @xcite , binned  @xcite , vector  @xcite or analysis - by - synthesis quantizations  @xcite ) , scalar quantization remains appealing for its implementation simplicity in most electronic devices , and for its robustness against measurement lost .",
    "conversely to other attempts , which consider quantization distortion as additive gaussian measurement noise @xcite and promote a euclidean ( @xmath3 ) fidelity with the signal observations as in the basis pursuit denoise ( bpdn ) program , better signal reconstruction methods are reached by forcing _ consistency _ between the re - observed signal estimate and the quantized observations  @xcite .",
    "we show here that a consistent version of the basis pursuit program @xcite , coined cobp , provides better signal estimates at large @xmath0 than those obtained by bpdn . when reconstructing sparse or compressible signals , cobp is similar , up to an additional normalization constraint , to former methods proposed in @xcite .",
    "we prove the efficiency of cobp from recent results on the proximity of signals when those are taken in a set @xmath4 of small `` dimension '' , _",
    "i.e. _ , with small gaussian width @xmath5  @xcite , and when their quantized random projections are consistent @xcite .",
    "in particular , we show that for sub - gaussian sensing matrices , the @xmath3-reconstruction error of cobp decays as @xmath6 , with an additional constant error bias arising in the case of non - gaussian sensing matrices .",
    "this contrasts with bpdn , whose reconstruction error is only guaranteed to saturate when @xmath0 increases .",
    "the rest of this paper is structured as follows .",
    "[ sec : problem - statement ] introduces the problem by explaining the low - complexity signal space , our quantized compressed sensing ( qcs ) model and the bpdn reconstruction procedure as generally used in qcs .",
    "[ sec : prox - conc - vect ] reviews important results on the proximity of consistent vectors ; in sec .",
    "[ sec : cons - basis - purs ] we introduce and analyze cobp .",
    "finally , sec .",
    "[ sec : experiments ] demonstrates experimentally the capabilities of this method in qcs of signals and matrices , before concluding .",
    "_ conventions : _ vectors and matrices are associated to bold symbols .",
    "the probability of an event @xmath7 is @xmath8 .",
    "the identity matrix is @xmath9 ( @xmath10 ) , @xmath11\\!]}}:=\\{1,\\,\\cdots , d\\}$ ] and @xmath12 is the cardinality of @xmath13\\!]}}$ ] .",
    "the @xmath14-norm of @xmath15 is @xmath16 and the unit @xmath14-ball is @xmath17 , with @xmath18 . assuming @xmath19 is a square number , for a matrix @xmath20 with _",
    "@xmath21 , @xmath22 , @xmath23 , @xmath24 and @xmath25 denote its rank , operator norm , nuclear norm and its frobenius norm , respectively",
    ". we will often assimilate matrices in @xmath26 with their vectorization in @xmath27 , _",
    "e.g. _ , identifying @xmath28 with @xmath29 .",
    "finally , we write @xmath30 or @xmath31 if @xmath32 for  @xmath33 , and similarly for @xmath34 and @xmath35 .",
    "this work focuses on the sensing of signals belonging to a low - complexity set @xmath4 .",
    "a typical example is the set of @xmath36-sparse vectors @xmath37 , as well as the set of rank-@xmath38 matrices @xmath39 .    as in @xcite , we assume that the ( bounded ) _ convex hull _ @xmath40 of @xmath41 is associated to the definition of an appropriate _ atomic _ norm is convex and centrally symmetric around the origin , @xmath42 can always be defined by the _ gauge _ of @xmath41 ( see @xcite for details ) . ]",
    "@xmath42 such that @xmath43 for some @xmath44 .",
    "for instance , for compressible signals in @xmath45 , @xmath46 and @xmath47 , while for matrices in @xmath48 , @xmath49 for  @xmath50  @xcite .",
    "the `` low - complexity '' nature of these sets stems from their small _ gaussian mean width _",
    "@xmath51 for instance , @xmath52 and @xmath53  @xcite .",
    "the quantity @xmath5 , also called gaussian complexity , has been recognized as central , _ e.g. _ , for random processes characterization @xcite , high - dimensional statistics and inverse problem solving @xcite or classification in randomly projected domains  @xcite .",
    "as explained below , @xmath5 also determines the minimal number of measurements for cs of signals in @xmath41  @xcite .",
    "given a certain quantization resolution @xmath54 , we focus on the impact of a uniform ( midrise ) quantizer @xmath55 , applied componentwise , in the quantized sensing model @xmath56 where @xmath57 is a random _ sensing matrix _ and @xmath58)$ ] ( _ i.e. _ , @xmath59)$ ] for @xmath60\\!]}}$ ] ) is a uniform dithering)$ ] for any @xmath61 . ] .",
    "this random dithering is known at the signal reconstruction and stabilizes the action of @xmath62  @xcite . by slightly abusing the notation , when senses an element @xmath63 of a matrix set in @xmath64 , @xmath65 amounts to the @xmath66-length vectorization of this element , assuming @xmath67 .    as often the case in cs , we consider that @xmath68 is a sub - gaussian random matrix , _",
    "i.e. _ , its entries are distributed as @xmath69 with @xmath70 a symmetric , zero - mean and unit - variance sub - gaussian random variable ( ) , having finite sub - gaussian norm @xmath71 for such a of sub - gaussian norm @xmath72 , we have in fact @xmath73 \\lesssim \\exp(-c t^2/\\alpha^2)$ ] for any @xmath74 .",
    "examples of such s are gaussian , uniform , bounded or bernoulli distributed s .",
    "below , we write @xmath75 , and the shorthand @xmath76 for the associated @xmath77 matrix , to specify that @xmath70 is a sub - gaussian of norm @xmath78 .    in the absence of quantization , if @xmath79 , with high probability , any @xmath80 can be reconstructed from sub - gaussian observations @xmath81 using convex optimization programs such as basis pursuit  @xcite .",
    "therefore , the minimal number of measurements needed for reconstructing @xmath36-sparse or compressible signals in @xmath82 grows like @xmath83 , and like @xmath84 for rank-@xmath38 and compressible @xmath85 matrices  @xcite .",
    "[ [ bit - quantization - regime ] ] 1-bit quantization regime : + + + + + + + + + + + + + + + + + + + + + + + + + +    the exponentially decaying tail bounds of the sub - gaussian entries of @xmath68 show that a suitable value of @xmath86 can essentially turn into a 1-bit cs model when @xmath41 is bounded @xcite . indeed , from the definition of @xmath62 and assuming @xmath87 , for @xmath88\\!]}}$ ] , @xmath89 = { \\mathbb}p[|{\\boldsymbol}\\varphi_i^t { \\boldsymbol}x + \\xi_i| { \\geqslant}\\delta ] = p_0 { \\leqslant}2\\exp(-{{\\textstyle\\frac{1}{2}}}\\delta^2)$ ] , with @xmath90 for @xmath91 .",
    "our study holds in such a regime with the interesting advantage of allowing the estimation of the signal norm , as opposed to the 1-bit cs model @xmath92  @xcite .",
    "this is due to the pre - quantization dithering in .",
    "interestingly , combining the sign operator with prequantization thresholds in 1-bit cs also removes this signal norm uncertainty  @xcite .",
    "the first method used to estimate @xmath93 from @xmath94 in was considering quantization as an additive noise of bounded power under high resolution assumption ( hra ) , _ i.e. _ , @xmath95  @xcite .",
    "in , the impact of the dithering provides @xmath96 with @xmath97)$ ] .",
    "therefore , @xmath98 holds with high probability for small @xmath99 ( _ e.g. _ , @xmath100 )  @xcite .",
    "in such a case , the general bpdn program , @xmath101 can be solved for estimating @xmath93 .",
    "when @xmath102 satisfies the restricted isometry property ( rip ) and when @xmath41 is the set of sparse signals , then , setting @xmath103 ,  @xcite show that @xmath104 a similar result holds in the case of qcs of low - rank matrices using a lasso reconstruction that minimizes a lagrangian formulation of bpdn  @xcite .",
    "notice that a variant of bpdn , called basis pursuit dequantizer of moment @xmath105 ( bpdq @xcite ) , replaces the @xmath3-norm of the bpdn constraint by an @xmath14-norm ( @xmath106 ) .",
    "its error decays like @xmath107  @xcite .",
    "this section summarizes a recent study showing that the proximity of vectors of a subset @xmath4 with small gaussian mean width can be bounded provided they share the same image through the random mapping @xmath108 , _",
    "i.e. _ , if they are consistent  @xcite . as will be clear in sec .",
    "[ sec : cons - basis - purs ] , this property is the key for characterizing the behavior of cobp .",
    "this proximity is impacted by the level of _ anisotropy _ of the sub - gaussian rows composing @xmath109",
    "@xcite , as measured by the smallest @xmath110 such that , for @xmath111 , @xmath112 and all @xmath113 , @xmath114 for gaussian ( isotropic ) random vectors @xmath115 , while for sub - gaussian @xmath111 , @xmath116 , with @xmath117 for bernoulli s  @xcite .    as clarified in prop .",
    "[ prop : consistency - width ] , when the mapping @xmath108 integrates a non - gaussian , but sub - gaussian sensing matrix  @xmath68 , the proximity of consistent elements @xmath118 in @xmath41 is guaranteed when @xmath119 is not `` too sparse '' , _",
    "i.e. _ , when it belongs to @xmath120 for @xmath121 large enough compared to @xmath122 .",
    "for instance , a @xmath36-sparse vector @xmath123 can not belong to @xmath124 for @xmath125 as then @xmath126 .",
    "[ prop : consistency - width ] given a quantization resolution @xmath127 , @xmath128 , a sub - gaussian distribution @xmath129 respecting for @xmath130 , and @xmath131 a bounded subset of @xmath82 , there exist some values @xmath132 depending only on @xmath78 and such that , if @xmath133 then , for @xmath76 , @xmath134)$ ] and @xmath135 , with probability exceeding @xmath136 , we have for all @xmath137 @xmath138 with @xmath108 defined in . moreover , for any orthonormal basis @xmath139 , if @xmath140 then simplifies to @xmath141 for some @xmath142 depending only on @xmath78 .",
    "we remark that for gaussian sensing matrices , the `` antisparse '' condition on @xmath143 ( and on @xmath121 ) vanishes since @xmath115 .",
    "this provides , in the special case of the sparse signal set , a proximity bound in formerly established in @xcite .",
    "the previous sections allow us now to define a suitable reconstruction procedure for estimating any signal @xmath144 ( for some @xmath44 in  ) observed through the model  , _",
    "e.g. _ , for reconstructing compressible signals or matrices belonging to @xmath45 or @xmath145 , respectively .",
    "we split the study according to the nature of the sensing matrix .      when @xmath68 is gaussian , _",
    "i.e. _ , @xmath115 , we propose to estimate @xmath93 with the following program coined consistent basis pursuit , @xmath146 this is a convex optimization as the first constraint is equivalent to @xmath147  @xcite . the proximity of @xmath148 to @xmath93",
    "is then guaranteed by prop .",
    "[ prop : consistency - width ] .",
    "[ prop : cons - basis - purs - stab - gauss ] if @xmath108 respects for all @xmath149 and @xmath150 , then for all @xmath151 , the estimate @xmath148 obtained by cobp from @xmath152 satisfies @xmath153 .",
    "since @xmath154 is a feasible vector of the cobp constraints , we necessarily have @xmath155 . by definition of cobp , @xmath156 so that @xmath157 .",
    "the result follows from   with @xmath158 and @xmath159 .",
    "[ prop : cons - basis - purs - stab - gauss ] assumes that @xmath160 in .",
    "this holds if @xmath115 , _",
    "e.g. _ , if @xmath161 .",
    "therefore , combining the conditions of prop .",
    "[ prop : consistency - width ] with this last proposition , we get the following corollary by saturating   with respect to @xmath0 .",
    "[ cor : decay - gauss - error ] given some universal constant @xmath33 , with probability exceeding @xmath162 over the draw of @xmath163 and @xmath164)$ ] , for every @xmath165 , the estimate @xmath148 obtained by cobp from @xmath152 satisfies @xmath166 _ i.e. _ , @xmath167 if only @xmath0 varies .    at first sight ,",
    "the error decay of cobp in @xmath1 could seem slow .",
    "however , as mentioned in sec .",
    "[ sec : basis - purs - deno ] , the best known error decay for bpdn under the sensing model   is  @xmath168  @xcite , which does not decay with @xmath0 .",
    "the same constant bound was found for a variant of cobp without the ball constraint  @xcite .",
    "+ [ qcs - gauss - sparse ]      for non - gaussian @xmath68 , @xmath169 in general . in order to reach a meaningful estimate of @xmath165",
    ", we further assume that @xmath170 , for some @xmath171 .",
    "as will be clear , this allows us to characterize the sparse nature of @xmath172 when @xmath148 is an estimate of @xmath93 produced by the modified program : @xmath173 with @xmath174 as soon as @xmath175 since @xmath176 .",
    "[ prop : cons - basis - purs - stab ] if @xmath108 respects for all @xmath149 and any @xmath177 , then for any @xmath178 , the solution obtained by cobp@xmath179 from @xmath180 respects @xmath181    as for the proof of prop .",
    "[ prop : cons - basis - purs - stab - gauss ] , @xmath182 implies that @xmath183 . if @xmath184 , then , since @xmath185 , @xmath186 .",
    "otherwise , we have @xmath187 . in this case , since is assumed satisfied for all pairs of vectors of @xmath188 , we have @xmath189 , which concludes the proof .",
    "taking @xmath190 , this corollary is easily established .",
    "[ cor : decay - nongauss - error ] given some universal constant @xmath33 , with probability exceeding @xmath191 over the draw of @xmath192 and @xmath164)$ ] , the cobp@xmath179 estimate @xmath148 of any @xmath193 satisfies @xmath194 _ i.e. _ , @xmath195 if only @xmath0 varies .    loosely speaking , cor .",
    "[ cor : decay - nongauss - error ] shows that the reconstruction error is not guaranteed to decay below a certain level fixed by @xmath196 .",
    "a similar behavior was already observed in the case of 1-bit cs with non - gaussian measurements   @xcite .",
    "in this section we run several numerical simulations in order to assess the experimental benefit of cobp compared to bpdn in various qcs settings .",
    "as cobp is a convex optimization problem containing non - smooth convex functions , we solve it with the versatile parallel proximal algorithm ( ppxa ) @xcite , this one being efficiently implemented in the unlocbox toolbox  @xcite .",
    "we refer the reader to @xcite for an example application of ppxa in the solution of low - rank matrix recovery .    for our experiments ,",
    "three different sensing contexts are tested : the two first ones consider qcs of sparse signals ( for gaussian or bernoulli sensing matrices ) , while the last one focuses on qcs of rank-1 matrices . in all cases ,",
    "the quantization resolution is fixed by @xmath197 with @xmath198\\!]}}$ ] . as explained in sec .",
    "[ sec : quant - compr - sens ] , each @xmath199 can then be essentially coded with @xmath200 bits , _",
    "e.g. _ , if @xmath201 , @xmath202 .",
    "some of our results are compared to those of bpdn with @xmath203 set as in sec .",
    "[ sec : basis - purs - deno ] .",
    "the constraint `` @xmath204 '' is also added to bpdn for reaching fair comparisons with cobp . ] .      in this experiment",
    ", we set @xmath205 , @xmath206 , @xmath207 and @xmath208 $ ] , _ i.e. _ , well after the _ phase transition _ ( here around @xmath209 ) where sparse signal reconstruction from noisy cs measurements is guaranteed  @xcite . for each value of @xmath0 ,",
    "20 different gaussian sensing matrices , dithering realizations and unit - norm @xmath36-sparse signals were randomly generated .",
    "each signal @xmath93 has its @xmath36-length support selected uniformly at random in @xmath210\\!]}}$ ] , with non - zero components drawn as @xmath211 before normalization .",
    "the reconstruction error decay averaged over these 20 trials is shown for bpdn , bpdq with @xmath212 ( see sec .",
    "[ sec : basis - purs - deno ] ) and cobp in fig .",
    "[ qcs - gauss - sparse](left ) in a @xmath213 plot . for indication ,",
    "a linear fitting over the last 4 values of @xmath214 provides slopes of value @xmath215 , @xmath216 and @xmath217 for bpdn , bpdq and cobp , respectively .",
    "as already observed experimentally in other works forcing tight or approximate consistency in signal reconstruction @xcite , this clearly highlights the advantage of consistent signal reconstruction when @xmath218 is large",
    ". moreover , cobp approaches an error decay of @xmath219 similar to the distance decay of consistent @xmath36-sparse vectors when is saturated , _",
    "i.e. _ , better than the `` @xmath1 '' of cor .",
    "[ cor : decay - gauss - error ]",
    ".      this second experiment stresses the impact of the sub - gaussian nature of the sensing matrix over the cobp reconstruction error .",
    "we focus on the case of gaussian qcs ( @xmath220 ) and bernoulli qcs ( _ i.e. _ , @xmath221 equals @xmath222 with probability @xmath223 ) when observing @xmath36-sparse signals for @xmath36 growing and @xmath218 constant .",
    "in particular , we set @xmath224 , @xmath225 , @xmath226 $ ] and @xmath227 . for each value of @xmath36 , 20",
    "different sensing matrices , dithering realizations and unit - norm @xmath36-sparse signals are generated as in the first experiment .",
    "cobp and cobp@xmath228 are compared ( with an oracle assisted @xmath229 ) . comparing the error bounds for gaussian and sub - gaussian qcs in cor .",
    "[ cor : decay - gauss - error ] and in cor .",
    "[ cor : decay - nongauss - error ] , respectively , we expect that at low @xmath36 and for @xmath218 constant , bernoulli qcs reaches worst reconstruction error than gaussian qcs , as then the bias @xmath230 can be high .",
    "this is indeed observed in fig .",
    "[ qcs - gauss - sparse](middle ) with a clear gap between bernoulli and gaussian qcs performances when @xmath231 .",
    "cobp@xmath228 does lead to clear improvements over cobp .",
    "we reconstruct here rank-1 matrices in @xmath232 ( _ i.e. _ , @xmath224 and @xmath233 ) from the gaussian qcs model with @xmath234 .",
    "both cobp and bpdn are solved with @xmath235 .",
    "the intrinsic complexity of such rank-1 matrices is @xmath236 .",
    "for each value of the oversampling ratio @xmath237 $ ] , we generate 20 different gaussian sensing matrices , dithering realizations and rank-1 matrices according to @xmath65 and @xmath238 with @xmath239 . as for the first experiment on @xmath36-sparse signals , cobp reaches a faster reconstruction error decay than bpdn . at @xmath240 , an indicative linear fitting over the last 4 values of @xmath218",
    "provides estimated decay exponents for cobp and bpdn of @xmath241 and @xmath216 , respectively .",
    "in the context of qcs of signals with low - complexity ( _ e.g. _ , sparse signals , low - rank matrices ) , we show that the consistent reconstruction method cobp has an estimation error decaying as @xmath1 , _",
    "i.e. _ , faster than the one of bpdn .",
    "this is confirmed numerically on several settings with even faster effective decaying rate at quantization resolution as low as one bit per measurement . as observed initially in 1-bit  cs  @xcite , qcs performances for general sub - gaussian sensing matrices are also impacted when the sensed signal is `` too sparse '' .",
    "finally , to the best of our knowledge , we provided the first theoretical analysis of cobp in the case of low - rank matrix reconstruction from qcs observations ."
  ],
  "abstract_text": [
    "<S> this paper focuses on the estimation of low - complexity signals when they are observed through @xmath0 uniformly quantized compressive observations . among such signals , </S>",
    "<S> we consider 1-d sparse vectors , low - rank matrices , or compressible signals that are well approximated by one of these two models . in this context , we prove the estimation efficiency of a variant of basis pursuit denoise , called consistent basis pursuit ( cobp ) , enforcing consistency between the observations and the re - observed estimate , while promoting its low - complexity nature . we show that the reconstruction error of cobp decays like @xmath1 when all parameters but @xmath0 are fixed . </S>",
    "<S> our proof is connected to recent bounds on the proximity of vectors or matrices when  _ ( i ) _  those belong to a set of small intrinsic `` dimension '' , as measured by the gaussian mean width , and _ </S>",
    "<S> ( ii ) _ they share the same quantized ( dithered ) random projections . by solving cobp with a proximal algorithm , we provide some extensive numerical observations that confirm the theoretical bound as @xmath0 </S>",
    "<S> is increased , displaying even faster error decay than predicted . the same phenomenon is observed in the special , yet important case of 1-bit cs . </S>",
    "<S> +   + _ keywords : _ quantized compressed sensing , quantization , consistency , error decay , low - rank , sparsity . </S>"
  ]
}