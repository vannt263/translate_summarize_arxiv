{
  "article_text": [
    "[ sec : intro ]    the theory of estimation in high - dimensional statistical models under the sparsity scenario has been considerably developed during the recent years .",
    "one of the main achievements was to derive _ sparsity oracle inequalities _",
    "( soi ) , i.e. , bounds on the risk of various sparse estimation procedures in terms of the @xmath0 norm ( number of non - zero components ) of the estimated vectors or their approximations ( see * ? ? ? * ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* and references therein ) .",
    "the main message of these results was to demonstrate that if the number of non - zero components of a high - dimensional target vector is small , then it can be reasonably well estimated even when the ambient dimension is larger than the sample size . however , there was relatively few discussion of the optimality of these bounds , mainly based on specific counter - examples or referring to the paper by @xcite , which treats the gaussian sequence model .",
    "the latter approach is , in general , insufficient as we will show below .",
    "an interesting point related to the optimality issue is that some of the bounds in the papers mentioned above involve not only the @xmath0 norm but also the @xmath1 norm of the target vector , which is yet another characteristic of sparsity .",
    "thus , a natural question is whether the @xmath1 norm plays an intrinsic role in the soi or it appears there due to the techniques employed in the proof .    in this paper ,",
    "considering the regression model with fixed design , we will show that the role of @xmath1 norm is indeed intrinsic .",
    "once we have a  rather general soi \" in terms the @xmath0 norm , a soi in terms of the @xmath1 norm follows as a consequence .",
    "this means that we can write the resulting bound with the rate which is equal to the minimum of the @xmath0 and @xmath1 rates ( see theorem  [ th : sparseub1 ] ) .",
    "unfortunately , the above mentioned  rather general soi \" is not available in the literature for the previously known sparse estimation procedures .",
    "we therefore suggest a new procedure called the _ exponential screening _ ( es ) , which satisfies the desired bound . it is based on exponentially weighted aggregation of least squares estimators with suitably chosen prior .",
    "the idea of using exponentially weighted aggregation for sparse estimation is due to @xcite .",
    "@xcite suggested several procedures of this kind based on continuous sparsity priors .",
    "our approach is different because we use a discrete prior in the spirit of earlier work by @xcite . unlike @xcite",
    ", we focus on high - dimensional models and treat explicitly the sparsity issue . because of the high dimensionality of the problem , we need efficient computational algorithms , and therefore we suggest a version of the metropolis - hastings algorithm to approximate our estimators ( subsection  [ sub : mh ] ) . regarding the sparsity issue , we prove that our method benefits simultaneously from three types of sparsity . the first one is expressed by the small rank of the design matrix @xmath2 , the second by the small number of non - zero components of the target vector , and the third by its small @xmath1 norm .",
    "finally , we mention that in a work parallel to ours , @xcite consider exponentially weighted aggregates with priors involving both discrete and continuous components and suggest another version of the metropolis - hastings algorithm to compute them .",
    "the contributions of this paper are the following :    * we propose the es estimator which benefits simultaneously from the above mentioned three types of sparsity .",
    "this follows from the oracle inequalities that we prove in section  [ sec : ub ] .",
    "we also provide an efficient and fast algorithm to approximately compute the es estimator and show that it outperforms several other competitive estimators in a simulation study .",
    "* we show that the es estimator attains the _ optimal rate of sparse estimation_. to this end , we establish a minimax lower bound which coincides with the upper bound on the risk of the es estimator on the intersection of the @xmath0 and @xmath1 balls ( theorem  [ th : low ] ) . * as a consequence , we find _ optimal rates of aggregation _ for the regression model with fixed design .",
    "we consider the five main types of aggregation , which are the linear , convex , model selection , subset selection and @xmath3-convex aggregation , cf .",
    "we show that the optimal rates are different from those for the regression model with random design established in @xcite .",
    "indeed , they turn out to be moderated by the rank of the regression matrix @xmath2 .",
    "the rates are faster for the smaller ranks .",
    "see section  [ sec : univ ] .",
    "this paper is organized as follows . after setting the problem and the notation in section  [ sec : notation ]",
    ", we introduce the es estimator in section  [ sec : ub ] and prove that it satisfies a soi with a remainder term obtained as the minimum of the @xmath0 and the @xmath1 rate .",
    "this result holds with no assumption on the design matrix @xmath4 , except for simple normalization .",
    "we put it into perspective in section  [ sec : lasso_bic ] where we compare it with weaker soi for the bic and the lasso estimators . in sections",
    "[ sec : disc ] and  [ sec : opt ] we discuss the optimality of soi . in particular , section  [ sec : disc ] comments why a minimax result in @xcite with normalization depending on the unknown parameter is not suitable to treat optimality . instead",
    ", we propose to consider minimax optimality on the intersection of @xmath0 and @xmath1 balls . in section",
    "[ sec : opt ] we prove the corresponding minimax lower bound for all estimators and show rate optimality of the es estimator in this sense .",
    "section  [ sec : univ ] discusses corollaries of our main results for the problem of aggregation ; we show that the es estimator solves simultaneously and optimally the five problems of aggregation mentioned in ( iii ) above .",
    "finally , section  [ sec : simul ] presents a simulation study demonstrating a good performance of the es estimator in numerical experiments .",
    "[ sec : notation ]    let @xmath5 be a collection of independent random couples such that @xmath6 , where @xmath7 is an arbitrary set . assume the regression model : @xmath8 where @xmath9 is the unknown regression function and the errors @xmath10 are independent gaussian @xmath11 .",
    "the covariates are deterministic elements @xmath12 of @xmath7 .",
    "consider the equivalence relation  @xmath13 on the space of functions @xmath14 such that @xmath15 if and only if @xmath16 for all @xmath17 .",
    "denote by @xmath18 the quotient space associated to this equivalence relation and define the norm @xmath19 by @xmath20 notice that @xmath19 is a norm on the quotient space but only a seminorm on the whole space of functions @xmath14 .",
    "hereafter , we refer to it as a norm .",
    "we also define the associated inner product @xmath21 let @xmath22 , be a dictionary of @xmath23 given functions @xmath24 .",
    "we approximate the regression function @xmath25 by a linear combination @xmath26 with weights @xmath27 , where possibly @xmath28 .",
    "we denote by @xmath2 , the @xmath29 design matrix with elements @xmath30 , @xmath31 .",
    "we also introduce the column vectors @xmath32 , @xmath33 and @xmath34 .",
    "let @xmath35 denote the @xmath36 norm in @xmath37 for @xmath38 and @xmath39 denote the @xmath0 norm of @xmath40 , i.e. , the number of non - zero elements of @xmath41 .",
    "for two real numbers @xmath42 and @xmath43 we use the notation @xmath44 , @xmath45 ; we denote by @xmath46 $ ] the integer part of @xmath42 and by @xmath47 the smallest integer greater than or equal to @xmath42 .",
    "a _ sparsity pattern _ is a binary vector @xmath48 .",
    "the terminology comes from the fact that the coordinates of any such vectors can be interpreted as indicators of presence ( @xmath49 ) or absence ( @xmath50 ) of a given feature indexed by @xmath51 .",
    "we denote by @xmath52 the number of ones in the sparsity pattern @xmath53 and by @xmath54 the space defined by @xmath55 where @xmath56 denotes the hadamard product between @xmath57 and @xmath53 and is defined as the vector @xmath58 .    for any @xmath59 ,",
    "let @xmath60 be any least squares estimator defined by @xmath61 the following simple lemma gives an oracle inequality for the least squares estimator .",
    "let @xmath62 denote the rank of the design matrix @xmath2 .",
    "[ lem : lse ] fix @xmath59 . then any least squares estimator @xmath60 defined in satisfies @xmath63 where @xmath64 is the dimension of the linear subspace @xmath65 and @xmath66 .",
    "moreover , the random variables @xmath67 need not be gaussian for   to hold .",
    "proof of the lemma is straightforward in view of the pythagorean theorem .",
    "let @xmath68 be a probability measure on @xmath69 , which we will further call a prior .",
    "the _ sparsity pattern aggregate _",
    "( spa ) estimator is defined as @xmath70 , where @xmath71 as shown in @xcite , the following oracle inequality holds : @xmath72 now , we consider a specific choice of the prior @xmath73 : @xmath74 where @xmath66 , we use the convention @xmath75 and @xmath76 is a normalization factor . in this paper",
    "we study the spa estimator with the prior defined in  .",
    "we call it the _ exponential screening _ ( es ) estimator , and denote by @xmath77 the estimator @xmath78 with the prior  .",
    "the es estimator is a mixture of least squares estimators corresponding essentially to sparsity patterns @xmath53 with small size and small residual sum of squares .",
    "note that the weight @xmath79 is assigned to the least squares estimator on the whole space ( case where @xmath80 ) and can be changed to any other constant in @xmath81 without modifying the rates presented below , as long as @xmath82 is modified accordingly .    since @xmath83",
    ", we obtain that @xmath84 .",
    "using this and considering separately the cases @xmath85 and @xmath86 , we obtain that the remainder term in   satisfies @xmath87\\\\ & \\le & \\frac{8\\sigma^2|{{\\sf p}}|}{n}\\log\\big(1+\\frac{em}{|{{\\sf p}}|\\vee 1}\\big ) + \\frac{8\\sigma^2}{n}\\log2\\,\\nonumber\\end{aligned}\\ ] ] for sparsity patterns @xmath53 such that @xmath88 . together with  , this inequality yields the following theorem .",
    "[ th : sparseub ] for any @xmath89 , the exponential screening estimator satisfies the following sparsity oracle inequality @xmath90 where @xmath91 denotes the rank of the design matrix @xmath2 .    proof .    combining the result of lemma  [ lem : lse ] and   with the sparsity prior defined in",
    ", we obtain that @xmath92 is bounded from above by @xmath93 and by @xmath94 combining   and   concludes the proof .",
    "an interesting corollary of theorem  [ th : sparseub ] is obtained for the linear regression model where it is assumed that @xmath95 for some @xmath96 . in this case",
    "yields @xmath97 however , even in this parametric case , theorem  [ th : sparseub ] provides a stronger result .",
    "indeed , if there exists @xmath98 , such that @xmath99 then theorem  [ th : sparseub ] gives a tighter bound on @xmath100 . a vector @xmath98 that satisfies   exists when @xmath101 can be well approximated by @xmath102 and @xmath103 is much sparser than @xmath104 . while the sparsity oracle inequality   indicates that the es estimator adapts to the underlying sparsity when measured in terms of the number of non - zero coefficients @xmath39 , it is also adaptive to the sparsity when measured in terms of the @xmath1 norm @xmath105 .",
    "this can become an advantage when @xmath57 has many small coefficients so that @xmath106 .",
    "indeed , the following theorem shows that the es estimator also enjoys adaptation in terms of its @xmath1 norm .",
    "[ th : sparseub1 ] assume that @xmath107",
    ". then for any @xmath89 the exponential screening estimator satisfies @xmath108 where @xmath109 and , for @xmath110 , @xmath111 furthermore , for any @xmath112 , such that @xmath113 , we have @xmath114 where @xmath115 and , for @xmath110 , @xmath116 in particular , if there exists @xmath96 such that @xmath95 , we have @xmath117    the proof of theorem  [ th : sparseub1 ] is obtained by combining theorem  [ th : sparseub ] and lemma  [ lem : maurey ] in the appendix . for brevity ,",
    "the constants derived from lemma  [ lem : maurey ] are rounded up to the closest integer .",
    "it is easy to see that in fact lemma  [ lem : maurey ] implies a more general result .",
    "not necessarily @xmath118 but , in general , any estimator satisfying a soi of the type   also obeys the oracle inequality of the form  , i.e. , enjoys adaptation simultaneously in terms of the @xmath0 and @xmath1 norms .",
    "this remains still a theoretical proposal , since we are not aware of estimators satisfying   apart from @xmath119 .",
    "however , there are estimators for which coarser versions of   are available as discussed in the next section .",
    "the aim of this section is to put theorem  [ th : sparseub1 ] in perspective by discussing weaker results in the same spirit for two popular estimators , namely , the bic and the lasso estimators .",
    "we consider the following version of the bic estimator , cf .",
    "@xcite : @xmath120 where @xmath121 with for some @xmath122 and @xmath123 . combining theorem  3.1 in  @xcite and lemma  [ lem : maurey ] in the appendix",
    "we get the following corollary .",
    "[ cor : bic ] assume that @xmath107 .",
    "then there exists a positive numerical constant @xmath124 such that for any @xmath125 and any @xmath122 the bic estimator satisfies @xmath126 where @xmath127 is defined in  .",
    "we note that theorem  3.1 in  @xcite is stated with @xmath128 and with the additional assumption that all the functions @xmath129 are uniformly bounded .",
    "nevertheless , this last condition is not used in the proof in  @xcite , and the result trivially extends to the framework that we consider here .",
    "the soi   ensures adaptation to sparsity simultaneously in terms of the @xmath0 and @xmath1 norms .",
    "however , it is less precise than the soi in theorem  [ th : sparseub1 ] because the leading constant @xmath130 is strictly greater than 1 and the rate deteriorates as the leading constant approaches 1 , i.e. , as @xmath131 .",
    "also the computation of the bic estimator is a hard combinatorial problem , exponential in @xmath132 , and it can be efficiently solved only when the dimension @xmath132 is small .",
    "consider now the lasso estimator @xmath133 , i.e. , a solution of the minimization problem @xmath134 where @xmath135 is a tuning parameter .",
    "this problem is convex , and there exist several efficient algorithms of computing @xmath136 in polynomial time .",
    "our aim here is to present results in the spirit of theorem  [ th : sparseub1 ] for the lasso .",
    "they have a weaker form than for the es estimator and for the bic . in the next theorem",
    ", we give a soi in terms of the @xmath1 norm that is similar to those that we have presented for the es and bic estimators but it is stated in probability rather than in expectation and the logarithmic factor in the rate is less accurate .",
    "note that it does not require any restrictive condition on the dictionary @xmath137 .",
    "[ th : lasso1 ] assume that @xmath107 .",
    "let @xmath138 and let @xmath133 be the lasso estimator defined by   with @xmath139 , where @xmath140 .",
    "then with probability at least @xmath141 we have @xmath142    proof . from the definition of @xmath133 by a simple algebra we get @xmath143 next ,",
    "note that @xmath144 for the random event @xmath145 ( cf .",
    "* eq . ( b.4 ) )",
    ". therefore , @xmath146 with probability at least @xmath141 .",
    "thus , follows by the triangle inequality and the definition of @xmath147 .",
    "the rate @xmath148 in   is slightly worse than the corresponding @xmath1 term of the rate of es estimator , cf .   and  .",
    "in contrast to theorem  [ th : lasso1 ] , a soi in terms of the @xmath0 norm for the lasso is available only under strong conditions on the dictionary @xmath137 .",
    "following @xcite , we say that the restricted eigenvalue condition re(@xmath149,@xmath150 ) is satisfied for some integer @xmath149 such that @xmath151 , and a positive number @xmath150 if we have : @xmath152 here @xmath153 is the cardinality of the index set @xmath154 and we denote by @xmath155 the vector in @xmath156 that has the same coordinates as @xmath157 on @xmath154 and zero coordinates on the complement @xmath158 of @xmath154 .",
    "a typical soi in terms of the @xmath0 norm for the lasso is given in theorem  6.1 of  @xcite .",
    "it guarantees that , under the condition re(@xmath149,@xmath159 ) and the assumptions of theorem  [ th : lasso1 ] , with probability at least @xmath141 , we have @xmath160 for all @xmath122 and some constant @xmath161 depending only on @xmath162 and @xmath163 . this oracle inequality is substantially weaker than and .",
    "indeed , it is valid under assumption re(@xmath149,@xmath159 ) , which is a strong condition .",
    "furthermore , the rank of the matrix @xmath2 does not appear , the minimum in is taken over the set of sparsity @xmath149 linked to the properties of the matrix @xmath2 , and the minimal restricted eigenvalue @xmath164 appears in the denominator .",
    "this contrasts with inequalities , and which hold under no assumption on @xmath2 , except for simple normalization : @xmath107 .",
    "finally , the leading constant in is strictly larger than @xmath165 , and the same comments as for the bic apply in this respect .",
    "section [ sec : ub ] provides upper bounds on the risk of es estimator .",
    "a natural question is whether these bounds are optimal . at first sight , to show the optimality it seems sufficient to prove that there exists @xmath41 and @xmath25 such that , for any estimator @xmath166 , @xmath167 where @xmath168 is some constant independent of @xmath169 and @xmath132 .",
    "this can be also written in the form @xmath170 where @xmath171 denotes the infimum over all estimators .",
    "we note that it is possible to prove   under some assumptions on the dictionary @xmath172 .",
    "however , we do not consider this type of results because they do not lead a valid notion of optimality . indeed , since the rate @xmath173 is a function of parameter @xmath57 , there exists infinitely many different rate functions @xmath174 for which   can be proved and complemented by the corresponding upper bounds . to illustrate this point , consider a basic example defined by the following conditions : + ( i ) @xmath175 , + ( ii ) @xmath95 for some @xmath176 , + ( iii ) the gram matrix @xmath177 is equal to the @xmath178 identity matrix , + ( iv ) @xmath179 . + this",
    "will be further referred to as the _ diagonal model_. it can be equivalently written as a gaussian sequence model @xmath180 where @xmath181 and @xmath182 are i.i.d .",
    "standard gaussian random variables .",
    "clearly , estimation of @xmath25 in the diagonal model is equivalent to estimation of @xmath104 in model  , and we have the isometry @xmath183 .",
    "moreover , it is easy to see that we can consider w.l.o.g",
    ". only estimators @xmath166 of the form @xmath184 for some statistic @xmath185 , and that   for the diagonal model follows from a simplified bound @xmath186 where we write @xmath187 to specify the dependence of the expectation upon @xmath57 , @xmath188 denotes the infimum over all estimators , and for brevity @xmath189 .    results of the type   are available in  @xcite where it is proved that , for the diagonal model , @xmath190 as @xmath191 , where @xmath192 the expression in curly brackets in   is the risk of 0 - 1 ( or  keep - or - kill \" ) oracle , i.e. , the minimal risk of the estimators @xmath185 whose components @xmath193 are either equal to @xmath194 or to 0 . a relation similar to  , with the infimum taken over a class of thresholding rules ,",
    "is proved in  @xcite .",
    "the result   is often wrongly interpreted as the fact that the factor @xmath195 is the  unavoidable \" price to pay for sparse estimation . in reality",
    "this is not true , and   can not be considered as a basis of valid notion of optimality . indeed , using the results of section [ sec : ub ] , we are going to construct an estimator whose risk is @xmath196 for all @xmath57 , and is of order @xmath197 for some @xmath57 , cf . theorem  [ th : donjohn1 ] below .",
    "so , this estimator improves upon   not only in constants but in the rate ; in particular , the exact asymptotic constant appearing in   is of no importance .",
    "the reason is that the lower bound for   in  @xcite is proved by restricting @xmath57 to a small subset of @xmath198 , and the behavior of the risk on other subsets of @xmath198 can be much better .",
    "define the rate @xmath199 + \\frac1{n}\\,,\\ ] ] which is an asymptotic upper bound on the rate in   for @xmath175 , @xmath191 .",
    "[ th : donjohn ] consider the diagonal model .",
    "then the exponential screening estimator satisfies @xmath200 and @xmath201 furthermore , @xmath202    proof .",
    "we first prove  . from  , lemma  [ lem : lse ] and   we obtain @xmath203 for any @xmath204 .",
    "let @xmath205 be the vector with components @xmath206 where @xmath207 denotes the indicator function",
    ". then @xmath208 and @xmath209 therefore , @xmath210 which implies  .",
    "next ,   is an immediate consequence of  . to prove   we consider , for example , the set @xmath211 where @xmath212 are constants . for all @xmath213 we have @xmath214 and @xmath215 so that @xmath216 hence , follows .",
    "theorem  [ th : donjohn ] shows that the normalizing function ( rate ) @xmath217 and the result   can not be considered as a benchmark .",
    "indeed , the risk of the es estimator is strictly below this bound .",
    "it attains the rate @xmath217 everywhere on @xmath198 ( cf .  ) and has strictly better rate on some subsets of @xmath198 ( cf .  ,  ) .",
    "in particular , the es estimator improves upon the soft thresholding estimator , which is known to asymptotically attain the bound   ( cf . * ? ? ? * ) .",
    "this is a kind of inadmissibility statement for the rate @xmath217 .",
    "observe also that the improvement that we obtain is not a `` marginal '' effect regarding signals @xmath57 with small intensity . indeed ,   is stronger than   and the set @xmath218 is rather massive .",
    "in particular , the @xmath0 norm @xmath39 in the definition of @xmath218 can be arbitrary , so that @xmath218 contains elements @xmath57 with the whole spectrum of @xmath1 norms , from small @xmath219 to very large @xmath220 .",
    "various other examples of @xmath218 satisfying   can be readily constructed .",
    "so far , we were interested only in the rates .",
    "the fact that the constant in   is equal to  2 was of no importance in this argument since on some subsets of @xmath198 we can improve the rate .",
    "notice that one can construct estimators having the same properties as those proved for @xmath77 in theorem  [ th : donjohn ] with constant  1 instead of  2 in  .",
    "in other words , one can construct an estimator @xmath221 whose risk is at least as small as @xmath222 everywhere on @xmath198 and attains strictly faster rate @xmath197 on some subsets of @xmath198 .",
    "such an estimator @xmath221 can be obtained by aggregating @xmath77 with the soft thresholding estimator , as shown in the next theorem .",
    "[ th : donjohn1 ] consider the diagonal model .",
    "then there exists a randomized estimator @xmath223 such that @xmath224 where the expectation includes that over the randomizing distribution , and where the normalizing functions @xmath225 satisfy @xmath226 and @xmath227    the proof of this theorem is given in the appendix .",
    "the rate in the upper bound of theorem  [ th : sparseub1 ] is the minimum of terms depending on the @xmath0 norm @xmath39 and on the @xmath1 norm @xmath228 , cf .  .",
    "we would like to derive a corresponding lower bound , i.e. , to show that this rate of convergence can not be improved in a minimax sense .",
    "since both @xmath0 and @xmath1 norms are present in the upper bound , a natural approach is to consider minimax lower bounds on the intersection of @xmath0 and @xmath1 balls .",
    "here we prove such a lower bound under some assumptions on the dictionary @xmath229 or , equivalently , on the matrix @xmath2 .",
    "along with the lower bound for one  worst case \" dictionary @xmath230 , we also state it uniformly for all dictionaries in a certain class .",
    "recall first , that all the results from section  [ sec : ub ] hold under the only condition that the dictionary @xmath230 is composed of functions @xmath129 such that @xmath231 .",
    "this condition is very mild compared to the assumptions that typically appear in the literature on sparse recovery using @xmath1 penalization such as the lasso or the dantzig selector .",
    "@xcite review a long list of such assumptions , including the restricted isometry ( ri ) property given , for example , in @xcite and the restricted eigenvalue ( re ) condition of @xcite described in section  [ sec : lasso_bic ] .",
    "we call them for brevity the @xmath232-conditions . loosely speaking",
    ", they ensure that for some integer @xmath233 , the design matrix @xmath2 forms a quasi - isometry from a suitable subset @xmath234 of @xmath54 into @xmath198 for any @xmath53 such that @xmath235 . here  quasi - isometry \" means that there exist two positive constants @xmath236 and @xmath237 such that @xmath238 while the general thinking is that a design matrix @xmath2 satisfying an @xmath232-condition is favorable , we establish below that , somewhat surprisingly , such matrices correspond to the least favorable case .",
    "we now formulate a weak version of the ri condition . for any integer @xmath239 and any @xmath240",
    "let @xmath241 denote the set of vectors @xmath242 such that @xmath243 .",
    "for any constants @xmath244 and @xmath245 let @xmath246 be the class of design matrices @xmath2 defined by the conditions :    * @xmath247 , * there exist @xmath248 such that @xmath249 and @xmath250    note that @xmath251 implies @xmath252 .",
    "examples of matrices @xmath2 that satisfy   are given in the next subsection .",
    "in the next subsection we show that the upper bound of theorem  [ th : sparseub1 ] matches a minimax lower bound which holds uniformly over the class of design matrices  @xmath253 .",
    "denote by @xmath254 the distribution of @xmath255 where @xmath256 , and by @xmath257 the corresponding expectation . for",
    "any @xmath258 and any integers @xmath259 such that @xmath260 , define the quantity @xmath261 note that @xmath262 where @xmath263 is the function   with @xmath264 and @xmath265 .",
    "let @xmath266 be the largest integer satisfying @xmath267 if such an integer exists .",
    "if there is no @xmath268 such that   holds , we set @xmath269 .",
    "note that @xmath270 .",
    "[ th : low ] fix @xmath258 and integers @xmath271 , @xmath272 .",
    "fix @xmath273 and let @xmath230 be any dictionary with design matrix @xmath274 , where @xmath275 and @xmath276 is defined in  . then , for any estimator @xmath277 , possibly depending on @xmath278 and @xmath230 , there exists a numerical constant @xmath279 , such that @xmath280 where @xmath281 denotes the rank of @xmath2 and @xmath282 is the positive cone of @xmath156 .",
    "moreover , @xmath283    the proof of this theorem is given in subsection  [ sub : proof_low ] of the appendix .",
    "it is worth mentioning that the result of theorem  [ th : low ] is stronger than the minimax lower bounds discussed in subsection  [ sec : disc ] ( cf .  ) in the sense that even if @xmath284 , where @xmath285 and @xmath286 are known a priori , the rate can not be improved .",
    "define @xmath287 $ ] for some constant @xmath288 to be chosen small enough .",
    "we now show that for each choice of @xmath289 such that @xmath290 , there exists at least one matrix @xmath291 such that @xmath292 .",
    "a basic example is the following .",
    "take the elements @xmath293 , of matrix @xmath2 as @xmath294 where @xmath295 are i.i.d .",
    "rademacher random variables , i.e. , random variables taking values @xmath165 and @xmath296 with probability @xmath79 .",
    "first , it is clear that then @xmath297 .",
    "next , condition ( ii ) in the definition of @xmath298 follows from the results on ri properties of rademacher matrices .",
    "many such results have been derived and we focus only on that of  @xcite because of its simplicity .",
    "indeed , theorem  5.2 in  @xcite ensures not only that for an integer @xmath299 there exist design matrices in @xmath300 but also that most of the design matrices @xmath2 with i.i.d .",
    "rademacher entries @xmath301 are in @xmath302 for some @xmath273 as long as there exists a constant @xmath303 small enough such that the condition @xmath304 is satisfied .",
    "specifically , theorem  5.2 in  @xcite ensures that if @xmath305 is the @xmath306 matrix composed of the first @xmath307 rows of @xmath2 with elements as defined in ( [ eq : radmat ] ) , and @xmath308 holds for small enough @xmath303 , then @xmath309 with probability close to 1 which in turn implies ( ii ) with @xmath310 . as a result , the above construction yields @xmath311 that has rank bracketed by @xmath312 and @xmath307 since   holds by our definition of @xmath307 .    in what follows @xmath303",
    "is the constant in   small enough to ensure that theorem  5.2 in  @xcite holds , and we assume w.l.o.g .",
    "that @xmath313 . using the above remarks and theorem  [ th : low ] we obtain the following result .",
    "[ cor : low ] fix @xmath258 and integers @xmath314 .",
    "moreover , assume that @xmath315 .",
    "then there exists a dictionary @xmath230 composed of functions @xmath129 with @xmath107 , @xmath316 , and a constant @xmath317 such that @xmath318 where the infimum is taken over all estimators .",
    "moreover , @xmath319    proof .",
    "let @xmath2 be a random matrix constructed as in   so that the rank of @xmath2 is bracketed by @xmath312 and @xmath307 and @xmath311 .",
    "we consider two cases .",
    "assume first that @xmath320 so that @xmath321 and the result follows trivially from theorem  [ th : low ] .",
    "next , if @xmath322 , observe that @xmath323 ( we used here that @xmath313 ) , so that @xmath324 it yields @xmath325 and the result follows from theorem  [ th : low ] , which ensures that @xmath326    as a consequence of theorem  [ th : low ] we get a lower bound on the @xmath0 ball @xmath327 by formally setting @xmath328 in  : @xmath329\\ ] ] and the same type of bound derived from  .",
    "analogous considerations lead to the following lower bound on the @xmath1 ball @xmath330 when setting @xmath331 : @xmath332and to the same type of bound derived from  .",
    "consider now the linear regression , i.e. , assume that there exists @xmath104 such that @xmath333 . comparing   with   we find that for @xmath334 the rate @xmath335 is the minimax rate of convergence on @xmath336 and that the es estimator is rate optimal .",
    "moreover , it is rate optimal separately on @xmath337 and @xmath338 , and the minimax rates on these sets are given by the right hand sides of   and   respectively .",
    "for the _ diagonal model _ ( cf . subsection  [ sec : disc ] ) , asymptotic lower bounds and exact asymptotics of the minimax risk on @xmath339 balls were studied by @xcite for @xmath340 and by @xcite for @xmath341 .",
    "these results were further refined by @xcite . in the @xmath0 case ,",
    "@xcite exhibit a minimax rate over @xmath337 that is asymptotically equivalent to @xmath342 in the @xmath1 case ,  @xcite prove that the minimax rate over an @xmath1 ball with radius @xmath343 is asymptotically equivalent to @xmath344 in both cases , the above rates are equivalent , up to a numerical constant , to the asymptotics of the right hand sides of   and   under the diagonal model .",
    "we note that the results of those papers are valid under some restrictions on asymptotical behavior of @xmath345 ( resp .",
    "@xmath343 ) as a function of @xmath169 .",
    "recently @xcite extended the study of asymptotic lower bounds on @xmath339 balls ( @xmath346 ) to the non - diagonal case with @xmath347 .",
    "their results hold under some restrictions on the joint asymptotic behavior of @xmath348 and @xmath345 ( respectively , @xmath343 ) .",
    "the minimax rates on the @xmath0 and @xmath1 balls obtained in ( * ? ? ?",
    "* theorem  3 ) are similar to   and   but , because of the specific asymptotics , some effects are wiped out there . for example , the @xmath1 rate in @xcite is @xmath349 , whereas   reveals an elbow effect that translates into different rates for @xmath350 .",
    "furthermore , the dependence on the rank of @xmath2 does not appear in @xcite , since under their assumptions @xmath351 .",
    "theorem  [ th : low ] above gives a stronger result since it is ( i ) non - asymptotic , ( ii ) it explicitly depends on the rank @xmath281 of the design matrix and ( iii ) it holds on the intersection of the @xmath0 and @xmath1 balls .",
    "moreover , theorem  [ th : sparseub1 ] shows that the @xmath352 lower bound is attained by one single estimator : the exponential screening estimator .",
    "alternatively , @xcite treat the two cases separately , providing two lower bounds and two different estimators that attain them in some specific asymptotics .",
    "[ sec : univ ]    combining the elements of a dictionary @xmath353 to estimate a regression function @xmath25 originates from the problem of aggregation introduced by  @xcite .",
    "it can be generally described as follows .",
    "given @xmath354 , the goal of aggregation is to construct an estimator @xmath355 that satisfies an oracle inequality of the form @xmath356 with the smallest possible ( in a minimax sense ) remainder term @xmath357 , in which case @xmath357 is called optimal rate of aggregation , cf .",
    "@xcite identified three types of aggregation : ( ms ) for _ model selection _ ,",
    "( c ) for _ convex _ and ( l ) for _ linear_. @xcite also considered another collection of aggregation problems , denoted by ( @xmath358 ) for _ subset selection _ and indexed by @xmath359 . to each of these problems corresponds a given set @xmath354 and an optimal remainder term @xmath357 . for ( ms ) aggregation , @xmath360 , where @xmath361 is the @xmath362-th vector of the canonical basis of @xmath156 . for ( c )",
    "aggregation , @xmath363 is a convex compact subset of the simplex @xmath364 .",
    "the main example of @xmath365 is the set of all convex combinations the @xmath129 s . for ( l )",
    "aggregation , @xmath366 , so that @xmath367 is the set of all linear combinations the @xmath129 s . given an integer @xmath368 , for ( @xmath358 ) aggregation , @xmath369 . for this problem , @xmath370 is the set of all linear combinations of at most @xmath3 of the @xmath129 s .",
    "note that all these sets @xmath371 are of the form @xmath336 for specific values of @xmath345 and @xmath343 .",
    "this allows us to apply the previous theory .",
    ".sets of parameters @xmath372 and @xmath373 and corresponding optimal rates of aggregation presented in  @xcite .",
    "note that  @xcite considered a slightly different definition in the ( c ) case : @xmath374 leading to the same rate . [ cols=\">,^,^\",options=\"header \" , ]     to conclude , we mention a byproduct of this simulation study .",
    "the coefficients of @xmath375 can be used to perform multi - class classification following the idea of  @xcite .",
    "the procedure consists in performing a majority vote on the features @xmath376 that are positively weighted by @xmath375 , i.e. , such that @xmath377 .",
    "for the particular instance illustrated in figure  [ fig:6_05 ]  ( c ) , we see in figure  [ fig : multi ] that only a few features @xmath376 receive a large positive weight and that a majority of those correspond to the digit `` 6 '' .    , @xmath378 and the corresponding image.,scaledwidth=50.0% ]",
    "proof . fix @xmath383 and an integer @xmath380 . set @xmath384 . consider the multinomial parameter @xmath385^m$ ] with @xmath386 , where @xmath387 are the components of @xmath104 .",
    "let @xmath388 be the random vector with multinomial distribution @xmath389 , i.e. , let @xmath390 where @xmath391 are i.i.d .",
    "random variables taking value @xmath392 with probability @xmath393 , @xmath394 . in particular",
    ", we have @xmath395 , where @xmath396 denotes the expectation with respect to the multinomial distribution . as a result , for the random vector @xmath397 with the components",
    "@xmath398 we have @xmath399 for @xmath394 with the convention that @xmath400 .",
    "moreover , using the fact that @xmath401 and @xmath402 for @xmath403 ( see , e.g. , * ? ? ?",
    "* eq . ( a.13.15 ) , p.  462 ) we find that the covariance matrix of @xmath404 is given by @xmath405=\\frac{|\\theta^*|_1}{k}{\\rm diag}(|\\theta^*_j|)-\\frac{1}{k}|\\theta^*||\\theta^*|^\\top\\,,\\ ] ] where @xmath406 .",
    "using a bias - variance decomposition together with the assumption @xmath407 , it yields that , for any function @xmath381 , @xmath408 where @xmath409 , @xmath17 .",
    "moreover , since @xmath410 is such that @xmath411 and @xmath412 , the lemma follows .",
    "[ lem : maurey ] fix @xmath413 and assume that @xmath407 . for any function @xmath25 and",
    "any constant @xmath414 we have @xmath415 where @xmath416 , @xmath417 and for @xmath418 , @xmath419 , & { \\rm if}\\ \\langle { { \\sf f}}_\\theta , \\eta \\rangle \\le \\|{{\\sf f}}_\\theta\\|^2,\\\\ \\frac{\\nu|\\theta|_1}{\\sqrt{n } } \\sqrt{\\log \\left(1+\\frac{em\\nu}{|\\theta|_1 \\sqrt{n}}\\right ) } + \\frac{\\nu^2\\log(1+em)}{{\\tilde c } n } , & { \\rm otherwise}\\ , .",
    "\\end{array } \\right.\\ ] ]    proof .",
    "set @xmath420 it suffices to consider @xmath421 instead of @xmath156 since @xmath422 . fix @xmath383 and define @xmath423 assume first @xmath424 . in this case",
    "we have @xmath425 the previous display yields that @xmath426 . moreover , if @xmath427 , it holds @xmath428 as a result , @xmath429 set @xmath430 , i.e. , @xmath431 is the minimal integer greater than or equal to @xmath432 . using the monotonicity of the mapping @xmath433 for @xmath434 , and lemma  [ lem : maurey0 ] we get , for any @xmath435 such that @xmath436 @xmath437 on the other hand , if @xmath383 and @xmath438 , we use the simple bound @xmath439 in view of the last two displays , to conclude the proof it suffices to show that @xmath440 for all @xmath441 .",
    "note first that if @xmath442 , then @xmath443 and @xmath444 where we used   in the first inequality .",
    "together with  , this proves that @xmath445 for all @xmath441 such that @xmath442 .",
    "thus , to complete the proof of the lemma we only need to consider the case @xmath446 . for @xmath446",
    "we have @xmath447 as a result , we have @xmath448 moreover , it holds @xmath449 and since the function @xmath450 is increasing , we obtain @xmath451 thus , for @xmath452 we have @xmath453 for @xmath454 we use the inequality @xmath455 , to obtain @xmath456 \\\\ & \\le & \\frac{2|\\theta^*|_1 } { \\sqrt{n } } \\left(\\frac { \\ell}{\\nu^2 } + \\frac{\\log(\\ell/\\nu)}{\\ell } \\right ) \\le \\left(2+\\frac1{e}\\right)\\frac{|\\theta^*|_1\\ell } { \\nu^2\\sqrt{n}}\\\\ & \\le & \\left(2+\\frac1{e}\\right)\\frac{1}{\\nu^2 }   \\bar \\varphi_{n , m}(\\theta^*)\\,.\\end{aligned}\\ ] ] thus , in both cases @xmath457 . combining this with",
    "we get  .      applying the randomization scheme described in nemirovski  ( 2000 ) , p.211",
    ", we create from the sample @xmath458 satisfying two independent subsamples with  equivalent \" sizes @xmath459 and @xmath460 .",
    "we use the first subsample to construct the es estimator and the soft thresholding estimator @xmath461 , the latter attaining asymptotically the rate @xmath217 for all @xmath462 .",
    "we then use the second subsample to aggregate them , for example , as described in nemirovski  ( 2000 ) . then the aggregated estimator denoted by @xmath223 satisfies , for all @xmath462 , @xmath463 where @xmath161 is an absolute constant and @xmath464 as @xmath191 uniformly in @xmath462 .",
    "set @xmath465 then follows immediately .",
    "next , @xmath466 , so that for all @xmath462 , @xmath467 which implies . finally , to prove it is enough to notice that since @xmath466 , @xmath468 and to use .",
    "we now prove .",
    "let @xmath353 be any dictionary in @xmath470 with the corresponding @xmath236 and @xmath237 such that @xmath471 .",
    "for any @xmath472 , let @xmath473 be the subset of @xmath474 defined by @xmath475 we consider the class of functions @xmath476 where @xmath477 will be chosen later .",
    "note that functions in @xmath478 are of the form @xmath479 with @xmath480 , @xmath481 and @xmath482 .",
    "thus , to prove  , it is sufficient to show that , for any estimator @xmath277 , @xmath483 for some subset @xmath484 where @xmath485 $ ] and @xmath486 $ ] denotes the integer part .",
    "note that @xmath487 since @xmath488 and @xmath489 .    in what follows we will use the fact that for @xmath490 the difference @xmath491 is of the form @xmath479 with some @xmath492 , so that in view of  , @xmath493 is bracketed by the multiples of @xmath494 with this value of @xmath57 .        .",
    "then @xmath500 and @xmath501 , so that we have @xmath502 , and lemma  [ lem : bm01 ] guarantees that there exists @xmath503 with cardinality @xmath504 and such that @xmath505 to bound from below the quantity @xmath506 , observe that from the definition of @xmath276 we have @xmath507 the previous two displays yield @xmath508 note that in this case @xmath509 so that @xmath510    .",
    "then @xmath511 < m$ ] .",
    "moreover , we have @xmath512 and using lemma  [ lem : bm01 ] , for any positive @xmath513 we can construct @xmath514 with cardinality @xmath515 and such that @xmath516 take @xmath517 where , in the last inequality , we used the definition of @xmath276 . next ,",
    "note that @xmath511 1   \\ge s/4 $ ] since @xmath239 .",
    "then @xmath518 in addition , we have @xmath519    since the random variables @xmath520 are i.i.d .",
    "gaussian @xmath521 , for any @xmath522 , @xmath523 , the kullback - leibler divergence @xmath524 between @xmath525 and @xmath526 is given by @xmath527 where @xmath528 .",
    "using respectively   in case @xmath529 , in case @xmath530 and in case @xmath531 , and choosing @xmath532 ( note that we need @xmath533 by construction ) we obtain @xmath534 combining  , and   together with  , we find that the conditions of theorem  2.7 in  @xcite are satisfied and use it to obtain  .      here",
    "we give a result related to subset extraction , which is a generalization of the varshamov - gilbert lemma used to prove minimax lower bounds ( see , e.g. , a recent survey in @xcite[chap .",
    "2 ] ) . for any @xmath535 , @xmath536 , let @xmath537 be the subset of @xmath538 defined by :",
    "@xmath539    the next lemma is a modification of ( * ? ? ?",
    "* lemma  4 ) .",
    "the difference is that we cover any @xmath540 , @xmath541 .",
    "the result of @xcite is proved for even integer @xmath542 such that @xmath543 .",
    "the price we pay for considering general @xmath544 is only in terms of constants , which is sufficient for our purposes .",
    "[ lem : bm01 ] let @xmath545 and @xmath541 be two integers and define @xmath546 .",
    "then there exists a subset @xmath547 of @xmath548 such that the hamming distance @xmath549 satisfies @xmath550 and @xmath551 satisfies @xmath552 for some numerical constant @xmath553 .    proof .",
    "( i ) consider first the case where @xmath554 for some integer @xmath555 and @xmath556 .",
    "lemma  4 in @xcite ensures the existence of a subset @xmath557 of @xmath548 such that @xmath558 for any @xmath559 and @xmath560= \\frac{k}{2}\\log\\left(\\frac{em}{8k}\\right)\\,.\\ ] ] ( ii ) next , if @xmath561 for some integer @xmath562 and @xmath563 , let @xmath564 be the set obtained by lemma  4 in @xcite .",
    "we have @xmath565 for any @xmath566 and @xmath567 where we used the fact that @xmath568 .",
    "define now the set @xmath569 we have @xmath570 , @xmath571 and @xmath565 for any @xmath572 .",
    "\\(iii ) if @xmath578 , @xmath579 , let @xmath580 be the integer part of @xmath581 : @xmath582 \\ge 36 $ ] , and observe that @xmath583 where @xmath584 .",
    "therefore , we can apply the preceding results to ensure that there exists a subset @xmath585 of @xmath586 such that @xmath587 and @xmath588 for any @xmath589 , @xmath590 . since @xmath591",
    ", we obtain @xmath592 to embed @xmath585 in @xmath548 , define @xmath593 we have @xmath594 , @xmath595 and @xmath596 for any @xmath597 .",
    "\\(iv ) if @xmath576 , consider the set @xmath598)}\\ } \\subset \\omega_k^m$ ] , such that , for any @xmath599 $ ] , the @xmath600-th coordinate of @xmath601 satisfies @xmath602 if and only if @xmath603 . we have @xmath604 for any @xmath605 and @xmath606\\right)&\\ge & \\frac{\\log 2}{\\log(1 + 2e)}\\log\\left(1+\\frac{em}{k}\\right ) \\nonumber\\\\ & \\ge & \\frac{k}{71}\\frac{\\log 2}{\\log(1 + 2e)}\\log\\left(1+\\frac{em}{k}\\right ) \\nonumber\\\\ & \\ge & 0.005k\\log\\left(1+\\frac{em}{k}\\right).\\end{aligned}\\ ] ]    note that ( i)(iv ) cover all @xmath607 and @xmath608 , and in these cases @xmath609 .",
    "we now use  , and   jointly with the following inequality @xmath610 this yields the result of the lemma for cases ( i ) , ( ii ) and ( iii ) since in these cases @xmath611 and @xmath612 . for case ( iv )",
    "we use directly  .",
    "thus , the lemma is proved for @xmath607 .",
    "\\(v ) finally , if @xmath577 , or equivalently , when @xmath613 , we can reproduce all the arguments above with @xmath542 replaced by @xmath614 which satisfies @xmath615 . in each case , @xmath616 , we obtain the subsets @xmath617 analogous to @xmath618 in ( i)(iv ) .",
    "they are uniquely mapped into @xmath548 by applying the bijection @xmath619 , where @xmath620 .",
    "dalalyan , a.  s. and tsybakov , a.  b. ( 2007 ) .",
    "aggregation by exponential weighting and sharp oracle inequalities . in _ learning theory _ ,",
    "4539 of _ lecture notes in comput .",
    "_ springer , berlin , 97111 .",
    "donoho , d.  l. , johnstone , i.  m. , hoch , j.  c. and stern , a.  s. ( 1992 ) .",
    "maximum entropy and the nearly black object .",
    "b _ , * 54 * 4181 . with discussion and a reply by the authors .",
    "hastie , t. , tibshirani , r. and friedman , j. ( 2001 ) . _ the elements of statistical learning_. springer series in statistics , springer - verlag , new york .",
    "data mining , inference , and prediction , http://www - stat.stanford.edu/~tibs / elemstatlearn/.            lecun , boser , b. , denker , j.  s. , henderson , d. , howard , r.  e. , hubbard , w. and jackel , l.  d. ( 1990 ) .",
    "handwritten digit recognition with a back - propagation network . in _ advances in neural information processing systems_.",
    "morgan kaufmann , 396404 ."
  ],
  "abstract_text": [
    "<S> in high - dimensional linear regression , the goal pursued here is to estimate an unknown regression function using linear combinations of a suitable set of covariates . </S>",
    "<S> one of the key assumptions for the success of any statistical procedure in this setup is to assume that the linear combination is sparse in some sense , for example , that it involves only few covariates . </S>",
    "<S> we consider a general , non necessarily linear , regression with gaussian noise and study a related question that is to find a linear combination of approximating functions , which is at the same time sparse and has small mean squared error ( mse ) . </S>",
    "<S> we introduce a new estimation procedure , called _ exponential screening _ </S>",
    "<S> that shows remarkable adaptation properties . </S>",
    "<S> it adapts to the linear combination that optimally balances mse and sparsity , whether the latter is measured in terms of the number of non - zero entries in the combination ( @xmath0 norm ) or in terms of the global weight of the combination ( @xmath1 norm ) . </S>",
    "<S> the power of this adaptation result is illustrated by showing that exponential screening solves optimally and simultaneously all the problems of aggregation in gaussian regression that have been discussed in the literature . </S>",
    "<S> moreover , we show that the performance of the exponential screening estimator can not be improved in a minimax sense , even if the optimal sparsity is known in advance . </S>",
    "<S> the theoretical and numerical superiority of exponential screening compared to state - of - the - art sparse procedures is also discussed .    </S>",
    "<S> * mathematics subject classifications : * primary 62g08 , secondary 62g05 , 62j05 , 62c20 , 62g20 .    </S>",
    "<S> * key words : * high - dimensional regression , aggregation , adaptation , sparsity , sparsity oracle inequalities , minimax rates , lasso , bic . </S>"
  ]
}