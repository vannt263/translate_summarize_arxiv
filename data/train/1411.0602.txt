{
  "article_text": [
    "in recent years , there is a growing trend to apply machine learning ( ml ) to massive and complex datasets @xcite . for many applications ,",
    "this leads to growing model sizes .",
    "a prime example are recommender systems where the size of the model is usually proportional to the number of users in the dataset @xcite . in cases with hundreds of millions of users this model exceeds the memory of an individual machine and distributed approaches to model training that leverage multiple machines become necessary .",
    "this leads to a set of challenges , e.g. how to partition the model among the participating machines and how to correctly execute learning algorithms in such a setting .",
    "+ in this work , we describe factorbird , a prototypical system that leverages a parameter server architecture  @xcite for learning large matrix factorization models for recommendation mining . after a short introduction to matrix factorization for recommender systems ( sec .",
    "[ sec : latentfactormodels ] ) , we describe the main challenges of our system , namely how to partition the model among the machines in the cluster and how to run stochastic gradient descent ( sgd ) in parallel .",
    "we discuss the design decisions taken in factorbird to overcome these challenges ( sec .",
    "[ sec : systemdesign ] ) .",
    "first , we partition one of the matrices to learn over dedicated machines in the cluster and co - partition the other one with the input data to localize a large number of updates and drastically reduce update conflicts and network traffic .",
    "next , we apply a lock - less hogwild!-style  @xcite execution scheme to efficiently run sgd in parallel . after giving insights into our software design and memory - efficient datastructures ( sec .  [",
    "sec : implementation ] ) , we describe techniques to assess model quality in factorbird ( sec .  [",
    "sec : modelquality ] ) .",
    "our approach here is to grid search over a large number of hyperparameter combinations in a single training run .",
    "finally , we present a set of experiments on user interaction data from twitter ( sec .",
    "[ sec : experiments ] ) .",
    "we run a scale - out experiment on a matrix built from a subset of twitter s interaction graph  @xcite , with more than 38 billion non - zeros and about 200 million rows and columns . to the best of our knowledge ,",
    "this is largest dataset on which matrix factorization experiments have been published so far .",
    "we build upon _ latent factor models _ , which leverage a low - rank matrix factorization of interaction data @xcite to characterize users and items by vectors of factors inferred from interaction patterns .",
    "these methods have been among the top - scoring contributions to the netflix prize @xcite .",
    "they factor a sparse partially - observed @xmath0 matrix @xmath1 , representing the interactions of @xmath2 users with @xmath3 items , into the product of two rank @xmath4 factor matrices @xmath5 and @xmath6 , such that their product @xmath7 approximates the observed parts of @xmath1 and generalizes well to unobserved parts of @xmath1 ( c.f .",
    "figure  [ fig : matrixfactorization ] ) .",
    "a user @xmath8 is associated to a factor vector @xmath9 ( a row of the @xmath10 matrix @xmath5 ) , while an item @xmath11 is associated to a factor vector @xmath12 ( a column of the @xmath13 matrix @xmath6 ) .",
    "the factorization maps users and items onto joint latent factor space of low dimensionality @xmath4 , such that @xmath14 estimates the strength of interaction between user @xmath8 and item @xmath11 .",
    "the standard approach to learning a latent factor model for recommender systems is to minimize the regularized squared error over the predictions to the observed parts of the matrix @xcite : @xmath15 this approach is closely related to the singular value decomposition ( svd ) of the matrix , which gives an optimal rank @xmath4 approximation ( w.r.t . to squared error ) of the original matrix using the top-@xmath4 singular values and singular vectors .",
    "the key difference here is that the svd is undefined when there are missing entries .",
    "+ we adopt a more sophisticated approach outlined in @xcite .",
    "first , we introduce a global bias term @xmath16 , which captures the average interaction strength between a user and an item in the dataset .",
    "next , we introduce a user - specific bias term @xmath17 for user @xmath8 and an item - specific bias term bias @xmath18 for item @xmath11 .",
    "these biases model how strongly the interactions of certain users and items tend to deviate from the global average .",
    "we substitute the dot product @xmath14 for predicting the strength of the interaction between a user @xmath8 and an item @xmath11 with a prediction function @xmath19 that takes the bias terms into account .",
    "furthermore , we introduce a function @xmath20 for determining the strength of the interaction between user @xmath8 and item @xmath11 .",
    "this allows us to transform the observed entries @xmath21 of @xmath1 .",
    "finally , we add a function @xmath22 for weighting the prediction error for interaction between user @xmath8 and item @xmath11 .",
    "this function becomes useful when @xmath1 consists of data from various sources with differing confidence in the observations .",
    "the loss function that we minimize for our latent factor model in factorbird is the following : @xmath23 we adopt a graph - specific terminology for the rest of the paper as well as for the apis of our system , as we think about the majority of twitter s datasets in terms of large networks .",
    "therefore , we assume that @xmath1 represents a network ( e.g. the network of user followings in twitter ) , @xmath8 and @xmath11 reference vertices in this network ( e.g. two users in this network ) and the edges of the network correspond to observed entries of @xmath1 , meaning that @xmath20 depicts the weight of a directed edge between a vertex @xmath8 and a vertex @xmath11 in this network .",
    "furthermore , we assume that the bias vectors @xmath24 and @xmath25 are stored in @xmath5 and @xmath6 ( e.g. , as first column and first row ) , to simplify notation .",
    "* factorization via stochastic gradient descent ( sgd ) .",
    "* there are various popular techniques to compute a matrix factorization .",
    "stochastic gradient descent ( sgd ) @xcite randomly loops through all observed interactions , computes the error of the prediction for each interaction and modifies the model parameters in the opposite direction of the gradient .",
    "another technique is alternating least squares ( als ) @xcite , which repeatedly keeps one of the unknown matrices fixed , so that the other one can be optimally re - computed .",
    "we chose sgd as optimization technique , as it is simple , provides fast convergence and is easy to adapt to different models and loss functions .",
    "algorithm [ alg : plainsgd ] shows the individual steps to conduct when learning the matrix factorization with sgd .",
    "first , we randomly initialize the factor matrices @xmath5 and @xmath6 ( c.f .",
    "next , we randomly pick an edge @xmath26 from the graph and compute the weighted error @xmath27 of the prediction @xmath28 against the actual edge strength @xmath20",
    "lines 3 & 4 ) .",
    "next , we update the global bias term as well as the bias terms for @xmath8 and @xmath11 proportional to the prediction error @xmath27 , the learning rate @xmath29 and the regularization constant @xmath30 ( c.f . lines 5 to 7 ) .",
    "we weight the regularization updates according the out - degree @xmath31 of vertex @xmath8 ( the number of observed entries in the @xmath8-th row of @xmath1 ) and the in - degree @xmath32 of @xmath11 ( the number of observed entries in the @xmath11-th column of @xmath1 ) .",
    "we update the factor vectors @xmath33 and @xmath34 analogously ( c.f .",
    "lines 8 & 9 ) .",
    "the whole process is repeated until convergence .",
    "randomly initialize @xmath5 and @xmath6",
    "having introduced the conceptual background of the models that we wish to learn , we proceed with describing the three main design goals of factorbird .",
    "+ first , factorbird must handle factorizations of twitter - scale graphs with hundreds of millions of vertices and dozens of billions of edges .",
    "scalability to datasets of this size is more important than high performance on small datasets commonly used in research ( such as the netflix @xcite dataset ) .",
    "the matrices representing these graphs are either square ( e.g. user to user followings ) or ` tall - and - wide ' for bipartite graphs ( e.g. user to tweets ) .",
    "second , the system has to be highly usable and adaptable for data scientists without making a systems background a necessity .",
    "no systems programming should be required to try out a variation of our model or a different loss function . instead , the system should offer interfaces that abstract from the distributed execution model and are intuitive to implement given an ml background .",
    "third , the system design shall be simple to keep the maintenance and debugging effort low .",
    "additionally , the system design should be easily extendable into a streaming system in the future , where a previously learned matrix factorization is updated online given a stream of new observations .",
    "this is the main reason why decided against using als , which is much harder to adapt to a streaming scenario than sgd .",
    "* challenges . *",
    "running sgd on datasets of such scale inevitably leads to a set of challenges that have to be overcome when designing a scalable system .",
    "the main focus of this paper is to present the prototype of a system that elegantly solves these challenges .",
    "\\(1 ) the resulting factor matrices for a huge network quickly become larger than the memory available on an individual commodity machine @xcite .",
    "for example , @xmath5 and @xmath6 with @xmath35 and a single precision factor representation for a graph with 250 million vertices already have a combined size of about 200 gb .",
    "this estimation does not even take operating system buffers and caches , object references and required statistics like degree distribution of the graph into account , which also compete for memory .",
    "\\(2 ) due to the sheer number of observations in large datasets , we aim to leverage multiple cores and machines to learn our desired matrix factorization .",
    "unfortunately , sgd is an inherently sequential algorithm .",
    "it randomly picks an observation and updates the model parameters before proceeding to the next observation ( c.f .",
    "algorithm [ alg : plainsgd ] ) .",
    "when we run sgd in parallel on multiple cores , there is a chance that we concurrently try to update the same @xmath33 or @xmath34 , which results in conflicting writes and lost updates .    in order to overcome the first challenge , we decided for a distributed architecture that allows us to partition the large model ( the factor matrices ) over several machines .",
    "we adapt a ` parameter server ' architecture @xcite .",
    "as illustrated in figure  [ fig : paramserver ] , we partition the factor matrices over a set of machines , to which we refer to as _ parameter machines_. at the same time , we partition the graph ( our input data ) over a set of so - called _",
    "learner machines_. each learner machine runs multi - threaded sgd on its portions of the input data . for every observation to process",
    ", the learner machine has to fetch the corresponding factor vectors from the parameter machines , update them and write them back over the network .",
    "this architecture inevitably leads to the second challege , the question of how to handle concurrent , possibly conflicting updates to the factor vectors .",
    "when two learner machines fetch , update and write back the same factor vector concurrently , one such update will be overridden .",
    "+ in the special case of matrix factorization , approaches to parallelizing sgd have been proposed that leverage a carefully chosen partitioning of the input data to avoid conflicting updates  @xcite .",
    "as these approaches require complex data movement and synchronization patterns and are at the same time hard to adapt to a streaming scenario , we decided for an alternative approach that is simpler to implement in a distributed setting . instead of taking complex actions to prevent conflicting updates , factorbird builds upon a recently proposed parallelization scheme for sgd - based learning called hogwild !",
    "this work states that parallel sgd can be implemented _ without any locking _",
    "if most updates only modify small parts of the model .",
    "the authors explicitly name latent factor models for matrix factorization as one such case .",
    "+ the special nature of the matrix factorization problem allows us for a further optimization in our system design , which reduces the required network traffic and at the same time greatly lowers the probability for conflicting overwrites",
    ". we can reduce the communication cost by 50% through intelligent partitioning , as follows .",
    "if we partition m by either rows or columns over the learning machines , than the updates to either @xmath5 or @xmath6 become local , when we co - partition one of the factor matrices ( @xmath5 in the case of partitioning by rows , @xmath6 in the case of partitioning by columns ) with @xmath1 on the learner machines .",
    "+ in the light of minimizing update conflict potential , we decide to co-locate @xmath6 on the learner machines and keep @xmath5 in the parameter machines ( c.f . figure  [ fig : bigpicture ] ) .",
    "we choose this scheme for the following reasons : in case of the follower graph , the number of updates to a factor vector @xmath33 in @xmath5 is equal to the out-degree of the corresponding vertex @xmath8 , while the number of updates to a factor vector @xmath34 in @xmath6 is equal to the in-degree of the corresponding vertex @xmath11 . as the in-degree distribution of the follower graph has much higher skew than the out-degree distribution  @xcite , we choose to localize the updates to @xmath6 , which gives us a higher reduction in conflict potential than localizing @xmath5 .",
    "other graphs in twitter have similar skew in the degree distribution .",
    "we implement factorbird using twitter s existing technology stack and infrastructure .",
    "we leverage an existing memcached cluster for the parameter machines and implement the learner machines in scala as a custom finagle application  @xcite .",
    "we use hadoop s distributed filesystem ( hdfs ) as persistent datastore for reading inputs and storing the final results . prepartitioning and statistics computation of the input",
    "is conducted via mapreduce using scalding jobs .",
    "the learner machines are assigned resources via apache mesos .",
    "+ the typical execution steps on a learner machine are as follows .",
    "first , the machine loads statistics about the input graph and its assigned partition from hdfs , e.g. the number of edges and vertices , the average edge weight and the degree distribution of the graph .",
    "these statistics are used for learning as well as efficiently allocating memory for the required datastructures .",
    "next , the learner machine instantiates its local partition of @xmath6 .",
    "subsequently , the model learning phase starts : the learner machine reads the edges of its assigned partition of the input graph in a streaming fashion . for each edge",
    "@xmath26 , it reads the corresponding factor vector @xmath33 from memcached and the factor vector @xmath34 from its local partition of @xmath6 .",
    "the reads from memcached are conducted in batches to increase throughput .",
    "next , the learner machine updates the factor vectors using sgd and writes them back , as memcached atomically updates the whole factor vector instead of individually updating the factors . ] . this process is repeated until a user - specified number of passes over the input graph has been conducted . finally , the learner machines persist the learned matrices @xmath5 and @xmath6 in a partitioned manner in hdfs .",
    "+ factorbird makes extensive use of memory - efficient data structures for managing factor vectors , the local partition of @xmath6 and graph statistics such as the degree per vertex .",
    "the main objective of these data structures is to use as little memory as possible as well as to avoid object allocation and full garbage collection invocations in the jvm .",
    "a factor vector as well as a partition of a factor matrix are therefore internally represented by byte buffers and large primitive arrays , which are efficiently preallocated .",
    "for example , a learning machine determines the size of its local partition of @xmath6 at startup time by reading the number of vertices assigned to its partition from the graph statistics .",
    "the partition of @xmath6 is internally represented by a huge float array , into which the individual factor vectors ( the columns of @xmath6 ) are packed .",
    "a mapping of vertex ids to offsets in this array is stored and update operations directly write to the underlying array . during the training phase",
    ", a learner machine directly reads the training edges from a compressed file in hdfs in a streaming fashion .",
    "if more than one pass through the training edges is necessary , the learner machine will on - the - fly create a copy of the hdfs file on local disk and switch to streaming edges from this local file for subsequent passes .",
    "furthermore , some learning approaches require synthetically generated negative examples  @xcite ( possibly taken into account with a lower confidence than observed positive examples ) .",
    "we therefore implement on - the - fly generation of such negative examples ( with a configurable probability ) and mix them into the original positive examples supplied on the learning machines .",
    "+ the central abstraction for implementing the sgd updates of the learning algorithm within factorbird is the _ learner _",
    "( c.f , listing  [ lst : learner ] ) .",
    "it is the main interface for data scientists wanting to try new models and shields the programmer from the complexity of the distributed nature of the underlying learning process .",
    "the first important method that a programmer has to implement is _ initialize _ which factorbird uses to randomly initialize of the factor vectors in @xmath5 and @xmath6 ( c.f .",
    "line in algorithm  [ alg : plainsgd ] ) .",
    "the method _ update _ is where the sgd - based update of two factor vectors @xmath33 and @xmath34 is implemented .",
    "the system provides the strength @xmath20 of the edge between vertex @xmath8 and @xmath11 , as well the vertex degrees @xmath31 and @xmath32 and the error weight @xmath22 as additional arguments .",
    "a typical implementation of this method conducts the steps from line 4 to 9 in algorithm  [ alg : plainsgd ] .",
    ".... trait learner {    def initialize(factors : factorvector ) : unit    def update(u_i : factorvector , v_j : factorvector ,                   a_ij : float , n_i : int , n_j : int , w_ij : float ) : float } ....",
    "the next aspect we focus on in factorbird is the quality of the learned models .",
    "ultimately , the models have to be evaluated using online experiments with real users , but during the development and batch training phase , we concentrate on a simple offline metric : the prediction quality on held - out data , measured by the root mean squared error ( rmse ) .",
    "unfortunately , this prediction quality is heavily influenced by the choice of hyperparameters for our model , such as @xmath29 ( which controls rate of learning ) , @xmath30 ( which controls regularization ) , number of factors @xmath4 as well as the rate of decay of the learning rate in our _ learner _ implementation . in order to find a well - working hyperparameter combination ,",
    "we conduct a grid search in the hyperparameter space .",
    "we extend factorbird to enable hold - out tests at scale . the scalding job which prepares the input graph randomly splits the edges into training set , validation set and test set .",
    "factorbird then learns a model on the training set , chooses the hyperparameter combination using the prediction quality on the validation set , and finally computes the rmse on the test set .",
    "+ however , conducting a single training run with factorbird for each hyperparameter combination to inspect is tedious and takes a long time .",
    "we therefore we describe how to learn many models with different hyperparameters at once to speed up the hyperparameter search .",
    "given that we aim to inspect @xmath36 hyperparameter combinations for a factorization of rank @xmath4 , we pack the @xmath36 factor vectors into a large @xmath5 of dimensionality @xmath37 and a large @xmath6 of dimensionality @xmath38 .",
    "we use a specialized learner implementation that is aware of the packing ( e.g. it knows that the factors for the @xmath39-th model are contained in the @xmath40-th to @xmath41-th entries of a factor vector ) and learns @xmath36 models at once .",
    "figure  [ fig : packing ] illustrates how the factor matrices for all possible combinations of two different learning rates @xmath42 , @xmath43 and two different regularization constants @xmath44 , @xmath45 would be packed into a large @xmath5 and @xmath6 .        in order to compute the prediction quality on held - out data",
    ", factorbird asks the _ learner _ implementation for _ _ predictor__s which the user has to provide .",
    "a _ predictor _ predicts the strength of an unobserved edge @xmath26 from the corresponding factor vectors @xmath33 and @xmath34 ( c.f .",
    ", listing  [ lst : predictor ] )",
    ".    .... trait predictor {    def predict(u_i : factorvector , v_j : factorvector ) : float } ....",
    "optionally , the user can provide a _ lossestimator _ , which estimates the current value of the loss function using samples of edges and factors  ( c.f .",
    ", listing  [ lst : loss ] ) . during training",
    ", factorbird continually invokes the loss estimator and make the estimates inspectable via an external dashboard .",
    ".... trait lossestimator {    def estimateregularizationcomponent (      numrowsofu : int , sampleofu : iterator[factorvector ] ,      numcolumnsofv : int , sampleofv : iterator[factorvector ] ) : double       def estimateerrorcomponent(numedges : long ,      sampleofedges : iterator[edge ] , partitionofu : factormatrix ,      partitionofv : factormatrix ) : double } ....",
    "we run experiments on various subsets of ` realgraph ' , a graph that models various interactions between twitter users  @xcite .",
    "the learner machines for our experiments are provisioned by apache mesos . in all our experiments ,",
    "we factorize the binarized adjacency matrix of the graph subset .",
    "that means the , transformation function @xmath20 returns @xmath46 if user @xmath8 interacted with user @xmath11 and @xmath47 otherwise ( in the case of a synthetic negative example ) .",
    "we equally weight all prediction errors ( @xmath48 ) . in this work ,",
    "we present only preliminary experiments , aimed at validating the correctness of our system and showing its capacity to handle twitter - scale graphs .",
    "there is still a large potential for improvements in accuracy and performance that we will tackle in future work .",
    "we run a first set of experiments on a small sample of the realgraph , consisting of 100 million interactions between 440 thousand popular users .",
    "additionally , we make factorbird generate 500 million synthetic negative edges .     of a sample of realgraph . ]     of a sample of realgraph . ]",
    "* benefits of increasing model complexity . * in the first experiment , we show the positive effects on prediction quality of the individual parts of our chosen model .",
    "we randomly split the dataset into 80% training set , 10% validation set and 10% test set .",
    "we train models with increasing complexity and measure their prediction quality in terms of rmse on the 10% held - out data in the test set ( c.f .",
    "figure  [ fig : validation ] ) .",
    "we start with a baseline that only uses the global bias ( the average edge strength ) , followed by a more complex model that uses the global bias as well as vertex - specific bias terms .",
    "finally , we train biased factorization models with @xmath49 .",
    "we choose the hyperparameters using the validation set .",
    "the outcome confirms that an increase in the complexity of our model results in an increase in prediction quality .",
    "the global bias baseline provides an rmse of @xmath50 , adding the vertex - specific bias terms reduces the error to @xmath51 and incorporating the factors gives additional improvements , reducing the error to @xmath52 for @xmath53 .    * visual inspection . *",
    "next , we plot a selection of factor vectors from the @xmath6 matrix of a biased factorization with @xmath54 . two users @xmath8 and @xmath11 will be close in the resulting low - dimensional space if their follower vectors @xmath55 and @xmath56 are roughly linear combinations of each other . due to homophily , we expect twitter users sharing a common interest or characteristic to have a large overlap in followers and therefore to be much closer in the resulting space .",
    "figure  [ fig : plot ] indicates that factorizations produced by factorbird have this property .",
    "we see several clusters of twitter accounts of similar type , e.g. a cluster of european politicians , containing sigmar gabriel ( @sigmargabriel ) and reinhard buetikofer ( @bueti ) from germany as well as arseniy yatsenyuk ( @yatsenuk_ap ) from ukraine .",
    "another cluster consists of popstars such as kanye west ( @kanyewest ) , justin bieber ( @justinbieber ) and the boy - band _ one direction _ ( @onedirection ) . a third one related to u.s .",
    "sports contains emma span ( @emmaspan ) , an editor for baseball at sports illustrated , detroit s football team ( @lions ) and an account of the daytona international speedway ( @disupdates ) .",
    "* scale - out .",
    "* for the scale - out experiments we build a matrix based on a large subset of twitter s realgraph  @xcite consisting of more than 38.5 billion non - zeros ( half of which are synthetic negative examples ) .",
    "the dimensions of the matrix are 229 million @xmath57 195 million . to the best of our knowledge ,",
    "this is the largest dataset on which collaborative filtering experiments have been published so far .",
    "often , the netflix prize dataset  @xcite is used for scale - out tests @xcite .",
    "our dataset is more than two orders of magnitude larger , having approximately 470 times more rows , 11,000 times more columns and 385 times more datapoints .",
    "we run factorbird using 50 instances for the learner machines , provisioned in a mesos cluster , with 16 cores and 16 gb of memory each .",
    "we leverage a shared memcached cluster for our parameter machines , with a guaranteed quota of 5 million commands per second .",
    "we run two passes of hyperparameter search on 80% of the data for 16 different learners with @xmath4 = 5 .",
    "the rank of the factor matrices @xmath5 and @xmath6 is @xmath58 , which means we train a model with approximately @xmath59 parameters . a single sgd pass through the training set finishes in about 2.5 hours  . in this experiment ,",
    "factorbird issues more than 4.2 million commands per second to memcached on average and updates around 400 m parameters per second .",
    "sgd- and als - based matrix factorization techniques for recommender systems have been extensively studied in the context of the netflix prize @xcite .",
    "these techniques have been extended to work on implicit feedback data @xcite and to optimize metrics different from rmse @xcite .",
    "+ a large body of work has been conducted with respect to parallelizing and distributing matrix factorization .",
    "this includes work on the scalability of the algorithms itself , e.g. by introducing biased sampling for sgd to avoid conflicting updates during concurrent execution @xcite or by proving convergence under a minor amount of update conflicts @xcite .",
    "furthermore , distributed implementations have been proposed and evaluated on mapreduce - based @xcite , graph - parallel @xcite and specialized systems @xcite .",
    "in future work , we aim to extend factorbird to a streaming scenario .",
    "we plan to bootstrap factorbird with a factorization that was trained offline and update this factorization model online from a stream of incoming real - time interactions ( e.g. , user follows ) .",
    "furthermore , we would like to experiment with factorizing multiple matrices at once , in order to incorporate different types of interactions in a single factorization model .",
    "another technical issue to work on in the future is fault tolerance . a possible approach to recovery in case of failures",
    "could be to restart learner and parameter machines from asynchronously written checkpoints of the partitions of @xmath5 and @xmath6 which they hold  @xcite .",
    "moreover , we plan to investigate ways to reduce the amount of network traffic caused by factorbird , e.g. by compression , factor vector caching or via a biased sampling of the edges to allow us to use retrieved factor vectors for more than a single update .",
    "we will potentially replace memcached with a custom application to be able to achieve higher throughput and conduct true hogwild - style updates on the parameter machines .",
    "moreover , this would allow us to run aggregations on the parameter machines .",
    "additionally , we would like to implement dynamic load adaption in factorbird to mitigate the negative effects of stragglers on the overall runtime .",
    "we aim to factorize the whole twitter interaction graph with factorbird in upcoming work ."
  ],
  "abstract_text": [
    "<S> we present ` factorbird ' , a prototype of a parameter server approach for factorizing large matrices with stochastic gradient descent - based algorithms . </S>",
    "<S> we designed factorbird to meet the following desiderata : ( a ) scalability to tall and wide matrices with dozens of billions of non - zeros , ( b ) extensibility to different kinds of models and loss functions as long as they can be optimized using stochastic gradient descent ( sgd ) , and ( c ) adaptability to both batch and streaming scenarios . </S>",
    "<S> factorbird uses a parameter server in order to scale to models that exceed the memory of an individual machine , and employs lock - free hogwild!-style learning with a special partitioning scheme to drastically reduce conflicting updates . </S>",
    "<S> we also discuss other aspects of the design of our system such as how to efficiently grid search for hyperparameters at scale . </S>",
    "<S> we present experiments of factorbird on a matrix built from a subset of twitter s interaction graph , consisting of more than 38 billion non - zeros and about 200 million rows and columns , which is to the best of our knowledge the largest matrix on which factorization results have been reported in the literature . </S>"
  ]
}