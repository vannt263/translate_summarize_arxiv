{
  "article_text": [
    "the work reported here extends results presented previously  @xcite , to which the reader is referred for details .",
    "we begin by recalling that the generalised hmc  @xcite algorithm is constructed from two kinds of update for a set of fields @xmath2 and their conjugate momenta @xmath3 .    * molecular dynamics monte carlo : * this consists of three parts : ( 1 )  _ md : _ an approximate integration of hamilton s equations on phase space which is exactly area - preserving and reversible ; @xmath4 where @xmath5 and @xmath6 .",
    "( 2 )  a momentum flip @xmath7 . ( 3 )  _ mc : _ a metropolis accept / reject test .    * partial momentum refreshment : * this mixes the gaussian - distributed momenta @xmath3 with gaussian noise @xmath8 : @xmath9 =      \\twomatrix [ \\cos\\pmomtheta , \\sin\\pmomtheta ;               -\\sin\\pmomtheta , \\cos\\pmomtheta ]        \\cdot f \\twovector[\\pi,\\xi]\\ ] ] the hmc algorithm  @xcite is the special case where @xmath10 .",
    "the l2mc / kramers algorithm @xcite corresponds to choosing arbitrary @xmath11 but mdmc trajectories of a single leapfrog integration step .",
    "the ghmc algorithm has three free parameters , the trajectory length @xmath12 , the momentum mixing angle @xmath11 , and the integration step size  @xmath13 .",
    "these may be chosen arbitrarily without affecting the validity of the method , except for some special values for which the algorithm ceases to be ergodic .",
    "we may adjust these parameters to minimise the cost of a monte carlo computation , and the main goal of this work is to carry out this optimisation procedure for free field theory .",
    "horowitz pointed out that the l2mc algorithm has the advantage of having a higher acceptance rate than hmc for a given step size , but he did not take in to account that it also requires a higher acceptance rate to get the same autocorrelations because the trajectory is reversed at each mc rejection .",
    "it is not obvious a priori which of these effects dominates .",
    "the parameters @xmath12 and @xmath11 may be chosen independently from some distributions @xmath14 and @xmath15 for each trajectory . in the following",
    "we shall consider various choices for the momentum refreshment distribution @xmath16 , but we shall always take a fixed value for @xmath11 .",
    "consider a system of harmonic oscillators @xmath17 for @xmath18 .",
    "the hamiltonian on phase space is @xmath19 .",
    "this describes free field theory in momentum space if the frequencies @xmath20 are chosen as @xmath21",
    "let @xmath22 be a sequence of field configurations generated by an equilibrated ergodic markov process , and let @xmath23 denote the expectation value of some connected operator @xmath24 .",
    "we may define an _ unbiased estimator _",
    "@xmath25 over the finite sequence of configurations by @xmath26 , as usual , we define @xmath27 as the _ autocorrelation function _ for  @xmath24 .",
    "the variance of the estimator @xmath25 is @xmath28,\\ ] ] where @xmath29 is the _ integrated autocorrelation function _ for the operator  @xmath24 and @xmath30 is the _ exponential autocorrelation time_. this result tells us that on average @xmath31 correlated measurements are needed to reduce the variance by the same amount as a single truly independent measurement .      in order to carry out these calculations",
    "we make the simplifying assumption that the acceptance probability @xmath32 for each trajectory may be replaced by its value averaged over phase space @xmath33 ; we neglect correlations between successive trajectories . including such correlations",
    "leads to seemingly intractable complications .",
    "it is not obvious that our assumption corresponds to any systematic approximation except , of course , that it is valid when @xmath34 .",
    "details of the calculation of @xmath35 for leapfrog integration are published elsewhere  @xcite .",
    "if we make the reasonable assumption that the cost of the computation is proportional to the total fictitious ( md ) time for which we have to integrate hamilton s equations , then the cost @xmath36 per independent configuration is proportional to @xmath37 with @xmath38 denoting the average length of a trajectory .",
    "the optimal trajectory length is obtained by minimising the cost , that is by choosing @xmath38 so as to satisfy @xmath39 .",
    "we wish to compare the performance of the hmc , l2mc and ghmc algorithms for one dimensional free field theory . to do this we compare the cost of generating a statistically independent measurement of the magnetic susceptibility @xmath40 , choosing the optimal values for the angle @xmath11 and the average trajectory length @xmath38 .",
    "we can minimise the cost with respect to @xmath11 without having to specify the form of the refresh distribution .",
    "the next step is to minimise the cost with respect to the average trajectory length @xmath41 . strictly speaking",
    "we should note that the acceptance probability @xmath42 is a function of @xmath38 , but to a good approximation we may assume that @xmath35 depends only upon the integration step size @xmath13 _ except _ in the case of very short trajectories .",
    "to proceed further we need to choose a specific form for the momentum refresh distribution . in this section",
    "we will present results for the case of exponentially distributed trajectory lengths , @xmath43 where the parameter @xmath44 is just the inverse average trajectory length @xmath45 .",
    "the cost at the point @xmath46 is @xmath47 @xmath48 this solution is a function of @xmath13 and @xmath42 which are not independent variables , and using the results for @xmath49  @xcite we can compute the cost as a function of @xmath42 as shown in figure  [ fig : laplace - opt ] .            for fixed length trajectories we shall only analyse the case of l2mc for which the trajectory length @xmath53 .",
    "in this case the value of @xmath54 and the corresponding cost are also plotted in figure  [ fig : laplace - opt ] . from this figure",
    "it is clear that the minimum cost occurs for @xmath42 very close to unity , where the scaling variable @xmath55 is very small .",
    "we may then express @xmath56 and @xmath42 as power series in @xmath57 , keeping only the first few terms . from these relations we find that the minimum cost for l2mc is @xmath58 this result tells us that not only does the tuned l2mc algorithm have a dynamical critical exponent @xmath59 , but also it has a volume dependence of exactly the same form as hmc  @xcite .",
    "we may understand why this behaviour occurs rather than the naive @xmath60 by the following simple argument .    if @xmath61 then the system will carry out a random walk backwards and forwards along a trajectory because the momentum , and thus the direction of travel , must be reversed upon a metropolis rejection .",
    "a simple minded analysis is that the average time between rejections must be @xmath62 in order to achieve @xmath63 .",
    "this time is approximately @xmath64 for small @xmath13 we have @xmath65 where @xmath66 is a constant , and hence we must scale @xmath13 so as to keep @xmath67 fixed .",
    "since the l2mc algorithm has a naive dynamical critical exponent @xmath63 , this means that the cost should vary as @xmath68 ."
  ],
  "abstract_text": [
    "<S> we study analytically the computational cost of the generalised hybrid monte carlo ( ghmc ) algorithm for free field theory . </S>",
    "<S> we calculate the autocorrelation functions of operators quadratic in the fields , and optimise the ghmc momentum mixing angle , the trajectory length , and the integration stepsize . </S>",
    "<S> we show that long trajectories are optimal for ghmc , and that standard hmc is much more efficient than algorithms based on the second order langevin ( l2mc ) or kramers equation . we show that contrary to naive expectations hmc and l2mc have the same volume dependence , but their dynamical critical exponents are @xmath0 and @xmath1 respectively . </S>"
  ]
}