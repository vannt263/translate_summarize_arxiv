{
  "article_text": [
    "given a very large dataset of time series ( presumably with some underlying , non - stationary time dependence between each other ) up to time @xmath0 , predict whether a time series @xmath1 will have an anomaly ( defined formally in section [ sec : detecting ] ) at time @xmath2 , moreover when the data stream arrives at time @xmath2 detect anomalies conclusively using a low latency model .",
    "[ sec : introduction ]      the underlying motivation of this piece is the usefulness of anomaly prediction in mission critical components .",
    "a fast anomaly detection platform can by itself be extremely useful for ensuring the reliability of a system , as a result , this problem has been studied extensively by academics and industry experts .",
    "we defer to the authors in @xcite for a literature survey of anomaly detection techniques .    in this work ,",
    "we extend the scope of our approach and goal to tackle anomaly prediction .",
    "most work in the literature has narrowed their focus on anomaly detection because it is in and of itself a very challenging problem .",
    "however , the benefits of an even slight temporal advantage in prediction can have huge impacts on the performance of a system .",
    "the contribution of this work can be categorized in two components :    * a highly accurate , low - latency anomaly detection system(described in detail in section [ sec : model ] ) where we seek to improve on some of the concepts and techniques introduced in @xcite . *",
    "an novel approach to anomaly prediction by modeling a bayesian network structured based on the coefficients of the lag regressors in the anomaly detection system , coupled with bayesian parameter learning to model the conditional dependency structure between the time series .",
    "our ( non - public ) data set consists of @xmath3 time series @xmath4 , sampled every minute for the past year with some underlying non - stationary dependence between each other and human labeled anomaly events by domain experts ( i.e. , the owner of a time series flagged the @xmath5 time series @xmath6 at time point @xmath7 as an anomaly .",
    "although the dataset has some time series source information , our anomaly detection model makes no assumptions about the inter - independence of the time series to allow us to solve the more general problem of having no apriori knowledge .    to illustrate the difficulty of the problem we are trying to solve , in figure [ fig : sample ] we show a subsample of 300 points of one of the time series with labels anomalies denoted by red dots .",
    "clearly the problem involves latent relationships between time series ; observing one time series in isolation is not enough , even for a human , to determine whether a time point is anomalous .",
    "another challenge that we face is that the time series have vastly different probability distributions . to show this",
    ", we normalize the time series and approximate the probability distribution of each time series using kernel density estimation ( with a gaussian kernel and hyperparameter search on the bandwidth ) .",
    "figure [ fig : kde ] shows the estimated probability densities of the 19 most important time series .",
    "a foundational tenet of our model is that our time series have a latent dependence structure between each other . to validate this assumption , for each pair of time series @xmath8 and @xmath9 ( from a selected set of 19 ) , we use kernel density estimation to approximate the marginal distributions of the times series , @xmath10 and @xmath11 , as well as the joint probability distribution @xmath12 .",
    "then we compute the mutual information between the approximate distributions : @xmath13        note that mutual information is a measure of the inherent dependence expressed in the joint distribution of @xmath14 and @xmath15 relative to the joint distribution of @xmath14 and @xmath15 under the assumption of independence , in a concrete sense @xmath16 if and only if x and y are independent random variables .",
    "figure [ fig : mi ] shows a grid plot of the mutual information between 19 selected time series , the @xmath17 coordinate of the figure denotes @xmath18 . as can be seen from the figure below , the time series are highly dependent between each other , moreover , the dependence structure is not uniform across pairs . although we could use a model like this to try to determine latent dependence structure , this method is not scalable as approximating joint probability distributions can be computationally intensive . in section [ sec : model ]",
    "we propose a method that will capture latent dependence structures in a computationally feasible way .",
    "combining previous techniques for anomaly detection @xcite and time series modeling @xcite , we propose an auto regressive distributed lag model on @xmath19 occurs over time rather than all at once . ] with @xmath20 regularization , that is horizontally scalable .",
    "we first define the following :    * @xmath21 : number of time series * @xmath22 : value of time series @xmath23 at timestamp @xmath24 * @xmath25 : the window size used in the auto regression * @xmath26 : the length of the time series    our model is @xmath27 where @xmath28 is the prediction of model @xmath23 at time @xmath0 , @xmath29 is the least squares coefficient vector , and @xmath30 is the model s error on time series @xmath23 at time @xmath0 . in other words , we use all past information from all time series @xmath31 where @xmath32 , @xmath33 for the prediction of the current value @xmath28 in each time series @xmath34 .",
    "now consider concatenating all the predictions for time series @xmath23 ( there are @xmath35 predictions ) . for each time series @xmath23 , we can write the @xmath35 regression predictions as @xmath0 varies in the matrix form : @xmath36 where @xmath37 are the vector of predictions and error term , @xmath38 and @xmath39 .",
    "x is the matrix of concatenated lag regressors .",
    "the model is trained using ordinary least squares method with @xmath20-regularization . in other words , for each time series @xmath34 , we find the parameter vector @xmath29 of length @xmath40 to minimize @xmath41 ^ 2 + \\lambda \\|\\beta^{(i)}\\|_1 \\\\    & =   \\|p^{(i ) } -   x \\beta^{(i)}\\|_{2}^{2 } + \\lambda \\|\\beta^{(i)}\\|_1 \\end{split } \\label{eq : loss}\\ ] ] here , @xmath42 is the error function and @xmath43 is the hyper - parameter which controls the severity of the penalty on complex models .",
    "we do this for each of @xmath21 time series , so we will end up applying ordinary least squares @xmath21 times .",
    "this model offers a sparse regression @xcite that selects only the time series that have predictive value and identifies the temporal correlation between these time series .",
    "the bias and size of the resulting model can be set by modifying the @xmath20 regularization parameter @xmath43 .",
    "this is crucial since the smaller the model is , the less coefficients it has , and thus , the less data it needs to make predictions .",
    "since retrieving data over a network has latency , and low latency when detecting anomalies is a priority in this algorithm , the size of the data to be retrieved should be as small as possible without greatly affecting the model s accuracy .",
    "finally , we store the estimated standard error corresponding to each of the @xmath21 time series to be used later for anomaly detection : @xmath44 here , @xmath45 is the number of non - zero coefficients .",
    "this estimator is motivated by the unbiased estimator for the error variance in a simple linear regression without regularization .",
    "the performance of its can be seen in reid , tibshirani , and friedman @xcite .",
    "the model described in section [ sec : model ] makes real time predictions of all time series in the system as it streams new data . in order to detect anomalies , we compared the prediction the model does at current time @xmath46 , @xmath47 , with its real value coming from the data stream , @xmath48 by running a t - test with the t - statistic defined in equation ( [ eq : t_one_example ] ) : @xmath49 this test gives us a p - value we use to compare against a given threshold p - value - threshold , which is a parameter of the model .",
    "if the p - value we get with t - statistic ( [ eq : t_one_example ] ) is smaller than p - value - threshold , then we are in presence of an anomaly and an alert should be raised .",
    "the parameter p - value - threshold is probably the most important parameter in the model because it directly specifies how sensitive the model is in detecting anomalies .",
    "usually its value is very low .",
    "a value of around 1e-5 would roughly detect an anomaly every 100,000 minutes .",
    "it s important to note that this detection procedure has very low latency because the model is sparse and it only needs to make few calculations in the linear combination .",
    "this is a key feature that puts this model ahead of lower latency , but accurate , models like dpca @xcite .",
    "there is another aspect of real life systems that has not been covered so far . since in real life systems",
    ", random and short spikes can be normal and not anomalies , we use a smoothed anomaly definition as opposed to the naive comparison .",
    "concretely , since these spikes may result in false positive classification of anomalies instead of testing for an anomaly with just the current data - point @xmath48 , we test for an anomaly with a sample of data - points .",
    "this sample of size @xmath50 for time series @xmath23 is defined as @xmath51\\right\\}.\\ ] ] our model checks the probability that @xmath52 comes from a normal distribution with mean @xmath53 by running a t - test with the t - statistic defined by @xmath54 here , @xmath55 is the sample @xmath52 standard error .",
    "as a baseline to highlight the difficulty of the problem we are trying to solve , we implement and test a model that characterizes the time series as multivariate gaussian . more concretely , we build a generative model where time series are generated from an @xmath45-dimensional multivariate gaussian distribution and then set a threshold of the probability of a time series deviating from its mean .",
    "we can visualize this method in figure [ gaussian ] .    the gaussian detector . ]      again , highlighting the difficulty of the problem that we are trying to solve , we present an oracle where we use the test set as the training set and calculate the  test error , \" since we know that the test error should be bounded by the training error , this oracle will allow us to understand the difficulty of our problem definition .",
    "the results shown in section iii , compare the error of the oracle formulation above .",
    "we compare our model s performance with our baseline model ( multivariate gaussian @xcite ) and with the state of the art dynamic principal components model ( dpca ) @xcite .",
    "we run the models on a year worth of server logs data , aggregated as @xmath3 time series . as noted in section [ sec : data ] , the data are labeled by humans ( with domain knowledge ) as anomaly or not anomaly .",
    "we use an auto regression window size @xmath25 of one week .",
    "the output of each model by minute ( anomaly or not anomaly class ) is compared against the real label of the data .",
    "finally , we calculate the @xmath56 score to evaluate the performance of each model .",
    ".accuracy [ cols=\"<,>\",options=\"header \" , ]     table [ table : latency ] shows that our model is faster than the very accurate dpca .",
    "it is also faster than the gaussian model , which is explained by the fact that our model is sparse and does not need to load all the data in the auto - regression window , just the data points that are highly correlated .",
    "since low latency is a crucial component in real time applications , we believe our model s combination between good accuracy and extremely low latency is very powerful .",
    "the second part of this piece deals with building a bayesian network to characterize the probability of an anomaly . recall that for every time series",
    "@xmath23 we run a distributed lag regression ( section [ sec : model ] ) where the regressors are lag variables of @xmath23 and every other time series @xmath32 . the model for time series",
    "@xmath23 is parametrized by a vector @xmath57 of coefficients on the regressors ( the @xmath20 regularization term ensures that the majority of these weights are set to zero ) .",
    "we denote by @xmath58 the vector of non - zero weights for time series @xmath23 , and by @xmath59 the non - zero weight component for time series @xmath23 that corresponds to lag regressors of time series @xmath7 with a lag of @xmath24 ( as in section [ sec : model ] )    similarly , let @xmath60 correspond to the lag _ variables _ with non - zero weights for time series @xmath23 regressing on lag regressors of time series @xmath7 where the lag for that regressor is @xmath24 . let the length of this vector of variables be @xmath61 .",
    "the random variables in our network include : @xmath62 for @xmath63 ; @xmath64 for @xmath63 ; and @xmath60 for @xmath63 , @xmath32 , and @xmath65 .",
    "note that although there may be overlap between @xmath66 and @xmath67 for @xmath68 , our bayesian network considers the set of all these variables .",
    "the lag variables @xmath60 represent the value of the lagged time series , their domain is continuous .",
    "the @xmath69 variables take values @xmath70 representing the presence of an anomaly at time @xmath2 and lastly the @xmath62 variables denote the how much our prediction deviates from the true value of the time series ( see section 6.4 ) .",
    "we can visualize the bayes net in figure [ bayesnet ] .",
    "/[count= ] in 1,2,3 ( input- ) at ( 0 , 7 - 2 * ) @xmath71 ;    /in missing ( input- ) at ( 0 , 7 - 2 * 4 ) ;    /in 4 ( input- ) at ( 0 , 7 - 2 * 5 ) @xmath72 ;    /[count= ] in 1,2,3 ( hidden1- ) at ( 3 , 7 - 2 * ) @xmath73 ;    /in missing ( hidden1- ) at ( 3 , 7 - 2 * 4 ) ;    /in 4 ( hidden1- ) at ( 3 , 7 - 2 * 5 ) @xmath74 ;    in cont , cont , cont , missing , cont ( hidden2- ) at ( 5 , 7 - 2 ) ;    /[count= ] in 1,2,3 ( hidden3- ) at ( 7 , 7 - 2 * ) @xmath75 ;    /in missing ( hidden3- ) at ( 7 , 7 - 2 * 4 ) ;    /in 4 ( hidden3- ) at ( 7 , 7 - 2 * 5 ) @xmath76 ;    in 1 ( output- ) at ( 11 , 7 - 2 ) @xmath62 ;    in 1 ( anomaly- ) at ( 13 , 7 - 2 ) @xmath64 ;    ( output-1 )  ( anomaly-1 ) ;    iin 1, ... ,4 ( hidden3-i )  ( output-1 ) ;    iin 1, ... ,4 ( hidden1-i ) to[out=-15,in=-90 ] ( output-1 ) ;    iin 1, ... ,4",
    "( input - i ) to[out=40,in=120 ] ( output-1 ) ;      the structure of the bayesian network is defined by adding , for each @xmath63 , an edge from node @xmath60 to node @xmath62 for @xmath32 , @xmath65 , and an edge from @xmath62 .",
    "fan and li @xcite show that under the approximation : @xmath77 the unconstrained formulation of the convex optimization problem as equation ( [ eq : loss ] ) can be solved by iterating : @xmath78 where @xmath79 is the estimate of @xmath29 .",
    "moreover , fan and li @xcite draw from the theory of k - step estimators in @xcite which proves that , with a good starting parameter , a on - step iteration can be as efficient as the penalized maximum likelihood estimator , when the newton - rapshon algorithm is used .",
    "we have @xmath80 \\left(x \\beta^{(i ) } + \\epsilon^{(i)}\\right).\\ ] ] thus , there is a direct relation between @xmath81 and @xmath62 .",
    "as @xmath81 determines the possibility of anomaly , the derived relation gives rise to the appearance of the node @xmath62 s in the bayesian network .      the generative model presented above proved to have many challenges . for concreteness , we highlight some of them .",
    "our initial attempt was discretizing the continuous random variables and using maximum likelihood method , an approach which is sometimes used in literature ( @xcite p. 186 ) .",
    "note however that discretization provides a trade - off between accuracy of the approximation and cost of the computation ( @xcite p. 607 ) .",
    "work done using this approach proved to be nonviable . for concreteness , note that the average number of parents ( @xmath60 s ) for each time series @xmath23 are @xmath82 . even for",
    "a coarse discretization scheme of 10 bins , each local cpd table would have @xmath83 entries , even with the decomposibility of factors in bayesian networks , the amount of data needed to accurately estimate the maximum likelihood ( not to mention the computational complexity of collecting the sufficient statistics ) would be intractable .",
    "even if we could estimate the local maximum likelihood , the complexity of many inference algorithms is exponential in the size of the domain of the conditional probability distributions ( e.g. variable elimination ) .",
    "one possible approach that we are exploring is tuning the hyperparameter of the cost of regularization term to restrict the number of variables in each time series to be used in the bayesian network .",
    "another approach we are exploring is using a hybrid model , which contains both continuous and discrete random variables .",
    "this structure in and of itself presents many challenges ( see , e.g. , @xcite , chapter 14 ) . in parcticular",
    ", we can show that inference on this class of networks is _ np - hard _ , even when the network structure is a polytree ( @xcite p. 615 ) .      solving the full bayesian network model is super complicated as discussed above .",
    "thus , we solve a simplified version of this problem .",
    "specifically , we ignore the prior distribution and use the conditional distribution as the posterior distribution .",
    "also , the relation between @xmath62 and @xmath64 is linear .",
    "the detailed procedure for characterizing anomalies is provided below .",
    "we focus on detecting anomalies for one time series only ; other time series in the bayesian network should work in the similar way .",
    "* step 1 : we normalize @xmath62 to be in the @xmath84 interval .",
    "+ @xmath85 + where @xmath86 is the minimum over all observed values of @xmath62 and @xmath87 is the maximum over those values .",
    "the constants @xmath88 and @xmath89 make the normalized @xmath62 fall strictly in @xmath84 .",
    "* step 2 : we specify the relation between @xmath62 and @xmath60 s as a linear gaussian model .",
    "more precisely , we determine the relation @xmath90 this relation can be rewritten as @xmath91 which can then be solved by least square method .",
    "* step 3 : we specify the probability that @xmath92 given @xmath62 that @xmath93 then @xmath64 is characterized as @xmath88 , which is anomaly , if and only if this probability is greater than @xmath94 and @xmath95 otherwise .",
    "the constant @xmath96 can be chosen using cross - validation so as to maximize the @xmath97 score .      in this section",
    "we use four time series as our data set in order to characterize anomalies for one of these four time series .",
    "the reason we drastically reduce the size of the data is because matrix @xmath38 ( as defined in equation ( [ eq : loss ] ) ) and limiting the size of our data set was the only way to be able to compute this in a single machine .",
    "our data set has the following size :    * @xmath98 ( 30 days of minute data ) * @xmath99 ( 5 days of minute data ) * @xmath100 ( four time series )    with this data set we generate a matrix @xmath14 of float elements and dimensions @xmath101 rows by @xmath102 columns .",
    "this matrix has a size of roughly 17 gb ( depending on the platform used ) .",
    "finally , we obtain @xmath103 and the corresponding @xmath97 score is @xmath104 .",
    "this value is certainly not as good as that of dpca or our original discriminative anomaly detection approach , but the comparison is nuanced since above we only processed one time series with a simplified bayesian network ( because of run time complexity ) .",
    "notwithstanding , this result is expected ; discriminative models will typically outperform generative models when the relationships expressed by the generative model only approximates the true underlying generative process ( particularly for our case where we follow some approximations to simplify the model complexity ) in terms of classification error rate .",
    "however , the advantage of our novel approach is that our model is richer . as a generative model",
    ", we can make explicit claims about the process that underlies the time series .",
    "future work may included running inference queries on the generative model ( after tuning it for better performance ) to better understand the underlying process where the data is coming from .",
    "equation ( [ eq : loss ] ) proposes a loss function @xmath42 composed by an @xmath105 error and an @xmath20 regularization .",
    "we think it is worth to test if an @xmath20 error and an @xmath20 regularization performs better .",
    "the reason behind this is that an @xmath20 error is more biased against outliers .",
    "if our regression @xmath106 is less affected by the anomalies in the training data set , then it will predict anomalies more accurately in the testing data .",
    "also , the model proposed in section [ sec : model ] may show false positives when the predictors have an anomaly .",
    "this is because after an anomaly exists , it still stays in the data that will be later use as an input in out regression .",
    "we did not see this in our experiment , but it is something we would like to test and evaluate ."
  ],
  "abstract_text": [
    "<S> we develop a supervised machine learning model that detects anomalies in systems in real time . </S>",
    "<S> our model processes unbounded streams of data into time series which then form the basis of a low - latency anomaly detection model . moreover , we extend our preliminary goal of just anomaly detection to simultaneous anomaly prediction . </S>",
    "<S> we approach this very challenging problem by developing a bayesian network framework that captures the information about the parameters of the lagged regressors calibrated in the first part of our approach and use this structure to learn local conditional probability distributions . </S>"
  ]
}