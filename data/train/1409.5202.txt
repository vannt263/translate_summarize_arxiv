{
  "article_text": [
    "markov jump linear systems is a class of switched linear systems whose switching is governed by a time - homogeneous markov process , called the markov state , and have been attracting continuing attention due to its simplicity as well as its ability of modeling systems in application such as robotic systems  @xcite , economy  @xcite , and networked systems  @xcite .",
    "it is known that , under the assumption that controllers can observe the markov state at any time instants , we can perform standard types of controller synthesis for markov jump linear systems such as state - feedback stabilization , quadratic optimal control , @xmath0 optimal control , and @xmath1 optimal control  ( see , e.g. , the monograph  @xcite ) .",
    "however it is often not realistic to assume that controllers _",
    "always _ have an access to the markov state and this fact has been motivating the investigation of the effect of limited or uncertain observations of the markov state . for example , the authors in  @xcite study the stabilization and @xmath0-control of discrete - time markov jump linear systems when the markov - state space is partitioned into subsets , called clusters , and an observation of the markov state only tells us to which cluster the markov - state belongs .",
    "similar studies in the continuous - time settings can be found in  @xcite . under an extreme situation when the markov - state space has only one cluster , i.e. , when one can not observe the markov state , vargas et al .",
    "@xcite investigate quadratic optimal control problems .",
    "another but not the only source of uncertainty comes from the randomness of the time instants at which one can observe the markov states . for the case",
    "when observation times follow a renewal process , the authors in  @xcite design almost - surely stabilizing state - feedback controllers whose gains are reset whenever an observation is performed .",
    "for the special case when observations are performed periodically , the same authors  @xcite derive stabilizing ( in the mean square sense ) state - feedback controllers using lyapunov - like functions .",
    "we here remark that other various methods such as , for example , adaptive strategies  @xcite could be used to study this type of problems , though we do not give a detailed survey of the field in this paper .    in this paper",
    "we propose a unified method for designing stabilizing state - feedback gains for a discrete - time markov jump linear system when the time instants at which a controller performs an observation of the markov state , called an observation process , is time - randomized by another markov chain .",
    "this markov chain can be used to model various types of observation processes including periodic observations @xcite and observations following a renewal process  @xcite . by embedding the original markov - state to another one ,",
    "we transform a markov jump linear system with time - randomized observations to another one with clustered observations , for which we apply the result in @xcite and derive linear matrix inequalities for finding stabilizing state - feedback gains .",
    "this paper is organized as follows . after preparing the notations used in this paper , in section  [ sec : mjls ]",
    "we give a brief overview of markov jump linear systems and their stabilization .",
    "then in section  [ sec : partobsv ] we formulate the stabilization problem with time - random observation of the markov state . after showing an embedding of the markov state to another markov chain in section  [ sec : hidden ] , we in section  [ sec : design ] derive linear matrix inequalities for the design of feedback gains .",
    "the notation used in this paper is standard .",
    "let @xmath2 denote the set of nonnegative integers .",
    "let @xmath3 and @xmath4 denote the vector spaces of real @xmath5-vectors and @xmath6 matrices , respectively .",
    "by @xmath7 we denote the euclidean norm on  @xmath3 .",
    "@xmath8 will be used to denote the probability of an event .",
    "the probability of an event conditional on an event @xmath9 is denoted by @xmath10 .",
    "expectations are denoted by @xmath11 $ ] .",
    "characteristic functions are denoted by @xmath12 . for a positive integer @xmath13",
    "we define the set @xmath14 } = \\{1 , \\dotsc , n\\}$ ] . for a positive integer @xmath15 define @xmath16 as the unique integer in @xmath17}$ ] such that @xmath18 is an integer multiple of @xmath15 .",
    "when a real symmetric matrix @xmath19 is positive definite we write @xmath20 .",
    "the aim of this section is to give a brief overview of markov jump linear systems in discrete - time  @xcite and also recall some basic definitions of their stability and stabilizability .",
    "let @xmath5 , @xmath21 , and @xmath13 be positive integers .",
    "let @xmath22 and @xmath23 . also let @xmath24 be a time - homogeneous markov chain taking its values in @xmath25}$ ] and having the transition probability matrix  @xmath26 .",
    "we call the stochastic difference equation @xmath27 a markov jump linear system  @xcite .",
    "we call @xmath28 the _ markov - state space _ of @xmath29 . both the initial state @xmath30 and the initial markov state @xmath31",
    "are assumed to be constants .",
    "the ( internal ) mean square stability of @xmath29 is defined in the following standard way .",
    "[ defn : mss : usual ] @xmath29 is said to be _",
    "mean square stable _ if there exist @xmath32 and @xmath33 such that the solution @xmath34 of satisfies @xmath35 < c\\epsilon^k{\\lvert x_0 \\rvert}^2\\ ] ] for all @xmath36 and @xmath37 , provided @xmath38 .    in this paper",
    "we mainly discuss the stabilization of @xmath29 via state - feedback controllers .",
    "if one assumes that the controller has an exact access to the markov state  @xmath39 at each time @xmath40 , then we can consider the following mode - dependent controller of the form @xmath41 where @xmath42 .",
    "we say that the state - feedback controller   @xmath29 if the following markov jump linear system without input @xmath43 is mean square stable .",
    "another scenario , which is closely related to the current paper , is the stabilization with so - called cluster observations of markov states  ( see @xcite ) . following the notation in @xcite , we assume that the markov state space  @xmath28 is decomposed as @xmath44 , where @xmath45 and @xmath46 are sets",
    ". thus each can be represented as @xmath47 by some @xmath48 and @xmath49 .",
    "the set @xmath45 ( @xmath46 ) represents the unobservable ( observable , respectively ) part of the markov state space  @xmath28 .",
    "let us define the projection  @xmath50 by @xmath51 .",
    "then , the state - feedback controller that can observe only the observable part of the markov state must take the form @xmath52 where @xmath53 for each @xmath54 .",
    "we say that this feedback controller stabilizes @xmath29 if the solution  @xmath34 of the closed loop equation @xmath55 satisfies the condition in definition  [ defn : mss : usual ] .",
    "the following proposition  ( * ? ? ? * theorem  6 ) gives linear matrix inequalities whose solutions yield stabilizing feedback gains for feedback control   with clustered observations . in order to state the proposition , for @xmath56 and a family of matrices  @xmath57",
    "we define the matrix @xmath58 by @xmath59 .",
    "[ prop : doval ] assume that the matrices @xmath60 , @xmath61 , and @xmath62 ( @xmath63 , @xmath64 ) satisfy the matrix linear inequalities @xmath65 for all @xmath66 and @xmath64 .",
    "define @xmath67 for each @xmath54 .",
    "then the feedback controller   stabilizes @xmath29 .",
    "the aim of this section is to state the stabilization problem with time - randomly observed markov states .",
    "we in particular introduce a novel class of random observation processes induced by markov chains .",
    "in particular the class contains observation processes that are not renewal processes .",
    "let us begin with the next general definition .",
    "an @xmath2-valued increasing stochastic process @xmath68 is called an _",
    "observation process_.    observation processes will be used to model the times at which a controller can access the markov state . given an observation process @xmath69 , define the stochastic process   by @xmath70 where @xmath71 is an arbitrary integer .",
    "this @xmath72 represents , for each time  @xmath73 , the most recent time the markov state was observed . in particular we have @xmath74 for every @xmath75 .",
    "notice that we augment the process with the arbitrary negative integer @xmath76 when @xmath77 because , before the time @xmath78 , no observation is performed yet .",
    "we then define another stochastic process  @xmath79 taking its values in @xmath14}$ ] by @xmath80 where @xmath81}$ ] is arbitrary .     and its observed version  @xmath82 . until the first observation time  @xmath83",
    ", the observed version is temporarily set to @xmath84.,width=227 ]    the process @xmath82 represents the most - updated information of the markov state that is available for a controller .",
    "we again notice that , by the same reason as above , the process @xmath82 is augmented by an arbitrary @xmath85 before the time @xmath86 , i.e. , before the first observation is performed .",
    "see fig .",
    "[ fig : r_and_signa ] for an illustration .    in this paper",
    "we assume that the controller has an access to , at each time @xmath40 , the state variable  @xmath87 , the most recent observation  @xmath88 of the markov state  @xmath89 , and @xmath90 , which is the time elapsed since the last observation",
    ". then we construct the state - feedback controller of the form @xmath91 where @xmath92 for each @xmath93}$ ] and @xmath94}$ ] .",
    "the first argument @xmath88 in allows the gain to be reset whenever a controller performs an observation of the markov state as in  @xcite .",
    "the second argument allows the controller to change feedback gains between two consecutive observations rather than keeping them to be constant , which can enhance the performance of the controller  @xcite .",
    "the reason for taking the operator @xmath95 in the second argument of @xmath96 is that , otherwise , we have to design infinitely many matrices @xmath97 where @xmath98 could be any nonnegative numbers .",
    "taking the operator @xmath95 forces @xmath98 to be in the finite set @xmath17}$ ] , which turns out to make our stabilization problem solvable in finite time .    combining and we obtain the closed loop equation @xmath99 extending definition  [ defn : mss : usual ]",
    ", we define the mean square stability of the system @xmath100 as follows .",
    "[ defn : ] let @xmath101 be a set of observation processes .",
    "we say that the pair @xmath102 is _ mean square stable _ if there exist @xmath32 and @xmath103 such that the solution  @xmath34 of satisfies for all @xmath104 , @xmath105}$ ] , @xmath106 , @xmath107}$ ] , and  .",
    "the feedback control is said to stabilize @xmath108 if @xmath109 is mean square stable .",
    "in this paper we deal with a class of observation processes induced by time - homogeneous markov chains . in order to introduce the class , we first need to define observation processes induced by deterministic sequences .",
    "let @xmath110}$ ] be an arbitrary sequence and let @xmath111 be a subset of @xmath112}$ ] .",
    "assume that @xmath113 intersects with @xmath114 infinitely many times , namely , that the set @xmath115 is infinite .",
    "then define the infinite sequence  @xmath116 as the one obtained by increasingly ordering the numbers in the infinite set  @xmath117 .",
    "thus , the sequence @xmath116 consists of the times @xmath73 at which the sequence @xmath113 intersects with @xmath111 .",
    "for example , the observation time instants  @xmath83 , @xmath118 , and @xmath119 shown in fig .  [ fig : r_and_signa ] are induced by the sequence  @xmath113 shown in fig .",
    "[ fig : s_and_t ] with @xmath120 .     and the induced observation time instants @xmath69 .",
    "an observation occurs whenever @xmath113 enters the set @xmath120.,width=227 ]    then we extend the above definition to markov chains as follows .",
    "let @xmath113 be a time - homogeneous markov chain taking its values in @xmath112}$ ] and let @xmath121}$ ] be a nonempty set that is recurrent with respect to the markov chain  @xmath113 .",
    "we define a family of observation processes @xmath122 by @xmath123 where @xmath124 denotes the markov chain  @xmath113 when its initial state equals @xmath125 .",
    "notice that , since @xmath111 is recurrent , @xmath113 intersects with @xmath111 infinitely many times with probability one and thereby @xmath126 is well defined .",
    "the following examples illustrate that the family @xmath127 can express various types of observation processes .",
    "[ ex : per : fail ] let @xmath128 be a positive integer .",
    "let @xmath129 and the transition probability matrix of @xmath113 be @xmath130\\in \\mathbb{r}^{(\\tau+1)\\times ( \\tau+1)},\\ ] ] where zero entries are omitted .",
    "then we can see that , if @xmath131 , then the difference @xmath132 of observation times independently follow the distribution @xmath133 on @xmath2 that is concentrated on the set @xmath134 and satisfies for every @xmath135 .",
    "in other words , this observation process expresses the observation of every @xmath128 time units with the probability of failure @xmath136 at each observation .",
    "in particular , if @xmath137 , then the observation process gives the observation with period @xmath128 , which is considered in  @xcite .",
    "let @xmath133 be an arbitrary distribution on @xmath138 having finite support .",
    "then there exist @xmath139 and @xmath140 such that @xmath141 and @xmath142 for every @xmath143 .",
    "define the positive integers  @xmath144 , @xmath145 , @xmath146 recursively by @xmath147 for @xmath148 .",
    "let @xmath113 be the markov chain having the transition probability matrix @xmath149 \\in \\mathbb{r}^{\\tau \\times \\tau } , \\ ] ] where zero entries are omitted again .",
    "also let @xmath150 .",
    "we can see that , if @xmath151 , then difference @xmath132 of observation times independently follow the distribution @xmath133 and therefore @xmath69 forms a renewal process .",
    "finally we present a simple example of observation processes  @xmath126 that are not renewal processes and therefore can not be treated by the method in  @xcite .",
    "let the transition probability matrix of @xmath113 be @xmath152 and also let @xmath153 .",
    "if @xmath154 , then we have @xmath155 , which can not be a renewal process because the differences of two consecutive observation times equal @xmath156 almost surely .",
    "now we state the main problem studied in this paper .",
    "[ prb : ] given a markov jump linear system  @xmath29 and a family of observation processes @xmath126 induced by a time - homogeneous markov chain  @xmath113 , find feedback gains that stabilize the pair  @xmath157 .",
    "the difficulty of solving problem  [ prb : ] is that the system  @xmath158 in is no longer a standard markov jump linear system and thus the techniques established in the literature  @xcite can not be used to it .",
    "also , by the generality of the observation processes  @xmath122 discussed in the previous section , we also can not use the results recently proposed in  @xcite .",
    "the aim of this section is to show that we can embed the markov state process  @xmath89 to another markov chain , with which we can express @xmath158 as a standard markov jump linear system .",
    "this fact will be used in the next section to reduce problem  [ prb : ] to the stabilization of a markov jump linear system with clustered observations .",
    "let @xmath89 be the markov state of the markov jump linear system  @xmath29 .",
    "let @xmath113 be the time - homogeneous markov chain that induces the family @xmath126 of observation processes .",
    "we let @xmath159_{i , j } \\in \\mathbb{r}^{n\\times n}$ ] and @xmath160_{i , j}\\in \\mathbb{r}^{m\\times m}$ ] denote the transition probability matrices of @xmath89 and @xmath113 , respectively .",
    "the next proposition is the main result of this section .",
    "[ prop : markov ] define @xmath161 } \\times { [ m ] } \\times { [ n ] } \\times { [ t]}.\\ ] ] then the @xmath162-valued stochastic process @xmath163 defined by @xmath164 is a time - homogeneous markov chain .",
    "moreover its transition probabilities are given by , for all @xmath165 and @xmath166 in @xmath162 , @xmath167    let @xmath168 and @xmath169 be arbitrary .",
    "take arbitrary @xmath170 ( @xmath171 ) .",
    "for each @xmath172 define the events @xmath173 by @xmath174 and @xmath175 under the assumption that @xmath176 is not the null set , we need to evaluate the conditional probability @xmath177 remark that this assumption implies that @xmath178 because otherwise @xmath176 equals a null set .",
    "first assume that @xmath179 .",
    "then we have so that , by the definition of @xmath126 , an observation occurs at time  @xmath180 , i.e. , we have @xmath181 and .",
    "this implies that @xmath182_t = \\delta_{k+1}\\}.   \\end{multlined}\\ ] ] therefore , since @xmath183 , @xmath184and hence @xmath185 the probability appearing in the last term of this equation can be computed as @xmath186 where we used the fact that both @xmath89 and @xmath113 are time - homogeneous markov chains .",
    "thus equations , , and conclude that , for the case of @xmath187 , @xmath188    then consider the case where @xmath189 . in this case ,",
    "markov state @xmath89 is not observed at time @xmath180 so that we have @xmath190 and @xmath191 .",
    "therefore , using equations , in the same way as we derived we can show that @xmath192 and hence @xmath193 therefore , equations , , and show that , for @xmath194 , @xmath195    since the probabilities and do not depend on @xmath196 , letting @xmath197 and @xmath198 in and we obtain @xmath199 for every @xmath40 .",
    "this shows that @xmath163 is a markov chain since @xmath200 were arbitrarily taken .",
    "moreover , since the probabilities and do not depend on @xmath73 , we conclude that the markov chain @xmath163 is time - homogeneous and its transition probabilities are actually given by .",
    "it is observed in @xcite that the process @xmath201 itself is indeed a markov chain but not time - homogeneous .",
    "proposition  [ prop : markov ] shows that , however , augmenting the third and fourth components in enables us to construct a time - homogeneous markov chain , which plays a crucial role in the next section .",
    "in this section we show a set of feedback gains @xmath96 that stabilizes @xmath202 can be found by solving a set of linear matrix inequalities . for the proof we will use the reduction of @xmath158 , which is not necessarily a markov jump linear system , to a markov jump linear system with markov state evolving in the same way as the markov chain  @xmath163 presented in the last section .",
    "we define @xmath203 as the time - homogeneous markov chain taking its values in @xmath162 and having the same transition probability as @xmath163 , i.e. , we assume that the transition probability @xmath204 of @xmath203 is given by @xmath205 where @xmath166 and @xmath206 . the following lemma will be used to prove the main result of this paper .",
    "[ lem : equvchains ] let @xmath105}$ ] , @xmath207}$ ] , @xmath106 , and @xmath107}$ ] be arbitrary .",
    "then there exists @xmath208 such that @xmath209 where the arguments following the time @xmath73 denote the initial conditions of @xmath203 and @xmath163 .",
    "now , for the markov jump linear system @xmath29 , we introduce another markov jump linear system having @xmath203 as its markov state as follows .",
    "we define the matrices   and for each @xmath210 by @xmath211 and @xmath212 .",
    "then define the markov jump linear system @xmath213 by @xmath214 with the initial states @xmath215 and @xmath216 .",
    "let us also consider the following standard state - feedback controller @xmath217 where @xmath218 for each @xmath219 .",
    "then we obtain the closed loop equation @xmath220 the next theorem is the first major result of this paper .",
    "assume that @xmath221 stabilizes @xmath213 and satisfies @xmath222 for all @xmath223}$ ] , @xmath224}$ ] , @xmath225}$ ] , and @xmath94}$ ] .",
    "for each @xmath226}$ ] and @xmath94}$ ] define @xmath227 by @xmath228 then @xmath96 stabilizes @xmath157 .    assume that @xmath221 stabilizes @xmath213 and satisfies .",
    "then there exist @xmath32 and @xmath229 such that the solution @xmath230 of @xmath231 satisfies @xmath232 < c \\epsilon^k { \\lvert \\bar x_0 \\rvert}^2 $ ] .",
    "define @xmath96 by and let us show that @xmath96 stabilizes @xmath157 .",
    "let @xmath233 , @xmath105}$ ] , @xmath207}$ ] , @xmath106 , @xmath107}$ ] be arbitrary .",
    "by lemma  [ lem : equvchains ] , we can take the corresponding @xmath208 such that holds .",
    "then we can see that @xmath234 this shows @xmath235 provided the initial states of @xmath29 and @xmath213 coincide as @xmath236 .",
    "therefore , since @xmath231 is mean square stable , we obtain @xmath237 < c\\epsilon^k { \\lvert x_0 \\rvert}^2 $ ] for every @xmath73 .",
    "hence @xmath96 stabilizes @xmath157 , as desired .",
    "the constraint leads us to decompose @xmath162 into the unobservable part  @xmath238 } \\times { [ m]}$ ] and the observable part   as @xmath239 .",
    "then the stabilization of  @xmath213 with the feedback control   satisfying the constraint   on feedback gains is equivalent to the stabilization of @xmath213 via clustered observation  @xcite reviewed in section  [ sec : mjls ] .",
    "therefore , using proposition  [ prop : doval ] we immediately obtain the next theorem .",
    "[ thm : lmi ] for @xmath240 ( @xmath241 ) define  @xmath242 .",
    "assume that @xmath243 , @xmath244 , and @xmath245 satisfy the linear matrix inequality @xmath246 for all @xmath247 .",
    "for each @xmath225}$ ] and @xmath94}$ ] define @xmath248 .",
    "then @xmath96 stabilizes @xmath157 .",
    "let @xmath249 and consider the markov jump linear system @xmath29 given by the matrices @xmath250 let the transition probabilities of the markov state  @xmath89 be given by @xmath251 for every @xmath172 and @xmath252 for all distinct @xmath172 and  @xmath253 .",
    "assume that a controller tries an observation of the markov state every 4 time units , but it fails with probability  @xmath254 .",
    "the corresponding markov chain  @xmath113 can be realized by letting and @xmath255 in example  [ ex : per : fail ] .",
    "also we set @xmath256 . solving the linear matrix inequalities   we obtain stabilizing feedback gains .",
    "we construct 100 sample paths of the solution @xmath34 of the stabilized system  @xmath158 .",
    "[ fig : graph3 ] and  [ fig : graph3log ] show the sample average and the sample paths of @xmath257 , respectively .",
    "we can see that , even though the observation of the markov state is not necessarily performed periodically , the designed controller attains stabilization .",
    ", width=226 ]      the controller designed by theorem  [ thm : lmi ] is stronger than the ones in  @xcite in the following sense : though the controller by theorem  [ thm : lmi ] does not necessarily need to know the markov state at the initial time @xmath258 for stabilization , the ones designed in  @xcite are supposed to know the markov state at @xmath258 for stabilization .",
    "the aim of this subsection is to show that the above two different types of stabilization are in fact equivalent provided the first observation time , @xmath83 , is uniformly bounded",
    ".    for a family of observation processes @xmath101 , we define another family of observation processes @xmath259 by .",
    "@xmath260 expresses the set of all observation processes in @xmath101 that observes the markov state at @xmath258 . in particular we can see that @xmath261 the difference from is that the initial state  @xmath125 of the markov chain  @xmath113 is confined to be in @xmath111 .    then we can prove the following theorem .",
    "[ thm : weak ] assume that there exists @xmath262 such that @xmath263 with probability one",
    ". then @xmath96 stabilizes @xmath264 if and only if @xmath96 stabilizes @xmath265 .",
    "sketch of the proof if @xmath96 stabilizes @xmath157 then @xmath96 clearly stabilizes @xmath266 because @xmath267 .",
    "next assume that @xmath96 stabilizes @xmath266 . then . by the above observation",
    ", there exist @xmath32 and @xmath229 such that the solution @xmath34 of @xmath158 satisfies for all , , and @xmath268 .",
    "let us show that @xmath96 stabilizes @xmath157 .",
    "let @xmath269 , , @xmath270}$ ] , @xmath81}$ ] , and @xmath71 be arbitrary . by the assumption , there exist such that @xmath271 and @xmath272 . fix a and",
    "consider the case @xmath273 .",
    "let @xmath274 } , \\delta\\in { [ n ] } , \\gamma \\in { [ t]}}$ ] , where @xmath7 denotes the maximum singular value of a matrix",
    ". then one can show @xmath275 \\leq c'^{2k_0 } { \\lvert x_0 \\rvert}^2 $ ] . since @xmath34 follows the stabilized dynamics after @xmath276",
    "we obtain @xmath237 \\leq c \\epsilon^{k - k_0 } c'^{2k_0 } { \\lvert x_0 \\rvert}^2 $ ] , which happens with probability  @xmath277 .",
    "therefore , taking the summation of this inequality with respect to @xmath196 we can actually derive hence @xmath96 stabilizes @xmath157 .",
    "in this paper we studied the state - feedback stabilization of discrete - time markov jump linear systems when the observation of the markov - state by a controller is time - randomized by another markov chain . using an embedding of the markov state to another markov chain , we transformed the markov jump linear system with time - randomized observations to the one with clustered observations .",
    "based on this transformation we derived linear matrix inequalities for finding state - feedback stabilizing gains .",
    "the proposed method can , in a unified way , treat time - random observations including periodic and renewal - type observations studied in the literature . a numerical example is presented to show the effectiveness of the proposed method .",
    "a.  n. vargas , w.  furloni , and j.  b. do  val , `` second moment constraints and the control problem of markov jump linear systems , '' _ numerical linear algebra with applications _",
    "20 , pp . 357368 , 2013 .",
    "a.  n. vargas , e.  f. costa , and j.  b. do  val , `` on the control of markov jump linear systems with no mode observation : application to a dc motor device , '' _ international journal of robust and nonlinear control _",
    "23 , pp . 11361150 , 2013 .",
    "b.  bercu , f.  dufour , and g.  yin , `` almost sure stabilization for feedback controls of regime - switching linear systems with a hidden markov chain , '' _ ieee transactions on automatic control _ , vol .  54 , pp .",
    "21142125 , 2009 ."
  ],
  "abstract_text": [
    "<S> in this paper we study the state - feedback stabilization of a discrete - time markov jump linear system when the observation of the markov chain of the system , called the markov state , is time - randomized by another markov chain . embedding the markov state into an extended markov chain , </S>",
    "<S> we transform the given system with time - randomized observations to another one having the enlarged markov - state space but with so - called cluster observations of markov states . </S>",
    "<S> based on this transformation we propose linear matrix inequalities for designing stabilizing state - feedback gains for the original markov jump linear systems . </S>",
    "<S> the proposed method can treat both periodic observations and many of renewal - type observations in a unified manner , which are studied in the literature using different approaches . </S>",
    "<S> a numerical example is provided to demonstrate the obtained result . </S>"
  ]
}