{
  "article_text": [
    "in machine learning , artificial neural networks ( anns ) are a family of computational models inspired from neuroscience , which can be used to approximate functions that may depend on a huge number of inputs and are usually unknown . as efficient models for statistical pattern recognition ,",
    "anns have been widely applied in applications .",
    "a most common model is the feed - forward neural network ( fnn ) [ 1 ] , in which the information moves along one direction , forward , from the input neurons , through the hidden neurons ( if any ) and to the output neurons .",
    "moreover , there is no cycle or loop in a ffn , which is different from another kind of neural network called recurrent neural network ( rnn ) .    due to the powerful abilities to deal with highly complex and ill - defined problems ,",
    "anns have been studied for many years in the hope of achieving human - like performance in lots of fields such as regression analysis , object classification , data / signal processing , robotics and control .",
    "for example , in recent years , deep anns [ 2 ] have won numerous competitions in the field of pattern recognition and machine learning .",
    "the deep anns have greatly advanced the application fields such as visual recognition systems , medical diagnosis , financial applications , data mining , e - mail spam filtering , game - playing and decision making .",
    "just recently , with deep anns , a google computer program named _ alphago _ [ 3 ] even beat a professional human player at the 2-player board game of _ go _ , which has around @xmath0 possible positions .",
    "steganography [ 4 ] is referred to the art of embedding secret data into innocent objects such as image , video and audio .",
    "it is required that , there should be impossible for an eavesdropper to distinguish ordinary objects and objects containing secret data .",
    "unlike cryptography , since steganography even conceals the presence of covert communication , it has been widely used to provide secret communication .",
    "steganographic algorithms for images usually embed secret data into a cover image by altering the pixel values without introducing obvious artifacts .",
    "as the resultant image called _ stego image _ hides the existence of secret data , only the recipient who owns the secret key can perfectly retrieve the embedded information .",
    ".,width=336 ]    for a steganographic system , the stego image can be considered as the output of an encoding function that relies on the cover image , the secret data and a secret key .",
    "the decoding function aims to reconstruct the secret data according to the stego image and secret key .",
    "since the anns have the ability to approximate functions from observations , it is straightforward to consider the anns for steganography . and",
    ", it is very likely to apply the anns for the data embedding or data extraction process of a steganographic system .",
    "for example , a steganographic algorithm may adopt the anns to find the optimal data embedding strategy ( e.g. , adaptively select the most suitable pixels to be embedded ) , while subjected to an upper - bounded distortion . also , since the anns has the ability to approximate very complex functions , any steganographic attacker should not be able to generate a legal stego image , extract the hidden information , or even hardly locate the probably embedded image regions .",
    "furthermore , in future , there may be no need for a human to design the detailed data embedding and data extraction functions , as they may be done by training the anns to achieve these features . while the above scenarios may seem to be incredible or even impossible",
    ", it would be very promising and valuable to pay attention to the anns for steganography .    in this paper",
    ", we aim to implement the lsb substitution and matrix coding steganography with the fnns .",
    "though both can be easily done by applying the fnns , we can draw some valuable arguments from the experiments , which leads us to study ann - based steganography in a good direction .",
    "the rest of this paper are organized as follows .",
    "section ii shows the details to achieve the lsb substitution with the fnns , followed by the matrix coding steganography in section iii .",
    "we present some discussion and analysis in section iv .",
    "finally , we conclude this paper in section v.",
    "let @xmath1 and @xmath2 denote the cover vector and stego vector .",
    "the lsb substitution hides the secret data by simply replacing the lsb of each @xmath3 with the secret bit to be embedded .",
    "hence , the data embedding function can be described as @xmath4 here , @xmath5 denotes the _ k_-th secret bit to be embedded .",
    "when an intended receiver acquires @xmath6 , he is able to retrieve the hidden information by collecting the stego lsbs , i.e. , @xmath7    note that , though theoretically @xmath3 can be arbitrary integer , only the lsb of @xmath3 works for data hiding .",
    "it indicates that , instead of @xmath3 , we can consider the lsbs of * x * as the input . for simplicity",
    ", we will consider both * x * and * y * as a binary vector .",
    "a single - layer fnn includes one input layer and one output layer of processing units .",
    "there is no hidden layers between the input layer and output layer .",
    "let @xmath8 and @xmath9 denote the output vector of the @xmath10-th layer and the input weights for the @xmath10-th layer .",
    "for example , for a single - layer fnn , @xmath11 and @xmath12 represent the output vector of the input layer ( or called the input vector of the output layer ) , and the input weights for the output layer . for the @xmath10-th layer in a fully - connected fnn",
    ", we have @xmath13 where @xmath14 represents the activation function .",
    "note that , in default , we consider @xmath15 as the bias input .",
    "= 0.01).,width=336 ]    we present two methods to implement the lsb substitution with single - layer fnns .",
    "one is to set @xmath16 , and @xmath17 for the other one .",
    "we use @xmath18 as the activation function in default . note that , we can also use other activation functions . in case @xmath16 , we process each of the cover elements \\{@xmath19 } with the identical single - layer fnn to produce the _ n _ stego elements \\{@xmath20}. fig . 1 shows the sketch for the case @xmath16 . in fig . 1",
    ", @xmath21 denote the bias inputs . for each triple - input ( @xmath22 ) , ( @xmath23 ) , we aim to find out reliable triple - weights ( @xmath24 ) such that @xmath25 can be correctly determined",
    ". all the triple - inputs will have the identical triple - weights , meaning that , we should only compute ( @xmath26 ) .",
    "we are to compute the weights by training observations . for consistency , we think of the used observations as the training data , and the testing data consist of * x * and * m * = @xmath27 .",
    "we employ randomly generated observations to quickly find the weights .",
    "specifically , we randomly generate a certain number of bit - pairs ( @xmath28 ) , ( @xmath29 ) , and their target outputs @xmath30 , ( @xmath29 ) . for each @xmath31",
    ", it should meet @xmath32 as required by the lsb substitution operation .",
    "the randomly generated training data are individually fed into the single - layer fnn to train the required parameters .",
    "due to its simplicity , during the training , we use the _ hill climbing _ [ 5 ] optimization technique to optimize the parameters , which can be described as algorithm 1 .",
    "2 shows the training weights versus iteration number by applying the algorithm 1 for two sets of randomly initialized weights , where ( @xmath26 ) converge to ( 0 , 1 , 0 ) .",
    "the optimized weights can be used for data hiding .",
    "for example , with randomly initialized weights ( 0.482945 , 0.979194 , 0.550665 ) , we have the optimized weights ( 0.000410723 , 0.999325 , 2.03115@xmath33 ) by applying 6000 iterations . for any @xmath34 and @xmath35",
    ", we have @xmath36 . by setting a threshold such as @xmath37",
    ", we can obtain the stego value @xmath38 as @xmath39 .    in case @xmath17",
    ", we divide both * x * and * m * into @xmath40 disjoint subvectors ( suppose that @xmath41 is divisible by @xmath42 ) .",
    "3 shows the sketch to implement the lsb substitution with single - layer fnns in case @xmath17 .",
    "similarly , we use randomly generated training data and employ the gradient descent [ 6 ] technique to optimize the weights .",
    "our experimental results have shown the fnn architecture in fig .",
    "3 can be successfully applied for data hiding .",
    "for example , _ appendix a _ provides the python code to optimize the weights in case @xmath43 .",
    "+  + algorithm 1 : optimizing weights with hill climbing technique .",
    "initialize * w * = ( @xmath26 ) @xmath44 ^ 3 $ ] and @xmath45 + 2 .  * for * @xmath46 * do * + 3 .  randomly set @xmath47 + 4 .",
    "set @xmath48 + 5 .",
    "* for * all possible * e * = ( @xmath49 ) @xmath50 * do * + 6 .  *",
    "if * @xmath51 * do * + 7 .  set @xmath52 + 8 .",
    "set * w * = @xmath53 + 9 .",
    "* return * * w * +       a multi - layer fnn has one input layer , one output layer , and one or more hidden layers of processing units .",
    "the lsb substitution can be also implemented by using multi - layer fnns ( in fact , the single - layer fnn can be considered as the special case of a multi - layer fnn ) .",
    "4 shows a multi - layer fnn designed for lsb substitution .",
    "the designed fnn can be successfully used for data hiding .",
    "for example , _ appendix b _ provides the python code to optimize the weights for the multi - layer fnn with one hidden layer .",
    "it is noted that , when applied to gray - scaled images , we can use the whole pixel lsbs to be embedded as a part of the input of a multi - layer fnn .",
    "also , all the pixel lsbs to be embedded can be divided into disjoint pixel vectors and then separately fed into the identical multi - layer fnn .",
    "we here prefer to recommend the latter since the training time will be reduced , comparing with the former .",
    "the well - known matrix coding used in [ 7 ] is an application of the binary hamming code .",
    "it can embed _ k _ bits into @xmath54 cover pixels by changing only one lsb with the probability of @xmath55 , which results in an average distortion @xmath56 with an embedding rate @xmath57 and embedding efficiency @xmath58 .",
    "let @xmath59 and @xmath60 denote the cover vector and the bit - vector to be embedded . in matrix coding ,",
    "the hash function is defined as @xmath61 the bit place is then computed as @xmath62 that we have to change , which results in the stego vector @xmath63    we implement the matrix coding operation with a multi - layer fnn . fig .",
    "5 shows the used fnn structure .",
    "it is noted that , the used activation functions in the hidden layers will be the sigmoid function ( other non - linear functions may be also usable ) since a completely linear fnn can not achieve _ the xor operation_. our experiments have shown that the used fnn structure provides correct outputs , e.g. , _ appendix c _ provides the python code to optimize the weights for the multi - layer fnn with one hidden layer in case _ k _ = 2 .",
    "similarly , it is recommended that , when applied to images , we should use multiple disjoint cover vectors for finally generating the stego images since it is indeed time consuming , when to collect the entire cover lsbs as the input of the fnn .",
    "that is , in terms of training time with gradient descent , the saturating nonlinearity is very slow [ 8 ] .",
    "in section ii and iii , we present the fnn structures used for both the lsb substitution and matrix coding .",
    "though both can be easily done by applying the fnns , we can draw some valuable arguments in this section .",
    ".,width=336 ]            at first , if the data embedding process of a steganographic system can be modelled by an ann , the usable ann should be not unique .",
    "the reason is that , we can always modify / extend the ann ( e.g. , changing the activation functions ) or insert never - activated perceptrons into an ann such that the new ann still works well for data hiding . therefore , there usually exists a lot of freedom to design a usable neural network for steganography .",
    "secondly , on the receiver side , we can still employ the anns to extract the hidden information .",
    "for example , for the lsb substitution , the designed fnn for a decoder should be trained by using the stego vector as input and message vector as output .",
    "thirdly , in our designed fnns , we did not take into account the secret key .",
    "the reason is that , the secret data * m * should be encrypted according to the secret key , which can maintain the security of the secret message . on the other hand , though the secret key can help us to find the suitable regions of data - embedding , e.g. , a secret key can control the process to ( adaptively ) find optimal data - embedding positions , we are expecting to achieve this feature by designing an independent neural network in the future .",
    "that is , we expect to merge two neural networks for steganography .",
    "one is called * policy network ( pn ) * , and the other is * embedding network ( en)*. as shown in fig .",
    "6 , the pn will be used for finding the optimal data - embedding regions from a cover image , and the en for data embedding in selected pixels .",
    "we are moving our research ahead along this direction .",
    "in addition , when an ann - based steganography uses neural networks for data embedding or data extraction , there may exist the problem of bit - embedding error rate ( bember ) or bit - extraction error rate ( bexter ) .",
    "it indicates that , we may need to adjust the designed ann structure or employ some robust technologies to correct / reduce the errors .",
    "steganalysis [ 9 ] , from an opponent s perspective , aims to detect the presence of covert communication caused by steganography .",
    "recently , steganalysis by exploiting deep convolutional neural network ( cnn ) [ 10 , 11 ] has been studied and achieved competitive detection performance , it indicates that steganalysis with deep neural networks has the potential to provide the best detection performance in the future .",
    "since the anns have the ability to approximate very complex functions from observations , it is possible to use the anns for steganography . in this paper",
    ", we present some preliminary studies on steganography based on the anns .",
    "though there may be some challenges to us , it is desirable to pay attention to anns for steganography .",
    "once steganography with neural networks has been deeply studied and achieved excellent results , the war between steganography and steganalysis may be the war among neural networks .",
    "this work was partly supported by the chinese scholarship council ( csc ) under the grant no .",
    "201407000030 .",
    "1    c. bishop , `` pattern recognition and machine learning ( information science and statistics ) , ''  springer , new york , aug . 2006 .",
    "y. lecun , y. bengio and g. hinton , `` deep learning , ''  _ nature _ , vol .",
    "436 - 444 , may 2015 .",
    "d. silver , a. huang , c. maddison _",
    "et al . _ ,",
    "`` mastering the game of go with deep neural networks and tree search , '' _ nature _ , vol .",
    "529 , no . 7587 , pp .",
    "484 - 489 , jan . 2016 .",
    "h. wu , h. wang , h. zhao and x. yu , `` multi - layer assignment steganography using graph - theoretic approach , ''  _ multimed .",
    "tools appl .",
    "8171 - 8196 , sept .",
    "[ online available ] https://en.wikipedia.org/wiki/hill_climbing [ online available ] https://en.wikipedia.org/wiki/stochastic_gradient_descent a. westfeld , `` f5 : a steganographic algorithm , '' in : _ proc .",
    "workshop inf . hiding _ ,",
    "vol . 2137 , pp .",
    "289 - 302 , apr . 2001 .",
    "a. krizhevsky , i. sutskever and g. hinton , `` imagenet classification with deep convolutional neural networks , '' in : _ adv .",
    "neural inf .",
    "_ , pp . 1097 - 1105 , dec . 2012",
    ". j. fridrich and j. kodovsky , `` rich models for steganalysis of digital images , '' _ ieee trans .",
    "forensics security _ , vol .",
    "868 - 882 , jun .",
    "g. xu , h. wu and y. shi , `` structural design of convolutional neural networks for steganalysis , '' _ ieee signal process . lett .",
    "708 - 712 , may 2016 .",
    "g. xu , h. wu and y. shi , `` ensemble of cnns for steganalysis : an empirical study , '' in : _ proc .",
    "acm inf . hiding multimed . security _ , accepted .",
    "from pybrain.structure import * # lib : pybrain.org    fnn = feedforwardnetwork ( ) ;    inlayer = linearlayer(7 , name = inlayer ) ;    outlayer = linearlayer(3 , name = outlayer ) ;    fnn.addinputmodule(inlayer ) ;    fnn.addoutputmodule(outlayer ) ;    in_to_out = fullconnection(inlayer , outlayer ) ;    fnn.addconnection(in_to_out ) ;    fnn.sortmodules ( ) ;    from pybrain.supervised.trainers import backproptrainer    from pybrain.datasets import superviseddataset    ds = superviseddataset(7 , 3 ) ;    from random import randint    for ind in range(0 , 8000 ) :    @xmath64x = [ randint(0 , 1 ) , randint(0 , 1 ) , randint(0 , 1 ) ] ;    @xmath64 m = [ randint(0 , 1 ) , randint(0 , 1 ) , randint(0 , 1 ) ] ;    @xmath64b = 1 ;    @xmath64z = m ;    @xmath64ds.addsample([x[0],x[1],x[2],m[0],m[1],m[2],b ] , z ) ;    trainer = backproptrainer(fnn , ds , verbose = false , learningrate = 0.01 ) ;    trainer.trainuntilconvergence(maxepochs = 7000 ) ;    for ind in range(0 , 32 ) : # _ user test _",
    "@xmath64x = [ randint(0 , 1 ) , randint(0 , 1 ) , randint(0 , 1 ) ] ;    @xmath64 m = [ randint(0 , 1 ) , randint(0 , 1 ) , randint(0 , 1 ) ] ;    @xmath64b = 1 ;    @xmath64z = m ;    @xmath64print(z , fnn.activate([x[0],x[1],x[2],m[0],m[1],m[2],b ] ) ) ;",
    "from pybrain.supervised.trainers import backproptrainer    from pybrain.tools.shortcuts import buildnetwork    from pybrain.structure.modules import linearlayer    net = buildnetwork(6 , 5 , 3 , bias = true , hiddenclass = linearlayer ) ;    from pybrain.datasets import superviseddataset    ds = superviseddataset(6 , 3 ) ;    from random import randint    for ind in range(0 , 8000 ) :    @xmath64x = [ randint(0 , 1 ) , randint(0 , 1 ) , randint(0 , 1 ) ] ;    @xmath64 m = [ randint(0 , 1 ) , randint(0 , 1 ) , randint(0 , 1 ) ] ;    @xmath64z = m ;    @xmath64ds.addsample([x[0],x[1],x[2],m[0],m[1],m[2 ] ] , z ) ;    trainer = backproptrainer(net , ds , verbose = false , learningrate = 0.01 ) ;    trainer.trainuntilconvergence(maxepochs = 7000 ) ;    for ind in range(0 , 32 ) : # _ user test _    @xmath64x = [ randint(0 , 1 ) , randint(0 , 1 ) , randint(0 , 1 ) ] ;    @xmath64 m = [ randint(0 , 1 ) , randint(0 , 1 ) , randint(0 , 1 ) ] ;    @xmath64z = m ;    @xmath64print ( z , net.activate([x[0],x[1],x[2],m[0],m[1],m[2 ] ] ) ) ;",
    "from pybrain.supervised.trainers import backproptrainer"
  ],
  "abstract_text": [
    "<S> in recent years , due to the powerful abilities to deal with highly complex tasks , the artificial neural networks ( anns ) have been studied in the hope of achieving human - like performance in many applications . </S>",
    "<S> since the anns have the ability to approximate complex functions from observations , it is straightforward to consider the anns for steganography . in this paper , we aim to implement the well - known lsb substitution and matrix coding steganography with the feed - forward neural networks ( fnns ) . </S>",
    "<S> our experimental results have shown that , the used fnns can achieve the data embedding operation of the lsb substitution and matrix coding steganography . </S>",
    "<S> for steganography with the anns , though there may be some challenges to us , it would be very promising and valuable to pay attention to the anns for steganography , which may be a new direction for steganography .    </S>",
    "<S> steganography , neural network , lsb , data hiding , matrix coding , machine learning , perceptron . </S>"
  ]
}