{
  "article_text": [
    "the generalization properties of most current statistical learning techniques are predicated on the assumption that the training data and test data come from the same underlying probability distribution . unfortunately , in many applications , this assumption is inaccurate .",
    "it is often the case that plentiful labeled data exists in one domain ( or coming from one distribution ) , but one desires a statistical model that performs well on another related , but not identical domain .",
    "hand labeling data in the new domain is a costly enterprise , and one often wishes to be able to leverage the original , `` out - of - domain '' data when building a model for the new , `` in - domain '' data .",
    "we do not seek to _ eliminate _ the annotation of in - domain data , but instead seek to minimize the amount of new annotation effort required to achieve good performance .",
    "this problem is known both as _ domain adaptation _ and _",
    "transfer_.    in this paper , we present a novel framework for understanding the domain adaptation problem . the key idea in our framework",
    "is to treat the in - domain data as drawn from a mixture of two distributions : a `` truly in - domain '' distribution and a `` general domain '' distribution .",
    "similarly , the out - of - domain data is treated as if drawn from a mixture of a `` truly out - of - domain '' distribution and a `` general domain '' distribution .",
    "we apply this framework in the context of conditional classification models and conditional linear - chain sequence labeling models , for which inference may be efficiently solved using the technique of conditional expectation maximization .",
    "we apply our model to four data sets with varying degrees of divergence between the `` in - domain '' and `` out - of - domain '' data and obtain predictive accuracies higher than any of a large number of baseline systems and a second model proposed in the literature for this problem .",
    "the domain adaptation problem arises very frequently in the natural language processing domain , in which millions of dollars have been spent annotating text resources for morphological , syntactic and semantic information .",
    "however , most of these resources are based on text from the news domain ( in most cases , the wall street journal ) .",
    "the sort of language that appears in text from the wall street journal is highly specialized and is , in most circumstances , a poor match to other domains .",
    "for instance , there has been a recent surge of interest in performing summarization @xcite or information extraction @xcite of biomedical texts , summarization of electronic mail @xcite , information extraction from transcriptions of meetings , conversations or voice - mail @xcite , among others .",
    "conversely , in the machine translation domain , most of the parallel resources that machine translation system depend on for parameter estimation are drawn from transcripts of political meetings , yet the translation systems are often targeted at news data @xcite .",
    "in the multiclass classification problem , one typically assumes the existence of a training set @xmath0 , where @xmath1 is the input space and @xmath2 is a finite set .",
    "it is assumed that each @xmath3 is drawn from a fixed , but unknown base distribution @xmath4 and that the training set is independent and identically distributed , given @xmath4 .",
    "the learning problem is to find a function @xmath5 that obtains high predictive accuracy ( this is typically done either by explicitly minimizing the regularized empirical error , or by maximizing the probabilities of the model parameters ) .      in the context of domain adaptation ,",
    "the situation becomes more complicated .",
    "we assume that we are given _ two _ sets of training data , @xmath6 and @xmath7 , the `` out - of - domain '' and `` in - domain '' data sets , respectively .",
    "we no longer assume that there is a single fixed , but known distribution from which these are drawn , but rather assume that @xmath6 is drawn from a distribution @xmath8 and @xmath7 is drawn from a distribution @xmath9 .",
    "the learning problem is to find a function @xmath10 that obtains high predictive accuracy on data drawn from @xmath9 .",
    "( indeed , our model will turn out to be symmetric with respect to @xmath7 and @xmath6 , but in the contexts we consider obtaining a good predictive model of @xmath7 makes more intuitive sense . )",
    "we will assume that @xmath11 and @xmath12 , where typically we have @xmath13 .",
    "as before , we will assume that the @xmath14 out - of - domain data points are drawn iid from @xmath8 and that the @xmath15 in - domain data points are drawn iid from @xmath9 .",
    "obtaining a good adaptation model requires the careful modeling of the relationship between @xmath9 and @xmath8 .",
    "if these two distributions are independent ( in the obvious intuitive sense ) , then the out - of - domain data @xmath6 is useless for building a model of @xmath9 and we may as well ignore it . on the other hand , if @xmath9 and @xmath8 are identical , then there is no adaptation necessary and we can simply use a standard learning algorithm . in practical problems , though , @xmath9 and @xmath8 are neither identical nor independent .",
    "there has been relatively little prior work on this problem , and nearly all of it has focused on specific problem domains , such as n - gram language models or generative syntactic parsing models . the standard approach used is to treat the out - of - domain data as `` prior knowledge '' and then to estimate maximum a posterior values for the model parameters under this prior distribution .",
    "this approach has been applied successfully to language modeling @xcite and parsing @xcite . also in the parsing domain , and have shown that simple techniques based on using carefully chosen subsets of the data and parameter pruning can improve the performance of an adapted parser .",
    "these models assume a data distribution @xmath16 with parameters @xmath17 and a prior distribution over these parameters @xmath18 with hyper - parameters @xmath19 .",
    "they estimate the @xmath19 hyperparameters from the out - of - domain data and then find the maximum a posteriori parameters for the in - domain data , with the prior fixed .    in the context of conditional and discriminative models , the only domain adaptation work of which we are aware is the model of .",
    "this model again uses the out - of - domain data to estimate a prior distribution , but does so in the context of a maximum entropy model .",
    "specifically , a maximum entropy model is trained on the out - of - domain data , yielding optimal weights for that problem .",
    "these weights are then used as the _ mean _ weights for the gaussian prior on the learned weights for the in - domain data .    though effective experimentally , the practice of estimating a prior distribution from out - of - domain data and fixing it for the estimation of in - domain data leaves much to be desired .",
    "theoretically , it is strange to estimate and _ fix _ a prior distribution from data ; this is made more apparent by considering the form of these models . denoting the in - domain data and parameters by @xmath20 and @xmath17 , respectively , and the out - of - domain data and parameters by @xmath21 and @xmath19",
    ", we obtain the following form for these `` prior '' estimation models :    @xmath22    one would have a very difficult time rationalizing this optimization problem by anything other than experimental performance .",
    "moreover , these models are unusual in that they do not treat the in - domain data and the out - of - domain data identically .",
    "intuitively , there is no difference in the two sets of data ; they simply come from different , related distributions . yet , the prior - based models are highly asymmetric with respect to the two data sets .",
    "this also makes generalization to more than one `` out of domain '' data set difficult .",
    "finally , as we will see , the model we propose in this paper , which alleviates all of these problems , outperforms them experimentally .",
    "a second generic approach to the domain adaptation problem is to build an out of domain model and use its predictions as features for the in domain data .",
    "this has been successfully used in the context of named entity tagging @xcite .",
    "this approach is attractive because it makes no assumptions about the underlying classifier ; in fact , multiple classifiers can be used .      in this paper , we propose the following relationship between the in - domain and the out - of - domain distributions .",
    "we assume that instead of two underlying distributions , there are actually _ three _ underlying distributions , which we will denote @xmath23 , @xmath24 and @xmath25 .",
    "we then consider @xmath8 to be a _ mixture _ of @xmath23 and @xmath24 , and consider @xmath9 to be a mixture of @xmath25 and @xmath24 .",
    "one can intuitively view the @xmath23 distribution as a distribution of data that is _ truly out - of - domain _",
    ", @xmath25 as a distribution of data that is _ truly in - domain _ and @xmath24 as a distribution of data that is general to both domains .",
    "thus , knowing @xmath24 and @xmath25 is sufficient to build a model of the in - domain data .",
    "the out - of - domain data can help us by providing more information about @xmath24 than is available by just considering the in - domain data .",
    "for example , in part - of - speech tagging , the assignment of the tag `` determiner '' ( dt ) to the word `` the '' is likely to be a _",
    "general _ decision , independent of domain .",
    "however , in the wall street journal , `` monitor '' is almost always a verb ( vb ) , but in technical documentation it will most likely be a noun .",
    "the @xmath24 distribution should account for the case of `` the / dt '' , the @xmath23 should account for `` monitor / vb '' and @xmath25 should account for `` monitor / nn . ''",
    "the domain adaptation framework outlined in section  [ sec : ourframework ] is completely general in that it can be applied to any statistical learning model . in this section",
    "we apply it to log - linear conditional maximum entropy models and their linear chain counterparts , since these models have proved quite effective in many learning tasks .",
    "we will first review the maximum entropy framework , then will extend it to the domain adaptation problem ; finally we will discuss domain adaptation in linear chain maximum entropy models .",
    "the maximum entropy framework seeks a conditional distribution @xmath26 that is closest ( in the sense of kl divergence ) to the uniform distribution but also matches a set of training data @xmath27 with respect to feature function expectations @xcite . by introducing one lagrange multiplier @xmath28 for each feature function @xmath29 , this optimization problem results in a probability distribution of the form :    @xmath30\\ ] ]    here",
    ", @xmath31 denotes the scalar product of two vectors @xmath32 and @xmath33 , given by : @xmath34 .",
    "the normalization constant in eq  , @xmath35 , is obtained by summing the exponential over all possible classes @xmath36 .",
    "this probability distribution is also known as an _ exponential distribution _ or a _",
    "gibbs distribution_. the learning ( or optimization ) problem is to find the vector @xmath37 that maximizes the likelihood in eq  . in practice , to prevent over - fitting , one typically optimizes a penalized ( log ) likelihood , where an isotropic gaussian prior with mean @xmath38 and covariance matrix @xmath39 is placed over the parameters @xmath37 @xcite .",
    "the graphical model for the standard maximum entropy model is depicted on the left of figure  [ fig : models ] . in this figure",
    ", circular nodes correspond to random variables and square nodes correspond to fixed variables .",
    "shaded nodes are observed in the training data and empty nodes are hidden or unobserved .",
    "arrows denote conditional dependencies .",
    "in general , the feature functions @xmath40 may be arbitrary real - valued functions ; however , in this paper we will restrict our attention to binary features . in practice , this is not a harsh restriction : many problems in the natural language domain naturally employ only binary features ( for real valued features , binning techniques can be applied ) . additionally , for notational convenience",
    ", we will assume that the features @xmath41 can be written in product form as @xmath42 for arbitrary binary functions",
    "@xmath43 over outputs and binary features @xmath44 over inputs .",
    "the latter assumption means that we can consider @xmath45 to be a binary vector where @xmath46 ; in the following this will simplify notation significantly ( the extension to the full case is straightforward , but messy , and is therefore not considered in the remainder of this paper ) . by considering @xmath47 as a vector",
    ", we may move the class dependence to the parameters and consider @xmath37 to be a matrix where @xmath48 is the weight for @xmath49 for class @xmath50 .",
    "we will write @xmath51 to refer to the column vector of @xmath52 corresponding to class @xmath50 . as @xmath47 is also considered a column vector , we write @xmath53 as shorthand for the dot product between @xmath47 and the weights for class @xmath50 . under this modified notation , we may rewrite eq   as :    @xmath54\\ ] ]    combining this with a gaussian prior on the weights , we obtain the following form for the log posterior of a data set :    @xmath55 \\right ] + \\textrm{const}\\ ] ]    the parameters @xmath37 can be estimated using any convex optimization technique ; in practice , limited memory bfgs @xcite seems to be a good choice @xcite and we will use this algorithm for the experiments described in this paper . in order to perform these calculations , one must be able to compute the gradient of eq   with respect to @xmath56 , which is available in closed form .      extending the maximum entropy model to account for both in - domain and out - of - domain data in the framework described earlier",
    "requires the addition of several extra model parameters .",
    "in particular , for each in - domain data point @xmath57 , we assume the existence of a binary indicator variable @xmath58 .",
    "a value @xmath59 indicates that @xmath57 is drawn from @xmath25 ( the truly in - domain distribution ) , while a value @xmath60 indicates that it is drawn from @xmath24 ( the general - domain distribution ) .",
    "similarly , for each out - of - domain data point @xmath61 , we assume a binary indicator variable @xmath62 , where @xmath63 means this data point is drawn from @xmath23 ( the truly out - of - domain distribution ) and a value of @xmath38 means that it is drawn from @xmath24 ( the general - domain distribution ) .",
    "of course , these indicator variables are not observed in the data , so we must infer their values automatically .    according to this model , the @xmath64s are",
    "binary random variables that we assume are drawn from a bernoulli distribution with parameter @xmath65 ( for in - domain ) and @xmath66 ( for out - of - domain ) .",
    "furthermore , we assume that there are three @xmath37 vectors , @xmath67 , @xmath68 and @xmath69 corresponding to @xmath25 , @xmath23 and @xmath24 , respectively .",
    "for instance , if @xmath70 , then we assume that @xmath71 should be classified using @xmath67 . finally , we model the binary vectors @xmath72s ( respectively @xmath73s ) as being drawn independently from bernoulli distributions parameterized by @xmath74 and @xmath75 ( respectively , @xmath76 and @xmath77 ) . again , when @xmath78 , we assume that @xmath79 is drawn according to @xmath80 .",
    "this corresponds to a nave bayes assumption over the generative probabilities of the @xmath79 vectors .",
    "finally , we place a common beta prior over the nave bayes parameters , @xmath81 .",
    "allowing @xmath82 to range over @xmath83 , the full hierarchical model is :    ps .",
    "^_f & a , b & ( a , b ) & ^ & ^2 & ( 0 , ^2 i ) + z^_n & ^ & ( ^ ) & z^_n & ^ & ( ^ ) + x^_nf & z^_n , ps .",
    "^_f , ps . ^_f & ( ps .",
    "^z^_n_f ) & x^_nf & z^_n , ps . ^_f , ps . ^_f & ( ps .",
    "^z^_n_f ) + y^_n & x^_n , z^_n , ^ , ^ & _ gibbs_(x^_n , ^z^_n ) & y^_n & x^_n , z^_n , ^ , ^ & _ gibbs_(x^_n,^z^_n ) [ eq : hier - megam ]    we term this model the `` maximum entropy genre adaptation model '' ( the mega model ) .",
    "the corresponding graphical model is shown on the right in figure  [ fig : models ] .",
    "the generative story for an in - domain data point @xmath84 is as follows :    1 .",
    "select whether @xmath84 will be truly in - domain or general - domain and indicate this by @xmath85 . choose",
    "@xmath86 with probability @xmath65 and @xmath87 with probability @xmath88 .",
    "2 .   for each component @xmath10 of @xmath84 , choose @xmath89 to be @xmath90 with probability @xmath91 and @xmath38 with probability @xmath92 .",
    "3 .   choose a class @xmath50 according to eq   using the parameter vector @xmath93 .",
    "the story for out - of - domain data points is identical , but uses the truly out - of - domain and general - domain parameters , rather than the truly in - domain parameters and general - domain parameters .",
    "the straightforward extension of the maximum entropy classification model to the maximum entropy markov model ( memm ) @xcite is obtained by assuming that the targets @xmath94 are sequences of labels .",
    "the canonical example for this model is part of speech tagging : each word in a sequence is assigned a part of speech tag . by introducing a first order markov assumption on the tag sequence , one obtains a linear chain model that can be viewed as the discriminative counterpart to the standard ( generative ) hidden markov model .",
    "the parameters of these models can be estimated again using limited memory bfgs .",
    "the extension of the mega model  to the linear chain framework is similarly straightforward , under the assumption that each label ( part of speech tag ) has its own indicator variable @xmath95 ( versus a global indicator variable @xmath95 for the entire tag sequence ) .",
    "the techniques described herein may also be applied to the conditional random field framework of , which fixes a bias problem of the memm by performing global normalization rather than per - state normalization .",
    "there is , however , a subtle difficulty in a direct application to crfs . specifically , one would need to decide if a single @xmath95 variable would be assigned to an entire sentence , or to each word individually . in the memm case , it is most natural to have one @xmath95 per word",
    ". however , to do so in a crf would be computationally more expensive .",
    "in the remainder , we continue to use the memm model for efficiency purposes .",
    "inference in the mega model  is slightly more complex than in standard maximum entropy models . however , inference can be solved efficiently using conditional expectation maximization ( cem ) , a variant of the standard expectation maximization ( em ) algorithm @xcite , due to . at a high level ,",
    "em is useful for computing in _ generative _ models with hidden variables , while cem is useful for computing in _ discriminative _ models with hidden variables ; the mega model  belongs to the latter family , so cem is the appropriate choice .",
    "the standard em family of algorithms maximizes a _ joint _ likelihood over data .",
    "in particular , if @xmath96 are data and @xmath95 is a ( discrete ) hidden variable , the m - step of em proceeds by maximizing the bound given in eq     @xmath97    in eq  , @xmath98 denotes an expectation .",
    "one may now apply jensen s inequality to this equation , which states that @xmath99 whenever @xmath10 is convex . taking @xmath100 , we are able to decompose the @xmath101 of an expectation into the expectation of a @xmath101 .",
    "this typically separates terms and makes taking derivatives and solving the resolution optimization problem tractable .",
    "unfortunately , em can not be directly applied to conditional models ( such as the mega model ) of the form in eq   because such models result in an m - step that requires the maximization of an equation of the form given in eq  .",
    "@xmath102    jensen s inequality can be applied to the first term in eq  , which can be maximized readily as in standard em . however , applying jensen s inequality to the second term would lead to an _ upper bound _ on the likelihood , since that term appears negated .",
    "the conditional em solution @xcite is to bound the _ change _ in log - likelihood between iterations , rather than the log - likelihood itself .",
    "the change in log - likelihood can be written as in eq  , where @xmath103 denotes the parameters at iteration @xmath104 .",
    "@xmath105    by rewriting the conditional distribution @xmath106 as @xmath107 divided by @xmath108 , we can express @xmath109 as the log of the joint distribution difference minus the log of the marginal distribution . here",
    ", we can apply jensen s inequality to the first term ( the joint difference ) , but not to the second ( because it appears negated ) . fortunately , jensen s is not the only bound we can employ .",
    "the standard variational upper bound of the logarithm function is : @xmath110 ; this leads to a lower bound of the negation , which is exactly what is desired .",
    "this bound is attractive for other reasons : ( 1 ) it is tangent to the logarithm ; ( 2 ) it is tight ; ( 3 ) it makes contact at the current operating point ( according to the maximization at the previous time step ) ; ( 4 ) it is a simply linear function ; and ( 5 ) in the terminology of the calculus of variations , it is the variational dual to the logarithm ; see @xcite .    applying jensen s inequality to the first term in eq   and the variational dual to the second term",
    ", we obtain that the change of log - likelihood in moving from model parameters @xmath111 at time @xmath112 to @xmath103 at time @xmath104 ( which we shall denote @xmath113 ) is bounded by @xmath114 , where @xmath113 is defined by eq  , where @xmath115 when @xmath116 and @xmath117 when @xmath118 , with expectations taken with respect to the parameters from the previous iteration .",
    "@xmath119    by applying the two bounds ( jensen s inequality and the variational bound ) , we have removed all `` sums of logs , '' which are hard to deal with analytically .",
    "the full derivation is given in appendix a. the remaining expression is a lower bound on the change in likelihood , and maximization of it will result in maximization of the likelihood .",
    "as in the map variant of standard em , there is no change to the e - step when priors are placed on the parameters .",
    "the assumption in standard em is that we wish to maximize @xmath120 where the prior probability of @xmath121 is ignored , leaving just the likelihood term of the parameters given the data . in map estimation , we do not make this assumption and instead use a true prior @xmath122 . in doing so",
    ", we need only to add a factor of @xmath123 to the definition of @xmath113 in eq  .",
    "it is important to note that although we do make use of a full joint distribution @xmath124 , the _ objective function _ of our model is _ conditional . _",
    "the joint distribution is only used in the process of creating the bound : the overall optimization is to maximize the conditional likelihood of the labels _ given _ the input .",
    "in particular , the bound using the full joint likelihood holds for any parameters of the marginal .      in this appendix , we derive eq   from eq   by making use of jensen s inequality and the variational bound .",
    "the interested reader is referred to the work of for further details .",
    "our discussion will consider a bound in the _ change _ of the log likelihood between iteration @xmath112 and iteration @xmath104 , @xmath109 , as given in eq  :    @xmath180    here , we have effectively rewritten the log - change in the ratio of the conditionals as the difference between the log - change in the ratio of the joints and the log - change in the ratio of the marginals .",
    "we may rewrite eq   by introducing the hidden variables @xmath95 as :    @xmath181    we can now apply jensen s inequality to the first term in eq   to obtain :    @xmath182        } _ { h_{x , y , z,\\th^{t-1 } } }        \\log \\frac { \\p{x , y , z ; \\th^t } } { \\p{x , y , z ; \\th^{t-1 } } }    - \\log \\frac { \\sum_z \\p{x , z    ; \\th^t } } { \\sum_z \\p{x , z    ; \\th^{t-1 } } }    \\label{eq : de - l - c - z2}\\ ] ]    in eq  , the expression denoted @xmath183 is the joint expectation of @xmath95 under the previous iteration s parameter settings .",
    "unfortunately , we can not also apply jensen s inequality to the remaining term in eq   because it appears negated . by applying the variational dual ( @xmath184 ) to this term",
    ", we obtain the following , final bound :    @xmath185    applying the bound from eq   to the distributions chosen in our model yields eq  .",
    "as made explicit in eq  , the relevant distributions for performing cem are the full joint distributions over the input variables @xmath45 , the output variables @xmath50 , and the hidden variables @xmath95 . additionally , we require the marginal distribution over the @xmath45 variables and the @xmath95 variables . finally , we need to compute expectations over the @xmath95 variables",
    ". we will derive the expectation step in this section and present the final solution for the maximization step for each class of variables .",
    "the derivation of the equations for the maximization is given in appendix b.    the @xmath125 bound on complete conditional likelihood for the mega modelis given below :    @xmath126 \\nonumber\\\\ & +    \\sum_{n=1}^{n{^{\\textsf{(o ) } } } } \\hspace{0.3 mm } \\left[\\sum_{z{^{\\textsf{(o)}}}_n }       h{^{\\textsf{(o)}}}_n      \\log \\frac { \\p{z{^{\\textsf{(o)}}}_n , \\vec x{^{\\textsf{(o)}}}_n , y{^{\\textsf{(o)}}}_n } } { \\pprime{z{^{\\textsf{(o)}}}_n , \\vec x{^{\\textsf{(o)}}}_n , y{^{\\textsf{(o)}}}_n } }    - \\frac { \\sum_{z{^{\\textsf{(o)}}}_n } \\p{z{^{\\textsf{(o)}}}_n,\\vec x{^{\\textsf{(o)}}}_n } } { \\sum_{z{^{\\textsf{(o)}}}_n } \\pprime{z{^{\\textsf{(o)}}}_n,\\vec x{^{\\textsf{(o)}}}_n } }    + 1 \\right ] \\label{eq : qmegam}\\end{aligned}\\ ] ]    in this equation , @xmath127 is the probability distribution at the previous iteration .",
    "the first term in eq   is the bound for the in - domain data , while the second term is the bound for the out - of - domain data . in all the optimizations",
    "described in this section , there are nearly identical terms for the in - domain parameters and the out - of - domain parameters .",
    "for brevity , we will only explicitly write the equations for the in - domain parameters ; the corresponding out - of - domain equations can be easily derived from these . moreover , to reduce notational overload , we will elide the superscripts denoting in - domain and out - of - domain when obvious from context . for notational brevity , we will use the notation depicted in table  [ tab : notation ] .",
    "@xmath128^{-1 } & \\ps_{n , z_n ,- f ' }      & = & \\prod_{f \\ne f ' } \\left(\\ps^{z_n}_{f}\\right)^{x_{nf } }        \\left(1-\\ps^{z_n}_{f}\\right)^{1-x_{nf } } \\nonumber \\end{array}\\ ] ]      the e - step is concerned with calculating @xmath129 given current model parameters . since @xmath130",
    ", we easily find @xmath131 , which can be calculated as follows :    @xmath132 \\label{eq : estep}\\end{aligned}\\ ] ]    here , @xmath133 is the partition function from before .",
    "this can be easily calculated for @xmath134 and the expectation can be found by dividing the value for @xmath116 by the sum over both .",
    "as shown in appendix b.1 , we can directly compute the value of @xmath135 by solving a simple quadratic equation .",
    "we can compute @xmath135 as @xmath136 , where :    @xmath137      viewing @xmath113 as a function of @xmath37 , it is easy to see that optimization for this variable is convex .",
    "an analytical solution is not available , but the gradient of @xmath113 with respect to @xmath67 can be seen to be identical to the gradient of the standard maximum entropy posterior , eq  , but where each data point is _ weighted _ according to its posterior probability , @xmath138 .",
    "we may thus use identical optimization techniques for computing optimal @xmath37 variables as for standard maximum entropy models ; the only difference is that the data points are now weighted .",
    "a similar story holds for @xmath68 . in the case of @xmath69",
    ", we obtain the standard maximum entropy gradient , computed over all @xmath139 data points , where each @xmath140 is weighted by @xmath129 and each @xmath141 is weighted by @xmath142 .",
    "this is shown in appendix b.2 .      like the case for @xmath37",
    ", we can not obtain an analytical solution for finding the @xmath81 that maximizes @xmath113 .",
    "however , we can compute simple derivatives for @xmath113 with respect to a single component @xmath143 which can be maximized analytically . as shown in appendix b.3",
    ", we can compute @xmath144 as @xmath136 , where :    @xmath145    the case for @xmath146 is identical . for @xmath147 ,",
    "the only difference is that we must replace each sum to over the data points with two sums , one for each of the in - domain and out - of - domain points ; and , as before , the @xmath148s must be replaced with @xmath129 ; this is made explicit in the appendix .",
    "thus , to optimize the @xmath81 variables , we simply iterate through and optimize each component analytically , as given above , until convergence .",
    "the full training algorithm is depicted in figure  [ fig : training - algorithm ] .",
    "convergence properties of the cem algorithm ensure that this will converge to a ( local ) maximum in the posterior space .",
    "if local optima become a problem in practice , one can alternatively use a stochastic optimization algorithm , in which a temperature is applied enabling the optimization to jump out of local optima early on .",
    "however , we do not explore this idea further in this work . in the context of our application ,",
    "this extension was not required .",
    "one immediate question about the conditional em model we have described is how many em iterations are required for the model to converge . in our experiments , @xmath149 iterations of cem is more than sufficient , and often only @xmath150 or @xmath151 are necessary .",
    "to make this more clear , in figure  [ fig : cll ] , we have plotted the negative complete log likelihood of the model on the first data set , described below in section  [ datasets ] .",
    "there are three separate maximizations in the full training algorithm ( see figure  [ fig : training - algorithm ] ) ; the first involves updating the @xmath135 variables , the second involves optimizing the @xmath37 variables and the third involves optimizing the @xmath81 variables .",
    "we compute the likelihood after each of these steps .    running a total @xmath149 cem iterations",
    "is still relatively efficient in our model .",
    "the dominating expense is in the weighted maximum entropy optimization , which , at @xmath149 cem iterations , must be computed @xmath152 times ( each iteration requires the optimization of each of the three sets of @xmath37 variables ) .",
    "at worst this will take @xmath152 times the amount of time to train a model on the complete data set ( the union of the in - domain and out - of - domain data ) , but in practice we can resume each optimization at the ending point of the previous iteration , which causes the subsequent optimizations to take much less time .",
    "once training has supplied us with model parameters , the subsequent task is to apply these parameters to unseen data to obtain class predictions .",
    "we assume all this test data is `` in - domain '' ( i.e. , is drawn either from @xmath153 or @xmath154 in the notation of the introduction ) , and obtain a decision rule of the form given in eq   for a new test point @xmath47 .",
    "@xmath155      \\frac { \\exp\\left[\\vec\\la_y{^{\\textsf{(g)}}}\\t \\vec x\\right ] } { z_{\\vec x , \\vec\\la{^{\\textsf{(g ) } } } } } \\nonumber\\\\ & \\hspace{6.0 mm } + ( 1-\\pi ) \\left[\\prod_{f=1}^f { \\(\\ps{^{\\textsf{(i)}}}_f\\hspace{0.7mm}\\)^{x_f } }    { \\(1-\\ps{^{\\textsf{(i)}}}_f \\hspace{0.8mm}\\)^{1-x_f } } \\hspace{.5mm}\\right ]      \\frac { \\exp\\left[\\vec\\la_y{^{\\textsf{(i)}}}\\t \\vec x\\right ] } { z_{\\vec x , \\vec\\la{^{\\textsf{(i ) } } } } }   \\label{eq : predict}\\end{aligned}\\ ] ]    thus , the decision rule is to simply select the class which has highest probability according to the maximum entropy classifiers , weighted linearly by the marginal probabilities of the new data point being drawn from @xmath153 versus @xmath154 . in this sense",
    ", our model can be seen as linearly interpolating an in - domain model and a general - domain model , but where the interpolation parameter is _ input specific_.",
    "in this section , we describe the result of applying the mega model  to several datasets with varying degrees of divergence between the in - domain and out - of - domain data .",
    "however , before describing the data and results , we will discuss the systems against which we compare .      though there has been little literature on this problem and thus few real systems against which to compare ,",
    "there are several obvious baselines , which we describe in this section .",
    "[ [ onlyi ] ] onlyi : + + + + + +    this model is obtained simply by training a standard maximum entropy model on the _ in - domain _ data .",
    "this completely ignores the out - of - domain data and serves as a baseline case for when such data is unavailable .",
    "[ [ onlyo ] ] onlyo : + + + + + +    this model is obtained by training a standard maximum entropy model on the _ out - of - domain _ data , completely ignoring the in - domain data .",
    "this serves as a baseline for expected performance without annotating any new data .",
    "it also gives a sense of how close the out - of - domain distribution is to the in - domain distribution .",
    "[ [ lini ] ] lini : + + + + +    this model is obtained by linearly interpolating the onlyi and onlyo systems .",
    "the interpolation parameter is estimated on held - out ( development ) in - domain data .",
    "this means that , in practice , extra in - domain data would need to be annotated in order to create a development set ; alternatively , cross - validation could be used .",
    "[ [ mix ] ] mix : + + + +    this model is obtained by training a maximum entropy model on the union of the out - of - domain and in - domain data sets .",
    "[ [ mixw ] ] mixw : + + + + +    this model is also obtained by training a maximum entropy model on the union of the out - of - domain and in - domain data sets , but where the out - of - domain data is _ down - weighted _ so that is effectively equinumerous with the in - domain data .",
    "[ [ feats ] ] feats : + + + + + +    this model uses the out - of - domain data to build one classifier and then uses this classifier s predictions as features for the in - domain data , as described by .    [ [ prior ] ] prior : + + + + + +    this is the adaptation model described in section  [ prior - work ] , where the out - of - domain data is used to estimate a prior for the in - domain classifier . in the case of the maximum entropy models we consider here , the weights learned from the out - of - domain data are used as the _ mean _ of the gaussian prior distribution placed over the weights in the training of the in - domain data , as is described by .    in all cases ,",
    "we tune model hyperparameters using performance on development data .",
    "this development data is taken to be a random @xmath156 of the training data in all cases .",
    "once appropriate hyperparameters are found , the @xmath156 is folded back in to the training set .",
    "we evaluate our models on three different problems .",
    "the first two problems come from the automatic content extraction ( ace ) data task .",
    "this data was selected because the ace program specifically looks at data in different domains .",
    "the third problem is the same as that tackled by , which required them to annotate data themselves .",
    "the first problem , * mention type * , is a subcomponent of the entity mention detection task ( an extension of the named entity tagging task , wherein pronouns and nominals are marked , in addition to simple names ) .",
    "we assume that the extents of the mentions are marked and we simply need to identify their type , one of : person , geo - political entity , organization , location , weapon or vehicle . as the out - of - domain data",
    ", we use the newswire and broadcast news portions of the ace 2005 training data ; as the in - domain data , we use the fisher conversations data .",
    "an example out - of - domain sentence is :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ once again , a prime battleground will be the constitutional allocation of power  between the federal @xmath157 and the @xmath157 , and between @xmath158 and federal regulatory @xmath159 .",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    an example in - domain sentence is :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ @xmath160 @xmath161 if @xmath160 had not been transported across the @xmath157 from @xmath162 @xmath160 was born and and _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    we use @xmath163 out - of - domain examples ( each mention corresponds to one example ) , @xmath164 in - domain examples and @xmath165 test examples .",
    "accuracy is computed as @xmath166 loss .",
    "we use the standard feature functions employed in named entity models , include lexical items , stems , prefixes and suffixes , capitalization patterns , part - of - speech tags , and membership information on gazetteers of locations , businesses and people .",
    "the accuracies reported are the result of running ten fold cross - validation .      the second problem , * mention tagging * is the precursor to the * mention type * task , in which we attempt to tag entity mentions in raw text .",
    "we use the standard begin / in / out encoding and use a maximum entropy markov model to perform the tagging @xcite . as the out - of - domain data , we use again the newswire and broadcast news data ; as the in - domain data , we use broadcast news data that has been transcribed by automatic speech recognition .",
    "the in - domain data lacks capitalization , punctuation , etc . , and also contains transcription errors ( speech recognition word error rate is approximately @xmath167 ) . for the tagging task , we have @xmath168 out - of - domain examples ( in the context of tagging , an example is a single word ) , but now @xmath169 in - domain examples and @xmath170 test examples .",
    "accuracy is f - measure across the segmentation .",
    "we use the same features as in the mention type identification task .",
    "the scores reported are after ten fold cross - validation .",
    "the final problem , * recap * , is the task of recapitalizing text . following , we again use a maximum entropy markov model , where the possible tags are : lowercase , capitalized , all upper case , punctuation or mixed case .",
    "the out - of - domain data in this task comes from the wall street journal , and two separate in - domain data sets come from broadcast news text from cnn / npr and abc primetime , respectively .",
    "we use @xmath171 out - of - domain examples ( one example is one word ) . for the cnn / npr data , we use @xmath172 in - domain training examples and @xmath173 test examples ; for the abc primetime data , we use @xmath174 in - domain training examples and @xmath175 test examples .",
    "we use identical features to . in order to maintain comparability to the results described by",
    ", we do not perform cross - validation for these experiments : we use the same train / test split as described in their paper .      while the maximum entropy models used for the classification are adept at dealing with many irrelevant and/or redundant features , the nave bayes generative model , which we use to model the distribution of the input variables , can overfit on such features .",
    "this turned out not to be a problem for the * mention type * and * mention tagging * problems , but for the * recap * problems , it caused some errors . to alleviate this problem , for the * recap * problem _ only _ , we applied a feature selection algorithm just to the features used for the nave bayes model ( the entire feature set was used for the maximum entropy model ) .",
    "specifically , we took the @xmath176 top features according to the information gain criteria to predict `` in - domain '' versus `` out - of - domain '' ( as opposed to feature selection for class label ) ; provides an overview of different selection techniques .",
    "was selected arbitrarily after an initial run of the model on development data ; it was not tuned to optimize either development or test performance . ]      [ cols=\"<,<,^,^,^,^,^ \" , ]     these values for @xmath135 make intuitive sense .",
    "the distinction between conversation data and news data ( for the mention type task ) is significantly stronger than the difference between manually and automatically transcribed newswire ( for the mention tagging task ) .",
    "the values for @xmath135 reflect this qualitative distinction . the rather strong difference between the @xmath135 values for the recapitalization tasks",
    "was not expected a priori . however , a post hoc analysis shows this result is reasonable .",
    "we compute the kl divergence between a unigram language model for the out - of - domain data set and each of the in - domain data sets .",
    "the kl divergence for the cnn data was @xmath177 , while the divergence for the abc data @xmath178 .",
    "this confirms that the abc data is perhaps more different from the baseline out - of - domain than the cnn data , as reflected by the @xmath135 values .",
    "we are also interested in cases where there is little difference between in - domain and out - of - domain data .",
    "to simulate this case , we have performed the following experiment .",
    "we consider again the mention type task , but use _ only _ the training portion of the out - of - domain data .",
    "we randomly split the data in half , assigning each half to `` in - domain '' and `` out - of - domain . '' in theory , the model should learn that it may rely only on the general domain model .",
    "we performed this experiment under ten fold cross - validation and found that the average value of @xmath135 selected by the model was @xmath179 .",
    "while this is strictly less than one , it does show that the model is able to identify that these are very similar domains .",
    "in this paper , we have presented the mega model  for domain adaptation in the discriminative ( conditional ) learning framework .",
    "we have described efficient optimization algorithms based on the conditional em technique .",
    "we have experimentally shown , in four data sets , that our model outperforms a large number of baseline systems , including the current state of the art model , and does so requiring significantly less in - domain data .",
    "although we focused specifically on discriminative modeling in a maximum entropy framework , we believe the novel , basic idea on which this work is founded  to break the in - domain distribution @xmath9 and out - of - domain distribution @xmath8 into three distributions , @xmath25 , @xmath23 and @xmath24is general .",
    "in particular , one could perform a similar analysis in the case of generative models and obtain similar algorithms ( though in the case of a generative model , standard em could be used ) .",
    "such a model could be applied to domain adaptation in language modeling or machine translation .    with the exception of the work described in section  [ prior - work ] , previous work",
    "in - domain adaptation is quite rare , especially in the discriminative learning framework .",
    "there is a substantial literature in the language modeling / speech community , but most of the adaptation with which they are concerned is based on adapting to new speakers @xcite . from a learning perspective , the mega model  is most similar to a mixture of experts model .",
    "our model can be seen as a _ constrained _ experts model , with three experts , where the constraints specify that in - domain data can only come from one of two experts , and out - of - domain data can only come from one of two experts ( with a single expert overlapping between the two ) . most attempts to build discriminative mixture of experts models make heuristic approximations in order to perform the necessary optimization @xcite , rather than apply conditional em , which gives us strict guarantees that we monotonically increase the data ( incomplete ) log likelihood of each iteration in training .",
    "the domain adaptation problem is also closely related to multitask learning ( also known as learning to learn and inductive transfer ) . in multitask learning ,",
    "one attempts to learn a function that solves many machine learning problems simultaneously .",
    "this related problem is discussed by , and , among others .",
    "the similarity between multitask learning and domain adaptation is that they both deal with data drawn from related , but distinct distributions .",
    "the primary difference is that domain adaptation cares only about predicting one label type , while multitask learning cares about predicting many .",
    "as the various sub - communities of the natural language processing family begin and continue to branch out into domains other than newswire , the importance of developing models for new domains without annotating much new data will become more and more important .",
    "the mega model  is a first step toward being able to migrate simple classification - style models ( classifiers and maximum entropy markov models ) across domains",
    ". continued research in the area of adaptation is likely to benefit from other work done in active learning and in learning with large amounts unannotated data .",
    "given the model structure and parameterization of the mega modelgiven in section  [ subsec : megam ] , eq  , we obtain the following expression for the joint probability of the data :    @xmath186\\right.\\nonumber\\\\ & & \\left.\\hspace{30 mm }            \\exp\\left[\\vec\\la_{y_n}^{z_n } \\t \\vec x_n\\right ]             \\left(\\sum_c \\exp\\left[\\vec\\la_c^{z_n } \\t \\vec x_n\\right]\\right)^{-1 }    \\right\\ } \\label{eq : joint}\\end{aligned}\\ ] ]    the marginal distribution is obtained by removing the last two terms ( the @xmath187 and the sum of @xmath187s ) from the final equation . plugging eq   into eq   and using the notation from eq  , we obtain the following expression for @xmath113 :    @xmath188 \\nonumber\\\\ & + &    \\sum_{n=1}^n \\bigg [      \\sum_{z_n } h_n \\bigg\\ {          z_n \\log \\pi + ( 1-z_n ) \\log ( 1-\\pi )        + \\log \\ps_{n , z_n } \\nonumber\\\\ & &   \\hspace{25 mm }        + \\sum_{f=1}^f x_{nf } \\log \\vec\\la_{y_n}^{z_n }        - \\log \\sum_c \\exp\\left[\\vec\\la_c^{z_n } \\t x_n\\right ]        - j_{n , z_n}^t        \\bigg\\ } \\nonumber\\\\ & &   \\hspace{15 mm }      - m_n^{t-1 } \\pi^{z_n } ( 1-\\pi)^{1-z_n }        \\ps_{n , z_n }      + 1 \\bigg ]   \\label{eq : megaq}\\end{aligned}\\ ] ]              @xmath192    \\nonumber\\\\    & + & \\pi^1 \\left[-1 + \\sum_{n=1}^n           \\(2 h_n - m_n^{t-1 } \\left(\\ps_{n,0 } - \\ps_{n,1}\\right)\\)\\right ]",
    "\\nonumber\\\\    & + & \\pi^0 \\left[-\\sum_{n=1}^n h_n \\right ]    \\label{eq : mstep - pi}\\end{aligned}\\ ] ]          @xmath193 =   \\sum_{n=1}^{n } \\(1-h_n\\ ) \\left\\ {      \\sum_{f=1}^{f } x_{nf } \\vec\\la_{y_n , f } -      \\log \\sum_c \\exp\\left[\\vec\\la_c \\t x_n\\right]\\right\\ }    + \\log \\nor(\\vec\\la ; 0 , \\si^2 i )     \\label{eq : mstep - phi-1}\\ ] ]    in eq  , the bracketed expression is exactly the log - likelihood term obtained for standard logistic regression models .",
    "thus , the optimization of @xmath125 with respect to @xmath67 and @xmath68 can be performed using a weighted version of standard logistic regression optimization , with weights defined by @xmath138 . in the case of @xmath69",
    ", we obtain a weighted logistic regression model , but over all @xmath194 data points , and with weights defined by @xmath129 .          due to the presence of the product term in @xmath81",
    ", we can not compute an analytical solution to this maximization problem .",
    "however , we can take derivatives component - wise ( in @xmath197 ) and obtain analytical solutions ( when combined with the prior ) .",
    "this admits an iterative solution for maximizing @xmath198 by maximizing each component separately until convergence .",
    "computing derivatives of @xmath113 with respect to @xmath199 requires differentiating @xmath200 with respect to @xmath199 ; this has a convenient form ( recalling the notation from table  [ tab : notation ] :        @xmath202 & = &     \\sum_{n=1}^n \\bigg [    \\(1-h_n\\ ) \\frac {            x_{nf } ( 1-\\ps_f ) -                    ( 1-x_{nf } ) \\ps_f }          { \\ps_f \\(1-\\ps_f\\ ) } \\label{eq : qbet00}\\\\ & & \\hspace{20mm}- j_{n,0}(1-\\pi ) \\ps_{n,0,-f } \\bigg ]    + \\frac 1 { \\ps_f \\(1-\\ps_f\\ ) } \\nonumber\\\\ & = &     \\frac 1 { \\ps_f \\(1-\\ps_f\\ ) }      \\left [ 1 + \\sum_{n=1}^n \\(1-h_n\\ ) ( x_{nf } - \\ps_f ) \\right ]    - \\sum_{n=1}^n j_{n,0}(1-\\pi ) \\ps_{n,0,-f } \\nonumber\\end{aligned}\\ ] ]      @xmath203 \\nonumber\\\\ & + & \\left(\\ps_f\\right)^1    \\left[-\\sum_{n=1}^n \\big(1-h_n + j_{n,0}(1-\\pi ) \\ps_{n,0,-f}\\big )      \\right ] \\nonumber\\\\ & + & \\left(\\ps_f\\right)^0 \\left[1 + \\sum_{n=1}^n",
    "\\(1-h_n\\ ) x_{nf}\\right ] \\label{eq : mstep - psi - beta00}\\end{aligned}\\ ] ]    this final equation can be solved analytically .",
    "a similar expression arises for @xmath204 . in the case of @xmath205",
    ", we obtain a quadratic form with sums over the entire data set and with @xmath129 replacing the occurrences of @xmath138 :    @xmath206 \\nonumber\\\\ & + & \\left(\\ps{^{\\textsf{(g)}}}_f\\right)^1    \\bigg[-\\sum_{n=1}^{n{^{\\textsf{(i ) } } } } \\big(h{^{\\textsf{(i)}}}_n + j{^{\\textsf{(i)}}}_{n,1}\\pi{^{\\textsf{(i)}}}\\ps{^{\\textsf{(i)}}}_{n,1,-f}\\big )          -\\sum_{n=1}^{n{^{\\textsf{(o ) } } } } \\big(h{^{\\textsf{(o)}}}_n + j{^{\\textsf{(o)}}}_{n,1}\\pi{^{\\textsf{(o)}}}\\ps{^{\\textsf{(o)}}}_{n,1,-f}\\big )      \\bigg ] \\nonumber\\\\ & + & \\left(\\ps{^{\\textsf{(g)}}}_f\\right)^0 \\left[1 + \\sum_{n=1}^{n{^{\\textsf{(i ) } } } } h{^{\\textsf{(i)}}}_n x{^{\\textsf{(i)}}}_{nf } +             \\sum_{n=1}^{n{^{\\textsf{(o ) } } } } h{^{\\textsf{(o)}}}_n x{^{\\textsf{(o)}}}_{nf } \\right ] \\label{eq : mstep - psi - beta00b}\\end{aligned}\\ ] ]                chelba , c.   acero , a. 2004 .",
    "adaptation of maximum entropy classifier : little data can help a lotin proceedings of the conference on empirical methods in natural language processing ( emnlp ) , barcelona , spain .",
    "lafferty , j. , mccallum , a. ,  pereira , f. 2001 .",
    "conditional random fields : probabilistic models for segmenting and labeling sequence datain proceedings of the international conference on machine learning ( icml ) .",
    "rambow , o. , shrestha , l. , chen , j. ,  lauridsen , c. 2004 .",
    "summarizing email threadsin proceedings of the conference of the north american chapter of the association for computational linguistics ( naacl ) short paper section .",
    "roark , b.   bacchiani , m. 2003 . supervised and unsupervised pcfg adaptation to novel domainsin proceedings of the conference of the north american chapter of the association for computational linguistics and human language technology ( naacl / hlt ) ."
  ],
  "abstract_text": [
    "<S> the most basic assumption used in statistical learning theory is that training data and test data are drawn from the same underlying distribution . </S>",
    "<S> unfortunately , in many applications , the `` in - domain '' test data is drawn from a distribution that is related , but not identical , to the `` out - of - domain '' distribution of the training data . </S>",
    "<S> we consider the common case in which labeled out - of - domain data is plentiful , but labeled in - domain data is scarce . </S>",
    "<S> we introduce a statistical formulation of this problem in terms of a simple mixture model and present an instantiation of this framework to maximum entropy classifiers and their linear chain counterparts . we present efficient inference algorithms for this special case based on the technique of conditional expectation maximization . </S>",
    "<S> our experimental results show that our approach leads to improved performance on three real world tasks on four different data sets from the natural language processing domain . </S>"
  ]
}