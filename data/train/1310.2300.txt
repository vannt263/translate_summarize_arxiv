{
  "article_text": [
    "the problem with interpreters for dynamically typed programming languages is that they are slow .",
    "the fundamental lack of performance is due to following reasons .",
    "first , their implementation is simple and does not perform known interpreter optimizations , such as threaded code  @xcite or superinstructions  @xcite .",
    "second , even if the interpreters apply these known techniques , their performance potential is severely constrained by expensive interpreter instruction implementations  @xcite .    unfortunately ,",
    "performance - conscious implementers suffer from having only a limited set of options at their disposal to solve this problem . for peak performance ,",
    "the current best - practice is to leverage results from dynamic compilation .",
    "but , implementing a just - in - time , or jit , compiler is riddled with many problems , e.g. , a lot of tricky details , hard to debug , and substantial implementation effort .",
    "an alternative route is to explore the area of purely interpretative optimization instead .",
    "these are optimizations that preserve innate interpreter characteristics , such as ease - of - implementation and portability , while offering important speedups .",
    "prior work in this area already reports the potential of doubling the execution performance  @xcite . as a result , investigating a general and principled strategy for optimizing high abstraction - level interpreters",
    "is particularly warranted .",
    "interpreting a dynamically typed programming language , such as javascript , python , or ruby , has its own challenges .",
    "frequently , these interpreters use one or a combination of the following features :    * dynamic typing to select type - specific operations , * reference counting for memory management , and * modifying boxed data object representations .    to cope with these features , interpreter instructions",
    "naturally become expensive in terms of assembly instructions required to implement their semantics .",
    "looking at successful research in just - in - time compilation , we know that in order to achieve substantial performance improvements , we need to reduce the complexity of the interpreter instructions implementation . put differently , we need to remove the overhead introduced by dynamic typing , reference counting , and operating on boxed data objects .    in this paper we combine ideas from staged compilation with partial evaluation and interpreter optimization to devise a general framework for interpreter optimization . from staged compilation ,",
    "we take the idea that optimizations can spread out across several stages . from partial evaluation",
    ", we take inspiration from the futamura projections to optimize programs . from interpreter optimization",
    ", we model a general theory of continuous optimization based on rewriting instructions .",
    "these ideas form the core of our framework , which is purely interpretative , i.e. , it offers ease - of - implementation and portability while avoiding dynamic code generation , and delivers high performance . as a result , close to three decades",
    "after deutsch and schiffman  @xcite described the ideas of what would eventually become the major field of just - in - time compilation , our framework presents itself as an alternative to implementing jit compilers .    summing up",
    ", this paper makes the following contributions .",
    "* we introduce a general theory for optimizing interpreter instructions that relies on speculatively staging of optimized interpreter instructions at interpreter compile - time and subsequent concerting at run - time ( see  ) .",
    "* we present a principled procedure to derive optimized interpreter instructions via partial evaluation . here , speculation allows us to remove previous approaches requirement to specialize towards a specific program ( see  ) .",
    "* we apply the general theory to the python interpreter and describe the relevant implementation details ( see  ) .",
    "* we provide results of a careful and detailed evaluation of our python based implementation ( see  ) , and report substantial speedups of up to more than four times faster than the cpython implementation for the benchmark . for this benchmark",
    "our technique outperforms the current state - of - the - art jit compiler , pypy 1.9 , by 14% .",
    "in this section we walk through a simple example that illustrates how interpreters address  or rather fail to address  the challenge of efficiently executing a high - level language .",
    "the following listing shows a python function ` sum ` that `` adds '' its parameters and returns the result of this operation .",
    ".... def sum(a , b ) :      return a + b ....    in fact , this code does not merely `` add '' its operands : depending on the actual types of the parameters ` a ` and ` b ` , the interpreter will select a matching operation . in python , this means that it will either concatenate strings , or perform arithmetic addition on either integers , floating point numbers , or complex numbers ; or the interpreter could even invoke custom python code  which is possible due to python s support for ad - hoc polymorphism .",
    "in 1984 , deutsch and schiffman  @xcite report that there exists a `` dynamic locality of type usage , '' which enables speculative optimization of code for any arbitrary but fixed and observed type @xmath0 .",
    "subsequent research into dynamic compilation capitalizes on this observed locality by speculatively optimizing code using _ type feedback _  @xcite . from their very beginning , these dynamic compilers  or just - in - time compilers as they are referred to frequently  had to operate within a superimposed latency time .",
    "put differently , dynamic compilers traditionally sacrifice known complex optimizations for predictable compilation times",
    ".    staged compilation provides a solution to the latency problem of jit compilers by distributing work needed to assemble optimized code among separate stages .",
    "for example , a staged compiler might break up an optimization to perform work at compile time , link - time , load - time , or finally at run - time .",
    "the problem with staged optimizations for high - level languages such as javascript , python , and ruby is that they require at least partial knowledge about the program .",
    "but , as the example of the ` sum ` function illustrates , only at run - time we will actually identify the concrete type @xmath0 for parameters ` a ` and ` b `",
    ".    for our target high - level languages and their interpreters , staged compilation is not possible , primarily due to two reasons .",
    "first , none of these interpreters have a jit compiler , i.e. , preventing staged partial optimizations .",
    "second , the stages of staged compilation and interpreters are separate .",
    "the traditional stages listed above need to be partitioned into stages that we need to assemble the interpreter ( viz .",
    "compile - time , link - time , and load - time ) , and separate stages of running the program : at interpreter run - time , it compiles , potentially links , loads and runs hosted programs .",
    "the previous section sketches the problem of performing traditional staged optimizations for interpreters . in this section we are first going to dissect interpreter performance to identify bottlenecks .",
    "next , we are going to describe which steps are necessary to formalize the problem , and subsequently use speculative staged interpreter optimizations to conquer them and achieve high performance .",
    "python s compiler will emit the following sequence of interpreter instructions , often called bytecodes , when compiling the ` sum ` function ( ignoring argument bytes for the ` load_fast ` instructions ) :    ( l1 ) [ draw , rectangle , minimum width=1.25cm , minimum height=1.25cm , align = center ] ` load ` + ` fast ` ; ( l2 ) at ( @xmath1 ) [ draw , rectangle , minimum width=1.25cm , minimum height=1.25cm , anchor = west , align = center ] ` load ` + ` fast ` ; ( add ) at ( @xmath2 ) [ draw , rectangle , minimum width=1.25cm , minimum height=1.25cm , anchor = west , align = center ] ` binary ` + ` add ` ; ( ret ) at ( @xmath3 ) [ draw , rectangle , minimum width=1.25cm , minimum height=1.25cm , anchor = west , align = center ] ` return ` + ` value ` ;    we see that the interpreter emits untyped , polymorphic instructions that rely on dynamic typing to actually select the operation .",
    "furthermore , we see that python s virtual machine interpreter implements a stack to pass operand data between instructions .",
    "let us consider the application ` sum(3 , 4 ) ` , i.e. , ` sum ` is used with the specific type @xmath4 . in this case , the ` binary_add ` instruction will check operand types and select the proper operation for the types . more precisely , assuming absence of ad - hoc polymorphism , the python interpreter will identify that both ` int ` operands are represented by a c ` struct ` called ` pylong_type ` .",
    "next , the interpreter will determine that the operation to invoke is ` pylong_type``->tp_as_number->nb_add ` , which points to the ` long_add ` function .",
    "this operation implementation function will then unbox operand data , perform the actual integer arithmetic addition , and box the result .",
    "in addition , necessary reference counting operations enclose these operations , i.e. , we need to decrease the reference count of the arguments , and increase the reference count of the result . both , ( un-)boxing and adjusting reference count operations add to the execution overhead of the interpreter .",
    "contrary to the interpreter , a state - of - the - art jit compiler would generate something like this    .... movq % rax , -8(%rsp ) movq % rbx , -16(%rsp ) addq % rax , % rbx ret ....",
    "the first two lines assume a certain stack layout to where to find operands ` a ` and ` b ` , both of which we assume to be unboxed .",
    "hence , we can use the native machine addition operation ( line 3 ) to perform arithmetic addition and return the operation result in ` % rax ` .    bridging the gap between the high abstraction - level representation of computation in python bytecodes and the low abstraction - level representation of native machine assembly instructions holds the key for improving interpreter performance .",
    "to that end , we classify both separate instruction sets accordingly :    * python s instruction set is untyped and operates exclusively on boxed objects . *",
    "native - machine assembly instructions are typed and directly modify native machine data .",
    "an efficient , low - level interpreter instruction set allows us to represent the ` sum ` function s computation for our assumed type in the following way :    ( l1 ) [ draw , rectangle , minimum width=1.25cm , minimum height=1.25cm , align = center ] ` load ` + ` int ` ; ( l2 ) at ( @xmath1 ) [ draw , rectangle , minimum width=1.25cm , minimum height=1.25cm , anchor = west , align = center ] ` load ` + ` int ` ; ( add ) at ( @xmath2 ) [ draw , rectangle , minimum width=1.25cm , minimum height=1.25cm , anchor = west , align = center ] ` int ` + ` add ` ; ( ret ) at ( @xmath3 ) [ draw , rectangle , minimum width=1.25cm , minimum height=1.25cm , anchor = west , align = center ] ` return ` + ` int ` ;    in this lower - level instruction set the instructions are typed , which allows using a different operand data passing convention , and directly modifying unboxed data  essentially operating at the same semantic level as the assembly instructions shown above ; disregarding the different architectures , i.e. , register vs.  stack .",
    "the previous section informally discusses the goal of our speculatively staged interpreter optimization : optimizing from high to low abstraction - level instructions .",
    "this section systematically derives required components for enabling interpreters to use this optimization technique .",
    "in contrast with staged compilation , staged interpreter optimization is purely interpretative , i.e. , it avoids dynamic code generation altogether .",
    "the key idea is that we :    * stage and compile new interpreter instructions at interpreter compile - time , * concert optimized instruction sequences at run - time by the interpreter .",
    "the staging process involves the ahead - of - time compiler that is used to compile the interpreter .",
    "therefore , this process exercises the compiler s backend to have portable code generation and furthermore allows the interpreter implementation to remain simple .",
    "the whole process is , however , speculative : only by actually interpreting a program , we know for sure which optimized interpreter instructions are required . as a result",
    ", we restrict ourselves to generate optimized code only for instructions that have a high likelihood of being used .    to assemble optimized instruction sequences at run - time , we rely on a technique known as _ quickening _  @xcite .",
    "quickening means that we replace instructions with optimized derivatives of the exact same instruction at run - time .",
    "prior work focuses on using only one level of quickening , i.e. , replacing one instruction with another instruction derivative . in this work ,",
    "we introduce _ multi - level quickening _ ,",
    "i.e. , the process of iteratively and incrementally rewriting interpreter instructions to ever more specialized derivatives of the generic instruction .      in this section",
    "we present a simplified substrate of a dynamically typed programming language interpreter , where we illustrate each of the required optimization steps .    .... data value = vbool bool             | vint int             | vfloat float             | vstring string    type stack = [ value ] type instr = stack - > stack ....",
    "we use ` value ` to model a small set of types for our interpreter . the operand stack ` stack `",
    "holds intermediate values , and an instruction ` instr ` is a function modifying the operand stack ` stack ` .",
    "consequently , the following implementation of the interpreter keeps evaluating instructions until the list of instructions is empty , whereupon it returns the top of the operand stack element as its result :    .... interp : : stack - > [ instr ] - > value interp ( x : xs ) [ ] = x interp s ( i : is )   =    let      s'= i s    in      eval s ' is ....    using ` interp ` as the interpreter , we can turn our attention to the actual interpreter instructions .",
    "the following example illustrates a generic implementation of a binary interpreter instruction , which we can instantiate , for example for an arithmetic add operation :    .... binaryop : : ( value - > value - > value ) - > stack - > stack binaryop f s =    let      ( x , s ' ) = pop s      ( y , s '' ) = pop s '    in     ( f x y):s ''    addop : : stack - > stack addop = binaryop dtadd    dtadd : : value - > value - > value dtadd ( vint x ) ( vint y ) = ( vint ( x + y ) ) dtadd ( vfloat x ) ( vfloat y ) = ( vfloat ( x + y ) ) dtadd ( vstring x ) ( vstring y ) = ( vstring ( x + + y ) ) dtadd ( vbool x ) ( vbool y ) = ( vbool ( x & & y ) ) $ \\ldots$ ....    the generic implementation ` binaryop ` shows the operand stack modifications all binary operations need to follow .",
    "the ` addop ` function implements the actual resolving logic of the dynamic types of ` value`s via pattern matching starting on line 13 in function ` dtadd ` .",
    "an instruction @xmath5 is an _ instruction derivative _ of an instruction @xmath6 , if and only if it implements the identical semantics for an arbitrary but fixed subset of @xmath6 s functionality .    in our example interpreter ` interp ` , the ` addop ` instruction has type @xmath7 .",
    "an instruction derivative ` intadd ` would implement the subset case of integer arithmetic addition only , i.e. , where operands have type ` vint ` . analogous cases are for all combinations of possible types , e.g. , for string concatenation ( ` vstring ` ) .    to obtain all possible instruction derivatives",
    "@xmath5 for any given interpreter instruction @xmath6 , we rely on insights obtained by partial evaluation  @xcite .",
    "the first futamura projection  @xcite states that we can derive a compiled version of an interpreted program @xmath8 by partially evaluating its corresponding interpreter ` interp ` written in language  @xmath9 : @xmath10\\ ] ] in our scenario , this is not particularly helpful , because we do not know program @xmath8 a priori .",
    "the second futamura projection tells us how to derive a compiler by applying ` mix ` to itself with the interpreter as its input : @xmath11\\ ] ] by using the interpreter ` interp ` as its input program , the second futamura projection eliminates the dependency on the input program  @xmath8 .",
    "however , for a dynamically typed programming language , a compiler derived by applying the second futamura projection without further optimizations is unlikely to emit efficient code  because it lacks information on the types  @xcite .",
    "our idea is to combine these two futamura projections in a novel way : @xmath12\\ ] ] that means that for all instructions @xmath6 of an interpreter ` interp ` , we derive an optimized instruction derivative @xmath5 specialized to a type @xmath0 by partially evaluating an instruction @xmath6 with the type @xmath0 .",
    "hence , we speculate on the likelihood of the interpreter operating on data of type @xmath0 but do eliminate the need to have a priori knowledge about the program @xmath8 .    to preserve semantics , we need to add a guard statement to @xmath5 that ensures that the actual operand types match up with the specialized ones .",
    "if the operand types do not match , we need to take corrective measures and redirect control back to @xmath6 . for example",
    ", we get the optimized derivative ` intadd ` from ` dtadd ` by fixing the operand type to ` vint ` : @xmath13\\ ] ]    .... intadd : : value - > value - > value intadd ( vint x ) ( vint y ) = ( vint ( x + y ) ) intadd x y = dtadd x y ....    the last line in our code example acts as a guard statement , since it enables the interpreter to execute the type - generic ` dtadd ` instruction whenever the speculation of ` intadd ` fails .",
    "the interpreter can now capitalize on the `` dynamic locality of type usage ''  @xcite and speculatively eliminate the overhead of dynamic typing by rewriting an instruction @xmath6 at position @xmath14 of a program @xmath8 to its optimized derivative @xmath5 : @xmath15\\ ] ]    it is worth noting that this rewriting , or _ quickening _ as it is commonly known , is purely interpretative , i.e. , it does not require any dynamic code generation  simply updating the interpreted program suffices :    .... quicken : : [ instr ] - > int - > instr - > [ instr ] quicken $ \\pi$ p derivative =    let      ( x , y : ys)=",
    "splitat p $ \\pi$      r= x + + derivative : ys ....    using this derivation step , we effectively create a typed interpreter instruction set for an instruction set originally only consisting of untyped interpreter instructions .      taking a second look at the optimized derivative instruction ` intadd ` shows that it still contains residual overhead : unboxing the ` vint ` operands and boxing the ` vint ` result ( cf .",
    "line three ) .",
    "it turns out that we can do substantially better by identifying complete expressions that operate on the same type and subsequently optimizing the whole sequence .",
    "for identifying a sequence of instructions that operate on the same type , we leverage the type information collected at run - time via the first - level quickening step described in the previous section . during interpretation",
    ", the interpreter will optimize programs it executes on the fly and single instruction occurrences will carry type information . to expand this information to bigger sequences",
    ", we use an abstract interpreter that propagates the collected type information .",
    "once we have identified a complete sequence of instructions operating on the same type , we then quicken the complete sequence to another set of optimized derivatives that directly modify unboxed data .",
    "since these instructions operate directly on unboxed data , we need to take care of several pending issues .",
    "first , operating on unboxed data requires modifying the operand stack data passing convention .",
    "the original instruction set , as well as the optimized typed instruction set , operates on boxed objects , i.e. , all elements on the operand stack are just pointers to the heap , having the type of one machine word ( ` uint64 ` on modern 64-bit architectures ) . if we use unboxed data elements , such as native machine integers and floats , we need to ensure that all instructions follow the same operand passing convention .",
    "all instructions operating on untyped data need to follow the same operand stack data passing convention .",
    "therefore , we define a conversion function  @xmath16 to map data to and from at least one native machine word : @xmath17    second , we need to provide and specify dedicated ( un-)boxing operations to unbox data upon entering a sequence of optimized interpreter instructions , and box results when leaving an optimized sequence .",
    "we define a function @xmath18 to map objects to at least one machine word and conversely from at least one machine word back to proper language objects of type @xmath19 .",
    "@xmath20    third , this optimization is speculative , i.e. , we need to take precautions to preserve semantics and recover from misspeculation . preserving semantics of operating on unboxed data",
    "usually requires to use a tagged data format representation , where we reserve a set of bits to hold type information .",
    "but , we restrict ourselves to sequences where we know the types _ a priori _ , which allows us to remove the restrictions imposed by using a tagged data format , i.e. , additional checking code and decreasing range of representable data . in general , whenever our speculation fails we need to generalize specialized instruction occurrences back to their more generic instructions and resume regular interpretation .",
    "in all definitions , we use the @xmath21 notation to indicate that a concrete instantiation of either @xmath16 or @xmath18 is able to project data onto multiple native machine words .",
    "for example , the following implementation section will detail one such case where we represent python s complex numbers by two native machine words .",
    "[ [ abstract - interpretation ] ] abstract interpretation + + + + + + + + + + + + + + + + + + + + + + +    taking inspiration from leroy s description of java bytecode verification  @xcite , we also use an abstract interpreter that operates over types instead of values . using the type information captured in the previous step , for example from quickening from the type generic ` addop ` to the optimized instruction derivative ` intadd ` , we can propagate type information from operation instructions to the instructions generating its operands .",
    "for example , we know that the ` intadd ` instruction expects its operands to have type ` int ` and produces an operand of type ` int ` : @xmath22 similar to our actual interpreter , the abstract interpreter uses a transition relation @xmath23 to represent an instruction @xmath24 s effect on the operand stack .",
    "all interpreter instructions of the original instruction set are denoted by type - generic rules that correspond to the top element of the type lattice , i.e. , in our case ` value ` .",
    "the following rules exemplify the representation , where we only separate instructions by their arity : @xmath25    the set of types our abstract interpreter operates on corresponds to the set of types we generated instruction derivatives for in the first - level quickening step , i.e. , ( ` int ` , ` bool ` , ` float ` , ` string ` ) . for simplicity",
    ", our abstract interpreter ignores branches in the code , which limits our scope to propagate types along straight - line code , but on the other hand avoids data - flow analysis and requires only one linear pass to complete .",
    "this is important insofar as we perform this abstract interpretation at run - time and therefore are interested to keep latency introduced by this step at a minimum .",
    "type propagation proceeds as follows .",
    "the following example shows an original example program representation as emitted by some other program , e.g. , another compiler : @xmath26\\ ] ] after executing this example program , the first - level quickening captures types encountered during execution : @xmath27\\ ] ] now , we propagate the type information by abstract interpretation .",
    "since ` intadd ` expects operands of type ` int ` , we can infer that the first two ` push ` instructions must push operands of type ` int ` onto the operand stack .",
    "analogously , the second occurrence of ` intadd ` allows us to infer that the result of the first ` intadd ` has type ` int ` , as does the third occurrence of the ` push ` instruction . finally ,",
    "by inspecting the type stack when the abstract interpreter reaches the ` pop ` instruction , we know that it must pop an operand of type ` int ` off the stack .",
    "therefore , after type propagation our abstract interpreter will have identified that the complete sequence of instructions actually operate exclusively on data of type ` int ` : @xmath28\\ ] ] we denote the start and end instructions of a candidate sequence by @xmath29 and @xmath30 , respectively . in our example",
    ", the six element sequence starts with the first ` push ` instruction , and terminates with the pop instruction terminates : @xmath31    [ [ unboxed - instruction - derivatives ] ] unboxed instruction derivatives + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    analogous to the previous partial evaluation step we use to obtain the typed instructions operating exclusively on boxed objects , we can use the same strategy to derive the even more optimized derivatives . for regular operations , such as our ` intadd ` example , this is simple and straightforward , as we just replace the boxed object representation ` value ` by its native machine equivalent ` int ` :    .... intadd ' : : int - > int - > int intadd ' x y =   x + y ....    as a result , the compiler will generate an efficient native machine addition instruction and completely sidestep the resolving of dynamic types , as well as ( un-)boxing and reference counting operations . the problem with this ` intadd ` instruction derivative is , however , that we can not perform a type check on the operands anymore , as the implementation only allows operands of type ` int ` . in consequence , for preserving semantics of the original interpreter instructions , we need a new strategy for type checking .",
    "our solution to this problem is to bundle type checks and unboxing operations together via function composition and move them to the load instructions which push unboxed operands onto the stack . assuming that we modify the declaration of ` stack ` to contain heterogeneous data elements ( i.e. , not only a list of ` value ` , but also unboxed native machine words , denoted by @xmath32 ) , pushing an unboxed integer value onto the operand stack looks like this :    .... pushint : : value - > $ \\mathfrak{s}$ - > $ \\mathfrak{s}$ pushint v s =    let      unboxconvert= $ \\mathfrak{c}_{\\mathtt{int64 } } \\cdot \\mathfrak{m}_{\\mathtt{vint}}$    in     case v of       vint value - > ( unboxconvert value ) : s       _           - > -- misspeculation , generalize ....    since the operand passing convention is type dependent , we can not use the previous implementation of ` binaryop ` anymore , and need a typed version of it :    .... binaryintop : : ( int - > int - > int ) - > $ \\mathfrak{s}$ - > $ \\mathfrak{s}$ binaryintop f s =    let      popint = $ \\mathfrak{c}^{-1}_{\\mathtt{int64 } } \\cdot \\mathtt{pop}$      pushint = $ \\mathfrak{c}_{\\mathtt{int64 } } \\cdot \\mathtt{f}$      ( x , s ' ) = popint s      ( y , s '' ) = popint s '    in     ( pushint x y ) : s '' ....    finally , we need to make sure that once we leave an optimized sequence of instructions , the higher level instruction sets continue to function properly .",
    "hence , we need to box all objects the sequence computes at the end of the optimized sequence . for example , if we have a store instruction that saves a result into the environment , we need to add boxing to its implementation :    .... storeintop : : $ \\mathfrak{s}$ - > env - > string - > $ \\mathfrak{s}$ storeintop s e ident =    let      boxpopint = $ \\mathfrak{m}^{-1}_{\\mathtt{vint } } \\cdot \\mathfrak{c}^{-1}_{\\mathtt{int64 } } \\cdot \\mathtt{pop}$      ( obj , s ' ) = boxpopint s    in      ( \\x ( update e ident obj ) ) - > s ' ....    [ [ generalizing - when - speculation - fails ] ] generalizing when speculation fails + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    in our example of the ` pushint ` interpreter instruction , we see on the last line that there is a case when speculation fails . a specific occurrence of ` pushint ` verifies that the operand matches an expected type such that subsequent instructions can modify on its unboxed representation .",
    "therefore , once the interpreter detects the misspeculation , we know that we have to invalidate all subsequent instructions that speculate on that specific type .    the interpreter can back out of the misspeculation and resume sound interpretation by i ) finding the start of the speculatively optimized sequence , and ii ) generalizing all specialized instructions up by at least one level .",
    "both of these steps are trivial to implement , particularly since the instruction derivation steps result in having three separate instruction sets .",
    "hence , the first step requires to identify the first instruction @xmath24 that does _ not _ belong to the current instruction set , which corresponds to the predecessor of the start instruction @xmath29 identified by our abstract interpreter .",
    "the second step requires that we map each instruction starting at offset @xmath33 back to its more general parent instruction  a mapping we can create and save when we create the initial mapping from an instruction @xmath6 to its derivative @xmath5 .",
    "to be sound , this procedure requires that we further restrict our abstract interpreter to identify only sequences that have no side - effects . as a result ,",
    "we eliminate candidate sequences that have call instructions in between .",
    "this is however only an implementation limitation and not an approach limitation , since all non - local side - effects possible through function calls do not interfere with the current execution .",
    "therefore , we would need to ensure that we do not re - execute already executed functions and instead push the boxed representation of a function call s result onto the operand stack .      [ 1 ] ;    \\(s ) @xmath29 ; ( e ) [ right of = s ] @xmath30 ; ( s1 ) [ below of = s ] @xmath34 ; ( e1 ) [ below of = e ] @xmath35 ; ( s0 ) [ above of = s ] @xmath36 ; ( e0 ) [ above of = e ] @xmath37 ; ( i ) at ( @xmath38 ) @xmath39 ;    ( s0a ) at ( @xmath40 ) @xmath36 ; ( e0a ) at ( @xmath41 ) @xmath37 ;    ( ideriv ) at ( @xmath42 ) @xmath43 ;    plot[smooth cycle ] coordinates ( @xmath44 ) ( @xmath45 ) ( @xmath46 ) ( @xmath47 ) ( @xmath48 ) ; ( befores ) [ node distance=1 cm , left of = s0 ] @xmath49 ; ( aftere ) [ node distance=1 cm , right of = e0 ] @xmath49 ;    ( step0 ) at ( @xmath50 ) 0 ; ( step1 ) at ( @xmath51 ) 1 ; ( step2 ) at ( @xmath52 ) 2 ; ( step3 ) at ( @xmath53 ) 3 ; ( desc0 ) at ( @xmath54 ) original instructions ; ( desc1 ) at ( @xmath55 ) 1@xmath56-level : type feedback ; ( desc2 ) at ( @xmath57 ) abstract interpretation ; ( desc3 ) at ( @xmath58 ) 2@xmath59-level : unboxed data ;    ( befores ) to node ( s0 ) ; ( e0 ) to node ( aftere ) ;    ( s0 ) to node ( i ) ; ( i ) to node ( e0 ) ;    ( s0a ) to node ( ideriv ) ; ( ideriv ) to node ( e0a ) ;    \\(i ) to node ( ideriv ) ; ( ideriv ) to node ( s ) ; ( ideriv ) to node ( e ) ;    \\(s ) to node [ swap ] @xmath60 ( s1 ) ; ( e1 ) to node [ swap ] @xmath61 ( e ) ;    \\(s ) to node ( e ) ; ( s1 ) to node[above ] @xmath62 node[below ] @xmath63 ( e1 ) ;    ( s.south east)(e.south west ) ; ( s1.north east)(e1.north west ) ;    ( @xmath64 ) to ( @xmath65 ) ;    shows , in a general form , how speculative staging of interpreter optimizations works .",
    "the first part of our technique requires speculatively staging optimized interpreter instructions at interpreter compile - time . in",
    ", the enclosing shaded shape highlights these staged instructions .",
    "we systematically derive these optimized interpreter instructions from the original interpreter instructions .",
    "the second part of our technique requires run - time information and concerts the speculatively staged interpreter instructions such that optimized interpretation preserves semantics .",
    "the interpreter starts out executing instructions belonging to its original instruction set ( see   in  ) . during execution",
    ", the interpreter can capture type feedback by rewriting itself to optimized instruction derivatives : shows this in step  , where instruction @xmath66 replaces the more generic instruction @xmath39 , thereby capturing the type information @xmath19 at offset @xmath67 .",
    "the next step ,  , propagates the captured type information @xmath19 to a complete sequence of instructions , starting with instruction @xmath29 and terminating in instruction @xmath30 .",
    "we use an abstract interpreter to identify candidate sequences operating on the same type .",
    "once identified ,   of   illustrates how we rewrite a complete instruction sequence @xmath68 to optimized instruction derivatives @xmath69 .",
    "this third instruction set operates on unboxed native machine data types , and therefore requires generic ways to handle ( un-)boxing of objects of type @xmath19 ( cf .",
    "@xmath60 and @xmath61 ) , as well as converting data to and from the operand stack ( cf .",
    "@xmath62 and @xmath63 ) .",
    "all instructions circled by the dotted line of   are instruction derivatives that we speculatively stage at interpreter compile - time .",
    "( orig ) at ( 0,0 ) ` load_fast ` ` load_fast ` ` load_fast ` ` binary_mult ` ` load_fast ` ` load_fast ` ` binary_mult ` ` binary_add ` ` load_fast ` ` load_fast ` ` binary_mult ` ` binary_add ` ` load_const ` ` binary_power ` ` binary_mult ` ` store_fast ` ;    ( inca ) at ( 6,0 ) ` load_fast ` ` load_fast ` ` load_fast ` ` load_fast ` ` load_fast ` ` load_fast ` ` load_fast ` ` load_const ` ` store_fast ` ;    ( namaste ) at ( 12,0 ) ` load_fast ` ` load_fast ` ` load_fast ` ` mult ` ` load_fast ` ` load_fast ` ` mult ` ` add ` ` load_fast ` ` load_fast ` ` mult ` ` add ` `",
    "load_const ` ` power ` ` mult ` ` store_fast ` ;    at ( @xmath70 ) ( origlabel ) original python + bytecode ; at ( @xmath71 ) ( incalabel ) after type feedback + via quickening ; at ( @xmath72 ) ( namalabel ) after type propagation + and quickening ;     ( orig.four east ) to node [ below ] ( inca.four west ) ; ( orig.seven east ) to node [ below ] ( inca.seven west ) ; ( orig.eight east ) to node [ below ] ` inca ` ( inca.eight west ) ; ( orig.eleven east ) to node [ below ] ( inca.eleven west ) ; ( orig.twelve east ) to node [ below ] ( inca.twelve west ) ; ( orig.fourteen east ) to node [ below ] ( inca.fourteen west ) ; ( orig.fifteen east ) to node [ below ] ( inca.fifteen west ) ;     ( inca.four east ) to [ out=0 , in=180 ] ( namaste.two west ) ; ( inca.four east ) to [ out=0 , in=180 ] node [ above right , near end ] @xmath73 ( namaste.three west ) ;     ( inca.seven east ) to [ out=0 , in=180 ] ( namaste.five west ) ; ( inca.seven east ) to [ out=0 , in=180 ] node [ above right , near end ] @xmath73 ( namaste.six west ) ;     ( inca.eleven east ) to [ out=0 , in=180 ] ( namaste.nine west ) ; ( inca.eleven east ) to [ out=0 , in=180 ] node [ above right , near end ] @xmath73 ( namaste.ten west ) ;     ( inca.fourteen east ) to [ out=0 , in=180 ] node [ above right , near end ] @xmath73 ( namaste.thirteen west ) ; ( inca.fifteen east ) to [ out=0 , in=180 ] node [ midway , fill = white ] @xmath73 ( namaste.text west ) ; ( inca.fifteen east ) to [ out=0 , in=180 ] node [ above right , near end ] @xmath73 ( namaste.sixteen west ) ;     ( inca.eight east ) to [ out=0 , in=180 ] ( namaste.seven west ) ; ( inca.eight east ) to [ out=0 , in=180 ] ( namaste.four west ) ;     ( inca.twelve east ) to [ out=0 , in=180 ] ( namaste.eleven west ) ; ( inca.twelve east ) to [ out=0 , in=180 ] ( namaste.eight west ) ;     ( inca.fourteen east ) to [ out=0 , in=180 ] ( namaste.twelve west ) ; ( inca.fifteen east ) to [ out=0 , in=180 ] ( namaste.fourteen west ) ;",
    "this section presents implementation details of how we use speculative staging of optimized interpreter instructions to substantially optimize execution of a python 3 series interpreter .",
    "this interpreter is an implementation vehicle that we use to demonstrate concrete instantiations of our general optimization framework .",
    "we use an example sequence of python instructions to illustrate both the abstract interpretation as well as deriving the optimized interpreter instructions .",
    "python itself is implemented in c and we use casts to force the compiler to use specific semantics",
    ".    shows our example python instruction sequence and how we incrementally and iteratively rewrite this sequence using our speculatively staged optimized interpreter instruction derivatives .",
    "we use the prefix ` inca ` to indicate optimized interpreter instruction derivatives used for inline caching , i.e. , the first - level quickening .",
    "the third instruction set uses the ` nama ` prefix , which abbreviates native machine execution since all instructions directly operate on machine data and hence use efficient machine instructions to implement their operation semantics .",
    "this corresponds to the second - level quickening .",
    "note that the  ` nama ` instruction set is portable by construction , as it leverages the back - end of the ahead - of - time compiler at interpreter compile - time .",
    "the ` load_fast ` instruction pushes a local variable onto the operand stack , and conversely ` store_fast ` pops an object off the operand stack and stores it in the local stack frame . lists the set of eligible start and end instructions for our abstract interpreter , and   illustrates the data flow of the instruction sequence as assembled by the abstract interpreter . in our example",
    ", the whole instruction sequence operates on a single data type , but in general the abstract interpreter needs to be aware of the type lattice implemented by the python interpreter . for example , dividing two long numbers results in a float number and comparing two complex numbers results in a long number .",
    "we model these type conversions as special rules in our abstract interpreter .",
    ".valid start and end instructions used for abstract interpretation.[tab : start - end - instrs ] [ cols=\"<,<\",options=\"header \" , ]     [ [ space - requirements ] ] space requirements + + + + + + + + + + + + + + + + + +    presents the effect of implementing our speculatively staged ` mlq ` python interpreter on the binary size of the executable .",
    "we see that going from a switch - based interpreter to a threaded code interpreter requires additional 12 kb of space .",
    "finally , we see that adding two additional instruction sets to our python interpreter requires less than 110 kb of additional space ( when discounting the space requirement from threaded code ) .",
    "the most obvious take - away from   is that there is clearly a varying optimization potential when using our optimization . upon close investigation",
    ", we found that this is due to our minimal set of eligible start and end instructions ( see  ) .",
    "for example , there are other candidates for start instructions that we do not currently support , such as ` load_attr ` , ` load_name ` , ` load_global ` , ` load_deref ` . in consequence , expanding the abstract interpreter to cover more cases , i.e. , more instructions and more types , will improve performance even further",
    ". performs best , because our abstract interpreter finds that all of the instructions of its most frequently executed function ( ` eval_a ` ) can be optimized .    finally , we were surprised about the performance comparison with pypy .",
    "first , it is striking that we outperform pypy 1.9 on the benchmark .",
    "since we include start - up and warm - up times , we decided to investigate whether this affects our result .",
    "we timed successive runs with higher argument numbers ( 1000 , 1500 , 2000 , and 4000 ) and verified that our interpreter maintains its performance advantage .",
    "besides this surprising result , we think that the performance improvement of our interpreter lays a strong foundation for further optimizations .",
    "for example , we believe that implementing additional instruction - dispatch based optimizations , such as superinstructions  @xcite or selective inlining  @xcite , should have a substantial performance impact .",
    "second , we report that the interpreter data from   compares favorably with pypy , too . using ` sloccount ` on the ` pypy ` directory on branch ` version-1.9 ` gives the following results . for the ` interpreter",
    "` directory , ` sloccount ` computes 25,991 , and for the ` jit ` directory 83,435 lines of python code .",
    "the reduction between the 100kloc of pypy and the 6.5kloc of ` mlq ` is by a factor of almost 17@xmath74 .",
    "this is a testament to the ease - of - implementation property of interpreters , and also of purely interpretative optimizations in general .",
    "[ [ partial - evaluation ] ] partial evaluation + + + + + + + + + + + + + + + + + +    in 1996 , leone and lee  @xcite present their implementation of an optimizing ml compiler that relies on run - time feedback .",
    "interestingly , they mention the basic idea for our system :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ it is possible to pre - compile several alternative templates for the same code sequence and choose between them at run time , but to our knowledge this has never been attempted in practice . _",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    substituting `` interpreter instructions''or derivatives , as we frequently refer to them  for the term `` templates '' in the quote , reveals the striking similarity .",
    "in addition , both approaches leverage the compiler back - end of the ahead - of - time compiler assembling the run - time system  in our case the interpreter .",
    "this approach therefore automatically supports all target architectures of the base - compiler and hence there is no need for building a custom back - end .    in similar vein to leone and lee , other researchers addressed the prohibitive latency requirements of dynamic compilation  @xcite by leveraging ideas from partial evaluation .",
    "while we take inspiration from these prior results , we address the latency problem superimposed by dynamic code generation by _ avoiding _ it altogether .",
    "instead , we speculate on the likelihood of the interpreter using certain kinds of types and derive optimized instructions for them . at run - time , we rely on our novel procedure of concerting these optimized derivatives via abstract interpretation driven multi - level quickening .",
    "that being said , since these approaches are orthogonal , we believe that there are further advancements to be had by combining these approaches . for example  c , or the recently introduced terra / lua  @xcite , could be used to either stage the optimized derivatives inside of the interpreter source code , or generate the necessary derivatives at run - time , thereby eliminating the speculation part .",
    "the initial optimization potential of partial evaluation applied to interpreters goes back to futamura in 1971  @xcite .",
    "but , prior work has repeatedly revisited this specific problem . in particular , thibault et al .",
    "@xcite analyze the performance potential of partially evaluated interpreters and report a speedup of up to four times for bytecode interpreters .",
    "this result is intimately related to our work , in particular since they note that partial evaluation primarily targets instruction dispatch when optimizing interpreters  similar to the first futamura projection . in 2009 ,",
    "brunthaler established that instruction dispatch is not a major performance bottleneck for our class of interpreters  @xcite . instead ,",
    "our approach targets known bottlenecks in instruction implementation : dynamic typing , reference counting operations , and modifying boxed value representations .",
    "glck and jrgensen also connect interpreters with partial evaluation  @xcite , but as a means to optimize results obtained by applying partial evaluation .",
    "our technique should achieve similar results , but since it is speculative in nature , it does not need information of the actual program p that is interpreted , which is also a difference between our work and thibault et al .",
    "@xcite .",
    "[ [ interpreter - optimization ] ] interpreter optimization + + + + + + + + + + + + + + + + + + + + + + + +    the most closely related work in optimizing high - level interpreters is due to brunthaler  @xcite .",
    "in fact , the first - level quickening step to capture type feedback goes back to the discovery by brunthaler , and we have compared his publicly available system against our new technique .",
    "in addition to the second - level quickening that targets the overheads incurred by using boxed object representations , we also describe a principled approach to using partial evaluation for deriving instructions .",
    "[ [ just - in - time - compilers ] ] just - in - time compilers + + + + + + + + + + + + + + + + + + + + + +    type feedback has a long and successful history in just - in - time compilation . in 1994 , hlzle and ungar  @xcite discuss how the compiler uses type feedback to inline frequently dispatched calls in a subsequent compilation run .",
    "this reduces function call overhead and leads to a speedup by up to a factor of 1.7 . in general , subsequent research gave rise to adaptive optimization in just - in - time compilers  @xcite .",
    "our approach is similar , except that we use type feedback for optimizing the interpreter .    in 2012",
    ", there has been work on `` repurposed jit compilers , '' or rjits , which take an existing just - in - time compiler for a statically typed programming language and add support for a dynamically typed programming language on top  @xcite .",
    "this approach is interesting , because it tries to leverage an existing just - in - time compilation infrastructure to enable efficient execution of higher abstraction - level programming languages ",
    "similar to what has been described earlier in 2009 by bolz et al .",
    "@xcite and yermolovich et al .",
    "@xcite , but more invasive . unfortunately",
    ", the rjit work is unaware of recent advances in optimizing interpreters , and therefore misses some important optimization opportunities available to a repurposed just - in - time compiler .",
    "wrthinger et al .",
    "@xcite found that obtaining information from the interpreter has substantial potential to optimize jit compilation , and we anticipate that this is going to have major impact on the future of dynamic language implementation .",
    "regarding traditional just - in - time compilers , python nowadays only has one mature project : pypy  @xcite .",
    "pypy follows a trace - based jit compilation strategy and achieves substantial speedups over standard cpython .",
    "however , pypy has downsides , too : because its internals differ from cpython , it is not compatible with many third party modules written in c. our comparison to pypy finds that it is a much more sophisticated system offering class - leading performance on some of our benchmarks .",
    "surprisingly , we find that our technique outperforms pypy by up to 14% on the benchmark , and requires substantially less implementation effort .    [ [ miscellaneous ] ] miscellaneous + + + + + + + + + + + + +    prior research addressed the importance of directly operating on unboxed data  @xcite .",
    "there are certain similarities , e.g. , leroy s use of the ` wrap ` and ` unwrap ` operators are related to our ( un-)boxing functions , and there exist similar concerns in how to represent bits in a uniform fashion .",
    "the primary difference to the present work is that we apply this to a different language , python , which has a different sets of constraints and is dynamically typed .    in 1998 , shields et al .",
    "@xcite address overhead in dynamic typing via staged type inference .",
    "this is an interesting approach , but it is unclear if or how efficient this technique scales to python - like languages .",
    "our technique is much simpler , but we believe it could very well benefit of a staged inference step .",
    "we present a general theory and framework to optimize interpreters for high - level languages such as javascript , python , and ruby .",
    "traditional optimization techniques such as ahead - of - time compilation and partial evaluation only have limited success in optimizing the performance of these languages .",
    "this is why implementers usually resort to the expensive implementation of dynamic compilers  evidenced by the substantial industry efforts on optimizing javascript .",
    "our technique preserves interpreter characteristics , such as portability and ease of implementation , while at the same time enabling substantial performance speedups .",
    "this important speedup is enabled by peeling off layers of redundant complexity that interpreters conservatively re - execute instead of capitalizing on the `` dynamic locality of type usage''almost three decades after deutsch and schiffman described how to leverage this locality for great benefit .",
    "we capitalize on the observed locality by speculatively staging optimized interpreter instruction derivatives and concerting them at run - time .",
    "first , we describe how speculation allows us to decouple the partial evaluation from any concrete program .",
    "this enables a principled approach to deriving the implementation of optimized interpreter instruction derivatives by speculating on types the interpreter will encounter with a high likelihood .",
    "second , we present a new technique of concerting optimized interpreter instructions at run - time . at the core",
    ", we use a multi - level quickening technique that enables us to optimize untyped instructions operating on boxed objects down to typed instructions operating on native machine data .    from a practical perspective",
    ", our implementation and evaluation of the python interpreter confirms that there is a huge untapped performance potential waiting to be set free .",
    "regarding the implementation , we were surprised how easy it was to provide optimized instruction derivatives even without automated support by partial evaluation .",
    "the evaluation indicates that our technique is competitive with a dynamic compiler w.r.t .",
    "performance _ and _ implementation effort : besides the speedups by a factor of up to 4.222 , we report a reduction in implementation effort by about  17@xmath74 .",
    "m.  berndl , b.  vitale , m.  zaleski , and a.  d. brown .",
    "context threading : a flexible and efficient dispatch technique for virtual machine interpreters . in _ proceedings of the 3rd ieee / acm international symposium on code generation and optimization , san jose , ca , usa ,",
    "march 20 - 23 , 2005 ( cgo  05 ) _ , pages 1526 , 2005 .    c.  f. bolz , a.  cuni , m.  fijakowski , and a.  rigo .",
    "tracing the meta - level : pypy s tracing jit compiler . in _ proceedings of the 4th workshop on the implementation , compilation , optimization of object - oriented languages and programming systems ( icooolps  09 ) _ , lecture notes in computer science , pages 1825 .",
    "springer , 2009 .",
    "isbn 978 - 3 - 642 - 03012 - 3 .",
    "doi : http://doi.acm.org/10.1145/1565824.1565827 .",
    "s.  brunthaler .",
    "virtual - machine abstraction and optimization techniques . in _ proceedings of the 4th international workshop on bytecode semantics , verification , analysis and transformation , york , united kingdom ,",
    "march 29 , 2009 ( bytecode  09 ) _ , volume 253(5 ) of _ electronic notes in theoretical computer science _",
    ", pages 314 , amsterdam , the netherlands , december 2009 .",
    "doi : http://dx.doi.org/10.1016/j.entcs.2009.11.011 .",
    "s.  brunthaler .",
    "inline caching meets quickening . in _ proceedings of the 24th european conference on object - oriented programming , maribor , slovenia , june 21 - 25 , 2010 ( ecoop  10 ) _ , volume 6183/2010 of _ lecture notes in computer science _ , pages 429451 .",
    "springer , 2010 .",
    "isbn 978 - 3 - 642 - 03012 - 3 .",
    "doi : http://dx.doi.org/10.1007/978-3-642-03013-0 .",
    "s.  brunthaler .",
    "efficient interpretation using quickening . in _ proceedings of the 6th symposium on dynamic languages , reno , nv , usa , october 18 , 2010 ( dls  10 ) _ , pages 114 , new york , ny , usa , 2010 .",
    "acm press .",
    "isbn 978 - 3 - 642 - 03012 - 3 .",
    "doi : http://dx.doi.org/10.1007/978-3-642-03013-0 .",
    "k.  casey , d.  gregg , and m.  a. ertl .",
    "tiger  an interpreter generation tool . in _ proceedings of the 14th international conference on compiler construction , edinburgh , united kingdom ,",
    "april 4 - 8 , 2005 ( cc  05 ) _ , volume 3443/2005 of _ lecture notes in computer science _ , pages 246249 .",
    "springer , 2005 .",
    "isbn 3 - 540 - 25411 - 0 .",
    "doi : http://dx.doi.org/10.1007/978-3-540-31985-6_18 .",
    "j.  g. castanos , d.  edelsohn , k.  ishizaki , p.  nagpurkar , t.  nakatani , t.  ogasawara , and p.  wu . on the benefits and pitfalls of extending a statically typed language jit compiler for dynamic scripting languages . in _ proceedings of the 27th acm sigplan conference on object oriented programming : systems , languages , and applications , tucson , az , usa , october 21 - 25 , 2012 ( oopsla  12 ) _ , pages 195212 , 2012 .",
    "doi : http://doi.acm.org/10.1145/2384616.2384631 .    c.  chambers . staged compilation .",
    "in _ proceedings of the acm sigplan workshop on partial evaluation and semantics - based program manipulation , portland , or , usa , january 14 - 15 , 2002 ( pepm  02 ) _ , pages 18 , 2002 .",
    "doi : http://doi.acm.org/10.1145/503032.503045 .",
    "l.  p. deutsch and d.  g. bobrow .",
    "an efficient , incremental , automatic garbage collector .",
    "_ communications of the acm _ , 190 ( 9):0 522526 , 1976 .",
    "issn 0001 - 0782 .",
    "doi : http://doi.acm.org/10.1145/360336.360345 .",
    "l.  p. deutsch and a.  m. schiffman .",
    "efficient implementation of the smalltalk-80 system . in _ proceedings of the sigplan",
    "84 symposium on principles of programming languages ( popl  84 ) _ , pages 297302 , new york , ny , usa , 1984 .",
    "isbn 0 - 89791 - 125 - 3 .",
    "doi : http://doi.acm.org/10.1145/800017.800542 .",
    "z.  devito , j.  hegarty , a.  aiken , p.  hanrahan , and j.  vitek .",
    "terra : a multi - stage language for high - performance computing . in _ proceedings of the acm sigplan conference on programming language design and implementation ,",
    "seattle , wa , usa , june 16 - 22 , 2013 ( pldi  13 ) _ , pages 105116 , 2013 .",
    "doi : http://doi.acm.org/10.1145/2462156.2462166 .",
    "m.  a. ertl and d.  gregg . combining stack caching with dynamic superinstructions . in _ proceedings of the 2004 workshop on interpreters , virtual machines and emulators ( ivme  04 ) _ , pages 714 , new york , ny , usa , 2004 .",
    "isbn 1 - 58113 - 909 - 8 .",
    "doi : http://doi.acm.org/10.1145/1059579.1059583 .",
    "m.  a. ertl , d.  gregg , a.  krall , and b.  paysan .",
    "vmgen : a generator of efficient virtual machine interpreters .",
    "_ software practice & experience _ , 32:0 265294 , march 2002 .",
    "issn 0038 - 0644 .",
    "doi : 10.1002/spe.434 .",
    "url http://portal.acm.org/citation.cfm?id=776235.776238 .",
    "b.  grant , m.  mock , m.  philipose , c.  chambers , and s.  j. eggers . . in _ proceedings of the acm sigplan workshop on partial evaluation and semantics - based program manipulation , amsterdam , the netherlands ,",
    "june 12 - 13 , 1997 ( pepm  97 ) _ , pages 163178 , 1997 .",
    "doi : http://doi.acm.org/10.1145/258993.259016 .",
    "u.  hlzle and d.  ungar . optimizing dynamically - dispatched calls with run - time type feedback . in _ proceedings of the acm sigplan conference on programming language design and implementation , orlando , fl , usa , june 20 - 24 , 1994 ( pldi  94 ) _ , pages 326336 , 1994 .",
    "isbn 0 - 89791 - 662-x .",
    "doi : http://doi.acm.org/10.1145/178243.178478 .",
    "intel turbo boost technology  on - demand processor performance , 2012 .",
    "url http://www.intel.com / content / www / us / en / architecture - and - technology / turb% o - boost / turbo - boost - technology.html[http://www.intel.com / content / www / us / en / architecture - and - technology / turb% o - boost / turbo - boost - technology.html ] .",
    "k.  ishizaki , t.  ogasawara , j.  g. castanos , p.  nagpurkar , d.  edelsohn , and t.  nakatani .",
    "adding dynamically - typed language support to a statically - typed language compiler : performance evaluation , analysis , and tradeoffs . in _ proceedings of the 8th acm sigplan international conference on virtual execution environments , london , united kingdom , march 3 - 4 , 2012 ( vee  12 ) _ , pages 169180 , 2012 .",
    "doi : http://doi.acm.org/10.1145/2151024.2151047 .",
    "lee and m.  leone . optimizing ml with run - time code generation . in _ proceedings of the acm sigplan conference on programming language design and implementation , philadephia ,",
    "pa , usa , may 21 - 24 , 1996 ( pldi  96 ) _ , pages 540553 , 1996 .",
    "doi : http://doi.acm.org/10.1145/989393.989448 .",
    "x.  leroy . .",
    "in _ proceedings of the 196h acm sigplan - sigact symposium on principles of programming languages , albuquerque , nm , usa , january 19 - 22 , 1992 ( popl  92 ) _ , pages 177188 , 1992 .",
    "doi : http://doi.acm.org/10.1145/143165.143205 .",
    "s.  l. peyton  jones and j.  launchbury . .",
    "in _ proceedings of the 5th acm conference on functional programming languages and computer architecture , in cambridge , uk , september 1991 _ , pages 636666 , 1991 .",
    "isbn 3 - 540 - 54396 - 1 .",
    "doi : http://dl.acm.org/citation.cfm?id=645420.652528 .",
    "m.  philipose , c.  chambers , and s.  j. eggers . .",
    "in _ proceedings of the 29th acm sigplan - sigact symposium on principles of programming languages , portland , or , usa , january 16 - 18 , 2002 ( popl  02 ) _ , pages 113125 , 2002 .",
    "doi : http://doi.acm.org/10.1145/503272.503284 .",
    "i.  piumarta and f.  riccardi . optimizing direct threaded code by selective inlining . in _ proceedings of the acm sigplan conference on programming language design and implementation , montral , qc , canada , june 17 - 19 , 1998 ( pldi  98 ) _ , pages 291300 , new york , ny , usa , 1998 .",
    "isbn 0 - 89791 - 987 - 4 .",
    "doi : http://doi.acm.org/10.1145/277650.277743 .",
    "t.  a. proebsting .",
    "optimizing an ansi c interpreter with superoperators . in _ proceedings of the 22nd acm sigplan - sigact symposium on principles of programming languages , san francisco , ca , usa , january 23 - 25 , 1995 ( popl  95 ) _ , pages 322332 , 1995 .",
    "doi : http://doi.acm.org/10.1145/199448.199526 .",
    "a.  rigo and s.  pedroni .",
    "s approach to virtual machine construction . in _ proceedings of the 21st acm sigplan conference on object oriented programming : systems , languages , and applications , portland , or , usa , october 22 - 26 , 2006 ( oopsla  06 ) _ , pages 944953 , 2006 .",
    "doi : http://doi.acm.org/10.1145/1176617.1176753 .",
    "oopsla companion .",
    "m.  shields , t.  sheard , and s.  l. peyton  jones . .",
    "in _ proceedings of the 25th acm sigplan - sigact symposium on principles of programming languages , san diego , ca , usa , january 19 - 21 , 1998 ( popl  98 ) _ , pages 289302 , 1998 .",
    "doi : http://doi.acm.org/10.1145/268946.268970 .",
    "t.  wrthinger , a.  w , l.  stadler , g.  duboscq , d.  simon , and c.  wimmer .",
    "self - optimizing ast interpreters . in _ proceedings of the 8th symposium on dynamic languages , tucson , az , usa , october 22 , 2012 ( dls  12 ) _ , pages 7382 , 2012 .",
    "doi : http://doi.acm.org/10.1145/2384577.2384587 .",
    "a.  yermolovich , c.  wimmer , and m.  franz .",
    "optimization of dynamic languages using hierarchical layering of virtual machines . in _ proceedings of the 5th symposium on dynamic languages , orlando , fl , usa , october 26 , 2009 ( dls  09 ) _ , pages 7988 , new york , ny , usa , 2009 .",
    "isbn 978 - 1 - 60558 - 769 - 1 .",
    "doi : http://doi.acm.org/10.1145/1640134.1640147 ."
  ],
  "abstract_text": [
    "<S> interpreters have a bad reputation for having lower performance than just - in - time compilers . </S>",
    "<S> we present a new way of building high performance interpreters that is particularly effective for executing dynamically typed programming languages . </S>",
    "<S> the key idea is to combine speculative staging of optimized interpreter instructions with a novel technique of incrementally and iteratively concerting them at run - time .    </S>",
    "<S> this paper introduces the concepts behind deriving optimized instructions from existing interpreter instructions  incrementally peeling off layers of complexity . when compiling the interpreter , these optimized derivatives will be compiled along with the original interpreter instructions . </S>",
    "<S> therefore , our technique is portable by construction since it leverages the existing compiler s backend . </S>",
    "<S> at run - time we use instruction substitution from the interpreter s original and expensive instructions to optimized instruction derivatives to speed up execution .    </S>",
    "<S> our technique unites high performance with the simplicity and portability of interpreters  we report that our optimization makes the cpython interpreter up to more than four times faster , where our interpreter closes the gap between and sometimes even outperforms pypy s just - in - time compiler . </S>"
  ]
}