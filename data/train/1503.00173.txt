{
  "article_text": [
    "there is an explosion of data , generated , measured , and stored at very fast rates in many disciplines , from finance and social media to geology and biology .",
    "much of this _ big data _ takes the form of simultaneous , long running time series . examples , among many others , include protein - to - protein interactions in organisms , patients records in health care , customers consumptions in ( power , water , natural gas ) utility companies , cell phone usage for wireless service providers , companies financial data , social interactions among individuals in a population .",
    "the internet - of - things ( iot ) is an imminent source of ever increasing large collection of time series . the diversity and _ _ un__structured nature of big data challenges our ability to derive models from first principles ; in alternative , because data is abundant , it is of great significance to develop methodologies that",
    ", in collaboration with domain experts , assist extracting low - dimensional representations for the data . networks or graphs are becoming prevalent as models to describe data relationships .",
    "these low - dimensional graph data representations are then used for further analytics , for example , to compute statistics , make inferences , perform signal processing tasks  @xcite , or quantify how topology influences diffusion in networks of agents  @xcite .    in many problems ,",
    "the first issue to address is inferring the unknown relations between entities from the data  @xcite .",
    "early work include dimensionality reduction approaches such as @xcite . this paper focus on this problem for time - series data , estimating the network structure in the form of a possibly directed , weighted adjacency matrix  @xmath0 .",
    "current work on estimating network structure largely associates graph structure with assuming that the process supported by the graph is markov  @xcite .",
    "our work instead associates the graph with causal network effects , drawing inspiration from the discrete signal processing on graphs ( )  framework  @xcite .",
    "we first provide a brief overview of the concepts and notations underlying the   theory in section  [ sec : dspg ] .",
    "then we introduce related prior work in section  [ sec : prior ] and our new network process in section  [ sec : gp ] .",
    "next , we present algorithms to infer the network structure from data generated by such processes in section  [ sec : estim ] .",
    "finally , we show simulation results in section  [ sec : exp ] and conclude the paper in section  [ sec : concl ] .",
    "provides a framework with which to analyze data with @xmath1 elements for which relational information between elements is known .",
    "we follow  @xcite in this brief review .",
    "consider a graph @xmath2 where the vertex set @xmath3 and @xmath0 is the weighted adjacency matrix of the graph .",
    "each data element corresponds to a node @xmath4 and weight @xmath5 is assigned to a directed edge from @xmath6 to @xmath4 .",
    "a graph signal is defined as a map @xmath7 since signals are isomorphic to complex vectors with @xmath1 elements , we can write graph signals as @xmath1 length vectors supported on @xmath8 , @xmath9      a graph filter is a system @xmath10 that takes a graph signal @xmath11 as input and outputs another graph signal @xmath12",
    ". a basic nontrivial graph filter on graph @xmath2 called the graph shift is a local operation given by the product of the input signal with the adjacency matrix @xmath13 . assuming shift invariance , graph filters in  are matrix polynomials of the form @xmath14 the output of the filter is @xmath15 . note that graph filters are linear shift - invariant filters . for a linear combination of graph signal inputs they produce the same linear combination of graph signal outputs , and consecutive application of multiple graph filters does not depend on the order of application ( i.e.",
    ", graph filters commute ) .",
    "graph filters also have at most @xmath16 taps @xmath17 , where @xmath18 is the degree of the minimal polynomial @xmath19 of @xmath0 .",
    "here we describe previous models and methods used to estimate graph structure , noting the similarities and distinctions with the method presented in this paper .",
    "in particular , in section  [ subsec : graphicalmodel ] we consider sparse gaussian graphical model selection , and in section  [ subsec : spvar ] we consider sparse vector autoregression .",
    "sparse inverse covariance estimation  @xcite combines the markov property with the assumption of gaussianity to learn a graph structure describing symmetric relations between the variables .",
    "a typical formulation of sparse inverse covariance estimation is graphical lasso  @xcite .",
    "suppose the data matrix representing all the observations is given , @xmath20&\\x[1]&\\ldots&\\x[k-1]\\big)\\end{array}\\in \\mathbb{r}^{n\\times k}\\ ] ] in this problem , the data is assumed to be gaussian , i.e. each @xmath21 \\sim \\mathcal{n}(\\0,\\sigma)$ ] , independent , identically distributed , and an estimate for @xmath22 is desired .",
    "the regularized likelihood function is maximized , leading to the optimization ,    @xmath23    where @xmath24 is the sample covariance matrix and @xmath25 .    for independent observations",
    ", the inverse covariance matrix corresponds to instantaneous second - order relations that can be useful for inference using graphical models .",
    "however , given time series data generated from a sparse graph process , the inverse covariance matrix @xmath26 can actually reflect higher order effects and be significantly less sparse than the graph underlying the process .",
    "for example , if a process is described by the dynamic equation with sparse state evolution matrix @xmath0 and @xmath27 , @xmath28=\\a\\x[k-1]+\\w[k]\\ ] ] where @xmath29 $ ] is a random noise process that is generated independently from @xmath30 $ ] for all @xmath31 , then @xmath32\\x[k]^t\\right]=\\sum\\limits_{i=0}^\\infty \\a^i \\r \\left(\\a^t\\right)^i \\\\",
    "\\rightarrow \\theta&=\\left(\\sum\\limits_{i=0}^\\infty \\a^i \\r \\left(\\a^t\\right)^i \\right)^{-1 } \\end{aligned}\\ ] ] even though the process can be described by sparse matrix @xmath0 , the true value of @xmath26 represents powers of @xmath0 and need not be as sparse as @xmath0 .",
    "in addition , @xmath0 may not be symmetric , i.e. the underlying graph may be directed , while @xmath26 is symmetric and the corresponding graph is undirected .      for time series data , instead of estimating inverse covariance structure , sparse vector autoregressive ( svar ) estimation  @xcite recovers matrix coefficients for multivariate processes .",
    "this svar model assumes the time series at each node are conditionally independent from each other according to a markov random field ( mrf ) with adjacency structure given by @xmath33 .",
    "this problem assumes given data matrix of the form in equation  ( [ eq : datamatrix ] ) that is generated by the dynamic equation with sparse evolution matrices @xmath34 , @xmath28=\\sum\\limits_{i=1}^m \\a^{(i)}\\x[k - i]+\\w[k]\\ ] ] where @xmath29 $ ] is a random noise process that is generated independently from @xmath30 $ ] for all times @xmath31 , and @xmath35 all have the same sparse structure .",
    "that is , @xmath36 for all @xmath37 .",
    "then svar solves the optimization , @xmath38-\\sum\\limits_{i=1}^{m } \\a^{(i ) } \\x[k - i ] \\right\\|_2 ^ 2\\\\ + & \\lambda\\sum\\limits_{i , j}\\|\\a_{ij}\\|_2 \\end{aligned } \\label{eq : svar}\\ ] ] where @xmath39 and the term @xmath40 promotes sparsity of the @xmath41-th entry of each @xmath35 matrix simultaneously and thus also sparsity of @xmath42 .",
    "this optimization can be solved using group lasso  @xcite .",
    "svar estimation methods estimate multiple weighted graphs @xmath35 that can be used to estimate the sparsity structure @xmath42 .",
    "this set of @xmath34 can be more challenging to interpret and analyze than a single weighted adjacency matrix , but using the single unweighted matrix @xmath42 discards temporal and weight information . also , it is not clear that the markov property between nodes is a valid assumption for various applications described by graph processes . in contrast",
    ", our model is defined by a single weighted graph @xmath0 , but the corresponding time filter coefficients ( introduced in section  [ subsec : cgp ] ) are modeled as graph filters , which allows describing processes that are non - markov on the graph .",
    "section  [ subsec : cgp ] presents our new model for graph processes , section  [ subsec : pde ] relates it to discretization of partial differential equations , and section  [ subsec : bigdata ] relates the approach to graph signal processing frameworks .",
    "consider @xmath43 $ ] , a discrete time series on node @xmath4 in graph @xmath2 , where @xmath44 indexes the nodes of the graph and @xmath37 indexes the time samples .",
    "let @xmath1 be the total number of nodes and @xmath45 be the total number of time samples , and @xmath28=\\begin{array}{cccc } \\big ( x_0[k ] & x_1[k ] & \\ldots & x_{n-1}[k ] \\big)^t\\end{array } \\in \\mathbb{c}^n\\ ] ] represents the graph signal at time sample @xmath37 .",
    "we consider a causal graph process ( cgp ) to be a discrete time series @xmath46 $ ] on a graph @xmath2 of the following form , @xmath47 = & \\w[k ] + \\sum\\limits_{i=1}^{m}p_{i}(\\a)\\x[k - i]&\\\\ = \\w[k ] & + \\sum\\limits_{i=1}^{m}\\left(\\sum\\limits_{j=0}^{i}c_{ij}\\a^j\\right)\\x[k - i]&\\\\ = \\w[k ] & + ( c_{10 } \\i+c_{11 } \\a)\\x[k-1]&\\\\ & + \\left(c_{20 } \\i + c_{21 } \\a + c_{22 } \\a^2",
    "\\right ) \\x[k-2 ] + \\ldots&\\\\ & + \\left(c_{m0 }",
    "\\i + \\ldots + c_{mm}\\a^m \\right)\\x[k - m ] \\end{aligned}\\ ] ] where @xmath48 is a matrix polynomial in @xmath0 , @xmath49 $ ] is statistical noise , @xmath50 are scalar polynomial coefficients , and @xmath51 is a vector collecting all the @xmath50 s .    note that this model does _ not _ assume markovianity on nodes and their neighbors .",
    "it instead asserts that the signal on a node at the current time is affected through network effects by signals on other nodes at past times .",
    "matrix polynomial @xmath48 is at most of order @xmath52 , reflecting that @xmath46 $ ] can not be influenced by more than @xmath53-th order network effects from @xmath53 time steps ago and in addition is limited by @xmath54 , the degree of the minimum polynomial of @xmath0 .",
    "typically , we take the model order @xmath55 .",
    "we provide an intuitive motivation for the cgp model of  .",
    "now consider @xmath56 , a continuous time signal on node @xmath4 in graph @xmath2 , where @xmath44 indexes the nodes of the graph and @xmath57 indexes the time samples .",
    "let @xmath1 be the total number of nodes and @xmath58 represents the graph signal at time @xmath57 .",
    "suppose @xmath59 is described by the @xmath60-th order differential equation , @xmath61 where @xmath62 and @xmath63 is a matrix approximating a first order differential operator in the space of @xmath11 .    discretizing  ( [ eq : pdemodel ] ) with time step size @xmath64 , @xmath65\\\\   = & ( c_{10 } \\i+c_{11 } \\a)\\sum\\limits_{i=0}^{m-1 } \\frac{(-1)^{i}}{\\delta^{m-1}}\\left(\\!\\!\\begin{array}{c}m-1\\\\i\\end{array}\\!\\!\\right)\\x[k-1-i]&\\\\   + & \\ldots+ \\left(c_{m0 } \\i + \\ldots + c_{mm}\\a^m \\right)\\x[k - m]&\\\\ \\end{aligned}\\ ] ] where @xmath66 is a matrix operation approximating the temporal discretization of the @xmath63 operator at the graph nodes , and @xmath46=\\x(t)|_{t = k\\delta}$ ]",
    ". grouping terms containing @xmath67 $ ] to simplify  ( [ eq : pdediscrete ] ) and renaming constants , we arrive at our model from  ( [ eq : cgpmodel ] ) .",
    "the @xmath48 from equation  ( [ eq : cgpmodel ] ) can be seen in the  framework as a causal graph filter .",
    "this allows us to naturally fit this model into the graph signal processing for big data framework , in which true graphs for large datasets can be expressed or approximated by graph products , which are sums of products , for example kronecker products  @xcite , @xmath68 in which @xmath69 are jointly diagonalized by one eigenvector basis and @xmath70 are jointly diagonalized by a second eigenvector basis , @xmath71 this allows the eigendecomposition of @xmath0 to be found in terms of simpler eigendecompositions of smaller matrices @xmath70 and @xmath69 , @xmath72    for our time - series process , we know that the form of one set of adjacency matrices is powers of the cyclic shift matrix @xmath73 , where @xmath74 ( only nonzero elements shown ) representing the directed cycle graph and @xmath75 .",
    "the diagonalizing matrix for the @xmath69 is the discrete fourier transform ( dft ) matrix .",
    "if this cyclic temporal structure is used to represent  ( [ eq : cgpmodel ] ) , we arrive at @xmath76 where @xmath77 is the data matrix defined as in  ( [ eq : datamatrix ] ) and @xmath78 is an error matrix .",
    "this results in a slightly different model than  ( [ eq : cgpmodel ] ) .",
    "however , for the remainder of this paper , we will use  ( [ eq : cgpmodel ] ) to represent and estimate @xmath48 from observed time series data .",
    "given a time series @xmath59 on graph @xmath2 with _ unknown _ @xmath0 , we wish to estimate the adjacency matrix @xmath0 . a first approach to its estimation",
    "can be formulated as the following optimization problem , @xmath79 - \\sum\\limits_{i=1}^{m}p_{i}(\\a)\\x[k - i ] \\big\\|_2 ^ 2 \\\\ & + \\lambda_1 \\|\\vc(\\a)\\|_1 + \\lambda_2 \\|\\c\\|_1 \\end{aligned}\\ ] ] where @xmath80&\\x[1]&\\ldots&\\x[k-1]\\big)\\end{array}\\ ] ] represents all the data and @xmath81 represents the matrix @xmath0 seen as a long vector .",
    "assuming that @xmath82 , then without further loss of generality we can fix @xmath83 and @xmath84 to ensure that @xmath0 is uniquely recoverable .    in equation  ( [ eq : optnonconvex2 ] ) , the first term in the right hand side models @xmath46 $ ] by the cgp model in section  [ subsec : cgp ] , the regularizing term @xmath85 promotes sparsity of the estimated adjacency matrix , and the term @xmath86 also promotes sparsity in the matrix polynomial coefficients .",
    "unfortunately , the matrix polynomial in the first term makes this problem highly nonconvex . that is , using a convex optimization based approach to solve  ( [ eq : optnonconvex2 ] )",
    "directly may result in finding a matrix @xmath87 and coefficients @xmath88 minimizing the objective function locally , finding a solution that is not near to the true globally minimizing matrix @xmath0 and coefficients @xmath89 .    instead , our approach here is to break this estimation down into three separate , more tractable steps :    solve for @xmath90    recover structure of @xmath0    estimate @xmath50      as previously stated , the graph filters @xmath91 are polynomials of a and are thus shift - invariant and must mutually commute .",
    "then their commutator @xmath92= p_i ( \\a ) p_j ( \\a ) - p_j ( \\a ) p_i ( \\a ) = 0 \\ ; \\ ; \\forall i , j\\ ] ] let @xmath93 ; @xmath94 is the estimate of @xmath48 .",
    "this leads to the optimization problem ,    @xmath95 - \\sum\\limits_{i=1}^{m}\\r_i \\x[k - i ] \\right\\|_2 ^ 2 \\\\ + & \\lambda_1 \\|\\vc(\\r_1 ) \\|_1 + \\lambda_2 \\sum\\limits_{i\\ne j}\\|[\\r_i,\\r_j]\\|_f^2 \\end{aligned}\\ ] ]    while this is still a non - convex problem , it is multi - convex .",
    "that is , when @xmath96 ( all @xmath97 except for @xmath98 ) are held constant , the optimization is convex in @xmath98 .",
    "this naturally leads to block coordinate descent as a solution ,    @xmath99 - \\sum\\limits_{i=1}^{m}\\r_i \\x[k - i ] \\right\\|_2 ^ 2 \\\\ + & \\lambda_1 \\|\\vc(\\r_1 ) \\|_1 + \\lambda_2\\sum\\limits_{j\\ne i}\\left\\|[\\r_i,\\r_j]\\right\\|_f^2 \\end{aligned}\\ ] ]    each of these sub - problems for estimating @xmath98 in a single sweep of estimating @xmath100 is formulated as an @xmath101-regularized least - squares problem that can be solved using standard methods  @xcite .",
    "after obtaining estimates @xmath94 , we find an estimate for @xmath0 .",
    "one approach is to take @xmath102 .",
    "this appears to ignore the information from the remaining @xmath94 .",
    "however , the information has already been incorporated during the iterations when solving for @xmath103 , especially if we begin one new sweep to estimate @xmath104 using  ( [ eq : optri ] ) with @xmath105 .",
    "a second approach is also possible , explicitly using all the @xmath94 together to find @xmath0 , @xmath106\\right\\|_f^2 \\end{aligned}\\ ] ] this can be seen as similar to running one additional step further in the block coordinate descent to find @xmath107 except that this approach does not explicitly use the data .",
    "we can estimate @xmath50 in one of two ways : we can estimate @xmath89 either from @xmath87 and @xmath94 or from @xmath87 and the data @xmath77 .    to estimate @xmath50 from @xmath87 and @xmath103",
    ", we set up the optimization , @xmath108 where @xmath109    alternatively , to estimate @xmath50 from @xmath87 and the data @xmath77 , we can use the optimization , @xmath110 where @xmath111 , @xmath112 & \\x[m+1 ] & ... & \\x[m+k - m-1]\\end{array}\\!\\right),\\end{aligned}\\ ] ] which can also be solved using standard @xmath101-regularized least squares methods .",
    "the methods discussed so far can be interpreted as assuming that 1 ) the process is a linear autoregressive process driven by white gaussian noise and 2 ) the parameters @xmath0 and @xmath89 a priori follow laplace distributions .",
    "the objective function in  ( [ eq : optnonconvex2 ] ) approximately corresponds to the log posterior density and its solution to an approximate maximum a posteriori ( map ) estimate .",
    "this framework can be extended to estimate more general autoregressive processes , such as those with a non - gaussian noise model and certain forms of nonlinear dependence of the current state on past values of the state .",
    "we formulate the general optimization @xmath113 where @xmath114 is a loss function that can correspond to a log - likelihood function dictated by the noise model , and @xmath115 and @xmath116 are regularization functions ( usually convex norms ) that can correspond to log - prior distributions imposed on the parameters and are dictated by modeling assumptions .",
    "again , the matrix polynomials @xmath48 introduce nonconvexity , so similarly as before , we can separate the estimation into three steps to reduce complexity .",
    "we next generalize equation  ( [ eq : optri ] ) used to find @xmath98 as estimates of @xmath48 with the optimization @xmath117,\\ldots,[\\r_i,\\widehat{\\r}_m ] ) \\end{aligned}\\ ] ] where @xmath118 is the part of the objective function that depends on @xmath48 when the other @xmath119 are fixed , the term @xmath120 regularizes the estimated matrix polynomial , and the term @xmath121 promotes commmutativity of the matrix polynomials .",
    "next , we can again take @xmath122 , or we can reformulate equation  ( [ eq : finda2 ] ) @xmath123 where @xmath124 is some loss function , @xmath115 regularizes the estimated adjacency matrix , and @xmath125 enforces commutativity of the adjacency matrix with the other matrix polynomials",
    ".    we can generalize  ( [ eq : findcspec ] ) as @xmath126 where @xmath124 is the objective function , and @xmath127 is a regularizing function on the matrix polynomial coefficients , and @xmath128 is defined as in section  [ subsec : estimcij ] ; and lastly generalize  ( [ eq : findc2spec ] ) as @xmath129 where @xmath124 and @xmath127 are the same functions as above in  ( [ eq : findc ] ) , and @xmath130 and @xmath63 are defined as in section  [ subsec : estimcij ] .      in sections  [ subsec : solvepi]-[subsec : estimcij ] ,",
    "we have outlined a 3-step algorithm to obtain estimates @xmath87 and @xmath88 for the adjacency matrix and filter coefficients as a more efficient and well - behaved alternative to directly using  ( [ eq : optnonconvex ] ) .",
    "initialize , @xmath131 , @xmath132 find @xmath133 with fixed @xmath134 , @xmath135 using  ( [ eq : optpi ] ) .",
    "@xmath136 set @xmath137 or estimate @xmath87 from @xmath138 using  ( [ eq : finda ] ) .",
    "solve for @xmath88 from @xmath77 , @xmath87 using  ( [ eq : findc ] ) or from @xmath77 , @xmath103 using  ( [ eq : findc2 ] ) .",
    "we call this 3-step procedure the basic algorithm , which is outlined in algorithm  [ alg : algbasic ] .",
    "superscripts denote the iteration number , @xmath134 denotes @xmath139 and likewise @xmath140 denotes @xmath141 , and @xmath142 is the final iteration performed before convergence of @xmath103 is determined or a preset maximum iteration count is exceeded .    as an extension of the basic algorithm",
    ", we can also choose the estimated matrix @xmath87 and filter coefficients @xmath88 to initialize the direct approach of using  ( [ eq : optnonconvex ] ) .",
    "starting from these initial points , we may find better local minima than with initializations at @xmath143 and @xmath144 or at random points .",
    "we call this procedure the extended algorithm , summarized in algorithm  [ alg : algext ]",
    ".    estimate @xmath145 , @xmath146 using basic algorithm .",
    "find @xmath87 , @xmath88 using initialization @xmath145 , @xmath146 from  ( [ eq : optnonconvex ] ) using convex methods .",
    "in this section , we discuss the convergence of both the basic and extended algorithms described above .      in estimating @xmath0 and @xmath89 , the forms of the optimization problems are well studied when choosing @xmath147 and @xmath101 norms as loss and regularization functions , as seen in equations  ( [ eq : finda2 ] ) and  ( [ eq : findc2spec ] ) .",
    "however , using these same norms , step 1 of the algorithm is a nonconvex optimization .",
    "hence we would like to ensure that step 1 converges .    when using block coordinate descent for general functions , neither the solution nor the objective function values are guaranteed to converge to a global or even a local minimum . however , under some mild assumptions , using block coordinate descent to estimate @xmath98 will converge . in equation  ( [ eq : optnonconvex ] ) ,",
    "if we assume the objective function to be continuous and to have convex , compact level sets in each coordinate block @xmath98 ( for example , if the functions for @xmath148 and @xmath149 are the @xmath147 and @xmath101 norms as in equation  ( [ eq : optnonconvex2 ] ) ) , then the block coordinate descent will converge  @xcite .",
    "now we discuss the convergence of the extended method described in section  [ subsec : extest ] assuming that the basic algorithm has converged to an initial point @xmath150 for the extended algorithm .",
    "we assume that the function @xmath151 in equation  ( [ eq : optnonconvex ] ) has compact level sets and is bounded below .",
    "then an iterative convex method that produces updates of @xmath152 such that @xmath153 ( e.g. , generalized gradient descent with appropriately chosen step size ) converges , possibly to a local optimum if the problem is nonconvex  @xcite . if the functions are the @xmath147 and @xmath101 norms as in equation  ( [ eq : optnonconvex2 ] ) , these conditions are satisfied as well .",
    "the algorithm was tested on randomly generated examples ( with varying @xmath1 and @xmath45 ) and a real temperature sensor network dataset ( with @xmath154 and @xmath155 ) , sampled once per day over a year at @xmath156 locations in the continental united states  @xcite . to solve the regularized least squares iterations for estimating the cgp matrices , we used gradient projection for sparse reconstruction  @xcite . to estimate the mrf matrices",
    ", we implemented a proximal gradient descent algorithm to estimate the svar matrix coefficients from  [ eq : svar ] since the code used in  @xcite is not tested for larger graphs .",
    "[ fig : toy ]        the random graph dataset was generated by first creating a random sparse @xmath0 matrix for a graph with @xmath157 nodes and @xmath89 coefficients that corresponded to a stable system .",
    "the matrix in our simulations had each off - diagonal element independently drawn from a unit normal @xmath158 distribution and made sparse by thresholding @xmath159 and then scaled to ensure stability @xmath160 where @xmath161 is the largest eigenvalue of @xmath42 .",
    "this results in a directed , weighted erds - rnyi graph topology with a constant probability @xmath162 of having an edge from node @xmath53 to node @xmath163 and with edge weights bounded away from @xmath164 .",
    "the diagonal elements were generated from a uniform distribution @xmath165 , also to ensure stability .",
    "then the adjacency matrix was formed @xmath166 .",
    "finally , the polynomial coefficients @xmath89 for a process of order @xmath167 were arbitrarily chosen to result in a stable process .",
    "the data matrix @xmath77 was formed by generating random initial states and zero - mean unit - covariance additive white gaussian noise @xmath49 $ ] , and computing @xmath168 samples of @xmath46 $ ] according to  ( [ eq : cgpmodel ] ) .    in figure",
    "[ fig : toy_a ] , we see the structure of the @xmath0 matrix with @xmath169 nodes used in the simulated graph to generate @xmath77 with @xmath170 samples according to a cgp .",
    "we also see the structure of the @xmath87 matrices estimated using the basic and extended methods , and also using the gradient method in the extended method but initialized at the origin ( @xmath171 , @xmath172 ) . while the actual graphs are directed , it is difficult to depict the direction of edges in larger graphs , so for presentation , they are shown as undirected in figure  [ fig : toy_a ] . in figure",
    "[ fig : toy_b ] , we see the individual values of the @xmath87 matrices estimated from the data @xmath77 .",
    "now we note the directed nature of the graph , as the matrix is asymmetric .",
    "we see that the estimated @xmath87 matrices all have almost the same support as the true @xmath0 matrix visually .",
    "qualitatively , the basic method produces a less sparse solution , and the gradient method produces a lower magnitude solution , while the solution produced by the extended method is both sparse and closer in magnitude solution to the true solution . thus we see that the extended method performs better than either the basic step or the gradient step alone .",
    "the mean squared errors ( mse s ) of entries of the @xmath0 matrix are computed as @xmath173 the mse s for the estimates shown in figure  [ fig : toy ] are : @xmath174 for the basic , @xmath174 for the extended , and @xmath175 for the gradient .    in figure",
    "[ fig : toy_kn_mse ] , we see mse for @xmath87 across different numbers of time samples @xmath45 on the same graph , as well as across different numbers of nodes @xmath1 corresponding to distinct graphs with the same level of sparsity . the mse is averaged over @xmath176 monte - carlo simulations of the data matrix @xmath77 for each problem size .",
    "as expected , the mse for each problem size decreases as the number of samples @xmath45 increases .",
    "while the estimate produced by the proposed method is biased for finite @xmath45 , the plot suggests that asymptotically in the number of samples @xmath45 , the graph produced may still be consistent .",
    "in addition , the mse s computed decrease with increasing @xmath1 , suggesting that the total error in estimating the matrix @xmath177 .",
    "the dependence of these error rates on @xmath1 and @xmath45 are of interest for further analysis .",
    "the temperature dataset is a collection of daily average temperature measurements taken over @xmath178 days at @xmath156 locations around the continental united states  @xcite .",
    "the time series @xmath179 is linearly detrended at each measurement station @xmath53 to form @xmath180 . then the seasonally detrended time series @xmath181 are obtained by applying an ideal high - pass filter with cutoff period of @xmath178 days to each @xmath180 .",
    "finally , the data matrix @xmath77 is formed from @xmath181    [ fig : temp_pred ]    we compare the sparsity and prediction errors of using cgp and mrf models as well as an undirected distance graph as described in  @xcite , all on the same set of temperature data .",
    "the distance graph model uses an adjacency matrix @xmath182 to model the process @xmath28=\\w[k]+\\sum\\limits_{i=1}^{m } h_i ( \\a^{\\textrm{dist } } ) \\x[k - i]\\ ] ] where @xmath183 are polynomials of the distance matrix with elements chosen as @xmath184 with @xmath185 representing the neighborhood of @xmath186 cities nearest to city @xmath44 . in this model",
    ", @xmath60 is taken to be fixed and the polynomial coefficients @xmath187 are to be estimated . in our experiments , we assumed @xmath188 .",
    "we separated the data into two segments , one consisting of the even time indices and one consisting of the odd time indices .",
    "one set was used as training data and the other set was left as testing data .",
    "this way , the test and training data were both generated from almost the same process but with different daily fluctuations . in this experiment",
    ", we compute the prediction mse as @xmath189-\\widehat{\\x}[i ] \\right\\|^2\\ ] ] here , since we do not have the ground truth graph for this data , the experiments can be seen as corresponding to the two related tasks of compression and prediction .",
    "the training error indicates how well the estimated graph can represent or compress the entire series , while the test error indicates how well the estimated graph can predict the following time instance from past observations on new series .    in figure",
    "[ fig : temp_pred_a ] , we see that using a directed graph ( either cgp or mrf ) performs better for prediction than using an undirected distance graph in both training and testing phases .",
    "in addition , for high sparsity or low nonzeros ( towards the left side of the graph with proportion of nonzeros @xmath190 ) , the cgp fits the data with better accuracy than the mrf model . at lower sparsity levels ( towards the right side of the graph @xmath191 ) , the mrf model performs better than the cgp model .",
    "figure  [ fig : temp_pred_b ] shows the average of test and training error labeled as total error .",
    "the same trends are present here as well , in which the cgp model outperforms the mrf model for @xmath190 . however ,",
    "when the proportion of nonzeros in the adjacency matrix is above @xmath192 , one can not claim that the model is truly `` sparse '' .",
    "thus , when using sparse models , the cgp captures the true dynamics of the process using higher sparsity than the mrf model .    in figure",
    "[ fig : temp ] , we compare the temperature networks estimated on the entire time series using cgp and mrf models that both have sparsity level @xmath193 . for the mrf estimate , we used the first matrix coefficient @xmath194 to represent the network , although as mentioned previously there is not the interpretation of a single weighted graph being estimated using this model .",
    "the @xmath195-axis corresponds to longitude while the @xmath196-axis corresponds to latitude .",
    "we note that the network produced by the mrf at the same sparsity level has similar support to that produced by the cgp , but the magnitudes of the edges are lower .",
    "we see that the cgp model clearly picks out the predominant west - to - east direction of wind in the @xmath197 portion of the country , as single points in this region are seen to predict multiple eastward points .",
    "it also shows the influence of the roughly north - northwest - to - south - southeast rocky mountain chain at @xmath198 .",
    "this yields easy interpretation consistent with knowledge of geographic and meteorological features .",
    "we have shown through experiments that the estimation algorithms presented are able to accurately estimate a sparse graph topology from data generated when the true model is a cgp .",
    "we have also demonstrated empirically that the cgp model can use a higher sparsity level than the mrf model to describe processes at the same levels of accuracy .",
    "for this reason , we believe that the cgp model reflects the dynamics of the process underlying the temperature sensor data more faithfully .",
    "we have presented a new type of graph - based network process and a computationally tractable algorithm for estimating such causal networks . the algorithm was demonstrated on several random graphs of varying size and a real temperature sensor network dataset .",
    "the estimated adjacency matrices in the random graph examples were shown to be close to the true graph . in the real dataset ,",
    "the adjacency matrices estimated using our method were consistent with prior physical knowledge and achieved lower prediction error compared to previous methods at the same sparsity level ."
  ],
  "abstract_text": [
    "<S> many _ big data _ applications collect a large number of time series , for example , the financial data of companies quoted in a stock exchange , the health care data of all patients that visit the emergency room of a hospital , or the temperature sequences continuously measured by weather stations across the us . a first task in the analytics of these data </S>",
    "<S> is to derive a low dimensional representation , a graph or discrete manifold , that describes well the _ _ </S>",
    "<S> inter__relations among the time series and their _ _ intra__relations across time . </S>",
    "<S> this paper presents a computationally tractable algorithm for estimating this graph structure from the available data . </S>",
    "<S> this graph is directed and weighted , possibly representing _ causation _ relations , not just correlations as in most existing approaches in the literature . </S>",
    "<S> the algorithm is demonstrated on random graph and real network time series datasets , and its performance is compared to that of related methods . </S>",
    "<S> the adjacency matrices estimated with the new method are close to the true graph in the simulated data and consistent with prior physical knowledge in the real dataset tested .    </S>",
    "<S> * keywords : * graph signal processing , graph structure , adjacency matrix , network , time series , big data , causal </S>"
  ]
}