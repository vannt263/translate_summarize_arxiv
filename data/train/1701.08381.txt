{
  "article_text": [
    "predictive modeling is an integral part of data analysis .",
    "it encompasses the process of developing models which can accurately predict yet - to - be - seen data .",
    "a multitude of regression models exist that can be used for prediction of univariate , or multivariate vectorial , responses .",
    "nevertheless , less work has been done on the difficult problem of modeling and predicting more complex objects that possess additional structure , be it morphological , directional , or otherwise @xcite .",
    "images @xcite , shapes @xcite , graphs @xcite , deformation tensors @xcite are examples of complex data types that appear naturally as responses in image analysis , computer vision , medical imaging and other application areas .",
    "while such objects are typically represented as points on very high - dimensional spaces , they can be meaningfully represented as points lying on smooth non - linear hyper - surfaces of lower dimensionality , a.k.a manifolds @xcite .",
    "manifolds can be understood as surfaces that locally resemble the euclidean plane , but have different global structures @xcite .",
    "ideally , a suitable predictive modeling methodology should work under the additional constrains imposed by the data s inherent manifold structure instead of simply treating these complex objects as points on euclidean spaces .",
    "major difficulties of course arise by forgoing the assumption of a euclidean space , such as lack of coordinates , vectors and no analytical definitions of expected values @xcite . in this work ,",
    "we are additionally interested in experimental settings where the input observations may be very - high dimensional , which poses further requirements in the construction of a predictive algorithm .",
    "three main families of methodologies concerning with regression for manifold - valued responses can be found in the literature ; intrinsic manifold regression , kernel - based and manifold learning ( ml)-based methods .",
    "intrinsic manifold regression models are generalizations of linear regression on manifolds @xcite .",
    "they require the analytical definition of the data manifold , since they fit a parametrized curve on the data.the assumption of an underlying manifold drives the choice of geometric elements employed during model definition and parameter estimation .",
    "unfortunately , this requirement can be rarely satisfied , either due to the nature of data or the inability to define a suitable manifold .",
    "furthermore , this methodology is not ideal for regression analysis with highly dimensional input observations .",
    "kernel methods first appeared in the literature as methodologies tailored for complex input objects , such as trees or graphs @xcite .",
    "the basic idea behind these methods is that , if the input data lie on a non - linear space , then they can be implicitly mapped on a very high ( or infinite ) dimensional inner - product space , in which standard regression methods can be applied @xcite .",
    "this implicit mapping is achieved through a kernel function that defines the objects inner - product in that space @xcite .",
    "kernel methods for complex responses have also been proposed @xcite , but suffer from a number of issues .",
    "first , depending on the data at hand , a meaningful kernel function must be found or constructed , a process that is not always intuitive .",
    "second , another problem is the formulation of a prediction methodology , which has to be described as a kernel minimization problem over the response space .",
    "this is most commonly solved by reducing the search space to the training dataset .",
    "the last family of manifold regression methods is based on non - linear dimensionality reduction , a.k.a manifold learning .",
    "given a set of observed points , ml methods aim to project these onto a space of lower dimensionality , whilst retaining as well as possible the original geometrical structure of the data .",
    "ml - based methods first apply ml on the response data , and subsequently use standard regression models trained on the learned response embedding .",
    "a suitable methodology must be formulated to map predicted points from the embedding to the manifold , a process that is referred to as backscoring @xcite .",
    "appropriate selection of the ml technique is largely based on intuition and previous experience .",
    "a decrease in model fitting accuracy , compared to an intrinsic manifold model , is to be expected , since the response embedding is not guaranteed to completely capture the structure of the underlying manifold .",
    "furthermore , ml techniques generate discrete and one way maps from the manifold to the embedding , and the existence of an inverse and continuous map can not always be guaranteed @xcite .    here , we present a new methodology for predictive modeling of manifold - valued responses , which addresses a number of key issues listed above . our objective was to propose a unified framework for regression and prediction of complex objects that is accurate , computationally efficient and can be readily deployed in a variety of applications . in the training phase , we employ our modified random forest ( rf ) regression algorithm that can be trained using only pairwise distances between response observations @xcite .",
    "the non - parametric rf methodology is efficient and can handle high dimensional input spaces .",
    "in contrast to intrinsic manifold regression , no analytical definition of the underlying response manifold is required , apart from the construction of an appropriate distance function for the responses . comparing our distance - based approach to kernel methods , we identify further advantages .",
    "first , a vast library of readily available distance metrics exists in the literature ( see for example @xcite ) .",
    "second , when the manifold metric is not known , distances can be intuitively approximated , using for example the isomap distance formulation @xcite .    in the prediction phase ,",
    "a response point estimate for a new input observation is found as follows .",
    "pairwise distances between the unseen response and all training observations are estimated .",
    "using this set of distances , the response is predicted on a euclidean embedding computed using the ml technique of metric multi - dimensional scaling ( mds ) and is finally mapped to the response manifold through a standardized backscoring procedure .",
    "our prediction methodology follows a two - step approach akin to ml - based methods , whilst offering two additional advantages .",
    "first , our regression model is trained on the original manifold , which enhances the quality of the fitted model .",
    "second , due to the fact that manifold distances are known , mds is employed for ml and predictions can be analytically computed and backscored to the response manifold .",
    "the rest of this article is organized as follows . in section [ sec : methods ] we present the details of our manifold regression methodology .",
    "our simulation and application experiments are included in section [ sec : exp ] .",
    "we conclude this work in section [ sec : disc ] .",
    "let @xmath0 be a dataset of @xmath1 observed input - response pairs @xmath2 , with inputs @xmath3 corresponding to responses @xmath4 .",
    "@xmath5 is a possibly unknown manifold , equipped with a distance metric @xmath6 .",
    "we refer to @xmath7 as the response representation space .",
    "let @xmath8 be the @xmath9 matrix of pairwise distances between the observed response points , with elements @xmath10 .",
    "[ man : pred : fig1](a ) for a graphical illustration of the described data .",
    "we want to construct a predictive methodology that leverages these distances in order to ensure that , for any given input @xmath11 , the predicted @xmath12 lies on @xmath5 .     of manifold distances between the observed responses ( section [ sec : drf ] ) .",
    "( b ) when a new input @xmath11 is observed , it is passed through the forest and a similarity vector , @xmath13 , between the training responses and the yet to be predicted response is extracted from the model ( section [ sec : dist ] ) .",
    "( c ) based on @xmath13 , the set of distances between the new point and the training responses on the manifold are predicted ( section [ sec : dist ] ) .",
    "( d ) knowledge of these distances allows prediction of the response on a euclidean embedding of the manifold , extracted through multi - dimensional scaling ( section [ sec : outofsample ] ) .",
    "( e ) a backscoring method is used to project the predicted point back to the original manifold ( section [ sec : back]).,scaledwidth=60.0% ]      we first concentrate on the construction of a suitable manifold regression learning algorithm .",
    "random forest ( rf ) is a non - parametric , non - linear regression and classification algorithm @xcite . in more detail",
    ", rf is a collection of classification and regression trees ( carts ) .",
    "a cart splits the input space recursively , according to a predefined split function , to small rectangular regions and then fits a simple model , commonly a constant value , in each one of them .",
    "see @xcite for a detailed description of rf . in @xcite",
    ", we presented a modified distance random forest ( drf ) algorithm , where the split criterion was formulated to depend only on pairwise distances between responses : @xmath14 with @xmath15 . here",
    "@xmath16 are the cardinalities of the data subsets @xmath17 , @xmath18 , @xmath19 belonging to a node @xmath20 and its children nodes ( left and right ) , respectively .",
    "the objective is a generalization of the cost function used in standard regression rf , decoupled from the use of euclidean norms and means , dependent only on pairwise response distances . as such",
    ", it enables the rf algorithm to be applied in general metric spaces .",
    "previously , we used drf for regression applications with graph and covariance - based response objects @xcite .",
    "drf lends itself naturally to manifold - valued data , whether the manifold in question is analytically defined or implied by the use of a specific distance metric .",
    "traditionally , when working with responses lying in euclidean spaces , an rf prediction in made simply by averaging response points ; a new input observation follows the split decision rules learned during training and reaches a terminal node -leaf- in the tree .",
    "it is then assigned a suitable value on the response space , most often the average response of that leaf s responses in the training set .",
    "this approach though is not valid under the assumption of non - vectorial manifold responses @xcite .",
    "our proposed methodology uses drf for learning a family of trees from the data , which only requires an appropriate distance metric for the responses , as described above .",
    "the prediction phase is substantially different from standard rf . for a new input point @xmath11",
    ", we use the trained drf model to predict all pairwise distances between the unknown response , @xmath21 , and all observed responses in the training dataset , @xmath22 .",
    "having estimated these distances , we then utilize them to to predict the response on an euclidean embedding of the manifold , learned from the observed response set using metric mds .",
    "subsequently , a backscoring method is employed to project the point back to the original response manifold @xmath5 .",
    "figure [ man : pred : fig1 ] provides an illustration of the proposed methodology , and the details are in order .",
    "the first step is to estimate the set of distances @xmath23 .",
    "we exploit the inherent ability of the drf model to provide a measure of similarity between pairs of response observations ( see fig .",
    "[ man : pred : fig1](b ) ) . when the new input point @xmath11 is ` dropped ' through each tree in the forest , it reaches a leaf associated with a subset of the training data .",
    "the @xmath21 can be considered similar to that leaf s training responses . to compute the drf - based similarities ,",
    "a vector @xmath24 is used , with each element @xmath25 corresponding to the similarity between @xmath21 and @xmath22 .",
    "initially @xmath25 is set to zero , for all @xmath26 . for each tree in rf , @xmath27 is incremented by one , each time @xmath11 ends in the same node as @xmath28 .",
    "normalization of similarities is performed by division with the number of trees .",
    "based on the similarity vector @xmath13 and the training distance matrix @xmath8 , the response distances are predicted using the algorithmic procedure [ man : pred : algo1 ] ( see fig .",
    "[ man : pred : fig1](c ) ) , which guarantees that the new point will lie in close proximity to at least its closest neighbor based on the drf affinities , and that predicted distance values respect the triangular inequality property of metric @xmath29 .",
    "in detail , the algorithm initially identifies the closest training point to the new response , according to @xmath13 , and assigns the minimum distance observed in the training set as the distance between these two responses .",
    "subsequently , we iterate over the remaining responses , in decreasing order of distance from the first point , and assign a value for the distance to the new observation as follows : for each triplet including the considered point , the new point and a point for which the distance to @xmath21 has been already estimated , we keep the maximum of the two known distances .",
    "subsequently , we assign the minimum of all identified maximums as the predicted distance .",
    "@xmath13 @xmath30 @xmath31 @xmath32 @xmath33 @xmath34 @xmath35 @xmath36 @xmath37 @xmath23      given @xmath23 , we can predict a point estimate of the response on a euclidean embedding of the manifold using mds @xcite ( see fig .",
    "[ man : pred : fig1](d ) ) .",
    "mds computes an approximation @xmath38 of the manifold - valued dataset that resides on a @xmath39-dimensional euclidean space , by minimizing a stress function of the form @xmath40 .",
    "let @xmath41 be the @xmath42 matrix of squared manifold distances and @xmath43 , where @xmath44 , with @xmath45 the @xmath42 identity matrix and @xmath46 an @xmath47 column vector of all ones .",
    "individual elements of @xmath48 are given by @xmath49 where @xmath50 are the @xmath51 row , @xmath52 column and overall element - wise sums of @xmath41 , respectively .",
    "in @xcite , it was shown that , if @xmath48 has rank @xmath53 , with @xmath54 the @xmath55 ordered non - zero eigenvalues of @xmath48 with corresponding eigenvectors @xmath56 , and @xmath57 for @xmath58 , then the solution to the mds problem , assuming @xmath59 , is given by @xmath60 , with @xmath61 , and @xmath62 .",
    "the result of the mds decomposition is a discrete and one - way mapping @xmath63 defined on the training data set . an out - of - sample ( oos ) method which allows mapping of a new manifold observation on the learned space of the embedding was constructed in @xcite .",
    "let @xmath64 be a random variable defined on the manifold surface and @xmath65 two observations of @xmath66 .",
    "a continuous kernel function @xmath67 , which gives rise to @xmath48 under the training observations , is defined as @xmath68 - e[d^2(y,\\mathbf{y}_b ) ] + e[d^2(y , y ) ] ) , $ ] where @xmath69 denotes expectation .",
    "then , an oos prediction @xmath70 is given by @xcite : @xmath71 with @xmath72 denoting the mean estimator of @xmath67 under the augmented dataset @xmath73 : @xmath74    the oos formula does not depend on the actual value of @xmath21 , but rather on the distances between the new response and points on the training dataset .",
    "we can thus leverage the predicted @xmath23 on equation in order to get a point estimate of our response @xmath75 on the embedding space @xmath76 .",
    "now we are left with the task of mapping @xmath75 to the original manifold ( see figure [ man : pred : fig1](e ) ) .",
    "the mapping of the response from the embedding to the manifold - backscoring - can be formulated as an interpolation problem .",
    "specifically , we are looking for a smooth continuous function @xmath77 , that minimizes the cost function @xmath78 .",
    "@xmath79 is a weight parameter balancing the smoothness of the interpolation and the adherence to the data and @xmath80 is a space of smooth functions equipped with the norm @xmath81 , which will be constructed in the following .",
    "a solution to the interpolation problem was presented in @xcite .",
    "let @xmath82 be a closed subset of @xmath76 and @xmath83 a kernel function of the form @xmath84 furthermore , assume that @xmath83 is semi - positive definite on @xmath85 , with @xmath86 for any finite set of points @xmath87 and real numbers @xmath88 .",
    "for a fixed @xmath89 , equation defines a function from @xmath76 to @xmath7 by the formula @xmath90    based on the above , let @xmath80 be the space of all finite linear combinations of functions of the form , as @xmath89 varies in @xmath91 , and its closure w.r.t the scalar inner product @xmath92 .",
    "it follows that @xmath93 .",
    "the interpolation problem can be solved over the space of functions @xmath80 as follows @xcite .",
    "let @xmath94 be the @xmath95 matrix of training responses with rows @xmath96 , and @xmath97 the @xmath98 matrix of kernel values @xmath99 .",
    "the minimizing function can be estimated as : @xmath100 where @xmath101 are elements of the @xmath95 coefficient matrix @xmath102 given by @xmath103 with @xmath104 denotes the vectorization of a matrix into a column vector .",
    "it is clear , from equations and , that the estimation of @xmath105 requires just the knowledge of pairwise kernel values between the @xmath106 points @xmath107 .    in our studies",
    ", we opted to simplify the minimizer , in favor of having just one tuning parameter , by choosing @xmath108 where @xmath109 is a free parameter adjusting the bandwidth of the kernels .",
    "we notice that the backscoring formulation does not take into explicit consideration the response manifold and @xmath105 is not guaranteed to lie exactly on @xmath5 .",
    "nevertheless , we justify our choice by pointing that since @xmath110 is a smooth interpolating function from the embedding to the training responses , predictions should also adhere well to the manifold .",
    "in this section , we present a comparative simulation study to assess the ability of our methodology to cast predictions that adhere to the response manifold .",
    "subsequently , we use our method in two illustrative image completion applications , where the objective is to predict one half of an image from the other half .",
    "we simulated @xmath111 paired input - response points @xmath2 , with inputs @xmath112 corresponding to responses @xmath113 , where @xmath114 denotes the 2-dimensional swiss roll manifold embedded in @xmath115 .",
    "only the first two input dimensions were built to be predictive of the output . in detail",
    ", we first sampled @xmath116 points @xmath117 from the uniform distribution @xmath118 and @xmath119 from @xmath120 .",
    "the response points on the swiss - roll were computed as @xmath121 , while the input variables @xmath122 and @xmath123 were computed by mapping @xmath124 and @xmath125 in the unit circle : @xmath126 .",
    "the values for the remaining input coordinate variables were drawn from the standard normal gaussian distribution .",
    "gaussian noise , with @xmath127 , was added on the response points .",
    "we compared our drf prediction methodology to @xmath128nn regression and standard rf , which do not take into consideration the special form of the response space , as well as kernel rf ( krf ) @xcite , an rf method that employs a kernel function to capture the structure of the response space during training .",
    "the simulated dataset was split into @xmath129 , comprising of @xmath130 input - response pairs and @xmath131 , consisting of the remaining @xmath132 . for @xmath128nn regression ,",
    "the value of @xmath128 was selected to be @xmath133 .",
    "all rfs were built with 150 trees , no pruning and the number of candidate split features in each node was set to @xmath134 . for krf , the training gram matrix was calculated using the gaussian kernel @xmath135 with @xmath136 .",
    "we followed the prediction methodology as described in @xcite , with @xmath137 , where @xmath138 is the rf - based affinity .",
    "the minimization problem was solved over the training set .    for drf ,",
    "the backscoring parameters were @xmath139 and @xmath140 .",
    "since there is no analytical form for computing distances on a swiss - roll manifold , we approximated manifold distances using the isomap distance formulation @xcite : a neighborhood graph @xmath80 was constructed , in which each point @xmath22 was connected to its @xmath141 nearest neighbors in @xmath7 .",
    "graph edges were assigned weights equal to the euclidean distances between the connected points in @xmath7 .",
    "for any two points @xmath22 and @xmath142 in @xmath0 , @xmath143 was then estimated by the shortest path connecting @xmath22 and @xmath142 in graph @xmath80 .",
    "figure [ man : sim : figure9 ] shows test error vectors for the various methodologies used , projected on the @xmath144 plane .",
    "we can see that @xmath133nn missed the goal of regressing on the manifold . for standard rf ,",
    "it is clear that the model constantly underestimated the radius of the predicted points around the @xmath145 axis .",
    "this can be justified by considering that predictions are taken as average points on the euclidean space @xmath115 , unaware of a possible structure in the response space .",
    "krf preserved the manifold structure of the predicted points better than rf , but suffered significantly from the added noise in the responses .",
    "drf outperformed the other methods both in terms of compliance to the underlying response manifold and regarding good robustness to the addition of noise .",
    "plane of the various regression models for the simulated swiss - roll dataset .",
    "( a ) @xmath133nn ( b ) rf ( c ) krf ( d ) drf .",
    "the figure highlights compliance to the response manifold.,scaledwidth=60.0% ]      in imaging analysis , it is common to assume that a collection of similar images lies on a manifold @xcite .",
    "this assumption is guided by the complex nature of images as data objects , as well as the observation that the euclidean metric and the corresponding geometric structure that it imposes on the space do not bode well with the human perception of similarity and difference between images @xcite .",
    "here , we used our manifold regression methodology to predict the bottom half of handwritten digits and human faces from their upper half .      for this application , we extracted @xmath146 gray - scale images of handwritten digits , from the uci machine learning repository @xcite .",
    "each digit class , from @xmath147 to @xmath148 , was represented in the dataset by @xmath149 @xmath150 pixel images .",
    "input data were constructed by vectorization of the @xmath151 upper half pixel intensities .",
    "the dataset was split into training and testing subsets with @xmath152 and @xmath153 .",
    "the upper part of each images was taken as input for the predictive models .",
    "responses comprised of the images bottom parts .",
    "the test set can be seen in fig .",
    "[ man : app : figureall](a ) .",
    "nn , ( c ) rf , ( d ) krf and ( e ) drf.,scaledwidth=40.0% ]    we compared predictions from @xmath154nn , rf , krf and drf models .",
    "all rfs were built with @xmath132 trees and @xmath133 candidate split features in each node .",
    "for drf , the distance matrix was constructed using the isomap distance , with the number of neighbors set to @xmath133 .",
    "a @xmath155-dimensional embedding space was used and the backscoring parameters were @xmath156 and @xmath157 . for krf , the training gram matrix was calculated using the gaussian kernel with @xmath158 . the reconstructed test digits for the various models",
    "are shown in fig .",
    "[ man : app : figureall](b)-(e ) .",
    "table [ man : app : table1 ] summarizes the prediction results for the test data . in the first column",
    "we include the test euclidean mean squared errors ( emse ) for all models . in the case of @xmath154nn and krf , which draw predictions from the training dataset ,",
    "we are also able to report misclassification errors , a.k.a the percentage of predicted lower parts that mismatched the ground truth , which are shown on the second column of table [ man : app : table1 ] .",
    "finally , in the last two columns , we report on the number and percentage of badly reconstructed test images from visual inspection , based on the following criteria : blurriness of the reconstructed image , smooth transition between the upper and bottom image parts , correct digit reconstruction .",
    "this qualitative performance measure is important due to the non - euclidean nature of the data , which , as will be discussed below , makes the emse unsuitable for judging the predictive performance .",
    ".test errors from the digit completion application .",
    "classification error was not applicable for rf and drf .",
    "the number and percentage of badly reconstructed images was visually ascertained from fig .",
    "[ man : app : figureall ] . [ cols=\"^,>,>,>,>,>\",options=\"header \" , ]     [ man : app : table1 ]    1nn and krf cast predictions drawn directly from the training images . as such , no blurriness existed on the reconstructed digits of fig .",
    "[ man : app : figureall](b ) and [ man : app : figureall](d ) .",
    "bad reconstructions were either misclassifications or reconstructions where the transition between the upper ( input ) and lower ( predicted ) image parts was not smooth .",
    "surprisingly , @xmath154nn outperformed krf in terms of classification error , as well as upon visual inspection of the images .",
    "it is obvious from fig .",
    "[ man : app : figureall](c ) that standard rf is ill - suited for the specific application",
    ". the rf prediction approach of averaging pixel intensities from various images resulted in a high number of blurry and nonsensical digit reconstructions .",
    "it is important to notice that while rf performed the worst , based on visual assessment of the reconstructed images , it had the lowest test emse , as shown in table [ man : app : table1 ] .",
    "this observation highlights the unsuitability of emse as a measure of performance , in the case of manifold - valued responses .",
    "reconstructed digits from our drf model are shown in fig .",
    "[ man : app : figureall](e ) .",
    "our predictions were not drawn directly from the training set .",
    "nevertheless , we notice that the large majority of reconstructions had no fuzziness and the transition from upper to lower parts was smooth . for cases where some blurriness could be noticed in the reconstructions , its effect was significantly less severe than in the rf predictions , resulting in the overall lowest number of badly reconstructed digits , based on visual inspection .      in the second application , we used @xmath159 images of faces from the olivetti dataset , as included in the scikit - learn python package @xcite . the dataset consisted of ten gray - scale @xmath160 pixel images for each of 40 distinct subjects , with same subject images taken at different times and with varying pose and facial expressions .",
    "input data were constructed by vectorization of the @xmath161 upper half pixel intensities .",
    "the dataset was split into training and testing subsets with @xmath162 and @xmath163 .",
    "images of the same subject were only allocated either to the training or the testing set .",
    "again , the upper parts of the images were taken as inputs , while bottom parts as responses .",
    "we compared results from @xmath154nn , linear regression ( lr ) , rf and drf .",
    "rfs were built with @xmath132 trees and",
    "@xmath164 candidate split features in each node .",
    "for drf , the training distance matrix was constructed using the isomap metric with the number of neighbors set to @xmath133 , a @xmath155-dimensional embedding space was used and the backscoring parameters were @xmath165 and @xmath166 .",
    "four test images and their reconstructions for the various models are included in fig .",
    "[ man : app : figure4 ] . as we noted in the previous application",
    ", emse does not provide a suitable descriptor of performance .",
    "we rely again on visual inspection of the reconstructed images .",
    "1nn exhibited a hits and miss behavior , with some predictions being similar to the original face , whilst others , such as the second and third depicted faces , being completely different .",
    "in addition , there was minimal smoothness in the transitions from the input to the predicted parts , accentuated specifically on the nasal and zygomatic edges .",
    "lr predictions were extremely blurry .",
    "rf also suffered from a large amount of blurriness , although transitions between the two parts of the faces looked more natural .",
    "finally , drf reconstructions exhibited the best transition smoothness of all methods , with the predicted half - images being well aligned to their inputs .",
    "although the predictions were not completely free of blur artifacts , the effect was less severe and facial details , such as nasolabial folds and lips , were clearly portrayed .",
    "nn , ( c ) lr , ( d ) rf and ( e ) drf .",
    ", scaledwidth=40.0% ]",
    "in this work we presented a predictive modeling approach for response objects occupying non - linear manifold spaces . the regression methodology is based on a distance modification of the rf algorithm that we previously published @xcite , which decouples the model s training from the problem of response representation . for prediction purposes",
    ", we constructed a framework in which point estimates are first predicted on a euclidean embedding of the response manifold , learned from the training dataset , and then projected back on the original space .",
    "our methodology , in contrast to intrinsic manifold methods , necessitates just the definition of a meaningful distance metric in the response space",
    ". this can be especially useful for real - life applications , such as image analysis , where the underlying manifolds are usually not known .",
    "our distance - based regression algorithm draws similarities to the family of kernel - based methodologies .",
    "one benefit over kernel methods is the vast library of readily available distance metrics for a plethora of data objects .",
    "furthermore , our methodology presents a unified framework , which deals with backscoring to the original response space , an issue that a lot of presented kernel methods do not tackle sufficiently well .    in the performed experiments , our method showed superior predictive performance in comparison to various regression methods , whilst being able to handle high - dimensional inputs and noise on the response observations . in the future",
    ", we aim to investigate the problem of automatic estimation of the euclidean embedding s dimensionality , as well as the use of more elaborate kernel functions in the backscoring formulation .",
    "this research did not receive any specific grant from funding agencies in the public , commercial , or not - for - profit sectors .",
    "the article is currently under consideration for publication at patter recognition letters .",
    "21 natexlab#1#1[1]`#1 ` [ 2]#2 [ 1]#1 [ 1]http://dx.doi.org/#1 [ ] [ 1]pmid:#1 [ ] [ 2]#2 , , , , , , . .",
    "volume  . .",
    ", , . , in : . , pp . . , . , in : .",
    ", pp . . , , ,",
    "http://archive.ics.uci.edu/ml . , .",
    ", , , , , , , , , , , , , , , , . . ,",
    ". , . . , . ,",
    ". . , . , , , , ,",
    ". . , . , ,",
    ". . . , , ,",
    ", , , . . ,",
    ", . , in : , .",
    "pp . . , , , , , ,"
  ],
  "abstract_text": [
    "<S> an increasing array of biomedical and computer vision applications requires the predictive modeling of complex data , for example images and shapes . </S>",
    "<S> the main challenge when predicting such objects lies in the fact that they do not comply to the assumptions of euclidean geometry . </S>",
    "<S> rather , they occupy non - linear spaces , a.k.a . manifolds , where it is difficult to define concepts such as coordinates , vectors and expected values . in this work , we construct a non - parametric predictive methodology for manifold - valued objects , based on a distance modification of the random forest algorithm . </S>",
    "<S> our method is versatile and can be applied both in cases where the response space is a well - defined manifold , but also when such knowledge is not available . </S>",
    "<S> model fitting and prediction phases only require the definition of a suitable distance function for the observed responses . </S>",
    "<S> we validate our methodology using simulations and apply it on a series of illustrative image completion applications , showcasing superior predictive performance , compared to various established regression methods . </S>"
  ]
}