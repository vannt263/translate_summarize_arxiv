{
  "article_text": [
    "the linear least squares ( ls ) approach is an important and extensively studied problem in many areas of signal processing with many practical applications from localization @xcite to battery state estimation @xcite . in applying the linear ls approach for a vector parameter @xmath0 , we assume a signal model @xmath1 disturbed by noise @xmath2 such that @xmath3 where @xmath4 is a known @xmath5 observation matrix ( @xmath6 ) with full rank @xmath7 , @xmath8 is a known @xmath9 vector ( typically from measurements ) , @xmath10 is an unknown @xmath11 parameter vector that is to be estimated and @xmath12 is an @xmath9 noise vector . for the ls approach ,",
    "the statistical properties of @xmath2 need not to be known . for simplicity",
    "we only consider real vectors and matrices in this work , however , the presented concepts can easily be extended for complex vectors and matrices .",
    "the vector @xmath13 that minimizes the cost function @xmath14 is the solution to the ls problem . here",
    "@xmath15 is the @xmath16 row of @xmath17 and @xmath18 is the @xmath16 element of @xmath19 , respectively .",
    "the ls solution is given by @xmath20 with @xmath21 as the pseudoinverse of @xmath4 .",
    "numerically more stable algorithms avoiding explicitly calculating @xmath22 , e.g. based on the qr decomposition , can for example be found in @xcite . a solution as in ( [ eqn : batch ] )",
    "is often called batch solution in literature @xcite .    for real time applications",
    "one usually wants to avoid the calculation of the batch solution due to its computational complexity and its large memory requirements .",
    "alternatives are sequential algorithms such as the sequential least squares ( sls ) algorithm  described in the next section  or gradient based approaches such as the iterative ls ( ils ) @xcite algorithm .",
    "the latter algorithm is based on the steepest descent approach and iteratively calculates @xmath23 for iteration @xmath24 . here",
    "@xmath25 is the gradient of @xmath26 at @xmath27 . for @xmath28",
    ", @xmath29 converges to @xmath13 given that the iteration step width @xmath30 fulfills @xmath31 @xcite , with @xmath32 as the largest singular value of @xmath17 .",
    "alternatively , ( [ eqn : ls_iteration ] ) can be written as @xmath33 analyzing the complexity of this approach one can see that @xmath34 multiplications are required per iteration .",
    "in addition , every iteration of ils requires the availability of all elements of the measurement vector @xmath19 .",
    "based on the principle of ils we propose a novel iterative way of approximating the least squares solution that we call approximate least squares ( als ) .",
    "as we will show , the complexity of this approach is significantly lower than for ils and it requires only one measurement value @xmath18 per iteration .",
    "when analyzing ( [ eqn : ils_grad ] ) , the gradient can be interpreted as a sum of the partial gradients @xmath35 as schematically depicted in fig .",
    "[ fig : partgrad ] .",
    "the idea of als is to use only _ one _ of these partial gradients per iteration .",
    "instead of moving a small step ( due to @xmath30 ) in a steepest descent way in the negative direction of the gradient as done by ils , als moves a small step in the negative direction of only a partial gradient .",
    "this has the advantage of a lower complexity , but  as we will discuss below  also has the disadvantage of a higher noise sensitivity . following this general idea , two issues have to be addressed .",
    "first , the number of iterations of the algorithm to achieve satisfying performance results may be higher than the number of rows of @xmath17 .",
    "second , the noise sensitivity has to be reduced . to cope with the first issue we suggest to re - use the rows @xmath15 of * h * in a cyclic manner .",
    "let the operator `` @xmath36 '' be defined such that for a positive natural number @xmath37 : @xmath38m@xmath39 . from this",
    "it follows that @xmath40 . for better readability we do not write the dependence of this operator on @xmath41 in the operator s symbol . for als",
    ", @xmath41 is always the number of rows of the matrix @xmath17 .",
    "an als iteration is now defined as @xmath42 that means for als that if @xmath24 reaches @xmath41 , for the following iterations the first rows of @xmath17 and the first elements of @xmath19 are used again in a cyclic manner .",
    "we will now address the second issue , namely the noise sensitivity . as we will discuss below , if @xmath43 then @xmath29 converges to @xmath13 as @xmath28 .",
    "for the usual case @xmath44 , a noise dependent error remains .",
    "this error can be greatly reduced by introducing a simple averaging process in the last @xmath41 iterations .",
    "a formal justification for this averaging will be given within the error analysis in sect .",
    "[ sec : conv_behav ] .",
    "summarizing , this leads to an overall formulation of the algorithm : + -6.5    ' '' ''    -1algorithm : als + -7    ' '' ''    @xmath45 @xmath46 @xmath47 @xmath48 @xmath49    -2    ' '' ''    here @xmath50 denotes the number of iterations of the algorithm and @xmath51 is the approximation of @xmath13 that is output by the algorithm . when analyzing the above algorithm , and only counting the multiplications",
    ", one can see that @xmath52 overall multiplications are required to perform the algorithm .",
    "compared to ils a factor of around @xmath41 fewer multiplications per iteration are required .",
    "although more iterations are usually needed for als its overall complexity is significantly lower as will be demonstrated in sect .",
    "[ sec : sim_res ] .",
    "this decrease in complexity is bought with only a small degradation in performance .",
    "an additional advantage of als is that per iteration only one value @xmath18 and only one row @xmath15 of @xmath17 are required .",
    "this significantly reduces the required number of other operations ( additions , memory accesses , ... ) and also simplifies the memory management as well as the architecture when thinking of a hardware implementation .",
    "als not only has similarities to ils but also to the sls approach @xcite . for sls the update equation @xmath53",
    "is sequentially calculated @xmath41 times , requiring an update of the gain vector @xmath54 at every iteration .",
    "although , the algorithm can deliver @xmath13 after @xmath41 iterations , the update of @xmath54 requires significant effort , including the multiplication of full matrices ( although symmetry can be exploited to reduce the complexity ) .",
    "als uses the same update equation ( usually more than @xmath41 times ) , with the simplified choice @xmath55 .    update equation ( 8) is arithmetically similar to the least mean squares ( lms ) filter update step @xcite .",
    "however , the lms update step uses a random ( filter input ) vector and one sample of a desired signal as input , whereas the als update step only uses one sample @xmath18 of the measurement vector @xmath19 as input . also the original formulation of the lms algorithm for the so - called adaline @xcite approach",
    "was based on a random input vector , providing an adaptive approach with a potentially unlimited set of input patterns . instead of the random input vector in the lms case",
    "the deterministic and fixed rows of the observation matrix h are used in the update equation of the als .",
    "the row vectors @xmath15 and the measurement values @xmath18 are cyclically re - used .",
    "another difference is the averaging at the last @xmath41 iterations which is unique for the als algorithm . and",
    "finally , the convergence behavior of als can be described in a completely deterministic manner , whereas the convergence of the lms is usually only described in the mean .",
    "anyhow , the authors are confident that some ideas improving lms  e.g. adjusting the step size @xcite might be also used to further improve the performance of als .",
    "by rewriting ( [ eqn : als_main_equation ] ) as @xmath56 and defining the error vector of als @xmath57 together with @xmath58 one gets @xmath59 subtracting @xmath0 left and right from the equation leads to @xmath60 when defining @xmath61 one can write the above equation as @xmath62 with @xmath63 as the initial error . here the product of the matrices is defined as @xmath64 .",
    "when analyzing the above equation one can see that the error at iteration @xmath24 depends on the initial error @xmath63 represented in @xmath65 by the part @xmath66 as well as on an error term introduced by noise represented by @xmath67",
    ". with this one can write @xmath68 if no noise is present then @xmath69 when choosing @xmath24 as an integer multiple of @xmath41 and defining @xmath70 one obtains @xmath71 in @xcite we show that for the choice @xmath72 the matrix @xmath73 has a @xmath74-norm smaller than one ( although the proof is not complicated it is omitted here due to length constraints ) .",
    "this implies that all eigenvalues of @xmath73 have absolute values smaller than one . from this",
    "it directly follows that @xmath75 converges to zero as @xmath28 , i.e. @xmath76 .",
    "this means that if no noise is present @xmath29 converges to @xmath10 .",
    "however if @xmath44 a persistent error @xmath77 remains .",
    "in @xcite we will give a more detailed analysis of @xmath77 , showing that @xmath65 features almost a periodic behavior from an index @xmath78 on , wherefrom @xmath75 can be considered negligible .",
    "this particular index @xmath78 , which can also be specified analytically , can be used to define @xmath50 e.g. as @xmath79 .    by analyzing the als algorithm",
    "one can see the importance of the averaging in the final @xmath41 iterations . as we already noted",
    "@xmath65 is highly dependent on the noise for large @xmath24 ( @xmath75 vanishes with increasing @xmath24 ) .",
    "the averaging over the last @xmath29 vectors yields    @xmath80    that means by averaging over the last @xmath41 vectors @xmath29 an averaging over the corresponding error vectors occurs . since for a practical application",
    "it is highly unlikely that all these error vectors have equal length and point in the same direction ( in this case averaging would have no effect ) this averaging step typically significantly reduces the error norm .",
    "the averaging only has to be done _ once _ , it therefore presents only a minor complexity increase ( overall only @xmath81 additions and @xmath7 multiplications with the constant @xmath82 ) .",
    "we first show simulation results of a typical example of least squares estimation : the estimation of amplitudes of sine signals in noise . for this demonstration example we chose @xmath4 as a @xmath83 matrix with elements @xmath84 .",
    "the frequencies @xmath85 are not necessarily integer multiples of a base frequency .",
    "the elements of the noise vector have been sampled indepentently from a normal distribution with zero mean and a standard deviation @xmath86 .",
    "the amplitudes @xmath0 have been estimated using @xmath87 values , forming the vector @xmath19 .",
    "the step size for ils was chosen as @xmath88 and for als as @xmath89 .",
    "the estimation performance has been measured by calculating the norm of the difference vector between the true vector @xmath0 and the estimated vectors , respectively .",
    "[ fig : error_sin ] shows a typical simulation result for ils and als . in this figure , @xmath51 is the estimated parameter vector resulting after averaging the final @xmath41 out of @xmath50 vectors @xmath29 , represented as a horizontal line for illustration purposes .",
    "as one can see , ils requires significantly less iterations than als , but with about @xmath90 times more multiplications per iteration .",
    "the performance of als is only slightly worse than the performance of ils but als features a significantly lower overall complexity . in this figure",
    "one can observe an interesting behavior of als .",
    "after a certain number of iterations the influence of @xmath75 becomes negligible .",
    "this reflects in an oscillatory behavior of the error norm as can be seen in fig .",
    "[ fig : error_sin ] .",
    "this oscillatory behavior comes from the fact that the values @xmath18 are cyclically re - used in the @xmath50 als iterations . as a consequence",
    "also the noise values appear in a cyclic manner .",
    "the averaging at the end of als is most effective if @xmath50 is chosen large enough so that the effects of @xmath75 are negligible .",
    "such a value for @xmath50 can be found with simulations or based on analytical results as will be presented in @xcite . to provide a fair comparison , in fig .",
    "[ fig : error_sin_mul ] we compared als , ils and sls in terms of its error norms over the number of calculated multiplications . as one can see , if the error performance of als is sufficient for a given application , its complexity is significantly lower . in this example",
    "ils needs about @xmath91 times more multiplications than als to obtain the same error norm .",
    "but as stated above , this complexity analysis is only based on the number of multiplications per iteration . including other operations ( additions , memory accesses )",
    "would furthermore favor als .",
    "due to page constraints we omitted a more detailed complexity analysis in this paper . if the error performance of als is not sufficient for a given application one could choose a different approach , but extended variants of the als , e.g. with adjusting @xmath30 during the iterations show promising first results towards further reducing the error norm .",
    "one can immediately see the benefits of such an approach in ( [ eqn : first_error_splitting ] ) because the noise dependent part of the error vector scales with @xmath30 , as will be described in detail in @xcite .     ]     ]    but as extensive performance simulations showed , als performance is on average very close to the ls solution . table .",
    "[ tab : perf_res ] shows performance results for random @xmath17 matrices .",
    "the entries of these matrices have been sampled from a uniform distribution out of @xmath92 $ ] .",
    "every simulation has been done for white gaussian noise with @xmath93 , respectively , with @xmath87 random matrices @xmath17 per @xmath94 value and @xmath87 random vectors @xmath0 ( with random entries also sampled from a uniform distribution out of @xmath92 $ ] ) per @xmath17 matrix . for every @xmath94 value the averages @xmath95 and @xmath96 over the simulated results have been calculated .",
    "[ tab : perf_res ] shows the maximum relative increase of als averaged error norm over the averaged error norms of ls , whereas the maximization has been done over the elements of @xmath97 : @xmath98 .",
    "we furthermore want to note that the relative increase of the averaged error norms remained nearly constant over all simulated @xmath94 values .",
    "as one can see in this table , the performance of als shows on average only a minor degradation compared to the ls solution .",
    ".performance results for random matrices .",
    "[ tab : perf_res ] [ cols=\"<,^,<,^\",options=\"header \" , ]",
    "we presented a novel algorithm for approximating the solution of the linear least squares problem .",
    "we discussed its convergence behavior and demonstrated that the algorithm provides a close solution to the least squares solution with low complexity .",
    "the presented algorithm shows promising potential for further extension in theory and implementation as well for use in a variety of applications .",
    "1 choi , k.h . ; ra , w .- s . , park , s .- y . ; park , j.b .",
    ", `` robust least squares approach to passive target localization using ultrasonic receiver array , '' _ ieee transactions on industrial electronics _ , vol .",
    "1993 - 2002 , apr . 2014 .",
    "unterrieder , c. , lunglmayr , m. , marsili , s. , huemer , m. , `` battery state - of - charge estimation using polynomial enhanced prediction , '' _ iet electronics letters _ , vol .",
    "1363 - 1365 , oct .",
    "2012 .",
    "b. widrow , j. r. glover , j. m. mccool , j. kaunitz , c. s. williams , r. h. heam , j. r. zeidler , e. dong , and r. c. goodlin , `` adaptive noise cancelling : principles and applications , '' _ proc .",
    "1692 - 1716 , dec ."
  ],
  "abstract_text": [
    "<S> we present a novel iterative algorithm for approximating the linear least squares solution with low complexity . after a motivation of the algorithm we discuss the algorithm s properties including its complexity , and we present theoretical results as well as simulation based performance results . </S>",
    "<S> we describe the analysis of its convergence behavior and show that in the noise free case the algorithm converges to the least squares solution .    </S>",
    "<S> least squares , approximation , iterative algorithm , complexity . </S>"
  ]
}