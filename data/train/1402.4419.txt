{
  "article_text": [
    "the principle of successively minimizing upper bounds of the objective function is often called _ majorization - minimization _ @xcite or _ successive upper - bound minimization _  @xcite .",
    "each upper bound is locally tight at the current estimate , and each minimization step decreases the value of the objective function . even though this principle does not provide any theoretical guarantee about the quality of the returned solution",
    ", it has been very popular and widely used because of its simplicity .",
    "various existing approaches can indeed be interpreted from the majorization - minimization point of view .",
    "this is the case of many gradient - based or proximal methods  @xcite , expectation - maximization ( em ) algorithms in statistics @xcite , difference - of - convex ( dc ) programming @xcite , boosting  @xcite , some variational bayes techniques used in machine learning  @xcite , and the mean - shift algorithm for finding modes of a distribution  @xcite .",
    "majorizing surrogates have also been used successfully in the signal processing literature about sparse estimation @xcite , linear inverse problems in image processing  @xcite , and matrix factorization @xcite .    in this paper , we are interested in making the majorization - minimization principle scalable for minimizing a large sum of functions : @xmath0 , \\label{eq : prob}\\ ] ] where the functions @xmath1 are continuous , and @xmath2 is a convex subset of @xmath3 .",
    "when  @xmath4 is non - convex , exactly solving  ( [ eq : prob ] ) is intractable in general , and when @xmath4 is also non - smooth , finding a stationary point of  ( [ eq : prob ] ) can be difficult .",
    "the problem above when @xmath5 is large can be motivated by machine learning applications , where  @xmath6 represents some model parameters and each function @xmath7 measures the adequacy of the parameters  @xmath6 to an observed data point indexed by  @xmath8 . in this context , minimizing  @xmath4 amounts to finding parameters  @xmath6 that explain well some observed data . in the last few years",
    ", stochastic optimization techniques have become very popular in machine learning for their empirical ability to deal with a large number @xmath5 of training points  @xcite .",
    "even though these methods have inherent sublinear convergence rates for convex and strongly convex problems  @xcite , they typically have a cheap computational cost per iteration , enabling them to efficiently find an approximate solution .",
    "recently , incremental algorithms have also been proposed for minimizing finite sums of functions  @xcite . at the price of a higher memory cost than stochastic algorithms ,",
    "these incremental methods enjoy faster convergence rates , while also having a cheap per - iteration computational cost .",
    "our paper follows this approach : in order to exploit the particular structure of problem  ( [ eq : prob ] ) , we propose an incremental scheme whose cost per iteration is independent of @xmath5 , as soon as the upper bounds of the objective are appropriately chosen .",
    "we call the resulting scheme `` miso '' ( _ minimization by incremental surrogate optimization _ ) . we present convergence results when the upper bounds are chosen among the class of `` first - order surrogate functions '' , which approximate the objective function up to a smooth error  that is , differentiable with a lipschitz continuous gradient . for non - convex problems",
    ", we obtain almost sure convergence and asymptotic stationary point guarantees .",
    "in addition , when assuming the surrogates to be strongly convex , we provide convergence rates for the expected value of the objective function . remarkably ,",
    "the convergence rate of miso is linear for minimizing strongly convex composite objective functions , a property shared with two other incremental algorithms for smooth and composite convex optimization : the _ stochastic average gradient _ method ( sag ) of schmidt , le roux and bach  @xcite , and the _ stochastic dual coordinate ascent _ method ( sdca ) of shalev - schwartz and zhang  @xcite .",
    "our scheme miso is inspired in part by these two works , but yields different update rules than sag or sdca , and is also appropriate for non - convex optimization problems .    in the experimental section of this paper ,",
    "we show that miso can be useful for solving large - scale machine learning problems , and that it matches cutting - edge solvers for large - scale logistic regression @xcite .",
    "then , we show that our approach provides an effective incremental dc programming algorithm , which we apply to sparse estimation problems with nonconvex penalties  @xcite .",
    "the paper is organized as follows : section  [ sec : generic ] introduces the majorization - minimization principle with first - order surrogate functions . section  [ sec : incremental ] is devoted to our incremental scheme miso .",
    "section  [ sec : exp ] presents some numerical experiments , and section  [ sec : ccl ] concludes the paper .",
    "some basic definitions are given in appendix  [ appendix : background ] .",
    "in this section , we present the generic majorization - minimization scheme for minimizing a function  @xmath4 without exploiting its structure  that is , without using the fact that @xmath4 is a sum of functions .",
    "we describe the procedure in algorithm  [ alg : generic_batch ] and illustrate its principle in figure  [ fig : mm ] . at iteration",
    "@xmath9 , the estimate  @xmath10 is obtained by minimizing a surrogate function  @xmath11 of  @xmath4 . when @xmath11 uniformly majorizes  @xmath4 and when @xmath12 , it is clear that the objective function value monotonically decreases .",
    "@xmath13 ( initial estimate ) ; @xmath14 ( number of iterations ) .",
    "compute a surrogate function @xmath11 of @xmath4 near @xmath15 ; minimize the surrogate and update the solution : @xmath16    @xmath17 ( final estimate ) ;    ( -5,-0.5)(5,4 ) ( -5,0)(5,0 ) ( 0,-0.5)(0,3 ) ( -4,0.9)(-4,2.5 ) ( 3.84,2.4 ) @xmath18 ( -3.33,1.9 ) @xmath19 ( -5.3,1.6 ) @xmath20 ( 0.62,1.40)(-1.3,0.3 ) ( 0.9,1.2 ) @xmath15 ( -1.4,0.6 ) @xmath21 ( 2.1,1.0 ) @xmath22    for this approach to be effective , intuition tells us that we need functions  @xmath11 that are easy to minimize and that approximate well the objective  @xmath4 .",
    "therefore , we measure the quality of the approximation through the smoothness of the error @xmath23 , which is a key quantity arising in the convergence analysis . specifically , we require  @xmath24 to be @xmath25-smooth for some constant  @xmath26 in the following sense :    [ def : lsmooth ] a function @xmath27 is called @xmath25-smooth when it is differentiable and when its gradient @xmath28 is @xmath25-lipschitz continuous .    with this definition in hand",
    ", we now introduce the class of `` first - order surrogate functions '' , which will be shown to have good enough properties for analyzing the convergence of algorithm  [ alg : generic_batch ] and other variants .",
    "[ def : surrogate_batch ] a function @xmath29 is a first - order surrogate function of @xmath4 near  @xmath30 in  @xmath2 when    @xmath31 for all minimizers @xmath32 of @xmath33 over @xmath2 . when the more general condition @xmath34 holds , we say that @xmath33 is a _ majorizing _ surrogate ;    the approximation error @xmath35 is @xmath25-smooth , @xmath36 , and @xmath37 .",
    "we denote by  @xmath38 the set of first - order surrogate functions and by  @xmath39 the subset of @xmath40-strongly convex surrogates .",
    "first - order surrogates are interesting because their approximation error  the difference between the surrogate and the objective  can be easily controlled .",
    "this is formally stated in the next lemma , which is a building block of our analysis :    [ lemma : basic ] let  @xmath33 be a surrogate function in @xmath38 for some @xmath30 in @xmath2 . define the approximation error @xmath35 , and let @xmath32 be a minimizer of @xmath33 over @xmath2 .",
    "then , for all  @xmath6 in  @xmath2 ,    * @xmath41 ; * @xmath42 .",
    "assume that @xmath33 is @xmath40-strongly convex , i , e . , @xmath33 is in @xmath43 .",
    "then , for all @xmath6 in @xmath2 ,    * @xmath44 .",
    "the first inequality is a direct application of a classical result ( lemma 1.2.3 of  @xcite ) on quadratic upper bounds for @xmath25-smooth functions , when noticing that @xmath36 and @xmath37 .",
    "then , for all @xmath6 in @xmath2 , we have @xmath45 and we obtain the second inequality from the first one .",
    "when @xmath33 is @xmath40-strongly convex , we use the following classical lower bound  ( see @xcite ) : @xmath46 since @xmath47 by definition  [ def : surrogate_batch ] and @xmath48 , the third inequality follows from the first one .",
    "we now proceed with a convergence analysis including four main results regarding algorithm  [ alg : generic_batch ] with first - order surrogate functions  @xmath11 .",
    "more precisely , we show in section  [ subsec : nonconvex ] that , under simple assumptions , the sequence of iterates asymptotically satisfies a stationary point condition .",
    "then , we present a similar result with relaxed assumptions on the surrogates  @xmath11 when @xmath4 is a composition of two functions , which occur in practical situations as shown in section  [ subsec : surrogates ] . finally",
    ", we present non - asymptotic convergence rates when  @xmath4 is convex in section  [ subsec : convex ] . by adapting convergence proofs of proximal gradient methods  @xcite to our more general setting , we recover classical sublinear rates @xmath49 and linear convergence rates for strongly convex problems .      for general non - convex problems ,",
    "proving convergence to a global ( or local ) minimum is impossible in general , and classical analysis studies instead asymptotic stationary point conditions  ( see , _ e.g. _ , @xcite ) .",
    "to do so , we make the following mild assumption when @xmath4 is non - convex :    * @xmath4 is bounded below and for all @xmath50 in @xmath2 , the directional derivative @xmath51 of @xmath4 at  @xmath6 in the direction @xmath52 exists .    the definitions of directional derivatives and stationary points are provided in appendix  [ appendix : background ] .",
    "a necessary first - order condition for  @xmath6 to be a local minimum of @xmath4 is to have @xmath53 for all  @xmath32 in  @xmath2 ( see , _",
    "e.g. _ , @xcite ) .",
    "in other words , there is no feasible descent direction  @xmath54 and @xmath6 is a stationary point .",
    "thus , we consider the following condition for assessing the quality of a sequence @xmath55 for non - convex problems :    under assumption  , a sequence @xmath55 satisfies the asymptotic stationary point condition if @xmath56 note that if @xmath4 is differentiable on @xmath3 and @xmath57 , @xmath58 , and the condition  ( [ eq : stationary ] ) implies that the sequence @xmath59 converges to @xmath60 .",
    "as noted , we recover the classical definition of critical points for the smooth unconstrained case .",
    "we now give a first convergence result about algorithm  [ alg : generic_batch ] .",
    "[ prop : conv1 ] assume that  holds and that the surrogates @xmath11 from algorithm  [ alg : generic_batch ] are in @xmath61 and are either majorizing @xmath4 or strongly convex .",
    "then , @xmath62 monotonically decreases , and  @xmath63 satisfies the asymptotic stationary point condition .    the fact that @xmath64 is non - increasing and convergent because bounded below is clear : for all @xmath65 , @xmath66 , where the first inequality and the last equality are obtained from definition  [ def : surrogate_batch ] .",
    "the second inequality comes from the definition of @xmath10 .",
    "let us now denote by @xmath67 the limit of the sequence @xmath68 and by @xmath69 the approximation error function at iteration  @xmath9 , which is @xmath25-smooth by definition  [ def : surrogate_batch ] and such that  @xmath70 .",
    "then , @xmath71 , and @xmath72 thus , the non - negative sequence @xmath73 necessarily converges to zero .",
    "then , we have two possibilities ( according to the assumptions made in the proposition ) .    *",
    "if the functions @xmath11 are majorizing  @xmath4 , we define @xmath74 , and we use the following classical inequality for @xmath25-smooth functions  @xcite : @xmath75 therefore , we may use the fact that @xmath76 because @xmath77 , and @xmath78 * if instead the functions @xmath11 are @xmath40-strongly convex , the last inequality of lemma  [ lemma : basic ] with @xmath79 and  @xmath80 gives us @xmath81 by summing over @xmath9 , we obtain that @xmath82 converges to zero , and @xmath83 since @xmath84 according to definition  [ def : surrogate_batch ] .",
    "we now consider the directional derivative of @xmath4 at @xmath10 and a direction @xmath85 , where @xmath86 and @xmath6 is in  @xmath2 , @xmath87 note that @xmath10 minimizes @xmath11 on @xmath2 and therefore @xmath88 .",
    "therefore , @xmath89 by cauchy - schwarz s inequality . by minimizing over @xmath6 and taking the infimum limit",
    ", we finally obtain @xmath90    this proposition provides convergence guarantees for a large class of existing algorithms , including cases where @xmath4 is non - smooth . in the next proposition , we relax some of the assumptions for objective functions that are compositions @xmath91 , where @xmath92 is the composition operator .",
    "in other words , @xmath93 for all @xmath6 in  @xmath3 .    [",
    "prop : conv1_separable ] assume that  holds and that the function @xmath4 is a composition @xmath94 , where @xmath95 is @xmath96-lipschitz continuous for some constant @xmath97 , and @xmath98 .",
    "assume that the function  @xmath11 in algorithm  [ alg : generic_batch ] is defined as @xmath99 , where @xmath100 is a majorizing surrogate in @xmath101 .",
    "then , the conclusions of proposition  [ prop : conv1 ] hold .",
    "we follow the same steps as the proof of proposition  [ prop : conv1 ] .",
    "first , it is easy to show that @xmath64 monotonically decreases and that @xmath102 converges to zero when @xmath9 grows to infinity .",
    "note that since we have made the assumptions that @xmath103 and that @xmath94 , the function @xmath104 can be written as @xmath105 , where @xmath106 is @xmath25-smooth .",
    "proceeding as in the proof of proposition  [ prop : conv1 ] , we can show that @xmath107 converges to zero .",
    "let us now fix @xmath86 and consider @xmath108 such that @xmath109 is in  @xmath2 .",
    "we have @xmath110 where @xmath111 is a vector whose @xmath112-norm is bounded by a universal constant  @xmath113 because the function  @xmath114 is lipschitz continuous .",
    "since @xmath115 is @xmath25-smooth , we also have @xmath116 plugging this simple relation with @xmath117 , for some @xmath118 and @xmath6 in  @xmath2 , into the definition of the directional derivative @xmath119 , we obtain the relation @xmath120 and since @xmath121 , and @xmath88 , @xmath122    in this proposition , @xmath11 is an upper bound of  @xmath91 , where the part  @xmath114 is lipschitz continuous but @xmath123 is not  @xmath25-smooth .",
    "this extension of proposition  [ prop : conv1 ] is useful since it provides convergence results for classical approaches that will be described later in section  [ subsec : surrogates ] .",
    "note that convergence results for non - convex problems are by nature weak , and our non - convex analysis does not provide any convergence rate .",
    "this is not the case when  @xmath4 is convex , as shown in the next section .",
    "the next proposition is based on a proof technique from nesterov  @xcite , which was originally designed for the proximal gradient method . by adapting it ,",
    "we obtain the same convergence rates as in  @xcite .",
    "[ prop : conv2 ] assume that @xmath4 is convex , bounded below , and that there exists a constant @xmath124 such that @xmath125 where @xmath126 is a minimizer of @xmath4 on @xmath2 . when the functions  @xmath11 in algorithm  [ alg : generic_batch ] are in @xmath127 , we have for all @xmath86 , @xmath128 where @xmath129 .",
    "assume now that @xmath4 is @xmath130-strongly convex .",
    "regardless of condition  ( [ eq : bounded ] ) , we have for all @xmath86 , @xmath131 where @xmath132 if @xmath133 or @xmath134 otherwise .",
    "we successively prove the two parts of the proposition .",
    "+ _ non - strongly convex case : _",
    "let us consider the function @xmath135 at iteration @xmath86 .",
    "by lemma  [ lemma : basic ] , @xmath136.\\ ] ] then , following a similar proof technique as nesterov in  @xcite , @xmath137 } \\left [ f(\\alpha\\theta^\\star+(1-\\alpha)\\theta_{n-1 } ) + \\frac{l\\alpha^2}{2 } \\|\\theta^\\star-\\theta_{n-1}\\|_2 ^ 2\\right ] \\\\                    & \\leq \\min_{\\alpha \\in [ 0,1 ] }   \\left [ \\alpha f(\\theta^\\star ) + ( 1-\\alpha)f(\\theta_{n-1 } ) + \\frac{l\\alpha^2}{2 } \\|\\theta^\\star-\\theta_{n-1}\\|_2 ^ 2\\right ] ,     \\end{split } \\label{eq : tmp_rate2}\\ ] ] where the minimization over @xmath2 is replaced by a minimization over the line segment @xmath138 $ ] .",
    "since the sequence @xmath64 is monotonically decreasing we may use the bounded level set assumption and we obtain @xmath139 } \\left [ ( 1-\\alpha)(f(\\theta_{n-1 } ) - f^\\star ) + \\frac{lr^2\\alpha^2}{2}\\right].\\label{eq : tmp_rate3}\\ ] ] to simplify , we introduce the notation  @xmath140 , and we consider two cases :    * _ first case : _ if @xmath141 , then the optimal value @xmath142 in  ( [ eq : tmp_rate3 ] ) is  @xmath143 and we consequently have @xmath144 ; * _ second case : _ otherwise @xmath145 and @xmath146 .",
    "thus , @xmath147 , where the second inequality comes from the convexity inequality @xmath148 for @xmath149 .",
    "we now apply recursively the previous inequalities , starting with @xmath150 .",
    "if @xmath151 , we are in the first case and then @xmath152 ; then , we will subsequently be in the second case for all @xmath153 and thus @xmath154 . otherwise , if @xmath155 , we are always in the second case and @xmath156 , which is sufficient to obtain the first part of the proposition .    _",
    "@xmath130-strongly convex case : _",
    "let us now assume that @xmath4 is @xmath130-strongly convex , and let us drop the bounded level sets assumption .",
    "the proof again follows  @xcite for computing the convergence rate of proximal gradient methods .",
    "we start from  ( [ eq : tmp_rate2 ] ) .",
    "we use the strong convexity of  @xmath4 which implies that @xmath157 , and we obtain @xmath158 }   1-\\alpha + \\frac{l\\alpha^2}{\\mu}\\right)(f(\\theta_{n-1})-f^\\star).\\ ] ] at this point , it is easy to show that if @xmath159 , the previous binomial is minimized for @xmath160 , and if @xmath161 , then we have @xmath162 . this yields the desired result .",
    "the result of proposition  [ prop : conv2 ] is interesting because it does not make any strong assumption about the surrogate functions , except the ones from definition  [ def : surrogate_batch ] .",
    "the next proposition shows that slightly better rates can be obtained with additional strong convexity assumptions .",
    "[ prop : conva ] assume that @xmath4 is convex , bounded below , and let @xmath126 be a minimizer of  @xmath4 on  @xmath2 . when the surrogates  @xmath11 of algorithm  [ alg : generic_batch ] are in @xmath163 with @xmath164 , we have for all @xmath86 , @xmath165 where @xmath129 .",
    "when @xmath4 is @xmath130-strongly convex , we have for all @xmath65 , @xmath166    as before , we successively prove the two parts of the proposition .",
    "_ non - strongly convex case : _   from lemma  [ lemma : basic ] ( with @xmath167 , @xmath168 , @xmath169 , @xmath170 ) , we have for all @xmath65 , @xmath171 after summation , @xmath172 where the first inequality comes from the inequalities @xmath173 for all @xmath174 .",
    "this is sufficient to prove the first part .",
    "note that proving convergence rates for first - order methods by finding telescopic sums is a classical technique  ( see , _",
    "e.g._,@xcite ) .    _",
    "@xmath130-strongly convex case : _",
    "let us now assume that @xmath4 is @xmath130-strongly convex .",
    "the strong convexity implies that @xmath175 for all @xmath9 .",
    "combined with  ( [ eq : rate1_proof ] ) , this yields @xmath176 and thus @xmath177    even though the constants obtained in the rates of proposition  [ prop : conva ] are slightly better than the ones of proposition  [ prop : conv2 ] , the condition @xmath11 in @xmath43 with @xmath164 is much stronger than the simple assumption that  @xmath11 is in @xmath178 . it can indeed be shown that  @xmath4 is necessarily @xmath179-strongly convex if @xmath180 , and convex if @xmath181 . in the next section ,",
    "we give some examples where such a condition holds .",
    "we now present practical first - order surrogate functions and links between algorithm  [ alg : generic_batch ] and existing approaches .",
    "even though our generic analysis does not always bring new results for each specific case , its main asset is to provide a unique theoretical treatment to all of them .",
    "when @xmath4 is  @xmath25-smooth , it is natural to consider the following surrogate : @xmath182 the function  @xmath33 is an upper bound of @xmath4 , which is a classical result  @xcite .",
    "it is then easy to see that  @xmath33 is  @xmath25-strongly convex and @xmath25-smooth . as a consequence ,",
    "the difference  @xmath183 is  @xmath184-smooth ( as a sum of two  @xmath25-smooth functions ) , and thus @xmath33 is in @xmath185 .    when @xmath4 is convex , it is also possible to show by using lemma  [ lemma : convexerror ] that @xmath33 is in fact in @xmath186 , and when @xmath4 is @xmath130-strongly convex , @xmath33 is in @xmath187 .",
    "we remark that minimizing  @xmath33 amounts to performing a gradient descent step : @xmath188 .",
    "let us now consider a composite optimization problem , meaning that @xmath4 splits into two parts @xmath189 , where @xmath190 is @xmath25-smooth .",
    "then , a natural surrogate of  @xmath4 is the following function : @xmath191 the function  @xmath33 majorizes  @xmath4 and the approximation error @xmath183 is the same as in section  [ subsubsec : gradient ] .",
    "thus , @xmath33 in in  @xmath192 or in @xmath185 when @xmath193 is convex .",
    "moreover ,    * when @xmath190 is convex , @xmath33 is in @xmath38 .",
    "if @xmath193 is also convex , @xmath33 is in @xmath186 ; * when @xmath190 is @xmath130-strongly convex , @xmath33 is in @xmath194 .",
    "if @xmath193 is also convex , @xmath33 is in @xmath187 .    minimizing  @xmath33 amounts to performing one step of the proximal gradient algorithm  @xcite .",
    "it is indeed easy to show that the minimum @xmath32 of @xmath33assuming it is unique  can be equivalently obtained as follows : @xmath195,\\ ] ] which is often written under the form @xmath196 $ ] , where `` @xmath197 '' is called the `` proximal operator ''  @xcite . in some cases ,",
    "the proximal operator can be computed efficiently in closed form , for example when @xmath193 is the  @xmath198-norm ; it yields the iterative soft - thresholding algorithm for sparse estimation  @xcite . for a review of proximal operators and their computations , we refer the reader to  @xcite .",
    "assume that @xmath189 , where @xmath193 is concave and  @xmath25-smooth .",
    "then , the following function @xmath33 is a majorizing surrogate in @xmath38 : @xmath199 such a surrogate appears in dc ( difference of convex ) programming  @xcite .",
    "when @xmath190 is convex , @xmath4 is indeed the difference of two convex functions .",
    "it is also used in sparse estimation for dealing with some non - convex penalties  @xcite .",
    "for example , consider a cost function of the form @xmath200| + \\varepsilon)$ ] , where @xmath201 $ ] is the @xmath202-th entry in  @xmath6 . even though the functions @xmath203|",
    "+ \\varepsilon)$ ] are not differentiable , they can be written as the composition of a concave smooth function @xmath204 on  @xmath205 , and a lipschitz function @xmath206|$ ] . by upper - bounding the logarithm function by its linear approximation , proposition  [ prop : conv1_separable ]",
    "justifies the following surrogate : @xmath207| + \\varepsilon ) + \\lambda\\sum_{j=1}^p \\frac{|\\theta[j]|-|\\kappa[j]|}{|\\kappa[j]|+\\varepsilon } , \\label{eq : upperbounddc}\\ ] ] and minimizing @xmath33 amounts to performing one step of a reweighted-@xmath198 algorithm ( see  @xcite and references therein ) . similarly , other penalty functions are adapted to this framework .",
    "for instance , the logarithm can be replaced by any smooth concave non - decreasing function , or group - sparsity penalties  @xcite can be used , such as @xmath208 , where @xmath209 is a partition of @xmath210 and @xmath211 records the entries of  @xmath6 corresponding to the set @xmath33 .",
    "proposition  [ prop : conv1_separable ] indeed applies to this setting .",
    "let us now consider a real - valued function @xmath4 defined on @xmath212 .",
    "let @xmath213 and @xmath214 be two convex sets . minimizing  @xmath4 over  @xmath215 is equivalent to minimizing the function  @xmath216 over  @xmath217 defined as @xmath218 .",
    "assume now that    * @xmath219 is @xmath130-strongly convex for all @xmath220 in  @xmath221 ; * @xmath222 is differentiable for all  @xmath223 ; * @xmath224 is @xmath225-lipschitz with respect to  @xmath220 and @xmath25-lipschitz with respect to  @xmath223 .",
    "denotes the gradient with respect to @xmath220 . ]",
    "let us fix @xmath226 in @xmath217 .",
    "then , the following function is a majorizing surrogate in @xmath227 : @xmath228 with @xmath229 .",
    "we can indeed apply lemma  [ lemma : danskin ] , which ensures that @xmath216 is differentiable with @xmath230 and @xmath231 for all @xmath220 .",
    "moreover , @xmath33 is @xmath225-smooth and @xmath216 is @xmath232-smooth according to lemma  [ lemma : danskin ] , and thus @xmath233 is @xmath234-smooth .",
    "note that a better constant @xmath235 can be obtained when  @xmath4 is convex , as noted in the appendix of @xcite .",
    "the surrogate  @xmath33 leads to an alternate minimization algorithm ; it is then interesting to note that proposition  [ prop : conv2 ] provides similar convergence rates as another recent analysis  @xcite , which makes slightly different assumptions on the function  @xmath4 .",
    "variational surrogates might also be useful for problems of a single variable  @xmath220 .",
    "for instance , consider a regression problem with a huber loss function  @xmath236 defined for all @xmath237 in  @xmath238 as @xmath239 where  @xmath108 is a positive constant .. ] the huber loss can be seen as a smoothed version of the  @xmath198-norm when  @xmath108 is small , or simply a robust variant of the squared loss @xmath240 that asymptotically grows linearly .",
    "then , it is easy to show that @xmath241 .",
    "\\label{eq : huber_var}\\ ] ] consider now a regression problem with @xmath242 training data points represented by vectors  @xmath243 in  @xmath3 , associated to real numbers  @xmath244 , for @xmath245 .",
    "the robust regression problem with the huber loss can be formulated as the minimization over  @xmath3 of @xmath246 } +   \\theta_2[i]\\right],\\ ] ] where @xmath220 is the parameter vector of a linear model .",
    "the conditions described at the beginning of this section can be shown to be satisfied with a lipschitz constant proportional to  @xmath247 ; the resulting algorithm is the iterative reweighted least - square method , which appears both in the literature about robust statistics  @xcite , and about sparse estimation where the huber loss is used to approximate the @xmath198-norm  @xcite .",
    "jensen s inequality also provides a natural mechanism to obtain surrogates for convex functions . following the presentation of lange , hunger and yang  @xcite",
    ", we consider a convex function @xmath248 , a vector @xmath249 in  @xmath3 , and define @xmath250 as @xmath251 for all @xmath6 .",
    "let @xmath252 be a weight vector in @xmath253 such that @xmath254 and @xmath255 \\neq 0 $ ] whenever @xmath256 \\!\\neq\\ ! 0 $ ] .",
    "then , we define for any @xmath30 in  @xmath3 : @xmath257 f \\left(\\frac{\\x[i]}{\\w[i ] } ( \\theta[i]-\\kappa[i ] ) + \\x^\\top \\kappa\\right),\\ ] ] when @xmath4 is @xmath25-smooth , and when @xmath255 \\defin |\\x[i]|^\\nu / \\|\\x\\|_\\nu^\\nu$ ] , @xmath33 is in @xmath258 with    * @xmath259 for @xmath260 ; * @xmath261 for @xmath262 ; * @xmath263 for @xmath264 .    to the best of our knowledge ,",
    "non - asymptotic convergence rates have not been studied before for such surrogates , and thus we believe that our analysis may provide new results in the present case .",
    "jensen surrogates are indeed quite uncommon ; they appear nevertheless in a few occasions .",
    "in addition to the few examples given in  @xcite , they are used for instance in machine learning by della pietra  @xcite for interpreting boosting procedures through the concept of _ auxiliary functions_.    jensen s inequality is also used in a different fashion in em algorithms  @xcite . consider @xmath5 non - negative functions  @xmath265 , and , for some  @xmath30 in  @xmath3 ,",
    "define some weights @xmath266=   f^t(\\kappa ) / \\sum_{t'=1}^t f^{t'}(\\kappa)$ ] . by exploiting the concavity of the logarithm , and assuming hat @xmath266 > 0 $ ] for all  @xmath8 to simplify , jensen s inequality yields @xmath267 \\log \\left(\\frac{f^t(\\theta)}{\\w[t]}\\right ) , \\label{eq : em}\\ ] ] the relation  ( [ eq : em ] ) is key to em algorithms minimizing a negative log - likelihood .",
    "the right side of this equation can be interpreted as a majorizing surrogate of the left side since it is easy to show that both terms are equal for  @xmath268 .",
    "unfortunately the resulting approximation error functions are not @xmath25-smooth in general and these surrogates do not follow the assumptions of definition  [ def : surrogate_batch ] . as a consequence , our analysis may apply to some em algorithms , but not to all of them .      when @xmath4 is twice differentiable and admits a matrix  @xmath269 such that @xmath270 is always positive definite , the following function is a first - order majorizing surrogate : @xmath271 the lipschitz constant of @xmath272 is the largest eigenvalue of @xmath273 over  @xmath2 .",
    "such surrogates appear frequently in the statistics and machine learning literature  @xcite .",
    "the goal is to to model the global curvature of the objective function during each iteration , without resorting to the newton method .",
    "even though quadratic surrogates do not necessarily lead to better theoretical convergence rates than simpler lipschitz gradient surrogates , they can be quite effective in practice  @xcite .",
    "in this section , we introduce an incremental scheme that exploits the structure  ( [ eq : prob ] ) of  @xmath4 as a large sum of @xmath5 components . the most popular method for dealing with such a problem",
    "when @xmath4 is smooth and @xmath274 is probably the _",
    "stochastic gradient descent _ algorithm ( sgd ) and its variants  ( see @xcite ) .",
    "it consists of drawing at iteration  @xmath9 an index  @xmath275 and updating the solution as @xmath276 , where the scalar  @xmath277 is a step size .",
    "another popular algorithm is the _",
    "stochastic mirror descent _",
    "algorithm  ( see @xcite ) for general non - smooth convex problems , a setting we do not consider in this paper since non - smooth functions do not always admit practical first - order surrogates .",
    "recently , linear convergence rates for strongly convex functions  @xmath7 have been obtained in @xcite and @xcite by using randomized incremental algorithms whose cost per iteration is independent of  @xmath5 .",
    "the method sag @xcite for smooth unconstrained convex optimization is a randomized variant of the incremental gradient descent algorithm of  blatt , hero and gauchman  @xcite , where an estimate of the gradient  @xmath28 is incrementally updated at each iteration .",
    "the method sdca  @xcite for strongly convex composite optimization is a dual coordinate ascent algorithm that performs incremental updates in the primal  ( [ eq : prob ] ) . unlike sgd , both sag and sdca require storing information about past iterates , which is a key for obtaining fast convergence rates .    in a different context",
    ", incremental em algorithms have been proposed by neal and hinton  @xcite , where upper bounds of a non - convex negative log - likelihood function are incrementally updated . by using similar ideas",
    ", we introduce the scheme miso in algorithm  [ alg : generic_incremental ] . at every iteration , a single function",
    "is observed , and an approximate surrogate of  @xmath4 is updated .",
    "note that in the same line of work , ahn et al .",
    "@xcite have proposed a block - coordinate descent majorization - minimization algorithm , which corresponds to miso when the variational surrogates of section  [ subsubsec : variational ] are used .",
    "@xmath13 ( initial estimate ) ; @xmath14 ( number of iterations ) .",
    "initialization : choose some surrogates @xmath278 of @xmath7 near @xmath279 for all @xmath8 ; randomly pick up one index @xmath280 and choose a surrogate @xmath281 of @xmath282 near @xmath15 ; set @xmath283 for all @xmath284 .",
    "update the solution : @xmath285 .",
    "@xmath17 ( final estimate ) ;    in the next two sections , we study the convergence properties of the scheme miso .",
    "we proceed as in section  [ sec : generic ] . specifically , we start with the non - convex case , focusing on stationary point conditions , and we show that similar guarantees as for the batch majorization - minimization algorithm hold .",
    "then , for convex problems , we present convergence rates that essentially apply to the proximal gradient surrogates",
    ". we obtain sublinear rates  @xmath286 for the general convex case , and linear ones for strongly convex objective functions .",
    "even though these rates do not show any theoretical advantage over the batch algorithm , we also present a more surprising result in section  [ subsec : strong ] ; in a large sample regime  @xmath287 , for @xmath130-strongly convex functions  @xmath7 , minorizing surrogates may be used and faster rates can be achieved .",
    "we start our analysis with the non - convex case , and make the following assumption :    * @xmath4 is bounded below and for all @xmath50 in @xmath2 and all @xmath8 , the directional derivative @xmath288 of @xmath7 at  @xmath6 in the direction @xmath52 exists .",
    "then , we obtain a first convergence result .",
    "[ prop : conv13 ] assume that  holds and that the surrogates @xmath289 from algorithm  [ alg : generic_incremental ] are majorizing  @xmath290 and are in @xmath291 .",
    "then , the conclusions of proposition  [ prop : conv1 ] hold with probability one .",
    "we proceed in several steps .",
    "_ almost sure convergence of @xmath64 : _",
    "let us define @xmath292 .",
    "we have the following relation for all @xmath86 , @xmath293 where the surrogates and the index @xmath294 are chosen in the algorithm .",
    "then , we obtain the following inequalities , which hold with probability one for all @xmath86 , @xmath295 the first inequality is true by definition of @xmath10 and the second one because @xmath296 is a majorizing surrogate of  @xmath297 .",
    "the sequence @xmath298 is thus monotonically decreasing , bounded below with probability one , and thus converges almost surely . by taking the expectation of these previous inequalities , we also obtain that the sequence @xmath299)_{n",
    "\\geq 0}$ ] monotonically converges .",
    "thus , the non - positive quantity @xmath300 $ ] is the summand of a converging sum and we have @xmath301 & = \\sum_{n=0}^{+\\infty } \\e[g_{n}^{{\\hat t}_{n+1}}(\\theta_{n } ) - f^{{\\hat t}_{n+1}}(\\theta_{n } ) ]   \\\\     & = \\sum_{n=0}^{+\\infty } \\e[\\e[g_{n}^{{\\hat t}_{n+1}}(\\theta_{n } ) - f^{{\\hat t}_{n+1}}(\\theta_{n})| \\ff_{n } ] ] \\\\     & = \\sum_{n=0}^{+\\infty } \\e[\\barg_{n}(\\theta_{n } ) - f(\\theta_{n } ) ] \\\\     & = \\e\\left[\\sum_{n=0}^{+\\infty } \\barg_{n}(\\theta_{n } ) - f(\\theta_{n})\\right ] < + \\infty , \\\\ \\end{split}\\ ] ] where we use beppo - lvy theorem to interchange the expectation and the sum in front of non - negative quantities , and  @xmath302 is the filtration representing all information up to iteration  @xmath9 ( including  @xmath10 ) . as a result",
    ", the sequence @xmath303 converges almost surely to @xmath60 , implying the almost sure convergence of @xmath64 .    _ asymptotic stationary point conditions : _",
    "let us define @xmath304 , which is @xmath25-smooth .",
    "then , for all @xmath6 in @xmath2 and @xmath86 , @xmath305 we have @xmath306 by definition of @xmath10 , and @xmath307 , following similar steps as in the proof of proposition  [ prop : conv1 ] .",
    "since we have previously shown that @xmath308 almost surely converges to zero , we conclude as in the proof of proposition  [ prop : conv1 ] , replacing @xmath24 by  @xmath309 and  @xmath11 by  @xmath310 .",
    "we also give the counterpart of proposition  [ prop : conv1_separable ] for algorithm  [ alg : generic_incremental ] .    [",
    "prop : conv13b ] assume that  is satisfied and that the functions @xmath7 are compositions @xmath311 , where the functions  @xmath312 are @xmath96-lipschitz continuous for some @xmath97 .",
    "assume also that the functions @xmath289 in algorithm  [ alg : generic_incremental ] are also compositions @xmath313 , where @xmath314 is majorizing @xmath315 and is in @xmath316 .",
    "then , the conclusions of proposition  [ prop : conv13 ] hold .",
    "we first remark that the first part of the proof of proposition  [ prop : conv13 ] does not exploit the fact that the approximation errors @xmath317 are @xmath25-smooth , but only the fact that @xmath318 is majorizing @xmath7 for all @xmath9 and @xmath8 .",
    "thus , the first part of the proof of proposition  [ prop : conv13 ] holds in the present case , such that @xmath64 almost surely converges , and the sequence @xmath319 almost surely converges to zero , where @xmath310 is defined in the proof of proposition  [ prop : conv13 ] .",
    "it remains to show that the asymptotic stationary point conditions are satisfied . to that effect",
    ", we follow the proof of proposition  [ prop : conv1_separable ] . we first have , for all @xmath86 , @xmath320 with @xmath306 and @xmath321 .",
    "then , following the proof of proposition  [ prop : conv1_separable ] , it is easy to show that @xmath322 where @xmath323 , and we conclude as in proposition  [ prop : conv1_separable ] .    the next lemma provides convergence rates for the convex case , under the assumption that the surrogate functions are @xmath40-strongly convex with @xmath324 .",
    "the result notably applies to the proximal gradient surrogates of section  [ subsubsec : proximal ] .",
    "[ prop : conv16 ] assume that @xmath4 is convex and bounded below , let @xmath126 be a minimizer of  @xmath4 on  @xmath2 , and let us define @xmath325 .",
    "when the surrogates  @xmath318 in algorithm  [ alg : generic_incremental ] are majorizing  @xmath7 and are in @xmath326 with @xmath164 , we have for all @xmath86 , @xmath327 \\leq \\frac{lt\\|\\theta^\\star-\\theta_0\\|_2 ^ 2}{2n } , \\label{eq : incr : rate}\\ ] ] where @xmath328 is the average of the iterates .",
    "assume now that @xmath4 is @xmath130-strongly convex .",
    "for all @xmath86 , @xmath329 \\leq   \\left(1- \\frac{2\\mu}{t(\\rho+\\mu)}\\right)^{n-1}\\frac{l\\|\\theta^\\star-\\theta_0\\|_2 ^ 2}{2}.\\label{eq : incr : ratemu}\\ ] ]    we proceed in several steps .",
    "_ preliminaries : _   for all  @xmath86",
    ", we introduce the point @xmath330 in @xmath2 such that @xmath318 is in @xmath331 .",
    "we remark that such points are drawn recursively according to the following conditional probability distribution : @xmath332 where @xmath333 , @xmath302 is the filtration representing all information up to iteration  @xmath9 ( including  @xmath10 ) , and @xmath334 for all @xmath8 .",
    "thus we have for all @xmath8 and all @xmath86 , @xmath335=    \\e[\\e[\\|\\theta^\\star-\\kappa_{n-1}^t \\|_2 ^ 2| \\ff_{n-1 } ] ] =   \\delta\\e[\\|\\theta^\\star-\\theta_{n-1}\\|_2 ^ 2 ] + ( 1-\\delta ) \\e[\\|\\theta^\\star-\\kappa_{n-2}^t\\|_2 ^ 2].\\label{eq : incr : tmp2}\\ ] ] we also need the following extension of lemma  [ lemma : basic ] to the incremental setting : for all  @xmath6 in  @xmath2 and @xmath86 , @xmath336 the proof of this relation is similar to that of lemma  [ lemma : basic ] , exploiting the @xmath40-strong convexity of @xmath337 .",
    "we can now study the first part of the proposition .",
    "_ non - strongly convex case ( @xmath338 ) : _   let us define the quantities @xmath339 $ ] and @xmath340 $ ] .",
    "then , we have from  ( [ eq : incr : tmp4 ] ) with @xmath170 , and by taking the expectation @xmath341 \\leq la_{n-1 } - l \\xi_n.\\ ] ] it follows from  ( [ eq : incr : tmp2 ] ) that @xmath342 and thus , for all @xmath86 , @xmath341 \\leq \\frac{l}{\\delta}(a_{n-1 } - a_n).\\ ] ] by summing the above inequalities , and using jensen s inequality , we obtain that @xmath343 \\leq   \\frac{1}{n}\\sum_{i=1}^n\\e[f(\\theta_i)-f^\\star ]   \\leq \\frac{l a_0}{\\delta},\\ ] ] leading to the convergence rate of eq .",
    "( [ eq : incr : rate ] ) , since @xmath344 .",
    "_ @xmath130-strongly convex case : _   assume now that the functions  @xmath7 are @xmath130-strongly convex . for all  @xmath86 , the strong convexity of  @xmath4 and  ( [ eq : incr : tmp4 ] ) give us the following inequalities @xmath345 \\leq l a_{n-1 } - \\rho\\xi_n,\\ ] ] combining this last inequality with  ( [ eq : incr : tmp2 ] ) , we obtain that for all @xmath86 , @xmath346 thus , @xmath347 with @xmath348 .",
    "since @xmath349 , @xmath350 \\leq l a_{n-1}$ ] , and @xmath351 , we finally have shown the desired convergence rate  ( [ eq : incr : ratemu ] ) .",
    "the convergence rate of the previous proposition in the convex case suggests that the incremental scheme and the batch one of section  [ sec : generic ] have the same overall complexity , assuming that each iteration of the batch algorithm is @xmath5 times the one of miso .",
    "for strongly convex functions  @xmath7 , we obtain linear convergence rates , a property shared by sag or sdca ; it is thus natural to make a more precise comparison with these other incremental approaches , which we present in the next two sections .      in this section",
    ", we assume that the optimization domain is unbounded  that is , @xmath274 , and that the functions  @xmath7 are @xmath25-smooth . when using the lipschitz gradient surrogates of section  [ subsubsec : gradient ] , miso amounts to iteratively using the following update rule : @xmath352 where the vectors @xmath353 are recursively defined for @xmath153 as @xmath354 and @xmath355 for @xmath356 , with @xmath357 for all @xmath8 .",
    "it is then easy to see that the complexity of updating @xmath10 is independent of  @xmath5 , by storing the vectors @xmath358 and performing the update @xmath359 . in comparison ,",
    "the approach sag yields a different , but related , update rule : @xmath360 where the value  @xmath361 is suggested in  @xcite .",
    "even though the rules  ( [ eq : miso1 ] ) and  ( [ eq : sag1 ] ) seem to be similar to each other at first sight , they behave differently in practice and do not have the same theoretical properties . for non - convex problems",
    ", miso is guaranteed to converge , whereas it is not known whether it is the case for sag or not . for convex problems ,",
    "both methods have a convergence rate of the same nature  that is , @xmath286 . for @xmath130-strongly - convex problems ,",
    "however , the convergence rate of sag reported in  @xcite is substantially better than ours . whereas the expected objective of sag decreases with the rate @xmath362 with @xmath363 , ours decreases with @xmath364 , which is larger than @xmath365 unless the problem is very well conditioned .    by maximizing the convex dual of  ( [ eq : prob ] ) when the functions @xmath7 are @xmath130-strongly convex",
    ", the approach sdca yields another update rule that resembles  ( [ eq : miso1 ] ) and  ( [ eq : sag1 ] ) , and offers similar convergence rates as  sag . as part of the procedure ,",
    "sdca involves large primal gradient steps  @xmath366 for updating the dual variables .",
    "it is thus appealing to study whether such large gradient steps can be used in  ( [ eq : miso1 ] ) in the strongly convex case , regardless of the majorization - minimization principle .",
    "in other words , we want to study the use of the following surrogates within miso : @xmath367 which are lower bounds of the functions @xmath7 instead of upper bounds .",
    "then , minimizing @xmath368 amounts to performing the update  ( [ eq : miso1 ] ) when replacing  @xmath25 by  @xmath130 .",
    "the resulting algorithm is slightly different than sdca , but resembles it .",
    "as shown in the next proposition , the method achieves a fast convergence rate when @xmath287 , but may diverge if  @xmath5 is small .",
    "note that at the same time as us , a similar result was independently obtained by defazio et al .",
    "@xcite , where a refined analysis provides a slightly better rate , namely the constant @xmath369 in  ( [ eq : incr : ratemu2 ] ) may be replaced by  @xmath370 .",
    "[ prop : conv17 ] assume that the functions @xmath7 are @xmath130-strongly convex , @xmath25-smooth , and bounded below .",
    "let @xmath126 be a minimizer of  @xmath4 on  @xmath2 .",
    "assume that @xmath287 .",
    "when the functions  @xmath318 of eq .",
    "( [ eq : lower_surrogates ] ) are used in algorithm  [ alg : generic_incremental ] , we have for all @xmath86 , @xmath329 \\leq \\left(1 - \\frac{1}{3t}\\right)^{n } \\frac{2t}{\\mu}\\|\\nabla f(\\theta_0)\\|_2 ^ 2.\\label{eq : incr : ratemu2}\\ ] ] when the functions @xmath7 are lower - bounded by the function  @xmath371 , we can use the initialization @xmath372 and @xmath373 for all @xmath8 . then , the quantity @xmath374 in  ( [ eq : incr : ratemu2 ] ) can be replaced by @xmath375 .    as in the proof of proposition",
    "[ prop : conv13 ] , we introduce the function @xmath376 , which is minimized by  @xmath10 for @xmath86 . since  @xmath310 is a lower bound on  @xmath4 , we have the relation  @xmath377 .",
    "inspired by the convergence proof of sdca  @xcite , which computes an convergence rate of an expected duality gap , we proceed by studying the convergence of the sequence @xmath378)_{n",
    "\\geq 1}$ ] .",
    "on the one hand , we have for all @xmath86 , @xmath379 where  @xmath380 .",
    "the first equality is true because @xmath310 is quadratic and is minimized by  @xmath10 , and the second one uses the relation  ( [ eq : recur ] ) . by definition of  @xmath381 , we have that @xmath382 , and by taking the expectation , @xmath383= \\e[f^{{\\hat t}_n}(\\theta_{n-1 } ) ] = \\e[\\e[f^{{\\hat t}_n}(\\theta_{n-1})| \\ff_{n-1 } ] ] = \\e[f(\\theta_{n-1})]$ ] , where @xmath302 is the the filtration representing all information up to iteration  @xmath9 .",
    "we also have that @xmath384 = \\e[\\e[g_{n-1}^{{\\hat t}_n}(\\theta_{n-1 } ) | \\ff_{n-1 } ] ] = \\e[\\barg_{n-1}(\\theta_{n-1})]$ ] .",
    "thus , we obtain a first useful relation : @xmath385 =   ( 1-\\delta)\\e[\\barg_{n-1}(\\theta_{n-1 } ) ] + \\delta \\e[f(\\theta_{n-1 } ) ] - \\frac{\\mu}{2}\\e\\left[\\|\\theta_n-\\theta_{n-1}\\|_2 ^ 2\\right].\\label{eq : tmpconv13a}\\ ] ] on the other hand , for all @xmath386 , @xmath387 we have used the fact that @xmath388 is a majorizing surrogate of @xmath297 , whereas @xmath389 is minorizing  @xmath290 . by adding twice  ( [ eq : tmpconv13c ] ) after taking the expectation and once  ( [ eq : tmpconv13a ] ) , we obtain that for all @xmath153 , @xmath390 & \\geq ( 3 - \\delta)\\e[\\barg_{n-1}(\\theta_{n-1 } ) ] + \\delta\\e [ f(\\theta_{n-1 } ) ] + \\left(\\frac{\\mu}{2}-\\delta l\\right)\\e[\\|\\theta_n-\\theta_{n-1}\\|_2 ^ 2 ]   \\\\           & \\geq   ( 3 - \\delta)\\e[\\barg_{n-1}(\\theta_{n-1 } ) ] + \\delta\\e [ f(\\theta_{n-1 } ) ] ,     \\end{split }        \\label{eq : tmpconv13b}\\ ] ] where the second inequality comes from the large sample size condition @xmath391 .",
    "since @xmath392 \\geq f^\\star$ ] , this immediately gives for @xmath386 , @xmath393 \\leq \\left (   1- \\frac{1}{3 t } \\right ) \\left(f^\\star - \\e\\left[\\barg_{n-1}(\\theta_{n-1})\\right]\\right).\\ ] ] to obtain a convergence rate for @xmath394-f^\\star$ ] , we use again eq .",
    "( [ eq : tmpconv13b ] ) . for @xmath153 , @xmath395 - f^\\star ) & \\leq \\delta(\\e [ f(\\theta_{n-1 } ) ] - \\e[\\barg_{n-1}(\\theta_{n-1 } ) ] ) \\\\           & \\leq 3 ( \\e[\\barg_n(\\theta_n)]- \\e[\\barg_{n-1}(\\theta_{n-1 } ) ] ) \\\\          & \\leq 3 ( f^\\star- \\e[\\barg_{n-1}(\\theta_{n-1 } ) ] ) \\\\          & \\leq 3 \\left (   1- \\frac{1}{3 t } \\right)^{n-2}\\left(f^\\star - \\barg_{1}(\\theta_{1})\\right ) ,     \\end{split } \\label{eq : tmpincr1}\\ ] ] and we obtain the convergence rate ( [ eq : incr : ratemu2 ] ) by first noticing that @xmath396 where we use the relation @xmath397 and  @xmath398 . then , we use the fact that @xmath399 since @xmath400 , such that @xmath401 .    to prove the last part of the proposition , we remark that all inequalities we have proved so far for @xmath386 , become true for @xmath150 .",
    "thus , the last inequality in  ( [ eq : tmpincr1 ] ) is also true when replacing @xmath402 by @xmath403 and @xmath404 by  @xmath405 .",
    "the proof technique is inspired in part by the one of sdca  @xcite ; the quantity @xmath406 is indeed a lower bound of @xmath67 , and plays a similar role as the dual value in sdca .",
    "we remark that the convergence rate  ( [ eq : incr : ratemu2 ] ) improves significantly upon the original one  ( [ eq : incr : ratemu ] ) , and is similar to the one of sag when @xmath5 is larger than @xmath407",
    ". however , proposition  [ prop : conv17 ] only applies to strongly convex problems .",
    "in other cases , the more conservative rule  ( [ eq : miso1 ] ) should be preferred in theory , even though we present heuristics in section  [ subsec : heuristics ] that suggest using larger step sizes than @xmath408 in practice .",
    "when @xmath4 can be written as @xmath409 , where the functions @xmath410 are @xmath25-smooth , we can use the proximal gradient surrogate presented in section  [ subsubsec : proximal ] ; it yields the following rule : @xmath411 where the vectors  @xmath330 are defined as in section  [ subsec : strong ] .",
    "this update is related to sdca , as well as to stochastic methods for composite convex optimization such as the regularized dual averaging algorithm of xiao  @xcite . as in the previous section ,",
    "we obtain guarantees for non - convex optimization , but our linear convergence rate for strongly convex problems is not as fast as the one of sdca . even though we do not have a similar result as proposition  [ prop : conv17 ] for the composite setting , we have observed that using a smaller value for  @xmath25 than the theoretical one could work well in practice .",
    "we detail such an empirical strategy in the next section .",
    "we have found the following strategies to improve the practical performance of miso .",
    "[ [ initialization ] ] initialization + + + + + + + + + + + + + +    a first question is how to initialize the surrogates  @xmath278 in practice . even though we have suggested the functions @xmath278 to be in  @xmath412 in algorithm  [ alg : generic_incremental ] , our analysis weakly relies on this assumption .",
    "in fact , most of our results hold when choosing surrogates computed at points  @xmath413 that are not necessarily equal to  @xmath279 ; at most only constants from the convergence rates would be affected by such a change .",
    "an effective empirical strategy is inspired by the second part of proposition  [ prop : conv17 ] : we first define functions  @xmath414 , and perform @xmath5 iterations of miso without randomization , selecting the function  @xmath7 at iteration  @xmath8 , such that each surrogate is updated exactly once .",
    "then , we use these updated surrogates for initializing the regular randomized scheme .    [ [ warm - restart - and - continuation ] ] warm restart and continuation + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    when available , warm restart can be used for initializing the surrogates .",
    "assume that we are interested in minimizing a composite function @xmath415 , which is parameterized by a scalar  @xmath416 , and that we want to obtain a minimizer for several parameter values @xmath417 .",
    "we first solve the problem for @xmath418 , and then use the surrogates obtained at the end of the optimization for initializing the algorithm when addressing the problem with @xmath419 .",
    "we proceed similarly going from larger to smaller values of @xmath416 .",
    "we have empirically observed that the warm restart strategy could be extremely efficient in practice , and would deserve further study in a future work .",
    "[ [ heuristics - for - selecting - step - sizes ] ] heuristics for selecting step sizes + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    choosing proximal gradient surrogates  @xmath420 requires choosing some lipschitz constant  @xmath25 ( or a strong convexity parameter @xmath130 for proposition  [ prop : conv17 ] ) , which leads to a specific step size in  ( [ eq : miso_composite ] ) . however , finding an appropriate step size can be difficult in practice for several reasons :",
    "( i ) in some cases , these parameters are unknown ; ( ii ) even though a global lipschitz constant might be available , a local lipschitz constant could be more effective ; ( iii ) the convergence rates of proposition  [ prop : conv16 ] can be obtained by choosing a smaller value for  @xmath25 than the `` true '' lipschitz constant , as long as the inequality @xmath394",
    "\\leq \\e[\\barg_n(\\theta_n)]$ ] is always satisfied , where @xmath421 .",
    "this motivates the following heuristics :    * first perform one pass over @xmath422 of the data to select a constant  @xmath423 with  @xmath424 chosen among positive integers , yielding the smallest objective on the data subset , where  @xmath425 is an upper bound of the true lipschitz constant . *",
    "proceed as in miso1 , but choose a more aggressive strategy  @xmath426 ; during the optimization , compute the quantities @xmath427 and @xmath428 defined as @xmath429 , @xmath430 if @xmath284 , and otherwise @xmath431 , @xmath432 , where we have parameterized the surrogates @xmath420 by @xmath433 .",
    "every @xmath5 iterations , compare the sums @xmath434 and @xmath435 . if @xmath436 , do nothing ; otherwise , increase the value of @xmath433 until this inequality is satisfied .",
    "the heuristic miso2 is more aggressive than miso1 since it starts with a smaller value for @xmath25 .",
    "after every iteration , this value is possibly increased such that on average , the surrogates `` behave '' as majorizing functions .",
    "even though this heuristic does not come with any theoretical guarantee , it was found to perform slightly better than miso1 for strongly - convex problems .    [ [ using - a - different - parameter - l_t - for - every - function - f_t ] ] using a different parameter @xmath437 for every function @xmath438 + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    even though our analysis was conducted with a global parameter  @xmath25 for simplicity , it is easy to extend the analysis when the parameter @xmath25 is adjusted individually for every surrogate .",
    "this is useful when the functions @xmath438 are heterogeneous .",
    "[ [ parallelization - with - mini - batches ] ] parallelization with mini - batches + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the complexity of miso is often dominated by the cost of updating the surrogates @xmath289 , which typically requires computing the gradient of a function .",
    "a simple extension is to update several surrogates at the same time , when parallel computing facilities are available .",
    "in this section , we evaluate miso on large - scale machine learning problems .",
    "our implementation is coded in c++ interfaced with matlab and is freely available in the open - source software package spams  @xcite .",
    "all experiments were conducted on a single core of a 2ghz intel cpu with @xmath439 gb of  ram .",
    "[ [ datasets ] ] datasets + + + + + + + +    we use six publicly available datasets , which consist of pairs  @xmath440 , where the @xmath441 s are labels in  @xmath442 and the @xmath443 s are vectors in  @xmath3 representing data points . the datasets are described in table  [ table : datasets ] .",
    ", , , and are obtained from the 2008 pascal large - scale learning challenge . and are obtained from the libsvm website .",
    "the datasets are pre - processed as follows : all dense datasets are standardized to have zero - mean and unit variance for every feature .",
    "the sparse datasets are normalized such that each @xmath443 has unit @xmath112-norm .",
    ".description of datasets used in our experiments . [ cols=\"<,^,^,^,^,^\",options=\"header \" , ]      we consider the @xmath112-regularized logistic regression problem , which can be formulated as follows : @xmath444 where  @xmath445 for all @xmath446 . following  @xcite , we report some results obtained with different methods with the parameter @xmath447 , which is argued to be of the same order of magnitude as the smallest value that would be used in practice for machine learning problems .",
    "we also performed experiments with the values  @xmath448 and  @xmath449 to study the impact of the strong convexity parameter ; the output of these two additional experiments is not reported in the present paper for space limitation reasons , but it will be discussed and taken into account in our conclusions . the algorithms included in the comparison are :    * the stochastic gradient descent algorithm with a heuristic for choosing the step - size similar to miso1 , and inspired by leon bottou s sgd toolbox for machine learning .",
    "a step - size of the form @xmath450 is automatically adjusted when performing one pass on @xmath451 of the training data .",
    "we obtain consistent results with the performance of sgd reported by schmidt et al .",
    "@xcite when the step - size is chosen from hindsight . based on their findings",
    ", we do not include in our figures other variants of sgd , _",
    "e.g. _ , @xcite . * the accelerated gradient method proposed by beck and teboulle  @xcite with a line - search for automatically adjusting the lipschitz constant . * the algorithm of shalev - schwartz and zhang  @xcite , efficiently implemented in the language c by mark schmidt . * a fast implementation in c also provided by mark schmidt  @xcite .",
    "we use the step - size @xmath408 since it performed similar to their heuristic line search . *",
    "the majorization - minimization algorithm miso , using the trivial upper bound @xmath452 on the lipschitz constant for example  @xmath8 . * the majorization - minimization heuristic miso1 described in section  [ subsec : heuristics ] . * the heuristic miso2 ,",
    "also described in section  [ subsec : heuristics ] .",
    "* the update rule corresponding to proposition  [ prop : conv17 ] .    for sparse datasets , miso0 , miso1 , and miso2 are not practical since they suffer from a @xmath453 memory cost .",
    "their update rules can indeed be rewritten @xmath454 where @xmath455 .",
    "thus , for every example  @xmath8 , the algorithm requires storing the dense vector @xmath456 .",
    "therefore , we use mini - batches of size @xmath457 , where @xmath458 is the density of the dataset ; the resulting algorithms , which we denote by  miso0-mb , miso1-mb , and miso2-mb , have a storage cost equal to @xmath459 , which is the same as the dataset .    on the other hand",
    ", the update rule miso@xmath130 applied to the @xmath416-strongly convex functions @xmath7 admits a simpler and computationally  cheaper  form .",
    "since  @xmath460 , the update becomes @xmath461 where @xmath462 denotes the derivative of  @xmath463 with respect to its second argument .",
    "assuming that the dataset fits into memory , the only extra quantities to store are the scalars  @xmath464 , and the resulting memory cost is simply @xmath465 .",
    "we present our comparison of the above methods with  @xmath447 on figures  [ fig : l2epochs ] and  [ fig : l2time ] , where we plot the relative duality gap defined as @xmath466 , where @xmath467 is the best value of the fenchel dual that we have obtained during our experiments .",
    "the conclusions of our empirical study are the following :    * _ sag , sdca and miso@xmath130 : _ these methods perform similarly and were consistently the fastest , except in the regime @xmath468 where miso@xmath130 can diverge ; * _ the four variants of miso : _ as predicted by its theoretical convergence rate , miso0 does not perform better than ista  @xcite without line - search ( not reported in the figures ) .",
    "miso1 and miso2 perform significantly better .",
    "miso@xmath130 is always better or as good as miso1 and miso2 , except for sparse datasets with @xmath448 where the condition  @xmath469 is not satisfied ; * _ influence of mini - batch : _ whereas miso2 performs equally well as sag / sdca for dense datasets , mini - batches for sparse datasets makes it slower ; * _ stochastic gradient descent : _ sgd - h performs always well at the beginning of the procedure , but is not competitive compared to incremental approaches after a few passes over the data .    note",
    "that an evaluation of a preliminary version of miso2 is presented in  @xcite for the @xmath198-regularized logistic regression problem , where the objective function is not strongly convex .",
    "our experimental findings showed that miso2 was competitive with state - of - the - art solvers based on active - set and coordinate descent algorithms  @xcite .",
    "+            +          the majorization - minimization principle is appealing for non - convex and non - smooth optimization , where only few algorithms apply . here , we address a sparse estimation problem presented in section  [ subsec : dc ] : @xmath470|+\\varepsilon ) ,     \\label{eq : expdc}\\ ] ] where the scalars @xmath441 and the vectors  @xmath443 are the same as in the previous section , and  @xmath471 is set to @xmath472 .",
    "the model parameter  @xmath416 controls the sparsity of the solution .",
    "even though  ( [ eq : expdc ] ) is non - convex and non - smooth , stationary points can be obtained in various ways . in this section",
    ", we consider majorization - minimization approaches where the penalty function @xmath473|+\\varepsilon)$ ] is upper - bounded as in eq .",
    "( [ eq : upperbounddc ] ) , whereas the functions @xmath474 are upper - bounded by the lipschitz gradient surrogates of section  [ subsubsec : gradient ] .",
    "we compare five approaches :    * algorithm  [ alg : generic_batch ] with the trivial lipschitz constant @xmath475 . * algorithm  [ alg : generic_batch ] with the line - search scheme of ista  @xcite for adjusting  @xmath25 .",
    "* we compare miso0 , miso1 , and miso2 , as in the previous section .",
    "we choose a parameter @xmath416 for each dataset , such that the solution with the lowest objective function obtained by any of the tested method has approximately a sparsity of  @xmath476 for datasets and , 100 for and , and @xmath477 for and .",
    "the methods are initialized with @xmath478 ; indeed , the initialization @xmath372 that was a natural choice in section  [ subsec : explog ] appears to be often a bad stationary point of problem  ( [ eq : expdc ] ) and thus an inappropriate initial point .",
    "we report the objective function values for different passes over the data in figure  [ fig : dcepochs ] , and the sparsity of the solution in figure  [ fig : dcspars ] .",
    "our conclusions are the following :    * methods with line searches do significantly better than those without , showing that adjusting the constant @xmath25 is important for these datasets ; * miso1 does asymptotically better than mm - ls for five of the datasets after @xmath479 epochs and slightly worse for ; in general , miso1 seems to converge substantially faster than other approaches , both in terms of objective function and in terms of the support of the solution . * the performance of miso2",
    "is mitigated . in one case",
    ", it does better than miso1 , but in some others , it converges to the stationary point  @xmath480 .       +          +",
    "in this paper , we have presented new algorithms based on the majorization - minimization principle for minimizing a large sum of functions .",
    "the main asset of our approach is probably its applicability to a large class of non - convex problems , including non - smooth ones , where we obtain convergence and asymptotic stationary point guarantees . for convex problems",
    ", we also propose new incremental rules for composite optimization , which are competitive with state - of - the - art solvers in the context of large - scale machine learning problems such as logistic  regression .",
    "we note that other majorization - minimization algorithms have recently been analyzed , such as block coordinate variants in  @xcite and stochastic ones in  @xcite .",
    "in particular , we have proposed in  @xcite a stochastic majorization - minimization algorithm that does not require to store information about past iterates , when the objective function is an expectation .",
    "since the first version of our work was published in  @xcite , miso has also been extended by other authors in  @xcite using the alternating direction method of multipliers framework .    for future work",
    ", we are currently investigating extensions of the scheme miso@xmath130 for strongly convex objective functions .",
    "we believe that the algorithm can be modified to remove the large sample condition  @xmath287 , that the convergence proof can be extended to the proximal setting , and that it is possible to use acceleration techniques in the sense of nesterov  @xcite .",
    "another interesting direction of research would be to study the stability of our result to inexact minimization of surrogate functions following for instance the analysis of  @xcite for proximal gradient methods .",
    "the author would like to thank zaid harchaoui , francis bach , simon lacoste - julien , mark schmidt , martin jaggi , the associate editor , and the anonymous reviewers for their useful comments .",
    "the following definitions can be found in classical textbooks , e.g ,  @xcite . for the sake of completeness ,",
    "we briefly introduce them here .",
    "[ def : derivative ] let us consider a function @xmath27 and @xmath481 be in  @xmath3 .",
    "when it exists , the following limit is called the directional derivative of @xmath4 at @xmath6 in the direction @xmath52 : @xmath482 when @xmath4 is differentiable at @xmath6 , directional derivatives exist in every direction , and @xmath483 .",
    "[ def : stationary ] let us consider a function @xmath484 , where @xmath2 is a convex set , such that @xmath4 admits a directional derivative @xmath51 for all @xmath50 in @xmath2 .",
    "we say that @xmath6 in  @xmath2 is a stationary point if for all @xmath32 in  @xmath2 , @xmath485 .",
    "[ def : strong_convexity ] let @xmath2 be a convex set .",
    "a function @xmath484 is called @xmath130-strongly convex when there exists a constant @xmath488 such that for all @xmath32 in @xmath2 , the function @xmath489 is convex .",
    "[ lemma : convexerror ] let @xmath490 be two functions . define @xmath35 . then ,",
    "if @xmath33 is @xmath40-strongly convex and @xmath4 is @xmath25-smooth , with @xmath491 , @xmath492 is @xmath179-strongly convex ; [ step : convexerror ] if @xmath33 and @xmath4 are convex and @xmath25-smooth , @xmath492 is also @xmath25-smooth ; if @xmath33 and @xmath4 are @xmath130-strongly convex and @xmath25-smooth , @xmath492 is @xmath493-smooth .",
    "* @xmath222 is differentiable for all  @xmath223 in @xmath495 ; * @xmath496 is @xmath25-lipschitz continuous for all @xmath220 in @xmath221 ; * @xmath219 is @xmath130-strongly convex for all @xmath220 in  @xmath221 .    also define @xmath497 .",
    "then , @xmath216 is differentiable and @xmath498 , where @xmath499 .",
    "moreover , if @xmath500 is @xmath225-lipschitz continuous for all @xmath220 in @xmath221 , the gradient  @xmath501 is @xmath502-lipschitz ."
  ],
  "abstract_text": [
    "<S> majorization - minimization algorithms consist of successively minimizing a sequence of upper bounds of the objective function . </S>",
    "<S> these upper bounds are tight at the current estimate , and each iteration monotonically drives the objective function downhill . </S>",
    "<S> such a simple principle is widely applicable and has been very popular in various scientific fields , especially in signal processing and statistics . </S>",
    "<S> we propose an incremental majorization - minimization scheme for minimizing a large sum of continuous functions , a problem of utmost importance in machine learning . </S>",
    "<S> we present convergence guarantees for non - convex and convex optimization when the upper bounds approximate the objective up to a smooth error ; we call such upper bounds `` first - order surrogate functions '' . </S>",
    "<S> more precisely , we study asymptotic stationary point guarantees for non - convex problems , and for convex ones , we provide convergence rates for the expected objective function value . </S>",
    "<S> we apply our scheme to composite optimization and obtain a new incremental proximal gradient algorithm with linear convergence rate for strongly convex functions . </S>",
    "<S> our experiments show that our method is competitive with the state of the art for solving machine learning problems such as logistic regression when the number of training samples is large enough , and we demonstrate its usefulness for sparse estimation with non - convex penalties .    </S>",
    "<S> non - convex optimization , convex optimization , majorization - minimization .    90c06 , 90c26 , 90c25 </S>"
  ]
}