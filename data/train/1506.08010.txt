{
  "article_text": [
    "computationally expensive computer codes are frequently needed to implement mathematical models which are assumed to be reliable approximations to physical processes .",
    "such simulators often require intensive use of computational resources that makes them inefficient if further exploitation of the code is needed , optimisation , uncertainty propagation and sensitivity analysis @xcite .",
    "for this reason , surrogate models are needed to perform fast approximations to the output of demanding simulators and enable efficient exploration and exploitation of the input space . in this context ,",
    "gaussian processes are a common choice to build statistical surrogates -also known as _ emulators_- which allow to take into account the uncertainty derived from the inability to evaluate the original model in the whole input space .",
    "gaussian processes have become popular in recent years due to their ability to fit complex mappings between outputs and inputs by means of a non - parametric hierarchical structure .",
    "such applications are found , amongst many other areas , in machine learning @xcite , spatial statistics @xcite ( with the name of kriging ) , likelihood - free bayesian inference @xcite and genetics @xcite .    to build an emulator ,",
    "a number of runs from the simulator is needed , but due to computing limitations only a small amount of evaluations can be performed . with a small amount of data ,",
    "it is possible that the uncertainty of the parameters of the model can not be described by a clearly uni - modal distribution .",
    "in such scenarios , model uncertainty analysis @xcite is capable of setting a proper framework in which we acknowledge all uncertainties related to the idealisations made through the modelling assumptions and the available , albeit limited information . to this end , _ hierarchical modelling _ should be taken into account .",
    "this corresponds to adding a layer of structural uncertainty to the assumed emulator either in a continuous or discrete manner ( see * ? ? ?",
    "* ) . in the case of gaussian processes ,",
    "continuous structural uncertainty can be accounted for as a natural by - product from a bayesian procedure .",
    "hence , this is pursued in this work by focusing on samplers capable of exploring multi - modal distributions .    in order for the gaussian process to be able to replicate the relation between inputs and outputs and make predictions",
    ", a training phase is necessary .",
    "such training involves the estimation of the parameters of the gaussian process from the data collected by running the simulator .",
    "these parameters are referred to as _ hyper - parameters_. the selection of the hyper - parameters is usually done by using maximum likelihood estimates ( mle ) , or their bayesian counterpart maximum a posteriori estimates ( map ) @xcite , or by sampling from the posterior distribution @xcite in a fully bayesian manner .    in this paper",
    "we assume a scenario where the task of generating new runs from the simulator is prohibitive .",
    "such limited information is not enough to completely identify either a candidate or a region of appropriate candidates for the hyper - parameters . in this scenario ,",
    "traditional optimisation routines @xcite are not able to guarantee global optima when looking for the mle or map , and a bayesian treatment is the only option to account for all the uncertainties in the modelling . in the literature , however , it is common to see that mle or map alternatives are preferred @xcite due to the numerical burden of maximising the likelihood function or because it is assumed that bayesian integration will not produce results worth the effort . though it is a strong argument in favour of estimating isolated candidates , in high - dimensional applications it is difficult to assess if the number of runs of the simulator is sufficient to produce robust hyper - parameters",
    "robustness is usually measured with a prediction - oriented metric such as root - mean - square error ( rmse ) @xcite , ignoring uncertainty and risk assessment of choosing a single candidate of the hyper - parameters by an inference process with limited data . in order to account for such uncertainty in the hyper - parameters when",
    "making predictions , numerical integration should be performed .",
    "however , methods as quadrature approximation become infeasible as the number of dimensions increases @xcite .",
    "therefore , an appropriate approach is to perform monte carlo integration @xcite .",
    "this allows to approximate any integral by means of a weighted sum , given a sample from the _ correct _ distribution .    in gaussian processes , as in many other applications of statistics ,",
    "the target distribution of the hyper - parameters can not be sampled directly and one should resort to markov chain monte carlo ( mcmc ) methods @xcite .",
    "mcmc methods are powerful statistical tools but have a number of drawbacks if not tuned properly , particularly if one wishes to sample from multi - modal distributions @xcite",
    ". one of such limitations is the tuning of the proposal distribution , which allows the generation of a candidate in the chain .",
    "this proposal function has to be tuned with parameters that define its ability to move through the sample space .",
    "if an excessively wide spread is selected , this will produce samples with space - filling properties but which are likely to be rejected . on the other hand , having a narrower spread will cause an inefficient exploration of the sample space by taking short updates of the states of the chain , known in the literature as _ random walk _ behaviour @xcite . in practice",
    "it is desirable to use a proposal distribution which is capable of balancing both extremes . to find an appropriate tuning in high - dimensional spaces with sets of highly correlated variables can be an overwhelming task and often mcmc samplers can become expensive due to the long time needed to reach stationarity @xcite . @xcite and @xcite favour the hybrid monte carlo ( hmc ) method to generate a sample from the posterior distribution , preventing the random walk behaviour of traditional mcmc methods .",
    "if tuned correctly , the hmc should be able to explore most of the input space @xcite .",
    "such tuning process is problem - dependent and there is no guarantee that the method will sample from all existing modes , thus failing to adapt well to multi - modal distributions @xcite .",
    "this paper proposes a sampler for the hyper - parameters of a gaussian process based on recently developed methods for bayesian inference problems .",
    "additionally , it uses the transitional markov chain monte carlo ( tmcmc ) method of @xcite to set a framework for the parallelisation of asymptotically independent markov sampling in both the context of hyper - parameter sampling ( aims ) @xcite and in stochastic optimisation ( aims - opt ) @xcite reminiscent of stochastic subset optimisation @xcite .",
    "such an extension is built using concepts of particle filtering methods @xcite , adaptive sequential monte carlo @xcite and delayed rejection samplers @xcite .",
    "aims is chosen since it provides a framework for sequential monte carlo sampling @xcite which automatically chooses the sequence of transitions .",
    "moreover , it uses most of the information generated in the previous step in the sequence as opposed to traditional sequential methods , thus building a robust sampler when applied to multi - modal distributions .    by selecting the hyper - parameters using the aims - opt framework",
    "the effect is twofold .",
    "first , the uncertainty inherent to the specification of the hyper - parameters is embedded in the set of suboptimal approximations to the solution .",
    "this uncertainty , expressed in a mixture of gaussian process emulators , yields a robust surrogate where model uncertainty is accounted for .",
    "second , computational implementation deficiencies of the inference procedure in gaussian processes is overcome by incorporating stabilising approaches exposed in the literature as in @xcite but in a bayesian framework .",
    "the problem is therefore treated from both a probabilistic and an optimisation perspective .",
    "the paper is organised as follows . in section [ sec : gps ] , a brief introduction to the gaussian processes and their treatment by bayesian inference is discussed .",
    "section [ sec : aims ] presents both the aims algorithm and the proper generalisation for a parallel implementation .",
    "section [ sec : implementation ] discusses several aspects of the computational implementation of the algorithm and their effect on the modelling assumptions .",
    "the efficiency and robustness of the proposed sampler are discussed in section [ sec : numerical ] with some illustrative examples . concluding remarks are given in section [ sec : conclusions ] .",
    "let @xmath0 be the set of trials run by the simulator where @xmath1 denotes a given configuration for the model .",
    "the set @xmath2 will be referred to as the set of _ design points_. let @xmath3 be the set of outputs observed for the design points .",
    "the pair @xmath4 will denote the _ training run _ being used to learn the emulator that approximates the simulator .",
    "the emulator is assumed to be a real - valued mapping @xmath5 which is an interpolator of the training runs , @xmath6 for all @xmath7 .",
    "this omits any random error in the output of the computer code in the observed simulations , that is , the simulator is deterministic .",
    "it is assumed that the output of the simulator can be represented by a gaussian process .",
    "therefore , the set of design points is assumed to have a joint gaussian distribution where the output satisfies the structure @xmath8 where @xmath9 is a vector of known basis ( location ) functions of the input , @xmath10 is a vector of regression coefficients , and @xmath11 is a gaussian process with zero mean and covariance function @xmath12 where @xmath13 is the signal noise and @xmath14 denotes the _ length - scale _ parameters of the correlation function @xmath15 . note that for a pair of design points@xmath16 , the function @xmath17 measures the correlation between @xmath18 and @xmath19 based on their respective input configurations .",
    "the effect of different values of @xmath20 in a one - dimensional example is depicted in figure [ fig : length_scale ] .    .",
    "for low values of the length - scale parameter the training runs are less dependent of each other . ]",
    "the role of the correlation function is to measure how close to each other the design points are , following the assumption that similar input configurations should produce similar outputs . for its analytical simplicity , interpretation and smoothness properties",
    ", this work uses the squared - exponential correlation function , namely @xmath21 note that other authors prefer the parametrisation with @xmath22 as denominators .",
    "however , this work uses a linear term in the denominator since the restriction of the length - scale parameters to lie in the positive orthant is more natural , as weights in the norm used to measure closeness and sensitivity to changes in such dimensions .    in summary ,",
    "the output of a design point , given the parameters @xmath23 and @xmath20 , has a gaussian distribution @xmath24 which can be rewritten as the joint distribution of the vector of outputs @xmath25 conditional on the design points @xmath2 and hyper - parameters @xmath23 and @xmath20 as @xmath26 where @xmath27 is the _ design matrix _ whose rows are the inputs @xmath28 and @xmath29 is the correlation matrix with elements @xmath30 for all @xmath31 .      the parameters of the process are not known beforehand and this induces uncertainty in the emulator itself .",
    "they can be estimated by maximum likelihood principles , but doing so lacks rigorous uncertainty quantification by concentrating all the density of the unknown quantities in a single value . the alternative is to treat them in a fully bayesian manner and marginalise them when performing predictions . this way their respective uncertainty is taken into account . in this scenario , the prediction @xmath32 for a non - observed configuration @xmath33 can be performed with the data available , @xmath34 , and the evidence they shed on the parameters of the gaussian process .",
    "therefore , the predictions should be made with the marginalised posterior distribution @xmath35 where @xmath36 denotes the complete vector of hyper - parameters .",
    "one should note that given the properties of a collection of gaussian random variables , a prediction for @xmath32 conditioned in the data and @xmath37 is also a gaussian random variable ( see * ? ? ?",
    "as in hierarchical modelling , each possible value of @xmath37 defines a specific realisation of a gaussian distribution , so it is appropriate to refer to @xmath37 as the hyper - parameters of the gaussian process .",
    "due to its computational complexity , the integral in is often omitted when making predictions .",
    "it is commonly assumed that the mle of the likelihood @xmath38 or the map estimate from the posterior distribution @xmath39 are robust enough to account for all the uncertainty in the modelling .",
    "however , when either the likelihood is a non - convex function or the posterior is a multi - modal distribution , conventional optimisation routines might only find local optima , thus failing to find the most probable candidate of such distribution .",
    "moreover , by selecting only one candidate , robustness and uncertainty quantification are lost in the process .",
    "additionally , there are degenerate cases when it is crucial to estimate the integral in by means of monte carlo simulation instead of by proposing a single candidate .",
    "as it has been noted by @xcite , two extreme cases for the gaussian process length - scale hyper - parameters can be identified .",
    "one possibility is for @xmath40 to approach infinity , which makes every design point dependent of each other ; the other , when @xmath40 approaches the origin where a multivariate regression model becomes the limiting case . in the first case ,",
    "high correlation among all the training runs results in a model which is not able to distinguish local dependencies . as for the second",
    ", it violates the assumptions that constitute a gaussian process , by completely ignoring the correlation structure in the design points to predict the output .",
    "consequently , if mcmc is performed one can approximate the integrated predictive distribution in by means of @xmath41 where @xmath42 is obtained through an appropriate sampler , one capable of sampling from multi - modal distributions .",
    "the coefficients @xmath43 denote the weights of each sample generated .",
    "since each term @xmath44 corresponds to a gaussian density function , the predictions are made by a mixture of gaussians .",
    "if the emulator output @xmath32 conditional on its configuration vector @xmath33 has a posterior density as in , then its mean function and covariance function can be computed as @xmath45 , \\label{eq : mix_cov}\\end{aligned}\\ ] ] where @xmath46 denotes the expected value of the likelihood distribution of @xmath32 conditional on the hyper - parameters @xmath47 , the training runs @xmath48 and the input configuration @xmath33 .",
    "equality in is a direct application of the tower property of conditional expectation and follows from the covariance decomposition formula using the vector of weights @xmath43 as an auxiliary probability distribution on the conditioning .    from equation we can compute the variance ,",
    "also known as the prediction error , of an untested configuration @xmath33 as @xmath49 by doing this , a more robust estimation of the prediction error is made since it balances the predicted error in one sample with how far the prediction of such sample is from the overall estimation of the mixture .      in order to perform a bayesian treatment for the prediction task in equation",
    "the prior distribution @xmath50 in equation has to be specified .",
    "weak prior distributions are commonly used for @xmath10 and @xmath13 @xcite .",
    "such weak prior has the form @xmath51 where it is assumed a priori that both the covariance and the mean hyper - parameters are independent .",
    "even more , @xmath10 and @xmath13 are assumed to have an improper non - informative distribution .    as for the length - scale",
    "hyper - parameter @xmath20 , a prior distribution @xmath52 is still needed . in this case",
    "the reference prior ( studied by * ? ? ?",
    "* ; * ? ? ?",
    "* ) sets an objective framework to account for the uncertainty of @xmath20 , thus avoiding any potential bias induced by the modelling assumptions .",
    "this prior is built based on shannon s expected information criteria and allows the use of a prior distribution in a setting where no previous knowledge is assumed . that way , the training runs are the only source of information for the inference process . additionally , the reference prior is capable of ruling out subspaces of the sample space of the hyper - parameters @xcite , thus reducing regions of possible candidates of gaussian distributions in the mixture model in equation . since this provides an off - the - shelf framework for the estimation of the hyper - parameters , the reference prior developed by @xcite is used in this work .",
    "however , there are no known analytical expressions for its derivatives which limits its application to mcmc samplers that use gradient information .",
    "note that there are other possibilities available for the prior distribution of @xmath20 .",
    "examples of these are the log - normal or log - laplacian distributions , which can be interpreted as a regularisation in the norm of the parameters . @xcite",
    "suggest a decaying prior .",
    "another option is to elicit prior distributions from expert knowledge as in @xcite .      the nature of the hyper - parameters @xmath23 and @xmath20 is potentially different in terms of scales and dynamics , as seen and explained in figure [ fig : dynamics ] .",
    "it is possible to cope with this limitations by using a gibbs sampling framework , but it is well - known that such sampling scheme can be inefficient if it is used for multi - modal distributions in higher dimensions .",
    "analogously , a metropolis - hastings sampler can also be overwhelmed .",
    "another alternative is to focus on @xmath20 and perform the inference in the correlation function .",
    "this is done by regarding @xmath10 and @xmath13 as nuisance parameters and integrating them out from the posterior distribution .",
    "the modelling assumptions in the training runs and the prior distribution , equations and respectively , allow to identify a gaussian - inverse - gamma distribution for @xmath10 and @xmath13 , which can be shown to yield the integrated posterior distribution @xmath53 and @xmath54 are estimators of the signal noise @xmath13 and regression coefficients @xmath10 ( see * ? ? ? * for further details ) .",
    "additionally , the predictive distribution conditioned on the hyper - parameters follows a gaussian distribution with mean and correlation functions @xmath55 where @xmath33 , @xmath56 denote a pair of test configurations and @xmath57 denotes the vector obtained by computing the covariance of the new proposal with every design point @xmath58 .",
    "note that both estimators depend only on the correlation function hyper - parameters @xmath20 since both @xmath10 and @xmath13 have been integrated out .",
    "considerations of when it is appropriate to integrate out the hyper - parameters in a model has been discussed by @xcite . in the gaussian process context",
    "it gains additional significance since it allows the development of appropriate mcmc samplers capable of overcoming the dynamics of different sets of hyper - parameters .    in the light of the above discussion",
    ", this work focuses on the inference drawn from the correlation function @xmath59 in equation , since the structure of dependencies of the training runs to predict the outputs is recovered by it .",
    "the main assumption is that the mean function hyper - parameter @xmath10 contains minor information on the structural dependencies of the data , relative to the correlation function hyper - parameters , which would prevent the use of integrated likelihoods ( see * ? ? ?",
    "* for further discussion ) .",
    "if prior information is available , then an additional effort can be made on eliciting an appropriate mean function for the gaussian process emulator .",
    "such information can be related to expert knowledge of the simulator which eventually allows the analyst to build a better mean function by adding significant regression covariates ( see * ? ? ? * for a detailed discussion ) .",
    "hyper - parameter marginalisation by means of monte carlo methods in gaussian processes is usually performed by hybrid monte carlo methods @xcite which are capable of suppressing the random walk behaviour of mcmc samplers if tuned correctly . in this work ,",
    "the sampling of the hyper - parameters is done by means of asymptotically independent markov sampling ( aims ) @xcite .",
    "this method combines techniques developed for bayesian inference such as importance sampling and simulated annealing @xcite to sample from the posterior distribution as done by other mcmc algorithms .",
    "additionally , aims can also be adapted for global optimisation ( aims - opt ) @xcite in a fashion of the traditional simulated annealing method for stochastic optimisation .",
    "let the problem be @xmath60 where @xmath61 denotes the negative log - posterior distribution conditional on the set of training runs @xmath48 .",
    "let the set of optimal solutions to the optimisation problem above be @xmath62 where @xmath63 .",
    "this formulation acknowledges the presence of multiple global optima in the posterior distribution conditional on the training runs .",
    "it is important to note that using the logarithm of the posterior distribution reduces the overflow in the computation of the equation , which is likely to arise due to ill - conditioning of the matrix @xmath29 @xcite .    in this context , aims - opt is capable of producing samples by means of a sequence of nested subsets @xmath64 that converges to the set of optimal solutions @xmath65 .",
    "thus , if the algorithm is terminated in a premature step , a set of sub - optimal approximations to will be recovered . let @xmath66 be the sequence of density distributions such that @xmath67 for a sequence of monotonically decreasing temperatures @xmath68 . by tempering the distributions in this manner ,",
    "the samples obtained in the first step of the algorithm are approximately distributed as a uniform random variable over a _ practical support _ ; while in the last annealing level , they are distributed uniformly on the set of optimal solutions , namely @xmath69 where @xmath70 denotes a uniform distribution over the set @xmath71 for every @xmath72 .",
    "the general framework for the aims - opt algorithm is presented , focusing on how to sample from the hyper - parameter space at level @xmath73 based on the sample of the previous level .",
    "let @xmath74 be samples of the hyper - parameters distributed as @xmath75 at level @xmath76 . for notational simplicity",
    ", the conditional on @xmath48 will be omitted from @xmath77 , however the training runs are crucial to build statistical surrogates .",
    "the objective is to use a kernel such that @xmath78 is the stationary distribution of the markov chain .",
    "let @xmath79 denote such markov transition kernel , which satisfies the continuous chapman - kolmogorov equation @xmath80 where @xmath81 denotes the probability measure . by applying importance sampling using the distribution at the previous annealing level",
    ", equation can be approximated as @xmath82 where @xmath83 is used as the _ global _ proposal distribution for a candidate in the chain and @xmath84 are the importance weights and the normalised importance weights respectively .",
    "note that for computing @xmath85 the normalising constant of the integrated posterior distribution is not needed .",
    "the proposals of candidates for the chain are done in two steps . in the first step ,",
    "a candidate is drawn as an update from a random _ marker _ from the sample of the previous annealing level , checking whether it is accepted or not .",
    "if the local candidate is rejected by a random walk metropolis - hastings evaluation , then the chain remains invariant , @xmath86 , and another marker is selected at random . in the second step ,",
    "given the candidate has been accepted as a local proposal , such candidate is considered as being drawn from the approximation in and accepted in an independent metropolis - hastings framework , hence called a global candidate for the chain .",
    "let @xmath87 denote the symmetric transition distribution used for local proposals for the markov chain .",
    "the subscript @xmath73 accounts for the adaptive nature of the transition steps in each annealing level .",
    "thus , the kernel distribution of the random walk , which leaves the intermediate density invariant , can be written as @xmath88 where @xmath89 denotes a delta density and @xmath90 is the probability of accepting the transition from @xmath91 to @xmath92 .",
    "it follows from that the approximated stationary condition of the target distribution at annealing level @xmath73 can be written as @xmath93 with @xmath94 the probability of accepting the local transition ; whereas @xmath95 denotes the probability of accepting such candidate for the markov chain , hence accepting a global transition ( see * ? ? ?",
    "* for a detailed discussion ) .",
    "this leads to the following two algorithms for each level in the annealing sequence .",
    "* @xmath96 , generated at previous level ; * @xmath97 , initial state of the chain ; * @xmath98 , symmetric local proposal ;    according to algorithm [ alg : annealing_level ] the initialising step should also be provided for the annealing level . in practical implementations",
    "it is suggested that it should be considered to be @xmath99 where @xmath100 , the sample with the largest normalised importance weight .      even though a random walk is performed in every local proposal ,",
    "aims - opt performs efficient sweeping of the sample space by producing candidates from neighbourhoods of the markers from the previous annealing level @xmath101 .",
    "this is accomplished if the transition distribution @xmath102 uses an appropriate proposal distribution where sampling is to be realised ; namely , the level curves of the tempered distribution . to be able to cope with the non - negative restriction and to neglect the effect of the scales on each dimension ,",
    "the transitions are performed in the log - space of the length - scale parameters @xmath20 , as suggested by @xcite .",
    "the symmetric transition distribution proposed is a gaussian distribution for such log - parameters .",
    "that is , each local candidate will be distributed as    @xmath103    where @xmath104 is a decaying parameter for the spread of the proposal , @xmath105 with @xmath106 commonly chosen as @xmath107 @xcite .",
    "the matrix @xmath108 denotes the covariance matrix for log - parameters where typical choices can be the identity matrix @xmath109 , a diagonal matrix or a symmetric positive definite matrix .",
    "we propose the use of the weighted covariance matrix estimated from the sample and their importance weights of the previous level @xmath110 . by doing so",
    ", the scale and directions of the ellipsoids of the gaussian steps are learned as in adaptive sequential monte carlo methods @xcite from the information gathered from the previous level in the sequence .",
    "the annealing sequence and its effective exploration of the sample space is dictated by the temperature @xmath68 of the intermediate distributions .",
    "moreover , it defines how different is one target distribution from the next one , so the effectiveness of the sample as markers from the previous annealing level depends strongly on how the scheduling is performed .",
    "it is clear that abrupt changes lead to rapid deterioration of the sample , whilst low paced changes could produce unnecessary steps in the annealing schedule . in order to cope with this compromise",
    ", @xcite used the _ effective sampling size _ to determine the value of the next temperature in the process . that is solving for @xmath111 , when a sample from level @xmath76 has been produced , in @xmath112 where @xmath113 defines a threshold for the proportion of the sample to be as effective from the importance sampling . note that the value of @xmath113 defines",
    "additionally how many annealing steps will be performed . as suggested from @xcite a value of 1/2",
    "is used for such parameter .",
    "if the temperature continues to drop along the sequence of intermediate distributions , eventually an _ absolute zero _ @xmath114 would be reached .",
    "however , such limit can not be achieved in practical implementations and a stopping condition is needed for the algorithm . by the same assumptions as in the original paper @xcite and without loss of generality , the objective function @xmath115 is assumed to be non - negative .",
    "similarly , let @xmath116 denote the coefficient of variation ( cov ) of the sample @xmath117 , @xmath118 therefore , @xmath116 is used as a measure of the sensitivity of the objective function to the hyper - parameters in the domain @xmath119 .",
    "if the samples are all located in @xmath65 then their cov will be zero , since @xmath120 @xmath121 . as the progression of the intermediate distributions advances with @xmath73",
    ", it is expected that @xmath122 . as a consequence , a criteria to stop the annealing sequence is needed , and the algorithm will stop when the following condition is attained @xmath123 where @xmath124 is assumed to be 0.10 in practical implementations to prevent the algorithm to generate redundant annealing levels in the last steps of the procedure .      as found in our earliest experiments ,",
    "aims - opt with the global acceptance rule as in algorithm [ alg : global_accep ] might degenerate quickly in higher dimensions since the starting of the chain comes from the highest normalised weighted sample and a transition might take too long to be performed , resulting in high rejection rates .",
    "furthermore , information from the markers is lost since they do not provide good transition neighbourhoods and the ability to create new samples for the next annealing level is maimed .",
    "this aside , aims - opt can become computationally expensive when the number of samples increases . to cope with these limitations we propose to incorporate the transitional markov chain monte carlo ( tmcmc ) and the delayed rejection methods into the aims - opt framework .",
    "this extension not only enhances the mixing properties of the sampler , improve acceptance rates , but also provides a computational framework in which parallel markov chains can be sampled from the intermediate distributions @xmath125 of the length - scale hyper - parameters .    the idea to enable parallelisation comes from the tmcmc algorithm ( see * ? ? ?",
    "* for further details ) . in the framework of algorithm [ alg : annealing_level ] ,",
    "every marker from the annealing level @xmath76 is a starting point for a markov chain .",
    "this produces not only specialised chains which are likely to explore the marker s neighbourhood on the sample space , but also allows an assessment of which markers will generate a better chain .",
    "the normalised weights @xmath126 will dictate how deeply a chain will evolve starting from its marker @xmath127 .",
    "consequently , the number of samples in each chain will be set with probability proportional to the normalised weight , a direct result from the tmcmc algorithm .    in order to guard against high rejection rates , and therefore degeneracy on the sampling scheme ,",
    "we propose to generate an additional candidate if the first one is rejected as in delayed rejection algorithms @xcite .",
    "let @xmath128 , @xmath129 be a one step and two steps proposal density distributions respectively ; @xmath130 the target distribution of the markov chain and @xmath131 the probability of accepting a transition in one step .",
    "then , the probability of accepting a transition in two steps , denoted by @xmath132 , is @xmath133 where @xmath134 denotes the starting point , @xmath135 the rejected candidate and @xmath136 the second stage candidate . in our context",
    ", the target distribution @xmath130 is each annealing level @xmath78 density distribution , the one step proposal distribution @xmath137 is the independent approximation in equation and the one - step acceptance probability is the global acceptance probability in .",
    "the two - step proposal density @xmath138 can be chosen from several alternatives . in this work",
    "we use a symmetric distribution centred at the starting point @xmath134 , since it can be seen as a back - guard against @xmath137 being a deficient independent sampler ( see * ? ? ?",
    "* for a detailed discussion ) .",
    "therefore , the previous equation can be rewritten in compact form as @xmath139 where @xmath140 is defined as in equation .",
    "the fact that @xmath138 is a symmetric distribution centred in the starting point @xmath134 has been used , @xmath141 , where @xmath142 denotes such symmetric proposal density . by performing the second stage proposal , the stationary condition of @xmath78",
    "is maintained as stated in the following proposition .",
    "aims - opt coupled with delayed rejection in two stages leaves the target distribution @xmath78 invariant at each annealing level .",
    "see [ appendix ] for a proof using a general transition distribution @xmath129 .    from the above discussion",
    ", the proposed scheme provides a fail - safe against any possible mismatch of the approximation done with . additionally , the results presented in this paper correspond to the second step candidate being a gaussian random variable , @xmath143 .",
    "the ideas to accept a global transition after having accepted a local proposition can be summarised in algorithm [ alg : global_accep_delayed ] .",
    "the computational complexity of the posterior distribution in equation is governed by the inverse of the covariance matrix @xmath29 as it scales with the number of training runs @xmath144 .",
    "several solutions have been developed in the literature , such as computation of inverse products of the form @xmath145 , with @xmath146 , by means of cholesky factors or spectral decomposition ( see * ? ? ? * for efficient implementations ) to preserve numerical stability in the matrix operations ( see * ? ? ?",
    "* ) . nonetheless , numerical stability is not likely to be achieved if the training runs are very limited , or if the sampling scheme for such training runs can not lead to stable covariance matrices , as depicted in figure [ fig : numstab ] .    to overcome this practical deficiency , a correction term in the covariance matrix can be added in order to preserve diagonal dominancy , that is , we add a _ nugget _ hyper - parameter @xmath147 to the covariance such that @xmath148 is positive definite . doing so results in the stochastic simulator @xmath149    note that the interpolating quality of the gaussian process is lost , however , the term @xmath150 accounts for the variability of the simulator that can not be explained by the emulator given the original assumptions ( adequacy of the covariance function , for example ) .",
    "the nugget can also provide further quantification of model uncertainty in the inference process as it provides an alternative to smoothing an already complex surface .",
    "as it is also noticed by @xcite and @xcite , the quality of the emulator changes with the inclusion of the nugget , since it modifies the objective function itself by introducing new modes in the landscape of the posterior distribution . is assessed as not appropriate for the model , a regularisation term can be added in the optimisation formulation @xcite . .",
    "however , by using a multi - modal sampler for stochastic optimisation as the one proposed , a robust emulator capable of mixing various possibilities can be provided .",
    "this results in an emulator that is able to cope with violations to the modelling assumptions originated by working with a limited amount of training runs .",
    "we incorporate the nugget term @xmath147 as a hyper - parameter of the correlation function in the bayesian inference process . as suggested by @xcite a uniform prior distribution @xmath151 for such parameter",
    "is considered .",
    "the effect of the bounds is twofold .",
    "first , the lower bound is used to guarantee stability in the covariance matrix .",
    "second , the upper bound is used to force the numerical noise of the simulator to be smaller than the signal noise of the emulator itself . note that this last assumption can be omitted if the problem requires it . by considering the correlation matrix as in equation ,",
    "this yields @xmath152 where @xmath153 denotes the corrected correlation matrix and @xmath154 has been used to denote the covariance matrix of the gaussian process . by doing",
    "so it is clear that previous considerations regarding @xmath13 , such as the ability of marginalising it as a nuisance parameter and the use of a non - informative prior remain unchanged @xcite .",
    "to illustrate the robustness of estimating the hyper - parameters of a gaussian process using the parallel aims - opt framework , three test cases have been selected .",
    "the first two are common examples that can be found in the literature .",
    "the first is known as the branin function and has been modified to resemble usual properties of engineering applications @xcite .",
    "the second one @xcite has been used as a two dimensional function with a challenging complexity for emulating purposes .",
    "the third example presented in this section comes from a real dataset also presented in @xcite .",
    "in all the examples it is assumed that @xmath155 . regarding the nugget ,",
    "a sigmoid transformation has been performed in order to sample from a gaussian distribution .",
    "namely , we sample an auxiliary @xmath156 as part of the multivariate gaussian in , and compute the nugget as @xmath157 where @xmath158 is the lower bound for the nugget , which is set equal to @xmath159 . additionally , the uniform meta - prior distribution of equation has been considered in a practical support of the length - scale parameters in the logarithmic space , namely a uniform distribution with support in @xmath160 $ ] . for the nugget , a truncated beta distribution with parameters",
    "@xmath161 has been considered since it corresponds to a non informative meta - prior in the interval @xmath162 $ ] . here",
    "the prefix _ meta _ has been used to refer to the algorithm s prior distribution and to set a clear distinction from the prior used in the modelling assumptions in equation .",
    "the code has been implemented in matlab and all examples have been run in a gnu / linux machine with an intel i5 processor with 8 gb of ram . for the purpose of reproducibility ,",
    "the code used to generate the examples is available for download at https://github.com/agarbuno/paims_codes .",
    "the version of the branin function used in this paper is a modification made by @xcite for the purpose of kriging prediction in engineering applications .",
    "it is a rescaled version of the original in order to bound the inputs to the rectangle @xmath163\\times [ 0,1]$ ] , with an additional term that modifies its landscape to include a global optimum .",
    "namely , @xmath164 + 5 \\overline{x}_1 , \\end{aligned}\\ ] ] where @xmath165 and @xmath166 .    for this case ,",
    "a sample of 18 design points were chosen with a latin hypercube sampling scheme .",
    "the resulting log - posterior function possesses 4 different modes in its landscape ( see figure [ subfig : branin ] ) leading to 4 possible configurations of the correlation function .",
    "thus , the impact of the training runs used to construct the emulator is evident . among these modes",
    ", 4 different types of emulators can be distinguished : an emulator with high sensitivity to changes in input @xmath167 ( mode a in figure [ subfig : branin ] ) ; an emulator with rapid changes in @xmath168 for the correlation structure of the training runs ( mode b ) ; a limiting case where dimension @xmath168 is disregarded in the correlation function , due to a high value in @xmath136 ( mode c ) ; or a second limiting emulator which approximates a bayesian linear regression model ( mode d ) ( see * ? ? ?",
    "* for a detailed discussion ) .",
    "for this example , two thousand samples were generated in each annealing level .",
    "the parallel aims - opt algorithm generated 7 annealing levels to produce the samples in figure [ subfig : branin_samples ] .",
    "the rmse of the map model is 7.068 whereas the rmse of the mixture is 15.099 which is an indication that in terms of brute prediction , the mixture model could be improved by taking more samples .",
    "figure [ subfig : branin_res ] depicts the standardised residuals from both the map approach ( top ) and the mixture model ( bottom ) using equations and with uniform weights in the sample .",
    "the standardised residuals are defined as @xmath169 where @xmath170 is the output for configuration @xmath171 , @xmath172 $ ] and @xmath173 , the posterior mean and variance for configuration @xmath171 ( see * ? ? ?",
    "* ) . by marginalising the hyper - parameters it is clear that our estimation is a more robust in terms of error prediction",
    "even with such limited amount of information the residuals suggest that the uncertainty is being incorporated appropriately in the marginalised predictive posterior distribution in equation .",
    "the standardised residuals are inside the , though not too close to 0 . this .",
    "this function has already been used as an example for emulation purposes and can be found in gem - sa software web page ( http://ctcd.group.shef.ac.uk/gem.html ) . even though it is a two dimensional problem it also serves as a good illustration of the importance of estimating the hyper - parameters of a gaussian process with a multi - modal sampler . the mathematical expression for this simulator",
    "is @xmath174 \\ , \\left(\\frac { 2300 x_1 ^ 3 + 1900 x_1 ^ 2 + 2092 x_1 + 60}{100 x_1 ^ 2 + 500 x_1 ^ 2 + 4 x_1 + 20 } \\right ) .",
    "\\end{aligned}\\ ] ]    as in the previous case , the training runs and the modelling assumptions fail to summarise the uncertainty in a uni - modal posterior distribution .",
    "the design points where selected using a latin hypercube in the rectangle @xmath163\\times[0,1]$ ] .",
    "it can be seen from figure [ subfig : bastos_1 ] that the modes are separated by a wide valley of low posterior probability , which can become an overwhelming task for traditional mcmc samplers .",
    "the proposed sampler is able to cope with all local and global spread dynamics present in the neighbourhoods of the modes it encounters , as shown in figure [ subfig : bastos_2 ] .",
    "+    depicted in figures [ subfig : bastos_1 ] and [ subfig : bastos_3 ] the use of the reference prior in the posterior distribution removes probability mass from the neighbourhood around the origin .",
    "this validates the use of the reference prior to cut out regions from the space for the sampling and exploit the most information contained in the data available , namely , the training runs @xmath48 . as in the previous example , two thousand samples were generated in each annealing level .",
    "the parallel aims - opt algorithm generated 7 annealing levels to produce the samples in figure [ subfig : bastos_2 ] . in terms of prediction accuracy",
    ", we now obtain that the rmse is 1.356 for the map estimate and 1.345 for the mixture model . while as for the residuals , we can see from figure [ subfig : bastos_res ] that the mixture model , resulting in a more robust estimation of the error .",
    "this simulator is built from the nilson - kuusk model for the reflectance for homogeneous plant canopy .",
    "such model is a five dimensional simulator whose inputs are the solar zenith angle , the leaf area index , relative leaf size , the markov clumping parameter and a model parameter @xmath175 ( see * ? ? ?",
    "* for further details on the model itself and the meaning of the inputs and outputs ) . for the analysis presented in this paper a single output emulator is assumed and the set of the inputs have been rescaled to fit the hyper - rectangle @xmath163 ^ 5 $ ] on a five dimensional space as in @xcite .    as in the previous test cases ,",
    "the design points were chosen by latin hypercube designs ( 100 for this case ) . in this example",
    ", the dimension of the problem makes it impossible to plot the level curves of the posterior distribution for the length scale hyper - parameters .",
    "however , strongly suggest that the samples come from a multi - modal posterior distribution .",
    "additionally , it can be noted that .",
    "furthermore , a limit - case emulator can be suggested by the plot in by considering a surrogate with no fourth inputs in the model .",
    "notice the scale for such hyper - parameter .",
    "due to the larger number of dimensions , five thousand samples were generated for each annealing level . in this case we have that the rmse of the map estimate is 0.022 while the rmse of the mixture proposal is 0.021 which is a consequence of the being highly concentrated around one mode . in figure",
    "[ fig : kuusk_res ] there is evidence that even with such behaviour the predictive error is improved by narrowing the spread of the standardised residuals , . in this case",
    "the residuals can not all be contained in the bands but as noted by @xcite in their experiments there is strong evidence that more runs of the simulator are needed to adequately built a statistical surrogate .",
    "this paper proposes to estimate the hyper - parameters of a gaussian process using a new sampler based on the asymptotically independent markov sampling ( aims ) method .",
    "the aims - opt algorithm , used in stochastic optimisation , provides a robust computation of the map estimates of the hyper - parameters .",
    "this is done by providing a set of approximations to the optimal solution instead of a single approximation as it is so frequently done in the literature .",
    "the problem is approached in a combined effort from the computational , optimisation and probabilistic perspectives which serve as solid foundations for building surrogate models for computationally expensive computer codes .",
    "the original aims algorithm has been extended to provide an efficient sampler in computational terms , by means of parallelisation , as well as an effective sampler with good mixing qualities , by means of both the delayed rejection and adaptive modification exposed .",
    "it has been demonstrated that by using the parallel aims - opt algorithm it is possible to acknowledge uncertainty in the structure of the emulator proposed as illustrated in the examples provided .",
    "structural uncertainty should be taken into account to determine when the training runs available are sufficient to narrow the posterior distribution of the hyper - parameters to a uni - modal convex distribution .",
    "even though it has been proven to be effective in lower and medium dimensional design spaces , research in high dimensional spaces has been left for future research .",
    "the first author gratefully acknowledges the consejo nacional de ciencia y tecnologa ( conacyt ) for the award of a scholarship from the mexican government .    [ [ appendix ] ]    in this appendix , a proof that using the delayed rejection algorithm in the aims framework leaves the target distribution @xmath78 invariant is provided .    a sufficient condition to prove that indeed @xmath78 is the stationary distribution for the markov chain is to prove that the detailed balance condition is satisfied .",
    "since the first stage approval has been proven to satisfy the detailed balance condition in @xcite , it will only be proved for the second stage sampling .",
    "let @xmath176 describe the aims - opt delayed transitions in the @xmath73-th annealing level from @xmath177 , with @xmath178 .",
    "let @xmath135 be the rejected transition in the first stage , for any @xmath179 .",
    "it will be proved that for such candidates the following holds : @xmath180 as seen from the description in section [ sec : parallel_delayed ] it follows that @xmath181 where it is used the fact that aims - opt generates first stage proposals with an independent approximate distribution .",
    "recall that the probability of a second stage proposal is @xmath182 and the fact that for any two positive numbers @xmath183 the equality @xmath184 is satisfied . with these two equalities we can substitute the left hand side of equation as @xmath185 \\ ,",
    "a_2(\\boldsymbol\\phi_0 , \\boldsymbol\\phi_2 ) \\nonumber \\\\ & = \\hat{p}_{k , n}(\\boldsymbol\\phi_1)\\ , \\left [ p_k(\\boldsymbol\\phi_2)\\ , s_2 ( \\boldsymbol\\phi_0 | \\boldsymbol\\phi_2 , \\boldsymbol\\phi_1 ) \\ , ( 1-a_1(\\boldsymbol\\phi_2 , \\boldsymbol\\phi_1 ) ) \\right ] \\ , a_2(\\boldsymbol\\phi_2 , \\boldsymbol\\phi_0)\\nonumber \\\\ & = p_k(\\boldsymbol\\phi_2 ) \\ , f_2(\\boldsymbol\\phi_0 | \\boldsymbol\\phi_2 ) , \\end{aligned}\\ ] ] which proves the detailed balance for the second stage proposal .",
    "note that the proof has been made with no further assumptions about the second stage proposal distribution @xmath186 , as it can be defined from several candidates . in this work , a symmetric proposal that ignores",
    "the rejected sample has been used since it can be interpreted as a random walk safeguard against a possible ill approximation done by the independent sampler .",
    "52 [ 1]#1 [ 1]`#1 ` urlstyle [ 1]doi : # 1                                                                                                a.  a. taflanidis and j.  l. beck .",
    "an efficient framework for optimal robust stochastic system design using stochastic simulation . _",
    "computer methods in applied mechanics and engineering _ , 1980 ( 1):0 88101 , 2008 ."
  ],
  "abstract_text": [
    "<S> gaussian process emulators of computationally expensive computer codes provide fast statistical approximations to model physical processes . </S>",
    "<S> the training of these surrogates depends on the set of design points chosen to run the simulator . due to computational cost </S>",
    "<S> , such training set is bound to be limited and quantifying the resulting uncertainty in the hyper - parameters of the emulator by uni - modal distributions is likely to induce bias . in order to quantify this uncertainty , this paper proposes a computationally efficient sampler based on an extension of asymptotically independent markov sampling , a recently developed algorithm for bayesian inference . </S>",
    "<S> structural uncertainty of the emulator is obtained as a by - product of the bayesian treatment of the hyper- parameters . </S>",
    "<S> additionally , the user can choose to perform stochastic optimisation to sample from a neighbourhood of the maximum a posteriori estimate , even in the presence of multimodality . </S>",
    "<S> model uncertainty is also acknowledged through numerical stabilisation measures by including a nugget term in the formulation of the probability model . </S>",
    "<S> the efficiency of the proposed sampler is illustrated in examples where multi - modal distributions are encountered . </S>",
    "<S> for the purpose of reproducibility , further development , and use in other applications the code used to generate the examples is freely available for download at https://github.com/agarbuno/paims_codes .    </S>",
    "<S> gaussian process , hyper - parameter , marginalisation , optimisation , mcmc , simulated annealing </S>"
  ]
}