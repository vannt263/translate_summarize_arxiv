{
  "article_text": [
    "let @xmath1 be independent copies of a random couple @xmath2 with values in @xmath3 , where @xmath4 is a measurable space with @xmath5-algebra @xmath6 ( typically , @xmath4 is a compact subset of a finite - dimensional euclidean space ) and @xmath7 is a borel subset of @xmath8 . in what follows , @xmath9 will denote the distribution of @xmath2 and @xmath10 the distribution of @xmath11 .",
    "the corresponding empirical distributions , based on @xmath12 and on @xmath13 , will be denoted by @xmath14 and @xmath15 , respectively . for a measurable function @xmath16 ,",
    "we denote @xmath17 similarly , we use the notations @xmath18 and @xmath19 for the integrals of a function @xmath20 with respect to the measures @xmath10 and @xmath15 .    the goal of prediction is to learn `` a reasonably good '' prediction rule @xmath21 from the empirical data @xmath22 . to be more specific , consider a loss function @xmath23 and define the risk of a prediction rule @xmath24 as @xmath25 where @xmath26 . an optimal prediction rule with respect to this loss",
    "is defined as @xmath27 where the minimization is taken over all measurable functions and , for simplicity , it is assumed that the minimum is attained . _",
    "the excess risk _ of a prediction rule @xmath24 is defined as @xmath28    throughout the paper , the notation @xmath29 means that there exists a numerical constant @xmath30 such that @xmath31 . by `` numerical constants '' we usually mean real numbers whose precise values are not necessarily specified , or , sometimes , constants that might depend on the characteristics of the problem that are of little interest to us ( e.g. , some constants that depend only on the loss function ) .",
    "let @xmath32 be a reproducing kernel hilbert space ( rkhs ) associated with a symmetric nonnegatively definite kernel @xmath33 such that for any @xmath34 , @xmath35 and @xmath36 for all @xmath37 [ @xcite ] . if it is known that if @xmath38 and @xmath39 , then it is natural to estimate @xmath40 by a solution @xmath41 of the following empirical risk minimization problem : @xmath42 the size of the excess risk @xmath43 of such an empirical solution depends on the `` smoothness '' of functions in the rkhs @xmath44 .",
    "a natural notion of `` smoothness '' in this context is related to the unknown design distribution @xmath10 .",
    "namely , let @xmath45 be the integral operator from @xmath46 into @xmath46 with kernel @xmath47 . under a standard assumption that the kernel @xmath47 is square integrable ( in the theory of rkhs",
    "it is usually even assumed that @xmath4 is compact and @xmath47 is continuous ) , the operator @xmath48 is compact and its spectrum is discrete . if @xmath49 is the sequence of the eigenvalues ( arranged in decreasing order ) of @xmath48 and @xmath50 is the corresponding @xmath46-orthonormal sequence of eigenfunctions , then it is well known that the rkhs - norms of functions from the linear span of @xmath50 can be written as @xmath51 which means that the `` smoothness '' of functions in @xmath44 depends on the rate of decay of eigenvalues @xmath52 that , in turn , depends on the design distribution  @xmath10 .",
    "it is also clear that the unit balls in the rkhs @xmath53 are ellipsoids in the space @xmath46 with `` axes '' @xmath54 .",
    "it was shown by @xcite that the function @xmath55,\\ ] ] provides tight upper and lower bounds ( up to constants ) on localized rademacher complexities of the unit ball in @xmath44 and plays an important role in the analysis of the empirical risk minimization problem ( [ simple_erm ] ) .",
    "it is easy to see that the function @xmath56 is concave , @xmath57 and , as a consequence , @xmath58 is a decreasing function of @xmath59 and @xmath60 is strictly decreasing .",
    "hence , there exists unique positive solution of the equation @xmath61 .",
    "if @xmath62 denotes this solution , then the results of @xcite imply that with some constant @xmath63 and with probability at least @xmath64 @xmath65 the size of the quantity @xmath66 involved in this upper bound on the excess risk depends on the rate of decay of the eigenvalues @xmath52 as @xmath67 .",
    "in particular , if @xmath68 for some @xmath69 , then it is easy to see that @xmath70 and @xmath71 . recall that unit balls in @xmath53 are ellipsoids in @xmath46 with `` axes '' of the order @xmath72 and it is well known that , in a variety of estimation problems , @xmath73 represents minimax convergence rates of the squared @xmath0-risk for functions from such ellipsoids ( e.g. , from sobolev balls of smoothness @xmath74 ) , as in famous pinsker s theorem [ see , e.g. , @xcite , chapter 3 ] .",
    "sobolev spaces @xmath75 of smoothness @xmath76 is a well - known class of concrete examples of rkhs .",
    "let @xmath77 denote the @xmath78-dimensional torus and let @xmath10 be the uniform distribution in @xmath79 .",
    "it is easy to check that , for all @xmath76 , the sobolev space @xmath80 is an rkhs generated by the kernel @xmath81 , where the function @xmath82 is defined by its fourier coefficients latexmath:[\\[\\hat k_n=(|n|^2 + 1)^{-\\alpha},\\qquad n=(n_1,\\ldots , n_d)\\in{\\mathbb z}^d,\\qquad    operator @xmath48 are the functions of the fourier basis and its eigenvalues are the numbers @xmath84 . for @xmath85 and @xmath86 , we have @xmath87 ( recall that @xmath49 are the eigenvalues arranged in decreasing order ) so , @xmath88 and @xmath89 , which is a minimax nonparametric convergence rate for sobolev balls in @xmath90 [ see , e.g. , @xcite , theorem 2.9 ] . more generally , for arbitrary @xmath91 and @xmath76 , we get @xmath92 and @xmath93 , which is also a minimax optimal convergence rate in this case .",
    "suppose now that the distribution @xmath10 is uniform in a torus @xmath94 of dimension @xmath95 .",
    "we will use the same kernel @xmath47 , but restrict the rkhs @xmath44 to the torus @xmath96 of smaller dimension .",
    "let @xmath97 .",
    "for @xmath98 , we will write @xmath99 with @xmath100 .",
    "it is easy to prove that the eigenvalues of the operator @xmath48 become in this case @xmath101 due to this fact , the norm of the space @xmath44 ( restricted to @xmath102 ) is equivalent to the norm of the sobolev space @xmath103 . since the eigenvalues of the operator @xmath48 coincide , up to a constant , with the numbers @xmath104 , we get @xmath105 [ which is again the minimax convergence rate for sobolev balls in @xmath103 ] . in the case of more",
    "general design distributions @xmath10 , the rate of decay of the eigenvalues @xmath52 and the corresponding size of the excess risk bound @xmath66 depends on @xmath10 .",
    "if , for instance , @xmath10 is supported in a submanifold @xmath107 of dimension @xmath108 , the rate of convergence of @xmath66 to @xmath109 depends on the dimension of the submanifold @xmath4 rather than on the dimension of the ambient space @xmath79 .    using the properties of the function @xmath110 , in particular , the fact that @xmath111 is decreasing , it is easy to observe that @xmath112 .",
    "$ ] moreover , if @xmath113 denotes the smallest value of @xmath114 such that the linear function @xmath115 $ ] provides an upper bound for the function @xmath116 $ ] , then @xmath117 note that @xmath118 also depends on @xmath119 , but we do not have to emphasize this dependence in the notation since , in what follows , @xmath119 is fixed . based on the observations above , the quantity @xmath120 coincides ( up to a numerical constant ) with the slope @xmath118 of the `` smallest linear majorant '' of the form @xmath121 of the function @xmath122 .",
    "this interpretation of @xmath120 is of some importance in the design of complexity penalties used in this paper .      instead of minimizing the empirical risk over an rkhs - ball [ as in problem ( [ simple_erm ] ) ] ,",
    "it is very common to define the estimator @xmath41 of the target function @xmath40 as a solution of the penalized empirical risk minimization problem of the form @xmath123,\\ ] ] where @xmath124 is a tuning parameter that balances the tradeoff between the empirical risk and the `` smoothness '' of the estimate and , most often , @xmath125 ( sometimes , ) .",
    "the properties of the estimator @xmath41 has been studied extensively . in particular , it was possible to derive probabilistic bounds on the excess risk @xmath43 ( oracle inequalities ) with the control of the random error in terms of the rate of decay of the eigenvalues @xmath49 , or , equivalently , in terms of the function @xmath126 [ see , e.g. , @xcite].=-1    in the recent years , there has been a lot of interest in a data dependent choice of kernel @xmath47 in this type of problems . in particular ,",
    "given a finite ( possibly large ) dictionary @xmath127 of symmetric nonnegatively definite kernels on  @xmath4 , one can try to find a `` good '' kernel @xmath47 as a convex combination of the kernels from the dictionary : @xmath128 the coefficients of @xmath47 need to be estimated from the training data along with the prediction rule . using this approach for problem",
    "( [ aa ] ) with @xmath129 leads to the following optimization problem : @xmath130 this learning problem , often referred to as the multiple kernel learning , has been studied recently by @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite and @xcite among others . in particular",
    "[ see , e.g. , @xcite ] , problem ( [ ab ] ) is equivalent to the following : @xmath131\\\\[-8pt ] & & \\hspace*{104.5pt}{}+\\epsilon \\sum_{j=1}^n \\|f_j\\|_{\\mathcal{h}_{k_j } } \\biggr),\\nonumber\\end{aligned}\\ ] ] which is an infinite - dimensional version of lasso - type penalization .",
    "@xcite studied this method in the case when the dictionary is large , but the target function @xmath40 has a `` sparse representation '' in terms of a relatively small subset of kernels @xmath132 .",
    "it was shown that this method is adaptive to sparsity extending well - known properties of lasso to this infinite - dimensional framework .    in this paper",
    ", we study a different approach to the multiple kernel learning . it is closer to the recent work on ``",
    "_ _ sparse additive models _ _ '' [ see , e.g. , @xcite and @xcite ] and it is based on a `` double penalization '' with a combination of empirical @xmath0-norms ( used to enforce the sparsity of the solution ) and rkhs - norms ( used to enforce the `` smoothness '' of the components ) . moreover , we suggest a data - driven method of choosing the values of regularization parameters that is adaptive to unknown smoothness of the components ( determined by the behavior of distribution dependent eigenvalues of the kernels )",
    ".    let @xmath133 .",
    "denote @xmath134 ( `` l.s . ''",
    "meaning `` the linear span '' ) , and @xmath135 note that @xmath136 if and only if there exists an additive representation ( possibly , nonunique ) @xmath137 where @xmath138 , @xmath139 .",
    "also , @xmath140 has a natural structure of a linear space and it can be equipped with the following inner product : @xmath141 to become _ the direct sum _ of hilbert spaces @xmath142 .    given a convex subset @xmath143 , consider the following penalized empirical risk minimization problem : @xmath144\\\\[-8pt ] & & \\hspace*{54.4pt}{}+\\sum_{j=1}^n \\bigl({\\epsilon } _",
    "j\\| f_j\\|_{l_2(\\pi_n ) } + { \\epsilon}_j^2\\|f_j\\|_{{\\mathcal h}_j } \\bigr ) \\biggr].\\nonumber\\end{aligned}\\ ] ] note that for special choices of set @xmath145 , for instance , for @xmath146 for some @xmath147 , one can replace each component @xmath148 involved in the optimization problem by its orthogonal projections in @xmath149 onto the linear span of the functions @xmath150 and reduce the problem to a convex optimization over a finite - dimensional space ( of dimension @xmath151 ) .",
    "the complexity penalty in the problem ( [ main_erm ] ) is based on two norms of the components @xmath148 of an additive representation : the empirical @xmath0-norm , @xmath152 , with regularization parameter @xmath153 , and an rkhs - norm , @xmath154 , with regularization parameter @xmath155 .",
    "the empirical @xmath0-norm ( the lighter norm ) is used to enforce the sparsity of the solution whereas the rkhs norms ( the heavier norms ) are used to enforce the `` smoothness '' of the components .",
    "this is similar to the approach taken in @xcite in the context of classical additive models , that is , in the case when @xmath156^n$ ] , @xmath157)$ ] for some smoothness @xmath86 and the space @xmath149 is a space of functions depending on the @xmath158th variable . in this case , the regularization parameters @xmath153 are equal ( up to a constant ) to @xmath159 .",
    "the quantity @xmath155 , used in the `` smoothness part '' of the penalty , coincides with the minimax convergence rate in a one component smooth problem . at the same time",
    ", the quantity @xmath153 , used in the `` sparsity part '' of the penalty , is equal to the square root of the minimax rate ( which is similar to the choice of regularization parameter in standard sparse recovery methods such as lasso ) .",
    "this choice of regularization parameters results in the excess risk of the order @xmath160 , where @xmath78 is the number of components of the target function ( the degree of sparsity of the problem ) .",
    "the framework of multiple kernel learning considered in this paper includes many generalized versions of classical additive models .",
    "for instance , one can think of the case when @xmath156^{m_1}\\times\\cdots\\times[0,1]^{m_n}$ ] and @xmath161^{m_j})$ ] is a space of functions depending on the @xmath158th block of variables . in this case",
    ", a  proper choice of regularization parameters ( for uniform design distribution ) would be @xmath162 ( so , these parameters and the error rates for different components of the model are different )",
    ". it should be also clear from the discussion in section  [ sec11 ] that , if the design distribution @xmath10 is unknown , the minimax convergence rates for the one component problems are also unknown .",
    "for instance , if the projections of design points on the cubes @xmath163^{m_j}$ ] are distributed in lower - dimensional submanifolds of these cubes , then the unknown dimensions of the submanifolds rather than the dimensions @xmath164 would be involved in the minimax rates and in the regularization parameters @xmath153 . because of this , data driven choice of regularization parameters @xmath153 that provides adaptation to the unknown design distribution @xmath10 and to the unknown `` smoothness '' of the components ( related to this distribution ) is a major issue in multiple kernel learning . from this point of view , even in the case of classical additive models , the choice of regularization parameters that is based only on sobolev type smoothness and ignores the design distribution is not adaptive .",
    "note that , in the infinite - dimensional lasso studied in @xcite , the regularization parameter @xmath114 is chosen the same way as in the classical lasso ( @xmath165 ) , so , it is not related to the smoothness of the components .",
    "however , the oracle inequalities proved in @xcite give correct size of the excess risk only for special choices of kernels that depend on unknown `` smoothness '' of the components of the target function  @xmath40 , so , this method is not adaptive either .      denote @xmath166 this @xmath167 gram matrix can be viewed as an empirical version of the integral operator @xmath168 from @xmath46 into @xmath46 with kernel @xmath169 .",
    "denote @xmath170 the eigenvalues of @xmath171 arranged in decreasing order .",
    "we also use the notation @xmath172 for the eigenvalues of the operator @xmath173 with kernel @xmath169 arranged in decreasing order . define functions @xmath174 , @xmath175 and , for a fixed given @xmath176 ,",
    "let @xmath177 \\biggr\\}.\\ ] ] one can view @xmath178 as an empirical estimate of the quantity @xmath179 that ( as we have already pointed out ) plays a crucial role in the bounds on the excess risk in empirical risk minimization problems in the rkhs context .",
    "in fact , since most often @xmath180 , we will redefine this quantity as @xmath181 \\biggr\\}.\\ ] ]    we will use the following values of regularization parameters in problem  ( [ main_erm ] ) : @xmath182 , where @xmath183 is a sufficiently large constant .",
    "it should be emphasized that the structure of complexity penalty and the choice of regularization parameters in ( [ main_erm ] ) are closely related to the following bound on rademacher processes indexed by functions from an rkhs @xmath44 : with a high probability , for all @xmath184 , @xmath185.\\ ] ] such bounds follow from the results of section  [ sec : sam2pop ] and they provide a way to prove sparsity oracle inequalities for the estimators ( [ main_erm ] ) .",
    "the rademacher process is defined as @xmath186 where @xmath187 is a sequence of i.i.d .",
    "rademacher random variables ( taking values @xmath188 and @xmath189 with probability @xmath190 each ) independent of @xmath191",
    ".    we will use several basic facts of the empirical processes theory throughout the paper .",
    "they include symmetrization inequalities and contraction ( comparison ) inequalities for rademacher processes that can be found in the books of @xcite and @xcite .",
    "we also use talagrand s concentration inequality for empirical processes [ see , @xcite , @xcite ] .",
    "the main goal of the paper is to establish oracle inequalities for the excess risk of the estimator @xmath192 . in these inequalities , the excess risk of @xmath41",
    "is compared with the excess risk of an oracle @xmath193 with an error term depending on the degree of sparsity of the oracle , that is , on the number of nonzero components @xmath138 in its additive representation .",
    "the oracle inequalities will be stated in the next section .",
    "their proof relies on probabilistic bounds for empirical @xmath0-norms and data dependent regularization parameters @xmath194 .",
    "the results of section  [ sec : sam2pop ] show that they can be bounded by their respective population counterparts . using these tools and some bounds on empirical processes derived in section  [ sec : empirical_process ] ,",
    "we prove in section  [ sec : mainres ] the oracle inequalities for the estimator  @xmath195 .",
    "considering the problem in the case when the domain @xmath145 of ( [ main_erm ] ) is not bounded , say , @xmath196 , leads to additional technical complications and might require some changes in the estimation procedure . to avoid this",
    ", we assume below that @xmath145 is a bounded convex subset of @xmath140 .",
    "it will be also assumed that , for all @xmath139 , @xmath197 which , by elementary properties of rkhs , implies that @xmath198 because of this , @xmath199 denote @xmath200 we will allow the constants involved in the oracle inequalities stated and proved below to depend on the value of @xmath201 ( so , implicitly , it is assumed that this value is not too large ) .",
    "we shall also assume that @xmath202 is large enough , say , so that @xmath203 .",
    "this assumption is not essential to our development and is in place to avoid an extra term of the order @xmath204 in our risk bounds .",
    "we will formulate the _",
    "assumptions on the loss function @xmath205_. the main assumption is that , for all @xmath206 , @xmath207 is a nonnegative convex function . in addition",
    ", we will assume that @xmath208 is uniformly bounded from above by a numerical constant .",
    "moreover , suppose that , for all @xmath206 , @xmath207 is twice continuously differentiable and its first and second derivatives are uniformly bounded in @xmath209 $ ] .",
    "denote @xmath210 and let @xmath211 we will assume that @xmath212 .",
    "denote @xmath213 clearly , for all @xmath206 , the function @xmath207 satisfies lipschitz condition with constant @xmath214 .",
    "the constants @xmath215 will appear in a number of places in what follows .",
    "without loss of generality , we can also assume that @xmath216 and @xmath217 ( otherwise , @xmath218 and @xmath214 can be replaced by a lower bound and an upper bound , resp . ) .",
    "the loss functions satisfying the assumptions stated above will be called _ the losses of quadratic type_. if @xmath205 is a loss of quadratic type and @xmath219 , then @xmath220 this bound easily follows from a simple argument based on taylor expansion and it will be used later in the paper . if @xmath221 is dense in @xmath46 , then ( [ excess_l_2 ] ) implies that @xmath222    the quadratic loss @xmath223 in the case when @xmath224 is a bounded set is one of the main examples of such loss functions . in this case , @xmath225 for all @xmath226 . in regression problems with a bounded response variable , more general loss functions of the form @xmath227",
    "can be also used , where @xmath228 is an even nonnegative convex twice continuously differentiable function with @xmath229 uniformly bounded in @xmath230 , @xmath231 and @xmath232 . in classification problems , the loss functions of the form @xmath233",
    "are commonly used , with @xmath228 being a nonnegative decreasing convex twice continuously differentiable function such that , again , @xmath229 is uniformly bounded in @xmath230 and @xmath232 .",
    "the loss function @xmath234 ( often referred to as the logit loss ) is a specific example .",
    "now we introduce several important geometric characteristics of dictionaries consisting of kernels ( or , equivalently , of rkhs ) .",
    "these characteristics are related to the degree of `` dependence '' of spaces of random variables @xmath235 and they will be involved in the oracle inequalities for the excess risk @xmath43 .    first , for @xmath236 and @xmath237 $ ] , denote @xmath238 clearly , the set @xmath239 is a cone in the space @xmath140 that consists of vectors @xmath240 whose components corresponding to @xmath241 `` dominate '' the rest of the components .",
    "this family of cones increases as @xmath242 increases . for @xmath243",
    ", @xmath244 coincides with the linear subspace of vectors for which @xmath245 . for @xmath246",
    ", @xmath239 is the whole space @xmath140",
    ".    the following quantity will play the most important role : @xmath247 : \\!&= & \\inf\\biggl\\{\\beta>0\\dvtx \\biggl(\\sum_{j\\in j}\\|h_j\\|_{l_2(\\pi)}^2 \\biggr)^{1/2 } \\leq\\beta\\biggl\\|\\sum_{j=1}^n h_j \\biggr\\|_{l_2(\\pi)},\\\\[-2pt ] & & \\hspace*{18.17pt}\\hspace*{117pt } ( h_1,\\ldots , h_n)\\in c_{j}^{(b ) } \\biggr\\}.\\end{aligned}\\ ] ] clearly , @xmath248 is a nondecreasing function of @xmath242 . in the case of `` simple dictionary '' that consists of one - dimensional spaces similar quantities",
    "have been used in the literature on sparse recovery [ see , e.g. , koltchinskii ( @xcite , @xcite , @xcite , @xcite ) ; bickel , ritov and tsybakov ( @xcite ) ] .",
    "the quantity @xmath248 can be upper bounded in terms of some other geometric characteristics that describe how `` dependent '' the spaces of random variables @xmath249 are .",
    "these characteristics will be introduced below .",
    "given @xmath250 , denote by @xmath251 the minimal eigenvalue of the gram matrix @xmath252 .",
    "let @xmath253 we will also use the notation @xmath254 the following quantity is the maximal cosine of the angle in the space @xmath46 between the vectors in the subspaces @xmath255 and @xmath256 for some @xmath257 : @xmath258 denote @xmath259 the quantities @xmath260 and @xmath261 are very similar to the notion of _ canonical correlation _ in the multivariate statistical analysis .",
    "there are other important geometric characteristics , frequently used in the theory of sparse recovery , including so called `` _ _ restricted isometry constants _ _ '' by candes and tao ( @xcite ) .",
    "define @xmath262 to be the smallest @xmath263 such that for all @xmath264 and all @xmath265 with @xmath266 , @xmath267 this condition with a sufficiently small value of @xmath268 means that for all choices of @xmath269 with @xmath266 the functions in the spaces @xmath270 are `` almost orthogonal '' in @xmath46 .",
    "the following simple proposition easily follows from some statements in koltchinskii ( @xcite , @xcite ) , ( @xcite ) ( where the case of simple dictionaries consisting of one - dimensional spaces @xmath149 was considered ) .    for all @xmath236 , @xmath271 also , if @xmath266 and @xmath272 , then @xmath273 .",
    "thus , such quantities as @xmath274 or @xmath275 , for finite values of @xmath242 , are reasonably small provided that the spaces of random variables @xmath276 satisfy proper conditions of `` weakness of correlations . ''      we are now in a position to formulate our main theorems that provide oracle inequalities for the excess risk @xmath43 . in these theorems , @xmath43 will be compared with the excess risk @xmath277 of an oracle @xmath278 .",
    "here and in what follows , @xmath279 .",
    "this is a little abuse of notation : we are ignoring the fact that such an additive representation of a function @xmath136 is not necessarily unique . in some sense",
    ", @xmath24 denotes both the vector @xmath280 and the function @xmath281 .",
    "however , this is not going to cause a confusion in what follows .",
    "we will also use the following notation : @xmath282    the error terms of the oracle inequalities will depend on the quantities @xmath179 related to the `` smoothness '' properties of the rkhs and also on the geometric characteristics of the dictionary introduced above . in the first theorem , we will use the quantity @xmath283 to characterize the properties of the dictionary . in this case , there will be no assumptions on the quantities @xmath284 : these quantities could be of different order for different kernel machines , so , different components of the additive representation could have different `` smoothness . '' in the second theorem , we will use a smaller quantity @xmath248 for a proper choice of parameter @xmath285 . in this case",
    ", we will have to make an additional assumption that @xmath286 are all of the same order ( up to a constant ) .    in both cases , we consider penalized empirical risk minimization problem  ( [ main_erm ] ) with data - dependent regularization parameters @xmath287 , where @xmath288 are defined by ( [ regul_par ] ) with some @xmath289 and @xmath290 for a numerical constant @xmath291 .",
    "[ co : homobd ] there exist numerical constants @xmath292 such that , for all all oracles @xmath278 , with probability at least @xmath293 , @xmath294\\\\[-8pt ] & & \\qquad \\le 2{\\mathcal e}(\\ell\\circ f)+c_2\\tau^2 \\sum_{j\\in j_f}\\breve{\\epsilon}_j^2 \\biggl(\\frac{\\beta_{2,\\infty}^2(j_f,\\pi)}{m_{\\ast}}+ \\|f_j\\|_{{\\mathcal h}_j } \\biggr).\\nonumber\\vadjust{\\goodbreak}\\end{aligned}\\ ] ]    this result means that if there exists an oracle @xmath278 such that :    a.   the excess risk @xmath277 is small ; b.   the spaces @xmath295 are not strongly correlated with the spaces @xmath296 c.   @xmath295 are `` well posed '' in the sense that @xmath297 is not too small ; d.   @xmath298 are all bounded by a reasonable constant ,    then the excess risk @xmath43 is essentially controlled by @xmath299 . at the same time , the oracle inequality provides a bound on the @xmath46-distances between the estimated components @xmath300 and the components of the oracle ( of course , everything is under the assumption that the loss is of quadratic type and @xmath218 is bounded away from @xmath109 ) .    not also that the constant @xmath301 in front of the excess risk of the oracle @xmath302 can be replaced by @xmath303 for any @xmath263 with minor modifications of the proof ( in this case , the constant @xmath304 depends on @xmath59 and is of the order @xmath305 ) .",
    "suppose now that there exists @xmath306 and a constant @xmath307 such that @xmath308    [ co : homobd-1 ] there exist numerical constants @xmath309 such that , for all oracles @xmath278 , with probability at least @xmath293 , @xmath310\\\\[-8pt ] & & \\qquad \\le2{\\mathcal e}(\\ell\\circ f)+ c_2 \\lambda\\tau^2\\breve \\epsilon^2 \\biggl(\\frac{\\beta_{2,b\\lambda^2}^2(j_f,\\pi)}{m_{\\ast } } d(f ) + \\sum_{j\\in j_f}\\|f_j\\|_{{\\mathcal h}_j } \\biggr).\\nonumber\\end{aligned}\\ ] ]    as before , the constant @xmath301 in the upper bound can be replaced by @xmath303 , but , in this case , the constants @xmath304 and @xmath242 would be of the order @xmath311 .",
    "the meaning of this result is that if there exists an oracle @xmath278 such that :    a.   the excess risk @xmath277 is small ; b.   the `` restricted isometry '' constant @xmath312 is small for @xmath313 c.   @xmath298 are all bounded by a reasonable constant ,    then the excess risk @xmath43 is essentially controlled by @xmath314 . at the same time , the distance @xmath315 between the estimator and the oracle is controlled by @xmath316 .",
    "in particular , this implies that the empirical solution @xmath317 is `` approximately sparse '' in the sense that @xmath318 is of the order @xmath319 .",
    "it is easy to check that theorems  [ co : homobd ] and  [ co : homobd-1 ] hold also if one replaces @xmath202 in the definitions ( [ regul_par ] ) of @xmath178 and ( [ regul_par_01 ] ) of @xmath284 by an arbitrary @xmath320 such that @xmath321 ( a similar condition on @xmath202 introduced early in section  [ sec : oracle ] is not needed here ) . in this case , the probability bounds in the theorems become @xmath322 .",
    "this change might be of interest if one uses the results for a dictionary consisting of just one rkhs ( @xmath323 ) , which is not the focus of this paper .",
    "\\2 . if the distribution dependent quantities @xmath324 are known and used as regularization parameters in ( [ main_erm ] ) , the oracle inequalities of theorems  [ co : homobd ] and  [ co : homobd-1 ] also hold ( with obvious simplifications of their proofs ) .",
    "for instance , in the case when @xmath325^n$ ] , the design distribution @xmath10 is uniform and , for each @xmath139 , @xmath326  is a sobolev space of functions of smoothness @xmath86 depending only on the @xmath158th variable , we have @xmath327 . taking in this case",
    "@xmath328 would lead to oracle inequalities for sparse additive models is spirit of @xcite .",
    "more precisely , if @xmath329\\dvtx\\int_0 ^ 1 h(x)\\,dx=0\\}$ ] , then , for uniform distribution @xmath10 , the spaces @xmath326 are orthogonal in @xmath46 ( recall that @xmath326 is viewed as a space of functions depending on the @xmath158th coordinate ) .",
    "assume , for simplicity , that @xmath205 is the quadratic loss and that the regression function @xmath40 can be represented as @xmath330 , where @xmath269 is a subset of @xmath331 of cardinality @xmath78 and @xmath332 .",
    "then it easily follows from the bound of theorem  [ co : homobd-1 ] that with probability at least @xmath333 @xmath334 note that , up to a constant , this essentially coincides with the minimax lower bound in this type of problems obtained recently by @xcite . of course , if the design distribution is not necessarily uniform , an adaptive choice of regularization parameters might be needed even in such simple examples and the approach described above leads to minimax optimal rates .",
    "in this section , the case of a single rkhs @xmath44 associated with a kernel @xmath47 is considered .",
    "we assume that @xmath335 .",
    "this implies that , for all @xmath336 , @xmath337      first , we study the relationship between the empirical and the population @xmath0 norms for functions in @xmath53 .",
    "[ th : empnorm ] assume that @xmath176 and @xmath338",
    ". then there exists a numerical constant @xmath63 such that with probability at least @xmath339 for all @xmath336 @xmath340 where @xmath341\\\\[-8pt ] : \\!&= & \\inf\\biggl\\{\\epsilon\\ge\\sqrt{a\\log n\\over n}\\dvtx { { \\mathbb{e}}\\mathop{\\sup_{\\|h\\|_{{\\mathcal h}_k}=1}}_{\\|h\\|_{l_2(\\pi)}\\le\\delta } }     observe that the inequalities hold trivially when @xmath342 .",
    "we shall therefore consider only the case when @xmath343 . by symmetrization inequality ,",
    "@xmath344    the definition of @xmath345 implies that @xmath346\\\\[-8pt ] & & \\qquad\\le { 8 { \\mathbb{e}}\\mathop{\\sup_{\\|h\\|_{{\\mathcal h}_k}=1}}_{\\|h\\|_{l_2(\\pi)}\\le 2^{-j+1 } } } |r_n(h ) | \\le8 ( \\bar{\\epsilon}2^{-j+1}+\\bar{\\epsilon}^2 ) .\\nonumber\\end{aligned}\\ ] ]    an application of talagrand s concentration inequality yields @xmath347 with probability at least @xmath348 for any natural number @xmath158 .",
    "now , by the union bound , for all @xmath158 such that @xmath349 , @xmath350\\\\[-8pt ] & & \\qquad \\le32 \\biggl(\\bar{\\epsilon}2^{-j}+\\bar{\\epsilon}^2 + 2^{-j}\\sqrt{t+2\\log j\\over n}+{t+2\\log j \\over",
    "n } \\biggr)\\nonumber\\end{aligned}\\ ] ] with probability at least @xmath351\\\\[-8pt ] & \\ge&1 - 2\\exp(-t).\\nonumber\\end{aligned}\\ ] ] recall that @xmath352 and @xmath353 .",
    "taking @xmath354 , we easily get that , for all @xmath184 such that @xmath355 and @xmath356 , @xmath357 with probability at least @xmath358 and with a numerical constant @xmath63 . in other words , with the same probability , for all @xmath184 such that @xmath359 , @xmath360 therefore , for all @xmath336 such that @xmath361 we have @xmath362 it can be now deduced that , for a proper value of numerical constant @xmath363 , @xmath364\\\\[-8pt ] \\|h\\|_{l_2(\\pi_n ) } & \\le & c \\bigl(\\|h\\|_{l_2(\\pi)}+ \\bar{\\epsilon}\\|h\\|_{{\\mathcal h}_k } \\bigr).\\nonumber\\end{aligned}\\ ] ]    it remains to consider the case when @xmath365 following a similar argument as before , with probability at least @xmath366 , @xmath367 under the conditions @xmath368 , @xmath369 then @xmath370 with probability at least @xmath358 , which also implies ( [ eq : empl2 - 1 ] ) and ( [ eq : empl2 - 2 ] ) , and the result follows .",
    "theorem  [ th : empnorm ] shows that the two norms @xmath371 and @xmath372 are of the same order up to an error term @xmath373 .",
    "recall the definitions @xmath374,\\ ] ] where @xmath49 are the eigenvalues of the integral operator @xmath48 from @xmath46 into @xmath46 with kernel @xmath47 , and , for some @xmath176 , @xmath375 \\biggr\\}.\\ ] ] it follows from lemma 42 of @xcite [ with an additional application of cauchy  schwarz inequality for the upper bound and hoffmann  jrgensen inequality for the lower bound ; see also @xcite ] that , for some numerical constants @xmath376 , @xmath377\\\\[-8pt ] & \\leq & c_2 \\biggl(n^{-1}\\sum_{k=1}^n ( \\lambda_k \\wedge\\delta^2 ) \\biggr)^{1/2}.\\nonumber\\end{aligned}\\ ] ]    this fact and the definitions of @xmath378 easily imply the following result .",
    "[ pr : bareps ] under the condition @xmath335 , there exist numerical constants @xmath292 such that @xmath379    if @xmath47 is the kernel of the projection operator onto a finite - dimensional subspace @xmath44 of @xmath46 , it is easy to check that @xmath380 ( recall the notation @xmath29 , which means that there exists a numerical constant @xmath30 such that ) .",
    "if the eigenvalues @xmath52 decay at a polynomial rate , that is , @xmath381 for some @xmath69 , then @xmath382 .    recall the notation @xmath383 \\biggr\\},\\hspace*{-35pt}\\ ] ] where @xmath384 denote the eigenvalues of the gram matrix @xmath385 it follows again from the results of @xcite [ namely , one can follow the proof of lemma 42 in the case when the rkhs @xmath44 is restricted to the sample @xmath386 and the expectations are conditional on the sample ; then one uses cauchy  schwarz and hoffmann  jrgensen inequalities as in the proof of ( [ thirtythree ] ) ] that for some numerical constants @xmath376 @xmath387\\\\[-8pt ] & \\leq & c_2 \\biggl(n^{-1}\\sum_{k=1}^n ( \\hat\\lambda_k \\wedge\\delta^2 ) \\biggr)^{1/2},\\nonumber\\end{aligned}\\ ] ] where @xmath388 indicates that the expectation is taken over the rademacher random variables only ( conditionally on @xmath386 ) .",
    "therefore , if we denote  by @xmath389 \\biggr\\}\\hspace*{-35pt}\\ ] ] the empirical version of @xmath390 , then @xmath391 . we will now show that @xmath392 with a high probability .",
    "[ th : empeps ] suppose that @xmath176 and @xmath338 .",
    "there exist numerical constants @xmath292 such that @xmath393 with probability at least @xmath339 .",
    "let @xmath394 .",
    "it follows from talagrand concentration inequality that @xmath395 with probability at least @xmath348 . on the other hand ,",
    "as derived in the proof of theorem  [ th : empnorm ] [ see ( [ pi_n - pi ] ) ] @xmath396 with probability at least @xmath348 .",
    "we will use these bounds only for @xmath158 such that @xmath397 . in this case",
    ", the second bound implies that , for some numerical constant @xmath30 and all @xmath398 satisfying the conditions @xmath399 we have @xmath400 ( again , see the proof of theorem  [ th : empnorm ] ) . combining these bounds",
    ", we get that with probability at least @xmath401 , @xmath402 where @xmath403 .",
    "applying now talagrand concentration inequality to the rademacher process conditionally on the observed data @xmath386 yields @xmath404 with conditional probability at least @xmath348 . from this and from the previous",
    "bound it is not hard to deduce that , for some numerical constants @xmath405 and for all @xmath158 such that @xmath397 , @xmath406 with probability at least @xmath407 . in obtaining the second inequality , we used the definition of @xmath408 and the fact that , for @xmath409 , @xmath410 , where @xmath411 is a numerical constant .",
    "now , by the union bound , the above inequality holds with probability at least @xmath412 for all @xmath158 such that @xmath349 simultaneously .",
    "similarly , it can be shown that @xmath413 with probability at least @xmath414 .    for @xmath415",
    ", we get @xmath416 for all @xmath417 , with probability at least @xmath418 now by the definition of @xmath345 , we obtain @xmath419 which implies that @xmath420 with probability at least @xmath421 .",
    "similarly one can show that @xmath422 for all @xmath417 , with probability at least @xmath421 , which implies that @xmath423 with probability at least @xmath421 .",
    "the proof can then be completed by the union bound .    define @xmath424\\\\[-8pt ] & : = & \\inf\\biggl\\{\\epsilon\\ge\\sqrt{a\\log n\\over n}\\dvtx \\mathop{\\sup_{\\|h\\|_{{{\\mathcal h}}_k}=1 } } _ { \\|h\\|_{l_2(\\pi ) } \\le\\delta}|r_n(h)|\\le \\epsilon\\delta+ \\epsilon^2 , \\forall\\delta\\in(0,1 ] \\biggr\\}.\\nonumber\\end{aligned}\\ ] ]    the next statement can be proved similarly to theorem  [ th : empeps ] .",
    "[ th : empeps-1 ] there exist numerical constants @xmath292 such that @xmath425 with probability at least @xmath339 .",
    "suppose now that @xmath426 is a dictionary of kernels .",
    "recall that @xmath427 , @xmath428 and @xmath429 .",
    "it follows from theorems  [ th : empnorm ] ,  [ th : empeps ] , [ th : empeps-1 ] and the union bound that with probability at least @xmath430 for all @xmath139 @xmath431\\\\[-8pt ] \\|h\\|_{l_2(\\pi_n)}&\\le & c \\bigl(\\|h\\|_{l_2(\\pi)}+ \\bar{\\epsilon}_j\\|h\\|_{{\\mathcal h}_k } \\bigr),\\qquad h\\in\\mathcal{h}_j,\\nonumber \\\\ \\label{conclusion_b } c_1\\bar{\\epsilon}_j&\\le&\\hat{\\epsilon}_j\\le c_2\\bar{\\epsilon}_j \\quad\\mbox{and}\\quad c_1\\bar{\\epsilon}_j\\le\\check{\\epsilon}_j\\le c_2\\bar{\\epsilon}_j.\\end{aligned}\\ ] ] note also that @xmath432 provided that @xmath289 and @xmath433 .",
    "thus , under these additional constraints , ( [ conclusion_a ] ) and ( [ conclusion_b ] ) hold for all @xmath139 with probability at least @xmath434 .",
    "for an arbitrary set @xmath435 and @xmath436 , denote @xmath437 and let @xmath438\\\\[-8pt ] & & \\hspace*{153pt } ( f_1,\\ldots , f_n)\\in\\mathcal { k}_j^{(b ) } \\biggr\\}.\\nonumber\\end{aligned}\\ ] ]    it is easy to see that , for all nonempty sets @xmath269 , @xmath439    theorems  [ co : homobd ] and  [ co : homobd-1 ] will be easily deduced from the following technical result .",
    "[ th : main ] there exist numerical constants @xmath440 and @xmath441 such that , for all @xmath442 in the definition of @xmath443 and for all oracles @xmath444 , @xmath445 with probability at least @xmath293 . here",
    ", @xmath289 is a constant involved in the definitions of @xmath446 .",
    "recall that @xmath447,\\end{aligned}\\ ] ] and that we write @xmath448 hence , for all @xmath278 , @xmath449 by a simple algebra , @xmath450 and , by the triangle inequality , @xmath451    we now take advantage of ( [ conclusion_a ] ) and ( [ conclusion_b ] ) to replace @xmath194 s by @xmath452 s and @xmath453 by @xmath454 .",
    "specifically , there exists a numerical constant @xmath455 and an event @xmath456 of probability at least @xmath434 such that @xmath457 and , for all @xmath458 , @xmath459 taking @xmath460 , we have that , on the event @xmath456 , @xmath461 similarly , @xmath462 therefore , by taking @xmath183 large enough , namely @xmath463 , we can find numerical constants @xmath464 such that , on the event @xmath456 , @xmath465    we now bound the empirical process @xmath466 , where we use the following result that will be proved in the next section .",
    "suppose that @xmath467 , @xmath138 and @xmath468 ( we will need it with @xmath469 ) .",
    "denote @xmath470    [ th : uniep ] there exists a numerical constant @xmath63 such that for an arbitrary @xmath176 involved in the definition of @xmath471 with probability at least @xmath472 , for all @xmath473 the following bound holds : @xmath474    assuming that @xmath475 and using the lemma , we get @xmath476 for some numerical constant @xmath477 . by choosing a numerical constant @xmath291 properly",
    ", @xmath183 can be made large enough so that @xmath478",
    ". then , we have @xmath479 which also implies @xmath480 we first consider the case when @xmath481\\\\[-8pt ] & & { } + ( c_2/2)\\tau e^{-n}.\\nonumber\\end{aligned}\\ ] ] then ( [ bound_x ] ) implies that @xmath482\\\\[-8pt ] & & \\qquad\\le6 c_2\\sum_{j\\in j_f}\\tau\\bar{\\epsilon}_j\\|f_j-\\hat{f}_j\\| _ { l_2(\\pi)},\\nonumber\\end{aligned}\\ ] ] which yields @xmath483 therefore , @xmath484 with @xmath485 .",
    "using the definition of @xmath486 , it follows from ( [ bound_y ] ) , ( [ bound_z ] ) and the assumption @xmath487 that @xmath488    recall that for losses of quadratic type @xmath489 then @xmath490 using the fact that @xmath491 , we get @xmath492 and @xmath493 therefore , @xmath494\\\\[-8pt ] & & \\qquad \\le{\\mathcal e}(\\ell\\circ f)+100 \\tau^2c_2 ^ 2m_{\\ast}^{-1}\\beta_b^2(j_f).\\nonumber\\end{aligned}\\ ] ]    we now consider the case when @xmath495\\\\[-8pt ] & & \\qquad < { \\mathcal e}(\\ell\\circ f)+ 2c_2\\sum_{j\\in j_f}\\tau^2\\bar{\\epsilon}_j^2\\|f_j\\|_{{\\mathcal h}_j}+(c_2/2)\\tau e^{-n}.\\nonumber\\end{aligned}\\ ] ] it is easy to derive from ( [ bound_y ] ) that in this case @xmath496\\\\[-8pt ] & & \\qquad \\le \\biggl(\\frac{3}{2}+\\frac{c_1}{8c_2 } \\biggr ) \\biggl({\\mathcal e}(\\ell\\circ f)+ 2c_2\\sum_{j\\in j_f}\\tau^2\\bar{\\epsilon}_j^2\\|f_j\\|_{{\\mathcal h}_j}+(c_2/2)\\tau e^{-n } \\biggr).\\nonumber\\end{aligned}\\ ] ] since @xmath497 [ see the comment after the definition of @xmath486 ] , we have @xmath499 where we also used the assumptions that @xmath500 and @xmath289 . substituting this in ( [ eq : riskbddom2 ] ) and then combining the resulting bound with ( [ eq : riskbddom1 ] ) concludes the proof of ( [ main_oracle ] ) in the case when conditions ( [ case_1 ] ) hold .",
    "it remains to consider the case when ( [ case_1 ] ) does not hold .",
    "the main idea is to show that in this case the right - hand side of the oracle inequality is rather large while we still can control the left - hand side , so , the inequality becomes trivial . to this end , note that , by the definition of @xmath41 , for some numerical constant @xmath411 , @xmath501 [ since the value of the penalized empirical risk at @xmath41 is not larger than its value at @xmath502 and , by the assumptions on the loss , @xmath503 is uniformly bounded by a numerical constant ] .",
    "the last equation implies that , on the event @xmath456 defined earlier in the proof [ see ( [ condition_01 ] ) , ( [ condition_02 ] ) ] , the following bound holds : @xmath504",
    "equivalently , @xmath505 as soon as @xmath506 , so that @xmath507 , we have @xmath508 note also that , by the assumptions on the loss function , @xmath509 where we used the lipschitz condition on @xmath205 , and also bound ( [ hatf ] ) and the fact that @xmath510 ( by its definition ) .",
    "recall that we are considering the case when ( [ case_1 ] ) does not hold .",
    "we will consider two cases : ( a ) when @xmath511 , where @xmath512 is a numerical constant , and ( b ) when @xmath513 .",
    "the first case is very simple since @xmath202 and @xmath119 are both upper bounded by a numerical constant ( recall the assumption @xmath338 ) . in this case , @xmath497 is bounded from below by a numerical constant . as a consequence of these observations ,",
    "bounds  ( [ hatf ] ) and ( [ excess_a ] ) imply that @xmath514 & & \\qquad\\leq c_2 \\tau^2 \\beta_b^2(j_f)\\vspace*{-2pt}\\end{aligned}\\ ] ] for some numerical constant @xmath515 . in the case",
    "( b ) , we have @xmath516 and , in view of ( [ hatf ] ) , this implies @xmath517 so , either we have @xmath518 or @xmath519 moreover , in the second case , we also have @xmath520 & \\geq&(e^n/4)\\sqrt { \\frac { a\\log n}{n}}.\\vspace*{-2pt}\\end{aligned}\\ ] ] in both cases we can conclude that , under the assumption that @xmath521 and @xmath513 for a sufficiently large numerical constant @xmath522 , @xmath523 & & \\qquad\\leq c_1 + 2c_1 c^2 l_{\\ast } \\frac{1}{\\tau}\\sqrt{\\frac{n}{a\\log n}}+ 2c_1 c^2\\\\ & & \\qquad\\leq\\frac{\\tau^2 e^n}{4}\\sqrt{\\frac{a\\log n}{n}}\\leq\\tau^2 \\sum_{j\\in j_f } \\bar\\epsilon_j^2 \\|f_j\\|_{\\mathcal{h}_j}.\\end{aligned}\\ ] ] thus , in both cases ( a ) and ( b ) , the following bound holds : @xmath524\\\\[-8pt ] & & \\qquad \\leq",
    "c_2 \\tau^2 \\biggl(\\sum_{j\\in j_f } \\bar\\epsilon_j^2 \\|f_j\\| _ { \\mathcal { h}_j}+\\beta_b^2(j_f ) \\biggr).\\nonumber\\end{aligned}\\ ] ]    to complete the proof , observe that @xmath525 note also that , by the definition of @xmath486 , for all @xmath441,@xmath526\\\\[-8pt ] & & \\qquad\\leq \\tau\\beta_b(j_f)\\|\\hat f - f\\|_{l_2(\\pi)}+ \\tau\\beta_b(j_f)\\sqrt{\\frac{n}{a\\log n}}\\sum_{j\\notin j_f}\\bar \\varepsilon_j \\|\\hat f_j\\|_{l_2(\\pi ) } \\nonumber\\\\ & & \\qquad\\leq \\tau\\beta_b(j_f)\\|\\hat f - f\\|_{l_2(\\pi)}+ \\tau\\beta_b(j_f ) \\frac{2c_1c^2}{\\tau } \\sqrt{\\frac{n}{a\\log n}},\\nonumber\\end{aligned}\\ ] ] where we used the fact that , for all @xmath158 , @xmath527 and also bound ( [ hatf ] ) . by an argument similar to ( [ quadr])([eq : riskbddom1 ] ) , it is easy to deduce from the last bound that @xmath528\\\\[-9pt ] & & { } + \\frac{2c_1 ^ 2c^4}{\\tau^2}\\frac{n}{a\\log n}.\\nonumber\\end{aligned}\\ ] ] substituting this in bound ( [ konec1 ] ) , we get @xmath529 & & \\qquad \\leq c_2 \\tau^2 \\biggl(\\sum_{j\\in j_f } \\bar\\epsilon_j^2 \\|f_j\\| _ { \\mathcal{h}_j}+ \\beta_b^2(j_f ) \\biggr ) \\nonumber\\\\[-2pt ] & & \\qquad\\quad { } + \\frac{3}{2}\\frac{c_2 ^ 2 \\tau^2}{m_{\\ast } } \\beta_b^2(j_f ) + \\frac{1}{2}\\mathcal{e}(\\ell\\circ f)+ \\frac{2c_1 ^ 2c^4}{\\tau^2}\\frac{n}{a\\log n } \\\\[-2pt ] & & \\qquad \\leq \\frac{1}{2}\\mathcal{e}(\\ell\\circ f)+ c_2^{\\prime } \\tau^2 \\biggl(\\sum_{j\\in j_f } \\bar\\epsilon_j^2 \\| f_j\\| _ { \\mathcal{h}_j}+ \\frac{\\beta_b^2(j_f)}{m_{\\ast } } \\biggr)\\nonumber\\\\[-2pt ] & & \\qquad\\quad{}+\\frac{2c_1 ^ 2c^2}{\\tau^2}\\frac{n}{a\\log n},\\nonumber\\end{aligned}\\ ] ] with some numerical constant @xmath530 .",
    "it is enough now to observe [ considering again the cases ( a ) and ( b ) , as it was done before ] , that either the last term is upper bounded by @xmath531 , or it is upper bounded by @xmath532 , to complete the proof .",
    "now , to derive theorem  [ co : homobd ] , it is enough to check that , for a numerical constant @xmath30 , @xmath533 & \\leq & c \\biggl(\\sum_{j\\in j_{f}}\\breve\\epsilon_j^2 \\biggr)^{1/2 } \\beta_{2,\\infty}(j_f),\\end{aligned}\\ ] ] which easily follows from the definitions of @xmath534 and @xmath535 .",
    "similarly , the proof of theorem  [ co : homobd-1 ] follows from the fact that , under the assumption that @xmath536 we have @xmath537 where @xmath538 , @xmath539 being a numerical constant .",
    "this easily implies the bound @xmath540 where @xmath411 is a numerical constant .",
    "we now proceed to prove lemma  [ th : uniep ] that was used to bound @xmath466 . to this end , we begin with a fixed pair @xmath541 . throughout the proof , we write @xmath542 . by talagrand",
    "s concentration inequality , with probability at least @xmath64 @xmath543 & & \\qquad\\le 2 \\biggl({\\mathbb{e}}\\bigl[{\\sup_{g\\in{\\mathcal g}(\\delta_- , \\delta_+,r ) } }    & & \\qquad\\quad\\hspace*{9.3pt}{}+\\|\\ell\\circ g-\\ell\\circ f\\|_{l_2(p ) } \\sqrt{t\\over n}+\\|\\ell\\circ g-\\ell\\circ f\\|_{l_{\\infty}}{t\\over n } \\biggr).\\end{aligned}\\ ] ] now note that @xmath544 & \\le & l_{\\ast}\\sum_{j=1}^n\\|g_j - f_j\\|_{l_2(\\pi)}\\\\[-3pt ] & \\le & l_{\\ast } \\bigl(\\min_j\\bar{\\epsilon}_j \\bigr)^{-1 } \\sum_{j=1}^n\\bar{\\epsilon}_j\\|g_j - f_j\\|_{l_2(\\pi)},\\end{aligned}\\ ] ] where we used the fact that the lipschitz constant of the loss @xmath205 on the range of functions from @xmath545 is bounded by @xmath214 .",
    "together with the fact that @xmath546 for all @xmath158 , this yields @xmath547 furthermore , @xmath548 & \\le & l_{\\ast}\\sum_{j=1}^n \\|g_j - f_j\\|_{\\mathcal{h}_j}\\\\[-3pt ] & \\le & l_{\\ast } \\frac{n}{a\\log n}\\delta_{+}.\\end{aligned}\\ ] ] in summary , we have @xmath549 & & \\qquad\\le 2 \\biggl({\\mathbb{e}}\\bigl[{\\sup_{g\\in{\\mathcal g}(\\delta_- , \\delta_+,r ) } }    \\bigr]\\\\[-3pt ] & & \\qquad\\quad\\hspace*{27pt}{}+ l_{\\ast}\\delta_-\\sqrt{t\\over a\\log n}+l_{\\ast}\\delta _ + { t\\over n}{n\\over a\\log n } \\biggr).\\end{aligned}\\ ] ] now , by symmetrization inequality , @xmath550\\nonumber\\\\[-8pt]\\\\[-8pt ] & & \\qquad\\le{2{\\mathbb{e}}\\sup_{g\\in{\\mathcal g}(\\delta _ - , \\delta_+,r ) } } |r_n(\\ell\\circ g-\\ell\\circ f ) |.\\nonumber\\end{aligned}\\ ] ] an application of rademacher contraction inequality further yields @xmath551\\nonumber\\\\[-8pt]\\\\[-8pt ] & & \\qquad\\le cl_{\\ast}{\\mathbb{e}}\\sup_{g\\in{\\mathcal g}(\\delta_- , \\delta_+,r ) }    numerical constant [ again , it was used here that the lipschitz constant of the loss @xmath205 on the range of functions from @xmath545 is bounded by @xmath214 ] . applying talagrand s concentration inequality another time",
    ", we get that with probability at least @xmath64 @xmath553 for some numerical constant @xmath63 .    recalling the definition of @xmath554",
    ", we get @xmath555 hence , with probability at least @xmath556 and with some numerical constant @xmath63 @xmath557    using ( [ conclusion_b ] ) , @xmath558 can be upper bounded by @xmath559 with some numerical constant @xmath30 on an event @xmath456 of probability at least @xmath434 .",
    "therefore , the following bound is obtained : @xmath560 it holds on the event @xmath561 , where @xmath562 .",
    "we will now choose @xmath563 and obtain a bound that holds uniformly over @xmath564 to this end , consider @xmath565 for any @xmath566 and @xmath567 satisfying ( [ eq : bddel ] ) , we have @xmath568 on the event @xmath569 .",
    "therefore , simultaneously for all @xmath566 and @xmath567 satisfying ( [ eq : bddel ] ) , we have @xmath570 on the event @xmath571 .",
    "the last intersection is over all @xmath572 such that conditions ( [ eq : bddel ] ) hold for @xmath573 .",
    "the number of the events in this intersection is bounded by @xmath574 .",
    "therefore , @xmath575 using monotonicity of the functions of @xmath576 involved in the inequalities , the bounds can be extended to the whole range of values of @xmath576 satisfying ( [ eq : bddel ] ) , so , with probability at least @xmath472 we have for all such @xmath576 @xmath577 if @xmath578 , or @xmath579 , it follows by monotonicity of the left - hand side that with the same probability @xmath580 which completes the proof .",
    "the authors are thankful to the referees for a number of helpful suggestions .",
    "the first author is thankful to evarist gin for useful conversations about the paper .",
    "koltchinskii , v. ( 2008 ) .",
    "oracle inequalities in empirical risk minimization and sparse recovery problems .",
    "ecole det de probabilits de saint - flour , lecture notes .",
    "koltchinskii , v. ( 2009a ) .",
    "sparsity in penalized empirical risk minimization .",
    "h. poincar probab .",
    "statist . _",
    "* 45 * 757 .",
    "raskutti , g. , wainwright , m. and yu , b. ( 2009 ) .",
    "lower bounds on minimax rates for nonparametric regression with additive sparsity and smoothness . in _ advances in neural information processing systems _ ( nips 22 ) 15631570 .",
    "curran associates , red hook , ny .",
    "ravikumar , p. , liu , h. , lafferty , j. and wasserman , l. ( 2008 ) .",
    "spam : sparse additive models . in _ advances in neural information processing systems _",
    "( nips 20 ) 12011208 .",
    "curran associates , red hook , ny ."
  ],
  "abstract_text": [
    "<S> the problem of multiple kernel learning based on penalized empirical risk minimization is discussed . </S>",
    "<S> the complexity penalty is determined jointly by the empirical @xmath0 norms and the reproducing kernel hilbert space ( rkhs ) norms induced by the kernels with a data - driven choice of regularization parameters . </S>",
    "<S> the main focus is on the case when the total number of kernels is large , but only a relatively small number of them is needed to represent the target function , so that the problem is sparse . </S>",
    "<S> the goal is to establish oracle inequalities for the excess risk of the resulting prediction rule showing that the method is adaptive both to the unknown design distribution and to the sparsity of the problem .    and    .    </S>"
  ]
}