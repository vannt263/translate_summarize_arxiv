{
  "article_text": [
    "image restoration and reconstruction play important roles in medical and astronomical imaging , image and video coding , file restoration , and many other applications .",
    "let @xmath1 be an original @xmath2 image , @xmath3 be a linear operator , and @xmath4 be an observation which satisfies the relationship @xmath5 where @xmath6 represents a noise contamination or corruption procedure .",
    "given @xmath7 , image restoration and reconstruction extract @xmath8 from @xmath9 , which is either under - determined ( @xmath10 ) or ill - possed ( e.g. , deconvolution / deblurring ) , making classical least - squares approximation alone not suitable .",
    "to stabilize recovery , regularization technique is frequently used , giving a general reconstruction model of the form @xmath11 where @xmath12 promotes solution regularity such as smoothness and sparseness , @xmath13 fits the observed data by penalizing the difference between @xmath14 and @xmath9 , and @xmath15 balances the two terms for minimization .",
    "the choice of @xmath16 depends on different noise , e.g. , the squared @xmath17 penalty is usually used for gaussian additive noise , while the @xmath18 penalty is more appropriate for certain non - gaussian noise , e.g. , salt - and - pepper noise . throughout this paper , we assume that @xmath6 represents an additive gaussian noise contamination and thus set @xmath19",
    ". among other regularization , total variation ( tv ) has been popular ever since its introduction by rubin , osher and fatemi @xcite .",
    "the remarkable property of tv is to preserve edges due to its linear penalty on differences between adjacent pixels .",
    "the most widely studied tv model for image deconvolution ( in which case @xmath20 is a convolution matrix ) is @xmath21 where @xmath22 denotes the local finite difference operator ( with ceratin boundary conditions ) at pixel @xmath23 , and @xmath24 is a discretization of the tv of @xmath25 . in this paper , we propose , analyze and test a fast alternating minimization algorithm for solving , in which @xmath7 is a compressive sensing encoding matrix ( @xmath26 ) and does not have structures .    in the following of this section",
    ", we review briefly compressive sensing ideas and algorithms , which provide theoretical guarantee for image reconstruction via solving , examine some existing algorithms for relevant tv problems , and describe the contributions and organization of this paper . throughout this paper , we refer to as tv / l@xmath27 .",
    "compressive sensing ( cs ) is an emerging methodology in digital signal processing brought to the research forefront by donoho @xcite , cand ` es , romberg and tao @xcite , and has attracted intensive research activities in the past few years . in a nutshell , cs first encodes a sparse signal ( possibly under certain sparsifying basis ) through hardware devices into a relatively small number of linear projections and then reconstructs it from the limited measurements .",
    "let @xmath28 be the sparse signal that we wish to capture , i.e. , the number of nonzeros in @xmath29 is much less than its length @xmath30 , and @xmath31 represent a set of @xmath32 ( usually much smaller than @xmath30 ) linear projections of @xmath33 . under certain desirable conditions",
    ", it is shown that with high probability the basis pursuit problem @xmath34 yields the sparsest solution of the linear system @xmath35 , see @xcite .",
    "more often than not , @xmath36 contains noise , in which case certain relaxation is desirable . for white gaussian noise ,",
    "the most widely used models are the basis pursuit denoising problem @xmath37 where , roughly speaking , @xmath15 is inversely proportional to the noise level , and its variants . since most signals of interests are sparse or nearly sparse ( called compressible ) under certain basis , the cs idea has extremely wide applications .",
    "recent results show that stable reconstruction can be obtained provided that @xmath7 possesses certain randomness .",
    "it has been clear from @xcite that for almost all random matrices the exact recoverability is approximatively identical .",
    "moreover , exact recoverability is attainable when @xmath7 contains randomly taken rows from orthonormal matrices , e.g. , partial fourier which arises from magnetic resonance imaging @xcite .    in the application of cs ,",
    "matrix @xmath7 is large and dense .",
    "furthermore , in certain applications @xmath7 contains structures that allow fast matrix - vector multiplication , e.g. , @xmath7 is a partial fourier matrix as in mri .",
    "these features make traditional powerful optimization approaches such as interior point methods not suitable . in comparison , first - order algorithms that depend on",
    "merely matrix - vector multiplications are more desirable .",
    "therefore , in the last few years numerous algorithms have been proposed for recovering sparse signals via solving certain @xmath18-norm regularized problems including , and thier variants .",
    "several well - known approaches in this area include the gradient projection method @xcite , the fixed - point continuation method @xcite , the spectral projected gradient method @xcite , and the bregman iterative method @xcite .",
    "more recent algorithms can be found in @xcite .",
    "the advantage of tv regularization compared with tikhonov - like @xcite regularization in recovering high quality image is not without a price .",
    "the nondifferentiability of tv causes the main difficulty .",
    "in addition , problems arising from signal and image reconstruction are usually large scale and ill - possed , which further make tv models difficult to be solved efficiently .",
    "since the introduction of tv regularization , many algorithms have been proposed for solving and its variants . in the pioneer work @xcite ,",
    "a time - marching scheme was used to solve a partial differential equation system , which in optimization point of view is equivalent to a constant step - length gradient descent method .",
    "this time - marching scheme suffers slow convergence especially when the iterate point approaches the solution set .",
    "another well - known method is the linearized gradient method proposed in @xcite for denoising and in @xcite for deblurring , which solves the euler - lagrangian equation via a fixed - point iteration . at each iteration of the linearized gradient method ,",
    "a linear system needs to be solved , which makes the per - iteration cost extremely expensive especially when the problem becomes more ill - conditioned . to overcome the linear convergence of first - order methods , the authors of @xcite incorporated newton method to solve ( [ tv ] ) , which achieved superlinear convergence at the cost of solving a large linear system at each iteration .",
    "another important approach for tv problems is the iterative shrinkage / thresholding ( ist ) method @xcite . in @xcite",
    ", bioucas - dias and figueiredo introduced a two - step ist ( twist ) algorithm , which exhibits much faster convergence than the primary ist algorithm for ill - conditioned problems .",
    "we note that ist - based algorithms require to solve a tv denoising subproblem at each iteration which requires its own iterations .    despite the progress",
    "have been achieved , algorithms for solving are still much slower than those for tikhonov regularization problems .",
    "recently , a fast tv deconvolution ( ftvd ) method is proposed in @xcite , which makes full use of problem structures ( both @xmath7 and finite difference operators have circulant structures under proper boundary conditions ) and thus converges very fast .",
    "ftvd solves a penalty approximation of , that is @xmath38 where , for each @xmath23 , @xmath39 is an auxiliary variable and @xmath40 is a penalty parameter .",
    "the advantage of considering is that it leads to fast and efficient alternating minimizations for deconvolution problems .",
    "the numerical results given in @xcite indicates that ftvd is much faster than the lagged diffusivity method in @xcite , which is known to be efficient previously . for more details on the ftvd algorithm and its performance ,",
    "see @xcite .",
    "given the practical efficiency of ftvd , this split and penalty idea has been extended to multichannel image restoration in @xcite , impulsive noise elimination in @xcite and medical reconstruction from partial fourier coefficients in @xcite .",
    "more algorithms for tv / l@xmath27 problem can be found in @xcite and references therein .",
    "the purpose of this paper is to develop a fast algorithm for solving , where @xmath7 is a general linear operator .",
    "specifically , we are interested in compressive sensing encoding matrices in which case @xmath7 contains smaller or even much smaller number of rows than columns .",
    "as is stated above , problem admits fast alternating minimization when @xmath7 is a convolution matrix . as a matter of fact",
    ", the minimization of with respect to @xmath41 , @xmath42 , reduces to @xmath43 two - dimensional problems ( no matter what @xmath7 is ) , which can be solved easily and exactly in linear time .",
    "however , different from deconvolution problems , @xmath7 does not have structures in our stated case .",
    "consequently , the solution of @xmath25-subproblems can not utilize any fast transforms .    in this paper",
    ", we first introduce a fast alternating minimization scheme for solving , which recurs to linearization and proximal techniques when solving the @xmath25-subproblems . under quite reasonable technical assumptions , we show that the proposed algorithm converges globally to a solution of .",
    "moreover , we establish @xmath0-linear convergence results which indicate that the @xmath0-linear factor depends on the spectral radius of certain submatrix .",
    "clearly , the solution of well approximates that of ( [ tv ] ) only when @xmath44 is sufficiently large , which causes numerical difficulties in computation . to overcome this drawback",
    ", we introduce an inexact alternating direction method , which accelerates the convergence of the alternating minimization approach and converges to a solution of without driving @xmath44 to infinity . since the proposed algorithms solve and with a cs encoding matrix , we name the resulting algorithms ftvcs .",
    "we present experimental results and compare with twist @xcite .",
    "the comparison results indicate that ftvcs is fast and efficient and performs comparable with the state - of - the art algorithm twist .",
    "now , we define our notation . for scalars @xmath45 , vectors @xmath46 , and matrices @xmath47 of appropriate sizes , @xmath48 ,",
    "we let @xmath49 , @xmath50 , and @xmath51 .",
    "let @xmath52 and @xmath53 be the two first - order finite difference matrices in horizontal and vertical directions , respectively . as is used before",
    ", @xmath54 is a two - row matrix formed by stacking the @xmath23th row of @xmath52 on that of @xmath53 . throughout this paper , we let @xmath55 , @xmath56 be the spectral radius of matrix @xmath57 , and @xmath58 be the projection operator under euclidean norm . the inner product of two vectors will be denoted by @xmath59 . in the rest of this paper , we let @xmath60 , and without misleading we abbreviate @xmath61 as @xmath62 .",
    "additional notation will be introduced when it occurs .",
    "the paper is organized as follows . in section [ ftvcs ]",
    ", we introduce our alternating minimization algorithm ftvcs and study its convergence properties .",
    "an accelerated scheme of ftvcs is proposed in section [ iadm ] by incorporating an inexact alternating direction technique .",
    "numerical results in comparison with twist are presented in section [ numericalresults ] .",
    "finally , we conclude the paper in section [ concludingremarks ] .",
    "the task of this section is to construct our algorithm for solving .",
    "as is stated above , our interest in this paper concentrates on cs encoding matrices , i.e. , @xmath63 with @xmath64 .",
    "the non - smoothness of tv causes the main difficulty .",
    "similar as in @xcite , we first consider the approximation problem and then propose an inexact alternating direction method for the solution of .",
    "the introduction of auxiliary variables @xmath65 in makes it easy to apply alternating minimization .",
    "it is easy to see that , for fixed @xmath25 , the minimization of ( [ atv ] ) with respect to @xmath66 reduces to the following two - dimensional problems @xmath67 for which the unique minimizers are given by the two - dimensional shrinkage formula @xmath68 where the convention @xmath69 is followed . on the other hand , for fixed @xmath66 ,",
    "the minimization of ( [ atv ] ) with respect to @xmath25 is a least squares problem , and the corresponding normal equations are given by @xmath70 or equivalently , @xmath71 where @xmath72 is an reordering of @xmath41 , @xmath42 .",
    "it is well - known that , under the periodic boundary condition for @xmath25 , @xmath73 is a block - circulant matrix and can be diagonalized by two - dimensional fast fourier transform ( fft ) .",
    "unfortunately , the matrix @xmath74 does not have circulant structures for general cs encoding matrices .",
    "therefore , the exact solution of is expensive , which causes the main difficulty to apply alternating minimization directly .    to avoid solution of linear system of equations at each iteration , we linearize @xmath75 at the current point @xmath76 and add a proximal term , resulting the following approximation problem @xmath77 where @xmath78 denotes the gradient of @xmath75 at @xmath76 , and @xmath79 is a parameter . clearly , problem ( [ line ] ) is equivalent to @xmath80 for fixed @xmath81 ( or @xmath66 ) , the minimization of ( [ eqline ] ) with respect to @xmath25 is equivalent to @xmath82 where we recall that @xmath83 . under the periodic boundary conditions for @xmath25",
    ", the coefficient matrix in can be diagonalized easily by fft . consequently",
    ", the solution of can be accomplished by two ffts ( including one inverse fft ) . to sum up",
    ", our alternating minimization algorithm , named fast total variation decoding from compressive sensing measurements or ftvcs , is described below .",
    "[ ftvdcsalg ] input @xmath9 , @xmath7 and @xmath84 .",
    "initialize @xmath85 and @xmath86 .",
    "( nr)ssjklbbbcccddd= ee `` not converged '' , * do * + 1 ) compute @xmath87 according to ( [ shrink ] ) for fixed @xmath88 .",
    "+ 2 ) compute @xmath89 according to ( [ eqnorm ] ) for fixed @xmath90 .",
    "+ 3 ) @xmath91 .",
    "+    to establish the convergence of ftvcs , we need the following technical assumption .",
    "[ assum1 ] @xmath92 , where @xmath93 represents the null space of a matrix .",
    "assumption [ assum1 ] is a quite loose condition and commonly used in the convergence analyses of similar studies , see e.g. , @xcite . under assumption [ assum1 ]",
    ", we have the following convergence results .    [ conver ] under assumption [ assum1 ] , for any fixed @xmath40 and @xmath94 , where @xmath95 denotes the spectral radius of @xmath96 , the sequence @xmath97 generated by algorithm [ ftvdcsalg ] from any starting point @xmath98 converges to a solution @xmath99 of .",
    "[ linear ] suppose the sequence @xmath97 generated by algorithm [ ftvdcsalg ] converges to @xmath99 .",
    "then , we have @xmath100 , @xmath101 after a finite number of iterations , where @xmath102 .    [ qline ] under the conditions of theorem [ conver ]",
    ", the sequence @xmath103 generated by algorithm [ ftvdcsalg ] converges to @xmath104 @xmath0-linearly .",
    "the proofs of theorems [ conver ] , [ linear ] and [ qline ] are given in appendix [ appendixa ] .",
    "it is well - known that problem well approximates only when @xmath44 is sufficiently large .",
    "however , it is generally difficult to determine theoretically how large a @xmath44 value must be to attain a given accuracy . in this section",
    ", we present an inexact alternating direction method ( adm ) , which converges to a solution of without requiring @xmath44 goes to infinity .",
    "first , we review briefly the idea of adm pioneered in @xcite .",
    "the classical adm is designed to solve the following structure optimization problem : @xmath105 where @xmath106 and @xmath107 are functions , and @xmath108 is a @xmath109 matrix .",
    "given @xmath110 and @xmath111 , the adm iterates as follows @xmath112 where @xmath113 is a parameter . in , @xmath114 is the lagrangian multiplier and @xmath115 severs as a penalty parameter .",
    "it can be shown that , under quite reasonable assumption , converges to a solution of for any fixed @xmath113 , see @xcite .",
    "we now consider the model ( [ tv ] ) in its equivalent form @xmath116 the augmented lagrangian problem of is given by @xmath117 where , for each @xmath23 , @xmath118 is the lagrangian multiplier attached to @xmath119 . inspired by the adm iterations , for given @xmath120 , we obtain the next triplet @xmath121 as follows .",
    "first , for fixed @xmath76 and @xmath122 , the minimization of with respect to @xmath65 is equivalent to @xmath123 the solutions of which are given by @xmath124 second , for fixed @xmath87 , @xmath76 and @xmath122 , the minimization of with respect to @xmath25 is approximated by linearizing @xmath75 and adding a proximal term as in , resulting the following problem @xmath125 where @xmath126 is defined in ( [ line ] ) .",
    "it is easy to show that the normal equations of are of the form @xmath127 under the periodic boundary conditions , the exact solution of can be attained by two ffts .",
    "finally , @xmath128 is updated via @xmath129 we note that the linearization technique makes the @xmath25-subproblem of is solved inexactly .",
    "therefore , we name the above iterative framework as an inexact adm or iadm , which is summarized below .",
    "[ iadmalg ] input @xmath9 , @xmath7 and @xmath84 .",
    "initialize @xmath85 and @xmath86 .",
    "( nr)ssjklbbbcccddd= ee `` not converged '' , * do * + 1 ) compute @xmath87 according to ( [ iadmw ] ) for fixed @xmath130 and @xmath88 .",
    "+ 2 ) compute @xmath89 according to ( [ eqnorm11 ] ) for fixed @xmath130 and @xmath90 . +",
    "3 ) update @xmath128 via and set @xmath91 .",
    "+    we have the following convergence results for algorithm [ iadmalg ] .",
    "[ augconv ] under assumption [ assum1 ] , the sequence @xmath97 generated by algorithm [ iadmalg ] from any starting point @xmath98 converges to a solution of .",
    "a closer examination shows that algorithm [ iadmalg ] is related to the proximal adm of he et al . @xcite for solving monotone variational inequalities .",
    "hence , the global convergence is followed directly , see appendix [ appendixb ] for details .",
    "in this section , we present numerical results to illustrate the feasibility and efficiency of ftvcs and its accelerated variant iamd .",
    "all experiments were accomplished in matlab 2009a running on a pc ( intel pentium(r ) 4 , 1.6 ghz , 1.0 gb sdram ) with windows xp operating system . as usual",
    ", we measure the quality of reconstruction by relative error to the original image @xmath8 , i.e. , @xmath131 in the following , we first present primary experimental results to show the feasibility of both algorithms , and then compare both algorithms with twist  a state - of - the - art algorithm for solving , to demonstrate their efficiency .      in the first experiment , we present reconstruction results of both algorithms to illustrate their feasibility for solving .",
    "we used a random matrix with independent identical distributed gaussian entries as cs encoding matrix and tested the shepp - logan phantom image , which has been widely used in simulations for tv models . due to storage limitations , we tested the image size @xmath132 .",
    "the sample ratio in this test is @xmath133 , which are selected uniformly at random . besides , we added gaussian noise of zero mean and standard deviation @xmath134 .",
    "similar as in ftvd @xcite , we implemented ftvcs with a continuation scheme on @xmath44 to speed up convergence .",
    "specifically , we tested the @xmath44-sequence @xmath135 and used the warm - start technique . in iadm ,",
    "the value of @xmath44 is fixed to be @xmath136 . in both algorithms , the weighting parameter @xmath137",
    "was set to be @xmath138 .",
    "both algorithms were terminated when the relative change between successive iterates fell below @xmath139 , i.e. , @xmath140 the original image , the initial guess , and the reconstructed ones by both algorithms are listed in figure [ figure1 ] .    .",
    "[ figure1 ]    as is shown in figure [ figure1 ] , both algorithms perform favorably and produce faithful recovery results in a few seconds .",
    "we note that the per - iteration cost of both algorithms is one shrinkage operation , two matrix - vector multiplications and two ffts .",
    "the results also indicate that the inexact adm approach described in algorithm [ iadmalg ] is indeed more efficient than the penalty approach ftvcs described in algorithm [ ftvdcsalg ] in the sense that better recovery results were obtained in less cup seconds . to closely examine the convergence behavior of both algorithms , we present in figure [ figure2 ] the decreasing of objective function values and relative errors as cpu time proceeded .",
    "it is clear from figure [ figure2 ] that both algorithms generated decreasing sequences of function values . from the right - hand plot ,",
    "iadm achieved a solution of lower relative error . in both plots , the curves of iadm fall",
    "bellow those of ftvcs throughout the whole iteration process .      in this subsection ,",
    "we present extensive numerical results to compare iadm with twist @xcite  a two - step iterative shringkage / thresholding algorithm for solving a class of optimization problems arising from image restoration , reconstruction and linear inverse problems .",
    "specifically , twist is designed to solve @xmath141 where @xmath142 is a general regularizer , which can be either the @xmath18-norm or the tv semi - norm , as well as others . in the comparison ,",
    "we used partial discrete cosine transform ( dct ) matrix as cs encoder , i.e. , the @xmath32 rows of @xmath7 were chosen uniformly at random from the @xmath2 dct matrix .",
    "since the dct matrix is implicity stored as fast transforms , this enables us to test larger images .",
    "we used the default parametric settings for twist and terminated it as the relative change in objective function values fell below @xmath143 .",
    "the parameters in iadm were set as follows : @xmath144 and @xmath145 . to obtain higher quality images",
    ", we used more stringent stopping tolerance and terminated iadm when @xmath146 was satisfied .",
    "we first compared iadm with twist using the shepp - logan phantom benchmark image of size @xmath147 .",
    "we randomly selected @xmath133 dct coefficients and added gaussian noise of mean zero and standard deviation @xmath148 .",
    "table [ table1 ] reports the detailed results of both algorithms for different values of @xmath137 , where re , obj , iter and time represent , respectively , the relative error of the reconstructed image to the original one , the final objective function value , the number of iterations , and the consumed cpu time in seconds .",
    "it can be seen from table [ table1 ] that , for both algorithms , the number of iterations becomes larger and larger as @xmath137 increases , and as a result longer cpu time is consumed . for larger @xmath137 ,",
    "the performance of both algorithms deteriorates , while the resulting relative errors were not improved .",
    "for @xmath137 between @xmath149 and @xmath150 , iadm always obtained comparable or higher recovery quality than twist . for @xmath137 between @xmath149 and @xmath151 , iadm is also faster than twist . in terms of final function values ,",
    "iadm obtained slightly smaller ones than those of twist .",
    "[ table1 ]    besides the shepp - logan phantom image , we also tested cameraman , lena , boat , sailboat , as well as two brain images . in this experiment",
    ", we simply set @xmath152 and keep all other parameters unchanged .",
    "the original and the recovered images by twist and iadm are given in figures [ figure4 ] and [ figure5 ] , and detailed results including relative errors ( re ) , cpu time ( time ) , final objective function values ( obj ) , and the number of iterations ( iter ) are presented in table [ table2 ] .",
    "it can be seen from table [ table2 ] that iadm attained comparable or better image quality in less cpu seconds . for each test , iadm consumed more iterations while the cpu time is less because the per - iteration cost of iadm is much less than that of twist .",
    "specifically , the per - iteration cost of iadm contains two matrix - vector multiplications and two ffts , while twist needs to solve a tv denoising problem at each iteration .",
    "in addition , iadm always attained smaller function values .",
    "in summary , the comparison results indicate that iamd performs favorably and can be competitive with the state - of - the - art algorithm twist .",
    "+   +   +   +     +   +",
    "in this paper , we proposed a fast alternating minimization algorithm for total variation image reconstruction from compressive sensing data ( ftvcs ) .",
    "the per - iteration cost of ftvcs includes a linear time shrinkage operation , two matrix - vector multiplications and two ffts . to overcome the difficulty caused by large penalty parameter in ftvcs",
    ", we have also developed an inexact alternating direction method ( iadm ) based on linearization and proximal techniques .",
    "our experimental results indicate that iadm indeed performs better than ftvcs and is comparable with the state - of - the - art algorithm twist for solving tv reconstruction models .",
    "given the promising performance of iadm and the wide applications of tv models , we believe that it is worthwhile to further accelerate iadm via certain line search strategy . in both ftvcs and iadm",
    ", we used a linearization technique and ffts to obtain a new point .",
    "a possible improvement is to solve the @xmath25-subproblem of by using certain gradient methods , e.g. , gradient descent method with bb steplengths @xcite and non - monotone line search .",
    "this should be interesting for further investigations .",
    "10    , _ two point step size gradient method _ , i m a j. numer .",
    ", 8 ( 1988 ) , 141 - 148 .    , _ a fast iterative shrinkage - thresholding algorithm for linear inverse problems _ , siam j. imaging sci .",
    ", 2(2009 ) , 183 - 202 .    , _ a new twist : two - step iterative thresholding algorithm for image restoration _ , ieee trans .",
    "precess . , 16 ( 2007 )",
    ", 2992 - 3004 .",
    ", _ linear bregman iterations for compressed sensing _",
    "comput . , 78(2009 ) , 15151536 . , _ convergence of the linearized bregman iteration for @xmath18-norm minimization _ ,",
    "comput . , 78(2009 ) , 21272136 .    , _ stable signal recovery from imcomplete and inaccurate information _ , communications on pure and applied math .",
    ", 59 ( 2005 ) , 1207 - 1233 .",
    ", _ robust uncertainty principles : exact signal reconstruction from highly incomplete frequence information _ , ieee trans .",
    "theory , 52 ( 2006 ) , 489 - 509 .    , _ proximal thresholding algorithm for minimization over orthonormal bases _ , siam j. optim . , 18 ( 2008 ) , 1351 - 1376 .    , _ an optimization - based multilevel algorithm for tatal variation image denoising _ , multiscal model .",
    "simul . , 5 ( 2006 ) , 615 - 645 .    ,",
    "_ a nonlinear diffusivity fixed point method in total variation based image restoration _ , siam j. sci .",
    "20 ( 1999 ) , 1964 - 1977 .",
    ", _ compressed sensing _ , ieee trans .",
    "theory , 52 ( 2006 ) , 1289 - 1306 .    ,",
    "_ for most large underdetemind systems of linear equations , the minimal l1-norm solution is also the sparsest solution _ , communications on pure and applied mathematics , 59 ( 2006 ) , 907 - 934 .",
    ", _ why simple shrinkage is still relevant for redundant representations ? _ , ieee transactions on information theory , 52 ( 2006 ) , pp . 55595569 .    ,",
    "_ image denoising with shrinkage and redundant representations _",
    ", in proc .",
    "ieee computer society conference on computer vision and pattern recognition , new york , 2006 .",
    ", _ gradient projection for sparse reconstruction , application to compressed sensing and other inverse problems _ , ieee journal of selected topics in signal processing : special issue on convex optimization methods for signal processing , 1 ( 2007 ) , 586 - 598 .    , _ a dual algorithm for the solution of nonliear variational problems via finite element approximation _ , computer math .",
    ", 2 ( 1976 ) , 17 - 40 .",
    ", _ augmented lagrangian and operator - splitting methods _ , in : nonlinear mechanice , siam studies in applied mathematics , philadephia , pa , 1989 .    , _ numerical methods for nonlinear variational problems _ , springer - verlat , new york , 1984 .    , _ a fixed - point continuation method for l1-regularized minimization with applications to compressed sensing _ , siam j. optim . ,",
    "19 ( 2008 ) , 1107 - 1130 .    ,",
    "_ a new inexact alteratin directions method for monotone variational inequalities _ ,",
    "math . program .",
    ", 92 ( 2002 ) , 103 - 118 .    ,",
    "_ a fast total variation minimization method for image restoration _",
    ", multiscale model .",
    ", 7 ( 2008 ) , 775 - 795 .",
    ", _ sparse mri : the application of compressed sensing for rapid mr imaging _ , magnetic resonance in medicine , 58 ( 2007 ) , 1182 - 1195 .    , _ on semismooth newton s methods for total variation minimization _ , j. math .",
    "imaging vision , 27 ( 2007 ) , 265 - 276 .    , _ an iterated regularization method fro total variation - based image restoration _ , multiscale model .",
    "simul . , 4 ( 2005 ) , 460 - 489 .    , _ fast linearized bregman iteration for compressed",
    "sensing and sparse denoising _ , tr08 - 07 , caam , rice university .    ,",
    "_ handbook of mathematical models in computer vision _ , springer , new york , 2006 .    , _ nonlinear total variation based noise removal algorithms _ , phys .",
    "d , 60 ( 1992 ) , 259 - 268 .    ,",
    "_ wavelets and curvelets for image deconvolution : a combined approach _ , signal processing , 83 ( 2003 ) , pp .",
    "22792283 .    , _ solution of ill - posed problems _ , winston , washington , dc , 1977 .    , _ probing the pareto frontier for basis pursuit solutions _ , siam j. sci .",
    "comput . , 31 ( 2008 ) , 890 - 912 .    , _ iterative methods for total variation denoising _ , siam j. sci .",
    "comput . , 17 ( 1996 ) ,",
    "pp.227238 .",
    " , _ a fast , robust total variation based reconstruction of noisy , blurred images _ , ieee trans .",
    "image process .",
    ", 7 ( 1998 ) , pp .",
    "813824 .    , _ a new alternating minimization algorithm for total variation image reconstruction _ , siam j. imaging sci .",
    ", 1 ( 2008 ) , 248 - 272 .    , _ iterative algorithms based on decoupling of deblurring and denoising for image restoration _ , siam j. sci .",
    "comput . , 30 ( 2008 ) , 2655 - 2674 .    , _ a fast algorithm for sparse rescontruction based on shringkage : subspace optimization and continuations _ , tr09 - 01 , caam , rice university .    ,",
    "_ a fast algorithm for edge - preserving variational multichannel image restoration _ , siam j. imaging sci . , 2 ( 2009 ) , 569 - 592 .    , _ alternating direction algorithms for @xmath18-problems in compressive",
    "sensing _ , tr09 - 37 , caam , rice university .    , _ an efficient tvl1 algorithm for deblurring multichannel images corrupted by impulsive noise _ , siam j. sci . comput .",
    ", 31 ( 2009 ) , 2842 - 2865 .    , _ a fast tvl1-l2 minimization algorithm for signal reconstruction from partial fourier data _ , ieee j. special topics signal processing , to appear .    , _ bregman iterative algorithms for l1-minimization with applications to compressed sensing _ , siam j. imaging sci .",
    ", 1 ( 2008 ) , 143 - 168 .    , _ on the theory of compressed sensing via @xmath18-minimization : simple derivations and extensions _ ,",
    "tr08 - 11 , caam , rice university .",
    "the purpose of this appendix is to establish convergence properties of algorithm [ ftvdcsalg ] for a fixed @xmath40 . for convenience , we define some notation . for fixed @xmath40 , the 2-dimensional ( 2d ) shrinkage operator @xmath153 is defined as @xmath154 where @xmath155 is the projection onto the closed disc @xmath156 , and the convention @xmath69 is followed . for vectors",
    "@xmath157 , @xmath158 , we define @xmath159 by @xmath160",
    "i.e. , @xmath161 applies 2d shrinkage to each pair @xmath162 , for @xmath163 . from the definition of @xmath164 , it is easy to see that ( [ fixu ] ) or ( [ shrink ] ) can be rewritten as @xmath165 .",
    "the following result shows that the operator @xmath166 is non - expansive .",
    "[ nonexp ] for any @xmath167 , it holds that @xmath168 furthermore , if @xmath169 , then @xmath170 .",
    "since the objective function in ( [ atv ] ) is convex , bounded below , and coercive ( i.e. , its goes to infinity as @xmath171 , it has at least one minimizer @xmath99 that can not be decreased by the alternating minimization scheme - and thus must satisfy @xmath172 by using the shrinkage operator , we can rewrite the iteration of algorithm [ ftvdcsalg ] as @xmath173    in the following , we show that ( [ uk1 ] ) converges to .",
    "the following matrices will be used in our analysis : @xmath174 assumption [ assum1 ] ensures the non - singularity of @xmath175 , while @xmath176 is always well defined under the circumstance .",
    "simple manipulation shows that @xmath177 , where @xmath178 . with these definitions ,",
    "( [ solution ] ) and ( [ uk1 ] ) can be , respectively , simplified as @xmath179 to further simplify the above equations , we define @xmath180 hence , the solution and iteration systems can be , respectively , rewritten as @xmath181 and @xmath182 where ",
    "@xmath183 \" denotes operator composition .",
    "furthermore , we define @xmath184 then ( [ solutionrew11 ] ) and ( [ uk1rew22 ] ) become @xmath185    [ lem41 ] @xmath186 is non - expansive .    given @xmath187 and @xmath188 , it holds that @xmath189 where ",
    "@xmath190 \" comes from the non - expansive of @xmath164 and @xmath191 it is easy to verify that @xmath192 recall that we require @xmath193 , which ensures the positive semi - definiteness of @xmath194 .",
    "therefore , @xmath195 which shows that @xmath186 is non - expansive .",
    "[ lem42 ] equality holds in ( [ nonequ ] ) if and only if @xmath196    we note that in the proof of lemma [ lem41 ] there exist three  @xmath190 \" .",
    "thus , equality holds in only when all the three inequalities become  = \" . for simplicity ,",
    "we let @xmath197 and @xmath198 .    1 .",
    "the first  @xmath190 \" becomes  @xmath199 \" if and only if @xmath200 2 .",
    "the second  @xmath190 \" becomes  @xmath199 \" if and only if @xmath201 3 .",
    "let @xmath202 be orthonormal and @xmath203 be its eigenvalue decomposition .",
    "the third  @xmath190 \" becomes  @xmath199 \" if and only if @xmath204 since @xmath205 , the above equality holds only when @xmath206 therefore , @xmath207 and thus @xmath208    from 1 and ( [ eq41 ] ) , we have @xmath209 from ( [ eq41 ] ) , the equality in 2 is equivalent to @xmath210 let @xmath211 be the eigenvalue decomposition of @xmath74 .",
    "the above equation is equivalent to @xmath212 since @xmath213 , we have @xmath214 if @xmath215 , then from the choice of @xmath216 we have @xmath217 , and thus @xmath218 . therefore , @xmath219 and @xmath220 sum the above discussions up , we have @xmath221 where the first equality is from the definition of @xmath222 ; the second one is from ( [ eq42 ] ) , the definition of @xmath223 and ( [ eq41 ] ) ; the third one is from the definition of @xmath57 ; and the final one is from @xmath224 .",
    "this completes the proof .",
    "suppose @xmath225 is a fixed point of @xmath0 , i.e. , @xmath226 .",
    "then for any @xmath227 it holds @xmath228 unless @xmath227 is also a fixed point of @xmath222 .    based on the above lemmas , now we are ready to give the proofs of theorems [ conver ] , [ linear ] and [ qline ] .",
    "( theorem [ linear ] ) first , the convergence of @xmath229 to @xmath99 can be established using exactly the same arguments as in theorem 3.4 in @xcite .",
    "the convergence of @xmath76 to @xmath230 follows from the convergence of @xmath231 to @xmath232 and @xmath233 to @xmath234 .",
    "therefore , we omit the details .    for any @xmath23 , we let @xmath235 , @xmath236 , where we recall that @xmath237 , and @xmath238    ( theorem [ linear ] ) from the non - expansive of @xmath164 , for each @xmath23 , it holds @xmath239 suppose that at iteration @xmath240 there exist at least one index @xmath241 such that @xmath242",
    ". then @xmath243 , @xmath244 , and @xmath245 .",
    "therefore , @xmath246",
    "^ 2\\nonumber\\\\ & \\leq & \\|h_i(w^k;v^k)-h_i(w^*;v^*)\\|^2-(1/\\beta-\\|h_i(w^*;v^*)\\|)^2\\nonumber\\\\ & \\leq & \\|h_i(w^k;v^k)-h_i(w^*;v^*)\\|^2-\\omega^2,\\nonumber\\end{aligned}\\ ] ] where the first  @xmath190 \" is the triangular inequality , the second one follows from the fact that @xmath247 , and the last one used the definition of @xmath248 in ( [ eq51 ] ) . combining with ( [ eq51 ] ) and ( [ eq53 ] ) , we obtain @xmath249 where the second  @xmath190 \" comes from the non - expansiveness of @xmath250 which can be easily derived .",
    "therefore , the number of iterations @xmath240 with @xmath251 does not exceed @xmath252 this completes the proof of theorem [ linear ] .",
    "( theorem [ qline ] ) from the iteration formulae for @xmath25 and @xmath227 , there holds @xmath253 and @xmath254 considering the finite convergence of @xmath255 , @xmath256 , we have @xmath257 where @xmath258 is a sub - matrix of @xmath259 formed by throwing away certain rows ( with indexes @xmath260 ) and corresponding columns . multiplying @xmath261 to the recursion of @xmath262 , we get @xmath263 which shows that @xmath103 converges q - linearly .",
    "in this section , we clarify the relationship between algorithm [ iadmalg ] and the proximal adm approach proposed in @xcite .",
    "the convergence of algorithm [ iadmalg ] follows directly .",
    "we briefly review the proximal adm approach in @xcite for structured variational inequality ( svi ) problems .",
    "let @xmath175 and @xmath264 be , respectively , @xmath265 and @xmath266 matrixes , @xmath267 and @xmath268 be nonempty closed convex sets , and @xmath269 be given monotone operators .",
    "the svi problem is to find @xmath270 such that @xmath271 where @xmath272 , @xmath273 given @xmath274 , the proximal adm proposed in @xcite iterates as follows    1 .",
    "compute @xmath275 via solving @xmath276+r_k(x - x^k)\\right\\}\\geq 0 , \\ \\",
    "x'\\in \\mathcal{x}.\\ ] ] 2 .",
    "compute @xmath277 via solving @xmath278+s_k(y - y^k)\\right\\}\\geq 0 , \\ \\",
    "y'\\in \\mathcal{y}.\\ ] ] 3 .",
    "update @xmath279 via @xmath280    where @xmath281 is a parameter , @xmath282 and @xmath283 are symmetric positive semidefinite matrices . under mild assumptions , global convergence of this proximal adm approach",
    "was established in @xcite .",
    "simple manipulation shows that algorithm [ iadmalg ] is a special case of the proximal adm approach described above by setting @xmath284 in ( [ sub1 ] ) , i.e. , the the @xmath81-subproblems are solved exactly in algorithm [ iadmalg ] , and @xmath285 in .",
    "hence , the global convergence of algorithm [ iadmalg ] to a solution of follows from ( * ? ? ?",
    "* theorem 4 ) ."
  ],
  "abstract_text": [
    "<S> total variation ( tv ) regularization is popular in image restoration and reconstruction due to its ability to preserve image edges . to date , most research activities on tv models concentrate on image restoration from blurry and noisy observations , while discussions on image reconstruction from random projections are relatively fewer . in this paper , we propose , analyze , and test a fast alternating minimization algorithm for image reconstruction from random projections via solving a tv regularized least - squares problem . </S>",
    "<S> the per - iteration cost of the proposed algorithm involves a linear time shrinkage operation , two matrix - vector multiplications and two fast fourier transforms . </S>",
    "<S> convergence , certain finite convergence and @xmath0-linear convergence results are established , which indicate that the asymptotic convergence speed of the proposed algorithm depends on the spectral radii of certain submatrix . </S>",
    "<S> moreover , to speed up convergence and enhance robustness , we suggest an accelerated scheme based on an inexact alternating direction method . </S>",
    "<S> we present experimental results to compare with an existing algorithm , which indicate that the proposed algorithm is stable , efficient and competitive with twist @xcite  a state - of - the art algorithm for solving tv regularization problems .    </S>",
    "<S> total variation , image restoration , image reconstruction , compressive sensing , alternating direction method    68u10 , 65j22 , 65k10 , 65t50 , 90c25 </S>"
  ]
}