{
  "article_text": [
    "this paper focuses on estimating the expectation @xmath1 of a test function @xmath2 against a distribution @xmath3 of interest , motivated by challenging settings in which ( i ) the variance @xmath4 is large relative to the number @xmath0 of test function evaluations , and ( ii ) the distribution @xmath3 is only available up to an unknown normalisation constant .",
    "such problems arise in bayesian statistics when the cost of sampling from the posterior is prohibitive , requiring that integrals be approximated based on a small number @xmath0 of function evaluations .",
    "techniques such as quasi monte carlo @xcite provide guidance on the choice of evaluation points and , in many cases , guarantee rate - optimal estimation ; however these methods require that @xmath3 is given explicitly .",
    "on the other hand , markov chain monte carlo methods @xcite are applicable to un - normalised distributions , but their intrinsic accuracy of @xmath5 can lead to unacceptably high integration error when @xmath0 is small . @xcite introduced a novel approach to this problem that can provide the `` best of both worlds '' , in the sense of being compatible with un - normalised densities while delivering @xmath6 estimation error .",
    "this was achieved by constructing estimators based on stein s famous identity @xmath7 which holds under suitable boundary conditions , where @xmath8 is a density for @xmath3 with respect to an appropriate reference measure .",
    "[ stein ] was originally presented in the context of goodness - of - fit testing for the normal distribution @xcite . since then , several statistical approaches have been proposed to assess how well one distribution approximates another , based on eqn . [ stein ] @xcite .",
    "an up - to - date and comprehensive account on these approaches is provided in @xcite .",
    "a key motivation for our interest in stein s identity is that the requirement of gradient information on the sampling distribution is not restrictive ; it can be obtained in the presence of unknown normalising constants and is in fact a pre - requisite for many modern langevin and hamiltonian markov chain monte carlo schemes ( e.g. * ? ? ?",
    "@xcite have developed sophisticated software for automatic differentiation of statistical models , that circumvents hand - calculation on a per - model basis and offers considerable opportunity to exploit gradient information in contemporary statistics .    * novel contributions : * the primary contribution of this paper is to establish convergence rates for a class of estimators that are based on stein s identity .",
    "our central results are explicit error rates ; these enable us to compare and quantify the improvement in estimator precision relative to standard markov chain monte carlo estimators",
    ". we will elaborate on the specific form of our theoretical results below .",
    "a secondary contribution comes from our analytical approach which appears , to the best of our knowledge , to represent a novel connection between scattered data approximation @xcite and stability theory for markov chains @xcite .",
    "we anticipate that our results will generate interest among researchers in both communities ; indeed , the technical results that we obtain herein are currently being applied to study the asymptotic properties of `` probabilistic integration '' @xcite .",
    "* related work : * a wider literature on variance reduction methods is reviewed in @xcite ; here we focus on variance reduction via control variates .",
    "classically , control variate methods proceeds by seeking a collection of non - trivial statistics @xmath9 , @xmath10 , that satisfy @xmath11 .",
    "then a surrogate function @xmath12 is constructed such that automatically @xmath13 and , for suitably chosen @xmath14 , a variance reduction @xmath15 is obtained .",
    "an optimal choice of coefficients @xmath16 can often be made using least - squares ; for further details see e.g. @xcite . for specific problems",
    "it is sometimes possible to design an effective collection of control variates , for example based on physical considerations ( e.g. * ? ? ?",
    ". alternatively control variates can be constructed based on gradient information on the sampling distribution @xcite . for monte carlo integration based on markov chains ,",
    "it is sometimes possible to construct control variates automatically based on statistics relating to the dynamics of the chain . in this direction ,",
    "the problem of constructing control variates for discrete state space case was essentially solved by @xcite . for continuous state spaces ,",
    "recent contributions include @xcite .",
    "the class of estimators considered here stem from a recent development in the variance reduction literature , that extends control variates to control _",
    "this idea is motivated by the observation that the methods listed above are ( in effect ) solving a misspecified regression problem , since in general @xmath2 does not belong to the linear span of the statistics @xmath9 .",
    "the recent work by @xcite alleviates model misspecification by increasing the number @xmath17 of statistics alongside the number @xmath0 of samples so that the limiting space spanned by the statistics @xmath18 is dense in a class of functions that contains the test function @xmath2 of interest .",
    "both methods provide a non - parametric alternative to classical control variates that deliver @xmath6 error rates .",
    "of these two proposed solutions , @xcite is not considered here since it is unclear how to proceed when @xmath3 is known only up to a normalisation constant . on the other hand",
    "the control functional method of @xcite is straight - forward to implement , being based on stein s identity , and has demonstrated wide - ranging empirical success .",
    "understanding the theoretical properties of this method is the focus of the present paper .    *",
    "technical summary : * an asymptotic analysis of gradient - based control functionals is currently absent from the literature .",
    "the only results to date are the preliminary analysis in @xcite , which established that mean square error can be bounded above by @xmath19 under weak and generic assumptions .",
    "however this rate is only slightly faster than the monte carlo rate ; it does not explain the dramatic variance reductions that are observed in experiments .",
    "the present paper establishes more refined error estimates that fully address this important issue . restricting attention to bounded domains of integration",
    ", we demonstrate that mean square error scales as @xmath20 , where @xmath21 characterises the smoothness of the distribution @xmath3 , @xmath22 characterises the smoothness of the test function @xmath2 , @xmath23 is the dimension of the domain of integration and @xmath24 can be arbitrarily small ( it is used to hide logarithmic factors ) .",
    "our analysis provides important insight into the excellent performance that has been observed for gradient - based control functionals in low - dimensional applications where @xmath25 @xcite . at the same time , the critical dependence on @xmath23 highlights the curse - of - dimensionality that appears inherent to such methods without further modification . going forward ,",
    "these results provide a benchmark for future development that aims to address scalability in dimension .    *",
    "outline : * below in section [ methods ] we describe the control functional method ( [ setup ] - [ reproducing kernel hilbert space construct ] ) . section [ consist approx asym ] contains our main result on error rates , including the case of non - independent samples arising from a markov chain .",
    "numerical results in section [ illustration ] confirm these error rates are realised in practice .",
    "our theoretical analysis combines error bounds from the scattered data approximation literature with stability results for markov chains ; this is presented in section [ appendix ] .",
    "finally the significance of our findings is discussed in section [ discuss ] .",
    "opening below , we fix notation before introducing the control functional estimators that are the focus of this paper .      define a probability space @xmath26 and a vector - valued random variable @xmath27 taking values in a measurable space @xmath28 and distributed according to @xmath3 .",
    "specialising relative to previous work , we assume @xmath29 is a compact subset of @xmath30 , @xmath31 , equipped with the usual topology . correspondingly , assume @xmath32 is the borel @xmath33-algebra and equip @xmath28 with the reference measure @xmath34 induced on @xmath29 from the restriction of lebesgue measure on @xmath30 .",
    "further , suppose @xmath3 admits a well - defined density @xmath8 on @xmath29 , via the radon - nikodym derivative @xmath35 .",
    "the following notation will be used below : @xmath36 , @xmath37 , @xmath38 , @xmath39^t$ ] , @xmath40 , @xmath41 and @xmath42^t$ ] .",
    "write @xmath43 for the interior of the set @xmath29 .",
    "write @xmath44 for the vector space of measurable functions @xmath45 for which @xmath46 exists and is finite .",
    "write @xmath47 for the space of measurable functions @xmath45 for which continuous partial derivatives exist on @xmath29 up to order @xmath48 .",
    "this section introduces the control functional method for integration , a non - parametric extension of classical control variates .",
    "initially we focus on independent sampling , before generalising results later to the case of non - independent sampling . recall that a well - known trade - off between random sampling and deterministic approximation has been studied on several separate occasions by authors including @xcite .",
    "our starting point is , instead , to establish a trade - off between random sampling and _ stochastic _ approximation .",
    "we assume throughout that the test function @xmath2 belongs to @xmath44 .",
    "consider initially an independent sample from @xmath3 , denoted @xmath49 , which is partitioned into two disjoint subsets @xmath50 and @xmath51 , where @xmath52 .",
    "although @xmath53 , @xmath0 are fixed , we will be interested in the asymptotic regime where @xmath54 for some @xmath55 $ ] .",
    "consider constructing an approximation @xmath56 to @xmath2 , based on @xmath57 .",
    "stochasticity in @xmath58 is induced via the sampling distribution of elements in @xmath57 .",
    "the expectation @xmath59 is required to be analytically tractable ; we will return to this point later .",
    "the estimators that we study take the form @xmath60 such sample - splitting estimators are unbiased , i.e. @xmath61 = \\int f { \\ensuremath{\\operatorname{d}\\!{\\pi}}}$ ] , where the expectation here is with respect to the sampling distribution @xmath3 of the @xmath62 random variables that constitute @xmath63 , and is conditional on @xmath57 .",
    "the corresponding estimator variance , again conditional on @xmath57 , is @xmath64 = ( n - m)^{-1 } \\sigma^2(f - s_{f,\\mathcal{d}_0})$ ] .",
    "this formulation encompasses control variates as a special case where @xmath65 , @xmath10 , and @xmath57 are used to select suitable values for the coefficients @xmath16 ( see e.g. * ? ? ?",
    "the insight required to go beyond control variates and achieve @xmath6 convergence is that we can construct an increasingly accurate approximations @xmath58 to @xmath2 as @xmath66 . indeed , under the scaling @xmath54 , if the expected functional approximation error ( efae ) satisfies @xmath67 = o(m^{-\\delta})$ ] for some @xmath68 , then @xmath69 = o(n^{-1-\\gamma\\delta } ) . \\label{analyse",
    "mse}\\end{aligned}\\ ] ] here we have written @xmath70 for the expectation with respect to the sampling distribution @xmath3 of the @xmath53 random variables that constitute @xmath57 . the rate above is optimised by taking @xmath71 , so that an optimal sample - split satisfies @xmath72 for some @xmath73 as @xmath74 ; this will be assumed in the sequel .",
    "when @xmath3 is given indirectly via an un - normalised density , this framework can only be exploited if it is possible to construct approximations @xmath58 whose integrals @xmath59 are analytically tractable . if and when this is possible , @xmath75 is known as a `` control functional '' .",
    "@xcite showed how to overcome this _",
    "impasse _ by introducing a flexible class of control functionals based on stein s identity .      to introduce the construction considered in @xcite",
    ", we make the following assumption :    1 .",
    "the density @xmath8 belongs to @xmath76 for some @xmath77 and is bounded away from 0 .",
    "denote the gradient function by @xmath78 , well - defined by ( a1 ) .",
    "crucially , @xmath79 can be evaluated even when @xmath8 is available only in un - normalised form .",
    "define the `` stein operator '' @xmath80 as follows : @xmath81 this definition can be motivated in several ways , for instance via schrdinger hamiltonians @xcite and via the generator method of @xcite applied to an overdamped langevin diffusion @xcite .",
    "our definition generalises the original construction of @xcite and also generalises def . 1 in @xcite to consider multi - variate random variables .    for functional approximation",
    "we follow @xcite and study approximations of the form ; @xmath82(\\bm{x } ) \\label{cfs defin}\\end{aligned}\\ ] ] where @xmath83 is a constant and @xmath84(\\bm{x})$ ] acts as a flexible function , parametrised by the choice of @xmath85 . under regularity assumptions ( a2,3 ) below ,",
    "stein s identity ( eqn .",
    "[ stein ] ) can be applied to obtain @xmath86 { \\ensuremath{\\operatorname{d}\\!{\\pi } } } = 0.\\ ] ] thus for this class of functions , @xmath87 $ ] is a control functional . unlike classical control variates , with @xmath17 dimensions , here the choice of @xmath88 presents us with an infinite - dimensional regression problem ; this justifies the _ functional _ nomenclature . in the next section",
    "we formulate the choice of @xmath89 and @xmath88 as an optimisation problem .",
    "this section formulates the construction of control functionals as approximation in a hilbert space @xmath90 , meaning that elements of @xmath91 are contained in the set @xmath44 .",
    "this construction first appeared in @xcite and has subsequently been explored by @xcite in the context of goodness - of - fit assessment .",
    "specification of a gradient - based control functional @xmath93 is equivalent to specification of @xmath88 .",
    "following @xcite we restrict each component function @xmath94 to a hilbert space @xmath95 with inner product @xmath96 .",
    "moreover we insist that @xmath97 is a ( non - trivial ) reproducing kernel hilbert space ( reproducing kernel hilbert space ) .",
    "this implies that there exists a ( non - zero ) symmetric positive definite function @xmath98 such that ( i ) for all @xmath99 we have @xmath100 and ( ii ) for all @xmath99 and @xmath101 we have @xmath102 ( * ? ? ?",
    "* def . 1 , p7 , and def . 2 , p10 )",
    ". the vector - valued function @xmath103 is defined in the cartesian product space @xmath104 , itself a hilbert space with the inner product @xmath105 .    to ensure @xmath106 we make an assumption on @xmath17 that is easily enforced by construction :    1 .",
    "the kernel @xmath17 belongs to @xmath107 for some @xmath108 .    additionally , to simplify the analysis we assume ( a3 - 5 ) below .",
    "note that the following assumptions appear in @xcite ; to keep the presentation self - contained we have chosen to reproduce the assumptions here :    1 .",
    "the boundary @xmath109 is piecewise smooth ( i.e. infinitely differentiable ) .",
    "2 .   for @xmath3-almost",
    "all @xmath99 the kernel @xmath17 satisfies 1 .",
    "@xmath110 , and 2 .",
    "@xmath111 .",
    "the notation @xmath112 denotes a surface integral over @xmath113 , @xmath114 denotes the unit normal vector and @xmath115 denotes the surface element at @xmath116 . under ( a1,2 ) , if @xmath117 then the function @xmath118 $ ] belongs to a reproducing kernel hilbert space @xmath92 characterised by the gradient - based kernel @xmath119 moreover , under ( a1,2,3,4 ) , the kernel @xmath120 satisfies @xmath121 for @xmath3-almost all @xmath99 , implying that @xmath122 { \\ensuremath{\\operatorname{d}\\!{\\pi } } } = 0 $ ] . for estimation purposes we require existence of a second moment :    1 .",
    "the kernel @xmath120 satisfies @xmath123 .",
    "@xcite showed that under ( a1,2,3,4,5 ) we have @xmath124 . in principle",
    "( a4,5 ) must be verified on a case - by - case basis ; they can be shown to hold in most applications . since @xmath2 will not in general",
    "integrate to zero , we actually need to consider a larger space of functional approximations , described in the next section .",
    "a generic construction for ensuring ( a4 ) starts with an arbitrary reproducing kernel hilbert space @xmath125 with reproducing kernel @xmath126 .",
    "let @xmath127 be a linear operator that enforces the boundary condition ( a4 ) .",
    "e.g. @xmath128 , where @xmath129 is a deterministic , smooth function that vanishes on @xmath130 .",
    "then @xmath97 is a reproducing kernel hilbert space whose kernel @xmath17 is defined by @xmath131 .",
    "this will be used later in section [ illustration ] .      write @xmath132 for the reproducing kernel hilbert space characterised by the kernel @xmath133 for all @xmath134 , consisting of constant functions @xmath135 .",
    "denote the norms associated to @xmath132 and @xmath92 respectively by @xmath136 and @xmath137 .",
    "write @xmath138 for the set @xmath139 .",
    "equip @xmath91 with the structure of a vector space , with addition operator @xmath140 and multiplication operator @xmath141 , each well - defined due to uniqueness of the representation @xmath142 , @xmath143 with @xmath144 and @xmath145 .",
    "in addition , equip @xmath91 with the norm @xmath146 , again , well - defined by uniqueness of representation .",
    "it can be shown that @xmath91 is a reproducing kernel hilbert space with kernel @xmath147 ( * ? ? ?",
    "* thm . 5 , p24 ) .",
    "from ( a1,2,3,4,5 ) it follows that @xmath90 .    to realise the control functional method we formulate the choice of @xmath89 and @xmath88 as a least - squares optimisation problem @xmath148 by the representer theorem @xcite we have @xmath149 where the coefficients @xmath150^t$ ] are the solution of the linear system @xmath151 where @xmath152 , @xmath153_{i , j } = k_+(\\bm{x}_{i},\\bm{x}_{j})$ ] , @xmath154 , @xmath155_i = f(\\bm{x}_i)$ ] .",
    "in situations where @xmath156 is not full - rank , we define @xmath157 ( for non - degenerate kernels @xmath156 is almost surely of full - rank ) .",
    "numerical inversion of this system may require regularisation ; this is discussed in section [ remarks ] .",
    "we are now ready to obtain convergence rates for control functionals .",
    "our analysis establishes a novel connection between the scattered data approximation literature @xcite and the study of the stability properties of markov chains @xcite .      in this section",
    "we focus on scattered data approximation and start by assuming a basic well - posedness condition :    1 .",
    "i.e. @xmath159 for some @xmath160 and @xmath161 .",
    "define the fill distance @xmath162 the proof strategy that we describe here decomposes into two parts ; ( i ) firstly , error bounds are obtained on the quality of the functional approximation @xmath58 terms of the fill distance @xmath163 , ( ii ) secondly , the fill distance @xmath163 is shown to vanish under sampling with high probability . for ( ii ) to occur , we require a constraint on the geometry of @xmath29 :    1 .   the domain @xmath29 satisfies an _ interior cone condition _",
    ", i.e. there exists and angle @xmath164 and a radius @xmath165 such that for every @xmath99 there exists a unit vector @xmath166 such that the cone @xmath167\\}\\ ] ] is contained in @xmath29 .",
    "the purpose of ( a7 ) is to rule our the possibility of ` pinch points ' on @xmath113 ( i.e. @xmath168-shaped regions ) , since intuitively sampling - based approaches can fail to ` get into the corners ' .",
    "the limiting behaviour of the fill distance under sampling is made clear with the following key technical result :    [ tech lemma ] let @xmath169 be continuous , monotone increasing , satisfy @xmath170 and @xmath171 .",
    "then under ( a1,7 ) we have @xmath172 = o ( g(m^{- 1 / d + \\epsilon } ) ) $ ] , where @xmath24 can be arbitrarily small .",
    "all proofs are reserved for section [ appendix ] .",
    "our first main result can now be stated :    [ independent ] assume ( a1 - 7 ) .",
    "then there exists @xmath173 , independent of @xmath174 , such that the estimator @xmath175 is an unbiased estimator of @xmath1 with @xmath176 = o(n^{-1 - 2(a \\wedge b ) / d + \\epsilon})\\ ] ] where @xmath24 can be arbitrarily small .",
    "this result demonstrates that control functionals are more efficient than monte carlo when @xmath25 .",
    "this provides new insight into the first set of empirical results reported in @xcite where , for assessment purposes , samples were generated independently from known , smooth densities .",
    "there , control functionals were constructed based on smooth kernels and integration errors were shown to be substantially lower compared to standard monte carlo estimation .",
    "a second consequence of this result is to show that efficiency is limited by the rougher of the density @xmath8 and the test function @xmath2 .",
    "an important application of these estimators is the computation of marginal likelihood @xcite ; in that setting the regularity of the test function coincides with the regularity of the density , so @xmath177 .",
    "thus for marginal likelihood computation , estimator accuracy is intrinsic to the density that is being marginalised over .    on the negative side , this result clearly illustrates a curse of dimensionality , that appears to be present in all methods that involve functional approximation .",
    "we return to this point in section [ discuss ] .    the results above hold for independent samples , yet the main area of application for control functionals is estimation based on the sample path of a markov chain . in the next section we prove that the assumption of independence can be relaxed to accommodate most markov chain constructions that would typically be used in this problem class .",
    "in practice , samples from the posterior are usually obtained via markov chain monte carlo .",
    "our analysis must therefore be extended to the non - independent setting : consider samples @xmath49 generated by a reversible markov chain targeting @xmath3 .",
    "we make the following stochastic stability assumption :    1 .",
    "the markov chain is uniformly ergodic .",
    "then our first step is to extend lemma [ tech lemma ] to the non - independent setting :    [ tech lemma 2 ] let @xmath169 be continuous , monotone increasing , satisfy @xmath170 and @xmath171",
    ". then under ( a1,7,8 ) we have @xmath172 = o ( g(m^{- 1 / d + \\epsilon } ) ) $ ] , where @xmath24 can be arbitrarily small .",
    "the generality of this result is highlighted , which represents a novel connection between scattered data approximation and the stochastic stability of markov chains .",
    "indeed , lemma [ tech lemma 2 ] forms the basis for ongoing research into the convergence of probabilistic integrators , including bayesian quadrature @xcite .",
    "non - independence presents us with the possibility that two of the states @xmath178 are identical ( @xmath179 ; for instance , when a metropolis - hastings sample is used and a rejection occurs ) . under our current definition , such an event would cause the kernel matrix @xmath156 to become singular and the control functional to become trivial @xmath180 .",
    "it is thus necessary to modify this definition .",
    "specifically , we assume that @xmath57 has been pre - filtered such that any repeated states have been removed .",
    "note that this does not introduce bias , since we are only pre - filtering @xmath57 , not @xmath63 .",
    "note also that we are not losing information , since the function evaluations @xmath181 are identical when @xmath179 .",
    "with this technical point safely surmounted , we present our second main result :    [ dependent ] assume ( a1 - 8 )",
    ". then there exists @xmath173 such that the estimator @xmath175 satisfies @xmath176 = o(n^{-1 - 2(a \\wedge b ) / d + \\epsilon})\\ ] ] where @xmath24 can be arbitrarily small .",
    "this result again demonstrates that control functionals are more efficient than monte carlo when @xmath25 and that efficiency is limited by the rougher of the density @xmath8 and the kernel @xmath17 .",
    "this helps to explain the second set of empirical results obtained in @xcite , where excellent performance was reported on problems that involved smooth densities , smooth kernels and markov chain monte carlo sampling schemes . on the other hand",
    ", we again observe a curse of dimensionality that is inherent to control functionals and , indeed , control variates in general .",
    "several points of discussion are covered below , on the appropriateness of the assumptions , the strength of the results and the numerical aspects of computation .",
    "* on the assumptions : * the fundamental assumptions ( a1 - 5 ) have previously been discussed in @xcite and ( a7 ) is not restrictive .",
    "below we discuss the remaining assumptions , ( a6 ) and ( a8 ) .",
    "our entire analysis was predicated on ( a6 ) , i.e. that @xmath2 belongs to the function space @xmath91 .",
    "it it is thus natural to ask how restrictive is this key assumption . to answer this , we provide the following : a reproducing kernel hilbert space @xmath97",
    "is called `` c - universal '' if it is dense in @xmath182 with respect to the @xmath183 norm .",
    "[ characteristic ] when @xmath97 is c - universal , the space @xmath91 is dense in @xmath44 .    for practical purposes ,",
    "it is therefore sufficient to work with a kernel @xmath17 that is @xmath184-universal .",
    "the notion of @xmath184-universality was introduced by @xcite , who showed that many widely - used kernels are @xmath184-universal on compact @xmath185 . indeed , prop .",
    "1 of @xcite proves that an reproducing kernel hilbert space with kernel @xmath17 is c - universal if and only if the map @xmath186 $ ] , from the space of finite signed borel measures @xmath187 to the reproducing kernel hilbert space @xmath97 , is injective , which is clearly a weak requirement .",
    "the other assumption that deserves discussion is ( a8 ) ; uniform ergodicity of the markov chain .",
    "since @xmath29 is compact and @xmath8 is bounded below , we argue that it is hard to imagine any markov chain that targets @xmath3 and is not uniformly ergodic . indeed , @xcite constructed an example where a ` pinch point ' in the domain causes a gibbs sampler targeting a uniform distribution to fail to be geometrically ergodic ; their construction violates our ( a7 ) . as a concrete example , under our assumptions , a metropolis - hastings sampler is uniformly ergodic whenever the proposal density @xmath188 is bounded below on @xmath189 .",
    "* on the results : * the intuition for the results in theorems [ independent ] and [ dependent ] can be described as `` accurate estimation with high probability '' , since the condition @xmath190 is satisfied when the samples @xmath57 cover the state space @xmath29 , which happens with high - probability when @xmath53 is large .",
    "there are two equivalent statements that can be made unconditionally on @xmath190 : ( i ) firstly , one can simply re - define @xmath191 whenever @xmath192 , i.e. when the states @xmath57 are poorly spaced we revert to the usual monte carlo estimator .",
    "( ii ) secondly , one could augment @xmath57 with additional fixed states , such as a grid , @xmath193 , to ensure that @xmath190 is automatically satisfied .",
    "however , we find both of these equivalent approaches to be less aesthetically pleasing , since in practice this requires that @xmath194 be explicitly computed",
    ". this explains our presentational approach above .    on the sharpness of our results",
    ", we refer to sec .",
    "11.7 of @xcite where an overview of the strengths and weaknesses of results in the scattered data approximation literature is provided .",
    "essentially , the bounds that we used to obtain the results in this paper are not sharp ; in general the optimal rate depends on the space @xmath97 through factors other than simply the smoothness of its elements .",
    "this is counter - balanced by the fact that , by using scattered data bounds , we were able to obtain results in the non - independent sampling setting . to the best of our knowledge ,",
    "this approach is a novel contribution to the literature on monte carlo integration .    * on computation : * it is important to emphasise the ease with which control functional estimators can be implemented .",
    "indeed , an explicit closed - form expression is available : @xmath195 where @xmath196 , @xmath197_i = f(\\bm{x}_{m+i})$ ] , @xmath198 and @xmath199_{i , j } = k_+(\\bm{x}_{m+i},\\bm{x}_{j})$ ] .",
    "a full derivation can be found in @xcite . as such",
    ", the only computation required is linear algebra .",
    "numerical inversion of the kernel matrix @xmath156 can benefit from regularisation ; such considerations are quite standard @xcite . in addition",
    "there is an active research effort to obtain computationally efficient approximations to kernel matrix inverses , with extremely promising results emerging ( e.g. * ? ? ?",
    "all of these methods can be used in combination with control functionals and could be used to mitigate the ( nave ) cubic computational cost associated with the matrix inversion . as a side remark ,",
    "note that when @xmath200 , even a a nave cubic computational cost is outweighed by gains in estimator precision in the setting of a finite - but - large computational budget .",
    "our theoretical results are illustrated here with a novel application of gradient - based control functionals to an inverse problem arising in partial differential equation modelling .",
    "specifically , we consider the following elliptic diffusion problem with mixed dirichlet and neumann boundary conditions : @xmath201 & = & 0 \\hspace{50pt } \\text{if } x_1,x_2 \\in ( 0,1 ) \\\\ w(\\bm{x } ) & = & \\left\\ { \\begin{array}{ll } x_1 & \\text{if } x_2 = 0 \\\\ 1 - x_1 & \\text{if } x_2 = 1 \\end{array } \\right",
    ". \\\\ \\nabla_{x_1 } w(\\bm{x } ) & = & 0 \\hspace{46.5pt } \\text { if } x_1 \\in \\{0,1\\}.\\end{aligned}\\ ] ] this partial differential equation serves as a simple model of steady - state flow in aquifers and other subsurface systems ; @xmath184 can represent the permeability of a porous medium while @xmath202 represents the hydraulic head @xcite .",
    "the aim is to make inferences on the field @xmath184 in a setting where the underlying solution @xmath202 is observed with noise on a regular grid of @xmath203 points @xmath204 , @xmath205 .",
    "the observation model @xmath206 takes the form @xmath207 where @xmath208 and @xmath209 are independent normal random variables with standard deviation @xmath210 .",
    "consider now the bayesian approach to this inverse problem @xcite , in which the field @xmath184 is endowed with a prior distribution via a karhunen - love series @xmath211 where @xmath212 and @xmath213 are orthonormal basis functions @xcite .",
    "for this illustration we follow @xcite who each took a truncated fourier basis .",
    "our aim here is to obtain accurate estimates for the posterior mean of the parameter @xmath214 . for the inference we imposed a uniform prior @xmath215 over the domain @xmath216^d$ ] .",
    "the posterior density @xmath217 is available up to an unknown normalising constant @xmath218 .",
    "each evaluation of the likelihood necessitates the solution of the partial differential equation ; control functionals offer the possibility to reduce the number of likelihood evaluations , and hence the computational cost , required to achieve a given estimator precision .    as an aside",
    ", we note that the standard approach to inference employs a numerical integrator for the forward - solve , typically based on finite element methods .",
    "this would provide us with gradient information on the posterior , but would also introduce some bias due to discretisation error . to ensure that we obtain exact gradient information , we instead exploited a probabilistic meshless method due to @xcite as our numerical integrator .",
    "the assumptions of our theory are verified in this example .",
    "smoothness of the prior , together with ellipticity , imply ( a1 ) holds for all @xmath219 .",
    "( a3,7 ) hold since the domain of integration is a hyper - cube .",
    "samples from the posterior @xmath220 were obtained using a metropolis - adjusted langevin sampler with fixed proposal covariance ; this ensures that ( a8 ) is satisfied .",
    "remaining assumptions are satisfied by construction of the kernel @xmath17 : following the approach outlines in section [ h0 sec ] , take @xmath221 to be a matrn kernel of order @xmath222 , @xmath108 ( see e.g. * ? ? ? * for details ) .",
    "then form @xmath223 as the product of @xmath221 and @xmath224 , where the boundary function @xmath225 satisfies @xmath226 on @xmath227^d$ ] , @xmath228 when @xmath229 for some @xmath230 , and @xmath225 is infinitely differentiable on @xmath216^d$ ] . with this construction , ( a2 ) holds .",
    "( a4 ) holds since @xmath17 has a twice repeated root at @xmath229 for each @xmath230 .",
    "( a5 ) holds since @xmath120 is bounded . for ( a6 ) , we note that @xmath231 is @xmath184-universal on @xmath232^d)$ ] and dense in @xmath44 .",
    "observations were generated from the model with data - generating parameter @xmath233 and collected over a coarse grid of @xmath234 locations .",
    "samples of size @xmath0 were obtained from the posterior and divided equally between the training set @xmath57 and test set @xmath63 .",
    "the performance of gradient - based control functionals was benchmarked against that of a standard arithmetic mean taken over all @xmath0 samples .",
    "we note that , in all experiments , all values of @xmath214 encountered were contained in @xmath235^d$ ] .",
    "thus it does not matter that we did not specify @xmath225 explicitly above , emphasising the weakness of assumption ( a4 ) in practical application .",
    "results shown in figure [ matern ] are based on the matrn kernel with @xmath236 . for dimensions",
    "@xmath237 and @xmath238 , the estimator that uses control functionals achieved a dramatic reduction in asymptotic variance compared to the arithmetic mean . on the other hand , for @xmath239 , the curse of dimensionality",
    "is clearly shown for the control functional method .",
    "in this section we fill in the remaining theoretical gaps .    in rigorously establishing this result",
    "there are six main steps .",
    "initially we fix @xmath99 and aim to show that with high probability there exists a state @xmath240 close to @xmath241 .",
    "then we consider implications for the distribution of the fill distance .    * step # 1 : * `` construct a reference grid . ''",
    "compactness of @xmath29 implies that @xmath29 is bounded ; without loss of generality suppose @xmath242^d$ ] . for @xmath243 , define a uniform grid of reference points @xmath244^d$ ] consisting of all @xmath245 states of the form @xmath246 where @xmath247 ( fig .",
    "[ fig : step1 ] ) .",
    "we will require that this grid has a sufficiently fine resolution ; specifically we suppose that @xmath248 where @xmath249 , @xmath250 are defined by the interior cone condition that @xmath29 is assumed to satisfy .",
    "0.46    0.46    * step # 2 : * `` find a grid point @xmath251 near to @xmath241 . ''",
    "define @xmath252 we claim that there exists a grid point @xmath251 such that @xmath253 .",
    "indeed , from the interior cone condition there exists a cone @xmath254 ( fig . [",
    "fig : step1 ] ) .",
    "define @xmath255 the fine grid resolution implies @xmath256 . from @xcite , lemma 3.7 , it follows that the cone @xmath257 contains the ball @xmath258 with centre @xmath259 and radius @xmath260 ( fig .",
    "[ fig : step2 ] ) .",
    "now , the fill distance for the grid @xmath193 relative to the space @xmath261^d$ ] can be shown to equal @xmath262 , which is exactly equal to the radius @xmath263 .",
    "it follows that @xmath258 must contain a grid point @xmath251 for some @xmath264 . from the triangle inequality",
    "; @xmath265 this establishes the claim .    * step # 3 : * `` a set of points that ` cover",
    "the grid ' must contain at least one point that is near to @xmath241 . ''",
    "consider the event @xmath266.\\ ] ] conditional on @xmath267 , there exists @xmath240 such that @xmath268 .",
    "it follows that , conditional on @xmath267 , we have @xmath269 where we have used the result of step # 1 . thus the event @xmath267 , or ` covering the grid ' , implies the event @xmath270 $ ] ( fig .",
    "[ fig : step3 ] ) .    0.46    0.46    * step # 4 :",
    "* `` the grid is covered with high probability . ''",
    "next , we upper - bound the probability @xmath271 $ ] of the event @xmath272 . for this",
    ", note that for all @xmath264 there exists a unit vector @xmath273 such that the cone @xmath274 is contained in @xmath29 ( fig .",
    "[ fig : step4 ] ) .",
    "it follows that @xmath275 where the final inequality follows since @xmath276 and the intersection of a cone with a ball is another cone .",
    "now , from @xcite , lemma 3.7 , the ball with centre @xmath277 and radius @xmath278 is contained in the cone @xmath279 .",
    "continuing , @xmath280 now , we have @xmath281 & = & \\mathbb{p}_{\\mathcal{d}_0}\\left[\\exists i : \\forall j , \\|\\bm{g}_i - \\bm{x}_j \\| > \\frac{1}{m-1}\\right ] \\\\ & \\leq & \\sum_{i=1}^g \\underbrace{\\mathbb{p}_{\\mathcal{d}_0 } \\left[\\forall j , \\|\\bm{g}_i - \\bm{x}_j\\| > \\frac{1}{m-1}\\right]}_{(*)}.\\end{aligned}\\ ] ] the probability @xmath282 can be bounded above using independence of the samples .",
    "indeed , the probability that a random draw from @xmath3 lands in @xmath283 can be lower - bounded by @xmath284 times @xmath285 since , under ( a1 ) , @xmath8 can be decomposed as a mixture density with two components , the first uniform over @xmath29 with mixture weight @xmath286 and the second a component with density @xmath287 and mixture weight @xmath288 .",
    "thus we have @xmath289 & \\leq & ( 1 - v \\rho ) ^m . \\label{using independence}\\end{aligned}\\ ] ]",
    "note that this is the only place in the proof that independence of the @xmath290 is required .",
    "* step # 5 : * `` the fill distance is small with high probability . ''",
    "the result of step # 4 can be used to derive a lower bound on the distribution function of the fill distance : @xmath291 \\ ; \\leq \\ ; \\mathbb{p}_{\\mathcal{d}_0}[e^c ] & \\leq & g ( 1 - v \\rho ) ^m \\\\ & = & g \\left ( 1 - \\frac { ( \\frac{\\sin\\theta}{1 + \\sin\\theta})^d \\pi^{d/2 } \\rho } { \\gamma(\\frac{d}{2 } + 1 ) ( m-1)^d } \\right)^m\\end{aligned}\\ ] ] letting @xmath292 implies that @xmath293 and @xmath294 . in this parametrisation , when @xmath295 we have @xmath296 and hence @xmath297 & \\leq &   \\left ( 1 + \\frac{r+1}{\\zeta } \\right)^d \\left [ 1 - \\frac { ( \\frac{\\sin\\theta}{1 + \\sin\\theta})^d \\pi^{d/2 } \\rho } { \\gamma(\\frac{d}{2}+1 ) ( r+1)^d } \\zeta^d \\right]^m \\nonumber \\\\ & \\leq   & \\left(\\frac{r+2}{\\zeta}\\right)^d ( 1 - c_d \\zeta^d)^m , \\label{tech eq1}\\end{aligned}\\ ] ] where we have written @xmath298 while eqn .",
    "[ tech eq1 ] holds only for @xmath299 of the form @xmath300 , it can be made to hold for all @xmath301 by replacing @xmath302 with @xmath303 .",
    "this is because for any @xmath301 there exists @xmath304 such that @xmath305 satisfies @xmath306 , along with the fact that @xmath307 \\leq \\mathbb{p}_{\\mathcal{d}_0}[h_{\\mathcal{d}_0 } > \\tilde{\\zeta}]$ ] .",
    "* step # 6 : * `` putting it all together . '' from @xmath242^d$ ] we have that @xmath308 $ ] and hence , since @xmath309 is continuous and @xmath261 $ ] is compact , @xmath310 } g(h ) < \\infty$ ] . without loss of generality , and with probability one , we have @xmath311 .",
    "( this is without loss of generality since we can simply redefine @xmath309 as @xmath312 . ) from the reverse markov inequality , for all @xmath313 $ ] , @xmath314 & \\geq & \\frac { \\mathbb{e}_{\\mathcal{d}_0}[g(h_{\\mathcal{d}_0 } ) ] - \\zeta}{1 - \\zeta } \\end{aligned}\\ ] ] and upon rearranging @xmath315 & \\leq & \\zeta + ( 1 - \\zeta ) \\mathbb{p}_{\\mathcal{d}_0}[g(h_{\\mathcal{d}_0 } ) > \\zeta ] .",
    "\\label{tech eq2}\\end{aligned}\\ ] ] since @xmath309 is continuous and monotone , @xmath316 exists and we have @xmath317 = \\mathbb{p}_{\\mathcal{d}_0}[h_{\\mathcal{d}_0 } \\leq g^{-1}(\\zeta)]$ ] .",
    "this allows us to combine eqns .",
    "[ tech eq1 ] and [ tech eq2 ] , obtaining @xmath315 \\quad & \\leq & \\zeta + ( 1 - \\zeta)\\left ( \\frac{r+2}{g^{-1}(\\zeta ) } \\right)^d ( 1 - \\tilde{c}_d(g^{-1}(\\zeta))^d)^m \\\\ & \\leq & \\zeta + \\left ( \\frac{r+2}{g^{-1}(\\zeta ) } \\right)^d ( 1 - \\tilde{c}_d(g^{-1}(\\zeta))^d)^m.\\end{aligned}\\ ] ] now , letting @xmath318 for some fixed @xmath225 , subject to @xmath319 , and varying @xmath53 , we have that @xmath315 & \\leq & g(m^{-\\delta } ) + ( r+2)^d \\underbrace{m^{d\\delta } ( 1 - \\tilde{c}_d m^{-d \\delta})^m } _ { ( * * ) } \\label{tech eq3}\\end{aligned}\\ ] ] where @xmath320 . indeed , since @xmath321 for all @xmath322 , @xmath323 & = & d\\delta \\log(m ) + m \\log(1 - \\tilde{c}_d m^{-d\\delta } ) \\\\ & \\leq & d\\delta \\log(m ) - \\tilde{c}_d m^{1 - d\\delta } \\ ; = \\ ; \\log [ m^{d\\delta } \\exp ( -\\tilde{c}_d m^{1 - d\\delta } ) ] .\\end{aligned}\\ ] ] writing @xmath324 , @xmath325 under the hypothesis on the limiting behavior of @xmath326 as @xmath327 , we have that @xmath328 thus the right hand side of eqn .",
    "[ tech eq3 ] is asymptotically minimised by taking @xmath225 as large as possible , subject to @xmath329 converging exponentially fast .",
    "i.e. @xmath330 for @xmath24 arbitrarily small .",
    "we therefore conclude that @xmath331 = o(g(m^{-1 / d + \\epsilon}))$ ] , as required .",
    "unbiasedness follows directly from the structure of splitting estimators ( eqn .",
    "[ splitting estimators ] ) . from eqn .",
    "[ analyse mse ] it suffices to consider the rate @xmath225 at which the efae @xmath332 vanishes .",
    "firstly , note that @xmath333 \\pi(\\bm{x } ' ) { \\ensuremath{\\operatorname{d}\\!{\\bm{x } } } } ' \\right]^2 \\pi(\\bm{x } ) { \\ensuremath{\\operatorname{d}\\!{\\bm{x } } } } \\\\   & = & \\int [ f(\\bm{x } ) - s_{f,\\mathcal{d}_0}(\\bm{x})]^2 \\pi(\\bm{x } ) { \\ensuremath{\\operatorname{d}\\!{\\bm{x } } } } - \\left[\\int [ f(\\bm{x } ' ) - s_{f,\\mathcal{d}_0}(\\bm{x } ' ) ] \\pi(\\bm{x } ' ) { \\ensuremath{\\operatorname{d}\\!{\\bm{x } } } } ' \\right]^2 \\\\   & \\leq & \\int [ f(\\bm{x } ) - s_{f,\\mathcal{d}_0}(\\bm{x})]^2 \\pi(\\bm{x } ) { \\ensuremath{\\operatorname{d}\\!{\\bm{x}.}}}\\end{aligned}\\ ] ] secondly , observe that from ( a1,2 ) the kernel @xmath334 .",
    "thus from theorem 11.13 of @xcite there exists @xmath173 , @xmath335 such that , whenever @xmath190 , we have latexmath:[\\ ] ] ( an arbitrary initial condition @xmath364 can be handled using standard renewal theory ; we do not present the details here . ) applying eqn .",
    "[ equivalent statement ] to the control functional estimator produces @xmath365 \\leq k \\sigma^2(f - s_{f,\\mathcal{d}_0}).\\ ] ] arguing as in the proof of theorem [ independent ] , we have @xmath366 \\leq c^2 \\|f\\|_{\\mathcal{h}_+}^2   \\mathbb{e}_{\\mathcal{d}_0}[h_{\\mathcal{d}_0}^{2(a \\wedge b)}]$ ] .",
    "finally , it remains only to show that the scaling relation @xmath340 = o(n^{-2(a \\wedge b ) / d + \\epsilon})$ ] , where @xmath24 can be arbitrarily small , holds in the non - independent setting .",
    "this was precisely the content of lemma [ tech lemma 2 ] .    for compact @xmath29 ,",
    "the notion of c - universality is equivalent to cc - universality , where the ( weaker ) topology of compact convergence is used in place of the @xmath183 norm topology ( * ? ? ?",
    "2.1 ) proved that when @xmath97 is cc - universal , @xmath92 is characteristic , meaning that @xmath367 if and only if @xmath368 , where @xmath369.\\ ] ] separately , ( * ? ? ?",
    "1 ) showed that , when a reproducing kernel hilbert space @xmath92 is characteristic , the sum @xmath138 is dense in @xmath44 .",
    "combining these these results completes the argument .",
    "this paper has established novel probabilistic error bounds for scattered data approximation and leveraged these bounds to provide an asymptotic analysis for the method of control functionals based on stein s identity .",
    "our analysis makes explicit the contribution of the smoothness @xmath21 of the distribution @xmath3 , the smoothness @xmath22 of the test function @xmath2 and the dimension @xmath23 of the domain of integration . as such",
    ", these results provide a rigorous theoretical explanation for the excellent performance of gradient - based control functionals in low - dimensions observed in previous work .",
    "extensions of this work are three - fold : ( i ) our results focused on bounded domains , since this is the usual setting for results in the scattered data approximation literature .",
    "however , the gradient - based control functional method does not itself require that the domain of integration be bounded .",
    "extending this analysis to the unbounded setting appears challenging at present and remains a goal for future research .",
    "( ii ) alternative literatures to the scattered data literature could form the basis of an analysis of control functionals , such as e.g. recent work by @xcite .",
    "these efforts have the advantage of providing @xmath370 error bounds , rather than @xmath183 error bounds .",
    "future research could aim to extend these results to the important case of non - independent sampling .",
    "( iii ) generally , our theoretical results clarify the need to develop estimation strategies that do not suffer from the curse of dimensionality . while this curse is intrinsic to functional approximation in general , due to the need to explore the state space , the observation that many test functions of interest are of low `` effective dimension '' suggests that stronger assumptions could reasonably be placed on the function space .",
    "active subspace methods @xcite and weighted function spaces @xcite may have an important role to play in future research efforts .    while we focused on control functionals ,",
    "our analysis may have consequences for the goodness - of - fit tests that were recently proposed in @xcite , as well as related ongoing work based on stein s identity .    *",
    "acknowledgements : * the authors wish to thank aretha teckentrup and felipe medina aguayo for useful comments .",
    "fxb was supported by epsrc [ ep / l016710/1 ] .",
    "mg was supported by epsrc [ ep / j016934/1 , ep / k034154/1 ] , an epsrc established career fellowship , the eu grant [ eu/259348 ] and a royal society wolfson research merit award .",
    "migliorati , g. , nobile , f. and tempone , r. ( 2015 ) convergence estimates in probability and in expectation for discrete least squares with noisy evaluations at random points .",
    "_ j. multivariate anal .",
    "_ , * 142 * , 167 - 182 .                                  stein , c. , diaconis , p. , holmes , s. and reinert , g. ( 2004 ) use of exchangeable pairs in the analysis of simulations . _ in : stein s method : expository lectures and applications _ , _ institute of mathematical statistics lecture notes - monograph series _ , * 46 * , 1 - 25 ."
  ],
  "abstract_text": [
    "<S> gradient information on the sampling distribution can be used to reduce monte carlo variance . </S>",
    "<S> an important application is that of estimating an expectation along the sample path of a markov chain , where empirical results indicate that gradient information enables improvement on root-@xmath0 convergence . </S>",
    "<S> the contribution of this paper is to establish theoretical bounds on convergence rates for estimators that exploit gradient information , specifically , for a class of estimator based on stein s identity . </S>",
    "<S> our analysis is refined and accounts for ( i ) the degree of smoothness of the sampling distribution and test function , ( ii ) the dimension of the state space , and ( iii ) the case of non - independent samples arising from a markov chain . </S>",
    "<S> these results provide much - needed insight into the rapid convergence of gradient - based estimators observed for low - dimensional problems , as well as clarifying a curse - of - dimensionality that appears inherent to such methods without further modification .    _ </S>",
    "<S> keywords : _ asymptotics , control functionals , reproducing kernel , scattered data , variance reduction </S>"
  ]
}