{
  "article_text": [
    "in this paper we are interested in studying the behaviour of the kolmogorov - sinai ( ks ) entropy for randomly perturbed dynamical systems . as it has been proved in @xcite for a typical situation ,",
    "the ks entropy of a dynamical system perturbed by a sequence of random variables is infinite for all @xmath0 ; here @xmath1 is a real parameter measuring the size of the perturbation .",
    "this result shows that the ks entropy is not the right quantity to look at in randomly perturbed dynamical systems . in @xcite",
    "it has been suggested to study the ks entropy relative to a partition .",
    "we study the perturbation of a dynamical system @xmath2 by some noise . by noise",
    "we mean a discrete stochastic processes of independent and identically distributed ( i.i.d . ) random variables @xmath3 defined on a probability space @xmath4 with values on the interval @xmath5 $ ] , for a positive real number @xmath1 .",
    "the number @xmath1 is the parameter that measures the size of the perturbation on the original system .",
    "we also assume that for all @xmath1 there is an ergodic invariant probability measure @xmath6 on the perturbed system @xmath7 such that the probability measure @xmath8 is the weak limit of the sequence @xmath6 as @xmath9 ( for the formal definition of @xmath6 see section [ sec : estim ] ) .",
    "in @xcite it has been proved that , under these hypothesis , it holds @xmath10 where @xmath11 and @xmath12 are the ks entropies relative to the partition @xmath13 of the original and the perturbed system respectively , and by @xmath14 we denote the ks entropy of the unperturbed dynamical system .",
    "the equality holds for certain hyperbolic systems ( @xcite ) .    in equation ( [ eq : kifer ] )",
    "it is used the ks entropy relative to a fixed partition , avoiding the problem of having infinite ks entropy , and it is performed the limit for the size of the perturbation going to zero .",
    "this result shows that for an unperturbed dynamical system , the ks entropy relative to a partition is well approximated by the same quantity for the perturbed system , when the perturbation is small enough .",
    "unfortunately when we want to study time series obtained by measurements of real systems , it is impossible to know how small is the perturbation to the original system . hence it is impossible to perform the limit in equation ( [ eq : kifer ] ) .",
    "real systems are observed using a symbolic representation of the measurements .",
    "that is the phase space is divided into a finite number of sets , and we study the time series given by the symbols of the sets of the partition visited by the system as time grows .",
    "the main idea to study real systems is to look at the system using different scales of observation , which corresponds to use different sizes for the partition of the phase space .",
    "when the partition is coarse the effects of the noise are negligible , whereas , when the partition is very fine , then the effects of the noise hide the dynamics of the original system .",
    "then the right approach to randomly perturbed systems is to study the behaviour of the ks entropy while the partition is refined and the noise is fixed .    in this paper",
    "we prove that , when the diameter of the partition is big with respect to the size of the noise , the ks entropy of the perturbed system is a good approximation for the unperturbed ks entropy .",
    "our main result is contained in equations ( [ eq : e > s ] ) and ( [ eq : e < s ] ) , which in particular imply equation ( [ eq : kifer ] ) .",
    "moreover we obtain precise estimates for the effects of the noise , in order to distinguish between the perturbation and the dynamics of the system , a very important problem in the analysis of time series ( @xcite ) .",
    "we remark that the main extension of the result contained in equation ( [ eq : kifer ] ) is its applicability to time series obtained by observations of real systems .    to apply our method we have to face the problem of the measurement of the ks entropy in real systems ( that is the ks entropy of observed time series ) .",
    "this is an important problem ( for a review see for example @xcite ) , and many papers have been dedicated to the approximation of the ks entropy by different indicators .",
    "we recall the approach of grassberger and procaccia @xcite based on the generalized renyi entropy , and that of cohen and procaccia @xcite based on the correlation integral for a time series ( @xcite ) . in this paper",
    "we use a method introduced in @xcite and @xcite , which is related to the notion of _ algorithmic information content ( aic ) _ ( see section [ sec : aic ] for the definitions ) .    moreover , when applying theoretical methods to real time series , it is always questionable whether all the hypothesis needed for the methods are verified by the single observations we study .",
    "this is this case , for example , for ergodicity that we suppose to be verified .    in the following section ,",
    "we recall some basic definitions and results related to the ks entropy and its computation for dynamical systems and purely random systems . in sections [ sec : estim ] and [ sec : aic_noise ] we apply the method of the aic to randomly perturbed systems , and finally in section [ sec : num ] we show some numerical experiments performed on the logistic map at the chaotic parameter @xmath15 .",
    "we now briefly recall the basic definitions we need . let @xmath16 be a probability space , where @xmath17 is a compact metric space and @xmath18 is the borel @xmath1",
    "-algebra . in this paper",
    "we restrict our attention on one dimensional spaces @xmath17 , but we believe that the techniques used can be generalized to higher dimensions .",
    "let @xmath19 be a continuous map , invariant with respect to @xmath8 and ergodic .",
    "given a finite measurable partition @xmath20 of the space @xmath17 , the _",
    "entropy @xmath21 _ of the partition is defined as @xmath22 let @xmath23 be the partition given by the counter images @xmath24 .",
    "then let @xmath25 be the partition given by the sets of the form @xmath26 varying @xmath27 among all the sets of @xmath13 .",
    "then the _ kolmogorov - sinai entropy @xmath11 relative to the partition @xmath13 _ is defined as the limit @xmath28 the _ kolmogorov - sinai entropy @xmath14 _ of the dynamical system @xmath2 is defined as @xmath29 moreover for the explicit computation of the ks entropy , the kolmogorov - sinai theorem says that there are some special partitions , called _ generating _ , for which the supremum among all the partitions is realized .",
    "hence it is enough to compute the ks entropy relative to a generating partition to obtain the ks entropy of the system .",
    "the existence of a generating partition for dynamical systems is given by the following theorem :    [ teo : krieg ] let @xmath2 be an ergodic dynamical system on a lebesgue space @xmath17 , such that the probability measure @xmath8 is invariant and @xmath30 .",
    "then there is a finite generating partition for @xmath31 .",
    "let @xmath32 be the diameter of a uniform partition @xmath13 of the space @xmath17 , and simply denote by @xmath33 the ks entropy @xmath34 relative to the partition @xmath13 .",
    "this notation has the aim to enhance the role of the diameter of the partition considered .",
    "the quantity @xmath33 is also called the _",
    "@xmath32-entropy _ of the dynamical system ( see @xcite for a review ) .",
    "the krieger generator theorem implies that , if @xmath14 is finite , there exists @xmath35 such that @xmath36 for all @xmath37 .",
    "it is also possible to establish the behaviour of the @xmath32-entropy for discrete stochastic processes of i.i.d . random variables .",
    "[ teo : gw ] let @xmath38 be a discrete stochastic process of i.i.d .",
    "random variables with values on the interval @xmath39 $ ] .",
    "then as @xmath40 it holds @xmath41 where the entropy is computed with respect to the invariant probability measure of the system ( the induced measure ) absolutely continuous with respect to the lebesgue measure .    when dealing with randomly perturbed systems , it has been shown using numerical experiments ( @xcite,@xcite ) that the expected behaviour is @xmath42 - \\log \\epsilon & \\hbox{for } \\epsilon < < \\sigma \\end{array } \\right.\\ ] ] where @xmath1 denotes the standard deviation of the noise ( that is the square root of the variance of the random variables @xmath43 ) .",
    "the same result is expected for the generalized renyi entropy with @xmath44 ( @xcite ) .",
    "we recall that for randomly perturbed systems the ks entropy @xmath45 is infinite , hence the krieger generator theorem is not applicable to the system @xmath7 .",
    "but we assume that the unperturbed system @xmath46 has finite ks entropy @xmath14 with generating partition of diameter @xmath35 .",
    "hence @xmath47 for all @xmath37 .",
    "this implies that if the size @xmath1 of the noise is small with respect to @xmath35 , approximating @xmath48 by @xmath49 we find a good approximation to the ks entropy of the unperturbed system .    in the following we prove ( [ eq : stime ] ) using the _ algorithmic information content ( aic )",
    "_ , briefly described in the following section .",
    "we remark that the behaviour of equation ( [ eq : stime ] ) is of great importance since it would give us a method to estimate the size of the random perturbation on the deterministic dynamics of the system we are observing .",
    "given a finite alphabet @xmath50 , let @xmath51 be the set of all words on the alphabet @xmath50 of length @xmath52 .",
    "the intuitive meaning of quantity of information contained in a finite string @xmath53 is the length of the smallest message from which you can reconstruct @xmath54 on some machine .",
    "thus , formally , the information @xmath55 is a function @xmath56 on the set of finite strings on a finite alphabet @xmath57 which takes values in the set of natural numbers .",
    "one of the most important information function is the _ algorithmic information content ( @xmath58)_. in order to define it , it is necessary to define the notion of partial recursive function .",
    "we limit ourselves to give an intuitive idea which is very close to the formal definition .",
    "we can consider a partial recursive function as a computer @xmath59 which takes a program @xmath60 ( namely a binary string ) as an input , performs some computations , and gives a string @xmath61 , written on the given alphabet @xmath57 , as an output .",
    "the @xmath58 of a string @xmath54 is defined as the shortest binary program @xmath60 which gives @xmath54 as its output , namely @xmath62 we require that our computer is a universal computing machine .",
    "roughly speaking , a computing machine is called _ universal _ if it can simulate any other machine .",
    "in particular every real computer is a universal computing machine , provided that we assume that it has virtually infinite memory . for a precise definition see for example @xcite or @xcite .",
    "we have the following theorem    [ teo : aic - kolm ] if @xmath59 and @xmath63 are universal computing machines then @xmath64 where @xmath65 is a constant which depends only on @xmath59 and @xmath63 .",
    "this theorem implies that the information content @xmath66 of @xmath54 with respect to @xmath59 depends only on @xmath54 up to a fixed constant , then its asymptotic behaviour does not depend on the choice of @xmath59 .",
    "for this reason from now on we will write @xmath67 instead of @xmath68 .    the shortest program which gives a string as its output is a sort of encoding of the string , and the information which is necessary to reconstruct the string is contained in the program .",
    "unfortunately the coding procedure associated to the algorithmic information content can not be performed by any algorithm .",
    "this is a very deep statement and , in some sense , it is equivalent to the turing halting problem or to the gdel incompleteness theorem",
    ". then the algorithmic information content is a function not computable by any algorithm .    using the notion of algorithmic information content it is possible to define a notion of _ complexity _ for infinite strings .",
    "let @xmath69 be an infinite string on the alphabet @xmath50 , that is @xmath70 .",
    "we denote by @xmath71 the first @xmath52 symbols of the string @xmath69",
    ". then @xmath72 .",
    "the complexity measures the mean quantity of information in each digit of the string @xmath69 .",
    "formally    [ def : compl ] the _ complexity @xmath73 _ of an infinite string @xmath74 is given by @xmath75    using the method of symbolic dynamics it is possible to consider the information and the complexity of the orbits of a dynamical system .",
    "let @xmath2 be an ergodic dynamical system and let @xmath76 be a finite measurable partition of the space @xmath17 . to the partition @xmath13",
    "it is associated the finite alphabet @xmath77 .",
    "define a map @xmath78 , where @xmath79 , by @xmath80 the sequence @xmath81 associated to a point @xmath82 is called the _ symbolic orbit _ of @xmath83 relative to the partition @xmath13 .",
    "then we have the following definition .",
    "[ def : compl - ds ] the _ complexity @xmath84 relative to the partition @xmath13 _ of the orbit with initial condition @xmath82 is given by @xmath85    [ rem:1 ] in @xcite and in @xcite , using open covers and computable structures , notions of complexity of orbits of a dynamical system are defined that do not depend on the partition .",
    "but in this paper we are only interested in the complexity dependent on a partition for the same reason for which we study ks entropy relative to a partition .",
    "this notion of complexity has been related to the notion of ks entropy by the following theorem .",
    "[ teo : brudno ] let @xmath2 be an ergodic dynamical system and @xmath13 be a finite measurable partition , then @xmath86 for @xmath8-almost any @xmath82 .",
    "[ teo : white ] in the same hypothesis as before , for @xmath8-almost any @xmath82 it holds @xmath87    if the aic were a computable function , using the previous theorems we could compute the ks entropy relative to a partition .",
    "this approach can still be useful using _ optimal compression algorithms _ , that is algorithms which encode symbolic strings , giving an approximation of the information contained in a string , hence an approximation of its aic .",
    "for this approach see @xcite , where also a new compression algorithm is presented and applied to some well known chaotic systems .",
    "[ rem:2 ] the notion of complexity for dynamical systems has been studied for some well known weakly chaotic systems ( @xcite,@xcite ) for which it holds @xmath88 for all the partitions , with the aim of giving a classification of these systems according to the asymptotic behaviour of the aic of the orbits .",
    "[ rem:3 ] the notion of aic has been linked to other indices for dynamical systems .",
    "for example it has been related to the sensitivity to initial conditions ( @xcite ) and to the poincar recurrence times ( @xcite ) .",
    "we now introduce formal definitions for random perturbations of a dynamical system ( following @xcite ) .",
    "we also give some estimates for the ks entropy relative to a partition for the perturbed system using the tool of symbolic dynamics .",
    "let @xmath89 be a stochastic process of i.i.d .",
    "random variables , with each @xmath43 defined on the probability space @xmath4 with values on the interval @xmath5 $ ] , and @xmath90 as the induced distribution .    [ def : rand - pert ] a _ random perturbation _ of the dynamical system @xmath2 is a family of markov chains @xmath91 on the probability space @xmath4 with values on @xmath17 and with transition probabilities given by @xmath92 for all borel sets @xmath93 .",
    "the meaning of this definition is that the point @xmath83 is moved to the point @xmath94 under the unperturbed dynamics and then it disperses randomly with distribution @xmath90 .",
    "we will consider two different possible actions of the random perturbation ( see @xcite ) .",
    "we talk of _ output noise _ when the dynamics is driven only by the unperturbed map @xmath31 , and the dispersion is caused by a non exact observation of the point @xmath94 .",
    "that is given a point @xmath82 , its orbit is given by @xmath95 and our data are measured observing the orbit @xmath96 .",
    "instead we talk of _ dynamical noise _ when the dispersion is intrinsic in the dynamics and it is not caused by the observation , in this case the dynamics is driven by @xmath97 .",
    "so , for a point @xmath82 we have @xmath98 .",
    "[ def : rand - meas ] a probability measure @xmath6 on @xmath17 is an _ invariant measure _ of the randomly perturbed dynamical system @xmath7 if @xmath99 for all borel sets @xmath93 .",
    "we now start from the following assumptions :    ( i ) : :    there is an ergodic dynamical system @xmath2 with    finite ks entropy , and denote by @xmath48 its ks    entropy relative to a finite measurable partition @xmath13 of    diameter @xmath32 ; ( ii ) : :    we can analyze data produced by a random perturbation    @xmath7 of the system ( output or dynamical noise )    with fixed @xmath1 ; ( iii ) : :    there is an invariant and ergodic probability measure    @xmath6 on @xmath7 and the ks    entropy relative to the partition @xmath13 for the randomly    perturbed system is denoted by    @xmath100 .    to analyze our system by the algorithmic information content we have to use the method of symbolic dynamics . given the finite measurable partition @xmath76 of diameter @xmath101 , we can associate a symbolic orbit in @xmath102 to any orbit in the space @xmath17 using the map @xmath103 defined in equation ( [ eq : orb - simb ] )    in the case of the unperturbed system @xmath2 we denote by @xmath104 the image of the map @xmath103 .",
    "the set @xmath105 is closed with respect to the action of the usual shift map @xmath106 defined on @xmath107 .",
    "it is also possible to define the probability measure @xmath108 on @xmath105 induced by @xmath103 .",
    "so we can work on the ergodic dynamical system @xmath109 with ks entropy @xmath110 .",
    "analogously we define the map @xmath111 from the product space @xmath112 to @xmath107 in the following way : for each @xmath82 the map @xmath113 is the symbolic map for the markov chain @xmath91 .",
    "so for every point @xmath82 there is a set of possible symbolic orbits in @xmath114 that correspond to different realizations of the stochastic process @xmath89 .",
    "let @xmath115 be the probability measure induced on @xmath116 by the measure @xmath6 .",
    "then we study the ergodic dynamical system @xmath117 with ks entropy @xmath118 .    since now we will denote by @xmath119 and by @xmath69 the symbolic orbits in @xmath105 and @xmath116 respectively .    from the previous definitions ,",
    "the following propositions easily follow    [ prop : spazi - simb ] @xmath120    [ prop : ent - simb ] @xmath121    [ prop : spazi - ug ] if @xmath122 and @xmath108 is absolutely continuous with respect to @xmath115 ( @xmath123 ) then @xmath124",
    ".    * proof . * for @xmath115-almost any @xmath125 it holds @xmath126 thanks to theorems [ teo : brudno ] and [ teo : white ] . hence , since for @xmath108-almost all @xmath127 it holds @xmath128 and @xmath123 then @xmath124 .",
    "0.5 cm we conclude this section with a lower bound for the @xmath32-entropy of the random perturbed system .",
    "[ teo : kifer ] suppose that all transition probabilities @xmath129 have bounded densities @xmath130 , then @xmath131 .",
    "hence the ks entropy of a randomly perturbed system is always infinite , since it can be computed as the limit of @xmath49 for @xmath32 going to 0 .",
    "we now study the behaviour of the aic relative to a partition in randomly perturbed dynamical systems .",
    "we start with the case of output noise ( see previous section ) which is easier to be studied and hence it is useful to introduce the arguments we will use for the more important case of dynamical noise .",
    "let @xmath105 and @xmath116 be the symbolic spaces of the unperturbed and the perturbed systems respectively .",
    "we will study the @xmath32-entropy of the perturbed system using the aic of the symbolic orbits , @xmath119 and @xmath69 , of the original system and of its perturbed version .    at any step of our dynamical system",
    "the perturbation induced by the noise changes the position of the point , and so it could change the set of the partition in which the point is .",
    "so the @xmath52-th symbol of the string @xmath69 could be different from @xmath132 , the corresponding symbol of the string @xmath119 .",
    "we recall that since we are in the case of output noise , at the step @xmath133 there is no memory of the noise at the previous step .",
    "moreover since the random variables @xmath43 are independent , the probability @xmath134 of @xmath135 being different from @xmath132 does not depend on the step @xmath52 , neither on the set of the partition we consider .",
    "let @xmath136 be the ks entropy of a _",
    "@xmath134-bernoulli trial _ ,",
    "namely the stochastic process of independent bernoulli variables @xmath137 with parameter @xmath134 .",
    "it is well known that @xmath138 hence applying theorems [ teo : brudno ] and [ teo : white ] it holds @xmath139 where @xmath140 denotes the average of the aic over all the sequences produced by the @xmath134-bernoulli trial .    in this framework , we have    [ prop : disug - alta ] if the random variables @xmath43 have values in the interval @xmath5 $ ] , then @xmath141 \\leq { \\mathbb e}_\\mu [ aic(\\psi^n ) ] + np \\log \\left(2 \\left\\lceil \\frac{\\sigma}{\\epsilon } \\right\\rceil \\right ) + h_n(p)\\ ] ] where @xmath142 denotes the upper integer part of a real number and @xmath143 $ ] denotes the mean value .    * proof . * to prove the proposition we show an algorithm to describe the string @xmath71 and compute the information it needs .",
    "let assume that we know the string @xmath144 .",
    "we have then just to specify the symbols of @xmath71 that are different from those of @xmath144 .",
    "we call @xmath145 this information .",
    "we show that for all strings @xmath71 and @xmath144 it holds @xmath146 the algorithm to describe @xmath71 works as follows .",
    "first it describes a binary string @xmath147 , where @xmath148 implies that @xmath149 and @xmath150 otherwise . to this aim",
    "we need on average @xmath140 bits of information , since the string @xmath54 is obtained as a string of a @xmath134-bernoulli trial .",
    "moreover the algorithm needs also to specify how far on the left or on the right the noise has moved the point . to this aim",
    "we need @xmath151 bits of information for each symbol of @xmath71 different from the corresponding symbol of @xmath144 .",
    "the factor @xmath152 counts exactly how many sets of the partition @xmath13 can be covered by the effect of the noise , and the factor 2 is needed to specify whether the point is moved to the right or to the left .    hence to specify the string @xmath71",
    "we do not need more than @xmath153 bits of information .",
    "if we evaluate the mean of @xmath154 over the measure @xmath6 then the thesis follows from the definition of the probability @xmath134 .",
    "[ teo : stima - noise ] if @xmath48 is the ks entropy relative to a partition @xmath13 of diameter @xmath32 of an unperturbed dynamical system @xmath2 , for the @xmath32-entropy @xmath100 of the system perturbed by an output noise @xmath89 with values in the interval @xmath5 $ ] it holds @xmath155    * proof .",
    "* the thesis follows from proposition [ prop : disug - alta ] , using equations ( [ eq : no - rum ] ) and ( [ eq : si - rum ] ) for @xmath48 and @xmath49 , and using equation ( [ eq : p_ber ] ) .",
    "we now study the case of dynamical noise .",
    "we assume to have an unperturbed dynamical system @xmath2 and a dynamical perturbation due to a stochastic process @xmath89 of i.i.d .",
    "random variables with values in the interval @xmath5 $ ] , such that the points @xmath82 follow orbits given by @xmath156 .",
    "let consider a partition @xmath13 of the space @xmath17 in a finite number @xmath157 of measurable sets @xmath158 , such that the diameter of each set of the partition is @xmath159 .",
    "let @xmath160 be the space of the symbolic orbits associated to the dynamical system , and denote by @xmath69 a single string in @xmath116 , given by the measurements made on the system .",
    "that is @xmath69 is the symbolic orbit of a point @xmath82 after the effect of the noise . to estimate the algorithmic information content of the string @xmath69 from above",
    ", we show how to construct it from the knowledge of the noise and of the unperturbed dynamics .",
    "let @xmath161 denote the initial point of an orbit of the system . by the classical symbolic representation of orbits",
    ", there is a symbolic string @xmath162 , relative to the partition @xmath13 , associated to the point @xmath161 for the unperturbed dynamical system , that is @xmath163 for all @xmath164 , and there is also the symbolic string @xmath165 that is given by the perturbed system .",
    "we have @xmath166 . in the case of dynamical noise",
    ", we have to consider the effect due to the fact that the action of the noise is not forgotten at each step .",
    "so we have to extend the knowledge of the unperturbed orbit to more than just one iteration . to find a good number of iterations to be stored",
    ", we use the approximation of the kolmogorov - sinai entropy @xmath167 relative to the partition @xmath13 by the decreasing sequence @xmath168 , where @xmath169 and @xmath170 for any two finite partitions @xmath171 and @xmath172 .",
    "hence for any @xmath173 there exists @xmath174 such that @xmath175    let @xmath176 be fixed , and find the corresponding integer @xmath177 according to equation ( [ eq : approx - ent ] ) .",
    "assume that for the initial point @xmath161 of the orbit , we know the first @xmath177 digits of the sequence @xmath178 .",
    "that is we assume to know the cylinder @xmath179 , where with this notation we mean the set of the partition @xmath180 containing @xmath161 .",
    "let @xmath181 and @xmath182 . as before we denote by @xmath183 the unperturbed symbolic orbit associated to the point @xmath184 .",
    "we assume to know the action of the noise @xmath185 with respect to the partition @xmath180 , so we compare the strength of the noise @xmath1 with the diameter to be uniform , so we consider the diameter to be the smaller diameter of the intervals that make the partition @xmath180 . ]",
    "@xmath186 of the partition @xmath180 .",
    "moreover , to know @xmath187 we only need the symbol @xmath188 , and to know @xmath189 , once we have @xmath187 , we only need the action of @xmath185 with respect to the partition @xmath180 .",
    "this is enough to obtain @xmath190 .",
    "so to obtain the first two symbols of the string @xmath69 it is enough the following information : @xmath191 , @xmath192 and @xmath193 .",
    "we have used @xmath194 to denote the conditional information .    at this point",
    "it is possible to iterate the argument with the string @xmath183 instead of the string @xmath178 , hence to find the symbol @xmath195 using the following information : @xmath196 , @xmath197 ( which is given by @xmath189 and is indeed known ) and @xmath198 .    iterating this argument we obtain the following proposition :    [ prop : dyn - disug - alta ]",
    "if the random variables @xmath43 have values in the interval @xmath5 $ ] , then for any @xmath173 there exists @xmath174 and a constant @xmath199 such that @xmath141 \\leq ( n-1 ) \\left [ h_\\mu(\\epsilon ) + \\delta + p \\log \\left(2 \\left\\lceil \\frac{\\sigma}{\\epsilon_{n_0 } } \\right\\rceil \\right ) \\right ] + h_{n-1}(p)+c\\ ] ] where @xmath200 denotes the upper integer part of a real number , the probability @xmath134 is defined as in proposition [ prop : disug - alta ] and @xmath201 .",
    "* to obtain the entropy on the right hand side we only have to use equation ( [ eq : approx - ent ] ) , and to remark that the conditional entropy @xmath202 is obtained as the integral over the space of all admissible strings of the information function @xmath203 .",
    "the constant @xmath59 is the information @xmath191 needed to start the iteration .    for the part related to the noise we repeat the same argument of proposition [ prop : disug - alta ] .",
    "then the action of the noise can be bounded by @xmath204=(n-1)p \\log \\left(2 \\left\\lceil \\frac{\\sigma}{\\epsilon_{n_0 } } \\right\\rceil \\right ) + h_{n-1}(p)\\ ] ] bits of information .",
    "0.5 cm let again @xmath100 be the @xmath32-entropy of the perturbed dynamical system , hence it follows    [ teo : dyn - stima - noise ] if @xmath48 is the @xmath32-entropy of an unperturbed dynamical system @xmath2 , for the @xmath32-entropy @xmath100 of the system perturbed by a dynamical noise @xmath89 it holds : for any @xmath173 there exists @xmath205 such that @xmath206      at this point we can make some conclusions about the estimates of the ks entropy of a randomly perturbed system .",
    "we recall the conditions we imposed in section [ sec : estim ] on the data we study .",
    "so , given a fixed @xmath1 ,",
    "we vary the diameter @xmath32 of the partition considered .    let @xmath207 . in this case",
    "we can assume that for some @xmath32 we have @xmath208 .",
    "this implies ( propositions [ prop : spazi - simb ] and [ prop : spazi - ug ] ) that @xmath209 . when @xmath210 we have that , fixed @xmath176 such that @xmath211 , if @xmath212 @xmath213 using proposition [ teo : kifer ] and theorem [ teo : dyn - stima - noise ] .",
    "we recall that @xmath136 takes its maximum value @xmath214 for @xmath215 , and it converges to @xmath216 as @xmath134 goes to @xmath216 or @xmath217 .",
    "then if @xmath218 we have that @xmath134 converges to @xmath216 , hence @xmath219 converges to @xmath216 , and equation ( [ eq : e > s ] ) becomes @xmath220 .",
    "in particular this implies equation ( [ eq : kifer ] ) . if instead @xmath221 then we have the same conclusion as in the following case .",
    "if @xmath222 then it holds @xmath223 \\le \\min \\left ( -\\log \\epsilon   , h_\\mu ( \\epsilon ) + \\delta + p \\log \\left(2 \\left\\lceil \\frac{\\sigma}{\\epsilon_{n_0 } } \\right\\rceil \\right ) + h(p ) \\right )   \\end{array}\\ ] ] using propositions [ prop : ent - simb ] , [ teo : kifer ] and theorem [ teo : dyn - stima - noise ] . again",
    "if @xmath224 then @xmath134 converges to @xmath217 , hence @xmath136 converges to @xmath216 , and the estimate on the right hand side of equation ( [ eq : e < s ] ) becomes @xmath225    from equations ( [ eq : e > s ] ) and ( [ eq : e < s ] ) it emerges that by computing the ks entropy relative to partitions , we obtain an indication of the smallness of the random perturbation present in the data we are studying . indeed",
    "if in the curve @xmath49 we find an almost flat region ( @xmath226 ) , and as @xmath32 decreases there is a change in the behaviour of the curve , that almost behaves as @xmath227 ( @xmath228 ) , then the interval @xmath229 should be a good approximation for the size @xmath1 of the perturbation . in the next section",
    "we apply this method to a perturbed dynamical system , using a compression algorithm to estimate the algorithmic information content .",
    "moreover this method can also be thought of as a method to distinguish between purely stochastic systems and randomly perturbed dynamical systems .",
    "indeed for randomly perturbed systems equations ( [ eq : e > s ] ) and ( [ eq : e < s ] ) suggest the presence of a change of behaviour for the @xmath32-entropy at the level of the perturbation .",
    "this change should not appear in purely stochastic systems .",
    "however this aspect has to be examined further .",
    "in the previous section we have proved that the algorithmic information content can be used for randomly perturbed dynamical systems to obtain information on the size of the random perturbation .",
    "we remark that unfortunately the aic , as defined in equation ( [ eq : aic ] ) , is not a computable function .",
    "so to compute the information content of a string one needs to use a _ compression algorithm _ , that is an algorithm which approximates the value of the information function aic .    formally , a _ compression algorithm _ is defined as a recursive reversible coding procedure @xmath230 ( for example , the data compression algorithms that are in any personal computer ) .",
    "the information content @xmath231 of a finite string @xmath54 computed by the algorithm @xmath13 is given by the binary length of the compressed string , that is @xmath232 .",
    "not all compression algorithms approximate the aic . in @xcite",
    "this point is discussed in details . for the purposes of this paper , it is sufficient that @xmath233 where @xmath73 is the complexity of an infinite string @xmath69 ( see definition [ def : compl ] ) . the algorithms with this property are called _",
    "optimal_.    in @xcite it is also introduced a compression algorithm called _ castore _ , which has been created with the specific aim of studying weakly chaotic dynamical systems , namely chaotic systems with null ks entropy .",
    "using castore we have analyzed an example of randomly perturbed dynamical system .",
    "we have chosen the logistic map @xmath234 on the interval @xmath39 $ ] with @xmath15 . to this map",
    "we have added a dynamical noise given by independent random variables @xmath235 uniformly distributed on the interval @xmath5 $ ] .",
    "keeping @xmath1 fixed we have varied the diameter of the partition from 0.5 to 0.004=1/250 , and we have computed the complexity of the @xmath236-long symbolic orbits of the perturbed system relative to the different partitions .    in figure [ fig:1 ] we show the results .",
    "it is a log - linear plot of the complexity of the symbolic orbits versus the diameter of the partition , plotted on the x axis .",
    "the solid line is the upper bound @xmath237 , and the other curves are the empirical curves which correspond to the following values of @xmath1 : 0.5 , 0.1 , 0.02 , 0.01 , 0.001 from the upper to the lower , respectively . for each of the curves",
    "@xmath100 it is possible to identify the intervals @xmath238 , that give good approximation of @xmath1 , in agreement with the theoretical result in equations ( [ eq : e > s ] ) and ( [ eq : e < s ] ) and the subsequent comments .",
    "when the diameter of the partition is high with respect to the size of the noise , then we have a good approximation of the unperturbed ks entropy of the system , since the partition with diameter @xmath239 is generating . in figure",
    "[ fig:1 ] it is possible to see that the complexity of the symbolic orbits for partitions with diameter close to @xmath239 is close to @xmath217 ( the ks entropy , but we use logarithms in base @xmath240 since we measure binary information contents .",
    "] of the logistic map ) for the empirical curves with @xmath241 ."
  ],
  "abstract_text": [
    "<S> in this paper we prove estimates on the behaviour of the kolmogorov - sinai entropy relative to a partition for randomly perturbed dynamical systems . </S>",
    "<S> our estimates use the entropy for the unperturbed system and are obtained using the notion of algorithmic information content . </S>",
    "<S> the main result is an extension of known results to study time series obtained by the observation of real systems . </S>"
  ]
}