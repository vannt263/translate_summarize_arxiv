{
  "article_text": [
    "this paper is motivated by a recent consulting case in which a company wants to evaluate the performance of its @xmath14 offices at different locations . the performance of office no .",
    "@xmath15 is quantified by a certain measure @xmath16 of costs per unit , but it is clear that it is influenced by various covariables @xmath17 describing , for instance , regional factors which can not be altered by the offices .",
    "the idea is to eliminate these effects via a ( linear ) regression model , assuming that @xmath18 for some unknown handycap function @xmath19 of the covariable vectors @xmath20 , and @xmath8 are the corrected costs per unit of the @xmath15-th office .",
    "now the proposal is to use a linear model for the regression function @xmath19 and to estimate it via least squares or least absolute deviations .",
    "that means , we determine a regression function @xmath21 within a given model @xmath22 such that @xmath23 becomes minimal , where @xmath24 is the residual of the fitted regression function @xmath21 .",
    "based on these residuals one computes the ranks @xmath25 as a surrogate for @xmath26    this procedure is simpler than an established method of benchmarking , called data envelopement analysis ( dea ) , initiated by farrel ( 1957 ) and charnes et al .",
    "very roughly saying , in that approach one assumes that the @xmath8 are non - negative , and @xmath27 describes the minimally achievable costs per unit .",
    "the corrected cost measures @xmath8 are estimated via a linear optimization method .",
    "the main reasons for the company to use the regression approach rather than dea were the higher complexity of dea , which made it difficult to communicate it to employees , and the known sensitivity of dea to errors in the data .",
    "moreover , normal quantile - quantile plots of the residuals showed no serious violation of a gaussian distribution , whereas the dea paradigm would predict a non - symmetric , right - skewed distribution .",
    "this is certainly a non - standard application of regression methods in the sense that the regression function is treated as a nuisance parameter while the `` errors '' @xmath8 are of primary interest .",
    "the problem with that approach is that these `` errors '' may fail to satisfy common assumptions such as independence , mean or median zero and homoscedasticity .",
    "indeed , it may happen that the numbers @xmath8 , as a measure of the offices individual performance ( motivation , efficiency etc . ) , are correlated with the covariable vectors @xmath28 .",
    "but then the residuals @xmath10 are systematically different from the numbers @xmath8 . by the way",
    ", dea may suffer from the same problem , in particular , when the performance of most offices is still far from optimal .",
    "even if the assumed model @xmath22 is correct and if the `` errors '' @xmath8 satisfy the standard assumptions of being independent and following all the same distribution @xmath29 for some @xmath30 , the average absolute difference between the ranks @xmath9 and @xmath7 may be substantial . in the present paper",
    "we derive an explicit expression for the `` rank distortions '' @xmath31 i.e.  upper bounds for @xmath32 , in case of traditional least - squares regression .",
    "section  [ sec : results ] contains the main results , an exact formula and approximations .",
    "section  [ sec : heuristics ] provides a heuristical derivation of an approximation of the rank distortions which also indicates what may happen in non - gaussian settings .",
    "presumably these arguments could be made rigorous by applying similar techniques and arguments as koul ( 1969 , 1992 ) , loynes ( 1980 ) and mammen ( 1996 ) .",
    "section  [ sec : proofs ] contains rigorous proofs which do rely on the errors @xmath8 being independent with the same gaussian distribution .",
    "the advantage of that is that minimal assumptions are imposed on the underlying design matrix .",
    "we consider a linear regression model with a random vector @xmath33 here @xmath34 is a given design matrix with @xmath35 , @xmath3 is an unknown parameter vector in @xmath36 , and @xmath5 is an unobserved random vector with distribution @xmath37 with unknown @xmath30 . in our specific setting ,",
    "@xmath38 with the observed covariable vectors @xmath39 and a given basis @xmath40 of the model @xmath22 .    let us recall some well - known facts from linear models ( cf .",
    "ryan 1997 ) .",
    "the least - squares estimator of @xmath3 is given by @xmath41 , and the fitted vector @xmath42 may be written as @xmath43 with the `` hat matrix '' @xmath44 this matrix describes the orthogonal projection onto the column space of @xmath1 and satisfies @xmath45 .",
    "moreover , since @xmath46 for any unit vector @xmath47 , one can easily verify that all `` leverages '' @xmath48 satisfy @xmath49 and @xmath50 .",
    "the residual vector @xmath51 may be written as @xmath52 under a mild regularity condition on the hat matrix @xmath53 , the residuals @xmath10 are pairwise different :    [ lemma : hat matrix ] for arbitrary indices @xmath54 , @xmath55 the condition @xmath56 implies that @xmath57 .",
    "this lemma remains valid if the errors @xmath58 are only assumed to be independent with continuous distributions .",
    "an immediate consequence of lemma  [ lemma : hat matrix ] is that the residuals @xmath10 are pairwise different almost surely , whenever @xmath57 for at most one index @xmath15 .",
    "now we are ready to state our first main result about the ranks @xmath7 and @xmath9 :    [ theorem : exact ] suppose that @xmath57 for at most one index @xmath59 . then for arbitrary indices @xmath60 , + & = & _ k,=1^n ( ( ) + ( ) + & & -  ( ) - ( ) ) , where @xmath61 , @xmath62 and @xmath63 . in particular , ( _ i - r_i)^2 & = & _ k=1^n ( - 2 ( ) ) + & & +   _ 1 k < n ( + ( ) + & & -  ( ) - ( ) ) .    here and throughout @xmath64 denotes kronecker s symbol , i.e.  @xmath64 equals one if @xmath65 and zero otherwise .",
    "theorem  [ theorem : exact ] is useful for exact numerical calculations .",
    "it was used in the aforementioned consulting case to show that rank distortions may be substantial .",
    "numerical experiments revealed also that the rank distortions are closely related to the leverages @xmath48 . recall that @xmath66 here is a theoretical result about the rank distortions in case of small maximal leverage :    [ theorem : approximate ] suppose that the column space of @xmath1 contains the constant vectors , i.e.  @xmath67 . then , as @xmath68 , @xmath69 uniformly in @xmath60 .",
    "[ [ a - numerical - example . ] ] a numerical example .",
    "+ + + + + + + + + + + + + + + + + + + +    suppose that @xmath70 , and let @xmath1 be equal to @xmath71 the design matrix for simple linear or quadratic regression , where @xmath72 are equispaced numbers .",
    "figure  [ fig : example ] shows the pairs @xmath73 for both cases . in addition the approximations @xmath74 and @xmath75 are shown as lines .",
    "equispaced @xmath76-values.,scaledwidth=100.0% ]",
    "asymptotic statements in this section are meant as @xmath77 .",
    "we assume that the errors @xmath58 are independent and identically distributed with finite standard deviation @xmath78 and c.d.f .",
    "@xmath79 with bounded and uniformly continuous density @xmath19 .",
    "one can easily deduce from @xmath80 that @xmath81 whereas the cauchy - schwarz inequality implies that @xmath82 hence @xmath83    pretending that the empirical c.d.f .",
    "@xmath84 of the errors @xmath8 and the empirical c.d.f .",
    "@xmath85 of the residuals @xmath10 are sufficiently close to @xmath79 , we write r_i  =  n ( _ i ) & & n f(_i ) , + _",
    "i  =  n ( _ i ) & & n f(_i )  =  n f ( _ i - ( ) _ i ) . but",
    "@xmath86 is quite small , precisely , @xmath87 by .",
    "hence we write @xmath88 moreover , for @xmath60 and @xmath89 , @xmath90 because @xmath91 is very small in the sense that @xmath92 by .",
    "thus we pretend that the random pairs @xmath93 and @xmath94 are stochastically independent and conjecture that @xmath95    now consider the special case of @xmath96 and @xmath97 with the standard gaussian c.d.f .",
    "@xmath98 and density @xmath99 . for @xmath100 , ^2 f(_i )",
    "f(_j ) & = & ( f(_i ) ) ^2 + & = & ( ^-1 ( ^-1 x)^2 dx ) ^2 + & = & ( ( 2)^-1/2 ( x ) dx ) ^2 + & = & ( 4)^-1 , and ^2 f(_i)^2 & = & ^-1 ( ^-1 x)^3 dx + & = & ( 2)^-1 ( x ) dx + & = & ( 2 ) ^-1 .",
    "hence the conjectured approximation equals @xmath101",
    "we write @xmath102 with the companion hat matrix @xmath103 describing the orthogonal projection on the orthogonal complement of the column space of @xmath1 . since @xmath104",
    "we may conclude that @xmath105 has a continuous distribution unless @xmath106 in the latter case , @xmath107 almost surely .",
    "but condition is equivalent to @xmath108 where we utilized @xmath109 .",
    "note further that entails that @xmath110 hence implies that @xmath111 obviously the latter condition yields .",
    "consequently , the three conditions , and are equivalent . since @xmath112 , one may reformulate as @xmath113          since the probability in question does not change when we replace @xmath16 with @xmath121 , we may assume without loss of generality that @xmath122 and @xmath123 .",
    "if @xmath124 denotes a random vector with standard gaussian distribution on @xmath125 , then @xmath117 has the same distribution as @xmath126^\\top$ ] , where @xmath127 .",
    "now we write @xmath128 and @xmath129 with @xmath130 $ ] , and @xmath131^\\top$ ] , where @xmath132 almost surely , and @xmath133 is uniformly distributed on @xmath134 $ ]",
    ". then ( y_1 0   y_2 0 ) & = & ( ( ) 0  ( ) ( ) + ( ) ( ) 0 ) + & = & ( ( ) 0  ( + ) 0 ) + & = & (  + + 2 ) + & = & ( ) + & = & .",
    "+    according to lemma  [ lemma : hat matrix ] , r_i & = & 1 + _ k i 1\\{_k _ i }  =  1 + _ k i 1\\ { _ ki^ 0 } + _ i & = & 1 + _ k i 1\\{_k _ i }  =  1 + _ k i 1\\ { _ ki^ 0 } almost surely , where @xmath135 and @xmath136 with the standard basis @xmath137 of @xmath138 .",
    "consequently it follows from lemma  [ lemma : arcsin ] that + & = & _ k i , j ( 1\\ { _ ki^ 0 } - 1\\ { _ ki^ 0 } ) ( 1\\ { _ j^ 0 } - 1\\ { _ j^ 0 } ) + & = & _ k i , j ( ( _ ki^ 0 , _",
    "j^ 0 ) + ( _ ki^ 0 , _ j^ 0 ) + & & -  ( _ ki^ 0 , _ j^ 0 ) - ( _ ki^ 0 , _ j^ 0 ) ) + & = & _ k i , j ( ( ( _ ki , _ j ) ) + ( ( _ ki , _ j ) ) + & & -  ( ( _ ki , _ j ) ) - ( ( _ ki,_j ) ) ) , where @xmath139 note that @xmath140 and with @xmath141 we may write _ ki^_j^ & = & ( _ k - _ i)^ ( _ - _ j ) + & = & g_k + g_ij - g_kj - g_i + & = & _ k , ij - h_k , ij , + _ ki^_j^ & = & ( _ k - _ i)^^ ( _ - _ j )  =  ( _ k - _ i)^ ( _ - _ j ) + & = & _ k , ij - h_k , ij .",
    "hence we obtain the formula + & = & _ k i , j ( ( ) + ( ) + & & -  ( ) - ( ) ) . but",
    "the restriction to indices @xmath142 and @xmath143 is superfluous , because @xmath144 whenever @xmath145 or @xmath146 .",
    "this yields the first asserted formula .",
    "in the special case of @xmath147 , note that @xmath148 if @xmath149 .",
    "if we replace @xmath150 with @xmath151 in our formula for @xmath152 , we end up with the expression & & _ k,= 1^n ( ( ) + ( ) + & & -  ( ) - ( ) ) . but for @xmath145 or @xmath153 the corresponding summands are equal to zero ,",
    "because @xmath145 implies that @xmath154 , and @xmath153 implies that @xmath155 . distinguishing the cases @xmath156 and @xmath157 yields ( _ i - r_i)^2 & = & _ k = 1^n ( - 2 ( ) ) + & & +   _ 1 k < n ( + ( ) + & & -  ( ) - ( ) ) . finally the assertion follows from the well - known fact that @xmath158 for @xmath159 .",
    "first recall that @xmath160 , whence @xmath161 .",
    "furthermore , @xmath162 whenever @xmath163 and @xmath157 , i.e.  @xmath164 for at most @xmath165 index pairs @xmath166 .",
    "elementary calculus shows that @xmath167 for some constant @xmath168 , the optimal one being @xmath169 .",
    "hence + & & -  ( ) - ( ) | + & = & o(^1/2 ) uniformly in @xmath170 and @xmath171 .",
    "consequently , + & = & _ k,=1^n ( ( ) + ( ) + & & -  ( ) - ( ) ) + o(^1/2 ) uniformly in @xmath60 .",
    "but for @xmath172 $ ] and @xmath173 $ ] , & & ( ) + ( ) - ( ) - ( ) + & &  =  ( ) - ( ) + & & + ( ) - ( ) + & & + ( ) - ( ) + & &  = ( ) - ( ) + & & + ( ) - ( ( 1 + + o(^2 ) ) ) + & & + ( ( 1 + + o(^2 ) ) ) - ( ) + & &  = ",
    "( + o ( ) ) + & & +  ( + o ( ) ) ( + o(^2 ) ) +  ( + o ( ) ) ( + o(^2 ) ) + & &  = + o(^2 ) .",
    "consequently , @xmath174 but it follows from @xmath175 that @xmath176 +    9 a.  charnes , w.w .",
    "cooper and e.  rhodes ( 1978 ) . measuring the efficiency of decision making units .",
    "_ europ .",
    "j.  oper .",
    "res .  * 2 * _ , 429 - 444 .",
    "farrell ( 1957 ) .",
    "the measurement of productive efficiency .",
    "_ j.  royal statist .",
    "a * 120 * _ , 253 - 281 .",
    "h.  koul ( 1969 ) .",
    "asymptotic behavior of wilcoxon type confidence regions in multiple linear regression .",
    "statist .",
    "* 40 * _ , 1950 - 1979 .",
    "h.  koul ( 1992 ) .",
    "_ weighted empiricals and linear models .",
    "_ ims , hayword , ca .",
    "r.m .  loynes ( 1980 ) . the empirical distribution function of residuals from generalised regression .",
    ".  statist .",
    "* 8 * _ , 285 - 298 .",
    "e.  mammen ( 1996 ) .",
    "empiricla process of residuals for high - dimensional linear models .",
    ".  statist .  * 24 * _ , 307 - 335 .",
    "ryan ( 1997 ) .",
    "_ modern regression methods",
    ". _ wiley , new york ."
  ],
  "abstract_text": [
    "<S> consider the standard linear regression model @xmath0 with given design matrix @xmath1 ( @xmath2 ) , unknown parameter @xmath3 ( @xmath4 ) and unobserved error vector @xmath5 ( @xmath6 ) with i.i.d </S>",
    "<S> .  centered gaussian components . </S>",
    "<S> motivated by an application in economics , we compare the ranks @xmath7 of the errors @xmath8 with the ranks @xmath9 of the residuals @xmath10 , where @xmath11 with the least squares estimator @xmath12 . </S>",
    "<S> exact and approximate formulae are given for the rank distortions @xmath13 .    </S>",
    "<S> [ [ key - words ] ] key words : + + + + + + + + + +    leverage , rank distortion . </S>"
  ]
}