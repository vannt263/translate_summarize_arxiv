{
  "article_text": [
    "wu and vos @xcite introduce a parameter - free distribution estimation framework and utilize the kullback  leibler ( kl ) divergence as a loss function .",
    "they show that the kl risk of a distribution estimator obtained from an i.i.d .",
    "sample decomposes in a fashion parallel to the mean squared error decomposition for a parameter estimator , and that an estimator is distribution unbiased , or simply unbiased , if and only if its distribution mean is equal to the true distribution .",
    "distribution unbiasedness can be defined without using any parameterization .",
    "we call this approach parameter - free even though there may be applications where it is desirable to use a particular parameterization .",
    "when the distributions are , in fact , parametrically indexed , distribution unbiasedness handles multiple parameters simultaneously and is consistent under reparametrization .",
    "wu and vos @xcite also show that the mle for distributions in the exponential family is always distribution unbiased .",
    "the kl expectation and variance functions @xmath3 and @xmath4 are defined by minimizing over the space of all distributions .",
    "these functions completely describe an estimator in terms of its kl divergence around any distribution . in this paper",
    ", we introduce distribution expectation and variance functions @xmath0 and @xmath1 that are defined by minimizing over a smaller space of distributions . for exponential and mixture families , the expected kl risk is a function only of these quantities .    even though the focus of this paper is on parametric exponential families ,",
    "our approach is parameter - free in that the definitions and results are provided without regard to the parameterization of the family .",
    "there are three advantages to this approach : one , the lack of invariance of bias across parameter transformations is avoided ; two , we can allow for estimators taking values outside of the exponential family ; three , the case where the true distribution does not belong to the family is easily addressed .",
    "section  [ sec : kullback - leibler - risk ,- variance , ] introduces the distribution expectation and variance functions and shows how these are a generalization of the mean and expectation functions for mean square error .",
    "exponential families and their extension are discussed in section  [ sec : exponential - family ] .",
    "the fundamental properties of the distribution mean and variance functions allow using the ideas of rao ",
    "blackwell @xcite to show that the mle is the unique uniformly minimum distribution variance unbiased estimator ( umv@xmath2ue ) .",
    "this result is proved in section  [ sec : rao - blackwell - and - the ] .",
    "three examples are given in section  [ sec : examples ] and section  [ sec : discussions ] contains further remarks .",
    "the parametric version of the rao ",
    "blackwell theorem can be proved using a pythagorean relationship that holds for mean square error ( mse ) and the expectation operator . to prove the distribution version of the rao ",
    "blackwell theorem , we use a similar relationship that holds for kl risk and the kl expectation along with a second pythagorean relationship that holds in exponential families for kl divergence and the kl projection .",
    "basic properties of the expectation operator for real - valued random variables used in the proof can be extended to distribution - valued random variables .",
    "we begin with the property that the expectation minimizes the mse .    for (",
    "real - valued ) random variable @xmath5 and @xmath6 we can define the average behavior of @xmath5 relative to @xmath7 using the risk function @xmath8,\\ ] ] where @xmath9 is a loss function , that is , a nonnegative convex function on @xmath10 . when @xmath11<\\infty$ ] for some @xmath7 , we define @xmath12\\ ] ] and @xmath13\\ ] ] if the minimum exists , in which case , @xmath14.\\ ] ]    when @xmath15 , that is , risk is mse , we have @xmath16    =    \\int y\\,\\mathrm{d}r_{0}\\defeq ey , \\\\",
    "\\label{eq : vequiv}v_{l}y&\\defeq&\\inf_{b\\in\\mathbb{r}}e\\bigl[l(y , b)\\bigr ]    =   e \\bigl[l(y , ey)\\bigr]\\defeq vy.\\end{aligned}\\ ] ] note that we use the loss function as subscript to indicate expectation and variance defined in terms of an argmin and infimum of the loss function , while expectations and variances without a subscript are defined in terms of an integral , or in terms of a sum if the sample space is discrete .",
    "the middle equality signs in equations ( [ eq : eequiv ] ) and ( [ eq : vequiv ] ) are well - known results for @xmath17 and @xmath18 .",
    "these two values completely characterize the risk because of the relationship @xmath19=\\l ( e_{l}y , a ) + \\v_{l}y \\quad\\quad\\forall a \\in\\mathbb{r}.\\ ] ] in particular , the mse for a random variable @xmath5 is completely determined by knowing its expectation @xmath20 and variance @xmath21 .",
    "note that ( [ eq : mse1 ] ) holds for any distribution function such that @xmath20 and @xmath21 exist . for general loss functions",
    "@xmath9 , the argmin @xmath22 and min @xmath23 do not characterize the risk ; that is , @xmath8-d ( e_{d}y , a ) \\ ] ] will be a function of @xmath7 .",
    "the expectation and variance also have the following conditional properties @xmath24 , \\\\",
    "\\label{eq : mse3}\\v y & = & \\v\\e [ y|x ] + \\e\\bigl[\\v(y|x)\\bigr].\\end{aligned}\\ ] ] in the next section , we consider random variables that take values on a space of distributions @xmath25 and show that when the kl divergence is used to compare distributions , equations ( [ eq : eequiv ] ) through ( [ eq : mse3 ] ) hold for kl risk .",
    "let @xmath26 be a sample space equipped with a @xmath27-finite measure @xmath28 .",
    "when @xmath29 is finite or countable , @xmath28 is usually the counting measure .",
    "when @xmath30 and @xmath29 contains an open set of @xmath31 for some @xmath32 , then @xmath28 is usually the lebesgue measure on @xmath31 . requiring @xmath29 to contain an open set implies that the dimension of @xmath29 is @xmath9 .",
    "let @xmath25 be the collection of all probability measures @xmath33 on @xmath26 that are absolutely continuous with respect to @xmath28 , that is , @xmath34 implies @xmath35 for all @xmath36 .",
    "this is denoted as @xmath37 .",
    "note that we allow the support of @xmath33 to be a proper subset of @xmath29 .",
    "let @xmath38 ( in bold font ) be a random quantity whose values are distributions in @xmath25",
    ". the density of the distribution @xmath33 with respect to @xmath28 will be denoted by @xmath39 ( in lower case ) , and the corresponding random variable by @xmath40 ( in bold font lower case ) . following definition 2.1 in @xcite",
    ", @xmath38 is an @xmath25-valued random variable if @xmath41 is a real - valued random variable for all @xmath36 .",
    "we are considering the problem of estimating a distribution so for this paper @xmath42 is any estimator of an unknown distribution @xmath43 where @xmath44 is an i.i.d .",
    "sample from @xmath45 .",
    "a random distribution is a mapping from @xmath46 to @xmath25 .",
    "let @xmath47 be another random quantity that is jointly distributed with @xmath38 .",
    "[ the : ptilde]for every @xmath48 , @xmath49 $ ] is a probability measure that is absolutely continuous with respect to @xmath28 , that is , @xmath50 , is unique up to measure zero ( @xmath28 ) , and has a density @xmath51   \\quad\\quad \\mbox{for }    y\\in \\mathbb{x}.\\ ] ] in addition , when @xmath52 is replaced with the random variable @xmath47 , @xmath53 $ ] is an @xmath25-valued random variable .",
    "for all @xmath52 it is easily seen that @xmath54 is a probability measure because @xmath54 is countably additive and @xmath55 , where @xmath56 is the empty set .",
    "the remaining claims of the theorem can be established by noting that equation ( [ eq : ptilde ] ) can be written as @xmath57 where @xmath58 is the conditional distribution of @xmath59 given @xmath52 . since @xmath60=\\int_{\\mathbb{x}^{n}}\\int _ { a}r_{\\mathbf{x}}(y)\\,\\mathrm{d}\\lambda(y)r_{0}^{n } ( \\mathbf{x}|s)\\,\\mathrm{d}\\lambda^{n}(\\mathbf{x}),\\ ] ]",
    "the set @xmath61 is arbitrary , and the integrals can be interchanged , we see that @xmath62 is the density for @xmath54 and @xmath50 for each @xmath52 so @xmath63 is an @xmath25-valued random variable .    for @xmath25-valued random variable @xmath64 and @xmath65",
    "we can define the average behavior of @xmath64 relative to @xmath33 using the risk function @xmath66,\\ ] ] where @xmath9 is a loss function , that is , a nonnegative convex function on @xmath67 . note that the expectation used to define the risk is with respect to some distribution @xmath43 ; @xmath45 will be fixed but arbitrary other than constraints to ensure that the quantities in the expressions below exist and that the support of @xmath45 is @xmath29 . for any function @xmath9 such that @xmath68<\\infty$ ] for some @xmath33 , we define @xmath69\\ ] ] and @xmath70\\ ] ] if the minimum exists , in which case , @xmath71.\\ ] ] for kl risk , that is , when @xmath72 , we have @xmath73    =    \\int\\r_{\\mathbf{x}}(y)r_{0}^{n}(\\mathbf{x})\\ , \\mathrm{d}\\lambda^{n}(\\mathbf{x})\\defeq e\\r , \\\\",
    "\\label{eq : vequiv-1}v_{d}\\r&\\defeq&\\inf_{r_{1}\\in\\mathcal{r}}e\\bigl[d ( \\r , r_{1})\\bigr ]    =    ed(\\r , e\\r)\\defeq v\\r.\\end{aligned}\\ ] ] the middle equalities in equations ( [ eq : eequiv-1 ] ) and ( [ eq : vequiv-1 ] ) are established in wu and vos @xcite . since these are equal when @xmath74 is the kl divergence and we consider no other divergence functions on @xmath67 , we will simply write @xmath75 and @xmath76 for the kl mean and variance .",
    "furthermore , @xmath77 and @xmath78 completely characterize the average behavior of the @xmath25-valued random variable @xmath64 relative to any distribution @xmath79 because of the relationship @xmath80=d ( \\e\\r , r ) + \\v\\r \\quad\\quad\\forall r\\in\\mathcal{r}.\\ ] ] this means the kl risk for an @xmath25-valued random variable @xmath64 , having any distribution function , is completely determined by knowing its argmin , @xmath81 , and minimum , @xmath82 . when @xmath83 , equation ( [ eq : klr1 ] ) gives the decomposition of the kl risk in terms of bias and variance .",
    "the relationship in ( [ eq : klr1 ] ) will not hold for general nonnegative convex functions @xmath9 .",
    "in this paper we only consider kl divergence @xmath84 .",
    "furthermore , a conditional expectation on @xmath25-valued random variables can be defined so that the following conditional properties hold @xmath85 , \\\\ \\label{eq : klr3}v\\r & = & v\\e[\\r|\\s]+\\e\\bigl[v(\\r|\\s)\\bigr],\\end{aligned}\\ ] ] where @xmath86 could be @xmath25-valued but could also be real or other valued since values of @xmath86 will only be used to generate sub sigma fields .",
    "let @xmath43 have support @xmath29 and let @xmath64 be an @xmath25-valued random variable such that the kl mean @xmath87 and the kl variance @xmath88 exist and are finite .",
    "then for any @xmath79 the mean divergence between @xmath64 and @xmath33 depends only on the kl mean @xmath77 and kl variance @xmath78 .",
    "furthermore , the kl mean and kl variance satisfy the classical conditional equalities ( [ eq : klr2 ] ) and ( [ eq : klr3 ] ) .    equation ( [ eq : klr1 ] ) follows from the definition of kl variance and theorem 5.2 in @xcite who show that the expected kl loss @xmath89 $ ] from an @xmath25-valued random variable @xmath38 to a distribution @xmath79 decomposes as @xmath90=\\e\\bigl[d(\\r,\\e\\mathbf{r})\\bigr]+d(\\e\\r , r).\\ ] ] equation ( [ eq : klr2 ] ) follows from the fact that the kl means @xmath87 and @xmath91 $ ] have densities with respect to @xmath28 and the order of integration can be interchanged .",
    "the steps are the same as those that establish @xmath92 $ ] for @xmath93-valued random variables @xmath94 and @xmath5 .",
    "we rewrite ( [ eq : klr1 ] ) as @xmath95-d(\\e\\r , r)=\\v\\r.\\ ] ] note that both expectations ( with domain @xmath93-valued random variables and with domain @xmath25-valued random variables ) and the variance depend on the data generation distribution @xmath45 , which can be any point in @xmath25 with support @xmath29 .",
    "if this equation holds for random sample @xmath96 then it also applies to the conditional distribution of @xmath96 given @xmath97 @xmath98-\\d\\bigl(e[\\r|s],r\\bigr)=\\v(\\r|s).\\ ] ] substituting @xmath47 into the equation above and taking expectation gives @xmath99-\\e\\bigl[\\d\\bigl(\\e[\\r|\\s],r\\bigr)\\bigr]=\\e\\bigl[\\v(\\r|\\s ) \\bigr].\\ ] ] substituting @xmath91 $ ] into @xmath64 in ( [ eq : v1 ] ) and using @xmath100=\\e\\r$ ] gives @xmath101,r\\bigr)\\bigr]-d(\\e\\r , r)=\\v\\bigl(\\e[\\r|\\s ] \\bigr).\\ ] ] adding ( [ eq : v3 ] ) to ( [ eq : v4 ] ) and substituting from ( [ eq : v1 ] ) proves ( [ eq : klr3 ] ) .",
    "the random variable @xmath64 is a distribution function defined on the sample space and it will be useful to relate @xmath64 to a statistic @xmath102 .",
    "we define @xmath103 and when we consider only one statistic we write @xmath104 .",
    "the @xmath31-valued random variable @xmath105 describes the behavior of the @xmath25-valued random variable @xmath64 and the mean of @xmath105 can be obtained from the kl mean .",
    "[ thm : expectation - property - on ] for any statistic @xmath102 such that @xmath106 a.e .",
    ", the mean of @xmath102 under @xmath87 equals the mean of @xmath31-valued random variable @xmath105 @xmath107.\\label{eq : klexpectationproperty}\\ ] ]    the density for @xmath87 can be written as @xmath108 so that @xmath109\\end{aligned}\\ ] ] because the order of integration can be switched .",
    "we typically are interested in a subfamily of distributions @xmath111 and we describe a distribution in terms of the kl risk @xmath112 $ ] for @xmath113 .",
    "we add the regularity condition that the support of each distribution in @xmath110 is @xmath29 .",
    "equation ( [ eq : klr1 ] ) shows that @xmath87 and @xmath88 give the kl risk for any @xmath113 .",
    "however , generally @xmath114 even if @xmath64 takes values only in @xmath110 .",
    "we consider whether an expectation can be defined that takes values in @xmath110 and so that ( [ eq : klr1 ] ) holds",
    ". we will define this expectation as a minimum over @xmath110 .",
    "we define @xmath115\\ ] ] and @xmath116\\ ] ] if the minimum exists , in which case @xmath117.\\ ] ] equation ( [ eq : klr1 ] ) now becomes @xmath118=d\\bigl(\\ed\\r , p\\bigr)+\\vd\\r+\\delta\\bigl(\\e\\r,\\ed\\r , p\\bigr ) \\quad\\quad\\forall p\\in \\mathcal{p},\\ ] ] where @xmath119 if @xmath120 vanishes for all @xmath113 then the argmin @xmath121 and the min @xmath122 completely characterize @xmath64 in terms of kl risk .",
    "when @xmath120 is small these functions can be used to approximate the kl risk of @xmath64 .",
    "we will show the term @xmath120 vanishes when @xmath110 is an exponential family .",
    "the relationship between the expectations @xmath77 and @xmath121 can be expressed by using the kl projection onto @xmath110 @xmath123 by equation ( [ eq : klr1 ] ) , @xmath124 for any @xmath125 we have that @xmath126 since @xmath111 .",
    "these results are summarized in the following theorem .",
    "let @xmath43 such that the support of @xmath45 is @xmath29 and let @xmath64 be an @xmath25-valued random variable such that the distribution mean @xmath121 and the distribution variance @xmath122 exist and are finite .",
    "then for any @xmath113 the mean divergence between @xmath64 and @xmath127 is given by ( [ eq : klp0 ] ) .",
    "the term @xmath120 measures the extent to which the kl mean , distribution mean , and @xmath127 depart from forming a dual pythagorean triangle .",
    "the kl variance is less than or equal to the distribution variance , @xmath126 , and the distribution mean is the kl projection of the kl mean onto @xmath110 , @xmath128 .",
    "wu and vos @xcite show that @xmath129 for all @xmath113 an exponential family .",
    "for mixture families @xmath130 .",
    "hence , @xmath120 vanishes when @xmath110 is either an exponential family or mixture family .",
    "while we do nt know how to write @xmath0 as an integral and the expectation property ( [ eq : klexpectationproperty ] ) does not hold for @xmath0 in general , we show equations ( [ eq : klr2 ] ) and ( [ eq : klr3 ] ) hold with @xmath3 replaced with @xmath0 and @xmath4 replaced with @xmath1 when @xmath110 is either an exponential or mixture family . furthermore , the expectation property will hold for @xmath0 when @xmath110 is an exponential family and @xmath102 is the canonical statistic .",
    "for a general subspace @xmath111 the distribution mean @xmath121 and distribution variance @xmath122 do not characterize @xmath112 $ ] for @xmath113 .",
    "however , when @xmath110 is an exponential family these quantities do characterize @xmath112 $ ] and the classical equalities relating conditional mean and variance hold .",
    "a standard reference for exponential families is brown @xcite , but the approach we take here is slightly different since our emphasis is on the distributions without regard to any particular parameterization",
    ". an exponential family @xmath110 will be defined by selecting a point @xmath131 and statistic @xmath132 taking values in @xmath31 .",
    "the defining property of an exponential family is that for any @xmath113 the log of the density of @xmath127 with respect to @xmath133 is a linear combination of @xmath132 and the constant function .",
    "we start with some definitions and basic properties .",
    "@xmath110 is an _ exponential family on _",
    "@xmath29 if there exists @xmath131 such that the support of @xmath133 is @xmath29 and a function @xmath134 such that for any @xmath113 @xmath135 the distribution @xmath133 is called a _ base point _ and @xmath102 is called the _ canonical statistic _ of @xmath110",
    ". the _ canonical parameter space _ is @xmath136    without loss of generality , we can choose a base point @xmath133 such that @xmath137 .",
    "we ll refer to exponential families using base points that belong to the family .",
    "let @xmath110 be an exponential family with base point @xmath133 , canonical statistic @xmath102 , and set @xmath138 .",
    "the _ cumulant function _ has domain @xmath139 and is defined as @xmath140 the density with respect to @xmath133 for any @xmath113 is @xmath141    the family @xmath110 is _ regular _ if @xmath142 is open and @xmath110 is _ full _ if @xmath143 .    by the factorization theorem ,",
    "@xmath102 is sufficient",
    ". it will often be useful to restrict the choice of @xmath102 so that it is complete for the full exponential family @xmath110 .",
    "a statistic @xmath102 is _ complete _ for @xmath110 if @xmath144    the following theorem shows that the projection operator on @xmath110 behaves like the expectation operator on @xmath25 ( theorem [ thm : expectation - property - on ] ) and will be used to show that the classical conditional expectation equation holds for @xmath0 .",
    "[ thm : expmu ] if @xmath145 is the kl projection onto @xmath110 , where @xmath110 is an exponential family having canonical statistic @xmath102 and @xmath146 , then for any @xmath79 such that @xmath147 , @xmath148 where @xmath149 is the mean parameter space of @xmath110 .",
    "this result follows from the relationship between the natural and expectation parameters for an exponential family @xmath110 .",
    "let @xmath150 for some @xmath151 .",
    "then the natural parameter @xmath152 of this distribution satisfies @xmath153\\ ] ] and since @xmath154 parameterizes @xmath110 , @xmath155.\\ ] ] the result now follows for exponential family @xmath110 by simple calculation @xmath156 where @xmath157 by ( [ eq : dual1 - 1 ] ) .",
    "[ cor : pythagorean - property - for ] let @xmath110 be an exponential family and let @xmath79 such that @xmath158 exists . for all @xmath113 @xmath159    this is a well - known result .",
    "see , for example , @xcite or @xcite .",
    "we define an extended projection @xmath160 to be any distribution in @xmath25 such that expectation and pythagorean properties hold and it belongs to the `` boundary '' of @xmath110 ; that is , @xmath161 note that @xmath158 satisfies these three equalities , and that the last two equalities imply @xmath162 the extended projection allows us to define the extended mle in the next section .",
    "the pythagorean property allows us to improve @xmath25-valued random variables by the projection @xmath145 or , more generally , by @xmath163 .    if @xmath164 exists a.e .",
    ", then @xmath165\\ge e\\bigl[d(\\overline{\\pi}\\r , p)\\bigr]\\ ] ] with equality holding if and only if @xmath166 a.e .    replacing @xmath33 with @xmath64 in equation ( [ eq : extended pythagorean ] ) and taking expectations shows @xmath165=e\\bigl[d(\\r,\\overline{\\pi}\\r)\\bigr]+e\\bigl[d(\\overline{\\pi } \\r , p)\\bigr ] \\quad\\quad\\forall p\\in\\mathcal{p}\\ ] ] and the result follows from the fact that @xmath167\\ge0 $ ] with equality holding if and only if @xmath168 a.e .      for exponential families , the distribution expectation and variance",
    "have the same properties as the kl expectation and variance .",
    "one distinction is that the expectation property of @xmath3 holds for any statistic while for @xmath0 the expectation property holds only for the canonical statistic @xmath102 .",
    "let @xmath43 have support @xmath29 and let @xmath64 be an @xmath25-valued random variable such that the distribution mean @xmath121 exists and the distribution variance @xmath122 is finite .",
    "then for any @xmath113 , where @xmath110 is an exponential family , the mean kl divergence between @xmath64 and @xmath127 depends only on the distribution mean and distribution variance @xmath169=d\\bigl(\\ed\\r , p\\bigr)+\\vd\\r \\quad\\quad\\forall p\\in\\mathcal{p}.\\ ] ] assuming the conditional expectations and variances exist , the distribution mean and distribution variance satisfy the classical conditional equalities @xmath170 , \\\\ \\label{eq : klp3}\\vd\\r & = & \\vd\\ed[\\r|\\s]+\\e\\bigl[\\vd(\\r|\\s)\\bigr],\\end{aligned}\\ ] ] where @xmath47 is a real - valued random vector . furthermore , the expectation property holds for the canonical statistic @xmath102 @xmath171.\\ ] ]    by corollary [ cor : pythagorean - property - for ] and equation ( [ eq : klproj ] ) the correction term ( [ eq : klcorrection ] ) vanishes showing that equation ( [ eq : klp1 ] ) holds . equation ( [ eq : klp4 ] ) follows from @xmath172 and the expectation property on @xmath25 ( [ eq : klexpectationproperty ] ) .",
    "equation ( [ eq : mued ] ) follows from the ( extended ) projection property for exponential families ( [ eq : klproj ] ) and ( [ eq : mupi ] ) and the relationship between @xmath173 and @xmath0 ( [ eq : klprojection ] ) .",
    "now equation ( [ eq : klp2 ] ) follows from @xmath174\\bigr ) & = & \\e\\bigl[\\mu\\bigl(\\ed[\\r|\\s]\\bigr)\\bigr ] \\\\ & = & \\e\\bigl[\\mu\\bigl(\\e[\\r|\\s]\\bigr)\\bigr ] \\\\ & = & \\mu\\bigl(\\e\\e[\\r|\\s]\\bigr ) \\\\ & = & \\mu(\\e\\r ) \\\\ & = & \\mu\\bigl(\\ed\\r\\bigr),\\end{aligned}\\ ] ] where the first equality follows from ( [ eq : klp4 ] ) , the second and fifth equalities follow from ( [ eq : mued ] ) , the third equality follows from the expectation property of the kl mean on @xmath25 , and the fourth equality follows from the conditional expectation property that holds on @xmath25 ( [ eq : klr2 ] ) .",
    "equation ( [ eq : klp3 ] ) follows again the same steps that justified ( [ eq : klr3 ] ) .",
    "we rewrite ( [ eq : klp1 ] ) as @xmath175-d\\bigl(\\ed\\r , r\\bigr)=\\vd\\r.\\ ] ] if this equation holds for random sample @xmath96 then it also applies to the conditional distribution of @xmath96 given @xmath97 @xmath176-d\\bigl(\\ed[\\r|s],r\\bigr)=\\vd(\\r|s).\\ ] ] substituting @xmath86 into the equation above and taking expectation gives @xmath177-\\e\\bigl[d\\bigl(\\ed[\\r|\\s],r\\bigr)\\bigr]=\\e\\bigl[\\vd(\\r|\\s ) \\bigr].\\ ] ] substituting @xmath178 $ ] into @xmath64 in ( [ eq : v1 - 1 ] ) and using @xmath179=\\ed\\r$ ] gives @xmath180,r\\bigr)\\bigr]-d\\bigl(\\ed\\r , r\\bigr)=\\vd\\ed[\\r|\\s].\\ ] ] adding ( [ eq : v3 - 1 ] ) to ( [ eq : v4 - 1 ] ) and substituting from ( [ eq : v1 - 1 ] ) proves ( [ eq : klp3 ] ) .",
    "an immediate corollary to the characterization theorem on @xmath110 ( equations ( [ eq : klp1 ] ) , ( [ eq : klp2 ] ) , and ( [ eq : klp3 ] ) ) is that for any random distribution @xmath64 and any statistic @xmath86 , the random distribution @xmath178 $ ] will have the same distribution mean and have distribution variance less than or equal to that of @xmath64 .",
    "if @xmath182 is sufficient then @xmath183 $ ] is an estimator and if @xmath102 is also complete @xmath183 $ ] will have smaller variance than @xmath64 unless they are equal with probability one .",
    "this conditional expectation is enough to establish a rao ",
    "blackwell result for distribution estimators if these were restricted to  @xmath110 .",
    "however , since we are allowing @xmath25-valued estimators we also need to project the distributions onto @xmath110 using @xmath163 .    for an exponential family @xmath184 having mean parameter @xmath185 and discrete sample space we typically have that @xmath186 while @xmath187 where @xmath188 is the closure of @xmath189 . in this case , the mle does not always exist",
    ". however , the characterization theorem applies to @xmath25-valued estimators so we can define an estimator that equals the mle @xmath190 when it exists and as a distribution @xmath191 such that @xmath192 and @xmath193 if @xmath194 .",
    "the _ extended mle _ as distribution estimator is @xmath195 unbiasedness of @xmath196 follows from the following theorem .",
    "[ thm : unbiasedestimatorsinexp ] let @xmath110 be an exponential family with complete sufficient statistic @xmath102 and let @xmath64 be a @xmath25-valued random variable .",
    "the estimator @xmath64 is distribution unbiased for @xmath197 if and only if @xmath198)=t$ ] a.e .",
    "we must show @xmath199 for all @xmath137 if and only if @xmath200)=t$ ] a.e .",
    "for all @xmath137 .",
    "consider the following equivalencies each of which holds for all @xmath137 : @xmath201\\bigr ) & = & \\mu(p_{0 } ) \\\\",
    "\\iff\\quad e\\bigl[\\mu\\bigl(e[\\r|t]\\bigr)\\bigr ] & = & \\mu(p_{0 } ) \\\\",
    "\\iff\\quad e\\bigl[\\mu\\bigl(e[\\r|t]\\bigr)\\bigr ] & = & e(t).\\end{aligned}\\ ] ] the first equivalence follows because the expectation of @xmath102 parameterizes @xmath110 , the second equivalence follows from the projection property for exponential families , the third equivalence follows from the conditional expectation defined for the kl mean , the fourth equivalence follows from the expectation property for the kl mean , and the fifth equivalence follows from the definition of the function @xmath202 . clearly , @xmath200)=t$ ] a.e . implies the last equality .",
    "since @xmath102 is complete and the last equality holds for all @xmath137 , this implies @xmath203\\bigr)=t\\quad\\quad \\mbox{a.e.}\\ ] ]    [ thm : optimality - of - the ] let @xmath96 be i.i.d . from a distribution @xmath43",
    "such that the support of @xmath45 is @xmath29 .",
    "let @xmath110 be an exponential family with complete sufficient statistic @xmath102 such that @xmath204 . if @xmath205 is the mle or an extended mle that exists a.e . , then @xmath205 is distribution unbiased for the @xmath206 and it is the unique uniformly minimum distribution variance estimator among all @xmath25-valued estimators that are distribution unbiased for @xmath206 and for which the extended projection @xmath164 exists a.e .",
    "uniqueness and uniform minimum distribution variance follow from the projection property for @xmath25-valued random variables , the characterization theorem on @xmath110 described above , and the unbiasedness from theorem [ thm : unbiasedestimatorsinexp ] .",
    "we consider the number of events or `` successes '' in @xmath207 trials .",
    "the sample space is @xmath208 under the assumptions that these trials are independent and each trial has the same success probability @xmath209 , the distribution of @xmath94 belongs to the @xmath207-binomial family @xmath210 the mle for the parameter @xmath154 is @xmath211 for @xmath212 but is undefined otherwise .",
    "the extended mle ( it will correspond in a natural way to the extended mle distribution estimator ) is @xmath211 for all @xmath213 and it is unbiased for @xmath154 . however , it is not unbiased for other parameterizations such as the odds @xmath214 , or the log odds @xmath215 . when viewed as a distribution , that is , @xmath216 , equivalently , @xmath217 or @xmath218 ( where we allow the odds @xmath219 and log odds @xmath220 to take values in the extended reals ) , the mle is the unique uniformly minimum distribution variance unbiased estimator .",
    "as is common practice , we have used the same notation @xmath221 for both the mle and the extended mle .",
    "estimators , whether real - valued or distribution - valued , are functions with domain @xmath29 . for the @xmath207-binomial family an estimator is given by a sequence of @xmath222 values , real numbers for @xmath221 and probability distributions for @xmath223 . for @xmath221 , we have the sequence @xmath224 let @xmath225 be a distribution in @xmath110 . if probabilities of @xmath225 are used to assign weights to the values in  ( [ eq : mlereal ] ) , then the real number that is closest to the weighted values of ( [ eq : mlereal ] ) is @xmath226 .",
    "that is , @xmath227 by the rao  blackwell theorem ,",
    "for any other sequence of @xmath222 real numbers @xmath228 that satisfy @xmath229 the realized minimum will be greater than the minimum obtained using the values in ( [ eq : mlereal ] ) unless the sequences are equal , @xmath230 for @xmath231 .    a distribution estimator @xmath223 obtained from the real valued estimator given in ( [ eq : mlereal ] )",
    "can be defined as @xmath232 where @xmath233 is the indicator function for its subscript ; that is , the degenerate distribution putting all mass on 0 or 1 . since @xmath234 it is easily checked that @xmath235 which means that the sequence in ( [ mledist ] ) is the extended mle @xmath196 .",
    "hence , @xmath236 .",
    "again , we let @xmath225 be any distribution in @xmath110 .",
    "if @xmath225 is used to assign weights to the distributions in ( [ mledist ] ) , then the distribution in @xmath110 that is closest to the weighted average of the distributions in ( [ mledist ] ) is @xmath225 .",
    "that is , @xmath237.\\ ] ] by the distribution version of the rao  blackwell theorem ( theorem [ thm : optimality - of - the ] ) for any estimator @xmath238 , expressed as a distribution estimator , @xmath239 that satisfies @xmath240,\\ ] ] the realized minimum will be greater than that of the mle ( [ mledist ] ) unless the two sequences of functions ( [ mledist ] ) and ( [ eq : otherdist ] ) are equal .",
    "theorem [ thm : optimality - of - the ] provides a stronger result than this since the distributions need not belong to @xmath110 . in the class of all distribution unbiased estimators of the form @xmath241",
    "for which the extended projections @xmath163 exists , the mle ( [ mledist ] ) has smallest distribution variance . in the hardy",
    " weinberg model estimators that do not belong to the family @xmath110 have been suggested .",
    "we consider the details in section  [ sub : hardy - weinberg - model ] .",
    "the simplex represents the trinomial model space on @xmath242 for @xmath243 , while the solid curve is the hw model space on @xmath244 , @xmath245 , and @xmath246 for @xmath209 .",
    "the open circles represent the ( extended ) mle under the trinomial model @xmath247 , and the solid dots are the ( extended ) mle under the hw model @xmath248 .",
    "the dashed curve shows the kl mean of the hw mle for each value of @xmath154 . ]",
    "the choice of the @xmath207-binomial model @xmath110 was based on the assumptions that the data represented independent and identical trials .",
    "if either of these assumptions were grossly violated , the binomial model would not be appropriate .",
    "however , this model can be used when these assumptions hold approximately in the sense that there is a distribution @xmath197 in @xmath110 that is close to the data generation distribution @xmath45 , that is , @xmath249 is small . in this case",
    ", the mle is the unique umv@xmath2u estimator for @xmath133 .      for a single pair of alleles a and a , which occur with probabilities @xmath154 and @xmath250 for @xmath251 , the hardy  weinberg ( hw ) model defines the relative frequency of genotypes aa , aa , and aa to be @xmath244 , @xmath245 , and @xmath246 .",
    "for this example , we can take @xmath25 to be the collection of trinomial models with probabilities @xmath242 for @xmath243 which can be represented by the simplex in 2-dimensional space .",
    "see figure  [ fig : hardy - weinberg - model ] for the simplex .",
    "the open circles in figure  [ fig : hardy - weinberg - model ] are the extended mle @xmath247 for the trinomial with @xmath252 trials , where @xmath253 and @xmath254 are the counts for aa and aa .",
    "the solid curve in the simplex is the hw model @xmath255 which is a one dimensional exponential family with canonical sufficient statistic @xmath256 and canonical natural parameter @xmath257 . chow and fong @xcite",
    "find the umvu for @xmath258 and @xmath259 using @xmath260+e_{\\theta}\\bigl[\\bigl(\\hat{\\pi}_{3}-(1- \\theta)^{2}\\bigr)^{2}\\bigr]\\ ] ] as squared - error loss .",
    "they show the umvu is inadmissible by exhibiting a dominating estimator . both the umvu and the dominating estimator take values outside the hw model . in terms of distribution estimators , these are @xmath25-valued estimators .",
    "the extended mle for the hw model is @xmath248 while the extended distribution mle is @xmath223 where @xmath133 is the degenerate distribution putting all its mass on @xmath261 ( the lower left vertex ) and @xmath262 is the degenerate distribution putting all its mass on @xmath263 ( the lower right vertex ) .",
    "the extended hw mle is represented by the solid dots in figure  [ fig : hardy - weinberg - model ] .    among the difficulties with the umvu estimator and the dominating estimator",
    "is that there are other ways to define squared - error loss ( using one bin or two other bins ) .",
    "these are avoided by using kl divergence .",
    "since @xmath110 is an exponential family the extended mle is the umv@xmath2u for all @xmath110-valued estimators but also for all @xmath25-valued estimators since the projection exists for all points in the simplex other than the two lower vertices which satisfy the extended projection . as a comparison ,",
    "the kl mean , represented by the dashed curve in figure  [ fig : hardy - weinberg - model ] , lives outside the model so the extended mle is nt kl unbiased .",
    "this is due to the curvature in the exponential family .",
    "the poisson family of distributions is @xmath264 where @xmath265 .",
    "let @xmath96 be a simple random sample from a poisson distribution @xmath266 .",
    "the sum @xmath267 is a complete sufficient statistic of the family .",
    "although the poisson family is typically parametrized by a single parameter , we consider estimates for the probability @xmath268 for some @xmath269 . a crude but unbiased estimator",
    "is @xmath270 given the sum @xmath271 , @xmath272 is distributed as a binomial(@xmath271 , @xmath273 ) random variable , the rao ",
    "blackwell theorem shows that @xmath274=\\cases{\\displaystyle { s_{n } \\choose i } \\biggl(\\frac{1}{n } \\biggr)^{i } \\biggl(1-\\frac{1}{n } \\biggr)^{s_{n}-i } & \\quad if $ i\\le s_{n } $ , \\vspace*{2pt } \\cr 0 & \\quad otherwise , } \\ ] ] is an unbiased estimator of @xmath275 .",
    "since @xmath276 depends on the complete sufficient statistic @xmath271 only , it must be the unique mvue of @xmath275 .",
    "using the criterion of distribution unbiasedness , these anomalous estimators do not arise . since @xmath271 is the canonical statistic ,",
    "the mle @xmath277 is the unique umvu estimator for @xmath28 and the extended distribution mle @xmath278 is the umv@xmath2u estimator for @xmath279 where @xmath278 is @xmath280 when @xmath281 .    to show how the umvu estimator can fail completely",
    ", lehmann @xcite considers the parameter @xmath282 for @xmath283 . in this case , the unique umvu estimator is @xmath284 .",
    "since the sample consists of nonnegative integers this estimator is represented by the following sequence of real numbers @xmath285 parametric unbiasedness means that if the poisson distribution that assigns probability @xmath286 to @xmath287 is used to assign probability to the terms in the sequence then @xmath288 .",
    "that is , the parameter is the real number that is closest to this sequence in terms of mean square error . in addition , the weighted average of the above sequence is  @xmath289 .    by focusing on distributions rather than the parameters that name the distributions",
    "these problems are avoided .",
    "the mle , as a distribution estimator , is represented by the following sequence of probability distributions @xmath290 distribution unbiasedness means that if the poisson distribution @xmath279 is used to assign probability to the terms in the sequence then @xmath291.\\ ] ] that is , the distribution that generates the data is the distribution in the exponential family that is closest to this sequence in terms of kl risk .",
    "any other sequence of distributions with this property will have greater distribution variance .",
    "the distribution version of the rao  blackwell theorem [ thm : optimality - of - the ] has been developed by analogy with important properties of mean square error for the parametric version . in particular , we have used a pythagorean - type property for two asymmetric distribution - like functions : the kl divergence @xmath292 and its expectation @xmath293 $ ] . for exponential family @xmath125",
    "we have @xmath294 while for all @xmath25 @xmath295=\\e\\bigl[d(\\r,\\e\\r)\\bigr]+\\e\\bigl[d(\\e\\r , r)\\bigr]\\ ] ] so that the expectation operator @xmath173 defined on @xmath25-valued random variables for the kl risk plays the role of the projection operator @xmath145 for the kl divergence .",
    "each operator is a map from a more complicated space to a simpler space , @xmath173 from @xmath25-valued random variables to a distribution in @xmath25 and @xmath145 from distributions in @xmath25 to a distribution in @xmath110 , that preserve the kl risk and kl divergence , respectively .",
    "the restriction to exponential families is essentially required by the criterion of having a sufficient statistic of fixed dimension for all sample sizes @xmath207 .",
    "specifically , the darmois  koopman  pitman theorem which follows from independent works of darmois @xcite , koopman @xcite and",
    "pitman @xcite shows that when only continuous distributions are considered , the family of distributions of the sample has a sufficient statistic of dimension less than @xmath207 if and only if the population distribution belong to the exponential family .",
    "denny @xcite shows that for a family of discrete distributions , if there is a sufficient statistic for the sample , then either the family is an exponential family or the sufficient statistic is equivalent to the order statistics .",
    "the mle is parameter - invariant which means that the same distribution is named by the parametric ml estimate regardless of the parameter chosen to index the family .",
    "one approach to studying parameter - invariant quantities is to use differential geometry ( e.g. , amari @xcite or kass and vos @xcite ) .",
    "the parameter - invariant approach does not work well for parameter - dependent quantities such as bias and variance of parametric estimators .",
    "our approach allows for the definition of parameter - free versions of bias and variance .",
    "furthermore , the distribution version of the rao ",
    "blackwell provides two extensions : ( 1 ) minimum variance is taken over a larger class of estimators that includes estimators that are not required to take values in the model space @xmath110 , ( 2 ) the true distribution need not belong to @xmath110 .",
    "the fact that the mle is the unique uniformly minimum distribution variance unbiased estimator for exponential families distinguishes the mle from other estimators .",
    "this is in contrast to asymptotic methods applied to mse that can be used to show superior properties of the mle but , being asymptotic results , do not apply uniquely to the mle .",
    "asymptotically , mse and kl risk are the same and the mse can be viewed as an approximation to kl risk for large @xmath207 .",
    "the distribution version of the rao ",
    "blackwell theorem [ thm : optimality - of - the ] provides support for fisher s claim of the superiority of the mle even in small samples .",
    "we thank the associate editor and external reviewers for their insightful comments and suggestions which have made great improvement on this paper ."
  ],
  "abstract_text": [
    "<S> we employ a parameter - free distribution estimation framework where estimators are random distributions and utilize the kullback  </S>",
    "<S> leibler ( kl ) divergence as a loss function . </S>",
    "<S> wu and vos [ _ j . </S>",
    "<S> statist . </S>",
    "<S> plann . inference _ * 142 * ( 2012 ) 15251536 ] show that when an estimator obtained from an i.i.d . </S>",
    "<S> sample is viewed as a random distribution , the kl risk of the estimator decomposes in a fashion parallel to the mean squared error decomposition when the estimator is a real - valued random variable . in this paper , we explore how conditional versions of distribution expectation ( @xmath0 ) can be defined so that a distribution version of the rao  </S>",
    "<S> blackwell theorem holds . </S>",
    "<S> we define distributional expectation and variance ( @xmath1 ) that also provide a decomposition of kl risk in exponential and mixture families . for exponential families , we show that the maximum likelihood estimator ( viewed as a random distribution ) is distribution unbiased and is the unique uniformly minimum distribution variance unbiased ( umv@xmath2u ) estimator . </S>",
    "<S> furthermore , we show that the mle is robust against model specification in that if the true distribution does not belong to the exponential family , the mle is umv@xmath2u for the kl projection of the true distribution onto the exponential families provided these two distribution have the same expectation for the canonical statistic . to allow for estimators taking values outside of the exponential family , we include results for kl projection and define an extended projection to accommodate the non - existence of the mle for families having discrete sample space . </S>",
    "<S> illustrative examples are provided .    ./style / arxiv - general.cfg </S>"
  ]
}