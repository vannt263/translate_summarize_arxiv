{
  "article_text": [
    "during the past decade , much attention has been devoted to accommodate single high - dimensional sources of molecular data ( omics ) in the calibration of prediction models for health traits .",
    "for example , microarray - based transcriptome profiling and mass spectometry proteomics have been established as promising omic predictors in oncology @xcite and , to lesser extent , in metabolic health @xcite .",
    "nowadays , due to technical advances in the field and evolving biological knowledge , novel omic measures , such as nmr proteomics and metabolomics @xcite or nano - lcms and uplc glycomics @xcite are emerging as potentially powerful new biomolecular marker sets . as a result",
    ", it is increasingly common for studies to collect a range of omic measurements in the same set of individuals , using different measurement platforms and covering different aspects of human biology .",
    "this causes new statistical challenges , among which the evaluation of the ability of novel biomolecular markers to improve predictions based on previously established predictive omic sources , often referred as their added or incremental predictive ability @xcite .",
    "an illustrative example of these new methodological challenges is given by our motivating problem .",
    "we have access to data from 248 individuals sampled from the helsinki area , finland , within the dietary , lifestyle , and genetic determinants of obesity and metabolic syndrome ( dilgom ) study @xcite .",
    "one - hundred - thirty - seven highly correlated nmr serum metabolites and 7380 beads from array - based transcriptional profiling from blood leukocytes were measured at baseline in 2007 , together with a large number of clinical and demographic factors which were also measured in 2014 , after seven years of follow - up .",
    "our primary goal is the prediction of future body mass index ( bmi ) , using biomolecular markers measured at baseline .",
    "more specifically , we would like to compare the predictive ability of the available metabolomics and transcriptomics , and to determine if both should be retained in order to improve single - omic source predictions of bmi at seven years of follow - up .    in our setting",
    ", it is necessary to both calibrate the predictive model based on each source of omic predictors and assess the incremental predictive ability of a secondary one relative to the first set , using the same set of observations .",
    "hence , in order to avoid overoptimism and provide realistic estimates of performance , it is necessary to control for the re - use of the data , which has already been employed for model fitting within the same observations @xcite .",
    "this is a very important issue in omic research , where external validation data are hard to obtain .",
    "it is well known that biased estimation of model performance due to re - use of the data increases with large number of predictors @xcite and omic sets are typically high - dimensional ( @xmath5 , @xmath6 sample size and @xmath7 the number of predictors ) .",
    "extra difficulties in our setting are the different dimensions ( number of features ) , scales and correlation structure of each omic source , and possible correlation between omic sources induced by partially common underlying biological information .    evaluating",
    "the added predictive ability of new biomarkers regarding classical , low dimensional , settings has been a topic of intense debate in the biostatistical literature in the last years ( see , for example , @xcite and references therein ) .",
    "getting meaningful summary measures and valid statistical procedures for testing the added predictive value are difficult tasks , even when considering the addition of a single additional biomarker in the classical regression context .",
    "in particular , widely used testing procedures for improvement in discrimination based on area under the roc curve ( auc ) differences @xcite and net reclassification index ( nri ) @xcite have shown unacceptable false positive rates in recent simulation studies @xcite .",
    "overfitting is a big problem when comparing estimated predictions coming from nested regression models fitted in the same dataset .",
    "moreover , the distributional assumptions of the proposed tests seem inappropriate , translating into poor performance of the aforementioned tests even when using independent validation sets @xcite . to date",
    ", little attention has been given to the evaluation of the added predictive ability in high - dimensional settings , where the aforementioned problems are larger and new ones appear , such as the simultaneous inclusion in an unique prediction model of predictors sets of very different nature .",
    "tibshirani and efron @xcite have shown that overfitting may dramatically inflate the estimated added predictive ability of omic sources with respect to a low - dimensional set of clinical parameters . to solve this issue",
    ", they proposed to first create a univariate ` pre - validated ' omic predictor based on cross - validation techniques @xcite and incorporate it as a new covariate to the regression with low - dimensional clinical parameters . in a subsequent publication , hoefling and tibshirani @xcite have shown that standard tests in regression models are biased for pre - validated predictors . as a solution , the authors suggest a permutation test which seems to perform well under independence of clinical and omic sets .",
    "boulesteix and hothorn @xcite have proposed an alternative method for the same setting of enriching clinical models with a high - dimensional set of predictors .",
    "in contrast to @xcite , they first obtain a clinical prediction based on traditional regression techniques . in a second step ,",
    "the clinical predictor is incorporated as an offset term in a boosting algorithm based on the omic source of predictors .",
    "previous calibration of the clinical prediction is not addressed in the second step and the same permutation strategy than hoefling and tibshirani @xcite is used to derive p - values .    in this paper",
    ", we propose a two - step procedure for the assessment of additive predictive ability regarding two high - dimensional and correlated sources of omic predictors . to the best of our knowledge",
    ", no previous work has addressed this problem before .",
    "our approach combines double cross - validation sequential prediction based on regularized regression models and a permutation test to formally assess the added predictive value of a second omic set of predictors over a primary omic source .",
    "let the observed data be given by @xmath8 , where @xmath9 is the continuous outcome measured in @xmath6 independent individuals and @xmath10 and @xmath11 are two matrices of dimension @xmath12 and @xmath13 , respectively , representing two omic predictor sources with @xmath7 and @xmath14 features .",
    "we assume that we are in a high - dimensional setting ( @xmath15 ) and that the main goal is to evaluate the incremental or added value of @xmath16 beyond @xmath17 in order to predict @xmath18 in new observations .",
    "our approach is based on comparing the performance of a primary model based only on @xmath10 with an extended model based on @xmath11 and adjusted by the primary fit based on @xmath10 .",
    "we propose a two - step procedure based on the replacement of the original ( high - dimensional ) sources of predictors by their corresponding estimated values of @xmath18 based on a single - source - specific prediction model .    in the first step ,",
    "we build a prediction model for @xmath18 based on @xmath10 and a given model specification @xmath19 .",
    "based on the fitted model , the fitted values @xmath20 are estimated . then , for each individual @xmath21 , we take the residual @xmath22 .",
    "we consider @xmath23 as new response and construct a second prediction model based on @xmath16 as predictor source : @xmath24 this is equivalent to including @xmath25 as an offset term ( fixed ) in the model based on @xmath26 for the prediction of the initial outcome @xmath18 : @xmath27    several statistical methods are available to derive prediction models of continuous outcomes in high - dimensional settings . in this work ,",
    "we focus on regularized linear regression models @xcite , where @xmath28 and the estimation of @xmath29 is conducted by solving @xmath30 , where @xmath31 .",
    "the penalty parameter @xmath32 regularizes the @xmath33 coefficients , by shrinking large coefficients in order to control the bias - variance trade - off .",
    "the pre - fixed parameter @xmath34 determines the type of imposed penalization .",
    "we used two widely used penalization types : @xmath35 ( ridge , i.e. ,  @xmath36 type penalty @xcite ) and @xmath37 ( lasso , i.e. ,  @xmath38 penalty @xcite ) . note that other model building strategies for prediction of continuous outcomes could have been used in this framework , such as the elastic net penalization @xcite by setting @xmath39 , or boosting methods @xcite , among others .      the use of a previously estimated quantity ( @xmath40 ) in the calibration of a prediction model based on @xmath16 ( expressions ( 1 ) and ( 2 ) ) requires , in absence of external validation data , the use of resampling techniques to avoid bias in the assessment of the role of @xmath40 and @xmath41 .",
    "we use double cross - validation algorithms @xcite , consisting of two nested loops . in the inner loop a cross - validated grid - selection is used to determine the optimal prediction rule , i.e. , for model selection , while the outer loop is used to estimate the prediction performance by application of models developed in the inner loop part of the data ( training sets ) to the remaining unused data ( validation sets ) . in this manner , double cross - validation is capable of avoiding the bias in estimates of predictive ability which would result from use of a single - cross - validatory approach only . in our setting ,",
    "the outer loop of a ` double ' cross - validatory calculation allows obtaining ` predictive'-deletion residuals which fully account for the inherent uncertainty of model fitting on the primary source ( @xmath10 ) , before assessing the added predictive ability of @xmath11 , given by @xmath41 .",
    "the basic structure of the double cross - validation procedure to estimate unbiased versions of @xmath40 and @xmath41 is as follows :    step 1 : :    obtain double cross - validation predictions of    @xmath18 ,    @xmath42 , based on    @xmath43 :    +    - ; ;      randomly split sample @xmath44 in @xmath45 mutually      exclusive and exhaustive sub - samples of approximately equal size      @xmath46    - ; ;      for each @xmath47 , merge @xmath48      subsamples into @xmath49      +      - : :        randomly split sample @xmath50 in @xmath51        sub - samples @xmath52      - : :        for each @xmath53 , merge @xmath54        subsubsamples into        @xmath55        +        * ; ;          fit regression model          @xmath56          for a grid of values of shrinkage parameters          @xmath57 , @xmath58 to          @xmath59        * ; ;          evaluate          @xmath60,@xmath58          in the @xmath61 held - out sub - sample          @xmath62 by calculating          @xmath63      - : :        compute overall cross - validation error :        @xmath64 ,        @xmath58      - : :        choose        @xmath65        and calculate predictions of @xmath18 in the        @xmath66 held - out sub - sample @xmath67 ,        @xmath68 ,        @xmath69    - ; ;      the vector of predictions of @xmath18 ,      @xmath42 is obtained by      concatenating the @xmath45      @xmath70 , @xmath47      vectors , i.e. ,      @xmath71 step 2 : :    repeat the process detailed in step 1 considering the double    cross - validated residuals    @xmath72 , as    outcome and @xmath26 as set of predictors and    obtain the double cross - validation predictions    @xmath73 .",
    "note    that this is equivalent to obtaining the double cross - validation    predictions of @xmath18 based on    @xmath26 considering @xmath25    as offset variable in the @xmath45 fits of model ( 1 ) .      in order to evaluate the performance of the sequential procedure introduced in subsection 2.1 .",
    ", we propose three measures of predictive accuracy , denoted by @xmath74 , @xmath75 , and @xmath76 , based on sum of squares of the double cross - validated predictions @xmath40 and @xmath41 , obtained following the procedure described in subsection 2.2 .. these summary measures can be regarded as high - dimensional equivalents of calibration measurements for continuous outcomes in low - dimensional settings @xcite , and an extension of previously discussed proposals in the cross - validation literature @xcite .    denote by @xmath77 the prediction sum of squares based on a vector of predictions @xmath78 , obtained according to some arbitrary model @xmath19 , @xmath79 and by @xmath80 the sum of squared differences between two cross - validated vectors of predictions , e.g. @xmath81 , @xmath82 .",
    "let @xmath83 be the simplest cross - validated predictor of @xmath18 , based on the sample mean of @xmath18 only . to summarize the first step of the sequential procedure , we use double cross - validation to estimate the predictive ability of @xmath10 by    @xmath84",
    "intuitively , @xmath85 represents the proportion of the variation of the response @xmath18 that is expected to be explained by @xmath86 in new individuals , re - scaled by the total amount of prediction variation in the response @xmath18 . in the worst case scenario , when @xmath87 ( @xmath10 as predictive as a null model based on the mean of @xmath18 ) @xmath88 and @xmath89 if @xmath90 . since the computation of @xmath91 , @xmath92 for each of the @xmath47 random splits of the sample @xmath44",
    "is based on the observations not belonging to @xmath67 , we proceed in an analogous way to compute the average predicted variation of @xmath18 .",
    "hence , in order to get an appropriate re - scaling factor , for each subset @xmath67 , we compute @xmath93 , the mean value of the outcome variable @xmath18 calculated without the observations belonging to @xmath67 .",
    "assume that @xmath94 , the contribution of the second omic source , @xmath11 , in the prediction of @xmath18 can be summarized by    @xmath95    @xmath96 accounts for the predictive capacity of @xmath97 , after removing the part of variation in @xmath18 that can be attributed to the first source of predictors @xmath10 .",
    "its computation relies on the squared difference between @xmath98 ( the double cross - validated predictions resulting from the second step of the proposed procedure in subsection 2.2 . ) and the corresponding residual from the step 1 ( @xmath99 ) based on @xmath10 , re - scaled by the remaining predicted variation on @xmath18 after the first step of the procedure . as a result",
    ", @xmath96 can be regarded as the expected ability of @xmath11 to predict the part of @xmath18 , after adjusting for the predictive capacity of @xmath86 and accounting for all model fitting in the first stage of the assessment .",
    "following the same arguments used in deriving expression ( 3 ) , for each subset @xmath67 , the computation of the average variation of @xmath100 is based on @xmath101 , i.e. , excluding the observations belonging to @xmath67 .",
    "note that in step 1 of the sequential procedure @xmath45 models are fitted , each based on @xmath50 , providing residuals with expected zero mean ( given specification ( 1 ) ) , i.e. , @xmath102 , @xmath47 .",
    "hence , @xmath103 and thus @xmath104 finally , we derive a third summarizing measurement of the overall sequential process , @xmath105 , defined as : @xmath106 @xmath107 represents the total predictive capacity of the overall sequential procedure based on @xmath10 and @xmath11 , i.e. , the combined predictive ability of @xmath10 and @xmath11 given by @xmath108 . note that @xmath107 is based on the same squared difference between @xmath98 and @xmath99 as @xmath96 , but the re - scaling factor refers to the total predictive variation of the original response @xmath18 .",
    "the three introduced measures jointly summarize the performance of the two omic sources under study and their interplay in order to predict the outcome @xmath18 .",
    "in all the cases higher values are indicative of higher predictive ability .",
    "the three measurements vary between 0 ( null predictive ability ) and the maximal value of 1 .",
    "the interpretation of @xmath74 is straightforward , as it simply captures the predictive capacity of the firstly evaluated omic source .",
    "note that the difference between @xmath75 and @xmath76 relies on the denominator .",
    "in general , if @xmath10 is informative , the denominator in expression ( 4 ) will be smaller than in expression ( 6 ) .",
    "thus , the residual variation after step 1 will be smaller than the total initial variation .",
    "the three summary measures are related by the following expression : @xmath109    consequently , we can rewrite @xmath110 as follows : @xmath111    note that in cases in which @xmath112 , we get that @xmath113 , and viceversa .",
    "however , @xmath96 and @xmath114 differ when not zero .",
    "specifically , from expression ( 8) , we obtain that @xmath115 . in short ,",
    "@xmath96 may be regarded as the conditional contribution of @xmath11 for the prediction of @xmath18 with respect to what may be predicted using @xmath10 alone .",
    "@xmath116 measures the absolute gain in predictive ability from adding @xmath26 to @xmath10 .",
    "note that a given source @xmath11 may present a large conditional @xmath96 but a small absolute @xmath116 ( if , for example , @xmath10 presents high predictive ability itself ) .",
    "moreover , due to the relation between @xmath25 and the resulting vector of predictions after combining @xmath10 and @xmath11 , @xmath108 , expression ( 8) implies that @xmath117 . this desirable property may not be fulfilled using alternative combination strategies .    in practice ,",
    "our sequential procedure relies on the realistic assumption of positive predictive ability of the first source of predictors , @xmath10 ( one would only be interested in assessing additional or incremental information on top of an informative source itself ) .",
    "accordingly , we advise to conduct our sequential procedure using @xmath10 as primary source only if @xmath118 , which is , furthermore , required to derive expression ( 8) .",
    "the summary measures may be used to introduce formal tests for assessing the added or augmented predictive value of @xmath11 over @xmath10 to predict @xmath18 .",
    "we propose a permutation procedure to test the null hypothesis @xmath119 against the alternative hypothesis @xmath120 .",
    "the test is based on permuting the residuals obtained after applying the first step of our two - stage procedure with the data at hand .",
    "our goal is to remove the potential association between @xmath16 and @xmath18 while preserving the original association between @xmath18 and @xmath17 .",
    "explicitly , we propose the following algorithm :    step 1 : :    calculate the residuals    @xmath121 based on the    predictions @xmath122 of @xmath18    based on @xmath10 , obtained in the first step of    the procedure presented in section 2.1 . step 2 : :    permute the values of @xmath100 , obtaining    @xmath123 and generate values of the response    @xmath18 under the null hypothesis :    @xmath124 .",
    "step 3 : :    repeat the two - stage procedure from section 2.1 . for predicting    @xmath125 and obtain the corresponding    @xmath126 .",
    "the procedure is repeated @xmath127 times and the resulting permutation p - values are obtained as follows : @xmath128 where @xmath127 is the number of permutations , and @xmath96 is the actual observed value with the data at hand .",
    "note that in * step 2 * , we generate a ` null ' version of the original response @xmath18 and then we repeat the overall two - stage procedure , which implies that the ` null ' residuals used in * step 3 * are not fixed and are , in general , different from @xmath123 .",
    "this is necessary in order to capture all the variability of the two - stage procedure and to correctly generate the null hypothesis of interest .",
    "moreover , the cross - validation nature of the procedure protects against systematic bias of the residuals obtained in step 3 based on @xmath129 ( see chapter 7 of * ? ? ?",
    ".    given the aforementioned relations between @xmath130 , @xmath110 , and @xmath107 specified by expression ( 6 ) , note that @xmath119 is equivalent to @xmath131 .",
    "this result immediately follows from expression ( 6 ) , given that @xmath132 if and only if @xmath133 ( assuming @xmath134 ) .",
    "hence , both tests are equivalent provided that the distribution under the null hypothesis is generated by the aforementioned permutation procedure , i.e. , the p - values , resulting from using @xmath96 as test statistic or @xmath135 are approximately the same .    due to the reliance of our method on resampling techniques ,",
    "computational cost is a potential limitation of our approach .",
    "however , our method is easy to split in @xmath127 independent realizations of the same computational procedure .",
    "hence , we can use parallel computing to speed up the procedure @xcite .",
    "we simulate two omic predictor sources @xmath17 ( dimension @xmath12 ) and @xmath16 ( dimension @xmath13 ) and a @xmath136 vector @xmath18 , the continuous outcome .",
    "we use matrix singular value decomposition ( svd , @xcite ) of each of the two omic sources to generate common ` latent ' factors associated with @xmath17 , @xmath16 and @xmath18 .",
    "common eigenvectors in the svd of @xmath10 and @xmath11 introduce correlation among the omic sources .",
    "we consider different patterns in terms of the conditional association between @xmath11 and @xmath18 ( see figure 1 ) .",
    "the details of the data generation procedure are as follows :    ( x1 ) @xmath10 ; ( x2 ) @xmath11 ; ( lj ) @xmath137 ; ( l1 ) @xmath138 ; ( l2 ) @xmath139 ; ( y ) @xmath18 ;    ( l1 )  ( x1 ) ; ( l2 )  ( x2 ) ; ( lj )  ( x2 ) ; ( lj )  ( x1 ) ; ( lj ) to [ out=120,in=120 , looseness=0.5 ] ( y ) ;    ( x1 ) @xmath10 ; ( x2 ) @xmath11 ; ( lj ) @xmath137 ; ( l1 ) @xmath138 ; ( l2 ) @xmath139 ; ( y ) @xmath18 ;    ( l1 )  ( x1 ) ; ( l2 )  ( x2 ) ; ( lj )  ( x2 ) ; ( lj )  ( x1 ) ; ( l1 ) to [ out=120,in=120 , looseness=0.5 ] ( y ) ; ( l2 ) to [ out=270,in=270 , looseness=0.5 ] ( y ) ;    * step 1 * generate @xmath140 , a matrix of @xmath141 i.i.d .",
    "latent factors of @xmath17 and @xmath16 .",
    "* step 2 * define @xmath142 ( @xmath143 ) and @xmath144 ( @xmath145 ) , the correlation matrices of @xmath10 and @xmath11 , respectively , according to a predefined covariance structure of interest .",
    "following the recent literature on pathway and network analysis of omics data @xcite , we generated @xmath146 , @xmath147 according to a hub observation model ( * ? ? ?",
    "* see figure 2 ) .",
    "* step 3 * draw @xmath148 , @xmath147 , and obtain the singular value decomposition for each of the independent matrices @xmath149 and @xmath150 : @xmath151 .",
    "* step 4 * generate the final correlated @xmath10 and @xmath11 by manipulation of @xmath152 and @xmath153 , the left eigenvectors matrices from @xmath154 and @xmath155 , respectively .",
    "specifically , for a certain number ( @xmath156 ) of predefined columns @xmath157 and @xmath158 , the original submatrices @xmath159 and @xmath160 ( independent ) are replaced by @xmath156 common independent latent factors @xmath161 , @xmath162 generated in * step 1*. in this manner , correlation between @xmath10 and @xmath11 is induced , while the within - omic source correlation structures @xmath163 and @xmath164 are preserved .    *",
    "step 5 * simulate the outcome @xmath165 , where @xmath166 and @xmath167 are vectors of regression coefficients of length @xmath7 and @xmath14 , respectively and @xmath168 . since @xmath169 , @xmath147 , we can rewrite @xmath170 and thus @xmath171 , where @xmath172 represents the association between @xmath173 and the outcome @xmath18 through the orthogonal directions given by @xmath174 . consequently , we first generate @xmath175 and we then transform it to the predictor space by using @xmath171 .",
    "( @xmath176 ) , 4 groups of 250 features each .",
    "right : correlation matrix of @xmath16 ( @xmath177 ) , 2 groups of 50 features each.,width=188,height=188 ]     ( @xmath176 ) , 4 groups of 250 features each .",
    "right : correlation matrix of @xmath16 ( @xmath177 ) , 2 groups of 50 features each.,width=188,height=188 ]    * simulation 1 ( ` null ' scenarios ) : * the second omic @xmath11 source is non - informative , i.e. , @xmath178 , but is strongly correlated to @xmath10 , by imposing common first columns of @xmath179 and @xmath180 ( @xmath181 , the correlation between omic sources is driven through the maximal variance subspace ) .",
    "we considered different assumptions regarding the regression dependence of @xmath18 on @xmath10 which has an impact on the ability to calibrate prediction rules based on @xmath17 for @xmath18 .",
    "we consider two situations in which the association with @xmath18 is unifactorial , in the sense that only one latent factor ( one column of @xmath182 ) is associated with @xmath18 and two multi - factorial situations .",
    "one of our objectives is to illustrate how changing the complexity of the calibration of a prediction rule based on @xmath10 ( by formulating the problem through regression on either larger or smaller variance latent factors ) may affect the results .",
    "we consider the following ` null ' scenarios :    scenario 1a : :    @xmath183 , @xmath184 ;    @xmath185 .",
    "@xmath18 is associated to high - variance subspace of    @xmath179 , corresponding to the largest eigenvalue    of @xmath10 .",
    "scenario 1b : :    @xmath186 , @xmath187 ,    @xmath188 .",
    "the association with    @xmath18 relies on a low - variance subspace of    @xmath179 .",
    "hence , we expect lower values of    @xmath74 , compared to scenario 1a . scenario 1c : :    @xmath189 ,    @xmath190 otherwise . in this setting we consider    a bifactorial regression , as association with @xmath18    is a combination of the effect of the two first eigenvectors of    @xmath17 .",
    "scenario 1d : :    @xmath191 ,    @xmath190 otherwise . in this",
    "setting we consider    a multifactorial regression , as association with    @xmath18 is a combination of the effect of the four    first eigenvectors of @xmath17 .",
    "* simulation 2 ( ` alternative ' scenarios ) : * @xmath11 is associated with @xmath18 through latent factors non - shared with @xmath10 .",
    "the following ` alternative ' scenarios are investigated :    scenario 2a : :    @xmath192 ,    @xmath184 ,    @xmath193 ,    @xmath194 .",
    "the eigenvector related to the largest    eigenvalue of each source is associated to @xmath18    and the association between @xmath10 and    @xmath11 is generated by sharing the second    eigenvectors , i.e. , by setting    @xmath195 .",
    "scenario 2b : :    @xmath196 , @xmath184 ,    @xmath197 , @xmath194 and    @xmath198 , @xmath199 ,    @xmath200 , @xmath201 , and    the association between @xmath10 and    @xmath11 is generated by setting    @xmath181 .",
    "scenario 2c : :    @xmath196 , @xmath187 ,    @xmath197 , @xmath194 and    @xmath198 , @xmath199 ,    @xmath200 , @xmath201 , and    the association between @xmath10 and    @xmath11 is generated by setting    @xmath181 .",
    "figure 3 shows a monte carlo approximation based on a sample of @xmath202 observations of the regression coefficients @xmath166 and @xmath167 in the studied simulated scenarios . from panels 3(a ) to 3(d ) , we can observe that the different simulation settings differ in the level of imposed sparsity in the association between @xmath203 and @xmath10 . on the one hand ,",
    "scenarios presented in panels 3(a ) ( scenarios 1a , 2a ad 2b ) and 3(b ) ( scenario 1b ) are relatively sparse , with most of the simulated coefficients close to zero . on the other hand , the @xmath166 of scenario 1c ( represented in panel 3(c ) ) and specially of scenario 1d ( represented in panel 3(d ) ) are less sparse , based on a large number of non - null regression coefficients in @xmath10 . with regard to @xmath167 , panel 3(e ) ( scenario 2a ) represents a sparser situation than panel 3(f ) ( scenarios 2b and 2b ) .    , y - axis ) corresponding to each of the @xmath7 predictors of @xmath10 ( x - axis ) .",
    "( c)-(d ) : regression coefficients ( elements of @xmath167 , y - axis ) corresponding to each of the @xmath14 predictors of @xmath11 ( x - axis ) . the outcome variable is generated as @xmath165 , @xmath168.(e)-(f ) provide information about association between @xmath18 and @xmath17 and ( e ) and ( f ) corresponds to the independent association between @xmath18 and @xmath16 in the alternative scenarios ( for the null scenarios 1a-1d the independent association between @xmath18 and @xmath16 is null ) .",
    "( a ) corresponds to scenarios 1a , 2a and 2b , ( b ) correspond to scenarios 1b and 2c respectively , while ( c ) and ( d ) correspond to scenario 1c and 1d .",
    "( e ) shows the association ( @xmath167 ) between @xmath16 and @xmath18 in scenario 2a and ( e ) shows @xmath167 for scenarios 2b and 2c.,title=\"fig : \" ] , y - axis ) corresponding to each of the @xmath7 predictors of @xmath10 ( x - axis ) .",
    "( c)-(d ) : regression coefficients ( elements of @xmath167 , y - axis ) corresponding to each of the @xmath14 predictors of @xmath11 ( x - axis ) . the outcome variable is generated as @xmath165 , @xmath168.(e)-(f ) provide information about association between @xmath18 and @xmath17 and ( e ) and ( f ) corresponds to the independent association between @xmath18 and @xmath16 in the alternative scenarios ( for the null scenarios 1a-1d the independent association between @xmath18 and @xmath16 is null ) .",
    "( a ) corresponds to scenarios 1a , 2a and 2b , ( b ) correspond to scenarios 1b and 2c respectively , while ( c ) and ( d ) correspond to scenario 1c and 1d .",
    "( e ) shows the association ( @xmath167 ) between @xmath16 and @xmath18 in scenario 2a and ( e ) shows @xmath167 for scenarios 2b and 2c.,title=\"fig : \" ] , y - axis ) corresponding to each of the @xmath7 predictors of @xmath10 ( x - axis ) .",
    "( c)-(d ) : regression coefficients ( elements of @xmath167 , y - axis ) corresponding to each of the @xmath14 predictors of @xmath11 ( x - axis ) . the outcome variable is generated as @xmath165 , @xmath168.(e)-(f ) provide information about association between @xmath18 and @xmath17 and ( e ) and ( f ) corresponds to the independent association between @xmath18 and @xmath16 in the alternative scenarios ( for the null scenarios 1a-1d the independent association between @xmath18 and @xmath16 is null ) .",
    "( a ) corresponds to scenarios 1a , 2a and 2b , ( b ) correspond to scenarios 1b and 2c respectively , while ( c ) and ( d ) correspond to scenario 1c and 1d .",
    "( e ) shows the association ( @xmath167 ) between @xmath16 and @xmath18 in scenario 2a and ( e ) shows @xmath167 for scenarios 2b and 2c.,title=\"fig : \" ] + , y - axis ) corresponding to each of the @xmath7 predictors of @xmath10 ( x - axis ) .",
    "( c)-(d ) : regression coefficients ( elements of @xmath167 , y - axis ) corresponding to each of the @xmath14 predictors of @xmath11 ( x - axis ) . the outcome variable is generated as @xmath165 , @xmath168.(e)-(f ) provide information about association between @xmath18 and @xmath17 and ( e ) and ( f ) corresponds to the independent association between @xmath18 and @xmath16 in the alternative scenarios ( for the null scenarios 1a-1d the independent association between @xmath18 and @xmath16 is null ) .",
    "( a ) corresponds to scenarios 1a , 2a and 2b , ( b ) correspond to scenarios 1b and 2c respectively , while ( c ) and ( d ) correspond to scenario 1c and 1d .",
    "( e ) shows the association ( @xmath167 ) between @xmath16 and @xmath18 in scenario 2a and ( e ) shows @xmath167 for scenarios 2b and 2c.,title=\"fig : \" ] , y - axis ) corresponding to each of the @xmath7 predictors of @xmath10 ( x - axis ) .",
    "( c)-(d ) : regression coefficients ( elements of @xmath167 , y - axis ) corresponding to each of the @xmath14 predictors of @xmath11 ( x - axis ) . the outcome variable is generated as @xmath165 , @xmath168.(e)-(f ) provide information about association between @xmath18 and @xmath17 and ( e ) and ( f ) corresponds to the independent association between @xmath18 and @xmath16 in the alternative scenarios ( for the null scenarios 1a-1d the independent association between @xmath18 and @xmath16 is null ) .",
    "( a ) corresponds to scenarios 1a , 2a and 2b , ( b ) correspond to scenarios 1b and 2c respectively , while ( c ) and ( d ) correspond to scenario 1c and 1d .",
    "( e ) shows the association ( @xmath167 ) between @xmath16 and @xmath18 in scenario 2a and ( e ) shows @xmath167 for scenarios 2b and 2c.,title=\"fig : \" ] , y - axis ) corresponding to each of the @xmath7 predictors of @xmath10 ( x - axis ) .",
    "( c)-(d ) : regression coefficients ( elements of @xmath167 , y - axis ) corresponding to each of the @xmath14 predictors of @xmath11 ( x - axis ) . the outcome variable is generated as @xmath165 , @xmath168.(e)-(f ) provide information about association between @xmath18 and @xmath17 and ( e ) and ( f ) corresponds to the independent association between @xmath18 and @xmath16 in the alternative scenarios ( for the null scenarios 1a-1d the independent association between @xmath18 and @xmath16 is null ) .",
    "( a ) corresponds to scenarios 1a , 2a and 2b , ( b ) correspond to scenarios 1b and 2c respectively , while ( c ) and ( d ) correspond to scenario 1c and 1d .",
    "( e ) shows the association ( @xmath167 ) between @xmath16 and @xmath18 in scenario 2a and ( e ) shows @xmath167 for scenarios 2b and 2c.,title=\"fig : \" ]    in our basic setting , we considered @xmath204 observations , @xmath176 features in @xmath10 and @xmath177 features in @xmath11 . for each scenario , we provide the mean values and standard deviations of @xmath85 , @xmath205 , and @xmath107 , based on 5-folds double cross - validation , jointly with the rejection proportions for testing @xmath206 along @xmath207 monte carlo trials .",
    "we evaluated the permutation test introduced in subsection 2.3 . using @xmath208 permutations .",
    "we complemented our empirical evaluations of the proposed sequential double cross - validation procedure by extending our basic simulation setting in two directions .",
    "we checked the impact on modifying sample size ( @xmath209 ) and the complexity of the problem by varying the number of variables considered in the first stage ( @xmath210 ) .",
    "additionally , we compared the performance of our procedure based on double - cross validation with two alternative strategies . on the one hand , we provide results based on a two - stage procedure using a single cross - validation loop ( cross - validation is used for model choice but predictions and therefore the residuals used as outcome in the second stage are directly computed on the complete sample ) . on the other hand",
    ", we check the impact on the results of over - penalization . specifically , instead of taking @xmath211 as defined in the inner loop of the double cross - validation procedure presented in subsection 2.1 . , we choose a larger value for @xmath32 , namely @xmath212 .",
    "both are usual strategies in penalized regression in single omic prediction frameworks , so it is of practical interest to quantify their impact from the added predictive value point of view .",
    "the results of these alternative strategies are provided as supplemental material but discussed in the main text .",
    "the results for the sequential double cross - validation procedure ( labeled as ` cv type= @xmath213 , @xmath211 ' ) are summarized in tables 1 and 2 .",
    "the top part of each table contains the results concerning the ` null ' scenarios ( no added value of @xmath11 ) , while the bottom part shows the results of the ` alternative ' scenarios ( added value of @xmath11 ) .",
    "table 1 contains results based on ridge regression , @xmath35 in expression ( 1 ) , while table 2 summarizes the results for the lasso penalty type ( @xmath37 ) .      for",
    "the four ` null ' scenarios 1a-1d , given that @xmath11 is not independently associated to @xmath18 , we expect @xmath112 and rejection proportions of @xmath214 about 0.05 .",
    "the results of the sequential double cross - validation procedure based on ridge regression are satisfactory in this regard , with rejection proportions close to the nominal level in all the studied null scenarios and for different sample sizes ( @xmath209 , @xmath204 ) and levels of complexity of the first step ( @xmath176 , @xmath210 ) .",
    "the top part of table 1 shows that the estimated @xmath215 for scenarios 1a , 1c and 1d are large and very similar ( @xmath216 ) .",
    "as it was expected , the estimated predictive ability of @xmath10 is lower in scenario 1b and presents a larger variability , since the association between @xmath18 and @xmath10 relies on a small variance subspace . in general , for all 1a-1d scenarios the estimated @xmath96 is close to zero .",
    "however , we observe that the sample size influences the estimated @xmath85 and hence , due to the correlation between @xmath10 and @xmath11 , also affects the estimation of @xmath96 .",
    "we observe systematically lower values of @xmath217 for @xmath209 than for @xmath204 in all the studied ` null ' scenarios .",
    "this feature translates in systematically larger values of @xmath96 for @xmath209 than for @xmath204 .",
    "however , the permutation test is able to account for this issue and the level of the test is respected independently of the sample size .",
    "analogously , increasing the number of features of the first source @xmath10 ( from @xmath176 to @xmath210 ) while keeping fixed the number of features of @xmath11 ( @xmath177 ) also affects the estimation of @xmath217 and @xmath96 . in this case , the values of @xmath217 are larger and hence , the values of @xmath96 tend to be closer to zero .",
    "worth noting is that the level of the test is also well respected in this case .",
    "the bottom part of table 1 shows the results for the alternative scenarios . as desirable",
    ", the power increases with sample size for all the three studied alternative scenarios .",
    "as it was the case for the ` null ' scenarios , increasing the sample size tends to lead to better predictive ability of @xmath10 .",
    "this result matches intuition , since larger sample sizes provide more information for model building , and hence , under correct model specification , the resulting predicting models are expected to behave better in new data .",
    "an exception to this is scenario 2c , where our double cross - validation procedure seems to overfit with @xmath209 .",
    "this is due to the fact that scenario 2c , unlike scenarios 2a and 2b , is characterized as a ` difficult ' prediction problem when considering @xmath10 ( association with @xmath18 is driven by a low - variance subspace of @xmath10 ) . in line with this , the power of the test is different for the three different studied scenarios . the greatest power is reached in scenario 2a , in which the independent association between @xmath11 and @xmath18 is driven through the subspace of maximum variation and the first step of the procedure relies on a relatively ` easy ' prediction problem .    even if scenarios 2b and 2c are based on the same independent association between @xmath16 and @xmath18 , the impact of the first source on the power of the test is large .",
    "scenario 2b , in which @xmath218 for @xmath204 reaches a power of 71 % , while the rejection rate reduces to 19% in scenario 2c , corresponding to a more ` difficult ' prediction problem in the first stage , reflected in a low and unstable @xmath215 ( @xmath219 for @xmath209 and @xmath220 for @xmath204 ) .",
    "[ table1 ]      table 2 shows the results for double cross - validation procedure based on the lasso specification ( @xmath37 ) . with regard to the ` null ' scenarios ,",
    "we observe a good performance for scenarios 1a and 1b , with rejection proportions close to the nominal level .",
    "interestingly , the rejection proportion of the permutation test increases with sample size and the number of features in the first source in scenarios 1c and 1d , which indicates a bad performance of the procedure based on laso regression in these settings .",
    "namely , the bad performance of the lasso specification for scenario 1d does not improve by increasing sample size ( 7% of rejections with @xmath209 , 9% of rejections with @xmath204 and 36% of rejections for @xmath210 ) .",
    "the reason behind this difference with the ridge - based results is the mis - specification of the lasso with respect to the underlying data - generating mechanism .",
    "lasso regression assumes that the true model is sparse , while , as mentioned , scenario 1c and specially 1d correspond to non - sparse solutions .",
    "these findings illustrate how model mis - specification may result in an improvement of predictions by adding a second source of predictors , not because of independent association to the outcome , but just because of the correlation with the first source of predictors .",
    "the bottom part of table 1 shows the results for the alternative scenarios . with respect to the alternative scenarios ( bottom part of table 2 ) , the conclusions are similar to those observed for ridge regression .",
    "the power increases with the sample size , and the rejection proportions differ across the three scenarios .",
    "however , we observe that ridge outperforms lasso in terms of power , specially for scenarios 2a and 2b .      tables s3 and s4 summarize the results for the two aforementioned alternative strategies in the basic setting ( @xmath176 , @xmath177 ) and two sample sizes ( @xmath209 , @xmath204 ) : ` @xmath213 , @xmath221 ' corresponds to the strategy in which the sequential double cross - validation is over - shrunk ( by taking @xmath222 instead of @xmath211 in the inner cross - validation loop ) and ` @xmath223 , @xmath211 ' represents the sequential procedure based on one single cross - validation loop ( standard residuals as opposed to deletion - based residuals ) .    in general ,",
    "these two alternative strategies provide different estimates for the predictive ability of the two studied sources of predictors . taking the double - cross validation approach as gold - standard",
    ", we observe that the over - shrinkage of the predictions in the first step of the ` @xmath213 , @xmath221 ' method provokes an under - estimation of @xmath215 , while the ` @xmath223 , @xmath211 ' provides an over - estimation , specially when the association between outcome and first source of predictors is driven through a low - variance space .",
    "for example , in the scenario 1b , for @xmath204 , @xmath224 when based on a single cross - validation approach , notably larger than @xmath225 estimated by the double cross - validation approach . moreover , we observe that the effect of re - using the data is larger for small sample sizes , with systematically larger @xmath215 for @xmath209 than for @xmath204 . however , under the null hypothesis , the introduced bias on the first step for both alternatives does not translate in an inflated type i error . the method labeled as ` @xmath213 ,",
    "@xmath221 ' , based on double cross - validation but based on under - fitting by over - penalization controls the false discovery rate under the null hypothesis in similar fashion than the procedure introduce in subsection 2.1 . with regard to the method based on single cross - validation ( ` @xmath223 , @xmath211 ' ) , its behavior is slightly conservative under the null hypothesis .    for the alternative scenarios , as @xmath85 , @xmath96 and @xmath105 , are underestimated by ` @xmath213 , @xmath221 ' , while ` @xmath223 , @xmath211 ' overfits both .",
    "even if power increases with sample size , both methods are systematically less powerful than our proposal,@xmath213 , @xmath211 , which makes it the preferable method from both an estimation and testing point of view .",
    "to illustrate the performance of the proposed sequential double cross - validation procedure , and to compare it to the alternative strategies discussed in section 3 , we analyzed data from the dilgom study .",
    "we are interested in the ability of serum nmr metabolites and microarray gene expression levels in blood to predict body mass index ( bmi ) at 7 years of follow - up .",
    "the metabolomic predictor data consists of quantitative information on 137 metabolic measures , mainly composed of measures on different lipid subclasses , but also amino acids , and creatine .",
    "the gene expression profiles were derived from illumina 610-quad snparrays ( illumina inc . ,",
    "san diego , ca , usa ) .",
    "initially , 35,419 expression probes were available after quality filtering .",
    "in addition to the pre - processing steps described by @xcite , we conducted a prior filtering approach and removed from our analyses probes with extremely low variation ( see @xcite for details on the conducted pre - processing ) . as a result , we retained measures from 7380 beads for our analyses .",
    "the analyzed sample contained @xmath226 individuals for which both types of omic measurements and the bmi after 7 years of follow - up ( mean=26 @xmath227 , sd=5 @xmath227 ) were available .",
    "we carried out two distinct analyses using the added predictive value assessment approach described in this paper .",
    "as a first analysis , we consider the metabolic profile as primary omic source for the prediction of the log - transformed bmi and we evaluated the added predictive value of blood transcriptomics profiles .",
    "this approach is the most relevant in practice , because of both biological and economical reasons .",
    "on the one hand , metabolome ( which contains , among other , cholesterol measures ) is presumably more predictive of bmi than gene expression in blood . on the other hand ,",
    "nmr technology is typically more affordable @xcite than available technologies for transcriptomic profiling , so favoring the nmr source seems a sensible approach in our setting .",
    "nevertheless , to illustrate the properties of our method , we also consider a second analysis in which we reversed the roles of the omic sources , first fitting a model based on gene expression and then evaluating the added predictive value of the metabolome . as in the simulation study , we considered ridge and lasso regression as prediction models , using the same alternative strategies to the sequential double cross - validation procedure presented in section 3 ( ` @xmath213 , @xmath211 ' ) : ` @xmath213 , @xmath221 ' , and ` @xmath223 , @xmath211 ' .",
    "the main findings are summarized in table 3 . to check stability of the results",
    ", we artificially reduced the sample size of the available dilgom data and checked the impact on the estimation of the added predictive ability with our sequential double cross - validation approach and its corresponding p - value .",
    "we also compared our method with a naive approach , consisting in stacking both metabolites and transcriptomics , and hence ignoring their different origin .",
    "the results of these two additional analyses are given as supplemental materials .",
    ".application to dilgom data .",
    "alternative cross - validation strategies .",
    "p - values based on 1000 permutations [ cols=\"^,^,^,^,^,^\",options=\"header \" , ]     [ tab : addlabel ]",
    "simulation results based on two modification of the two - stage procedure presented in section 2 : @xmath223,@xmath211 relies on single cross - validation ( cross - validation is used for model choice but predictions and therefore the residuals used as outcome in the second stage are directly computed on the complete sample ) ; @xmath213,@xmath221 relies on over - penalization . specifically , instead of taking @xmath211 as defined in the inner loop of the double cross - validation procedure presented in subsection 2.1 .",
    ", we choose a larger value for @xmath32 , namely @xmath212 . simulations for these two approaches are based on the same specifications detailed in subsection 3.1 . , considering @xmath176 and @xmath177 ."
  ],
  "abstract_text": [
    "<S> enriching existing predictive models with new biomolecular markers is an important task in the new multi - omic era . </S>",
    "<S> clinical studies increasingly include new sets of omic measurements which may prove their added value in terms of predictive performance . </S>",
    "<S> we introduce a two - step approach for the assessment of the added predictive ability of omic predictors , based on sequential double cross - validation and regularized regression models . </S>",
    "<S> we propose several performance indices to summarize the two - stage prediction procedure and a permutation test to formally assess the added predictive value of a second omic set of predictors over a primary omic source . </S>",
    "<S> the performance of the test is investigated through simulations . </S>",
    "<S> we illustrate the new method through the systematic assessment and comparison of the performance of transcriptomics and metabolomics sources in the prediction of body mass index ( bmi ) using longitudinal data from the dietary , lifestyle , and genetic determinants of obesity and metabolic syndrome ( dilgom ) study , a population - based cohort from finland .     * sequential double cross - validation for assessment of added predictive ability in high - dimensional omic applications * +    mar rodrguez - girondo@xmath0 , perttu salo@xmath1 , tomasz burzykowski@xmath2 , markus perola@xmath1 , jeanine houwing - duistermaat@xmath3 and bart mertens@xmath0    @xmath0department of medical statistics and bioinformatics , leiden university medical center , leiden , the netherlands .    </S>",
    "<S> @xmath1national institute for health and welfare , helsinki , finland .    </S>",
    "<S> @xmath2interuniversity institute for biostatistics and statistical bioinformatics ( i - biostat ) , hasselt university , hasselt , belgium .    </S>",
    "<S> @xmath4department of statistics , leeds university , leeds , united kingdom .    </S>",
    "<S> * keywords : * _ added predictive ability ; double cross - validation ; regularized regression ; multiple omics sets _ </S>"
  ]
}