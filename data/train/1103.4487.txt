{
  "article_text": [
    "current automatic handwriting recognition algorithms are already pretty good at learning to recognize handwritten digits . more than a decade ago , multilayer perceptrons or mlps @xcite were among the first classifiers tested on the now famous mnist handwritten digit recognition benchmark .",
    "most had few layers or few artificial neurons ( units ) per layer @xcite , but apparently back then these were the biggest feasible mlps , trained when cpu cores were at least 20 times slower than today .",
    "a more recent mlp with a single hidden layer of 800 units achieved 0.70% error @xcite . the latest substantial improvement by others occurred in 2003 @xcite ( error rate 0.4% ) .",
    "it was obtained with a convolutional neural network ( cnn ) , using then novel elastic training image deformations .",
    "@xcite pre - trained each hidden cnn layer one by one in unsupervised fashion ( this seems promising especially for small training sets ) , then used supervised learning to achieve 0.39% error rate .",
    "recently we were able to significantly improve this result , obtaining an error rate of 0.35% using graphics cards ( gpus ) to greatly speed up training of plain but deep mlps @xcite .",
    "deformations proved essential to prevent mlps with up to 12 million free parameters from overfitting . to let the deformation process keep up with network training speed we had to port it onto the gpu as well .    at some stage in the classifier design process one",
    "usually has collected a set of possible classifiers .",
    "often one of them yields best performance .",
    "intriguingly , however , the sets of patterns misclassified by the different classifiers do not necessarily overlap .",
    "this information could be harnessed in a committee . in the context of handwritten recognition ,",
    "@xcite already showed how a combination of various classifiers can be trained more quickly than a single classifier yielding the same error rate . here",
    "we focus on improving recognition rate using a committee of mlps .",
    "our goal is to produce a group of classifiers whose errors on various parts of the training set differ as much as possible @xcite .",
    "we show that for handwritten digit recognition this can be achieved by training identical classifiers on data normalized in different ways prior to training .",
    "mnist consists of two datasets , one for training ( 60,000 images ) and one for testing ( 10,000 images ) .",
    "many studies divide the training set into two sets consisting of 50,000 images for training and 10,000 for validation .",
    "so far , however , the best results on mnist were obtained by deforming training images , thus greatly increasing their number .",
    "this allows for training networks with many weights , making them insensitive to in - class variability .",
    "our network is also trained on numerous slightly deformed images , continually generated in online fashion ; hence we may use the whole un - deformed training set for validation , without wasting training images .",
    "pixel intensities of the original gray scale images range from 0 ( background ) to 255 ( max foreground intensity ) .",
    "@xmath0 pixels per image get mapped to real values @xmath1 in @xmath2 $ ] , and are fed into the nn input layer .",
    "we combine affine ( rotation , scaling and horizontal shearing ) and elastic deformations , characterized by the following real - valued parameters :    * @xmath3 and @xmath4 : for elastic distortions emulating uncontrolled oscillations of hand muscles @xcite .",
    "they are obtained by applying a displacement field to each digit .",
    "the displacement field is built by convolving a randomly initialized field with a gaussian kernel whose standard deviation is defined by @xmath3 .",
    "@xmath4 is a scaling factor controlling the amplitude of the applied elastic deformations ; * @xmath5 : a random angle from @xmath6 $ ] describes either rotation or horizontal shearing . in case of shearing",
    ", @xmath7 defines the ratio between horizontal displacement and image height ; * @xmath8 , @xmath9 : for horizontal and vertical scaling , randomly selected from @xmath10 $ ] .    at the beginning of every epoch the entire mnist training set",
    "gets deformed .",
    "initial experiments with small networks suggested the following deformation parameters : @xmath11 , @xmath12 , @xmath13 . since digits 1 and 7 are similar they get rotated / sheared less ( @xmath14 ) than other digits ( @xmath15 ) .",
    "it takes 83 cpu seconds to deform the 60,000 mnist training images , most of them ( 75 seconds ) for elastic distortions . only the most time - consuming part of the latter ",
    "convolution with a gaussian kernel  is ported to the gpu . the mnist training set is split into 600 sequentially processed batches .",
    "mnist digits are scaled from the original @xmath16 pixels to @xmath17 pixels , to get a proper center , which simplifies convolution . each batch grid",
    "( @xmath18 images ) has 290 @xmath19 290 cells , zero - padded to 310 @xmath19 310 , thus avoiding margin effects when applying a gaussian convolution kernel of size @xmath20 .",
    "the gpu program groups many threads into a block , where they share the same gaussian kernel and parts of the random field .",
    "all 29 @xmath19 290 blocks contain 21 ( the kernel size ) @xmath1910 threads , each computing a vertical strip of the convolution ( figure  [ fig : conv ] ) .",
    "generating the elastic displacement field takes only 1.5 seconds .",
    "deforming the whole training set is more than 10 times faster , taking 6.5 instead of the original 83 seconds .",
    "further optimizations would be possible by porting all deformations onto gpu , and by using the hardware s interpolation capabilities to perform the final bilinear interpolation .",
    "we omitted these since deformations are already pretty fast ( deforming all images of one epoch takes only 3 - 10 % of total computation time , depending on mlp size ) .",
    "preprocessing of the original mnist data is mainly motivated by practical experience .",
    "mnist digits are normalized such that the width or height of the bounding box equals 20 pixels .",
    "the variation of the aspect ratio for various digits is quite large , and we normalize the width of the bounding box to range from 10 to 20 pixels with a step - size of 2 pixels prior to training for all digits except ones . normalizing the original mnist training data results in 6 normalized training sets . in total",
    "we perform experiments with seven different data sets ( 6 normalized and the original mnist )",
    ".    the training procedure of a network is summarized in figure [ fig : training ] .",
    "each network is trained separately on normalized or original data .",
    "the normalization is done for all digits in the training set prior to training ( normalization stage ) . during each training epoch",
    "the digits are distorted ( sec .",
    "[ sec : distortions ] ) in a different way . for the network trained on original mnist data",
    "the normalization step is omitted .",
    "we perform six experiments to analyze performance improvements due to committees .",
    "each committee consists of seven randomly initialized one - hidden - layer mlps with 800 hidden units , trained with the same algorithm on randomly selected batches .",
    "the five committees differ only in how the data are preprocessed ( or not ) prior to training and on how the data are deformed during training .",
    "the committees are formed by simply averaging the corresponding outputs as shown in figure [ fig : testing ] .",
    "the first two experiments are performed on undeformed original mnist images .",
    "we train a committee of seven mlps on original mnist and we also form a committee of mlps trained on preprocessed data . in table",
    "[ table : results_orig ] the error rates are listed for each of the individual nets and the committees .",
    "the improvement of the committee with respect to the individual nets is marginal for the first experiment .",
    "adding preprocessing , the individual experts as well as the corresponding committee of the second experiment achieve substantially better recognition rates .",
    ".__error rates of individual nets and of the two resulting committees . for experiment",
    "1 seven nets are trained on the original mnist , whereas for experiment 2 seven nets are trained on preprocessed data : wn x - width normalization of the bounding box to be x pixels wide ; orig - original mnist . _ _ [ cols= \" < , < , > , < , > \" , ]",
    "current gpus are more than 50 times faster than standard microprocessors when it comes to training big and deep neural networks with online back - propagation ( weight update rate up to @xmath21 , and more than @xmath22 per trained network ) . on the competitive mnist handwriting benchmark ,",
    "single precision floating - point gpu - based committees of neural nets ( each with a a different preprocessor motivated by observed variations in aspect ratio and slant of handwritten digits ) outperform all previously published methods , including complex ones involving specialized architectures , unsupervised pre - training , combinations of machine learning classifiers etc . to avoid overfitting , training sets of sufficient size",
    "are obtained by appropriately distorting images .",
    "this work was partially funded by the swiss commission for technology and innovation ( cti ) , project n. 9688.1 iff : intelligent fill in form .",
    "kumar chellapilla , michael shilman , and patrice simard .",
    "combining multiple classifiers for faster optical character recognition . in _ document analysis systems vii _ , pages 358367 .",
    "springer berlin / heidelberg , 2006 .",
    "doi : 10.1007/11669487_32 .",
    "ranzato , christopher poultney , sumit chopra , and yann lecun .",
    "efficient learning of sparse representations with an energy - based model . in j.",
    "platt et  al . , editor , _ advances in neural information processing systems ( nips 2006)_. mit press , 2006 .",
    "ranzato , fu jie huang , y - lan boureau , and yann lecun .",
    "unsupervised learning of invariant feature hierarchies with applications to object recognition . in _ proc .",
    "computer vision and pattern recognition conference ( cvpr07)_. ieee press , 2007 .",
    "d.  e. rumelhart , geoffrey  e. hinton , and ronald .",
    "j. williams .",
    "learning internal representations by error propagation . in _ parallel distributed processing : explorations in the microstructure of cognition , vol . 1 : foundations _ , pages 318362 . mit press , cambridge , ma , usa , 1986 .",
    "isbn 0 - 262 - 68053-x .",
    "patrice  y. simard , dave .",
    "steinkraus , and john  c. platt .",
    "best practices for convolutional neural networks applied to visual document analysis . in _",
    "seventh international conference on document analysis and recognition _ , pages 958963 , 2003 ."
  ],
  "abstract_text": [
    "<S> the competitive mnist handwritten digit recognition benchmark has a long history of broken records since 1998 . </S>",
    "<S> the most recent substantial improvement by others dates back 7 years ( error rate 0.4% ) . </S>",
    "<S> recently we were able to significantly improve this result , using graphics cards to greatly speed up training of simple but deep mlps , which achieved 0.35% , outperforming all the previous more complex methods . </S>",
    "<S> here we report another substantial improvement : 0.31% obtained using a committee of mlps . </S>"
  ]
}