{
  "article_text": [
    "in recent years , motivated in large part by technological advances in both scientific and internet domains , there has been a great deal of interest in what may be broadly termed _",
    "large - scale data analysis_. in this chapter , i will focus on what seems to me to be a remarkable and inevitable trend in this increasingly - important area .",
    "this trend has to do with the convergence of two very different perspectives or worldviews on what are appropriate or interesting or fruitful ways to view the data . at the risk of oversimplifying a large body of diverse work ,",
    "i would like to draw a distinction between what i will loosely term the _ algorithmic perspective _ and the _ statistical perspective _ on large - scale data analysis problems . by the former ,",
    "i mean roughly the approach that one trained in computer science might adopt . from this perspective ,",
    "primary concerns include database issues , algorithmic questions such as models of data access , and the worst - case running time of algorithms for a given objective function . by the latter ,",
    "i mean roughly the approach that one trained in statistics ( or some application area where strong domain - specific assumptions about the data may be made ) might adopt . from this perspective",
    ", primary concerns include questions such as how well the objective functions being considered conform to the phenomenon under study and whether one can make reliable predictions about the world from the data at hand .",
    "although very different , these two approaches are certainly not incompatible .",
    "moreover , in spite of peoples best efforts _ not _ to forge the union between these two worldviews , and instead to view the data from one perspective or another , depending on how one happened to be trained , the large - scale data analysis problems that we are increasingly facing  in scientific , internet , financial , etc . applications",
    "are so important and so compelling  either from a business perspective , or an academic perspective , or an intellectual perspective , or a national security perspective , or from whatever perspective you find to be most compelling  that we are being forced to forge this union .",
    "e.g. _ , if one looks at the sig - kdd meeting , acm s flagship meeting on data analysis , now ( in @xmath0 ) versus @xmath1 or @xmath2 years ago , one sees a substantial shift away from more database - type issues toward topics that may be broadly termed statistical , _",
    "e.g. _ , that either involve explicit statistical modeling or involve making generative assumptions about data elements or network participants . and vice - versa  there has been a substantial shift in statistics , and especially in the natural and social sciences more generally , toward thinking in great detail about computational issues . moreover , to the extent that statistics as an academic discipline is deferring on this matter , the relatively new area of machine learning is filling in the gap .",
    "i should note that i personally began thinking about these issues some time ago .",
    "my ph.d . was in computational statistical mechanics , and it involved a lot of monte carlo and molecular dynamics computations on liquid water and dna - protein - water interactions .",
    "after my dissertation , i switched fields to theoretical computer science , where i did a lot of work on the theory ( and then later on the application ) of randomized algorithms for large - scale matrix problems .",
    "one of the things that struck me during this transition was the deep conceptual disconnect between these two areas . in computer science , we have a remarkable infrastructure of machinery  from complexity classes and data structuring and algorithmic models , to database management , computer architecture , and software engineering paradigms  for solving problems .",
    "on the other hand , it seems to me that there tends to be a remarkable lack of appreciation , and thus associated cavalierness , when it comes to understanding how the data can be messy and noisy and poorly - structured in ways that adversely affect how one can be confident in the conclusions that one draws about the world as a result of the output of one s fast algorithms .    in this chapter",
    ", i would like to describe some of the fruits of this thought .",
    "to do so , i will focus on two very particular very applied problems in large - scale data analysis on which i have worked .",
    "the solution to these two problems benefited from taking advantage of the complementary algorithmic versus statistical perspectives in a novel way .",
    "in particular , we will see that , by understanding the statistical properties _ implicit _ in worst - case algorithms , we can make very strong claims about very applied problems .",
    "these claims would have been _ much _ more difficult to make and justify if one chose to view the problem from just one perspective or the other .",
    "the first problem has to do with selecting good features from a data matrix representing dna microarray or dna single nucleotide polymorphism data .",
    "this is of interest , _",
    "e.g. _ , to geneticists who want to understand historical trends in population genetics , as well as to biomedical researchers interested in so - called personalized medicine .",
    "the second problem has to do with identifying , or certifying that there do not exist , good clusters or communities in a data graph representing a large social or information network .",
    "this is of interest in a wide range of applications , such as finding good clusters or micro - markets for query expansion or bucket testing in internet advertising applications .",
    "the applied results for these two problems have been reported previously in appropriate domain - specific ( in this case , genetics and internet ) publications  @xcite , and i am indebted to my collaborators with whom i have discussed some of these ideas in preliminary form . thus , rather than focusing on the genetic issues _ per se _ or the internet advertising issues _ per se _ or the theoretical analysis _ per se _ , in this chapter",
    "i would like to focus on what was going on `` under the hood '' in terms of the interplay between the algorithmic and statistical perspectives .",
    "the hope is that these two examples can serve as a model for exploiting the complementary aspects of the algorithmic and statistical perspectives in order to solve very applied large - scale data analysis problems more generally .",
    "as we will see , in neither of these two applications did we _",
    "first _ perform statistical modeling , independent of algorithmic considerations , and _ then _ apply a computational procedure as a black box .",
    "this approach of more closely coupling the computational procedures used with statistical modeling or statistical understanding of the data seems particularly appropriate more generally for very large - scale data analysis problems , where design decisions are often made based on computational constraints but where it is of interest to understand the implicit statistical consequences of those decisions .",
    "before proceeding , i would like in this section to set the stage by describing in somewhat more detail some of diverse approaches that have been brought to bear on modern data analysis problems  @xcite . in particular , although the significant differences between the algorithmic perspective and the statistical perspective have been highlighted previously  @xcite , they are worth reemphasizing .",
    "a common view of the data in a database , in particular historically among computer scientists interested in data mining and knowledge discovery , has been that the data are an accounting or a record of everything that happened in a particular setting .",
    "for example , the database might consist of all the customer transactions over the course of a month , or it might consist of all the friendship links among members of a social networking site . from this perspective , the goal is to tabulate and process the data at hand to find interesting patterns , rules , and associations .",
    "an example of an association rule is the proverbial `` people who buy beer between @xmath3 p.m.  and @xmath4 p.m.  also buy diapers at the same time . ''",
    "the performance or quality of such a rule is judged by the fraction of the database that satisfies the rule exactly , which then boils down to the problem of finding frequent itemsets .",
    "this is a computationally hard problem , and much algorithmic work has been devoted to its exact or approximate solution under different models of data access .",
    "a very different view of the data , more common among statisticians , is one of a particular random instantiation of an underlying process describing unobserved patterns in the world . in this case",
    ", the goal is to extract information about the world from the noisy or uncertain data that are observed . to achieve this",
    ", one might posit a model : @xmath5 and @xmath6 , where @xmath7 is a distribution that describes the random variability of the data around the deterministic model @xmath8 of the data . then , using this model , one would proceed to analyze the data to make inferences about the underlying processes and predictions about future observations . from this perspective , modeling the noise component or variability well is as important as modeling the mean structure well , in large part since understanding the former is necessary for understanding the quality of predictions made . with this approach , one can even make predictions about events that have yet to be observed . for example , one can assign a probability to the event that a given user at a given web site will click on a given advertisement presented at a given time of the day , even if this particular event does not exist in the database .",
    "although these two perspectives are certainly not incompatible , they are very different , and they lead one to ask very different questions of the data and of the structures that are used to model data . recall that in many applications , graphs and matrices are common ways to model the data  @xcite .",
    "for example , a common way to model a large social or information network is with an interaction graph model , @xmath9 , in which nodes in the vertex set @xmath10 represent `` entities '' and the edges ( whether directed , undirected , weighted or unweighted ) in the edge set @xmath11 represent `` interactions '' between pairs of entities .",
    "alternatively , these and other data sets can be modeled as matrices , since an @xmath12 real - valued matrix @xmath13 provides a natural structure for encoding information about @xmath14 objects , each of which is described by @xmath15 features .",
    "thus , in the next two sections , i will describe two recent examples  one having to do with modeling data as matrices , and the other having to do with modeling data as a graph  of particular data analysis problems that benefited from taking advantage in novel ways of the respective strengths of the algorithmic and statistical perspectives .",
    "in this section , i will describe an algorithm for selecting a `` good '' set of exactly @xmath16 columns ( or , equivalently , exactly @xmath16 rows ) from an arbitrary @xmath12 matrix @xmath13 .",
    "this problem has been studied extensively in scientific computing and numerical linear algebra ( nla ) , often motivated by the goal of finding a good basis with which to perform large - scale numerical computations .",
    "in addition , variants of this problem have recently received a great deal of attention in theoretical computer science ( tcs ) .",
    "more generally , problems of this sort arise in many data analysis applications , often in the context of finding a good set of features with which to describe the data or to perform tasks such as classification or regression .      recall that `` the human genome '' consists of a sequence of roughly @xmath17 billion base pairs on @xmath18 pairs of chromosomes , roughly @xmath19 of which codes for approximately @xmath20  @xmath21 proteins .",
    "a dna microarray is a device that can be used to measure simultaneously the genome - wide response of the protein product of each of these genes for an individual or group of individuals in numerous different environmental conditions or disease states .",
    "this very coarse measure can , of course , hide the individual differences or polymorphic variations",
    ". there are numerous types of polymorphic variation , but the most amenable to large - scale applications is the analysis of single nucleotide polymorphisms ( snps ) , which are known locations in the human genome where two alternate nucleotide bases ( or alleles , out of @xmath13 , @xmath22 , @xmath23 , and @xmath24 ) are observed in a non - negligible fraction of the population .",
    "these snps occur quite frequently , ca .",
    "@xmath25 b.p . per thousand , and thus they are effective genomic markers for the tracking of disease genes ( _ i.e. _ , they can be used to perform classification into sick and not sick ) as well as population histories ( _ i.e _ , they can be used to infer properties about population genetics and human evolutionary history ) .    in both cases , @xmath12 matrices",
    "@xmath13 naturally arise , either as a people - by - gene matrix , in which @xmath26 encodes information about the response of the @xmath27 gene in the @xmath28 individual / condition , or as people - by - snp matrices , in which @xmath26 encodes information about the value of the @xmath27 snp in the @xmath28 individual .",
    "thus , matrix computations have received attention in these applications  @xcite .",
    "a common _ modus operandi _ in applying nla and matrix techniques such as pca and the svd to to dna microarray , dna snps , and other data problems is :    * model the people - by - gene or people - by - snp data as an @xmath12 matrix @xmath13 . *",
    "perform the svd ( or related eigen - methods such as pca or recently - popular manifold - based methods  @xcite that boil down to the svd ) to compute a small number of eigengenes or eigensnps or eigenpeople that capture most of the information in the data matrix .",
    "* interpret the top eigenvectors as meaningful in terms of underlying biological processes ; or apply a heuristic to obtain actual genes or actual snps from the corresponding eigenvectors in order to obtain such an interpretation .    in certain cases ,",
    "such reification may lead to insight and such heuristics may be justified .",
    "( for instance , if the data happen to be drawn from a guassian distribution , then the eigendirections tend to correspond to the axes of the corresponding ellipsoid , and there are many vectors that , up to noise , point along those directions . ) in such cases , however , the justification comes from domain knowledge and not the mathematics  @xcite .",
    "the reason is that the eigenvectors themselves , being mathematically defined abstractions , can be calculated for any data matrix and thus are not easily understandable in terms of processes generating the data : eigensnps ( being linear combinations of snps ) can not be assayed ; nor can eigengenes be isolated and purified ; nor is one typically interested in how eigenpatients respond to treatment when one visits a physician .    for this and other reasons ,",
    "a common task in genetics and other areas of data analysis is the following : given an input data matrix @xmath13 and a parameter @xmath16 , find the best subset of exactly @xmath16 _ actual _ dna snps or _",
    "i.e. _ , _ actual _ columns or rows from @xmath13 , to use to cluster individuals , reconstruct biochemical pathways , reconstruct signal , perform classification or inference , etc .",
    "unfortunately , common formalizations of this algorithmic problem  including looking for the @xmath16 actual columns that capture the largest amount of information or variance in the data or that are maximally uncorrelated  lead to intractable optimization problems  @xcite . in this chapter",
    ", i will consider the so - called column subset selection problem ( cssp ) : given as input an arbitrary @xmath12 matrix @xmath13 and a rank parameter @xmath16 , choose the set of exactly @xmath16 columns of @xmath13 s.t .",
    "the @xmath29 matrix @xmath22 minimizes ( over all @xmath30 sets of such columns ) the error : @xmath31 where @xmath32 represents the spectral or frobenius norm of @xmath13 and where @xmath33 is the projection onto the subspace spanned by the columns of  @xmath22 .    within nla ,",
    "a great deal of work has focused on this cssp problem  @xcite .",
    "several general observations about the nla approach include :    * the focus in nla is on _ deterministic algorithms_. moreover , these algorithms are greedy , in that at each iterative step , the algorithm makes a decision about which columns to keep according to a pivot - rule that depends on the columns it currently has , the spectrum of those columns , etc .",
    "differences between different algorithms often boil down to how deal with such pivot rules decisions , and the hope is that more sophisticated pivot - rule decisions lead to better algorithms in theory or in practice .",
    "* there are deep _ connections with qr factorizations _ and in particular with the so - called rank revealing qr factorizations .",
    "moreover , there is an emphasis on optimal conditioning questions , backward error analysis issues , and whether the running time is a large or small constant multiplied by @xmath34 or @xmath35 .",
    "* good _ spectral norm bounds _ are obtained .",
    "a typical spectral norm bound is : @xmath36 and these results are algorithmic , in that the running time is a low - degree polynomial in @xmath14 and @xmath15  @xcite . on the other hand ,",
    "the strongest results for the frobenius norm in this literature is @xmath37 but it is only an existential result , _",
    "i.e. _ , the only known algorithm essentially involves exhaustive enumeration  @xcite .",
    "( in these two expressions , @xmath38 is the @xmath29 matrix consisting of the top @xmath16 left singular vectors of @xmath13 , and @xmath39 is a projection matrix onto the span of @xmath38 . )    within tcs , a great deal of work has focused on the related problem of choosing good columns from a matrix  @xcite .",
    "several general observations about the tcs approach include :    * the focus in tcs is on _ randomized algorithms_. in particular , with these algorithms , there exists some nonzero probability , which can typically be made extremely small , say @xmath40 , that the algorithm will return columns that fail to satisfy the desired quality - of - approximation bound .",
    "* the algorithms select _ more than @xmath16 columns _ , and the best rank-@xmath16 projection onto those columns is considered .",
    "the number of columns is typically a low - degree polynomial in @xmath16 , most often @xmath41 , where the constants hidden in the big - o notation are quite reasonable .",
    "* very good _ frobenius norm bounds _ are obtained .",
    "for example , the algorithm ( described below ) that provides the strongest frobenius norm bound achieves : @xmath42 while running in time of the order of computing an exact or approximate basis for the top-@xmath16 right singular subspace  @xcite .",
    "the tcs literature also demonstrates that there exists a set of @xmath16 columns that achieves a constant - factor approximation : @xmath43 but note that this is an existential result  @xcite .",
    "( here , @xmath44 is the best rank-@xmath16 approximation to the matrix @xmath22 , and @xmath45 is the projection matrix onto this @xmath16-dimensional space . )    much of the early work in tcs focused on randomly sampling columns according to an importance sampling distribution that depended on the euclidean norm of those columns  @xcite .",
    "this had the advantage of being `` fast , '' in the sense that it could be performed in a small number of `` passes '' over that data from external storage , and also that additive - error quality - of - approximation bounds could be proved .",
    "this had the disadvantage of being less immediately - applicable to scientific computing and large - scale data analysis applications .",
    "for example , columns are often normalized during data preprocessing ; and even when not normalized , column norms can still be uninformative , as in heavy - tailed graph data where they often correlate strongly with simpler statistics such as node degree .",
    "the algorithm from the tcs literature that achieves the strongest frobenius norm bounds of the form  ( [ eqn : rel - err ] ) is the following . and",
    "a rank parameter @xmath16 , one can express the svd as @xmath46 , in which case the best rank-@xmath16 approximation to @xmath13 can be expressed as @xmath47 . in the text ,",
    "i will sometimes overload notation and use @xmath48 to refer to any @xmath49 orthonormal matrix spanning the space spanned by the top-@xmath16 right singular vectors .",
    "the reason is that this basis is used only to compute the importance sampling probabilities ; since those probabilities are proportional to the diagonal elements of the projection matrix onto the span of this basis , the particular basis does not matter . ]",
    "given an @xmath12 matrix @xmath13 and a rank parameter @xmath16 :    * compute the importance sampling probabilities @xmath50 , where @xmath51 , where @xmath48 is _ any _ @xmath49 orthogonal matrix spanning the top-@xmath16 right singular subspace of  @xmath13 .",
    "( note that these quantities are proportional to the diagonal elements of the projection matrix onto the span of @xmath48 . ) * randomly select and rescale @xmath52 columns of @xmath13 according to these probabilities .",
    "a more detailed description of this algorithm may be found in  @xcite , where it is proven that  ( [ eqn : rel - err ] ) holds with extremely high probability .",
    "the computational bottleneck for this algorithm is computing @xmath50 , for which it suffices to compute _ any _ @xmath49 matrix @xmath48 that spans the top-@xmath16 right singular subspace of @xmath13 .",
    "( that is , it suffices to compute any orthonormal basis spanning @xmath48 , and it is not necessary to compute the full svd . ) it is an open problem whether these importance sampling probabilities can be approximated more rapidly .    to motivate the importance sampling probabilities used by this algorithm ,",
    "recall that if one is looking for a worst - case relative - error approximation of the form  ( [ eqn : rel - err ] ) to a matrix with @xmath53 large singular values and one much smaller singular value , then the directional information of the @xmath54 singular direction will be hidden from the euclidean norms of the matrix .",
    "intuitively , the reason is that , since @xmath55 , the euclidean norms of the columns of @xmath13 are convolutions of `` subspace information '' ( encoded in @xmath38 and @xmath48 ) and `` size - of-@xmath13 information '' ( encoded in @xmath56 ) .",
    "this suggests deconvoluting subspace information and size - of-@xmath13 information by choosing importance sampling probabilities that depend on the euclidean norms of the columns of @xmath48 .",
    "thus , this importance sampling distribution defines a nonuniformity structure over @xmath57 that indicates _ where _ in the @xmath15-dimensional space the information in @xmath13 is being sent , independent of _ what _ that ( singular value ) information is . as we will see in the next two subsections , by using these importance sampling probabilities",
    ", we can obtain novel algorithms for two very traditional problems in nla and scientific computing .",
    "the analysis of the relative - error algorithm described in the previous subsection and of the algorithm for the cssp described in the next subsection boils down to a least - squares approximation result .",
    "intuitively , these algorithms find columns that provide a space that is good in a least - squares sense , when compared to the best rank-@xmath16 space , at reconstructing every row of the input matrix  @xcite .",
    "thus , consider the problem of finding a vector @xmath58 such that @xmath59 , where the rows of @xmath13 and elements of @xmath60 correspond to constraints and the columns of @xmath13 and elements of @xmath58 correspond to variables . in the very overconstrained case where the @xmath12 matrix @xmath13 has @xmath61 , .",
    "] there is in general no vector @xmath58 such that @xmath62 , and it is common to quantify `` best '' by looking for a vector @xmath63 such that the euclidean norm of the residual error is small , _",
    "i.e. _ , to solve the least - squares ( ls ) approximation problem @xmath64 this problem is ubiquitous in applications , where it often arises from fitting the parameters of a model to experimental data , and it is central to theory .",
    "moreover , it has a natural statistical interpretation as providing the best estimator within a natural class of estimators , and it has a natural geometric interpretation as fitting the part of the vector @xmath60 that resides in the column space of @xmath13 . from the viewpoint of low - rank matrix approximation and the cssp",
    ", this ls problem arises since measuring the error with a frobenius or spectral norm , as in  ( [ eqn : error - measure ] ) , amounts to choosing columns that are `` good '' in a least squares sense .    from an algorithmic perspective , the relevant question is : how long does it take to compute @xmath63 ?",
    "the answer here is that is takes @xmath65 time  @xcite_e.g .",
    "_ , depending on numerical issues , condition numbers , etc .",
    ", this can be accomplished with the cholesky decomposition , a variant of the qr decomposition , or by computing the full svd .    from a statistical perspective",
    ", the relevant question is : when is computing this @xmath63 the right thing to do ?",
    "the answer to this is that this ls optimization is the right problem to solve when the relationship between the `` outcomes '' and `` predictors '' is roughly linear and when the error processes generating the data are `` nice '' ( in the sense that they have mean zero , constant variance , are uncorrelated , and are normally distributed ; or when we have adequate sample size to rely on large sample theory )  @xcite .    of course , in practice these assumptions do not hold perfectly , and a prudent data analyst will check to make sure that these assumptions have not been too violated . to do this , it is common to assume that @xmath66 , where @xmath60 is the response , the columns @xmath67 are the carriers , and @xmath68 is the nice error process",
    ". then @xmath69 , and thus @xmath70 , where the projection matrix onto the column space of @xmath13 , @xmath71 , is the so - called _ hat matrix_. it is known that @xmath72 measures the influence or statistical leverage exerted on the prediction @xmath73 by the observation @xmath74",
    "relatedly , if the @xmath28 diagonal element of @xmath75 is particularly large then the @xmath28 data point is particularly sensitive or influential in determining the best ls fit , thus justifying the interpretation of the elements @xmath76 as _ statistical leverage scores _  @xcite .    to gain insight into these statistical leverage scores ,",
    "consider the so - called `` wood beam data '' example  @xcite , which is visually presented in figure  [ fig : leverage : wooddata ] , along with the best - fit line to that data .",
    "in figure  [ fig : leverage : woodscores ] , the leverage scores for these ten data points are shown .",
    "intuitively , data points that `` stick out '' have particularly high leverage_e.g .",
    "_ , the data point that has the most influence or leverage on the best - fit line to the wood beam data is the point marked `` 4 '' , and this is reflected in the relative magnitude of the corresponding statistical leverage score . indeed , since @xmath77 , a rule of thumb that has been suggested in diagnostic regression analysis to identify errors and outliers in a data set is to investigate the @xmath28 data point if @xmath78  @xcite , _ i.e. _ , if @xmath76 is larger that @xmath79 or @xmath17 times the `` average '' size . on the other hand , of course ,",
    "if it happens to turn out that such a point is a legitimate data point , then one might expect that such an outlying data point will be a particularly important or informative data point .",
    "+   +    returning to the algorithmic perspective , consider the following random sampling algorithm for the ls approximation problem  @xcite .",
    "given a very overconstrained least - squares problem , where the input matrix @xmath13 and vector @xmath60 are _ arbitrary _ , but @xmath61 :    * compute normalized statistical leverage scores @xmath80 , where @xmath81 , where @xmath82 is the @xmath12 matrix consisting of the left singular vectors of @xmath13 .",
    "is _ any _ orthogonal matrix spanning the column space of @xmath13 , then @xmath83 and thus @xmath84 , _",
    "i.e. _ , the statistical leverage scores equal the euclidean norm of the rows of any such matrix @xmath82  @xcite .",
    "clearly , the columns of @xmath82 are orthonormal , but the rows of @xmath82 in general are not  they can be uniform if , _",
    "e.g. _ , @xmath82 consists of columns from a truncated hadamard matrix ; or extremely nonuniform if , _",
    "e.g. _ , the columns of @xmath82 come from a truncated identity matrix ; or anything in between . ] * randomly sample and rescale @xmath85 constraints , _",
    "i.e. _ , rows of @xmath13 and the corresponding elements of @xmath60 , using these scores as an importance sampling distribution .",
    "* solve ( using any appropriate ls solver as a black box ) the induced subproblem to obtain a vector @xmath86 .    since this algorithm samples constraints and not variables , the dimensionality of the vector @xmath86 that solves the subproblem is the same as that of the vector @xmath63 that solves the original problem .",
    "this algorithm is described in more detail in  @xcite , where it is shown that relative error bounds of the form @xmath87 hold .",
    "even more importantly , this algorithm highlights that the essential nonuniformity structure for the worst - case analysis of the ls ( and , as we will see , the related cssp ) problem is defined by the statistical leverage scores ! that is , the same `` outlying '' data points that the diagnostic regression analyst tends to investigate are those points that are biased toward by the worst - case importance sampling probability distribution .",
    "clearly , for this ls algorithm , which holds for arbitrary input @xmath13 and @xmath60 , @xmath65 time suffices to compute the sampling probabilities ; in addition , it has recently been shown that one can obtain a nontrivial approximation to them in @xmath88 time  @xcite . for many applications , such as those described in subsequent subsections ,",
    "spending time on the order of computing an exact or approximate basis for the top-@xmath16 singular subspace is acceptable , in which case immediate generalizations of this algorithm are of interest . in other cases , one can preprocess the ls system with a `` randomized hadamard '' transform ( as introduced in the `` fast '' johnson - lindenstrauss lemma  @xcite ) .",
    "application of such a hadamard transform tends to `` uniformize '' the leverage scores , intuitively for the same reason that a fourier matrix delocalizes a localized @xmath89-function , in much the same way as application of a random orthogonal matrix or a random projection does .",
    "this has led to the development of relative - error approximation algorithms for the ls problem that run in @xmath88 time in theory  @xcite  essentially @xmath90 time , which is much less than @xmath65 when @xmath61and whose numerical implementation performs faster than traditional deterministic algorithms for systems with as few as thousands of constraints and hundreds of variables  @xcite .      in this subsection",
    ", i will describe an algorithm for the cssp that uses the concept of statistical leverage to combine the nla and tcs approaches , that comes with worst - case performance guarantees , and that performs well in practical data analysis applications .",
    "i should note that , prior to this algorithm , it was not immediately clear how to combine these two approaches .",
    "for example , if one looks at the details of the pivot rules in the deterministic nla methods , it is nt clear that keeping more columns will help at all in terms of reconstruction error .",
    "similarly , since there is a version of the `` coupon collecting '' problem at the heart of the usual tcs analysis , keeping fewer than @xmath91 will fail with respect to this worst - case analysis .",
    "moreover , the obvious hybrid algorithm of first randomly sampling @xmath41 columns and then using a deterministic qr procedure to select exactly @xmath16 of those columns does not seem to perform so well ( either in theory or in practice ) .",
    "consider the following more sophisticated version of a two - stage hybrid algorithm .",
    "given an arbitrary @xmath12 matrix @xmath13 and rank parameter @xmath16 :    * ( randomized phase ) let @xmath48 be _ any _ @xmath49 orthogonal matrix spanning the top-@xmath16 right singular subspace of @xmath13 . compute the importance sampling probabilities @xmath50 , where @xmath92 randomly select and rescale @xmath93 columns of @xmath48 according to these probabilities . *",
    "( deterministic phase ) let @xmath94 be the @xmath95 non - orthogonal matrix consisting of the down - sampled and rescaled columns of @xmath48 . run a deterministic qr algorithm on @xmath94 to select exactly @xmath16 columns of @xmath94 .",
    "return the corresponding columns of @xmath13 .    in particular , note that both the original choice of columns in the first phase , as well as the application of the qr algorithm in the second phase , involve the matrix @xmath48 , _ i.e. _ , the matrix defining the relevant non - uniformity structure over the columns of @xmath13 in the ( @xmath96)-relative - error algorithm  @xcite , rather than the matrix @xmath13 itself , as is more traditional . a more detailed description of this algorithm may be found in  @xcite , were it is shown that with extremely high probability the following spectral ) importance sampling probabilities , but this may be an artifact of the analysis . ] and frobenius norm bounds hold : @xmath97 ) , which are a generalization of the concept of _ statistical _ leverage described in the previous subsection , for its worst - case _ algorithmic _ performance guarantees .",
    "moreover , it is critical to the success of this algorithm that the qr procedure in the second phase be applied to the randomly - sampled version of @xmath48 , _",
    "i.e. _ , the matrix defining the worst - case nonuniformity structure in @xmath13 , rather than of @xmath13 itself",
    ".    with respect to running time , the computational bottleneck for this algorithm is computing @xmath50 , for which it suffices to compute _ any _ @xmath49 matrix @xmath48 that spans the top-@xmath16 right singular subspace of @xmath13 .",
    "( in particular , a full svd computation is _ not _ necessary . )",
    "thus , this running time is of the same order as the running time of the qr algorithm used in the second phase when applied to the original matrix @xmath13 .",
    "moreover , this algorithm easily scales up to matrices with thousands of rows and millions of columns , whereas existing off - the - shelf implementations of the traditional algorithm may fail to run at all . with respect to the worst - case quality of approximation bounds",
    ", this algorithm selects columns that are comparable to the state - of - the - art algorithms for constant @xmath16 ( _ i.e. _ , @xmath98 worse than previous work ) for the spectral norm and only a factor of at most @xmath99 worse than the best previously - known existential result for the frobenius norm .      in the applications we have considered  @xcite ,",
    "the goals of dna microarray and dna snp analysis include the reconstruction of untyped genotypes , the evaluation of tagging snp transferability between geographically - diverse populations , the classification and clustering into diseased and non - diseased states , and the analysis of admixed populations with non - trivial ancestry ; and the goals of selecting good columns more generally include diagnostic data analysis and unsupervised feature selection for classification and clustering problems . here",
    ", i will give a flavor of when and why and how the cssp algorithm of the previous subsection might be expected to perform well in these and other types of data applications .",
    "to gain intuition for the behavior of leverage scores in a typical application , consider figure  [ fig : leverage : zachary ] , which illustrates the so - called zachary karate club network  @xcite , a small but popular network in the community detection literature .",
    "given such a network @xmath9 , with @xmath15 nodes , @xmath14 edges , and corresponding edge weights @xmath100 , define the @xmath101 laplacian matrix as @xmath102 , where @xmath103 is the @xmath12 edge - incidence matrix and @xmath104 is the @xmath105 diagonal weight matrix .",
    "the effective resistance between two vertices is given by the diagonal entries of the matrix @xmath106 ( where @xmath107 denotes the moore - penrose generalized inverse ) and is related to notions of `` network betweenness ''  @xcite . for many large graphs , this and",
    "related betweenness measures tend to be strongly correlated with node degree and tend to be large for edges that form articulation points between clusters and communities , _",
    "i.e. _ , for edges that `` stick out '' a lot",
    ". it can be shown that the effective resistances of the edges of @xmath23 are proportional to the statistical leverage scores of the @xmath14 rows of the @xmath12 matrix @xmath108consider the @xmath105 matrix @xmath109 where @xmath110 , and note that if @xmath111 denotes any orthogonal matrix spanning the column space of @xmath112 , then @xmath113 figure  [ fig : leverage : zachary ] presents a color - coded illustration of these scores for zachary karate club network .    next , to gain intuition for the ( non-)uniformity properties of statistical leverage scores in a typical application , consider a term - document matrix derived from the publicly - released enron electronic mail collection  @xcite , which is an example of social or information network we will encounter again in the next section and which is also typical of the type of data set to which svd - based latent semantic analysis ( lsa ) methods",
    "@xcite have been applied .",
    "i constructed a @xmath114 matrix , as described in  @xcite , and i chose the rank parameter as @xmath115 .",
    "figure  [ fig : leverage : cumlev ] plots the cumulative leverage , _",
    "i.e. _ , the running sum of top @xmath116 statistical leverage scores , as a function of increasing @xmath116 . since @xmath117 , we see that the highest leverage term has a leverage score nearly two orders of magnitude larger than this `` average '' size scale , that the second highest - leverage score is only marginally less than the first , that the third highest score is marginally less than the second , etc .",
    "thus , by the traditional metrics of diagnostic data analysis  @xcite , which suggests flagging a data point if @xmath118 there are a _",
    "huge _ number of data points that are _ extremely _ outlying .",
    "in retrospect , of course , this might not be surprising since the enron email corpus is extremely sparse , with nowhere on the order of @xmath119 nonzeros per row .",
    "thus , even though lsa methods have been successfully applied , plausible generative models associated with these data are clearly not gaussian , and the sparsity structure is such that there is no reason to expect that nice phenomena such as measure concentration occur .",
    "finally , note that dna microarray and dna snp data often exhibit a similar degree of nonuniformity , although for somewhat different reasons .",
    "to illustrate , figure  [ fig : leverage : bio ] presents two plots .",
    "first , it plots the normalized statistical leverage scores for a data matrix , as was described in  @xcite , consisting of @xmath120 patients with @xmath17 different cancer types with respect to @xmath121 genes .",
    "a similar plot illustrating the remarkable nonuniformity in statistical leverage scores for dna snp data was presented in  @xcite .",
    "empirical evidence suggests that two phenomena may be responsible .",
    "first , as with the term - document data , there is no domain - specific reason to believe that nice properties like measure concentration occur  on the contrary , there are reasons to expect that they do not .",
    "recall that each dna snp corresponds to a single mutational event in human history .",
    "thus , it will `` stick out , '' as its description along its one axis in the vector space will likely not be well - expressed in terms of the other axes , _",
    "i.e. _ , in terms of the other snps , and by the time it `` works its way back '' due to population admixing , etc .",
    ", other snps will have occurred elsewhere .",
    "second , the correlation between statistical leverage and supervised mutual information - based metrics is particularly prominent in examples where the data cluster well in the low - dimensional space defined by the maximum variance axes .",
    "considering such data sets is , of course , a strong selection bias , but it is common in applications .",
    "it would be of interest to develop a model that quantifies the observation that , conditioned on clustering well in the low - dimensional space , an unsupervised measure like leverage scores should be expected to correlate well with a supervised measure like informativeness  @xcite or information gain  @xcite .    with respect to some of the more technical and implementational issues , several observations  @xcite",
    "shed light on the inner workings of the cssp algorithm and its usefulness in applications . recall that an important aspect of qr algorithms is how they make so - called pivot rule decisions about which columns to keep  @xcite and that such decisions can be tricky when the columns are not orthogonal or spread out in similarly nice ways .",
    "* we looked at several versions of the qr algorithm , and we compared each version of qr to the cssp using that version of qr in the second phase .",
    "one observation we made was that different qr algorithms behave differently_e.g .",
    "_ , some versions such as the low - rrqr algorithm of  @xcite tend to perform much better than other versions such as the qrxp algorithm of  @xcite .",
    "although not surprising to nla practitioners , this observation indicates that some care should be paid to using `` off the shelf '' implementations in large - scale applications .",
    "a second less - obvious observation is that preprocessing with the randomized first phase tends to improve more poorly - performing variants of qr more than better variants .",
    "part of this is simply that the more poorly - performing variants have more room to improve , but part of this is also that more sophisticated versions of qr tend to make more sophisticated pivot rule decisions , which are relatively less important after the randomized bias toward directions that are `` spread out . ''",
    "* we also looked at selecting columns by applying qr on @xmath48 and then keeping the corresponding columns of @xmath13 , _",
    "i.e. _ , just running the classical deterministic qr algorithm with no randomized first phase on the matrix @xmath48 .",
    "interestingly , with this `` preprocessing '' we tended to get better columns than if we ran qr on the original matrix @xmath13 .",
    "again , the interpretation seems to be that , since the norms of the columns of @xmath48 define the relevant nonuniformity structure with which to sample with respect to , working directly with those columns tends make things `` spread out , '' thereby avoiding ( even in traditional deterministic settings ) situations where pivot rules have problems .",
    "* of course , we also observed that randomization further improves the results , assuming that care is taken in choosing the rank parameter @xmath16 and the sampling parameter @xmath122 . in practice",
    ", the choice of @xmath16 should be viewed as a `` model selection '' question .",
    "then , by choosing @xmath123 , we often observed a `` sweet spot , '' in bias - variance sense , as a function of increasing @xmath122 .",
    "that is , for a fixed @xmath16 , the behavior of the deterministic qr algorithms improves by choosing somewhat more than @xmath16 columns , but that improvement is degraded by choosing too many columns in the randomized phase .",
    "i will conclude this section with two general observations raised by these theoretical and empirical results having to do with using the concept of statistical leverage to obtain columns from an input data matrix that are good both in worst - case analysis and also in large - scale data applications .",
    "one high - level question raised by these results is : why should statistical leverage , a traditional concept from regression diagnostics , be useful to obtain improved worst - case approximation algorithms for traditional nla matrix problems ?",
    "the answer to this seems to be that , intuitively , if a data point has a high leverage score and is not an error then it might be a particularly important or informative data point .",
    "since worst - case analysis takes the input matrix as given , each row is assumed to be reliable , and so worst - case guarantees are obtained by focusing effort on the most informative data points .",
    "it would be interesting to see if this perspective is applicable more generally in the design of matrix and graph algorithms .",
    "a second high - level question is : why are the statistical leverage scores so nonuniform in many large - scale data analysis applications . here",
    ", the answer seems to be that , intuitively , in many very large - scale applications , statistical models are _ implicitly _ assumed based on computational and not statistical considerations . in these cases , it is not surprising that some interesting data points `` stick out '' relative to obviously inappropriate models .",
    "this suggests the use of these importance sampling scores as cheap signatures of the `` inappropriateness '' of a statistical model ( chosen for algorithmic and not statistical reasons ) in large - scale exploratory or diagnostic applications .",
    "it would also be interesting to see if this perspective is applicable more generally .",
    "in this section , i will describe a novel perspective on identifying good clusters or communities in a large graph .",
    "the general problem of finding good clusters in ( or good partitions of ) a data graph has been studied extensively in a wide range of applications . for example , it has been studied for years in scientific computation ( where one is interested in load balancing in parallel computing applications ) , machine learning and computer vision ( where one is interested in segmenting images and clustering data ) , and theoretical computer science ( where one is interested in it as a primitive in divide - and - conquer algorithms ) .",
    "more recently , problems of this sort have arisen in the analysis of large social and information networks , where one is interested in finding communities that are meaningful in a domain - specific context .",
    "sponsored search is a type of contextual advertising where web site owners pay a fee , usually based on click - throughs or ad views , to have their web site search results shown in top placement position on search engine result pages .",
    "for example , when a user enters a term into a search box , the search engine typically presents not only so - called `` algorithmic results , '' but it also presents text - based advertisements that are important for revenue generation . in this context , one can construct a so - called _ advertiser - bidded - phrase graph _ , the simplest variant of which is a bipartite graph @xmath124 , in which @xmath82 consists of some discretization of the set of advertisers , @xmath10 consists of some discretization of the set of keywords that have been bid upon , and an edge @xmath125 is present if advertiser @xmath126 had bid on phrase @xmath127 .",
    "it is then of interest to perform data mining on this graph in order to optimize quantities such as the user click - through - rate or advertiser return - on - investment .",
    "numerous community - related problems arise in this context .",
    "for example , in _ micro - market identification _ , one is interested in identifying a set of nodes that is large enough that it is worth spending an analyst s time upon , as well as coherent enough that it can be usefully thought about as a `` market '' in an economic sense",
    ". such a cluster can be useful for a / b bucket testing , as well as for recommending to advertisers new queries or sub - markets .",
    "similarly , in _ advanced match _ , an advertiser places bids not only when an exact match occurs to a set of advertiser - specified phrases , but also when a match occurs to phrases `` similar to '' the specified bid phrases . ignoring numerous natural language and game theoretic issues , one can imagine that if the original phrase is in the middle of a fairly homogeneous concept class , then there may be a large number of similar phrases that are nearby in the graph topology , in which case it might be advantageous to both the advertiser and the search engine to include those phrases in the bidding .",
    "on the other hand , if the original phrase is located in a locally very unstructured part of the graph , then there may be a large number of phrases that are nearby in the graph topology but that have a wide range of meanings very different than the original bid phrase , in which case performing such an expansion might not make sense .    as in many other application areas , in these clustering and community identification applications , a common _ modus operandi _ in applying data analysis tools is :    * define an objective function that formalizes the intuition that one has as a data analyst as to what constitutes a good cluster or community .",
    "* since that objective function is almost invariably intractable to optimize exactly , apply some approximation algorithm or heuristic to the problem . * if the set of nodes that are thereby obtained look plausibly good in an application - dependent sense , then declare success .",
    "otherwise , modify the objective function or heuristic or algorithm , and iterate the process .",
    "such an approach can lead to insight_e.g .",
    "_ , if the data are `` morally '' low - dimensional , as might be the case , as illustrated in figure  [ fig : communities : toy - kmeans ] , when the singular value decomposition , manifold - based machine learning methods , or @xmath16-means - like statistical modeling assumptions are appropriate ; or if the data have other `` nice '' hierarchical properties that conform to one s intuition , as suggested by the schematic illustration in figure  [ fig : communities : ad - bid - schematic ] ; or if the data come from a nice generative model such as a mixture model",
    ". in these cases , a few steps of such a procedure will likely lead to a reasonable solution .    on the other hand ,",
    "if the size of the data is larger , or if the data arise from an application where the sparsity structure and noise properties are more adversarial and less intuitive , then such an approach can be problematic .",
    "for example , if a reasonable solution is not readily obtained , then it is typically not clear whether one s original intuition was wrong ; whether this is due to an improper formalization of the correct intuitive concept ; whether insufficient computational resources were devoted to the problem ; whether the sparsity and noise structure of the data have been modeled correctly , etc . that common visualization algorithms applied to large networks lead to largely non - interpretable figures , as illustrated in figure  [ fig : communities : real - network - vis ] , which reveal more about the inner workings of the visualization algorithm than the network being visualized , _ suggests _ that this more problematic situation is more typical of large social and information network data . in such cases , it would be of interest to have principled tools , the algorithmic and statistical properties of which are well - understood , to `` explore '' the data .",
    "[ cols=\"^,^,^ \" , ]      one of the most basic question that one can ask about a data set ( and one which is intimately related to questions of community identification ) is : what does the data set `` look like '' if it is cut into two pieces ?",
    "this is of interest since , _",
    "e.g. _ , one would expect very different properties from a data set that `` looked like '' a hot dog or a pancake , in that one could split it into two roughly equally - sized pieces , each of which was meaningfully coherent in and of itself ; as opposed to a data set that `` looked like '' a moderately - dense expander - like random graph , in that there did nt exist any good partitions of any size of the data into two pieces ; as opposed to a data set in which there existed good partitions , but those involved nibbling off just @xmath128 of the nodes of the data and leaving the rest intact .    a common way to formalize this question of qualitative connectivity",
    "is via the _ graph partitioning _",
    "problem  @xcite .",
    "graph partitioning refers to a family of objective functions and associated approximation algorithms that involve cutting or partitioning the nodes of a graph into two sets with the goal that the cut has good quality ( _ i.e. _ , not much edge weight crosses the cut ) as well as good balance ( _ i.e. _ , each of the two sets has a lot of the node weight ) .",
    "there are several standard formalizations of this bi - criterion . in this chapter ,",
    "i will be interested in the quotient cut formulations , balance and the @xmath129-balanced cut problem , with @xmath129 set to a fraction such as @xmath130 , which requires at least a @xmath131 balance . ] which require the small side to be large enough to `` pay for '' the edges in the cut .",
    "given an undirected , possibly weighted , graph @xmath9 , the _ expansion @xmath132 of a set of nodes @xmath133 _ is : @xmath134 where @xmath135 denotes the set of edges having one end in @xmath136 and one end in the complement @xmath137 , and where @xmath138 denotes cardinality ( or weight ) ; and the _ expansion of the graph @xmath23 _ is : @xmath139 alternatively , if there is substantial variability in node degree , then normalizing by a related quantity is of greater interest .",
    "the _ conductance @xmath140 of a set of nodes @xmath141 _ is : @xmath142 where @xmath143 , where @xmath13 is the adjacency matrix of a graph . in this case , the _ conductance of the graph @xmath23 _ is : @xmath144 in either case , one could replace the `` min '' in the denominator with a `` product , '' _",
    "e.g. _ , replace @xmath145 in the denominator of eqn .",
    "( [ eqn : conductance_set ] ) with @xmath146 .",
    "the product formulation provides a slightly greater reward to a cut for having a big big - side , and it so has a slightly weaker preference for balance than the minimum formulation .",
    "both formulations are equivalent , though , in that the objective function value of the set of nodes achieving the minimum with the one is within a factor of @xmath79 of the objective function value of the ( in general different ) set of nodes achieving the minimum with the other .",
    "generically , the minimum conductance cut problem refers to solving any of these formulations ; and importantly , all of these variants of the graph partitioning problem lead to intractable combinatorial optimization problems .    within scientific computing  @xcite , and more recently within statistics and machine learning  @xcite , a great deal of work",
    "has focused on the conductance ( or normalized cut ) versions of this graph partitioning problem .",
    "several general observations about the approach adopted in these literatures include :    * the focus is on _ spectral approximation algorithms_. spectral algorithms use an exact or approximate eigenvector of the graph s laplacian matrix to find a cut that achieves a `` quadratic approximation , '' in the sense that the cut returned by the algorithm has conductance value no bigger than @xmath147 if the graph actually contains a cut with conductance @xmath148  @xcite . *",
    "the focus is on _ low - dimensional graphs _ , _",
    "e.g. _ , bounded - degree planar graphs and finite element meshes ( for which quality - of - approximation bounds are known that depend on just the number of nodes and not on structural parameters such as the conductance value ) .",
    "moreover , there is an acknowledgement that _ spectral methods are inappropriate for expander graphs _ that have constant expansion or conductance ( basically , since for these graphs any clusters returned are not meaningful in applications ) .",
    "* since the algorithm is typically applied to find partitions of a graph that are useful in some downstream application , there is a _",
    "strong interest in the actual pieces returned by the algorithm_. relatedly , there is interest in the robustness or consistency properties of spectral approximation algorithms under assumptions on the data .",
    "recursive bisection heuristics ",
    "recursively divide the graph into two groups , and then further subdivide the new groups until the desired number of clusters groups is achieved  are also common here .",
    "they may be combined with local improvement methods  @xcite , which when combined with multi - resolution ideas leads to programs such as metis  @xcite , cluto  @xcite , and graclus  @xcite .    within tcs ,",
    "a great deal of work has also focused on this graph partitioning problem  @xcite .",
    "several general observations about the traditional tcs approach include :    * the focus is on _ flow - based approximation algorithms_. these algorithms use multi - commodity flow and metric embedding ideas to find a cut whose conductance is within an @xmath149 factor of optimal  @xcite , in the sense that the cut returned by the algorithm has conductance no bigger than @xmath149 , where @xmath15 is the number of nodes in the graph , times the conductance value of the optimal conductance set in the graph . *",
    "the focus is on worst - case analysis of _ arbitrary graphs_. although flow - based methods achieve their worst - case @xmath149 bounds on expanders , it is observed that _",
    "spectral methods are appropriate for expander graphs _",
    "( basically , since the quadratic of a constant is a constant , which implies a worst - case constant - factor approximation ) .",
    "* since the algorithmic task is simply to approximate the value of the objective function of eqn .",
    "( [ eqn : expansion_graph ] ) or eqn .",
    "( [ eqn : conductance_graph ] ) , _ there is no particular interest in the actual pieces returned by the algorithm _ , except insofar as those pieces have objective function value that is close to the optimum .",
    "most of the tcs work is on the expansion versions of the graph partitioning problem , but it is noted that most of the results obtained `` go through '' to the conductance versions by considering appropriately - weighted functions on the nodes .    there has also been recent work within tcs that has been motivated by achieving improved worst - case bounds and/or being appropriate in data analysis applications where very large graphs , say with millions or billions or nodes , arise .",
    "these methods include :    * _ local spectral methods_. these methods  @xcite take as input a seed node and a locality parameter and return as output a `` good '' cluster `` nearby '' the seed node .",
    "moreover , they often have computational cost that is proportional to the size of the piece returned , and they have roughly the same kind of quadratic approximation guarantees as the global spectral method . * _ cut - improvement algorithms_. these methods  @xcite , _ e.g. _ , mqi  @xcite , take as input a graph and an initial cut and use spectral or flow - based methods to return as output a `` good '' cut that is `` within '' or `` nearby '' the original cut . as such , they , can be combined with a good method ( say , from spectral or metis ) for initially splitting the graph into two pieces to obtain a heuristic method for finding low conductance cuts in the whole graph  @xcite .",
    "* _ combining spectral and flow_. these methods can be viewed as combinations of spectral and flow - based techniques which exploit the complementary strengths of these two classes of techniques .",
    "they include an algorithm that used semidefinite programming to find a solution that is within a multiplicative factor of @xmath150 of optimal  @xcite , as well as several related algorithms  @xcite that are more amenable to medium- and large - scale implementation .",
    "most work on community detection in large networks begins with an observation such as : `` it is a matter of common experience that communities exist in networks .",
    "although not precisely defined , communities are usually thought of as sets of nodes with better connections amongst its members than with the rest of the world . ''",
    "( i have not provided a reference for this quotation since variants of it could have been drawn from any one of scores or hundreds of papers on the topic .",
    "most of this work then typically goes on to apply the _ modus operandi _ described previously . )",
    "although far from perfect , conductance is probably the combinatorial quantity that most closely captures this intuitive bi - criterial notion of what it means for a set of nodes to be a good community .",
    "even more important from the perspective of this chapter , the use of conductance as an objective function has the following benefit . although exactly solving the combinatorial problem of eqn .",
    "( [ eqn : expansion_graph ] ) or eqn .",
    "( [ eqn : conductance_graph ] ) is intractable , there exists a wide range of heuristics and approximation algorithms , the respective strengths and weaknesses of which are well - understood in theory and/or in practice , for approximately optimizing conductance .",
    "in particular , recall that spectral methods and multi - commodity flow - based methods are complementary in that :    * the worst - case @xmath149 approximation factor is obtained for flow - based methods on expander graphs  @xcite , a class of graphs which does not cause problems for spectral methods ( to the extent than any methods are appropriate ) . *",
    "the worst - case quadratic approximation factor is obtained for spectral methods on graphs with `` long stringy '' pieces  @xcite , basically since spectral methods can confuse `` long path '' with `` deep cuts , '' a difference that does not cause problems for flow - based methods .",
    "* both methods perform well on `` low - dimensional graphs''_e.g .",
    "_ , road networks , discretizations of low - dimensional spaces as might be encountered in scientific modeling , graphs that are easily - visualizable in two dimensions , and other graphs that have good well - balanced cuts  although the biases described in the previous two bullets still manifest themselves .",
    "i should note that empirical evidence  @xcite clearly demonstrates that large social and information networks have all of these properties  they are expander - like when viewed at large size scales ; their sparsity and noise properties are such that they have structures analogous to stringy pieces that are cut off or regularized away by spectral methods ; and they often have structural regions that at least locally are meaningfully low - dimensional .",
    "all of this suggests a rather novel approach  that of using approximation algorithms for the intractable graph partitioning problem ( or other intractable problems ) as `` experimental probes '' of the structure of large social and information networks . by this ,",
    "i mean using these algorithms to `` cut up '' or `` tear apart '' a large network in order to provide insights into its structure .",
    "from this perspective , although we are defining an edge - counting metric and then using it to perform an optimization , we are not particularly interested _ per se _ in the clusters that are output by the algorithms . instead , we are interested in what these clusters , coupled with knowledge of the inner workings of the approximation algorithms , tell us about network structure .",
    "relatedly , given the noise properties and sparsity structure of large networks , we are not particularly interested _ per se _ in the solution to the combinatorial problem .",
    "for example , if we were provided with an oracle that returned the solution to the intractable combinatorial problem , it would be next to useless to us . instead",
    ", much more information is revealed by looking at ensembles of output clusters in light of the artifactual properties of the heuristics and approximation algorithms employed .",
    "that is , from this perspective , we are interested in the _ statistical properties implicit in worst - case approximation algorithms _ and what these properties tell us about the data .    the analogy between approximation algorithms for the intractable graph partitioning problem and experimental probes is meant to be taken more seriously rather than less .",
    "e.g. _ , that it is a non - trivial exercise to determine what a protein or dna molecule `` looks like''after all , such a molecule is very small , it ca nt easily be visualized with traditional methods , it has complicated dynamical properties , etc . to determine the structure of a protein",
    ", an experimentalist puts the protein in solution or in a crystal and then probes it with x - ray crystallography or a nuclear magnetic resonance signal . in more detail ,",
    "one sends in a signal that scatters off the protein ; one measures a large quantity of scattering output that comes out the other side ; and then , using what is measured as well as knowledge of the physics of the input signal , one reconstructs the structure of the hard - to - visualize protein . in an analogous manner , we can analyze the structure of a large social or information by tearing it apart in numerous ways with one of several local or global graph partitioning algorithms ; measuring a large number of pieces output by these procedures ; and then , using what was measured as well as knowledge of the artifactual properties of the heuristics and approximation algorithms employed , reconstructing structural properties of the hard - to - visualize network .    by adopting this approach",
    ", one can hope to use approximation algorithms to test commonly - made data analysis assumptions , _",
    "e.g. _ , that the data meaningfully lie on a manifold or some other low - dimensional space , or that the data have structures that are meaningfully - interpretable as communities .      as a proof - of - principle application of this approach ,",
    "let me return to the question of characterizing the small - scale and large - scale community structure in large social and information networks .",
    "given the conductance ( or some other ) community quality score , define the _ network community profile _",
    "( ncp ) as the conductance value , as a function of size @xmath16 , of the minimum conductance set of cardinality @xmath16 in the network : @xmath151 intuitively , just as the `` surface - area - to - volume '' aspect of conductance captures the `` gestalt '' notion of a good cluster or community , the ncp measures the score of `` best '' community as a function of community size in a network .",
    "see figure  [ fig : communities : toy - fig2 ] for an illustration of small network and figure  [ fig : communities : toy - fig2-ncp ] for the corresponding ncp .",
    "operationally , since the ncp is np - hard to compute exactly , one can use approximation algorithms for solving the minimum conductance cut problem in order to compute different approximations to it . although we have experimented with a wide range of procedures on smaller graphs , in order to scale to very large social and information networks , we used :    * _ metis+mqi_this flow - based method consists of using the popular graph partitioning package metis  @xcite followed by a flow - based mqi cut - improvement post - processing step  @xcite , and it provides a surprisingly strong heuristic method for finding good conductance cuts ; * _ local spectral_although this spectral method  @xcite is worse than metis+mqi at finding very good conductance pieces , the pieces it finds tend to be `` tighter '' and more `` community - like '' ;  and * _ bag - of - whiskers_this is a simple heuristic to paste together small barely - connected sets of nodes that exert a surprisingly large influence on the ncp of large informatics graphs ;    and we compared and contrasted the outputs of these different procedures .",
    "the ncp behaves in a characteristic downward - sloping manner  @xcite for graphs that are well - embeddable into an underlying low - dimensional geometric structure , _",
    "e.g. _ , low - dimensional lattices , road networks , random geometric graphs , and data sets that are well - approximatable by low - dimensional spaces and non - linear manifolds .",
    "e.g. _ , figure  [ fig : communities : toy - fig2-ncp ] . )",
    "relatedly , a downward sloping ncp is also observed for small commonly - studied networks that have been widely - used as testbeds for community identification algorithms , while the ncp is roughly flat at all size scales for well - connected expander - like graphs such as moderately - dense random graphs .",
    "perhaps surprisingly , common generative models , including preferential attachment models , as well as `` copying '' models and hierarchical models specifically designed to reproduce community structure , also have flat or downward - sloping ncps .",
    "thus , when viewed from the perspective of @xmath152 meters , all of these graphs `` look like '' either a hot dog ( in the sense that one can split the graph into two large and meaningful pieces ) or a moderately - dense expander - like random graph ( in the sense that there are no good partitions of any size in the graph ) .",
    "this fact is used to great advantage by common machine learning and data analysis tools .    from the same perspective of @xmath152 meters ,",
    "what large social and information networks `` look like '' is very different . in figure",
    "[ fig : communities : realncp - livejournal ] and figure  [ fig : communities : realncp - epinions ] , i present the ncp computed several different ways for two representative large social and information networks .",
    "several things are worth observing : first , up to a size scale of roughly @xmath153 nodes , the ncp goes down ; second , it then achieves its global minimum ; third , above that size scale , the ncp gradually increases , indicating that the community - quality of possible communities gets gradually worse and worse as larger and larger purported communities are considered ; and finally , even at the largest size scales there is substantial residual structure above and beyond what is present in a corresponding random graph .",
    "thus , good network communities , at least as traditionally conceptualized , tend to exist only up to a size scale of roughly @xmath153 nodes , while at larger size scales network communities become less and less community - like .",
    "the explanation for this phenomenon is that large social and information networks have a _ nested core - periphery structure _ , in which the network consists of a large moderately - well connected core and a large number of very well - connected communities barely connected to the core , as illustrated in figure  [ fig : communities : core - periphery ] .",
    "if we define a notion of a _ whisker _ to be maximal sub - graph detached from network by removing a single edge and the associated notion of the _ core _ to be the rest of the graph , _",
    "i.e. _ , the 2-edge - connected core , then whiskers contain ( on average ) @xmath154 of nodes and @xmath155 of edges .",
    "moreover , in nearly every network , the global minimum of the ncp is a whisker .",
    "importantly , though , even if _ all _ the whiskers are removed , then the core itself has a downward - then - upward - sloping ncp , indicating that the core itself has a nested core - periphery structure .",
    "in addition to being of interest in community identification applications , this fact explains the inapplicability of many common machine learning and data analysis tools for large informatics graphs .",
    "for example , it implies that recursive - bisection heuristics are largely inappropriate for this class of graphs ( basically since the recursion depth will be very large ) .",
    "there are numerous ways one can be confident in these domain - specific conclusions .",
    "most relevant for the interplay between the algorithmic and statistical perspectives on data that i am discussing in this chapter are :    * _ modeling considerations_. several lines of evidence point to the role that sparsity and noise have in determining the large - scale clustering structure of large networks . * * extremely sparse erds - rnyi random graphs with connection probability parameter roughly @xmath156_i.e .",
    "_ , in the parameter regime where measure fails to concentrate sufficiently to have a fully - connected graph or to have observed node degrees close to expected node degrees  provide the simplest mechanism to reproduce the qualitative property of having very - imbalanced deep cuts and no well - balanced deep cuts .",
    "* * so - called power law random graphs  @xcite with power law parameter roughly @xmath157which , due to exogenously - specified node degree variability , have an analogous failure of measure concentration  also exhibit this same qualitative property . * * a model in which new edges are added randomly but with an iterative `` forest fire '' burning mechanism provides a mechanism to reproduce the qualitative property of a relatively _ gradually - increasing _ ncp . *",
    "_ algorithmic - statistical considerations_. ensembles of clusters returned by different approximation algorithms , _",
    "e.g. _ , local spectral versus the flow - based methods such as metis+mqi versus the bag - of - whiskers heuristic , have very different properties , in a manner consistent with known artifactual properties of how those algorithms operate . *",
    "* for example , metis+mqi finds sets of nodes that have very good conductance scores  at very small size scales , these are similar to the pieces from local spectral and these sets of nodes could plausibly be interpreted as good communities ; but at size scales larger than roughly @xmath153 nodes , these are often tenuously - connected ( and in some cases unions of disconnected ) pieces , for which such an interpretation is tenuous at best . * * similarly , the ncp of a variant of flow - based partitioning that permits disconnected pieces to be output mirrors the ncp from the bag - of - whiskers heuristic that combines disconnected whiskers , while the ncp of a variant of flow - based partitioning that requires connected pieces more closely follows that of local spectral , at least until much larger size scales where flow has problems since the graph becomes more expander - like . * * finally , local spectral finds sets of nodes with good ( but not as good as metis+mqi ) conductance value that that are more `` compact '' and thus more plausibly community - like than those pieces returned by metis+mqi .",
    "for example , since spectral methods confuse long paths with deep cuts , empirically one obtains sets of nodes that have worse conductance scores but are internally more coherent in that , _",
    "e.g. _ , they have substantially smaller diameter and average diameter .",
    "this is illustrated in figures  [ fig : communities : spectral - cmty-1 ] through  [ fig : communities : flow - cmty-2 ] : figures  [ fig : communities : spectral - cmty-1 ] and  [ fig : communities : spectral - cmty-2 ] show two ca .",
    "@xmath158-node communities obtained from local spectral , which are thus more internally coherent than the two ca .",
    "@xmath158-node communities illustrated in figures  [ fig : communities : flow - cmty-1 ] and  [ fig : communities : flow - cmty-2 ] which were obtained from metis+mqi and which thus consist of two tenuously - connected smaller pieces .",
    "i will conclude this section with a few general thoughts raised by these empirical results .",
    "note that the modeling considerations suggest , and the algorithmic - statistical considerations verify , that statistical issues , and in particular _ regularization issues _",
    ", are particularly important for extracting domain - specific understanding from approximation algorithms applied this class of data .    by regularization , of course , i mean the traditional notion that the computed quantities  clusters and communities in this case  are empirically `` nicer , '' or more `` smooth '' or `` regular , '' in some useful domain - specific sense  @xcite .",
    "recall that regularization grew out of the desire to find meaningful solutions to ill - posed problems in integral equation theory , and it may be viewed as adding a small amount of bias to reduce substantially the variance or variability of a computed quantity .",
    "it is typically implemented by adding a `` loss term '' to the objective function being optimized .",
    "although this manner of implementation leads to a natural interpretation of regularization as a trade - off between optimizing the objective and avoiding over - fitting the data , it typically leads to optimization problems that are harder ( think of @xmath159-regularized @xmath160-regression ) or at least no easier ( think of @xmath160-regularized @xmath160-regression ) than the original problem , a situation that is clearly unacceptable in the very large - scale applications i have been discussing .",
    "interestingly , though , at least for very large and extremely sparse social and information networks , intuitive notions of cluster quality tend to fail as one aggressively optimizes the community - quality score  @xcite . for instance , by aggressively optimizing conductance with metis+mqi , one obtains disconnected or barely - connected clusters that do not correspond to intuitive communities .",
    "this suggests the rather interesting notion of implementing _ implicit regularization via approximate computation_approximate optimization of the community score in - and - of - itself introduces a systematic bias in the properties of the extracted clusters , when compared with much more aggressive optimization or computation of the intractable combinatorial optimum .",
    "of course , such a bias may in fact be preferred since , as in case of local spectral , the resulting `` regularized communities '' are more compact and correspond to more closely to intuitive communities . explicitly incorporating the tradeoff between the conductance of the bounding cut of the cluster and the internal cluster compactness into a new modified objective function",
    "would not have led to a more tractable problem ; but implicitly incorporating it in the approximation algorithm had both algorithmic and statistical benefits .",
    "it is clearly of interest to formalize this approach more generally .",
    "as noted above , the algorithmic and statistical perspectives on data and what might be interesting questions to ask of the data are quite different , but they are not incompatible . some of the more prominent recent examples of this include :    * statistical , randomized , and probabilistic ideas are central to much of the recent work on developing improved worst - case approximation algorithms for matrix problems .",
    "* otherwise intractable optimization problems on graphs and networks yield to approximation algorithms when assumptions are made about the network participants .",
    "* much recent work in machine learning draws on ideas from both areas ; similarly , in combinatorial scientific computing , implicit knowledge about the system being studied often leads to good heuristics for combinatorial problems . * in boosting , a statistical technique that fits an additive model by minimizing an objective function with a method such as gradient descent , the computation parameter , _",
    "i.e. _ , the number of iterations , also serves as a regularization parameter .    in this chapter ,",
    "i have focused on two examples that illustrate this point in a somewhat different way .",
    "for the first example , we have seen that using a concept fundamental to statistical and diagnostic regression analysis was crucial in the development of improved algorithms that come with worst - case performance guarantees and that , for related reasons , also perform well in practice . for the second example",
    ", we have seen that an improved understanding of worst - case algorithms ( _ e.g. _ , when they perform well , when they perform poorly , and what regularization is implicitly implemented by them ) suggested methods for using those approximation algorithms as `` experimental probes '' of large informatics graphs and for better inference and prediction on those graphs .",
    "to conclude , i should re - emphasize that in neither of these applications did we _",
    "first _ perform statistical modeling , independent of algorithmic considerations , and _ then _ apply a computational procedure as a black box .",
    "this approach of more closely coupling the computational procedures used with statistical modeling or statistical understanding of the data seems particularly appropriate more generally for very large - scale data analysis problems , where design decisions are often made based on computational constraints but where it is of interest to understand the implicit statistical consequences of those decisions .",
    "i would like to thank schloss dagstuhl and the organizers of dagstuhl seminar 09061 for an enjoyable and fruitful meeting ; peristera paschou , petros drineas , and christos boutsidis for fruitful discussions on dna snps , population genetics , and matrix algorithms ; jure leskovec and kevin lang for fruitful discussions on social and information networks and graph algorithms ; and my co - organizers of as well as numerous participants of the mmds meetings , at which many of the ideas described here were formed .",
    "r.  andersen , f.r.k .",
    "chung , and k.  lang .",
    "local graph partitioning using pagerank vectors . in _",
    "focs 06 : proceedings of the 47th annual ieee symposium on foundations of computer science _ , pages 475486 , 2006 .",
    "s.  arora , e.  hazan , and s.  kale .",
    "@xmath161 approximation to sparsest cut in @xmath162 time . in _",
    "focs 04 : proceedings of the 45th annual symposium on foundations of computer science _ , pages 238247 , 2004 .                          c.  boutsidis , m.w .",
    "mahoney , and p.  drineas .",
    "unsupervised feature selection for the @xmath16-means clustering problem . in _ annual advances in neural information processing systems 22 : proceedings of the 2009 conference _ , 2009 .",
    "aide - memoire .",
    "high - dimensional data analysis : the curses and blessings of dimensionality , 2000 . `",
    "http://www-stat.stanford.edu/  donoho / lectures / ams2000/curses.pdf ` accessed december , 17 2008 .",
    "p.  drineas , m.w .",
    "mahoney , and s.  muthukrishnan .",
    "sampling algorithms for @xmath160 regression and applications . in _ proceedings of the 17th annual acm - siam symposium on discrete algorithms _ , pages 11271136 , 2006 .",
    "d.  lambert . what use is statistics for massive data ? in j.",
    "e. kolassa and d.  oakes , editors , _ crossing boundaries : statistical essays in honor of jack hall _ , pages 217228 .",
    "institute of mathematical statistics , 2003 .",
    "k.  lang and s.  rao . a flow - based method for improving the expansion or conductance of graph cuts . in _",
    "ipco 04 : proceedings of the 10th international ipco conference on integer programming and combinatorial optimization _ , pages 325337 , 2004 .",
    "t.  leighton and s.  rao .",
    "an approximate max - flow min - cut theorem for uniform multicommodity flow problems with applications to approximation algorithms . in _",
    "focs 88 : proceedings of the 28th annual symposium on foundations of computer science _ , pages 422431 , 1988 .",
    "j.  leskovec , k.j .",
    "lang , a.  dasgupta , and m.w",
    ". mahoney .",
    "statistical properties of community structure in large social and information networks . in _ www 08 : proceedings of the 17th international conference on world wide web _ , pages 695704 , 2008 .",
    "j.  leskovec , k.j .",
    "lang , and m.w .",
    "empirical comparison of algorithms for network community detection . in _ www 10 : proceedings of the 19th international conference on world wide web _ , pages 631640 , 2010 .",
    "jordan , and y.  weiss .",
    "on spectral clustering : analysis and an algorithm . in",
    "_ nips 01 : proceedings of the 15th annual conference on advances in neural information processing systems _ , 2001 .",
    "l.  orecchia , l.  schulman , u.v .",
    "vazirani , and n.k .",
    "vishnoi . on partitioning graphs via single commodity flows . in _ proceedings of the 40th annual acm symposium on theory of computing _ , pages 461470 , 2008 .",
    "a.  pothen .",
    "graph partitioning algorithms with applications to scientific computing . in d.",
    "e. keyes , a.  h. sameh , and v.  venkatakrishnan , editors , _ parallel numerical algorithms_. kluwer academic press , 1996 .",
    "l.  k. saul , k.  q. weinberger , j.  h. ham , f.  sha , and d.  d. lee .",
    "spectral methods for dimensionality reduction . in o.",
    "chapelle , b.  schoelkopf , and a.  zien , editors , _ semisupervised learning _ , pages 293308 .",
    "mit press , 2006 .",
    "spielman and s .- h . teng .",
    "spectral partitioning works : planar graphs and finite element meshes . in _",
    "focs 96 : proceedings of the 37th annual ieee symposium on foundations of computer science _ , pages 96107 , 1996 .",
    "spielman and s .- h .",
    "nearly - linear time algorithms for graph partitioning , graph sparsification , and solving linear systems . in _",
    "stoc 04 : proceedings of the 36th annual acm symposium on theory of computing _ ,",
    "pages 8190 , 2004 ."
  ],
  "abstract_text": [
    "<S> in recent years , ideas from statistics and scientific computing have begun to interact in increasingly sophisticated and fruitful ways with ideas from computer science and the theory of algorithms to aid in the development of improved worst - case algorithms that are useful for large - scale scientific and internet data analysis problems . in this chapter , </S>",
    "<S> i will describe two recent examples  one having to do with selecting good columns or features from a ( dna single nucleotide polymorphism ) data matrix , and the other having to do with selecting good clusters or communities from a data graph ( representing a social or information network)that drew on ideas from both areas and that may serve as a model for exploiting complementary algorithmic and statistical perspectives in order to solve applied large - scale data analysis problems . </S>"
  ]
}