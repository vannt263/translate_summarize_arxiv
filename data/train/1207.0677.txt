{
  "article_text": [
    "diffusion weighted magnetic resonance imaging ( dw - mri ) is able to quantify the anisotropic diffusion of water molecules in biological tissues such as the human brain white matter .",
    "the great success of dw - mri comes from its capability to accurately describe the geometry of the underlying microstructure .",
    "dw - mri captures the average diffusion of water molecules , which probes the structure of the biological tissue at scales much smaller than the imaging resolution  @xcite .",
    "new dmri techniques for high angular resolution diffusion imaging ( hardi ) are now able to recover one or more directions of fiber populations at each imaging voxel and thus , overcome some of the limitations of diffusion tensor imaging ( dti ) in regions of complex fiber configurations where fibers cross , branch and kiss .",
    "most current classification techniques are based on dti measures of westin et al  @xcite .",
    "for example , a recent paper from  @xcite classifies the white matter voxels into single and crossing fibers simply by hard thresholding the linear , planar and spherical measures computed from the eigen - values of the diffusion tensor .",
    "other techniques have been developed to better handle fiber crossings using apparent diffusion coefficient  @xcite modeling from hardi and other hardi models representations based on spherical harmonics ( sh ) decomposition  @xcite .",
    "it was previously suggested using automatic bayesian relevance determination  @xcite that between 1/3 and 2/3 of wm voxels contained crossings .",
    "jeurissen et al  @xcite have just showed that this number is an underestimation and that wm crossings can take up to 90% of wm voxels using maxima extracted from a robust sh - based fiber orientation distribution estimation using constrained spherical deconvolution .",
    "otherwise , the first attempts to classify hardi voxels using machine learning techniques was done in  @xcite using the space of sh coefficients from q - ball imaging .",
    "these techniques are based on diffusion maps and spectral clustering but were never applied to neurodegenerative datasets .",
    "more recently , schnell et al .  @xcite have designed a classification process based on support vector machines , also based on the sh representation of the dmri signal and have compared their results with the classical classification of westin s measures , as done in  @xcite .",
    "the main contribution of this paper is the creation of a system allowing to automatically classify voxels of hardi data in order to segment the white matter among different classes .",
    "the proposed method goes deeper than the approach presented in  @xcite because it takes into account the neighbourhood of the voxels by using convolved data .",
    "using the spherical harmonic ( sh ) representation of each voxel  @xcite , we want to be able to classify it as : ( a ) white matter with a single fiber bundle ( wmsf ) , ( b ) white matter with crossing fiber bundles ( wmcf ) , ( c ) non - white matter ( n - wm ) of type gray matter ( gm ) or cerebrospinal fluid ( csf ) .",
    "section  [ sec_apparatus ] presents the dataset .",
    "we use a support vector machine ( svm )  @xcite classifier to obtain the label of a voxel .",
    "the svm tries to find the best separating hyperplane between two classes .",
    "as we work with several classes , a one - against - one approach is used ( we train @xmath0 classifiers ) .",
    "we easily see that using only the sh information may be not enough accurate , as there is no knowledge of the neighborhood while classifying the voxel . to address this problem ,",
    "we operate a 2d convolution of each slice of the brain against a kernel .",
    "this allows to work with a voxel representation resulting of a weighted sum of the neighboring voxels .",
    "the convolution kernels are chosen in order to obtain the best accuracy ( see section  [ sec_selection ] ) .",
    "say we have @xmath1 the selected feature space of cardinality @xmath2 ( _ i.e. _ , each voxel is represented by a vector of size @xmath2 ; @xmath2 depends on the sh order ) and @xmath3 , the convolution kernels with one convolution kernel per dimension of the features space .",
    "we apply the convolution kernel @xmath4 on each feature @xmath5 of each voxel of each slice @xmath6 : @xmath7 , \\forall j \\in [ 1,n ] % \\end{array}\\ ] ] with @xmath8 $ ] the feature @xmath5 of the voxel located at position @xmath9 of the slice @xmath6 , @xmath10 the slice @xmath6 after convolution of all voxels and features and @xmath11 the number of slices .",
    "now , we consider all the voxels as being in a whole set @xmath12 of couples @xmath13 ( @xmath14 is the feature vector and @xmath15 its label ) whatever their slice and localisation in the slice : @xmath16 , label^j[x , y]\\}\\ ] ] with @xmath17 $ ] the label of the voxel at ( @xmath18 ) in slice @xmath6 ) .    we apply a 6 fold stratified cross validation ( we obtain 6 subsets , where each label is represented at the same ratio in each subset : there is the same recognition difficulty in each subset ) .",
    "each subset serves as a testing set , while the other subsets serve as training set ( the following procedure is then applied 6 times ) .",
    "the mean ( @xmath19 ) and the standard deviation ( @xmath20 ) of the feature vectors of the training set is computed in order to apply a zscore normalisation of the training and testing sets ( @xmath21 ) .",
    "the normalised training samples serve to train a svm with a gaussian kernel . to quickly evaluate the couple ( @xmath22 , @xmath1 ) we do not try to search the best svm parameters .",
    "so @xmath23 and @xmath24 are at the default value of libsvm ( @xmath25 ) .",
    "the learned svm is used to predict the label of the testing samples .",
    "there is a large disparity in the number of individuals of each class .",
    "so , we do not want to simply compute an averaged classification error rate .",
    "instead we compute the following errors : ( i ) _ missed wm ratio ( mwmr ) _ : ratio of wmsf and wmcf voxels recognized as being n - wm ( csf or gm ) voxels ; ( ii ) _ exchanged wm ratio ( ewmr ) _ : ratio of wmsf voxels recognized as being wmcf voxels and wmcf voxels recognized as wmsf voxels ; ( iii ) _ imagined wm ratio ( iwmr ) _ : ratio of n - wm ( csf and gm ) voxels recognised as being wmsf or wmcf voxels .",
    "then the final error score is computed as following : @xmath26 with @xmath27 , @xmath28 and @xmath29 in this experiment .",
    "this way , we give a low weight on the ewmr and bigger weights on iwmr and mwmr .",
    "although we try to differenciate them , we do not care of exchanging csf and gm .      as , for each feature set",
    ", the search space is very huge , we use a genetic algorithm in order to search the best convolution kernels .",
    "each kernel is an array of size @xmath30 , with @xmath31 the width of the kernel .",
    "the genome consists of the @xmath2 kernels stored in a 1d array using values in the interval @xmath32 $ ] ( it means that values inside the kernel will be in this interval ) , so it has a size of @xmath33 .",
    "we do not pay attention to have a kernel summing to @xmath34 as we do not care if the data obtained after convolution is not in the same metric as the original data ( we do not manipulate or visualize the convolved data , it only serves for classification ) .",
    "population contains 500 individuals .",
    "the initial population contains :    * one individual build with gaussian kernels which are supposed to be good by doing a weighted average based on the distance around the voxel of interested * 250 individuals created with modification of the first individual ( mean of the gaussian kernels based individual and a random one * 249 individuals generated totally randomly .",
    "the procedure stops after 100 generations or 3 days of computing .",
    "the cross - over rate is set at 0.9 and use 2 points .",
    "the mutation rate is set at 0.1 ( a higher mutation did not implyied a faster convergence nor better results ) and apply gaussian mutations .",
    "the number of elites is set at 20 .",
    "the fitness value of a chromosome is the @xmath35 explained in  ( [ eq_fitness ] ) and the genetic algorithm wants to reduce this rate .",
    "ground truth datasets and validation is one of the biggest challenge of the dmri community .",
    "hence , there is an important effort to build _ ex - vivo _ phantoms that produce realistic datasets , more realistic than simulated synthetic data .",
    "this is the case of the fibercup data , mimicking a coronal slice of the brain .",
    "it is a simple 3d dmri dataset , but is quite unique because it reproduces complex fiber crossing configurations , similar to configurations in the cemtrum semioval and u fibers of the brain .",
    "the underlying ground truth is known and thus will serve as our learning / testing dataset . for this paper , we have focused on the @xmath36 mm isotropic , 64 directions , @xmath37 s / mm@xmath38 dataset  @xcite .",
    "the phantom and ground truth fibers are illustrated in fig .",
    "[ fig : fibercup ] .",
    "thus , we use a good compromise between synthetic data as in  @xcite and _ in vivo _ data for which it almost impossible to have ground truth .",
    "phantom border represents gm , fibers in one direction ( resp .",
    "several directions ) is wmsf ( resp .",
    "wmcf ) , the rest is csf .",
    "phantom dataset with ground truth fibers .",
    ", width=124,height=124 ]    from this hardi dataset , we provide one input among the following choices to the svm : i)-ii ) a sh order 4 and 8 representation of raw signal ( sh4 , sh8 respectively )  @xcite , or iii ) the eigenvalues of the diffusion tensor  @xcite ( eig ) .",
    "results of the proposed method are compared to a baseline classifier .",
    "this classifier does not use convolution operations , but it works with far more information .",
    "each voxel is classified by several svm classifier using a different feature space ( sh4 , sh4 rotation insensitive  @xcite , sh8 , sh8 rotation insensitive , eigenvalues sh4 after deconvolution , odf4 , odf8 ) .",
    "each classifier is trained using the best parameters ( @xmath24,@xmath23 ) by getting them through a grid search and 10 fold cross validation .",
    "a final svm classifier operates a fusion ( in order to obtain better results than a single svm ) of the results of the previous classifier in order to give the label of the voxel .",
    "note that this svm fusion gives better results than a majority vote and that each individual classifier .",
    ".recognition performance ( fitness value / global classification error rate ) , using the best convolution kernels , and for the baseline classifiers [ cols= \" < , < , < , < \" , ]      using a computer having 4 gb of ram and a processor of 4 cores , scripts written in python and consuming tasks compiled using cython , it takes around 24 hours to run the evolution procedure for most couples of feature / kernel width ( we need to evaluate @xmath39 classification tasks on a high quantity of voxels for each couple of selected feature and kernel width ) . as the feature space is smaller , results or obtained quickly for sh4 data than for sh8 .",
    "the evolution procedure was not able to find a better individual than the standard kernel matrices for some configurations .",
    "this can be because of the size of the search space which is too big in comparison to the size of the quantity of individuals involved in the procedure .",
    "[ tab_perf_results ] presents the performances obtained using the best filter sets for each couple of features ( sh4 , sh8 ) and convolution kernel s width ( 5 , 7 , 9 ) as well as the performances of the baseline classifiers .",
    "[ fig_comp_performance ] visually presents the recognition performance using our method ( sh8 , kernel width of 5 ) against the best baseline classifier .",
    "baseline classifiers mainly do mistakes by detecting crossing fibers as single fibers .",
    "if we use the global classification error rates , they seem to perform well , however if we use the fitness value or look at fig .",
    "[ fig_class_base ] , we understand they perform badly .",
    "this may be explained because it is trained in order to reduce the global error rate whatever is the location of the errors .",
    "the proposed convolution based classifier performs better .",
    "most error are miss recognition of the border of the phantom .",
    "this can be explained by the fact that the fitness function does not try to minimize this error .",
    "most of the other errors are located at the frontier between two different classes . if we ignore the recognition error between csf and gm ( because there may not be important in our context ) , the error rate drops from 14.65% to 6.61% ( sh8/window of size 5 ) .",
    "the huge amount of time involved in the experiment is not representative of a real use of the system as a lot of classifiers are learned and tried during the optimisation procedure . to give a more accurate representation of the duration of the computation ,",
    "we have computed the mean time taken to learn the classifier and the mean time taken to classify the voxels by using the best kernels for each configuration of features and kernel width .",
    "it takes in average less than 11s to learn the classifier and a bit more than 1.50s to classify each voxel . in a real life application ,",
    "only the classification process is used ; so we think it is fast enough to be used in a real world application even if processing time would be slightly larger with more voxels .",
    "we can approximate an overestimate of the classification duration of a whole brain by : @xmath40 with v the number of voxels to classify .",
    "it is an overestimate as it does not take into account a potential reduction of the number of voxels to classify after having applyied a manual or automatic localisation of the brain .",
    "thus with a matrix of @xmath41 , the classification duration would be approximatly @xmath42 .",
    "ten minutes seems to be a correct amount of time .",
    "we have proposed a new svm classifier taking into account a local spatial neighbourhood to classify each voxel of hardi data according to its water diffusion phenomenon .",
    "we can successfully classify voxels containing single fiber bundles , voxels with several directions of diffusion reflecting crossing fiber populations and random isotropic voxels without fiber bundles .",
    "we believe that this opens many possible perspectives in quantitative white matter analysis in healthy and patients with neurodegenerative diseases .",
    "future experiment could use large margin filtering  @xcite in order to optimize the svm parameters while optimizing the convolution matrices .",
    "it is also important to apply the results on real brain datasets manually labelled by neurologists or neurosurgeons .",
    "morphological operators could also decrease the recognition error rate .",
    "p.  fillard , m.  descoteaux , a.  goh , s.  gouttard , b.  jeurissen , j.  malcolm , a.  ramirez - manzanares , m.  reisert , k.  sakaie , f.  tensaouti , t.  yo , j .- f .",
    "cois mangin , and c.  poupon . quantitative evaluation of 10 tractography algorithms on a realistic diffusion mr phantom .",
    ", 56(1):220234 , 2011 .",
    "b.  jeurissen , a.  leemans , j .- d .",
    "tournier , d.  k. jones , and j.  sijbers .",
    "investigating the prevalence of complex fiber configurations in white matter tissue with diffusion magnetic resonance imaging .",
    ", in press(-): , 2012 ."
  ],
  "abstract_text": [
    "<S> the understanding of neurodegenerative diseases undoubtedly passes through the study of human brain white matter fiber tracts . to date </S>",
    "<S> , diffusion magnetic resonance imaging ( dmri ) is the unique technique to obtain information about the neural architecture of the human brain , thus permitting the study of white matter connections and their integrity . </S>",
    "<S> however , a remaining challenge of the dmri community is to better characterize complex fiber crossing configurations , where diffusion tensor imaging ( dti ) is limited but high angular resolution diffusion imaging ( hardi ) now brings solutions . </S>",
    "<S> this paper investigates the development of both identification and classification process of the local water diffusion phenomenon based on hardi data to automatically detect imaging voxels where there are single and crossing fiber bundle populations . </S>",
    "<S> the technique is based on knowledge extraction processes and is validated on a dmri phantom dataset with ground truth . </S>"
  ]
}