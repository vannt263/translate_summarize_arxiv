{
  "article_text": [
    "transcription is one of the most challenging problems in music information processing . to obtain music scores , we need to extract pitch information from music audio signals .",
    "recently pitch analysis for polyphonic ( e.g.  piano ) music has been receiving much attention @xcite . to solve the other part of the transcription problem",
    ", many studies have been devoted to so - called rhythm transcription , that is , the problem of recognising quantised note lengths ( or note values ) of the musical notes in midi performances @xcite .    since early studies in the 1980s ,",
    "various methods have been proposed for rhythm transcription . as we explain in detail in sec .",
    "[ sec : relatedwork ] , the general trend has shifted to using machine learning techniques to capture what natural music scores are and how music performances fluctuate in time .",
    "one of the models most frequently used in recent studies @xcite is the hidden markov model ( hmm ) @xcite . in spite of its importance and",
    "about 30 years of history , however , little comparative evaluations on rhythm transcription have been reported in the literature and the state - of - the - art method has not been known .",
    "rhythm transcription also raises challenging problems of representing and modelling scores and performances for polyphonic music .",
    "this is because a polyphonic score has multilayer structure , where concurrently sounding notes are grouped into several streams , or _",
    "voices _ , for example , has two voices corresponding to the left and right hands . ] . as explained in sec .",
    "[ sec : relatedwork ] , a conventional way of representing a polyphonic score as a linear sequence of chords @xcite may not retain sequential regularities within voices , such as those in polyrhythmic scores , nor it can capture the loose synchrony between voices @xcite in polyphonic performances . therefore solutions to explicitly describe the multiple - voice structure must be sought .        from this point of view , in a recent conference @xcite , we reported a statistical model that can describe the multiple - voice structure of polyphonic music .",
    "the model is based on the merged - output hmm @xcite , which describes polyphonic performances as merged outputs from multiple component hmms , called _ voice hmms _ , each of which describes the generative process of music scores and performances of one voice ( fig .",
    "[ fig : overview ] ) .",
    "it was confirmed that the model outperformed conventional hmm - based methods for transcribing polyrhythmic performances .",
    "the purpose of this paper is to discuss in detail the merged - output hmm and its inference technique . due to the large size of the state space and the complex dependencies between variables , the standard viterbi algorithm or its refined version @xcite",
    "can not be applied and a new inference technique is necessary .",
    "this problem typically arises when a voice hmm is an autoregressive hmm , which is commonly used as music score / performance models where output probabilities of events ( e.g.  pitch , time , etc . )",
    "depend on past events . using a trick of introducing an auxiliary variable to trace the history of output symbols similarly as in ref .",
    "@xcite , we develop an inference technique that can work in a practical computer environment and could be applied for any merged - output hmms with autoregressive voice hmms .",
    "we provide a complete description of the proposed model and examine the influence of its architecture and parameters .",
    "first , we explain details omitted in the previous paper including the description of the chord model and a switch of a coupling parameter between voice hmms depending on pitch contexts .",
    "the effects are examined in terms of accuracies .",
    "second , the determination of model parameters based on supervised learning is discussed and the influence of parameters of the performance model is investigated . finally , a feature of the proposed method is its simultaneous voice separation and rhythm recognition .",
    "we examine this effect by evaluating accuracies of both voice separation and rhythm recognition and comparing with a cascading algorithm that performs voice separation first and then recognises rhythm .",
    "another contribution of this paper is to present results of systematic comparative evaluations to find the state - of - the - art method .",
    "in addition to two hmm - based methods @xcite previously tested in ref .",
    "@xcite , we tested frequently cited methods and theoretically important methods whose source codes were available : connectionist quantizer @xcite , melisma analyzers ( version 1 @xcite and version 2 @xcite ) and two - dimensional ( 2d ) probabilistic context - free grammar ( pcfg ) model @xcite .",
    "an evaluation measure for rhythm transcription , which is briefly sketched in ref .",
    "@xcite , is explained in full detail together with its calculation algorithm .",
    "we make public the source codes for the best models found ( the proposed model and other two hmms ) as well as the evaluation tool to enable future comparisons @xcite .",
    "we hope that these materials would encourage researchers interested in music transcription and symbolic music processing .",
    "previous studies on rhythm transcription are reviewed in this section .",
    "the purpose is two - fold : first , we describe the historical development of models for rhythm transcription , some of which form bases of our model and some are subjects of our comparative evaluation .",
    "second , we review how polyphony has been treated in previous studies in rhythm transcription and related fields and explain in details the motivations for explicitly modelling multiple voices .",
    "part of discussions in secs .",
    "[ sec : statisticalmethods ] and [ sec : polyphonicextension ] and the figures are quoted from ref .",
    "@xcite to make this section more informative and self - contained .",
    "until the late 1990s , studies on rhythm transcription used models describing the process of quantising note durations and/or recognising the metre structure .",
    "longuet - higgins @xcite developed a method for estimating the note values and the metre structure simultaneously by recursively subdividing a time interval into two or three almost equally spaced parts that are likely to begin at note onsets .",
    "a similar method of dividing a time interval using template grids and an error function of onsets and inter - onset intervals ( iois ) has also been proposed @xcite .",
    "methods using preference rules for the ratios of quantised note durations have been developed by chowning et al .",
    "@xcite and temperley et al .",
    "desain et al .",
    "@xcite proposed a connectionist approach that iteratively converts note durations so that adjacent durations tend to have simple integral ratios .    despite some successful results , these methods have limitations in principle .",
    "first , they use little or no information about sequential regularities of note values in music scores .",
    "since there are many logically possible sequences of note values , such sequential regularities are important clues to finding the one that is most likely to appear in actual music scores .",
    "second , tendencies of temporal fluctuations in human performances are described only roughly . in particular , the chord clustering  that is , the identification of notes whose onsets are exactly simultaneous in the score  is handled with thresholding or is not treated at all . finally , the parameters of most of those algorithms are tuned manually and optimisation methods have not been developed .",
    "this means that one can not utilise a data set of music scores and performances to learn the parameters , or only inefficient optimisation methods like grid search can be applied .      since around the year 2000",
    ", it has become popular to use statistical models , which enable us to utilise the statistical nature of music scores and performances .",
    "usually two models , one describing the probability of a score ( _ score model _ ) and the other describing the probability of a performance given a score ( _ performance model _ ) , are combined as a bayesian model , and rhythm transcription can be formulated as maximum a posteriori estimation .",
    "below we review representative models for rhythm transcription .",
    "we here consider only monophonic performances ; polyphonic extensions are described in sec .",
    "[ sec : polyphonicextension ] .        in one class of hmms for rhythm transcription , which we call",
    "_ note hmms _",
    ", a score is represented as a sequence of note values and described with a markov model ( fig .",
    "[ fig : previousmodels ] ) @xcite . to describe the temporal fluctuations in performances ,",
    "one introduces a latent variable corresponding to a ( local ) tempo that is also described with a markov model .",
    "an observed duration is described as a product of the note value and the tempo that is exposed to noise of onset times .    in another class of hmms , which we call _",
    "metrical hmms _",
    ", a different description is used for the score model @xcite . instead of a markov model of note values ,",
    "a markov process on a grid space representing beat positions of a unit interval , such as a bar , is considered ( fig .",
    "[ fig : previousmodels ] ) .",
    "the note values are given as differences between successive beat positions .",
    "incorporation of the metre structure is an advantage of metrical hmms .",
    "_ pcfg models _ have also been proposed @xcite . as in @xcite , a time interval in a score is recursively divided into shorter intervals until those corresponding to note values are obtained , and probabilities describe what particular divisions are likely . as an advantage , modifications of rhythms by inserting ( splitting ) notes can be naturally described with these models .      the note hmm has been extended to handle polyphonic performances @xcite .",
    "this is done by representing a polyphonic score as a linear sequence of chords or , more precisely , _",
    "note clusters _ consisting of one or more notes .",
    "such score representation is also familiarly used for music analysis @xcite and score - performance matching @xcite .",
    "chordal notes can be represented as self - transitions in the score model ( fig .",
    "[ fig : previousmodels ] ) and their iois can be described with a probability distribution with a peak at zero .",
    "polyphonic extension of metrical hmms is possible in the same way .",
    "although this simplified representation of polyphonic scores is logically possible , there are instances in which score and performance models based on this representation can not describe the nature of polyphonic music appropriately .",
    "first , complex polyphonic scores such as polyrhythmic scores are forced to have unrealistically small probabilities .",
    "this is because such scores consist of rare rhythms in the simplified representation even if the component voices have common rhythms ( fig .",
    "[ fig : homophonizationexample ] ) .",
    "second , the phenomenon of loose synchrony between voices ( e.g.  two hands in piano performances @xcite ) , called _ voice asynchrony _ , can not be described .",
    "for example , the importance of incorporating the multiple - voice structure in the presence of voice asynchrony is well investigated in studies on score - performance matching @xcite .    to describe the multiple - voice structure of polyphonic scores ,",
    "an extension of the pcfg model called 2d pcfg model has been proposed @xcite .",
    "this model describes , in addition to the divisions of a time interval , duplications of intervals into two voices .",
    "unfortunately , a tractable inference algorithm could not be obtained for the model , and the correct voice information had to be provided for evaluations . in a recent report , takamune et al .",
    "@xcite state that this problem is solved using a generalised lr parser .",
    "however , as we shall see in sec .",
    "[ sec : comparison ] , their algorithm often fails to output results and the computational cost is quite high .       and @xmath0 represent auxiliary states to define the initial transitions.,title=\"fig : \" ]    based on the fact that hmm is effective for monophonic music @xcite ,",
    "an hmm - based model that can describe multiple - voice structure of symbolic music , called merged - output hmm , has been proposed @xcite . in the model ,",
    "each voice is described with an hmm , called a voice hmm , and the total polyphonic music signal is represented as merged outputs from multiple voice hmms ( fig .  [ fig : mergedoutputhmm ] ) .",
    "mathematically the model is described as follows .",
    "let us consider the case of two voices indexed by a variable @xmath1 , and let @xmath2 denote the state variable , let @xmath3 denote the transition probability and let @xmath4 denote the output probability of each voice hmm ( for some output symbol @xmath5 ) .",
    "for each instance @xmath6 , one voice @xmath7 is chosen by a bernoulli process as @xmath8 where ber is the bernoulli distribution and its probability parameter @xmath9 represents how likely the @xmath6-th output is generated from the hmm of voice @xmath7 . the chosen voice hmm then makes a state transition and outputs @xmath10 while the other voice hmm stays at the current state .",
    "the whole process is described as an hmm with a state space indexed by @xmath11 and the transition and output probabilities ( in the non - interacting case @xcite ) are given as @xmath12 where @xmath13 is kronecker s delta .",
    "a merged - output hmm with more than two voices can be constructed similarly .    as discussed in ref .",
    "@xcite , the merged - output hmm can be seen as a variant of factorial hmm @xcite in its most general sense . unlike the standard factorial hmm ,",
    "only one of the voice hmms makes a state transition and outputs a symbol at each instant . owing to this property the sequential regularity within each voice can be described efficiently in the merged - output hmm , even when notes in one voice are interrupted ( in the time order ) by notes of other voices .",
    "accordingly necessary inference algorithms are also different as we will see in sec .",
    "[ sec : inferencealgorithm ] .",
    "we present a complete description of a rhythm transcription method based on merged - output hmm @xcite that describes polyphonic performances with multiple - voice structure .",
    "the generative model is presented in sec .",
    "[ sec : modelformulation ] , the determination of model parameters is discussed in sec .  [",
    "sec : parameters ] and its inference algorithm that simultaneously yields rhythm transcription and voice separation is derived in sec .",
    "[ sec : inferencealgorithm ] .      a merged - output hmm for rhythm transcription proposed in ref .",
    "@xcite is reviewed here with additional details .",
    "first , the description of the chord model is given , which was explained as a ` self - transition ' in the note - value state space .",
    "since self - transition is also used to represent repeated note values of two note clusters , it should be treated with care and we introduce a two - level hierarchical markov model to solve the problem .",
    "second , a refinement of switching the probability of choosing voice hmms is given , which was not mentioned previously but necessary to improve the accuracy of voice separation .    in the following ,",
    "a music score is specified by multiple sequences , corresponding to voices , of pitches and note values and a midi performance signal is specified by a sequence of pitches and onset times . in this paper",
    "we only consider note onsets and thus note length and ioi mean the same thing .",
    "a voice hmm is constructed based on the note hmm @xcite , which is extended to explicitly model pitches in order to appropriately describe voices .",
    "if there are no chords , a score note is specified by a pair of pitch and note value .",
    "note that to define @xmath14 note lengths we need @xmath15 note onsets and thus @xmath15 score notes should be considered .",
    "let @xmath15 be the number of score notes in one voice and let @xmath16 denote the note value of the @xmath6-th note .",
    "if there are no chords , the note values @xmath17 are generated by a markov chain with the following probabilities : @xmath18 where @xmath19 is the initial probability and @xmath20 is the ( stationary ) transition probability .        to describe chords ,",
    "we extend the above markov model to a two - level hierarchical markov model with state variables @xmath21 .",
    "the variable @xmath22 represents the note value of a note cluster and @xmath23 indicates whether the next note onset belongs to the same note cluster or not : if @xmath24 the @xmath6-th and @xmath25-th notes are in a note cluster and if @xmath26 , they belong to different note clusters .",
    "the variable @xmath23 also takes the values ` in ' and ` out ' to define the initial and exiting probabilities .",
    "the internal markov model has the topology illustrated in fig .",
    "[ fig : chordmodel ] and is described with the following transition probabilities ( @xmath27 ) : @xmath28 where @xmath29 and @xmath30 are parameters controlling the number of notes in a note cluster . denoting @xmath31 and @xmath32 , the transition probability of the hierarchical model is given as @xmath33 the initial probability is given as @xmath34 .",
    "we notate @xmath35 .",
    "to describe the temporal fluctuations , we introduce a tempo variable , denoted by @xmath36 , that describes the local ( inverse ) tempo for the time interval between the @xmath6-th and @xmath25-th note onsets @xcite .",
    "to represent the variation of tempos , we put a gaussian markov process on the logarithm of the tempo variable @xmath37 as @xmath38 where @xmath39 denotes the normal distribution with mean @xmath40 and variance @xmath41 , and @xmath42 , @xmath43 and @xmath44 are parameters .",
    "the parameter @xmath44 describes the amount of tempo changes .",
    "if the @xmath6-th and @xmath25-th notes belong to a note cluster ( i.e.  @xmath24 ) , their ioi approximately obeys an exponential distribution @xcite and the probability of the onset time of the @xmath25-th note , denoted by @xmath45 , is then given as @xmath46 where exp denotes the exponential distribution and @xmath47 is the scale parameter , which controls the asynchrony of note onsets in a note cluster .",
    "otherwise , @xmath48 has a duration corresponding to note value @xmath16 and the probability is described with a normal distribution as @xmath49 intuitively the parameter @xmath50 describes the amount of onset - time fluctuations due to human motor noise when a performer keeps a tempo .",
    "we do not put a distribution on the onset time of the first note @xmath51 because we formulate the model to be invariant under time translations and this value would not affect any results of inference .",
    "we notate @xmath52 and @xmath53 .",
    "finally we describe the generation of pitches @xmath54 as a markov chain ( we introduce an auxiliary symbol @xmath55 for later convenience ) .",
    "the probabilities are @xmath56 where @xmath57 denotes the ( stationary ) transition probability and if @xmath58 it denotes the initial probability",
    ".     indicates that a distribution is not given for this variable.,title=\"fig : \" ]    the above model can be summarised as an autoregressive hmm with hidden states @xmath59 and outputs @xmath60 ( fig .",
    "[ fig : graphvoicehmm ] ) , which will be a voice hmm . although so far the probabilities of pitches are independent of other variables , they will be significant once multiple voice hmms are merged and the posterior probabilities are inferred .",
    "we combine the multiple voice hmms in sec .  [",
    "sec : parthmm ] using the framework of merged - output hmms ( sec .",
    "[ sec : mohmm ] ) .",
    "since in piano performances , which are our main focus , polyrhythm and voice asynchrony usually involve the two hands , we consider a model with two voices , leaving a note that it is not difficult to formalise a model with more than two voices . in",
    "what follows , voices are indexed by a variable @xmath1 , corresponding to the left and right hand in practice .",
    "all the variables and parameters are now considered for each voice and thus @xmath61 is the sequence of note values in voice @xmath62 , @xmath63 their transition probability , etc .",
    "simply speaking , the sequence of merged outputs is obtained by gathering the outputs of the voice hmms and sorting them according to onset times . to derive inference algorithms that are computationally tractable , however",
    ", we should formulate a model that outputs notes incrementally in the order of observations .",
    "this can be done by introducing stochastic variables @xmath64 , which indicate that the @xmath6-th observed note belongs to voice @xmath7 and follow the probability @xmath8 .",
    "the parameter @xmath9 represents how likely the @xmath6-th note is generated from the hmm of voice @xmath7 .",
    "the variable @xmath7 is determined in advance to the pitch , note value or onset time of the corresponding note in the generative process ( which is described below ) . for rhythm transcription , however , dependence of the parameter @xmath65 on features of the given input ( midi performance ) can be introduced to improve the accuracy of voice separation .",
    "as such a feature , we use contexts of pitch that reflects the constraint on pitch intervals that can be simultaneously played by one hand . defining @xmath66 and @xmath67 as the highest and lowest pitch that is sounding simultaneously ( but not necessarily having a simultaneous onset ) with @xmath68",
    ", we switch the value of @xmath9 depending on whether @xmath69 or not and whether @xmath70 or not ( total of four cases ) , reflecting the fact that a pitch interval larger than 15 semitones is rarely played with one hand at a time . the effect of using this context - dependent @xmath65 is examined in sec .  [",
    "sec : examiningmodel ] .    if voice @xmath7 is chosen , then the hmm of voice @xmath7 outputs a note , and the hidden state of the other voice hmm is unchanged . such a model can be described with an hmm with a state space labelled by @xmath71 .",
    "here we have a single tempo variable @xmath72 that is shared by the two voices in order to assure loose synchrony between them .",
    "the transition probability @xmath73 , for @xmath74 , is given as @xmath75 \\label{eq : trprobmergedoutputhmm}\\end{aligned}\\ ] ] where the expression ` @xmath76 ' means that the previous term is repeated with 1 and 2 interchanged and we have defined @xmath77 and @xmath13 denotes kronecker s delta for discrete variables and dirac s delta function for continuous variables . the probability @xmath78 is defined in eq .",
    "( [ eq : tempotrprob ] ) , and @xmath79 is defined in eqs .",
    "( [ eq : onsettimeprob1 ] ) and ( [ eq : onsettimeprob2 ] ) . for note values",
    "the initial probability is given as @xmath80 , and for pitches the initial probability is given in eq .",
    "( [ eq : pitchiniprob ] ) .",
    "the first onset times @xmath81 and @xmath82 do not have distributions , as explained in sec .",
    "[ sec : parthmm ] , and we practically set @xmath83 ( the first observed onset time ) .",
    "finally the output of the model is given as @xmath84 and thus the complete - data probability is written as @xmath85 here @xmath86 denotes the total number of score notes , and the following notations are used : @xmath87 , @xmath88 , @xmath89 , and @xmath90 . note that whereas @xmath91 and @xmath92 are observed quantities , @xmath93 and @xmath94 are not because we can not directly observe the voice information encrypted in @xmath95 .     and",
    "@xmath96 ) represent note values for each voice without redundancies ( see sec .",
    "[ sec : inferencealgorithm ] ) .",
    "see also the caption of fig .",
    "[ fig : graphvoicehmm ] .",
    "here we have independent initial distributions for note values and pitches of different voice hmms.,title=\"fig : \" ]    the graphical representation of the model is illustrated in fig .",
    "[ fig : graphmohmm ] .",
    "we here summarise model parameters , explain how they can be determined from data and describe some reasonable constraints to improve the efficiency of parameter learning .",
    "let @xmath97 and @xmath98 be the number of pitches and note values , which are set as 88 and 15 in our implementation in sec .",
    "[ sec : evaluation ] .",
    "the score model for each voice hmm has the following parameters : @xmath99 [ @xmath98 ] , @xmath101 [ @xmath102 , @xmath29 [ @xmath98 ] , @xmath30 [ @xmath98 ] , @xmath103 [ @xmath97 ] and @xmath105 [ @xmath106 , where the number in square brackets indicate the number of parameters .",
    "( the number of independent parameters may reduce because of normalisation conditions , which we shall not care here for simplicity . )",
    "these parameters can be determined with a data set of music scores with voice indications . for piano pieces ,",
    "the two staffs in the grand staff notation can be used for the two voice hmms . after representing notes in each voice as a sequence of note clusters as in fig .",
    "[ fig : homophonizationexample ] , @xmath99 and @xmath101 can be obtained in a standard way . determining @xmath103 and @xmath105 is also straightforward . to determine the parameters @xmath29 and @xmath30",
    ", we first define the frequency of note clusters containing @xmath107 notes with note value @xmath22 as @xmath108 .",
    "since @xmath29 is the proportion of note clusters containing more than one notes , it is given by @xmath109 the @xmath30 can be obtained by matching the expected staying time at state with @xmath110 ( fig .",
    "[ fig : chordmodel ] ) as follows : @xmath111 in practice , the transition probability @xmath105 and the initial probabilities @xmath103 and @xmath99 are often subject to the sparseness problem since the first one has a rather large number of parameters and for the last two only one sample from each piece can be used . to overcome this problem",
    ", we can reduce the number of parameters in the following way , which is used in our implementation .",
    "first , we approximate @xmath105 as a function of the interval @xmath112 , which reduces the number of parameters from @xmath113 to @xmath114 .",
    "second , we can approximate @xmath103 by a gaussian function as @xmath115 finally , for @xmath99 , the stationary ( unigram ) probability obtained from @xmath101 can be used .",
    "note that the pitch probabilities are only used to improve voice separation and their precise values do not much influence the results of rhythm transcription . likewise",
    "the initial probabilities do not influence the results for most notes due to the markov property .",
    "the performance model has the following five parameters : @xmath44 , @xmath116 , @xmath117 , @xmath50 and @xmath47 .",
    "these can be determined from performance data , for example , midi recordings of piano performances whose notes are matched to the corresponding notes in the scores . among these the initial values ,",
    "@xmath116 and @xmath117 are most difficult to determine from data but again have limited influence as a prior for global tempo , which is supposed to be musically less important ( see discussion in sec .",
    "[ sec : evaluationmethodology ] ) . in our implementation , they are simply set by hand . the method for determining the other parameters based on a principle of minimal prediction error",
    "is discussed in a previous study @xcite and will not be repeated here .",
    "an additional parameter for the merged - output hmm is @xmath65 , which is generally obtained by simply counting the number of notes in each voice or can be approximated simply by @xmath118 . in our implementation , we obtain four @xmath65 s depending on the context as described in sec .",
    "[ sec : multivoicemodel ] , which is also straightforward .      to obtain the result of rhythm transcription using the model just described , we must estimate the most probable hidden state sequence @xmath119 given the observations @xmath60 .",
    "this gives us the voice information @xmath120 and the estimated note values @xmath121 and @xmath122 .",
    "let @xmath123 be the reduced sequence of note values for voice @xmath62 , which is obtained by , for all @xmath6 , deleting the @xmath6-th element with @xmath124 in @xmath125 .",
    "then the score time @xmath126 of the @xmath6-th note onset in voice @xmath62 is given by @xmath127 the inference algorithm of merged - output hmm has been discussed previously @xcite . since a merged - output hmm can be seen as an hmm with a product state space , the viterbi algorithm @xcite can be applied for inference in principle .",
    "it was shown that owing to the specific form of transition probability matrix as in eq .",
    "( [ eq : trprobstandardmohmm ] ) , the computational complexity for one viterbi update can be reduced from @xmath128 to @xmath129 where @xmath130 is the size of the state space of the @xmath62-th voice hmm .",
    "however , since the state space of the model in sec .",
    "[ sec : multivoicemodel ] involves both discrete and continuous variables , an exact inference in this way is difficult .    to solve this , we discretise the tempo variable , which practically has little influence when the step size is sufficiently small since tempo is restricted in a certain range in conventional music and @xmath36 always has uncertainty of @xmath131 .",
    "discretisation of tempo variables has also been used for audio - to - score alignment @xcite and beat tracking @xcite .",
    "other continuous variables @xmath132 and @xmath94 can take only values of observed onset times and thus can , in effect , be treated as discrete variables .",
    "unfortunately the direct use of the viterbi algorithm is impractical even with this discretisation .",
    "let us roughly estimate the computational cost to see this .",
    "let @xmath97 , @xmath133 and @xmath134 be the sizes of the state space for pitch , note value and tempo . since onset times @xmath135 could take @xmath14 values ,",
    "the size of the state space for @xmath136 is @xmath137 , and the computational cost for one viterbi update is @xmath138 . a rough estimation ( @xmath139 , @xmath140 , @xmath141 , @xmath142 ) yields @xmath143 , which is intractable . even after using the constraints of the transition probabilities in eq .",
    "( [ eq : trprobmergedoutputhmm ] ) , we have @xmath144 , which is still intractable",
    ".    we can avoid this intractable computational cost and derive an efficient inference algorithm by appropriately relating the hidden variables @xmath145 to observed quantities @xmath60 .",
    "we first introduce a variable @xmath146 , which is defined as the smallest @xmath147 satisfying @xmath148 for each @xmath6 .",
    "we find the following relations : @xmath149 this means that @xmath145 are determined if we are given @xmath150 and the effective number of variables is reduced by using @xmath151 . with this change of variables ,",
    "we find @xmath152 \\notag\\\\ & \\cdot \\big [ \\delta_{s_ns_{n-1}}\\delta_{h_n(h_{n-1}+1)}a_n^{\\rm same}+(1-\\delta_{s_ns_{n-1}})\\delta_{h_n1}a_n^{\\rm diff}\\big]\\bigg\\},\\notag\\\\ & a_n^{\\rm same}=a_{s_n}(w_n^{(s_n)},p_n , t_n|w_{n-1}^{(s_n)},p_{n-1},t_{n-1};v_{n-1}),\\notag\\\\ & a_n^{\\rm diff}=a_{s_n}(w_n^{(s_n)},p_n , t_n|w_{n-1}^{(s_n)},p_{\\tilde{n}},t_{\\tilde{n}};v_{n-1})\\end{aligned}\\ ] ] where @xmath153 in the last line should be replaced by @xmath154 .",
    "we can now apply the viterbi algorithm on the state space @xmath155 . noting that the maximum possible value of @xmath156 is @xmath14 and using the constraints of the transition probabilities , one finds that @xmath157 , which is significantly smaller than the previous values .",
    "note that so far no ad - hoc approximations have been introduced to reduce the computational complexity .",
    "practically , we can set a smaller maximal value @xmath158 of @xmath156 to obtain approximate optimisation , which further reduces the computational cost to @xmath159 .",
    "the number @xmath160 can be regarded as the maximum number of succeeding notes played by one hand without being interrupted by the other hand .",
    "the choice of @xmath160 and its dependency is discussed in sec .",
    "[ sec : dependencyonnh ] .",
    "in a few studies that reported systematic evaluations of rhythm transcription @xcite , editing costs ( i.e.  the number of necessary operations to correct an estimated result ) are used as evaluation measures .",
    "these studies used the shift operation , which changes the score time of a particular note or equivalently , changes a note value , to count the number of note - wise rhythmic errors . musically speaking , on the other hand ,",
    "the relative note values are more important than the absolute note values , and the tempo error should also be considered .",
    "this is because there is arbitrariness in choosing the unit of note values : for example , a quarter note played in a tempo of 60 bpm has the same duration as a half note played in a tempo of 120 bpm .",
    "since results of rhythm transcription often contain note values that are uniformly scaled from the correct values , which should not be considered as completely incorrect estimations @xcite , we must take into account the scaling operation as well as the shift operation .        as shown in the example in fig .",
    "[ fig : rhythmedition ] , there can be local scaling operations and shift operations , and a reasonable definition of the editing cost is the least number @xmath161 of operations consisting of @xmath162 scaling operations and @xmath163 shift operations ( @xmath164 ) .",
    "as explained in detail in the appendix , this _ rhythm correction cost _ can be calculated by a dynamic programming similarly as the levenshtein distance . definition and calculation of the rhythm correction cost in the polyphonic case",
    "are also discussed there .",
    "we use the _ rhythm correction rate _ @xmath165 as an evaluation measure .",
    "we first present results of comparative evaluations .",
    "the purpose is to find out the state - of - the - art method of rhythm transcription and its relation to the proposed model . among previous methods described in sec .",
    "[ sec : relatedwork ] , the following six were directly compared : connectionist quantizer @xcite , melisma analyzers ( the first @xcite and second @xcite versions ) , the note hmm @xcite , the metrical hmm @xcite and the 2d pcfg model @xcite .",
    "the first five are relatively frequently cited and the last one is theoretically important as it provides an alternative way of statistically modelling multiple - voice structure ( sec .",
    "[ sec : polyphonicextension ] ) .",
    "two data sets of midi recordings of classical piano pieces were used .",
    "one ( _ polyrhythmic _ data set ) consisted of 30 performances of different ( excerpts of ) pieces that contained 2 against 3 or 3 against 4 polyrhythmic passages , and the other ( _ non - polyrhythmic _ data set ) consisted of 30 performances of different ( excerpts of ) pieces that did not contain polyrhythmic passages . pieces by various composers , ranging from j.  s.  bach to debussy , were chosen and the players were also various : some of the performances were taken from the pedb database @xcite , a few were performances we recorded , and the rests were taken from collections in public domain websites .",
    "for the proposed method , all normal , dotted , and triplet note values ranging from the whole note to the 32nd note were used as candidate note values .",
    "parameters for the score model , @xmath101 , @xmath29 , @xmath30 and @xmath105 , and the value of @xmath65 were learned from a data set of classical piano pieces that had no overlap with the test data@xmath166 .",
    "we set @xmath167 for the left - hand voice hmm and @xmath168 for the right - hand voice hmm ( see sec .",
    "[ sec : parameters ] ) .",
    "values for the parameters for the performance model , @xmath44 , @xmath50 and @xmath47 , were taken from a previous study @xcite ( which used performance data different from ours ) .",
    "the used values were @xmath169 , @xmath170 s and @xmath171 s. for the tempo variable , we discretised @xmath36 into 50 values logarithmically equally spaced in the range of 0.3 to 1.5 s per quarter note ( corresponding to 200 bpm and 40 bpm ) .",
    "we set @xmath172 as the central value of the range ( @xmath173 s per quarter note or 89.4 bpm ) and @xmath174 .",
    "@xmath160 was chosen as 30 , which will be explained later ( sec .",
    "[ sec : dependencyonnh ] ) .    for other methods",
    ", we used the default values provided in the source codes , except for the note hmm and the metrical hmm , which are closely related to our method .",
    "for these models , the parameters of the score models were also trained with the same score data set and the performance model was the same as that for the proposed model .",
    "the metrical hmm was build and learned for two cases , duple metres ( 2/4 , 4/4 , etc . ) and triple metres ( 3/4 , 6/4 , etc . ) , and one of these models were chosen for each performance according to the likelihood .    for melisma analyzers ,",
    "results of the metre analysis were used and the estimated tactus was scaled to a quarter note to use the results as rhythm transcriptions . for connectionist quantizer , which accepts only monophonic inputs ,",
    "chord clustering was performed beforehand with a threshold of 35 ms on the iois of chordal notes .",
    "the algorithm was run for 100 iterations for each performance .",
    "because this algorithm outputs note lengths in units of 10 ms without indications for tactus , the most frequent note length was taken as the quarter note value .",
    "+    the distributions of rhythm correction rates , their averages and standard deviations are shown in fig .",
    "[ fig : rhythmcorrectionrates ] . for clear illustration , the results for connectionist quantizer , which was much worse than the others , were omitted : the average ( standard deviation , first , third quantiles ) was 53.7% ( 18.5% , 43.8% , 67.3% ) for the polyrhythmic data and 38.9% ( 13.9% , 28.2% , 47.3% ) for the non - polyrhythmic data .",
    "as shown in table [ tab : result ] , some performances were not properly processed by the 2d pcfg model and melisma analyzers . for the 2d pcfg model , because it took much time in processing some performances ( executions lasted more than a week for some performances ) , every performance was run for at least 24 hours and only performances for which execution ended were treated as processed cases . among 29 ( out of 60 ) performances for which execution ended , 12 performances did not receive any results ( because the parser did not succeed in accepting the performances ) and those were also treated as ` unprocessed ' cases . to compare the results in the presence of these unprocessed cases",
    ", we calculated for each of the algorithms and for successfully processed performances the differences in rhythm corrections rates relative to the proposed model . their average and standard error ( corresponding to 1@xmath175 deviation in the @xmath176-test ) are shown in table [ tab : result ] .",
    "= 2pt    .averages and standard errors of differences of the rhythm correction rate for listed models @xmath177 and that of the proposed model @xmath178 ( lower is better ) .",
    "the number of unprocessed pieces ( see text ) is shown in the third column .",
    "values with a statistical significance @xmath179 are illustrated in bold font . [ cols=\"<,<,>,>\",options=\"header \" , ]             for the polyrhythmic data , it is clear that the proposed model outperformed the other methods , by more than 12 points in the accuracies and more than 3@xmath175 deviations in the statistical significances . among the other algorithms , the note hmm and metrical hmm had similar accuracies and were second best , and connectionist quantizer was the worst .",
    "these results quantitatively confirmed that modelling multiple - voice structure is indeed effective for polyrhythmic performances .",
    "contrary to our expectation , the result for the 2d pcfg model was second worst for these data .",
    "this might be because that the algorithm using pruning can not always find the optimal result and the model parameters have not been trained from data , both of which are difficult to ameliorate currently but could possibly be improved in the future .",
    "the results show that the statistical models with ( almost fully ) learned parameters ( the proposed model , the note hmm and the metrical hmm ) had better accuracies than the other statistical models with partly learned parameters or without parameter learning ( the 2d pcfg model and melisma analyzer version 2 ) and other methods .",
    "a typical example of polyrhythmic performance that is almost correctly recognised by the proposed model but not by other methods is shown in fig .",
    "[ fig : example1 ] .",
    "one finds that the 3 against 4 polyrhythm was properly recognised only by the proposed model ( cf .",
    "[ fig : homophonizationexample ] ) .    for the non - polyrhythmic data ,",
    "connectionist quantizer again had a far worst result and the differences among the other methods were much smaller ( within 2 points ) compared to the polyrhythmic case .",
    "the note hmm and metrical hmm had similar and best accuracies and the proposed model was the third best .",
    "the difference in the average values between the proposed model and the note hmm or the metrical hmm was less than 1 point and the statistical significance was @xmath180 and @xmath181 , respectively . presumably , the main reason that the note and metrical hmms worked better is that the rhythmic pattern in the reduced sequence of note clusters is often simpler than that of melody / chords in each voice in the non - polyrhythmic case because of the principle of complementary rhythm @xcite . in particular , notes / chords in a voice can have tied note values that are not contained in our candidate list ( e.g.  a quarter note @xmath182 16th note value for the last note of the first bar in the upper staff in the example of fig .  [",
    "fig : example2 ] ) , which can also appear as a result of incorrect voice separation .",
    "it is observed that the transcription by the merged - output hmm can produce desynchronised cumulative note values in different voices ( e.g. , the quarter note e@xmath1835 in the upper voice in fig .",
    "[ fig : example2 ] has a time span different from that of the corresponding notes in the lower voice ) .",
    "this is due to the lack of constraints to assure the matching of these cumulative note values and the simplification of independent voice hmms . for the note hmm and the proposed model ,",
    "there were grammatically wrong sequences of note values , for example , triplets that appear in single or two notes without completing a unit of beat ( e.g.  the triplet notes in the left - hand part in fig .",
    "[ fig : example2 ] ) .",
    "further improvements are desired by incorporating such constraints and interactions between voices into the model .",
    "we here examine the proposed method in more details .      in sec .",
    "[ sec : inferencealgorithm ] we introduced a cutoff @xmath160 in the inference algorithm to reduce the computational cost . in a previous study @xcite that discussed the same cutoff , it has been empirically confirmed that @xmath184 yields almost exact results for piano performances . since it is difficult for our model to run the exact algorithm corresponding to @xmath185 , we compared results of varying @xmath160 up to 50 to investigate its dependency .",
    ".,title=\"fig : \" ]    as shown in fig .",
    "[ fig : variousnh ] , the results were similar for @xmath186 and were exactly same for @xmath187 .",
    "based on this result , we used the value @xmath188 for all other evaluations in this paper . note that the sufficient value of @xmath160 for exact inference may depend on data and that smaller values with sub - optimal estimations could yield better accuracies ( as the case of @xmath189 for our algorithm and data ) .",
    "as explained in sec .",
    "[ sec : modelformulation ] , we propose a two - level hierarchical hmm for the description of chords , replacing self - transitions used in previous studies @xcite . to examine its effect in terms of accuracies , we directly compared the two cases implemented in the merged - output hmm . since in the former case a self - transition is also used to describe repeated note values of two note clusters , post - processing using the onset - time output probabilities",
    "was performed on the results of viterbi decoding to determine whether a self - transition describe chordal notes or not .",
    "the average rhythm correction rate by the chord model using self - transitions was 15.69% for the polyrhythmic data and",
    "9.12% for the non - polyrhythmic data . by comparing with the values in fig .",
    "[ fig : rhythmcorrectionrates ] , our chord model was slightly better and the differences are @xmath190 and @xmath191 ( statistical significance @xmath192 and @xmath193 ) for the two data sets .",
    "these results indicate that our chord model is not only conceptually simple but also seems to improve the accuracy slightly .",
    "a feature of our method is the simultaneous estimation of voice and note values .",
    "an alternative approach is to use a cascading algorithm that performs voice separation first and then estimates note values using the estimated voices . to examine the effectiveness of the joint estimation approach",
    ", we implemented a cascading algorithm consisting of voice separation using only the pitch part of the model in sec .",
    "[ sec : multivoicemodel ] and rhythm transcription using two note hmms with coupled tempos and compared it with the proposed method .",
    "the average rhythm correction rate by the cascading algorithm was 16.89% for the polyrhythmic data and",
    "9.67% for the non - polyrhythmic data . by comparing with the values in fig .",
    "[ fig : rhythmcorrectionrates ] , we see that the proposed method was slightly better and the differences are @xmath194 and @xmath195 ( statistical significance @xmath196 and @xmath197 ) for the two data sets .",
    "these results indicate the effectiveness of the joint estimation approach of the proposed method while the cascading algorithm may have practical importance because of its smaller computational cost .",
    "we also measured the accuracy of voice separation ( into two hands ) . the accuracy with the proposed model was 94.2% for the polyrhythmic data and 88.0% for the non - polyrhythmic data and with the cascading algorithm it was 93.8% and 92.5% .",
    "this indicates firstly that a similar ( or higher ) accuracy can be obtained by using only the pitch information and secondly that a higher accuracy of voice separation does not necessarily lead to a better rhythm recognition accuracy .",
    "in our implementation , parameters of the tempo variables ( mainly @xmath44 , @xmath50 and @xmath47 ) were not optimised but adjusted to values measured in a completely different experiment @xcite . since these parameters play important roles of describing ` naturalness ' of temporal fluctuations in music performance , we performed experiments to examine their influence .",
    "+   +    fig .",
    "[ fig : influence ] shows the results of measuring average rhythm correction rates for varying @xmath44 , @xmath50 and @xmath47 around the value used for our implementation .",
    "when one parameter was varied , the other parameters were fixed to the original values .",
    "results for the note hmm are also shown as references .",
    "we see that overall ( with some exceptions ) the parameters were optimal around the original values , which implies the universality of these parameters . for both models , we found values with a better accuracy ( at least for one of the data sets ) than the original values , suggesting the possibility of further optimisations .",
    "we see relatively large influence of @xmath50 on the merged - output hmm and @xmath47 on the note hmm .",
    "this can be explained by the fact that compared to the note hmm , the merged - output hmm must handle a more number of inter - note - cluster durations and a less number of chordal notes because of the presence of two voices .",
    "accordingly the @xmath50 , which controls the fluctuation of inter - note - cluster durations , has more chances and the @xmath47 , which controls the asynchrony of chordal notes , has less chances to influence the results of the merged - output hmm .",
    "finally we examined the effect of context - dependent @xmath65 described in sec .",
    "[ sec : multivoicemodel ] . for this purpose",
    "we simply run the proposed method with uniformly distributed @xmath65 ( @xmath118 ) .",
    "the average rhythm correction rate was slightly worse ( @xmath198 ) for the polyrhythmic data and slightly better ( @xmath199 ) for the non - polyrhythmic data .",
    "on the other hand , the accuracy of voice separation was 30.4% ( 50.0% ) for the polyrhythmic ( non - polyrhythmic ) data , which is much worse .",
    "the results confirm that the context - dependent @xmath65 is important to improve voice separation and provide yet another example that a more precise voice separation does not necessarily induce better rhythm recognition accuracy .",
    "we have described and examined a rhythm transcription method based on a merged - output hmm of polyphonic symbolic performance .",
    "this model has an internal structure consisting of multiple hmms to solve the long - standing problem of properly describing the multiple - voice structure of polyphonic music . with the inference method derived in this paper ,",
    "the algorithm can perform voice separation and note - value recognition simultaneously .",
    "the technique of deriving inference algorithms with reduced computational cost can be applied to other merged - output hmms with autoregressive voice hmms , which are expectedly effective models of polyphonic music where the multiple - voice structure is significant .    by examining the proposed method",
    ", we also confirmed that simultaneously inferring the voice and rhythm information improved the accuracy of rhythm transcription compared to a cascading approach , even though it did not necessarily improve the accuracy of voice separation . on the other hand ,",
    "transcribed results sometimes contained unwanted asynchrony between notes in different voices that have almost simultaneous notes onset times .",
    "this is because the model describes no information about the absolute onset time and there are no strong interactions between voices other than the shared tempo .",
    "the use of merged - output hmms with interacting voice hmms @xcite could provide a solution in principle , but how to describe synchrony of global score times while retaining computational tractability is a remaining problem .    with evaluations comparing seven rhythm transcription methods",
    ", we found that the proposed method performed significantly better than others for polyrhythmic performances .",
    "for non - polyrhythmic performances , we found that the note hmm and metrical hmm had the best accuracies and the proposed method was almost as good as ( but slightly worse than ) these methods .",
    "these results revealed the state - of - the - art methods for rhythm transcription that were different for the two kinds of data . while practically running two or more methods simultaneously and choosing the best result can be effective , developing a unified method that yields best results for both kinds of data",
    "is desired .",
    "solving the above problem of unwanted asynchrony would be one key and constructing a model with variable number of voices would be another .",
    "let us formulate the rhythm correction cost introduced in sec .  [",
    "sec : evaluationmethodology ] and derive an algorithm for calculating it .",
    "we first consider the monophonic case .",
    "let @xmath200 and @xmath201 be the correct and estimated note value of the @xmath6-th note length in the performance input ( @xmath202 ) .",
    "we consider a sequence of pairs @xmath203 of scaling factor @xmath204 and shift interval @xmath205 for @xmath202 .",
    "to recover @xmath206 from @xmath207 with the scaling and shift operations , we must have @xmath208 for @xmath209 .",
    "the number of scaling operations and that of shift operations are formally defined as @xmath210 and @xmath211 .",
    "the minimum number of editing operations @xmath161 is determined by minimising @xmath212 for all sequences @xmath213 satisfying eq .",
    "( [ eq : notevalueedition ] ) .",
    "this is a special case of a generalised rhythm correction cost , which can be defined similarly as the minimum of @xmath214 for some non - negative real numbers @xmath215 and @xmath216 .",
    "let us now present a dynamic programming to calculate the rhythm correction cost @xmath161 .",
    "we describe a general algorithm valid for any values of @xmath215 and @xmath216 .",
    "the algorithm can be derived in the same form as the viterbi algorithm for hmms .",
    "we define the ` state space ' @xmath217 as the set of all possible scaling operations , which can be constructed by taking ratios of all possible note values .",
    "the space @xmath217 is finite since the set of note values is finite .",
    "the scaling cost ( analogous to transition probability ) @xmath218 is defined as @xmath219 for the initial value @xmath220 the cost is defined as @xmath221 if @xmath222 and @xmath215 otherwise . to describe whether a shift operation is necessary for the @xmath6-th note value after scaling , the shift cost ( analogous to output probability )",
    "@xmath223 is defined as @xmath224 defining the total cost as @xmath225 ( we understand @xmath226 as @xmath227 ) , we have the relation @xmath228 the right - hand side of eq.([eq : rhythmcorrectioncostrelation ] ) can be calculated by the viterbi algorithm @xcite with computational complexity @xmath229 where @xmath98 is the number of note - value types .",
    "note that the above formulation is already valid in the presence of chords .",
    "chordal notes are represented as notes with @xmath230 .",
    "the error in clustering a chord , i.e. , @xmath231 but @xmath232 or vice versa , can be corrected by a shift operation .",
    "when there are separated multiple voices , we can apply shift operations on each note in each voice and scaling operations on all voices simultaneously .",
    "if the estimated score time duration between the first notes of any two voices is different from that in the correct score , it must be corrected as well .",
    "the rhythm correction cost for multiple voices can be calculated by the same manner as above using as @xmath233 a sequence of by merging all @xmath234 for all @xmath62 and @xmath235 and @xmath236 for all @xmath237 in the order of onset time , where @xmath238 is the score time of @xmath6-th note onset in voice @xmath62 as defined in eq .",
    "( [ eq : scoretime ] ) .",
    "we are grateful to david temperley , norihiro takamune and henkjan honing for providing their source codes .",
    "the author en  thanks hiroaki tanaka for useful discussions on merged - output hmm and yoshiaki",
    "bando for his help with running computer programs .",
    "this work is partially supported by jsps kakenhi nos .  24220006 , 26240025 , 26280089 , 26700020 , 15k16054 , 16h01744 and 16j05486 , and jst crestmuse , ongacrest and ongaaccel projects .",
    "e.  benetos , s.  dixon , d.  giannoulis , h.  kirchhoff and a.  klapuri , `` automatic music transcription : challenges and future directions , '' _ j. intelligent information systems _ , vol .",
    "41 , no .  3 , pp .",
    "407434 , 2013 .",
    "t.  otsuki , n.  saitou , m.  nakai , h.  shimodaira and s.  sagayama , `` musical rhythm recognition using hidden markov model ( in japanese ) , '' _ j.  information processing society of japan _ ,",
    "43 , no .  2 , pp .",
    "245255 , 2002 .",
    "m.  tsuchiya , k.  ochiai , h.  kameoka and s.  sagayama , `` probabilistic model of two - dimensional rhythm tree structure representation for automatic transcription of polyphonic midi signals , '' _ proc .",
    "apsipa _ , pp .  16 , 2013 .",
    "n.  takamune , h.  kameoka and s.  sagayama , `` automatic transcription from midi signals of music performance using 2-dimensional lr parser ( in japanese ) , '' _ tech .  rep .",
    "sigmus _ , vol .  2014-mus-104 , no .  7 , pp .",
    "16 , 2014 .",
    "e.  nakamura , k.  itoyama and k.  yoshii , `` rhythm transcription of midi performances based on hierarchical bayesian modelling of repetition and modification of musical note patterns , '' _ proc .",
    "eusipco _ , pp .  19461950 , 2016 .",
    "h.  heijink , l.  windsor and p.  desain , `` data processing in music performance research : using structural information to improve score - performance matching , '' _ behavior research methods , instruments , & computers _ , vol .",
    "32 , no .  4 , pp .",
    "546554 , 2000 .",
    "e.  nakamura , y.  saito , n.  ono and s.  sagayama , `` merged - output hidden markov model for score following of midi performance with ornaments , desynchronized voices , repeats and skips , '' _ proc .",
    "joint icmc@xmath239smc 2014 _ , pp .  11851192 , 2014 .",
    "h.  kameoka , k.  ochiai , m.  nakano , m.  tsuchiya and s.  sagayama , `` context - free 2d tree structure model of musical notes for bayesian modeling of polyphonic spectrograms , '' _ proc .",
    "ismir _ , pp .  307312 , 2012 .",
    "d.  conklin , `` representation and discovery of vertical patterns in music , '' in a.  smaill et al .",
    "( eds . ) , _ music and artificial intelligence _ , lecture notes in artificial intelligence , springer , pp .",
    "3242 , 2002 .                      eita nakamura he received a ph.d .",
    "degree in physics from the university of tokyo in 2012 .",
    "after having been a post - doctoral researcher at the national institute of informatics , meiji university and kyoto university , he is currently a jsps research fellow in the speech and audio processing group at kyoto university .",
    "his research interests include music modelling and analysis , music information processing and statistical machine learning .",
    "kazuyoshi yoshii he received the ph.d .",
    "degree in informatics from kyoto university , japan , in 2008 .",
    "he is currently a senior lecturer at kyoto university .",
    "his research interests include music signal processing and machine learning .",
    "he has received several awards including the ipsj yamashita sig research award and the best - in - class award of mirex 2005 .",
    "he is a member of the information processing society of japan and institute of electronics , information , and communication engineers .",
    "shigeki sagayama he received the b.e .",
    ", m.s . , and",
    "degrees from the university of tokyo , tokyo , japan , in 1972 , 1974 , and 1998 , respectively , all in mathematical engineering and information physics .",
    "he joined nippon telegraph and telephone public corporation ( currently , ntt ) in 1974 and started his career in speech analysis , synthesis , and recognition at ntt labs in musashino , japan . from 1990 , he was head of the speech processing department , atr interpreting telephony laboratories , kyoto , japan where he was in charge of an automatic speech translation project . in 1993 , he was responsible for speech recognition , synthesis , and dialog systems at ntt human interface laboratories , yokosuka , japan . in 1998 , he became a professor of the graduate school of information science , japan advanced institute of science and technology ( jaist ) , ishikawa .",
    "in 2000 , he was appointed professor at the graduate school of information science and technology ( formerly , graduate school of engineering ) , the university of tokyo .",
    "after his retirement from the university of tokyo , he is a professor of meiji university from 2014 .",
    "his major research interests include the processing and recognition of speech , music , acoustic signals , handwriting , and images .",
    "he was the leader of anthropomorphic spoken dialog agent project ( galatea project ) from 2000 to 2003 .",
    "sagayama received the national invention award from the institute of invention of japan in 1991 , the director general s award for research achievement from the science and technology agency of japan in 1996 , and other academic awards including paper awards from the institute of electronics , information and communications engineers , japan ( ieicej ) in 1996 and from the information processing society of japan ( ipsj ) in 1995 .",
    "he is a member of the acoustical society of japan , ieicej , and ipsj ."
  ],
  "abstract_text": [
    "<S> in a recent conference paper , we have reported a rhythm transcription method based on a merged - output hidden markov model ( hmm ) that explicitly describes the multiple - voice structure of polyphonic music . </S>",
    "<S> this model solves a major problem of conventional methods that could not properly describe the nature of multiple voices as in polyrhythmic scores or in the phenomenon of loose synchrony between voices . in this paper </S>",
    "<S> we present a complete description of the proposed model and develop an inference technique , which is valid for any merged - output hmms for which output probabilities depend on past events . </S>",
    "<S> we also examine the influence of the architecture and parameters of the method in terms of accuracies of rhythm transcription and voice separation and perform comparative evaluations with six other algorithms . using midi recordings of classical piano pieces </S>",
    "<S> , we found that the proposed model outperformed other methods by more than 12 points in the accuracy for polyrhythmic performances and performed almost as good as the best one for non - polyrhythmic performances . </S>",
    "<S> this reveals the state - of - the - art methods of rhythm transcription for the first time in the literature . </S>",
    "<S> publicly available source codes are also provided for future comparisons .    </S>",
    "<S> rhythm transcription , statistical music language model , model for polyphonic music scores , hidden markov models , music performance model . </S>"
  ]
}