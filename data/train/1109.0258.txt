{
  "article_text": [
    "this paper focuses on nonconvex _ composite objective _ problems having the form @xmath0 where @xmath1 is continuously differentiable and possibly nonconvex , @xmath2 is lower semi - continuous ( lsc ) and convex ( possibly nonsmooth ) , and @xmath3 is a compact convex set . we also make the common assumption that @xmath4 has a lipschitz continuous gradient within @xmath3 , written as @xmath5 ; that is , there is a constant @xmath6 such that @xmath7    problem   is a natural but far - reaching generalization of _ composite objective _ convex problems , which continue to enjoy tremendous importance in machine learning ; see e.g. ,  @xcite .",
    "although , convex formulations are extremely useful , for many difficult problems a nonconvex formulation is more natural .",
    "familiar examples include matrix factorization  @xcite , blind deconvolution  @xcite , dictionary learning  @xcite , and neural networks  @xcite .",
    "the primary contribution of this paper is theoretical and algorithmic . specifically , we present a new framework : nonconvex inexact proximal splitting ( nips ) .",
    "our framework solves   by `` splitting '' the task into smooth ( gradient ) and nonsmooth ( proximal ) parts . beyond splitting , the most notable feature of nipsis that it allows",
    "_ computational errors_. this capability proves critical to obtaining a scalable , incremental - gradient variant of nips , which , to our knowledge , is the first incremental nonconvex proximal - splitting method in the literature .    a further distinction of nipslies in how it models computational errors .",
    "notably , nips__does not _",
    "_ require the errors to vanish in the limit , a realistic assumption as often one has limited to no control over computational errors inherent to a complex system . in accord with the errors ,",
    "nipsalso _ does not _ require stepsizes ( learning rates ) to shrink to zero .",
    "in contrast , most incremental - gradient methods  @xcite and stochastic gradient algorithms  @xcite _ do assume _ that the computational errors and stepsizes decay to zero .",
    "we do not make these simplifying assumptions , which complicates the convergence analysis but results perhaps in a more satisfying description .",
    "our analysis builds on the remarkable work of  @xcite , who studied the simpler setting of _ differentiable _ nonconvex problems ( corresponding to the choice @xmath8 in  ) .",
    "nipsis strictly more general : unlike  @xcite it solves a _ non - differentiable _ problem by allowing a nonsmooth regularizer @xmath9 , which it tackles by invoking the fruitful idea of _ proximal - splitting",
    "_  @xcite .",
    "proximal splitting has proved to be exceptionally effective and practical  @xcite .",
    "it retains the simplicity of gradient - projection while handling the nonsmooth regularizer @xmath10 via its proximity operator .",
    "this style is especially attractive because for several important choices of @xmath10 , efficient implementations of the associated proximity operators exist  @xcite .",
    "for convex problems , an alternative to proximal splitting is the subgradient method ; similarly , for nonconvex problems one could use a generalized subgradient method  @xcite .",
    "however , as in the convex case , the use of subgradients has drawbacks : it fails to exploit the composite structure , and even when using sparsity promoting regularizers it does not generate intermediate sparse iterates  @xcite .    among batch nonconvex splitting methods ,",
    "an early paper is  @xcite .",
    "more recently , in his pioneering paper on convex composite minimization , @xcite also briefly discussed nonconvex problems .",
    "both  @xcite and  @xcite , however , enforced monotonic descent in the objective value to ensure convergence . very recently",
    ",  @xcite have introduced a generic method for nonconvex nonsmooth problems based on kurdyka - ojasiewicz theory , but their entire framework too hinges on descent .",
    "this insistence on descent makes these methods unsuitable for incremental , stochastic , or online variants , all of which usually lead to a nonmonotone sequence of objective values . among nonmonotonic methods that apply to  , we are aware of the generalized gradient - type algorithms of  @xcite and the stochastic generalized gradient methods of  @xcite . both methods , however , are analogous to the usual subgradient - based algorithms , and fail to exploit the composite objective structure .    but",
    "despite its desirability and potential benefits , proximal - splitting for exploiting composite objectives does not apply out - of - the - box to  : nonconvexity raises significant obstructions , especially because nonmonotonic descent in the objective function values is allowed . overcoming these obstructions to achieve",
    "a scalable non - descent based method is what makes the nipsframework novel .",
    "to simplify presentation , we replace @xmath10 by the penalty function @xmath11 where @xmath12 is the _ indicator function _ for @xmath3 : @xmath13 for @xmath14 , and @xmath15 for @xmath16 . with this notation ,",
    "we may rewrite   as the _ unconstrained _ problem : @xmath17 and this particular formulation is our primary focus .",
    "we solve   via a proximal - splitting approach , so let us begin by defining our most important component .",
    "[ def.prox ] let @xmath18 be an lsc , convex function .",
    "the _ proximity operator _ for @xmath19 , indexed by @xmath20 , is the nonlinear map  ( see e.g. , * ? ? ?",
    "* def .  1.22 ) : @xmath21    the operator   was introduced by  @xcite ( 1962 ) as a generalization of orthogonal projections .",
    "it is also key to rockafellar s classic _ proximal point algorithm _",
    "@xcite , and it arises in a host of _ proximal - splitting _",
    "methods  @xcite , most notably in _ forward - backward splitting _ ( fbs )  @xcite .",
    "fbs is particularly attractive because of its simplicity and algorithmic structure .",
    "it minimizes convex composite objective functions by alternating between `` forward '' ( gradient ) steps and `` backward '' ( proximal ) steps . formally , suppose @xmath4 in   is convex ; for such @xmath4 , fbs performs the iteration @xmath22 where @xmath23 is a suitable sequence of stepsizes .",
    "the usual convergence analysis of fbs is intimately tied to convexity of @xmath4 .",
    "therefore , to tackle nonconvex @xmath4 we must take a different approach .",
    "as previously mentioned , such approaches were considered by  @xcite and  @xcite , but both proved convergence by enforcing monotonic descent .",
    "this insistence on descent severely impedes scalability .",
    "thus , the key challenge is : _ how to retain the algorithmic simplicity of fbs and allow nonconvex losses , without sacrificing scalability ? _    we address this challenge by introducing the following _ inexact _ proximal - splitting iteration : @xmath24 where @xmath25 models the _ computational errors _ in computing the gradient @xmath26 .",
    "we also assume that for @xmath20 smaller than some stepsize @xmath27 , the computational error is uniformly _ bounded _ , that is , @xmath28 condition   is weaker than the typical vanishing error requirements @xmath29 which are stipulated by most analyses of methods with gradient errors  @xcite",
    ". obviously , since errors are nonvanishing , exact stationarity can not be guaranteed .",
    "we will , however , show that the iterates produced by   do progress towards reasonable _ inexact stationary points_. we note in passing that even if we assume the simpler case of vanishing errors , nipsis still the first nonconvex proximal - splitting framework that does not insist on monotonicity , which which complicates convergence analysis but ultimately proves crucial to scalability .    ' '' ''    @xmath30 ; select arbitrary @xmath31 compute approximate gradient @xmath32 update : @xmath33 @xmath34      we begin by characterizing inexact stationarity .",
    "a point @xmath35 is a stationary point for  ( [ eq.8 ] ) if and only if it satisfies the _ inclusion _ @xmath36 where @xmath37 denotes the clarke subdifferential  @xcite .",
    "a brief exercise shows that this inclusion may be equivalently recast as the fixed - point equation ( which augurs the idea of proximal - splitting ) @xmath38 this equation helps us define a measure of inexact stationarity : the _ proximal residual _",
    "@xmath39 note that for an exact stationary point @xmath35 the residual norm @xmath40 .",
    "thus , we call a point @xmath41 to be @xmath42-_stationary _ if for a prescribed error level @xmath43 , the corresponding residual norm satisfies @xmath44 assuming the error - level @xmath43 ( say if @xmath45 ) satisfies the bound  , we prove below that the iterates @xmath46 generated by  ( [ eq.main ] ) satisfy an approximate stationarity condition of the form  , by allowing the stepsize @xmath47 to become correspondingly small ( but strictly bounded away from zero ) . we start by recalling two basic facts , stated without proof as they are standard knowledge .    [ lem.lip ] let @xmath5 .",
    "then , @xmath48    [ lem.nex ] the operator @xmath49 is _ nonexpansive _ , that is , @xmath50    next we prove a crucial monotonicity property that actually subsumes similar results for projection operators derived by  ( * ? ? ?",
    "* lem .  1 ) , and may therefore be of independent interest .",
    "[ lem.mono ] let @xmath51 , and @xmath20 .",
    "define the functions @xmath52 then , @xmath53 is a decreasing function of @xmath47 , and @xmath54 an increasing function of @xmath47 .",
    "our proof exploits properties of moreau - envelopes  @xcite , and we present it in the language of proximity operators .",
    "consider the `` deflected '' proximal objective @xmath55 associate to objective @xmath56 the _ deflected moreau - envelope _ @xmath57",
    "whose infimum is attained at the unique point @xmath58 .",
    "thus , @xmath59 is differentiable , and its derivative is given by @xmath60 .",
    "since @xmath61 is convex in @xmath47 , @xmath62 is increasing ( ( * ? ? ?",
    "* thm .  2.26 ) ) , or equivalently @xmath63 is decreasing .",
    "similarly , define @xmath64 ; this function is concave in @xmath65 as it is a pointwise infimum ( indexed by @xmath41 ) of functions linear in @xmath65  ( see e.g. ,   3.2.3 in * ? ? ?",
    "thus , its derivative @xmath66 , is a decreasing function of @xmath65 .",
    "set @xmath67 to conclude the argument about @xmath54 .",
    "we now proceed to bound the difference between objective function values from iteration @xmath68 to @xmath69 , by developing a bound of the form @xmath70 obviously , since we do _ not _ enforce strict descent , @xmath71 may be negative too .",
    "however , we show that for sufficiently large @xmath68 the algorithm makes enough progress to ensure convergence .",
    "[ lem.descent ] let @xmath72 , @xmath73 , @xmath74 , and @xmath3 be as in  ( [ eq.main ] ) , and that @xmath75 holds .",
    "then , @xmath76    for the deflected moreau envelope  ( [ eq.18 ] ) , consider the directional derivative @xmath77 with respect to @xmath41 in the direction @xmath78 ; at @xmath79 , this derivative satisfies the optimality condition @xmath80 set @xmath81 , @xmath82 , and @xmath83 in  , and rearrange to obtain @xmath84 from lemma  [ lem.lip ] it follows that @xmath85 whereby upon adding and subtracting @xmath25 , and then using   we further obtain @xmath86 the second inequality above follows from convexity of @xmath19 , the third one from cauchy - schwarz , and the last one by assumption on @xmath87 .",
    "now flip signs and apply   to conclude the bound  .",
    "next we further bound   by deriving two - sided bounds on @xmath88 .",
    "[ lem.xubounds ] let @xmath72 , @xmath73 , and @xmath87 be as before ; also let @xmath89 and @xmath74 satisfy  . then , @xmath90    first observe that from lemma  [ lem.mono ] that for @xmath91 it holds that @xmath92 using  , the triangle inequality , and lemma  [ lem.nex ] , we have @xmath93 from   it follows that for sufficiently large @xmath68 we have @xmath94 .",
    "for the upper bound note that @xmath95    lemma  [ lem.descent ] and lemma  [ lem.xubounds ] help prove the following crucial corollary .",
    "[ corr.main ] let @xmath73 , @xmath72 , @xmath74 , and @xmath89 be as above and @xmath68 sufficiently large so that @xmath89 and @xmath74 satisfy  .",
    "then , @xmath96 holds with @xmath71 given by @xmath97    to simplify notation , we drop the superscripts .",
    "thus , let @xmath98 , @xmath99 , @xmath100 .",
    "then , we must show that @xmath101 where @xmath102 is given by @xmath103 where the constants @xmath104 , @xmath105 , and @xmath106 are defined as @xmath107 moreover , we also note that the scalars @xmath108 .",
    "recall that for sufficiently large @xmath68 , condition implies that @xmath109 which immediately implies that @xmath110 thus , we may replace   by the bound @xmath111 now plug in the two - sided bounds   on @xmath112 to obtain @xmath113 all that remains to show is that the respective coefficients of @xmath102 are positive . since @xmath114 and @xmath115 , the positivity of @xmath104 and @xmath105 is immediate . since @xmath116 ( see assumption  ) . reducing inequality @xmath117",
    ", shows that it holds as long as @xmath118 , which is obviously true since @xmath119 .",
    "thus , the three scalars @xmath120 defined by  ( [ eq.5 ] ) are all positive .",
    "we now have all the ingredients to state the main convergence theorem .",
    "[ thm.cvg ] let @xmath5 such that @xmath121 and let @xmath19 be lsc , convex on @xmath3 .",
    "let @xmath122 be a sequence generated by  ( [ eq.main ] ) , and let condition  ( [ eq.60 ] ) on each @xmath123 hold .",
    "there exists a limit point @xmath35 of the sequence @xmath46 , and a constant @xmath124 , such that @xmath125 .",
    "if @xmath126 converges , then for every limit point @xmath35 of @xmath46 it holds that @xmath125 .",
    "lemma  [ lem.descent ] , [ lem.xubounds ] , and corollary  [ corr.main ] have done all the hard work .",
    "indeed , they allow us to reduce our convergence proof to the case where the analysis of the differentiable case becomes applicable , and an appeal to the analysis of  ( * ? ? ?",
    "2.1 ) grants us our claim .",
    "theorem  [ thm.cvg ] says that we can obtain an approximate stationary point for which the norm of the residual is bounded by a linear function of the error level .",
    "the statement of the theorem is written in a conditional form , because nonvanishing errors @xmath127 prevent us from making a stronger statement . in particular , once the iterates enter a region where the residual norm falls below the error threshold , the behavior of @xmath46 may be arbitrary .",
    "this , however , is a small price to pay for having the added flexibility of nonvanishing errors . under the stronger assumption of vanishing errors ( and diminishing stepsizes )",
    ", we can also obtain guarantees to exact stationary points .",
    "we now apply nipsto the large - scale setting , where we have composite objectives of the form @xmath128 where each @xmath129 is a @xmath130 function . for simplicity ,",
    "we use @xmath131 in the sequel .",
    "it is well - known that for such decomposable objectives it can be advantageous to replace the full gradient @xmath132 by an _ incremental gradient _",
    "@xmath133 , where @xmath134 is some suitable index .",
    "nonconvex incremental methods for the differentiable case been extensively analyzed in the setting of backpropagation algorithms  @xcite , which corresponds to @xmath135",
    ". however , when @xmath136 , the only incremental methods that we are aware of are stochastic generalized gradient methods of  @xcite or the generalized gradient methods of  @xcite .",
    "as previously mentioned , both of these fail to exploit the composite structure of the objective function , a disadvantage even in the convex case  @xcite .    in stark contrast",
    ", we _ do _ exploit the composite structure of  .",
    "formally , we propose the following incremental nonconvex proximal - splitting iteration : @xmath137 where @xmath138 and @xmath139 are appropriate operators , different choices of which lead to different algorithms .",
    "for example , when @xmath140 , @xmath135 , @xmath141 , and @xmath142 , then   reduces to the classic incremental gradient method ( igm )  @xcite , and to the igm of  @xcite , if @xmath143 . if @xmath3 a closed convex set , @xmath135 , @xmath139 is orthogonal projection onto @xmath3 , @xmath144 , and @xmath142 , then iteration reduces to ( projected ) igm  @xcite . we may consider four variants of   in table  [ tab.var ] ; to our knowledge , all of these are new . which of the four variants one prefers depends on the complexity of the constraint set @xmath3 and cost to apply @xmath49 .",
    "the analysis of all four variants is similar , so we present details only for the most general case .",
    ".different variants of incremental nips  . [ cols=\"^,^,^,^,<,<\",options=\"header \" , ]",
    "we presented a new framework called nips , which solves a broad class of nonconvex composite objective problems .",
    "nipspermits nonvanishing computational errors , which can be practically useful .",
    "we specialized nipsto also obtain a scalable incremental version .",
    "our numerical experiments on large scale matrix factorization indicate that nipsis competitive with state - of - the - art methods .",
    "we conclude by mentioning that nipsincludes numerous other algorithms as special cases .",
    "for example , batch and incremental convex fbs , convex and nonconvex gradient projection , the proximal - point algorithm , among others .",
    "theoretically , however , the most exciting open problem resulting from this paper is : _ extend nipsin a scalable way when even the nonsmooth part is nonconvex_. this case will require very different convergence analysis , and is left to the future .",
    "33 [ 1]#1 [ 1]`#1 ` urlstyle [ 1]doi : # 1    h.  attouch , j.  bolte , and b.  f. svaiter .",
    "convergence of descent methods for semi - algebraic and tame problems : proximal algorithms , forward - backward splitting , and regularized gauss - seidel methods .",
    "programming series a _ , aug .",
    "online first .",
    "f.  bach , r.  jenatton , j.  mairal , and g.  obozinski .",
    "convex optimization with sparsity - inducing norms . in s.",
    "sra , s.  nowozin , and s.  j. wright , editors , _",
    "optimization for machine learning_. mit press , 2011 .",
    "a.  beck and m.  teboulle . .",
    "_ siam j. imgaging sciences _ , 20 ( 1):0 183202 , 2009 .",
    "d.  p. bertsekas .",
    "_ nonlinear programming_. athena scientific , second edition , 1999 .",
    "d.  p. bertsekas . .",
    "technical report lids - p-2848 , mit , august 2010 .",
    "s.  boyd and l.  vandenberghe .",
    "_ convex optimization_. cambridge university press , march 2004 .",
    "f.  h. clarke . _",
    "optimization and nonsmooth analysis_. john wiley & sons , inc . , 1983 .",
    "p.  l. combettes and j.  pesquet . .",
    "_ arxiv:0912.3522v4 _ , may 2010 .",
    "l. combettes and v.  r. wajs .",
    "signal recovery by proximal forward - backward splitting .",
    "_ multiscale modeling and simulation _ , 40 ( 4):0 11681200 , 2005 .    t.  a. davis and y.  hu . .",
    "_ acm transactions on mathematical software _ , 2011 . to appear",
    "j.  duchi and y.  singer . .",
    "_ j. mach . learning res .",
    "( jmlr ) _ , sep .",
    "y.  m. ermoliev and v.  i. norkin .",
    "stochastic generalized gradient method for nonconvex nonsmooth stochastic optimization .",
    "_ cybernetics and systems analysis _ , 34:0 196215 , 1998 .",
    "m.  fukushima and h.  mine .",
    "a generalized proximal point algorithm for certain non - convex minimization problems .",
    "j. systems science _ , 120 ( 8):0 9891000 , 1981 .",
    "e.  m. gafni and d.  p. bertsekas .",
    "two - metric projection methods for constrained optimization .",
    "_ siam journal on control and optimization _",
    ", 220 ( 6):0 936964 , 1984 .    a.  a. gaivoronski .",
    "convergence properties of backpropagation for neural nets via theory of stochastic gradient methods .",
    "_ optimization methods and software _ , 40 ( 2):0 117134 , 1994 .    s.  haykin . _",
    "neural networks : a comprehensive foundation_. prentice hall ptr , 1st edition , 1994 .",
    "k.  kreutz - delgado , j.  f. murray , b.  d. rao , k.  engan , t .- w .",
    "lee , and t.  j. sejnowski .",
    "dictionary learning algorithms for sparse representation . _ neural computation _ , 15:0 349396 , 2003 .",
    "d.  kundur and d.  hatzinakos .",
    "blind image deconvolution .",
    "_ ieee signal processing magazine _ , 130 ( 3 ) , may 1996 .",
    "d.  d. lee and h.  s. seung . .",
    "in _ nips _ , 2000 .",
    "k.  lee , j.  ho , and d.  kriegman .",
    "acquiring linear subspaces for face recognition under variable lighting .",
    "_ ieee trans .",
    "pattern anal .",
    "intelligence _ , 270 ( 5):0 684698 , 2005 .    j.  liu and j.  ye . . in _",
    "icml _ , jun .",
    "j.  mairal , f.  bach , j.  ponce , and g.  sapiro . .",
    "_ jmlr _ , 11:0 1060 , 2010 .",
    "j.  j. moreau .",
    "fonctions convexes duales et points proximaux dans un espace hilbertien . _ c. r. acad .",
    "paris sr . a math .",
    "_ , 255:0 28972899 , 1962 .",
    "y.  nesterov .",
    "_ introductory lectures on convex optimization : a basic course_. springer , 2004 .",
    "y.  nesterov .",
    "gradient methods for minimizing composite objective function .",
    "technical report 2007/76 , universit catholique de louvain , sept . 2007 .",
    "r.  t. rockafellar .",
    "monotone operators and the proximal point algorithm .",
    "siam j. control and optimization _ , 14 , 1976 .",
    "r.  t. rockafellar and r.  j .- b",
    "_ variational analysis_. springer , 1998",
    ".    m.  v. solodov .",
    "convergence analysis of perturbed feasible descent methods .",
    "_ j. optimization theory and applications _ , 930 ( 2):0 337353 , 1997 .    m.  v. solodov .",
    "incremental gradient algorithms with stepsizes bounded away from zero . _",
    "computational optimization and applications _ , 11:0 2335 , 1998 .",
    "m.  v. solodov and s.  k. zavriev .",
    "error stability properties of generalized gradient - type algorithms .",
    "_ j. optimization theory and applications _ , 980 ( 3):0 663680 , 1998 .",
    "nonconvex proximal - splitting : batch and incremental algorithms .",
    "_ arxiv:1109.0258 _ , sep .",
    "sung . _ learning and example selection for object and pattern recognition_. phd thesis , mit , 1996 .",
    "l.  xiao . .",
    "in _ nips _ , 2009 .",
    "if @xmath3 is the nonnegative orthant @xmath145 , then the proximity operator @xmath146 often simplifies as @xmath147 additionally , if @xmath148 is an elementwise separable function , then one can easily admit a box - plus - hyperplane constraint set @xmath3 of the form @xmath149 for more general constraint sets , we can invoke _ dykstra splitting _",
    "@xcite , which solves the problem @xmath150 by using the following algorithm"
  ],
  "abstract_text": [
    "<S> we study a class of large - scale , nonsmooth , and nonconvex optimization problems . </S>",
    "<S> in particular , we focus on nonconvex problems with _ composite _ objectives . </S>",
    "<S> this class includes the extensively studied convex , composite objective problems as a subclass . to solve composite nonconvex problems we introduce a powerful new framework based on asymptotically _ nonvanishing _ errors , avoiding the common stronger assumption of vanishing errors . within our new framework </S>",
    "<S> we derive both batch and incremental proximal splitting algorithms . to our knowledge , </S>",
    "<S> our work is first to develop and analyze incremental _ nonconvex _ proximal - splitting algorithms , even if we were to disregard the ability to handle nonvanishing errors . </S>",
    "<S> we illustrate one instance of our general framework by showing an application to large - scale nonsmooth matrix factorization . </S>"
  ]
}