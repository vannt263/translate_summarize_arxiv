{
  "article_text": [
    "the present paper addresses a constrained optimization problem , subsuming and extending optimization tasks which arise in various areas of applications such as ( i ) low - rank tensor approximation problems from signal processing and data compression , ( ii ) geometric measures of pure state entanglement from quantum computing , ( iii ) subspace reconstruction problems from image processing and ( iv ) combinatorial problems .",
    "the problem can be stated as follows : given a collection of integer pairs @xmath2 with @xmath3 for @xmath4 and a hermitian @xmath5 matrix @xmath6 with @xmath7 , find the global maximizer of the trace function @xmath8 . here",
    ", @xmath9 is restricted to the set of all hermitian projectors @xmath10 of rank @xmath11 , which can be represented as a tensor product @xmath12 of hermitian projectors @xmath13 of rank @xmath14 .",
    "thus , one is faced with the constrained optimization task @xmath15 where @xmath1 denotes the set of all hermitian projectors of the above tensor type and @xmath16 is a shortcut for @xmath17 .",
    "we will see that it makes sense to call the above objective function @xmath18 the _ generalized rayleigh - quotient _ of @xmath6 with respect to the partitioning @xmath16 .    to the best of the authors knowledge , problem ( [ grq1 ] )",
    "has not been discussed in the literature in this general setting . however , depending on the structure of @xmath6 as well as on the choice of @xmath16 , problem ( [ grq1 ] ) relates to well - known numerical linear algebra issues :    \\(i ) for hermitian matrices of rank-@xmath19 , i.e. @xmath20 , it reduces to a best low - rank approximation problem for the tensor @xmath21 which satisfies @xmath22 , cf .  @xcite .",
    "classical application areas of such low - rank approximations can be found in statistics , signal processing and data compression @xcite .",
    "\\(ii ) a recent application in quantum computing plays a central role in characterizing and quantifying pure state entanglement . here , the distance of a pure state ( tensor ) to the set of all product states ( rank-@xmath19 tensors ) provides a geometric measure for entanglement @xcite .",
    "\\(iii ) moreover , the challenging task of recovering subspaces of possibly different dimensions from noisy data  known as subspace detection or subspace clustering problem in computer vision and image processing @xcite  can also be cast into the above setting .",
    "more precisely , for an appropriately chosen hermitian matrix @xmath6 the subspace clustering task can be characterized by problem ( [ grq1 ] ) in the sense that for unperturbed data the global minima of the generalized rayleigh - quotient are in unique correspondence with the sought subspaces .",
    "numerical experiments in section 4 support that even for noisy data the proposed optimization yields reliable approximations of the unperturbed subspaces .",
    "\\(iv ) in @xcite a certain class of combinatorial problems are recast as optimization problems for trace functions on the special unitary group . for the case",
    "when @xmath6 is a diagonal matrix , optimization task is a generalization of the applications mentioned in @xcite .",
    "our solution to problem ( @xmath23 ) is based on the fact that the constraint set @xmath24 can be equipped with a riemannian submanifold structure .",
    "this admits the use of techniques from riemannian optimization  a rather new approach towards constrained optimization exploiting the geometrical structure of the constraint set in order to develop numerical algorithms @xcite .",
    "in particular , we pursue two approaches : a newton and a conjugated gradient method .    on a riemannian manifold , the intrinsic newton method is usually described by means of the levi - civita connection , performing iterations along geodesics , see @xcite .",
    "a more general approach via local coordinates was initiated by shub in @xcite and further discussed in @xcite . here ,",
    "we follow the ideas in @xcite and use a pair of local parametrizations  normal coordinates for the push - forward and qr - type coordinates for the pull - back  satisfying an additional compatibility condition to preserve quadratic convergence .",
    "thus we obtain an intrinsically defined version of the classical newton algorithm with some computational flexibility",
    ". nevertheless , for high - dimensional problems its iterations are expensive , both in terms of computational complexity and memory requirements .",
    "therefore , we alternatively propose a conjugated gradient method , which has the advantage of algorithmic simplicity at a satisfactory convergence rate . in doing so",
    ", we suggest to replace the global line - search of the classical conjugated gradient method by a one - dimensional newton - step , which yields a better convergence behavior near stationary points than the commonly used armijo - rule .",
    "as mentioned earlier , depending on the structure of @xmath6 , the above - specified problems ( i ) , ( ii ) , ( iii ) and ( iv ) are particular cases of the optimization task ( @xmath23 ) . for the best low - rank approximation of a tensor",
    "the standard numerical approach is an alternating least - squares algorithm , known as higher - order orthogonal iteration ( hooi ) @xcite .",
    "recently , several new methods also exploiting the geometric structure of the problem have been published .",
    "newton algorithms have been proposed in @xcite , quasi - newton methods in @xcite , conjugated gradient and trust region methods in @xcite . for high - dimensional tensors , all riemannian newton algorithms manifest similar problems : too high computational complexity and memory requirements .",
    "our conjugated gradient method is however , a good candidate to solve large scale problems .",
    "it exhibits locally a good convergence behavior , comparable to that of the quasi - newton methods in @xcite at much lower computational costs , which considerably reduces the necessary cpu time .    for the problem of estimating a mixture of linear subspaces from sampled data points , cf .",
    "( iii ) , our numerical approach is an efficient alternative to the classical ones : _ ad - hoc _ type methods such as k - subspace algorithms @xcite , or probabilistic methods using a maximum likelihood framework for the estimation @xcite .",
    "the paper is organized as follows . in section 2",
    ", we familiarize the reader with the basic ingredients of riemannian optimization . in particular , we address the following topics : the riemannian submanifold structure of the constraint set @xmath1 , its isometry to the @xmath25-fold cartesian product of grassmannians , geodesics and parallel transport and the computation of the intrinsic gradient and hessian for smooth objective functions .",
    "section 3 is dedicated to the problem of optimizing the generalized rayleigh - quotient @xmath0 , including also a detailed discussion on its relation to problems",
    "( i ) , ( ii ) , ( iii ) and ( iv ) . moreover ,",
    "an analogy to the classical rayleigh - quotient is also the subject of this section .",
    "we compute the gradient and the hessian of the generalized rayleigh - quotient and derive critical point conditions .",
    "we end the section with a result on the generic non - degeneracy of its critical points . in section 4 , a newton - like and a conjugated gradient algorithm as well as numerical simulations tailored to the previously mentioned applications are given .",
    "we start our study on the optimization task ( @xmath23 ) with a brief summary on the necessary notations and basic concepts .",
    "let @xmath26 be the set of all hermitian @xmath27 matrices @xmath6 , i.e. @xmath28 , where @xmath29 refers to the conjugate transpose of @xmath30 moreover , let @xmath31 be the lie group of all special unitary matrices and @xmath32 its lie - algebra , i.e. @xmath33 if and only if @xmath34 and , respectively , @xmath35 if and only if @xmath36 and @xmath37 the _ grassmannian _ , @xmath38 is the set of all rank @xmath39 hermitian projection operators of @xmath40 .",
    "it is a smooth and compact submanifold of @xmath26 with real dimension @xmath41 , whose tangent space at @xmath42 is given by @xmath43:=p\\omega-\\omega p\\;|\\;\\omega\\in\\mathfrak{su}_{n}\\},\\ ] ] cf .",
    "hence , every element @xmath44 and every tangent vector @xmath45 can be written as @xmath46 where @xmath47 is the _ standard projector _ of rank @xmath39 acting on @xmath40 and @xmath48 denotes a tangent vector in the corresponding tangent space , i.e. @xmath49 , & \\zeta_{m , n}=\\left[\\begin{array}{cc } 0 & z\\\\ z^{\\dagger } & 0 \\end{array}\\right],\\;\\ ; z\\in{{\\mathbb c}}^{m\\times ( n - m)}. \\end{array}\\ ] ] whenever the values of @xmath39 and @xmath50 are clear from the context , we will use the shortcuts @xmath51 and @xmath52 .",
    "with respect to the riemannian metric induced by the frobenius inner product of @xmath26 , the grassmannian @xmath53 is a riemannian submanifold and the unique orthogonal projector onto @xmath54 is given by @xmath55,\\;\\;\\ ; x\\in\\mathfrak{her}_n.\\ ] ]    we define the _",
    "@xmath56fold tensor product of grassmannians _",
    "@xmath57 as the set @xmath58 of all rank-@xmath59 hermitian projectors of @xmath60 with @xmath11 and @xmath61 , which can be represented as a kronecker product @xmath62 here , @xmath16 stands for the multi index @xmath63 then , @xmath64 can be naturally equipped with a submanifold structure as the following result shows .",
    "the @xmath56fold tensor product of grassmannians @xmath64 is a smooth and compact submanifold of @xmath65 of real dimension @xmath66 .",
    "we consider the following smooth action @xmath67 of the compact lie group @xmath68 let @xmath69 be of the form @xmath70 where @xmath71 denotes the standard projector in @xmath72 then , the orbit @xmath73 of @xmath74 coincides with @xmath64 . by @xcite ( pp .",
    "4446 ) we conclude that the @xmath56fold tensor product of grassmannians is a smooth and compact submanifold of @xmath65 .",
    "moreover , @xmath75 , where the stabilizer subgroup of @xmath74 is given by @xmath76     & = \\{{\\boldsymbol \\theta}\\in\\mathrm{su}({\\bf n})\\;|\\ ; \\theta_j\\pi_j\\theta_j^{\\dagger}=\\pi_j,\\ ; j=1,\\dots , r\\}.   \\end{array}\\ ] ] it follows easily that the dimension of @xmath77 is @xmath78 $ ] and therefore , @xmath79 is the dimension of the @xmath56fold tensor product of grassmannians .",
    "@xmath80 let @xmath81 denote the tensor product of finite dimensional vector spaces @xmath82 and @xmath83 , cf .",
    "@xcite and let @xmath84 be the tensor product of @xmath85 and @xmath86 given by @xmath87 for all @xmath88 and @xmath89 moreover , let @xmath90 and @xmath91 be bases of @xmath82 and @xmath83 , respectively . then , the matrix representation of @xmath92 with respect to the product basis @xmath93 of @xmath81 is given by the kronecker product of the matrix representations of @xmath6 and @xmath94 with respect to @xmath90 and @xmath91 .",
    "this clarifies the relation between the `` abstract '' tensor product of linear maps and the kronecker product of matrices and justifies the term `` tensor product '' of grassmannians when we refer to @xmath1 .",
    "+ @xmath95 it is a well - known fact that the grassmannian @xmath53 is diffeomorphic to the grassmann manifold @xmath96 of all @xmath97dimensional subspaces of @xmath40 , cf.@xcite . therefore , @xmath98 is diffeomorphic to @xmath99 where @xmath100 and @xmath101 + both items ( a ) and ( b ) readily generalize to an arbitrary number of grassmannians .    we conclude this subsection by pointing out an isometry between the @xmath56fold tensor product of grassmannians @xmath1 and the _ direct @xmath56fold product of grassmannians",
    "_ @xmath102 the vector spaces @xmath65 and @xmath103 endowed with the inner products @xmath104 and @xmath105 induce a riemannian submanifold structure on @xmath106 and @xmath107 , respectively .    [ diffeo ] the map @xmath108is a diffeomorphism between @xmath109 and @xmath110 moreover , @xmath111 is a global riemannian isometry when the right - hand side of ( [ innerprod3 ] ) is replaced by @xmath112 with @xmath113 for @xmath114    note that the isometry between @xmath115 and @xmath1 is very special , as in general the map @xmath116 fails even to be injective . for the proof of proposition @xmath117",
    "we refer to the appendix .",
    "it is well - known that every riemannian manifold @xmath118 carries a unique _ riemannian _ or _ levi - civita connection _",
    "@xmath119 , e.g.  @xcite . by means of @xmath119",
    ", one defines parallel transport and geodesics as follows .",
    "let @xmath120 be a vector field along a curve @xmath121 on @xmath118 , i.e.  @xmath122 for all @xmath123 .",
    "then , @xmath124 is defined to be _ parallel along _",
    "@xmath121 if @xmath125 for all @xmath126 .",
    "given @xmath127 , there exists a unique parallel vector field @xmath124 along @xmath121 such that @xmath128 and the vector @xmath129 is called the _ parallel transport _ of @xmath130 to @xmath131 along @xmath132 in particular , @xmath121 is called a _ geodesic _ on @xmath118 , if @xmath133 is parallel along @xmath121 . for the grassmann manifold @xmath53",
    ", the curve @xmath134}pe^{t[\\xi , p]}$ ] describes the geodesic through @xmath44 in direction @xmath135 , i.e. @xmath136 satisfies equation with initial conditions @xmath137 and @xmath138 .",
    "similarly , it can be verified that the parallel transport of @xmath139 to @xmath140 along the geodesic through @xmath42 in direction @xmath130 is given by @xmath141}\\eta e^{t[\\xi , p]}$ ] .",
    "these notions can be straight - forward generalized to the direct product of grassmannians @xmath115 .",
    "first , let us recall that the _ riemannian gradient _ at @xmath142 of a smooth objective function @xmath143 on a riemannian manifold @xmath118 is defined as the unique tangent vector @xmath144 satisfying @xmath145 @xmath146 , where @xmath147 denotes the differential ( tangent map ) of @xmath148 at @xmath42 . moreover , if @xmath119 is the _ levi - civita connection _ on @xmath118 , then the _ riemannian hessian _ of @xmath148 at @xmath42 is the linear map @xmath149 defined by @xmath150 for all @xmath151 now , if @xmath118 is a submanifold of a vector space @xmath82 , then and simplify as follows .",
    "let @xmath152 and @xmath153 be smooth extensions of @xmath148 and of the vector field @xmath154 , respectively .",
    "then , @xmath155 where @xmath156 is the orthogonal projection onto @xmath157 and @xmath158 denotes the standard gradient of @xmath152 on @xmath82 .    for the generalized rayleigh - quotient @xmath0 on @xmath115 , explicit formulas of the gradient and hessian",
    "will be given in section 3.3 .",
    "let @xmath1 be the @xmath56fold tensor product of grassmannians with @xmath16 as in ( @xmath159 ) and let @xmath160 @xmath161 in the following , we analyze the constrained optimization problem @xmath162 which comprises problems from different areas , such as _ multilinear low - rank approximations of a tensor _ , _ geometric measures of entanglement _ , _ subspace clustering _ and _ combinatorial optimization_. these applications are naturally stated on a tensor product space .",
    "however , for the special case of the grassmann manifold they can be reformulated on a direct product space . to this purpose , we define the _ generalized rayleigh - quotient _ of the matrix @xmath6 as @xmath163 based on the isometry between @xmath1 and @xmath164 we can rewrite problem ( @xmath165 ) as an optimization task for @xmath0 @xmath166 in general this is not the case , as we have already pointed out in .    the term",
    " generalized rayleigh - quotient \" is justified , since for @xmath167 we obtain the classical rayleigh - quotient @xmath168 in the sequel we want to point out some similarities and differences between the generalized and the classical rayleigh - quotient .",
    "it is well known that under the assumption that there is a spectral gap between the eigenvalues of @xmath169 , there is a unique maximizer and a unique minimizer of the classical rayleigh - quotient of @xmath6 . unfortunately , this is no longer the case for the generalized rayleigh - quotient @xmath0 .",
    "global maximizers and global minimizers exist since the generalized rayleigh - quotient is defined on a compact manifold , but unlike the classical case , it admits also local extrema as the following example shows . for the case",
    "when @xmath6 is of rank one we refer to example 3 in @xcite .",
    "[ ex ] let @xmath170 be a diagonal matrix with @xmath171 and @xmath172 of the form @xmath173 & \\textrm{and } \\;\\ ; p_2^*=\\left[\\begin{array}{cc } 0 & 0\\\\ 0 & 1 \\end{array}\\right ] .",
    "\\end{array}\\ ] ] the maximum of @xmath0 is obvious less or equal to @xmath174 . since @xmath175 , we have @xmath176 as the global maximizer of @xmath0 . from it",
    "follows that all @xmath177 with @xmath178 and @xmath179 diagonal , are critical points of @xmath0 .",
    "in particular @xmath180 is a critical point of @xmath0 with @xmath181 .",
    "moreover , one can check by computing the hessian of @xmath0 at @xmath180 , see , that @xmath180 is actually a local maximizer of @xmath0 .",
    "comparative to the classical rayleigh - quotient , this strange behavior results from the fact that not all @xmath182 permutation matrices are of the form @xmath183 , with @xmath184 .    while for the classical rayleigh - quotient one knows that the maximizer and minimizer are orthogonal projectors onto the space spanned by the eigenvectors corresponding to the largest and , respectively , smallest eigenvalues of @xmath6 , it is difficult to provide an analog characterization for the global extrema of the generalized rayleigh - quotient for an arbitrary matrix @xmath6 .",
    "but , for particular @xmath6 and @xmath25 such a characterization is possible . + * ( a ) * if @xmath185 and @xmath6 is of rank one , i.e. @xmath186 , with @xmath187 , then the generalized rayleigh - quotient can be rewritten as @xmath188 = { \\operatorname{tr}}(y^{\\dagger}p_1yp_2).\\ ] ] under the assumption that @xmath189 has full rank and distinct singular values there exist one maximizer and one minimizer .",
    "the maximizer @xmath190 of @xmath0 is given by the orthogonal projectors onto the space spanned by the @xmath191 left , respective right singular vectors corresponding to the largest @xmath192 singular values .",
    "similar for the minimizer , the singular vectors corresponding to the smallest @xmath192 singular values .",
    "+ * ( b ) * if @xmath25 is arbitrary and @xmath6 diagonalizable via a transformation of @xmath193 , then we can assume without loss of generality that @xmath6 is diagonal .",
    "moreover , if @xmath6 can be written as @xmath194 diagonal , which is always the case when @xmath195 , @xmath196 , then the generalized rayleigh - quotient becomes a product of @xmath25 decoupled classical rayleigh - quotients .",
    "hence , there is one maximizer and one minimizer .",
    "however , there is a dramatic change if @xmath6 can not be written as a kronecker product of diagonal matrices . in this case",
    "@xmath0 has also local extrema , as example [ ex ] shows . from one can immediately formulate the following critical point characterization .",
    "[ prop_diag ] let @xmath197 be diagonal .",
    "then , @xmath198 is a critical point of @xmath0 if and only if @xmath199 are permutations of the standard projectors @xmath71 , for all @xmath200 .",
    "there is a wide range of applications for problem ( @xmath201 ) in areas such as signal processing , computer vision and quantum information .",
    "we briefly illustrate the broad potential of ( @xmath201 ) by four examples .",
    "the problem of best approximation of a tensor by a tensor of lower rank is important in areas such as statistics , signal processing and pattern recognition . unlike in the matrix case",
    ", there are several rank concepts for a higher order tensor , @xcite . for the scope of this paper ,",
    "we focus on the _ multilinear rank _ case .",
    "a finite dimensional complex tensor @xmath203 of order @xmath25 is an element of a tensor product @xmath204 , where @xmath205 are complex vector spaces with @xmath206 such an element can have various representations , a common one is the description as an @xmath56way array , i.e.  after a choice of bases for @xmath205 , the tensor @xmath203 is identified with @xmath207_{i_1=1 , \\dots , i_r=1}^{n_1 ,   \\dots , n_r}\\in{{\\mathbb c}}^{n_1\\times n_2\\times\\dots\\times n_r}$ ] , see e.g.  @xcite .",
    "the @xmath208th way of the array is referred to as the @xmath208th _ mode _ of @xmath203 .",
    "a matrix @xmath209 acts on a tensor @xmath21 via _ mode@xmath210 multiplication _",
    "@xmath211 , i.e. @xmath212 cf .  @xcite .",
    "it is always possible to rearrange the elements of @xmath203 along one or , more general , several modes such that they form a matrix .",
    "let @xmath213 and @xmath214 be ordered subsets of @xmath215 such that @xmath216 . moreover , consider the products @xmath217 for @xmath218 and @xmath219 , respectively .",
    "then , the _ matrix unfolding _ of @xmath203 along @xmath220 is a matrix @xmath221 of size @xmath222 such that the element in position @xmath223 of @xmath203 moves to position @xmath224 in @xmath221 , where @xmath225 as an example , for a third order tensor @xmath226 we obtain the following matrix unfoldings as in @xcite @xmath227 , & a_{(2)}=\\left[\\begin{array}{cccc }   a_{111 } & a_{112 } & a_{211 } & a_{212}\\\\   a_{121 } & a_{122 } & a_{221 } & a_{222 }   \\end{array }   \\right ] ,   \\end{array}\\\\[3 mm ]   a_{(3)}=\\left[\\begin{array}{cccc }   a_{111 } & a_{121 } & a_{211 } & a_{221}\\\\   a_{112 } & a_{122 } & a_{212 } & a_{222 }   \\end{array }   \\right ] .",
    "\\end{array}\\ ] ] the _ multilinear rank _ of @xmath228 is the @xmath56tuple @xmath229 such that @xmath230 to refer to the multilinear rank of @xmath203 we will use the notation rank-@xmath202 or @xmath231 given a tensor @xmath232 we are interested in finding the best rank-@xmath233 approximation of @xmath203 , i.e. @xmath234 here , @xmath235 is the frobenius norm of a tensor , i.e. @xmath236 with @xmath237 here , @xmath238 refers to the matrix unfolding @xmath239 . in the matrix case ,",
    "the solution of the optimization problem ( @xmath240 ) is given by a truncated svd , cf .",
    "eckart - young theorem @xcite .",
    "however , for the higher - order case , there is no equivalent of the eckart - young theorem . according to the tucker decomposition @xcite or the higher order singular value decomposition ( hosvd ) @xcite",
    ", any rank-@xmath202 tensor can be written as a product of a _ core _ tensor @xmath241 and @xmath25 stiefel matrices @xmath242 , i.e. @xmath243 thus , solving ( @xmath240 ) is equivalent to solving the maximization problem @xmath244 with @xmath245 , see e.g.  @xcite . using @xmath246operation and kronecker product language ,",
    "one has @xmath247 according to and the properties of the _ trace _ function , the best multilinear rank-@xmath202 approximation problem becomes @xmath248 with @xmath249 and @xmath250      the task of characterizing and quantifying entanglement is a central theme in quantum information theory .",
    "there exist various ways to measure the difference between entangled and product states . here",
    ", we discuss a geometric measure of entanglement , which is given by the euclidean distance of @xmath251 with @xmath252 to the set of all product states @xmath253 , i.e. @xmath254 since any minimizer of @xmath255 is also a maximizer of @xmath256 and vice versa , computing the entanglement measure ( [ eq12 ] ) is equivalent to solving @xmath257 with @xmath258 and @xmath259 note that actually constitutes a best rank@xmath260 tensor approximation problem @xcite .",
    "subspace segmentation is a fundamental problem in many applications in computer vision ( e.g. image segmentation ) and image processing ( e.g. image representation and compression ) .",
    "the problem of clustering data lying on multiple subspaces of different dimensions can be stated as follows :    given a set of data points @xmath261 which lie approximately in @xmath262 distinct subspaces @xmath263 of dimension @xmath264 , identify the subspaces @xmath263 without knowing in advance which points belong to which subspace .",
    "every @xmath265 dimensional subspace @xmath266 can be defined as the kernel of a rank @xmath267 orthogonal projector @xmath268 of @xmath269 with @xmath270 as @xmath271 therefore , any point @xmath272 satisfies @xmath273 which is equivalent to @xmath274 thus , the problem of recovering the subspaces @xmath263 from the data points @xmath74 can be treated as the following optimization task : @xmath275 with @xmath276 and @xmath277 we mention that here we have used the same notation @xmath107 to refer to the direct @xmath56fold product of _ real _ grassmannians .    for best multilinear rank tensor approximation and subspace clustering applications ,",
    "numerical experiments are presented at the end of section 4 .",
    "let @xmath278 be a given array of positive real numbers and let @xmath279 be fixed .",
    "find @xmath280 columns and @xmath281 rows such that the sum of the corresponding entries @xmath282 is maximal , i.e. solve the combinatorial maximization problem @xmath283 we can permute @xmath280 columns and @xmath281 rows of @xmath284 by right and left multiplication with permutations of the standard projectors @xmath285 and @xmath286 respectively .",
    "hence , problem is solved by finding permutation matrices @xmath287 and @xmath288 which maximize : @xmath289 where @xmath290 is the sum over all entries and @xmath291 .",
    "the sum in can be written as @xmath292 where @xmath293 .",
    "the last equality in holds since @xmath294 is diagonal , too .",
    "according to proposition [ prop_diag ] , we have the following equivalence @xmath295 hence , we can embed the combinatorial maximization problem into our continuous optimization task .",
    "the generalization of to @xmath284 being an arbitrary multi - array is straight - forward .",
    "problems of this type arise in multi - decision processes such as the following .",
    "assume that a company has @xmath296 branches and each branch produces @xmath297 goods .",
    "if @xmath282 denotes the gain of the @xmath208th branch with the @xmath298th good , then one could be interested to reduce the number of producers and goods to @xmath280 and @xmath281 , respectively , which give maximum benefit .",
    "we continue our investigation of problem ( @xmath201 ) by computing the gradient and the hessian of @xmath0 . in the following lemma we establish multilinear maps @xmath299 , which will enable us to derive clear expressions for the gradient and the hessian of @xmath300    [ lemma1 ] let",
    "@xmath169 and @xmath301 then , for all @xmath302 there exists a unique map @xmath303 such that @xmath304 holds for all @xmath305 in particular , one has @xmath306     & = \\dots = { \\operatorname{tr}}\\bigg(\\psi_{a , r}(x_1,\\dots , x_{r-1 } , i_{n_r } ) x_r\\bigg ) .",
    "\\end{array}\\ ] ] moreover , for @xmath307 the maps @xmath299 exhibit the explicit form @xmath308    fix @xmath309 and consider the linear functional @xmath310 by the riesz representation theorem , there exists a unique @xmath311 such that @xmath312 for all @xmath313 .",
    "therefore , the map @xmath299 is given by + @xmath314 .",
    "it is straightforward to show that @xmath299 is multilinear in @xmath315 .",
    "now , choosing @xmath316 and @xmath317 in immediately yields .",
    "moreover , follows from the trace equality @xmath318 thus the proof of lemma [ lemma1 ] is complete .",
    "the linear maps @xmath299 constructed in the above proof are almost identical to the so - called partial trace operators  a well - known concept from multilinear algebra and quantum mechanics @xcite .",
    "next , we show how to compute @xmath319 for given @xmath320 if @xmath6 is not a pure tensor product @xmath321 .",
    "[ t_d ] let @xmath169 and @xmath320 .",
    "then , the @xmath322position of @xmath323 is given by @xmath324 where @xmath325 denotes the standard basis of @xmath326 .",
    "let @xmath327 .",
    "then , the element in the @xmath224 position of the matrix @xmath328 is given by @xmath329   & = { \\operatorname{tr}}\\biggl(a(x_1\\otimes\\cdots\\otimes x_j\\mathrm{e}_{t}\\mathrm{e}_{s}^{\\top}\\otimes\\cdots\\otimes x_r ) \\biggr)\\\\[3 mm ]   & = { \\operatorname{tr}}\\biggl(a ( x_1\\otimes\\cdots\\otimes x_r)(i_{n_1}\\otimes\\cdots\\otimes \\mathrm{e}_{t}\\mathrm{e}_{s}^{\\top}\\otimes\\cdots\\otimes i_{n_r } ) \\biggr ) .",
    "\\end{array}\\ ] ] hence , follows from the identity @xmath330 .",
    "let @xmath169 and @xmath320 .",
    "a straightforward consequence of the identity @xmath331 for all @xmath332 , shows that @xmath333 is hermitian .    for simplicity of writing ,",
    "whenever @xmath334 is understood from the context , we use the following shortcut @xmath335 now , we can give an explicit formula for the riemannian gradient of @xmath0 and derive necessary and sufficient critical point conditions .",
    "[ critic ] let @xmath336 and let @xmath0 be the generalized rayleigh - quotient on @xmath337 then , one has the following :    @xmath338 the gradient of @xmath0 at @xmath42 with respect to the riemannian metric @xmath339 is @xmath340    @xmath341 the critical points of @xmath0 on @xmath115 are characterized by @xmath342 = 0 \\ ] ] i.e. @xmath199 , @xmath302 is the orthogonal projector onto an @xmath343dimensional invariant subspace of @xmath344 .    @xmath345 fix @xmath346 and let @xmath347 denote the canonical smooth extension of @xmath0 to @xmath348 then , @xmath349 for all @xmath350 from ( [ innerprod3 ] )",
    ", we obtain that the gradient of @xmath347 at @xmath42 is given by @xmath351 thus , according to and ( [ subgrad ] ) , @xmath352 @xmath353 @xmath346 is a critical point of @xmath0 iff @xmath354 .",
    "this is equivalent to @xmath355=\\displaystyle [ p_j,\\widehat{a}_j]p_j , \\ ] ] for all @xmath114 by multiplying once from the left with @xmath199 and once from the right with @xmath199 , we obtain that @xmath356 and @xmath357 .",
    "hence , the conclusion @xmath358=0 $ ] holds for all @xmath200 .    as a consequence of theorem @xmath359",
    ", we immediately obtain the following necessary and sufficient critical point condition .",
    "[ critic1 ] let @xmath336 and let @xmath360 be such that @xmath361 where @xmath71 is the standard projector in @xmath72 we write @xmath362   \\psi_j'''^{\\dagger } & \\psi_{j}''\\\\ \\end{array } \\right],\\ ] ] with @xmath363 and @xmath364 .",
    "then @xmath42 is a critical point of @xmath0 if and only if @xmath365 for all @xmath114    for the rest of this section we are concerned with the computation of the riemannian hessian of @xmath0 and also with its non - degeneracy at critical points .",
    "[ thhess ] let @xmath169 and @xmath366 . then",
    ", the riemannian hessian of @xmath0 at @xmath42 is the unique self - adjoint operator @xmath367 \\xi:=(\\xi_1,\\dots,\\xi_r)\\mapsto { \\bf h}_{\\rho_a}(p)(\\xi ) : = \\big({\\bf h}_{1}(\\xi),\\dots , { \\bf h}_{r}(\\xi)\\big ) , \\end{array}\\ ] ] defined by @xmath368 where @xmath344 is the shortcut for @xmath369 .",
    "let @xmath370 denote a smooth extension of @xmath371 to @xmath372 . according to ( [ grad ] )",
    ", we can choose @xmath373 then , @xmath374    & + \\sum\\limits_{k=1 , k\\neq j}^r   \\mathrm{ad}^2_{p_j } \\psi_{a , j } ( p_1,\\dots , i_{n_j},\\dots , x_k , \\dots , p_r ) ,   \\end{array}\\ ] ] for all @xmath375 and @xmath376 in @xmath348 notice that , the derivative of the linear map @xmath377 in direction @xmath378 ( @xmath379 ) is @xmath380 . applying",
    "( [ ad ] ) and ( [ subgrad ] ) , the riemannian hessian of @xmath0 at @xmath381 is given by and .",
    "here , we have used the following two facts :    \\(i ) clearly , @xmath382 is skew - hermitian and hence @xmath383 is in the tangent space @xmath384 for all @xmath385    \\(ii ) a straightforward computation shows that @xmath386 is in the orthogonal complement of @xmath384 and hence @xmath387 for all @xmath385    by restricting the tangent vectors @xmath388 to the vectors of the form @xmath389 , it follows immediately a necessary condition for the non - degeneracy of the hessian at local extrema .",
    "let @xmath160 and @xmath390 be a local maximizer ( local minimizer ) of @xmath300 if @xmath391 is non - degenerate , then for all @xmath302 the equality @xmath392 holds with @xmath393 and @xmath394 as in @xmath395 . here",
    ", @xmath396 denotes the spectrum of @xmath74 .    in the case when @xmath169 can be diagonalized by elements in @xmath397 , condition is also sufficient for the nondegeneracy of the hessian of @xmath0 at local extrema .    in the remaining part of the section we derive a genericity statement concerning the critical points of the generalized rayleigh - quotient .",
    "the result is a straightforward consequence of the parametric transversality theorem @xcite .",
    "let @xmath398 , @xmath118 , @xmath399 be smooth manifolds and @xmath400 a smooth map",
    ". moreover , let @xmath401 denote the tangent map of @xmath402 at @xmath403 .",
    "we say that @xmath402 is _ transversal _ to a submanifold @xmath404 and write @xmath405 if @xmath406 for all @xmath407 .",
    "then , the parametric transversality theorem states the following .",
    "( @xcite)[thmtransv ] let @xmath408 be smooth manifolds and @xmath409 a closed submanifold of @xmath399 .",
    "let @xmath410 be a smooth map , let @xmath411 and define @xmath412 , @xmath413 . if @xmath405 , then the set @xmath414 is open and dense .",
    "now , let @xmath415 be a smooth function depending on a parameter @xmath416 and consider the map @xmath417 where @xmath418 is the cotangent bundle of @xmath118 and @xmath419 denotes the differential of @xmath420 at @xmath142 . with these notations , our genericity result reads as follows .",
    "[ f_trans ] let @xmath59 , @xmath82 and @xmath402 be as above and let @xmath409 be the image of the zero section in @xmath418 .",
    "if @xmath405 then for a generic @xmath421 the critical points of the smooth function @xmath415 are non - degenerate .",
    "fix @xmath411 and define @xmath422 from the transversality theorem [ thmtransv ] it follows that the set @xmath423 is open and dense in @xmath82 if @xmath405 . in the following , we will prove that @xmath424 is equivalent to the fact that the hessian of @xmath420 is non - degenerate in the critical points .",
    "this will prove the theorem .",
    "first , notice that @xmath425 if and only if @xmath426 is a critical point of @xmath420 .",
    "therefore , the transversality condition @xmath427 is equivalent to @xmath428 to rewrite this condition in local coordinates , let @xmath429 be a chart on an open subset @xmath430 around @xmath431 such that @xmath432 and @xmath433 . then define @xmath434 moreover",
    ", @xmath111 induces a chart @xmath435 around @xmath436 via @xmath437 here , @xmath438 refers to the natural projection and @xmath439 .",
    "thus , for @xmath440 one has @xmath441 .",
    "since transversality of @xmath442 to @xmath409 is preserved in local coordinates , is equivalent to @xmath443 then @xmath444 yields that is fulfilled if and only if @xmath445 is nonsingular .",
    "finally , the conclusion follows form the identity @xmath446 which is satisfied due to the fact that @xmath431 is a critical point and @xmath433 . here , @xmath447 denotes the hessian form corresponding to the hessian operator via @xmath448 for all @xmath449 .    for the generalized rayleigh - quotient ,",
    "we obtain the following result .",
    "[ cor_generic ] the critical points of the generalized rayleigh - quotient are generically non - degenerate .",
    "set @xmath450 , @xmath451 . for the simplicity",
    ", we will identify the cotangent bundle @xmath418 with the tangent bundle @xmath452 and work with the map @xmath453 instead of , where @xmath454 is the riemannian gradient of @xmath0 at @xmath42 .",
    "we will show that @xmath405 , where @xmath409 is now the image of the zero section in @xmath452 , i.e. @xmath455 for all @xmath456 with @xmath457 .",
    "as in the proof of theorem [ f_trans ] , we rewrite the transversality condition in local coordinates , i.e. @xmath458 where @xmath459 here , @xmath460 is a chart around @xmath42 with @xmath461 and @xmath433 and @xmath462 is the corresponding induced chart around @xmath463 . with this choice of charts ,",
    "we obtain @xmath464 where @xmath465 .",
    "since @xmath466 is linear , one has @xmath467 thus , condition holds if and only if @xmath468 finally , we will show that @xmath469 which clearly guarantees .",
    "let @xmath470 , then we obtain @xmath471 & = { \\operatorname{tr}}\\biggl(x(\\sum\\limits_{j=1}^r p_1\\otimes \\cdots\\otimes \\xi_j\\otimes \\cdots\\otimes p_r)\\biggr ) , \\end{split}\\ ] ] for all @xmath472 .",
    "notice , that the equality @xmath473 follows from @xmath433 .",
    "therefore , @xmath474 and this holds if and only if @xmath475 , since alls summands in are orthogonal to each other .",
    "thus , we have proved that @xmath476 and hence @xmath405 . from the theorem [ f_trans ] it follows immediately that the critical points of the generalized rayleigh - quotient are generically non - degenerate .",
    "unfortunately , for best multilinear rank tensor approximation and subspace clustering problems , we can not conclude from corollary [ cor_generic ] that the critical points of @xmath0 are generically non - degenerate . in these cases ,",
    "the resulting matrices @xmath6 are restricted to a thin subset of @xmath65 and thus the genericity statement with respect @xmath65 in corollary [ cor_generic ] does not carry over straight - forwardly .",
    "exploiting the geometrical structure of the constraint set @xmath164 we develop two numerical methods , a newton - like and a conjugated gradient algorithm , for optimizing the generalized rayleigh - quotient @xmath0 , with @xmath477      the intrinsic riemannian newton algorithm is described by means of the levi - civita connection taking iteration steps along geodesics @xcite .",
    "sometimes geodesics are difficult to determine , thus , here we are interested in a more general approach , which introduces the newton iteration via local coordinates , see  @xcite .",
    "more precisely , we follow the ideas in @xcite and use a pair of local coordinates on @xmath115 , i.e. normal coordinates and qr - coordinates .",
    "recall that , a _ _ local parametrization _ _ of @xmath478 around a point @xmath479 is a smooth map @xmath480 satisfying the additional conditions @xmath481 _ riemannian normal coordinates _ are given by the riemannian exponential map @xmath482}p_1e^{[\\xi_1,p_1 ] } , \\dots , e^{-[\\xi_r , p_r]}p_re^{[\\xi_r , p_r]}\\bigr),\\end{array}\\ ] ] while _ qr - type coordinates _ are defined by the qr - approximation of the matrix exponential , i.e. @xmath483_q^{\\dagger}\\;p_1\\;[x_1]_q , \\dots , [ x_r]_q^{\\dagger}\\;p_r\\;[x_r]_q\\bigr ) .",
    "\\end{array}\\ ] ] here @xmath484_q$ ] denotes the @xmath485factor from the unique @xmath486 decomposition of @xmath487.$ ]    now , let @xmath488 be a critical point of @xmath0 .",
    "choose @xmath489 in a neighborhood of @xmath490 and perform the following newton - like iteration @xmath491 where @xmath492 is a solution of the newton equation @xmath493 replacing the objects in ( @xmath494 ) by their explicit form computed in the previous section , we get the following newton equation : @xmath495 for all @xmath114 as mentioned before , let @xmath496 . solving this system in the embedding space",
    "@xmath372 requires a higher number of parameters than necessary .",
    "however , exploiting the particular structure of the tangent vectors @xmath497\\theta_j^{\\dagger } , \\ ] ] allows us to solve ( [ sylvester1 ] ) with the minimum number of parameters equal to the dimension of @xmath107 .",
    "thus , by multiplying ( @xmath498 ) from the left with @xmath499 and from the right with @xmath500 we obtain an equation in the variables @xmath501 i.e. @xmath502 where the terms @xmath503 and @xmath504 are computed in the following .",
    "let @xmath505\\in\\mathrm{su}_{n_j},\\ ] ] where @xmath506 and @xmath507 are @xmath508 and @xmath509 matrices , respectively .",
    "then , @xmath510 for expressing @xmath511 with @xmath512 , we introduce the multilinear operators @xmath513 defined in a similar way as @xmath299 by @xmath514 for all @xmath515 and @xmath516 for convenience , we will use the following shortcut @xmath517    [ tabel_n ]    furthermore , we partition the matrix @xmath518 into block form @xmath519_{s , t=1}^{n_j},\\ ] ] where each @xmath520 is an @xmath521 matrix . then , the linear map @xmath522 is given by @xmath523_{s , t=1}^{n_j } v_j.\\ ] ] finally , the complete newton - like algorithm for the optimization of @xmath0 on @xmath115 is given by algorithm 1 .",
    "\\(a ) for an arbitrary matrix @xmath169 , the computation of @xmath344 and @xmath518 is performed according to formula ( [ tau_delta ] ) .",
    "this can be simplified in the case of the applications described in section 3.3 . +",
    "* case 1 . *",
    "if @xmath524 with @xmath525 @xmath526 , then @xmath527 where @xmath528 and @xmath529 are the @xmath208th mode and respectively @xmath530th mode matrices of the tensors @xmath531 and @xmath532 , respectively . +",
    "* case 2 . *",
    "if @xmath533 with @xmath534 and @xmath535 , then @xmath536 ( b ) to solve the system ( [ sylvester2 ] ) , one can rewrite it as a linear equation on @xmath537 ( @xmath538 is the dimension of @xmath115 ) using matrix kronecker products and @xmath246operations , then solve this by any linear equation solver .",
    "+ ( c ) the computation of geodesics on matrix manifolds usually requires the matrix exponential map , which is in general an expensive procedure of order @xmath539 . yet , for the particular case of the grassmann manifold @xmath53 , gallivan et.al .",
    "@xcite have developed an efficient method to compute the matrix exponential , reducing the complexity order to @xmath540 ( @xmath541 ) .",
    "our approach , however , is based on a first order approximation of the matrix exponential @xmath542}$ ] followed by a qr - decomposition to preserve orthogonality / unitarity .",
    "explicitly , it is given by @xmath543_q = w\\left [ \\begin{array}{ccc }   d^{-1 } & \\sigma d^{-1 } & 0\\\\ -\\sigma^{\\dagger}d^{-1 } & d^{-1 } & 0\\\\ 0 & 0 & i_{n-2 m } \\end{array}\\right ] w^{\\dagger},\\ ] ] where @xmath544 with @xmath545 , @xmath546 and @xmath547 diagonal .",
    "furthermore , @xmath548\\in\\mathrm{su}_{n } , & d:=\\sqrt{i_{m}+\\sigma^{\\dagger}\\sigma } , \\end{array}\\ ] ] where @xmath549\\in\\mathrm{su}_{n - m}$ ] is an unitary completion of @xmath189 .",
    "the computational complexity of this qr - factorization is of order @xmath550 .",
    "+ ( d ) the convergence of the algorithm is not guaranteed for arbitrary initial conditions @xmath489 and even in the case of convergence the limiting point need not be a local maximizer of the function . to overcome this , one could for example test if the computed direction is ascending , else take the gradient as the new direction .",
    "furthermore , one can make an iterative line - search in the ascending direction .    in the following theorem we prove that the sequence generated by algorithm 1 converges quadratically to a critical point of the generalized rayleigh - quotient @xmath0 if the sequence starts in a sufficiently small neighborhood of the critical point .",
    "let @xmath169 and @xmath551 be a non - degenerate critical point of the generalized rayleigh - quotient @xmath0 , then the sequence generated by the n - like algorithm converges locally quadratically to @xmath490 .    for the critical point @xmath552 ,",
    "the riemannian coordinates ( @xmath553 ) and the qr- coordinates ( @xmath554 ) satisfy the condition @xmath555 .",
    "thus , according to theorem 4.1 . from @xcite",
    "there exists a neighborhood @xmath556 such that the sequence of iterates generated by the n - like algorithm converges quadratically to @xmath490 when the initial point @xmath42 is in @xmath82 .",
    "the quadratic convergence of the newton - like algorithm has the drawback of high computational complexity .",
    "solving the newton equation ( [ sylvester2 ] ) yields a cost per iteration of order @xmath557 , where @xmath538 is the dimension of @xmath337 in what follows , we offer as an alternative to reduce the computational costs of the newton - like algorithm by a conjugated gradient method .",
    "the linear conjugated gradient ( lcg ) method is used for solving large systems of linear equations with a symmetric positive definite matrix , which is achieved by iteratively minimizing a convex quadratic function @xmath558 . the initial direction @xmath559 is chosen as the steepest descent and every forthcoming direction @xmath560 is required to be conjugated to all the previous ones , i.e. @xmath561 for all @xmath562 .",
    "the exact maximum along a direction gives the next iterate .",
    "hence , the optimal solution is found in at most @xmath50 steps , where @xmath50 is the dimension of the problem .",
    "nonlinear conjugated gradient ( ncg ) methods use the same approach for general functions @xmath563 , not necessarily convex and quadratic .",
    "the update in this case reads as @xmath564 where the step - size @xmath565 is obtained by a line search in the direction @xmath538 @xmath566 and @xmath567 is given by one of the formulas : fletcher - reeves , polak - ribiere , hestenes - stiefel , or other .",
    "we refer to @xcite for the generalization of the ncg method to a riemannian manifold . for the computation of the step - size along the geodesic in direction @xmath130 , an exact line search",
    " as in the classical case  is an extremely expensive procedure .",
    "therefore , one commonly approximates ( [ stepsize ] ) by an armijo - rule , which ensures at least that the step length decreases the function sufficiently .",
    "we , however , have decided to compute the step - size by performing a one - dimensional newton - step along the geodesic , since in the neighborhood of a critical point one newton step can lead very close to the solution .",
    "therefore , at @xmath568 the step - size in direction @xmath569 is given by @xmath570 where @xmath571 is the unique geodesic through @xmath42 in direction @xmath130 .",
    "let @xmath572 be such that @xmath573 furthermore , let @xmath574 denote the updated point in @xmath575 via the qr - coordinates as in ( [ up ] ) .",
    "for the computation of the new direction , a `` transport '' of the old direction @xmath576 from @xmath577 to the tangent space @xmath578 is required .",
    "we use the following approximation for the paralle transport of @xmath130 along the geodesic through @xmath42 in direction @xmath130 @xmath579 z_j^{\\dagger } & 0 \\end{array}\\right]\\theta_j^{\\mathrm{new}^{\\dagger } } , & \\textrm{where    } \\theta_j^{\\mathrm{new}}=\\theta_j\\left[\\begin{array}{cc } i_{m_j } & -z_j\\\\[2 mm ] z_j^{\\dagger } & i_{n_j - m_j } \\end{array}\\right]_q , \\end{array}\\ ] ] for all @xmath114    the complete riemannian conjugated gradient is presented as algorithm 2 .",
    "it is recommended to reset the search direction to the steepest descent direction after @xmath538 iterations , i.e.  @xmath580 , where @xmath538 refers to the dimension of the manifold . for the maximization of the generalized rayleigh - quotient",
    "the initial direction is @xmath581 and the update @xmath582    the convergence properties of the ncg methods are in general difficult to analyze . yet , under moderate supplementary assumptions on the cost function one can guarantee that the ncg converges to a stationary point @xcite .",
    "it is expected that the proposed riemannian conjugated gradient method has properties similar to those of the ncg .      in this section",
    "we run several numerical experiments suitable for the applications mentioned in section 3.2 , i.e.  best rank approximation for tensors and subspace clustering , to test the newton - like ( n - like ) and riemannian conjugated gradient ( rcg ) algorithms .",
    "the algorithms were implemented in matlab on a personal notebook with 1.8 ghz intel core 2 duo processor .      to test the performance of our algorithms we have considered several examples of large size tensors of order @xmath583 and @xmath584 with entries chosen from the standard normal distribution and estimated their best low - rank approximation .",
    "we have started with a truncated hosvd ( @xcite ) and performed several hooi iterates before we run our n - like and rcg algorithms .",
    "depending on the size of the tensor , the number of hooi iterations necessary to reach the region of attraction of a stationary point @xmath585 , ranges from 10 to 100 . as stopping criterion",
    "we have chosen that the relative norm of the gradient @xmath586 is approximately @xmath587 .",
    "* computational complexity . * the computational complexity of the n - like method is determined by the computation of the hessian and the solution of the newton equation ( [ sylvester3 ] ) .",
    "thus , for the best rank-@xmath588 approximation of a @xmath589 tensor , the computation of the hessian is dominated by tensor - matrix multiplications and is of order @xmath590 . solving the newton equation by gaussian",
    "elimination gives a computational complexity of order @xmath591 , i.e. the dimension of the manifold to the power of three .",
    "for the computational costs of the rcg method we have to take into discussion only tensor - matrix multiplications , which give a cost per rcg iteration of order @xmath590 .    * experimental results and previous work . * the problem of best low - rank tensor approximation has enjoyed a lot of attention recently .",
    "apart from the well known higher order orthogonal iterations ",
    "hooi ( @xcite ) , various algorithms which exploit the manifold structure of the constraint set have been developed .",
    "we refer to @xcite for newton methods , to @xcite for quasi - newton methods and to @xcite for conjugated gradient and trust region methods on the grassmann manifold .",
    "similar to the newton methods in @xcite , our n - like method converges quadratically to a stationary point of the generalized rayleigh - quotient when starting in its neighborhood .",
    "we have compared our algorithms with the existing ones in the literature : quasi - newton with bfgs , riemannian conjugated gradient method which uses the armijo - rule for the computation of the step - size ( cg - armijo ) , and hooi .",
    "the algorithms were run on the same platform , identically initialized and with the same stopping criterion . for the bfgs quasi - newton and limited memory quasi - newton ( l - bfgs ) methods we have used the code available in @xcite .",
    "[ fig1 ] shows convergence results for two large size tensors @xmath592 and @xmath593 approximated by rank-@xmath594 and rank-@xmath595 tensors , respectively . in fig .",
    "[ fig11 ] we plot the convergence behavior of the rcg method for the best rank-@xmath596 approximation of a @xmath597 tensor ( left ) and for the best rank-@xmath598 approximation of a @xmath599 tensor . due to the limited memory space",
    ", we were not able to run the n - like and bfgs quasi - newton algorithms for the example on the left .",
    "yet it was still possible to run rcg , l - bfgs , cg - armijo and hooi . in table",
    "[ tab:1 ] we display the average cpu times necessary to compute a low rank best approximation for tensors of different sizes and orders by n - like , rcg , bfgs and l - bfgs quasi - newton methods .",
    "we have performed 100 runs for each example .    [",
    "cols=\"<,^,^,^,^ \" , ]     [ tab:1 ]    * resume .",
    "* first we mention that there is no guarantee that the n - like and rcg iterations converge to a local maximizer of the generalized rayleigh - quotient .",
    "however , in the examples presented in fig.[fig1 ] and fig.[fig11 ] the limiting points are local maximizers . as the numerical experiments have shown , the n - like method has the advantage of fast convergence .",
    "unfortunately , for large scale problems , the n - like algorithm can not be applied , as mentioned before .",
    "even when it is possible to apply n - like algorithm , it needs a large amount of time per iteration . as an example , for the best rank-@xmath596 of a @xmath600 tensor , one n - like iteration took three minutes .",
    "related algorithms which explicitly compute the hessian and solve the newton equation , such as @xcite , and those which approximately solve the newton equation such as the trust region method @xcite , face the same difficulty for large scale problems . on the other hand , the low cost iterations of the rcg method",
    "makes it a good candidate to solve large size problems .",
    "the convergence rate is comparative to that of the bfgs quasi - newton method in @xcite , but at much lower computational costs .",
    "our experiments exhibit the shortest cpu time for the rcg method . in the examples in which the tensor was a small perturbation of a low - rank tensor , the rcg algorithm exhibits quadratic convergence .",
    "the experimental setup consists in choosing @xmath25 subspaces in @xmath601 ( @xmath602 and @xmath584 ) and collections of @xmath603 randomly chosen$ ] .",
    "] points on each subspace .",
    "then , the sample points are perturbed by adding zero - mean gaussian noise with standard deviation varying from @xmath604 to @xmath605 in the different experiments .",
    "now , the goal is to detect the exact subspaces or to approximate them as good as possible . for this purpose",
    ", we apply our n - like and rcg algorithms to solve the associated optimization task , cf .",
    "section 3.1 .",
    "the error between the exact subspaces and the estimated ones is measured as in @xcite , i.e. @xmath606 where @xmath199 is the orthogonal projector corresponding to the exact subspace and @xmath607 the orthogonal projector corresponding to the estimated one .",
    "it can be easily checked that in the case of unperturbed data there is a unique non - degenerate minimizer of @xmath0 , and it yields the exact subspaces .",
    "thus , we expect that for noisy data the global minimizer still gives a good approximation .",
    "since @xmath0 has many local optima , for an arbitrary starting point our algorithms can converge to stationary points which lead to a significant error between the exact subspaces and their approximation .",
    "thus , in what follows , we briefly describe a method ( pda , see below ) for computing a suitable initial point which guarantees the convergence of our algorithms towards a good approximation of the exact subspaces in our numerical experiment :    the polynomial differential algorithm ( pda ) was proposed in @xcite .",
    "it is a purely algebraic method for recovering a finite number of subspaces from a set of data points belonging to the union of these subspaces . from the data set finitely",
    "many homogeneous polynomials are computed such that their zero set coincides with the union of the sought subspaces .",
    "then , an evaluation of their derivatives at given data points yields successively a basis of the orthogonal complement of subspaces one is interested in . for noisy data , a slightly modified version of pda @xcite yields an approximation of the unperturbed subspaces .",
    "this  first \" approximation turned out to be a good starting point for our iterative algorithms which significantly improved the approximation quality .",
    "for each noise level we perform 500 runs of the n - like and local - cg algorithms for different data sets and compute the mean error between the exact subspaces and the computed approximations . as a preliminary step ,",
    "we normalize all data points , such that no direction is favored .    in fig .",
    "[ fig2 ] , @xmath608 randomly chosen data points which lie exactly in the union of two @xmath609-dimensional subspaces of @xmath601 ( left ) and their perturbed standard deviation ] images ( right ) are depicted .",
    "moreover , the two plots display the exact subspaces ( left ) as well as the ones computed by our n - like algorithm ( right ) .",
    "the error between the exact subspaces and our approximation is ca .",
    "@xmath610 , whereas the error for the pda approximation is ca .",
    "@xmath611 .    in fig .",
    "[ fig3 ] , we plot the mean error ( left ) for different noise levels and different number of subspaces .",
    "we have included also the mean error for the starting point of our algorithms , i.e.  for the pda approximation .",
    "on the right we demonstrate the fast convergence rate of the n - like and rcg algorithms for the case of @xmath583 and , respectively , @xmath584 subspaces .",
    "* resume . *",
    "our numerical experiments have proven that ( i ) the minimization task proposed in section 3 is capable to solve subspace detection problems and ( ii ) our numerical algorithms initialized with the pda starting point yield an effective method for computing a reliable approximation of the perturbed subspaces .",
    "how the approximation of the perturbed subspaces varies when the noise in the data follows some law of distribution , is the subject of future investigation .",
    "here we provide a proof of proposition @xmath117 , which states that there exists a global riemannian isometry @xmath111 between @xmath1 and @xmath337    the surjectivity of @xmath111 is clear from the definition of @xmath1 . to prove the injectivity of @xmath111 we use induction over @xmath25 .",
    "choose @xmath612 such that @xmath613 i.e. @xmath614 where @xmath615 and @xmath616 are the entries of @xmath617 and @xmath618 , respectively . +",
    "thus it exists @xmath619 such that @xmath620 .",
    "since @xmath621 and @xmath622 have only @xmath623 and @xmath19 as eigenvalues it follows that @xmath624 and @xmath625 .",
    "therefore , @xmath626 implies that @xmath627 and the procedure can be repeated until we obtain @xmath628 , for all @xmath302 .",
    "thus the injectivity of @xmath111 is proven .",
    "so @xmath111 is a continuous bijective map with continuous inverse @xmath629 due to the compactness of @xmath115 .",
    "moreover , the map @xmath111 is smooth since the components of @xmath630 are polynomial functions .",
    "let @xmath479 and @xmath631 .",
    "consider the tangent map of @xmath111 at @xmath42 , i.e. @xmath632 ( \\xi_1,\\dots , \\xi_r)\\mapsto \\displaystyle\\sum\\limits_{j=1}^r p_1\\otimes\\dots\\otimes \\xi_j\\otimes \\dots\\otimes p_r .",
    "\\end{array}\\ ] ] with the inner products ( [ inner1 ] ) and ( [ innerprod3 ] ) defined on @xmath65 and @xmath372 , respectively , one has @xmath633 this implies that the tangent map @xmath634 is a linear isometry .",
    "thus , it is invertible and therefore @xmath111 is a local diffeomorphism .",
    "moreover , since @xmath111 is bijective it is a global diffeomorphism , giving thus a global riemannian isometry when the metric on @xmath115 is defined by .",
    "this work has been supported by the federal ministry of education and research ( bmbf ) through the project fhprofund 2007 : cooperation program between universities of applied science and industry , `` development and implementation of novel mathematical algorithms for identification and control of technical systems '' ."
  ],
  "abstract_text": [
    "<S> we introduce a generalized rayleigh - quotient @xmath0 on the tensor product of grassmannians @xmath1 enabling a unified approach to well - known optimization tasks from different areas of numerical linear algebra , such as best low - rank approximations of tensors ( data compression ) , geometric measures of entanglement ( quantum computing ) and subspace clustering ( image processing ) . </S>",
    "<S> we briefly discuss the geometry of the constraint set @xmath1 , we compute the riemannian gradient of @xmath0 , we characterize its critical points and prove that they are generically non - degenerated . </S>",
    "<S> moreover , we derive an explicit necessary condition for the non - degeneracy of the hessian . finally , we present two intrinsic methods for optimizing @xmath0  a newton - like and a conjugated gradient  and compare our algorithms tailored to the above - mentioned applications with established ones from the literature .    </S>",
    "<S> riemannian optimization , grassmann manifold , multilinear rank , best approximation of tensors , subspace clustering , entanglement measure , newton method , conjugated gradient method .    14m15 , 15a69 , 65d19 , 65f99 , 65k10 , 81p68 </S>"
  ]
}