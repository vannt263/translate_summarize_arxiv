{
  "article_text": [
    "in this paper , we consider a classical problem of recovering an unknown vector @xmath7 in the standard linear model @xmath8 where @xmath2 is a known @xmath3-matrix and @xmath9 is a standard white gaussian noise in @xmath10 with @xmath11 .",
    "the noise level @xmath12 in ( [ equ.0 ] ) is assumed to be known .",
    "we start out by considering the maximum likelihood estimate of @xmath0 @xmath13 where @xmath14 it is easily seen that @xmath15 and that the mean square risk of this estimator is computed as follows : @xmath16 \\nonumber\\\\[-8pt]\\\\[-8pt ] & = & \\sigma^2\\sum_{k=1}^n \\lambda^{-1}(k),\\nonumber\\end{aligned}\\ ] ] where @xmath17 and @xmath18 are eigenvalues and eigenvectors of @xmath19 @xmath20    in this paper , it is assumed solely that @xmath21 .",
    "so , @xmath2 may be severely ill - posed and ( [ equ.1 ] ) reveals the principal difficulty in @xmath22 : _ its risk may be very large when @xmath5 is large or when @xmath2 has a large condition number . _",
    "the simplest way to improve @xmath23 is to suppress large @xmath24 in ( [ equ.1 ] ) with the help of a linear smoother ; that is , to estimate @xmath0 by @xmath25 , where @xmath26 is a properly chosen @xmath27-matrix . in",
    "what follows , we deal with smoothing matrices admitting the following representation @xmath28 , where @xmath29 is a function @xmath30 $ ] which depends on a regularization parameter @xmath31 $ ] such that @xmath32    this method is called _ spectral regularization _ [ see @xcite ] since @xmath33 and @xmath34 have the same eigenvectors . summarizing , we estimate @xmath0 with the help of the following family of linear estimators @xmath35 and our main goal is to choose the best estimator within this family , or equivalently , the best regularization parameter @xmath36 .",
    "note that @xmath36 controls the mean square risk of  @xmath37 , @xmath38 ^ 2\\langle\\theta,\\psi_k\\rangle^2+\\sigma^2 \\sum_{k=1}^n \\lambda^{-1}(k ) h_\\alpha^2(k),\\hspace*{-25pt}\\ ] ] where here and below we denote for brevity @xmath39 \\quad\\mbox{and}\\quad \\langle\\theta,\\psi_k\\rangle\\stackrel{\\mathrm{def}}{= } \\sum_{l=1}^n \\theta(l)\\psi_k(l).\\ ] ]    according to ( [ equ.2 ] ) , the variance of @xmath37 is always smaller than that of the maximum likelihood estimate , but @xmath40 has a nonzero bias and adjusting properly @xmath36 we may improve @xmath23 .",
    "note that this improvement may be significant if @xmath41 are small for large @xmath42 .    in practice ,",
    "a good choice of @xmath43 is a delicate problem related to the numerical complexity of @xmath37 .",
    "for instance , to make use of the spectral cut - off with @xmath44 , one has to compute the singular value decomposition ( svd ) of @xmath2 . for very large @xmath5",
    ", this numerical problem may be difficult or even infeasible .",
    "the very popular tikhonov  phillips [ see , e.g. , @xcite ] regularization @xmath45 does not require svd . in this case , @xmath37 is computed as a root of the linear equation @xmath46 and therefore @xmath47 .",
    "it is worth pointing out that this regularization technique is good solely for ill - posed @xmath2 .",
    "another widespread regularization technique is due to @xcite .",
    "this method is based on a very simple idea : to compute recursively a root of equation @xmath48 since @xmath49 \\theta+a \\theta$ ] for all @xmath50 , we get @xmath51\\theta + a^{-1}a^{\\top}y .",
    "$ ] this formula motivates landweber s iterations defined by @xmath52\\hat{\\theta}_{k-1 } + a^{-1}a^{\\top}y.\\ ] ] thus , we can estimate @xmath0 without computing svd and without solving linear equations .",
    "it is easily seen that these iterations converge if @xmath53 and that the corresponding spectral regularization function is given by @xmath54 the regularization parameter of the landweber method is usually defined by @xmath55 .",
    "note that in spite of its iterative character , the numerical complexity of the landweber method may be hight .",
    "indeed , when the noise is very small , @xmath56 should be close to @xmath57 , and ( [ equ.3 ] ) implies that @xmath58 this means that if @xmath2 is severely ill - posed , the number of iterations may be very large , thus making the method infeasible .",
    "a substantial improvement of landweber s iterations is provided by the so - called @xmath59-method [ see , e.g. , @xcite and @xcite ] .",
    "all the above - mentioned regularization methods are particular cases of the so - called ordered smoothers [ see @xcite ] defined as follows .    [ d1 ] the family of sequences @xmath60 , k \\in\\mathbb{n}^+\\}$ ] is called ordered smoother if :    1 .   for",
    "any given @xmath61 $ ] , @xmath62 $ ] is a monotone function of @xmath42 .",
    "2 .   if for some @xmath63 $ ] and @xmath64 , @xmath65 , then @xmath66 for all @xmath67 .    it was kneip who noted that from a probabilistic viewpoint , all ordered smoothers are equivalent to the spectral cut - off with @xmath68 .",
    "this profound fact plays an essential role in adaptive estimation since it helps to analyze precisely statistical risks of feasible data - driven regularization methods .",
    "this is why in this paper we deal solely with the ordered smoothers .",
    "whatever inversion method is used , the principal question usually arising in practice is how to choose its regularization parameter .",
    "traditional theoretic approach to this problem is related to the minimax theory ; see , for example , @xcite and @xcite .",
    "however , this approach provides the smoothing parameters depending strongly on an a priory information about @xmath69 which is hardly available in practice .",
    "the only one way to improve this drawback is to use data - driven regularizations . in statistical literature",
    ", one can find several general approaches for constructing such methods .",
    "we cite here , for instance , the lepski method which has been adopted to inverse problems in @xcite , @xcite , @xcite and the model selection technique which was implemented in @xcite .    in this paper",
    ", we take the classical way related to the famous principle of unbiased risk estimation which goes back to @xcite . the heuristical motivation of this approach",
    "is based on the idea that a good data - driven regularization should minimize in some sense the risk @xmath70 [ see ( [ equ.2 ] ) ] .",
    "this idea is put into practice with the help of the empirical risk minimization suggesting to compute data - driven regularization parameters as follows : @xmath71 } { r}_{\\alpha}[y,\\operatorname{pen}],\\ ] ] where @xmath72= \\|\\hat\\theta_0-\\hat\\theta_\\alpha\\|^2+\\sigma^2\\operatorname{pen}(\\alpha),\\ ] ] and @xmath73\\rightarrow\\mathbb{r}^+$ ] is a given penalty function .",
    "the most important problem in this approach is related to the choice of the penalty .",
    "intuitively , we want that the method mimics the oracle smoothing parameter @xmath74 .",
    "this is why we are looking for a minimal penalty that ensures the following inequality : @xmath75+\\mathcal{c},\\ ] ] where @xmath76 is a random variable that does not depend on @xmath36 .",
    "it is easily seen that in the considered statistical model , @xmath77    traditional approach to solving ( [ equ.5 ] ) is based on the unbiased risk estimation defining the penalty as a root of the equation @xmath78+\\mathbf{e}\\mathcal{c}.\\ ] ] unfortunately , in spite of its very natural motivation , this penalty is not good for ill - posed problems [ see @xcite for more details ] .",
    "the main idea in this paper is to compute the penalty in a little bit different way , namely as a minimal function assuring the following inequality : @xmath79-\\mathcal{c } \\bigr]_+ \\le k\\mathbf{e } \\bigl [ l_{\\bar { \\alpha}}(\\theta)- { r}_{\\bar{\\alpha}}[y,\\operatorname{pen}]-\\mathcal{c } \\bigr]_+,\\ ] ] where @xmath80_+=\\max\\{0,x\\}$ ] and @xmath81 is a constant .",
    "the heuristical motivation behind this approach is rather transparent : we are looking for a minimal penalty that balances all excess risks uniformly in @xmath61 $ ] .",
    "recall that the excess risk is defined as the difference between the risk of the estimate and its empirical risk .",
    "note that according to ( [ equ.5 ] ) , we may focus on the positive part of the excess risk , and that equation ( [ equ.6 ] ) guarantees that for any data driven smoothing parameter @xmath82 @xmath83-\\mathcal{c } \\bigr]_+\\le k \\mathbf{e } \\bigl [ l_{\\bar { \\alpha}}(\\theta)- { r}_{\\bar{\\alpha}}[y,\\operatorname{pen}]-\\mathcal{c } \\bigr]_+.\\ ] ]    in order to explain how one can compute good penalties assuring ( [ equ.6 ] ) , we begin with the spectral representation of the underlying statistical problem .",
    "we can check easily that @xmath84 where @xmath85 are i.i.d .",
    "@xmath86 . with this notation",
    ", @xmath37 admits the following representation : @xmath87 where @xmath88 , and @xmath89 ^ 2 y^2(k),\\nonumber\\\\[-8pt]\\\\[-8pt ] \\|\\theta-\\hat\\theta_\\alpha\\|^2&=&\\sum_{k=1}^n[\\theta(k)-h_\\alpha ( k)y(k)]^2.\\nonumber\\end{aligned}\\ ] ]    in what follows , it is assumed that the penalty has the following structure : @xmath90 where @xmath91 is a positive number and @xmath92 , is a positive function to be defined later on .",
    "then the excess risk is computed as follows : @xmath93-\\mathcal{c}\\nonumber\\\\ & & \\qquad = \\sigma^2 \\sum_{k=1}^n \\lambda^{-1}(k)[2h_\\alpha(k)-h_\\alpha ^2(k)]\\bigl(\\xi^2(k)-1\\bigr)-(1+\\gamma ) \\sigma^2q(\\alpha)\\\\ & & \\qquad\\quad { } -2\\sigma\\sum_{k=1}^n \\lambda^{-1/2}(k)[1-h_\\alpha(k)]^2\\xi ( k)\\theta(k).\\nonumber\\end{aligned}\\ ] ]    our first idea in solving ( [ equ.6 ] ) is to use the fact that the absolute value of the cross term @xmath94 ^ 2\\xi ( k)\\theta(k)\\ ] ] is typically smaller than @xmath70 ( for more details , see lemma [ lemma.9 ] below ) .",
    "therefore , omitting this term in ( [ equ.6 ] ) , we get the following inequality for @xmath95 : @xmath96_+ \\le k\\mathbf{e}[\\eta_{\\bar{\\alpha}}-(1+\\gamma ) q(\\bar{\\alpha})]_+ , \\ ] ] where @xmath97\\bigl(\\xi^2(k)-1\\bigr).\\ ] ] usually , computing the minimal function @xmath95 assuring ( [ equ.9 ] ) is a hard numerical problem .",
    "however , when @xmath98 is a family of ordered smoothers it can be solved relatively easily .",
    "the main idea is to find a feasible solution @xmath99 of the marginal inequality @xmath100_+\\le\\mathbf{e}[\\eta _ { \\bar{\\alpha}}-q^\\circ(\\bar\\alpha)]_+\\ ] ] and then to show that @xmath101 satisfies ( [ equ.9 ] ) . to solve ( [ equ.10 ] ) , we use the following inequality : @xmath102_+^p\\le\\gamma(p+1)\\lambda^{-p}\\exp(-\\lambda x ) \\mathbf{e}\\exp(\\lambda\\eta),\\ ] ] which holds for any random variable @xmath103 and for any @xmath104 .",
    "its proof follows from the chernoff bound . without loss of generality , we may assume that @xmath105 .",
    "therefore , according to the cauchy ",
    "schwarz inequality @xmath106_+ \\le\\sqrt { \\mathbf{e}\\eta_{\\bar{\\alpha}}^2}= d(\\bar\\alpha),\\ ] ] where @xmath107 ^ 2 \\biggr\\}^{1/2}.\\ ] ] hence , @xmath99 is computed as a root of equation @xmath108\\mathbf{e}\\exp ( \\lambda\\eta_\\alpha)=d(\\bar{\\alpha}).\\ ] ]    it is not difficult to check with a little algebra that @xmath109 where @xmath110 is a root of equation @xmath111=\\log\\frac{d(\\alpha ) } { d(\\bar{\\alpha})}\\ ] ] and @xmath112\\\\[-8pt ] \\rho_\\alpha(k)&=&\\sqrt{2}d^{-1}(\\alpha)\\lambda^{-1}(k)[2h_\\alpha(k ) -h_\\alpha^2(k)].\\nonumber\\end{aligned}\\ ] ]    the only one numerical difficulty in computing @xmath99 is related to ( [ equ.13 ] ) .",
    "however note that in the proof of lemma [ lemma.7 ] it is shown that @xmath113\\ ] ] is a strictly monotone function and therefore ( [ equ.13 ] ) may be solved exponentially fast .",
    "note also that lemma [ lemma.7 ] provides lower and upper bounds for @xmath114 and @xmath110 .",
    "in particular , for some constant @xmath115 @xmath116    the next theorem shows that @xmath99 computed as a root of the marginal inequality ( [ equ.10 ] ) satisfies the global inequality ( [ equ.9 ] ) .",
    "[ theorem.1 ] let @xmath99 be defined by ( [ equ.12])([equ.14 ] ) .",
    "then for any @xmath117 , @xmath118_+^{1+r}\\le\\frac{cd^{1+r}(\\bar{\\alpha } ) } { ( \\gamma - r)^3},\\ ] ] where here and throughout the paper @xmath119 denotes a generic constant .",
    "the following theorem represents the main result in this paper .",
    "it controls the performance of the empirical risk minimization by the penalized oracle risk defined by @xmath120 where @xmath121+\\mathcal{c}\\ } = l_\\alpha(\\theta ) +",
    "( 1+\\gamma ) \\sigma^2q^{\\circ}(\\alpha).\\ ] ]    [ theorem.2 ] let @xmath122 with @xmath99 defined by ( [ equ.12])([equ.14 ] ) .",
    "then the mean square risk of @xmath123 with the data - driven smoothing parameter @xmath82 defined by ( [ equ.4 ] ) satisfies the following upper bound : @xmath124^{1/2 } \\biggr\\},\\ ] ] which holds true uniformly in @xmath125 .",
    "below , we discuss briefly some statistical aspects of this theorem .    1 .",
    "equation ( [ equ.15 ] ) represents a particular form of the so - called oracle inequality @xmath126 \\biggr\\},\\ ] ] where @xmath127 is a bounded function such that @xmath128 .",
    "this means that if the ratio @xmath129 is small , then the risk of the method is close to the risk of the penalized oracle . on the other hand , if this ratio is large , then the risk of the method is of order of the oracle risk .",
    "+ note also that ( [ equ.15 ] ) is a universal oracle inequality which holds true whatever is the ill - posedness of the underlying inverse problem .",
    "it generalizes the corresponding oracle inequalities in @xcite and @xcite obtained for the spectral cut - off method .",
    "2 .   theorem [ theorem.2 ] reveals some difficulties related to the data - driven choice of the regularization parameter in the tikhonov ",
    "phillips method . recalling that",
    "for this method @xmath130 , we obtain @xmath131 ^ 2\\ge 2\\sum_{k=1}^n \\lambda^{-2}(k ) h_\\alpha^2(k)\\\\&= & 2\\sum_{k=1}^n \\frac{1}{[\\alpha+\\lambda(k)]^2}\\ge\\frac{2n}{[\\alpha + \\lambda(n)]^2}.\\end{aligned}\\ ] ] since @xmath132 , it is clear that the penalized oracle risk of the tikhonov  phillips regularization may be very large compared to the risk of the method computed for given @xmath36 .",
    "this means that in practice , the tikhonov ",
    "phillips regularization with a data - driven smoothing parameter may fail .",
    "+ note however , that this drawback can be easily improved with the help of high - order tikhonov ",
    "phillips regularizations computed as follows : @xmath133 where @xmath134 stands for the standard tikhonov ",
    "phillips regularization .",
    "one can check with a little algebra that the corresponding smoothers are given by @xmath135 , and everything goes smoothly in this case .",
    "if the inverse problem is not severely ill - posed ; that is , @xmath136 for some @xmath137 , then , according to ( [ equ.67 ] ) , for reasonable spectral regularizations @xmath138 when @xmath36 is small .",
    "this means that the risk of the penalized oracle is close to the risk of the ideal oracle @xmath139 .",
    "this remark together with the famous @xcite minimax theorem shows that our method results in adaptive asymptotically ( as @xmath140 ) minimax regularizations . to demonstrate this ,",
    "suppose for simplicity that @xmath141 and that @xmath0 belongs to the following ellipsoidal body @xmath142\\le w \\biggr\\},\\ ] ] where @xmath143 is a nondecreasing function such that for some @xmath144 , @xmath145 then it follows from @xcite that as @xmath146 @xmath147\\\\ & = & \\bigl(1+o(1)\\bigr)\\sup_{\\theta\\in\\theta(w)}\\inf_{h}l[\\theta , h]\\\\ & = & \\bigl(1+o(1)\\bigr)\\sup_{\\theta\\in\\theta(w)}\\inf_{\\alpha\\in(0,\\bar { \\alpha}]}l[\\theta , h^*_\\alpha],\\end{aligned}\\ ] ] where @xmath148 at the left - hand side is taken over all estimators , @xmath149= \\sum_{k=1}^n[1- h(k)]^2\\langle\\theta,\\psi_k\\rangle^2+\\sigma^2 \\sum_{k=1}^n \\lambda^{-1}(k ) h^2(k)\\ ] ] and @xmath150|]_+ .",
    "$ ] recall that from a statistical viewpoint , the main drawback in this minimax result is that the optimal smoothing parameter @xmath151 } \\sup _ { \\theta\\in\\theta(w)}l[\\theta , h^*_\\alpha]\\ ] ] depends on the size @xmath152 of @xmath153 which is hardly known in practice . in order to overcome this difficulty ,",
    "one may use the data - driven regularization @xmath82 with @xmath154 . noticing that this family of smoothers is ordered ,",
    "we get according to theorem [ theorem.2 ] and ( [ equ.a ] ) @xmath155 + another interesting situation is related to the case when the inverse problem is severely ill - posed ; that is , when the eigenvalues of @xmath156 are exponentially decreasing , @xmath157 with some @xmath137 .",
    "then for small @xmath36 @xmath158 this means that the risk of the penalized oracle is essentially greater than that of the ideal oracle .",
    "in this situation , theorem [ theorem.2 ] provides an upper bound similar to @xcite .",
    "it is worth pointing out that neither ( [ equ.15 ] ) nor the extra penalty can be improved in this case [ for more details , see @xcite ] .",
    "let @xmath159 be a separable zero mean random process on @xmath160 . denote for brevity @xmath161 we begin with a general fact similar to dudley s entropy bound [ see , e.g. , @xcite ] .",
    "[ lemma.1 ] let @xmath162 , be a continuous strictly increasing function with @xmath163 .",
    "then for any @xmath104 , @xmath164\\\\[-8pt ] & & \\qquad\\le\\frac{\\log(2)\\sqrt { 2}}{\\sqrt{2}-1 } + \\max_{0 < u < v\\le t}\\max_{| z|\\le\\sqrt{2}/(\\sqrt{2}-1)}\\log \\mathbf{e}\\exp\\biggl\\{z\\lambda \\frac{\\delta_\\xi(u , v)}{\\bar{\\delta}_\\sigma(v , u ) } \\biggr\\},\\nonumber\\end{aligned}\\ ] ] where @xmath165    the proof of ( [ equ.16 ] ) is based on the standard chaining argument [ for more details , see @xcite ] .",
    "denote for brevity by @xmath166 and @xmath167 left and right elements of a closed subset @xmath168 in @xmath169 .",
    "first , we construct a dyadic partition of @xmath170 $ ] .",
    "let @xmath171 next , we partition @xmath172 and @xmath173 as follows : @xmath174}{2 } \\biggr\\},\\\\ \\mathcal{t}_4 ^ 2 & = & \\biggl\\{u\\in\\mathcal{t}_2 ^ 1\\dvtx\\sigma ^2_{u } > \\frac { \\bar{\\delta}^2_\\sigma[t_+(\\mathcal { t}_2 ^ 1),t_-(\\mathcal{t}_2 ^ 1)]}{2 } \\biggr\\}.\\end{aligned}\\ ] ] doing so , after @xmath175 steps , we get partitions @xmath176 , such that for any @xmath177 @xmath178 with the sets @xmath179 we associate the set of their right points @xmath180 and for any point @xmath181 we denote by @xmath182 the nearest point in @xmath183 .",
    "so , by  ( [ equ.17 ] ) , for any @xmath184 @xmath185    with this notation , for any @xmath186 , setting @xmath187 , we obtain @xmath188 \\le\\sum_{k=1}^{p } \\sup_{v\\in\\tau^{k}}\\bigl[\\xi_v-\\xi_{\\tau _ { k-1}(v)}\\bigr]\\nonumber\\\\[-8pt]\\\\[-8pt ] & = & \\sum_{k=1}^{p } \\sup_{v\\in\\tau^{k}}\\bar{\\delta}_\\sigma[v,\\tau_{k-1}(v ) ] \\times\\frac{\\delta_\\xi[v,\\tau_{k-1}(v)]}{\\bar{\\delta}_\\sigma [ v,\\tau_{k-1}(v)]}.\\nonumber\\end{aligned}\\ ] ]    to bound the right - hand side at the above display , we use the elementary inequality @xmath189",
    "\\le \\sum_{k } q(k ) \\log\\mathbf{e } \\exp[\\eta(k)],\\ ] ] which holds for any random variables @xmath190 and any given @xmath191 with @xmath192 .",
    "the proof of ( [ equ.20 ] ) follows immediately from the convexity of @xmath193 which implies @xmath194 \\ } \\biggr\\ } \\\\ & & \\qquad \\le\\biggl\\ { \\sum_{k } q(k ) \\mathbf{e } \\exp\\bigl [ \\eta(k)- \\log\\mathbf{e } \\exp[\\eta(k ) ] \\bigr ] \\biggr\\ } = 1.\\end{aligned}\\ ] ]    applying ( [ equ.20 ] ) with @xmath195 \\times\\frac{\\delta_\\xi[v,\\tau_{k-1}(v)]}{\\bar{\\delta}_\\sigma [ v,\\tau_{k-1}(v)]},\\end{aligned}\\ ] ] we obtain by ( [ equ.18 ] ) and ( [ equ.19 ] ) @xmath196.\\ ] ]    it is easily seen that for any @xmath104 @xmath197\\nonumber\\\\ & & \\qquad = \\mathbf{e}\\exp\\biggl\\{\\frac{\\lambda}{q(k)\\sigma_t } \\sup_{v\\in\\tau^{k}}\\bar{\\delta}_\\sigma[\\tau_{k-1}(v),v ] \\times\\frac{\\delta_\\xi[v,\\tau_{k-1}(v)]}{\\bar{\\delta}_\\sigma [ \\tau_{k-1}(v),v ] } \\biggr\\}\\nonumber\\\\[-8pt]\\\\[-8pt ] & & \\qquad \\le\\sum_{v\\in\\tau^{k}}\\mathbf{e } \\exp\\biggl\\ { \\frac{\\lambda}{q(k ) \\sigma_t}\\bar{\\delta}_\\sigma[\\tau_{k-1}(v),v ] \\times\\frac{\\delta_\\xi[v,\\tau_{k-1}(v)]}{\\bar{\\delta}_\\sigma [ \\tau_{k-1}(v),v ] } \\biggr\\}\\nonumber\\\\ & & \\qquad \\le2^{k}\\sup_{u < v}\\sup_{|z|\\le\\sqrt{2}/(\\sqrt{2}-1)}\\mathbf{e } \\exp\\biggl\\{\\lambda z \\frac{\\delta_\\xi(u , v)}{\\bar{\\delta}_\\sigma(v , u ) } \\biggr\\}.\\nonumber\\end{aligned}\\ ] ] in the above equation , it was used that @xmath198 .    finally , substituting ( [ equ.22 ] ) into ( [ equ.21 ] )",
    ", we arrive at @xmath199 thus completing the proof .      a zero mean process",
    "@xmath200 is called ordered if there exists a continuous strictly monotone scaling function @xmath201 and some @xmath202 such that @xmath203<\\infty.\\ ] ]    a banal example of an ordered process is a standard wiener process @xmath204 . in this case ,",
    "@xmath205 and obviously @xmath206=\\exp ( \\lambda^2/2).\\ ] ]    [ lemma.2 ] let @xmath159 be an ordered process with @xmath207 .",
    "then there exists a constant @xmath119 such that for all @xmath208 , uniformly in @xmath209 , @xmath210_+^p \\le \\frac{c}{z^{p/(q-1)}},\\ ] ] where @xmath80_+=\\max(0,x)$ ] .    without loss of generality",
    ", we may assume that that @xmath211 .",
    "for any integer @xmath212 , define @xmath213 as a root of the equation @xmath214 then we have @xmath215_+^p & \\le & \\sum _ { k=0}^\\infty\\mathbf{e}\\sup_{t\\in[t_k(\\gamma),t_{k+1}(z ) ] } [ \\xi_t - z\\sigma^q_t]_+^p \\nonumber\\\\ & \\le & \\sum_{k=0}^\\infty\\mathbf{e}\\sup_{t\\in[t_k(z),t_{k+1}(z ) ] } [ \\xi_t - z\\sigma^q_{t_k}]_+^p\\\\ & \\le & { \\mathbf{e}\\sup_{0\\le t \\le t_{1}(z)}}|\\xi_t|^p+ \\sum_{k=1}^\\infty \\mathbf{e } \\bigl[\\sup_{0\\le t\\le t_{k+1}(z)}\\xi_t- z\\sigma^q_{t_k(z ) } \\bigr]_+^p.\\nonumber\\end{aligned}\\ ] ] according to lemma [ lemma.1 ] , the first term at the right - hand side of the above inequality is bounded as follows : @xmath216 whereas the second one , by ( [ equ.11 ] ) and lemma [ lemma.1 ] , is controlled by @xmath217_+^p \\\\ & & \\qquad= \\sum_{k=1}^\\infty\\sigma^{p}_{t_{k+1}(z ) } \\mathbf{e } \\biggl[\\sup _ { t\\le t_{k+1}(z)}\\frac{\\xi_t}{\\sigma_{t_{k+1}(z)}}\\ge\\frac { z\\sigma^q_{t_k(z)}}{\\sigma_{t_{k+1}(z ) } } \\biggr]^p_+\\\\ & & \\qquad\\le c\\sum_{k=1}^\\infty \\sigma^{p}_{t_{k+1}(z)}\\exp\\biggl[-\\lambda\\frac{z\\sigma^q_{t_k(z ) } } { \\sigma_{t_{k+1}(z ) } } \\biggr]\\\\ & & \\qquad\\le \\frac{c}{z^{p/(q-1)}\\lambda ^{1/(q-1)}}\\sum_{k=1}^\\infty{(k+1)^{p/(q-1 ) } } \\exp\\biggl[-\\frac { 2^{1/(q-1 ) } pk}{(1 + 1/k)^{1/(q-1 ) } } \\biggr]\\\\ & & \\qquad\\le\\frac{c}{z^{p/(q-1)}}.\\end{aligned}\\ ] ] so , combining the above inequality with ( [ equ.25 ] ) and ( [ equ.26 ] ) , we arrive at ( [ equ.24 ] )",
    ".    the next very simple lemma is useful for understanding the fact that the ordered process is controlled by its variance @xmath218 .",
    "[ lemma.3 ] let @xmath200 , be a random process such that @xmath219_+^p \\le \\frac{c}{z^{p/(q-1)}},\\ ] ] for any @xmath209 and some @xmath220 , @xmath221 .",
    "then there exists a constant @xmath222 such that for any random variable @xmath223 @xmath224^{1/p}\\le c'[\\mathbf{e}\\sigma^{qp}_\\tau]^{1/(pq)}.\\ ] ]    according to ( [ equ.27 ] ) and minkowski s inequality , we obviously have @xmath225^{1/p}&\\le & \\{\\mathbf{e}|\\xi_\\tau - z\\sigma_\\tau^q+z\\sigma_\\tau^q|^p\\}^{1/p}\\\\ & \\le & \\bigl\\{\\mathbf{e}\\max_t[\\xi_t - z\\sigma_t^q]^p\\bigr\\}^{1/p } + z[\\mathbf{e}\\sigma_\\tau^{pq}]^{1/p}\\\\ & \\le & z[\\mathbf{e}\\sigma_\\tau^{pq}]^{1/p}+\\frac{c}{z^{1/(q-1)}}\\end{aligned}\\ ] ] and minimizing the right - hand side in @xmath226 we finish the proof .      in this section ,",
    "we focus on typical ordered processes related to the empirical risk minimization .    for given @xmath227 $ ] , define the following gaussian processes : @xmath228b(k ) \\xi(k),\\qquad 0 < \\alpha\\le\\bar\\alpha-\\alpha^\\circ , \\\\ \\xi^{-}_\\alpha&=&\\sum_{k=1}^n [ h_{\\alpha^\\circ}(k)-h_{\\alpha^\\circ-\\alpha}(k)]b(k ) \\xi(k),\\qquad 0\\le \\alpha\\le \\alpha^\\circ,\\end{aligned}\\ ] ] where @xmath85 are i.i.d . @xmath86 and @xmath229 it is easily seen that @xmath230 and @xmath231 are ordered processes . indeed , since they are gaussian , we can choose @xmath232 and it suffices to check that @xmath233 or equivalently ,",
    "@xmath234 if @xmath235 , then we have @xmath236 ^ 2&= & \\sum_{k=1}^n [ h_{\\alpha^\\circ}(k)-h_{\\alpha^\\circ\\pm\\alpha_1}(k ) ] [ h_{\\alpha^\\circ}(k)-h_{\\alpha^\\circ\\pm\\alpha_1}(k)]b^2(k)\\\\ & \\le&\\sum_{k=1}^n [ h_{\\alpha^\\circ}(k)-h_{\\alpha^\\circ\\pm\\alpha_1}(k ) ] [ h_{\\alpha^\\circ}(k)-h_{\\alpha^\\circ\\pm\\alpha_2}(k)]b^2(k ) \\\\ & = & \\mathbf{e}\\xi^\\pm_{\\alpha_1}\\xi^\\pm_{\\alpha_2}.\\end{aligned}\\ ] ]    therefore , according to lemma [ lemma.2 ] , we get @xmath237 ^ 2 b^2(k ) \\biggr)^{q/2 } \\biggr]_+^p & \\le&\\frac{c(p , q)}{z^{p/(q-1 ) } } , \\\\",
    "\\mathbf{e}\\sup_{0\\le\\alpha\\le \\alpha^\\circ } \\biggl[\\xi^-_{\\alpha}-z\\biggl(\\sum_{k=1}^n [ h_{\\alpha^\\circ}(k)-h_{\\alpha^\\circ-\\alpha}(k)]^2 b^2(k ) \\biggr)^{q/2 } \\biggr]_+^p & \\le&\\frac{c(p , q)}{z^{p/(q-1)}},\\end{aligned}\\ ] ] and combining these inequalities we arrive at the following lemma .    [ lemma.4 ] for any @xmath209 , @xmath238 b(k ) \\xi(k ) \\\\ & & \\qquad\\hspace*{17.77pt } { } -z\\biggl[\\sum_{k=1}^n [ h_{\\alpha^\\circ}(k)-h_\\alpha(k)]^2 b^2(k ) \\biggr]^{q/2 } \\biggr\\}_+^p \\le\\frac{c(p , q)}{z^{p/(q-1)}}.\\end{aligned}\\ ] ]    the next fact is essential for bounding cross terms in the empirical risk .",
    "[ lemma.5 ] let @xmath239 be a given smoothing parameter .",
    "then for any @xmath240 , there exists a constant @xmath241 such that for any data - driven smoothing parameter @xmath82 , @xmath242\\lambda^{-1/2}(k ) \\theta(k ) \\xi(k ) \\biggr|^p\\nonumber\\\\ & & \\qquad \\le c(p ) \\bigl\\{\\mathbf{e}\\max_{k } \\lambda^{-1}(k ) h_{\\hat\\alpha}^2(k ) \\bigr\\}^{p/2 } \\biggl\\ { \\sum_{k=1}^\\infty [ 1-h_{\\alpha^\\circ}(k)]^2\\theta^2(k ) \\biggr\\}^{p/2 } \\\\ & & \\qquad\\quad { } + c(p ) \\bigl\\{\\max_{k } \\lambda^{-1}(k ) h_{\\alpha^\\circ}^2(k ) \\bigr\\}^{p/2 } \\biggl\\{\\mathbf{e } \\sum_{k=1}^\\infty [ 1-h_{\\hat\\alpha}(k)]^2\\theta^2(k ) \\biggr\\}^{p/2}.\\nonumber\\end{aligned}\\ ] ]    from lemmas [ lemma.4 ] and [ lemma.3 ] , it follows that @xmath243\\lambda^{-1/2}(k ) \\theta(k ) \\xi(k ) \\biggr|^p\\nonumber\\\\[-8pt]\\\\[-8pt ] & & \\qquad \\le c(p ) \\biggl\\{\\mathbf{e } \\sum_{k=1}^\\infty [ h_{\\alpha^\\circ}(k)-h_{\\hat\\alpha}(k)]^2\\lambda^{-1}(k)\\theta^2(k ) \\biggr\\}^{p/2}.\\nonumber\\end{aligned}\\ ] ]    to bound from above the right - hand side of ( [ equ.19 ] ) , we use that @xmath244 is a family of ordered smoothers . with this in mind , let us assume for definiteness that @xmath245 for @xmath235 . then ,",
    "if @xmath246 , we get @xmath247 and thus we obtain @xmath248 ^ 2\\lambda^{-1}(k)\\theta^2(k ) \\nonumber\\\\ & & \\qquad = \\sum_{k=1}^\\infty h^2_{\\alpha^\\circ}(k ) \\biggl[1-\\frac{h_{\\hat\\alpha}(k)}{h_{\\alpha^\\circ}(k ) } \\biggr]^2\\lambda^{-2}(k ) \\theta^2(k)\\\\ & & \\qquad \\le\\max_k \\lambda^{-1}(k ) h_{\\alpha^\\circ}^2(k ) \\sum _ { k=1}^\\infty [ 1-h_{\\hat\\alpha}(k)]^2\\theta^2(k).\\nonumber\\end{aligned}\\ ] ] similarly , if @xmath249 , then @xmath250 ^ 2\\lambda^{-1}(k)\\theta^2(k ) \\nonumber\\\\ & & \\qquad \\le\\max_k \\lambda^{-1}(k ) h_{\\hat\\alpha}^2(k ) \\sum_{k=1}^\\infty [ 1-h_{\\alpha^\\circ}(k)]^2\\theta^2(k)\\\\ & & \\qquad\\quad { } + \\max_k \\lambda^{-1}(k ) h_{\\alpha^\\circ}^2(k ) \\sum_{k=1}^\\infty [ 1-h_{\\hat\\alpha}(k)]^2\\theta^2(k).\\nonumber\\end{aligned}\\ ] ]    therefore , combining ( [ equ.30 ] ) and ( [ equ.31 ] ) , we get ( [ equ.28 ] ) .",
    "the following important ordered process is defined by @xmath251,\\ ] ] where @xmath85 are i.i.d .",
    "@xmath86 . let @xmath252^{1/2}.\\ ] ] it is easy to check that @xmath253 and thus @xmath254 where @xmath255 ^ 2.\\ ] ] hence , in order to apply lemma [ lemma.2 ] , it remains to check that for some @xmath256 @xmath257 < \\infty.\\ ] ]    we have @xmath258\\nonumber\\\\ & & \\qquad = \\exp\\biggl\\{-\\frac{\\lambda}{\\sqrt{2}\\|h_{\\alpha_1}-h_{\\alpha_2}\\| _ b}\\sum_{k=1}^n b(k)[h_{\\alpha_1}(k)-h_{\\alpha_2 } ( k ) ] \\\\ & & \\qquad\\quad\\hspace*{20.58pt } { } -\\frac{1}{2}\\sum_{k=1}^n \\log\\biggl[1-\\sqrt{2}\\lambda\\frac{b(k)[h_{\\alpha_1}(k)-h_{\\alpha _ 2}(k)]}{\\|h_{\\alpha_1}-h_{\\alpha_2}\\|_b } \\biggr ] \\biggr\\}.\\nonumber\\end{aligned}\\ ] ] since obviously @xmath259 then using the taylor expansion for @xmath260 at the right - hand side of ( [ equ.32 ] ) , we get for @xmath261 @xmath262 thus proving ( [ equ.23 ] ) .",
    "hence , with the help of lemma [ lemma.2 ] we obtain the following fact .",
    "[ lemma.6 ] for any @xmath209 , @xmath263 } \\biggl[\\sum_{k=1}^n h_\\alpha(k)b(k)[\\xi^2(k)-1]-z \\biggl[2\\sum_{k=1}^n h_\\alpha^2(k)b^2(k ) \\biggr]^{q/2 } \\biggr]_+^p \\le \\frac{c(p , q)}{z^{p/(q-1)}}.\\ ] ]      the next lemma describes some basic properties of the universal penalty defined by ( [ equ.12])([equ.14 ] ) .",
    "[ lemma.7 ] for any @xmath264 $ ] , @xmath265    it follows from ( [ equ.12])([equ.14 ] ) that @xmath266+\\mu_\\alpha\\rho _",
    "\\alpha(k ) \\biggr\\}=\\log\\frac{d(\\alpha)}{d(\\bar{\\alpha})}\\ ] ] and together with the inequality @xmath267 , we get ( [ equ.33 ] ) .    to verify ( [ equ.34 ] ) , note that @xmath268 and therefore , for any",
    "@xmath269 $ ] , @xmath270\\le4\\mu^2.\\ ] ] so , if @xmath271 , then @xmath272=\\frac14\\log\\frac{d(\\alpha)}{d(\\bar{\\alpha})},\\ ] ] thus proving ( [ equ.34 ] ) .    next ,",
    "note that the following inequality holds : @xmath273 since @xmath274 is a nonnegative function for @xmath275 because @xmath276 and @xmath277 .",
    "according to ( [ equ.38 ] ) , @xmath278 and therefore , by ( [ equ.13 ] ) , @xmath279 substituting this inequality into ( [ equ.33 ] ) , we get ( [ equ.35 ] ) .",
    "we now turn to the proof of ( [ equ.36 ] ) .",
    "again , combining ( [ equ.38 ] ) with ( [ equ.12])([equ.14 ] ) , we arrive at @xmath280\\nonumber\\\\[-8pt]\\\\[-8pt ] & = & \\frac{2d(\\alpha)}{\\mu_\\alpha } \\log \\frac{d(\\alpha)}{d(\\bar{\\alpha})}\\nonumber\\end{aligned}\\ ] ] and to get ( [ equ.36 ] ) it remains to invert this equation .",
    "we proceed to show that if @xmath281 , then the inequality @xmath282 implies @xmath283 it is clear that @xmath284 is an increasing function when @xmath285 and ( [ equ.40 ] ) holds since @xmath286 inverting ( [ equ.39 ] ) with the help of ( [ equ.40 ] ) , we finish the proof of ( [ equ.36 ] ) .",
    "finally , ( [ equ.37 ] ) follows from the fact that @xmath287 is a decreasing function in @xmath288 . to check this ,",
    "let us note that @xmath289 is a root of the equation @xmath290 \\bigr\\}=\\frac{d(\\bar{\\alpha})}{d(\\alpha)},\\ ] ] where @xmath291 \\biggr].\\ ] ] however @xmath292 $ ] is obviously a decreasing function in @xmath293 and therefore if @xmath294 is decreasing in @xmath36 , then @xmath289 is decreasing in @xmath36 too .",
    "we are now in a position to prove theorem [ theorem.1 ] .",
    "let @xmath295 be the decreasing sequence defined as follows : @xmath296 where @xmath297 is a small positive number which will be chosen later on , and @xmath298 is a root of equation @xmath299 denote for brevity @xmath300 and @xmath301 .",
    "we begin with the simple inequality @xmath302_+^{1+r}\\\\ & & \\qquad \\le\\sum_{k=1}^n \\mathbf{e}\\sup_{\\alpha_{k}\\le\\alpha < \\alpha _",
    "{ k-1 } } [ \\zeta_\\alpha-(1+\\gamma){q}_{k-1 } ] _ + ^{1+r}\\\\ & & \\qquad = \\sum_{k=1}^n \\mathbf{e } \\bigl[\\zeta_{\\alpha_{k}}-\\varepsilon\\gamma { q}_{k-1}+\\sup_{\\alpha_{k}\\le\\alpha",
    "< \\alpha_{k-1 } } [ \\zeta_\\alpha-\\zeta_{\\alpha_{k}}]-(1+\\gamma-\\varepsilon\\gamma ) { q}_{k-1 } \\bigr]_+^{1+r}.\\end{aligned}\\ ] ] using that @xmath303_+^{1+r}\\le2^r[x]_+^{1+r}+2^r[y]_+^{1+r}$ ] , we can continue the above equation as follows : @xmath304_+^{1+r } \\nonumber\\\\ & & \\qquad \\le2^r\\sum_{k=1}^n \\mathbf{e } [ \\zeta_{\\alpha_{k}}-(1+\\gamma-\\varepsilon\\gamma){q}_{k-1 } ] _",
    "+ ^{1+r}\\\\ & & \\qquad\\quad { } + 2^r\\sum_{k=1}^n \\mathbf{e } \\bigl[\\sup_{\\alpha_{k}\\le\\alpha < \\alpha_{k-1}}[\\zeta_\\alpha-\\zeta_{\\alpha_{k}}]-\\varepsilon \\gamma{q}_{k-1 } \\bigr]_+^{1+r}.\\nonumber\\end{aligned}\\ ] ]    we control the first term @xmath305_+^{1+r}\\ ] ] at the right - hand side of ( [ equ.41 ] ) with the help of ( [ equ.11 ] ) .",
    "thus , we obtain for any @xmath306 @xmath307_+^{1+r}\\nonumber\\\\ & \\le & c{d}^{1+r}(\\bar{\\alpha})\\nonumber\\\\ & & { } + \\gamma(1+r)\\sum_{k=2}^n \\tilde{\\lambda}^{-1-r}_{k } { d}^{r}_{k}\\exp\\biggl\\{-\\tilde{\\lambda}_{k } \\frac{{q}_{k}}{{d}_{k } } \\biggl [ \\frac{\\gamma(1-\\varepsilon){q}_{k-1}}{{q}_{k } } \\\\ & & \\qquad\\quad\\hspace*{141.6pt } { } - \\biggl(1-\\frac{{q}_{k-1}}{{q}_k } \\biggr ) \\biggr ] \\biggr\\}\\nonumber\\\\ & & \\qquad\\quad\\hspace*{-23pt}{}\\times { d}_{k}\\mathbf{e}\\exp\\biggl(\\tilde{\\lambda}_{k } \\frac{\\zeta_{\\alpha_{k}}}{{d}_{k}}-\\tilde{\\lambda}_{k } \\frac{{q}_{k}}{{d}_{k } } \\biggr).\\nonumber\\end{aligned}\\ ] ] according to ( [ equ.12 ] ) and ( [ equ.13 ] ) , we have with @xmath308 @xmath309 and substituting this into ( [ equ.42 ] ) , we get @xmath310\\\\[-8pt ] & & \\hspace*{175.5pt } { } + r\\log \\frac{{d}_{k}}{d_0 } \\biggr ] \\biggr\\}.\\nonumber\\end{aligned}\\ ] ] since by ( [ equ.33 ] ) @xmath311 and according to ( [ equ.34 ] ) , @xmath312 is bounded from below by a constant , we obtain from ( [ equ.43.1 ] ) @xmath313 \\biggr\\}.\\ ] ] next , according to ( [ equ.36 ] ) and ( [ equ.34 ] ) , we get @xmath314 and with this inequality we obtain from ( [ equ.43.2 ] ) @xmath315 \\biggr\\}\\\\ & \\le & cd^{1+r}_0\\sum_{k=2}^n \\exp\\biggl\\{- \\biggl(\\frac{\\gamma(1-\\varepsilon)-\\delta } { 1+\\delta}-r \\biggr ) [ k\\log(1+\\delta)-\\log(k\\delta ) ] \\biggr\\}.\\end{aligned}\\ ] ] finally , one can check by the laplace method that @xmath316\\le\\frac{c}{z_1 } \\biggl(\\frac { z_2}{z_1 } \\biggr)^{z_2},\\qquad z_1,z_2 > 0,\\ ] ] thus yielding @xmath317_+ } .\\ ] ]    our next step is to bound from above the last term in ( [ equ.41 ] ) , namely , @xmath318-\\varepsilon\\gamma{q}_{k-1 } \\bigr]_+^{1+r}.\\ ] ] consider the following random processes : @xmath319.\\ ] ] denote for brevity @xmath320 . noticing that @xmath321 is a family of ordered smoothers , it is easy to check that @xmath322 ^ 2,\\qquad u\\le v.\\ ] ] according to the taylor formula , for all @xmath323 and all @xmath324 , @xmath325 ^ 2 } \\biggr\\}\\le\\exp\\biggl(\\frac{\\lambda^2}{2 } \\biggr),\\ ] ] and applying lemma [ lemma.1 ] and ( [ equ.11 ] ) , we obtain for any @xmath326 @xmath327}\\tilde{\\zeta}_\\alpha(k)-\\varepsilon\\gamma{q}_{k-1 } \\bigr]_+^{1+r } \\\\ & \\le & cd^{1+r}_0 + c\\sum_{k=2}^n\\tilde\\sigma_{\\alpha_{k-1}-\\alpha _ { k}}^{1+r}(k)\\lambda_k^{-1-r}\\\\ & & \\hspace*{69pt}{}\\times \\exp\\biggl[- \\frac { \\lambda_k\\varepsilon\\gamma{q}_{k-1}}{\\tilde\\sigma _ { \\alpha_{k-1}-\\alpha_{k}}(k ) } + \\frac{\\lambda_k^2}{(\\sqrt{2}-1)^2 } \\biggr].\\end{aligned}\\ ] ] substituting @xmath328 into the above equation and noticing that @xmath329 , we obtain @xmath330 \\nonumber\\\\[-8pt]\\\\[-8pt ] & \\le & cd^{1+r}_0 + \\frac{c}{(\\varepsilon\\gamma)^{1+r}}\\sum _ { k=2}^n[{d}^2_k-{d}^2_{k-1}]^{1+r } { q}^{-1-r}_{k-1 } \\nonumber\\\\ & & \\hspace*{98.3pt}{}\\times \\exp\\biggl[- \\frac { ( \\sqrt{2}-1)^2\\varepsilon^2\\gamma ^2{q}^2_{k-1}}{4({d}^2_k-{d}^2_{k-1 } ) } \\biggr].\\nonumber\\end{aligned}\\ ] ] according to ( [ equ.37 ] ) , @xmath331 and with this inequality we continue ( [ equ.44.1 ] ) as follows : @xmath332.\\end{aligned}\\ ] ] next , substituting ( [ equ.33.x ] ) and ( [ equ.34.36 ] ) into this equation , we get @xmath333 \\\\ & \\le & cd^{1+r}_0 + \\frac{c\\delta^{1+r}}{(\\varepsilon\\gamma ) ^{1+r}}\\sum_{k=2}^n { q}_{k-1}^{1+r } \\exp\\biggl[- \\frac { c\\varepsilon^2\\gamma^2}{\\delta}\\log^2 \\frac{{q}_{k-1}}{{d}_0 } \\biggr ] \\\\ & \\le & \\frac{c\\delta^{1+r}d^{1+r}_0}{(\\varepsilon\\gamma)^{1+r}}\\sum_{k=2}^n \\exp\\biggl[\\delta(r+1 ) k-\\frac{c\\varepsilon^2\\gamma^2 } { \\delta}[k\\log ( 1+\\delta)-\\log(\\delta k)]^2 \\biggr].\\end{aligned}\\ ] ] bounding the last sum in this display with the help of the laplace method , we get @xmath334 ^ 2 \\biggr\\}\\\\ & & \\qquad\\le\\exp\\biggl[\\frac{c\\delta ^3}{\\varepsilon^2\\gamma^2\\log^2(1+\\delta ) } \\biggr]\\frac{c\\sqrt{\\delta}}{\\varepsilon\\gamma\\log(1+\\delta)},\\end{aligned}\\ ] ] and therefore with @xmath335 we obtain @xmath336 with this @xmath337 , equation ( [ equ.43 ] ) becomes @xmath338_+}.\\ ] ]    let @xmath339 be a positive root of the equation @xmath340 that is , @xmath341 substituting this @xmath339 into ( [ equ.44 ] ) and ( [ equ.45 ] ) and combining the obtained inequalities with ( [ equ.41 ] ) , we finish the proof .",
    "the first step in the proof of this theorem is to show that the data - driven parameter @xmath82 defined by ( [ equ.4 ] ) can not be very small , or equivalently , that the ratio @xmath342 is not large .",
    "[ lemma.8 ] for any given @xmath343 and @xmath344 the following upper bound holds @xmath345^{1+\\gamma /4 }",
    "\\biggr\\}^{1/(1+\\gamma/4)}\\le \\frac{c\\bar{r}_{\\alpha^\\circ}(\\theta)}{\\sigma^2\\gamma d(\\bar { \\alpha } ) } \\log^{-1/2}\\frac{c\\bar{r}_{\\alpha^\\circ}(\\theta)}{\\sigma^2d(\\bar { \\alpha } ) } + \\frac{c}{\\gamma^4}.\\ ] ]    according to the definition of the empirical risk minimization , for any given @xmath239 , @xmath346\\le r_{\\alpha ^\\circ}[y,\\operatorname{pen}]$ ] .",
    "one can check with a little algebra that this inequality is equivalent to [ see ( [ equ.7 ] ) ] @xmath347 ^ 2\\langle \\theta,\\psi_k\\rangle^2 + \\sigma^2\\sum_{k=1}^n \\lambda^{-1}(k ) h_{\\hat\\alpha}^2(k)\\nonumber\\\\ & & \\quad { } -\\sigma^2 \\sum_{k=1}^n \\lambda^{-1}(k)\\tilde{h}_{\\hat\\alpha } ( k)[\\xi^2(k)-1]+(1+\\gamma ) \\sigma^2q^{\\circ}(\\hat\\alpha)\\nonumber\\\\ & & \\quad { } + 2\\sigma\\sum_{k=1}^n \\lambda^{-1/2}(k)[1-h_{\\hat\\alpha } ( k)]^2\\xi(k)\\theta(k ) \\nonumber\\\\[-8pt]\\\\[-8pt ] & & \\qquad\\le\\sum_{k=1}^n[1-h_{\\alpha^\\circ}(k)]^2\\langle \\theta,\\psi_k\\rangle^2 + \\sigma^2\\sum_{k=1}^n \\lambda^{-1}(k ) h_{\\alpha^\\circ}^2(k)\\nonumber\\\\ & & \\qquad\\quad { } -\\sigma^2 \\sum_{k=1}^n \\lambda^{-1}(k)\\tilde{h}_{\\alpha^\\circ } ( k)[\\xi^2(k)-1]+(1+\\gamma ) \\sigma^2q^{\\circ}(\\alpha^\\circ)\\nonumber\\\\ & & \\qquad\\quad { } + 2\\sigma\\sum_{k=1}^n \\lambda^{-1/2}(k)[1-h_{\\alpha^\\circ } ( k)]^2\\xi(k)\\theta(k),\\nonumber\\end{aligned}\\ ] ] where @xmath348 .",
    "next , representing @xmath349 we obtain from ( [ equ.47 ] ) @xmath350\\nonumber\\\\ & & { } + \\sigma^2\\sup_{\\alpha\\le\\bar{\\alpha } } \\biggl[\\sum_{k=1}^n \\lambda ^{-1}(k)\\tilde{h}_{\\alpha}(k)[\\xi^2(k)-1 ] - \\biggl(1+\\frac{\\gamma}{2 } \\biggr ) q^{\\circ}(\\alpha ) \\biggr]_+\\\\ & & { } + 2\\sigma\\sum_{k=1}^n \\lambda^{-1/2}(k)[\\tilde{h}_{\\hat\\alpha } ( k)-\\tilde{h}_{\\alpha^\\circ}(k ) ] \\xi(k)\\theta(k ) - l_{\\hat\\alpha}(\\theta).\\nonumber\\end{aligned}\\ ] ]    since @xmath239 is fixed , we get by jensen s inequality @xmath351 \\biggr|^{1+\\gamma/4 } & \\le & c \\biggl[\\sum_{k=1}^n \\lambda^{-2}(k)\\tilde{h}_{\\alpha^\\circ}^2(k ) \\biggr]^{1/2+\\gamma/8}\\nonumber\\\\ & = & c[d(\\alpha^\\circ)]^{1+\\gamma/4}\\\\ & \\le & c[\\sigma^{-2}\\bar{r}_{\\alpha^\\circ}(\\theta)]^{1+\\gamma/4}.\\nonumber\\end{aligned}\\ ] ]    next , by theorem [ theorem.1 ] , @xmath352 - \\biggl(1+\\frac{\\gamma}{2 } \\biggr ) q^{\\circ}(\\alpha ) \\biggr]_+^{1+\\gamma/4}\\nonumber\\\\[-8pt]\\\\[-8pt ] & & \\qquad \\le\\frac{cd^{1+\\gamma/4}(\\bar{\\alpha})}{\\gamma^3}.\\nonumber\\end{aligned}\\ ] ]    the upper bound for the last line in ( [ equ.48 ] ) is a little bit more tricky .",
    "noticing that @xmath353 is a family of ordered smoothers , we get by lemma [ lemma.4 ] that , for any @xmath354 and given @xmath355 , @xmath356\\xi(k)\\theta(k)\\nonumber\\\\ & & \\quad { } - \\varepsilon\\biggl[4\\sigma^2\\sum_{k=1}^n \\lambda^{-1}(k)[\\tilde{h}_{\\hat \\alpha}(k)-\\tilde{h}_{\\alpha^\\circ}(k ) ] ^2\\theta^2(k ) \\biggr]^{p/2 } \\biggr|^{1+\\gamma/4}\\\\ & & \\qquad \\le\\frac{c(p)}{\\varepsilon^{(1+\\gamma/4)/(p-1)}}.\\nonumber\\end{aligned}\\ ] ]    to continue this inequality , note that if @xmath246 , then @xmath357 and therefore @xmath358 ^ 2\\lambda ^{-1}(k)\\theta^2(k ) \\nonumber\\\\ & & \\qquad = \\sum_{k=1}^\\infty \\tilde{h}^2_{\\alpha^\\circ}(k ) \\biggl[1-\\frac{\\tilde{h}_{\\hat\\alpha}(k)}{\\tilde{h}_{\\alpha^\\circ}(k ) } \\biggr]^2\\lambda^{-2}(k ) \\theta^2(k)\\nonumber\\\\[-8pt]\\\\[-8pt ] & & \\qquad \\le\\max_k \\lambda^{-1}(k ) \\tilde{h}_{\\alpha^\\circ}^2(k ) \\sum _ { k=1}^\\infty [ 1-\\tilde{h}_{\\hat\\alpha}(k)]^2\\theta^2(k ) \\nonumber\\\\ & & \\qquad \\le4 \\max_k \\lambda^{-1}(k ) { h}_{\\alpha^\\circ}^2(k ) \\sum _ { k=1}^\\infty [ 1-h_{\\hat\\alpha}(k)]^2\\theta^2(k).\\nonumber\\end{aligned}\\ ] ] analogously , if @xmath249 , then @xmath359 ^ 2\\lambda ^{-1}(k)\\theta^2(k ) \\nonumber\\\\ & & \\qquad \\le\\max_k \\lambda^{-1}(k ) \\tilde{h}_{\\hat\\alpha}^2(k ) \\sum_{k=1}^n [ 1-\\tilde{h}_{\\alpha^\\circ}(k)]^2\\theta^2(k)\\\\ & & \\qquad \\le4\\max_k \\lambda^{-1}(k ) { h}_{\\hat\\alpha}^2(k ) \\sum_{k=1}^n [ 1-{h}_{\\alpha^\\circ}(k)]^2\\theta^2(k).\\nonumber\\end{aligned}\\ ] ]    next , combining ( [ equ.51])([equ.53 ] ) with young s inequality , @xmath360,\\qquad x , y\\ge0 , q<1,\\ ] ] gives @xmath361\\xi(k)\\theta(k ) - l_{\\hat\\alpha}(\\theta ) \\biggr|^{1+\\gamma/4}\\\\ & & \\qquad\\le c\\mathbf{e } \\biggl|2\\sigma\\sum_{k=1}^n \\lambda^{-1/2}(k)[\\tilde { h}_{\\hat\\alpha}(k)-\\tilde{h}_{\\alpha^\\circ}(k ) ] \\xi(k)\\theta(k)\\\\ & & \\qquad\\quad\\hspace*{15.9pt } { } - \\varepsilon\\biggl[4\\sigma^2\\sum_{k=1}^n \\lambda^{-1 } ( k)[\\tilde { h}_{\\hat\\alpha}(k)-\\tilde{h}_{\\alpha^\\circ}(k ) ] ^2\\theta^2(k ) \\biggr]^{p/2 } \\biggr|^{1+\\gamma/4}\\\\ & & \\qquad\\quad { } + c \\mathbf{e } \\biggl| \\varepsilon\\biggl[4\\sigma^2\\sum_{k=1}^n { \\lambda ^{-1}(k)}[\\tilde{h}_{\\hat\\alpha}(k)-\\tilde{h}_{\\alpha^\\circ}(k ) ] ^2\\theta^2(k ) \\biggr]^{p/2 } - l_{\\hat\\alpha}(\\theta ) \\biggr|^{1+\\gamma/4 } \\\\ & & \\qquad \\le\\frac{c}{\\varepsilon^{(1+\\gamma/4)/(p-1 ) } } + \\frac{c}{\\varepsilon ^{2(1+\\gamma/4)/(p-2 ) } } \\bigl[\\sigma^2\\max_k \\lambda^{-1}(k ) { h}_{\\alpha ^\\circ}^2(k ) \\bigr]^{{p(1+\\gamma/4)}/({2-p } ) } \\\\ & & \\qquad\\quad { } + \\frac{c}{\\varepsilon^{2(1+\\gamma/4)/(p-2 ) } } \\biggl\\{\\sum_{k=1}^\\infty [ 1-{h}_{\\alpha^\\circ}(k)]^2\\theta^2(k ) \\biggr\\}^{{p(1+\\gamma/4)}/({2-p})}.\\end{aligned}\\ ] ] therefore , minimizing the right - hand side at the above equation in @xmath354 , we get @xmath362\\xi(k)\\theta(k ) - l_{\\hat\\alpha}(\\theta ) \\biggr|^{1+\\gamma/4}\\\\ & & \\qquad \\le c \\biggl\\{\\sum_{k=1}^\\infty [ 1-{h}_{\\alpha^\\circ}(k)]^2\\theta^2(k)+\\sigma^2\\max_k \\lambda ^{-1}(k ) { h}_{\\alpha^\\circ}^2(k ) \\biggr\\}^{1+\\gamma/4}.\\end{aligned}\\ ] ] this equation and ( [ equ.48])([equ.50 ] ) imply @xmath363^{1+\\gamma/4 } \\le c\\bar{r}_{\\alpha^\\circ}^{1+\\gamma/4}(\\theta)+ \\frac{c[\\sigma ^2d(\\bar{\\alpha})]^{1+\\gamma/4}}{\\gamma^3}\\ ] ] and by ( [ equ.35 ] ) we get @xmath364^{1+\\gamma/4 } \\le c \\biggl[\\frac{\\bar{r}_{\\alpha^\\circ}(\\theta)}{\\sigma^2 d(\\bar { \\alpha } ) } \\biggr]^{1+\\gamma/4}+ \\frac{c}{\\gamma^3}.\\ ] ] it is easily seen that @xmath365^{1+\\gamma/4}\\nonumber\\\\[-8pt]\\\\[-8pt ] & & \\qquad = \\frac{1}{(1+\\gamma/4)^{1/2+\\gamma/8 } } \\mathbf{e } \\biggl[\\frac { d(\\hat\\alpha)}{d(\\bar{\\alpha } ) } \\biggr]^{1+\\gamma/4 } \\biggl[\\log\\biggl(\\frac { d(\\hat\\alpha)}{d(\\bar{\\alpha } ) } \\biggr)^{1+\\gamma/4 } \\biggr]^{1/2+\\gamma/8}.\\nonumber\\end{aligned}\\ ] ]    to finish the proof , let us consider the function @xmath366 , @xmath367 .",
    "computing its second order derivative , one can easily check that @xmath368 is convex for all @xmath369 .",
    "so , @xmath370 is convex for @xmath371 . note also that there exists a constant @xmath115 such that for all @xmath371 , @xmath372 therefore according to ( [ equ.56 ] ) and jensen s inequality , @xmath373 \\\\ & & \\qquad\\quad { } \\times\\biggl\\{\\log\\biggl[\\mathbf{e } \\biggl(\\frac{d(\\hat\\alpha)}{d(\\bar{\\alpha } ) } \\biggr)^{1+\\gamma/4}+\\mathrm{e}-1 \\biggr ] \\biggr\\}^{1/2+\\gamma/8}-c.\\end{aligned}\\ ] ]    finally , substituting this inequality into ( [ equ.55 ] ) and inverting @xmath368 , we arrive at ( [ equ.46 ] ) .",
    "the next lemma controls the cross term in the empirical risk .",
    "[ lemma.9 ] let @xmath374/(1+\\varepsilon)$ ] .",
    "then for any given @xmath375 and @xmath376 $ ] , @xmath377\\theta(k)\\lambda^{-1/2}(k ) \\xi(k ) \\biggr|\\nonumber\\\\ & & \\qquad \\le\\biggl[\\frac{c\\bar{r } _ { \\alpha{^\\circ}}(\\theta)}{\\gamma}\\log^{-1/2}\\frac{c\\bar{r } _",
    "{ \\alpha{^\\circ}}(\\theta)}{\\sigma^2d(\\bar\\alpha)}+\\frac{c\\sigma ^2d(\\bar\\alpha)}{\\gamma^4 } \\biggr]^{1/2}\\\\ & & \\qquad\\quad { } \\times\\biggl [ \\mathbf{e}\\sum_{k=1}^n[1-h_{\\hat\\alpha}(k ) ] ^2\\theta^2(k ) + \\sum_{k=1}^n[1-h_{\\alpha^\\circ}(k ) ] ^2\\theta^2(k ) \\biggr]^{1/2}.\\nonumber\\end{aligned}\\ ] ]    since @xmath378 is a family of ordered smoothers , combining lemma [ lemma.5 ] with the obvious inequalities @xmath379 and @xmath380 , we obtain @xmath381\\theta(k)\\lambda^{-1/2}(k ) \\xi(k ) \\biggr|\\nonumber\\\\ & & \\qquad = 2\\sigma\\mathbf{e } \\biggl| \\sum_{k=1}^n[h_{\\alpha^\\circ}^\\varepsilon(k)-h_{\\hat\\alpha}^\\varepsilon ( k)]\\theta(k ) \\lambda^{-1/2}(k ) \\xi(k ) \\biggr| \\nonumber\\\\[-8pt]\\\\[-8pt ] & & \\qquad \\le c \\sigma\\biggl[\\mathbf{e}d(\\hat\\alpha ) \\sum_{k=1}^n [ 1-{h}_{\\alpha^\\circ}(k)]^2\\theta^2(k ) \\biggr]^{1/2}\\nonumber\\\\ & & \\qquad\\quad { } + c\\sigma\\biggl[d ( \\alpha^\\circ ) \\mathbf{e } \\sum_{k=1}^n [ 1-{h}_{\\hat\\alpha}(k)]^2\\theta^2(k ) \\biggr]^{1/2}.\\nonumber\\end{aligned}\\ ] ]    next , according to ( [ equ.35 ] ) , @xmath382 } $ ] , and we get @xmath383 substituting this inequality and ( [ equ.46 ] ) in ( [ equ.62 ] ) , we obtain ( [ equ.59 ] ) .    we are now in a position to prove theorem [ theorem.2 ] .",
    "let @xmath384 $ ] be a given number to be defined later on . according to ( [ equ.7 ] ) and ( [ equ.8 ] )",
    ", we obtain the following equation for the skewed excess risk : @xmath385+\\mathcal{c}\\ } \\bigr\\}\\nonumber\\\\ & = & \\sup_{\\theta\\in \\mathbb{r}^n}\\mathbf{e } \\biggl\\ { -\\varepsilon \\sum_{k=1}^n[1-h_{\\hat\\alpha}(k)]^2 \\theta^2(k)-\\varepsilon\\sigma^2 \\sum_{k=1}^n \\lambda^{-1}(k ) h_{\\hat\\alpha}^2(k)\\nonumber\\\\ & & \\hspace*{33.4pt } { } - ( 1+\\varepsilon)(1+\\gamma)\\sigma^2 q^{\\circ}(\\hat\\alpha ) \\nonumber\\\\[-8pt]\\\\[-8pt ] & & \\hspace*{33.4pt } { } -2\\sigma \\sum_{k=1}^n\\{1+\\varepsilon-[(1 + 2\\varepsilon)h_{\\hat\\alpha } ( k)-\\varepsilon h_{\\hat\\alpha}^2(k ) ] \\}\\nonumber\\\\ & & \\hspace*{72.8pt}{}\\times\\theta(k)\\lambda^{-1/2}(k ) \\xi(k ) \\nonumber\\\\ & & \\hspace*{33.4pt } { } + \\sigma^2\\sum_{k=1}^n \\lambda^{-1}(k)[2(1+\\varepsilon ) h_{\\hat\\alpha}(k)-\\varepsilon h_{\\hat \\alpha}^2(k)][\\xi^2(k)-1 ] \\biggr\\}.\\nonumber\\end{aligned}\\ ] ] to control the last line at the right - hand side of this equation , we use that @xmath386/(2+\\varepsilon)$ ] is a family of ordered smoothers .",
    "hence , lemmas [ lemma.3 ] , [ lemma.6 ] and [ lemma.8 ] imply @xmath387[\\xi^2(k)-1]\\nonumber\\\\[-8pt]\\\\[-8pt ] & & \\qquad\\le c \\frac{\\bar{r}_{\\alpha^\\circ}(\\theta)}{\\gamma } \\log^{-1/2}\\frac{c\\bar{r}_{\\alpha^\\circ}(\\theta)}{\\sigma^2\\gamma d(\\bar{\\alpha})}+\\frac{c\\sigma^2d(\\bar{\\alpha})}{\\gamma^4}.\\nonumber\\end{aligned}\\ ] ]    next , substituting ( [ equ.58 ] ) and ( [ equ.59 ] ) into ( [ equ.57 ] ) , we obtain the following upper bound for the skewed excess risk : @xmath388.\\ ] ]    finally , substituting this upper bound into @xmath389 and minimizing the obtained inequality in @xmath339 , we get @xmath390 \\biggr\\}\\\\ & \\le & r(\\theta ) \\biggl\\{1 + \\biggl[\\frac{c}{\\gamma } \\log^{-1/2}\\frac{cr(\\theta)}{\\sigma^2 d(\\bar{\\alpha})}+\\frac { c\\sigma^2d(\\bar{\\alpha})}{\\gamma^4r(\\theta ) } \\biggr]^{1/2 } \\biggr\\},\\end{aligned}\\ ] ] thus finishing the proof .",
    "the author wishes to thank two anonymous referees for stimulating comments and remarks .",
    "akaike , h. ( 1973 ) .",
    "information theory and an extension of the maximum likelihood principle . in _ proc .",
    "2nd intern .",
    "inf . theory _ ( p. n. petrov and f. csaki , eds . ) 267281 .",
    "akadmiai kiad , budapest .",
    "bissantz , n. , hohage , t. , munk , a. and ruymgaart , f. ( 2007 ) .",
    "convergence rates of general regularization methods for statistical inverse problems and applications .",
    "_ siam j. numer .",
    "* 45 * 26102636 ."
  ],
  "abstract_text": [
    "<S> this paper deals with recovering an unknown vector @xmath0 from the noisy data @xmath1 , where @xmath2 is a known @xmath3-matrix and @xmath4 is a white gaussian noise . </S>",
    "<S> it is assumed that @xmath5 is large and @xmath2 may be severely ill - posed . </S>",
    "<S> therefore , in order to estimate @xmath0 , a spectral regularization method is used , and our goal is to choose its regularization parameter with the help of the data @xmath6 . for spectral regularization methods related to the so - called ordered smoothers [ see @xcite _ ann . </S>",
    "<S> statist . </S>",
    "<S> _ * 22 * ( @xcite ) 835866 ] , we propose new penalties in the principle of empirical risk minimization . </S>",
    "<S> the heuristical idea behind these penalties is related to balancing excess risks . </S>",
    "<S> based on this approach , we derive a sharp oracle inequality controlling the mean square risks of data - driven spectral regularization methods .    .    . </S>"
  ]
}