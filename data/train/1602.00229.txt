{
  "article_text": [
    "signal processing problems such as coding , restoration , classification , regression or synthesis greatly depend on an appropriate description of the underlying probability density function ( pdf ) @xcite . however , density estimation is a challenging problem when dealing with high - dimensional signals because direct sampling of the input space is not an easy task due to the curse of dimensionality @xcite . as a result",
    ", specific problem - oriented pdf models are typically developed to be used in the bayesian framework .",
    "the conventional approach is to transform data into a domain where _ interesting _ features can be easily ( i.e. marginally ) characterized . in that case , one can apply well - known marginal techniques to each feature independently and then obtain a description of the multidimensional pdf .",
    "the most popular approaches rely on linear models and statistical independence . however , they are usually too restrictive to describe general data distributions . for instance ,",
    "principal component analysis ( pca ) @xcite , that reduces to dct in many natural signals such as speech , images and video , assumes a gaussian source @xcite .",
    "more recently , linear ica , that reduces to wavelets in natural signals , assumes that observations come from the linear combination of independent non - gaussian sources @xcite . in general",
    ", these assumptions may not be completely correct , and residual dependencies still remain after the linear transform that looks for independence . as a result , a number of problem - oriented approaches have been developed in the last decade to either describe or remove the relations remaining in these linear domains .",
    "for example , parametric models based on joint statistics of wavelet coefficients have been successfully proposed for texture analysis and synthesis @xcite , image coding @xcite or image denoising @xcite .",
    "non - linear methods using non - explicit statistical models have been also proposed to this end in the denoising context @xcite and in the coding context @xcite . in function approximation and classification problems",
    ", a common approach is to first linearly transform the data , e.g. with the most relevant eigenvectors from pca , and then applying nonlinear methods such as artificial neural networks or support vector machines in the reduced dimensionality space @xcite .    identifying the _ meaningful _ transform for an easier pdf description in the transformed domain",
    "strongly depends on the problem at hand . in this work",
    "we circumvent this constraint by looking for a transform such that the transformed pdf is known . even in the case",
    "that this transform is qualitatively _ meaningless _ , being differentiable , allows us to estimate the pdf in the original domain .",
    "accordingly , in the proposed context , the role ( _ meaningfulness _ ) of the transform is not that relevant .",
    "actually , as we will see , an infinite family of transforms may be suitable to this end , so one has the freedom to choose the most convenient one .    in this work ,",
    "we propose to use a unit covariance gaussian as target pdf in the transformed domain and iterative transforms based on arbitrary rotations .",
    "we do so because the match between spherical symmetry and rotations makes it possible to define a cost function ( negentropy ) with nice theoretical properties .",
    "the properties of negentropy allow us to show that one gaussianization transform is always found no matter the selected class of rotations .",
    "the remainder of the paper is organized as follows . in section [ motivation ]",
    "we present the underlying idea that motivates the proposed approach to gaussianization . in section [ ourapproach ] ,",
    "we give the formal definition of the rotation - based iterative gaussianization ( rbig ) , and show that the scheme is invertible , differentiable and it converges for a wide class of orthonormal transforms , even including random rotations .",
    "section [ relationspp ] discusses the similarities and differences of the proposed method and projection pursuit ( pp ) @xcite .",
    "links to other techniques ( such as single - step gaussianization transforms @xcite , one - class support vector domain descriptions ( svdd ) @xcite , and deep neural network architectures @xcite ) are also explored .",
    "section [ applications ] shows the experimental results .",
    "first , we experimentally show that the proposed scheme converges to an appropriate gaussianization transform for a wide class of rotations .",
    "then , we illustrate the usefulness of the method in a number of high - dimensional problems involving pdf estimation : image synthesis , classification , denoising and multi - information estimation . in all cases ,",
    "rbig is compared to related methods in each particular application .",
    "finally , section [ conclusions ] draws the conclusions of the work .",
    "this section considers a solution to the pdf estimation problem by using a differentiable transform to a domain with known pdf . in this setting , different approaches can be adopted which will motivate the proposed method .",
    "let @xmath0 be a @xmath1-dimensional random variable with ( unknown ) pdf , @xmath2 .",
    "given some bijective , differentiable transform of @xmath0 into @xmath3 , @xmath4 , such that @xmath5 , the pdfs in the original and the transformed domains are related by @xcite :    @xmath6    where @xmath7 is the determinant of the jacobian matrix .",
    "therefore , the unknown pdf in the original domain can be estimated from a transform of known jacobian leading to an appropriate ( known or straightforward to compute ) target pdf , @xmath8 .",
    "one could certainly try to figure out direct ( or even closed form ) procedures to transform particular pdf classes into a target pdf @xcite .",
    "however , in order to deal with any possible pdf , iterative methods seem to be a more reasonable approach . in this case",
    ", the initial data distribution should be iteratively transformed in such a way that the target pdf is progressively approached in each iteration .    the appropriate transform in each iteration would be the one that maximizes a similarity measure between pdfs . a sensible cost function here is the kullback - leibler divergence ( kld ) between pdfs . in order to apply well - known properties of this measure @xcite , it is convenient to choose a unit covariance gaussian as target pdf , @xmath9 . with this choice , the cost function describing the divergence between the current data , @xmath0 , and",
    "the unit covariance gaussian is the hereafter called negentropy and a multivariate gaussian of the same mean and covariance .",
    "however , note that this difference has no consequence assuming the appropriate input data standardization ( zero mean and unit covariance ) , which can be done without loss of generality . ] , @xmath10 = @xmath11 .",
    "negentropy can be decomposed as the sum of two non - negative quantities , the multi - information and the marginal negentropy : @xmath12 this can be readily derived from eq .",
    "( 5 ) in @xcite , by considering as contrast pdf @xmath13 .",
    "the multi - information is @xcite : @xmath14 multi - information measures statistical dependence , and it is zero if and only if the different components of @xmath0 are independent . the marginal negentropy is defined as : @xmath15    given a data distribution from the unknown pdf , in general both @xmath16 and @xmath17 will be non - zero . the decomposition in suggests two alternative approaches to reduce @xmath18 :    1 .   _ reducing @xmath16 _ : this implies looking for interesting ( independent ) components . if one is able to obtain @xmath19 , then @xmath20 , and this reduces to solving a marginal problem .",
    "marginal negentropy can be set to zero with the appropriate set of dimension - wise gaussianization transforms , @xmath21 .",
    "this is easy as will be shown in the next section .",
    "+ however , this is an ambitious approach since looking for independent components is a non - trivial ( intrinsically multivariate and nonlinear ) problem . according to this",
    ", linear ica techniques will not succeed in completely removing the multi - information , and thus a nonlinear post - processing is required .",
    "reducing @xmath17 _ : as stated above , this univariate problem is easy to solve by using the appropriate @xmath21 . note that @xmath16 will remain constant since it is invariant under dimension - wise transforms @xcite . in this way",
    ", one ensures that the cost function is reduced by @xmath17 .",
    "then , a further processing has to be taken in order to come back to a situation in which one may have the opportunity to remove @xmath17 again .",
    "this additional transform may consist of applying a rotation @xmath22 to the data , as will be shown in the next section .",
    "the relevant difference between the approaches is that , in the first one , the important part is looking for the interesting representation ( multivariate problem ) , while in the second approach the important part is the univariate gaussianization . in this second case",
    ", the class of rotations has no special qualitative relevance : in fact , marginal gaussianization is the only part reducing the cost function .    [ cols=\"^,^,^,^,^ \" , ]",
    "in this work , we proposed an alternative solution to the pdf estimation problem by using a family of rotation - based iterative gaussianization ( rbig ) transforms .",
    "the proposed procedure looks for differentiable transforms to a gaussian so that the unknown pdf can be computed at any point of the original domain using the jacobian of the transform .",
    "the rbig transform consists of the iterative application of univariate marginal gaussianization followed by a rotation .",
    "we show that a wide class of orthonormal transforms ( including trivial random rotations ) is well suited to gaussianization purposes . the freedom to choose the most convenient rotation is the difference with formally similar techniques , such as projection pursuit , focused on looking for interesting projections ( which is an intrinsically more difficult problem ) . in this way , here we propose to shift the focus from ica to a wider class of rotations since interesting projections as found by ica are not critical to solve the pdf estimation problem in the original domain .",
    "the suitability of multiple rotations to solve the pdf estimation problem may help to revive the interest of classical iterative gaussianization in practical applications . as an illustration , we showed promising results in a number of multidimensional problems such as image synthesis , classification , denoising , and multi - information estimation .",
    "particular issues in each of the possible applications , such as stablishing a convenient family of rotations for a good jacobian or convenient criteria to ensure the generalization ability , are a matter for future research .",
    "the authors would like to thank matthias bethge for his constructive criticism to the work , and eero simoncelli for the estimulating discussion on ` meaningful - vs - meaningless transforms ' .",
    "athinodoros  s. georghiades , peter  n. belhumeur , and david  j. kriegman , `` from few to many : illumination cone models for face recognition under variable lighting and pose , '' , vol .",
    "23 , pp . 643660 , 2001 .",
    "l.  gmez - chova , d.  fernndez - prieto , j.  calpe , e.  soria , j.  vila - francs , and g.  camps - valls , `` urban monitoring using multitemporal sar and multispectral data , '' , vol .",
    "4 , pp . 234243 , mar 2006 ,"
  ],
  "abstract_text": [
    "<S> most signal processing problems involve the challenging task of multidimensional probability density function ( pdf ) estimation . in this work , </S>",
    "<S> we propose a solution to this problem by using a family of rotation - based iterative gaussianization ( rbig ) transforms . </S>",
    "<S> the general framework consists of the sequential application of a univariate marginal gaussianization transform followed by an orthonormal transform . </S>",
    "<S> the proposed procedure looks for differentiable transforms to a known pdf so that the unknown pdf can be estimated at any point of the original domain . in particular </S>",
    "<S> , we aim at a zero mean unit covariance gaussian for convenience .    </S>",
    "<S> rbig is formally similar to classical iterative projection pursuit ( pp ) algorithms . </S>",
    "<S> however , we show that , unlike in pp methods , the particular class of rotations used has no special qualitative relevance in this context , since looking for _ interestingness _ is not a critical issue for pdf estimation . </S>",
    "<S> the key difference is that our approach focuses on the univariate part ( marginal gaussianization ) of the problem rather than on the multivariate part ( rotation ) . </S>",
    "<S> this difference implies that one may select the most convenient rotation suited to each practical application .    </S>",
    "<S> the differentiability , invertibility and convergence of rbig are theoretically and experimentally analyzed . </S>",
    "<S> relation to other methods , such as radial gaussianization ( rg ) , one - class support vector domain description ( svdd ) , and deep neural networks ( dnn ) is also pointed out . </S>",
    "<S> the practical performance of rbig is successfully illustrated in a number of multidimensional problems such as image synthesis , classification , denoising , and multi - information estimation .    </S>",
    "<S> gaussianization , independent component analysis ( ica ) , principal component analysis ( pca ) , negentropy , multi - information , probability density estimation , projection pursuit . </S>"
  ]
}