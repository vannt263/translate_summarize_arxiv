{
  "article_text": [
    "in this paper , we analyze statistical properties of semi - supervised learning . in the standard supervised learning , only the labeled data @xmath0 is observed , and the goal is to estimate the relation between @xmath1 and @xmath2 . in semi - supervised learning @xcite ,",
    "the unlabeled data @xmath3 is also obtained in addition to labeled data . in real - world data such as the text data",
    ", we can often obtain both labeled and unlabeled data .",
    "a typical example is that @xmath1 and @xmath2 stand for the text of an article , and the tag of the article , respectively .",
    "tagging the article demands a lot of effort .",
    "hence , the labeled data is scarce , while the unlabeled data is abundant . in semi - supervised learning",
    ", studying methods of exploiting unlabeled data is an important issue .    in the standard",
    "semi - supervised learning , statistical models of the joint probability @xmath4 , i.e. , generative models , are often used to incorporate the information involved in the unlabeled data into the estimation .",
    "for example , under the statistical model @xmath5 having the parameter @xmath6 , the information involved in the unlabeled data is used to estimate the parameter @xmath6 via the marginal probability @xmath7 .",
    "the amount of information in unlabeled samples is studied by @xcite .",
    "this approach is developed to deal with a various data structures .",
    "for example , semi - supervised learning with manifold assumption or cluster assumption has been studied along this line @xcite . under some assumptions on generative models",
    ", it is revealed that unlabeled data is useful to improve the prediction accuracy .",
    "statistical models of the conditional probability @xmath8 , i.e. , discriminative models , are also used in semi - supervised learning .",
    "it seems that the unlabeled data is not useful that much for the estimation of the conditional probability , since the marginal probability does not have any information on @xmath8 @xcite .",
    "indeed , the maximum likelihood estimator using a parametric model of @xmath8 is not affected by the unlabeled data .",
    "sokolovska , et al .",
    "@xcite , however , proved that even under discriminative models , unlabeled data is still useful to improve the prediction accuracy of the learning method with only labeled data .",
    "semi - supervised learning methods basically work well under some assumptions on the population distribution and the statistical models .",
    "however , it was also reported that the semi - supervised learning has a possibility to degrade the estimation accuracy , especially when a misspecified model is applied @xcite . hence , a _",
    "safe _ semi - supervised learning is desired .",
    "the learning algorithms proposed by sokolovska , et al .",
    "@xcite and li and zhou @xcite have a theoretical guarantee such that the unlabeled data does not degrade the estimation accuracy . in this paper",
    ", we develop the study of @xcite . to incorporate the information involved in unlabeled data into the estimator , sokolovska , et al .",
    "@xcite used the weighted estimator . in the estimation of the weight function , a well - specified model for the marginal probability @xmath9",
    "was assumed .",
    "this is a strong assumption for semi - supervised learning . to overcome the drawback ,",
    "we apply the density - ratio estimator for the estimation of the weight function @xcite .",
    "we prove that the semi - supervised learning with the density - ratio estimation improves the standard supervised learning .",
    "our method is available not only classification problems but also regression problems , while many semi - supervised learning methods focus on binary classification problems .",
    "this paper is organized as follows . in section [ sec : problem_setup ] , we show the problem setup . in section [ sec : inference_using_weighted_estimator ] , we introduce the weighted estimator investigated by sokolovska ,  et al.,@xcite . in section [ sec : density - ratio_estimation ] , we briefly explain the density - ratio estimation . in section",
    "[ sec : semi - supervised_learning_with_density - ratio_estimation ] , the asymptotic variance of the estimators under consideration is studied .",
    "section [ sec : maximum_improvement_ssl ] is devoted to prove that the weighted estimator using labeled and unlabeled data outperforms the supervised learning using only labeled data . in section [ sec : numerical_experiments ] , numerical experiments are presented .",
    "we conclude in section [ sec : conclusion ] .",
    "we introduce the problem setup .",
    "we suppose that the probability distribution of training samples is given as @xmath10 where @xmath8 is the conditional probability of @xmath11 given @xmath12 , and @xmath9 and @xmath13 are the marginal probabilities on @xmath14 . here , @xmath13 is regarded as the probability in the testing phase , i.e. , the test data @xmath0 is distributed from the joint probability @xmath15 , and the estimation accuracy is evaluated under the test probability .",
    "the paired sample @xmath16 is called `` labeled data '' , and the unpaired sample @xmath17 is called `` unlabeled data '' . our goal is to estimate the conditional probability @xmath8 or the conditional expectation @xmath18 $ ] based on the labeled and unlabeled data in .",
    "when @xmath19 is a finite set , the problem is called the classification problem . for @xmath20 , the estimation of @xmath18 $ ]",
    "is referred to as the regression problem .",
    "we describe the assumption on the marginal distributions , @xmath9 and @xmath13 in . in the context of the _ covariate shift _",
    "adaptation @xcite , the assumption that @xmath21 is employed in general .",
    "the weighted estimator with the weight function @xmath22 is used to correct the estimation bias induced by the covariate shift ; see @xcite for details .",
    "hence , the estimation of the weight function @xmath22 is important to achieve a good estimation accuracy . on the other hand , in the _ semi - supervised learning _",
    "@xcite , the equality @xmath23 is assumed , and often @xmath24 is much larger than @xmath25 .",
    "this setup is also quite practical .",
    "for example , in the text data mining , the labeled data is scarce , while the unlabeled data is abundant . in this paper , we assume that the equality @xmath26 holds .",
    "we define the following semiparametric model , @xmath27 for the estimation of the conditional probability @xmath8 , where @xmath28 is the set of all probability densities of the covariate @xmath1 .",
    "the parameter of interest is @xmath29 , and @xmath30 is the nuisance parameter",
    ". the model @xmath31 does not necessarily include the true probability @xmath15 , i.e. , there may not exist the parameter @xmath29 such that @xmath32 holds .",
    "this is the significant condition , when we consider the improvement of the inference with the labeled and unlabeled data .",
    "our target is to estimate the parameter @xmath33 satisfying @xmath34=e[\\log{}p(y|x;{{\\bm \\alpha}}^ * ) ] ,    \\label{eqn : true_parameter}\\end{aligned}\\ ] ] in which @xmath35 $ ] denotes the expectation with respect to the population distribution .",
    "if the model @xmath31 includes the true probability , we have @xmath36 due to the non - negativity of kullback - leibler divergence @xcite . in the misspecified setup ,",
    "however , the equality @xmath36 is not guaranteed .",
    "we introduce the weighted estimator . for the estimation of @xmath8 under the model",
    ", we consider the maximum likelihood estimator ( mle ) .",
    "for the statistical model @xmath37 , let @xmath38 be the score function @xmath39 where @xmath40 denotes the gradient with respect to the model parameter .",
    "then , for any @xmath41 , we have @xmath42 in addition , the extremal condition of leads to @xmath43 hence , we can estimate the conditional density @xmath8 by @xmath44 , where @xmath45 is a solution of the estimation equation @xmath46 under the regularity condition , the mle has the statistical consistency to the parameter @xmath33 in ; see @xcite for details .",
    "in addition , the score function @xmath47 is an optimal choice among z - estimators @xcite , when the true probability density is included in the model @xmath31 .",
    "this implies that the efficient score of the semiparametric model @xmath31 is the same as the score function of the model @xmath37 .",
    "this is because , in the semiparametric model @xmath31 , the tangent space of the parameter of interest is orthogonal to that of the nuisance parameter . here",
    ", the asymptotic variance matrix of the estimated parameter is employed to compare the estimation accuracy .",
    "next , we consider the setup of the semi - supervised learning . when the model @xmath31 is specified , we find that the estimator using only the labeled data is efficient .",
    "this is obtained from the results of numerous studies about the semiparametric inference with missing data ; see @xcite and references therein .",
    "suppose that the model @xmath31 is misspecified .",
    "then , it is possible to improve the mle in by using the weighted mle @xcite .",
    "the weighted mle is defined as a solution of the equation , @xmath48 where @xmath49 is a weight function .",
    "suppose that @xmath50 . then the law of large numbers leads to the probabilistic convergence , @xmath51 hence the estimator @xmath44 based on will provide a good estimator of @xmath8 under the marginal probability @xmath13 .",
    "this indicates that @xmath44 is expected to approximate @xmath8 over the region on which @xmath13 is large .",
    "the weight function @xmath49 has a role to adjust the bias of the estimator under the covariate shift @xcite . on the setup of the semi - supervised learning , however , @xmath52 holds , and it is known beforehand .",
    "hence , one may think that there is no need to estimate the weight function .",
    "sokolovska , et al.,@xcite showed that estimation of the weight function is useful , even though it is already known in the semi - supervised learning .",
    "we briefly introduce the result in @xcite .",
    "let the set @xmath14 be finite .",
    "then , @xmath28 is a finite dimensional parametric model .",
    "suppose that the sample size of the unlabeled data is enormous , and that the probability function @xmath13 on @xmath14 is known with a high degree of accuracy .",
    "the probability @xmath9 is estimated by the maximum likelihood estimator @xmath53 based on the samples @xmath54 in the labeled data .",
    "then , sokolovska , et al .",
    "@xcite showed that the weighted mle with the estimated weight function @xmath55 improves the naive mle , when the model @xmath31 is misspecified , i.e. , @xmath56 .",
    "shimodaira @xcite pointed out that the weighted mle using the exact density ratio @xmath50 has the statistical consistency to the target parameter @xmath33 , when the covariate shift occurs . under the regularity condition ,",
    "it is rather straightforward to see that the weighted mle using the estimated weight function @xmath55 also converges to @xmath33 in probability , since @xmath53 converges to @xmath9 in probability .",
    "sokolovska s result implies that when @xmath23 holds , the weighted mle using the estimated weight function improves the weighted mle using the true density ratio in the sense of the asymptotic variance of the estimator .",
    "the phenomenon above is similar to the statistical paradox analyzed by @xcite . in the semi - parametric estimation",
    ", henmi and eguchi @xcite pointed out that the estimation accuracy of the parameter of interest can be improved by estimating the nuisance parameter , even when the nuisance parameter is known beforehand .",
    "hirano , et al .",
    ",  @xcite also pointed out that the estimator with the estimated propensity score is more efficient than the estimator using the true propensity score in the estimation of the average treatment effects .",
    "here , the propensity score corresponds to the weight function @xmath49 in our context .",
    "the degree of improvement is described by using the projection of the score function onto the subspace defined by the efficient score for the semi - parametric model . in our analysis , also the projection of the score function @xmath57 plays an important role as shown in section [ sec : maximum_improvement_ssl ] .",
    "for the estimation of the weight function in , we apply the density - ratio estimator @xcite instead of estimating the probability densities separately .",
    "we show that the density - ratio estimator provides a practical method for the semi - supervised learning . in the next section ,",
    "we introduce the density - ratio estimation .",
    "density - ratio estimators are available to estimate the weight function @xmath50 .",
    "recently , methods of the direct estimation for density - ratios have been developed in the machine learning community @xcite .",
    "we apply the density - ratio estimator to estimate the weight function @xmath49 instead of using the estimator of each probability density .",
    "we briefly introduce the density - ratio estimator according to @xcite .",
    "suppose that the following training samples are observed , @xmath58 our goal is to estimate the density - ratio @xmath50 . the @xmath59-dimensional parametric model for the density - ratio",
    "is defined by @xmath60 where @xmath61 is assumed . for any function @xmath62 which may depend on the parameter @xmath63 , one has the equality @xmath64 hence",
    ", the empirical approximation of the above equation is expected to provide an estimation equation of the density - ratio .",
    "the empirical approximation of the above equality under the parametric model of @xmath65 is given as @xmath66 let @xmath67 be a solution of , and then , @xmath68 is an estimator of @xmath49 .",
    "note that we do not need to estimate probability densities @xmath9 and @xmath13 separately .",
    "the estimation equation provides a direct estimator of the density - ratio based on the moment matching with the function @xmath69 .",
    "qin  @xcite proved that the optimal choice of @xmath69 is given as @xmath70 where @xmath71 . by using @xmath69 above",
    ", the asymptotic variance matrix of @xmath67 is minimized among the set of moment matching estimators , when @xmath49 is realized by the model @xmath65 .",
    "hence , is regarded as the counterpart of the score function for parametric probability models .",
    "we study the asymptotics of the weighted mle using the estimated density - ratio . the estimation equation is given as @xmath72 here ,",
    "the statistical models and are employed .",
    "the first equation is used for the estimation of the parameter @xmath29 of the model @xmath73 , and the second equation is used for the estimation of the density - ratio @xmath65 .",
    "the estimator defined by is refereed to as density - ratio estimation based on semi supervised learning , or",
    "_ dress _ for short .    in sokolovska , et al.@xcite ,",
    "the marginal probability density @xmath9 is estimated by using a well - specified parametric model . clearly , preparing the well - specified parametric model is not practical , when @xmath14 is not finite set . on the other hand , it is easy to prepare a specified model of the density - ratio @xmath49 , whenever @xmath23 holds in .",
    "the model is an example .",
    "indeed , @xmath74 holds .",
    "hence , the assumption that the true weight function is realized by the model @xmath65 is not of an obstacle in semi - supervised learning .",
    "we show the asymptotic expansion of the estimation equation .",
    "let @xmath45 and @xmath67 be a solution of .",
    "in addition , define @xmath75 be a solution of @xmath76 and @xmath77 be the parameter such that @xmath78 , i.e. , @xmath79 .",
    "we prepare some notations : @xmath80 , @xmath81 .",
    "the jacobian of the score function @xmath47 with respect to the parameter @xmath29 is denoted as @xmath82 , i.e. , the @xmath83 by @xmath83 matrix whose element is given as @xmath84 .",
    "the variance matrix and the covariance matrix under the probability @xmath85 are denoted as @xmath86 $ ] and @xmath87 $ ] , respectively . without loss of generality , we assume that @xmath88 at @xmath89 is represented as @xmath90 where @xmath91 is an arbitrary function orthogonal to @xmath92 , i.e. , @xmath93=o$ ] holds . if @xmath94 does not have any component which is represented as a linear transformation of @xmath92 , the estimator would be degenerated . under the regularity condition ,",
    "the estimated parameters , @xmath95 and @xmath67 , converge to @xmath33 and @xmath77 , respectively .",
    "the asymptotic expansion of around @xmath96 leads to @xmath97\\delta{{\\bm \\alpha}}+e[{{\\bm u}}{\\bm \\phi}^t]\\delta{{\\bm \\theta}}&=-\\frac{1}{n}\\sum_{i=1}^{n}{{\\bm u}}_i+o_p(n^{-1/2}),\\\\   e[{\\bm \\phi}{\\bm \\phi}^t]\\delta{{\\bm \\theta}}&=   \\frac{1}{n'}\\sum_{j=1}^{n'}{{\\bm \\eta}}_j ' -\\frac{1}{n}\\sum_{i=1}^{n}{{\\bm \\eta}}_i+o_p(n^{-1/2}).\\end{aligned}\\ ] ] hence ,",
    "we have @xmath97\\delta{{\\bm \\alpha}}=   \\frac{1}{n}\\sum_{i=1}^{n }   \\big\\{e[{{\\bm u}}{\\bm \\phi}^t]e[{\\bm \\phi}{\\bm \\phi}^t]^{-1}{{\\bm \\eta}}_i-{{\\bm u}}_i \\big\\ }   -\\frac{1}{n'}\\sum_{j=1}^{n ' } e[{{\\bm u}}{\\bm \\phi}^t]e[{\\bm \\phi}{\\bm \\phi}^t]^{-1}{{\\bm \\eta}}_j '   + o_p(n^{-1/2 } ) . \\end{aligned}\\ ] ] therefore , we obtain the asymptotic variance , @xmath98v[\\delta{{\\bm \\alpha}}]e[\\nabla{{\\bm u}}]^t\\\\   & =   v[{{\\bm u}}]+   \\bigg(1+\\frac{n}{n'}\\bigg )   e[{{\\bm u}}{\\bm \\phi}^t]e[{\\bm \\phi}{\\bm \\phi}^t]^{-1}v[{{\\bm \\eta}}]e[{\\bm \\phi}{\\bm \\phi}^t]^{-1}e[{\\bm \\phi}{{\\bm u}}^t]\\\\   & \\phantom{= }   - e[{{\\bm u}}{\\bm \\phi}^t]e[{\\bm \\phi}{\\bm \\phi}^t]^{-1}{{\\rm cov}}[{{\\bm \\eta}},{{\\bm u } } ]   - { { \\rm cov}}[{{\\bm u}},{{\\bm \\eta}}]e[{\\bm \\phi}{\\bm \\phi}^t]^{-1}e[{\\bm \\phi}{{\\bm u}}^t]+o(1 ) \\ ] ] on the other hand , the variance of the naive mle , @xmath99 , defined as a solution of is given as @xmath100v[\\delta\\widetilde{{{\\bm \\alpha}}}]e[\\nabla{{\\bm u}}]^t = v[{{\\bm u}}]+o(1 ) , \\end{aligned}\\ ] ] where @xmath101 .",
    "given the model for the density - ratio @xmath65 , we compare the asymptotic variance matrices of the estimators , @xmath99 and @xmath95 .",
    "first , let us define @xmath102 i.e. , @xmath103 is the projection of the score function @xmath104 onto the subspace consisting of all functions depending only on @xmath1 , where the inner product is defined by the expectation under the joint probability @xmath85 .",
    "note that the equality @xmath105={\\mbox{\\bf 0}}$ ] holds .",
    "let the matrix @xmath106 be @xmath107e[{\\bm \\phi}{\\bm \\phi}^t]^{-1}. \\end{aligned}\\ ] ] then , a simple calculation yields that the difference of the variance matrix between @xmath108 and @xmath45 is equal to @xmath109   &   : = n\\cdot{}e[\\nabla{{\\bm u}}]v[\\delta\\widetilde{{{\\bm \\alpha}}}]e[\\nabla{{\\bm u}}]^t   -n\\cdot{}e[\\nabla{{\\bm u}}]v[\\delta{{{\\bm \\alpha}}}]e[\\nabla{{\\bm u}}]^t \\nonumber\\\\   & =   \\frac{n'}{n+n'}e[\\bar{{{\\bm u}}}\\bar{{{\\bm u}}}^t]-\\bigg(1+\\frac{n}{n'}\\bigg)v[b{{\\bm \\eta}}-\\frac{n'}{n+n'}\\bar{{{\\bm u } } } ]   + o(1 ) .",
    "\\label{eqn : diff - bar_u - expression } \\ ] ] in the second equality , we supposed that @xmath110 converges to a positive constant .",
    "when @xmath111 $ ] is positive definite , the estimator @xmath95 using the labeled and unlabeled data improves the estimator @xmath99 using only the labeled data .",
    "it is straightforward to see that the improvement is not attained if @xmath112 holds . in general , the score function @xmath113 satisfies @xmath112 , if the model is specified .",
    "when the model of the conditional probability @xmath8 is misspecified , however , there is a possibility that the proposed estimator outperforms the mle @xmath99 .",
    "we derive the optimal moment function @xmath88 for the estimation of the parameter @xmath33 .",
    "the optimal @xmath88 can be different from .",
    "we prepare some notations .",
    "let @xmath114 be the @xmath115-valued function on @xmath14 , each element of which is the projection of each element of @xmath116 onto the subspace spanned by @xmath117 . here",
    ", the inner product is defined by the expectation under the marginal probability @xmath9 .",
    "in addition , let @xmath118 be the projection of @xmath116 onto the orthogonal complement of the subspace , i.e. , @xmath119 .",
    "[ theorem : optimal - moment - func ] we assume that the model of the density - ratio is defined as @xmath120 with the basis functions @xmath121 satisfying @xmath61 .",
    "suppose that @xmath122\\in{\\mathbb{r}}^{r\\times{r}}$ ] is invertible , and that the rank of @xmath123e[{\\bm \\phi}{\\bm \\phi}^t]^{-1}$ ] is equal to the dimension of the parameter @xmath29 , i.e. , row full rank .",
    "we assume that the moment function @xmath69 at @xmath89 is represented as @xmath124 where @xmath91 is a function orthogonal to @xmath92 , i.e. , @xmath125=o$ ] holds .",
    "then , an optimal @xmath126 is given as @xmath127 for the optimal choice of @xmath88 , the maximum improvement is given as @xmath128    & =    \\frac{n'}{n+n'}e[\\bar{{{\\bm u}}}\\bar{{{\\bm u}}}^t ]    -\\frac{n^2}{n'(n+n')}e[\\pi_{{\\bm \\phi}}\\bar{{{\\bm u}}}(\\pi_{{\\bm \\phi}}\\bar{{{\\bm u}}})^t]+o(1)\\nonumber\\\\    & =     \\frac{n'}{n+n'}e[{\\pi_{{\\bm \\phi}}^\\bot}\\bar{{{\\bm u}}}({\\pi_{{\\bm \\phi}}^\\bot}\\bar{{{\\bm u}}})^{t } ]    + \\frac{n'-n}{n'}e[{\\pi_{{\\bm \\phi}}}\\bar{{{\\bm u}}}({\\pi_{{\\bm \\phi}}}\\bar{{{\\bm u}}})^{t}]+o(1 )    \\label{eqn : maximum - improvement - opt }   \\end{aligned}\\ ] ]    due to @xmath61 , one has @xmath129={\\mbox{\\bf 0}}$ ] and @xmath130=e[1\\cdot\\pi_{{\\bm \\phi}}^\\bot{\\bar{{{\\bm u}}}}]={\\mbox{\\bf 0}}$ ] .",
    "hence , one has @xmath131=e[\\bar{{{\\bm",
    "u}}}]-e[\\pi_{{\\bm \\phi}}^\\bot{\\bar{{{\\bm u}}}}]={\\mbox{\\bf 0}}$ ] .",
    "our goad is to find @xmath126 which minimizes @xmath132 $ ] in in the sense of positive definiteness .",
    "the orthogonal decomposition leads to @xmath133    =    v[b{\\bm \\phi}-\\frac{n'}{n+n'}\\pi_{{\\bm \\phi}}\\bar{{{\\bm u}}}]+v[b\\widetilde{{\\bm \\phi}}-\\frac{n'}{n+n'}\\pi_{{\\bm \\phi}}^{\\bot}\\bar{{{\\bm u } } } ] ,    \\end{aligned}\\ ] ] because of the orthogonality between @xmath134 and @xmath135 , and the equality @xmath136={\\mbox{\\bf 0}}$ ] .",
    "hence , @xmath126 satisfying @xmath137 is an optimal choice .",
    "since the matrix @xmath106 is row full rank , a solution of the above equation is given by @xmath138 we obtain the maximum improvement of @xmath111 $ ] by using the equalities @xmath139=e[\\pi_{{\\bm \\phi}}\\bar{{{\\bm u}}}(\\pi_{{\\bm \\phi}}\\bar{{{\\bm u}}})^t]$ ] and @xmath140e[{\\bm \\phi}{\\bm \\phi}^t]^{-1}{\\bm \\phi}=\\pi_{{\\bm \\phi}}\\bar{{{\\bm u}}}$ ] .",
    "suppose that the optimal moment function @xmath141 presented in theorem [ theorem : optimal - moment - func ] is used with the score function @xmath142 .",
    "then , the improvement is maximized when @xmath143 $ ] is minimized .",
    "hence , the model @xmath65 with the lower dimensional parameter @xmath63 is preferable as long as the assumption in theorem [ theorem : optimal - moment - func ] is satisfied .",
    "this is intuitively understandable , because the statistical perturbation of the density - ratio estimator is minimized , when the smallest model is employed .",
    "suppose that the basis functions , @xmath144 , are closely orthogonal to @xmath116 , i.e. , @xmath123 $ ] is close to the null matrix .",
    "then , the improvement @xmath111 $ ] is close to @xmath145 $ ] . as a result",
    ", we have @xmath146=\\frac{n'}{n+n'}e[\\bar{{{\\bm u}}}\\bar{{{\\bm u}}}^t]$ ] in which the supremum is taken over the basis of the density - ratio model satisfying the assumption in theorem [ theorem : optimal - moment - func ] .",
    "however , the basis functions satisfying the exact equality @xmath123=o$ ] is useless .",
    "because , the equality @xmath123=o$ ] leads to @xmath147 and thus , the equality is reduced to @xmath128=\\frac{n'}{n+n'}e[\\bar{{{\\bm u}}}\\bar{{{\\bm u}}}^t]-\\frac{n+n'}{n'}v[\\frac{n'}{n+n'}\\bar{{{\\bm u}}}]+o(1)=o(1 ) .",
    "\\end{aligned}\\ ] ] this result implies that there is the singularity at the basis function @xmath148 such that @xmath123=o$ ] .",
    "it is not practical to apply the optimal function @xmath69 defined by .",
    "the optimal moment function depends on @xmath116 , and one needs information on the probability @xmath8 to obtain the explicit form of @xmath116 .",
    "the estimation of @xmath116 needs non - parametric estimation , since the model misspecification of @xmath31 is significant in our setup .",
    "thus , we consider more practical estimator for the density ratio .",
    "suppose that @xmath149 holds for the moment function @xmath94 .",
    "for example , the optimal moment function satisfies @xmath150 at @xmath89 , i.e. , @xmath149 . for the density - ratio model @xmath151 with @xmath61 and the moment function satisfying @xmath152",
    ", a brief calculation yields that @xmath109=   \\frac{n'-n}{n'}e[{\\pi_{\\bm\\phi}}\\bar{{{\\bm u}}}({\\pi_{\\bm\\phi}}\\bar{{{\\bm u}}})^{t}]+o(1 ) .",
    "\\label{eqn : maximum - improvement - tildephi0 } \\end{aligned}\\ ] ] hence , the improvement is attained , when @xmath153 holds . as an interesting fact",
    ", we see that the larger model @xmath65 attains the better improvement in . indeed , @xmath154 gets close to @xmath116 , when the density - ratio model @xmath155 becomes large .",
    "hence , the non - parametric estimation of the density - ratio may be a good choice to achieve a large improvement for the estimation of the conditional probability .",
    "this is totally different from the case that the optimal @xmath126 presented in theorem [ theorem : optimal - moment - func ] is used in the density - ratio estimation .",
    "the relation between @xmath111 $ ] using the optimal @xmath126 and @xmath111 $ ] with @xmath149 is illustrated in figure [ fig : improvement ] . in the limit of the dimension of @xmath63 ,",
    "both variance matrices converge to @xmath156 $ ] monotonically .",
    "$ ] is depicted as the function of the dimension of the density - ratio model .",
    "since the improvement is represented by the matrix , the figure gives a view showing a frame format of the inequality relation .",
    "when the dimension of @xmath63 tends to infinity and @xmath157 holds , the two curves converges to the common positive definite matrix @xmath156 $ ] . ]",
    "let @xmath142 be the score function of the model @xmath158 , where @xmath159 is the vector consisting of basis functions and @xmath160 is a known parameter .",
    "then , one has @xmath161 .",
    "suppose that the true conditional probability leads to the regression function @xmath162 , where @xmath163=0 $ ] for all @xmath1 .",
    "then , one has @xmath164 and @xmath165=e[(f(x)-{{\\bm \\alpha}}^t{{\\bm b}}(x))^2{{\\bm b}}(x){{\\bm b}}(x)^t]$ ] .",
    "hence , the upper bound of the improvement is governed by the degree of the model misspecification @xmath166 . according to theorem [ theorem : optimal - moment - func ] ,",
    "an optimal moment function @xmath69 is given as @xmath167 at @xmath89 , where @xmath168e[{\\bm \\phi}{\\bm \\phi}^t]^{-1}$ ] .",
    "we show numerical experiments to compare the standard supervised learning and the semi - supervised learning using dress .",
    "both regression problems and classification problems are presented .",
    "we consider the regression problem with the @xmath83-dimensional covariate variable shown below .    labeled data : : :    @xmath169 unlabeled data : : :    @xmath170 .",
    "regression model : : :    @xmath171 .",
    "score function : : :    @xmath172 .",
    "the parameter @xmath173 in implies the degree of the model misspecification .",
    "let @xmath174 be the target function , @xmath175 , and define @xmath176 , \\end{aligned}\\ ] ] which implies the squared distance from the true function @xmath174 to the linear regression model .",
    "on the other hand , the mean square error of the naive least mean square ( lms ) estimator @xmath99 , i.e. , @xmath177 $ ] , is asymptotically equal to @xmath178 , when the model is specified .",
    "we use the ratio @xmath179 as the normalized measure of the model misspecification .",
    "when @xmath180 holds , the misspecification of the model can be statistically detected .",
    "first , we use a parametric model for density ratio estimation . for any positive integer @xmath181 ,",
    "let @xmath182 be the @xmath83-dimensional vector @xmath183 .",
    "the density - ratio model is defined as @xmath184 having @xmath185 dimensional parameter @xmath186 .",
    "we apply the estimator presented by qin @xcite .",
    "note that the estimator satisfies @xmath149 at @xmath89 .",
    "hence , the improvement is asymptotically given by . under the setup of @xmath187 and @xmath188 , we compute the mean square errors for lms estimator @xmath108 and dress @xmath45 . the difference of test errors , @xmath189-e[(\\widehat{{\\mbox{\\boldmath $ \\alpha$}}}^t{{\\bm x}}-f_\\varepsilon({{\\bm x}}))^2 ] ) , \\end{aligned}\\ ] ] is evaluated for each @xmath173 and each dimension of the density ratio , @xmath185 , where the expectation is evaluated over the test samples .",
    "the mean square error is calculated by the average over 500 iterations .",
    "figure [ fig : sim_result_parametric_drmodel ] shows the results .",
    "when the model is specified , i.e. , @xmath190 , lms estimator presents better performance than dress . under the practical setup such as @xmath191 , however , we see that dress outperforms lms estimator .",
    "the dependency on the dimension of the density - ratio model is not clearly detected in this experiment .",
    "overall , larger density - ratio model presents rather unstable result .",
    "indeed , in dress with large density ratio model , say the right bottom panel in figure [ fig : sim_result_parametric_drmodel ] , the mean square error of dress can be large , i.e. , the improvement is negative , even when the model misspecification @xmath192 is large .",
    "[ cols=\"^,^,^ \" , ]",
    "in this paper , we investigated the semi - supervised learning with density - ratio estimator .",
    "we proved that the unlabeled data is useful when the model of the conditional probability @xmath8 is misspecified .",
    "this result agrees to the result given by sokolovska , et al .",
    "@xcite , in which the weight function is estimated by using the estimator of the marginal probability @xmath9 under a specified model of @xmath9 .",
    "the estimator proposed in this paper is useful in practice , since our method does not require the well - specified model for the marginal probability .",
    "numerical experiments present the effectiveness of our method .",
    "we are currently investigating semi - supervised learning from the perspective of semiparametric inference with missing data .",
    "a positive use of the statistical paradox in semiparametric inference is an interesting future work for semi - supervised learning .",
    "the authors are grateful to dr .",
    "masayuki henmi , dr .  hironori fujisawa and prof .",
    "shinto eguchi of institute of statistical mathematics .",
    "tk was partially supported by grant - in - aid for young scientists ( 20700251 ) ."
  ],
  "abstract_text": [
    "<S> in this paper , we study statistical properties of semi - supervised learning , which is considered as an important problem in the community of machine learning . in the standard supervised learning , only the labeled data is observed . </S>",
    "<S> the classification and regression problems are formalized as the supervised learning . in semi - supervised learning , unlabeled data </S>",
    "<S> is also obtained in addition to labeled data . </S>",
    "<S> hence , exploiting unlabeled data is important to improve the prediction accuracy in semi - supervised learning . </S>",
    "<S> this problems is regarded as a semiparametric estimation problem with missing data . under the the discriminative probabilistic models </S>",
    "<S> , it had been considered that the unlabeled data is useless to improve the estimation accuracy . </S>",
    "<S> recently , it was revealed that the weighted estimator using the unlabeled data achieves better prediction accuracy in comparison to the learning method using only labeled data , especially when the discriminative probabilistic model is misspecified . </S>",
    "<S> that is , the improvement under the semiparametric model with missing data is possible , when the semiparametric model is misspecified . in this paper </S>",
    "<S> , we apply the density - ratio estimator to obtain the weight function in the semi - supervised learning . </S>",
    "<S> the benefit of our approach is that the proposed estimator does not require well - specified probabilistic models for the probability of the unlabeled data . </S>",
    "<S> based on the statistical asymptotic theory , we prove that the estimation accuracy of our method outperforms the supervised learning using only labeled data . </S>",
    "<S> some numerical experiments present the usefulness of our methods . </S>"
  ]
}