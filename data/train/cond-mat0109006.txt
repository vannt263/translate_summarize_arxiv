{
  "article_text": [
    "automatic text categorization has many interesting applications in science and business . for instance formatting the results of a web - search query , sorting news messages according to topics or sorting incoming email with different concerns .",
    "irrespective of the application two principal cases are distinguished . in the first case",
    "the categories are known and the algorithm should assign any document to one of the known categories . then it is useful to teach the algorithm the considered categories and their word fields by using a training set of labeled documents before it will be applied to unknown documents .",
    "the algorithmic solutions to this problem fall into the category of _ supervised _ learning . in the other case",
    "the categorization of documents has to be done without knowing the categories nor their number .",
    "then the algorithm should find a reasonable partition of the document set such that documents in the same subset of the partition are similar and documents of different subsets are dissimilar .",
    "this task is termed _ unsupervised _ text categorization and can be handled with clustering algorithms @xcite .    in this work",
    "we focus on the second task only . as an illustrative example for an application one could think of the results obtained from web search engines .",
    "usually the query results are on several subjects and only a fraction of the documents is about what one is interested in .",
    "it will help the user if instead of an unsorted list the results are presented in several folders that gather web - documents of similar content .",
    "further each folder could be characterized by a list of key words .",
    "then one can investigate the mass of documents that match a query in a more efficient way and the indicated keywords may help refining the search . in the general case of web search results we are not supplied with any training data such that we can only use clustering algorithms in order to classify the links . such a combination of a web search engine and",
    "a clustering tool has been proposed e.g.  by boley @xcite .",
    "the aims of this work are threefold .",
    "first we want to compare several methods measuring their performance on unsupervised text categorization .",
    "second we want to apply superparamagnetic clustering ( spc ) @xcite , a rather new method that has so far not been considered for text categorization .",
    "and third we want to present two methods of unsupervised feature selection and estimate the improvements that can be achieved by their application .",
    "clustering of data is usually done in three steps : representation , calculation of similarities and application of a clustering algorithm . as mentioned above the aim",
    "is to provide a partition of a data set @xmath0 that reflects the similarities between data points .",
    "so we should define a similarity measure @xmath1 which in turn requires a numerical representation of the data .",
    "usually the representation is done by constructing a vector space spanned by a set of selected features of the data .",
    "this means that one defines certain features and for all data assigns numbers according to how much the features apply .",
    "then a data point is represented by a vector in the feature space .    for text categorization one",
    "commonly uses the `` bag of words '' representation . in order to do",
    "so one enlists a dictionary @xmath2 of all the words that appear at least once in at least two of the documents .",
    "documents are then represented by counting the number of occurrences of each word in the document .",
    "one thus obtains an @xmath3-matrix @xmath4 of word frequencies .",
    "@xmath5 is the number of times the word @xmath6 appears in document @xmath7 and document @xmath7 is represented by the feature ( row-)vector @xmath8 .",
    "typically that feature space is very high - dimensional and the matrix is filled very sparsely , in our case the fraction of nonzero entries is @xmath9 .    to measure the similarity of two documents one could consider for instance the dot product of two corresponding normalized feature vectors .",
    "alternatively one can of course use a dissimilarity measure .",
    "the choice of the similarity measure has to be done carefully and influences the performance of the clustering .",
    "see e.g. @xcite for a comparison of some similarity measures used for text categorization .    for the text categorization we found useful the @xmath10- and @xmath11-norms as well as other dissimilarity measures ( see below ) .",
    "generally , in order to avoid skewness of the data due to the different length of the documents , it is helpful to normalize the data such that the length of a row vector is one .    finally , given the similarity measure @xmath12 the task of a clustering algorithm is to compute a clustering solution , i.e. a partition of the set of data points @xmath0 into subsets ( clusters ) @xmath13 such that @xmath14 is large when @xmath7 and @xmath15 are in the same cluster and @xmath14 is small when @xmath7 and @xmath15 are in different clusters .",
    "this is a rather fuzzy description of the aim of a clustering algorithm and we are not going to refine that point here",
    ". a good clustering solution can be found at different resolutions , so that a proper optimization problem can not easily be formulated .",
    "consider for example the biological classification of the animals .",
    "there we find phyla that are further subdivided into classes , orders , etc . and each level of partitioning has some justification . if we consider a clustering of the animals cat , dog , jellyfish , mouse and snake , we find dog and cat in the same cluster but well separated from the jellyfish when we a take a look from a large distance and consider a coarse classification .",
    "a finer classification however will separate dog and cat into different clusters .    for many data sets this resolution is an arbitrary parameter which has to be determined in accordance with the desired classification task .",
    "we therefore do not consider a single partition of the data set as a clustering solution but rather a hierarchy of partitions with increasing resolution that can be represented in a tree .",
    "agglomerative clustering methods @xcite successively merge two clusters until finally all data points are united . by doing",
    "so these methods implicitly provide such a tree .",
    "spc and @xmath16-means provide single partitions which depend on a resolution parameter that has to be specified . running these algorithms with different values of",
    "the resolution parameter yields several partitions that can be transformed into a tree",
    ". generally the partitions obtained at the next higher resolution are not proper subpartitions . in order to fix",
    "this one usually considers the intersections of high resolution clusters and low resolution clusters .",
    "each triplet of a representation , a distance measure and a clustering algorithm is considered as a clustering method . in section [ results ]",
    "we specify in detail the methods we apply to the test data .",
    "all methods we compare yield a hierarchical clustering tree .    for practical purposes it is then often important to reduce the amount of information in the tree , and present only some selected clusters as the essence of the clustering",
    ". then one can apply a search algorithm that selects the `` most meaningful '' clusters in the tree for presentation as the clustering result .    applying an algorithm that searches a tree for good clusters",
    "can be considered the fourth step of clustering and can be done in various ways .",
    "when one is using spc one can look at the change of the susceptibility versus the temperature and from that function one can determine the `` best '' resolution @xcite . but the search is not restricted to finding an optimal single resolution . for the text categorization task we found that the _ natural _ classes of the documents as classified by human readers , and so specified by the labels , are best approximated at _ different _ levels of resolution .",
    "therefore we prefer other methods that individually judge a single cluster as good or bad and therefore allow picking clusters from different levels of resolution .",
    "this can be done by measuring the stability of the cluster with respect to the resolution parameter @xcite or with respect to thinning out the dataset by considering subsamples @xcite .    however , the unsupervised identification of good clusters is a complex issue that we do not discuss in this paper , cp .",
    "section [ eval ] .",
    "a crucial point within the representation of the documents that bears some potential to improve the clustering results is the selection of words from the dictionary .",
    "everybody will immediately agree that the words `` and '' , `` or '' , `` while '' and `` with '' are useless for document categorization .",
    "these words are not characteristic for the content of the document and spoil the signal to noise ratio in the representation .",
    "usually words like prepositions , conjunctions , etc .  are read from a stoplist that contains about 400 known stop - words .",
    "they are taken out of the dictionary , i.e. out of the feature set and , as we will show in our results , this rejection of unwanted features yields some improvement on the results .",
    "so far this is not new , but this procedure , however , does only a part of the job . a more difficult problem is to get rid of those many noise words that are not on the stoplist .    after the application of the stoplist we remain with 5036 and 9019 words respectively to our two test databases . on the other hand ,",
    "looking at the dsr experiments that are described below , we find improved clustering results on the basis of 350 automatically selected words .",
    "so more than 90% of the words that are not on the stoplist are not needed .",
    "more than that : they make the task harder because they add noise to the document representation .",
    "finding those good words is not easy . whether a word is a useful feature for document classification depends on the categories that appear in the document set and",
    "can not be told by looking at the mere word .",
    "the word `` italy '' for instance might be discriminative in some set of documents where one of the categories is tourism , but it can be a pure noise feature in another set of documents .    a widely used method is the application of lower and upper thresholds for the coverage of a word , i.e. the number of documents a word appears in .",
    "this can improve the results but the quality of the clustering depends sensitively on these thresholds and one can not tell in general which values of these thresholds are the best .",
    "an alternative way to select relevant features , that has been proven useful for the analysis of microarray data , is the application of clustering algorithms to the feature set @xcite .    in order to identify good words and throw away the bad words in an unsupervised way we developed two strategies",
    "that are both based on resampling .",
    "experiments with different thresholds for minimal and maximal coverage have shown that choosing a different word subset for clustering the same database can strongly affect the global clustering tree structure .",
    "such sensitivity is probably due to `` shot noise '' arising from the finite size of the dictionary ( and an even smaller number of words that appear in a single category or a single document ) .",
    "one way to eliminate such noise is taking different , e.g. randomly chosen , word subsets from the dictionary , clustering the documents on the basis of these subsets and then averaging the result somehow .",
    "this is the idea of the word set resampling algorithm described in the following .",
    "let us first explain the procedure that is applied for each word subset ( `` probe clustering '' ) .",
    "we cluster the documents represented only through the words in the subset by applying an agglomerative clustering algorithm ( see below ) .",
    "each agglomeration process is continued until the stop criterion  ( [ stopc ] ) holds  @xcite , where @xmath17 and @xmath18 are the two largest clusters and @xmath19 : @xmath20 the clustering is then considered as `` good '' if the following quality condition is not violated .",
    "@xmath21 one can see that if both conditions ( [ stopc ] ) and ( [ good ] ) hold , then @xmath17 , @xmath18 are of comparable size and contain the majority of the documents ( there is no other aim of ( [ stopc ] ) and ( [ good ] ) ) . if the probe clustering is not `` good '' then it is rejected . for `` good '' clusterings we calculate for every word the entropy with respect to the two biggest clusters : @xmath22 where @xmath23 further , at that stage we check for each pair of documents if they are in the same cluster : @xmath24 we repeat this probe clustering until we have @xmath25 `` good '' ones . the word subsets are obtained by throwing out one randomly drawn word from each document . in order to avoid empty documents in some cases some of the thrown out words have to be replaced .",
    "after doing all subsample clusterings we average @xmath26 and @xmath27 over all @xmath25 `` good '' probe clusterings .",
    "intuitively we consider @xmath28 as the quality of the word @xmath6 , and @xmath29 as similarity of the documents @xmath30 and @xmath31 ( @xmath32 denotes average ) .",
    "we throw away all the words whose average entropy exceeds the threshold : @xmath33 where @xmath34 $ ] .",
    "next , we find the maximal value @xmath35 and merge together all the documents @xmath30 , @xmath31 with @xmath36 .    in the next step",
    "we remove all the words that appear solely in documents of one cluster .    unless we are left with one huge cluster of all the documents we will repeat this procedure . in doing",
    "so we consider as single documents in the next step those that were merged in the current step .",
    "the parameters of this algorithm are @xmath25 and @xmath37 .",
    "this method is motivated by the good results obtained with wsr .",
    "it is meant to be an alternative that does not require such a high computing time as wsr .",
    "whereas wsr calculates the entropy of the words only at a single stage of the subsample clustering , dsr tries to get hints for the good words by looking at the whole clustering of the subsamples .",
    "dsr is based on two assumptions that have been found to be true in our experiments : first we found that the really useful words have a considerable coverage , i.  e.  these words appear in many documents .",
    "second we assume that in agglomerative clustering , when we consider the number of mergings that have been done as a monotonically decreasing measure of the clustering resolution , the cluster entropy of the good words often decreases earlier with decreasing resolution than the cluster entropy of the bad words .",
    "the detailed description of the algorithm is as follows :    we create @xmath25 subsamples @xmath38 , each consisting of @xmath39 randomly drawn documents .    to each subset",
    "@xmath40 we apply an agglomerative clustering algorithm .",
    "we number the successive mergings of the agglomeration and we will refer to the intermediate clustering solutions at step @xmath41 .",
    "by @xmath42 we refer to the initialization with each single document being in a separate cluster and @xmath43 corresponds to the final step where all documents are in a single cluster .",
    "we then consider the words that appear in at least @xmath44 documents of the current subsample and for each of these words we calculate the development of its cluster entropy in the early steps of the agglomeration process , i.e. @xmath45 .    thus at step @xmath41 we find the clusters @xmath46 with the labels @xmath47 and we calculate @xmath48 where @xmath49    in order to highlight the effect of successive mergings on the cluster entropy we normalize the cluster entropies with respect to the initial cluster entropies at step @xmath42 @xmath50    now @xmath51 decreases as the resolution becomes lower with successive mergings , i.e. as @xmath41 increases , and finally , when all documents have been merged to the same cluster the entropy is zero for all words .",
    "we found that if we consider only the words the entropy of which decreases early in the agglomeration process we find a higher fraction of good words that have more value for the document clustering .",
    "however , the entropy of words that appear only in two or three documents naturally decreases to zero within two or three mergings and thus ( again ) produces some sort of shot noise that spoils the statistics .",
    "thus we consider only the words that appear in at least @xmath44 documents of the subsample and we keep track of the number of those words that have low entropy , i.e. we count @xmath52    at first the number of low - entropy - words @xmath53 increases slowly and later increases in larger steps .",
    "we consider the first increment @xmath54 that is significantly higher than the average increment as a cutoff criterion at which we decide to keep the words that have low entropy according to ( [ lec ] ) at that stage .",
    "that is we look for the _ minimal _ value @xmath55 fulfilling @xmath56 and we select good words as @xmath57    finally we consider the union @xmath58 as our selected dictionary and we cluster all documents in @xmath0 using the words ( features ) in @xmath59 .",
    "the parameters of this algorithm are @xmath25 , @xmath39 , @xmath44 and @xmath37 .",
    "in order to compare the performance of the different clustering algorithms we extracted two test data sets from the known reuters-21578 test database for text categorization @xcite .",
    "this database contains 21578 reuters news messages that were manually labeled as belonging to certain categories .",
    "we use the labels not for the clustering procedure but in order to evaluate the quality of the clustering results as will be described in the next section .    for each of our two test datasets we took all documents of eight selected categories .",
    "in order to keep things simple we considered some preprocessing of the labels as helpful . those few documents that have been labeled as belonging to more than one category were assigned unambiguously to the first label given in the database .",
    "one should keep in mind that the labels were given by human readers and are therefore subject to individual perception .",
    "the resulting test databases , i.e. the categories that are to be separated from one another and the number of documents extracted from the reuters database are listed in table  [ database ] .",
    "please note that some categories appear in both tasks .",
    "this is meant as shedding some light onto whether the good or bad separability of a category is an individual property of that category and its word field or if it rather a question of interference with another category using the same words .",
    "we found that the quality of clustering results is sensitive to the input dataset , particularly to the composition of the news categories that are used .",
    "for instance , in clustering experiments done with the first database all clustering algorithms separate documents of the category `` coffee '' much better than they separate documents of the category `` oilseed '' .",
    "we thus concluded that the quality of the clustering results depends on the categories and the distribution of documents rather than on the individual documents . obviously clustering documents of a certain category",
    "is easier if there is a set of discriminative words that are used in most documents of that category and only in those documents .",
    "nevertheless the quality of the clustering results varies also when the same algorithm is applied to different randomly chosen subsets of the database with specified numbers of documents from each category . in order to estimate the performance of the clustering algorithms on the clustering task we therefore create 50 different subsets of the test database",
    "each of which contains a specified number of documents from each category .",
    "the clustering accuracy is then averaged among the 50 individual realizations of each experiment .    in particular we constructed four experiments for each database .",
    "we chose 50 different subsets of 200 , 500 and 800 documents preserving the ratios of the number of documents in the categories and in another experiment the number of documents was the same in each category , see table [ experiments ] .",
    ".[database]the composition of the two test databases . [ cols=\"<,<,<,<,<,<,<,<,<\",options=\"header \" , ]",
    "we find that the quality of the clustering results depends to a large extent on the dataset .",
    "in particular we observe that the performance as measured in this paper is almost always better on larger categories . when comparing the results of single categories in the 800 and eq experiments , we find that they are better in the case where the number of documents of that category is larger .",
    "for example in the second database categories `` trade '' , `` crude '' , `` grain '' , `` money - fx '' are larger in the 800 experiment and the results are also better in the 800 experiment .",
    "all other categories are larger in the eq experiment and also there are the better results for these categories .",
    "also in this way the categories `` ship '' and `` money - supply '' can be better resolved in the context of the first database .    also we think that the resolvability of a category is influenced by interference with other categories in the dataset through an overlap of the characteristic word fields .",
    "we believe that if the characteristic words of a category are also used in documents of other categories that category can not be resolved as good as if there were no close categories .    comparing the results for the `` money - supply '' , `` ship '' and `` sugar '' categories in the eq experiments of the two databases",
    "gives a clue to possible interference .",
    "we find that `` money - supply '' and `` ship '' are better resolved in the eq experiments on the first database , whereas `` sugar '' is better in the second database .",
    "further we observe that some categories appear to have a preferred algorithm or vice versa .",
    "so spc performance in the eq experiment of the second database peaks in categories `` trade '' , `` money - supply '' and `` interest '' whereas the results for the other categories are only moderate .",
    "however , the ranking of the performance of different clustering methods does not sensitively depend on the data .",
    "we found that the level of performance of spc , arg and aib is almost the same .",
    "results obtained with pddp are not as good , whereas the advantage of this method is that it is much faster on large databases .",
    "pddp does not require the calculation of a ( dis-)similarity matrix and its time consumption scales linear with the number of documents .",
    "the feature selecting methods that we propose in this paper can improve the results .",
    "wsr yields the highest performance but has on the other hand a very high computational cost .",
    "as it is implemented , the required time scales with @xmath60 .",
    "dsr gives moderate improvement of the clustering quality but is in comparison to wsr much faster .",
    "the time consumption of dsr is dominated by the size of the subsets .",
    "thus for large datasets , if one can probe the discriminative words with comparably small subsets it will be faster than spc , aib and arg that all rely on the computation of a complete distance matrix on the basis of the whole word set .",
    "another little advantage of the feature selecting methods is that the application of a stoplist becomes obsolete , wsr and dsr perform as good on the raw data matrix .",
    "d.  boley , m.  gini , r.  gross , e.h .",
    "han , k.  hastings , g.  karypis , v.  kumar , b.  mobasher , and j.  moore , _ document categorization and query generation on the world wide web using webace _",
    "intell . rev .",
    "* 13 * ( 5 - 6 ) 365 - 391 ( 1999 ) .",
    "a.  strehl , j.  ghosh , and r.  mooney , _ impact of similarity measures on web - page clustering _",
    ", in _ proc .",
    "of the 17th national conference on artificial intelligence : workshop of artificial intelligence for web search _ , p.  58",
    "- 64 ( aaai , 2000 ) ."
  ],
  "abstract_text": [
    "<S> we compare the performance of different clustering algorithms applied to the task of unsupervised text categorization . </S>",
    "<S> we consider agglomerative clustering algorithms , principal direction divisive partitioning and ( for the first time ) superparamagnetic clustering with several distance measures . </S>",
    "<S> the algorithms have been applied to test databases extracted from the reuters-21578 text categorization test database . </S>",
    "<S> we find that simple application of the different clustering algorithms yields clustering solutions of comparable quality . in order to achieve considerable improvements of the clustering results it </S>",
    "<S> is crucial to reduce the dictionary of words considered in the representation of the documents . </S>",
    "<S> significant improvements of the quality of the clustering can be obtained by identifying discriminative words and filtering out indiscriminative words from the dictionary . </S>",
    "<S> we present two methods , each based on a resampling scheme , for selecting discriminative words in an unsupervised way .    </S>",
    "<S> keywords : clustering , text categorization , document classification , feature selection , random subsampling    2 </S>"
  ]
}