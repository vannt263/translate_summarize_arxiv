{
  "article_text": [
    "[ [ finding - approximate - near - neighbors . ] ] finding ( approximate ) near neighbors .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    a key computational problem in various research areas , including machine learning , pattern recognition , data compression , coding theory , and cryptanalysis  @xcite , is finding near neighbors : given a data set @xmath12 of cardinality @xmath13 , design a data structure and preprocess @xmath14 in a way that , when given a query vector @xmath15 , one can efficiently find a near point to @xmath16 in @xmath14 . due to the `` curse of dimensionality ''  @xcite this problem",
    "is known to be hard to solve exactly ( in the worst case ) in high dimensions @xmath1 , so a common relaxation of this problem is the _",
    "@xmath17-approximate near neighbor problem _ ( @xmath17-ann ) : given that the nearest neighbor lies at distance at most @xmath18 from @xmath16 , design an algorithm that finds an element @xmath19 at distance at most @xmath20 from @xmath16 .",
    "[ [ locality - sensitive - hashing - lsh - and - filtering - lsf . ] ] locality - sensitive hashing ( lsh ) and filtering ( lsf ) .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    a prominent class of algorithms for finding near neighbors in high dimensions is formed by locality - sensitive hashing ( lsh )  @xcite and locality - sensitive filtering ( lsf )  @xcite .",
    "these solutions are based on partitioning the space into regions , in a way that nearby vectors have a higher probability of ending up in the same hash region than distant vectors . by carefully tuning ( i )",
    "the number of hash regions per hash table , and ( ii ) the number of randomized hash tables , one can then guarantee that with high probability ( a ) nearby vectors will _ collide _ in at least one of the hash tables , and ( b ) distant vectors will not collide in any of the hash tables .",
    "for lsh , a simple lookup in all of @xmath16 s hash buckets then provides a fast way of finding near neighbors to @xmath16 , while for lsf the lookups are slightly more involved . for various metrics ,",
    "lsh and lsf currently provide the best performance in high dimensions  @xcite .",
    "[ [ near - neighbors - on - the - sphere . ] ] near neighbors on the sphere .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + +    in this work we will focus on the near neighbor problem under the _ angular distance _",
    ", where two vectors @xmath21 are considered nearby iff their common angle @xmath22 is small  @xcite .",
    "this equivalently corresponds to near neighbor searching for the @xmath23-norm , where the entire data set is assumed to lie on a sphere .",
    "a special case of @xmath17-ann on the sphere , often considered in the literature , is the _ random _ case @xmath24 and @xmath25 , in part due to a reduction from near neighbor under the euclidean metric for general data sets to @xmath17-ann on the sphere with these parameters  @xcite .",
    "[ [ upper - bounds . ] ] upper bounds .",
    "+ + + + + + + + + + + + +    perhaps the most well - known and widely used solution for ann for the angular distance is charikar s hyperplane lsh  @xcite , where a set of _ random _ hyperplanes is used to partition the space into regions . due to its low computational complexity and the simple form of the collision probabilities ( with no hidden order terms in @xmath1 )",
    ", this method is easy to instantiate in practice and commonly achieves the best performance out of all lsh methods when @xmath1 is not too large . for large @xmath1 , both spherical cap lsh  @xcite and cross - polytope lsh",
    "@xcite are known to perform better than hyperplane lsh .",
    "experiments from  @xcite showed that using _",
    "orthogonal _ hyperplanes , partitioning the space into voronoi regions induced by the vertices of a hypercube , also leads to superior results compared to hyperplane lsh ; however , no theoretical guarantees for the resulting hypercube lsh method were given , and it remained unclear whether the improvement persists in high dimensions .",
    "[ [ lower - bounds . ] ] lower bounds .",
    "+ + + + + + + + + + + + +    for the case of _ random _ data sets , lower bounds have also been found , matching the performance of spherical cap and cross - polytope lsh for large @xmath6  @xcite .",
    "these lower bounds are commonly in a model where it is assumed that collision probabilities are `` not too small '' , and in particular not exponentially small in @xmath1 .",
    "therefore it is not clear whether one can further improve upon cross - polytope lsh when the number of hash regions is exponentially large , which would for instance be the case for hypercube lsh .",
    "together with the experimental results from  @xcite , this naturally begs the question : how efficient is hypercube lsh ?",
    "is it better than hyperplane lsh and/or cross - polytope lsh ? and how does hypercube lsh compare to other methods in practice ?      [ [ hypercube - lsh . ] ] hypercube lsh .",
    "+ + + + + + + + + + + + + +    by carefully analyzing the collision probabilities for hypercube lsh using results from large deviations theory , we show that hypercube lsh is indeed different from , and superior to hyperplane lsh for large @xmath1 .",
    "the following main theorem states the asymptotic form of the collision probabilities when using hypercube lsh , which are also visualized in figure  [ fig : main ] in comparison with hyperplane lsh .",
    "[ collision probabilities for hypercube lsh ] [ thm : main ] let @xmath26 , let @xmath27 $ ] denote the angle between @xmath28 and @xmath29 , and let @xmath30 denote the probability that @xmath28 and @xmath29 are mapped to the same hypercube hash region . for @xmath31 ( respectively @xmath32 ) , let @xmath33 ( resp .",
    "@xmath34 ) be the unique solution to : @xmath35 then , as @xmath1 tends to infinity , @xmath30 satisfies : @xmath36 ; \\\\[3ex ] \\left(\\displaystyle\\frac{(\\beta_1 + \\cos \\theta)^2}{\\pi \\beta_1 ( \\beta_1 \\cos \\theta + 1 ) \\sin \\theta}\\right)^{d + o(d ) } , & \\qquad \\text{if } \\theta \\in [ \\arccos \\tfrac{2}{\\pi } , \\tfrac{\\pi}{3 } ] ; \\\\[3ex ] \\left(\\displaystyle\\frac{1 + \\cos",
    "\\theta}{\\pi \\sin \\theta}\\right)^{d + o(d ) } , & \\qquad \\text{if } \\theta \\in [ \\tfrac{\\pi}{3 } , \\tfrac{\\pi}{2 } ) ; \\\\[3ex ] 0 , & \\qquad \\text{if } \\theta \\in [ \\tfrac{\\pi}{2 } , \\pi ] .",
    "\\end{cases}\\end{aligned}\\ ] ]    , and the dashed vertical lines correspond to boundary points of the piecewise parts of theorem  [ thm : main ] .",
    "the blue line indicates hyperplane lsh with @xmath1 random hyperplanes .",
    "[ fig : main],width=529 ]    denoting the query complexity of lsh methods by @xmath37 , the parameter @xmath7 for hypercube lsh is up to @xmath38 times smaller than for hyperplane lsh . for large @xmath1 ,",
    "hypercube lsh is dominated by cross - polytope lsh ( unless @xmath39 ) , but as the convergence to the limit is rather slow , in practice either method might be better , depending on the exact parameter setting . for the random",
    "setting , figure  [ fig : rho ] shows limiting values for @xmath7 for hyperplane , hypercube and cross - polytope lsh .",
    "we again remark that these are asymptotics for @xmath40 , and may not accurately reflect the performance of these methods for moderate @xmath1 .",
    "we further briefly discuss how the hashing for hypercube lsh can be made efficient .     when using hyperplane lsh , hypercube lsh , and cross - polytope lsh , for @xmath41-ann with @xmath25 .",
    "the curve for hyperplane lsh is exact for arbitrary @xmath1 , while for the other two curves , order terms vanishing as @xmath40 have been omitted.[fig : rho],width=529 ]    [ [ partial - hypercube - lsh . ] ] partial hypercube lsh .",
    "+ + + + + + + + + + + + + + + + + + + + + +    as the number of hash regions of a full - dimensional hypercube is often prohibitively large , we also consider _ partial _ hypercube lsh , where a @xmath42-dimensional hypercube is used to partition a data set in dimension @xmath1 .",
    "building upon a result of jiang  @xcite , we characterize when hypercube and hyperplane lsh are asymptotically equivalent in terms of the relation between @xmath42 and @xmath1 , and we empirically illustrate the convergence towards either hyperplane or hypercube lsh for larger @xmath42 . an important open problem remains to identify how large the ratio @xmath43 must be for the asymptotics of partial hypercube lsh to be equivalent to those of full - dimensional hypercube lsh .",
    "[ [ application - to - lattice - sieving . ] ] application to lattice sieving .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    finally , we consider a specific use case of different lsh methods , in the context of lattice cryptanalysis .",
    "we show that the heuristic complexity of lattice sieving with hypercube lsh is expected to be slightly better than when using hyperplane lsh , and we discuss how experiments have previously indicated that in this application , hypercube lsh is superior to other dimensions up to dimensions @xmath44 .",
    "[ [ notation . ] ] notation .",
    "+ + + + + + + + +    we denote probabilities with @xmath45 and expectations with @xmath46 .",
    "capital letters commonly denote random variables , and boldface letters denote vectors .",
    "we informally write @xmath47 for continuous @xmath48 to denote the density of @xmath48 at @xmath49 . for probability distributions",
    "@xmath50 , we write @xmath51 to denote that @xmath48 is distributed according to @xmath50 . for sets @xmath52 , with abuse of notation",
    "we further write @xmath53 to denote @xmath48 is drawn uniformly at random from @xmath52 .",
    "we write @xmath54 for the normal distribution with mean @xmath55 and variance @xmath56 , and @xmath57 for the distribution of @xmath58 when @xmath59 .",
    "for @xmath60 the latter corresponds to the _ half - normal _ distribution .",
    "we write @xmath61 to denote a @xmath1-dimensional vector where each entry is independently distributed according to @xmath50 . in",
    "what follows , @xmath62 denotes the euclidean norm , and @xmath63 denotes the standard inner product .",
    "we denote the angle between two vectors by @xmath64 .",
    "[ lem : sphericalcap ] let @xmath26 be two independent standard normal vectors",
    ". then @xmath65 .    [",
    "[ locality - sensitive - hashing . ] ] locality - sensitive hashing .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + +    locality - sensitive hash functions  @xcite are functions @xmath66 mapping a @xmath1-dimensional vector @xmath67 to a low - dimensional _ sketch _",
    "@xmath68 , such that vectors which are nearby in @xmath69 are more likely to be mapped to the same sketch than distant vectors . for the angular distance @xmath70",
    ", we quantify a set of hash functions @xmath71 as follows ( see  @xcite ) :    a hash family @xmath71 is called @xmath72-sensitive if for @xmath73 we have :    * if @xmath74 then @xmath75 ; * if @xmath76 then @xmath77 .",
    "the existence of locality - sensitive hash families implies the existence of fast algorithms for ( approximate ) near neighbors , as the following lemma describes ) are omitted here for brevity . ] . for more details on the general principles of lsh",
    ", we refer the reader to e.g.  @xcite .",
    "[ lem : lsh ] suppose there exists a @xmath72-sensitive family @xmath71 .",
    ". then w.h.p .",
    "we can either find an element @xmath79 at angle at most @xmath80 from @xmath16 , or conclude that no elements @xmath79 at angle at most @xmath81 from @xmath16 exist , in time @xmath37 with space and preprocessing costs @xmath82 .",
    "[ [ hyperplane - lsh . ] ] hyperplane lsh .",
    "+ + + + + + + + + + + + + + +    for the angular distance , charikar  @xcite introduced the hash family @xmath83 where @xmath50 is any spherically symmetric distribution on @xmath69 , and @xmath84 satisfies : @xmath85 the vector @xmath86 can be interpreted as the normal vector of a random hyperplane , and the hash value depends on which side of the hyperplane @xmath67 lies on .",
    "for this hash function , the probability of a collision is directly proportional to the angle between @xmath67 and @xmath87 : @xmath88 for any two angles @xmath89 , the above family @xmath71 is @xmath90-sensitive .    [",
    "[ large - deviations - theory . ] ] large deviations theory .",
    "+ + + + + + + + + + + + + + + + + + + + + + + +    let @xmath91 be a sequence of random vectors corresponding to an empirical mean , i.e.  @xmath92 with @xmath93 i.i.d .",
    "we define the logarithmic moment generating function @xmath94 of @xmath95 as : @xmath96.\\end{aligned}\\ ] ] define @xmath97 .",
    "the _ fenchel - legendre _ transform of @xmath94 is defined as : @xmath98 the following result describes that under certain conditions on @xmath99 , the asymptotics of the probability measure on a set @xmath100 are related to the function @xmath101 .",
    "[ grtner - ellis theorem ( * ? ? ?",
    "* theorem 2.3.6 and corollary 6.1.6 ) ] [ lem : ge ] let @xmath102 be contained in the interior of @xmath103 , and let @xmath95 be an empirical mean . then for arbitrary sets @xmath100 , @xmath104    the latter statement can be read as @xmath105 , and thus tells us exactly how @xmath106 scales as @xmath1 tends to infinity , up to order terms .",
    "in this section , we will analyze full - dimensional hypercube hashing , with hash family @xmath107 where @xmath108 denotes the rotation group , and @xmath109 satisfies : @xmath110 in other words , a hypercube hash function first applies a uniformly random rotation , and then maps the resulting vector to the orthant it lies in .",
    "this equivalently corresponds to a concatenation of @xmath1 hyperplane hash functions , where all hyperplanes are orthogonal .",
    "collision probabilities for prescribed angles @xmath22 between @xmath67 and @xmath87 are denoted by : @xmath111 above , the randomness is over @xmath112 , with @xmath67 and @xmath87 arbitrary vectors at angle @xmath22 ( e.g.  @xmath113 and @xmath114 ) .",
    "alternatively , the random rotation @xmath115 inside @xmath109 may be omitted , and the probability can be computed over @xmath116 drawn uniformly at random from a spherically symmetric distribution , _ conditioned _ on their common angle being @xmath22 .      although theorem  [ thm : main ] is a key result , due to space restrictions we have decided to defer the full proof ( approximately @xmath117  pages ) to the appendix .",
    "the approach of the proof can be summarized by the following four steps :    * rewrite the collision probabilities in terms of ( normalized ) half - normal vectors @xmath116 ; * introduce dummy variables @xmath118 for the norms of these half - normal vectors , so that the probability can be rewritten in terms of unnormalized half - normal vectors ; * apply the grtner - ellis theorem ( lemma  [ lem : ge ] ) to the three - dimensional vector given by + @xmath119 to compute the resulting probabilities for arbitrary @xmath118 ; * maximize the resulting expressions over @xmath120 to get the final result .",
    "the majority of the technical part of the proof lies in computing @xmath121 , which involves a somewhat tedious optimization of a multivariate function through a case - by - case analysis .",
    "[ [ a - note - on - gaussian - approximations . ] ] a note on gaussian approximations .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    from the ( above outline of the ) proof , and the observation that the final optimization over @xmath118 yields @xmath122 as the optimum , one might wonder whether a simpler analysis might be possible by assuming ( half-)normal vectors are already normalized .",
    "such a computation however would only lead to an approximate solution , which is perhaps easiest to see by computing collision probabilities for @xmath123 . in the exact computation , where vectors are normalized",
    ", @xmath124 implies @xmath125 .",
    "if however we do not take into account the norms of @xmath28 and @xmath29 , and do not condition on the norms being equal to @xmath126 , then @xmath124 could also mean that @xmath116 are slightly longer than @xmath126 and have a small , non - zero angle .",
    "in fact , such a computation would indeed yield @xmath127 as @xmath128 .      from theorem  [ thm : main ]",
    ", we can draw several conclusions . substituting values for @xmath22",
    ", we can find asymptotics for @xmath30 , such as @xmath129 and @xmath130 .",
    "we observe that the limiting function of theorem  [ thm : main ] ( without the order terms ) is continuous everywhere except at @xmath131 .",
    "to understand the boundary @xmath132 of the piece - wise limit function , note that two ( normalized ) half - normal vectors @xmath116 have expected inner product @xmath133 .",
    "[ [ lsh - exponents - rho - for - random - settings . ] ] lsh exponents @xmath7 for random settings .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    using theorem  [ thm : main ] , we can explicitly compute lsh exponents @xmath7 for given angles @xmath81 and @xmath80 for large @xmath1 . as an example , consider the random setting , i.e. @xmath20 approaches @xmath134 from below .",
    "alternatively , one might interpret this as that if distant points lie at distance @xmath135 , then we might expect approximately half of them to lie at distance less than @xmath134 , with query complexity @xmath136 . if however @xmath137 then clearly @xmath138 , regardless of @xmath1 and @xmath6 . ] with @xmath139 , corresponding to @xmath140 and @xmath141 .",
    "substituting the collision probabilities from theorem  [ thm : main ] , we get @xmath142 as @xmath40 . to compare ,",
    "if we had used random hyperplanes , we would have gotten a limiting value @xmath143 . for the random case ,",
    "figure  [ fig : rho ] compares limiting values @xmath7 using random and orthogonal hyperplanes , and using the asymptotically superior cross - polytope lsh .",
    "[ [ scaling - at - theta - to-0-and - asymptotics - of - rho - for - large - c . ] ] scaling at @xmath128 and asymptotics of @xmath7 for large @xmath6 .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    for @xmath22 close to @xmath144 , by theorem  [ thm : main ] we are in the regime defined by @xmath145 . for @xmath146 with @xmath147",
    "small , observe that @xmath148 satisfies @xmath149 . computing a taylor expansion around @xmath150 , we eventually find @xmath151 . substituting this value @xmath145 into @xmath30 with @xmath146 ,",
    "we find : @xmath152 to compare this with hyperplane lsh , recall that the collision probability for @xmath1 random hyperplanes is equal to @xmath153 . since @xmath146 translates to @xmath154 ,",
    "the collision probabilities for hyperplane hashing in this regime are also @xmath155 .",
    "in other words , for angles @xmath128 , the collision probabilities for hyperplane hashing and hypercube hashing are similar .",
    "this can also be observed in figure  [ fig : main ] .",
    "based on this result , we further deduce that in random settings with large @xmath6 , for hypercube lsh we have : @xmath156 for hyperplane lsh , the numerator is the same , while the denominator is @xmath157 instead of @xmath158 , leading to values @xmath7 which are a factor @xmath159 larger .",
    "both methods are inferior to cross - polytope lsh for large @xmath1 , as there @xmath160 for large @xmath6  @xcite .",
    "to get an idea how hypercube lsh compares to other methods when @xmath1 is not too large , we start by giving explicit collision probabilities for the first non - trivial case , namely @xmath161 .",
    "[ prop : square ] for @xmath161 , @xmath162 for @xmath163 and @xmath164 otherwise .    in two dimensions , two randomly rotated vectors @xmath116 at angle @xmath22",
    "can be modeled as @xmath165 and @xmath166 for @xmath167 .",
    "the conditions @xmath168 are then equivalent to @xmath169 , which for @xmath170 occurs with probability @xmath171 over the randomness of @xmath172 .",
    "as a collision can occur in any of the four quadrants , we finally multiply this probability by @xmath173 to obtain the stated result .",
    "figure  [ fig : exp - full ] depicts @xmath174 in green , along with hyperplane lsh ( blue ) and the asymptotics for hypercube lsh ( red ) . for larger @xmath1 ,",
    "computing @xmath30 exactly becomes more complicated , and so instead we performed experiments to empirically obtain estimates for @xmath30 as @xmath1 increases .",
    "these estimates are also shown in figure  [ fig : exp - full ] , and are based on @xmath175 trials for each @xmath22 and @xmath1 .",
    "observe that as @xmath176 and/or @xmath1 grows larger , @xmath30 decreases and the empirical estimates become less reliable .",
    "points are omitted for cases where no successes occurred .    .",
    "the green curve denotes the exact collision probabilities for @xmath161 from proposition  [ prop : square ] .",
    "[ fig : exp - full],width=529 ]    based on these estimates and our intuition , we conjecture that ( 1 ) for @xmath177 , the scaling of @xmath178 is similar for all @xmath1 , and similar to the asymptotic behavior of theorem  [ thm : main ] ; ( 2 ) the normalized collision probabilities for @xmath179 approach their limiting value from below ; and ( 3 ) @xmath30 is likely to be continuous for arbitrary @xmath1 , implying that for @xmath176 , the collision probabilities tend to @xmath144 for each @xmath1 .",
    "these together suggest that values for @xmath7 are actually _ smaller _ when @xmath1 is small than when @xmath1 is large , and the asymptotic estimate from figure  [ fig : rho ] might be pessimistic in practice . for the random setting",
    ", this would suggest that @xmath180 regardless of @xmath6 , as @xmath181 as @xmath176 for arbitrary @xmath1 .",
    "[ [ comparison - with - hyperplanecross - polytope - lsh . ] ] comparison with hyperplane / cross - polytope lsh .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    finally , ( * ? ? ?",
    "* figures 1 and 2 ) previously illustrated that among several lsh methods , the smallest values @xmath7 ( for their parameter sets ) are obtained with hypercube lsh with @xmath182 , achieving smaller values @xmath7 than e.g.  cross - polytope lsh with @xmath183 .",
    "an explanation for this can be found in :    * the ( conjectured ) convergence of @xmath7 to its limit from _ below _ , for hypercube lsh ; * the slow convergence of @xmath7 to its limit ( from above ) for cross - polytope lsh scales as @xmath184 , with a first order term scaling as @xmath185 , i.e.  a relative order term of the order @xmath186 . ] .",
    "this suggests that the actual values @xmath7 for moderate dimensions @xmath1 may well be smaller for hypercube lsh ( and hyperplane lsh ) than for cross - polytope lsh .",
    "based on the limiting cases @xmath161 and @xmath40 , we further conjecture that compared to hyperplane lsh , hypercube lsh achieves smaller values @xmath7 for arbitrary @xmath1 .      to further assess the practicality of hypercube lsh , recall that hashing is done as follows :    * apply a uniformly random rotation @xmath115 to @xmath67 ; * look at the signs of @xmath187 .",
    "theoretically , a uniformly random rotation will be rather expensive to compute , with @xmath115 being a real , dense matrix . as previously discussed in e.g.  @xcite",
    ", it may suffice to only consider a sparse subset of all rotation matrices with a large enough amount of randomness , and as described in  @xcite pseudo - random rotations may also be help speed up the computations in practice . as described in  @xcite , this can even be made provable , to obtain a reduced @xmath188 computational complexity for applying a random rotation .    finally , to compare this with cross - polytope lsh , note that cross - polytope lsh in dimension @xmath1 partitions the space in @xmath189 regions , as opposed to @xmath190 for hypercube hashing . to obtain a similar fine - grained partition of the space with cross - polytopes",
    ", one would have to concatenate @xmath191 random cross - polytope hashes , which corresponds to computing @xmath191 ( pseudo-)random rotations , compared to only one rotation for hypercube lsh .",
    "we therefore expect hashing to be up to a factor @xmath191 less costly .",
    "since a high - dimensional hypercube partitions the space in a large number of regions , for various applications one may only want to use hypercubes in a lower dimension @xmath192 . in those cases , one would first apply a random rotation to the data set , and then compute the hash based on the signs of the first @xmath42 coordinates of the rotated data set .",
    "this corresponds to the hash family @xmath193 , with @xmath194 satisfying : @xmath195 when `` projecting '' down onto the first @xmath42 coordinates , observe that distances and angles are distorted : the angle between the vectors formed by the first @xmath42 coordinates of @xmath67 and @xmath87 may not be the same as @xmath196 .",
    "the amount of distortion depends on the relation between @xmath42 and @xmath1 .",
    "below , we will investigate how the collision probabilities @xmath197 for partial hypercube lsh scale with @xmath42 and @xmath1 , where @xmath198 .",
    "first , observe that for @xmath199 , partial hypercube lsh is _ equal _ to hyperplane lsh , i.e.  @xmath200 .",
    "for @xmath201 , we first observe that both ( partial ) hypercube lsh and hyperplane lsh can be modeled by a projection onto @xmath42 dimensions :    * hyperplane lsh : @xmath202 with @xmath203 ; * hypercube lsh : @xmath204 with @xmath203 .    here",
    "@xmath205 denotes the matrix obtained from @xmath115 after applying gram - schmidt orthogonalization to the rows of @xmath115 . in both cases ,",
    "hashing is done after the projection by looking at the signs of the projected vector .",
    "therefore , the only difference lies in the projection , and one could ask : for which @xmath42 , as a function of @xmath1 , are these projections equivalent ? when is a set of random hyperplanes already ( almost ) orthogonal ?",
    "this question was answered in  @xcite : if @xmath206 , then @xmath207 in probability as @xmath40 ( implying @xmath208 ) , while for @xmath209 this maximum does not converge to @xmath144 in probability . in other words , for large @xmath1 a set of @xmath42 random hyperplanes in @xmath1 dimensions is ( approximately ) orthogonal iff @xmath206 .",
    "let @xmath197 denote the collision probabilities for partial hypercube lsh , and let @xmath206",
    ". then @xmath210 .",
    "as @xmath209 random vectors in @xmath1 dimensions are asymptotically _ not _ orthogonal , in that case one might expect either convergence to full - dimensional hypercube lsh , or to something in between hyperplane and hypercube lsh .      to characterize when partial hypercube lsh is equivalent to full hypercube lsh , we first observe that if @xmath42 is large compared to @xmath211 , then convergence to the hypercube lsh asymptotics follows from the johnson - lindenstrauss lemma .",
    "let @xmath212 .",
    "then the same asymptotics for the collision probabilities as those of full - dimensional hypercube lsh apply .",
    "let @xmath213 . by the johnson - lindenstrauss lemma  @xcite , we can construct a projection @xmath202 from @xmath1 onto @xmath42 dimensions , preserving all pairwise distances up to a factor @xmath214 for @xmath215 . for fixed @xmath213 ,",
    "this implies the angle @xmath216 between @xmath217 and @xmath218 will be in the interval @xmath219 , and so the collision probability lies in the interval @xmath220 . for large @xmath1 , this means that the asymptotics of @xmath30 are the same .    to analyze collision probabilities for partial hypercube lsh when neither of the previous two propositions applies , note that through a series of transformations similar to those for full - dimensional hypercube lsh , it is possible to eventually end up with the following probability to compute , where @xmath221 and @xmath222 : @xmath223 here @xmath224 is some function of @xmath216 and @xmath22 .",
    "the approach is comparable to how we ended up with a similar probability to compute in the proof of theorem  [ thm : main ] , except that we split the summation indices @xmath225 $ ] into two sets @xmath226 of size @xmath227 and @xmath228 of size @xmath229 .",
    "we then substitute @xmath230 and @xmath231 , and add dummy variables @xmath232 for the norms of the four partial vectors , and a dummy angle @xmath216 for the angle between the @xmath227-dimensional vectors , given the angle @xmath22 between the @xmath1-dimensional vectors .",
    "although the vector @xmath233 formed by the six random variables in   is not an empirical mean over a fixed number @xmath1 of random vectors ( the first three are over @xmath227 terms , the last three over @xmath229 terms ) , one may expect a similar large deviations result such as lemma  [ lem : ge ] to apply here . in that case , the function @xmath234 would be a function of six variables , which we would like to evaluate at @xmath235 .",
    "the function @xmath101 itself involves an optimization ( finding a supremum ) over another six variables @xmath236 , so to compute collision probabilities for given @xmath237 exactly , using large deviations theory , one would have to compute an expression of the following form : @xmath238 as this is a very complex task , and the optimization will depend heavily on the parameters @xmath237 defined by the problem setting , we leave this optimization as an open problem .",
    "we only mention that intuitively , from the limiting cases of small and large @xmath42 we expect that depending on how @xmath42 scales with @xmath1 ( or @xmath13 ) , we obtain a curve somewhere in between the two curves depicted in figure  [ fig : main ] .",
    "to get an idea of how @xmath197 scales with @xmath42 in practice , we empirically computed several values for fixed @xmath239 . for fixed @xmath22",
    "we then applied a least - squares fit of the form @xmath240 to the resulting data , and plotted @xmath241 in figure  [ fig : exp - partial ] .",
    "these data points are again based on at least @xmath175 experiments for each @xmath42 and @xmath22 .",
    "we expect that as @xmath42 increases , the collision probabilities slowly move from hyperplane hashing towards hypercube hashing , this can also be seen in the graph  for @xmath242 , the least - squares fit is almost equal to the curve for hyperplane lsh , while as @xmath42 increases the curve slowly moves down towards the asymptotics for full hypercube lsh .",
    "again , we stress that as @xmath42 becomes larger , the empirical estimates become less reliable , and so we did not consider even larger values for @xmath42 .",
    ", for different values @xmath42 , compared with the asymptotics for hypercube lsh ( red ) and hyperplane lsh ( blue).[fig : exp - partial],width=529 ]    compared to full hypercube lsh and figure  [ fig : exp - full ] , we observe that we now approach the limit from above ( although the fitted collision probabilities never seem to be smaller than those of hyperplane lsh ) , and therefore the values @xmath7 for partial hypercube lsh are likely to lie in between those of hyperplane and ( the asymptotics of ) hypercube lsh .",
    "we finally consider an explicit application for hypercube lsh , namely lattice sieving algorithms for the shortest vector problem . given a basis @xmath243 of a lattice @xmath244 , the shortest vector problem ( svp ) asks to find a shortest non - zero vector in this lattice .",
    "various different methods for solving svp in high dimensions are known , and currently the algorithm with the best heuristic time complexity in high dimensions is based on lattice sieving , combined with nearest neighbor searching  @xcite .    in short , lattice sieving works by generating a long list @xmath245 of pairwise reduced lattice vectors , where @xmath21 are reduced iff @xmath246 .",
    "the previous condition is equivalent to @xmath247 , and so the length of @xmath245 can be bounded by the kissing constant in dimension @xmath1 , which is conjectured to scale as @xmath248 .",
    "therefore , if we have a list of size @xmath249 , any newly sampled lattice vector can be reduced against the list many times to obtain a very short lattice vector .",
    "the time complexity of this method is dominated by doing @xmath250 reductions ( searches for nearby vectors ) with a list of size @xmath13 .",
    "a linear search trivially leads to a heuristic complexity of @xmath251 ( with space @xmath252 ) , while nearest neighbor techniques can reduce the time complexity to @xmath253 for @xmath254 ( increasing the space to @xmath253 ) . for more details , see e.g.  @xcite .",
    "based on the collision probabilities for hypercube lsh , and assuming the asymptotics for partial hypercube lsh ( with @xmath255 ) are similar to those of full - dimensional hypercube lsh , we obtain the following result . an outline of the proof is given in the appendix .",
    "[ prop : lattice ] suppose the asymptotics for full hypercube lsh also hold for partial hypercube lsh with @xmath256 .",
    "then lattice sieving with hypercube lsh heuristically solves svp in time and space @xmath257 .",
    "as expected , the conjectured asymptotic performance of ( sieving with ) hypercube lsh lies in between those of hyperplane lsh and cross - polytope lsh .",
    "* linear search  @xcite : @xmath258 .",
    "* hyperplane lsh  @xcite : @xmath259 . *",
    "* hypercube lsh * : @xmath257 .",
    "* spherical cap lsh  @xcite : @xmath260 .",
    "* cross - polytope lsh  @xcite : @xmath260 . *",
    "spherical lsf  @xcite : @xmath261 .    in practice",
    "however , the picture is almost entirely reversed  @xcite .",
    "the lattice sieving method used to solve svp in the highest dimension to date ( @xmath262 ) used a very optimized linear search  @xcite .",
    "the furthest that any nearest neighbor - based sieve has been able to go to date is @xmath263 , using hypercube lsh  @xcite .",
    "experiments further indicated that spherical lsf only becomes competitive with hypercube lsh as @xmath264  @xcite , while sieving with cross - polytope lsh turned out to be rather slow compared to other methods  @xcite . although it remains unclear which nearest neighbor method is the `` most practical '' in the application of lattice sieving , hypercube lsh is one of the main contenders .    [ [ acknowledgments . ] ] acknowledgments .",
    "+ + + + + + + + + + + + + + + +    the author is indebted to ofer zeitouni for his suggestion to use results from large deviations theory , and for his many helpful comments regarding this application .",
    "the author further thanks brendan mckay and carlo beenakker for their comments .",
    "the author is supported by the snsf erc transfer grant cretp2 - 166734 felicity .",
    "alrw17    dimitris achlioptas .",
    "database - friendly random projections . in _ pods _ , pages 274281 , 2001 .",
    "alexandr andoni , piotr indyk , thijs laarhoven , ilya razenshteyn , and ludwig schmidt .",
    "practical and optimal lsh for angular distance . in _ nips _ , pages 12251233 , 2015 .",
    "alexandr andoni , piotr indyk , huy  l nguyn , and ilya razenshteyn . beyond locality - sensitive hashing . in _ soda _",
    ", pages 10181028 , 2014 .",
    "alexandr andoni , thijs laarhoven , ilya razenshteyn , and erik waingarten .",
    "optimal hashing - based time - space trade - offs for approximate near neighbors . in _ soda _ , pages 4766 , 2017 .",
    "alexandr andoni . .",
    "phd thesis , massachusetts institute of technology , 2009 .",
    "alexandr andoni and ilya razenshteyn .",
    "optimal data - dependent hashing for approximate near neighbors . in _ stoc _ , pages 793801 , 2015 .",
    "milton abramowitz and irene  a. stegun . .",
    "dover publications , 1972 .",
    "anja becker , lo ducas , nicolas gama , and thijs laarhoven .",
    "new directions in nearest neighbor searching with applications to lattice sieving . in _ soda _ , pages 1024 , 2016 .    christopher  m. bishop . .",
    "springer - verlag , 2006 .",
    "anja becker and thijs laarhoven .",
    "efficient ( ideal ) lattice sieving using cross - polytope lsh . in _",
    "africacrypt _ , pages 323 , 2016 .",
    "moses  s. charikar .",
    "similarity estimation techniques from rounding algorithms . in",
    "_ stoc _ , pages 380388 , 2002 .",
    "tobias christiani . a framework for similarity search with space - time tradeoffs using locality - sensitive filtering . in _ soda _ , pages 3146 , 2017 .",
    "richard  o. duda , peter  e. hart , and david  g. stork . .",
    "wiley , 2000 .",
    "moshe dubiner .",
    "bucketing coding and information theory for the statistical high - dimensional nearest - neighbor problem . , 56(8):41664179 , aug 2010 .",
    "amir dembo and ofer zeitouni . .",
    "springer , 2010 .",
    "kave eshghi and shyamsundar rajaram . locality sensitive hash functions based on concomitant rank order statistics . in _",
    "kdd _ , pages 221229 , 2008 .",
    "piotr indyk and rajeev motwani .",
    "approximate nearest neighbors : towards removing the curse of dimensionality . in _ stoc _ , pages 604613 , 1998 .",
    "tiefeng jiang .",
    "how many entries of a typical orthogonal matrix can be approximated by independent normals ?",
    ", 34(4):14971529 , 2006 .",
    "william  b. johnson and joram lindenstrauss .",
    "extensions of lipschitz mappings into a hilbert space .",
    ", 26(1):189206 , 1984 .",
    "thorsten kleinjung .",
    "private communication , 2014 .",
    "christopher kennedy and rachel ward .",
    "fast cross - polytope locality - sensitive hashing . in _",
    "itcs _ , 2017 .",
    "thijs laarhoven .",
    "sieving for shortest vectors in lattices using angular locality - sensitive hashing . in _ crypto _ , pages 322 , 2015 .",
    "thijs laarhoven and benne de  weger .",
    "faster sieving for shortest lattice vectors using spherical locality - sensitive hashing . in _ latincrypt _ , pages 101118 , 2015 .",
    ". private communication .",
    ", 2016 .",
    "artur mariano and christian bischof . enhancing the scalability and memory usage of hashsieve on multi - core cpus . in",
    "_ pdp _ , pages 545552 , 2016 .",
    "artur mariano , thijs laarhoven , and christian bischof .",
    "parallel ( probable ) lock - free hashsieve : a practical sieving algorithm for the svp . in _ icpp _ ,",
    "pages 590599 , 2015 .",
    "artur mariano , thijs laarhoven , and christian bischof . a parallel variant of ldsieve for the svp on lattices .",
    ", 2017 .",
    "rajeev motwani , assaf naor , and rina panigrahy .",
    "lower bounds on locality sensitive hashing .",
    ", 21(4):930935 , 2007 .",
    "alexander may and ilya ozerov . on computing nearest neighbors with applications to decoding of binary linear codes .",
    "in _ eurocrypt _ , pages 203228 , 2015 .",
    "phong  q. nguyn and thomas vidick .",
    "sieve algorithms for the shortest vector problem are practical .",
    ", 2(2):181207 , 2008 .",
    "ryan odonnell , yi  wu , and yuan zhou .",
    "optimal lower bounds for locality sensitive hashing ( except when @xmath265 is tiny ) . in _ ics _ , pages 276283 , 2011 .",
    "gregory shakhnarovich , trevor darrell , and piotr indyk .",
    ". mit press , 2005 .",
    "michael schneider and nicolas gama .",
    "challenge , 2015 .",
    "ludwig schmidt , matthew sharifi , and ignacio lopez - moreno . large - scale speaker identification . in _",
    "icassp _ , pages 16501654 , 2014 .",
    "narayanan sundaram , aizana turmukhametova , nadathur satish , todd mostak , piotr indyk , samuel madden , and pradeep dubey .",
    "streaming similarity search over one billion tweets using parallel locality - sensitive hashing .",
    ", 6(14):19301941 , 2013 .",
    "kengo terasawa and yuzuru tanaka .",
    "spherical lsh for approximate nearest neighbor search on unit hypersphere . in _ wads _ , pages 2738 , 2007 .",
    "kengo terasawa and yuzuru tanaka .",
    "approximate nearest neighbor search for a dataset of normalized vectors . in _",
    "ieice transactions on information and systems _ , volume  92 , pages 16091619 , 2009 .",
    "theorem  [ thm : main ] will be proved through a series of lemmas , each making partial progress towards a final solution .",
    "reading only the claims made in the lemmas may give the reader an idea how the proof is built up . before starting the proof , we begin with a useful lemma regarding integrals of ( exponentials of ) quadratic forms .    [",
    "lem : exp ] let @xmath266 with @xmath267 and @xmath268 .",
    "then : @xmath269    the proof below is based on substituting @xmath270 ( and @xmath271 ) before computing the integral over @xmath49 .",
    "an integral over @xmath272 then remains , which leads to the arctangent solution in case @xmath273 .",
    "@xmath274_{x=0}^{\\infty } \\ , ds \\\\ & = \\int_{0}^{\\infty } \\left[0 - \\frac{1}{2 ( a + bs + cs^2)}\\right ] \\ , ds \\\\ & = \\frac{-1}{2 } \\int_{0}^{\\infty } \\frac{1}{a + bs + cs^2 } \\ , ds.\\end{aligned}\\ ] ] the last equality used the assumptions @xmath267 and @xmath273 so that @xmath275 for all @xmath276 .",
    "we then solve the last remaining integral ( see e.g.  ( * ? ? ?",
    "* equation ( 3.3.16 ) ) ) to obtain : @xmath277_{s=0}^{\\infty } \\\\ & = \\frac{-1}{2 \\sqrt{4 a c - b^2 } } \\left(-\\pi - 2 \\arctan \\left(\\frac{b}{\\sqrt{4 a c - b^2}}\\right)\\right).\\end{aligned}\\ ] ] eliminating minus signs and substituting @xmath278 , we obtain the stated result .",
    "next , we begin by restating the collision probability between two vectors in terms of half - normal vectors .",
    "[ lem : step1 ] let @xmath71 denote the hypercube hash family in @xmath1 dimensions , and as before , let @xmath279 be defined as : @xmath280 let @xmath281 and let the sequence @xmath282 be defined as : @xmath283 then : @xmath284    first , we write out the definition of the conditional probability in @xmath279 , and use the fact that each of the @xmath190 hash regions ( orthants ) has the same probability mass . here",
    "@xmath26 denote random gaussian vectors , and subscripts denoting what probabilities are computed over are omitted when implicit .",
    "@xmath285 by lemma  [ lem : sphericalcap ] , the denominator is equal to @xmath286 .",
    "the numerator of   can further be rewritten as a conditional probability on @xmath287 , multiplied with @xmath288 . to incorporate the conditionals @xmath168",
    ", we replace @xmath26 by half - normal vectors @xmath281 , resulting in : @xmath289 to incorporate the normalization over the ( half - normal ) vectors @xmath290 and @xmath291 , we introduce dummy variables @xmath118 corresponding to the norms of @xmath292 and @xmath293 , and observe that as the probabilities are exponential in @xmath1 , the integrals will be dominated by the maximum value of the integrand in the given range : @xmath294 substituting @xmath295 , we obtain the claimed result .    note that @xmath296 are pairwise but not jointly independent . to compute the density of @xmath95 at @xmath297 for @xmath40",
    ", we use the grtner - ellis theorem stated in lemma  [ lem : ge ] .    [ applying the grtner - ellis theorem to @xmath95 ] [ lem : ge2 ]",
    "let @xmath282 as in lemma  [ lem : step1 ] , and let @xmath94 and @xmath101 as in section  [ sec : pre ] .",
    "then @xmath102 lies in the interior of @xmath103 , and therefore @xmath299    essentially , all that remains now is computing @xmath101 at the appropriate point @xmath300 . to continue",
    ", we first compute the logarithmic moment generating function @xmath301 of @xmath95 :    [ computing @xmath94 ] let @xmath95 as before , and let @xmath303 . then",
    "for @xmath304 we have : @xmath305    by the definition of the lmgf , we have : @xmath306.\\end{aligned}\\ ] ] we next compute the inner expectation over the random variables @xmath307 , by writing out the double integral over the product of the argument with the densities of @xmath308 and @xmath309 . @xmath310 \\\\   & = \\int_0^{\\infty } \\sqrt{\\frac{2}{\\pi } } \\ , \\exp\\left(-\\frac{x^2}{2}\\right ) d x \\int_0^{\\infty } \\sqrt{\\frac{2}{\\pi } } \\ , \\exp\\left(-\\frac{y^2}{2}\\right ) dy \\ , \\exp \\left ( \\lambda_1 x y + \\lambda_2 x^2 + \\lambda_3 y^2 \\right ) \\\\   & = \\frac{2}{\\pi } \\int_0^{\\infty } \\int_0^{\\infty } \\exp \\left ( \\lambda_1 x y + \\left(\\lambda_2 - \\tfrac{1}{2}\\right ) x^2 + \\left(\\lambda_3 - \\tfrac{1}{2}\\right ) y^2 \\right ) \\ , dx \\ , dy \\ , .\\end{aligned}\\ ] ] applying lemma  [ lem : exp ] with @xmath311 yields the claimed expression for @xmath94 , as well as the bounds stated in @xmath103 which are necessary for the expectation to be finite .",
    "we now continue with computing the fenchel - legendre transform of @xmath94 , which involves a rather complicated maximization ( supremum ) over @xmath312 .",
    "the following lemma makes a first step towards computing this supremum .",
    "let @xmath313 such that @xmath314 .",
    "then the fenchel - legendre transform @xmath101 of @xmath94 at @xmath300 satisfies @xmath315    first , we recall the definition of @xmath101 and substitute the previous expression for @xmath94 : @xmath316 here as before @xmath317 .",
    "let the argument of the supremum above be denoted by @xmath318 .",
    "we make a change of variables by setting @xmath319 and @xmath320 , so that @xmath321 becomes @xmath322 : @xmath323 we continue by making a further change of variables @xmath324 so that @xmath325 . as a result",
    "the dependence of @xmath224 on @xmath326 is only through the fourth and fifth terms above , from which one can easily deduce that the supremum over @xmath326 occurs at @xmath327 .",
    "this also implies that @xmath328 . substituting these values for @xmath329",
    ", we obtain : @xmath330 finally , we use the substitution @xmath331 . from @xmath332",
    "it follows that @xmath333 .",
    "this substitution and some rewriting of @xmath224 leads to the claimed result .",
    "the previous simplifications were regardless of @xmath334 , where the only assumption that was made during the optimization of @xmath326 was that @xmath314 . in our application , we want to compute @xmath101 at @xmath335 for certain @xmath120 and @xmath213 . substituting these values for @xmath300 , the expression from lemma  [ lem : step1 ] becomes : @xmath336 the remaining optimization over @xmath337 now takes slightly different forms depending on whether @xmath338 or @xmath339 .",
    "we will tackle these two cases separately , based on the identity : @xmath340    [ computing @xmath121 for positive @xmath341 let @xmath335 with @xmath120 and @xmath213 .",
    "for @xmath31 , let @xmath342 be the unique solution to  .",
    "then the fenchel - legendre transform @xmath101 at @xmath300 , restricted to @xmath339 , satisfies @xmath343 0 , & \\text{if } \\theta \\in [ \\arccos \\frac{2}{\\pi } , \\frac{\\pi}{2 } ) .",
    "\\end{cases}\\end{aligned}\\ ] ]    substituting @xmath339 into  , we obtain : @xmath344 differentiating w.r.t . @xmath345 gives @xmath346 .",
    "recall that @xmath347 . for @xmath348",
    "the derivative is therefore positive , for @xmath349 it is negative , and there is a global maximum at the only root @xmath350 . in that case , the expression further simplifies and we can pull out more terms that do not depend on @xmath351 , to obtain : @xmath352 here we used the identity @xmath353 .",
    "now , for @xmath354 we have @xmath355 , while for @xmath356 , we have @xmath357 in other words , if @xmath358 or @xmath359 , we have @xmath360 ( the second order term is negative for @xmath361 ) , while for @xmath362 we approach the same limit from above as @xmath363 .",
    "for @xmath362 there is a non - trivial maximum at some value @xmath364 , while for @xmath359 , we can see from the derivative @xmath365 that @xmath366 is strictly increasing on @xmath367 , and the supremum is attained at @xmath356 .",
    "we therefore obtain two different results , depending on whether @xmath362 or @xmath359 .",
    "* case 1 * : @xmath368 .",
    "the supremum is attained in the limit of @xmath356 , which leads to @xmath369 and the stated expression for @xmath370 . *",
    "case 2 * : @xmath371 . in this case",
    "there is a non - trivial maximum at some value @xmath372 , namely there where the derivative @xmath373 .",
    "after computing the derivative , eliminating the ( positive ) denominator and rewriting , this condition is equivalent to  .",
    "this allows us to rewrite @xmath374 and @xmath101 in terms of @xmath145 , by substituting the given expression for @xmath375 , which ultimately leads to the stated formula for @xmath376 .",
    "[ computing @xmath121 for negative @xmath341 let @xmath335 with @xmath120 and @xmath213 .",
    "for @xmath32 , let @xmath34 be the unique solution to  .",
    "then the fenchel - legendre transform @xmath101 at @xmath300 , restricted to @xmath338 , satisfies @xmath377 ; \\\\[3ex ] \\ln\\left(\\dfrac{\\pi \\beta_1 ( \\beta_1 \\cos \\theta + 1)}{2 ( \\cos \\theta + \\beta_1)^2}\\right ) , & \\text{if } \\theta \\in ( \\arccos \\frac{2}{\\pi } , \\frac{\\pi}{3 } ) ; \\\\[3ex ] \\ln\\left(\\dfrac{\\pi}{2(1 + \\cos \\theta)}\\right ) , & \\text{if } \\theta \\in [ \\frac{\\pi}{3 } , \\frac{\\pi}{2 } ) .",
    "\\end{cases}\\end{aligned}\\ ] ]    we again start by substituting @xmath338 into  : @xmath378 differentiating w.r.t . @xmath345",
    "gives @xmath379 .",
    "for @xmath380 this is positive , for @xmath381 this is negative , and so the maximum is at @xmath382 . substituting this value for @xmath345 , and pulling out terms which do not depend on @xmath351 yields : @xmath383 above we used the identity @xmath384 , where the factor @xmath385 has been pulled outside the supremum .",
    "now , differentiating @xmath386 w.r.t .",
    "@xmath351 results in :",
    "@xmath387 clearly the denominator is positive , while for @xmath354 the limit is negative iff @xmath388 .",
    "for @xmath356 we further have @xmath389 for @xmath358 and @xmath390 for @xmath391 .",
    "we therefore analyze three cases separately below .",
    "* case 1 * : @xmath392 . in this parameter range , @xmath393 is negative for all @xmath394 , and the supremum lies at @xmath354 with limiting value @xmath395 .",
    "this yields the given expression for @xmath396 .",
    "* case 2 * : @xmath397 .",
    "for @xmath22 in this range , @xmath393 is positive for @xmath354 and negative for @xmath356 , and changes sign exactly once , where it attains its maximum . after some rewriting , we find that this is at the value @xmath398 satisfying the relation from  . substituting this expression for @xmath399 into @xmath386 , we obtain the result for @xmath396 . * case 3 * : @xmath400 . in this case",
    "@xmath401 is positive for all @xmath394 , and the supremum lies at @xmath356 . for @xmath356 we have @xmath402 ( regardless of @xmath22 ) and we therefore get the final claimed result .    combining the previous two results with lemma  [ lem : ge2 ] and equation  [ eq : split ]",
    ", we obtain explicit asymptotics for @xmath403 .",
    "what remains is a maximization over @xmath120 of @xmath279 , which translates to a minimization of @xmath101 . as @xmath404 attains its minimum at @xmath122 with value",
    "@xmath144 , we obtain theorem  [ thm : main ] .",
    "we will assume the reader is familiar with ( the notation from )  @xcite .",
    "let @xmath405 denote the number of hash tables , and @xmath249 .",
    "going through the proofs of  ( * ? ? ?",
    "* appendix a ) and replacing the explicit instantiation of the collision probabilities @xmath406 by an arbitrary function @xmath30 , we get that the optimal number of hash functions concatenated into one function for each hash table , denoted @xmath407 , satisfies @xmath408 the latter equality follows when substituting @xmath409 and substituting the collision probabilities for partial hypercube lsh in some dimension @xmath410 .",
    "as we need @xmath411 , the previous relation translates to a condition on @xmath42 as @xmath412 . as we expect the collision probabilities to be closer to those of full - dimensional hypercube lsh when @xmath42 is closer to @xmath1",
    ", we replace the above inequality by an equality , and what remains is finding the minimum value @xmath413 satisfying the given constraints .    by carefully checking the proofs of ( * ? ? ?",
    "* appendix a.2-a.3 ) , the exact condition on @xmath413 to obtain the minimum asymptotic time complexity is the following : @xmath414 here @xmath415 , and @xmath416 corresponds to the exponent @xmath7 for given angles @xmath417 .",
    "note that in the above equation , only @xmath413 is an unknown . substituting the asymptotic collision probabilities from theorem  [ thm : main ]",
    ", we find a solution at @xmath418 , with maximizing angle @xmath419 .",
    "this corresponds to a time and space complexity of @xmath420 as claimed ."
  ],
  "abstract_text": [
    "<S> a celebrated technique for finding near neighbors for the angular distance involves using a set of _ random _ hyperplanes to partition the space into hash regions [ charikar , stoc 2002 ] . </S>",
    "<S> experiments later showed that using a set of _ orthogonal _ hyperplanes , thereby partitioning the space into the voronoi regions induced by a hypercube , leads to even better results [ terasawa and tanaka , wads 2007 ] . </S>",
    "<S> however , no theoretical explanation for this improvement was ever given , and it remained unclear how the resulting hypercube hash method scales in high dimensions .    in this work , we provide explicit asymptotics for the collision probabilities when using hypercubes to partition the space . </S>",
    "<S> for instance , two near - orthogonal vectors are expected to collide with probability @xmath0 in dimension @xmath1 , compared to @xmath2 when using random hyperplanes . </S>",
    "<S> vectors at angle @xmath3 collide with probability @xmath4 , compared to @xmath5 for random hyperplanes , and near - parallel vectors collide with similar asymptotic probabilities in both cases .    for @xmath6-approximate nearest neighbor searching , </S>",
    "<S> this translates to a decrease in the exponent @xmath7 of locality - sensitive hashing ( lsh ) methods of a factor up to @xmath8 compared to hyperplane lsh . for @xmath9 </S>",
    "<S> , we obtain @xmath10 for hypercube lsh , improving upon the @xmath11 for hyperplane lsh . </S>",
    "<S> we further describe how to use hypercube lsh in practice , and we consider an example application in the area of lattice algorithms . </S>"
  ]
}