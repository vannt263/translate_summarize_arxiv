{
  "article_text": [
    "in various scientific fields there is a need to reduce the initial problem to a set of linear algebraic equations .",
    "this is true in physics when solving eigenstates or eigenfunction problems in quantum mechanics . also , it is applicable in applied physics and mathematics in the areas of linear programming , optimization methods and the root mean square method",
    ". therefore this reduction of the initial problem to a set of linear algebraic equations can be viewed as a critical step in obtaining the correct solution to the initial problem .",
    "the methods used in obtaining a solution for a set of the linear algebraic equations are well known .",
    "typically , the process begins with cramer s formulas , deriving a final solution from the linear systems coefficients or similar numerical methods , then using a gaussian method to obtain further improvement and modifications or even applying the jacobi method to obtain further refinement .",
    "however , the real problem starts with the attempt to effectively solve a set of linear equations containing a large number of variables in which cramer s method is not effective , because we need to calculate determinants , which in itself is a problem when a large number of variables are involved because there will be a loss of accuracy and the numerical stability of the calculation procedure is reduced .",
    "this would also apply in the application of both the gaussian and jacobi methods as well . in obtaining practical solutions",
    "we may also have observed linear algebraic set coefficients with error bars which makes the solution obtained even more sensitive to the method used in obtaining a numerical solution .    in practice one of the main requirements desired in the numerical method selected",
    "is its stability and fast convergence in obtaining the final solution . in this paper",
    "the goal is to present a simple and rather convenient blueprint to describe the programming iteration method that could be used to solve the linear algebraic equation problem mentioned above .",
    "in this section we want reformulate the algebraic problem involving the linear system solution using a geometrical language . therefore , we need to obtain a number such as @xmath0 , which satisfies the set of the linear system : @xmath1 which can be written in a more convenient way , as : @xmath2 where @xmath3 and @xmath4 completely define the set of linear equations and have the following bounds @xmath5 . from a geometrical point of view",
    "we can consider every line in the set of linear equations , for any fixed number @xmath6 , as a hyperplane in the @xmath7 - dimensional space of the solution set .",
    "this means , that we can consider the solution as a vector , defined by @xmath8 , such that the matrix @xmath3 transforms the vector @xmath9 into the vector @xmath10 which can be defined as @xmath11 .",
    "for instance , let s take some fixed point @xmath12 , where @xmath13 is a @xmath6-th hyperplane , such that @xmath14 . then for the arbitrary point @xmath15 which also belongs to this hyperplane , we can write , that the scalar product @xmath16 , where @xmath17 , is defined as @xmath18 is @xmath19 to the @xmath20 vector . this case can be described as : @xmath21 therefore , we can see , that from a geometrical point of view the solution of a linear set of algebraic equations is some point in @xmath7 space , which is an intersection of all @xmath7 hyperplanes .",
    "one of the possibly solutions therefore , is the construction of an iterative process , which will allow convergence at this point .",
    "for the simplification of the numerical procedure , we can rewrite the set of the linear equations in the following way : @xmath22 where we delineate , that : @xmath23 here we want to underline , that within the normalization procedure , the @xmath6 index is fixed .",
    "thus , we can now rewrite the set of the linear equations using the same form , but now our coefficients are normalized which means that for any indices pair of @xmath6 , and @xmath24 we observe : @xmath25 , which leads to the conclusion that all of our vectors which are perpendicular to their hyperplanes are interrelated .",
    "for the iteration procedure we need to choose the first point , from which the iteration procedure will start . it is obvious , that the iteration procedure convergence will be dependent on this starting point .",
    "it would be preferred to derive this point as close as possible to the final solution . which of course is impossible to do in a general case .",
    "however , at a minimum we can require that within the iteration procedure itself that the chosen initial starting point must be numerically stable . in this case",
    ", we can start from a point which for example might belong to the first hyperplane and have coordinates:@xmath26 , where @xmath27 .",
    "for the start of the iteration procedure we need in that procedure a method for the projection of any arbitrary point @xmath28 ( defined by vector @xmath29 ) on the @xmath6 -th hyperplane . in this case",
    "it is easy to see , that if the projection of this point is some point @xmath30 ( defined by vector @xmath31 ) , which belongs to the @xmath6-th hyperplane , then : @xmath32 here @xmath33 is a unit vector , which is @xmath19 to the @xmath6-th hyperplane . because @xmath34 , then we can state that : @xmath35 from where we determine : @xmath36 and it follows that @xmath37 after determination of the projection parameter @xmath38 we can now describe the components of the projection point @xmath30 : @xmath39 where our iteration procedure consists of the following steps : + 1 .",
    "the first step starts from the procedure of choosing the 1-st point , which will belong to the 1-st hyperplane and we will project it on to the next or second hyperplane .",
    "the second step consists of the projection of the 2-nd point which belongs to the 2-nd hyperplane , on to the next or 3-rd hyperplane and so forth until the last @xmath7 hyperplane is reached .",
    "the third step then consists of the projection of the iteration point from the @xmath7-th hyperplane onto the first hyperplane , in other words the one which began our iteration procedure .",
    "after that we then can repeat all of the above listed procedures again until a complete convergence to the final solution is reached .",
    "proving the convergence in the numerical procedure described above can be accomplished in the following way .",
    "if we take some arbitrary initial point @xmath28 , which belongs to a hyperplane and its projection point @xmath30 on the @xmath6-th hyperplane , then it is clear , that the distance between projection point @xmath30 and our final point of solution , noted as @xmath40 , is : @xmath41 therefore for the @xmath24-th number of iterations we can to write , that : @xmath42 .",
    "then the distance between solution @xmath40 and our projection point @xmath30 after @xmath7 projections can be written as : @xmath43 for example , in the case , when all our hyperplanes are mutually orthogonal , then we have , for any number @xmath24 a value of @xmath44 and then we can automatically obtain a solution after , at least for @xmath45 iterations .",
    "however , in a case in which the difference between vectors , which happen to be @xmath19 to the hyperplanes is @xmath46 for any two given hyperplanes then in this case the number of iteration procedures becomes @xmath47 .",
    "it is clear , from a geometric point of view that in this case we have almost parallel hyperplanes and the convergence will proceed slowly .",
    "it is possible however , to roughly estimate the approximate number of iterations : @xmath48 from which we arrive at : @xmath49 here @xmath50 - is the accuracy of the solution .",
    "this example simply shows how the number of iterations depends on the choice of the first point selected for any arbitrary system , in general .",
    "however , the main result is that this iteration procedure is numerically stable , it is not dependent on the choice of the initial starting point to achieve convergence , if a solution to the set of linear systems exists .",
    "let s consider a simple numerical example .",
    "we will use our method to solve the following set of the linear equations : @xmath51 the exact solution for this set of the linear equations is : @xmath52 for the first starting point we have : @xmath53 the result of this iteration procedure is presented in table 1 .",
    "based on these results we are able to state that @xmath54 is the maximum deviation from the solution at the given number of iterations when @xmath55 .",
    "ccccccc & & & number of iterations & + & 10 & 30 & 50 & 70 & 100 & 200 + @xmath56 & 6.36729084 & 0.24225408 & 1.00692936 & 1.00648839 & 0.99979049 & 1.0 + @xmath57 & -1.70446795 & 2.26483449 & 2.011421866 & 1.99653770 & 2.00010026 & 2.0 + @xmath58 & -5.01380157 & 3.74870823 & 3.02248973 & 2.99148333 & 3.00025466 & 3.0 + @xmath59 & 13.2914602 & 4.28298482 & 3.89357617 & 4.00503095 & 3.99990957 & 4.0 + @xmath60 & 1.96186307 & 4.89304449 & 5.03535119 & 4.99844825 & 5.00002627 & 5.0 + @xmath61&5.9 & 0.3677 & 0.0377 & 0.0046 & @xmath62 & @xmath63 +",
    "in the current mathematical literature there exists many different and useful methods for obtaining a solution for a set of the linear algebraic equations .",
    "the most notable include works by cramer , gauss , jacobi , gauss - seidel .",
    "these methods provide a very useful solution to the problems described in the works @xcite .",
    "however , a major problem arises in attempts to solve problems involving a very high number of variables , for example when the number of variables is equal to @xmath64 or more . in this case",
    "we need to allow more time for convergence , especially if we use an iteration method .",
    "however , at the same time many of these existing methods become numerically unstable . for example , even a solution of obtaining the inverse matrix to @xmath3 becomes a non trivial problem .",
    "the application of our method is rather simple at the programming level and it has a transparent geometrical explanation and easy to understand pictorial representation .",
    "the efficiency of this method is much higher in solutions involving problems with large numbers of variables .",
    "the iteration procedure works rather quickly , for example in our case when the number of iterations is equal to @xmath65 it takes less then 5 sec .",
    "in other methods the number of iterations required are sometimes much less than in our method , but they may require some special properties and/or conditions .",
    "this is the case of the jacobi method .",
    "our approach is not sensitive to these problems .    a.k .",
    "express his gratitude to the bogoliubov laboratory of theoretical physics ( jinr , dubna , russia ) for the invitation and warm hospitality . a grant from osp ,",
    "cloud state university",
    "( st . cloud , minnesota , usa ) is also gratefully acknowledged .",
    "99 ilyin v.a .",
    ", pozniak e.g. ( 2004 ) . _ linear algebra _ ,",
    "fizmatlit , moscow .",
    "maksimov , y. a. ( 1982 ) .",
    "_ alhoritms of the solution of nonlinear programming problems _ , mifi publisher , moscow .",
    "volkov , e. a. ( 2003 ) . _",
    "numerical methods _ ,",
    "fizmatlit , moscow ."
  ],
  "abstract_text": [
    "<S> a simple iteration methodology for the solution of a set of a linear algebraic equations is presented . </S>",
    "<S> the explanation of this method is based on a pure geometrical interpretation and pictorial representation . </S>",
    "<S> convergence using this method is obtained and a simple numerical example is provided . </S>"
  ]
}