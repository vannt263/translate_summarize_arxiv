{
  "article_text": [
    "the learning of models from imprecise data , such as interval data or , more generally , data modeled in terms of fuzzy subsets of an underlying reference space , has gained increasing interest in recent years @xcite .",
    "indeed , while problems such as fuzzy regression analysis @xcite have already been studied for a long time , the scope is currently broadening , both in terms of the problems tackled ( e.g. , classification , clustering , ranking ) and the uncertainty formalisms used ( e.g. , probability distributions , histograms , intervals , fuzzy sets , belief functions ) .    needless to say , learning from imprecise and uncertain data also requires the extension of corresponding learning algorithms .",
    "unfortunately , this is often done without clarifying the actual meaning of an uncertain observation , although representations such as intervals or fuzzy sets can obviously be interpreted in different ways . in particular ,",
    "ontic _ interpretation of ( fuzzy ) set - valued data should be carefully distinguished from an _ epistemic _",
    "one @xcite .",
    "this difference is reflected , for example , in different approaches to fuzzy statistics , where fuzzy random variables can be formalized in an epistemic @xcite as well as an ontic way @xcite ; see @xcite for a comparison of these views in this context .",
    "surprisingly , however , the fact that these two interpretations also call for very different types of extensions of existing learning algorithms and methods for data analysis seems to be largely ignored in the literature .    under the ontic view",
    ", a variable can assume a fuzzy set as its `` true value '' ; for example , one may argue that assigning a precise value to the variable `` daily sunshine duration '' is not very meaningful , and that a specification of sunshine durations in terms of intervals or fuzzy sets is more appropriate .",
    "this interpretation suggests the learning of models that produce fuzzy sets as predictions , that is to say , models that _ reproduce _ the observed data .",
    "as opposed to this , a reproduction of the data appears less reasonable under the epistemic view , where fuzzy sets are used to describe , not the data itself , but the uncertain or imprecise _ knowledge _ about the data : a fuzzy set defines a possibility distribution that specifies a degree of plausibility for each potential precise value . as we shall explain in more detail later on , one should then rather try to `` disambiguate '' the data instead of reproducing it .",
    "the possibilistic interpretation of fuzzy sets in the epistemic case , that we focus on in this paper , naturally suggests a `` fuzzification '' of learning algorithms based on an application of the generic extension principle @xcite . as we shall argue , however , this approach is not appropriate and prone to fail in the context of data analysis .",
    "the main reason , to be detailed in section  3 , is a lack of differentiation between the possible data instantiations ( i.e. , the instantiation of each imprecise observation by a precise value ) .",
    "such a differentiation , however , is typically suggested by the model assumptions through which the learning algorithm justifies its generalization beyond the data observed .",
    "this idea of differentiating between instantiations of the data leads us to the notion of `` data disambiguation '' that we already mentioned above : _ when learning from imprecise data under the epistemic view , model identification and data disambiguation should go hand in hand_. to this end , we propose an approach based on the generalization of loss functions in empirical risk minimization .",
    "the rest of the paper is organized as follows . in the next section ,",
    "we introduce the basic setting that we consider and the main notation that we shall use throughout the paper ( see table [ tab : notation ] for a summary ) . in section  3",
    ", we explain the aforementioned problems caused by the use of the extension principle and elaborate on our idea of data disambiguation .",
    "our new approach to learning from fuzzy data based on generalized loss functions is then introduced in section  4 .",
    "section  5 is devoted to a comparison with an alternative and closely related method that was recently introduced by denoeux @xcite . in section  6",
    ", we illustrate our approach on a concrete learning problem .",
    "finally , we conclude with a summary and some additional remarks in section  7 .",
    "we consider the problem of _ model induction _ , which , roughly speaking , consists of passing from a specific data sample to a general ( though hypothetical ) model describing the data generating process or at least certain properties of this process . in this setting , a learning ( data analysis ) algorithm @xmath0 is given as input a set @xmath1 of data points @xmath2 . as output",
    ", the algorithm produces a model @xmath3 , where @xmath4 is a predefined model class .",
    "formally , the algorithm can hence be seen as a mapping @xmath5 where @xmath6 is the space of potentially observable data samples .",
    "for instance , the data points might be vectors in @xmath7 , and the model could be a partitioning of the data into a finite set of disjoint groups ( clusters ) . or , the model could be a probability density function characterizing the underlying data generating process .",
    "in fact , the data points @xmath8 are typically assumed to be independent and identically distributed ( i.i.d . ) according to an underlying ( though unknown ) probability distribution .",
    "moreover , the model class @xmath4 is often parameterized , which means that each model @xmath9 is uniquely identified by a parameter @xmath10 ( in other words , there is a bijection between the model space @xmath4 and the parameter space @xmath11 ) .    in _ supervised learning _",
    ", the data space is split into an input ( instance ) space @xmath12 and an output space @xmath13 , that is , @xmath14 .",
    "the interest , then , is to learn a mapping from @xmath12 to @xmath13 that models , in one way or the other , the dependence of outputs ( responses ) on inputs ( predictors ) ; correspondingly , the model space @xmath4 typically consists of a class of such mappings . to this end , the learning algorithm @xmath0 is given a set @xmath15 of _ training examples _ @xmath16 as input .",
    "important special cases of this setting include _ classification _ , where @xmath13 is a finite ( usually small ) set comprised of @xmath17 classes @xmath18 , and _ regression _ , where outputs are real numbers ( @xmath19 ) .    in this paper , we are interested in the case where observations are imprecise and , therefore , characterized in terms of set - valued or fuzzy set - valued data .",
    "subsequently , we therefore assume that , instead of precise data , the observations are given in the form of a sample of fuzzy data @xmath20 where @xmath21 is the set of all fuzzy subsets of the underlying data space @xmath22 .",
    "we like to emphasize that , in this setting , a fuzzy set @xmath23 is supposed to represent information about an _ observation _ , not about any kind of underlying `` true '' value or distribution ; correspondingly , the specification of @xmath23 will typically not involve any kind of statistical inference .",
    "in particular , our setting is completely coherent with the common statistical view of a data point @xmath8 as the realization of a random variable characterized by a probability distribution , for example a normal distribution @xmath24 with mean @xmath25 and standard deviation @xmath26 .",
    "then , @xmath23 would represent knowledge about the realization @xmath8 and not about its expectation @xmath25 .",
    ".summary of the main notation used throughout the paper . [ cols=\"<,<\",options=\"header \" , ]",
    "given a learning algorithm @xmath0 for precise data , the most straightforward approach to handling a fuzzy sample ( [ eq : fuzzydata ] ) is to apply the well - known extension principle @xcite to the mapping ( [ eq : amap ] ) .",
    "more formally , we define an _ instantiation _ of the fuzzy sample ( [ eq : fuzzydata ] ) as a sample @xmath27 of precise data points , where @xmath28 for all @xmath29=\\{1 , \\ldots , n \\}$ ] . the degree of membership of @xmath30 in the fuzzy set of instantiations is given by @xmath31 \\big\\ } \\enspace , \\ ] ] with @xmath32 the degree of membership of @xmath8 in @xmath23 .",
    "then , according to the extension principle , the result of applying @xmath0 to the fuzzy data ( [ eq : fuzzydata ] ) is a fuzzy set of models in @xmath4 , with the degree of membership of @xmath3 given by @xmath33 we argue , however , that the application of the extension principle is not very meaningful in the context of learning from data . to ease the explanation for our reservations ,",
    "let us consider the special case where the imprecise data is set - valued , i.e. , the @xmath23 are sets instead of fuzzy sets ; as will be seen , our arguments obviously apply ( `` level - wise '' ) to the more general fuzzy case in exactly the same way . if data is set - valued , then the extension principle simply yields a subset of models from @xmath4 , namely @xmath34 where @xmath35 is the ( crisp ) set of instantiations of @xmath36 .",
    "now , according to ( [ eq : e1 ] ) , all instantiations are treated as equal , in the sense that each instantiation contributes a possible model and all the models thus produced are seen as equally plausible candidates . while this equal treatment of all instantiations is reasonable in common applications of the extension principle , where the variables of the function to be extended do not interact with each other",
    ", it can be questioned in the context of learning from data : a method inducing a model from a set of data always comes with certain _ model assumptions _ , and under these assumptions , specific selections may appear more plausible than others ! or , stated differently , the underlying model assumptions introduce an implicit _ dependency _ between the data points @xmath28 .",
    "this dependency , however , is ignored by the extension principle , which simply selects the @xmath8 independently of each other .",
    "this point is best explained by means of a simple example .",
    "consider the problem of learning a regression function @xmath37 from observations of the form @xmath38 .",
    "more specifically , suppose that the observed outputs are imprecise and therefore modeled as intervals @xmath39 ( whereas the inputs @xmath40 are precise ) .",
    "our learning algorithm @xmath0 assumes a linear dependency ( i.e. , the model space is given by @xmath41 ) and fits the intercept @xmath42 and the slope @xmath43 of the regression line using the method of least squares .",
    "figure [ fig1 ] shows a concrete example with two different instantiations of the same set - valued data and the corresponding regression lines . in this case , the first data / model combination ( left picture ) is arguably more plausible than the second one ( right picture ) , simply because the first instantiation allows for a much better fit than the second one .",
    "in fact , the first instantiation is much more in agreement with the assumption of a linear relationship between inputs and outputs than the second one .",
    "consequently , we argue that the first regression line should be considered as more plausible than the second one , at least in light of our assumption of a linear dependency .",
    "according to ( [ eq : e1 ] ) and the extension principle ( [ eq : ext ] ) , however , there is no difference between the two models .",
    "another example is shown in figure [ fig : cluster ] , where the problem is to cluster data points @xmath44 . for three of the observations ,",
    "the @xmath45-value is not known precisely and only characterized in terms of an interval ; these observations are shown as grey rectangles in the picture .",
    "now , assuming that the data is indeed well separated into subgroups , the instantiation in the left figure ( red triangles ) is arguably more plausible than the one in the right figure ( blue circles ) .",
    "in fact , while the first instantiation allows for inducing a simple structure with two well - formed clusters , the second would imply a much less convenient structure .",
    "what these examples show is that , in the context of learning from data , not only the data is providing information about the ( unknown ) model , but also the other way around : against the background of the model assumptions underlying the model class @xmath4 and learning algorithm @xmath0 , some instantiations of the imprecise or ambiguous data appear to be more plausible than others .",
    "exploiting this insight in order to differentiate between more and less plausible instantiations is something that we refer to as _ data disambiguation _ @xcite . in other words",
    ", we consider an extension of standard model induction , in which we are not only interested in inferring properties of the data generating process , but also of the imprecisely observed data",
    ". or , stated differently , we are not only interested in learning about the model given the data , but in learning about the model and the data simultaneously .",
    ": the left instantiation appears more plausible than the right one.,title=\"fig : \" ] : the left instantiation appears more plausible than the right one.,title=\"fig : \" ]    as an aside , we note that , just like in standard statistics and machine learning , our approach takes the underlying model assumptions for granted and does not question them .",
    "thus , model induction should be seen as a kind of _ conditional _ inference , making hypothetical claims about the data generating process ( and in our case even about the observed data itself ) _ given the validity of the underlying model assumptions_. needless to say , these assumptions are not always correct and , therefore , are often adapted or corrected by a data analyst if they seem to be incoherent with the data .",
    "the corresponding search for a proper model class , however , is outside the model induction process itself .",
    "how can model induction be combined with data disambiguation ? here , we propose an approach based on the notion of ( direct ) loss minimization . roughly speaking , instead of generalizing the learning algorithm , as done by the extension principle , we `` fuzzify '' an underlying loss function to be minimized by this algorithm .",
    "thus , instead of fixing an instantiation first and fitting a model to this data afterward , we look for an optimal instantiation given a model ; the model itself is then evaluated on the basis of this instantiation .",
    "in supervised learning , the main goal is typically to find a model @xmath3 with minimal _ risk _ , that is , expected loss @xmath46 where @xmath47 is a loss function : for an input @xmath48 , this function compares the prediction @xmath49 with the true output @xmath50 and quantifies a corresponding penalty in terms of @xmath51 . roughly speaking , the risk is a weighted average of these losses , with each input / output tuple @xmath52 weighted according to its probability of occurrence .",
    "thus , a risk minimizer @xmath53 is a model that , on average , performs well in terms of the loss @xmath54 .",
    "obviously , the risk of a model @xmath55 can not be computed directly , since the probability measure @xmath56 in ( [ eq : risk ] ) , which specifies the data generating process , is unknown .",
    "what is often minimized as a substitute , therefore , is the _ empirical risk _ @xmath57 i.e. , the average loss on the training data @xmath58 . or , in order to avoid the problem of possibly _ overfitting _ the data , a",
    "_ regularized _ version of ( [ eq : erisk ] ) is minimized : @xmath59 where @xmath60 is a measure of the complexity of the model @xmath55 and @xmath61 is a regularization parameter . in the following , we shall mostly stick to ( [ eq : erisk ] ) , keeping in mind that an extension to the regularized version ( [ eq : rerisk ] ) can be realized in a rather straightforward way .",
    "again , for the ease of exposition , we consider the set - valued case first , before turning to the more general fuzzy case ; moreover , we consider imprecision only for the output part while the inputs are supposed to be precise .",
    "consider a candidate model @xmath55 and an imprecise observation @xmath62 . with @xmath49 , the set of possible losses of @xmath55 on this observation",
    "is then given by @xmath63 in agreement with the idea of data disambiguation , we should look at the smallest of these losses , namely @xmath64 and the value for which it is obtained : is closed and the minimum exists . ] @xmath65 given the model @xmath55 , this value appears to be the most plausible in @xmath66 .",
    "the function @xmath67 as defined in ( [ eq : gloss ] ) can be seen as a generalized loss function , which , instead of comparing a ( precise ) prediction with a precise observation , compares a ( precise ) prediction with an imprecise ( set - valued ) observation .",
    "on the basis of this loss function , we can also generalize the empirical risk ( [ eq : erisk ] ) : @xmath68 a minimizer @xmath69 of this risk ( or , alternatively , a regularized version thereof ) is an optimal model and , at the same time , suggests a disambiguation of the data : for each imprecise observation @xmath70 , the most plausible precise value is @xmath71 thus , the minimization of ( [ eq : gerisk ] ) serves our original purpose and solves two problems simultaneously , namely the induction of a plausible model ( [ eq : grm ] ) and a plausible disambiguation of the data ( [ eq : disamb ] ) .",
    "so far , we have assumed that only the output value is imprecise , while the input values are precisely observed .",
    "obviously , the whole approach can be generalized quite easily to the case of imprecise observations of the form @xmath72 . to this end ,",
    "the loss function ( [ eq : gloss ] ) is further generalized as follows : @xmath73      in the set - valued case , each candidate model @xmath55 is evaluated in terms of a generalized empirical risk , that is , a risk function based on a generalized loss .",
    "this evaluation can be expressed equivalently in terms of a standard empirical risk on a properly selected ( instantiated ) data sample : @xmath74 where @xmath75 is the disambiguation of @xmath76 under @xmath55 .",
    "a best model @xmath77 supposed to be unique here , is then chosen , which in turn leads to a unique disambiguation @xmath78 of the original ( imprecise ) data . in the more general case of fuzzy data , the same approach can be realized _ level - wise _ ,",
    "i.e. , for each level - cut @xmath79_{\\alpha } , [ y_i]_{\\alpha } \\right )   \\big\\}_{i=1}^n\\ ] ] of the fuzzy data @xmath80 then , for a fixed model @xmath55 , data disambiguation does not yield a unique selection ( [ eq : sel ] ) , but instead a potentially different selection for each level cut . in other words ,",
    "the selection is now a mapping @xmath81_{\\alpha } \\times [ y_i]_{\\alpha } \\ , \\big\\ } .\\ ] ] in @xcite , a mapping of that type is called a _ gradual element _ ( in a fuzzy set ) .",
    "likewise , a mapping from levels to ( empirical ) risk values can be associated with each model @xmath55 : @xmath82 \\rightarrow \\mathbb{r } , \\",
    ",   \\alpha \\mapsto    \\frac{1}{n } \\sum_{i=1}^n l \\big ( y_i^m(\\alpha ) , m \\left({\\boldsymbol{x}}_i^m(\\alpha)\\right ) \\big)\\ ] ] note that the risk function @xmath83 thus defined is non - decreasing .     in the same color and line style.,title=\"fig : \" ]   in the same color and line style.,title=\"fig : \" ]    the problem of comparing models now comes down to comparing risk functions .",
    "this problem is non - trivial , since there is no natural total order on such functions .",
    "obviously , a model @xmath55 is ( weakly ) preferred to another model @xmath84 , written @xmath85 , if @xmath86 , i.e. , @xmath87 for all @xmath88 .",
    "the relation @xmath89 thus defined is only a partial order on the model class @xmath4 , as models @xmath55 and @xmath84 may also be incomparable ( i.e. , neither @xmath85 nor @xmath90 ) .",
    "figure [ fig2 ] shows a simple ( one - dimensional ) example for the case of regression , namely three regression lines approximating four observations with fuzzy output ; all three models are incomparable amongst each other , that is , none of them dominates any other one in terms of the associated risk function .",
    "this situation can be handled in different ways .",
    "first , one may accept the non - uniqueness of the result , i.e. , the existence of several ( pareto ) optimal models ; here , a model @xmath55 is optimal ( non - dominated ) if there is no model @xmath84 such that @xmath91 , that is , @xmath90 and @xmath92 .",
    "second , one may refine the partial oder @xmath89 as defined above into a total order .",
    "for example , a model @xmath55 could be evaluated in terms of the _ aggregated _ risk @xmath93 and models could then be compared in terms of these values : @xmath94 the model induction problem then comes down to finding a minimizer of ( [ eq : arisk ] ) : @xmath95 interestingly , by exchanging summation and integration , ( [ eq : arisk ] ) can also be written as a standard ( empirical ) risk with a modified loss function : @xmath96 where @xmath97_{\\alpha } , \\hat{y } \\big ) \\ , d \\ , \\alpha\\",
    "] ] is a `` fuzzy '' loss function that compares a ( precise ) prediction with a fuzzy set - valued observation .",
    "expression ( [ eq : arisk2 ] ) holds in the case of precise input and fuzzy output data but needs to be generalized further if input data is fuzzy , too .",
    "the fuzzy loss function ( [ eq : fuzzyloss ] ) compares a fuzzy value @xmath66 with a ( predicted ) precise value @xmath98 .",
    "an example of such a loss is shown in figure [ fig3 ] for the case of regression .",
    "more specifically , this function is a fuzzy version of the absolute ( @xmath99 ) loss @xmath100 which is shown as a dashed line ( as a function of @xmath98 for fixed @xmath101 ) . the fuzzy loss ( solid line )",
    "is given by the map @xmath102 , where @xmath66 is the trapezoidal fuzzy set shown in grey .",
    ", where @xmath66 is the trapezoidal fuzzy set shown in grey , and @xmath54 is the @xmath99 loss .",
    "right : the same function in the case of an asymmetric fuzzy set.,title=\"fig : \" ] , where @xmath66 is the trapezoidal fuzzy set shown in grey , and @xmath54 is the @xmath99 loss .",
    "right : the same function in the case of an asymmetric fuzzy set.,title=\"fig : \" ]    interestingly , a fuzzification of the @xmath99 loss based on a _ triangular _ fuzzy set @xmath66 with mid - point @xmath50 and support @xmath103 leads to a kind of huber - loss @xcite : @xmath104 this loss behaves like the quadratic ( @xmath105 ) loss for small errors and like the @xmath99 loss for larger deviations .",
    "this kind of loss function is very popular in robust statistics , as it combines two interesting properties : like the absolute error @xmath99 , it is much less sensitive toward outliers than , for example , @xmath105 , but at the same time , it avoids the non - differentiability of @xmath99 .",
    "as can be seen , our approach to learning from fuzzy data based on generalized loss functions includes methods such as m - estimation with huber - loss as specific cases ; methods for huber m - estimation have been studied quite intensively in the literature @xcite .",
    "it needs to be mentioned , however , that our approach is in a sense more general , especially as it allows for modeling each fuzzy value and , therefore , the corresponding loss function _ individually _ instead of applying the same loss function to each observation ( recall , for instance , the example of an asymmetric fuzzy loss in figure [ fig3 ] ( right ) ) . in other words , our approach is _ sample - specific _ in the sense that a specific loss function can be defined for each sample point . to make this more clear",
    ", we may also write the fuzzy loss ( [ eq : fuzzyloss ] ) using a slightly different notation : @xmath106 where @xmath50 could be an observed value , and the fuzzy set @xmath66 is used to specify a region of imprecision around this observation .",
    "thus , the fuzzy loss @xmath107 is a standard loss function ( defined on pairs of precise values ) `` modulated '' by the fuzzy set @xmath66 around @xmath50 .",
    "another important loss function we can mimic is the @xmath108-insensitive loss that plays an important role in support vector regression @xcite : @xmath109 this loss is obtained as a special case of ( [ eq : fuzzyloss ] ) with @xmath66 given by the interval @xmath110 $ ] .",
    "the use of a trapezoidal fuzzy set @xmath66 with core @xmath110 $ ] and support @xmath111 $ ] nicely combines the two types of loss discussed above : @xmath107 is insensitive in the core , behaves quadratically in the boundary region @xmath112 and like @xmath99 outside the support .      in classification problems ,",
    "the output space @xmath13 is a finite set comprised of @xmath17 classes @xmath18 .",
    "the most typical loss function is the 0/1 loss @xmath113 . now , suppose the output is characterized by a fuzzy subset @xmath66 of @xmath13 , that is , by a membership degree @xmath114 for each class label @xmath115 . the fuzzy loss function ( [ eq : fuzzyloss ] ) is then given as follows : @xmath116 thus , the higher the membership degree of the predicted class @xmath98 , the smaller the loss .",
    "an interesting special case is obtained for a fuzzy set of the type @xmath117 for some @xmath118 $ ] and @xmath119 $ ] . using this fuzzy set for modeling",
    "the observation of class label @xmath120 corresponds to a _ discounting _ of this observation : although @xmath120 is regarded as completely plausible , the other class labels are not fully excluded either ; or , stated differently , @xmath121 can be seen as a degree of certainty that the observed class is indeed @xmath120 . for a fuzzy observation of that kind , @xmath122 which means that the penalty for a misclassification is effectively reduced from 1 to @xmath121 . in other words ,",
    "the training example is _ weighted _ by the factor @xmath121 .",
    "again , learning from weighted examples ( aka _ instance weighting _ ) has been studied intensively in the literature @xcite .",
    "an important class of loss functions in binary classification is the so - called _ margin losses _",
    "@xcite . instead of merely checking",
    "whether a prediction is on the right or the wrong side of the decision boundary , as the 0/1 loss does , such losses depend on _ how much _ on the right or wrong side the prediction is .",
    "by preferring `` very correct '' predictions to simply correct ones , they enforce a `` large margin '' between the classes , i.e. , they tend to separate the classes as much as possible",
    ".    more formally , let @xmath123 encode the two classes ( negative and positive ) , and suppose that @xmath4 is a class of _ scoring classifiers _ @xmath124 ; a positive score @xmath125 suggests that @xmath126 belongs to the positive class , whereas a negative score suggests that @xmath126 is negative . a margin loss is a function of the form @xmath127 where @xmath128 is non - increasing .",
    "thus , a margin loss penalizes scores instead of binary predictions , and the larger ( smaller ) the score in the case of a positive ( negative ) class , the smaller the loss .",
    "important examples of ( [ marginloss ] ) include the hinge loss @xmath129 used in support vector machines @xcite , the exponential loss @xmath130 used in boosting algorithms @xcite , and the logistic loss @xmath131 closely connected with logistic regression .",
    "now , suppose again that the output is characterized by a fuzzy subset @xmath66 of @xmath13 , that is , by a membership degrees @xmath132 and @xmath133 for the negative and positive class , respectively .",
    "more specifically , consider again the special case ( [ eq : sc ] ) : @xmath134 where @xmath135 and @xmath121 can be interpreted as a degree of confidence in @xmath50 .",
    "then , it is not difficult to show that the fuzzy loss function ( [ eq : fuzzyloss ] ) is given by @xmath136 please note that @xmath137 coincides with the original margin loss @xmath138 if @xmath139 , i.e. , if the prediction @xmath140 is in favor of the more likely class @xmath50 ; thus , the difference only concerns the negative part .",
    "figure [ fig : mlosses ] shows the graph of ( [ fmloss ] ) for different margin losses and different values of @xmath121 .    ) for different values of @xmath121 : hinge ( top ) , exponential ( middle ) and logistic ( bottom ) . ]    ) for different values of @xmath121 : hinge ( top ) , exponential ( middle ) and logistic ( bottom ) . ]    ) for different values of @xmath121 : hinge ( top ) , exponential ( middle ) and logistic ( bottom ) . ]",
    "as can be seen , the loss ( [ fmloss ] ) looses the properties of monotonicity and convexity for sufficiently small values of @xmath121 .",
    "apart from the fact that this is certainly undesirable from a computational perspective , as it makes optimization more difficult , the non - monotone behavior of the loss may also be surprising at first sight . at second sight",
    ", however , it makes perfect sense .",
    "in fact , one has to keep in mind that , in contrast to the simple 0/1 loss , a margin loss pursues two goals at the same time , namely correct classification and separation of the data . to comply with the first goal , the penalty should decrease with decreasing @xmath121 , just like in the case of the 0/1 loss ;",
    "this is why @xmath141 for @xmath142 . at the same time , however , an increase of the margin is rewarded .",
    "taking both effects together , that is , a discounted penalty for misclassification and a reward for an increased margin , it is possible that an incorrect classification with a large margin in penalized less than a correct classification with a small margin .",
    "moreover , one should note that the fuzzy margin losses are fully in agreement with our idea of data disambiguation .",
    "this can be seen most clearly for @xmath143 , which corresponds to the case where both labels , positive and negative , are considered completely plausible ( in other words , no label information is given ) . here , the loss is a symmetric function around 0 : putting an instance directly on the decision boundary , and thereby expressing maximal ambiguity , is the worst solution and penalized with the highest loss .",
    "the larger the distance from the decision boundary , regardless to what side , the smaller the loss becomes . or , stated differently , the more pronounced the prediction in favor of one of the classes , the better it is .",
    "so far , we only considered imprecision of the dependent variables and assumed the predictor variables to be precise . without going into detail , we note that the predictors can of course be affected by imprecision , too , and that the effect on the loss function is different in this case . for example , suppose that a predictor @xmath126 is represented by a ( closed ) contiguous region @xmath144 , such as a rectangle or a ball .",
    "the scores that can be produced for this instance by a model @xmath55 are then given in the form of an interval @xmath145 \\enspace , \\ ] ] which can also be written as @xmath146 $ ] with some @xmath147 and @xmath148 the middle point .",
    "applying the generic loss function ( [ eq : ggloss ] ) to this case ( with precise output @xmath50 ) , and assuming @xmath54 to be a margin loss , we obtain @xmath149 thus , the loss function @xmath54 is `` shifted to the left '' by @xmath150 units ; see figure [ fig : mloss2 ] for an illustration .    ) as a function of @xmath148 for different values of @xmath150 , with @xmath54 the logistic loss function . ]",
    "denoeux addressed a quite similar problem in his recent articles @xcite .",
    "more specifically , he addressed the problem of learning from imprecise data , represented in terms of fuzzy sets or belief functions , within a probabilistic framework and , for this purpose , proposed an extension of maximum likelihood inference . without going into technical details ,",
    "we shall try to highlight the main conceptual differences between denoeux s approach ( subsequently referred to as gmli for generalized maximum likelihood inference ) and ours , presenting ideas of the former in terms of our notation .",
    "roughly speaking , given a sample of imprecise data @xmath151 , denoeux defines the plausibility of a model @xmath152 identified by a parameter @xmath153 in terms of a normalized likelihood ; the likelihood of @xmath153 is in turn defined by the probability that the data - generating process specified by @xmath153 produces an instantiation @xmath154 : @xmath155 this probability can also be written as @xmath156 where @xmath157 measures the plausibility of instantiations .",
    "this already reveals the most important difference between denoeux s approach and ours : in the former , a model is evaluated , not by looking at how it fits the _ most favorable _ instantiation of the imprecise data , but how it fits _ all possible instantiations _ simultaneously .",
    "in fact , as can be seen from ( [ eq : avg ] ) , the score of a model is obtained by summing ( averaging ) its likelihood degrees ( on precise samples ) over all instantiations .",
    "the difference may perhaps become even more clear when looking at gmli from our loss minimization perspective .",
    "as already mentioned earlier , likelihood maximization and loss minimization are closely connected , and maximizing the log - likelihood can typically be considered as minimizing an additive loss on the training data , namely the log - loss . as an illustration ,",
    "consider the simple case of ( one - dimensional ) regression , where the observed response is supposed to follow a normal distribution .",
    "thus , given ( precise ) training data @xmath158 , the likelihood is of the form @xmath159 where @xmath160 is a normalizing constant , and the minimizer of the logarithm of that likelihood is obviously equivalent to the least squares estimator @xmath161 in the case where a response @xmath162 is imprecise , the contribution to the likelihood is a factor of the form @xmath163 , and the logarithm of this factor can be seen as the loss caused by @xmath55 on the observation @xmath164 ; thus , in gmli , the counterpart to our generalized loss function ( [ eq : fuzzyloss ] ) is given by @xmath165 where @xmath166 is a random variable defined by @xmath167 and the underlying probabilistic model . more concretely ,",
    "suppose that @xmath70 is an interval , say , @xmath168 $ ] , and recall our assumption of an underlying normal distribution ( [ eq : gaussian ] ) .",
    "then , @xmath166 is a gaussian centered at @xmath169 . as shown in figure [ fig : pic - den-1 ] , the loss caused by the prediction @xmath170 corresponds to the logarithm of the area of this distribution outside the interval @xmath168 $ ] .    :",
    "the loss is given by the negative logarithm of the area outside the observed interval @xmath171 $ ] , which essentially corresponds to the shaded area on the right side . ]    : @xmath172 $ ] , @xmath173 $ ] , @xmath174 $ ] , and @xmath171 $ ] . ]",
    "the overall loss function produced in this way ( with @xmath26 in ( [ eq : gaussian ] ) given by 1 ) is shown in figure [ fig : pic - den-2 ] , together with the loss functions for other intervals ( of different width ) for comparison .",
    "it is noteworthy that the loss in gmli is never 0 , not even when predicting the center of the interval : even in that case , the gaussian centered at that value is not completely inside the interval @xmath70 , i.e. , @xmath175 .",
    "the loss can only become 0 if either @xmath70 is very large or the gaussian is very narrow , i.e. , if the standard deviation @xmath26 in ( [ eq : gaussian ] ) is very small .",
    "this standard deviation , however , is normally estimated globally and not specifically adapted to a single observation ; and even if this could be done ( the case of heteroscedasticity ) , the standard deviation would need to be fitted to the data - generating process , not to our _ knowledge _ about the data .    anyway , for a fixed standard deviation , there is a constant and unavoidable penalty that only depends on the width of the interval ( and similarly for fuzzy sets ) : the smaller the interval , the higher the penalty .",
    "please note , however , that this shift of the function does not have any influence on loss minimization : it is simply a constant term in the empirical risk that does not change its minimizer . for better comparison with our approach , we can therefore `` normalize '' the gmli loss functions by subtracting the constant penalty .",
    "the result is shown in figure [ fig : pic - den-3 ] .",
    "as can be seen , the discounting of the loss due to an increased imprecision of the observation is quite different in gmli and our approach : the former is favoring the mid - point of the interval , while the width of the interval ( imprecision ) leads to a global scaling of the whole loss function : the smaller the interval , the steeper the loss function . in ( [ eq : lossd ] ) reduces from a proper set to a singleton , the probability goes to 0 and hence the logarithm to infinity . ] as opposed to this , our approach treats all points inside the interval as equal ; likewise , the increase of the loss outside the interval is always the same . roughly speaking",
    ", our approach leads to stretching the loss function `` horizontally '' , while gmli scales it `` vertically '' .",
    "qualitative differences of a similar kind can also be seen for other types of loss functions , for example the logistic loss ( [ logloss ] ) used in binary classification .",
    "for the special type of a discounted ( weighted ) observation ( [ eq : disc ] ) , gmli yields the loss @xmath176 figure [ fig : pic - den-4 ] shows these loss functions for different values of @xmath121 and , for comparison , reproduces the corresponding functions for our approach ( already shown in figure  [ fig : mlosses ] ) . in section  [ sec : illu ] below , these two loss functions will be compared with each other in a numerical experiment .",
    "although the cases we considered here are specific ones , they already suggest that denoeux s approach is not in agreement with our notion of _ data disambiguation_which is perhaps not surprising , given that it was never intended to implement this idea . in gmli",
    ", the _ compatibility _ of a model with an imprecise sample is based on the idea of _ data inclusion _ : when comparing a predicted data point @xmath177 with an imprecise observation @xmath23 , the loss ( log - likelihood ) depends on how well @xmath177 ( or , more specifically , the probability distribution associated with that point ) is included in @xmath23 .",
    "naturally , this leads to a preference for points `` in the middle '' of @xmath23 .",
    "as already mentioned above , the approach therefore tends to fit these middle points , while the imprecision of the information leads to a global decrease of the loss .",
    "our method , on the other hand , starts without any bias in the form of preferences on instantiations and instead tries to figure out the most likely ones .",
    "this section presents an illustration of our approach in a simple classification setting . before explaining the setup",
    ", we emphasize that our experiments are not meant as an empirical validation of our approach , let alone a comparison with alternative methods in terms of specific performance measures .",
    "since we consider the contribution of this paper as being more of a conceptual than methodological nature , and indeed proposed a conceptual framework rather than a concrete method , such a comparison is arguably not appropriate at this point .",
    "nevertheless , we would like to show the potential usefulness of our fuzzy loss functions by means of a practical example .",
    "to this end , we consider a simple binary classification problem with normally distributed classes in @xmath178 , the positive one with mean @xmath179 and the negative one with mean @xmath180 . as training data ,",
    "we assume a sample consisting of 100 randomly generated instances from both classes ; a typical example is shown in figure [ fig : example0 ] . on a sample of that kind , we train a linear classifier using logistic regression . since the true conditional class distributions are known , it is not difficult to determine the generalization performance of such a model in terms of the error rate , i.e. , the probability of an incorrect classification ( which corresponds to the risk ( [ eq : risk ] ) with @xmath54 the @xmath181 loss ) .    in a first experiment ,",
    "the class information was partly removed from the training instances .",
    "more specifically , each of the 200 instances was declared `` unlabeled '' with a fixed probability @xmath182 ( while the original label was kept with probability @xmath183 ) ; thus , we are in a setting of _ semi - supervised learning _ , in which approximately @xmath184 of the instances are labeled ( see figure [ fig : example0 ] for a typical data set of that kind ) . in our approach , the unlabeled instances can be modeled in terms of a fuzzy set that assigns a membership degree of 1 to both the positive and the negative class .",
    "then , a model is trained using the fuzzy loss function ( [ fmloss ] ) with @xmath138 the log - loss ( [ logloss ] ) .",
    "standard logistic regression , on the other hand , can not directly exploit the unlabeled instances , and therefore only used the remaining labeled ones .",
    "the results are shown in figure [ fig : example1 ] in terms of the expected classification error ( derived as an average over a large number of repetitions of this experiment ) as a function of @xmath182 .",
    "as expected , the larger @xmath182 becomes , i.e. , the less labeled and the more unlabeled examples the training data contains , the worse the generalization performance of both methods . obviously , however , the drop in performance",
    "is much more significant for standard logistic regression . from these results",
    ", we may conclude that our fuzzy loss ( [ fmloss ] ) allows for exploiting the unlabeled instances , in addition to the labeled ones , in a meaningful way .    as a side remark",
    ", we note that denoeux s gmli will produce exactly the same result as standard logistic regression : although the unlabeled data could be modeled in the same way as in our approach , it will effectively be ignored by gmli : since the probability to observe either the positive or the negative label ( the sure event ) is 1 , the unlabeled instances will not influence the likelihood function .    ) and negative ( @xmath185 ) instances in @xmath178 .",
    "( b ) example of a data set with ( @xmath186 ) missing class information , indicated in light grey .",
    "( c ) example of a data set with ( @xmath187 ) noise.,title=\"fig : \" ] ) and negative ( @xmath185 ) instances in @xmath178 .",
    "( b ) example of a data set with ( @xmath186 ) missing class information , indicated in light grey .",
    "( c ) example of a data set with ( @xmath187 ) noise.,title=\"fig : \" ] ) and negative ( @xmath185 ) instances in @xmath178 .",
    "( b ) example of a data set with ( @xmath186 ) missing class information , indicated in light grey .",
    "( c ) example of a data set with ( @xmath187 ) noise.,title=\"fig : \" ]            in a second experiment , we assume that the label of each example is switched ( from positive to negative and vice versa ) with a fixed probability @xmath182 , which can be seen as a kind of noise level .",
    "this noise level is supposed to be known , whereas for each individual training example , it is not known whether the observed label corresponds to the original one or has been switched . in our approach as well as in gmli",
    ", we can use the idea of attaching a degree of certainty to an observation : the label information is modeled in terms of a fuzzy set ( [ eq : disc ] ) , assigning a membership degree of 1 to the observed and of @xmath182 to the other label . for our approach",
    ", we again use the fuzzy loss function ( [ fmloss ] ) with @xmath138 the log - loss ( [ logloss ] ) , whereas gmli is based on the minimization of the loss ( [ eq : gmlilogloss ] ) .",
    "standard logistic regression simply uses the observed label information , which is the best it can do .",
    "figure [ fig : example2 ] shows the average classification error of the three methods as a function of the noise level @xmath182 .",
    "overall , the picture is quite similar to the first experiment : compared to our approach , the drop in performance is much more significant for standard logistic regression .",
    "this time , gmli is not exactly equivalent to standard logistic regression , but the difference in performance is negligible",
    ". apparently , our fuzzy loss function ( [ fmloss ] ) is more apt to exploit the uncertain training information than the modified loss ( [ eq : gmlilogloss ] ) underlying gmli .",
    "we have introduced a conceptual framework for ( supervised ) learning from imprecise and fuzzy data , which is based on the generalization of loss functions in empirical risk minimization .",
    "in contrast to the generic extension principle , our approach implicitly exploits the inductive bias underlying the learning method and performs model identification and data disambiguation simultaneously .",
    "our extended loss functions allow for directly `` comparing '' a ( precise ) prediction with an imprecise observation , and thereby provide the basis for fitting a precise model to imprecise data .",
    "the principle that we used for extending a standard loss function is coherent with our idea of data disambiguation and can be seen as a sample - specific `` modulation '' of the original loss .",
    "interestingly enough , our fuzzy set - based generalization of loss functions covers several existing methods as special cases , including instance weighting , robust regression ( huber loss ) and support vector regression ( @xmath108-insensitive loss ) .",
    "thus , it may have the potential to serve as a unifying framework of such methods .",
    "apart from that , however , it also allows for deriving new methods in a systematic and conceptually sound manner .",
    "for example , while the well - known huber loss and the @xmath108-insensitive loss are obtained by modulating the @xmath99 loss with a symmetric triangular fuzzy set and an interval , respectively , a trapezoidal fuzzy set leads to a new loss function that elegantly combines both effects ( insensitivity and robustness ) at the same time .",
    "needless to say , while being conceptually simple , our framework can become quite challenging from a computational perspective .",
    "in particular , solving the generalized risk minimization problems ( [ bestmodel ] ) and ( [ bestfuzzymodel ] ) is far from trivial .",
    "therefore , developing efficient algorithms for specific problem classes is an important topic of future work .",
    "such algorithms will also provide the basis for a proper empirical evaluation of our framework .",
    "dubois , d. ( 2011 ) .",
    "ontic vs.  epistemic fuzzy sets in modeling and data processing tasks . in madani ,",
    "k. , kacprzyk , j. , and filipe , j. , editors , _ proc .",
    "ijcci ( ncta ) , international conference on neural computation theory and applications _ ,",
    "paris .",
    "utkin , l. and coolen , f. ( 2011 ) . interval - valued regression and classification models in the framework of machine learning . in _ proc .",
    "isipta 2011 , 7th international symposium on imprecise probability : theories and applications _ , innsbruck , austria ."
  ],
  "abstract_text": [
    "<S> methods for analyzing or learning from `` fuzzy data '' have attracted increasing attention in recent years . in many cases , however , existing methods ( for precise , non - fuzzy data ) </S>",
    "<S> are extended to the fuzzy case in an ad - hoc manner , and without carefully considering the interpretation of a fuzzy set when being used for modeling data . </S>",
    "<S> distinguishing between an _ </S>",
    "<S> ontic _ and an _ epistemic _ interpretation of fuzzy set - valued data , and focusing on the latter , we argue that a `` fuzzification '' of learning algorithms based on an application of the generic extension principle is not appropriate . </S>",
    "<S> in fact , the extension principle fails to properly exploit the inductive bias underlying statistical and machine learning methods , although this bias , at least in principle , offers a means for `` disambiguating '' the fuzzy data . </S>",
    "<S> alternatively , we therefore propose a method which is based on the generalization of loss functions in empirical risk minimization , and which performs model identification and data disambiguation simultaneously . </S>",
    "<S> elaborating on the fuzzification of specific types of losses , we establish connections to well - known loss functions in regression and classification . </S>",
    "<S> we compare our approach with related methods and illustrate its use in logistic regression for binary classification . </S>",
    "<S> + * keywords : * imprecise data , fuzzy sets , machine learning , extension principle , inductive bias , data disambiguation , loss function , risk minimization , logistic regression . </S>"
  ]
}