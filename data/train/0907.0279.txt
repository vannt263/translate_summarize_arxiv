{
  "article_text": [
    "this work is supported by temasek laboratories at national university of singapore through the dsta project pod0613356 .",
    "hartwell l. h. , hopfield j. j. , leibler s. , and murray a. w. , nature 499 , c47-c52 ( 1999 ) .",
    "ravasz e. somera a. l. , mongru d. a. , oltvai z. n. , and barabsi a. l. , science 297 , 1551 - 1555 ( 2002 ) . i. t. jolliffe , `` principal component analysis '' ( 2nd ed . ) , springer , ny , 2002 .",
    "m. e. j. newman and m. girvan , phys .",
    "e 69 , 026113 ( 2004 ) .",
    "m. e. j. newman , eur phys .",
    "j. b 38 , 321 - 330 ( 2004 ) .",
    "m. e. j. newman , phys .",
    "e 74 , 036104 ( 2006 ) .",
    "j. duch and a. arenas , phys .",
    "e 72 , 027104 ( 2005 ) .",
    "f. wu and b. a. huberman , eur .",
    "j b 38 , 331 - 338 ( 2004 ) .",
    "j. reichardt and s. bornholdt , phys .",
    "93 , 218701 ( 2004 ) .",
    "jocob kogan , `` introduction to clustering large and high - dimensional data '' , cambridge university press , 2007 .",
    "keinosuke fukunaga , `` introduction to statistical pattern recognition '' , academic press , 1990 ."
  ],
  "abstract_text": [
    "<S> we propose a multi - phase approach to explore network structures . in this method , </S>",
    "<S> structure analysis is not carried out on the observed network directly . instead , certain similarity measures of the nodes are derived from the network firstly , which are then projected onto an appropriate lower - dimensional feature space . </S>",
    "<S> the clustering structure can be defined in the feature space , and analyzed by conventional clustering algorithms . </S>",
    "<S> the classified data are finally mapped back to the original network space if necessary to complete the analysis of network structures . by mapping onto the feature space , some difficulties due to the diversity of micro - structures and scale of the network can be circumvented . </S>",
    "<S> this makes it possible for the proposed method to deal with more general structures such as detecting groups in a random background , as well as identifying usual community structures in networks .    </S>",
    "<S> networks ( or graphs ) are natural representations for many complex systems , where the vertices ( or nodes ) stand for certain entities , and the edges ( or links ) represent the inter - connections ( dynamical or stationary ) which can be physically existing channels , or certain relationship in a more general sense </S>",
    "<S> . there are various substructures in complex networks in general . </S>",
    "<S> when the underlying system is well understood , we are usually able to figure out different substructures in light of the global picture of the whole system . sometimes , </S>",
    "<S> even very detailed structure such as a single edge can be identified and related to certain function . </S>",
    "<S> inversely , it is interesting to think about whether deeper insights of the underlying system ( such as unseen relationships ) can be inferred by investigating the strucure(s ) of a representive network . in biological networks for example </S>",
    "<S> , it is widely believed that the modular structures play a crucial role in biological functions@xcite . </S>",
    "<S> unfortunately , when inferring from the functions , the network links appear bewildering , and the intrinsic structures of the network is often obscured , not to mention their relation to the functions of the underlying system . in many situations , </S>",
    "<S> identification of communities is a highly nontrial problem .    </S>",
    "<S> currently , there is no universally accepted rigorous definition for communities in a network . </S>",
    "<S> it is usually thought of as subsets of nodes which are densely interconnected ( intra - cluster ) and sparsely connected to the rest of the network ( inter - cluster ) . </S>",
    "<S> based on this intuitive understanding , many methods are proposed to detect and identify communities in networks@xcite .    however , there are some important aspects which are largely ignored . </S>",
    "<S> firstly , most of the studies focus on networks which are exclusively covered by communities . in other words , </S>",
    "<S> each node has to be assigned to one community or another . </S>",
    "<S> this is not the case in many realistic situations . </S>",
    "<S> it is quite possible that an otherwise sparsely connected network has one or several groups of nodes densely interconnected . although the nodes in these groups can be regarded as in clusters , a conceptual difficulty would arise if the rest of the nodes had to be assigned to one or more clusters , since there is obviously not much difference between the intra- and inter - cluster connection densities for these nodes . in such a case , the whole picture is more like one where there are some substructures embedded in a certain background . detecting and identifying these small communities is certainly very useful in practice .    </S>",
    "<S> another consideration is that a structure is essentially a relative concept . </S>",
    "<S> inter connections within any subset of nodes , by themselves , say nothing about whether if these nodes can be identified as a community . </S>",
    "<S> for example , even fully connected group of nodes does not form a community if each of them connects to all outside nodes , while several sparsely connected nodes can be a legimate community if they effectively do not link to outside peers . </S>",
    "<S> one of the consequences is that prominent network structures may depend on the scale of the investigation . </S>",
    "<S> for instance , consider a network with a multi - centered structure , where all peripheral nodes are connected to several mutually connected center nodes . </S>",
    "<S> such a structure , if it exists in a large sparse network , can be considered as one community . however </S>",
    "<S> , if the investigation scale is zoomed in to focus on this structure , it is more reasonable to take the center nodes only as a community . for an extreme example , let us consider a bipartite subnetwork . </S>",
    "<S> again , in a large network , this subnetwork can be identified as a community in the usual sense . </S>",
    "<S> if the whole network has an approximately bipartite structure , one can not define a community strucutre in the usual sense , even though there are two families of nodes with clearly distinct connection patterns . </S>",
    "<S> a good analysis method should be able to adapt with the network scale automatically .    in this paper , we propose a different approach to analyze network structures , which allows us to avoid these difficulties . in this method </S>",
    "<S> , the structure analysis is not performed on the network data directly . instead , the network is first projected onto some appropriate low - dimensional feature space based on some similarity measures of the nodes . in the feature space , </S>",
    "<S> the mapped data points corresponding to nodes with similar characteristics always group together and form certain structures with different densities . </S>",
    "<S> cluster strucutures thus can be easier defined in the feature space based on various criteria as in conventional clustering analysis@xcite , and can be identified by well understood clustering algorithms such as the k - means method .    </S>",
    "<S> one of advantages to carry out clustering analysis in feature space is that the structure appearing in feature space simply depends on the relative similarity measures of the nodes . </S>",
    "<S> data points in a well defined cluster in feature space may not always correspond to a community in the original network in the sense discussed above . </S>",
    "<S> for example , the nodes which contribute the background of random connections may form a clear cluster in feature space ; and two families of nodes consisting a bipartite structure may appear as two clusters in the feature space . by this way </S>",
    "<S> , the intricacies arising from the structure heterogeneity of the network can be circumvented . in practice , </S>",
    "<S> an extra step of mapping the clusters in feature space back to the original network may be taken , depending on the problem at hand , to further investigate their implications in the context of the original network , e.g. , if two clusters in feature space actually make up a bipartite structure and need to be merged .    </S>",
    "<S> the proposed analysis method thus works at four different phases : 1 ) we need to derive similarity measures of nodes of the network under investigating ; 2 ) we extract relevant features from the similarity measures , and project them onto an appropriate lower - dimensional feature space ; 3 ) we carry out conventional clustering analysis in feature space ; and 4 ) we interpret the analysis results in the context of the original network . in the remaining parts of the paper , we will first describe a specific implementation of the algorithm , and the method is then applied to some model networks to demonstrate its advantages .    </S>",
    "<S> given an undirected network , there are many ways to measure the similarity between nodes . for the purpose of structure analysis , </S>",
    "<S> the most straightforward one is based on connection patterns of the nodes , which are completely encoded by the corresponding rows of the adjacency matrix @xmath0 associated with the network . </S>",
    "<S> let @xmath1 be the degrees of the nodes . </S>",
    "<S> the column vector @xmath2 can be regarded as the centered connection pattern of node @xmath3 , where @xmath4 is @xmath3th column of @xmath5 and @xmath6^t$ ] . </S>",
    "<S> the internal correlated structure of @xmath7 can be studied by principal component analysis ( pca ) . </S>",
    "<S> pca is mathematically defined@xcite as an orthogonal linear transformation that transforms the data to a new coordinate system such that the greatest variance by any projection of the data comes to lie on the first coordinate ( called the first principal component ) , the second greatest variance on the second coordinate , and so on . </S>",
    "<S> pca is theoretically the optimum transform for given data in least square terms . </S>",
    "<S> the first few principal components thus can be taken as effecitve features of the original data that contribute most to its variance . </S>",
    "<S> we prefer to extract these features through singular decomposition of the data matrix @xmath8 $ ] , which can be applied even when only partial information is available .    </S>",
    "<S> let @xmath9 be the singular decomposition of @xmath10 , where @xmath11 and @xmath12 are the left and right singular vectors respectively , and @xmath13 is a diagonal matrix whose elements are singular values . </S>",
    "<S> the connection patterns are then projected onto the feature space which is spanned by a few leading singular vectors @xmath14 . </S>",
    "<S> the mapped data points @xmath15 $ ] in feature space will be further analyzed . </S>",
    "<S> the dimension of the feature space used depends on the problem on hand . </S>",
    "<S> usually , a low dimensional feature space ( e.g. @xmath16 or @xmath17 ) is preferred . </S>",
    "<S> this is not only because of the lower computational load for later clustering analysis : in a low dimensional feature space , a clear picture of the distribution of mapped data often provides good suggestion of crucial parameters such as cluster number and initial partitions . </S>",
    "<S> the singular value spectrum gives useful information regarding the dimension of the feature space . </S>",
    "<S> in general , when cluster structures are clear , several leading singular values are significantly above the rest and suggest the proper number of singular vectors involved in the features set . </S>",
    "<S> however , the gap will be smeared as the cluster structure becomes vague .    </S>",
    "<S> after projecting the network onto an appropriate feature space , clustering anaysis can be carried out on those mapped data . </S>",
    "<S> for this , many well developed algorithms are ready to be used . in this study </S>",
    "<S> , we apply an improved version of the k - means algorithm@xcite , which is one of the most widely used and well understood clustering methods . </S>",
    "<S> k - means is an iterative algorithm to minimize the objective function @xmath18 , which is the sum of point - to - centroid distances , summed over all @xmath19 clusters . </S>",
    "<S> starting from the initial assignment of the data points to each cluster and determining the corresponding cluster centroid ( euclidean distance in feature space are used ) , each iteration consists of reassigning points to their nearest cluster centroid , all at once , followed by recalculation of cluster centroids . </S>",
    "<S> this procedure will be stopped if no improvement could be achieved .    to improve the overall performance , </S>",
    "<S> a refinement phase is applied . </S>",
    "<S> after a stable partition @xmath20 has been created by the above procedure , the first variation partition @xmath21 , which can be obtained by removing a single point from a cluster @xmath22 and assigning this point to an existing cluster @xmath23 , is generated . </S>",
    "<S> the cluster centroids are then recalculated . if a smaller @xmath18 or a better partition can be found in any @xmath21 , the ordinary k - means procedure in the first step is restarted again from this new partition . </S>",
    "<S> the two procedures above are repeated until no improvement can be achieved . </S>",
    "<S> the algorithm can still converge to a local optimum , even using the refinement step , which in this case is a partition of points in which moving any single point to a different cluster increases the total sum of distances . </S>",
    "<S> this problem can only be solved by a clever ( or lucky , or exhaustive ) choice of starting points . in our simulation , the same procedure is repeated @xmath24 times using random initial conditions , and the best one is picked as the final result .    to illustrate how the multi - phase algorithm works , </S>",
    "<S> we first apply it to the modular network studied in @xcite , which is a random network consists @xmath25 nodes with @xmath26 densely connected clusters ( each contains @xmath27 nodes ) . </S>",
    "<S> the connection patterns in this network can be modeled by @xmath28 parameters concisely : @xmath29 and @xmath30 , where @xmath29 stands for the connection probability of two nodes in the same cluster , and @xmath30 the connection probability of two nodes in different clusters . </S>",
    "<S> the values of @xmath29 and @xmath30 are chosen to make the expected degree of each node equals to @xmath31 . in this model , </S>",
    "<S> every node in each community has the same connection pattern statistically , and the communities cover the whole network .    </S>",
    "<S> the overall performance of clustering on this network is shown in figure [ fig : fig1 ] , where the fraction of nodes classified correctly is shown as a function of the mean number of inter - cluster links @xmath32 . </S>",
    "<S> the results are the average of @xmath33 different realization of random networks based on the same model . </S>",
    "<S> it can be seen clearly in the figure that the clustering results are almost perfect when @xmath32 is relative small until @xmath32 approachs @xmath34 . </S>",
    "<S> the errors of misclassification increase quickly after @xmath35 . </S>",
    "<S> however , in contrast to the results shown in @xcite , in this study , even when @xmath32 is around @xmath36 , the error is still significantly smaller . </S>",
    "<S> in fact , at this point , though the intra - cluster links @xmath37 is the same as the inter - cluster links @xmath32 , @xmath29 is still significantly larger than @xmath30 , since there are much more outside nodes than inside nodes ( @xmath38 times ) .    to better reveal how the method works , more details </S>",
    "<S> are shown in figure [ fig : fig2 ] . </S>",
    "<S> the clustering structures can be seen clearly from the projections of the orignal connection patterns in @xmath17 feature space ( as shown in figure 2(a ) ) when @xmath39 . in this case , first @xmath38 leading singular values gap up significantly ( as shown in figure 2(b ) ) , suggesting the appropriate feature space dimension . </S>",
    "<S> however , this kind of information become less useful when modular structures become vague ( see figure 2(c ) and figure 2(d ) in the case when @xmath40 )    when applying the method to some real example such as the karate club network and the dolphins social network , we first determine the dimension of the feature space by observing the singular spectrum . the cluster number </S>",
    "<S> then have to be guessed based on the distribution of the data points in the feature sapce . </S>",
    "<S> the clustering results are similar to what reported in the literatures@xcite .    </S>",
    "<S> now let us consider more interesting examples to address the points we discussed earlier . </S>",
    "<S> firstly , consider a random network with @xmath41 nodes where a relatively densely connnected group of nodes are embedded . </S>",
    "<S> the purpose of the analysis is to identify this group . to construct a network </S>",
    "<S> having a desired structure , we need a random netwrok with a preassigned degree distribution , which is generated by the following procedure .    </S>",
    "<S> * use a configuration model to generate a random network with the required degree distribution@xcite . in this network , </S>",
    "<S> multiple and self - connections are allowed , and will be removed in the next steps . * to remove each self - connections of node @xmath42 , we first find two connected nodes @xmath43 and @xmath3 which are not connected to node @xmath42 . </S>",
    "<S> a pair of new edges @xmath44(and @xmath45 ) and @xmath46(and @xmath47 ) are created , while one original self - connection of node @xmath42 and the edge @xmath48(and @xmath49 ) are removed . * to remove one of the multiple connections between node @xmath43 and node @xmath3 , we first search for a pair of connnected nodes @xmath50 and @xmath51 . </S>",
    "<S> each of them does not connected to nodes @xmath43 and @xmath3 simultaneously . </S>",
    "<S> one of the multiple connections @xmath48 ( and @xmath49 ) and the edges @xmath52 ( and @xmath53 ) are replaced by the edges @xmath54 ( and @xmath55 ) and @xmath56 ( and @xmath57 ) . * </S>",
    "<S> the rewiring procedure is repeated until there is no multiple and self - connection . </S>",
    "<S> if no legitimate nodes can be found to be rewired to , a random network candidate is regenerated using the configuration model .    </S>",
    "<S> the smaller hidden group is modeled by a random network of @xmath58 nodes with average degree @xmath59 . </S>",
    "<S> these two networks are then superimposed randomly and repeated edges are removed .    by applying the proposed multi - phase clustering analysis </S>",
    "<S> , we get two sets of nodes finally . </S>",
    "<S> the smaller set @xmath10 is taken to be an estimate of the hidden group , and the other set @xmath60 corresponds to the background nodes . </S>",
    "<S> both @xmath10 and @xmath60 may contain nodes coming from the group and the background in general . </S>",
    "<S> a complete measurement of clustering performance thus requires a @xmath61 confusion matrix@xcite . </S>",
    "<S> since we mainly focus on the identification of the hidden group , only two quantities corresponding to two terms in the confusion matrix are used to measure the performance . </S>",
    "<S> suppose the sizes of @xmath10 and @xmath60 are @xmath62 and @xmath63 ; we have @xmath64 . </S>",
    "<S> let @xmath65 , where @xmath66 and @xmath67 are the numbers of nodes in @xmath10 coming from the hidden group and background respectively . </S>",
    "<S> then the quantity @xmath68 measures the fraction of nodes in the hidden group which are correctly assigned to @xmath10 . to further describe the quality of @xmath10 , the quantity @xmath69 is used to measure the fraction of misclassified nodes in @xmath10 . </S>",
    "<S> @xmath70 and @xmath71 together give the overall performance of a particular result . </S>",
    "<S> for a perfect partition , we have @xmath72 and @xmath73 </S>",
    "<S> . a good clustering result shall show large @xmath70 and small @xmath71 simultaneously . in practice , </S>",
    "<S> usually one is treated as a more important measure than the other depending on the nature of the problems analyzed .    in figure </S>",
    "<S> [ fig : fig3 ] , we show the averaged results of clustering analysis on @xmath74 different realizations of random networks described above . </S>",
    "<S> the performance depends on the connection density in the hidden group . </S>",
    "<S> the results are acceptable even when the average number of edges in the group is similar to that of the background . </S>",
    "<S> for instance , in one particular test , the degree of the background network is uniformly distributed in the range of @xmath75 $ ] , and the average degree of the whole network ( @xmath76 ) is @xmath77 . </S>",
    "<S> we then construct a small network ( @xmath78 ) with average degree @xmath59=8 . </S>",
    "<S> after superimposing randomly these two networks , the average number of inside edges for each node in the group is slightly less than @xmath79 , while the average number of outside edges for each node is about @xmath80 . in this case , @xmath70 is above @xmath81 and @xmath71 is around @xmath82 . </S>",
    "<S> the value of @xmath71 is a bit larger than expected due to the fact that by chance there are some nodes in the background which show very similar connection patterns as the node in the hidden group and can not be classified correctly by the algorithm .    </S>",
    "<S> typical distributions of mapped data points in @xmath16 feature space are shown in figure [ fig : fig4 ] when the connection density within the hidden group changes . </S>",
    "<S> it is intresting to observe how the points corresponding to nodes in the background group together . </S>",
    "<S> this demonstrates the advantage of making clustering analysis in feature space .    </S>",
    "<S> we also study a more complicated situation , where on the backgroud of a random network ( @xmath83 ) , there are two clusters with different micro - structures . </S>",
    "<S> one of them is a uniformly densely connected cluster ( @xmath84 ) and the other is an approximately bipartite cluster ( @xmath85 ) . </S>",
    "<S> the network is constructed by superimposing randomly a densely connected subnetwork and a perfect bipartite subnetwork on the background of a random network ( in a similar way as in the case of figure 3 and 4 ) . in a perfect bipartite network , </S>",
    "<S> all nodes can be divided into two families . </S>",
    "<S> any node in one family can only be connected to the nodes in the other family . in our example , two nodes in the same family may be connected due to the existing connection in background network . in figure </S>",
    "<S> [ fig : fig5 ] , the projections of the original connection patterns in a @xmath17 feature space are shown for two particular cases . @xmath26 clusters can be identified satisfactorily by the method , where one corresponding to the dense uniform cluster , two of them corresponding to two different families of the bipartite cluster , and the last one for background nodes ( as indicated in the figure ) . as shown in the case of figure 5(b ) , more accurate results can be obtained when there are more connections in the bipartite cluster . </S>",
    "<S> the clustering performances of both cases are described by confusion matrices as shown in table i.    .confusion matrix for clustering analysis in cases of figure 5(a ) and ( b ) [ cols=\"^,^,^,^,^ \" , ]     the manifested prominent structures of a network depends on the investigation scale . </S>",
    "<S> a micro - strucrture of a cluster can be the dominant one at the appropriate scale . </S>",
    "<S> the proposed multi - phase method can adapt automatically according to the different scales . </S>",
    "<S> an illustration is given in figure [ fig : fig6 ] . </S>",
    "<S> a densely connected cluster of size @xmath86 and average degree @xmath87 is embedded in a random network of size @xmath88 and average degree @xmath89 . </S>",
    "<S> the distribution of the projected connection patterns in feature space is shown in figure 6(a ) , which clearly reveal the structure of the network . however </S>",
    "<S> , when a larger network is studied , which consists of above subnetwork and other coexisting strong clusters , the structure in the subnetwork would become less important micro - structure in a cluster , and not the dominant structure of the whole network . </S>",
    "<S> consider a larger network of size @xmath90 , which consists of the above subnetwork of size @xmath91 and other @xmath33 nodes which form a densely connected cluster with average degree about @xmath92 . </S>",
    "<S> the two groups are then sparsely connected ( on average , one edge per node is added to connect to other group ) . </S>",
    "<S> obviousely , the dominant structure is that of two clusters with similar sizes in the whole network . </S>",
    "<S> this is correctly reflected in figure 6(b ) . </S>",
    "<S> the micro - structure in the first cluster has been supressed in feature space by the coexisting more dominant structure in the larger scale . </S>",
    "<S> this characteristic makes the proposed method valuable when applying to network with hierarchical structures .    in previous sections , </S>",
    "<S> we describe a specific implementation of the multi - phase analysis method . </S>",
    "<S> however , the essential merits of the proposed method do not rely much on specific similarity measurement and techniques ( such as pca ) used . </S>",
    "<S> if more information on network can be incorporated , better tools exist for analysis in each phase depending on the problem at hand . </S>",
    "<S> it is the analysis strategy , i.e. , working on certain feature space instead of the original network , that makes the proposed method a more general way to analyze network structures .    in summary </S>",
    "<S> , we propose here a novel multi - phase approach to analyze the network structures . by focusing on the clustering structure in feature space , </S>",
    "<S> we are able to circumvent several difficulties caused by the diversity of micro - structures and different scales . </S>",
    "<S> the method has been tested on several model networks which have not been extensively discussed up to now . </S>",
    "<S> the demonstrated advantages show that it can be applied to networks with more general structures . </S>",
    "<S> we believe that there are many situations in practice where the proposed method may be applied effectively . </S>"
  ]
}