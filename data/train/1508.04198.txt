{
  "article_text": [
    "dictionary learning has been proven very effective to find sparse representation for high dimensional data , widely used in many machine learning applications , such as classification @xcite , recognition @xcite and image restoration @xcite . under this model ,",
    "each data point can be recovered/ or represented by using a linear combination of a small number of atoms under a dictionary .",
    "the underlying linear process requires that both data points and the atoms are from a linear space embedded in an euclidean space and the reconstruction error is measured using @xmath0-norm . as learning a dictionary is quite time consuming , the data point themselves are generally used as a dictionary based on the so - called self - representative principle @xcite .",
    "given a collection of data points @xmath1 consisting of @xmath2 @xmath3-dimensional column vectors @xmath4 ( @xmath5 ) , the self - representative coding seeks a joint sparse representation of @xmath6 using data points themselves as the dictionary , which can be formulated as , @xmath7 where @xmath8 is the sparse representation matrix under the dictionary @xmath6 , each column @xmath9 ( @xmath10 ) is the corresponding representation of @xmath4 , @xmath11 is the sparsity regularizer term for the new representation @xmath12 and @xmath13 is a penalty parameter to balance the sparsity regularizer term and the reconstruction error . in general , the @xmath14 norm @xmath15 ( i.e. the sum of the absolute values of all the element in a matrix ) is used in favor of independent sparse representation for each data point , while the nuclear norm @xmath16",
    "( i.e. the sum of all the singular values of a matrix ) is employed to holistically reveal the latent sparse property embedded in the whole data set .",
    "however , in the context of machine learning and vision applications , feature data acquired actually satisfy extra constraints which make them so - called manifold - valued .",
    "take diffusion weighted magnetic resonance imaging ( mri ) @xcite as an example , this non - invasive imaging technique helps explore the complex micro - structure of fibrous tissues through sensing the brownian motion of water molecules .",
    "water diffusion is fully characterized by the diffusion probability density function called the diffusion propagator ( dp ) @xcite , which can be represented in the form of the square root density functions .",
    "more specifically , for a probability density function @xmath17 and its continuous square root @xmath18 , we have @xmath19    @xmath20 in eq .",
    "can be identified as a point on the unit sphere in a hilbert space @xcite by expanding it using orthogonal basis functions .",
    "the geodesic distance between two points along the unit sphere manifold is longer than the corresponding hilbert distance .",
    "accordingly , when generalizing traditional dictionary learning or sparse coding algorithms to a sphere manifold , one of the key challenges to be resolved is to exploit the manifold geometry .",
    "the reason is that in hilbert or euclidean space ( in the cases of finite dimension ) , the global linear structure can make sure the data synthesized from the atoms is contained in the same space , whereas on the sphere manifold , the manifold geometry provides only local linear structures using the log - euclidean metrics @xcite .    by taking advantage of this local linear property , a novel nonlinear dictionary learning framework @xcite is proposed for data lying on the manifold of square root densities and applied to the reconstruction of dp fields given a multi - shell diffusion mri data set @xcite . under this paradigm ,",
    "the loss of global linear structure is compensated by the local linear structure given by the tangent space using riemannian exponential and logarithm maps . in particular , the @xmath21-norm regularization is employed to produce sparse solutions with many zeros . however , this sparsity scheme does not consider the global structure of data , which is very important in some computer vision tasks , especially in classification and clustering applications .",
    "moreover , it is not robust to noise in data .",
    "in contrast , low rank representation @xcite uses holistic constraints as its sparse representation condition , which can reveal the latent sparse structure property embedded in a data set in high dimensional space.the lowest - rank criterion can enforce to correct corruptions and could be robust to noise .",
    "low rank representations on non - euclidean geometry have received comparatively little attention .",
    "recently , a low rank representation on grassmann manifolds has been proposed in @xcite by mapping the grassmann manifold onto the euclidean space of symmetric matrices .",
    "the authors of @xcite introduced a low rank representation for symmetric positive definite matrices measured by the extrinsic distance defined by the metric on tangent spaces and the nuclear norm simultaneously .",
    "some researchers exploited the local manifold structure of the data by adding a manifold regularization term characterized by a laplacian graph @xcite or manifold matrix factorization @xcite .",
    "however , the loss functions are still formulated in the euclidean space .",
    "others generalized riemannian geometry to the matrix factorization with fixed - rank cases @xcite .    to our best knowledge ,",
    "none of the existing work is specialized for the low rank representation on the manifold of square root densities measured by the riemmanian distance and nuclear norm simultaneously , which motivates our study .",
    "the contribution of this work is threefold :    * we propose a novel lrr model on the manifold of square root densities .",
    "the approximation quality is measured by the extrinsic distance defined by the metric on tangent spaces and the intrinsic geodesic distance on the manifold simultaneously . *",
    "we describe a simple and effective approach for optimizing our objective function as the objective function is a composition of a quadratic term , a linear term and the nuclear norm term .",
    "* we present results on a few computer vision tasks on several image sets to demonstrate our new lrr model s superior performance in classification , clustering and noise robustness over other state - of - the - art methods .",
    "a manifold @xmath22 of dimension @xmath23 @xcite is a topological space that locally resembles a euclidean space @xmath24 in a neighbourhood of each point @xmath25 . for example ,",
    "lines and circles are @xmath26 manifolds , and surfaces , such as a plane , a sphere , and a torus , are @xmath27 manifolds .",
    "geometrically , a tangent vector is a vector that is tangent to a manifold at a given point @xmath28 .",
    "abstractly , a tangent vector is a function , defined on the set of all the smooth functions over the manifold @xmath22 , which satisfies the leibniz differential rule .",
    "all the possible tangent vectors at @xmath29 constitute a euclidean space , named the _ tangent space _ of @xmath22 at @xmath29 and denoted by @xmath30 .",
    "if we have a smoothly defined metric ( inner - product ) across all the tangent spaces @xmath31 on every point @xmath25 , then we call @xmath22 _ riemannian manifold_. with a globally defined differential structure , manifold @xmath22 becomes a differentiable manifold .",
    "the @xmath23-dimensional _ sphere _ denoted by @xmath32 is a specific riemannian manifold , which has unit radius and is centered at the origin of the @xmath33 dimensional euclidean space .",
    "\\mathcal{m}$ ] is a smooth curve with a vanishing covariant derivative of its tangent vector field , and in particular , the riemmannian distance between two points @xmath35 is the shortest smooth path connecting them on the manifold , that is the infimum of the lengths of all geodesics joining @xmath36 and @xmath37 .",
    "there are predominantly two operations for computations on the riemannian manifold , namely ( 1 ) the exponential map at point @xmath28 , denoted by @xmath38 , and ( 2 ) the logarithmic map , at point @xmath28 , @xmath39 .",
    "the former projects a tangent vector in the tangent space onto the manifold , the latter does the reverse . locally both mappings are diffeomorphic .",
    "note that these maps depend on the manifold point @xmath28 at which the tangent spaces are computed .",
    "given two points @xmath35 that are close to each other , the distance between them can be calculated through the following formula as the norm in tangent space . @xmath40",
    "the squared distance function @xmath41 is a smooth function for all @xmath25 .",
    "the euclidean lrr model uses the frobenius norm based on metric to model the reconstruction error , as shown in eq .",
    "( if @xmath421 ) . however , in many real world applications , high dimension data have a riemannian manifold structure , such as face recognition @xcite and object detection @xcite . in ideal scenarios",
    ", error should be measured according to the manifold s geometry .",
    "inspired by the lrr formulation for data on the riemannian manifold of symmetric positive definite matrices in @xcite , the lrr model in eq .",
    "can be changed to the following manifold form : @xmath43 where @xmath44 is a penalty parameter , @xmath45 is a distance threshold ( default:@xmath461 ) , the @xmath47-th column of matrix @xmath12 is @xmath48 , denoting the low rank representation for @xmath36 , and @xmath49 denotes the geodesic distance between the two points on the riemannian manifold .",
    "there is an intuitive explanation for the formulation in eq . .",
    "for each point @xmath36 on the manifold , @xmath36 can be projected onto 0 tangent vector of tangent space at @xmath36 , other points @xmath37 are projected as @xmath50 .",
    "the norm in the first term of means the distance between 0 tangent vector and the linearly combined tangent vector from all the other projected tangent vectors .",
    "the weight term @xmath51 for sparse representation is used to avoid assigning large weights to the far - away points in the manifold s embedding space ( see further explanation in figure.[distancecom](a ) ) .",
    "therefore , minimization aims at finding an appropriate linear combination for the `` best '' approximation in terms of tangent vectors , and meanwhile , effectively avoiding sparse representations via far - away points that are unrelated to the local manifold structure .",
    "the difference between the euclidean lrr and our proposed method is illustrated in figure [ distancecom ] . given a toy mr brain image dataset of 6 people from young and old groups respectively , the goal is to classify these sample images according to their age group . to start with",
    ", we extract a histogram vector for each image , the square root of which lies on a finite dimensional sphere manifold .",
    "traditional lrr simply regards the histograms as conventional feature vectors , and measures data distance using euclidean geometry as shown in figure  [ distancecom](b ) .",
    "in contrast , our proposed method uses the square root of the histogram on the riemannian manifold as input features , and the distance between input feature vectors on the manifold is approximated by the euclidean distance between their corresponding mapped points on the tangent space , as shown in figure  [ distancecom](a ) . in figure",
    "[ distancecom ] , we note that two points on the manifold having a large geodesic distance separating them may be near each other in the euclidean space , which demonstrates that the important intrinsic properties of the data manifold can not be captured using the euclidean geometry .",
    "however , using riemannian exponential and logarithm maps alone may fail to detect the local structure at point @xmath28 in the manifold setting , because two far - away points may have a short distance on the tangent space .",
    "accordingly , we use a term @xmath51 to adjust weight values , thereby increasing the effect of nearby points in addition to their sparsity .        when the underlying riemannian manifold is the square root densities in the @xmath23-dimensional space @xmath32 , the geodesic distance @xmath52 between two points @xmath53 is simply the great circle distance between two points , which is defined formally as @xmath54 where @xmath55\\rightarrow [ 0,\\pi]$ ] is the usual inverse cosine function .",
    "the tangent space at @xmath56 is @xmath57    then the geodesic started at @xmath28 in the direction @xmath58 is given by the formula @xmath59 and the exponential and logarithm maps are given by @xmath60 @xmath61 where @xmath62 .",
    "hence the lrr on the manifold of unit sphere is defined by @xmath63 where @xmath64 .",
    "the metric here is the inner product inherited from the standard inner product on @xmath24 given by @xmath65 accordingly , the lrr problem can be re - written as @xmath66 where @xmath67 and @xmath68 .",
    "problem can be reformulated into the following problem @xmath69 where @xmath70\\ ] ] is a @xmath71 matrix , and the @xmath72th element of @xmath73 is + @xmath74 .",
    "in this section , we consider an algorithm to solve the constrained optimization problem in eq . .",
    "we propose to use the augmented lagrange multiplier ( alm ) method @xcite to solve it .",
    "the reason we choose the alm to solve this optimization problem is threefold : ( 1 ) superior convergence property of alm makes it very attractive ; ( 2 ) parameter tuning is much easier than the iterative thresholding algorithm @xcite ; and ( 3 ) it converges to an exact optimal solution .",
    "first of all , the augmented lagrange problem of ( [ spherelrr3 ] ) can be written as @xmath75 where @xmath76 are lagrangian multipliers , and @xmath77 is a weight to tune the error term of @xmath78 .",
    "in fact , the above problem can be solved by updating one variable at a time with all the other variables fixed .",
    "more specifically , the iterations of alm go as follows :    \\1 ) fix all others to update @xmath12 : we define a function @xmath79 by @xmath80 and it is easy to prove that @xmath81 where @xmath82 is a column vector of all ones .",
    "now at the current location @xmath83 , we take a linearization of @xmath79 , @xmath84 where @xmath85 is a gradient matrix whose @xmath47-th row is given by @xmath86 taking eq . into eq . , we have @xmath87 the above problem admits a closed form solution by using svd thresholding operator to @xmath88 .",
    "taking svd for @xmath89 , then the new iteration is given by @xmath90 where @xmath91 is the soft thresholding operator for a diagonal matrix , see @xcite .",
    "\\2 ) fix all others to update @xmath76 by @xmath92 after a number of iterations by alternately updating @xmath12 and @xmath76 respectively , we achieved a complete solution to lrr on the manifold of square root densities .",
    "the whole procedure of lrr on the manifold of square root densities is summarized in algorithm [ agalm ] .",
    "@xmath93 , @xmath94 @xmath95 , @xmath461 , the initial @xmath12 . : the low rank representation @xmath12 initialize : @xmath96 ( @xmath97 .",
    "@xmath98 for @xmath99 ; @xmath100 $ ] for @xmath101 to @xmath2 ; @xmath102 ; @xmath12 is computed by solving the problem using svd thresholding operator ; @xmath76 is updated by .",
    "the convergence of alm algorithm has been guaranteed by theorem 3 in @xcite . to ensure the convergence of the algorithm [ agalm ]",
    ", we only need prove that problem ( [ spherelrr3 ] ) is a convex optimization problem .",
    "* theorem 1 * each @xmath73 is a semi - positive definite matrix , thus problem ( [ spherelrr3 ] ) is a convex optimization .",
    "* proof*. the @xmath104-th element @xmath105 of @xmath73 can be represented by @xmath106 where @xmath107 .    if we define a new matrix @xmath108 by @xmath109\\ ] ] then we can see @xmath110 , which means @xmath73 has a decomposition as the product of a matrix with its transpose , hence @xmath73 is semi - positive definite .",
    "for ease of analysis , we assume that @xmath36 is a @xmath23-dimensional square root densities sample , and the iteration number of alm algorithm is @xmath111 .",
    "the complexity of algorithm [ agalm ] can be decomposed into two parts : the data preparation part ( steps 1 - 4 ) and the alm solution part ( steps 5 - 9 ) .    in the data preparation part , @xmath112 and @xmath113 for @xmath114",
    "can be computed with a complexity of @xmath115 .",
    "the bottleneck of the data preparation procedure is the computation of all @xmath73 .",
    "as we could store the inner product values of @xmath36 and @xmath37 computed in step 2 , the time complexity of computing @xmath73 should be @xmath116 . as @xmath117 , to break this complexity bound , we use a parallel matrix multiplication scheme @xcite to reduce the complexity to @xmath118 .",
    "therefore , the complexity of data preparation is @xmath119 .    in the alm",
    ", the major computation cost is for svd of an @xmath71 matrix in step 6 with a complexity of @xmath116 .",
    "fortunately , we can utilize the accelerated method in @xcite to reduce the complexity of partial svd computation to @xmath120 , where @xmath121 is predicted rank of @xmath122 . for @xmath111 iterations ,",
    "the complexity of alm solution is @xmath123 .    with the above analysis",
    ", the overall complexity of algorithm [ agalm ] is given as @xmath124 as @xmath125 , the complexity of algorithm [ agalm ] mainly depends on the size of data set @xmath2 .",
    "it can be approximated as @xmath126 , which is similar to the complexity of the original lrr algorithm @xmath123 .",
    "to evaluate the proposed lrr model on the manifold of square root densities , we apply it to both clean and corrupted image datasets for image classification and image segmentation .    to apply our method for image classification",
    ", we firstly employ our proposed lrr model to obtain the low rank features , and the classification accuracy is computed by training a svm classifier on the low rank features . in order to compare performances with the state - of - the - art methods , we use the following four baseline methods :    * svm on vectorized data",
    ": we directly vectorize the manifold data to form their euclidean features and train an svm using these euclidean features . *",
    "lrr + svm : we firstly apply the euclidean lrr model in @xcite on the euclidean features ( vectorized the manifold data ) to obtain the low rank features , and then train an svm classifier on the low rank features . * gknn : it is a k - nearest neighbour classifier that uses the geodesic distance on the manifold for determining neighbours , and solves the classification problem on the manifold directly without lrr transforms . *",
    "sc+svm : it is a variant of our proposed method within the same framework . the only difference between sc + svm and our proposed model is that @xmath21 regularization on @xmath12 is used in the objective function eq ..    all svms used in the experiments are trained using the libsvm package @xcite .    to apply our method for subspace clustering ,",
    "we compute the low rank features using the proposed method , and then use these low rank features as the input of the ncut clustering algorithm .",
    "the baselines used in the experiments are listed as follows :    * ncut @xcite : we simply conduct the ncut clustering algorithm over the vectorized manifold data .",
    "* lrr+ncut : we use the euclidean lrr method on the euclidean features to obtain the low rank features , and then apply ncut to the low rank features . *",
    "geodesic distance based ncut ( gncut ) : it is a variant of the traditional ncut method , based on the riemannian geodesic distance introduced in this paper as the node similarity measure .",
    "* sc+ncut : it is similar to our proposed method , except @xmath21 regularization on @xmath12 used in the objective function eq ..      in this experiment , we evaluate the performance of our proposed method in terms of image classification using oasis @xcite and lumar @xcite datasets . sample images from both data sets are shown in figure [ figclassifydata ] .",
    "the oasis dataset consists of t1-weighted mr brain images from a cross - sectional collection of 416 subjects aged 18 to 96 .",
    "the subjects are all right - handed and include both men and women .",
    "each mri scan has a resolution of 176@xmath127208 pixels .",
    "the whole oasis images are categorized into three groups : young subjects ( younger than 40 ) , middle - aged subjects ( between 40 and 60 ) and old subjects ( older than 60 ) .",
    "we aim to classify each mri image into its corresponding group .",
    "we note that the subtle differences in anatomical structure across different age groups in figure .",
    "[ figclassifydata ] are apparent .",
    "the lumbar dataset collects mri scans of human lumbar spine from 60 subjects , who are randomly sampled from normal people and patients with lumbar degenerative disease groups respectively .",
    "each image has a resolution of 643@xmath127574 pixels .",
    "the classification problem is to recognize the abnormal lumbar spines from the normal ones .",
    "two sample images from each group in the lumbar dataset are shown in figure .",
    "[ figclassifydata ] .",
    "to generate the square root densities data , we use some pre - processing techniques on the above two mri datasets .",
    "first of all , we obtain a displacement field for each mri image in the dataset using a nonrigid group - wise registration method described in @xcite .",
    "then we compute the histogram of the displacement vector for each image as the feature for classification . in our experiment ,",
    "the number of bins in each direction is set to 4 , and the resulting 16-dimensional histogram is generated as the feature vector for the svm and lrr+svm methods .",
    "the square root of the histogram is used in the gknn , sc+svm and our new method .",
    "the classification results on the above two datasets are reported in table [ tb - ic ] .",
    "it shows that lrr+svm outperforms svm , which indicates that low rank features are more discriminative for classification than the original high - dimensional features .",
    "as gknn considers the intrinsic property of the data manifold , its performance is superior to the counterparts using extrinsic euclidean metric ( svm and lrr+svm ) , which confirms the importance of intrinsic geometry . as we expected , our method and sc+svm utilizing both intrinsic geometry and sparse feature transform ( i.e. sparse coding and low rank representation ) outperform all the other baselines .",
    "the performance of our proposed method also outperforms its variant method sc+svm under the same framework , which demonstrates that the lowest - rank criterion is accurate for modeling the embedding manifold structures of high dimensional data .",
    ".classification accuracy comparisons on clean data sets . for the oasis dataset",
    ", there are three binary classifications ( ym : young vs middle - aged , mo : middle - aged vs old and yo : young vs old ) , and one three - class classification ( ymo : young , middle - aged and old ) . for the lumbar dataset ,",
    "there is one binary classification ( na : normal vs abnormal ) . [ cols=\"^,^,^,^,^,^ \" , ]     [ tb - ic ]      in this section , our proposed method is used to solve the subspace clustering problem .",
    "we evaluate our model on 4 hyperspectral images , including pavia centre scene @xcite , salinas - a scene @xcite , samson @xcite and jasper ridge @xcite .",
    "the pavia centre scene was acquired by the rosis sensor during a flight campaign over pavia , nothern italy .",
    "it is a 1096@xmath1271096 pixels image , with 102 spectral bands .",
    "the geometric resolution is 1.3 meters , and the image groundtruths differentiate 9 classes .",
    "the salinas - a scene is a small subscene of salinas image , which was collected by the 224-band aviris sensor with a spatial resolution of 3.7-meter pixels .",
    "the salinas - a scene consists of 86@xmath12783 pixels located within the salinas scene at [ rows , columns ] = [ 591 - 676 , 158 - 240 ] and includes six classes .",
    "the samson consists of 952@xmath127952 pixels , each of which has 156 channels covering the wavelengths from 401 nm to 889 nm .",
    "the spectral resolution is highly up to 3.13 nm . to reduce the computational cost , a sub region of 95@xmath12795 pixels",
    "is used , starting from the ( 252,332)-th pixel in the original image .",
    "there are three targets in this image , i.e. , soil , tree and water respectively .",
    "the jasper ridge image has 512 x 614 pixels , with 224 channels ranging from 380 nm to 2500 nm .",
    "the spectral resolution is up to 9.46 nm .",
    "since this hyperspectral image is too complex to get the ground truth , we consider a subimage of 100 @xmath127 100 pixels , starting from the ( 105,269)-th pixel in the original image . in our experiments , the channels 1 - 3 , 108 - 112 ,",
    "154 - 166 and 220 - 224 are removed due to dense water vapor and atmospheric effect .",
    "there are four classes in this data : road , soil , water and tree .    in order to extract the square root densities information embedded in the original hyperspectral images , we compute the histogram of the band values for each pixel . in our experiment ,",
    "the number of bins is set to 6 , so the resulting 6-dimensional histogram vector is used as the pixel feature vector for the ncut and lrr@xmath128ncut methods , while the square root of the histogram vector is used in gncut , sc+ncut and our new method .",
    "the subspace clustering results are detailed in table [ tbcluster ] . obviously , the performance of ncut is inferior to that of lrr+ncut , as the low rank features are more discriminative and useful than the data themselves for clustering problems .",
    "similarly , the clustering accuracy of gnut is 3% lower than that of our proposed method on average .",
    "this is mainly because gncut directly works on the original high - dimensional manifold data , without further exploring the latent subspace structure hidden in the manifold data . as gncut , sc+ncut and our proposed model",
    "use the riemannian distance measurement , they perform better than ncut and lrr+ncut , in which the euclidean distance is used to measure the similarities between points on the manifold .",
    "we can assert that taking inherent manifold structure into account can effectively improve clustering accuracy .",
    "we also note that our proposed method has the highest accuracy among all the baseline methods , which suggests that the combination of riemannian distance and lrr model brings good accuracy for ncut clustering .",
    "to visualize our proposed model s effectiveness in subspace clustering , we illustrate the clustering results of the above 4 images in figure [ cluster123]-[cluster4 ] .",
    "classes & 9&6&3&4 + ncut & 76.85 & 78.59 & 84.52 & 83.75 + lrr+ncut & 79.21 & 81.23 & 87.76 & 85.02 + gncut & 83.56 & 85.97 & 90.89 & 88.46 + sc+ncut & 85.01 & 86.33 & 93.61 & 90.05 + proposed & 88.67 & 88.99 & 97.01 & 94.71 +    [ tbcluster ]      to further examine the noise robustness of the proposed model , we add guassian white noises with different signal - to - noise ratio ( snr ) values to the above 6 baseline datasets used for both classification and subspace clustering tasks .",
    "figure [ fignoise ] illustrates the performances on all the methods , with snr values ranging from 0.8 to 12.8 . unlike sc related methods ( sc+svm and sc+ncut ) having comparable performances on the clean datasets , their performances deteriorate when data are corrupted .",
    "in contrast , lrr based methods , no matter whether designed in the euclidean space or on the manifold space , always have stable performances with different snr values , which shows that lrr is more noisetolerable and robust than sc .",
    "moreover , the geodesic distance based baselines like gknn and gncut outperform their linear euclidean counterparts ( i.e. lrr+svm / ncut , svm / ncut ) .",
    "this demonstrates that exploring data spatial correlation information in the intrinsic riemannian geometry helps to boost prediction performance .",
    "we also observe that our new method performs best among all the methods on the corrupted data sets with different snr values , confirming the importance of intrinsic geometry and sparse feature transform again .",
    "in this paper , we propose a novel lrr model on the manifold of square root densities , in which we exploit the intrinsic property of the square root densities manifold in the riemannian geometric context .",
    "compared with the existing euclidean lrr algorithms , the loss of the global linear structure is compensated by the local linear structures given by the tangent spaces of the manifold . a weight term for sparse representation is also integrated into the model to avoid assigning large weights to the far - away points on the manifold .",
    "furthermore , we derive an easily solvable optimization problem , which incorporates the structured embedding mapping and the intrinsic geodesic distance on the manifold into the lrr model .",
    "our experiments demonstrate that our proposed method is efficient and robust to noise , and produces superior results compared to other state - of - the - art methods for classification and subspace clustering applications on computer vision datasets .",
    "s.  kong and d.  wang . a dictionary learning approach for classification : separating the particularity and the commonality . in _",
    "european conference on computer vision ( eccv ) _ , volume 7572 of _ lecture notes in computer science _ , pages 186199 .",
    "springer berlin heidelberg , 2012 .",
    "z.  lin , r.  liu , and z.  su .",
    "linearized alternating direction method with adaptive penalty for low - rank representation . in _ advances in neural information processing systems _ , pages 612620 .",
    "curran associates , inc .",
    ", 2011 .",
    "d.  s. marcus , t.  h. wang , j.  parker , j.  g. csernansky , j.  c. morris , and r.  l. buckner .",
    "open access series of imaging studies ( oasis ) : cross - sectional mri data in young , middle aged , nondemented , and demented older adults .",
    ", 19:14981507 , 2007 .",
    "a.  srivastava , i.  jermyn , and s.  joshi .",
    "riemannian analysis of probability density functions with applications in vision . in _",
    "ieee conference on computer vision and pattern recognition ( cvpr ) _ ,",
    "pages 18 , june 2007 .",
    "j.  sun , y.  xie , w.  ye , j.  ho , a.  entezari , s.  j. blackband , and b.  c. vemuri .",
    "dictionary learning on the manifold of square root densities and application to reconstruction of diffusion propogator fields .",
    ", 23:619631 , 2013 ."
  ],
  "abstract_text": [
    "<S> in this paper , we present a novel low rank representation ( lrr ) algorithm for data lying on the manifold of square root densities . unlike traditional lrr methods which rely on the assumption that the data points are vectors in the euclidean space , </S>",
    "<S> our new algorithm is designed to incorporate the intrinsic geometric structure and geodesic distance of the manifold . </S>",
    "<S> experiments on several computer vision datasets showcase its noise robustness and superior performance on classification and subspace clustering compared to other state - of - the - art approaches . </S>"
  ]
}