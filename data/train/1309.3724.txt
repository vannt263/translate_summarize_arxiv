{
  "article_text": [
    "optically imaging the activity in a neuronal circuit is limited by the scanning speed of the imaging device .",
    "therefore , commonly , only a small fixed part of the circuit is observed during the entire experiment .",
    "however , in such an experiment it can be hard to infer from the observed activity patterns whether ( 1 ) a neuron a directly affects neuron b , or ( 2 ) another , unobserved neuron c affects both a and b. to deal with this issue , we propose a shotgun observation scheme , in which , at each time point , we randomly observe a small percentage of the neurons from the circuit . and so , no neuron remains completely hidden during the entire experiment and we can eventually distinguish between cases ( 1 ) and ( 2 ) .",
    "previous inference algorithms can not handle efficiently so many missing observations .",
    "we therefore develop a scalable algorithm , for data acquired using shotgun observation scheme - in which only a small fraction of the neurons are observed in each time bin . using this kind of simulated data , we show the algorithm is able to quickly infer connectivity in spiking recurrent networks with thousands of neurons .",
    "the simultaneous activity of hundreds - and even thousands - of neurons is now being routinely recorded in a wide range of preparations .",
    "the number of recorded neurons is expected to grow exponentially over the years @xcite .",
    "this , in principle , provides the opportunity to infer the `` functional connectivity '' of neuronal networks , _",
    "i.e. _ a statistical estimate of how neurons are affected by each other , and by stimulus .",
    "the ability to accurately estimate large , possibly time - varying , neural connectivity diagrams would open up an exciting new range of fundamental research questions in systems and computational neuroscience @xcite . therefore , the task of estimating connectivity from neural activity can be considered one of the central problems in statistical neuroscience - attracting much attention in recent years ( e.g. , see @xcite and references therein ) .",
    "perhaps the biggest challenge for inferring neural connectivity from activity  and in more general network analysis  is the presence of hidden nodes which are not observed directly @xcite . despite swift progress in simultaneously recording activity in massive populations of neurons , it is still beyond the reach of current technology to simultaneously monitor a complete network of spiking neurons at high temporal resolution ( though see @xcite for some impressive recent progress in that direction ) .",
    "since estimation of functional connectivity relies on the analysis of the inputs to target neurons in relation to their observed spiking activity , the inability to monitor all inputs can result in persistent errors in the connectivity estimation due to model miss - specification . more specifically , ",
    "common input errors  in which correlations due to shared inputs from unobserved neurons are mistaken for direct , causal connections  plague most naive approaches to connectivity estimation . developing a robust approach for incorporating the latent effects of such unobserved neurons remains an area of active research in connectivity analysis @xcite .",
    "in this paper we propose an experimental design which greatly ameliorates these common - input problems .",
    "the idea is simple : if we can not observe all neurons in a network simultaneously , maybe we can instead observe many overlapping sub - networks in a serial manner over the course of a long experiment",
    ". then we use statistical techniques to patch the full estimated network back together , analogously to",
    " shotgun genetic sequencing @xcite .",
    "obviously , it is not feasible to purposefully sample from many distinct sub - networks at many different overlapping locations using multi - electrode recording arrays , since multiple re - insertions of the array would lead to tissue damage .",
    "however , fluorescence - based imaging of neuronal calcium @xcite or voltage @xcite dynamics make this approach experimentally feasible . in the ideal experiment",
    ", we would target our observations so they fully cover a neuronal circuit together with all its inputs ( figure [ fig : observation schemes ] ) .",
    "however , in each time step , we would only observe a random fraction of all targeted neurons .    in this shotgun approach only a small fraction of the network",
    "is observed at any single time .",
    "however , connectivity estimation with missing observations is particularly challenging since exact bayesian inference with unobserved spikes is generally intractable .",
    "therefore , markov chain monte - carlo ( mcmc ) methods have been previously used to infer the unobserved activity ( spikes ) by sampling @xcite . however , such methods typically do not scale well for large networks .",
    "mcmc methods are computationally expensive ( though , sometimes highly parallelizable ) , and usually take a long time to converge .",
    "variational approaches @xcite , may speed up inference , but have not been shown to be robust to missing observations .",
    "fortunately , as we show here , given a shotgun sampling scheme , it is not actually required to infer the unobserved spikes .",
    "we considerably simplify the loglikelihood using the expected loglikelihood approximation@xcite , and a generalized central limit theorem ( clt ) @xcite argument to approximate neuronal input as a gaussian variable when the size of the network is large .",
    "the simplified loglikelihood depends only on the empiric second order statistics of the spiking process .",
    "importantly , these sufficient statistics can be calculated even with partial observations , by simply `` ignoring '' any unobserved activity .    in order to obtain an accurate estimation of the connectivity",
    ", this simplified loglikelihood can be very efficiently maximized  together with various types of prior information .",
    "an abundance of such prior information is available on both connection probabilities and synaptic weight distributions as a function of cell location and identity @xcite . in addition , cutting edge labeling and tissue preparation methods such as brainbow @xcite and clarity @xcite are beginning to provide rich anatomical data about  potential connectivity ( e.g. , the degree of coarse spatial overlap between a given set of dendrites and axons ) that can be incorporated into these priors . exploiting this prior information can greatly improve inference quality , as was already demonstrated in previous network inference papers @xcite .",
    "we present numerical results which demonstrate the effectiveness of our approach on a on a simulated recurrent network of spiking neurons .",
    "first , we demonstrate that the shotgun experimental design can largely eliminate the biases induced by common input effects , as originally intended ( figure [ fig : common input problem ] ) .",
    "specifically , we show that we can quickly infer the network connectivity for large networks , with a low fraction of neurons observed in each time bin .",
    "for example , our algorithm can be used to infer the connectivity of a sparse network with @xmath0 neurons and @xmath1 connections , given @xmath1 timebins of spike data in which only @xmath2 of the neurons are observed in each time bin , after running less than a minute on a standard laptop ( figure [ fig : sparsity-1 ] ) .",
    "this is achieved by assuming only a sparsity inducing prior on the weights .",
    "our parameter scans suggest that our method could be used for arbitrarily low observation ratios ( figure [ fig : parameter - scans p_obs t ] ) and arbitrarily high number of neurons ( figure [ fig : parameter - scans n - t ] ) , given long enough experiments .",
    "then , we demonstrate ( figure [ fig : prior results ] ) the usefulness of the following additional pieces of prior information : ( 1 ) dale s law - all outgoing synaptic connections from the same neuron have the same sign .",
    "( 2 ) neurons have several types - and connection strength between two types is affected by the identity of these types .",
    "( 3 ) the probability of connection between neurons is distance dependent .",
    "performance can also be improved by using external stimuli ( figure [ fig : the - effect - of - stimulus ] ) , similar to the stimulus that can be induced by persistent light sensitive ion channels @xcite .",
    "suppose we wish to perform an experiment in order to measure the functional connectivity in a neuronal circuit by simply observing its activity . in this experiment",
    ", we optically capture the neuronal spikes , visible through the use of genetically encoded calcium @xcite or voltage @xcite indicators .",
    "current imaging methods ( _ e.g. _ , two - photon or light sheet microscopy ) record this neuronal activity by scanning through the neuron tissue . the scanning protocol , and consequently , the identity of the observed neurons , have various constraints .",
    "an important constraint is the maximal scanning speed of the recording device .",
    "since the scanning speed is limited , we can not observe all the neurons in the circuit all the time with sufficient spatio - temporal resolution .",
    "we must decide where to focus our observations .",
    "commonly , observations are focused on a fixed subset of the neurons in the circuit . however , as we will show here , this procedure can generate persistent errors due to unobserved common inputs , that generate spurious correlations in the network activity . to prevent this , we propose a shotgun observation approach , in which we scan the network at a random order ( figure [ fig : observation schemes ] , _ top _ ) .",
    "thus , at each time bin of experimental data , only a random fraction of the neurons in the network are observed .",
    "intuitively , the main goal of such an approach is to evenly distribute our observations on all pairs of neurons , so that all the relevant spike correlations could be eventually estimated .",
    "note that simple serial scanning of the circuit in contiguous blocks does not accomplish this purpose ( figure [ fig : observation schemes ] , _ bottom _ ) . however , several other scanning schemes do ( random or deterministic ) - including the random shotgun scheme , on which we focus here .",
    "in this section we test the shotgun scheme on simulated @xmath3-long spike data from a recurrent spiking neural network with @xmath4 neurons .",
    "specifically , we use a generalized linear network model ( glm @xcite , see eqs .",
    "[ eq : logistic]-[eq : u ] ) with synaptic connectivity matrix @xmath5 .",
    "typically , @xmath5 is sparse , so @xmath6 , the average probability that two neurons are directly connected to each other , is low . to infer @xmath5 we use the inference method described in section [ sec : bayesian inference ] .",
    "this approximate bayesian method can exploit various priors ( section [ sub : priors ] ) , such as the sparsity of @xmath5 , to improve estimation quality .",
    "we define @xmath7 as the fraction of neurons observed at each timebin , _",
    "i.e. _ , the observation probability in the shotgun scheme . for a general list of basic notation ,",
    "see table [ tab : basic - notation ] .",
    "we start ( section [ sub : dealing - with - common ] ) with a qualitative demonstration that the shotgun approach can be used without the usual persistent bias resulting from common inputs .",
    "then , in section [ sub : connectivity - estimation - with ] , we perform quantitative tests to show that our estimation method is effective for various network sizes , observation probabilities , and stimulus input amplitudes .",
    "this is done using only a sparsity prior .",
    "lastly , in section [ sub : additional - priors ] , we show how more advanced priors can improve estimation quality .",
    "we conclude ( section [ sec : discussion ] ) that the limited scanning speed of the any imaging device recording neuronal activity is not a fundamental barrier which prevents consistent estimation of functional connectivity .",
    "in this section we use a toy network with @xmath8 neurons to visualize the common input problem , and its suggested solution - the `` shotgun '' approach .",
    "errors caused by common inputs are particularly troublesome for connectivity estimation , since they can persist even when @xmath9 .",
    "therefore , for simplicity , in this case we work in a regime where the experiment is long and data is abundant ( @xmath10 ) . in this regime ,",
    "any prior information we have on the connectivity becomes unimportant so we simply use the maximum likelihood ( ml ) estimator ( eq . [ eq : w_mle ] ) .",
    "we chose the weight matrix @xmath5 to illustrate a  worst - case common input condition ( figure [ fig : common input problem]a ) .",
    "note that the upper - left third of @xmath5 is diagonal ( figure [ fig : common input problem]b ) : _ i.e. _ , neurons @xmath11 share no connections to each other , other than the self - connection terms @xmath12 .",
    "however , we have seeded this @xmath5 with many common - input motifs , in which neurons @xmath13 and @xmath14 ( with @xmath15 ) both receive common input from neurons @xmath16 with @xmath17 .",
    "if we use a `` shotgun '' approach and observe the whole network with @xmath18 , we obtain a good ml estimate of the network connectivity , including the @xmath19 upper - left submatrix ( figure [ fig : common input problem]c ) .",
    "now suppose instead we concentrate all our observations on these @xmath20 neurons , so @xmath21 within that sub - network , but all the other neurons are unobserved .",
    "if common input was not a problem then our estimation quality should improve on that submatrix ( since we have more measurements per neuron ) .",
    "however , if common noise is problematic then we will ",
    "hallucinate many nonexistent connections ( i.e. , off - diagonal terms ) in this submatrix .",
    "figure [ fig : common input problem]d illustrates exactly this phenomenon .",
    "in contrast to the shotgun case , the resulting estimates are significantly corrupted by the common input effects .",
    "next , we quantitatively test the performance of maximum a posteriori ( map ) estimate of the network connectivity matrix @xmath5 , using a sparsity prior ( section [ sub : sparse - prior ] ) on a simulated neural network model .",
    "the neurons are randomly placed on a 1d lattice ring in locations @xmath22 . to determine",
    "the connection probability ( the probability that @xmath23 is not zero ) we used @xmath24 , where @xmath25 is the distance between two neurons , and @xmath26 was chosen so that the network sparsity is @xmath27 .",
    "for self connectivity , we simply used @xmath28 to account for the refractory period .",
    "all the non - zero off - diagonal weights @xmath23 are sampled uniformly from the range @xmath29 $ ] .",
    "also , all outgoing weights from a neuron have the same sign , following dale s law , where @xmath30 of the neurons are inhibitory ( the rest excitatory ) .",
    "these parameters were chosen to obtain a balanced spontaneous network activity ( _ i.e. _ , without ` epileptic ' activity where all the neurons are extremely synchronous ) .",
    "initially , we do not have any external inputs .",
    "the neurons are firing spontaneously , with no external input .",
    "we sampled each neuronal bias from @xmath31 to obtain a mean firing probability of @xmath32 in each time bin .",
    "the rest of the parameters are individually set for each figure - @xmath4 , the number of neurons , @xmath3 , the number of timebins and @xmath7 , the observation probability .",
    "we use four measures to assess the quality of the estimated matrix @xmath33 in comparison with the `` ground truth '' matrix @xmath5 .",
    "first , we define @xmath34 the measures we use are :    * the square root of the coefficient of determination ( @xmath35 ) : @xmath36 * correlation : @xmath37 * zero matching : @xmath38 * sign matching : @xmath39    high values indicate high quality of estimation , with @xmath40 indicating zero error . to simplify the presentation scale ,",
    "if some measure becomes negative or imaginary , we just set it to zero .      in figure",
    "[ fig : sparsity ] , for visualization purposes , we examine a toy network with @xmath8 neurons . in figure",
    "[ fig : sparsity-1 ] , we examine another network with @xmath41 neurons , which is closer to the scale of the number of recorded neurons in current calcium imaging experiments .    as can be seen in both figures ,",
    "the weight matrix can be estimated accurately for high values of observation probability @xmath7 , and reasonably well even for low value of @xmath7 .",
    "for example , in the network with @xmath8 neurons ( figure [ fig : sparsity ] ) we see a quite reasonable performance when @xmath42 , _ i.e. _ , only five neurons are observed in each timestep .",
    "even if @xmath43 , i.e. , only two neurons observed in each timestep , we can still infer quite well the sign of inferred connections .",
    "note that our method works well even if the neuron model is not a glm , as we assumed .",
    "we demonstrate this using a leaky integrate and fire neuron model ( figure [ fig : lif ] ) .",
    "next , we aimed to quantify how inference performance changes with the number of neurons , @xmath4 ( figure [ fig : parameter - scans n - t ] ) and observation probability @xmath7 ( figure [ fig : parameter - scans p_obs t ] ) . on all measures ,",
    "the performance changed qualitatively the same ( therefore , in subsequent figures , we will focus the correlation measure ) . for",
    "any given fixed parameter ( @xmath4 or @xmath7 ) , performance smoothly improves when @xmath3 increases .",
    "these scans suggest we can maintain a good quality of connectivity estimation for arbitrarily values of @xmath4 or @xmath7 - as long as we sufficiently increase @xmath3 . specifically ,",
    "if we closely examine these figures , we find that , approximately , @xmath3 should be scaled as @xmath44 in order to maintain good estimation quality .",
    "this scaling can be explained intuitively .",
    "suppose first that @xmath21 .",
    "then the total number of spike measurements is @xmath45 , and the number of non - zero network parameters ( synaptic weights ) is @xmath46 - where we recall that @xmath6 is the connection probability between two neurons .",
    "therefore , in order to maintain a fixed ratio of measurements per parameter we must scale @xmath47 ( to first order in @xmath4 ) .",
    "now suppose @xmath48 .",
    "then the total number of spike measurements is @xmath49 , and the number of non - zero parameters is still @xmath46 .",
    "therefore , it might seem that in order to maintain a fixed ratio of measurements per parameter we must scale @xmath50 . however , this ignores the adverse affect of the unobserved common input . let s examine the neuronal input ( eq . [ eq : u ] ) @xmath51 where @xmath52 is the input current arriving from the observed neurons and @xmath53 is the input current arriving from the unobserved neurons .",
    "the latter , @xmath54 , can be effectively considered as `` noise '' , which makes it harder to infer the weights in @xmath55 .",
    "since the number of terms in @xmath54 is @xmath56 times more than the number in @xmath55 , we have , approximately @xmath57 , if the neurons are not highly correlated and @xmath7 is small .",
    "therefore , the signal to noise ratio for estimating the weights should also scale approximately as @xmath7 . in order to compensate for this , we should increase @xmath3 by @xmath56 .",
    "taking this into account gives the quadratic scaling in @xmath56 that we observe ( eq . [ eq : t to n - p scaling ] ) .",
    "for exact analytical results on this issue see @xcite .",
    "next , we demonstrate how stimulus inputs can be used to improve the quality of estimation . here",
    "we use a simple periodic pulse stimulus , similar to that generated by light activated persistent ion channels @xcite .",
    "since only a brief optical stimulus is required to activate these channels , this minimizes cross - talk between optical recording ( _ e.g. _ , calcium imaging ) and optical stimulation .",
    "therefore , such type of stimulus is generally beneficial in all - optical experiments ( but may not be necessary , given recent advances @xcite ) .",
    "here , we used a single external input @xmath58 which similarly stimulates all the neurons in the circuit ( _ i.e. _ , * @xmath59 * in eq .",
    "[ eq : u ] ) .",
    "the stimulus @xmath58 has a period of @xmath60 time - steps , where the pulse is `` on '' @xmath30 of the time .",
    "we tested various pulse magnitudes , and added white noise to the pulse with a variance which is @xmath61 of the pulse magnitude .",
    "we used the same simulation parameters as in section [ sub : network - model ] , with @xmath8 , @xmath62 and @xmath63 .",
    "the only exception is that we decreased @xmath64 by @xmath65 to achieve a low firing rate regime at zero stimulus .",
    "as can be seen in figure [ fig : the - effect - of - stimulus ] , this type of stimulus has a strong effect on reconstruction performance , and there is an `` optimal magnitude '' for improving reconstruction .",
    "a main reason for this effect is that the stimulus magnitude directly determines @xmath66 , the mean spike probability ( _ i.e. _ , the firing rate ) .",
    "this directly affects the number of spike pair combinations observed during the experiment .",
    "such spike pairs are necessary for a high quality of inference , since they constitute the sufficient statistics in our posterior ( eq . [ eq : p(s|w , b * ) ] ) .",
    "therefore if the stimulus ( and therefore , firing rate @xmath67 ) is too low or high , we observe fewer spike pair combinations , and this decreases inference quality .",
    "however , introducing strong variable stimulus can also reduce inference quality , by masking the effect of weak connections on spiking .",
    "therefore , the optimal inference quality is achieved here at for @xmath68 , and not at @xmath69 - the mean spike probability in which the maximal number of spike pair combinations is observed when the pulse is `` on '' .",
    "therefore , the average firing rate in this case is @xmath69 , since the neurons hardly fire when the pulse is `` off '' ( which is @xmath30 of the time ) ] .",
    "next , we incorporate into our inference procedure additional prior information .",
    "we show that incorporating stronger prior information improves inference performance .",
    "the network is the same as in section [ sub : network - model ] , except we now choose the weight magnitude differently , as described in figure [ sub : priors ] .",
    "we use @xmath70 to select non - zero weights ( @xmath71 in figure [ fig : schematic - description ] ) , and choose connection amplitudes according to @xmath72 where @xmath73 and @xmath74 is a block matrix ( with a @xmath75 block structure ) , corresponding to the two types of neurons inhibitory and excitatory ( each covers half the network ) .",
    "we choose @xmath76 for excitatory to excitatory connections , @xmath77 for excitatory to inhibitory connections , @xmath78 for inhibitory to excitatory connections , and @xmath79 for inhibitory to inhibitory connections .",
    "again , these specific parameters were chosen in order to create a balanced spontaneous network activity .",
    "we tested various combinations of priors :    1 .   that most connections are zero ( sparsity ) .",
    "we use either @xmath80 or @xmath81 penalties ( section [ sub : sparse - prior ] ) .",
    "[ enu : spar ] 2 .   that neurons are either excitatory or inhibitory ( dale s law ) .",
    "we use the method in @xcite .",
    "[ enu : dale ] 3 .   that neuronal types affect connection amplitude .",
    "we use the stochastic block model ( section [ sub : stochastic - block - model ] ) , in which we either know the number of these types ( _ i.e. _ , we just know the rank of @xmath74 ) , or we know exactly how the types affect connection amplitude ( _ i.e. _ , we know @xmath74).[enu : sbm ] 4",
    ".   that the connection probability is distance dependent .",
    "we modify the sparsity penalty ( section [ sub : distance - dependence ] ) using @xmath82 , where we either do not know @xmath26 and @xmath83 , or do know them .",
    "[ enu : dd ]    in figure [ fig : prior results ] , we show the correlation quality measure ( eq . [ eq : c ] ) for different prior combinations .",
    "as can be seen , each additional prior incorporated into our connectivity estimate improves its quality .",
    "we find that usually the prior on the distance dependence of the connectivity has a more significant impact then the prior on type - dependence .",
    "also , both sparsity - inducing priors , @xmath80 and @xmath81 , give similar performance .",
    "typically @xmath80 is slightly better when @xmath3 is low , while @xmath81 is slightly better when @xmath3 is high ( not shown ) .",
    "the @xmath81 penalty allows somewhat faster optimization then @xmath80 .",
    "however , the @xmath80 penalty allows us to more easily incorporate neuronal type - related connectivity information .",
    "therefore when combining prior [ enu : spar ] with priors [ enu : sbm]-[enu : dd ] , we only use @xmath80 .",
    "current technology limits the number of neurons that can be simultaneously observed . therefore ,",
    "common approaches to infer functional connectivity of a neural circuit focus all the observations in one experiment on a small part of the circuit - in which all neurons are fully observed at all timebins .",
    "however , unobserved input into this sub - circuit can generate significant error in the estimation - and this error does not vanish with longer experiments .    to deal with this `` common input '' problem ,",
    "we propose a `` shotgun '' observation scheme , in which we randomly observe neurons from the circuit , so only a small percentage of the network is observed at each time point .",
    "therefore , despite the limited scanning speed of the imaging device , using this method we can arbitrary expand the field of view to include an entire circuit , together with any input to the circuit , so no neuron is completely hidden .",
    "however , existing inference algorithms can not handle efficiently so many missing observations .    for this purpose ,",
    "we develop ( section [ sec : bayesian inference ] ) a new scalable bayesian inference method .",
    "as we demonstrate numerically ( section [ sec :- results ] ) this method can be used to estimate the synaptic connectivity of a spiking neural network from spike data sub - sampled at arbitrarily low observation ratios ( e.g. , @xmath61 ) .",
    "previously , the lowest observation ratio demonstrated was @xmath30 , in a two - neuron network @xcite , or in a simple linear dynamical model for neuronal activity @xcite .",
    "moreover , our method is very fast computationally , and be can be easily used on networks with thousands of neurons .",
    "previous works , which used standard inference methods , in which the unobserved spikes are treated as latent variables ( section [ sec : other - inference - methods ] ) , are slower by several orders of magnitude .",
    "specifically , previous mcmc methods @xcite did not go beyond a @xmath84 neurons , while variational approaches @xcite did not go beyond @xmath85 .",
    "though the latter may be scaled for a larger population , it is not clear if this approach can handle missing observations .",
    "the proposed method is capable of incorporating various kinds of prior information ( figure [ fig : schematic - description ] ) , such as the sparsity of synaptic connections , dale s law , the division of neurons into types and the fact that the probability of having a connection between two neurons typically decreases with the distance between two neurons .",
    "we show that each piece of information can be used to improve estimation quality ( figure [ fig : prior results ] ) .",
    "another way to significantly improve estimation quality , is to adjust the baseline firing rate of the network using the stimulus ( figure [ fig : the - effect - of - stimulus ] ) .",
    "specifically , we use a pulse - like stimulus , similar to that generated by persistent ion channels @xcite , which can be used in the context of all - optical experiments with minimal cross - talk between stimulation and recording .",
    "more sophisticated types of stimulation schemes ( e.g. , @xcite ) may improve performance even further @xcite .",
    "we conclude that , using the shotgun observation scheme , we can remove the persistent bias resulting from the common input problem ( figure [ fig : common input problem ] ) .",
    "therefore , the limited scanning speed of a imaging device is not a fundamental obstacle hindering functional connectivity estimation .",
    "a complete removal of the bias is possible only if all the neurons in the circuit are observed together with all inputs to the circuit ( a sufficient number of times ) . however , in most experimental setups , some neurons would never be observed ( _ e.g. _ , due to opacity of the brain tissue ) .",
    "therefore , some persistent bias may remain . to deal with this",
    "we will need to incorporate any unobserved input into the circuit using a small number of latent variable @xcite .",
    "due to intrinsic fluctuations in the neuronal excitability @xcite , such a latent variable approach can be relevant even in cases where potentially the entire circuit could be observed ( _ e.g. _ , in zerbrafish @xcite ) .    even if all the neuronal inputs are eventually observed , the variance due to the unobserved inputs is still high , since , at any given time , most of the inputs to each neuron will be unobserved ( see also @xcite ) . as a result ,",
    "the duration of the experiment required for accurate inference increases quadratically with the inverse of observation probability ( eq . [ eq : t to n - p scaling ] and figure [ fig : parameter - scans p_obs t ] ) .",
    "this issue will persist for any fixed observation strategy that does not take into account any prior information on the network connectivity .",
    "however , there might be a significant improvement in performance if we can focus the observations on synaptic connections which are more probable . this way we can effectively reduce input noise from unobserved neurons , and improve the signal to noise ratio . as a simple example , suppose we know the network is divided in to several disconnected components . in this case",
    ", we should scan each sub - network separately , _",
    "i.e. _ , there is no point in interleaving spike observations from two disconnected sub - networks .",
    "how should one focus observations in the more general case ? we leave this as an interesting open question in bayesian experimental design methods ( `` active learning '' ) .",
    "in this section we describe the statistical methods used for inferring connectivity given data , observed using the shotgun scheme .",
    "first , we describe the basic statistical model and framework ( section [ sec : preliminaries ] ) ; our main analytical results on the simplified loglikelihood ( section [ sec : bayesian inference ] ) ; how to incorporate various types of prior with this loglikelihood ( section [ sub : priors ] ) ; and a few alternative inference methods ( section [ sec : other - inference - methods ] ) . in the supplementary appendix s1",
    "we provide all the technical details of the algorithms used .      a boldfaced letter @xmath86 denotes a vector with components @xmath87 , a boldfaced capital letter @xmath88 denotes a matrix with components @xmath89",
    ", @xmath90 denotes the @xmath16-th matrix in a list , and @xmath91 @xmath92 the @xmath16-th column ( row ) vector of matrix @xmath88 .",
    "for @xmath93 we define the empiric average and variance , despite the @xmath94 index , which is maintained for notational convenience . ]",
    "@xmath95 we define the matrices @xmath96 and @xmath97 as @xmath98 matrices in which all the components are equal to zero or one , respectively . for any condition @xmath99 , we make use of @xmath100 , the indicator function ( _ i.e. _ , @xmath101 if @xmath99 holds , and zero otherwise ) . also , @xmath102 , kronecker s delta function . if @xmath103 then @xmath86 is gaussian random vector with mean @xmath104 and covariance matrix @xmath105 , and we denote its density by @xmath106 .",
    "we use a discrete - time neural network .",
    "the neurons , indexed from @xmath107 to @xmath4 , produce spikes in time bins indexed from @xmath108 to @xmath3 .",
    "the spiking matrix @xmath109 is composed of variables @xmath110 indicating the number of spikes neuron @xmath13 produces at time bin @xmath94 .",
    "we assume each neuron @xmath13 generates spikes @xmath111 according to a generalized linear network model ( glm @xcite ) , with a logistic probability function @xmath112 depending on the the input @xmath113 it receives from other neurons , as well as from some external stimulus .",
    "the input to all the neurons in the network is therefore @xmath114 where @xmath115 is the bias of neuron @xmath13 ; @xmath116 are the external inputs ( with @xmath117 being the number of inputs ) ; * @xmath118 * is the input gain ; and @xmath119 is the ( unknown ) network connectivity matrix .",
    "the diagonal elements @xmath12 of the connectivity matrix correspond to the post spike filter accounting for the cell s own post - spike effects ( _ e.g. _ , refractory period ) , while the off - diagonal terms @xmath23 represent the connection weights from neuron @xmath14 to neuron @xmath13 .",
    "the bias @xmath120 controls the mean spike probability ( firing rate ) of neuron @xmath13 .",
    "the external input @xmath88 can represent a direct ( _ e.g. _ , light activated ion channels ) or sensory ( _ e.g. _ , moving grating ) stimulation of neurons in the network .",
    "the input gain @xmath121 is a spatial filter that gives the effect of this input @xmath88 on the network .",
    "the input gain represents the affect of this input on the network .",
    "we assume that spiking starts from some fixed distribution @xmath122 .    to simplify notation",
    "we have assumed in eq .",
    "[ eq : u ] that @xmath123 is affected by spiking activity only in the previous time bin ( @xmath124 ) . however , it is straightforward to generalize our results to the case that the input ( eq . [ eq : u ] ) includes a longer history of the spiking activity , where @xmath125 and @xmath126 determine the neuronal timescales . ] .",
    "it is also possible to change eq .",
    "[ eq : logistic ] and assume other spiking models ( _ e.g. _ , poisson or binomial ) .",
    "our goal is to infer the connectivity matrix @xmath5 , biases @xmath64 and the stimulus gain @xmath127 .",
    "we assume that we have some prior information on the weights ( including @xmath4 ) , that we know the external input @xmath88 , and that we noiselessly observe a subset of the generated spikes .",
    "we use a binary matrix @xmath128 to indicate which neurons were observed , so @xmath129\\,.\\ ] ] we assume the observation process is uncorrelated with the spikes , and that @xmath130 and @xmath131 converge to strictly positive limits when @xmath9 , for all @xmath132 and for @xmath133 . in other words , we need to observe a large number of timebins ( proportional to @xmath3 ) from each pair of neurons @xmath134 - both simultaneous spike pairs @xmath135 and spike pairs in which one spike arrives one timestep after the other @xmath136 . for example , these assumptions are fulfilled if the spikes are uniformly and randomly observed , or if at each time bin a random observation block is chosen ( figure [ fig : observation schemes ] , _ top _ ) .",
    "however , some spike pairs are never observed , if , for example , we only observe a fixed subset of the network , or scan serially in a continuous manner ( figure [ fig : observation schemes ] , _ bottom _ ) .",
    "we use a bayesian approach in order to infer the unknown weights . for simplicity , initially assume that all spikes are observed and that there is no external input @xmath137 . in this case , the log - posterior of the weights , given the spiking activity , is    @xmath138    where @xmath139 is the loglikelihood , @xmath140 is some prior on the weights ( we do not assume a prior on the biases @xmath64 ) , and @xmath141 is some unimportant constant ( which does not depend on @xmath5 or @xmath64 ) . our aim would be to find the maximum a posteriori ( map ) estimator for @xmath5 , together with the maximum likelihood ( ml ) estimator for @xmath64 , by solving @xmath142    next , we show that under some reasonable assumptions , the loglikelihood can be transformed to a simple form . importantly , this simple form can be easily calculated even if there are some missing observations , and we have an external stimulus @xmath143 . specifically , we consider a glm spiking network model , as in eq .",
    "[ eq : logistic ] , and simplify its loglikelihood @xmath139 .",
    "using similar techniques as in @xcite ( and a few additional ` tricks ' ) we use the expected loglikelihood approximation together with a generalized central limit theorem ( clt ) argument @xcite , in which we approximate the neuronal input to be gaussian near the limit @xmath144 ; then we calculate the `` profile likelihood '' @xmath145 in which the bias term has been substituted for its maximizing value ( appendix s1 , section [ sub : derivation ] ) .",
    "the end result is @xmath146-h\\left(m_{i}\\right)\\sqrt{1+\\frac{\\pi}{8}\\sum_{k , j}w_{i , j}\\sigma_{k , j}^{\\left(0\\right)}w_{i , k}}\\right]\\,,\\label{eq : p(s|w , b*)}\\end{aligned}\\ ] ] where we denoted @xmath147 as the mean spike probability , spike covariance and the entropy function , respectively .",
    "we make a few comments :    1 .",
    "importantly , this loglikelihood depends on the data only through the sufficient statistics @xmath148 and @xmath149 ( defined in eqs . [ eq : m]-[eq : sigma ] ) , which can be estimated even when some observations are missing . specifically ,",
    "if we have a only partial observation of @xmath109 , we define @xmath150 recall the observation process is uncorrelated with the spikes , and that @xmath151 and @xmath152 , @xmath130 and @xmath131 converge to strictly positive limits when @xmath9 . from slutsky",
    "s theorem @xcite we obtain that for @xmath9 @xmath153 given that @xmath154 and @xmath155 converge to some limit when @xmath9 .",
    "therefore , when some observations are missing we can simply replace @xmath156 with @xmath157 and @xmath158 with @xmath159 in the profile loglikelihood ( eq . [ eq : p(s|w , b * ) ] ) .",
    "as we show in appendix s1 , section [ sec : likelihood - derivatives ] the profile loglikelihood ( eq . [ eq : p(s|w , b * ) ] ) is concave , and so it is easy to maximize the log - posterior and obtain the map estimate of @xmath5 .",
    "additionally , it is straightforward to calculate the gradient , hessian and the relevant lipschitz constant of this loglikelihood .",
    "moreover , since the profile loglikelihood ( eq . [ eq : p(s|w , b * ) ] ) decomposes over the rows of @xmath5 @xmath160 as do many of the log - priors we will use ( section [ sub : priors ] ) , the optimization problem of finding the map estimate can be parallelized over the rows of @xmath5 .",
    "3 .   the ml estimator of @xmath5 ( the maximizer of eq .",
    "[ eq : p(s|w , b * ) ] ) can be derived analytically ( appendix s1 , section [ sec : maximum - likelihood - estimator ] ) by equating the gradient to zero .",
    "the result is @xmath161 where @xmath162 this estimate would coincide with any map estimate when @xmath9 .",
    "4 .   note a finite solution exists only if the last expression has a real value .",
    "interestingly , this ml estimate is a rescaled version of the ml estimate in a simple linear gaussian neuron model .",
    "a similar ml estimate was obtained , using similar approximations , for a poisson neuron model @xcite , albeit with a different re - scaling .",
    "this may hint at the generality of this form , and its applicability for other non - linear models .",
    "due to an approximation we make in the derivation ( eq . [ eq : wang approx ] ) of the profile loglikelihood ( eq . [ eq : p(s|w , b * ) appendix ] ) , the scale factor @xmath163 tends to be smaller then it should be .",
    "this `` shrinkage '' affects all estimators ( ml or map ) based on the profile loglikelihood .",
    "however , this issue can be corrected by re - fitting the scale factors , as explained in appendix s1 , section [ sec : correcting - ampitudes - and ] .",
    "if the neural input is small ( due to weak weights , low firing rates or a small number of neurons ) , the profile loglikelihood ( eq . [ eq : p(s|w , b * ) appendix ] ) reduces to a simple quadratic form @xmath164-h\\left(m_{i}\\right)\\frac{\\pi}{8}\\sum_{k , j}w_{i , j}\\sigma_{k , j}^{\\left(0\\right)}w_{i , k}\\right]\\,.\\label{eq : quadratic loglikelihood}\\ ] ] this is similar to the loglikelihood that we would have a obtained if we would have assumed a linear gaussian neuron model , albeit with a different constants .",
    "therefore , the non - linear nature of the neurons in our model only becomes important when the neuronal input is strong .",
    "though we assumed the network does not have a stimulus ( @xmath165 ) , a stimulus can be incorporated into the inference procedure . to do this",
    ", we treat the stimulus @xmath166 simply as the activity of additional , fully observed , neurons ( albeit @xmath167 while @xmath111 ) .",
    "specifically , we define a new `` spikes '' matrix @xmath168 , a new connectivity matrix @xmath169 and a new observation matrix @xmath170 repeating the derivations for @xmath171 and @xmath172 , we obtain the same profile loglikelihood .",
    "once it is used to infer @xmath173 , we extract the estimates of @xmath5 and @xmath121 from their corresponding blocks in @xmath173 .",
    "formally , we need to make sure the conditions of clt - based approximation @xcite are fulfilled for our approximated method to work , and this can become even more challenging with the addition of ( arbitrary ) external inputs . however , as can be seen from the simulations , such a generalized clt - based approximation tends to work quite well even when the neuronal input is not strictly gaussian @xcite .",
    "for example , in figure [ fig : sparsity ] , on average , there are about three non - zero spikes in the input of each neuron . in figure [",
    "fig : the - effect - of - stimulus ] , our algorithm is still accurate even though the empirical distribution of the inputs can not be gaussian - since it is bimodal due to strong the periodical pulse input .      in this section",
    "we examine different priors @xmath140 on the network connectivity that can be incorporated into the posterior : the sparsity of inter - neuron connections ( section [ sub : sparse - prior ] ) ; the division of neurons into several `` types '' ( section [ sub : stochastic - block - model ] ) ; and the fact that connection probability decreases with distance ( section [ sub : distance - dependence ] ) .      the diagonal elements @xmath12 of the connectivity matrix are typically negative , corresponding to the post spike filter accounting for the cell s own ( refractory ) post - spike effects , while the off - diagonal terms @xmath23 represent the connection weights from neuron @xmath14 to neuron @xmath13 . as most neurons",
    "are not connected , most of the off - diagonal @xmath23 are equal to zero , so * @xmath5 * is a sparse matrix .",
    "[ [ l1-norm . ] ] l1 norm .",
    "+ + + + + + + +    the most popular approach for incorporating prior information about sparsity is to use a laplace distribution as prior , which adds an @xmath174 norm penalty : @xmath175 for some set of sparsity parameters @xmath176 . in order to take into account the fact that the off - diagonal terms are zero , we set @xmath177 where @xmath178 .",
    "this prior has a number of advantages : the resulting log - posterior of @xmath5 ( given the full spike train @xmath109 and the other system parameters ) turns out to be a concave function of @xmath5 , and the maximizer of this posterior , @xmath179 is often sparse , i.e. , many values of @xmath179 are zero . combining this prior in to the posterior ( eq . [ eq : log posterior ] ) , together with the simplified profile loglikelihood ( eq . [ eq : p(s|w , b * ) ] )",
    ", we obtain a lasso problem for @xmath5 @xcite . by solving this objective",
    "we obtain a sparse maximum a posteriori ( map ) estimate @xmath179 .",
    "there are many algorithms that can be used to solve such an objective .",
    "usually , the most efficient @xcite is the fista algorithm @xcite ; details are given in appendix s1 , section [ sec : the - fista - algorithm ] . in order to set the value of @xmath180 , we assume some prior knowledge ( _ e.g. _ , statistics from anatomical data ) about the sparsity of @xmath5 - _ i.e. _ , how many neurons are connected . using this knowledge it straightforward to set the value of @xmath180 using a binary search algorithm , as explained in appendix s1 , section [ sec : setting lambda ] .",
    "[ [ l0-norm . ] ] l0 norm .",
    "+ + + + + + + +    a more direct approach to sparse optimization is to replace the l1 norm with the `` l0 norm '' penalty    @xmath181+c\\,,\\label{eq : l0 prior}\\ ] ]    where the @xmath176 are given by eq .",
    "[ eq : lambda mask ] .",
    "this makes the resulting log - posterior non - concave and hard to maximize .",
    "fortunately , forward greedy algorithms ( appendix s1 , section [ sec : greedy - algorithms ] ) usually work quite well with such a penalty @xcite .",
    "briefly , these algorithms initially assume that all weights are zero , and then iteratively repeat the following steps : ( 1 ) fix all the weights from the previous step , and choose one ( previously zero ) weight which would maximally increase the profile loglikelihood ( eq .",
    "[ eq : p(s|w , b * ) ] ) if it is allowed to be non - zero .",
    "( 2 ) extend the support of non - zero weights to include this weight .",
    "( 3 ) maximize the profile loglikelihood ( eq .",
    "[ eq : p(s|w , b * ) ] ) over this support , with all the other weights being zero .",
    "a few comments are in order :    1 .",
    "in our case ( eq .",
    "[ eq : p(s|w , b * ) ] ) , all steps can be performed analytically .",
    "2 .   in such an approach",
    "there is no need to set regularization parameters @xmath176 , since we only need to stop the algorithm when the required level of sparsity has been reached .",
    "3 .   in the case",
    "that weights are weak and the profile loglikelihood becomes quadratic ( eq . [ eq : quadratic loglikelihood ] ) we get the familiar orthogonal matching pursuit algorithm ( omp , @xcite ) , which is typically faster .",
    "forward - backward greedy algorithms @xcite can also be used here , and may offer improved performance .",
    "we leave this for future work .",
    "as we will show later , empirically , @xmath80 and @xmath81 give comparable performance , with @xmath80 being slower .",
    "however , the main advantage of this @xmath80 approach is that it can be more easily extended to include other types of prior information , as we discuss next .",
    "neurons can be classified into distinct types @xcite , based on structural , synaptic , genetic and functional characteristics .",
    "the connection strength between two neurons usually depends on the types of the pre and post synaptic neurons @xcite .",
    "a well known example for this is `` dale s law '' - that all neurons are either excitatory or inhibitory .",
    "therefore , all the outgoing synaptic connection from a neuron should have the same sign .",
    "this specific information can be incorporated into our estimates of the weight matrix @xmath5 , by using a greedy approach , as described in @xcite . in this approach",
    "we first maximize the posterior without considering dale s law , and find an estimate @xmath33 .",
    "then , neurons are classified as excitatory or inhibitory according to the difference in the count of outgoing weights of each sign @xmath182 . specifically , setting @xmath183 as some threshold ( we arbitrarily chose @xmath184 ) , a neuron @xmath14 was classified as excitatory if @xmath185 , as inhibitory if @xmath186 , and is not classified if @xmath187 .",
    "next , we maximize again the posterior with additional constraints - that in all classified neurons , all outgoing weights have the proper sign for their class .",
    "this procedure is then iterated .",
    "however , in order to include a more general type - related information into the connectivity prior , here we also used a `` stochastic block model '' approach @xcite .",
    "we will assume that all the non - zero weights are sampled from a gaussian distribution with a fixed variance , and a mean @xmath188 which depends only on the pre and post synaptic types , and thus have a block structure , where each block has a fixed value ( * @xmath74 * in * * figure [ fig : schematic - description ] ) .",
    "as a result , in addition to any previous sparsity - inducing log - prior , we will add also the following penalty @xmath189\\left(w_{i , j}-v_{i , j}\\right)^{2}\\,,\\label{eq : sbm penalty}\\ ] ] where @xmath190 is a regularization parameter , and @xmath74 is the aforementioned block structured matrix . if @xmath74 is known , we can add this penalty to the @xmath80 norm penalty , and solve using the same greedy algorithm , with a modified objective function",
    ".    however , usually @xmath74 is not known , and all we know ( approximately ) is the number of types .",
    "since the number of types is greater or equal to the rank of @xmath74 , and this rank is usually much lower then @xmath4 , we can penalize the rank of @xmath74 .",
    "a convex relaxation of this is @xmath191 , the nuclear norm @xcite , which we add to the log - posterior .",
    "next , we describe how to maximize the full log - posterior ( which is a sum of eqs .",
    "[ eq : p(s|w , b * ) ] , [ eq : sbm penalty ] and @xmath192 ) .",
    "we first initialize @xmath193 .",
    "then we alternate between : ( 1 ) holding @xmath74 fixed and maximizing the log - posterior over @xmath5 ( the sum of eqs .",
    "[ eq : p(s|w , b * ) ] and [ eq : sbm penalty ] ) ( 2 ) holding @xmath5 fixed and maximizing the log - posterior over @xmath74 ( the sum of eqs .",
    "[ eq : sbm penalty ] and @xmath192 ) .",
    "the first step can be done using a forward greedy approach , as we did for the @xmath80 penalty .",
    "the second step can be done using soft - impute algorithm @xcite , assuming we know in advance this rank ( which determines @xmath194 ) .",
    "thus , we update @xmath5 and @xmath74 iteratively until convergence .",
    "the value of @xmath190 is determined using a cross - validation set .",
    "more details are given in appendix s1 , section [ sec : greedy - algorithms ] .",
    "the probability of a connection between two neurons often declines with distance @xcite .",
    "this prior information can also be used to our advantage .",
    "suppose first that the probability of two neurons being connected is given by some known function @xmath195 , where @xmath196 is the distance between the neurons @xmath13 and @xmath14 ( figure [ fig : schematic - description ] ) . a simple way to incorporate this knowledge to modify the regularization constants @xmath176 ( eq . [ eq : lambda mask ] ) in either the l1 penalty ( eq . [ eq : l1 prior ] ) or the l0 penalty ( eq . [ eq : l0 prior ] ) so they are distance dependent @xmath197 this way the prior probability for a connection reflects this distance dependence - if a connection is less probable , its regularization constant is higher .    if @xmath71 is unknown , we give it some parametric form .",
    "for example , we will use here @xmath198 . to find @xmath199 , we alternate between maximizing the posterior over @xmath5 and then over @xmath199 .",
    "more details are given in appendix s1 , section [ sec : greedy - algorithms ] .",
    "conceptually , having missing spike observations in the data should make it much harder to estimate @xmath5 from data .",
    "this is because , in this case , we need to modify the posterior ( eq . [ eq : log posterior ] ) , and replace the complete likelihood @xmath200 with the incomplete likelihood @xmath201 where @xmath202 _ i.e. , _ the set of all observed spikes from * @xmath109*. however , in order to obtain @xmath201 from @xmath200 we need to perform an intractable marginalization over an exponential number of unobserved spike configurations .",
    "the method we derived here sidesteps this problem .",
    "we infer the connectivity matrix @xmath5 using a map estimate , derived using an approximation of the complete loglikelihood @xmath139 ( eq . [ eq : log posterior ] ) .",
    "though the complete loglikelihood includes all the spikes ( even the unobserved ones ) , in its approximated form we can also handle missing observations .",
    "this is done by simply ignoring missing observations and re - normalizing accordingly the sufficient statistics ( eq . [ eq : m partial]-[eq : sigma partial ] ) of the approximated loglikelihood ( eq . [ eq : p(s|w , b * ) ] ) .",
    "this procedure does not affect the asymptotic value of the sufficient statistics , since the observations and spikes are uncorrelated .",
    "in contrast , classical inference methods @xcite treat the unobserved spikes as latent variables , and attempt to infer them using one of these standard approaches :    * markov chain monte carlo ( mcmc ) - which samples the latent variables and weights from the complete posterior . * expectation maximization ( em ) - which provides an estimator that locally optimizes the posterior of @xmath203 given the observed data . * variational bayes ( vb ) - which approximates the complete posterior using a factorized form .",
    "we experimented with all these approaches on the posterior , without using any of the simplifying approximations used in our main method ( section [ sec : bayesian inference ] ) . in order to clarify the motivation for our main method",
    ", we briefly describe these alternative approaches below , including their advantages and disadvantages .",
    "we assume a sparsity promoting spike and slab prior on the weights @xmath204\\label{eq : spike and slab}\\ ] ] and use a gibbs approach to sample jointly from @xmath205 : first we sample @xmath109 given the observed data and the current sample from @xmath5 , and then sample @xmath5 given @xmath109 .",
    "note the standard l1 prior on @xmath5 ( eq . [ eq : l1 prior ] ) would not promote sparsity in this mcmc setting , since then every sample of @xmath5 would be non - zero with probability @xmath40 .",
    "in the first step we draw one sample from the posterior of @xmath109 given the observed data and an estimated @xmath5 , using the improved metropolized gibbs sampler @xcite , as described in appendix s1 , section [ sub : sampling - the - spikes ] .",
    "this sampler is quite simple and is highly parallelizable in this setting , because the graphical model representing the posterior @xmath206 is local : @xmath207 is conditionally independent of @xmath208 given @xmath209 since the spiking at time @xmath94 only directly affects the spiking in the next time step ( _ i.e. _ , model [ eq : logistic]-[eq : u ] can be considered a markov chain in @xmath207 ) .",
    "therefore we can alternately sample the spiking vectors @xmath207 at all odd times @xmath94 completely in parallel , and then similarly for the even times . in more general glm models , where the neuronal input depends on previous @xmath16 spikes , _",
    "i.e. _ , @xmath210 , then the algorithm could be executed using @xmath211 parallel computations .",
    "we note that more sophisticated sampling methods have been developed for this type of problem @xcite ; these specialized methods are significantly more efficient if implemented serially , but do not parallelize as well as the simple gibbs - based approach we used here . the performance of the sampler is exemplified in figure [ fig : estimating - the - spikes ] , bottom . as can be seen in this figure , usually ( unless connectivity weights are very high ) the spike sampler can predict the spikes only in a `` small neighborhood '' of the visible spikes .    in the second step , we sample @xmath5 given @xmath109 .",
    "we first note that , with the spike and slab prior the , again the posterior factorizes : @xmath212 .",
    "thus , we can sample from each @xmath213 in parallel .",
    "to sample from @xmath213 , we simply gibbs sample , one element @xmath23 at a time .",
    "one way to do this is described in appendix s1 , section [ sub : sampling - the - weights ] .",
    "the derivation requires the assumption that the weights are small ( empirically , it works well if @xmath214 ) . alternatively , this assumption is not necessary if we use instead the ( slower ) approach discussed in @xcite , which requires the computation of some one - dimensional integrals . in our case",
    "these integrands are log - concave and therefore uni - modal , and can be calculated using a laplace approximations .",
    "since both methods contain some approximation , we first need to use them to obtain proposals densities for a metropolis - hastings scheme , which then generates the @xmath23 samples @xcite .    in general , computational speed is the main disadvantage of the mcmc method - both for the sampling the spikes and for sampling the weights . for large networks ,",
    "this approach can only be used if the sampling scheme is completely parallelized over many processors .",
    "however , mcmc performs rather well on small networks , similarly to our main method ( section [ sec : bayesian inference ] ) .",
    "an important advantage of the mcmc method over our main method is that it can be used even if some `` spike pairs '' are never observed , _",
    "i.e. _ if for some @xmath132 , @xmath131 for @xmath215 or @xmath40 .",
    "therefore , the mcmc method might be used complement our main method to infer the weights in this case .",
    "it should also be noted that the mcmc approach offers somewhat simpler methods for hyperparameter selection ( via sampling from the hyperparameter posterior ) , and easier incorporation into larger hierarchical models that can include richer prior information about the network and enable proper sharing of information across networks .",
    "the em approach is similar to the methods discussed in @xcite .",
    "the e step requires the computation of an integral over @xmath216 .",
    "since this integral is high - dimensional and not analytically tractable , we again resort to mcmc methods to sample the spikes , using the same sampler as before .",
    "we found that in most cases it was not necessary to take many samples from the posterior ; for large enough network sizes @xmath4 ( and for correspondingly long experimental times @xmath3 ) , @xmath109 is large enough that a single sample contains enough information to adequately estimate the necessary sufficient statistics in the e step .",
    "see @xcite for further discussion . in the m step",
    "we perform maximization over the log - posterior averaged over @xmath216 , with an l1 prior the em approach is is still rather slow , since we need to sample from the spikes .",
    "moreover , the em approach typically suffered from shrinkage and exhibited worse performance then the mcmc approach .      using the standard vb approach @xcite and the spike and slab prior on the weights ( eq . [ eq : spike and slab ] ) we can approximate the posterior @xmath205 using a fully factorized distribution @xmath217 in which the factors are calculated using @xmath218 where , for any random variable @xmath219 , @xmath220 is an expectation performed using the distribution @xmath221 .",
    "this calculation proceeds almost identically to the derivation of the gibbs samplers ( appendix s1 , section [ sec : mcmc - approach ] ) , except we need to perform the expectation @xmath220 over the results .",
    "these integrals can be calculated using the clt and the approximations given in ( * ? ? ?",
    "* and 4.1 ) .    the factorized form in eq .",
    "[ eq : q factorized ] assumes that , approximately , all the weights and all the spikes are independent . in this method , which is faster then",
    "mcmc , the mean firing rates of the neurons could be estimated reasonably well , as is exemplified in figure [ fig : estimating - the - spikes ] , bottom .",
    "unfortunately , the weight matrix @xmath5 could not be estimated if less than @xmath222 of the neurons were observed in each timestep .",
    "this is in contrast to the other methods , in which the observed fraction can be arbitrarily low .",
    "we believe this happens because the vb approximation for the spikes ignores spike correlations - which determine the sufficient statistics in this case ( as suggested by eq .",
    "[ eq : p(s|w , b * ) ] ) . however , it is possible that more sophisticated factorized forms ( which do not assume spike independence ) will still work .",
    "the authors are thankful to eftychios pnevmatikakis , ari pakman and ben shabado for their help and support , and to ran rubin , yuriy mishchenko and ron meir for their helpful comments .",
    "this work was partially supported by the gruss lipper charitable foundation , grant aro muri w911nf-12 - 1 - 0594 , grant darpa w91nf-14 - 1 - 0269 , and grant nsf career ios-0641912 .",
    "additional support was provided by the gatsby foundation .",
    "10 [ 1]`#1 ` urlstyle [ 1]doi:#1    [ 1 ] [ 2 ]    _ _ _ _ _ _ _ _ _ _ _ _ _ _ key : # 1 + annotation :  # 2 _ _ _ _ _ _ _ _ _ _ _ _ _ _    stevenson ih , kording kp ( 2011 ) how advances in neural recording affect data analysis .",
    "nature neuroscience 14 : 13942 .",
    "alivisatos ap , chun m , church gm , greenspan rj , roukes ml , et  al .",
    "( 2012 ) the brain activity map project and the challenge of functional connectomics .",
    "neuron 74 : 970974 .",
    "nykamp dq ( 2003 ) reconstructing stimulus - driven neural networks from spike times .",
    "nips 15 : 309316 .",
    "paninski l ( 2004 ) maximum likelihood estimation of cascade point - process neural encoding models .",
    "network : computation in neural systems 15 : 243262 .",
    "pillow jw , paninski l , uzzell v , simoncelli e , chichilnisky ej ( 2005 ) prediction and decoding of retinal ganglion cell responses with a probabilistic spiking model .",
    "journal of neuroscience 25 : 1100311013 .",
    "rigat f , de  gunst m , van pelt j ( 2006 ) bayesian modelling and analysis of spatio - temporal neuronal networks .",
    "bayesian analysis 1 : 733764 .",
    "nykamp dq ( 2007 ) a mathematical framework for inferring connectivity in probabilistic neuronal networks .",
    "mathematical biosciences 205 : 20451 .",
    "pillow jw , latham p ( 2007 ) neural characterization in partially observed populations of spiking neurons",
    ". nips .",
    "pillow jw , shlens j , paninski",
    "l , sher a , litke am , et  al .",
    "( 2008 ) spatio - temporal correlations and visual signalling in a complete neuronal population .",
    "nature 454 : 9959 .",
    "mishchenko y , vogelstein jt , paninski l ( 2011 ) a bayesian approach for inferring neuronal connectivity from calcium fluorescent imaging data .",
    "the annals of applied statistics 5 : 12291261 .    stetter o , battaglia d , soriano j , geisel t ( 2012 ) model - free reconstruction of excitatory neuronal connectivity from calcium imaging signals .",
    "plos computational biology 8 : e1002653 .",
    "paninski l , vidne m , depasquale b , ferreira dg ( 2012 ) inferring synaptic inputs given a noisy voltage trace via sequential monte carlo methods .",
    "journal of computational neuroscience 33 : 119 .",
    "pakman a , huggins j , smith c , paninski l ( 2014 ) fast penalized state - space methods for inferring dendritic synaptic connectivity .",
    "journal of computational neuroscience 36 : 415443 .",
    "gerhard f , kispersky t , gutierrez gj , marder e , kramer m , et  al .",
    "( 2013 ) successful reconstruction of a physiological circuit with known connectivity from spiking activity alone .",
    "plos computational biology 9 : e1003138 .",
    "ltcke h , gerhard f , zenke f , gerstner w , helmchen f ( 2013 ) inference of neuronal network spike dynamics and topology from calcium imaging data .",
    "frontiers in neural circuits 7 : 201 .",
    "memmesheimer rm , rubin r , olveczky bp , sompolinsky h ( 2014 ) learning precisely timed spikes .",
    "neuron 82 : 92538 .",
    "fletcher ak , rangan s ( 2014 ) scalable inference for neuronal connectivity from calcium imaging . in : nips .",
    "1409.0289 .",
    "nykamp dq ( 2008 ) pinpointing connectivity despite hidden nodes within stimulus - driven networks .",
    "physical review e 78 : 021902 .",
    "vidne m , ahmadian y , shlens j , pillow jw , kulkarni j , et  al .",
    "( 2012 ) modeling the impact of common noise inputs on the network activity of retinal ganglion cells .",
    "j computational neuroscience 33 : 97121 .",
    "mishchenko y , paninski l ( 2011 ) efficient methods for sampling spike trains in networks of coupled neurons .",
    "the annals of applied statistics : 126 .",
    "turaga s , buesing l , packer am , dalgleish h , pettit n , et  al .",
    "( 2013 ) inferring neural population dynamics from multiple partial recordings of the same neural circuit . in : nips .",
    "romano l , opper m ( 2014 ) inferring hidden states in a random kinetic ising model : replica analysis .",
    "arxiv preprint arxiv:14054164 .",
    "tyrcha j , hertz j ( 2014 ) network inference with hidden units . mathematical biosciences and engineering : mbe 11 : 14965 .",
    "ahrens mb , orger mb , robson dn , li jm , keller pj ( 2013 ) whole - brain functional imaging at cellular resolution using light - sheet microscopy .",
    "nature methods 10 : 41320 .",
    "venter jc , adams md , sutton gg , kerlavage ar , smith ho , et  al .",
    "( 1998 ) shotgun sequencing of the human genome .",
    "science 280 : 15402 + .",
    "reddy gd , kelleher k , fink r , saggau p ( 2008 ) three - dimensional random access multiphoton microscopy for functional imaging of neuronal activity .",
    "nature neuroscience 11 : 713720 .",
    "grewe bf , langer d , kasper h , kampa bm , helmchen f ( 2010 ) high - speed in vivo calcium imaging reveals neuronal network activity with near - millisecond precision .",
    "nature methods 7 : 399405 .",
    "hochbaum dr , zhao y , farhi sl , klapoetke n , werley ca , et  al .",
    "( 2014 ) all - optical electrophysiology in mammalian neurons using engineered microbial rhodopsins .",
    "nature methods 11 .",
    "mishchenko y , paninski l ( 2011 ) efficient methods for sampling spike trains in networks of coupled neurons .",
    "the annals of applied statistics : 126 .",
    "park i m , pillow jw ( 2011 ) bayesian spike - triggered covariance analysis . nips .",
    "sadeghi k , gauthier j l , field",
    "gd , greschner m , agne m , et  al .",
    "( 2012 ) monte carlo methods for localization of cones given multielectrode retinal ganglion cell recordings .",
    "network : 125 .",
    "ramirez ad , paninski l ( 2013 ) fast inference in generalized linear models via expected log - likelihoods .",
    "journal of computational neuroscience : 129 .",
    "diaconis p , freedman d ( 1984 ) asymptotics of graphical projection pursuit .",
    "the annals of statistics 12 : 793815 .",
    "song s , sjstrm pjpj , reigl m , nelson sb , chklovskii db ( 2005 ) highly nonrandom features of synaptic connectivity in local cortical circuits .",
    "plos biology 3 : e68 .",
    "lichtman jw , livet j , sanes jr ( 2008 ) a technicolour approach to the connectome .",
    "nature reviews neuroscience 9 : 41722 .",
    "chung k , wallace j , kim s , kalyanasundaram s , andalman as , et  al .",
    "( 2013 ) structural and molecular interrogation of intact biological systems .",
    "nature 497 : 332337 .",
    "jonas e , kording k ( 2014 ) automatic discovery of cell types and microcircuitry from neural connectomics .",
    "arxiv preprint arxiv:14074137 : 119 .",
    "berndt a , yizhar o , gunaydin la , hegemann p , deisseroth k ( 2009 ) bi - stable neural state switches .",
    "nature neuroscience 12 : 22934 .",
    "wallace d , borgloh sza , astori s ( 2008 ) single - spike detection in vitro and in vivo with a genetic ca2 + sensor .",
    "nature  .",
    "brillinger d ( 1988 ) maximum likelihood analysis of spike trains of interacting nerve cells .",
    "biological cyberkinetics 59 : 189200 .",
    "mishchenko y consistency of the complete neuronal population connectivity reconstructions using shotgun imaging . in prep .",
    "rickgauer jp , deisseroth k , tank dw ( 2014 ) simultaneous cellular - resolution optical perturbation and imaging of place cell firing fields .",
    "nature neuroscience 17 : 18161824 .",
    "shababo b , brooks p , pakman a , paninski l ( 2013 ) bayesian inference and online experimental design for mapping neural microcircuits .",
    "advances in neural information processing systems .",
    "vidne m , ahmadian y , shlens j , pillow jw , kulkarni j , et  al . ( 2012 ) modeling the impact of common noise inputs on the network activity of retinal ganglion cells .",
    "journal of computational neuroscience 33 : 97121 .",
    "gal a , eytan d , wallach a , sandler m , schiller j , et  al .",
    "( 2010 ) dynamics of excitability over extended timescales in cultured cortical neurons .",
    "journal of neuroscience 30 : 1633216342 .",
    "soudry d , meir r ( 2014 ) the neuronal response at extended timescales : long - term correlations without long - term memory .",
    "frontiers in computational neuroscience .",
    "stirzaker d , grimmett d ( 2001 ) probability and random processes .",
    "oxford , 3rd edition .",
    "teh y , newman d , welling m ( 2006 ) a collapsed variational bayesian inference algorithm for latent dirichlet allocation .",
    "advances in neural  .",
    "ribeiro f , opper m ( 2011 ) expectation propagation with factorizing distributions : a gaussian approximation and performance results for simple models .",
    "neural computation 23 : 104769 .",
    "soudry d , hubara i , meir r ( 2014 ) expectation backpropagation : parameter - free training of multilayer neural networks with real and discrete weights . in : nips ( in press ) .",
    "montreal , pp .",
    "tibshirani r ( 1996 ) regression shrinkage and selection via the lasso .",
    "journal of the royal statistical society series b 58 : 267288 .",
    "bach f , jenatton r ( 2011 ) convex optimization with sparsity - inducing norms .",
    "optimization for machine learning : 135 .",
    "beck a , teboulle m ( 2009 ) a fast iterative shrinkage - thresholding algorithm for linear inverse problems .",
    "siam journal on imaging sciences 2 : 183202 .",
    "elad m ( 2010 ) sparse and redundant representations : from theory to applications in signal and image processing .",
    "new york , ny : springer new york .",
    "zhang t ( 2009 ) adaptive forward - backward greedy algorithm for sparse learning with linear models .",
    "advances in neural information processing systems : 18 .",
    "kandel er , schwartz jh , jessell tm ( 2000 ) principles of neural science , volume  3 .",
    "mcgraw hill , 4th edition .",
    "perin r , berger tk , markram h ( 2011 ) a synaptic organizing principle for cortical neuronal groups .",
    "proceedings of the national academy of sciences 108 : 54195424 .",
    "seung h , smbl u ( 2014 ) neuronal cell types and connectivity : lessons from the retina .",
    "neuron 83 : 12621272 .",
    "nowicki k , snijders t ( 2001 ) estimation and prediction for stochastic blockstructures .",
    "journal of the american statistical association 96 : 10771087 .",
    "linderman s , adams r ( 2014 ) discovering latent network structure in point process data .",
    "arxiv 1402.0914 .",
    "fazel m , hindi h , boyd sp ( 2001 ) a rank minimization heuristic with application to minimum order system approximation .",
    "american control conference 6 : 47344739 .",
    "mazumder r , hastie t , tibshirani r ( 2010 ) spectral regularization algorithms for learning large incomplete matrices .",
    "the journal of machine learning  11 : 22872322 .",
    "bishop cm ( 2006 ) pattern recognition and machine learning .",
    "singapore : springer .",
    "liu js ( 1996 ) metropolized independent sampling with comparisons to rejection sampling and importance sampling .",
    "statistics and computing 6 : 113119 .",
    "liu j ( 2002 ) monte carlo strategies in scientific computing .",
    "springer .    mohamed s , heller k , ghahramani z ( 2012 ) bayesian and l1 approaches to sparse unsupervised learning .",
    "proceedings of the 29th international conference on machine learning ( icml-12 ) .",
    "diebolt j , ip e , olkin i ( 1994 ) a stochastic em algorithm for approximating the maximum likelihood estimate .",
    "technical report , stanford university .",
    "wang s , manning cd ( 2013 ) fast dropout training .",
    "( a zero - one matrix ) for a random scanning scheme ( _ left _ ) and its observed pairs * * @xmath223 ( _ right _ ) .",
    "all possible neuron pairs are observed ( _ i.e. _ , @xmath224 for all pairs ) , so this is a valid `` shotgun '' observation scheme .",
    "bottom : the observation matrix @xmath128 for a continuous serial scanning scheme ( _ left _ ) and * * its observed pairs @xmath225 .",
    "since we do not observe spikes pairs for which @xmath226 ( _ i.e. _ , @xmath227 for these pairs ) , this is not a valid `` shotgun '' observation scheme .",
    "[ fig : observation schemes ] ]    ) , and its suggested solution - the shotgun approach . * ( a ) * the true connectivity - the weight matrix @xmath5 of a network with @xmath8 neurons . *",
    "( b ) * a zoomed - in view of the top 16 neurons in a ( white rectangle in a ) . * ( c ) * the same zoomed - in view of the top 16 neurons in the ml estimate of the weight matrix @xmath5 ( eq . [ eq : w_mle ] ) , where we used shotgun observation scheme on the whole network , with a random observation probability of @xmath18 . *",
    "( d ) * the ml estimator of the weight matrix @xmath5 of the top 16 neurons if we observe only these neurons . note the unobserved neurons cause false positives in connectivity estimation .",
    "these `` spurious connections '' do not vanish even though we have a large amount of spike data .",
    "in contrast , the shotgun approach ( c ) , does not have these persistent errors , since it spreads the same number of observations evenly over the network .",
    "@xmath228.[fig : common input problem ] ]     neurons , the experiment length is @xmath229 , and we examine various observation probabilities : @xmath230 . left column - weight matrix , middle column - inferred weight vs. true weight , last column - quality of estimation ( based on the measures in eqs .",
    "[ eq : r]-[eq : s ] ) . in the first row",
    "we have the true weight matrix @xmath5 . in the other rows we have @xmath33 -",
    "the map estimate of the weight matrix with l1 prior ( section [ sub : sparse - prior ] ) , with @xmath180 chosen so that the sparsity of @xmath33 matches that of @xmath5 .",
    "estimation is possible even with very low observation ratios .",
    "[ fig : sparsity ] ]    , where instead of a logistic glm ( eq . [ eq : logistic ] ) , we used a stochastic , discrete time , leaky integrate and fire neuron model . in this model ,",
    "@xmath231 $ ] ( @xmath232 defined in eq .",
    "[ eq : u ] ) , @xmath233 $ ] .",
    "we used @xmath234 as a white noise source . also , we set @xmath235 so that the neuronal timescale would be restricted to a few time bins , as we assumed for simplicity in the current glm model .",
    "we conclude that our estimation method is robust to modeling errors .",
    "[ fig : lif ] ]    , except now @xmath41 , @xmath229 , and @xmath236 . since this estimate can be produced in less then a minute on a standard laptop",
    ", this demonstrates that our algorithm is scalable to networks with thousands of neurons .",
    "[ fig : sparsity-1 ] ]     and observation time @xmath3 .",
    "* we examine the quality ( based on the measures in eqs .",
    "[ eq : r]-[eq : s ] ) of the map estimate of the weight matrix with l1 prior ( section [ sub : sparse - prior ] ) . closely inspecting these figures , we find that performance is approximately maintained constant when @xmath237 is constant ( but less so when @xmath238 ) .",
    "we used an observation probability of @xmath63 , and different values of * @xmath4 * and @xmath3 .",
    "performance was averaged over three repetitions .",
    "[ fig : parameter - scans n - t ] ]     and observation probability @xmath7 . * quality ( based on the measures in eqs .",
    "[ eq : r]-[eq : s ] ) of the map estimate of the weight matrix with l1 prior ( section [ sub : sparse - prior ] ) . closely inspecting these figures",
    ", we find that performance is approximately maintained constant when @xmath239 is constant ( but less so when @xmath21 ) .",
    "we used a network size of @xmath240 and different values of * @xmath7 * and @xmath3 .",
    "performance was averaged over three repetitions.[fig : parameter - scans p_obs t ] ]     and reconstruction correlation @xmath141 ( eq . [ eq : c ] ) .",
    "see section [ fig : the - effect - of - stimulus ] ) for more details .",
    "parameters : @xmath241 , a low firing rate regime .",
    "we find that there is an optimal stimulus magnitude near mean spike probability of @xmath242 .",
    "[ fig : the - effect - of - stimulus ] . ]    , which , when corrupted by noise , gives the connection strength of the non - zero connections . in this figure",
    "we have 2 types - excitatory and inhibitory .",
    "distance dependence ( right ) : the connection probability between neurons declines with distance , which affects which synaptic connections exist .",
    "we randomly set neuron locations @xmath22 on a ring lattice ( so we have more connections near the diagonal ) .",
    "thus , after the neuronal types and the distance dependence produce the connectivity matrix @xmath5 . naturally , the neuronal type also affects connection probability and distance also affects connection amplitude , but here we did not use this information . [",
    "fig : schematic - description ] ]        ) .",
    "each raster plot shows the spikes activity in a circular network with local connectivity ( spikes are in white ) . for neurons @xmath243 and times @xmath244 the spiking activity is unobserved .",
    "we try to estimate this activity using variational bayes method ( vb _ _ , top _ _ ) , or using gibbs sampling ( _ bottom _ ) . in the unobserved rectangle ,",
    "the shade indicates the probability of having a spike - between @xmath245 ( black ) and @xmath40 ( white ) .",
    "each neuron in the network has excitatory connections to its neighbors and self - inhibition .",
    "given this connectivity , we can see both methods give reasonable estimates near the edges of unobserved rectangle .",
    "further from the edges , the estimation becomes less certain and converges to the mean spike probability for that neuron .",
    "[ fig : estimating - the - spikes ] ]",
    "* appendix s1 . * mathematical appendix with full derivations and algorithmic details .",
    ".basic notation[tab : basic - notation ] [ cols= \" < , < \" , ]     code will be available on author s website after publication .",
    "recall eq . [ eq : logistic ] @xmath246 and eq .",
    "[ eq : u ] with @xmath165 @xmath247 combining both together for times @xmath248 and neurons @xmath249 , we obtain    @xmath250\\nonumber \\\\   & = & \\sum_{i=1}^{n}\\sum_{t=1}^{t}\\left[s_{i , t}u_{i , t}-\\ln\\left(1+e^{u_{i , t}}\\right)\\right],\\nonumber \\\\   & = & t\\sum_{i=1}^{n}\\left[\\left\\langle s_{i , t}u_{i , t}\\right\\rangle _ { t}-\\left\\langle \\ln\\left(1+e^{u_{i , t}}\\right)\\right\\rangle _ { t}\\right]\\nonumber \\\\   & \\overset{\\left(1\\right)}{\\approx } & t\\sum_{i=1}^{n}\\left[\\left\\langle s_{i , t}u_{i , t}\\right\\rangle _ { t}-\\int\\ln\\left(1+e^{x}\\right)\\mathcal{n}\\left(x|\\left\\langle u_{i , t}\\right\\rangle _ { t},\\mathrm{var}_{t}\\left(u_{i , t}\\right)\\right)dx\\right]\\nonumber \\\\   & \\overset{\\left(2\\right)}{\\approx } & t\\sum_{i=1}^{n}\\left[\\left\\langle s_{i , t}u_{i , t}\\right\\rangle _ { t}-\\sqrt{1+\\pi\\mathrm{var}_{t}\\left(u_{i , t}\\right)/8}\\ln\\left(1+\\exp\\left(\\frac{\\left\\langle u_{i , t}\\right\\rangle _ { t}}{\\sqrt{1+\\pi\\mathrm{var}_{t}\\left(u_{i , t}\\right)/8}}\\right)\\right)\\right]\\nonumber \\\\   & \\overset{\\left(3\\right)}{\\approx } & t\\sum_{i=1}^{n}\\sum_{j=1}^{n}w_{i , j}\\sigma_{k , j}^{\\left(1\\right)}+m_{i}b_{i}\\label{eq : p(s|w , b ) simplified}\\\\   & - & \\sqrt{1+\\pi\\sum_{k , j}w_{i , j}\\sigma_{k , j}^{\\left(0\\right)}w_{i , k}/8}\\ln\\left(1+\\exp\\left(\\frac{\\sum_{k=1}^{n}w_{i , k}m_{k}+b_{i}}{\\sqrt{1+\\pi\\sum_{k , j}w_{i , j}\\sigma_{k , j}^{\\left(0\\right)}w_{i , k}/8}}\\right)\\right)\\,,\\nonumber \\end{aligned}\\ ] ]    where we used the following    1 .   the neuronal input , as a sum of @xmath4 variables , tends to converge to a gaussian distribution , in the limit of large @xmath4 , under rather general conditions @xcite .",
    "2 .   eq . 8 from @xcite , an approximation which builds upon the following `` empiric '' observation ( figure [ fig : visualization - of - the - wangapprox ] ) @xmath251    .",
    "* we compare of right hand side ( red ) and left hand side ( blue ) of the approximation .",
    "note this approximation breaks if @xmath252 is too large ( _ e.g. _ , @xmath253 ) .",
    "this is because the asymptotic forms of both sides of eq .",
    "[ eq : wang approx ] are different .",
    "for example , it is easy to show that , as @xmath254 , the left hand side converges as @xmath255 while the right hand side converges as @xmath256 .",
    "[ fig : visualization - of - the - wangapprox ] ]    1 .",
    "eq . [ eq : u ] for the neuronal input and eqs .",
    "[ eq : m]-[eq : sigma ] for the spike statistics , which yields @xmath257    though in eq .",
    "[ eq : p(s|w , b ) simplified ] the loglikelihood has already become tractable ( and depends only on the sufficient statistics from eqs .",
    "[ eq : m]-[eq : sigma ] ) , we can simplify it further by maximizing it over @xmath64 . to do so",
    ", we equate to zero the derivative of the simplified loglikelihood ( eq . [ eq : p(s|w , b ) simplified ] ) @xmath258 solving this equation , we obtain @xmath259 so @xmath260 substituting this maximizer into eq .",
    "[ eq : p(s|w , b ) simplified ] , we obtain    @xmath261-h\\left(m_{i}\\right)\\sqrt{1+\\pi\\sum_{k , j}w_{i , j}\\sigma_{k , j}^{\\left(0\\right)}w_{i , k}/8}\\right]\\label{eq : p(s|w , b * ) appendix}\\end{aligned}\\ ] ]    which is eq .",
    "[ eq : p(s|w , b * ) ] .",
    "we examine the profile loglikelihood in eq .",
    "[ eq : p(s|w , b * ) ] , divided by @xmath3 for convenience @xmath262-h\\left(m_{i}\\right)\\sqrt{1+\\frac{\\pi}{8}\\sum_{k , j}w_{i , j}\\sigma_{k , j}^{\\left(0\\right)}w_{i , k}}\\right]\\,,\\ ] ] its gradient is @xmath263 its hessian is @xmath264^{3/2}}\\label{eq : hessian}\\\\   & \\cdot & \\delta_{\\gamma,\\alpha}\\left[\\frac{\\pi}{8}\\sum_{k , j}w_{\\alpha , k}w_{\\alpha , j}\\left(\\sigma_{k,\\beta}^{\\left(0\\right)}\\sigma_{j,\\eta}^{\\left(0\\right)}-\\sigma_{k , j}^{\\left(0\\right)}\\sigma_{\\eta,\\beta}^{\\left(0\\right)}\\right)-\\sigma_{\\eta,\\beta}^{\\left(0\\right)}\\right]\\,.\\nonumber \\end{aligned}\\ ] ]    [ [ concavity ] ] concavity + + + + + + + + + +    in order show that @xmath265 is concave , we need to prove that for any @xmath5 and any matrix @xmath266 @xmath267 therefore , we need to calculate the sign of @xmath268\\\\   & = & \\sum_{\\alpha}\\left[\\frac{\\pi}{8}\\left(\\mathbf{w}_{\\alpha,\\cdot}^{\\top}\\boldsymbol{\\sigma}^{\\left(0\\right)}\\mathbf{z}_{\\alpha,\\cdot}\\right)^{2}-\\left(1+\\frac{\\pi}{8}\\mathbf{w}_{\\alpha,\\cdot}^{\\top}\\boldsymbol{\\sigma}^{\\left(0\\right)}\\mathbf{w}_{\\alpha,\\cdot}\\right)\\mathbf{z}_{\\alpha,\\cdot}^{\\top}\\boldsymbol{\\sigma}^{\\left(0\\right)}\\mathbf{z}_{\\alpha,\\cdot}\\right]\\,.\\end{aligned}\\ ] ] since ( given @xmath3 is large enough ) @xmath269 is positive definite ( since @xmath270 is a covariance matrix ) , we can decompose @xmath271 . denoting @xmath272 and @xmath273",
    "we get that the last line can be written as @xmath274\\\\   & \\leq & -\\sum_{\\alpha}\\boldsymbol{v}_{\\alpha}^{\\top}\\boldsymbol{v}_{\\alpha}\\\\   & \\leq & 0\\,,\\end{aligned}\\ ] ] where in the second line we used the cauchy - schwarz inequality @xmath275 , and in the last we used the fact that @xmath276 for any vector @xmath86 . therefore , @xmath265 is concave .",
    "[ [ lipschitz - constant ] ] lipschitz constant + + + + + + + + + + + + + + + + + +    for the fista algorithm ( appendix [ sec : the - fista - algorithm ] ) we are required to calculate the lipschitz constant of @xmath277 .",
    "a lipschitz constant @xmath278 of a function @xmath279 is defined through @xmath280 we obtain that , in our case , @xmath281\\label{eq : lipshitz constant}\\ ] ] is the lipschitz constant by observing that @xmath282^{2}\\\\   & \\leq & \\sum_{\\alpha,\\beta}\\left[-\\frac{\\pi}{8}h\\left(m_{\\alpha}\\right)\\sum_{k}\\sigma_{k,\\beta}^{\\left(0\\right)}\\left(w_{\\alpha , k}-w_{\\alpha , k}^{\\prime}\\right)\\right]^{2}\\\\   & \\leq & \\left[\\frac{\\pi}{8}\\max_{\\alpha}h\\left(m_{\\alpha}\\right)\\lambda_{\\mathrm{max}}\\left[\\boldsymbol{\\sigma}^{\\left(0\\right)}\\right]\\right]^{2}\\sum_{\\alpha , k}\\left(w_{\\alpha , k}-w_{\\alpha , k}^{\\prime}\\right)^{2}\\,,\\end{aligned}\\ ] ] where @xmath283 $ ] is the maximal eigenvalue of @xmath88 .",
    "we wish to find @xmath5 that solves @xmath284 using eq .",
    "[ eq : grad ] , we obtain @xmath285 we define    @xmath286    so we can write eq .",
    "[ eq : grad l = 00003d0 ] as @xmath287 which is solved by @xmath288 substituting this into eq .",
    "[ eq : q ] , we have @xmath289 so @xmath290",
    "we noticed empirically that using eq .",
    "[ eq : p(s|w , b * ) appendix ] as the simplified loglikelihood tends to cause some inaccuracy in our estimates of the weight gains and biases .",
    "to correct for this error , we re - estimate the gains and biases in the following way .",
    "suppose we obtain a map estimate @xmath33 ( eq . [ eq : ml ] ) after using the above profile likelihood .",
    "next , we examine again the original likelihood ( without any approximation ) @xmath291+c\\,.\\ ] ] we assume that the map estimate is accurate , up to a scaling constant in each row , so @xmath292@xmath293 , and we obtain @xmath294+c\\nonumber \\\\   & = & t\\sum_{i=1}^{n}\\left[a_{i}\\sum_{j=1}^{n}\\hat{w}_{i , j}\\left\\langle s_{i , t}s_{j , t-1}\\right\\rangle _ { t}+b_{j}\\left\\langle s_{i , t}\\right\\rangle _ { t}-\\left\\langle \\ln\\left(1+\\exp\\left(a_{i}\\sum_{j=1}^{n}\\hat{w}_{i , j}s_{j , t-1}+b_{j}\\right)\\right)\\right\\rangle _ { t}\\right]\\label{eq : l(a , b)}\\\\   & \\approx & t\\sum_{i=1}^{n}\\left[a_{i}\\sum_{j=1}^{n}\\hat{w}_{i , j}\\left(\\tilde{\\sigma}_{i , j}^{\\left(1\\right)}+\\tilde{m}_{i}\\tilde{m}_{j}\\right)+b_{j}\\tilde{m}_{i}-\\left\\langle \\ln\\left(1+\\exp\\left(a_{i}z_{i , t}+b_{j}\\right)\\right)\\right\\rangle _ { t}\\right]+c\\,,\\end{aligned}\\ ] ] where in the last line we used the expected loglikelihood approximation with clt again , and denoted ( recall eqs .",
    "[ eq : m]-[eq : sigma ] ) @xmath295 sampling @xmath296 from [ eq : z_i ] ( we found that about @xmath297 samples was usually enough ) , we can calculate the expectation in ( eq . [ eq : l(a , b ) ] ) .",
    "next , we can now maximize the likelihood in ( eq . [ eq : l(a , b ) ] ) for each gain @xmath298 and bias @xmath120 , separately @xmath299 , by solving an easy 2d unconstrained optimization problem .",
    "the new gains can now be used to adjust our estimation of @xmath5 .",
    "we define the proximal operator @xcite @xmath300 fista solves the following minimization problem @xmath301\\,.\\ ] ] from ( * ? ? ?",
    "4.1 - 4.3 ) , we have the following algorithm [ alg : the - fista - algorithm . ] , where the component of the gradient @xmath302 are given by eq .",
    "[ eq : grad ] , and the lipschitz constant @xmath278 is given by eq .",
    "[ eq : lipshitz constant ] .",
    "input : :    initial point    @xmath303,@xmath304 and    @xmath278 ( lipschitz constant of @xmath304 ) .",
    "initialize : :    @xmath305 ,    @xmath306,@xmath307 .",
    "repeat : :    for @xmath308 compute @xmath309\\\\    t_{k+1 } & = & \\left(1+\\sqrt{1 + 4t_{k}^{2}}\\right)/2\\\\    \\mathbf{y}^{\\left(k+1\\right ) } & = & \\mathbf{w}^{\\left(k\\right)}+\\left(\\frac{t_{k}-1}{t_{k+1}}\\right)\\left(\\mathbf{w}^{\\left(k\\right)}-\\mathbf{w}^{\\left(k-1\\right)}\\right)\\,.\\end{aligned}\\ ] ]",
    "as explained in section [ sub : sparse - prior ] , we use a sparsity promoting prior ( eqs .",
    "[ eq : l1 prior]-[eq : lambda mask ] ) , which depends on a regularization constant @xmath180 , to generate and estimate @xmath310 of the connectivity matrix .",
    "though this constant is unknown in advance , we can set it using the sparsity level of * @xmath311 * , defined as @xmath312 we aim for this to be approximately equal to some target sparsity level @xmath6 .",
    "this is done using a fast binary search algorithm ( algorithm [ alg : a - binary - search algorithm ] ) , that exploits the fact that @xmath313 is non - increasing in @xmath180 .",
    "this monotonic behavior can be observed from the fixed point of the fista algorithm @xmath314\\,,\\ ] ] for which the number of zeros components is clearly non - decreasing with @xmath180 .",
    "input : :    target sparsity level - @xmath6 , tolerance    level - @xmath315 , measured sparsity -    @xmath313 .",
    "initialize : :    initial points @xmath316 and    @xmath317 , where    @xmath318 .",
    "repeat : :       +    @xmath319 ; ;         if ; ;      @xmath320      +      return : :        @xmath180    end ; ;         if ; ;      * @xmath321 *      +      @xmath322 : :           else ; ;           +      @xmath323 : :           end ; ;",
    "in this appendix we explain how greedy forward algorithms can be used to solve some of the optimization problems described in section [ sub : priors ] . in this section ,",
    "we use the notation @xmath324 .      in this appendix",
    "we explain in detail how to greedily maximize @xmath325 exactly for each row in the weight matrix @xmath5 by incrementally extending the support of non - zero weights .",
    "since the problem is separable ( and parallelizable ) over the rows ( eq . [ eq : posterior decomposition ] ) , we do this on a single row @xmath326 - @xmath327 . for that row ,",
    "the normalized profile loglikelihood is @xmath328-h\\left(m_{\\alpha}\\right)\\sqrt{1+\\frac{\\pi}{8}\\sum_{k , j}w_{\\alpha , j}\\sigma_{k , j}^{\\left(0\\right)}w_{\\alpha , k}}\\,,\\label{eq : l(w_alpha)}\\ ] ]    first we define the support of our estimate @xmath329 ( the set of non - zero components ) to be @xmath330 , and we initialize @xmath331 .",
    "then , we define @xmath332 , the complement of the support ( the set of zero weights ) , and initialize @xmath333 .",
    "lastly , we define @xmath334 , the set of non - zero weights . since @xmath331 , then we also initialize @xmath335 .    given @xmath336 and @xmath334 from the previous step , we extend the support , by finding for each potential new weight index @xmath337 the best weight    @xmath338    where @xmath339 is simply @xmath340 under the constraint that @xmath341 @xmath342 , and @xmath343 @xmath344 substituting these constraints into eq .",
    "[ eq : l(w_alpha ) ] , we obtain @xmath345 we can find the maximizing values for @xmath346 exactly , as the equation @xmath347 is quadratic in @xmath346 :    @xmath348 with    @xmath349    @xmath350    @xmath351 once we have updated the support by finding @xmath352 , the weight that maximizes this log - likelihood , we can update the support @xmath353 then , we find the maximum likelihood estimate of weights , constrained to the new support , to be    @xmath354    where @xmath355 and",
    "we used similar derivations as in appendix [ sec : maximum - likelihood - estimator ] .",
    "we repeat this process until @xmath356 is within tolerance of our sparsity constraint ( @xmath6 ) .      in the stochastic block model ( section [ sub : stochastic - block - model ] ) , we add a quadratic penalty to the profile loglikelihood ( eq . [ eq : sbm penalty ] ) , which penalizes non - zero weights which are far from some block matrix @xmath74 , which represent the mean value of the non - zero connection strength between the different types of neurons .",
    "we do this by first replacing @xmath339 in eqs .",
    "[ eq : greedy step 1]-[eq : greedy step 2 ] with @xmath357 where @xmath74 represents the underlying block matrix .",
    "in order to maximize this , we again differentiate and equate to zero    @xmath358 this equation , when expanded out , takes the form @xmath359 where we defined @xmath360 since the polynomial is quartic , we can find the optimal weight by running a standard polynomial solver ( the roots ( ) function in matlab ) .",
    "once the support for the row has the desired sparsity , we include this penalty in the calculation of the optimal set of weights , so instead of eq .",
    "[ eq : greedy step 3 ] , we have @xmath361      as explained in section [ sub : distance - dependence ] , we can also incorporate into the model a prior on @xmath362 , the probability of a connection between neurons @xmath132 as a function of the distance between them @xmath196 . to promote the selection of more probable connections using this prior",
    ", we subtract the penalty @xmath363 from @xmath339 in eqs .",
    "[ eq : greedy step 1]-[eq : greedy step 2 ] , or eq .",
    "[ eq : modified penalty ] if we want also to include the stochastic block model prior .",
    "while this penalty modifies the model selection step of extending the support , it has no effect on the values of the weights , determined by eq .",
    "[ eq : modified_w_est ] .      in many situations , we will not know what is the true block model matrix or distance - dependent connectivity function .",
    "however , we can infer these parameters from estimates of the weight matrix , as we explain next .",
    "recall section [ sub : stochastic - block - model ] .",
    "we wish to estimate @xmath74 from @xmath364 by solving ( see eq .",
    "[ eq : sbm penalty ] and explanation below ) @xmath365\\left(\\hat{w}_{i , j}-v_{i , j}\\right)^{2}+\\lambda_{*}\\left\\vert \\mathbf{v}\\right\\vert _ { * } \\,,\\label{eq : low rank estimation}\\ ] ] where the last nuclear norm penalty @xmath366 promotes a low rank solution @xmath367 .",
    "suppose we know the target @xmath368 , _",
    "i.e. _ , the minimal number of neuron types affecting neuronal connectivity . then we can use noisy low - rank matrix completion techniques to estimate @xmath367 from @xmath364 .",
    "specifically , we found that soft - impute ( @xcite , described here in algorithm [ alg : soft - impute ] ) works well in this estimation task , where all components @xmath134 for which @xmath369 , are considered unobserved . in the algorithm we iterate over different values of @xmath194 , starting with a small value ( so initially @xmath370 has a larger rank than desired ) and slowly incrementing , until @xmath367 is of the desired rank @xmath371 .",
    "recall section [ sub : distance - dependence ] .",
    "we assume the distance - dependent connectivity relationship to be of a sigmoidal form @xmath372 if @xmath279 is unknown , we can run logistic regression on the binarized inferred weight matrix to estimate @xmath373 , _",
    "@xmath374(ad_{ij}+b))}{1+\\exp((ad_{ij}+b))}\\\\ \\hat{f } & = & f(d_{ij};\\hat{a},\\hat{b})\\,.\\end{aligned}\\ ] ]      finally , we can combine distance - dependent connectivity and a low - rank mean matrix and infer both @xmath370 and @xmath373 from @xmath33 at the same time . furthermore ,",
    "once we have an estimate for these parameters , we can use them to find a new estimate @xmath375 which in turn , allows us to re - estimate both @xmath370 and @xmath373 .",
    "thus , we can iterate estimating @xmath364 and the penalty parameters @xmath370 and @xmath373 until we reach convergence ( algorithm [ alg : infer_w_unknown_params ] ) .",
    "if our model does not include a low - rank mean matrix or distance - dependent connectivity , we can just set the corresponding penalty coefficient to zero .",
    "the penalty coefficients @xmath190 and @xmath376 are selected by finding each penalty coefficient that maximizes the correlation between @xmath33 and @xmath5 while the other coefficient is set to 0 . on actual data , where the ground truth is unknown , one may instead maximize prediction of observed spikes ( figure [ fig : estimating - the - spikes ] ) or spike correlations @xcite .",
    "we found it is also possible to set the regularization parameters in the case @xmath370 and @xmath373 are inferred from the data , to their optimal value in the case where @xmath74 and @xmath279 are known - multiplied by @xmath377 to account for uncertainty .",
    "define @xmath378 as in eq .",
    "[ eq : l(w_a , b|w_a , q ) ] .",
    "initialize solution @xmath379 .    * for @xmath380",
    "*    initialize support @xmath331    while * @xmath381 *    find the optimal penalized weight index : @xmath382\\ ] ]    update the support : @xmath383    update the weights : @xmath384    * return * @xmath33    define @xmath385 where @xmath386 being the singular value decomposition of @xmath387 , with @xmath388 , and @xmath389 with @xmath390 .",
    "initialize @xmath391 , and some decreasing set * @xmath392 .",
    "*    * for @xmath392 *    @xmath393 @xmath394z_{i , j}\\,.$ ]    @xmath395    * if @xmath396 , return @xmath397 *    * end *    @xmath398    @xmath399    * for @xmath400 *    @xmath401infer_w_known_params(@xmath402    @xmath403@xmath404(ad_{ij}+b))}{1+\\exp((ad_{ij}+b))}$ ]    @xmath405    @xmath406=soft - impute(@xmath407 )    * return @xmath408 *",
    "in this section we give the details of the mcmc approach for inferring the weights ( summarized in section [ sub : markov - chain - monte ] ) .",
    "to do this we alternate between sampling the spikes ( section [ sub : sampling - the - spikes ] ) , and sampling the weights ( section [ sub : sampling - the - weights ] ) .",
    "@xmath410 where we can neglect any additive constant that does not depend on @xmath110 . on the right hand side ,",
    "the first term is @xmath411\\\\   & = & s_{i , t}u_{i , t}+c\\,,\\\\   & = & s_{i , t}\\left(\\sum_{k=1}^{n}w_{i , k}s_{k , t-1}+b_{i}\\right)\\,,\\end{aligned}\\ ] ] while the second term is @xmath412\\\\   & = & \\sum_{j}\\left[s_{j , t+1}u_{j , t+1}-\\ln\\left(1+e^{u_{j , t+1}}\\right)\\right]\\\\   & = & \\sum_{j}\\left[s_{j , t+1}\\left(\\sum_{k=1}^{n}w_{j , k}s_{k , t}+b_{j}\\right)-\\ln\\left(1+\\exp\\left(\\sum_{k=1}^{n}w_{j , k}s_{k , t}+b_{j}\\right)\\right)\\right]+c\\\\   & = & c+\\sum_{j}s_{j , t+1}s_{i , t}w_{j , i}\\\\   & - & \\sum_{j}\\left[\\ln\\left(1+\\exp\\left(\\sum_{k\\neq i}^{n}w_{j , k}s_{k , t}+b_{j}+w_{j , i}\\right)\\right)-\\ln\\left(1+\\exp\\left(\\sum_{k\\neq i}^{n}w_{j , k}s_{k , t}+b_{j}\\right)\\right)\\right]s_{i , t}\\,.\\end{aligned}\\ ] ] therefore , we can sample the spikes from @xmath413 where @xmath414.\\end{aligned}\\ ] ] note that , for a given @xmath13 , @xmath415 depends only on spikes from time @xmath416 and @xmath417 . therefore , @xmath110 samples generated at odd times @xmath94 are independent from samples @xmath418 generated at even times @xmath419 .",
    "therefore , we can sample @xmath110 simultaneously for all odd times @xmath94 , and then sample simultaneously at all even times @xmath94",
    ". such a simple block - wise gibbs sampling scheme can be further accelerated by using the metropolized gibbs method @xcite , in which we propose a `` flip '' of our previous sample .",
    "so if @xmath110 is out previous sample and @xmath420 is our new sample , we propose that @xmath421 and then accept this proposal with probability @xmath422 if the proposal is not accepted , we keep our previous sample @xmath110 .",
    "we denote here @xmath423 to be all the components of @xmath5 without the @xmath23 component , and @xmath424 in order to do gibbs sampling , we need to calculate @xmath425 where , as before , we can neglect on the right hand side any additive constant that does not depend on @xmath23 .",
    "the first term on the right hand side is @xmath426\\\\   & = & \\sum_{i}\\sum_{t}\\left[s_{i , t}u_{i , t}-\\ln\\left(1+e^{u_{i , t}}\\right)\\right],\\\\   & = & \\sum_{i}\\sum_{t}\\left[w_{i , j}s_{i , t}s_{j , t-1}-\\ln\\left(1+\\exp\\left(\\sum_{k=1}^{n}w_{i , k}s_{k , t-1}+b_{i}\\right)\\right)\\right]+c\\\\   & = & \\sum_{i}\\sum_{t}\\left[w_{i , j}s_{i , t}s_{j , t-1}-\\ln\\left(1+\\exp\\left(w_{i , j}s_{j , t-1}\\right)\\exp\\left(\\sum_{k\\neq j}^{n}w_{i , k}s_{k , t-1}+b_{i}\\right)\\right)\\right]+c\\\\   & \\approx & \\sum_{i}\\sum_{t}\\left[w_{i , j}s_{i , t}s_{j , t-1}-\\ln\\left(1+\\left(1+w_{i , j}s_{j , t-1}+\\frac{1}{2}w_{i , j}^{2}s_{j , t-1}\\right)\\exp\\left(\\sum_{k\\neq j}^{n}w_{i , k}s_{k , t-1}+b_{i}\\right)\\right)\\right]+c\\\\   & = & \\sum_{i}\\sum_{t}\\left[w_{i , j}s_{i , t}s_{j , t-1}-\\ln\\left(1+f\\left(\\sum_{k\\neq j}^{n}w_{i ,",
    "k}s_{k , t-1}+b_{i}\\right)\\left(w_{i , j}s_{j , t-1}+\\frac{1}{2}w_{i , j}^{2}s_{j , t-1}\\right)\\right)\\right]+c\\\\   & \\approx & \\sum_{i}\\sum_{t}\\left[w_{i , j}s_{j , t-1}\\left[s_{i , t}-f\\left(\\sum_{k\\neq j}^{n}w_{i , k}s_{k , t-1}+b_{i}\\right)\\right]\\right.\\\\   & - & \\left.\\frac{1}{2}w_{i , j}^{2}s_{j , t-1}\\left[f\\left(\\sum_{k\\neq j}^{n}w_{i , k}s_{k , t-1}+b_{i}\\right)-f^{2}\\left(\\sum_{k\\neq j}^{n}w_{i , k}s_{k , t-1}+b_{i}\\right)\\right]\\right]\\,,\\end{aligned}\\ ] ] where in both approximations we used the fact that a single weight is typically small @xmath427 . therefore , denoting    @xmath428s_{j , t-1}\\\\ \\epsilon_{i , j } & \\triangleq & \\sum_{t}\\left[f\\left(\\sum_{k\\neq j}^{n}w_{i , k}s_{k , t-1}+b_{i}\\right)-f^{2}\\left(\\sum_{k\\neq j}^{n}w_{i , k}s_{k , t-1}+b_{i}\\right)\\right]s_{j , t-1}\\,,\\end{aligned}\\ ] ]            with @xmath433\\\\ h_{i , j } & \\triangleq & h_{0}+\\frac{1}{2}\\ln\\left(\\frac{\\sigma_{i , j}^{2}}{\\sigma_{0}^{2}}\\right)+\\frac{\\mu_{i , j}^{2}}{2\\sigma_{i , j}^{2}}-\\frac{\\mu_{0}^{2}}{2\\sigma_{0}^{2}}\\,.\\end{aligned}\\ ] ] we can than proceed and sample @xmath434 from this spike - and - slab distribution ( in eq .",
    "[ eq : w_ij distribution for gibbs ] ) - sampling @xmath23 simultaneously for all @xmath13 .",
    "now , since we used the approximation that assuming the weights are weak , so this sampling is not exact . therefore , even if the approximation is very good , we can not use direct sampling , or the error will accumulate over time catastrophically . to correct his",
    ", we use this approximation as a proposal distribution in a metropolis hastings simulation scheme @xcite ."
  ],
  "abstract_text": [
    "<S> inferring connectivity in neuronal networks remains a key challenge in statistical neuroscience . </S>",
    "<S> the `` common input '' problem presents the major roadblock : it is difficult to reliably distinguish causal connections between pairs of observed neurons from correlations induced by common input from unobserved neurons . since </S>",
    "<S> available recording techniques allow us to sample from only a small fraction of large networks simultaneously with sufficient temporal resolution , naive connectivity estimators that neglect these common input effects are highly biased .    </S>",
    "<S> this work proposes a `` shotgun '' experimental design , in which we observe multiple sub - networks briefly , in a serial manner . </S>",
    "<S> thus , while the full network can not be observed simultaneously at any given time , we may be able to observe most of it during the entire experiment . using a generalized linear model for a spiking recurrent neural network , </S>",
    "<S> we develop scalable approximate bayesian methods to perform network inference given this type of data , in which only a small fraction of the network is observed in each time bin .    </S>",
    "<S> we demonstrate in simulation that , using this method : ( 1 ) the shotgun experimental design can eliminate the biases induced by common input effects . </S>",
    "<S> ( 2 ) networks with thousands of neurons , in which only a small fraction of the neurons is observed in each time bin , could be quickly and accurately estimated . </S>",
    "<S> ( 3 ) performance can be improved if we exploit prior information about the probability of having a connection between two neurons , its dependence on neuronal cell types ( e.g. , dale s law ) , or its dependence on the distance between neurons . </S>"
  ]
}