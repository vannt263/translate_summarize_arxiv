{
  "article_text": [
    "due to the development of depth sensors , 3d human activity analysis @xcite has attracted more interest than ever before .",
    "recent manifold - based approaches are quite successful at 3d human action recognition thanks to their view - invariant manifold - based representations for skeletal data .",
    "typical examples include shape silhouettes in kendall s shape space @xcite , linear dynamical systems on the grassmann manifold @xcite , histograms of oriented optical flow on a hyper - sphere @xcite , and pairwise transformations of skeletal joints on a lie group @xcite . in this paper , we focus on studying manifold - based approaches @xcite to learn more appropriate lie group representations of skeletal action data , that have achieved state - of - the - art performances for some 3d human action recognition benchmarks .    as studied in @xcite , lie group feature learning methods often suffer from speed variations ( i.e. , temporal misalignment ) , that tend to deteriorate classification accuracy .",
    "to handle this issue , they typically employ dynamic time warping ( dtw ) , as originally used in speech processing @xcite .",
    "unfortunately , such process costs additional time , and also results in a two - step system that typically performs worse than an end - to - end system .",
    "moreover , such lie group representations for action recognition tend to be extremely high - dimensional , in part because the features are extracted per skeletal segment and then stacked . as a result ,",
    "any computation on such non - linear trajectories is expensive and complicated . to address this problem , @xcite attempt to first flatten the underlying manifold via tangent approximation or rolling maps , and then exploit svm or pca - like method to learn features in the resulting flattened space .",
    "although these methods achieve some success , they merely adopt shallow linear learning schemes , yielding sub - optimal solutions on the specific non - linear manifolds .",
    "deep neural networks have shown their great power in learning compact and discriminative representations for images and videos , thanks to their ability to perform non - linear computations and the effectiveness of gradient - descent training with backpropagation .",
    "this has motivated us to build a deep neural network architecture for representation learning on lie groups .",
    "in particular , inspired by the classical manifold learning theory @xcite , we equip the new network structure with rotation mapping layers , with which the input lie group features are transformed to new ones with better alignment . as a result , the effect of speed variations can be appropriately mitigated . in order to reduce the high dimensionality of the lie group features , we design special pooling layers to compose them in terms of bone - based and skeleton - based ones , respectively . as the output data reside on non - linear manifolds",
    ", we also propose a riemannian computation layer , whose outputs could be fed into any regular output layers such as a softmax layer . in short ,",
    "our main contributions are :    * a novel neural network architecture is introduced to deeply learn more desirable lie group representations for the problem of skeleton - based action recognition . *",
    "the proposed network provides a paradigm to incorporate the lie group structure into deep learning , which generalizes the traditional neural network paradigm to non - euclidean lie groups . * to train the network within the backpropagation framework ,",
    "a variant of stochastic gradient descent optimization is exploited in the context of lie groups .",
    "already quite some works @xcite have applied aspects of lie group theory to deep neural networks . for example , @xcite investigated how stability properties of a continuous recursive neural network can be altered within neighbourhoods of equilibrium points by the use of lie group projections operating on the synaptic weight matrix .",
    "@xcite studied the behavior of unsupervised neural networks with orthonormality constraints , by exploiting the differential geometry of lie groups .",
    "in particular , two sub - classes of the general lie group learning theories were studied in detail , tackling first - order ( gradient - based ) and second - order ( non - gradient - based ) learning .",
    "@xcite introduced deep symmetry networks ( symnets ) , a generalization of convolutional networks that forms feature maps over arbitrary symmetry groups that are basically lie groups .",
    "the symnets utilize kernel - based interpolation to tractably tie parameters and pool over symmetry spaces of any dimension .",
    "moreover , recently some deep learning models have emerged @xcite that deal with data in a non - euclidean domain .",
    "for instance , @xcite proposed a spectral version of convolutional networks to handle graphs .",
    "it exploits the notion of non shift - invariant convolution , relying on the analogy between the classical fourier transform and the laplace - beltrami eigenbasis .",
    "@xcite developed a scalable method for treating an arbitrary spatio - temporal graph as a rich recurrent neural network mixture , which can be used to transform any spatio - temporal graph by employing a certain set of well - defined steps .",
    "for shape analysis , @xcite proposed a ` geodesic convolution ' on local geodesic coordinate systems to extract local patches on the shape manifold .",
    "this approach performs convolutions by sliding a window over the manifold , and local geodesic coordinates are used instead of image patches . to deeply learn symmetric positive definite ( spd ) matrices - used in many tasks - @xcite developed a riemannian network on the manifold of spd matrices , with some layers specially designed to deal with such structured matrices .    in summary ,",
    "such works have applied some theories of lie groups to regular networks , and even generalized the common networks to non - euclidean domains . nevertheless , to the best of our knowledge , this is the first work that studies a deep learning architecture on lie groups to handle the problem of skeleton - based action recognition .",
    "let @xmath0 be a body skeleton , where @xmath1 denotes the set of body joints , and @xmath2 indicates the set of edges , i.c . oriented rigid body bones . as studied in @xcite , the relative geometry of a pair of body parts @xmath3 and",
    "@xmath4 can be represented in a local coordinate system attached to the other .",
    "the local coordinate system of body part @xmath3 is calculated by rotating with minimum rotation so that its stating joint becomes the origin and it coincides with the @xmath5-axis .",
    "then we can compute the rotation matrix @xmath6 ( @xmath7 ) from @xmath4 to the local coordinate system of @xmath3 . specifically",
    ", we can firstly calculate the axis - angle representation @xmath8 for the rotation matrix @xmath6 by @xmath9 where @xmath10 are outer and inner products respectively , and @xmath11 indicate the normalized 3d vectors of the two edges @xmath12 respectively .",
    "subsequently , we can transform the axis - angle representation to the rotation matrix @xmath6 . in the same way",
    ", the rotation matrix @xmath13 from @xmath3 to the local coordinate system of @xmath4 can be computed . as a result ,",
    "both @xmath6 and @xmath13 are used to represent the relative geometry between @xmath4 and @xmath3 .",
    "accordingly , a skeleton @xmath14 at time instance @xmath15 is represented by @xmath16 , where @xmath17 is the number of body parts .",
    "note that there are @xmath18 ( @xmath19 is the combination computation ) rotation matrices in total for a skeleton .",
    "the set of @xmath20 rotation matrices in @xmath21 forms the special orthogonal group @xmath22 which is actually a matrix lie group @xcite .",
    "accordingly , each motion sequence of a moving skeleton is represented with a curve on the lie group @xmath23 .",
    "it is known that the matrix lie group is endowed with a riemannian manifold structure that is differentiable .",
    "hence , at each point @xmath24 on @xmath22 , one can derive the tangent space @xmath25 that is a vector space spanned by the set of skew - symmetric matrices .",
    "when the anchor point is the identity matrix @xmath26 , the resulting tangent space is known as the lie algebra @xmath27 . as",
    "the tangent spaces are equipped with the inner product , the riemannian metric on @xmath22 can be defined by the frobenius inner product : @xmath28    the logarithm map @xmath29 and exponential map @xmath30 at @xmath24 on @xmath22 associated with the riemannian metric can be expressed in terms of the usual matrix logarithm @xmath31 and exponential @xmath32 as @xmath33",
    "for the problem of skeleton - based action recognition , we build a deep network architecture to learn the lie group representations for skeletal data .",
    "the new network architecture is dubbed as lienet , where each input is an element on the lie group .",
    "like other convolutional networks ( convnets ) , the lienet also exhibits fully connected convolution - like layers and pooling layers , named rotation mapping ( rotmap ) layers and rotation pooling ( rotpooling ) layers respectively . in particular",
    ", the proposed rotmap layers perform transformations on input rotation matrices to generate new rotation matrices , which have the same manifold property , and are expected to be aligned more accurately for more reliable matching .",
    "the rotpooling layers aim to pool the resulting rotation matrices at a bone - based and a skeleton - based level such that the lie group feature dimensionality can be reduced .",
    "since the rotation matrices reside on non - euclidean manifolds , we have to design a layer named logarithm mapping ( logmap ) layer , to perform the riemannian computations on .",
    "this transforms the rotation matrices into the usual skew - symmetric matrices , which lie in euclidean space and hence can be fed into any regular output layers .",
    "the structure of the proposed lienet is shown in fig.[fig1 ] .      as well - known from classical manifold learning theory @xcite",
    ", one can learn or preserve the original data structure to faithfully maintain geodesic distances for better classification .",
    "accordingly , we design a rotmap layer to transform the input rotation matrices to new ones that are more suitable for the final classification . formally , the rotmap layers adopt a rotation mapping @xmath34 as @xmath35 where @xmath36 ( @xmath17 is the number of body bones in one skeleton , @xmath19 is the combination computation ) , @xmath37 is the input lie group feature ( i.e. , product of rotation matrices ) for one skeleton in the @xmath38-th layer , @xmath39 is the transformation matrix ( connection weights ) , and @xmath40 is the resulting lie group representation . note that although there is only one transformation matrix for each rotation matrix , it would be easily extended with multiple projections for each input . to ensure the matrix @xmath40 becomes a valid product of rotation matrices residing on @xmath41 , the transformation matrices @xmath42 are all basically required to be rotation matrices . in other words ,",
    "both the data space and the weight space on each rotmap layer correspond to one product manifold @xmath41 .",
    "since the rotmap layers are designed to work together with the classification layer , each resulting skeleton representation is tuned for more accurate classification in an end - to - end deep learning manner .",
    "in other words , the major purpose of designing the rotmap layers is to align the lie group representations of a moving skeleton for more faithful matching .      in order to reduce the complexity of deep models",
    ", it is typically useful to reduce the size of the representations to decrease the amount of parameters and computation in the network . for this purpose , it is common to insert a pooling layer in - between successive convolutional layers in a typical convnet architecture .",
    "the pooling layers are often designed to compute statistics in local neighborhoods , such as the average energy or maximum activation .",
    "the same type of layers can be defined in the setting of the proposed network by providing the equivalent notion of neighborhood . since the input and output of this pooling layer",
    "are both rotation matrices , we call this kind of layers as rotation pooling ( rotpooling ) layer . for the rotpooling ,",
    "we propose two different concepts of neighborhood in this work .",
    "the first one is on the bone - based level . as shown in fig.[fig2](a)@xmath43(b ) , we first pool the lie group features on each pair of basic bones @xmath12 in the @xmath44-th frame , which is represented by the two rotation matrices @xmath45 ( here @xmath46 is the order of the layer ) as aforementioned .",
    "then , as depicted in fig.[fig2](b)@xmath43(c ) , we can perform pooling on the adjacent bones that belong to the same group ( here , we can define five part groups , i.e. , torso , two arms and two legs , of the body ) . however",
    ", the second step would inevitably result in a serious spatial misalignment problem , and thus lead to bad matching performances .",
    "therefore , we finally only adopt the first step pooling . in this setting ,",
    "the function of the max pooling is given by @xmath47 where @xmath48 is the representation of the given rotation matrix such as quaternion , euler angle or euler axis - angle .",
    "for example , the euler axis @xmath49 and angle @xmath50 representations are typically calculated by @xmath51 where @xmath52 is the @xmath44-the row , @xmath53-th column element of @xmath13 . unfortunately , except the angle representation , it is non - trivial to define an ordering relation for a quaternion or an axis - angle representation .",
    "accordingly , in this paper , we finally adopt the angle form eqn.[eq6 ] of rotation matrices and its simple ordering relation to calculate the function @xmath48 .",
    "( b)@xmath43(c ) and skeleton - based pooling scheme ( c)@xmath43(d ) . ]    [ fig2 ]    the other pooling scheme is on the skeleton - based level . as shown in fig.[fig2 ] ( c)@xmath43(d ) , the aim of the skeleton - based pooling is to obtain more compact representations for a motion sequence .",
    "this is because a sequence often contains many frames and thus gives rise to the problem of extremely high - dimensional representations .",
    "thus , pooling in the temporal domain could reduce the model complexity .",
    "formally , the function of this kind of max pooling is defined as @xmath54 where @xmath17 is the number of body parts in one skeleton , @xmath55 is the number of skeleton frames for pooling , and the function @xmath56 is defined in the way of eqn.[eq5 ] .",
    "classification of curves on the lie group @xmath23 is a complicated task due to the non - euclidean nature of the underlying space . to address the problem as in @xcite",
    ", we design the logarithm map ( logmap ) layer to flatten the lie group @xmath23 to its lie algebra @xmath57 . accordingly , by using the logarithm map eqn.[eq2 ] , the function of this layer can be defined as @xmath58    one typical approach to calculate the logarithm map is to use the approach @xmath59 , where @xmath60 , @xmath61 is the diagonal matrix of the eigenvalue logarithms . however",
    ", the spectral operation not only suffers from the problem of zeroes occurring in @xmath62 due to the property of the rotation matrix @xmath63 , but also consumes too much time for matrix gradient computation  @xcite .",
    "therefore , we resort to other approaches to perform the function of this layer .",
    "fortunately , we can explore the relationship between the logarithm map and the axis - angle representation as : @xmath64 where @xmath65 is the angle of the rotation matrix @xmath63 . with this equation",
    ", the corresponding matrix gradient can be easily derived by traditional element - wise matrix calculation .      after performing the logmap layer ,",
    "the outputs can be transformed into vector form and concatenated directly frame by frame within one sequence due to their euclidean nature . to handle the euclidean forms",
    ", we can add regular network layers .",
    "for example , the rectified linear unit ( relu ) layer and the regular fully connected ( fc ) layer could be employed .",
    "in particular for the relu layer , we can simply set relatively small elements to zero as done in classical relu . in the fc layer ,",
    "the dimensionality of the weight is set to @xmath66 , where @xmath67 and @xmath68 are the class number and the vector dimensionalities , respectively .",
    "we employ a common softmax layer ( or softmax log - loss ) as the final output layer for the problem of skeleton - based action recognition .",
    "in order to train the proposed lienet , we exploit the stochastic gradient descent ( sgd ) algorithm , which is one of the most popular tools for optimizing deep networks . to begin with ,",
    "let the model of the lienet be represented as a sequence of function compositions @xmath69 with a parameter tuple @xmath70 , where @xmath71 is the function for the @xmath38-th layer , @xmath72 is the weight parameter of the @xmath38-th layer , and @xmath73 is the number of layers .",
    "the loss of the @xmath38-th layer could be defined by a function like @xmath74 , where @xmath75 is the loss function for the final output layer .    to optimize the deep model ,",
    "one classical sgd algorithm needs to compute the gradient of the objective function , which is typically achieved by the backpropagation chain rule .",
    "in particular , the gradients of the weight @xmath72 and the data @xmath76 for the @xmath38-th layer can be respectively computed by the chain rule : @xmath77 where @xmath78 is the class label , @xmath79 .",
    "eqn.[eq12 ] is the gradient for updating @xmath72 , while eqn.[eq13 ] computes the gradients in the layers below to update @xmath76 .",
    "the gradients of the data involved in rotpooling , logmap and regular output layers can be calculated by eqn.[eq13 ] as usual .",
    "in particular , the gradient for the data in rotpooling can be computed with the same gradient computing approach used in a regular max pooling layer in the context of traditional convnets . for the data in the logmap layer",
    ", the gradient can be obtained by the element - wise gradient computation on the involved rotation matrices .     from a point @xmath80 to another point @xmath81 on the @xmath82 manifold . ]    [ fig3 ]    on the other hand , the computation of the gradients of the parameter weights defined in the rotmap layers is non - trivial .",
    "this is because the weight matrices are enforced to be on the riemannian manifold @xmath82 of the rotation matrices , i.e. the lie group . as a consequence , merely using eqn.[eq12 ] to compute their euclidean gradients rather than riemannian gradients in the procedure of backprop would not generate valid rotation weights . to handle this problem ,",
    "we propose a new approach of updating the weights used in eqn.[eq4 ] for the rotmap layers .",
    "as studied in @xcite , the steepest descent direction for the used loss function @xmath83 with respect to @xmath72 on the manifold @xmath82 is the riemannian gradient @xmath84 , which can be obtained by parallel transporting the euclidean gradients onto the corresponding tangent space . for a better intuition , fig.[fig3 ]",
    "shows the process of the parallel transport . in particular , transporting the gradient from a point @xmath80 to another point @xmath81 requires subtracting the normal component @xmath85 at @xmath81 , which can be obtained as follows : @xmath86 where the euclidean gradient @xmath87 is computed by using eqn.[eq12 ] as @xmath88 thanks to the parallel transport",
    ", the riemannian gradient can be calculated by @xmath89    searching along the tangential direction takes the update in the tangent space of the @xmath82 manifold .",
    "then , such update is mapped back to the @xmath82 manifold with a retraction operation .",
    "consequently , an update of the weight @xmath72 on the @xmath82 manifold is of the following form @xmath90 where @xmath80 is the current weight , @xmath91 is the retraction operation , @xmath92 is the learning rate .",
    "in order to measure lienet s performance for skeleton - based action recognition , we conduct experiments on three standard 3d human action datasets .",
    "* g3d - gaming dataset @xcite * contains 663 sequences of 20 different gaming motions such as tennis serve , golf swing , bowling .",
    "each subject performed every action more than two times .",
    "in addition , the 3d locations of 20 joints ( i.e. , 19 bones ) are provided with the dataset .",
    "* hdm05 dataset @xcite * consists of 2,337 sequences of 130 action classes executed by various actors .",
    "most of the motion sequences have been performed several times by all five actors according to the guidelines in a script . as g3d - gaming @xcite dataset ,",
    "3d locations of 31 joints ( i.e. , 30 bones ) of the subjects are provided as well with this dataset .",
    "* ntu rgb+d dataset @xcite * is , to the best of our knowledge , currently the largest 3d action recognition dataset .",
    "different from most datasets , it is collected by kinect v2 and contains more than 56,000 action sequences",
    ". a total of 60 different action classes are performed by 40 subjects .",
    "the 3d coordinates of 25 joints ( i.e. , 24 bones ) are also offered with this dataset . due to its large scale , the dataset is highly suitable for deep learning .      for the feature extraction",
    ", we use the source code of  @xcite to represent a 3d human skeleton with relative 3d geometry relations between all pairs of body parts . as introduced before , each time instance ( frame ) of a 3d human skeleton",
    "can be represented as a point on the lie group @xmath23 .",
    "as preprocessed in @xcite , we normalize any sequence of motion into a fixed @xmath93-length one , with an equal number of samples . as a consequence",
    ", we finally get 100 , 16 , 64 frames for g3d - gaming , hdm05 and ntu rgb - d datasets , respectively . in this",
    "setting , each motion sequence can be represented as a curve of length @xmath93 on the lie group @xmath23 .",
    "as the focus of this work is on skeleton - based action recognition , we mainly utilize manifold - based approaches for comparison .",
    "the two baseline approaches are the special euclidean group ( se ) @xcite and the special orthogonal group ( so ) @xcite representations based on shallow learning methods . for a fair comparison",
    ", we use the source codes from the original authors , and set the involved parameters as in the original papers . for the proposed lienet",
    ", we build its architecture with single or multiple block(s ) of rotmap / rotpooling layers illustrated in fig.[fig1 ] before the three final layers , that are logmap , fc and softmax layers . the learning rate @xmath92 is fixed to 0.01 , the batch size is set to 30 , the weights in the rotmap layers are initialized as random rotation matrices , the number of samples for the skeleton - based level rotpooling layer is set to 4 . for training the lienet",
    ", we just use an i7 - 6700k ( 4.00ghz ) pc without any gpus . as the lienet",
    "gets promising results on all datasets with the same configuration , this shows its insensitivity to the parameter settings .",
    "note that , for the lienet , we do not employ the dynamic time warping ( dtw ) technique @xcite , which has been used in the so and se methods to solve the problem of speed variations .",
    "* g3d - gaming dataset @xcite*. for the dataset , we follow a cross - subject test setting , where half the subjects are used for training and the other half are employed for testing .",
    "all the results reported for this dataset are averaged over ten different combinations of training and testing datasets .",
    "table [ tab1 ] compares the proposed lienet with the state - of - the - art methods ( i.e. , rbm - hmm @xcite , se @xcite and so @xcite ) reported for the g3d - gaming dataset . for fair comparison , we report their results without using the fourier temporal pyramid ( ftp ) post - processing ( their accuracies are 91.09% and 90.94% after using ftp ) . as shown in table [ tab1 ] , the lienet shows its superiority over the two baseline methods so and se . besides , as we can see , our lienet with 3 blocks of rotmap and rotpooling layers achieves the best performance . for this dataset , we also study the performances of different block numbers in the lienet architecture .",
    "as the number of frames in each sequence was fixed to 100 as mentioned before , using 3 blocks has pooled each sequence into 7 frames .",
    "thus , we study 3 blocks at most for our lienet .",
    "as observed from table [ tab1 ] , the improvement by adding more blocks demonstrates the effectiveness of stacking more rotmap / rotpooling blocks .",
    ".experimental results ( recognition accuracies ) on the g3d - gaming database . [ cols=\"<,^\",options=\"header \" , ]     * ntu rgb+d dataset",
    "@xcite*. this dataset has two standard testing protocols .",
    "one is cross - subject test , for which half of the subjects are used for training and the rest is for testing .",
    "the other one is cross - view test , for which two views are employed for training and the rest one is utilized for testing .",
    "since this dataset is large enough to train deep networks , recent works @xcite studied typical recurrent neural networks ( deep rnn and deep lstm ) as well as two variants , i.e. , part - aware ( pa ) and spatio - temporal ( st ) versions of lstm .",
    "the common advantage of these deep networks is to learn temporal information , and to significantly outperform the lie group representation learning methods se and so , which are good at learning spatial information , but are no deep learning models . in this paper ,",
    "our lienet fills the gap by showing the effectiveness of deep learning for the spatial representations . as shown in table",
    "[ tab3 ] , our lienet with more stacked blocks can significantly improve the two baseline methods se and so , which validates the effectiveness of the deep learning . by comparing with the state - of - the - art methods on this database",
    ", our lienet behaves better or equally well as most deep networks ( e.g. , deep rnn and deep lstm ) that exploit temporal information .",
    "the lienet is still outperformed by the recently proposed networks ( pa - lstm and st - lstm ) however , which jointly learn spatial and temporal features for skeletal motion sequences .",
    "this is reasonable because the lienet is mainly designed to learn the spatial features .",
    "we studied a deep network architecture in the domain of lie group features , that is successful for skeleton - based action recognition . in order to handle the key issues of speed variation and high dimensionality of the lie group features , we designed special mapping layers and pooling layers to process the resulting rotation matrices .",
    "in addition , we also exploited logarithm mapping layers to perform riemannian computing on the representations , with which regular output layers are supplied in the new network structure .",
    "the final evaluations on three standard 3d action datasets not only demonstrated the effectiveness of the proposed network , but also compared its different configurations .",
    "moreover , we also showed an interesting visualization for the network , which somewhat discloses its intrinsic mechanism .",
    "as the proposed network is , to the best of our knowledge , the first attempt to perform deep learning on lie groups for skeleton - based action recognition , there are quite a few open issues . for example , studying multiple rotation mappings per rotmap layer and exploiting a relu - like layer in the context of a lie group network are worth paying attention to . besides , building a deeper network , beginning from the raw 3d joint locations up to the lie group features in an end - to - end learning manner , could be more effective .",
    "last but not least , encouraged by the success of the deep spatio - temporal networks @xcite , exploring the potential of the proposed network in the temporal setting would also be an interesting direction .",
    "d.  boscaini , j.  masci , s.  melzi , m.  bronstein , u.  castellani , and p.  vandergheynst . learning class - specific descriptors for deformable shapes using localized spectral convolutional networks . in _",
    "computer graphics forum _ , volume  34 , pages 1323 .",
    "wiley online library , 2015 .",
    "y.  moreau and j.  vandewalle",
    ". a lie algebraic approach to dynamical system prediction . in _ circuits and systems , 1996 .",
    "iscas96 . , connecting the world .",
    ", 1996 ieee international symposium on _ , volume  3 , pages 182185 .",
    "ieee , 1996 ."
  ],
  "abstract_text": [
    "<S> in recent years , skeleton - based action recognition has become a popular 3d classification problem . </S>",
    "<S> state - of - the - art methods typically first represent each motion sequence as a high - dimensional trajectory on a lie group with an additional dynamic time warping , and then shallowly learn favorable lie group features . in this paper </S>",
    "<S> we incorporate the lie group structure into a deep network architecture to learn more appropriate lie group features for 3d action recognition . within the network structure , </S>",
    "<S> we design rotation mapping layers to transform the input lie group features into desirable ones , which are aligned better in the temporal domain . to reduce the high feature dimensionality , </S>",
    "<S> the architecture is equipped with rotation pooling layers for the elements on the lie group . </S>",
    "<S> furthermore , we propose a logarithm mapping layer to map the resulting manifold data into a tangent space that facilitates the application of regular output layers for the final classification . </S>",
    "<S> evaluations of the proposed network for standard 3d human action recognition datasets clearly demonstrate its superiority over existing shallow lie group feature learning methods as well as most conventional deep learning methods . </S>"
  ]
}