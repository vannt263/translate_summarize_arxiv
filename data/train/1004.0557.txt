{
  "article_text": [
    "the phenomenon of universality is common to many disciplines of science and engineering .",
    "a well known example is the central limit theorem which , in a simple version , says the following .",
    "let @xmath0 be a collection of i.i.d .",
    "random variables with mean zero and variance @xmath1 = 1 $ ] .",
    "then @xmath2 where @xmath3 denotes convergence in distribution as @xmath4 , and @xmath5 is a gaussian random variable with mean zero and variance one . in particular",
    ", the central limit theorem implies that the distribution of @xmath6 is asymptotically independent of the details of the distribution of the summands @xmath7 . in other words ,",
    "its limit is `` universal '' for a large class of summands distributions .",
    "other examples include the limiting spectrum of random matrices @xcite , and various properties of statistical mechanics models @xcite .",
    "examples in communications theory where universal properties have been established include the mimo communications problem @xcite . in these problems",
    "it was shown the capacity of the system is independent of the distribution of the fading coefficients and the spreading sequences respectively .",
    "a different research area in which universality ideas appear ubiquitous is compressed sensing .",
    "donoho and tanner @xcite carried out a systematic empirical investigation of universality in this context .",
    "in particular they showed that the phase transition boundary in the sparsity - undersampling tradeoff is universal for a large class of sensing matrices .",
    "the precise location of this phase transition was determined earlier on in the case of gaussian sensing matrices @xcite .",
    "a related phenomenon which we study here is the sparse - dense equivalence . as an example consider a uniformly random regular graph @xmath8 of degree @xmath9 over @xmath10 vertices .",
    "let @xmath11 be the symmetric matrix whose non - vanishing entries correspond to edges in @xmath8 and take values in @xmath12 independently and uniformly at random . as @xmath4 the spectral measure of such",
    "a matrix converges almost surely @xcite to a well defined limit @xmath13 supported on @xmath14 $ ] , where : @xmath15 if we now consider the @xmath16 limit , this distribution converges weakly to the celebrated semi - circle law @xmath17 this is the limiting spectrum of the standard ( dense ) wigner matrices .",
    "we refer to this type of property as to a sparse - dense equivalence .",
    "showing such a relationship can be particularly useful when the analysis of the sparse system is easier than its dense counterpart .",
    "specific examples will be provided below .",
    "universality and sparse - dense equivalence can have far reaching consequences in communications and information theory . in this paper",
    ", we demonstrate this by studying both phenomena within a common framework , and obtaining new results in each of the above mentioned problems .",
    "the main tool that we use is the following generalization of lindeberg s principle that was proved in @xcite .",
    "given @xmath18 , the generalized lindeberg principle provides conditions under which the distribution of @xmath19 is approximately insensitive to the distribution of its arguments @xmath20 which are assumed to be independent .",
    "this generalizes the classical lindeberg proof of the central limit theorem , that focused on @xmath21 .",
    "let us restate here the main result of @xcite .",
    "[ thm : sourav ] let @xmath22 and @xmath23 be two random vectors with mutually independent components . for @xmath24 , define @xmath25 - { \\mathbb{e}}[v_i]|,\\\\ b_i & = |{\\mathbb{e}}[u_i^2 ] - { \\mathbb{e}}[v_i^2]|.\\end{aligned}\\ ] ] and further assume @xmath26 .",
    "suppose @xmath27 is a thrice continuously differentiable function , and for @xmath28 , let @xmath29 be a finite constant such that @xmath30 for each @xmath31 and @xmath32 , where @xmath33 denotes the @xmath34-fold derivative in the @xmath31th coordinate .",
    "then @xmath35 , it gives control on its distribution as well , by applying it to @xmath36 , for @xmath37 belonging to a suitable class of test functions .",
    "in many problems of interest for this paper , the bound on the derivatives of @xmath38 required by the last theorem does not hold , and a more careful analysis is needed . for that purpose",
    "we use the following theorem .",
    "the proof is analogous to the one of theorem  [ thm : sourav ] , and is provided in section  [ sec : proof1 ] .",
    "[ thm : souravmodified ] let @xmath22 and @xmath23 be two random vectors with mutually independent components .",
    "let @xmath39 and @xmath40 be as defined in theorem  [ thm : sourav ] .",
    "then @xmath41 + \\frac{1}{2}b_i { \\mathbb{e}}[|\\partial_i^2f(u_1^{i-1},0,v_{i+1}^n)|]\\\\ & + \\frac{1}{2}{\\mathbb{e}}\\int_0^{u_i } \\big[|\\partial^3_{i}f(u_1^{i-1},s , v_{i+1}^n)|\\big](u_i - s)^2\\ , \\de s\\\\   & + \\frac{1}{2}{\\mathbb{e}}\\int_0^{v_i } \\big[|\\partial^3_{i}f(u_1^{i-1},s , v_{i+1}^n)|\\big](v_i - s)^2\\ , \\de s \\big\\}\\ , .\\end{aligned}\\ ] ]",
    "in this section we discuss the application of theorem [ thm : souravmodified ] to a problem from communications theory ( code division multiple access channels ) , and one from statistical learning theory ( estimation via lasso ) .",
    "we also revisit a standard model from statistical mechanics ( the sherrington - kirkpatrick model ) , and the spectrum of wishart matrices , which is related to capacity of mimo channels . in each of these cases , theorem",
    "[ thm : souravmodified ] implies both universality and sparse - dense equivalence results .",
    "we will not try to be exhaustive , but rather to point out some selected conclusion .",
    "this section contains definitions and statements , while proofs are deferred to section [ sec : proof2 ] .    in the following , we use uppercase letters , e.g , @xmath42 , to denote random variables and their lowercase counterparts , e.g. @xmath43 , to denote realizations of such random variables .",
    "we also use boldface characters to denote random matrices , with the subscript to indicate their dimension , e.g. @xmath44 , @xmath45 .",
    "most of our results concern random matrices with i.i.d .",
    "entries and apply under some simple centering and normalization conditions , provided the entries have finite sixth moment .",
    "rather than repeating these conditions at each of the results below , we introduce them once and for all .",
    "let @xmath46 be a sequence of random matrices indexed by their dimensions @xmath47 and @xmath10 ( with @xmath48 an appropriate sequence of integers ) .",
    "we say that @xmath44 is a random matrix of _ standard type _",
    "if the entries @xmath49 form an array of independent and identically distributed random variables with @xmath50=0 $ ] , @xmath51=1 $ ] and @xmath52\\le { k } < \\infty$ ] , for some @xmath53 independent of @xmath54 .",
    "code division multiple access ( cdma ) is a widely used communication system between multiple users and a common receiver @xcite .",
    "the scheme consists of @xmath10 users modulating their information sequence by a signature sequence ( spreading sequence ) of length @xmath47 and transmitting the resulting signal .",
    "the number @xmath47 is sometimes referred to as the spreading gain or the number of chips per sequence .",
    "the receiver obtains the sum of all transmitted signals and the noise which is often assumed to be white and gaussian ( awgn ) .    for the sake of simplicity",
    ", we will assume antipodal signals : each user wishes to communicate a symbol @xmath55 , to the common receiver .",
    "user @xmath56 uses a signature sequence @xmath57 , with @xmath58 .",
    "the received signal @xmath59 in the @xmath31-th time interval is given by @xmath60 where @xmath61 are i.i.d .",
    "copies of @xmath5 and therefore the noise power is @xmath62 .",
    "we use @xmath63 to denote any specific realization of the transmitted symbols , and will assume that a realization of such symbols is used uniformly at random .",
    "the corresponding random vector is @xmath64 while @xmath65 is the received signal .",
    "typically @xmath66 is chosen to be uniformly distributed over @xmath67 . in this paper",
    "we restrict to this case .",
    "however it is possible to generalize the results below to a large class of distributions for the symbol @xmath7 .",
    "we write @xmath68 for the @xmath69 matrix @xmath70 .",
    "let @xmath71 denote the capacity of such system , i.e. the number of bits per user that can be reliably transmitted to the common receiver under the above constraints .",
    "explicitly we have @xmath72 here expectation @xmath73 is taken over the received signal .",
    "random spreading sequences were initially considered in @xcite . here , the signature sequences are modeled as random vectors with i.i.d .",
    "components @xmath74 .",
    "without loss of generality we can assume @xmath75 and @xmath76 .",
    "we will be interested in the large system limit @xmath77 with @xmath78 fixed .    in order to keep the average power ( per symbol ) equal to @xmath79",
    ", we will rescale the signature matrix by a factor @xmath80 . for a random signature matrix @xmath44",
    ", we consider therefore the capacity @xmath81 , which is itself random .",
    "as proved in @xcite , @xmath81 does in fact concentrate exponentially around its expectation .",
    "this motivates us to focus on its expectation .",
    "[ thm : cdmauniversal ] let @xmath46 and @xmath82 denote two @xmath69 dimensional random spreading matrices of standard type",
    ". then @xmath83 -{\\mathbb{e}}[c_n(m^{-1/2}{{\\mathbf{b}}}_n ) ] \\big\\ } = 0\\ , .\\end{aligned}\\ ] ]    the above theorem establishes that the per - user capacity of a cdma channel is asymptotically independent of the distribution of the spreading sequences .",
    "the conditions required to be satisfied by the distributions are milder than the ones imposed in @xcite .",
    "our next result concerns the sparse - dense equivalence .",
    "sparse signature schemes were proposed in @xcite both as a tool for simplifying mathematical analysis and as a design option with potential practical advantages . given a signature matrix @xmath84 defined as above , its sparsification @xmath85 is given by @xmath86 with @xmath87 a design parameter that is kept fixed in the large system limit . under a sparse signature scheme ,",
    "the power per symbol is normalized to @xmath79 if we rescale the signatures by a factor @xmath88 .",
    "the channel output is therefore @xmath89 we can then prove the following sparse - dense equivalence result .",
    "[ thm : cdmasparsedense ] let @xmath46 and @xmath82 denote two @xmath69 dimensional random spreading matrices of standard type . for @xmath87 , let @xmath90 be the sparsification of @xmath44 .",
    "then @xmath91 -{\\mathbb{e}}[c_n(n^{-1/2}{{\\mathbf{b}}}_n)]\\big\\ } = 0 .\\end{aligned}\\ ] ]    as already mentioned , establishing sparse - dense equivalence is particularly useful when the analysis of a sparse system is simpler than for its dense counterpart . in @xcite",
    "it was shown that there exists @xmath92 such that , for all @xmath93 , @xmath94 = \\min_{m\\in[0,1]}c_{\\rm rs}(q)\\ , , \\end{aligned}\\ ] ] where @xmath95 where @xmath96 denotes expectation with respect to @xmath97 .",
    "the parameter @xmath98 is defined as the largest @xmath99 such that the maximizer in is unique .",
    "numerically @xmath100 .",
    "the same formula was derived earlier by tanaka @xcite using the non - rigorous replica method from statistical physics .    combining this with theorem  [ thm : cdmasparsedense ] we can conclude the following result for the capacity of a random cdma system .",
    "let @xmath44 denote an @xmath69 dimensional random spreading matrix with i.i.d .",
    "assume @xmath50 = 0 $ ] , @xmath101 = 1 $ ] and @xmath52\\leq k < \\infty$ ] .",
    "then for @xmath102 @xmath103 =   \\min_{q\\in[0,1]}c_{\\rm rs}(q).\\end{aligned}\\ ] ]      the lasso ( also known as basis pursuit de - noising ) is a popular strategy in statistical learning , used for reconstructing high - dimensional parameter vectors from noisy measurements @xcite .",
    "it is particularly well suited when the underlying parameters vector is sparse in an appropriate basis . for this very reason ,",
    "it is object of intense study within the compressed sensing literature .",
    "we assume here that a signal @xmath104 is observed through the sensing matrix @xmath68 which has dimensions @xmath105 .",
    "the measurements @xmath106 are modeled as a noisy linear functions @xmath107 with @xmath108 a noise vector .",
    "let the noise vector @xmath109 be i.i.d .",
    "gaussian vector .",
    "the recovery of @xmath110 from @xmath111 is done using the following convex optimization problem @xmath112 for some applications the sensing matrix @xmath68 is not far from random or pseudo - random .",
    "it is important to ask to which degree results obtained for a specific distribution of @xmath68 generalize to other distributions @xcite .",
    "we consider the case in which the entries @xmath113 of @xmath110 are uniformly bounded , i.e. , @xmath114 for some constant @xmath115 independent of @xmath116 .",
    "we further assume that the noise vector @xmath109 has i.i.d .",
    "entries @xmath117 and focus on the limit @xmath118 with @xmath119 fixed .",
    "the next result provides rigorous evidence towards the broader universality picture , by proving universality for the normalized cost @xmath120^n}\\ , \\big\\{\\frac{1}{2}\\vert y   - a_n x\\vert_2 ^ 2+\\lambda\\ , \\vert x\\vert_1 \\big\\}\\ , . \\ ] ]    [ thm : lassouniversal ] let @xmath46 and @xmath82 denote two @xmath69 dimensional random sensing matrices of standard type",
    ". then @xmath121-{\\mathbb{e}}[l(n^{-1/2}{{\\mathbf{b}}}_n)]\\big\\}=0\\ , .\\end{aligned}\\ ] ]      given an @xmath122 symmetric matrix @xmath123 , let @xmath124 denote its eigenvalues .",
    "the spectral measure of @xmath123 is the probability measure @xmath125 the study of the limit of @xmath126 as @xmath4 , for a sequence of random matrices @xmath127 is a central topic in random matrix theory , with important applications in multi - antenna communications .",
    "a well - studied example is the family of wishart matrices . here , @xmath128 , where @xmath44 is an @xmath69 matrix , whose entries are i.i.d .",
    "realizations of a zero mean random variable with variance @xmath79 .    a standard approach to characterizing",
    "the spectral measure is through its stieltjes transform @xcite which is defined as @xmath129 where @xmath130 and @xmath131 is the @xmath10-dimensional identity matrix .",
    "the limiting spectrum of the family @xmath132 can be obtained by computing @xmath133 .",
    "the universality of wishart matrices is a well known result @xcite .",
    "the following is a sparse - dense equivalence result for this class of matrices .",
    "[ thm : wishartsparsedense ] let @xmath46 and @xmath82 denote two @xmath69 dimensional random matrices of standard type . for @xmath87 , let @xmath90 be the sparsification of @xmath44 .",
    "let @xmath134 and @xmath135 .",
    "then for all @xmath130 @xmath136 -{\\mathbb{e}}[s_n({\\mathbf{w}}_{{{\\mathbf{b}}},n},z)]\\big\\ } = 0\\ , .\\end{aligned}\\ ] ]    under appropriate tightness conditions , convergence of stieltjes transforms implies weak convergence of the spectrum @xmath126 , which further implies the convergence of the empirical average @xmath137 for any continuous bounded function @xmath38 . as a particular application of this remark , we consider the capacity of multi - input multi - output ( mimo ) communication systems .",
    "the channel model is very similar to the cdma system discussed in section  [ sec : cdma ] . for a channel input @xmath138 ,",
    "the channel output is a vector @xmath139 in @xmath140 , with components @xmath141 where @xmath61 are i.i.d .",
    "realizations of @xmath5 .",
    "however , in this case it is customary to not restrict the inputs to be @xmath142 , but rather to impose a power constraint @xmath143 .",
    "given a channel gains matrix @xmath144 , the average capacity per input antenna @xcite is then given by @xmath145 when the input covariance is @xmath146 for the case of @xmath147 being i.i.d .",
    "symmetric gaussian random variables it was shown in @xcite that the above maximum is achieved for @xmath148 . here",
    ", we assume that little is known about the channel gains and therefore this covariance matrix is used for other matrices @xmath149 as well . under this assumption , the achievable average rate is given by @xmath150 under the above theorem implies the following result for the mimo channels .",
    "[ cor : mimosparsedense ] let @xmath46 and @xmath82 denote two @xmath69 dimensional random matrices of standard type .",
    "for @xmath87 , let @xmath90 be the sparsification of @xmath44 .",
    "then @xmath151 - { \\mathbb{e}}[c_n(n^{-1/2}{{\\mathbf{b}}}_n)]\\big\\ } = 0\\ , .\\end{aligned}\\ ] ]      spin glass models have been object of intense interest within statistical mechanics , mathematical physics and probability theory . both rigorous and heuristic",
    "techniques from this domain have been applied with success in information theory @xcite .",
    "a number of universality and sparse - dense equivalence results have been proved in this context @xcite .",
    "we re - derive two of these results here because they provide a very simple and instructive illustration of the proof technique that is used in the more intricate examples listed in the previous sections .",
    "we focus in particular on the sherrington - kirkpatrick ( sk ) model .",
    "the model is defined by the hamiltonian function @xmath152 given by @xmath153 for an @xmath122 dimensional matrix @xmath68 and @xmath154 .",
    "an important object of interest in this context is the free entropy density at inverse temperature @xmath155 , which is defined by @xmath156    universality of the free energy for the sk model was established in @xcite and was later extended to general distributions in @xcite . as shown in @xcite the current approach gives a stronger result .",
    "[ thm : skuniversal ] let @xmath157 and @xmath158 be two @xmath122 dimensional random matrices .",
    "assume that both @xmath159 and @xmath160 are collections of i.i.d .",
    "random variables with @xmath50={\\mathbb{e}}[b_{ij}]=0 $ ] , @xmath51={\\mathbb{e}}[b^2_{ij}]=1 $ ] , and @xmath161 , { \\mathbb{e}}[|b_{ij}|^3]\\leq k < \\infty$ ] . then @xmath162-{\\mathbb{e}}[f(\\beta , n^{-1/2}{{\\mathbf{b}}}_n)]\\big\\}=0\\,.\\end{aligned}\\ ] ]    the sparse - dense equivalence was proved in @xcite under the slightly stronger assumption of uniformly bounded entries @xmath163 with even distribution .",
    "[ thm : sksparsedense ] let @xmath157 and @xmath158 be two @xmath122 dimensional random matrices .",
    "assume that both @xmath159 and @xmath160 are collections of i.i.d .",
    "random variables with @xmath50={\\mathbb{e}}[b_{ij}]=0 $ ] , @xmath51={\\mathbb{e}}[b^2_{ij}]=1 $ ] , and @xmath161 , { \\mathbb{e}}[|b_{ij}|^3]\\leq k < \\infty$ ] . for @xmath87 ,",
    "let @xmath90 be the sparsification of @xmath44 .",
    "then @xmath164 -{\\mathbb{e}}[f(\\beta , n^{-1/2}{{\\mathbf{b}}}_n)]\\big\\}=0\\ , .\\end{aligned}\\ ] ]",
    "[ sec : proof1 ]    let @xmath165 denote @xmath166 .",
    "let @xmath167 @xmath168 then @xmath169 - { \\mathbb{e}}[f(v ) ] = \\sum_{i=1}^n ( { \\mathbb{e}}[f({\\overline{w}}_i ) ] - { \\mathbb{e}}[f({\\overline{w}}_{i-1})])\\ , .\\end{aligned}\\ ] ] from the third - order taylor expansion , we have @xmath170 similarly , we get @xmath171 from eq .  ,",
    "using and , we get @xmath172 - { \\mathbb{e}}[f(v ) ] = \\sum_{i=1}^n\\big\\{{\\mathbb{e}}[(u_i - v_i)\\partial_if({\\overline{w}}_i^0 ) ] + \\frac12{\\mathbb{e}}[(u_i^2-v_i^2)\\partial_i^2f({\\overline{w}}_i^0)]\\\\ & + { \\mathbb{e}}\\big[\\frac{1}{2}\\int_0^{u_i}\\partial_i^3f(u_1^{i-1},s , v_{i+1}^n)(u_i - s)^2 \\de s\\big ] + { \\mathbb{e}}\\big[\\frac{1}{2}\\int_0^{v_i}\\partial_i^3f(u_1^{i-1},s , v_{i+1}^n)(v_i - s)^2 \\de s\\big]\\big\\}.\\end{aligned}\\ ] ] the result follows by noting that @xmath173 is independent of @xmath174 .",
    "we will present the proofs starting from the last example , i.e. the sherrington - kirkpatrick model in section [ sec : sk ] . as mentioned , this is a particularly simple example of the general proof strategy .      as mentioned in section  [ sec : sk ] , the hamiltonian for this model is given by @xmath175 where @xmath68 is an @xmath122 dimensional matrix . for a function @xmath176 ,",
    "we denote by @xmath177 its expectation with respect to the probability distribution @xmath178 on @xmath67 .",
    "explicitly : @xmath179 denote by @xmath180 the @xmath56-th partial derivative with respect to @xmath181 ( row @xmath34 , column @xmath182 ) .",
    "a straightforward calculation shows that third derivative @xmath183 is given by @xmath184 which implies @xmath185 ( with @xmath186 defined as in theorem [ thm : sourav ] ) .    from the definition of the random matrices @xmath44 and @xmath45 , we have we have @xmath50 = { \\mathbb{e}}[b_{ij}]$ ] , @xmath101= { \\mathbb{e}}[b_{ij}^2]$ ] and @xmath161\\leq ( 1+k)$ ] , @xmath187\\leq ( 1+k)$ ] . using theorem  [ thm : sourav ]",
    "we get @xmath188}\\big\\{{\\mathbb{e}}\\big[\\frac{|a_{rc}|^3}{n^{3/2}}\\big ] , { \\mathbb{e}}\\big[\\frac{|b_{rc}|^3}{n^{3/2}}\\big]\\big\\}=o\\big(\\frac{1}{\\sqrt n}\\big).\\end{aligned}\\ ] ]    from the definition of the random matrices @xmath189 and @xmath45 , we have we have @xmath50 = { \\mathbb{e}}[b_{ij}]$ ] , @xmath101= { \\mathbb{e}}[b_{ij}^2]$ ] and @xmath190\\leq ( 1+k)\\gamma / n$ ] , @xmath187\\leq ( 1+k)$ ] ( with @xmath53 independent of @xmath191 and @xmath10 ) . therefore using the estimate on @xmath192",
    "fro the previous proof , together with theorem  [ thm : sourav ] we have @xmath193}\\big\\{{\\mathbb{e}}\\big[\\frac{|a^\\gamma_{rc}|^3}{\\gamma^{3/2}}\\big ] , { \\mathbb{e}}\\big[\\frac{|b_{rc}|^3}{n^{3/2}}\\big]\\big\\ } \\leq k'\\beta^3 \\max\\big\\{\\frac{1}{\\sqrt\\gamma},\\frac{1}{\\sqrt n}\\big\\}.\\end{aligned}\\ ] ] therefore , @xmath194-{\\mathbb{e}}[f(n^{-1/2}{{\\mathbf{b}}}_n)]\\big\\ } = 0.$ ]      for any @xmath69 matrix @xmath68 , the capacity can be expressed as @xmath195 where @xmath196 is an @xmath47-dimensional random vector , whose entries are i.i.d .",
    "@xmath197 . by a simple change of variables in the sum over @xmath198",
    ", we get @xmath199 for a matrix @xmath200 , and a vector @xmath201 , define @xmath202 by letting @xmath203_{ij } = a_{ij}x^{{{\\sf in}}}_j$ ] . further , define the hamiltonian function @xmath204 by @xmath205 then we have @xmath206 if @xmath44 is a random matrix of standard type , and @xmath201 , then @xmath207 is also a random matrix of standard type . in order to prove the universality results , theorems [ thm : cdmauniversal ] and [ thm : cdmasparsedense ] ,",
    "it is therefore sufficient to fix say @xmath208 , and prove universality of @xmath209 .",
    "analogously to the proof in the previous section , for a function @xmath210 , we let @xmath211 in order use theorem  [ thm : souravmodified ] we need to estimate the third derivatives of @xmath38 .",
    "again , @xmath212 denote the @xmath56-th derivative of @xmath38 with respect to the @xmath213 . the third derivative is then given by @xmath214    let @xmath44 and @xmath45 be as defined in the theorem .",
    "let @xmath215 denote the matrix with entries @xmath216 from now onwards we use @xmath217 to denote @xmath218 and let @xmath219 denote the corresponding average , as per eq .",
    "( [ eqn : gibbs ] ) .",
    "further , for @xmath220 $ ] , let @xmath221 and @xmath222 .",
    "notice that @xmath223}\\theta_i(x)^2\\ , , \\;\\;\\;\\;\\;\\;\\;\\ ;    { { \\mathcal h}}_{\\sim r}(x )   = \\sum_{i\\in [ m]\\setminus r}\\theta_i(x)^2\\ , . \\ ] ] accordingly , we let @xmath224 denote the average as defined in with the hamiltonian @xmath225 .",
    "the derivative of @xmath217 with respect to @xmath181 is @xmath226 its fourth moment can then be bounded as @xmath227 since the random variables @xmath228 and @xmath229 are negatively correlated , we have @xmath230 which implies @xmath231 using the inequality @xmath232 and the definition of @xmath159 and @xmath160 in theorem  [ thm : cdmauniversal ] , we get @xmath233 + { \\mathbb{e}}[\\<(d_{rc}x_c)^4\\>_{\\sim r}]+ { \\mathbb{e}}[\\<(\\sum_{i\\in [ n]\\setminus c}d_{ri}x_i)^4\\>_{\\sim r}]\\big\\}\\\\ & \\le k_1 + k_1\\ ,",
    "s^4 + k_1\\ , { \\mathbb{e}}[\\<(\\sum_{i\\in [ n]\\setminus c}d_{ri}x_i)^4\\>_{\\sim r}\\ , , \\ ] ] where @xmath234 is a constant independent of @xmath54 .",
    "if we use the subscript @xmath235 to denote all the tuples of distinct indices and we expand the power , we get @xmath236\\setminus c}d_{ri}x_i)^4\\>_{\\sim r } ] & =   \\sum_{i , j , k , l\\in [ n]\\setminus c } { \\mathbb{e}}[d_{ri}d_{rj}d_{rk}d_{rl}\\<x_ix_jx_kx_l\\>_{\\sim r}]\\\\ & = \\sum_{i , j , k , l\\in [ n]\\setminus c } { \\mathbb{e}}[d_{ri}d_{rj}d_{rk}d_{rl}]{\\mathbb{e}}[\\<x_ix_jx_kx_l\\>_{\\sim r}]=\\\\ & = \\sum_{i\\in [ n]\\setminus c } { \\mathbb{e}}[d_{ri}^4]{\\mathbb{e}}[\\<x_i^4\\>_{\\sim r}]+ + 3\\sum_{i\\neq j\\in [ n]\\setminus c}{\\mathbb{e}}[d_{ri}^2d_{rj}^2 ] { \\mathbb{e}}[\\<x_i^2x_j^2\\>_{\\sim r}]\\ , , \\ ] ] here we used the fact that @xmath237 are independent of @xmath225 , and therefore of @xmath238 further all the terms with one of the indices @xmath239 distinct from the all others vanish because @xmath240 for all @xmath241 by our assumption on @xmath44 , @xmath45 . using @xmath242 , we then get @xmath236\\setminus c}d_{ri}x_i)^4\\>_{\\sim r } ] & \\le   \\sum_{i\\in [ n]\\setminus c } \\frac{(1+k)^2}{m^2}\\ , \\cdot 16 + 3\\sum_{i\\neq j\\in [ n]\\setminus c}\\frac{1}{m^2}\\cdot 16\\le k_2 \\ ] ] where @xmath243 is another constant . putting everything together ,",
    "we get @xmath244 and therefore , by jensen inequality , we get @xmath245 ( by eventually enlarging the constant @xmath246 . using eq .",
    "( [ eq : freeenergyderivative ] ) , this finally implies that @xmath247 \\leq \\frac{k_4}{n } ( 1+|s|^3)\\ , . \\ ] ] we are now in position to apply theorem [ thm : souravmodified ] .",
    "since the means and variances of the entries of @xmath44 and @xmath45 are equal , we have @xmath248 .",
    "we get therefore @xmath249 + { \\mathbb{e}}\\big[\\big(\\frac{b_{rc}}{\\sqrt m}\\big)^i\\big]\\big\\ } = o\\big(\\frac{1}{\\sqrt n}\\big).\\end{aligned}\\ ] ]      let @xmath189 and @xmath45 be as defined in the statement .",
    "we modify the definition of @xmath215 used in the last proof , as follows @xmath250 now following the proof of theorem  [ thm : cdmauniversal ] , and assuming without loss of generality @xmath251 , we get again @xmath252 ( the final step consists as in the previous proof , in bounding the sums @xmath253\\setminus c}{\\mathbb{e}}[d_{ri}^4]$ ] and + @xmath254\\setminus c}{\\mathbb{e}}[d_{ri}^2d_{rj}^2 ] { \\mathbb{e}}[\\<x_i^2x_j^2\\>_{\\sim r}]$ ] . ) this in turn implies @xmath255 \\leq ( k_1'/n)(1 + |s|^3)$ ] . since the means and variances of the entries of @xmath85 and @xmath45 are equal , we have @xmath248 .",
    "applying theorem  [ thm : souravmodified ] , we get @xmath256 + { \\mathbb{e}}\\big[\\big(\\frac{b_{rc}}{\\sqrt m}\\big)^i\\big]\\big\\}\\\\ & \\le k_3\\big(\\frac{1}{\\sqrt\\gamma}+\\frac{1}{\\sqrt n}\\big).\\end{aligned}\\ ] ] now taking the limit @xmath4 first and then the limit @xmath257 gives the result .",
    "the proof of theorem  [ thm : lassouniversal ] repeats some arguments already present in the proof of theorem  [ thm : cdmauniversal ] presented in the previous section .",
    "we shall omit such repetitions and instead focus on the new ideas required .    without loss of generality",
    ", we will assume @xmath258 .",
    "define @xmath259 $ ] and , for @xmath260 , define @xmath261 . in words",
    "@xmath262 is a grid of points in the interval @xmath263 $ ] with spacing @xmath264 .",
    "recall that @xmath110 is a fixed deterministic signal with @xmath265 , and the resulting measurements read @xmath266 , where @xmath196 is noise vector with i.i.d .",
    "gaussian component .",
    "define the hamiltonian function @xmath267 by letting @xmath268 with this definition , @xmath269 .",
    "let @xmath270 .",
    "our proof follows by first showing that there exists a constant @xmath271 such that @xmath272 -{\\mathbb{e}}[l(n^{-1/2}{{\\mathbf{a}}}_n)]\\big| \\leq c\\ , \\delta,\\;\\ ; \\big|{\\mathbb{e}}[l_\\delta(n^{-1/2}{{\\mathbf{b}}}_n ) ] -{\\mathbb{e}}[l(n^{-1/2}{{\\mathbf{b}}}_n)]\\big| \\leq c\\ , \\delta\\ , . \\ ] ] obviously @xmath273 . in order to prove the converse bound ,",
    "let @xmath274 be a minimizer of @xmath275 in @xmath276 , and denote by @xmath277 its closest approximation in @xmath278 .",
    "obviously @xmath279 for all @xmath280 $ ] .",
    "we then have @xmath281 where we used @xmath282 , @xmath283 , @xmath284 and @xmath285 . here",
    "@xmath286 is the largest singular value of @xmath68 . from @xcite",
    "we know that @xmath287 < k$ ] , for some constant @xmath288 . combining this with eq .  , and using the cauchy - schwartz inequality",
    ", we get @xmath272 -{\\mathbb{e}}[l(n^{-1/2}{{\\mathbf{a}}}_n)]\\big|   \\leq c\\delta\\ , . \\ ] ] a similar result obviously holds for the matrix ensemble @xmath45 as well . by triangular inequality",
    ", we have @xmath289 -{\\mathbb{e}}[l(n^{-1/2}{{\\mathbf{b}}}_n)]\\big|   \\le   c\\delta + \\lim_{n\\to\\infty}\\big|{\\mathbb{e}}[l_\\delta(n^{-1/2}{{\\mathbf{a}}}_n ) ] - { \\mathbb{e}}[l_\\delta(n^{-1/2}{{\\mathbf{b}}}_n)]\\big|\\ , . \\ ] ] since this inequality holds for any @xmath260 , the proof of the theorem reduces to showing that @xmath290 - { \\mathbb{e}}[l_\\delta(n^{-1/2}{{\\mathbf{b}}}_n)]\\big| = 0 $ ] .    in order to prove this ,",
    "define @xmath291 it is easy to see that @xmath292 further , a straightforward calculation shows that @xmath293 where @xmath294 denotes shannon s entropy of the probability distribution @xmath295 and @xmath296 .",
    "of course @xmath297 whence @xmath298 therefore , @xmath299 - { \\mathbb{e}}[l_\\delta(n^{-1/2}{{\\mathbf{b}}}_n)]\\big| \\nonumber\\\\ & \\stackrel{}{= } \\lim_{n\\to\\infty}\\big|\\lim_{\\beta\\to\\infty}{\\mathbb{e}}[f(\\delta,\\beta , z , n^{-1/2}{{\\mathbf{a}}}_n ) ] -\\lim_{\\beta\\to\\infty}{\\mathbb{e}}[f(\\delta,\\beta , z , n^{-1/2}{{\\mathbf{b}}}_n)]\\big|\\nonumber\\\\ & \\stackrel{}{\\leq}\\lim_{n\\to\\infty}\\big|{\\mathbb{e}}[f(\\delta,\\beta , z , n^{-1/2}{{\\mathbf{a}}}_n ) ] -{\\mathbb{e}}[f(\\delta,\\beta , z , n^{-1/2}{{\\mathbf{b}}}_n)]\\big| + \\int_{\\beta}^\\infty\\frac{1}{s^2}\\log\\big(\\frac{2}{\\delta}\\big)\\,\\ , \\de s\\ , , \\end{aligned}\\ ] ] where the first step follows from and the second from .",
    "notice the close resemblance between the function @xmath300 defined here and the one used in the previous section . using the same arguments developed there for the proof of theorem [ thm : cdmauniversal ] it is immediate to show that @xmath301-{\\mathbb{e}}[f(\\delta,\\beta , z , n^{-1/2 } { { \\mathbf{b}}}_n)]\\big| \\leq o\\big(\\frac{1}{\\sqrt n}\\big)\\ , .",
    "\\ ] ] combining this with eq .  , we get @xmath302 - { \\mathbb{e}}[l_\\delta(n^{-1/2}{{\\mathbf{b}}}_n)]\\big| \\leq\\frac{1}{\\beta } \\log\\big ( \\frac{2}{\\delta}\\big)\\ , . \\ ] ] the proof is completed by letting @xmath303 .",
    "for an @xmath69 matrix @xmath68 and any @xmath305 , let @xmath306 in order to simplify the notation we drop the subscript @xmath10 and denote the partial derivative with respect to @xmath307 by @xmath308 .",
    "define @xmath309 .",
    "therefore @xmath310 , which implies @xmath311 .",
    "this yields @xmath312 let @xmath313 denote the matrix with @xmath314-th entry equal to @xmath79 and the remaining entries equal to @xmath315 .",
    "then @xmath316 using the identity @xmath317 , we get @xmath318 note that @xmath319 is a symmetric matrix and therefore is diagonalizable",
    ". moreover , note that the singular values of @xmath320 are bounded by @xmath321 , where @xmath322 .",
    "let @xmath323 and @xmath324 denote the frobenius norm and the spectral norm of @xmath325 respectively . from cauchy - schwartz inequality we have @xmath326 .",
    "therefore , we can bound the first term as @xmath327 where we have used @xmath328 in @xmath329 and @xmath330 in both @xmath329 and @xmath331 .",
    "similarly one can bound the second and third terms of as @xmath332 finally , we can bound @xmath333 as follows @xmath334 let us now consider the random matrices @xmath189 and @xmath45 as defined in the theorem .",
    "let @xmath335 denote the matrix as defined in section  [ sec : cdmaproof ] , i.e. , @xmath336 using the equations , , , and , we get @xmath337 the proof is finished as for theorem  [ thm : cdmasparsedense ] .        convergence of stieltjes transform implies weak convergence of the expected distribution of eigenvalues ( * ? ? ?",
    "* theorem  2.4.4 ) .",
    "this means that for any continuous bounded function @xmath38 is equivalent to saying that @xmath340 along any sequence of @xmath341s satisfying @xmath342 . ]",
    "@xmath343 =   \\lim_{n\\to\\infty } \\frac{1}{n}\\sum_{i=1}^n { \\mathbb{e}}[f(\\lambda_i(n^{-1}{{\\mathbf{b}}}_n^\\top{{\\mathbf{b}}}_n))]\\ , .\\label{eq : weakconvergence}\\end{aligned}\\ ] ] the limit on the right hand side exists because the expected distribution of eigenvalues of wishart matrices converges @xcite",
    ". moreover , the limiting distribution function is continuous .",
    "therefore , the convergence of the distributions implies the convergence of expectations for any bounded measurable function , not necessarily continuous ( by the bounded convergence theorem ) .",
    "we are interested in estabilishing a result of the form ( [ eq : weakconvergence ] ) for the function @xmath344 , which is not bounded .",
    "however , note that only the behavior of @xmath38 in the region @xmath345 is relevant , because @xmath346 . in the domain of interest",
    "the function @xmath38 is bounded from below . in order to tackle the issue of boundedness from above ,",
    "we use a standard truncation trick .",
    "we define @xmath347 , for some @xmath348 . note that the function @xmath349 is bounded on @xmath350 . therefore @xmath351",
    "=   \\lim_{n\\to\\infty } \\frac{1}{n}\\sum_{i=1}^n { \\mathbb{e}}[g_m(\\lambda_i(n^{-1}{{\\mathbf{b}}}_n^\\top{{\\mathbf{b}}}_n))]\\ , .\\end{aligned}\\ ] ] note that @xmath352 for a constant @xmath53 independent of @xmath353 , @xmath191 as long as @xmath251 . using a similar argument",
    "we can show that @xmath354 -f(\\lambda_i(n^{-1}{{\\mathbf{b}}}_n^\\top{{\\mathbf{b}}}_n))|\\}\\ \\leq \\frac{k'}{m}\\end{aligned}\\ ] ] for a constant @xmath355 independent of @xmath353 . from , , we get @xmath356 - { \\mathbb{e}}[c_n(n^{-1/2}{{\\mathbf{b}}}_n)]\\big| \\leq \\frac{k+k'}{m}\\ , .\\end{aligned}\\ ] ] now taking the @xmath357 gives the desired result .              d.  l. donoho and j.  tanner , `` observed universality of phase transitions in high - dimensional geometry , with implications for modern data analysis and signal processing , '' phil .",
    "a 13 , november 2009 , 4273 - 4293                a.  montanari and d.  tse , `` analysis of belief propagation for non - linear problems : the example of cdma ( or : how to prove tanaka s formula ) , '' in _ proc . of the ieee inform .",
    "theory workshop _ , punta del este , uruguay , mar 13mar 17 2006 ."
  ],
  "abstract_text": [
    "<S> we use a generalization of the lindeberg principle developed by sourav chatterjee to prove universality properties for various problems in communications , statistical learning and random matrix theory . </S>",
    "<S> we also show that these systems can be viewed as the limiting case of a properly defined sparse system . </S>",
    "<S> the latter result is useful when the sparse systems are easier to analyze than their dense counterparts . </S>",
    "<S> the list of problems we consider is by no means exhaustive . </S>",
    "<S> we believe that the ideas can be used in many other problems relevant for information theory . </S>"
  ]
}