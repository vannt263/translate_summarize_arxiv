{
  "article_text": [
    "we consider the reinforcement learning ( rl ) problem of optimizing rewards in an unknown markov decision process ( mdp ) @xcite . in this",
    "setting an agent makes sequential decisions within its enironment to maximize its cumulative rewards through time .",
    "we model the environment as an mdp , however , unlike the standard mdp planning problem the agent is unsure of the underlying reward and transition functions . through exploring poorly - understood policies ,",
    "an agent may improve its understanding of its environment but it may improve its short term rewards by exploiting its existing knowledge @xcite .",
    "the focus of the literature in this area has been to develop algorithms whose performance will be close to optimal in some sense . there are numerous criteria for statistical and computational efficiency that might be considered .",
    "some of the most common include pac ( probably approximately correct ) @xcite , mb ( mistake bound ) @xcite , kwik ( knows what it knows ) @xcite and regret @xcite .",
    "we will focus our attention upon regret , or the shortfall in the agent s expected rewards compared to that of the optimal policy .",
    "we believe this is a natural criteria for performance during learning , although these concepts are closely linked . a good overview of various efficiency guarantees is given in section 3 of li et al .",
    "@xcite .",
    "broadly , algorithms for rl can be separated as either model - based , which build a generative model of the environment , or model - free which do not .",
    "algorithms of both type have been developed to provide pac - mdp bounds polynomial in the number of states @xmath4 and actions @xmath5 @xcite .",
    "however , model - free approaches can struggle to plan efficient exploration .",
    "the only near - optimal regret bounds to time @xmath1 of @xmath6 have only been attained by model - based algorithms @xcite .",
    "but even these bounds grow with the cardinality of the state and action spaces , which may be extremely large or even infinite .",
    "worse still , there is a lower bound @xmath7 for the expected regret in an arbitrary mdp @xcite .    in special cases , where the reward or transition function is known to belong to a certain functional family",
    ", existing algorithms can exploit the structure to move beyond this  ` tabula rasa' ( where nothing is assumed beyond @xmath4 and @xmath5 ) lower bound .",
    "the most widely - studied parameterization is the degenerate mdp with no transitions , the mutli - armed bandit @xcite .",
    "another common assumption is that the transition function is linear in states and actions .",
    "papers here establigh regret bounds @xmath8 for linear quadratic control @xcite , but with constants that grow exponentially with dimension .",
    "later works remove this exponential dependence , but only under significant sparsity assumptions @xcite .",
    "the most general previous analysis considers rewards and transitions that are @xmath9-hlder in a @xmath10-dimensional space to establish regret bounds @xmath11 @xcite .",
    "however , the proposed algorithm uccrl is not computationally tractable and the bounds approach linearity in many settings .    in this paper we analyse the simple and intuitive algorithm _ posterior sampling for reinforcement learning _ ( psrl ) @xcite .",
    "psrl was initially introduced as a heuristic method @xcite , but has since been shown to satisfy state of the art regret bounds in finite mdps @xcite and also exploit the structure of factored mdps @xcite .",
    "we show that this same algorithm satisfies general regret bounds that depends upon the dimensionality , rather than the cardinality , of the underlying reward and transition function classes . to characterize the complexity of this learning problem we extend the definition of the eluder dimension , previously introduced for bandits @xcite , to capture the complexity of the reinforcement learning problem .",
    "our results provide a unified analysis of model - based reinforcement learning in general and provide new state of the art bounds in several important problem settings .",
    "we consider the problem of learning to optimize a random finite horizon mdp @xmath12 in repeated finite episodes of interaction .",
    "@xmath13 is the state space , @xmath14 is the action space , @xmath15 is the reward distribution over @xmath16 and @xmath17 is the transition distribution over @xmath13 when selecting action @xmath18 in state @xmath19 , @xmath20 is the time horizon , and @xmath21 the initial state distribution .",
    "all random variables we will consider are on a probability space @xmath22 .    a policy @xmath23 is a function mapping each state @xmath24 and @xmath25 to an action @xmath26 . for each mdp @xmath27 and policy @xmath23 , we define a value function @xmath28 : @xmath29\\ ] ] where @xmath30 $ ] and the subscripts of the expectation operator indicate that @xmath31 , and @xmath32 for @xmath33 .",
    "a policy @xmath23 is said to be optimal for mdp @xmath27 if @xmath34 for all @xmath24 and @xmath35 .",
    "we will associate with each mdp @xmath27 a policy @xmath36 that is optimal for @xmath27 .",
    "we require that the state space @xmath13 is a subset of @xmath37 for some finite @xmath10 with a @xmath38-norm induced by an inner product .",
    "these result actually extend to general hilbert spaces , but we will not deal with that in this paper .",
    "this allows us to decompose the transition function as a mean value in @xmath13 plus additive noise @xmath39 . at first this may seem to exclude discrete mdps with @xmath4 states from our analysis .",
    "however , we can represent the discrete state as a probability vector @xmath40^s \\subset { \\mathds{r}}^s$ ] with a single active component equal to 1 and 0 otherwise .",
    "in fact , the notational convention that @xmath41 should not impose a great restriction for most practical settings .    for any distribution @xmath42 over @xmath13 , we define the one step future value function @xmath43 to be the expected value of the optimal policy with the next state distributed according to @xmath42 .",
    "@xmath44.\\ ] ] one natural regularity condition for learning is that the future values of similar distributions should be similar .",
    "we examine this idea through the lipschitz constant on the means of these state distributions .",
    "we write @xmath45 \\in { \\mathcal{s}}$ ] for the mean of a distribution @xmath42 and express the lipschitz continuity for @xmath46 with respect to the @xmath47-norm of the mean : @xmath48 to be a global lipschitz contant for the future value function with state distributions from @xmath49 .",
    "where appropriate , we will condense our notation to write @xmath50 where @xmath51 is the set of all possible one - step state distributions under the mdp @xmath27 .",
    "the reinforcement learning agent interacts with the mdp over episodes that begin at times @xmath52 , @xmath53 .",
    "let @xmath54 denote the history of observations made _ prior _ to time @xmath55 .",
    "a reinforcement learning algorithm is a deterministic sequence @xmath56 of functions , each mapping @xmath57 to a probability distribution @xmath58 over policies which the agent will employ during the @xmath59th episode .",
    "we define the regret incurred by a reinforcement learning algorithm @xmath60 up to time @xmath1 to be @xmath61 where @xmath62 denotes regret over the @xmath59th episode , defined with respect to the mdp @xmath63 by @xmath64 with @xmath65 and @xmath66 .",
    "note that regret is not deterministic since it can depend on the random mdp @xmath63 , the algorithm s internal random sampling and , through the history @xmath57 , on previous random transitions and random rewards .",
    "we will assess and compare algorithm performance in terms of regret and its expectation .",
    "we now review the algorithm psrl , an adaptation of thompson sampling @xcite to reinforcement learning .",
    "psrl was first proposed by strens @xcite and later was shown to satisfy efficient regret bounds in finite mdps @xcite .",
    "the algorithm begins with a prior distribution over mdps . at the start of episode @xmath59 ,",
    "psrl samples an mdp @xmath67 from the posterior .",
    "psrl then follows the policy @xmath68 which is optimal for this _ sampled _ mdp during episode @xmath59 .",
    "* input : * prior distribution @xmath69 for @xmath63 , t=1    to state our results we first introduce some notation . for any set @xmath70 and @xmath71 for @xmath10 finite",
    "let @xmath72 be the family the distributions from @xmath70 to @xmath73 with mean @xmath47-bounded in @xmath74 $ ] and additive @xmath75-sub - gaussian noise .",
    "we let @xmath76 be the @xmath9-covering number of @xmath77 with respect to the @xmath47-norm and write @xmath78 for brevity .",
    "finally we write @xmath79 for the eluder dimension of @xmath77 at precision @xmath80 , a notion of dimension specialized to sequential measurements described in section [ sec : eluder ] .    our main result , theorem [ thm : main regret ] , bounds the expected regret of psrl at any time @xmath1",
    ".    [ thm : main regret ] fix a state space @xmath13 , action space @xmath14 , function families @xmath81 and @xmath82 for any @xmath83 .",
    "let @xmath63 be an mdp with state space @xmath13 , action space @xmath14 , rewards @xmath84 and transitions @xmath85 .",
    "if @xmath69 is the distribution of @xmath63 and @xmath86 is a global lipschitz constant for the future value function as per then : @xmath87 \\le \\big [ c_{\\mathcal{r}}+ c_{\\mathcal{p}}\\big ] + \\tilde{d}({\\mathcal{r } } )      + + { \\mathds{e}}[k^*]\\left(1+\\frac{1}{t-1}\\right ) \\tilde{d}({\\mathcal{p}})\\end{aligned}\\ ] ] where for @xmath77 equal to either @xmath88 or @xmath89 we will use the shorthand : + @xmath90 .",
    "theorem [ thm : main regret ] is a general result that applies to almost all rl settings of interest . in particular",
    ", we note that any bounded function is sub - gaussian .",
    "to clarify the assymptotics if this bound we use another classical measure of dimensionality .",
    "[ def : kol ] the kolmogorov dimension of a function class @xmath77 is given by : @xmath91    using definition [ def : kol ] in theorem [ thm : main regret ] we can obtain our corollary .",
    "[ cor : ass regret ] under the assumptions of theorem [ thm : main regret ] and writing @xmath92 : @xmath93 = \\tilde{o } \\left ( \\ \\sigma_{\\mathcal{r}}\\sqrt{d_k({\\mathcal{r } } ) d_e({\\mathcal{r } } ) t }          + { \\mathds{e}}[k^ * ] \\sigma_{\\mathcal{p}}\\sqrt{d_k({\\mathcal{p } } ) d_e({\\mathcal{p } } ) t } \\",
    "\\right)\\ ] ] where @xmath94 ignores terms logarithmic in @xmath1 .",
    "in section [ sec : eluder ] we provide bounds on the eluder dimension of several function classes . these lead to explicit regret bounds in a number of important domains such as discrete mdps , linear - quadratic control and even generalized linear systems . in all of these cases",
    "the eluder dimension scales comparably with more traditional notions of dimensionality . for clarity ,",
    "we present bounds in the case of linear - quadratic control .",
    "[ cor : lqr ] let @xmath63 be an @xmath95-dimensional linear - quadratic system with @xmath75-sub - gaussian noise .",
    "if the state is @xmath38-bounded by @xmath96 and @xmath69 is the distribution of @xmath63 , then : @xmath93 = \\tilde{o } \\left ( \\sigma c \\lambda_1 n^2 \\sqrt{t } \\ \\right).\\ ] ] here @xmath97 is the largest eigenvalue of the matrix @xmath98 given as the solution of the ricatti equations for the unconstrained optimal value function @xmath99 @xcite .",
    "we simply apply the results of for eluder dimension in section [ sec : eluder ] to corollary [ cor : ass regret ] and upper bound the lipschitz constant of the constrained lqr by @xmath100 , see appendix [ app : bounded lqr ] .",
    "algorithms based upon posterior sampling are intimately linked to those based upon optimism @xcite . in appendix",
    "[ app : ucrl - eluder ] we outline an optimistic variant that would attain similar regret bounds but with high probility in a frequentist sense .",
    "unfortunately this algorithm remains computationally intractable even when presented with an approximate mdp planner .",
    "further , we believe that psrl will generally be more statistically efficient than an optimistic variant with similar regret bounds since the algorithm is not affected by loose analysis @xcite .",
    "to quantify the complexity of learning in a potentially infinite mdp , we extend the existing notion of eluder dimension for real - valued functions @xcite to vector - valued functions . for any @xmath101 we define the set of mean functions @xmath102 : = \\{f | f={\\mathds{e}}[g ] \\text { for } g \\in { \\mathcal{g}}\\}$ ] . if we consider sequential observations @xmath103 we can equivalently write them as @xmath104 for some @xmath105 $ ] and @xmath106 zero mean noise . intuitively , the eluder dimension of @xmath77 is the length @xmath10 of the longest possible sequence @xmath107 such that for all @xmath108 , knowing the function values of @xmath109 will not reveal @xmath110 .",
    "we will say that @xmath111 is @xmath112-dependent on @xmath113 @xmath114 @xmath111 is @xmath115-independent of @xmath116 iff it does not satisfy the definition for dependence .",
    "[ def : eluder ] the eluder dimension @xmath117 is the length of the longest possible sequence of elements in @xmath70 such that for some @xmath118 every element is @xmath119-independent of its predecessors .    traditional notions from supervised learning , such as the vc dimension , are not sufficient to characterize the complexity of reinforcement learning . in fact , a family learnable in constant time for supervised learning may require arbitrarily long to learn to control well @xcite .",
    "the eluder dimension mirrors the linear dimension for vector spaces , which is the length of the longest sequence such that each element is linearly independent of its predecessors , but allows for nonlinear and approximate dependencies .",
    "we overload our notation for @xmath101 and write @xmath120,\\epsilon)$ ] , which should be clear from the context .",
    "theorem [ thm : main regret ] gives regret bounds in terms of the eluder dimension , which is well - defined for any @xmath121 .",
    "however , for any given @xmath122 actually calculating the eluder dimension may take some additional analysis .",
    "we now provide bounds on the eluder dimension for some common function classes in a similar approach to earlier work for real - valued functions @xcite .",
    "these proofs are available in appendix [ app : eluder dims ] .",
    "a counting argument shows that for @xmath123 finite , any @xmath124 and any function class @xmath77 : @xmath125 this bound is tight in the case of independent measurements .",
    "[ prop : eluder lin ] let @xmath126 then @xmath127 : @xmath128 + 1 = \\tilde{o}(np)\\ ] ]    let @xmath129 then @xmath130 : @xmath131 + 1 = \\tilde{o}(p^2).\\ ] ]    let @xmath132 be a component - wise independent function on @xmath133 with derivative in each component bounded @xmath134 $ ] with @xmath135 . define @xmath136 to be the condition number . if @xmath137 then for any @xmath70 : @xmath138 \\right ) + 1 = \\tilde{o}(r^2 np)\\ ] ]",
    "we now follow the standard argument that relates the regret of an optimistic or posterior sampling algorithm to the construction of confidence sets @xcite .",
    "we will use the eluder dimension build confidence sets for the reward and transition which contain the true functions with high probability and then bound the regret of our algorithm by the maximum deviation within the confidence sets . for observations from @xmath139",
    "we will center the sets around the least squares estimate @xmath140 where @xmath141 is the cumulative squared prediciton error .",
    "the confidence sets are defined @xmath142 where @xmath143 controls the growth of the confidence set and the empirical 2-norm is defined @xmath144 .    for @xmath145",
    ", we define the distinguished control parameter : @xmath146 this leads to confidence sets which contain the true function with high probability .",
    "[ prop : conf sets ] for all @xmath147 and @xmath148 and the confidence sets @xmath149 for all @xmath150 then : @xmath151    we combine standard martingale concentrations with a discretization scheme .",
    "the argument is essentially the same as proposition 6 in @xcite , but extends statements about @xmath16 to vector - valued functions .",
    "a full derivation is available in the appendix [ app : conf sets ] .",
    "we now bound the deviation from @xmath152 by the maximum deviation within the confidence set .",
    "for any set of functions @xmath77 we define the width of the set at @xmath153 to be the maximum l2 deviation between any two members of @xmath77 evaluated at @xmath153 .",
    "@xmath154    we can bound for the number of large widths in terms of the eluder dimension .    [ lem : big widths ] if @xmath155 is a nondecreasing sequence with @xmath156 then @xmath157    this result follows from proposition 8 in @xcite but with a small adjustment to account for episodes . a full proof is given in appendix [ app : large widths ] .",
    "we now use lemma [ lem : big widths ] to control the cumulative deviation through time .",
    "[ prop : widths ] if @xmath155 is nondecreasing with @xmath156 and @xmath158 for all @xmath159 then : @xmath160    once again we follow the analysis of russo @xcite and strealine notation by letting @xmath161 abd @xmath162 . reordering the sequence @xmath163 such that @xmath164 we have that : @xmath165 .    by the reordering we know",
    "that @xmath166 means that @xmath167 . from lemma [ lem : big widths ] , @xmath168 . so that if @xmath169 then @xmath170",
    ". therefore , @xmath171",
    "we will now show reproduce the decomposition of expected regret in terms of the bellman error @xcite . from here",
    ", we will apply the confidence set results from section [ sec : conf ] to obtain our regret bounds .",
    "we streamline our discussion of @xmath172 and @xmath173 by simply writing @xmath174 in place of @xmath63 or @xmath175 and @xmath59 in place of @xmath67 or @xmath176 where appropriate ; for example @xmath177 .    the first step in our ananlysis breaks down the regret by adding and subtracting the _ imagined _ optimal reward of @xmath176 under the mdp @xmath67 . @xmath178 here @xmath179 is a distinguished initial state , but moving to general @xmath180 poses no real challenge .",
    "algorithms based upon optimism bound @xmath181 with high probability .",
    "for psrl we use lemma [ lem : ps ] and the tower property to see that this is zero in expectation .",
    "[ lem : ps ] if @xmath69 is the distribution of @xmath63 then , for any @xmath182-measurable function @xmath183 , @xmath184 = { \\mathds{e}}[g(m_k ) | h_{t_k } ] \\ ] ]    we introduce the bellman operator @xmath185 , which for any mdp @xmath12 , stationary policy @xmath186 and value function @xmath187 , is defined by @xmath188 this returns the expected value of state @xmath19 where we follow the policy @xmath23 under the laws of @xmath27 , for one time step .",
    "the following lemma gives a concise form for the dynamic programming paradigm in terms of the bellman operator .",
    "[ lem : dpl ] for any mdp @xmath12 and policy @xmath189 , the value functions @xmath190 satisfy @xmath191 for @xmath192 , with @xmath193 .    through repeated application of the dynamic programming operator and taking expectation of martingale differences we can mirror earlier analysis @xcite to equate expected",
    "regret with the cumulative bellman error : @xmath194 = \\sum_{i=1}^\\tau ( { \\mathcal{t}}^k_{k , i}-{\\mathcal{t}}^*_{k , i})v^k_{k , i+1}(s_{t_k+i})\\ ] ]      efficient regret bounds for mdps with an infinite number of states and actions require some regularity assumption .",
    "one natural notion is that nearby states might have similar optimal values , or that the optimal value function function might be lipschitz .",
    "unfortunately , any discontinuous reward function will usually lead to discontious values functions so that this assumption is violated in many settings of interest .",
    "however , we only require that the _ future _ value is lipschitz in the sense of equation",
    ". this will will be satisfied whenever the underlying value function is lipschitz , but is a strictly weaker requirement since the system noise helps to smooth future values .    since @xmath89 has @xmath195-sub - gaussian noise we write @xmath196 in the natural way .",
    "we now use equation to reduce regret to a sum of set widths . to reduce clutter and",
    "more closely follow the notation of section [ sec : eluder ] we will write @xmath197 .",
    "@xmath198 & \\le & { \\mathds{e}}\\left [ \\sum_{i=1}^\\tau \\left\\ {   \\overline{r}^k(x_{k , i } ) - \\overline{r}^*(x_{k , i } )      +   u^k_{i}(p^k(x_{k , i } ) ) - u^k_{i}(p^*(x_{k , i } ) ) \\right\\ } \\right ] \\nonumber \\\\      & \\le & { \\mathds{e}}\\left [ \\sum_{i=1}^\\tau \\left\\ { | \\overline{r}^k(x_{k , i } ) - \\overline{r}^*(x_{k , i})|      + k^k \\|\\overline{p}^k(x_{k , i } ) - \\overline{p}^*(x_{k , i } ) \\|_2 \\right\\}\\right]\\end{aligned}\\ ] ] where @xmath199 is a global lipschitz constant for the future value function of @xmath67 as per .",
    "we now use the results from sections [ sec : eluder ] and [ sec : conf ] to form the corresponding confidence sets @xmath200 and @xmath201 for the reward and transition functions respectively .",
    "let @xmath202 and @xmath203 and condition upon these events to give : @xmath204 & \\le &      { \\mathds{e}}\\left [ \\sum_{k=1}^m \\sum_{i=1}^\\tau          \\left\\ { | \\overline{r}^k(x_{k , i } ) - \\overline{r}^*(x_{k , i})|          + k^k \\|\\overline{p}^k(x_{k , i } ) - \\overline{p}^*(x_{k , i } ) \\|_2 \\right\\}\\right ] \\nonumber \\\\      & & \\hspace{-10 mm } \\le \\sum_{k=1}^m \\sum_{i=1}^\\tau \\left\\ { w_{{\\mathcal{r}}_k}(x_{k , i } ) +           { \\mathds{e}}[k^k|a , b ] w_{{\\mathcal{p}}_k}(x_{k , i } ) + 8 \\delta ( c_{\\mathcal{r}}+ c_{\\mathcal{p } } ) \\right\\}\\end{aligned}\\ ] ] the posterior sampling lemma ensures that @xmath205 = { \\mathds{e}}[k^*]$ ] so that @xmath206 \\le \\frac{{\\mathds{e}}[k^*]}{{\\mathds{p}}(a , b ) } \\le \\frac{{\\mathds{e}}[k^*]}{1 - 8\\delta}$ ] by a union bound on @xmath207 .",
    "we fix @xmath208 to see that : @xmath93 \\le ( c_{\\mathcal{r}}+ c_{\\mathcal{p } } ) +      \\sum_{k=1}^m \\sum_{i=1}^\\tau w_{{\\mathcal{r}}_k}(x_{k , i } )      + { \\mathds{e}}[k^ * ] \\left(1+\\frac{1}{t-1 } \\right)\\sum_{k=1}^m \\sum_{i=1}^\\tau w_{{\\mathcal{p}}_t}(x_{k , i})\\ ] ] we now use equation together with proposition [ prop : widths ] to obtain our regret bounds . for ease of notation we will write @xmath209 and @xmath210 .",
    "@xmath211 & \\le &      2 + ( c_{\\mathcal{r}}+ c_{\\mathcal{p } } ) + \\tau(c_{\\mathcal{r}}d_e({\\mathcal{r}})+ c_{\\mathcal{p}}d_e({\\mathcal{p } } ) ) + \\nonumber \\\\      & & \\   4 \\sqrt{\\beta_t^*({\\mathcal{r}},1/8t,\\alpha ) d_e({\\mathcal{r } } ) t } + 4 \\sqrt{\\beta_t^*({\\mathcal{p}},1/8t,\\alpha ) d_e({\\mathcal{p } } ) t}\\end{aligned}\\ ] ] we let @xmath212 and write @xmath78 for @xmath88 and @xmath89 to complete our proof of theorem [ thm : main regret ] : @xmath87 \\le \\big [ c_{\\mathcal{r}}+ c_{\\mathcal{p}}\\big ] + \\tilde{d}({\\mathcal{r } } )       + { \\mathds{e}}[k^*]\\left(1+\\frac{1}{t-1}\\right ) \\tilde{d}({\\mathcal{p}})\\end{aligned}\\ ] ] where @xmath213 is shorthand for @xmath214 .",
    "the first term @xmath215 $ ] bounds the contribution from missed confidence sets .",
    "the cost of learning the reward function @xmath216 is bounded by @xmath217 . in most problems",
    "the remaining contribution bounding transitions and lost future value will be dominant .",
    "corollary [ cor : ass regret ] follows from the definition [ def : kol ] together with @xmath218 and @xmath219 .",
    "we present a new analysis of _ posterior sampling for reinforcement learning _ that leads to a general regret bound in terms of the dimensionality , rather than the cardinality , of the underlying mdp .",
    "these are the first regret bounds for reinforcement learning in such a general setting and provide new state of the art guarantees when specialized to several important problem settings .",
    "that said , there are a few clear shortcomings which we do not address in the paper .",
    "first , we assume that it is possible to draw samples from the posterior distribution exactly and in some cases this may require extensive computational effort .",
    "second , we wonder whether it is possible to extend our analysis to learning in mdps without episodic resets . finally , there is a fundamental hurdle to model - based reinforcement learning that planning for the optimal policy even in a _ known _ mdp may be intractable .",
    "we assume access to an approximate mdp planner , but this will generally require lengthy computations . we would like to examine whether similar bounds are attainable in model - free learning @xcite , which may obviate complicated mdp planning , and examine the computational and statistical efficiency tradeoffs between these methods .",
    "osband is supported by stanford graduate fellowships courtesy of paccar inc .",
    "this work was supported in part by award cmmi-0968707 from the national science foundation .",
    "in this appendix we will build up to a proof of proposition [ prop : conf sets ] , that the confidence sets defined by @xmath220 in equation [ eq : beta star ] hold with high probability .",
    "we begin with some elementary results from martingale theory .",
    "let @xmath221 be real - calued random variables adapted to @xmath222 .",
    "we define the conditional mean @xmath223 $ ] and conditional cumulant generating function @xmath224 $ ] , then @xmath225 is a martingale with @xmath226 = 1 $ ] .",
    "[ lem : mg conc ] for @xmath227 adapted real @xmath228 random variables adapted to @xmath222 .",
    "we define the conditional mean @xmath223 $ ] and conditional cumulant generating function @xmath224 $ ] .",
    "@xmath229    both of these lemmas are available in earlier discussion for real - valued variables @xcite .",
    "we now specialize our discussion to the vector space @xmath71 where the inner product @xmath230 . to simplify notation",
    "we will write @xmath231 and @xmath232 for arbitrary @xmath233 .",
    "we now define @xmath234 so that clearly @xmath235 . now since we have said that the noise is @xmath75-sub - gaussian , @xmath236 \\le \\exp \\left ( \\frac { \\| \\phi \\|_2 ^ 2 \\sigma^2}{2 } \\right ) \\text { } \\forall \\phi \\in \\mathcal{y}$ ] . from here",
    "we can deduce that : @xmath237   \\\\      & = & \\log { \\mathds{e } } [ \\exp ( 2 \\lambda < f_t - f_t^ * , \\epsilon_t > ) ] \\\\      & \\le & \\frac{\\|2 \\lambda ( f_t - f_t^ * ) \\|_2 ^ 2 \\sigma^2}{2}.\\end{aligned}\\ ] ]    we now write @xmath238 according to our earlier definition of @xmath239 .",
    "we can apply lemma [ lem : mg conc ] with @xmath240 , @xmath241 to obtain : @xmath242 substituting @xmath243 to be the least squares solution which minimizes @xmath244 we can remove @xmath245 .",
    "from here we use an @xmath9-cover discretization argument to complete the proof of proposition [ prop : conf sets ] .",
    "let @xmath246 be an @xmath9 - 2 cover of @xmath247 such that @xmath248 there is some @xmath249 .",
    "we can use a union bound on @xmath250 so that @xmath248 : @xmath251 we will now seek to bound this discretization error with high probability .    [",
    "lem : disc err ] if @xmath252 for all @xmath253 then with probability at least @xmath254 : @xmath255\\ ] ]    for non - trivial bounds we will consider the case of @xmath256 and note that via cauchy - schwarz : @xmath257 from here we can say that @xmath258 summing these expressions over time @xmath259 and using sub - gaussian high probability bounds on @xmath260 gives our desired result .",
    "finally we apply lemma [ lem : disc err ] to equation [ eq : l err bounds ] and use the fact that @xmath261 is the @xmath239 minimizer to obtain the result that with probability at least @xmath262 : @xmath263 which is our desired result .",
    "if @xmath155 is a nondecreasing sequence with @xmath156 then @xmath157    we first imagine that @xmath264 and is @xmath265-dependent on @xmath266 disjoint subsequences of @xmath267 . if @xmath268 is @xmath265-dependent on @xmath266 disjoint subsequences then there exist @xmath269 . by the triangle inequality",
    "@xmath270 so that @xmath271 .    in the case without episodic delay , russo went on to show that in any sequence of length @xmath272 there is some element which is @xmath265-dependent on at least @xmath273 disjoint subsequences @xcite .",
    "our analysis follows similarly , but we may lose up to @xmath274 proper subsequences due to the delay in updating the episode .",
    "this means that we can only say that @xmath275 .",
    "considering the subsequence @xmath276 we see that @xmath277 as required .",
    "in this section of the appendix we will provide bounds upon the eluder dimension for some canonical function classes . recalling definition [ def : eluder ] , @xmath117 is the length @xmath10 of the longest sequence @xmath107 such that for some @xmath118 : @xmath278 for each @xmath279 .",
    "any @xmath253 is @xmath265-dependent upon itself for all @xmath124 .",
    "therefore for all @xmath124 the eluder dimension of @xmath77 is bounded by @xmath280 .",
    "let @xmath126 . to simplify our notation we will write @xmath282 and @xmath283 . from here",
    ", we may manipulate the expression @xmath284 @xmath285 we next require a lemma which gives an upper bound for trace constrained optimizations .    [ lem : norms trace ] let @xmath286 and @xmath287 , the set of positive definite @xmath288 matrices , then : @xmath289 is bounded above by @xmath290 where @xmath291 .",
    "we first note that @xmath292 by jensen s inequality .",
    "we define @xmath293 such that each row of @xmath294 .",
    "then this inequality can be expressed as : @xmath295 where @xmath296 we can now obtain an upper bound for our original problem through this convex relaxation : @xmath297    we can now form the lagrangian @xmath298 . solving for first order optimality @xmath299 .",
    "from here we form the dual objective @xmath300 here we solve for the dual - optimal @xmath301 @xmath302 . from the definition of @xmath303 , @xmath304 and @xmath305 . from this",
    "we can simplify our expression to conclude :    @xmath306    from here we conclude that the optimal value of @xmath307 .    using this lemma",
    ", we will be able to address the eluder dimension for linear functions . using the definition of @xmath308 from equation [ eq : w_k ] together with @xmath309 we may rewrite : @xmath310 let @xmath311 so that @xmath312 through a triangle inequality",
    ". now applying lemma [ lem : norms trace ] we can say that @xmath313 .",
    "this means that if @xmath314 then @xmath315 .",
    "we now imagine that @xmath316 for each @xmath317 . then since @xmath318 we can use the matrix determinant together with the above observation to say that : @xmath319 for @xmath320 .",
    "to get an upper bound on the determinant we note that @xmath321 is maximized when all eigenvalues are equal or equivalently : @xmath322    now using equations [ eq : det lower ] and [ eq : det upper ] together we see that @xmath59 must satistfy the inequality @xmath323 .",
    "we now write @xmath324 and @xmath325 so that we can epress this as : @xmath326 we now use the result that @xmath327 .",
    "we complete our proof of proposition [ prop : eluder lin ] through computing this upper bound at @xmath328 , @xmath128 + 1 = \\tilde{o}(np).\\ ] ]      let @xmath129 then for any @xmath70 we can say that : @xmath131 + 1 = \\tilde{o}(p^2).\\ ] ] where we have simply applied the linear result with @xmath330 .",
    "this is valid since if we can identify the linear function @xmath331 to within this tolerance then we will certainly know @xmath332 as well .",
    "let @xmath132 be a component - wise independent function on @xmath133 with derivative in each component bounded @xmath134 $ ] with @xmath135 .",
    "define @xmath136 to be the condition number . if @xmath137 then for any @xmath70 : @xmath138 \\right ) + 1 = \\tilde{o}(r^2 np)\\ ] ]    this proof follows exactly as per the linear case , but first using a simple reduction on the form of equation .",
    "@xmath333 to which we can now apply lemma [ lem : norms trace ] with the @xmath265 rescaled by @xmath334 .",
    "following the same arguments as for linear functions now completes our proof .",
    "we imagine a standard linear quadratic controller with rewards with @xmath335 the state - action vector .",
    "the rewards and transitions are given by : @xmath336 where @xmath337 is positive semi - definite and @xmath338 projects x onto the @xmath38-ball at radius @xmath96 .",
    "in the case of unbounded states and actions the ricatti equations give the form of the optimal value function @xmath99 for @xmath339 . in this case",
    "we can see that the difference in values of two states : @xmath340 where @xmath97 is the largest eigenvalue of @xmath98 and @xmath96 is an upper bound on the @xmath38-norm of both @xmath19 and @xmath341 .",
    "we note that @xmath342 works as an effective lipshcitz constant when we know what @xmath96 can bound @xmath343 .",
    "we observe that for any projection @xmath344 for @xmath345 $ ] and that for all positive semi - definite matrices @xmath337 , @xmath346 .",
    "using this observation together with reward and transition functions we can see that the value function of the bounded lqr system is always greater than or equal to that of the unconstrained value function .",
    "the effect of excluding the low - reward outer region , but maintaining the higher - reward inner region means that the value function becomes more flat in the bounded case , and so @xmath342 works as an effective lipschitz constant for this problem too .",
    "for completeness , we explicitly outline an optimistic algorithm which uses the confidence sets in our analysis of psrl to guarantee similar regret bounds with high probability over all mdp @xmath63 .",
    "the algorithm follows the style of ucrl2 @xcite so that at the start of the @xmath59th episode the algorithm form @xmath347 and then solves for the optimistic policy that attains the highest reward over any @xmath27 in @xmath348 ."
  ],
  "abstract_text": [
    "<S> we consider the problem of learning to optimize an unknown markov decision process ( mdp ) . </S>",
    "<S> we show that , if the mdp can be parameterized within some known function class , we can obtain regret bounds that scale with the dimensionality , rather than cardinality , of the system . </S>",
    "<S> we characterize this dependence explicitly as @xmath0 where @xmath1 is time elapsed , @xmath2 is the kolmogorov dimension and @xmath3 is the _ </S>",
    "<S> eluder dimension_. these represent the first unified regret bounds for model - based reinforcement learning and provide state of the art guarantees in several important settings </S>",
    "<S> . moreover , we present a simple and computationally efficient algorithm _ posterior sampling for reinforcement learning _ </S>",
    "<S> ( psrl ) that satisfies these bounds . </S>"
  ]
}