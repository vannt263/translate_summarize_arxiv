{
  "article_text": [
    "a common strategy in visual object recognition tasks is to combine different image representations to capture relevant traits of an image .",
    "prominent representations are for instance built from color , texture , and shape information and used to accurately locate and classify the objects of interest .",
    "the importance of such image features changes across the tasks .",
    "for example , color information increases the detection rates of stop signs in images substantially but it is almost useless for finding cars .",
    "this is because stop sign are usually red in most countries but cars in principle can have any color . as additional but nonessential features not only slow down the computation time but may even harm predictive performance , it is necessary to combine only relevant features for state - of - the - art object recognition systems .    we will approach visual object classification from a machine learning perspective . in the last decades , support vector machines ( svm ) @xcite",
    "have been successfully applied to many practical problems in various fields including computer vision @xcite .",
    "support vector machines exploit similarities of the data , arising from some ( possibly nonlinear ) measure .",
    "the matrix of pairwise similarities , also known as kernel matrix , allows to abstract the data from the learning algorithm @xcite .",
    "that is , given a task at hand , the practitioner needs to find an appropriate similarity measure and to plug the resulting kernel into an appropriate learning algorithm .",
    "but what if this similarity measure is difficult to find ? we note that @xcite and @xcite were the first to exploit prior and domain knowledge for the kernel construction .    in object recognition , translating information from various image descriptors into several kernels has now become a standard technique .",
    "consequently , the choice of finding the right kernel changes to finding an appropriate way of fusing the kernel information ; however , finding the right combination for a particular application is so far often a matter of a judicious choice ( or trial and error ) .    in the absence of principled approaches , practitioners frequently resort to heuristics such as uniform mixtures of normalized kernels @xcite that have proven to work well",
    "nevertheless , this may lead to sub - optimal kernel mixtures .",
    "an alternative approach is multiple kernel learning ( mkl ) that has been applied to object classification tasks involving various image descriptors @xcite .",
    "multiple kernel learning @xcite generalizes the support vector machine framework and aims at learning the optimal kernel mixture and the model parameters of the svm simultaneously . to obtain a well - defined optimization problem ,",
    "many mkl approaches promote sparse mixtures by incorporating a @xmath0-norm constraint on the mixing coefficients .",
    "compared to heuristic approaches , mkl has the appealing property of learning a kernel combination ( wrt .  the @xmath1-norm constraint ) and converges quickly as it can be wrapped around a regular support vector machine @xcite .",
    "however , some evidence shows that sparse kernel mixtures are often outperformed by an unweighted - sum kernel @xcite . as a remedy ,",
    "@xcite propose @xmath2-norm regularized mkl variants , which promote non - sparse kernel mixtures and subsequently have been extended to @xmath3-norms @xcite .",
    "multiple kernel approaches have been applied to various computer vision problems outside our scope such multi - class problems @xcite which require mutually exclusive labels and object detection @xcite in the sense of finding object regions in an image .",
    "the latter reaches its limits when image concepts can not be represented by an object region anymore such as the _ outdoor_,_overall quality _ or _",
    "boring _ concepts in the imageclef2010 dataset which we will use .    in this contribution , we study the benefits of sparse and non - sparse mkl in object recognition tasks",
    ". we report on empirical results on image data sets from the pascal visual object classes ( voc ) 2009 @xcite and imageclef2010 photoannotation @xcite challenges , showing that non - sparse mkl significantly outperforms the uniform mixture and @xmath1-norm mkl .",
    "furthermore we discuss the reasons for performance gains and performance limitations obtained by mkl based on additional experiments using real world and synthetic data .",
    "the family of mkl algorithms is not restricted to svm - based ones .",
    "another competitor , for example , is multiple kernel learning based on kernel discriminant analysis ( kda ) @xcite .",
    "the difference between mkl - svm and mkl - kda lies in the underlying single kernel optimization criterion while the regularization over kernel weights is the same .    outside the mkl family",
    ", however , within our problem scope of image classification and ranking lies , for example , @xcite which uses a logistic regression as base criterion and results in a number of optimization parameters equal to the number of samples times the number of input features .",
    "since the approach in @xcite uses a priori much more optimization variables , it poses a more challenging and potentially more time consuming optimization problem which limits the number of applicable features and can be evaluated for our medium scaled datasets in detail in the future .",
    "alternatives use more general combinations of kernels such as products with kernel widths as weighting parameters @xcite . as @xcite point out",
    "the corresponding optimization problems are no longer convex",
    ". consequently they may find suboptimal solutions and it is more difficult to assess using such methods how much gain can be achieved via learning of kernel weights .",
    "this paper is organized as follows . in section [ mlt ] ,",
    "we briefly review the machine learning techniques used here ; the following section[experiment ] we present our experimental results on the voc2009 and imageclef2010 datasets ; in section  [ section : disc_toy ] we discuss promoting and limiting factors of mkl and the sum - kernel svm in three learning scenarios .",
    "this section briefly introduces multiple kernel learning ( mkl ) , and kernel target alignment . for more details we refer to the supplement and the cited works in it .",
    "given a finite number of different kernels each of which implies the existence of a feature mapping @xmath4 onto a hilbert space @xmath5 the goal of multiple kernel learning is to learn svm parameters @xmath6 and linear kernel weights @xmath7 simultaneously .",
    "this can be cast as the following optimization problem which extends support vector machines @xcite @xmath8 the usage of kernels is permitted through its partially dualized form : @xmath9 for details on the solution of this optimization problem and its kernelization we refer to the supplement and @xcite .",
    "while prior work on mkl imposes a @xmath0-norm constraint on the mixing coefficients to enforce sparse solutions lying on a standard simplex @xcite , we employ a generalized @xmath3-norm constraint @xmath10 for @xmath11 as used in @xcite .",
    "the implications of this modification in the context of image concept classification will be discussed throughout this paper .      the kernel alignment introduced by @xcite measures the similarity of two matrices as a cosine angle of vectors under the frobenius product @xmath12",
    "it was argued in @xcite that centering is required in order to correctly reflect the test errors from svms via kernel alignment . centering in the corresponding feature spaces @xcite",
    "can be achieved by taking the product @xmath13 , with @xmath14 @xmath15 is the identity matrix of size @xmath16 and @xmath17 is the column vector with all ones . the centered kernel which achieves a perfect separation of two classes is proportional to @xmath18 , where @xmath19 and @xmath20 and @xmath21 are the sizes of the positive and negative classes , respectively",
    "in this section , we evaluate @xmath3-norm mkl in real - world image categorization tasks , experimenting on the voc2009 and imageclef2010 data sets . we also provide insights on _ when _ and _ why _ @xmath3-norm mkl can help performance in image classification applications .",
    "the evaluation measure for both datasets is the average precision ( ap ) over all recall values based on the precision - recall ( pr ) curves .",
    "we experiment on the following data sets :    * 1 .",
    "pascal2 voc challenge 2009 *   we use the official data set of the _ pascal2 visual object classes challenge 2009 _ ( voc2009 ) @xcite , which consists of 13979 images .",
    "the use the official split into 3473 training , 3581 validation , and 6925 test examples as provided by the challenge organizers .",
    "the organizers also provided annotation of the 20 objects categories ; note that an image can have multiple object annotations .",
    "the task is to solve 20 binary classification problems , i.e. predicting whether at least one object from a class @xmath22 is visible in the test image . although the test labels are undisclosed , the more recent voc datasets permit to evaluate ap scores on the test set via the challenge website ( the number of allowed submissions per week being limited ) .",
    "imageclef 2010 photoannotation *   the imageclef2010 photoannotation data set @xcite consists of 8000 labeled training images taken from flickr and a test set with undisclosed labels .",
    "the images are annotated by 93 concept classes having highly variable concepts  they contain both well defined objects such as _ lake , river , plants , trees , flowers , _ as well as many rather ambiguously defined concepts such as _ winter , boring , architecture , macro , artificial , motion blur,_however , those concepts might not always be connected to objects present in an image or captured by a bounding box .",
    "this makes it highly challenging for any recognition system .",
    "unfortunately , there is currently no official way to obtain test set performance scores from the challenge organizers .",
    "therefore , for this data set , we report on training set cross - validation performances only . as for voc2009",
    "we decompose the problem into 93 binary classification problems . again",
    ", many concept classes are challenging to rank or classify by an object detection approach due to their inherent non - object nature .",
    "as for the previous dataset each image can be labeled with multiple concepts .      in all of our experiments we deploy 32 kernels capturing various aspects of the images .",
    "the kernels are inspired by the voc 2007 winner @xcite and our own experiences from our submissions to the voc2009 and imageclef2009 challenges .",
    "we can summarize the employed kernels by the following three types of basic features :    * histogram over a bag of visual words over sift features ( bow - s ) , 15 kernels * histogram over a bag of visual words over color intensity histograms ( bow - c ) , 8 kernels * histogram of oriented gradients ( hog ) , 4 kernels * histogram of pixel color intensities ( hoc ) , 5 kernels .",
    "we used a higher fraction of bag - of - word - based features as we knew from our challenge submissions that they have a better performance than global histogram features .",
    "the intention was , however , to use a variety of different feature types that have been proven to be effective on the above datasets in the past  but at the same time obeying memory limitations of maximally 25 gb per job as required by computer facilities used in our experiments ( we used a cluster of 23 nodes having in total 256 amd64 cpus and with memory limitations ranging in 3296 gb ram per node ) .",
    "the above features are derived from histograms that contain _ no _ spatial information .",
    "we therefore enrich the respective representations by using spatial tilings @xmath23 , which correspond to single levels of the pyramidal approach @xcite ( this is for capturing the spatial context of an image ) .",
    "furthermore , we apply a @xmath24 kernel on top of the enriched histogram features , which is an established kernel for capturing histogram features @xcite .",
    "the bandwidth of the @xmath24 kernel is thereby heuristically chosen as the mean @xmath24 distance over all pairs of training examples @xcite .",
    "the bow features were constructed in a standard way @xcite : at first , the sift descriptors @xcite were calculated on a regular grid with 6 pixel pitches for each image , learning a code book of size @xmath25 for the sift features and of size @xmath26 for the color histograms by @xmath22-means clustering ( with a random initialization ) .",
    "finally , all sift descriptors were assigned to visual words ( so - called _ prototypes _ ) and then summarized into histograms within entire images or sub - regions .",
    "we computed the sift features over the following color combinations , which are inspired by the winners of the pascal voc 2008 challenge winners from the university of amsterdam @xcite : red - green - blue ( rgb ) , normalized rgb , gray - opponentcolor1-opponentcolor2 , and gray - normalized opponentcolor1-opponentcolor2 ; in addition , we also use a simple gray channel .",
    "we computed the 15-dimensional local color histograms over the color combinations red - green - blue , gray - opponentcolor1-opponentcolor2 , gray , and hue ( the latter being weighted by the pixel value of the value component in the hsv color representation ) .",
    "this means , for bow - s , we considered five color channels with three spatial tilings each ( @xmath27 , @xmath28 , and @xmath29 ) , resulting in 15 kernels ; for bow - c , we considered four color channels with two spatial tilings each ( @xmath27 and @xmath28 ) , resulting in 8 kernels .",
    "the hog features were computed by discretizing the orientation of the gradient vector at each pixel into 24 bins and then summarizing the discretized orientations into histograms within image regions @xcite .",
    "canny detectors @xcite are used to discard contributions from pixels , around which the image is almost uniform .",
    "we computed them over the color combinations red - green - blue , gray - opponentcolor1-opponentcolor2 , and gray , thereby using the two spatial tilings @xmath30 and @xmath31 .",
    "for the experiments we used four kernels : a product kernel created from the two kernels with the red - green - blue color combination but using different spatial tilings , another product kernel created in the same way but using the gray - opponentcolor1-opponentcolor2 color combination , and the two kernels using the gray channel alone ( but differing in their spatial tiling ) .",
    "the hoc features were constructed by discretizing pixel - wise color values and computing their 15 bin histograms within image regions . to this end , we used the color combinations red - green - blue , gray - opponentcolor1-opponentcolor2 , and gray . for each color combination the spatial tilings @xmath29 , @xmath32 , and @xmath30 were tried . in the experiments we deploy five kernels : a product kernel created from the three kernels with different spatial tilings with colors red - green - blue , a product kernel created from the three kernels with color combination gray - opponentcolor1-opponentcolor2 , and the three kernels using the gray channel alone(differing in their spatial tiling ) .    note that building a product kernel out of @xmath24 kernels boils down to concatenating feature blocks ( but using a separate kernel width for each feature block ) .",
    "the intention here was to use single kernels at separate spatial tilings for the weaker features ( for problems depending on a certain tiling resolution ) and combined kernels with all spatial tilings merged into one kernel to keep the memory requirements low and let the algorithms select the best choice .    in practice ,",
    "the normalization of kernels is as important for mkl as the normalization of features is for training regularized linear or single - kernel models .",
    "this is owed to the bias introduced by the regularization : optimal feature / kernel weights are requested to be small , implying a bias to towards excessively up - scaled kernels . in general",
    ", there are several ways of normalizing kernel functions .",
    "we apply the following normalization method , proposed in @xcite and entitled _ multiplicative normalization _ in @xcite ; on the feature - space level this normalization corresponds to rescaling training examples to unit variance , @xmath33      we treat the multi - label data set as binary classification problems , that is , for each object category we trained a one - vs .- rest classifier .",
    "multiple labels per image render multi - class methods inapplicable as these require mutually exclusive labels for the images .",
    "the respective svms are trained using the shogun toolbox @xcite . in order to shed light on the nature of the presented techniques from a statistical viewpoint , we first pooled all labeled data and then created 20 random cross - validation splits for voc2009 and 12 splits for the larger dataset imageclef2010 .",
    "for each of the 12 or 20 splits , the training images were used for learning the classifiers , while the svm / mkl regularization parameter @xmath34 and the norm parameter @xmath35 were chosen based on the maximal ap score on the validation images .",
    "thereby , the regularization constant @xmath34 is optimized by class - wise grid search over @xmath36 .",
    "preliminary runs indicated that this way the optimal solutions are attained inside the grid .",
    "note that for @xmath37 the @xmath3-norm mkl boils down to a simple svm using a uniform kernel combination ( subsequently called sum - kernel svm ) . in our experiments",
    ", we used the average kernel svm instead of the sum - kernel one .",
    "this is no limitation in this as both lead to identical result for an appropriate choice of the svm regularization parameter .    for a rigorous evaluation",
    ", we would have to construct a separate codebook for each cross validation split .",
    "however , creating codebooks and assigning descriptors to visual words is a time - consuming process .",
    "therefore , in our experiments we resort to the common practice of using a single codebook created from all training images contained in the official split .",
    "although this could result in a slight overestimation of the ap scores , this affects all methods equally and does not favor any classification method more than another  our focus lies on a _ relative _ comparison of the different classification methods ; therefore there is no loss in exploiting this computational shortcut .",
    "0.05 in    [ cols=\"^,^,^,^,^,^,^,^\",options=\"header \" , ]     furthermore , we also investigate the single - kernel performance of each kernel : we observed the best single - kernel svm ( which attained ap scores of @xmath38 , @xmath39 , and @xmath40 for experiment 1 ) being inferior to both mkl ( regardless of the employed norm parameter @xmath35 ) and the sum - kernel svm .",
    "the differences were significant with fairly small p - values ( for example , for @xmath41-mkl the p - value was about @xmath42 ) .    we emphasize that we did not design the example in order to achieve a maximal performance gap between the non sparse mkl and its competitors .",
    "for such an example , see the toy experiment of @xcite , which is replicated in the supplemental material including additional analysis .",
    "our focus here was to confirm our hypothesis that kernels in semantic concept classification are based on varying subsets of the data  although mkl computes global weights , it emphasizes on kernels that are relevant on the largest informative set and thus approximates the infeasible combinatorial problem of computing an optimal partition / grid of the space into regions which underlie identical optimal weights .",
    "though , in practice , we expect the situation to be more complicated as informative subsets may overlap between kernels .",
    "nevertheless , our hypothesis also opens the way to new directions for learning of kernel weights , namely restricted to subsets of data chosen according to a meaningful principle .",
    "finding such principles is one the future goals of mkl  we sketched one possibility : locality in feature space . a first starting point may be the work of @xcite on localized mkl",
    "when measuring data with different measuring devices , it is always a challenge to combine the respective devices uncertainties in order to fuse all available sensor information optimally . in this paper",
    ", we revisited this important topic and discussed machine learning approaches to adaptively combine different image descriptors in a systematic and theoretically well founded manner .",
    "while mkl approaches in principle solve this problem it has been observed that the standard @xmath1-norm based mkl often can not outperform svms that use an average of a large number of kernels .",
    "one hypothesis why this seemingly unintuitive result may occur is that the sparsity prior may not be appropriate in many real world problems  especially , when prior knowledge is already at hand .",
    "we tested whether this hypothesis holds true for computer vision and applied the recently developed non - sparse @xmath3 mkl algorithms to object classification tasks .",
    "the @xmath3-norm constitutes a slightly less severe method of sparsification . by choosing @xmath35 as a hyperparameter , which controls the degree of non - sparsity and regularization , from a set of candidate values with the help of a validation data",
    ", we showed that @xmath3-mkl significantly improves svms with averaged kernels and the standard sparse @xmath1 mkl .",
    "future work will study localized mkl and methods to include hierarchically structured information into mkl , e.g. knowledge from taxonomies , semantic information or spatial priors .",
    "another interesting direction is mkl - kda @xcite .",
    "the difference to the method studied in the present paper lies in the base optimization criterion : kda @xcite leads to non - sparse solutions in @xmath43 while ours leads to sparse ones ( i.e. , a low number of support vectors ) .",
    "while on the computational side the latter is expected to be advantageous , the first one might lead to more accurate solutions .",
    "we expect the regularization over kernel weights ( i.e. , the choice of the norm parameter @xmath35 ) having similar effects for mkl - kda like for mkl - svm .",
    "future studies will expand on that topic .",
    "this work was supported in part by the federal ministry of economics and technology of germany ( bmwi ) under the project theseus ( fkz 01mq07018 ) , by federal ministry of education and research ( bmbf ) under the project remind ( fkz 01-is07007a ) , by the deutsche forschungsgemeinschaft ( dfg ) , and by the fp7-ict program of the european community , under the pascal2 network of excellence ( ict-216886 ) .",
    "marius kloft acknowledges a scholarship by the german academic exchange service ( daad ) ."
  ],
  "abstract_text": [
    "<S> combining information from various image features has become a standard technique in concept recognition tasks . however , the optimal way of fusing the resulting kernel functions is usually unknown in practical applications . </S>",
    "<S> multiple kernel learning ( mkl ) techniques allow to determine an optimal linear combination of such similarity matrices . </S>",
    "<S> classical approaches to mkl promote sparse mixtures . unfortunately , so - called 1-norm mkl variants </S>",
    "<S> are often observed to be outperformed by an unweighted sum kernel . </S>",
    "<S> the contribution of this paper is twofold : we apply a recently developed non - sparse mkl variant to state - of - the - art concept recognition tasks within computer vision . </S>",
    "<S> we provide insights on benefits and limits of non - sparse mkl and compare it against its direct competitors , the sum kernel svm and the sparse mkl . </S>",
    "<S> we report empirical results for the pascal voc 2009 classification and imageclef2010 photo annotation challenge data sets . about to be submitted to plos one . </S>"
  ]
}