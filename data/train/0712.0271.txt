{
  "article_text": [
    "in recent years , distributed source coding ( dsc ) has received an increasing attention from the signal processing community .",
    "dsc considers a situation in which two ( or more ) statistically dependent sources @xmath0 and @xmath1 must be encoded by separate encoders that are not allowed to talk to each other . performing separate lossless compression",
    "may seem less efficient than joint encoding .",
    "however , dsc theory proves that , under certain assumptions , separate encoding is optimal , provided that the sources are decoded jointly @xcite .",
    "for example , with two sources it is possible to perform  standard \" encoding of the first source ( called _ side information _ ) at a rate equal to its entropy , and  conditional \" encoding of the second one at a rate lower than its entropy , with no information about the first source available at the second encoder ; we refer to this as  asymmetric \" slepian - wolf ( s - w ) problem .",
    "alternatively , both sources can be encoded at a rate smaller than their respective entropy , and decoded jointly , which we refer to as  symmetric \" s - w coding .",
    "dsc theory also encompasses lossy compression @xcite ; it has been shown that , under certain conditions , there is no performance loss in using dsc @xcite , and that possible losses are bounded below 0.5 bit per sample ( bps ) for quadratic distortion metric @xcite . in practice , lossy dsc",
    "is typically implemented using a quantizer followed by lossless dsc , while the decoder consists of the joint decoder followed by a joint dequantizer .",
    "lossless and lossy dsc have several potential applications , e.g. , coding for non co - located sources such as sensor networks , distributed video coding @xcite , layered video coding @xcite , error resilient video coding @xcite , and satellite image coding @xcite , just to mention a few .",
    "the interested reader is referred to @xcite for an excellent tutorial .",
    "traditional entropy coding of an information source can be performed using one out of many available methods , the most popular being arithmetic coding ( ac ) and huffman coding .  conditional \" ( i.e. , dsc ) coders are typically implemented using channel codes , by representing the source using the syndrome or the parity bits of a suitable channel code of given rate .",
    "the syndrome identifies sets of codewords (  cosets \" ) with maximum distance properties , so that decoding an ambiguous description of a source at a rate less than its entropy ( given the side information ) incurs minimum error probability . if the correlation between @xmath0 and @xmath1 can be modeled as a  virtual \" channel described as @xmath2 , with @xmath3 an additive noise process , a good channel code for that transmission problem is also expected to be a good s - w source code @xcite .",
    "regarding asymmetric s - w coding , the first practical technique has been described in @xcite , and employs trellis codes .",
    "recently , more powerful channel codes such as turbo codes have been proposed in @xcite , and low - density parity - check ( ldpc ) @xcite codes have been used in @xcite .",
    "turbo and ldpc codes can get extremely close to channel capacity , although they require the block size to be rather large .",
    "note that the constituent codes of turbo - codes are convolutional codes , hence the syndrome is difficult to compute . in @xcite",
    "the cosets are formed by all messages that produce the same parity bits , even though this approach is somewhat suboptimal @xcite , since the geometrical properties of these cosets are not as good as those of syndrome - based coding . in @xcite a syndrome former is used to deal with this problem .",
    "multilevel codes have also be addressed ; in @xcite trellis codes are extended to multilevel sources , whereas in @xcite a similar approach is proposed for ldpc codes .",
    "besides techniques based on channel coding , a few authors have also investigated the use of source coders for dsc .",
    "this is motivated by the fact that existing source coders obviously exhibit nice compression features that should be retained in a dsc coder , such as the ability to employ flexible and adaptive probability models , and low encoding complexity . in @xcite",
    "the problem of designing a variable - length dsc coder is addressed ; it is shown that the problem of designing a zero - error such coder is np - hard . in @xcite a similar approach is followed ; the authors consider the problem of designing huffman and arithmetic dsc coders for multilevel sources with zero or almost - zero error probability .",
    "the idea is that , if the joint density of the source and the side information satisfies certain conditions , the same codeword ( or the same interval for the ac process ) can be associated to multiple symbols .",
    "this approach leads to an encoder with a complex modeling stage ( np - hard for the optimal code , though suboptimal polynomial - time algorithms are provided in @xcite ) , while the decoding process resembles a classical arithmetic decoder .    as for symmetric s - w codes , a few techniques have been recently proposed .",
    "a symmetric code can be obtained from an asymmetric one through time sharing , whereby the two sources alternatively take the role of the source and the side information ; however , current dsc coders can not easily accommodate this approach .",
    "syndrome - based channel code partitioning has been introduced in @xcite , and extended in @xcite to systematic codes .",
    "a similar technique is described in @xcite , encompassing non - systematic codes .",
    "syndrome formers have also been proposed for symmetric s - w coding @xcite .",
    "moreover , techniques based on the use of parity bits can also be employed , as they can typically provide rate compatibility .",
    "a practical code has been proposed in @xcite using two turbo codes that are decoded jointly , achieving the equal rate point ; in @xcite an algorithm is introduced that employs turbo codes to achieve arbitrary rate splitting .",
    "symmetric s - w codes based on ldpc codes have also been developed @xcite .",
    "although several near - optimal dsc coders have been designed for simple ideal sources ( e.g. , binary and gaussian sources ) , the applications of practical dsc schemes to realistic signals typically incurs the following problems .",
    "* channel codes get very close to capacity only for very large data blocks ( typically in excess of @xmath4 symbols ) . in many applications , however , the basic units to be encoded are of the order of a few hundreds to a few thousands symbols . for such block lengths ,",
    "channel codes have good but not optimal performance . * the symbols contained in a block",
    "are expected to follow a stationary statistical distribution .",
    "however , typical real - world sources are not stationary .",
    "this calls for either the use of short blocks , which weakens the performance of the s - w coder , or the estimation of conditional probabilities over contexts , which can not be easily accommodated by existing s - w coders . * when the sources are strongly correlated ( i.e. , in the most favorable case ) , very high - rate channel codes are needed ( e.g. , rate-@xmath5 codes )",
    ". however , capacity - achieving channel codes are often not very efficient at high rate .",
    "* in those applications where dsc is used to limit the encoder complexity , it should be noted that the complexity of existing s - w coders is not negligible , and often higher than that of existing non - dsc coders .",
    "this seriously weakens the benefits of dsc . *",
    "upgrading an existing compression algorithm like jpeg 2000 or h.264/avc to provide dsc functionalities requires at least to redesign the entropy coding stage , adopting one of the existing dsc schemes .    among these issues , the block length is particularly important . while it has been shown that , on ideal sources with very large block length , the performance of some practical dsc coders can be as close as 0.09 bits to the theoretical limit @xcite ,",
    "so far dsc of real - world data has fallen short of its expectations , one reason being the necessity to employ much smaller blocks .",
    "for example , the prism video coder @xcite encodes each macroblock independently , with a block length of 256 samples .",
    "for the coder in @xcite , the block length is equal to the number of 8x8 blocks in one picture ( 1584 for the cif format ) .",
    "the performance of both coders is rather far from optimal , highlighting the need of dsc coders for realistic block lengths .",
    "a solution to this problem has been introduced in @xcite , where an extension of ac , named distributed arithmetic coding ( dac ) , has been proposed for asymmetric s - w coding . moreover , in @xcite dac has been extended to the case of symmetric s - w coding of two sources at the same rate ( i.e. , the mid - point of the s - w rate region ) .",
    "dac and its decoding process do not currently have a rigorous mathematical theory that proves they can asymptotically achieve the s - w rate region ; such theory is very difficult to develop because of the non - linearity of ac .",
    "however , dac is a practical algorithm that was shown in @xcite to outperform other existing distributed coders . in this paper",
    ", we build on the results presented in @xcite , providing several new contributions . for asymmetric coding ,",
    "we focus on i.i.d .",
    "sources as these are often found in many dsc applications ; for example , in transform - domain distributed video coding , dac could be applied to the bit - planes of transform coefficients , which can be modeled as i.i.d .",
    "we optimize the dac using an improved encoder termination procedure , and we investigate the rate allocation problem , i.e. , how to optimally select the encoding parameters to achieve a desired target rate . we evaluate the performance of this new design comparing it with turbo and ldpc codes , including the case of extremely correlated sources with highly skewed probabilities .",
    "this is of interest in multimedia applications because the most significant bit - planes of the transform coefficients of an image or video sequence are almost always equal to zero , and are strongly correlated with the side information . for symmetric coding ,",
    "we extend our previous work in @xcite by introducing dac encoding and rate allocation procedures that allow to encode an arbitrary number of sources with arbitrary combination of rates .",
    "we develop and test the decoder for two sources .",
    "finally , it should be noted that an asymmetric dac scheme has been independently and concurrently developed in @xcite using quasi - arithmetic codes .",
    "quasi - arithmetic codes are a low - complexity approximation to arithmetic codes , providing smaller encoding and decoding complexity @xcite .",
    "these codes allow the interval endpoints to be only a finite set of points .",
    "while this yields suboptimal compression performance , it makes the arithmetic coder a finite state machine , simplifying the decoding process with side information .",
    "this paper is organized as follows . in sect .",
    "[ sec : dac_decoder ] we describe the dac encoding process for the asymmetric case , in sect .",
    "[ sec : dac_decoder ] we describe the dac decoder , and in sect .",
    "[ sec : rate_sel ] we study the rate allocation and parameter selection problem . in sect .",
    "[ sec : symm ] we describe the dac encoder , decoder and rate allocator for the symmetric case . in sect .",
    "[ sec : results_asymm ] and [ sec : results_symm ] we report the dac performance evaluation results in the asymmetric and symmetric case respectively . finally , in sect .",
    "[ sec : concl ] we draw some conclusions .",
    "before describing the dac encoder , it should be noted that the ac process typically consists of a modeling stage and a coding stage . the modeling stage has the purpose of computing the parameters of a suitable statistical model of the source , in terms of the probability that a given bit takes on value 0 or 1 .",
    "this model can be arbitrarily sophisticated , e.g. , by using contexts , adaptive probability estimation , and so forth .",
    "the coding stage takes the probabilities as input , and implements the actual ac procedure , which outputs a binary codeword describing the input sequence .",
    "let @xmath0 be a binary memoryless source that emits a semi - infinite sequence of random variables @xmath6 , @xmath7 , with probabilities @xmath8 and @xmath9 .",
    "we are concerned with encoding the sequence @xmath10 $ ] consisting in the first @xmath11 occurrences of this source . the modeling and coding stages",
    "are shown in fig .",
    "[ fig : modeling]-a .",
    "the modeling stage takes as input the sequence @xmath12 , and outputs an estimate of the probabilities @xmath13 and @xmath14 .",
    "the coding stage takes as input @xmath12 , @xmath13 and @xmath14 , and generates a codeword @xmath15 .",
    "the expected length of @xmath15 depends on @xmath13 and @xmath14 , and is determined once these probabilities are given .    in order to use the dac , we consider two sources @xmath0 and @xmath1 , where @xmath1 is a binary memoryless source that emits random variables @xmath16 , @xmath7 , with probabilities @xmath17 and @xmath18 .",
    "the first @xmath11 occurrences of this source form the side information @xmath19 $ ] .",
    "we assume that @xmath0 and @xmath1 are i.i.d .",
    "sources , and that @xmath6 and @xmath16 are statistically dependent for a given @xmath20 .",
    "the entropy of @xmath0 is defined as @xmath21 , and similarly for @xmath1 .",
    "the conditional entropy of @xmath0 given @xmath1 is defined as @xmath22 .    for dac ,",
    "three blocks can be identified , as in fig .",
    "[ fig : modeling]-b , namely the modeling , rate allocation , and coding stages .",
    "the modeling stage is exactly the same as in the classical ac .",
    "the coding stage will be described in sect .",
    "[ sec : dac_enc ] ; it takes as inputs @xmath12 , the probabilities @xmath13 and @xmath14 , and the parameter @xmath23 , and outputs a codeword @xmath24 . unlike a classical ac , where the expected rate is function of the source probabilities , and hence can not be selected _ a priori _ , the dac allows to select any desired rate not larger than the expected rate of a classical ac",
    "this is very important , since in a dsc setting the rate for @xmath12 should depend not only on how much  compressible \" the source is , but also on how much correlated @xmath6 and @xmath16 are .",
    "for this reason , in dac we also have a rate allocation stage that takes as input the probabilities @xmath13 and @xmath14 and the conditional entropy @xmath25 , and outputs a parameter @xmath23 that drives the dac coding stage to achieve the desired target rate .    in this paper",
    "we deal with the coding and rate allocation stages , and assume that the input probabilities @xmath13 , @xmath14 and conditional entropy @xmath25 are known _ a priori_. this allows us to focus on the distributed coding aspects of the proposed scheme , and , at the same time , keeps the scheme independent of the modeling stage .",
    "we first review the classical ac coding process , as this sets the stage for the description of the dac encoder ; an overview can be found in @xcite .",
    "the binary ac process for @xmath12 is based on the probabilities @xmath13 and @xmath14 , which are used to partition the @xmath26 interval into sub - intervals associated to possible occurrences of the input symbols . at initialization the  current \" interval is set to @xmath27 . for each input symbol @xmath28",
    ", the current interval @xmath29 is partitioned into two adjacent sub - intervals of lengths @xmath30 and @xmath31 , where @xmath32 is the length of @xmath33 .",
    "the sub - interval corresponding to the actual value of @xmath28 is selected as the next current interval @xmath34 , and this procedure is repeated for the next symbol . after all @xmath11 symbols have been processed ,",
    "the sequence is represented by the final interval @xmath35 .",
    "the codeword @xmath15 can consist in the binary representation of any number inside @xmath35 ( e.g. , the number in @xmath35 with the shortest binary representation ) , and requires approximately @xmath36 bits .      similarly to other s - w coders , dac is based on the principle of inserting some ambiguity in the source description during the encoding process .",
    "this is obtained using a modified interval subdivision strategy . in particular",
    ", the dac employs a set of intervals whose lengths are proportional to the modified probabilities @xmath37 and @xmath38 , such that @xmath39 and @xmath40 . in order to fit the enlarged sub - intervals into the @xmath26 interval",
    ", they are allowed to partially overlap .",
    "this prevents the decoder from discriminating the correct interval , unless the side information is used .",
    "the detailed dac encoding procedure is described in the following . at initialization the  current \" interval is set to @xmath41 . for each input symbol @xmath28 , the current interval @xmath42",
    "is subdivided into two partially overlapped sub - intervals whose lengths are @xmath43 and @xmath44 .",
    "the interval representing symbol @xmath28 is selected as the next current interval @xmath45 .",
    "after all @xmath11 symbols have been processed , the sequence is represented by the final interval @xmath46 .",
    "the codeword @xmath24 can consist in the binary representation of any number inside @xmath46 , and requires approximately @xmath47 bits .",
    "this procedure is sketched in fig .",
    "[ fig : dac ] . at the decoder side ,",
    "whenever the codeword points to an overlapped region , the input symbol can not be detected unambiguously , and additional information must be exploited by the joint decoder to solve the ambiguity .",
    "it is worth noticing that the dac encoding procedure is a generalization of ac . letting @xmath48 and @xmath49 leads to the ac encoding process described in sect .",
    "[ sec : ac ] , with @xmath50 and @xmath51 .",
    "it should also be noted that , for simplicity , the description of the ac and dac provided above assumes infinite precision arithmetic . the practical implementation used in sect . [",
    "sec : results_asymm ] and [ sec : results_symm ] employs fixed - point arithmetic and interval renormalization .",
    "the objective of the dac decoder is joint decoding of the sequence @xmath12 given the correlated side information @xmath52 .",
    "the arithmetic decoding machinery of the dac decoder presents limited modifications with respect to standard arithmetic decoders ; a fixed - point implementation has been employed , with the same interval scaling and overlapping rules used at the encoder . in the following the arithmetic decoder state at the @xmath20-th decoding step",
    "is denoted as @xmath53 .",
    "the data stored in @xmath54 represent the interval @xmath55 and the codeword at iteration @xmath20 .",
    "the decoding process can be formulated as a symbol - driven sequential search along a proper decoding tree , where each node represents a state @xmath54 , and a path in the tree represents a possible decoded sequence .",
    "the following elementary decoding functions are required to explore the tree :    * @xmath56_test - one - symbol_@xmath57 : it computes the sub - intervals at the @xmath20-th step , compares them with @xmath24 and outputs either an unambiguous symbol @xmath58 ( if @xmath24 belongs to one of the non - overlapped regions ) , or an ambiguous symbol @xmath59 . in case of unambiguous decoding , the new decoder state @xmath60 is returned for the following iterations .",
    "* @xmath61_force - one - symbol_@xmath62 : it forces the decoder to select the sub - interval corresponding to the symbol @xmath63 regardless of the ambiguity ; the updated decoder state is returned .    in fig .",
    "[ fig : dec_tree ] an example of a section of the decoding tree is shown . in this example",
    "the decoder is not able to make a decision on the @xmath20-th symbol , as _ test - one - symbol _ returns @xmath59 . as a consequence ,",
    "two alternative decoding attempts are pursued by calling _ force - one - symbol _ with @xmath58 respectively . in principle , by iterating this process , the tree @xmath64 , representing all the possible decoded sequences , can be explored .",
    "the best decoded sequence can finally be selected applying the _ maximum a posteriori _ ( map ) criterion @xmath65 .    in general , exhaustive search can not be applied due to the exponential growth of @xmath64 .",
    "a viable solution is obtained applying the breadth - first sequential search known as @xmath66-algorithm @xcite ; at each tree depth , only the @xmath66 nodes with the best partial metric are retained .",
    "this amounts to visiting only a subset of the most likely paths in @xmath64 .",
    "the map metric for a given node can be evaluated as follows : @xmath67 metric can be expressed into additive terms by setting : @xmath68 where @xmath69 and @xmath70 represent the additive metric to be associated to each branch of @xmath64 .",
    "the pseudocode for the dac decoder is given in algorithm  [ alg : dac_decoder ] , where @xmath71 represents the list of nodes in @xmath64 explored at depth @xmath20 ; each tree node stores its corresponding arithmetic decoder state @xmath72 and the accumulated metric @xmath73 .",
    "initialize @xmath74 with root node ( @xmath75 ) set symbol counter @xmath76 @xmath56_test - one - symbol_@xmath57 @xmath61_force - one - symbol_@xmath77 @xmath78 insert @xmath79 in @xmath80 @xmath81 insert @xmath79 in @xmath80 sort nodes in @xmath80 according to metric @xmath82 keep only the @xmath66 nodes with best metric in @xmath80 output @xmath83 ( sequence corresponding to the first node stored in @xmath84 )        it is worth pointing out that @xmath66 has to be selected as a trade - off between the memory / complexity requirements and the error probability , i.e. , the probability that the path corresponding to the original sequence @xmath12 is accidentally dropped .",
    "as in the case of standard viterbi decoding , the path metric turns out to be stable and reliable as long as a significant amount of terms , i.e. , number of decoded symbols @xmath63 , are taken into account . in the pessimistic case when all symbol positions @xmath20 trigger a decoder branching , given @xmath66 , one can guarantee that at least @xmath85 symbols are considered for metric comparisons and pruning . on the other hand , in practical cases ,",
    "the interval overlap is only partial and branching does not occur at every symbol iteration .",
    "all the experimental results presented in sect .",
    "[ sec : results_asymm ] have been obtained using @xmath86 , while the trade - off between performance and complexity is analyzed in sect .",
    "[ sec : perf_compl ] .    finally , metric reliability can not be guaranteed for the very last symbols of a finite - length sequence @xmath12 . for channel codes , e.g. , convolutional codes , this issue is tackled by imposing a proper termination strategy , e.g. , forcing the encoded sequence to end in the first state of the trellis .",
    "a similar approach is necessary when using dac .",
    "examples of ac termination strategies are encoding a known termination pattern or end - of - block symbol with a certain probability or , in the case of context - based ac , driving the ac encoder in a given context . for dac",
    ", we employ a new termination policy that is tailored to its particular features . in particular , termination is obtained by encoding the last @xmath87 symbols of the sequence without interval overlap , i.e. , using @xmath88 , for all symbols @xmath28 with @xmath89 . as a consequence ,",
    "no nodes in the dac decoding tree will cause branching in the last @xmath87 steps , making the final metrics more reliable for the selection of the most likely sequence .",
    "however , there is a rate penalty for the termination symbols .",
    "the length of codeword @xmath24 is determined by the length @xmath90 of the final interval , which in turn depends on how much @xmath37 and @xmath38 are larger than @xmath13 and @xmath14 . as a consequence , in order to select the desired rate , it is important to quantitatively determine the dependence of the expected rate on the overlap , because this will drive the selection of the desired amount of overlap .",
    "moreover , we also need to understand how to split the overlap in order to achieve good decoding performance . in the following we derive the expected rate obtained by the dac as a function of the set of input probabilities and the amount of overlap .",
    "we are interested in finding the expected rate @xmath91 ( in bps ) of the codeword used by the dac to encode the sequence @xmath12 .",
    "this is given by the following formula : @xmath92 this can be derived straightforwardly from the property that the codeword generated by an ac has an expected length that depends on the size of the final interval , that is , on the product of the probabilities @xmath93 , and hence on the amount of overlap .",
    "the expectation is computed using the true probabilities @xmath94 .",
    "we set @xmath95 , where @xmath96 , so that @xmath97 .",
    "this amounts to enlarging each interval by an amount proportional to the overlap factors @xmath98 .",
    "the expected rate achieved by the dac becomes @xmath99 where @xmath100 , and @xmath101 .",
    "note that @xmath102 represents the rate contribution of symbol @xmath103 yielded by standard ac , while @xmath104 represents the decrease of this contribution , i.e. , the average number of bits saved in the binary representation of the @xmath103-th input symbol .",
    "once a target rate has been selected , the problem arises of selecting @xmath105 . as an example",
    ", a possible choice is to take equal overlap factors @xmath106 .",
    "this implies that each interval is enlarged by a factor @xmath107 that does not depend on the source probability @xmath108 .",
    "this leads to a target rate @xmath109 it can be shown that this choice minimizes the rate @xmath91 for a given total amount of overlap @xmath110 ; the computations are simple and are omitted for brevity .",
    "this choice is not necessarily optimal in terms of the decoder error probability .",
    "however , optimizing for the error probability is impractical because of the nonlinearity of the arithmetic coding process .    in practice",
    ", one also has to make sure that the enlarged intervals @xmath111 and @xmath112 are both contained inside the @xmath26 interval .",
    "e.g. , taking equal overlap factors as above does not guarantee this .",
    "we have devised the following rule that allows to achieve any desired rate satisfying the constraint above .",
    "we apply the following constraint : @xmath113 with @xmath23 a positive constant independent of @xmath103 .",
    "this leads to @xmath114 this can be interpreted as an additional constraint that the rate reduction for symbols  0 \" and  1 \" depends on their probabilities , i.e. , the least probable symbol undergoes a larger reduction . using ( [ eq : criterion ] )",
    ", it can be easily shown that the expected rate achieved by the dac can be written as @xmath115    thus , the allocation problem for an i.i.d .",
    "source is very simple .",
    "we assume that the conditional entropy @xmath25 is available as in fig .",
    "[ fig : modeling]-b , modeling the correlation between @xmath0 and @xmath1 . in asymmetric dsc",
    ", @xmath12 should be ideally coded at a rate arbitrarily close to @xmath25 . in practice , due to the suboptimality of any practical coder",
    ", some margin @xmath116 should be taken .",
    "hence , we assume that the allocation problem can be written as @xmath117 . since @xmath118 is a constant and @xmath119 and @xmath120",
    "are given , one can solve for @xmath23 and then perform the encoding process .",
    "finally , it should be noted that , while we have assumed that @xmath0 and @xmath1 are i.i.d . , the dac concept can be easily extended to a nonstationary source .",
    "this simply requires to consider all probabilities and overlap factors as depending on index @xmath20 ; all computations , including the design of the overlap factors and the derivation of the target rate , can be extended straightforwardly .",
    "a possible application is represented by context - based coding or markov modeling of correlated sources .",
    "there is one caveat though , in that , if the probabilities and context of each symbol are computed by the decoder from past symbols , decoding errors can generate significant error propagation .",
    "in many applications , it is preferable to encode the correlated sources at similar rather than unbalanced rates ; in this case , symmetric s - w coding can be used . considering a pair of sources , in symmetric s - w coding both @xmath0 and @xmath1",
    "are encoded using separate dacs .",
    "we denote as @xmath24 and @xmath121 the codewords representing @xmath0 and @xmath1 , and @xmath122 and @xmath123 the respective rates . with dac",
    ", the rate of @xmath0 and @xmath1 can be adjusted with a proper selection of the parameters @xmath23 and @xmath124 for the two dac encoders .",
    "however , it should be noted that , for the same total rate , not all possible choices of @xmath23 and @xmath124 are equally good , because some of them could complicate the decoder design , or be suboptimal in terms of error probability . to highlight the potential problems of a straightforward extension of the asymmetric dac ,",
    "let us assume that @xmath23 and @xmath124 can be chosen arbitrarily .",
    "this would require a decoder that performs a search in a symbol - synchronous tree where each node represents _ two _ sequential decoder states @xmath125 for @xmath0 and @xmath1 respectively .",
    "if the interval selection is ambiguous for both sequences , the four possible binary symbol pairs ( 00,01,10,11 ) need to be included in the search space ; this would accelerate the exponential growth of the tree , and quickly make the decoder search unfeasible .",
    "this example shows that some constraints need to be put on @xmath23 and @xmath124 in order to limit the growth rate of the search space .    to overcome this problem",
    ", we propose an algorithm that applies the idea of time - sharing to the dac .",
    "the concept of time - shared dac has been preliminarly presented in @xcite for a pair of sources in the subcase @xmath126 , i.e. providing only the mid - point of the s - w rate region . in the following",
    "we extend this to an arbitrary combination of rates , and show how this can be generalized to an arbitrary number of sources . for two sources ,",
    "the idea is to divide the set of input indexes @xmath127 in two disjoint sets such that , at each index @xmath20 , ambiguity is introduced in at most one out of the two sources . in particular , for sequences @xmath12 and @xmath52 of length @xmath11 , let @xmath128 and @xmath129 be the subsets of even and odd integer numbers in @xmath130 respectively .",
    "we employ a dac on @xmath12 and @xmath52 , but the choice of parameters @xmath23 and @xmath124 differs . in particular , we let the parameters depend on the symbol index @xmath20 , i.e. , @xmath131 and @xmath132 .",
    "the dac of @xmath12 employs parameter @xmath133 for all @xmath134 , and @xmath135 otherwise .",
    "vice versa , @xmath52 is encoded with parameter @xmath136 for all @xmath137 , and @xmath138 otherwise . as a consequence of these constraints , at each step of the decoding process , ambiguity appears in at most one out the two sequences . in this way",
    ", the growth rate of the decoding tree remains manageable , as no more than two new states are generated at each transition , exactly as in the asymmetric dac decoder ; this also makes the map metric simpler .",
    "the conceptual relation with time - sharing is evident . since , during the dac encoding process , for each input symbol the ambiguity is introduced in at most one out the two encoders , this corresponds to switching the role of side information between either source on a symbol - by - symbol basis . by varying the parameters @xmath23 and @xmath124 , all combinations of rates can be achieved .",
    "the achieved rates can be derived repeating the same computations described in sect .",
    "[ sec : rate_sel ] , and can be expressed as @xmath139 and @xmath140 .",
    "the rate allocation problem amounts to selecting suitable rates @xmath122 and @xmath123 such that @xmath141 , @xmath142 , and @xmath143 . in practice one",
    "will typically take some margin @xmath144 , such that @xmath145 ; for safety , a margin should also be taken on @xmath122 and @xmath123 with respect to the conditional entropy .",
    "since the prior probabilities of @xmath0 and @xmath1 are given , one can solve for @xmath23 and @xmath124 , and then perform the encoding process .",
    "thus , the whole s - w rate region can be swept .      similarly to the asymmetric case",
    ", the symmetric decoding process can be viewed as a search along a tree ; however , specifically for the case of two correlated sources , each node in tree represents the decoding states @xmath146 of two sequential arithmetic decoders for @xmath12 and @xmath52 respectively . at each iteration , sequential decoding",
    "is run from both states .",
    "the time - sharing approach guarantees that , for a given index @xmath20 , the ambiguity can be found only in one of the two decoders .",
    "therefore , at most two branches must be considered , and the tree can be constructed using the same functions introduced in sect .",
    "[ sec : dac_decoder ] for the asymmetric case .",
    "this would be the same also for @xmath147 sources .",
    "in particular , for @xmath148 , _ test - one - symbol_(@xmath149 ) yields an unambiguous symbol @xmath150 , whereas ambiguity can be found only while attempting decoding for @xmath12 with _ test - one - symbol_(@xmath149 ) .",
    "in conclusion , from the node @xmath146 the function _ test - one - symbol _ is used on both states . if ambiguity is found on @xmath63 , _ force - one - symbol _",
    "is then used to explore the two alternative paths for @xmath63 , whereas @xmath151 is used as side information for branch metric evaluation . in the case",
    "that @xmath152 , the roles of @xmath12 and @xmath52 are exchanged .",
    "therefore , algorithm [ alg : dac_decoder ] can be easily extended to the symmetric case by alternatively probing either @xmath12 or @xmath52 for ambiguity , and possibly generating a branching .",
    "the joint probability distribution can be written as @xmath153    the symmetric encoder and decoder can be easily generalized to an arbitrary number @xmath147 of sources .",
    "the idea is to identify @xmath147 subsets of input indexes @xmath127 such that , at each symbol index @xmath20 , ambiguity is introduced in at most one out of the @xmath147 sources . in particular , for sequences @xmath154 of length @xmath11 , let @xmath155 be disjoint subsets of @xmath156 .",
    "we denote the dac parameters as @xmath157 .",
    "the dac of @xmath158 employs parameter @xmath159 for all @xmath160 , and @xmath161 otherwise . as a consequence of these constraints , at each step of the decoding process , ambiguity appears in at most one out the @xmath147 sequences .",
    "note that this formulation also encompasses the case that one or more sources are independent of each other and from all the others ; these sources can be coded with a classical ac , taking @xmath162 for this source .",
    "the selection of the sets @xmath163 and the overlap factors @xmath164 , for @xmath165 , is still somewhat arbitrary , as the expected rate of source @xmath103 depends on both the cardinality of @xmath163 and the value of @xmath164 . in a realistic application",
    "it would be more practical to fix the sets @xmath163 once and for all , and to modify the parameters @xmath164 so as to obtain the desired rate .",
    "this is because , for time - varying correlations , one has to update the rate on - the - fly . in",
    "a distributed setting , varying one parameter @xmath164 requires to communicate the change only to source @xmath103 , while varying the sets @xmath163 requires to communicate the change to all sources .",
    "therefore , we define @xmath166 such that the @xmath147 statistically dependent sources take in turns the role of the side information .",
    "any additional independent sources are coded separately using @xmath167 . in particular , we set @xmath168 , where @xmath169 denotes the remainder of the division between two integers , and @xmath170 .",
    "the dac encoder for the @xmath103-th source inserts ambiguity only at time instants @xmath160 . at each node ,",
    "the decoder stores the states of the @xmath147 arithmetic decoders , and possibly performs a branching if the codeword related to the only potentially ambiguous symbol at the current time @xmath20 is actually ambiguous .",
    "although this encoding and decoding structure is not necessarily optimal , it does lead to a viable decoding strategy .",
    "in the following we provide results of a performance evaluation carried out on dac . we implement a communication system that employs a dac and a joint decoder , with no feed - back channel ; at the decoder , pruning is performed using the m - algorithm @xcite , with m=2048 .",
    "the side information is obtained by sending the source @xmath0 through a binary symmetric channel with transition probability @xmath171 , which measures the correlation between the two sources .",
    "we simulate a source with both balanced ( @xmath172 ) and skewed ( @xmath173 ) symbol probabilities .",
    "the first setting implies @xmath174 and @xmath175 , where @xmath25 depends on @xmath171 .",
    "the closer @xmath171 to 0.5 , the less correlated the sources , and hence the higher @xmath25 . in the skewed case , given @xmath176 , @xmath120",
    "is fixed , whereas both @xmath177 and @xmath25 depend on @xmath171 . unless otherwise specified , each point of the figures / tables presented in the following has been generated averaging the results obtained encoding @xmath178 samples .      as a first experiment ,",
    "the benefit of the termination policy is assessed .",
    "stationary source @xmath0 emits sequences @xmath12 of @xmath179 symbols , with @xmath172 and @xmath180 , which are encoded with dac at fixed rate @xmath181 bps , i.e. , @xmath182 bps higher than the theoretical s - w bound .",
    "for @xmath1 we assume ideal lossless encoding at average rate @xmath183 bps , so that the total average rate of @xmath0 and @xmath1 is 1.5 bps .",
    "the bit error rate ( ber ) yielded by the decoder is measured for increasing values of the number of termination symbols @xmath87 . the same simulation is performed with @xmath184 . in all simulated cases , the dac overlap has been selected to compensate for the rate penalty incurred by the termination , so as to achieve the 1.5 bps overall target rate .",
    "the overlap factors @xmath185 are selected according to .",
    "the results are shown in fig .",
    "[ fig : term ] ; it can be seen that the proposed termination is effective at reducing the ber .",
    "there is a trade - off in that , for a given rate , increasing @xmath87 reduces the effect of errors in the last symbols , but requires to overlap the intervals more .",
    "it is also interesting to consider the position of the first decoding error as , without termination , errors tend to cluster at the end of the block . for @xmath179 ,",
    "the mean position value is 191 , 178 , 168 , 161 and 95 , with standard deviation 13 , 18 , 25 , 36 and 49 , respectively for @xmath87 equal to 0 , 5 , 10 , 15 and 20 .",
    "for @xmath184 , the mean value is 987 , 954 , 881 , 637 and 536 , with standard deviation 57 , 124 , 229 , 308 and 299 .",
    "the optimal values of @xmath87 are around 15 - 20 symbols .",
    "therefore , we have selected @xmath186 and used this value for all the experiments reported in the following .",
    "( number of termination symbols ) ; @xmath172 , total rate = 1.5 bps , rate of @xmath12 = 0.5 bps , @xmath180.,width=453 ]      next , an experiment has been performed to validate the theoretical analysis of the effects of different overlap designs shown in sect .",
    "[ sec : rate_sel]-b . in fig .",
    "[ fig : overlap ] the performance obtained by using the design of equations and respectively is shown .",
    "the experimental settings are @xmath179 , @xmath187 , fixed rate for @xmath12 of 0.5 bps , and total average rate for @xmath0 and @xmath1 equal to 1.5 bps , with ideal lossless encoding of @xmath1 at rate @xmath177 .",
    "the ber is reported as a function of the source correlation expressed in terms of @xmath188 .",
    "it is worth noticing that the performance yielded by different overlap design rules are almost equivalent .",
    "note that the rule in consistently outperforms that in , confirming that this latter is only optimal for the rate .",
    "there is some difference when @xmath188 is very high ( i.e. , for weakly correlated sources ) .",
    "however , this case is of marginal interest since the performance is poor ( the ber is of the order of 0.1 ) .    , total rate = 1.5 bps).,width=453 ]      the performance of the proposed system is compared with that of a system where the dac encoder and decoder are replaced by a punctured turbo code similar to that in @xcite .",
    "we use turbo codes with rate-@xmath189 generator ( 17,15 ) octal ( 8 states ) and ( 31,27 ) octal ( 16 states ) , and employ s - random interleavers , and 15 decoder iterations .",
    "we consider the case of balanced source ( @xmath190 ) and skewed source ( in particular @xmath191 and @xmath187 ) . for a skewed source , as an improvement with respect to @xcite ,",
    "the turbo decoder has been modified by adding to the decoder metric the _ a priori _ term , as done in @xcite .",
    "block sizes @xmath192 , @xmath179 and @xmath184 have been considered ( with s - random interleaver spread of 5 , 11 and 25 respectively ) ; this allows to assess the dac performance at small and medium block lengths .",
    "besides turbo codes , we also considered the rate - compatible ldpc codes proposed in @xcite .",
    "for these codes , a software implementation is publicly available on the web ; among the available pre - designed codes , we used the matrix for @xmath193 , which is comparable with the block lengths considered for the dac and the turbo code .",
    "the results are worked out in a fixed - rate coding setting as in @xcite , i.e. , the rate is the same for each sample realization of the source . fig .",
    "[ fig:1 ] reports the results for the balanced source case ; the abscissa is @xmath188 , and is related to @xmath171 .",
    "the performance is measured in terms of the residual ber after decoding , which is akin to the distortion in the wyner - ziv binary coding problem with hamming metric .",
    "both the dac and the turbo code generate a description of @xmath12 at fixed rate 0.5 bps ; the total average rate of @xmath0 and @xmath1 is 1.5 bps , with ideal lossless encoding of @xmath1 at rate @xmath177 .",
    "since @xmath183 , we also have that @xmath175 .",
    "this makes it possible to compare these results with the case of skewed sources which is presented later in this section , so as to verify that the performance is uniformly good for all distributions .",
    "the wyner - ziv bound for a doubly symmetric binary source with hamming metric is also reported for comparison .    as can be seen",
    ", the performance of dac slightly improves as the block length increases .",
    "this is mostly due to the effect of the termination . as the number of bits used to terminate the encoder is chosen independently of the block length , the rate penalty for non overlapping the last bits weights more when the block length is small , while the effect vanishes for large block length . in @xcite , where the termination effect is not considered ,",
    "the performance is shown to be almost independent of the block size .",
    "it should also be noted that the value of @xmath66 required for near - optimal performance grows exponentially with the block size . as a consequence ,",
    "the memory which leads to near - optimal performance for @xmath192 or @xmath179 limits the performance for @xmath184 .",
    "we compared both 8-states and 16-states turbo codes .",
    "the 8-states code is often used in practical applications , as it exhibits a good trade - off between performance and complexity ; the 16-states code is more powerful , and requires more computations .",
    "it can be seen that , for block length @xmath192 and @xmath179 , the proposed system outperforms the 8-states and 16-states turbo codes . for block length @xmath184 ,",
    "the dac performs better than the 8-states turbo code , and is equivalent to the 16-states code .",
    "it should be noted that , in this experiment , only the  channel coding performance \" of the dac is tested , since for the balanced source no compression is possible as @xmath194 .",
    "consequently , it is remarkable that the dac turns out to be generally more powerful than the turbo code at equal block length .",
    "note that the performance of the 16-states code is limited by the error floor , and could be improved using an ad - hoc design of the code or the interleaver ; the dac has no error floor , but its waterfall is less steep . for @xmath195 , a result",
    "not reported in fig .",
    "[ fig:1 ] shows that the dac with @xmath179 and @xmath184 also outperform the 8-state turbo - coder with @xmath196 . in fig .",
    "[ fig:1 ] and in the following , it can be seen that turbo codes do not show the typical cliff - effect .",
    "this is due to the fact that , at the block lengths considered in this paper , the turbo code is still very far from the capacity ; its performance improves for larger block lengths , where the cliff - effect can be seen . in terms of the rate penalty , setting a residual ber threshold of @xmath197 , for @xmath179 the dac is almost 0.3 bps away from the s - w limit , while the best 16-state turbo code simulated in this paper is 0.35 bps away ; for @xmath184 the dac is 0.26 bpp away , while the best 8-state turbo code is 0.30 bps away .",
    "the performance of the ldpc code for @xmath193 is halfway between the turbo codes for @xmath179 and @xmath184 , and hence very similar to the dac .",
    ", total rate = 1.5 bps , rate for @xmath12 = 0.5 bps ) : dac versus turbo coding , balanced source .",
    "dac : distributed arithmetic coding ; tc8s and tc16s : 8- and 16-state turbo code with s - random interleaver ; ldpc - r and ldpc - i : regular and irregular ldpc codes from @xcite.,width=453 ]    the results for a skewed source are reported in fig . [ fig:3 ] for @xmath187 . in this",
    "setting , we select various values of @xmath188 , and encode @xmath12 at fixed rate such that the total average rate for @xmath0 and @xmath1 equals 1.5 bps , with ideal lossless encoding of @xmath1 at rate @xmath177 . for fig .",
    "[ fig:3 ] , from left to right , the rates of @xmath12 are respectively 0.68 , 0.67 , 0.66 , 0.64 , 0.63 , 0.61 , 0.59 , and 0.58 bps . consistently with @xcite , all turbo codes considered in this work perform rather poorly on skewed sources . in",
    "@xcite this behavior is explained with the fact that , when the source is skewed , the states of the turbo code are used with uneven probability , leading to a smaller equivalent number of states . on the other hand ,",
    "the dac has good performance also for skewed sources , as it is designed to work with unbalanced distributions .",
    "the performance of the ldpc codes is similar to that of the best turbo codes , and slightly worse than the dac .",
    "similar remarks can be made in the case of @xmath191 , which is reported in fig .",
    "[ fig:2 ] . in this case",
    ", we have selected a total rate of 1 bps , since the source is more unbalanced and hence easier to compress .",
    "the rates for @xmath12 are respectively 0.31 , 0.34 , 0.37 , 0.39 , 0.42 , 0.44 , and 0.47 bps . in this case",
    "the turbo code performance is better than in the previous case , although it is still poorer than dac .",
    "this is due to the fact that the sources are more correlated , and hence the crossover probability on the virtual channel is lower .",
    "therefore , the turbo code has to correct a smaller number of errors , whereas for @xmath187 the correlation was weaker and hence the crossover probability was higher .    , total rate = 1.5 bps ) : dac versus turbo coding , skewed source .",
    "dac : distributed arithmetic coding ; tc8s and tc16s : 8- and 16-state turbo code with s - random interleaver ; ldpc - r and ldpc - i : regular and irregular ldpc codes from @xcite.,width=453 ]    , total rate = 1 bps ) : dac versus turbo coding , skewed source .",
    "dac : distributed arithmetic coding ; tc8s and tc16s : 8- and 16-state turbo code with s - random interleaver ; ldpc - r and ldpc - i : regular and irregular ldpc codes from @xcite.,width=453 ]      we also considered the case of strongly correlated sources , for which high - rate channel codes are needed .",
    "these sources are a good model for the most significant bit - planes of several multimedia signals . due to the inefficiency of syndrome - based coders",
    ", practical schemes often assume that no dsc is carried out on those bit - planes , e.g. , they are not transmitted , and at the decoder they are directly replaced by the side information @xcite .",
    "the results are reported in tab .",
    "[ tab : corr ] for the dac and the 16-state turbo code , when a rate of 0.1 bps is used for @xmath12 .",
    "the table also reports the cross - over probability @xmath171 , corresponding , for a balanced source , to the performance of an uncoded system that reconstructs @xmath12 as the side information @xmath52 .",
    "as can be seen , the dac has similar performance to the turbo codes and ldpc codes , and becomes better when the source is extremely correlated , i.e. , @xmath198 .",
    "@xmath25 & @xmath171 & dac & tc16s + 0.1 & @xmath199 & @xmath200 & @xmath201 + 0.01 & @xmath202 & @xmath203 & @xmath204 + 0.001 & @xmath205 & @xmath206 & @xmath207 +     + @xmath25 & @xmath171 & dac & tc16s + 0.1 & @xmath199 & @xmath208 & @xmath209 + 0.01 & @xmath202 & @xmath210 & @xmath211 + 0.001 & @xmath205 & @xmath212 & @xmath213 +     + @xmath25 & @xmath171 & ldpc - r & ldpc - i + 0.1 & @xmath199 & @xmath214 & @xmath215 + 0.01 & @xmath202 & @xmath216 & @xmath217 + 0.001 & @xmath205 & @xmath218 & @xmath219 +    [ tab : corr ]      finally , the coding efficiency of dac is measured in terms of expected rate required to achieve error - free decoding .",
    "this amounts to re - encoding the sequence at increasing rates , and represents the optimal dac performance if the encoder could exactly predict the decoder behavior .",
    "since each realization of the source is encoded using a different number of bits , this case is referred to as variable - rate encoding .",
    "this scenario is representative of practical distributed compression settings , e.g. , @xcite , in which one seeks the shortest code that allows to reconstruct without errors each realization of the source process .    for this simulation ,",
    "the following setup is used .",
    "the source correlation @xmath25 is kept constant and , for each sample realization of the source , the total rate is progressively increased beyond the s - w bound , in steps of 0.01 bps , until error - free decoding is obtained .",
    "this operation is repeated on 1000 different realizations of the source ; the mean value and standard deviation of the rates yielding correct decoding are then computed .",
    "the results have been worked out for block length @xmath179 , with probabilities @xmath172 and @xmath191 . for @xmath172 , the conditional entropy @xmath25 (",
    "i.e. , the s - w bound ) has been set to 0.5 bps . for @xmath191 , the joint entropy",
    "@xmath188 has been set to 1 bps ; this amounts to coding @xmath1 at the ideal rate of @xmath220 bps , with a s - w bound @xmath221 bps .",
    "the results are reported in tab . [",
    "tab : vrate ] .",
    "as can be seen , the dac has a rate loss of about 0.06 bps with respect to the s - w bound for both the symmetric and skewed source .",
    "the turbo code exhibits a loss of about 0.2 bps and 0.13 bps .",
    "the ldpc - r code has a relatively small loss , i.e. , 0.12 bps in the symmetric case and 0.10 in the skewed one .",
    "the ldpc - i code has a slightly smaller loss , i.e. , 0.09 bps in the symmetric case and 0.075 in the skewed one .",
    "however , the dac still performs slightly better .",
    "it should be noted that , while for ldpc and turbo codes the encoding is done only once thanks to rate - compatibility , for the dac multiple encodings are necessary , leading to higher complexity .",
    ".performance comparison for variable - rate coding : mean and standard deviation of rate needed for lossless compression .",
    "[ cols=\"^,^,^,^,^ \" , ]     [ tab : complexity ]",
    "in the following we provide results for the symmetric dac . we consider two sources with balanced ( @xmath172 ) and unbalanced ( @xmath191 ) distribution with arbitrary rate splitting , and use @xmath86 .      for fixed rate",
    ", we set the total rate of @xmath12 and @xmath52 equal to 1.5 bps .",
    "we consider two cases of rate splitting . in the first case",
    "the rate is equally split ; we choose @xmath222 so as to achieve a rate of 0.75 bps for each source . in the second case",
    "we encode @xmath12 at 0.6 bps and @xmath52 at 0.9 bps .",
    "the performance of the symmetric dac is worked out for @xmath179 and @xmath184 . since symmetric dsc coders typically reconstructs each sequence either without any errors or with a large number of errors @xcite , we report the frame error rate ( fer ) instead of the residual ber , i.e. the probability that a data block contains at least one error after joint decoding . for each point , we simulated at least @xmath178 bits .",
    "[ fig : symm_fr ] shows the results for the symmetric dac .",
    "comparisons with other algorithms can be done based on the following remarks . in @xcite , a symmetric s - w coder is proposed employing turbo codes , which can obtain any rate splitting . in the case that one source is encoded without ambiguity , this reduces to the asymmetric turbo - based s - w coder we have employed in sect .",
    "[ sec : results_asymm ] .",
    "in @xcite it is reported that this algorithm achieves its best performance in the asymmetric points of the s - w region , while it is slightly poorer in the intermediate points .",
    "therefore , in fig . [",
    "fig : symm_fr ] we report the fer corresponding to the best turbo code shown in fig .",
    "[ fig:1 ] for @xmath179 and @xmath184 , as this lower - bounds the fer achieved by @xcite over the entire s - w region .",
    "moreover , we also report the fer achieved by irregular ldpc codes with block length @xmath193 @xcite .",
    "the asymmetric algorithm in @xcite has been extended in @xcite to arbitrary rate splitting , showing that the performance is uniformly good over the entire s - w region .",
    "finally , we also report the fer curve of the asymmetric dac for @xmath184 .    , total rate = 1.5 bps ) .",
    "dac : distributed arithmetic coding ; tc16s : 16-state turbo code with s - random interleaver ; ldpc - i : irregular ldpc codes from @xcite.,width=453 ]    in fig .",
    "[ fig : symm_fr ] , the results for symmetric coding are very similar to what has been observed in the asymmetric case .",
    "the dac achieves very similar ber for @xmath179 and @xmath184 ; hence , the fer is smaller for @xmath179 .",
    "the results are almost independent of the rate splitting between @xmath12 and @xmath52 , as can be seen by comparing the two rate - splitting cases as well as the asymmetric dac .",
    "the turbo codes for @xmath179 and @xmath184 , and the irregular ldpc code , exhibit poorer performance than dac .      for variable rate coding",
    ", we consider the same two settings as in sect .",
    "[ sec : res_vr ] , i.e. , block length @xmath179 , with probabilities @xmath172 and @xmath191 ; in the first case the conditional entropy has been set to 0.5 bps , while in the second case the joint entropy @xmath188 has been set to 1 bps .",
    "the results are shown in fig .",
    "[ fig : symm_vr ] . as can be seen , the performance of the symmetric dac is uniformly good over the entire s - w region , and is significantly better than turbo codes and ldpc codes . in particular ,",
    "the dac suboptimality is between 0.03 - 0.06 bps , as opposed to 0.07 - 0.09 for the irregular ldpc code , and 0.14 - 0.21 for the turbo code .",
    "it should be noted , however , that variable rate coding requires feedback , while the s - w bound is achievable with no feedback , with vanishing error probability as @xmath223 . in our simulations we re -",
    "encode the sequence at increasing rates ( in steps of 0.01 bps ) , which represents the optimal dac performance if the encoder could exactly predict the decoder behavior .    , and those in the bottom - left corner to @xmath191 .",
    "dac : distributed arithmetic coding ; tc16s : 16-state turbo code with s - random interleaver ; ldpc - i : irregular ldpc codes from @xcite .",
    "the solid curves represent the s - w bound.,width=453 ]",
    "we have proposed dac as an alternative to existing dsc coders based on channel codes .",
    "dac can operate in the entire s - w region , providing both asymmetric and symmetric coding .",
    "dac achieves good compression performance , with uniformly good results over the s - w rate region ; in particular , its performance is comparable with or better than that of turbo and ldpc codes at small and medium block lengths .",
    "this is very important in many applications , e.g. , in the multimedia field , where the encoder partitions the compressed file into small units ( e.g. , packets in jpeg 2000 , slices and nalus in h.264/avc ) that have to be coded independently .    as for encoding complexity , which is of great interest for dsc",
    ", dac has linear encoding complexity , like a classical ac @xcite .",
    "turbo codes and the ldpc codes in @xcite also have linear encoding complexity , whereas general ldpc codes typically have more than linear , and typically quadratic complexity @xcite . as a consequence ,",
    "the complexity of dac is suitable for dsc applications .",
    "a major advantage of dac lies in the fact that it can exploit statistical prior knowledge about the source very easily .",
    "this is a strong asset of ac , which is retained by dac .",
    "probabilities can be estimated on - the - fly based on past symbols ; context - based models employing conditional probabilities can also be used , as well as other models providing the required probabilities .",
    "these models allow to account for the nonstationarity of typical real - world signals , which is a significant advantage over dsc coders based on channel codes .",
    "in fact , for channel codes , accounting for time - varying correlations requires to adjust the code rate , which can only be done for the next data block , incurring a significant adaptation delay .",
    "moreover , with channel codes it is not easy to take advantage of prior information ; for turbo codes it has been shown to be possible @xcite , employing a more sophisticated decoder .",
    "another advantage of the proposed dac lies in the fact that the encoding process can be seen as a simple extension of the ac process . as a consequence , it is straightforward to extend an existing scheme employing ac as final entropy coding stage in order to provide dsc functionalities .",
    "liveris , z.  xiong , and c.n .",
    "georghiades , `` distributed compression of binary sources using conventional parallel and serial concatenated convolutional codes , '' in _ proc . of ieee data compression conference _ , 2003 , pp . 193202 .",
    "a.  majumdar , j.  chou , and k.  ramchandran , `` robust distributed video compression based on multilevel coset codes , '' in _ proceedings of thirty - seventh asilomar conference on signals , systems and computers _ , 2003 , pp ."
  ],
  "abstract_text": [
    "<S> distributed source coding schemes are typically based on the use of channels codes as source codes . in this paper </S>",
    "<S> we propose a new paradigm , named  distributed arithmetic coding \" , which extends arithmetic codes to the distributed case employing sequential decoding aided by the side information . </S>",
    "<S> in particular , we introduce a distributed binary arithmetic coder for the slepian - wolf coding problem , along with a joint decoder . the proposed scheme can be applied to two sources in both the asymmetric mode , wherein one source acts as side information , and the symmetric mode , wherein both sources are coded with ambiguity , at any combination of achievable rates . </S>",
    "<S> distributed arithmetic coding provides several advantages over existing slepian - wolf coders , especially good performance at small block lengths , and the ability to incorporate arbitrary source models in the encoding process , e.g. , context - based statistical models , in much the same way as a classical arithmetic coder . </S>",
    "<S> we have compared the performance of distributed arithmetic coding with turbo codes and low - density parity - check codes , and found that the proposed approach is very competitive .    </S>",
    "<S> distributed source coding , arithmetic coding , slepian - wolf coding , wyner - ziv coding , compression , turbo codes , ldpc codes . </S>"
  ]
}