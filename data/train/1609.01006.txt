{
  "article_text": [
    "in biomedical image analysis , a fundamental problem is the segmentation of 3d images , to identify target 3d objects such as neuronal structures @xcite and knee cartilage @xcite . in biomedical imaging",
    ", 3d images often consist of highly anisotropic dimensions @xcite , that is , the scale of each voxel in depth ( the @xmath0-axis ) can be much larger ( e.g. , @xmath1 times ) than that in the @xmath2 plane .    on various biomedical image segmentation tasks ,",
    "deep learning ( dl ) methods have achieved tremendous success in terms of accuracy ( outperforming classic methods by a large margin @xcite ) and generality ( mostly application - independent @xcite ) . for 3d segmentation ,",
    "known dl schemes can be broadly classified into four categories .",
    "( i ) 2d fully convolutional networks ( fcn ) , such as u - net @xcite and dcan @xcite , can be applied to each 2d image slice , and 3d segmentation is then generated by concatenating the 2d results .",
    "( ii ) 3d convolutions can be employed to replace 2d convolutions @xcite , or combined with 2d convolutions into a hybrid network @xcite .",
    "( iii ) tri - planar schemes ( e.g. , @xcite ) apply three 2d convolutional networks based on orthogonal planes ( i.e. , the @xmath2 , @xmath3 , and @xmath4 planes ) to perform voxel classification .",
    "( iv ) 3d segmentation can also be conducted by recurrent neural networks ( rnn ) .",
    "a most representative rnn based scheme is pyramid - lstm @xcite , which uses six generalized long short term memory networks to exploit the 3d context .",
    "there are mainly three issues to the known dl - based 3d segmentation methods .",
    "first , simply linking 2d segmentations into 3d can not leverage the spatial correlation along the @xmath0-direction .",
    "second , incorporating 3d convolutions may incur extremely high computation costs ( e.g. , high memory consumption and long training time @xcite ) .",
    "third , both 3d convolution and other circumventive solutions ( to reduce intensive computation of 3d convolution ) , like tri - planar schemes or pyramid - lstm , perform 2d convolutions with isotropic kernel on anisotropic 3d images .",
    "this could be problematic , especially for images with substantially lower resolution in depth ( the @xmath0-axis ) .",
    "for instance , both the tri - planar schemes and pyramid - lstm perform 2d convolutions on the @xmath4 and @xmath3 planes . for two orthogonal one - voxel wide lines in the @xmath4 plane , one along the @xmath0-direction and the other along the @xmath5-direction",
    ", they may correspond to two structures at very different scales , and consequently may correspond to different types of objects  or even may not both correspond to objects of interest .",
    "but , 2d convolutions on the @xmath4 plane with isotropic kernel are not able to differentiate these two lines . on the other hand , 3d objects of a same type ,",
    "if rotated in 3d , may have very different appearances in the @xmath4 or @xmath3 plane .",
    "this fact makes the features extracted by such 2d isotropic convolutions in the @xmath4 or @xmath3 plane suffer poor generality ( e.g. , may cause overfitting ) .    in common practice ,",
    "a 3d biomedical image is often represented as a sequence of 2d slices ( called a @xmath0-stack ) .",
    "recurrent neural networks , especially lstm @xcite , are an effective model to process sequential data @xcite .",
    "inspired by these facts , we propose a new framework combining two dl components : a fully convolutional network ( fcn ) to extract intra - slice contexts , and a recurrent neural network ( rnn ) to extract inter - slice contexts .",
    "our framework is based on the following ideas .",
    "our fcn component employs a new deep architecture for 2d feature extraction .",
    "it aims to efficiently compress the intra - slice information into hierarchical features . comparing to known fcn for 2d biomedical imaging ( e.g. , u - net @xcite )",
    ", our new fcn is considerably more effective in dealing with objects of very different scales by simulating human behaviors in perceiving multi - scale information .",
    "we introduce a generalized rnn to exploit 3d contexts , which essentially applies a series of 2d convolutions on the @xmath2 plane in a recurrent fashion to interpret 3d contexts while propagating contextual information in the @xmath0-direction .",
    "our key idea is to hierarchically assemble intra - slice contexts into 3d contexts by leveraging the inter - slice correlations .",
    "the insight is that our rnn can distill 3d contexts in the same spirit as the 2d convolutional neural network ( cnn ) extracting a hierarchy of contexts from a 2d image .",
    "comparing to known rnn models for 3d segmentation , such as pyramid - lstm @xcite , our rnn model is free of the problematic isotropic convolutions on anisotropic images , and can exploit 3d contexts more efficiently by combining with fcn .",
    "the essential difference between our new dl framework and the known dl - based 3d segmentation approaches is that we explicitly leverage the anisotropism of 3d images and efficiently construct a hierarchy of discriminative features from 3d contexts by performing systematic 2d operations .",
    "our framework can serve as a new paradigm of migrating 2d dl architectures ( e.g. , cnn ) to effectively exploit 3d contexts and solve 3d image segmentation problems .",
    "u - net and bdc - lstm .",
    "@xmath6u - net is a type of fcn and is applied to 2d slices to exploit intra - slice contexts .",
    "bdc - lstm , a generalized lstm network , is applied to a sequence of 2d feature maps , from 2d slice @xmath7 to 2d slice @xmath8 , extracted by @xmath6u - nets , to extract hierarchical features from the 3d contexts .",
    "finally , a softmax function ( the green arrows ) is applied to the result of each slice in order to build the segmentation probability map.,width=384 ]",
    "a schematic view of our dl framework is given in fig .  [",
    "fig : overview ] .",
    "this framework is a combination of two key components : an fcn ( called @xmath6u - net ) and an rnn ( called bdc - lstm ) , to exploit intra - slice and inter - slice contexts , respectively .",
    "section  [ sec : kunet ] presents the @xmath6u - net , and section  [ sec : rnn ] introduces the derivation of the bdc - lstm .",
    "we then show how to combine these two components in the framework to conduct 3d segmentation . finally , we discuss the training strategy .",
    "the fcn component aims to construct a feature map for each 2d slice , from which object - relevant information ( e.g. , texture , shapes ) will be extracted and object - irrelevant information ( e.g. , uneven illumination , imaging contrast ) will be discarded . by doing so , the next rnn component can concentrate on the inter - slice context .",
    "a key challenge to the fcn component is the multi - scale issue .",
    "namely , objects in biomedical images , specifically in 2d slices , can have very different scales and shapes .",
    "but , the common fcn @xcite and other known variants for segmenting biomedical images ( e.g. , u - net @xcite ) work on a fixed - size perception field ( e.g. , a @xmath9 region in the whole 2d slice ) .",
    "when objects are of larger scale than the pre - defined perception field size , it can be troublesome for such fcn methods to capture the high level context ( e.g. , the overall shapes ) . in the literature ,",
    "a multi - stream fcn was proposed in pronet @xcite to address this multi - scale issue in natural scene images . in pronet ,",
    "the same image is resized to different scales and fed in parallel to a shared fcn with the same parameters .",
    "however , the mechanism of shared parameters may make it not suitable for biomedical images , because objects of different scales may have very different appearances and require different fcns to process .",
    "we propose a new fcn architecture to simulate how human experts perceive multi - scale information , in which multiple submodule fcns are employed to work on different image scales systematically . here",
    ", we use u - net @xcite as the submodule fcn and call the new architecture @xmath6u - net .",
    "u - net @xcite is chosen because it is a well - known fcn achieving huge success in biomedical image segmentation .",
    "u - net @xcite consists of four downsampling steps followed by four upsampling steps . skip - layer connections exist between each downsampled feature map and the commensurate upsampled feature map .",
    "we refer to @xcite for the detailed structure of u - net .",
    "we observed that , when human experts label the ground truth , they tend to first zoom out the image to figure out where are the target objects and then zoom in to label the accurate boundaries of those targets .",
    "there are two critical mechanisms in @xmath6u - net to simulate such human behaviors .",
    "( 1 ) @xmath6u - net employs a sequence of submodule fcns to extract information at different scales sequentially ( from the coarsest scale to the finest scale ) .",
    "( 2 ) the information extracted by the submodule fcn responsible for a coarser scale will be propagated to the subsequent submodule fcn to assist the feature extraction in a finer scale .",
    "first , we create different scales of an original input 2d image by a series of connections of @xmath10 max - pooling layers .",
    "let @xmath11 be the image of scale @xmath12 ( @xmath13 ) , i.e. , the result after @xmath14 max - pooling layers ( @xmath15 is the original image ) .",
    "each pixel in @xmath11 corresponds to @xmath16 pixels in the original image .",
    "then , we use u - net-@xmath12 ( @xmath13 ) , i.e. , the @xmath12-th submodule , to process @xmath17 .",
    "we keep the input window size the same across all u - nets by using crop layers . intuitively ,",
    "u - net-@xmath18 to u - net-@xmath6 all have the same input size , while u - net-@xmath18 views the smallest region with the highest resolution and u - net-@xmath6 views the largest region with the lowest resolution . in other words , for any @xmath19 , u - net-@xmath20 is responsible for a larger image scale than u - net-@xmath21 .",
    "second , we need to propagate the higher level information extracted by u - net-@xmath12 ( @xmath22 ) to the next submodule , i.e. , u - net-(@xmath14 ) , so that clues from a coarser scale can assist the work in a finer scale .",
    "a natural strategy is to copy the result from u - net-@xmath12 to the commensurate layer in u - net-(@xmath14 ) . as shown in fig .",
    "[ fig : kunet ] , there are four typical ways to achieve this : ( a ) u - net-(@xmath14 ) only uses the final result from u - net-@xmath12 and uses it at the start ; ( b ) u - net-(@xmath14 ) only uses the final result from u - net-@xmath12 and uses it at the end ; ( c ) u - net-(@xmath14 ) only uses the most abstract information from u - net-@xmath12 ; ( d ) u - net-(@xmath14 ) uses every piece of information from u - net-@xmath12 .",
    "based on our trial studies , type ( a ) and type ( d ) achieved the best performance . since type ( a ) has fewer parameters than ( d ) , we chose type ( a ) as our final architecture to organize the sequence of submodule fcns .    from a different perspective , each submodule u - net can be viewed as a  super layer \" .",
    "therefore , the @xmath6u - net is a `` deep '' deep learning model . because the parameter @xmath6 exponentially increases the input window size of the network , a small @xmath6 is sufficient to handle many biomedical images ( we use @xmath23 in our experiments ) .",
    "appended with a @xmath24 convolution ( to convert the number of channels in the feature map ) and a softmax layer , the @xmath6u - net can be used for 2d segmentation problems .",
    "we will show ( see table  [ tab : exp ] ) that @xmath6u - net ( i.e. , a sequence of collaborative u - nets ) can achieve better performance than a single u - net in terms of segmentation accuracy .      in this section , we first review the classic lstm network @xcite , and sketch how to generalize the classic lstm to convolutional lstm @xcite ( denoted by clstm ) .",
    "next , we describe how our rnn component , called bdc - lstm , is extended from clstm .",
    "finally , we propose a deep architecture for bdc - lstm , and discuss its advantages over other variants .    *",
    "classic lstm * : rnn ( e.g. , lstm ) is a neural network that maintains a self - connected internal status acting as a  memory \" .",
    "the ability to `` remember '' what has been seen allows rnn to attain exceptional performance in processing sequential data .",
    "classic lstm @xcite can be defined as follows .",
    "@xmath25    here , @xmath26 and @xmath27 are logistic sigmoid and hyperbolic tangent functions ; @xmath28 , @xmath29 , @xmath30 are the input gate , forget gate , and output gate , @xmath31 , @xmath32 , @xmath33 , @xmath34 are bias terms , and @xmath35 , @xmath36 , @xmath37 are the input , the cell activation state , and the hidden state , at time @xmath12 .",
    "@xmath38 are diagonal weight matrices governing the value transitions . for instance",
    ", @xmath39 controls how the forget gate takes values from the hidden state .",
    "* clstm * : recently , a generalized lstm , denoted by clstm , was developed @xcite .",
    "clstm explicitly assumes that the input is images and replaces the vector multiplication in lstm gates by convolutional operators .",
    "it is particularly efficient in exploiting image sequences .",
    "for instance , it can be used for image sequence prediction either in an encoder - decoder framework @xcite or by combining with optical flows @xcite .",
    "specifically , clstm can be formulated as follows .",
    "@xmath40    here , @xmath41 denotes convolution and @xmath42 denotes element - wise product . except different operators , clstm has the same formulation as classic lstm in equ .",
    "( [ equ : lstm ] ) .",
    "we replace the index @xmath12 by @xmath0 in order to emphasize that clstm works slice by slice in the depth ( @xmath0 ) direction .",
    "the input to clstm is a feature map of size @xmath43 , and the output is a feature map of size @xmath44 , @xmath45 and @xmath46 , and @xmath47 and @xmath48 depend on the size of the convolution kernels and whether padding is used .    * bdc - lstm * : we extend clstm to bi - directional convolutional lstm ( bdc - lstm ) . the key extension is to stack two layers of clstm , which work in two opposite directions ( see fig .",
    "[ fig : bdclstm](a ) ) . the contextual information carried in the two layers , one in @xmath49-direction and the other in @xmath50-direction , is concatenated as output .",
    "it can be interpreted as follows . to determine the hidden state at a slice @xmath0 , we take the 2d hierarchical features in slice @xmath0 ( i.e. , @xmath51 ) and the contextual information from both the @xmath50 and @xmath49 directions .",
    "one layer of clstm will integrate the information from the @xmath49-direction ( resp .",
    ", @xmath50-direction ) and @xmath51 to capture the minus - side ( resp . , plus - side ) context ( see fig .  [",
    "fig : bdclstm](b ) ) .",
    "then , the two one - side contexts ( @xmath50 and @xmath49 ) will be fused .",
    "in fact , pyramid - lstm @xcite can be viewed as a different extension of clstm , which employs six clstms in six different directions ( @xmath52 , @xmath53 , and @xmath54 ) and sums up the outputs of the six clstms . however ,",
    "useful information may be lost during the output summation .",
    "intuitively , the sum of six outputs can only inform a simplified context instead of the exact situations in different directions .",
    "it should be noted that concatenating six outputs may greatly increase the memory consumption , and is thus impractical in pyramid - lstm .",
    "hence , besides avoiding problematic convolutions on the @xmath4 and @xmath3 planes ( as discussed in section  [ sec : intro ] ) , bdc - lstm is in principle more effective in exploiting inter - slice contexts than pyramid - lstm .    * deep architectures * : multiple bdc - lstms can be stacked into a deep structure by taking the output feature map of one bdc - lstm as the input to another bdc - lstm . in this sense , each bdc - lstm can be viewed as a super  layer \" in the deep structure .",
    "besides simply taking one output as another input , we can also insert other operations , like max - pooling or deconvolution , in between bdc - lstm layers . as a consequence ,",
    "deep architectures for 2d cnn can be easily migrated or generalized to build deep architectures for bdc - lstm .",
    "this is shown in fig .",
    "[ fig : bdclstm](c)-(d ) .",
    "the underlying relationship between deep bdc - lstm and 2d deep cnn is that deep cnn extracts a hierarchy of non - linear features from a 2d image and a deeper layer aims to interpret higher level information of the image , while deep bdc - lstm extracts a hierarchy of hierarchical contextual features from the 3d context and a deeper bdc - lstm layer seeks to interpret higher level 3d contexts .    in @xcite ,",
    "multiple clstms were simply stacked one by one , maybe with different kernel sizes , in which a clstm `` layer '' may be viewed as a degenerated bdc - lstm `` layer '' .",
    "when considering the problem in the context of cnn , as discussed above , one can see that no feature hierarchy was even formed in these simple architectures . usually , convolutional layers are followed by subsampling , such as max - pooling , in order to form the hierarchy .",
    "we propose a deep architecture combining max - pooling , dropout and deconvolution layers with the bdc - lstm layers .",
    "the detailed structure is as follows ( the numbers in parentheses indicate the size changes of the feature map in each 2d slice ) . input ( @xmath55 ) , dropout layer with @xmath56 , two bdc - lstms with 64 hidden units and @xmath57 kernels ( @xmath58 ) , @xmath59 max - pooling ( @xmath60 ) , dropout layer with @xmath56 , two bdc - lstms with 64 hidden units and @xmath57 kernels ( @xmath61 ) , @xmath59 deconvolution ( @xmath62 ) , dropout layer with @xmath56 , @xmath63 convolution layer without recurrent connections ( @xmath64 ) , @xmath24 convolution layer without recurrent connections ( @xmath65 ) .",
    "( note : all convolutions in bdc - lstm use the same kernel size as indicated in the layers . ) thus , to predict the probability map of a @xmath66 region , we need the @xmath67 region centered at the same position as the input . in the evaluation stage , the whole feature map can be processed using the overlapping - tile strategy @xcite , because deep bdc - lstm is fully convolutional along the @xmath0-direction .",
    "suppose the feature map of a whole slice is of size @xmath68 .",
    "the input tensor will be padded with zeros on the borders to resize into @xmath69 .",
    "then , a sequence of @xmath70 patches will be processed each time .",
    "the results are stitched to form the 3d segmentation .",
    "the motivation of solving 3d segmentation by combining fcn ( @xmath6u - net ) and rnn ( bdc - lstm ) is to distribute the burden of exploiting 3d contexts .",
    "@xmath6u - net extracts and compresses the hierarchy of intra - slice contexts into feature maps , and bdc - lstm distills the 3d context from a sequence of abstracted 2d contexts .",
    "these two components work coordinately , as follows .",
    "suppose the 3d image consists of @xmath71 2d slices of size @xmath72 each .",
    "first , @xmath6u - net extracts feature maps of size @xmath73 , denoted by @xmath74 , from each slice @xmath0 .",
    "the overlapping - tile strategy @xcite will be adopted when the 2d images are too big to be processed by @xmath6u - net in one shot .",
    "second , bdc - lstm works on @xmath74 to build the hierarchy of non - linear features from 3d contexts and generate another @xmath73 feature map , denoted by @xmath75 , @xmath76 . for each slice",
    "@xmath0 , @xmath77 ( @xmath78 ) will serve as the context ( @xmath79 in our implementation ) .",
    "finally , a softmax function is applied to @xmath80 to generate the 3d segmentation probability map .",
    "our whole network , including @xmath6u - net and bdc - lstm , can be trained either end - to - end or in a decoupled manner .",
    "sometimes , biomedical images are too big to be processed as a whole .",
    "overlapping - tile is a common approach @xcite , but can also reduce the range of the context utilized by the networks .",
    "the decoupled training , namely , training @xmath6u - net and bdc - lstm separately , is especially useful in situations where the effective context of each voxel is very large .",
    "given the same amount of computing resources ( e.g. , gpu memory ) , when allocating all resources to train one component only , both @xmath6u - net and bdc - lstm can take much larger tiles as input . in practice , even though the end - to - end training has its advantage of simplicity and consistency , the decoupled training strategy is preferred for challenging problems .    @xmath6u - net is initialized using the strategy in @xcite and trained using adam @xcite , with _",
    "first moment coefficient _",
    "( @xmath81)=0.9 , _ second moment coefficient _ ( @xmath82)=0.999 , @xmath83=@xmath84 , and a constant learning rate @xmath85 .",
    "the training method for bdc - lstm is rms - prop @xcite , with _ smoothing constant _ ( @xmath86)=0.9 and @xmath83=@xmath87 .",
    "the initial learning rate is set as @xmath88 and halves every 2000 iterations , until @xmath87 . in each iteration , one training example is randomly selected .",
    "the training data is augmented with rotation , flipping , and mirroring . to avoid gradient explosion ,",
    "the gradient is clipped to @xmath89 $ ] in each iteration .",
    "the parameters in bdc - lstm are initialized with random values uniformly selected from @xmath90 $ ] .",
    "we use a weighted cross - entropy loss in both the @xmath6u - net and bdc - lstm training . in biomedical image segmentation",
    ", there may often be certain important regions in which errors should be reduced as much as possible .",
    "for instance , when two objects touch tightly to each other , it is important to make correct segmentation along the separating boundary between the two objects , while errors near the non - touching boundaries are of less importance .",
    "hence , we adopt the idea in @xcite to assign a unique weight for each voxel in the loss calculation .",
    "our framework was implemented in torch7 @xcite and the rnn package @xcite .",
    "we conducted experiments on a workstation with 12 gb nvidia tesla k40 m gpu , using cudnn library ( v5 ) for gpu acceleration .",
    "our approach was evaluated in two 3d segmentation applications and compared with several state - of - the - art dl methods .",
    ".experimental results on the isbi neuron dataset and in - house 3d fungus datasets . [ cols=\"<,^,^,^ \" , ]     * 3d neuron structures * : the first evaluation dataset was from the isbi challenge on the segmentation of neuronal structures in 3d electron microscopic ( em ) images @xcite .",
    "the objective is to segment the neuron boundaries .",
    "briefly , there are two image stacks of @xmath91 voxels , where each voxel measures @xmath92 . noise and",
    "section alignment errors exist in both stacks .",
    "one stack ( with ground truth ) was used for training , and the other was for evaluation .",
    "we adopted the same metrics as in @xcite , i.e. , foreground - restricted rand score ( @xmath93 ) and information theoretic score ( @xmath94 ) after border thinning .",
    "as shown in @xcite , @xmath93 and @xmath94 are good approximation to the difficulty for human to correct the segmentation errors , and are robust to border variations due to the thickness .",
    "* 3d fungus structures * : our method was also evaluated on in - house datasets for the segmentation of tubular fungus structures in 3d images from serial block - face scanning electron microscope .",
    "the ratio of the voxel scales is @xmath95 .",
    "there are five stacks , in all of which each slice is a grayscale image of @xmath96 pixels .",
    "we manually labeled the first 16 slices in one stack as the training data and used the other four stacks , each containing 81 sections , for evaluation .",
    "the metric to quantify the segmentation accuracy is _ pixel error _",
    ", defined as the euclidean distance between the ground truth label ( 0 or 1 ) and segmentation probability ( a value in the range of @xmath97 $ ] ) .",
    "note that we do not use the same metric as the neuron dataset , because the  border thinning \" is not applicable to the fungus datasets .",
    "the pixel error was actually adopted at the time of the isbi neuron segmentation challenge , which is also a well - recognized metric to quantify pixel - level accuracy .",
    "it is also worth mentioning that it is impractical to label four stacks for evaluation due to intensive labor .",
    "hence , we prepared the ground truth every 5 sections in each evaluation stack ( i.e. , 5 , 10 , 15 , @xmath98 , 75 , 80 ) .",
    "totally , 16 sections were selected to estimate the performance on a whole stack .",
    "namely , all 81 sections in each stack were segmented , but 16 of them were used to compute the evaluation score in the corresponding stack .",
    "the reported performance is the average of the scores for all four stacks .",
    "recall the four categories of known deep learning based 3d segmentation methods described in section  [ sec : intro ] .",
    "we selected one typical method from each category for comparison .",
    "( 1 ) u - net @xcite , which achieved the state - of - the - art segmentation accuracy on 2d biomedical images , is selected as the representative scheme of linking 2d segmentations into 3d results .",
    "( note : we are aware of the method @xcite which is another variant of 2d fcn and achieved excellent performance on the neuron dataset .",
    "but , different from u - net , the generality of @xcite in different applications is not yet clear .",
    "our test of @xcite on the in - house datasets showed an at least @xmath99 lower f1-score than u - net .",
    "thus , we decided to take u - net as the representative method in this category . )",
    "( 2 ) 3d - conv @xcite is a method using cnn with 3d convolutions .",
    "( 3 ) tri - planar @xcite is a classic solution to avoid high computing costs of 3d convolutions , which replaces 3d convolution with three 2d convolutions on orthogonal planes .",
    "( 4 ) pyramid - lstm @xcite is the best known generalized lstm networks for 3d segmentation .    * results * : the results on the 3d neuron dataset and the fungus datasets are shown in table  [ tab : exp ] .",
    "it is evident that our proposed @xmath6u - net , when used alone , achieves considerable improvement over u - net @xcite .",
    "our approach outperforms the known dl methods utilizing 3d contexts .",
    "moreover , one can see that our proposed deep architecture achieves better performance than simply stacking multiple bdc - lstms together . as discussed in section  [ sec : rnn ] , adding subsampling layers like in 2d cnn makes the rnn component able to perceive higher level 3d contexts .",
    "it worth mentioning that our two evaluation datasets are quite representative .",
    "the fungus data has small anisotropism ( @xmath0 resolution is close to @xmath2 resolution ) .",
    "the 3d neuron dataset has large anisotropism ( @xmath0 resolution is much less than @xmath2 resolution ) .",
    "the effectiveness of our framework on handling and leveraging anisotropism can be demonstrated .",
    "we should mention that we re - implemented pyramid - lstm @xcite in torch7 and tested it on the fungus datasets .",
    "but , the memory requirement of pyramid - lstm , when implemented in torch7 , was too large for our gpu .",
    "for the original network structure , the largest possible cubical region to process each time within our gpu memory capacity was @xmath100 . using the same hyper - parameters in @xcite , we can not obtain acceptable results due to the limited processing cube .",
    "( the result of pyramid - lstm on the 3d neuron dataset was fetched from the isbi challenge leader board on may 10 , 2016 . ) here , one may see that our method is much more efficient in gpu memory , when implemented under the same deep learning framework and tested on the same machine .",
    "some results are shown in fig .",
    "[ fig : visual ] to qualitatively compare the results using the fcn component alone and the results of combining rnn and fcn . in general , both methods make nearly no false negative errors .",
    "but , the rnn component can help to ( 1 ) suppress false positive errors by maintaining inter - slice consistency , and ( 2 ) make more confident prediction in ambiguous cases by leveraging the 3d context . in a nutshell",
    ", fcn collects as much discriminative information as possible within each slice and rnn makes further refinement according to inter - slice correlation , so that an accurate segmentation can be made at each voxel .",
    "in this paper , we introduce a new deep learning framework for 3d image segmentation , based on a combination of an fcn ( i.e. , @xmath6u - net ) to exploit 2d contexts and an rnn ( i.e. , bdc - lstm ) to integrate contextual information along the @xmath0-direction .",
    "evaluated in two different 3d biomedical image segmentation applications , our proposed approach can achieve the state - of - the - art performance and outperform known dl schemes utilizing 3d contexts .",
    "our framework provides a new paradigm to migrate the superior performance of 2d deep architectures to exploit 3d contexts . following this new paradigm",
    ", we will explore bdc - lstms in different deep architectures to achieve further improvement and conduct more extensive evaluations on different datasets , such as brats ( http://www.braintumorsegmentation.org/ ) and mrbrains ( http://mrbrains13.isi.uu.nl ) .",
    "this research was support in part by nsf grants ccf-1217906 and ccf-1617735 and nih grants r01-gm095959 and u01-hl116330 .",
    "also , we would like to thank dr .",
    "viorica patraucean at university of cambridge ( uk ) for discussion of bdc - lstm , and prof .",
    "david p. hughes and dr .",
    "maridel fredericksen at pennsylvania state university ( us ) for providing the 3d fungus datasets ."
  ],
  "abstract_text": [
    "<S> segmentation of 3d images is a fundamental problem in biomedical image analysis . </S>",
    "<S> deep learning ( dl ) approaches have achieved state - of - the - art segmentation performance . to exploit the 3d contexts using neural networks , </S>",
    "<S> known dl segmentation methods , including 3d convolution , 2d convolution on planes orthogonal to 2d image slices , and lstm in multiple directions , all suffer incompatibility with the highly anisotropic dimensions in common 3d biomedical images . in this paper </S>",
    "<S> , we propose a new dl framework for 3d image segmentation , based on a combination of a fully convolutional network ( fcn ) and a recurrent neural network ( rnn ) , which are responsible for exploiting the intra - slice and inter - slice contexts , respectively . to our best knowledge , this is the first dl framework for 3d image segmentation that explicitly leverages 3d image anisotropism . </S>",
    "<S> evaluating using a dataset from the isbi neuronal structure segmentation challenge and in - house image stacks for 3d fungus segmentation , our approach achieves promising results comparing to the known dl - based 3d segmentation approaches .    </S>",
    "<S> = 1 </S>"
  ]
}