{
  "article_text": [
    "weak gravitational lensing has proven to be a versatile method for measuring the mass distribution of galaxy clusters . with the detection of cosmic shear it has turned into an important tool for providing constraints on cosmological parameters such as @xmath2 and @xmath3",
    "( see @xcite ) .",
    "common to all applications of weak lensing studies is the requirement for statistical analysis of a great number of objects .",
    "single sheared galaxies , due to their unknown intrinsic ellipticity and further observational uncertainties , can only give a reasonable shear estimate as part of a large sample .",
    "the accuracy of shear estimation methods poses a bottleneck to observational cosmology . especially as surveys are getting larger ( pan - starrs-3@xmath0 , des , euclid ) , shear calibration bias could annihilate the gain from improved statistics for larger galaxy samples .",
    "for this reason several competitions for the calibration of shear measurement pipelines and the development of improved methods using simulated data have been hosted .",
    "the elimination of biases among the many methods presented there has , however , only been partially successful .    in this paper",
    "we make use of artificial neural networks in order to improve the existing shear measurement pipeline introduced by @xcite ( ksb ) and further developed by @xcite and @xcite .",
    "we apply these to the data simulated by great08 @xcite and show that existing biases can be almost entirely removed .",
    "the fundamentals of the ksb method are to measure galaxy shapes derived from second moments @xmath4 integrated within a circular gaussian weight function . from these",
    ", _ polarizations _ can be defined as @xmath5    observed polarizations @xmath6 must be corrected for psf anisotropy @xmath7 and their responsitivity to shear @xmath8 as based on psf size and the galaxy s shape .",
    "ksb achieves this by linear corrections , such that @xmath9 where @xmath10 is the galaxy s smear polarizability tensor and @xmath11 is calculated as @xmath12 from smear polarizability tensors and shear polarizability tensors @xmath13 measured on the galaxy and the psf ( denoted with a star ) .",
    "these tensors are weighted fourth order moments of the respective light distributions .    assuming that galaxies show no intrinsic alignment on the sky , one can thus obtain a shear estimate as @xmath14    there exist a large number of implementations of ksb , differing from each other in subtle choices of the method for source extraction , determination of the radius for the weight function @xmath15 , @xmath11 tensor inversion , weighting , cuts ( e.g. , eliminating objects with large ellipticities or small values of @xmath11 ) , correction factors and further details @xcite .",
    "it is not clear a priori and will likely depend on the particular data set which is the right choice on each of these points .",
    "different decisions imply different biases ( see step2 , @xcite ) , which have to be taken into account .    the bias introduced by not having calibrated a method correctly",
    "can only be avoided by carefully simulating the process for the very data that is to be analyzed .",
    "this means that data with known shear have to be simulated .",
    "it must be the aim of these simulations to reproduce the properties of the respective sample of galaxies as accurately as possible ( e.g. , in terms of intrinsic light distribution , psf , errors introduced by the data reduction and noise properties ) in order for the calibrations to be appropriate .    in some cases using simulations a shear calibration bias has been found and corrected manually .",
    "for instance , t. schrabback and @xcite multiply their shear estimates by @xmath16 and @xmath17 , respectively , with factors found by calibration for step1 and step2 data .",
    "after such manual corrections , however , it is very likely that there is a residual bias , not only because the corrections done are usually very simple but also because bias likely differs for different galaxy properties and data sets .",
    "we propose to make best use of the anyhow required simulations by letting neural networks analyze the data and estimate shear after training them on simulated data with known shear .",
    "using a pipeline s ellipticity estimates and further parameters that might be indicative for the bias present on the respective galaxy , we test how well neural networks are able to eliminate biases and improve the shear estimate .",
    "provided that such a scheme is successful , correct simulations allow for optimal calibration of shear measurement pipelines .",
    "we point out that it is not the subject of this work to use neural networks on the pixelated light distribution of the galaxies itself , but to start from data which are quite close to an exact shear measurement already .",
    "the advantage of this method is that the network is fed with the most relevant parameters for shape estimates on a catalog basis , keeping training and application comparatively computation inexpensive .",
    "neural networks are nowadays commonly used in astronomy , be it for the detection and classification of objects or for finding photometric redshift estimates @xcite .",
    "the flavor of networks most frequently used and also to be employed in this paper is multi - layer perceptrons . from inputs @xmath18 fed to an input layer of neurons",
    ", parameters are transferred through a number of hidden layers to finally make for one or more network outputs @xmath19 ( see figure  [ fig : perceptron ] ) .",
    "a neuron @xmath20 s output @xmath21 depends on the incoming signal from connected nodes @xmath22 in the previous layer , weighted by connection weights @xmath23 and transformed by the neuron s activation function @xmath24 : @xmath25    while in general @xmath24 could be chosen differently for each node , it is usually taken to be the same nonlinear function for all hidden nodes and the identity for the input and output layers nodes . in our case",
    "we use a sigmoidal function @xmath26 , where node @xmath20 is a hidden node .",
    "the weights @xmath27 of the connections between two adjacent layers nodes @xmath20 and @xmath22 and from an additional bias node which accounts for an individual node threshold are to be optimized such that a cost function @xmath28 of the network output becomes minimal .",
    "a typical choice for the cost function is the sum over squared errors of outputs @xmath29 on training sets @xmath30 , for which true answers @xmath31 are known .",
    "an additional term quadratic in the weights is added for regularization , penalizing large weights which characterize overfitting to specific data .",
    "thus @xmath28 becomes @xmath32    for training such a network , true answers for each training set have to be known , such that error back - propagation @xcite can be used for optimizing the weights . ]    however , for weak lensing measurements we can only expect the network to return a true shear component @xmath33 _ on average _ for a large sample @xmath34 of galaxies .",
    "training with true shear as the expected network output for all single galaxy data is counterproductive .",
    "we would rather like the networks to minimize the squared error between @xmath35 and @xmath36 , i.e. the average output of galaxies @xmath30 on a sample of galaxies @xmath34 , for each shear / ellipticity component .",
    "we can express this with a cost function of the form @xmath37 the back - propagation algorithm must in this case be adapted accordingly .",
    "we start from code provided by @xcite and implement the algorithm as described in the appendix ( section  [ sec : bpap ] ) .",
    "for training and testing our network with the scheme described in more detail in the appendix , we use simulated galaxy images with known shear from the `` realnoise blind '' data sets of great08 @xcite .",
    "table  [ tbl : samples ] gives an overview of the six samples analyzed in our work .",
    "each of the samples contains sets of six image files of 10,000 simulated galaxies , each set being sheared with a true shear @xmath38 ^ 2 $ ] as plotted in figure  a1 of @xcite .    from the 2700 sets of galaxies in the `` realnoise blind ''",
    "sample of great08 , we pick those 1500 sets from the fiducial ( medium ) signal to noise group which share the same psf ( the fiducial psf also labeled psf 1 by great08 ) but differ in terms of galaxy size and type . in the following analysis",
    "we denote these as sample 1 .    in order to find how well such corrections can work on galaxies with different signal - to - noise levels ,",
    "we pick two more samples .",
    "as these are homogeneous in all galaxy property distributions , they are not as realistic as sample 1 but still can give an indication of the dependence of network performance on signal - to - noise ratio .",
    "therefore , we perform a similar analysis with the 300 high signal to noise data sets ( sample 2 ) and the 300 low signal to noise sets ( sample 3 ) , which all share the same galaxy properties and psf . it should be noted , though , that in the latter case signal is so low that without an input catalog source extraction suffers a significant rate of false detections . on real single frame data with similar signal to noise ratio , the centroid position on stacked frames could be used to eliminate false detections and have well - defined centroid positions , improving the accuracy of the measurement . in our case",
    "we cross - correlate catalogs against the great08 grid positions to achieve complete and clean detections .      in order to study the influence of psf circularization as an alternative method of psf anisotropy correction , we repeat the analysis with three more samples .",
    "a sample of all galaxies with medium signal - to - noise level is denoted as sample 1c . unlike sample 1",
    ", it contains data with all three psfs used in the great08 challenge . for this sample",
    "we circularize all three psfs to the same circular target psf before running ksb .",
    "we are using the method described by @xcite in its implementation by @xcite .",
    "the target psf is similar to the initial ones but slightly larger and circular , with a moffat profile of 3 pixels fwhm and @xmath39 . after having built a model of the initial psf using 100 stars each , we convolve the data with a kernel model consisting of a superposition of four gaussians with @xmath40 multiplied with polynomials in @xmath41 and @xmath42 of order @xmath43 , respectively .",
    "we do a @xmath44 fit of the 50 model parameters to reach our target psf by convolution with this kernel .    due to the larger size of sample 1c , we reserve a larger number of galaxies for blind testing the networks later on .",
    "we prepare two more similarly circularized samples 2c and 3c using the data from samples 2 and 3 .",
    "these data sets are homogeneous in all galaxy properties and therefore not as realistic as sample 1c .",
    "llllllll 1 & 1500 & 20 & 1 & mixed & 1394 & 53 & 53 + 1c & 2100 & 20 & 1/2/3 & mixed & 867 & 33 & 1200 + 2 & 300 & 40 & 1 & fiducial & 260 & 10 & 30 + 2c & 300 & 40 & 1 & fiducial & 260 & 10 & 30 + 3 & 300 & 10 & 1 & fiducial & 260 & 10 & 30 + 3c & 300 & 10 & 1 & fiducial & 260 & 10 & 30 +      on the 2700 sets of 10000 galaxies each from the great08 `` realnoise blind '' challenge we run a ksb implementation ksb@xmath45 , based on the version assembled by t. schrabback and denoted as ts in @xcite .",
    "( see also @xcite ) .",
    "after source extraction with sextractor the pipeline uses ` analyse ` to calculate for each galaxy the tensors @xmath10 and @xmath13 . to [ eqn : ksb3 ] ] psf anisotropy correction is done with this and @xmath11 is computed as @xmath46 stars denoting quantities measured on the psf .",
    "this tensor is applied to the measured polarizations using trace inversion to find individual galaxy shear estimates @xmath47 objects with @xmath48 are discarded .",
    "the estimate labeled as ksb@xmath45 in the following analysis is always @xmath49 , scaled with a calibration factor as optimized for this implementation of ksb using step1 data .      from the output of the ksb@xmath45 pipeline",
    "we take @xmath50 as the starting point for neural network analysis . as potential predictors for bias",
    "we add the weight function radius @xmath15 which in our pipeline is equal to sextractor s ` flux_radius ` , the flux as measured by ` analyse ` , all four components of @xmath11 and the pipeline s error estimates for the initial shape measurements @xmath51 .",
    "the networks used feature three hidden layers of ten nodes each for both components and are trained using the algorithm described ] and the true shears published by great08 after the end of the challenge .",
    "we split each sample into a subset used for training and a subset for later blind testing of network performance .",
    "following the optimized ratio of gradient to validation sets derived by @xcite for the asymptotic case of many available sets , we split the training sets again into subsets used for determining the gradient and others required for validation during training .",
    "the respective sizes of subsets are given in table  [ tbl : samples ] .",
    "for each sample the training process is iterated 500 times with different random initial weight configurations and a random allocation of training sets into gradient and validation sets .",
    "it is necessary to ensure for evaluating the networks or any real - world application that the networks trained really perform consistently well on the data used for training and similar data not used for training or selecting the networks , in our case the blind sets ( cf .",
    "table  [ tbl : samples ] ) .",
    "overtrained networks are generally characterized by some weights becoming comparatively large . while a penalization of large weights by the second term in eqn .",
    "[ eqn : costfunction ] already reduces overfitting to the training sample , where the number of training sets is sufficiently small overtraining may still occur because the reduction in errors from overfitting outweights the penalization due to large weights . for all following analyses ,",
    "we therefore use the sum of squared weights , @xmath52 to discard networks which are more than @xmath53 above the average in @xmath54 for the sample of networks cropped at 1.2 times the median @xmath54 .",
    "this deselects about half of the networks on each of the samples , some of which might in fact not be overtrained . in the presence of larger samples , therefore , when more careful selection of networks is possible , performance might still increase . for all following analyses ,",
    "we only use the weight - selected networks not discarded by these criteria .    to compare the performance of the networks left on training and blind data",
    ", we plot the root mean square error of the shear @xmath55 measured against the true shear @xmath56 , @xmath57 which the weight - selected networks achieve on the data used for training and the blind sets .",
    "the plot shown in figure  [ fig : blindr1 ] shows the result for sample 1c , component 1 , for which the blind rms and training rms are equal within statistical uncertainty . for the other samples and components , due to the smaller number of blind sets ,",
    "scatter is significantly larger and small constant offsets from the identity in both directions occur , likely due to the particular properties of the blind sets . in all cases ,",
    "the networks performing best on the training data perform consistently well on blind data .",
    "we select the best networks on each sample and component simply by taking the network with the smallest squared errors on the training data , not taking into account their performance on the blind data . after having discarded overfitted networks according to their weights as described above , this results in networks performing consistently well on training data and blind data .",
    "we perform the following analyses on the blind data sets only .",
    "as the results found on training and blind data agree within the statistical uncertainty , we use the complete sample of data sets for the analysis done in section  [ sec : sixpack ] , as this is necessary here . for sample 1c , also",
    "the analysis in section  [ sec : sixpack ] is done exclusively on the blind sample .      we analyze the performance of plain ksb@xmath45 and the neural networks selected in the previous section . for each sample and component , we calculate both for the blind and the training sets a quality parameter @xmath58 averaging the rms ( cf . eqn .",
    "[ eqn : rms ] ) over all galaxy sets within each sample . resulting values of @xmath59",
    "are shown in tables  [ tbl : bias ] and [ tbl : biasc ] .",
    "note that @xmath59 is smaller than the great08 quality parameter @xmath60 because we do not average the residuals over similar sets until section  [ sec : sixpack ] .",
    "results of the circularized samples 1c to 3c are generally better in comparison to similar sets with anisotropic psfs which have to be corrected for by a @xmath61 term . ] a more detailed discussion of the advantages of circularization combined with bias correction is given in section  [ sec : circ ] .",
    "we calculate additive and multiplicative biases , following @xcite and @xcite .",
    "we apply a linear fit of residual shears @xmath62 against true shears @xmath56 , i.e. @xmath63    results for the six samples are shown in tables  [ tbl : bias ] and [ tbl : biasc ] and figure  [ fig : linbias ] . for neural network analysis , both multiplicative and additive bias are well within the range of most successful methods in the great08 competition @xcite . the requirements for future surveys as computed by @xcite are always fulfilled for medium and high signal to noise in terms of @xmath64 .",
    "for the multiplicative bias criterion , @xmath65 , the network estimate is successful at least within an order of magnitude .",
    "the higher the signal - to - noise ratio , the better multiplicative bias can also be corrected , while especially for smaller signal there seems to be a tendency of @xmath66 for the neural network estimate , potentially due to the weaker shear signal .",
    "a modified plot of @xmath67 and @xmath68 at the three different signal - to - noise levels with all methods participating in great08 and including neural network blind estimates both with and without circularization is shown in figure  [ fig : cm ] .",
    "the fact that the @xmath68 and @xmath69 found for blind data are consistent with the @xmath68 and @xmath69 found on training data is additional evidence that the networks we use are not overfitted to the training data . in 22 out of the 24 sets ,",
    "components and bias parameters , linear bias corresponds within 1@xmath70 , in the other two cases within 2@xmath70 of the bias measurement uncertainty .",
    "lll|rr|rr|rr|rrrr ksb@xmath45 & 1 & 1 & & & & @xmath71 & 0.20 & 3.06 + & & 2 & & & & @xmath72 & 0.20 & 0.74 + ksb@xmath45 aff & 1 & 1 & & & & @xmath73 & 0.20 & 1.20 + & & 2 & & & & @xmath74 & 0.20 & 0.73 + ksb@xmath45+nn & 1 & 1 & @xmath75 & @xmath76 & @xmath77 & @xmath78 & @xmath79 & @xmath80 & @xmath81 & 0.19 & 0.37 + & & 2 & @xmath75 & @xmath82 & @xmath83 & @xmath84 & @xmath85 & @xmath86 & @xmath87 & 0.19 & 0.35 + ksb@xmath45 & 2 & 1 & & & & @xmath88 & 0.07 & 3.64 + & & 2 & & & & @xmath89 & 0.07 & 0.74 + ksb@xmath45 aff & 2 & 1 & & & & @xmath90 & 0.07 & 0.13 + & & 2 & & & & @xmath91 & 0.07 & 0.00 + ksb@xmath45+nn & 2 & 1 & @xmath92 & @xmath93 & @xmath94 & @xmath95 & @xmath96 & @xmath97 & @xmath90 & 0.07 & 0.16 + & & 2 & @xmath75 & @xmath98 & @xmath99 & @xmath100 & @xmath101 & @xmath102 & @xmath103 & 0.06 & 0.00 + ksb@xmath45 & 3 & 1 & & & & @xmath104 & 0.30 & 4.32 + & & 2 & & & & @xmath105 & 0.32 & 2.87 + ksb@xmath45 aff & 3 & 1 & & & & @xmath106 & 0.37 & 0.00 + & & 2 & & & & @xmath107 & 0.38 & 0.63 + ksb@xmath45+nn & 3 & 1 & @xmath108 & @xmath109 & @xmath110 & @xmath111 & @xmath112 & @xmath113 & @xmath106 & 0.31 & 0.79 + & & 2 & @xmath114 & @xmath115 & @xmath110 & @xmath116 & @xmath117 & @xmath118 & @xmath119 & 0.32 & 0.88 +    lll|rr|rr|rr|rrrr ksb@xmath45 & 1c & 1 & & & & @xmath120 & 0.16 & 0.87 + & & 2 & & & & @xmath121 & 0.16 & 1.93 + ksb@xmath45 aff & 1c & 1 & & & & @xmath122 & 0.16 & 0.82 + & & 2 & & & & @xmath123 & 0.16 & 0.95 + ksb@xmath45+nn & 1c & 1 & @xmath124 & @xmath125 & @xmath126 & @xmath127 & @xmath128 & @xmath129 & @xmath130 & 0.16 & 0.22 + & & 2 & @xmath131 & @xmath132 & @xmath133 & @xmath134 & @xmath135 & @xmath136 & @xmath137 & 0.15 & 0.44 + ksb@xmath45 & 2c & 1 & & & & @xmath138 & 0.07 & 0.67 + & & 2 & & & & @xmath139 & 0.07 & 2.03 + ksb@xmath45 aff & 2c & 1 & & & & @xmath140 & 0.07 & 0.25 + & & 2 & & & & @xmath91 & 0.07 & 0.00 + ksb@xmath45+nn & 2c & 1 & @xmath141 & @xmath98 & @xmath94 & @xmath95 & @xmath142 & @xmath143 & @xmath90 & 0.06 & 0.18 + & & 2 & @xmath144 & @xmath145 & @xmath146 & @xmath100 & @xmath101 & @xmath102 & @xmath147 & 0.06 & 0.00 + ksb@xmath45 & 3c & 1 & & & & @xmath148 & 0.29 & 4.81 + & & 2 & & & & @xmath149 & 0.29 & 2.24 + ksb@xmath45 aff & 3c & 1 & & & & @xmath150 & 0.33 & 0.82 + & & 2 & & & & @xmath151 & 0.32 & 0.61 + ksb@xmath45+nn & 3c & 1 & @xmath152 & @xmath153 & @xmath154 & @xmath155 & @xmath112 & @xmath156 & @xmath106 & 0.29 & 0.91 + & & 2 & @xmath157 & @xmath115 & @xmath158 & @xmath77 & @xmath159 & @xmath160 & @xmath161 & 0.28 & 0.69 +      the bias of a method likely differs depending on properties such as signal - to - noise ratio , psf , galaxy size and profiles .",
    "therefore in the case of inhomogeneous samples like sample 1 and 1c , the @xmath67 and @xmath68 we find for the whole sample are not necessarily equal to the true additive and multiplicative biases of each homogeneous subsample .",
    "for this reason , a method simply calibrated for @xmath162 by an affine transformation for the whole inhomogeneous sample is potentially still biased on each of the homogeneous subsamples and consequently for any real - world application .",
    "thus in order to correctly analyze the bias by means of fitting multiplicative and additive biases , each sample would have to be splitted into homogeneous subsamples first .",
    "we develop another scheme of analyzing bias here which can be applied to homogeneous and inhomogeneous samples alike .",
    "the great08 data sets which we used for training the networks are made up of files containing 10,000 galaxies each , six of which again share the exact same shear values , observing conditions in terms of psf , signal - to - noise ratio and galaxy properties .",
    "this is a very favorable setting for bias analysis , as the composition of errors from bias and scatter can be estimated by comparing the accuracy of shear estimates on single sets and on six times larger overall sets .",
    "when measuring the shear of a very large homogeneous set @xmath22 of @xmath163 galaxies with true shear @xmath164 , the only residual for component @xmath20 will be the bias @xmath165 . for a linear bias @xmath166 and @xmath167 , we would find @xmath168 . in the limit of infinitely large sets the squared error of the shear estimate",
    "@xmath169 will be @xmath170    for smaller @xmath171 , the scatter @xmath172 of the individual galaxy measurement in that set will add to the errors , leading to @xmath173 this bias and the scatter will of course depend of the properties of the particular set @xmath22 .",
    "for the following analysis , we are interested only in a decomposition of our total mean squared error into bias and scatter",
    ". we will thus calculate @xmath174 at two different sample sizes @xmath175 to find the root mean square of the bias @xmath176 and the scatter @xmath177 .     at different sample sizes .",
    "plotted are measured @xmath178 values for @xmath179 galaxies on component 1 of sample 1c for the neural network blind data estimate ( triangles / solid line ) and the affine transformation of ksb outputs with @xmath180 , @xmath181 ( squares / dashed line ) .",
    "the curves show the @xmath182 from eqn .",
    "[ eqn : sixplot ] for the bias and scatter as found in table  [ tbl : biasc ] for the two methods.,scaledwidth=40.0% ]    we perform the analysis on the six samples .",
    "we calculate @xmath183 , similar to equation  [ eqn : q ] yet averaging residuals over the six sets before taking the square .",
    "this is what the leaderboard for the great08 challenge used for ranking the submissions .",
    "note that for pure scatter we would find @xmath184 , whereas for pure bias @xmath185 .",
    "this favors methods with low bias which perform consistently well for different conditions and galaxy parameters . for large future surveys it is of particular importance to achieve a bias which is low even compared to the smaller scatter of the large sample sizes ( i.e. @xmath186 ) , regardless of the individual galaxy properties or observing conditions .",
    "except for sample 1c where the number of blind sets available is large enough and completeness of the sets of 60000 galaxies has been conserved when separating the blind sets , we have to use the complete sample of galaxy sets ( i.e. both the sets used for training and for blind testing ) for calculating @xmath183 . where both @xmath59 and the linear bias are consistent between the training and blind sets , this is not expected to give different results than a test on purely blind data .",
    "we also compare the network results to the results of ksb@xmath45 output scaled with the parameters @xmath187 and @xmath67 found for ksb@xmath45 on each of the samples in section [ sec : linbias ] .",
    "the result , defined as @xmath188 is denoted as ksb aff . in figure",
    "[ fig : cm ] and tables  [ tbl : bias ] and [ tbl : biasc ] . by definition ,",
    "has @xmath189 on any of the samples .",
    "note that signal - to - noise dependence of the bias is likely the strongest influence on bias of all the great08 parameters .",
    "a correction of this dependence by splitting into different signal - to - noise subsamples with different affine scalings , as has been done in the case of @xmath190 , is therefore the most promising method if one additional parameter is taken into account for bias correction .",
    "the dependence of the bias on the signal - to - noise ratio has in fact been taken into account empirically in @xcite .",
    "the fact that the neural network estimate outperforms @xmath190 on the inhomogeneous samples 1 and 1c shows that the networks successfully take other dependences of the bias into account . for direct comparison , a plot of the quality parameter @xmath178 for the network estimate and ksb aff . as found for component 1 at different set sizes @xmath171 on the circularized sample 1c",
    "is shown in figure  [ fig : sixplot ] . drawn",
    "are measured @xmath178 at sample sizes of @xmath191 .",
    "note that this corresponds very well with the curve for eqn .",
    "[ eqn : sixplot ] drawn in terms of @xmath192 and @xmath70 as found in table  [ tbl : biasc ] . the larger residual bias of ksb aff",
    ". on the subsets of the inhomogeneous sample leads to a much smaller asymptotic @xmath178 than for the neural network estimate , which is projected to reach @xmath193 at sufficiently large sample sizes .",
    "complete results for plain ksb@xmath45 , ksb aff . and",
    "the output of best networks are shown in tables  [ tbl : bias ] and [ tbl : biasc ] .",
    "a plot of the composition of squared errors is plotted in figure  [ fig : mse ] , while the great08 quality parameter @xmath183 as a function of signal - to - noise ratio is shown in figure  [ fig : q6 ] . the bias is predominant in plain ksb@xmath45 outputs , especially for the first component .",
    "this is particularly harmful as sample size and signal strength increases . despite lowered statistical uncertainty",
    "the strong bias leads to overall improvements being only slight .",
    "affine transformations according to a fit to the known true shears greatly reduce the bias on the homogeneous subsamples 2 , 2c , 3 and 3c .",
    "this is not surprising , as data sets here only differ by the shear applied to them and the change in bias due to this can be accounted for . on samples with inhomogeneous galaxy properties such as samples 1 and 1c ,",
    "however , bias can not be removed by this and remains significant .",
    "also , the remaining scatter due to noise and differences in bias depending on the individual galaxy properties within the sample can not be decreased by the affine transformation .",
    "neural networks , on the contrary , greatly reduce the bias in any of the samples such that it does not dominate the statistical errors at this sample size .",
    "this works remarkably well even on the inhomogeneous samples .",
    "the observed reduction in scatter indicates that bias depending on the individual galaxy properties has been successfully reduced as well . for a bias reduction scheme to be applied to real data with diverse properties ,",
    "this is of crucial importance .",
    "therefore , as has been done with the neural networks , it can be seen that a linear bias of inhomogeneous samples close to zero should merely be achieved as a side effect of a proper overall calibration , which can be validated using other types of analyses as well .      in order to compare the effects of psf circularization to traditional @xmath10 anisotropy correction",
    ", we compare the results of sample 1 , 2 and 3 to sample 1c , 2c and 3c .",
    "while these are similar in terms of signal - to - noise levels and galaxy properties , they differ in the method used for anisotropy correction .    from the individual measurement @xmath70 calculated in tables  [ tbl : bias ] and",
    "[ tbl : biasc ] we find that scatter can be reduced by circularization , especially for the more noisy data sets .",
    "this can also be seen from comparing the left and right panels of figure  [ fig : linbias ] and is not surprising as the smear responsitivity tensor @xmath10 of the individual galaxy otherwise used for anisotropy correction certainly is influenced by the noise .",
    "the additional term @xmath194 in eqn .",
    "[ eqn : ksb3 ] adds to the scatter . because @xmath195 for a circular psf , this is not the case in the circularized samples .",
    "circularization , however , appears to add to or at least change the bias present in the ksb@xmath45 output . on the medium to low signal - to - noise data , calibration by the neural networks",
    "is successful such that the overall result can be improved . on the contrary ,",
    "a fitted affine transformation of ksb@xmath45 output on sample 1c is still strongly biased .    for sufficiently noisy data",
    ", circularization can thus successfully be used as a method of reducing scatter .",
    "plain ksb@xmath45 outputs , however , even after traditional corrections , do not greatly profit from this as the residual bias dominates here . in order to benefit from the reduced scatter in circularized samples one has to combine circularization with a means of reducing bias , as has been done with the neural networks in this work .",
    "we continue to analyze the single galaxy network output as a function of ksb @xmath50 for different subsamples of great08 .",
    "the network output for shear component @xmath22 is a nonlinear function @xmath196 of the input vector with ksb shear estimates and additional parameters @xmath197 of each galaxy @xmath20 .",
    "one may interpret this , although not unambiguously , as an additive bias correction @xmath198 and a weighting and multiplicative bias correction @xmath199 @xmath200 where @xmath201 is the average variance of the measurements from the sample galaxy @xmath20 has been taken from , @xmath202 the variance of the measurement of the particular galaxy and @xmath203 a galaxy set property dependent multiplicative bias .",
    "the network output can then be written as @xmath204    consider now a version of the galaxy rotated by 90@xmath205 so as to take the true ellipticity @xmath206 to its negative @xmath207 and @xmath208 .",
    "as the variance @xmath202 and multiplicative bias @xmath203 should remain constant under such a transformation , an ideal network should use an unchanged @xmath209 .",
    "the additive bias correction should be taken such that @xmath210 .",
    "this results in a point symmetric distribution of neural network outputs as a function of @xmath211 with respect to a zero shifted by the additive bias .",
    "differences in @xmath212 for different galaxies will cause asymmetries in the distributions , but for @xmath213 these will only be slight .",
    "we therefore expect a non - overfitted network to give an almost point symmetric output distribution .    for @xmath50 bins we find percentiles of the network output corresponding to the median and 1 - 3@xmath70 in the case of a normal distribution .",
    "plots of the resulting percentile curves of the networks trained on circularized data are shown in figure  [ fig : percentiles ] .",
    "they show point symmetry , which is additional evidence that the networks we use are not overfitted . the general shape can be interpreted as a high weighting of relatively circular galaxies and a downweighting of more elliptical galaxies .",
    "slight differences can also be seen , for instance , between the output for the subsamples of sample 1c with larger and smaller than fiducial galaxy fwhm on the left and right of the lower panel , respectively .",
    "none of the network corrections can be interpreted as a single affine transformation .    the network output , therefore ,",
    "must not be interpreted as the _ true _ ellipticity of the galaxies .",
    "it is merely a quantity that , if averaged arithmetically , gives a good ensemble shear estimate . for different applications , such as shear",
    "correlation measurements , different network inputs and cost functions can be used . apart from defining a figure of merit and constructing the cost function such that the figure of merit is being maximized during training , one may in these cases make use of the rotational and permutational invariance of the expected output .",
    "we have presented a scheme of neural networks which is capable of reducing bias in ksb shear measurements to a level where it no longer inhibits the success of future surveys .",
    "bias correction was most successful on the medium to high signal - to - noise data sets .",
    "this result might give hints as to the most promising setup of future pipelines .",
    "we showed that circularization of the psf reduces the scatter as compared to psf anisotropy correction based on a single galaxy @xmath194 term .",
    "therefore , circularization of varying psfs in combination with neural networks seems to be very promising for shear measurement on real data .",
    "overall results in terms of shear measurement accuracy are very encouraging . by means of neural networks , it was possible to calibrate traditional ksb shape measurement to an accuracy competitive with the most successful methods and well above traditionally calibrated shape measurement approaches participating in the great08 challenge",
    ". on real data ksb remains the method most commonly used , which makes this improvement extremely valuable .",
    "the neural network scheme presented in this paper is , however , also a general approach .",
    "it can be applied to any other shear measurement pipeline that is using single galaxy parameters to find true shears by an averaging procedure .",
    "we expect that neural networks are able to reduce bias in these methods as well .",
    "the success of any shear measurement calibration scheme , including neural networks , depends on the availability of data with known shear similar to the data to be analyzed . in the case of the great08 challenge ,",
    "this was available from the simulations themselves .",
    "for the application on real data , it is necessary to simulate training data sets with known shear values from and similar to the real data .",
    "this can be done by either fitting galaxy models to the objects to be analyzed and simulating sheared data from the fitted profiles or by applying a finite - resolution shear operator @xcite to the original image data itself .",
    "_ this work was supported by the tr33 `` the dark universe '' , the dfg cluster of excellence on the `` origin and structure of the universe '' and the rtn - network `` duel ''  ( dark universe through extragalactic lensing ) gravitational lensing .",
    "we thank the bonn lensing group for an introduction to ksb and for providing us with some of their scripts .",
    "we are also grateful to a. collister and o. lahav for making available their neural network implementation annz which the scheme presented has been built upon . _",
    "the learning procedure of a neural network consists in finding a set of weights for the connections between nodes of adjacent layers @xmath27 such that some cost function @xmath28 of the network output becomes minimal . in principle , it is possible to achieve this using gradient descent , i.e. changing the weights in each step according to @xmath214 with @xmath215 being a small , positive parameter .",
    "the updating of weights can be done on the basis of individual data sets or after a batch of data sets have been processed .",
    "however , an efficient method of calculating the required derivative @xmath216 needs to be found .    for the output layer the desired output of the nodes and",
    "thus the required change of the weights is obvious .",
    "@xcite presented an algorithm based on the back - propagation of errors through the network which makes it possible to find @xmath216 efficiently for all weights @xmath27 in a multi - layer perceptron .",
    "we are going to introduce this algorithm here , as it is also the foundation of the neural networks we use .",
    "since the activations @xmath219 for any node can easily be found by feeding the respective data set to the network , the remaining task consists in calculating for each node @xmath20 the quantity @xmath220 often referred to as _",
    "@xmath221      in the common case of identity activation functions for the output layer and squared error cost functions @xmath224 with true results @xmath225 for single training sets this leads to the simple residuals of the output nodes activations , @xmath226    for a node @xmath22 from any other layer we can simplify the expression for @xmath227 using a sum over the nodes @xmath30 of the following layer , requiring that the activation functions @xmath228 be differentiable : @xmath229    thus with eqn .",
    "( [ eqn : deltaprop ] ) we can calculate one layer s @xmath230s using the derivative of its activations functions , the subsequent layer s @xmath230s and the weights of the connections between the two layers . the output layer s @xmath230s given in eqn .",
    "( [ eqn : outputdeltas ] ) , we can back - propagate the errors through the network to find @xmath220 for each node .",
    "finally , we can change the weights accordingly ( using equations ( [ eqn : graddesc ] ) , ( [ eqn : rewritederiv ] ) and ( [ eqn : deltadef ] ) ) :      applying these changes with a small enough parameter @xmath232 , it is possible to gradually minimize the cost function and therefore improve the network performance .",
    "also , the gradient found can be used with a quasi - newton algorithm .      for averaging perceptrons",
    "we define the cost function as the sum of squared errors of the output averages @xmath233 of a meta set @xmath20 of single data sets against known true average outputs @xmath225 for the meta data set : @xmath234    we define errors @xmath230 for each node , this time referring to a single data set @xmath68 with unknown true result which is part of a meta data set with known average true result . recalling definition ( [ eqn : deltadef ] ) and the form of arithmetic averages we find @xmath235 which simplifies for the output nodes to a constant error for all single data sets @xmath236 thus @xmath237 as defined in eqn .",
    "( [ eqn : deltao1 ] ) above .    for hidden nodes we find , in analogy to eqn .",
    "( [ eqn : deltaprop ] ) summing over nodes @xmath30 of the subsequent layer , @xmath238 which depends on @xmath68 due to the different activations and therefore activation derivatives of the nodes for each primitive data set .",
    "note that @xmath239 is no longer equal to @xmath227 as defined in eqn .",
    "( [ eqn : deltaprop ] ) , as @xmath240 and @xmath241 are not necessarily independent .",
    "an algorithm following this calculation therefore first has to find network averages for the meta data set under consideration . after having calculated the constant @xmath243 from that",
    ", it has to feed each primitive data set @xmath68 in turn , back - propagate @xmath244 through the network and sum @xmath245 for each weight @xmath27 .",
    "alard , c. & lupton , r. h. 1998 , , 503 , 325 amara , a. & rfrgier , a. 2008 , , 391 , 228 amari , s. et al .",
    "1995 , ieee transactions on neural networks , 8 , 985 bridle , s. et al .",
    "2009 , annals of applied statistics , vol .",
    "3 , p. 6 - 37",
    "bridle , s. et al .",
    "2010 , , 405 , 2044 collister , a. a. & lahav , o. 2004 , , 116 , 345 erben et al .",
    "2001 , , 366 , 717 fu , l. et al .",
    "2008 , , 479 , 1 goessl , c. a. & riffeser , a. 2002 , , 381 , 1095 heymans , c. et al .",
    "2003 , , 368 , 1323 hoekstra , h. , franx , m. , kuijken , k. & squires , g. 1998 , , 504 , 636 kaiser , n. , squires , g. & broadhurst , t. 1995 , , 449 , 460 kaiser , n. , 2000 , 537 , 555 luppino , g. a. & kaiser , n. , 1997 , 475 , 20 massey , r. et al .",
    "2007 , , 376 , 13 mcinnes , r. n. et al .",
    "2009 , , 399 , 84 schrabback , t. et al .",
    "2010 , , 516 , a63 rumelhart , d. , hinton , g. e. & williams , r. j. 1986 , , 323 , 533"
  ],
  "abstract_text": [
    "<S> bias due to imperfect shear calibration is the biggest obstacle when constraints on cosmological parameters are to be extracted from large area weak lensing surveys such as pan - starrs-3@xmath0 , des or future satellite missions like euclid .    </S>",
    "<S> we demonstrate that bias present in existing shear measurement pipelines ( e.g. ksb ) can be almost entirely removed by means of neural networks . in this way </S>",
    "<S> , bias correction can depend on the properties of the individual galaxy instead on being a single global value . </S>",
    "<S> we present a procedure to train neural networks for shear estimation and apply this to subsets of simulated great08 realnoise data .    </S>",
    "<S> we also show that circularization of the psf before measuring the shear reduces the scatter related to the psf anisotropy correction and thus leads to improved measurements , particularly on low and medium signal - to - noise data .    </S>",
    "<S> our results are competitive with the best performers in the great08 competition , especially for the medium and higher signal - to - noise sets . expressed in terms of the quality parameter defined by great08 we achieve a @xmath1 40 , 140 and 1300 without and 50 , 200 and 1300 with circularization for low , medium and high signal - to - noise data sets , respectively . </S>"
  ]
}