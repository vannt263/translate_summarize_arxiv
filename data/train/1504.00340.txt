{
  "article_text": [
    "address the widespread problem of how to take into account differences in standards , confidence and bias in assessment panels , such as those evaluating research quality or grant proposals , employment or promotion applications and classification of university degree courses , in situations where it is not feasible for every assessor to evaluate every object to be assessed .    a common approach to assessment of a range of objects by such a panel is to assign to each object the average of the scores awarded by the assessors who evaluate that object .",
    "this approach is represented by the cell labelled `` simple averaging '' ( sa ) in the top left of a matrix of approaches listed in table  1 , but it ignores the likely possibility that different assessors have different levels of stringency , expertise and bias @xcite .",
    "some panels shift the scores for each assessor to make the average of each take a normalised value , but this ignores the possibility that the set of objects assigned to one assessor may be of a genuinely different standard from that assigned to another . for an experimental scientist ,",
    "the issue is obvious : _",
    "calibration_.    one is to seek to calibrate the assessors beforehand on a common subset of objects , perhaps disjoint from the set to be evaluated @xcite .",
    "this means that they each evaluate all the objects in the subset and then some rescaling is agreed to bring the assessors into line as far as possible .",
    "this would not work well , however , in a situation where the range of objects is broader than the expertise of a single assessor .",
    "also , regardless of how well the assessors are trained , differences between individuals assessments of objects remain in such ad hoc approaches @xcite .",
    ".panel assessment methods : the matrix of four approaches according to use of calibration and/or confidences .",
    "simple averaging ( sa ) is the base for comparisons .",
    "fisher s iba does not deal with varying degrees of confidence and the confidence - weighted averaging does nt achieve calibration .",
    "the method proposed herein ( cwc ) accommodates both calibration and confidences . [ cols= \" < ,",
    "< , < \" , ]     on this basis , the most precise results are given by cwc .",
    "none of them are very precise , however .",
    "a posterior uncertainty of 8 means that we should consider values for the objects to have a @xmath0 chance of differing by more than 8 from the outputted values .",
    "this means that for iba and cwc , only the top three proposals of table  [ ranks ] are reasonably assured of being in the top ten .",
    "as the object of the competition was only to choose the best 10 proposals to fund , rather than assign values to each proposal , it might have been more appropriate to design just a classifier system ( with a tunable parameter to make the right number in the `` fund '' class ) but our goal was to use it as a test of cwc .",
    "the fact that three different methods with roughly equal evidence lead to drastically different allocation of the grants , and with large posterior uncertainties , highlights that better design of the panel assessment was required .",
    "a moral of our analysis is that to achieve a reliable outcome , the assessment procedure needs substantial advance design .",
    "we continue a discussion of design in appendices c and f.      we also tested the method on undergraduate examination results for a degree with a flexible options system @xcite and on the assessment of a multi - lecturer postgraduate module .    in the former case , as surrogates for the confidences in the marks we took the number of credit accumulation and transfer scheme ( cats ) points for the module , which indicate the amount of time a student is expected to devote to the module ( for readers used to the european credit transfer and accumulation system , 2 cats points are equivalent to 1 ects point ) .",
    "the amount of assessment for a module is proportional to the cats points .",
    "if it can be regarded as consisting of independent assessments of subcomponents , e.g.  one per cats point , with roughly equal variances , then the variance of the total score would be proportional to the number of cats points .",
    "as the score is then normalised by the cats points , the variance becomes inversely proportional to the cats points , making confidence directly proportional to cats points .",
    "the outcome indicated significant differences in standards for the assessment of different modules , but as most modules counted for 15 or 18 cats , this was not a strong test of the merits of including confidences in the analysis , so we do not report on it here .    for the postgraduate module",
    ", there were four lecturers plus module coordinator , who each assessed oral and written reports for some but not all of the students , according to availability and expertise ( except the coordinator assessed them all ) .",
    "each assessor provided a score and an uncertainty for each assessment .",
    "the results were combined using our method and the resulting value for each student was reported as the final mark .",
    "the lecturers agreed that the outcome was fair .",
    "we have presented and tested a method to calibrate assessors , taking account of differences in confidence that they express in their assessments . from a test on simulated data we found that calibration with confidence ( cwc ) generated closer estimates of the true values than additive ncomplete lock nalysis or simple veraging . a test on some real data , suggesting that the assessment procedure for that context needed more robust design .",
    "nevertheless , cwc came ahead on posterior precision .",
    "there are a number of refinements which one could introduce to the core method .",
    "these include how to deal with different types of bias , different scales for confidence , different ways to remove the degeneracy in the equations , how to deal with the endpoints on a marking scale , and how to choose the assessment graph .",
    "some suggestions are made in the appendices , along with mathematical treatment of the robustness of the method and of computation of the bayesian evidence for the models .",
    "an advantage of type of calibration is that it does not produce the artificial discontinuities across field boundaries that tend to arise if the domain is partitioned into fields and evaluation in each field carried out separately .",
    "we suggest that a method such as this , which takes into account declared confidences in each assessment , is well suited to a multitude of situations in which a number of objects is assessed by a panel .",
    "we are grateful to the mathematics department , university of warwick , for providing us with examination data to perform an early test of the method , to the applied mathematics research centre , coventry university for funding to make a professional implementation of the method and to marcus ong and daniel sprague of spectra analytics for producing it .",
    "we also thank john winn for pointing us to the sigkdd09 method , and david mackay for pointing us to the nips method .",
    "software implementing the method is free to download from the website + .",
    "software and data for the two case studies are available from .",
    "rm conceived and developed the theory .",
    "sp tested it using an early case study .",
    "rl performed case study 1 .",
    "rk performed case study 2 .",
    "rm , sp , rk and rl discussed and interpreted the results and wrote the paper .",
    "the work of rm was supported by the esrc under the network on integrated behavioural science ( es / k002201/1 ) and the centre for evaluation of complexity in the nexus ( es / n012550/1 ) .",
    "rk was supported by the eu marie curie irses network pirses - ga-2013 - 612707 dionicos - dynamics of and in complex systems funded by the european commission within the fp7-people-2013-irses programme ( 2014 - 2018 ) .",
    "we have no competing interests .",
    "xxx meadows m. can we predict who will be a reliable marker ?",
    "manchester : aqa centre for education research and policy , 2006 .",
    "bayesian methods for calibration of examiners .",
    "british journal of mathematical and statistical psychology ( 1981 ) * 34 * , 213 - 223 .",
    "ns t , brockhoff p and tomic o. statistics for sensory and consumer science .",
    "wiley , chicester , 2010 .",
    "fisher ra .",
    "an examination of the different possible solutions of a problem in incomplete blocks .",
    "annals of eugenics ( 1940 ) * 10 * , 5275 .",
    "giesbrecht fg .",
    "analysis of data from incomplete block designs .",
    "biometrics ( 1986 ) * 42 * , 437448 .",
    "flach pa , spiegler s , golenia b , price s , guiver j , harbrich r , graepel t , zaki mj .",
    "novel tools to streamline the conference review process ; experiences from sigkdd09 , + http://research.microsoft.com/pubs/122784/reviewercalibration.pdf    guiver , j. calibrating reviews of conference submissions .",
    "+ http://blogs.msdn.com/b/infernet_team_blog/archive2011/09/30/calibrating-reviews-of-conference-submissions.aspx    platt j , burges c. regularised least squares to remove reviewer bias .",
    "http:// research.microsoft.com/en-us/um/people/cburges/papers/reviewerbias.pdf    ge h , welling m , ghahramani z , a bayesian model for calibrating conference review scores .",
    "http://mlg.eng.cam.ac.uk/hong/nipsrevcal.pdf , 2013 .",
    "thorngate w , dawes rm and foddy m. judging merit .",
    "psychology press , new york , 2008 .",
    "hubbard dw . how to measure anything .",
    "wiley , 2007 , 2010 , 2014 .",
    "matrix analysis and applied linear algebra .",
    "siam philadelphia 2001 .",
    "golub gh and van loan cf . matrix computations .",
    "johns hopkins university press , baltimore , 1996 .",
    "parker s. a test of a method for calibration of assessors .",
    "final year undergraduate project report , university of warwick , april 2014 .",
    "chung  frk , spectral graph theory ( am math soc , 1996 )    mackay  djc , information theory , inference and learning algorithms ( cambridge univ press , 2003 ) .",
    "song  t , wolfe  ew , hahn  l , less - petersen  m , sanders  r and vickers  d. relationship between rater background and rater performance .",
    "+ http://researchnetwork.pearson.com/wp-content/uploads/ song_raterbackground_04_21_2014.pdf    fuchs d and fuchs ls .",
    "test procedure bias : a meta - analysis of examiner familiarity effects .",
    "review of educational research ( 1986 ) * 56 * , 243 - 262 .",
    "we motivated the model by proposing that the noise terms be of the form @xmath1 with the @xmath2 independent zero - mean random variables with unit variance , so that the @xmath3 are standard deviations .",
    "nevertheless , multiplying all the confidences by the same number does not change the results of the least squares fit , nor our quantifications of robustness ( appendices c and d ) .",
    "thus the @xmath2 can be taken to have any variance @xmath4 , as long as it is the same for all assessments .",
    "it is only ratios of confidences that have significance .",
    "the fitting procedure can be extended to infer a best fit value for @xmath4 . even if the assessors provide confidences based on assuming @xmath5 , the best fit for @xmath4 is not 1 in general .",
    "assuming independent gaussian errors , the maximum likelihood value for @xmath4 comes out to be @xmath6 where @xmath7 is the residual from the least squares fit ( @xmath8 for @xmath9 and @xmath10 is the total number of assessments . the posterior distribution for @xmath4 , given a prior distribution , is obtained in appendix d.",
    "we can remove the degeneracy in the equations ( [ eq : system1 ] ) and ( [ eq : system1b ] ) in different manners equation  ( [ bias ] ) used here .",
    "indeed , use of ( [ bias ] ) can lead to an average shift from the scores to the true values .",
    "this does not matter if only a ranking is required , but if the actual values are important , then a better choice of degeneracy - breaking condition is needed .",
    "a preferable confidence - weighted degeneracy - breaking condition is @xmath11 which from ( [ eq : system1b ] ) automatically implies @xmath12 , thus avoiding the possibility of such systematic shifts .    from a theoretical perspective",
    ", however , the best choice of degeneracy - breaking condition is to choose a reference value @xmath13 ( think of a notional desired mean ) and require @xmath14 where @xmath15 using the notation in ( [ eq : vob ] ) and ( [ eq : voc ] ) this can equivalently be written as @xmath16 to reduce the possible average shift from confidence - weighted average scores to true values , the reference value @xmath13 should be chosen near the confidence - weighted average score @xmath17 choosing @xmath18 exactly equal to @xmath19 gives ( [ eq : avbias ] ) , which makes the confidence - weighted average bias come out to 0 and the confidence - weighted average value come out to @xmath19 .",
    "we will show in appendix c , however , that the results are a factor @xmath20 more robust to changes in the scores if @xmath13 is chosen to be fixed rather than dependent on the scores .",
    "here we present our approach to the quantification of the robustness of our method to small changes in the scores , using norms that take into account the confidences .    for @xmath21 , define the operator @xmath22 by @xmath23 , \\ ] ] as a shorthand for the definitions in equations  ( [ referee1a ] ) and ( [ referee1b ] ) , so that the equations ( [ eq : system1 ] , [ eq : system1b ] ) can be written as @xmath24 = k s .\\ ] ]",
    "thus , if a change @xmath25 is made to the scores , we obtain changes @xmath26 , @xmath27 of magnitude bounded by @xmath28 where @xmath29 is defined by restricting the domain of @xmath30 to ( [ bias ] ) and its range to @xmath31 , and appropriate norms are chosen . in this appendix",
    ", we propose that appropriate choices of norms are @xmath32 @xmath33 and the associated operator norm from scores to results for @xmath34 . with the confidence - weighted degeneracy - breaking condition @xmath35 ( [ eq : avbias ] ) instead of ( [ bias ] ) we obtain @xmath36 where @xmath37 is the second smallest eigenvalue of a certain matrix @xmath38 formed from the confidences ( see ( [ eq : m ] ) ) .",
    "in particular , this gives @xmath39 the factor of @xmath20 can be removed if one switches to an ideal degeneracy - breaking condition as in ( [ tightest ] ) of appendix b.    as a consequence , to maximise the robustness of the results , the task for the designer of @xmath40 is to make none of the @xmath41 much smaller than the others and to make @xmath37 significantly larger than 0 .",
    "the former is evident ( no object should receive significantly less assessment or less expert assessment than the others ) .",
    "the latter is the mathematical expression of how well connected is the graph @xmath42 ( equivalently @xmath43 ) . to design the graph @xmath42 requires a guess of the confidence levels that assessors are likely to give to their assessments ( based on knowing their areas of expertise and their thoroughness or otherwise ) and a compromise between assigning an object to only the most expert assessors for that object and the need to achieve a chain of comparisons between any pair of assessors .",
    "we now go into detail , derive the above bounds and describe some computational shortcuts .    one can measure the size of a change @xmath44 to a score @xmath45 by comparing it to the declared uncertainty @xmath3 .",
    "thus we take the size of @xmath44 to be @xmath46 .",
    "we propose to measure the size of an array @xmath25 of changes @xmath44 to the scores by the square root of the sum of squares of the sizes of the changes to each score , as in ( [ eq : scoresnorm ] ) .",
    "supremum or sum - norms could also be considered but we will stick to this choice here .",
    "it is also reasonable to measure the size of a change @xmath47 to a true value @xmath48 by comparing it to the uncertainty implied by the sum of confidences in the scores for object @xmath49 .",
    "thus the size of @xmath47 is defined to be @xmath50 , where @xmath41 is the total confidence in the assessment of object @xmath49 .",
    "similarly , we measure the size of a change @xmath51 in bias @xmath52 by @xmath53 where @xmath54 is the total confidence expressed by a given assessor .",
    "finally , we measure the size of a change @xmath55 to the vector of values and biases by the square root of sum of squares of the individual sizes , as in ( [ eq : resultsnorm ] ) .",
    "the size of the operator @xmath56 is measured by the operator norm from scores to results , i.e. @xmath57 the operator @xmath56 is equivalent to orthogonal projection with respect to the norm ( [ eq : scoresnorm ] ) from the scores to the subspace @xmath58 of the form @xmath59 with a degeneracy - breaking condition to eliminate the ambiguity in direction of the vector @xmath60 .    the tightest bounds in ( [ bound ] )",
    "are obtained by choosing the degeneracy - breaking condition to correspond to a plane perpendicular to this vector with respect to the inner product corresponding to equation  ( [ eq : resultsnorm ] ) .",
    "thus we choose degeneracy - breaking condition ( [ tightest ] ) .",
    "1ex * theorem * : for a connected graph @xmath42 and with the degeneracy - breaking condition ( [ tightest ] ) , the size of the change @xmath55 resulting from a given array of changes @xmath25 in scores is bounded by @xmath61 where @xmath37 is the second smallest eigenvalue of the matrix @xmath62 ,   \\label{eq : m}\\ ] ] @xmath63 @xmath64 , @xmath65 are the numbers of assessors and objects respectively , and for @xmath66 @xmath67 is the identity matrix of rank @xmath68 .",
    "1ex * proof * : firstly , the orthogonal projection in metric ( [ eq : scoresnorm ] ) from @xmath69 to the subspace @xmath58 never increases length . secondly , if @xmath70 with @xmath71 then @xmath72 where @xmath73 is the vector with components @xmath74 then , because we restricted to the orthogonal subspace to the null vector in results - norm and @xmath38 is non - negative and symmetric , @xmath75 where index @xmath76 ranges over all objects and assessors .",
    "positivity of @xmath37 holds as soon as the graph @xmath42 is connected , because @xmath38 is a transformation of the weighted graph - laplacian to scaled variables @xcite , so dividing by @xmath37 and taking the square root yields the result .",
    "@xmath77    1ex    the computation of the eigenvalue @xmath37 of @xmath38 can be reduced from dimension @xmath78 to dimension @xmath64 by    1ex * proposition * : if @xmath79 , the second smallest eigenvalue @xmath37 of @xmath38 is related to the second largest eigenvalue @xmath80 of @xmath81 by @xmath82 if @xmath83 and @xmath84 then @xmath85 . if both are 1 then @xmath86 .",
    "1ex * proof * : the equations for an eigenvalue - eigenvector pair @xmath87 of @xmath38 are @xmath88 applying @xmath89 to the first equation , multiplying the second by @xmath90 , and then substituting for @xmath91 in the second yields @xmath92 thus either @xmath93 or @xmath94 is an eigenvalue @xmath95 of @xmath81 .",
    "in the first case , equation ( [ eq : evec ] ) implies @xmath96 , so if @xmath97 then @xmath94 is an eigenvalue of @xmath98 .",
    "conversely , if @xmath99 is an eigenvalue - eigenvector pair for @xmath98 with @xmath100 then @xmath101 because @xmath81 is non - negative , so put @xmath102 to see that @xmath103 is an eigenvector of @xmath38 with eigenvalue @xmath104 .",
    "if @xmath105 and @xmath106 then @xmath107 is an eigenvalue of @xmath38 with eigenvector @xmath103 for any @xmath108 with @xmath109 , e.g.  @xmath110 .    thus there is a two - to - one correspondence between eigenvalues @xmath111 of @xmath38 not equal to 1 and positive eigenvalues @xmath95 of @xmath81 ( counting multiplicity ) : @xmath104 .",
    "any remaining eigenvalues are 1 for @xmath38 and 0 for @xmath98 .",
    "the degeneracy gives an eigenvector @xmath112 of @xmath38 with eigenvalue 0 and it corresponds to an eigenvalue 1 of @xmath98 .",
    "all other eigenvalues of @xmath38 are non - negative because @xmath38 is .",
    "all other eigenvalues of @xmath98 are less than or equal to 1 by the cauchy - schwarz inequality .",
    "so if the second largest eigenvalue @xmath80 of @xmath98 ( counting multiplicity ) is positive then the second smallest eigenvalue @xmath37 of @xmath38 ( counting multiplicity ) is @xmath113 .",
    "if @xmath114 then @xmath115 because existence of @xmath80 implies @xmath116 so @xmath38 has dimension at least 3 and we have only two simple eigenvalues @xmath117 and @xmath118 from the simple eigenvalue 1 of @xmath98 , so @xmath38 must have another one but any other value than 1 would give a positive @xmath80 ; so the same formula holds . if there is no second eigenvalue of @xmath98 (",
    "because @xmath119 ) then if @xmath120 the second largest eigenvalue of @xmath38 must be 1 by the same argument . if both @xmath64 and @xmath65 are 1 then the second largest eigenvalue of @xmath38 is the other one associated with the eigenvalue 1 of @xmath98 , namely 2 .",
    "@xmath77    1ex    note that @xmath121 is a similarity transformation of ( [ eq : aweights ] ) . as examples of second eigenvalues ,",
    "putting unit confidences on the graphs in the left column of figure  [ fig1:graphs ] we calculate @xmath122 for cases ( a),(b),(c ) in the right column , giving @xmath123 , respectively .",
    "finally , a user may prefer to use the degeneracy - breaking condition ( [ eq : avbias ] ) rather than ( [ tightest ] ) , perhaps out of uncertainty about what value of @xmath18 to use . or a user may be happy to use ( [ tightest ] ) with @xmath18 equal to the confidence - weighted average score , but wants @xmath18 to follow this average score if changes are made to the scores .",
    "that comes out equivalent to using ( [ eq : avbias ] ) .",
    "so we extend our discussion of robustness to treat this case .",
    "we find it makes the bounds increase by a factor of only @xmath20 .",
    "1ex * proposition * : for @xmath42 connected and using degeneracy - breaking condition ( [ eq : avbias ] ) , the size of @xmath55 resulting from changes @xmath25 to the scores is at most @xmath124 .",
    "1ex    * proof * : if the degeneracy - breaking condition ( [ tightest ] ) gives a change @xmath55 for a change @xmath25 to the scores , then switching to degeneracy - breaking condition ( [ eq : avbias ] ) just adds an amount @xmath68 of the null vector @xmath125 to achieve @xmath126 , i.e. @xmath127 in the results metric , the null vector has length @xmath128 .",
    "thus the correction has length @xmath129 . using the condition ( [ tightest ] )",
    "we can write @xmath130 which one can recognise as one half of the inner product of @xmath131 with @xmath55 in results - norm , so it is bounded by @xmath132 .",
    "thus the length of the correction vector is at most that of @xmath55 .",
    "the correction is perpendicular to @xmath55 , thus the vector sum has length at most @xmath133 .",
    "@xmath77    1ex    one may also ask about robustness with respect to changes in the confidences @xmath134 . if an assessor declares extra high confidence for an evaluation , for example , that can significantly skew the resulting @xmath135 and @xmath136 . the analysis is more subtle , however , because of how the @xmath134 appear in the equations and we do not treat it here .",
    "another point of view on robustness is the bayesian one . from a prior probability on @xmath9 and a model for the @xmath2 ,",
    "one can infer a posterior probability for @xmath9 , whose inverse width tells one how robust is the inference .    in the case of",
    "flat prior on @xmath9 , prescribed @xmath4 , gaussian noise , and an affine degeneracy - breaking condition , the posterior is gaussian with mean at the value solving equations  ( [ eq : system1 ] ) , ( [ eq : system1b ] ) and the degeneracy - breaking condition , and with covariance matrix related to @xmath29 .",
    "specifically , the posterior probability density for @xmath9 is proportional to @xmath137 constrained to the degeneracy - breaking hyperplane , where @xmath138 using ( [ eq : gtmg ] ) and ( [ eq : r ] ) , this can be written as @xmath139 with @xmath55 being the deviations of @xmath9 from the least squares fit .",
    "thus the covariance matrix in these scaled variables is @xmath140 . using the degeneracy - breaking condition ( [ tightest ] ) or equivalently ( [ eq : tightest ] )",
    ", we obtain widths @xmath141 for the posterior on @xmath73 in the eigendirections of @xmath38 , where @xmath142 are the positive eigenvalues of @xmath38 .",
    "thus the robustness of the inference is again determined by @xmath37 , but scaled by @xmath143 .",
    "a slightly more sophisticated approach is to consider @xmath4 to be unknown also . given a prior density @xmath144 for @xmath4 ( which could be peaked around 1 if the assessors are assigning confidences via uncertainties , but following jeffreys would be better chosen to be @xmath145 if there is no information about the scale for the confidences ) , the posterior density for @xmath146 is proportional to @xmath147 where again @xmath10 is the number of assessments .",
    "the maximum of the posterior probability density is determined by the least squares fit for @xmath9 ( which is independent of @xmath4 ) and the following equation for @xmath4 : @xmath148 for @xmath10 large , the peak of the posterior has @xmath4 near the previously determined maximum likelihood value @xmath149 . , taking jeffreys prior , the peak is at @xmath150 . integrating over @xmath4 ( with jeffreys prior )",
    "e find the marginal posterior for @xmath9 to be proportional to @xmath151 incorporating an affine degeneracy - breaking condition , this is a @xmath152-variate student distribution with @xmath153 degrees of freedom .",
    "its covariance matrix is @xmath154 with @xmath155 and @xmath156 interpreted by imposing the chosen degeneracy - breaking condition .",
    "so for the degeneracy - breaking condition ( [ tightest ] ) , the robustness of the inference is given by widths @xmath157 for @xmath158 , in the eigendirections of @xmath38 on @xmath73 .",
    "in particular , the confidence - weighted root mean square uncertainty @xmath159 for the components of the vector @xmath9 is @xmath160 where @xmath161 denotes the trace and , again , @xmath156 is interpreted by restricting to the degeneracy - breaking plane .",
    "marginal posteriors for each @xmath48 and @xmath52 can be extracted , but it must be understood that in general they are significantly correlated .    for the case of simple averaging , the root mean - square posterior uncertainty in the values , weighted by the numbers @xmath162 of assessors for object @xmath49 , is @xmath163 where @xmath164 is defined in ( [ eq : rsa ] ) of appendix e. this can be derived in an analogous fashion to ( [ eq : sigma ] ) via a student distribution again , but with @xmath165 .",
    "here we describe the method used in case study 2 to compare the three models .",
    "bayesian model comparison is based on computing how much evidence there is for each proposed model , e.g.  ch.28 of @xcite .",
    "the evidence for a model @xmath38 given data @xmath166 is @xmath167 .",
    "given strength of belief @xmath168 in model @xmath38 prior to the data ( relative to other models ) , one can multiply it by the evidence to obtain the posterior strength of belief in model @xmath38 .",
    "it is convenient to replace multiplication by addition , thus we define the log - evidence @xmath169 if the model @xmath38 has free parameters @xmath111 then @xmath170 where @xmath171 is a prior probability density on @xmath111 .",
    "let there be @xmath65 objects , @xmath64 assessors , let @xmath45 be the score returned by assessor @xmath172 for object @xmath49 , @xmath134 the confidence in this score in the case of calibration with confidence , @xmath69 be the collection of scores and @xmath10 be their number .",
    "first we compute the evidence for simple averaging ( sa ) .",
    "then we treat calibrate with confidence ( cwc ) and lastly incomplete block analysis ( iba ) because it is a special case of cwc .      for simple averaging ( sa ) ,",
    "the model is that @xmath173 for some unknown vector @xmath135 of `` true '' values @xmath48 , with @xmath174 iid normal @xmath175 for some unknown variance @xmath4 .",
    "then the probability density for the scores @xmath69 is @xmath176 with the product and sum being over the assessments that were carried out .    to work out the evidence for sa",
    "the model must include a prior probability density for @xmath135 and @xmath4 .",
    "the simplest proposal would be @xmath177 on @xmath178 $ ] , @xmath179 $ ] , where @xmath180 and @xmath181 .",
    "this is the product of a `` box '' prior on @xmath135 and jeffreys prior on @xmath4 ( truncated to an interval and normalised ) . for comparison with the other models ,",
    "however , it is easier to replace the box prior on @xmath135 by a `` ball '' prior , giving @xmath182 on @xmath183 for some anticipated average score @xmath184 and upper estimate of the width @xmath185 of the distribution of values @xmath48 .",
    "the normalisation is @xmath186 where @xmath42 is the gamma function . for @xmath187",
    "is it reasonable to choose @xmath188 where @xmath189 is the smallest change any assessor could contemplate .",
    "for @xmath190 it is reasonable to choose @xmath191 .    for each object @xmath49 , @xmath192 where @xmath162 is the number of assessors for object @xmath49 , @xmath193 is the mean of their scores , and the residual @xmath194 thus @xmath195 where @xmath196 to integrate this over @xmath135 and @xmath4 , we assume the bulk of the probability distribution lies in the product of the ball and the interval , and so approximate by extending the range of integration to @xmath197 . integrating the exponential over @xmath48 produces a factor @xmath198 thus , integrating over all components of @xmath135 yields @xmath199 integrating this over @xmath4 , we obtain the evidence @xmath200 and the log - evidence @xmath201      for calibrate with confidence ( cwc ) , the model is @xmath202 for some unknown vectors @xmath135 of true values @xmath48 , and @xmath136 of assessor biases @xmath52 , with @xmath2 iid normal @xmath175 for some unknown variance @xmath4 .",
    "the uncertainties @xmath3 correspond to confidences @xmath134 by @xmath203 , which are considered as given ( one could propose a generative model for them too , but that would require further analysis ) . then the probability density for @xmath69 is @xmath204    for prior probability density over the parameters @xmath205 , we want to build in a degeneracy - breaking condition .",
    "we used @xmath206 in our calculation , thus we take prior `` density '' @xmath207 on the product of the balls ( [ eq : ballo ] ) and @xmath208 and interval @xmath209 $ ] , where @xmath210 is the delta function .",
    "here , @xmath211 is an estimated upper bound for the standard deviation of the biases , and the normalisation is @xmath212 note that the interpretation of @xmath4 is not the same as for sa , so one might choose a different prior for it .",
    "for example , if the @xmath3 are fairly accurate values for the uncertainties in the scores then the prior for @xmath4 should be peaked around @xmath5 , but if they are on an undetermined scale a truncated jeffreys prior is sensible .",
    "the only thing is that one might want to choose a different interval for it , but for application to iba where the @xmath213 or to cwc if the @xmath134 are on a scale centred around 1 , such as we have used to translate the quantitative high / medium / low confidence ratings , the same interval should be reasonable .",
    "similarly , one might want to use a different value for @xmath185 if one believes that the spread in values is more due to variation in assessor bias than true value , but in our case we think it reasonable to use the same @xmath185 .",
    "thus @xmath214 again we assume the bulk of this lies in the product of balls and interval , so we approximate its integral by extending the domains to infinity . now",
    "@xmath215 where @xmath216 is the vector with @xmath217 components , @xmath218 , @xmath219 , @xmath220 is any least squares fit to this model ( without loss of generality satisfying the degeneracy - breaking condition ) , the residual @xmath164 is now @xmath221 ( as in ( [ eq : r ] ) ) and @xmath222 is the matrix with block form @xmath223.\\ ] ]    choose one assessor , say @xmath224 , and integrate over @xmath225 .",
    "this yields @xmath226 with @xmath227 being the remaining components of @xmath216 and @xmath228,\\ ] ] where @xmath229 and @xmath230 , restricted to @xmath231 , which takes into account that @xmath232 .",
    "thus the integral over @xmath227 is @xmath233 where @xmath234 .",
    "finally , we integrate over @xmath4 to obtain @xmath235 and the log - evidence is @xmath236      the model for incomplete block analysis ( iba ) is the same as for cwc but taking the confidences @xmath213 for all the assessments .",
    "thus the log - evidence for iba given the scores @xmath69 is @xmath237 with the appropriate changes to @xmath164 and @xmath238 .",
    "one could develop refinements to the basic model ( [ eq : model1 ] ) .",
    "for example , assessors might have not only an additive bias but also different scales , so for example @xmath239 fitting @xmath240 is more complicated , however , than just @xmath241 .",
    "an assessor may have a bias correlated with their confidence @xcite or with some other feature like familiarity @xcite .",
    "assessors may like to give round - number scores . may have different scales for confidence , so their confidences may need calibrating as well as their scores .",
    "another problem is that often assessors are asked to assign scores in a fixed range @xmath242 $ ] , e.g.  @xmath243 .",
    "then any model for bias really ought to be nonlinear to respect the endpoints .",
    "one way to treat this is to apply a nonlinear transformation to map a slightly larger interval @xmath244 onto @xmath245 , e.g. @xmath246 or @xmath247 apply our method to the transformed scores , scaling the confidences by the inverse square of the derivative of the transformation , and then apply the inverse transformation to the  true \" values . on the other hand , it may be inadvisable to specify a fixed range because it requires an assessor to have knowledge of the range of the objects before starting scoring",
    ". thus one could propose asking assessors to use any real numbers and then use equation  ( [ eq : model2 ] ) to extract true values @xmath135 .",
    "a simpler strategy that might work nearly as well is to allow assessors to use any positive numbers but then to take logarithms and fit equation  ( [ eq : model1 ] ) to the log - scores .",
    "the assessor biases would then be like logarithms of exchange rates .",
    "the confidences would need translating appropriately too .",
    "one issue with our method is that the effect of an assessor who assesses only one object is only to determine their own bias , apart from an overall shift along the null vector @xmath248 for the rest . to rectify",
    "this one could incorporate a prior probability distribution for the biases ( indeed , this was done by @xcite in the form of a regulariser ) .",
    "an interesting future project is to design the graph @xmath42 optimally , given advance guesses of confidences and constraints or costs for the number of assessments per assessor .",
    "`` optimality '' would mean to achieve maximum precision or robustness of the resulting values .",
    "for instance , in each case of figure  [ fig1:graphs ] , each assessor has the same amount of work and each object receives the same amount of attention , but ( a ) achieves full connectivity with a resulting value for @xmath37 of @xmath249 , whereas ( b ) achieves moderate connectivity and a smaller value of @xmath250 , and ( c ) is not even connected and has @xmath251 ."
  ],
  "abstract_text": [
    "<S> frequently , a set of objects has to be evaluated by a panel of assessors , but not every object is assessed by every assessor . </S>",
    "<S> a problem facing such panels is how to take into account different standards amongst panel members and varying levels of confidence in their scores . here </S>",
    "<S> , a mathematically - based algorithm is developed to calibrate the scores of such assessors , addressing both of these issues . </S>",
    "<S> the algorithm is based on the connectivity of the graph of assessors and objects evaluated , incorporating declared confidences as weights on its edges . </S>",
    "<S> if the graph is sufficiently well connected , relative standards can be inferred by comparing how assessors rate objects they assess in common , weighted by the levels of confidence of each assessment . by removing these biases ,  true \" values are inferred for all the objects . </S>",
    "<S> reliability estimates for the resulting values are obtained . </S>",
    "<S> the algorithm is tested in two case studies , one by computer simulation and another based on realistic evaluation data . </S>",
    "<S> the process is compared to the simple averaging procedure in widespread use , and to fisher s additive incomplete block analysis . </S>",
    "<S> it is anticipated that the algorithm will prove useful in a wide variety of situations such as evaluation of the quality of research submitted to national assessment exercises ; appraisal of grant proposals submitted to funding panels ; ranking of job applicants ; and judgement of performances on degree courses wherein candidates can choose from lists of options .    </S>",
    "<S> keywords : calibration , evaluation , assessment , confidence , uncertainty , model comparison . </S>"
  ]
}