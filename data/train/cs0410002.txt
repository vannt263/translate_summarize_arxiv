{
  "article_text": [
    "_ shannon information _ theory , usually called just ` information ' theory was introduced in 1948 , @xcite , by c.e .",
    "shannon ( 19162001 ) .",
    "_ kolmogorov complexity _ theory , also known as ` algorithmic information ' theory , was introduced with different motivations ( among which shannon s probabilistic notion of information ) , independently by r.j .",
    "solomonoff ( born 1926 ) , a.n .",
    "kolmogorov ( 19031987 ) and g. chaitin ( born 1943 ) in 1960/1964 , @xcite , 1965 , @xcite , and 1969 @xcite , respectively .",
    "both theories aim at providing a means for measuring ` information ' .",
    "they use the same unit to do this : the _ bit_. in both cases , the amount of information in an object may be interpreted as the length of a description of the object . in the shannon approach , however , the method of encoding objects is based on the presupposition that the objects to be encoded are outcomes of a known random source  it is only the characteristics of that random source that determine the encoding , not the characteristics of the objects that are its outcomes . in the kolmogorov complexity approach",
    "we consider the individual objects themselves , in isolation so - to - speak , and the encoding of an object is a short computer program ( compressed version of the object ) that generates it and then halts . in the shannon approach",
    "we are interested in the minimum expected number of bits to transmit a message from a random source of known characteristics through an error - free channel .",
    "says shannon @xcite :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ `` the fundamental problem of communication is that of reproducing at one point either exactly or approximately a message selected at another point .",
    "frequently the messages have _ meaning _ ; that is they refer to or are correlated according to some system with certain physical or conceptual entities .",
    "these semantic aspects of communication are irrelevant to the engineering problem .",
    "the significant aspect is that the actual message is one _ selected from a set _ of possible messages .",
    "the system must be designed to operate for each possible selection , not just the one which will actually be chosen since this is unknown at the time of design . ''",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    in kolmogorov complexity we are interested in the minimum number of bits from which a particular message or file can effectively be reconstructed : the minimum number of bits that suffice to store the file in reproducible format .",
    "this is the basic question of the ultimate compression of given individual files .",
    "a little reflection reveals that this is a great difference : for _ every _ source emitting but two messages the shannon information ( entropy ) is at most 1 bit , but we can choose both messages concerned of arbitrarily high kolmogorov complexity .",
    "shannon stresses in his founding article that his notion is only concerned with _ communication _ , while kolmogorov stresses in his founding article that his notion aims at supplementing the gap left by shannon theory concerning the information in individual objects .",
    "kolmogorov @xcite :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ `` our definition of the quantity of information has the advantage that it refers to individual objects and not to objects treated as members of a set of objects with a probability distribution given on it . the probabilistic definition can be convincingly applied to the information contained , for example , in a stream of congratulatory telegrams .",
    "but it would not be clear how to apply it , for example , to an estimate of the quantity of information contained in a novel or in the translation of a novel into another language relative to the original .",
    "i think that the new definition is capable of introducing in similar applications of the theory at least clarity of principle . ''",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    to be sure , both notions are natural : shannon ignores the object itself but considers only the characteristics of the random source of which the object is one of the possible outcomes , while kolmogorov considers only the object itself to determine the number of bits in the ultimate compressed version irrespective of the manner in which the object arose . in this paper",
    ", we introduce , compare and contrast the shannon and kolmogorov approaches . an early comparison between shannon entropy and kolmogorov complexity",
    "is @xcite .",
    "[ [ how - to - read - this - paper ] ] how to read this paper : + + + + + + + + + + + + + + + + + + + + + + +    we switch back and forth between the two theories concerned according to the following pattern : we first discuss a concept of shannon s theory , discuss its properties as well as some questions it leaves open .",
    "we then provide kolmogorov s analogue of the concept and show how it answers the question left open by shannon s theory . to ease understanding of the two theories and how they relate , we supplied the overview below and then sections  [ sec : coding ] and section  [ sec : basic ] , which discuss preliminaries , fix notation and introduce the basic notions .",
    "the other sections are largely independent from one another . throughout the text , we assume some basic familiarity with elementary notions of probability theory and computation , but we have kept the treatment elementary .",
    "this may provoke scorn in the information theorist , who sees an elementary treatment of basic matters in his discipline , and likewise from the computation theorist concerning the treatment of aspects of the elementary theory of computation .",
    "but experience has shown that what one expert views as child s play is an insurmountable mountain for his opposite number .",
    "thus , we decided to ignore background knowledge and cover both areas from first principles onwards , so that the opposite expert can easily access the unknown discipline , possibly helped along by the familiar analogues in his own ken of knowledge .",
    "a summary of the basic ideas is given below .",
    "in the paper , these notions are discussed in the same order .    1 .",
    "coding : prefix codes , kraft inequality : :    ( section  [ sec : coding ] ) since descriptions or _",
    "encodings _ of objects    are fundamental to both theories , we first review some elementary    facts about coding .",
    "the most important of these is the _ kraft    inequality_. this inequality gives the fundamental relationship    between _ probability density functions and prefix codes _ , which are    the type of codes we are interested in .",
    "prefix codes and the kraft    inequality underly most of shannon s , and a large part of kolmogorov s    theory .",
    "2 . shannon s fundamental concept : entropy : :    ( section  [ sec : shannon ] ) entropy is defined by a functional that maps    _ probability distributions _ or , equivalently , _",
    "random variables _ , to    _ real numbers_. this notion is derived from first principles as the    only ` reasonable ' way to measure the ` average amount of information    conveyed when an outcome of the random variable is observed ' . the    notion is then related to encoding and communicating messages by    shannon s famous ` coding theorem ' .",
    "3 . kolmogorov s fundamental concept : kolmogorov complexity : :    ( section  [ sec : kolmogorov ] ) kolmogorov complexity is defined by a    function that maps _ objects _ ( to be thought of as natural numbers or    sequences of symbols , for example outcomes of the random variables    figuring in the shannon theory ) to the _ natural numbers_. intuitively ,    the kolmogorov complexity of a sequence is the length ( in bits ) of the    shortest computer program that prints the sequence and then halts .",
    "4 . relating entropy and kolmogorov complexity : :    ( section  [ sec : kcse ] and appendix  [ sec : universal ] ) although their    primary aim is quite different , and they are functions defined on    different spaces , there are close relations between entropy and    kolmogorov complexity .",
    "the formal relation `` entropy = expected    kolmogorov complexity '' is discussed in section  [ sec : kcse ] .",
    "the    relation is further illustrated by explaining ` universal coding ' ( also    introduced by kolmogorov in 1965 ) which combines elements from both    shannon s and kolmogorov s theory , and which lies at the basis of most    practical data compression methods . while related to the main theme of    this paper , universal coding plays no direct role in the later    sections , and therefore we delegated it to appendix  [ sec : universal ] .",
    "entropy and kolmogorov complexity are the basic notions of the two theories .",
    "they serve as building blocks for all other important notions in the respective theories .",
    "arguably the most important of these notions is _ mutual information _ :    5 .",
    "mutual information  shannon and kolmogorov style : :    ( section  [ sec : mutual ] ) entropy and kolmogorov complexity are concerned    with information in a single object : a random variable ( shannon ) or an    individual sequence ( kolmogorov ) .",
    "both theories provide a ( distinct )    notion of _ mutual information _ that measures the information that _",
    "one    object gives about another object_. in shannon s theory , this is the    information that one random variable carries about another ; in    kolmogorov s theory ( ` algorithmic mutual information ' ) , it is the    information one sequence gives about another . in an appropriate    setting the former notion can be shown to be the expectation of the    latter notion .",
    "mutual information non - increase : :    ( section  [ sect.mini ] ) in the probabilistic setting the mutual    information between two random variables can not be increased by    processing the outcomes .",
    "that stands to reason , since the mutual    information is expressed in probabilities of the random variables    involved .",
    "but in the algorithmic setting , where we talk about mutual    information between two strings this is not evident at all .    nonetheless , up to some precision , the same non - increase law holds .",
    "this result was used recently to refine and extend the celebrated    gdel s incompleteness theorem .",
    "sufficient statistic : :    ( section  [ sect.sufstat ] ) although its roots are in the statistical    literature , the notion of probabilistic `` sufficient statistic '' has a    natural formalization in terms of mutual shannon information , and can    thus also be considered a part of shannon theory .",
    "the probabilistic    sufficient statistic extracts the information in the data about a    model class . in the algorithmic",
    "setting , a sufficient statistic    extracts the meaningful information from the data , leaving the    remainder as accidental random `` noise '' . in a certain sense",
    "the    probabilistic version of sufficient statistic is the expectation of    the algorithmic version .",
    "these ideas are generalized significantly in    the next item .",
    "rate distortion theory versus structure function : :    ( section  [ sect.rdsf ] ) entropy , kolmogorov complexity and mutual    information are concerned with _ lossless _",
    "description or compression :    messages must be described in such a way that from the description ,    the original message can be completely reconstructed . extending the    theories to _",
    "lossy _ description or compression leads to    rate - distortion theory in the shannon setting , and the kolmogorov    structure function in the kolmogorov section .",
    "the basic ingredients of    the lossless theory ( entropy and kolmogorov complexity ) remain the    building blocks for such extensions .",
    "the kolmogorov structure function    significantly extends the idea of `` meaningful information '' related    to the algorithmic sufficient statistic , and can be used to provide a    foundation for inductive inference principles such as minimum    description length ( mdl ) .",
    "once again , the kolmogorov structure    function can be related to shannon s rate - distortion function by    taking expectations in an appropriate manner .",
    "[ [ strings ] ] strings : + + + + + + + +    let @xmath0 be some finite or countable set .",
    "we use the notation @xmath1 to denote the set of finite _ strings _ or _ sequences _ over @xmath2 .",
    "for example , @xmath3 with @xmath4 denoting the _ empty word _ ` ' with no letters .",
    "let @xmath5 denotes the natural numbers .",
    "we identify @xmath5 and @xmath6 according to the correspondence @xmath7 the _ length _ @xmath8 of @xmath9 is the number of bits in the binary string @xmath9 .",
    "for example , @xmath10 and @xmath11 .",
    "if @xmath9 is interpreted as an integer , we get @xmath12 and , for @xmath13 , @xmath14 here , as in the sequel , @xmath15 is the smallest integer larger than or equal to @xmath9 , @xmath16 is the largest integer smaller than or equal to @xmath9 and @xmath17 denotes logarithm to base two .",
    "we shall typically be concerned with encoding finite - length binary strings by other finite - length binary strings .",
    "the emphasis is on binary strings only for convenience ; observations in any alphabet can be so encoded in a way that is ` theory neutral ' .",
    "[ [ precision - and - stackrel_-stackrel_-notation ] ] precision and @xmath18 notation : + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    it is customary in the area of kolmogorov complexity to use `` additive constant @xmath19 '' or equivalently `` additive @xmath20 term '' to mean a constant , accounting for the length of a fixed binary program , independent from every variable or parameter in the expression in which it occurs . in this paper",
    "we use the prefix complexity variant of kolmogorov complexity for convenience .",
    "since ( in)equalities in the kolmogorov complexity setting typically hold up to an additive constant , we use a special notation .",
    "we will denote by @xmath21 an inequality to within an additive constant .",
    "more precisely , let @xmath22 be functions from @xmath6 to @xmath23 , the _",
    "real numbers_. then by ` @xmath24 ' we mean that there exists a @xmath19 such that for all @xmath25 , @xmath26 .",
    "we denote by @xmath27 the situation when both @xmath21 and @xmath28 hold .",
    "[ [ probabilistic - notions ] ] probabilistic notions : + + + + + + + + + + + + + + + + + + + + + +    let @xmath2 be a finite or countable set .",
    "a function @xmath29 $ ] is a _ probability mass function _ if @xmath30 .",
    "we call @xmath31 a _ sub - probability mass function _ if @xmath32 .",
    "such sub - probability mass functions will sometimes be used for technical convenience .",
    "we can think of them as ordinary probability mass functions by considering the surplus probability to be concentrated on an undefined element @xmath33 .    in the context of ( sub- ) probability mass functions",
    ", @xmath34 is called the _",
    "sample space_. associated with mass function @xmath31 and sample space @xmath2 is the _ random variable _ @xmath35 and the probability distribution @xmath36 such that @xmath35 takes value @xmath37 with probability @xmath38 .",
    "a subset of @xmath2 is called an _",
    "event_. we extend the probability of individual outcomes to events . with this terminology , @xmath39 is the probability that the singleton event @xmath40 occurs , and @xmath41 . in some cases ( where the use of @xmath42 would be confusing )",
    "we write @xmath43 as an abbreviation of @xmath44 . in the sequel",
    ", we often refer to probability distributions in terms of their mass functions , i.e. we freely employ phrases like ` let @xmath35 be distributed according to @xmath31 ' .    whenever we refer to probability mass functions without explicitly mentioning the sample space @xmath2",
    "is assumed to be @xmath5 or , equivalently , @xmath45 .    for a given probability mass function @xmath46 on sample space @xmath47 with random variable @xmath48",
    ", we define the _ conditional probability mass function _",
    "@xmath49 of outcome @xmath50 given outcome @xmath51 as @xmath52 note that @xmath35 and @xmath53 are not necessarily independent .    in some cases ( esp .",
    "section  [ sec : relpa ] and appendix  [ sec : universal ] ) , the notion of _ sequential information source _ will be needed .",
    "this may be thought of as a probability distribution over arbitrarily long binary sequences , of which an observer gets to see longer and longer initial segments .",
    "formally , a sequential information source @xmath36 is a probability distribution on the set @xmath54 of one - way infinite sequences .",
    "it is characterized by a _ sequence of probability mass functions _",
    "@xmath55 where @xmath56 is a probability mass function on @xmath57 that denotes the _ marginal _ distribution of @xmath36 on the first @xmath58-bit segments . by definition ,",
    "the sequence @xmath59 represents a sequential information source if for all @xmath60 , @xmath56 is related to @xmath61 as follows : for all @xmath62 , @xmath63 and @xmath64 .",
    "this is also called kolmogorov s _ compatibility condition _ @xcite .    some ( by no means all ! )",
    "probability mass functions on @xmath45 can be thought of as information sources .",
    "namely , given a probability mass function @xmath65 on @xmath66 , we can define @xmath67 as the conditional distribution of @xmath9 given that the length of @xmath9 is @xmath58 , with domain restricted to @xmath9 of length @xmath58 .",
    "that is , @xmath68 $ ] is defined , for @xmath69 , as @xmath70 . then @xmath65 can be thought of as an information source if and only if the sequence @xmath71 represents an information source .    [",
    "[ computable - functions ] ] computable functions : + + + + + + + + + + + + + + + + + + + + +    partial functions on the natural numbers @xmath5 are functions @xmath31 such that @xmath42 can be ` undefined ' for some @xmath9 .",
    "we abbreviate ` undefined ' to ` @xmath72 ' . a central notion in the theory of computation",
    "is that of the _ partial recursive functions_. formally , a function @xmath73 is called _ partial recursive _ or _",
    "computable _ if there exists a turing machine @xmath74 that implements @xmath31 .",
    "this means that for all @xmath9    1 .   if @xmath75 , then @xmath74 , when run with input @xmath9 outputs @xmath42 and then halts .",
    "if @xmath76 ( ` @xmath42 is undefined ' ) , then @xmath74 with input @xmath9 never halts .",
    "readers not familiar with computation theory may think of a turing machine as a computer program written in a general - purpose language such as c or java .",
    "a function @xmath77 is called _ total _ if it is defined for all @xmath9 ( i.e. for all @xmath9 , @xmath75 ) .",
    "total recursive _ function is thus a function that is implementable on a turing machine that halts on all inputs .",
    "these definitions are extended to several arguments as follows : we fix , once and for all , some standard invertible pairing function @xmath78 and we say that @xmath79 is computable if there exists a turing machine @xmath74 such that for all @xmath80 , @xmath74 with input @xmath81 outputs @xmath82 and halts if @xmath83 and otherwise @xmath74 does not halt . by repeating this construction , functions with arbitrarily many arguments can be considered .    _ real - valued functions : _ we call a distribution @xmath84 _ recursive _ or _ computable _ if there exists a turing machine that , when input @xmath85 with @xmath25 and @xmath86 , outputs @xmath42 to precision @xmath87 ; more precisely , it outputs a pair @xmath88 such that @xmath89 and an additional bit to indicate whether @xmath42 larger or smaller than @xmath90 . here",
    "@xmath91 is the standard pairing function . in this paper",
    "all real - valued functions we consider are by definition total .",
    "therefore , in line with the above definitions , for a real - valued function ` computable ' ( equivalently , recursive ) , means that there is a turing machine which for _ all _ @xmath9 , computes @xmath42 to arbitrary accuracy ; ` partial ' recursive real - valued functions are not considered .",
    "it is convenient to distinguish between _ upper _ and _ lower semi - computability_. for this purpose we consider both the argument of an auxiliary function @xmath92 and the value of @xmath92 as a pair of natural numbers according to the standard pairing function @xmath93 .",
    "we define a function from @xmath5 to the reals @xmath23 by a turing machine @xmath74 computing a function @xmath92 as follows .",
    "interpret the computation @xmath94 to mean that the quotient @xmath95 is the rational valued @xmath96th approximation of @xmath42 .",
    "[ def.enum.funct ] [ def.semi ] a function @xmath84 is _ lower semi - computable _ if there is a turing machine @xmath74 computing a total function @xmath92 such that @xmath97 and @xmath98 .",
    "this means that @xmath31 can be computably approximated from below .",
    "a function @xmath31 is _ upper semi - computable _ if @xmath99 is lower semi - computable , note that , if @xmath31 is both upper- and lower semi - computable , then @xmath31 is computable .    _",
    "( sub- ) probability mass functions : _ probability mass functions on @xmath6 may be thought of as real - valued functions on @xmath5 .",
    "therefore , the definitions of ` computable ' and ` recursive ' carry over unchanged from the real - valued function case .",
    "we repeatedly consider the following scenario : a _ sender _ ( say , a ) wants to communicate or transmit some information to a _ receiver _ ( say , b ) .",
    "the information to be transmitted is an element from some set @xmath2 ( this set may or may not consist of binary strings ) .",
    "it will be communicated by sending a binary string , called the _",
    "message_. when b receives the message , he can decode it again and ( hopefully ) reconstruct the element of @xmath2 that was sent . to achieve this , a and b need to agree on a _ code _ or _ description method _ before communicating . intuitively , this is a binary relation between _ source words _ and associated _",
    "code words_. the relation is fully characterized by the _",
    "decoding function_. such a decoding function @xmath100 can be any function @xmath101 .",
    "the domain of @xmath100 is the set of _ code words and the range of @xmath100 is the set of _ source words .",
    "@xmath102 is interpreted as `` @xmath103 is a code word for the source word @xmath9 '' .",
    "the set of all code words for source word @xmath9 is the set @xmath104 .",
    "hence , @xmath105 can be called the _ encoding substitution ( @xmath106 is not necessarily a function ) . with each code",
    "@xmath100 we can associate a _ length function _",
    "@xmath107 such that , for each source word @xmath9 , @xmath108 is the length of the shortest encoding of @xmath9 : @xmath109 we denote by @xmath110 the shortest @xmath103 such that @xmath102 ; if there is more than one such @xmath103 , then @xmath110 is defined to be the first such @xmath103 in some agreed - upon order  for example , the lexicographical order . _",
    "_ _    in coding theory attention is often restricted to the case where the source word set is finite , say @xmath111 .",
    "if there is a constant @xmath112 such that @xmath113 for all code words @xmath103 ( which implies , @xmath114 for all source words @xmath9 ) , then we call @xmath100 a _ fixed - length code .",
    "it is easy to see that @xmath115 .",
    "for instance , in teletype transmissions the source has an alphabet of @xmath116 letters , consisting of the 26 letters in the latin alphabet plus 6 special characters .",
    "hence , we need @xmath117 binary digits per source letter . in electronic computers",
    "we often use the fixed - length ascii code with @xmath118 .",
    "_    [ [ prefix - code ] ] prefix code : + + + + + + + + + + + +    it is immediately clear that in general we can not uniquely recover @xmath9 and @xmath103 from @xmath119 .",
    "let @xmath106 be the identity mapping .",
    "then we have @xmath120 .",
    "we now introduce _ prefix codes _ , which do not suffer from this defect .",
    "a binary string @xmath9 is a _ proper prefix _ of a binary string @xmath103 if we can write @xmath121 for @xmath122 .",
    "a set @xmath123 is _ prefix - free _ if for any pair of distinct elements in the set neither is a proper prefix of the other .",
    "a function @xmath124 defines a _ prefix - code _ if its domain is prefix - free . in order to decode a code sequence of a prefix - code ,",
    "we simply start at the beginning and decode one code word at a time .",
    "when we come to the end of a code word , we know it is the end , since no code word is the prefix of any other code word in a prefix - code .",
    "suppose we encode each binary string @xmath125 as @xmath126 the resulting code is prefix because we can determine where the code word @xmath127 ends by reading it from left to right without backing up .",
    "note @xmath128 ; thus , we have encoded strings in @xmath6 in a prefix manner at the price of doubling their length .",
    "we can get a much more efficient code by applying the construction above to the length @xmath8 of @xmath9 rather than @xmath9 itself : define @xmath129 , where @xmath8 is interpreted as a binary string according to the correspondence ( [ eq : correspondence ] ) .",
    "then the code @xmath130 with @xmath131 is a prefix code satisfying , for all @xmath132 , @xmath133 ( here we ignore the ` rounding error ' in ) .",
    "@xmath130 is used throughout this paper as a standard code to encode natural numbers in a prefix free - manner ; we call it the _ standard prefix - code for the natural numbers_. we use @xmath134 as notation for @xmath135 . when @xmath9 is interpreted as an integer ( using the correspondence ( [ eq : correspondence ] ) and ( [ eq : intlength ] ) ) , we see that , up to rounding , @xmath136 .    [ [ prefix - codes - and - the - kraft - inequality ] ] prefix codes and the kraft inequality : + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    let @xmath2 be the set of natural numbers and consider the straightforward non - prefix representation ( [ eq : correspondence ] ) .",
    "there are two elements of @xmath2 with a description of length @xmath137 , four with a description of length @xmath138 and so on .",
    "however , for a prefix code @xmath100 for the natural numbers there are less binary prefix code words of each length : if @xmath9 is a prefix code word then no @xmath139 with @xmath122 is a prefix code word .",
    "asymptotically there are less prefix code words of length @xmath58 than the @xmath140 source words of length @xmath58 .",
    "quantification of this intuition for countable @xmath2 and arbitrary prefix - codes leads to a precise constraint on the number of code - words of given lengths .",
    "this important relation is known as the _ kraft inequality _ and is due to l.g .",
    "kraft @xcite .",
    "[ kraft ] let @xmath141 be a finite or infinite sequence of natural numbers .",
    "there is a prefix - code with this sequence as lengths of its binary code words iff @xmath142    [ [ uniquely - decodable - codes ] ] uniquely decodable codes : + + + + + + + + + + + + + + + + + + + + + + + + +    we want to code elements of @xmath2 in a way that they can be uniquely reconstructed from the encoding .",
    "such codes are called ` uniquely decodable ' .",
    "every prefix - code is a uniquely decodable code .",
    "for example , if @xmath143 , @xmath144 , @xmath145 , @xmath146 then @xmath147 is encoded as @xmath148 , which can be easily decoded from left to right in a unique way .",
    "on the other hand , not every uniquely decodable code satisfies the prefix condition .",
    "prefix - codes are distinguished from other uniquely decodable codes by the property that the end of a code word is always recognizable as such .",
    "this means that decoding can be accomplished without the delay of observing subsequent code words , which is why prefix - codes are also called instantaneous codes .",
    "there is good reason for our emphasis on prefix - codes .",
    "namely , it turns out that theorem  [ kraft ] stays valid if we replace `` prefix - code '' by `` uniquely decodable code . ''",
    "this important fact means that every uniquely decodable code can be replaced by a prefix - code without changing the set of code - word lengths . in shannon s and kolmogorov s theories , we are only interested in code word",
    "_ lengths _ of uniquely decodable codes rather than actual encodings . by the previous argument",
    ", we may restrict the set of codes we work with to prefix codes , which are much easier to handle .",
    "[ [ probability - distributions - and - complete - prefix - codes ] ] probability distributions and complete prefix codes : + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    a uniquely decodable code is _",
    "complete if the addition of any new code word to its code word set results in a non - uniquely decodable code .",
    "it is easy to see that a code is complete iff equality holds in the associated kraft inequality .",
    "let @xmath149 be the code words of some complete uniquely decodable code .",
    "let us define @xmath150 . by definition of completeness , we have @xmath151 .",
    "thus , the @xmath152 can be thought of as _ probability mass functions _ corresponding to some probability distribution @xmath153 .",
    "we say @xmath153 is the distribution _ corresponding _ to @xmath154 . in this way , each complete uniquely decodable code is mapped to a unique probability distribution .",
    "of course , this is nothing more than a formal correspondence : we may choose to encode outcomes of @xmath35 using a code corresponding to a distribution @xmath155 , whereas the outcomes are actually distributed according to some @xmath156 .",
    "but , as we show below , if @xmath35 is distributed according to @xmath157 , then the code to which @xmath157 corresponds is , in an average sense , the code that achieves optimal compression of @xmath35 . _",
    "it seldom happens that a detailed mathematical theory springs forth in essentially final form from a single publication .",
    "such was the case with shannon information theory , which properly started only with the appearance of c.e .",
    "shannon s paper `` the mathematical theory of communication '' @xcite . in this paper , shannon proposed a measure of information in a distribution , which he called the ` entropy ' .",
    "the entropy @xmath158 of a distribution @xmath36 measures the ` the inherent uncertainty in @xmath36 ' , or ( in fact equivalently ) , ` how much information is gained when an outcome of @xmath36 is observed ' . to make this a bit more precise ,",
    "let us imagine an observer who knows that @xmath35 is distributed according to @xmath36 .",
    "the observer then observes @xmath51 .",
    "the entropy of @xmath36 stands for the ` uncertainty of the observer about the outcome @xmath9 _ before _ he observes it ' .",
    "now think of the observer as a ` receiver ' who receives the message conveying the value of @xmath35 . from this dual point of view",
    ", the entropy stands for    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ the average amount of information that the observer has gained _ after _ receiving a realized outcome @xmath9 of the random variable @xmath35 . @xmath159",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    below , we first give shannon s mathematical definition of entropy , and we then connect it to its intuitive meaning @xmath159 .",
    "let @xmath2 be a finite or countable [ def.entropy ] set , let @xmath35 be a random variable taking values in @xmath2 with distribution @xmath160 .",
    "then the ( shannon- ) _ entropy of random variable @xmath35 is given by @xmath161 entropy is defined here as a functional mapping random variables to real numbers . in many texts ,",
    "entropy is , essentially equivalently , defined as a map from _ distributions _ of random variables to the real numbers .",
    "thus , by definition : @xmath162 . _    [ [ motivation ] ] motivation : + + + + + + + + + + +    the entropy function can be motivated in different ways .",
    "the two most important ones are the _",
    "axiomatic _ approach and the _ coding interpretation_. in this paper we concentrate on the latter , but we first briefly sketch the former .",
    "the idea of the axiomatic approach is to postulate a small set of self - evident axioms that any measure of information relative to a distribution should satisfy .",
    "one then shows that the only measure satisfying all the postulates is the shannon entropy .",
    "we outline this approach for finite sources @xmath163 .",
    "we look for a function @xmath164 that maps probability distributions on @xmath2 to real numbers . for",
    "given distribution @xmath36 , @xmath158 should measure ` how much information is gained on average when an outcome is made available ' .",
    "we can write @xmath165 where @xmath166 stands for the probability of @xmath167 .",
    "suppose we require that    1 .",
    "@xmath168 is continuous in @xmath169 .",
    "if all the @xmath166 are equal , @xmath170 , then @xmath164 should be a monotonic increasing function of @xmath171 .",
    "with equally likely events there is more choice , or uncertainty , when there are more possible events .",
    "if a choice is broken down into two successive choices , the original @xmath164 should be the weighted sum of the individual values of @xmath164 .",
    "rather than formalizing this condition , we will give a specific example .",
    "suppose that @xmath172 , and @xmath173 .",
    "we can think of @xmath174 as being generated in a two - stage process .",
    "first , an outcome in @xmath175 is generated according to a distribution @xmath176 with @xmath177 . if @xmath178 , we set @xmath179 and the process stops .",
    "if @xmath180 , then outcome ` @xmath138 ' is generated with probability @xmath181 and outcome ` @xmath182 ' with probability @xmath183 , and the process stops .",
    "the final results have the same probabilities as before . in this particular case",
    "we require that @xmath184 thus , the entropy of @xmath36 must be equal to entropy of the first step in the generation process , plus the weighted sum ( weighted according to the probabilities in the first step ) of the entropies of the second step in the generation process .",
    "+ as a special case , if @xmath2 is the @xmath58-fold product space of another space @xmath185 , @xmath186 and the @xmath187 are all independently distributed according to @xmath188 , then @xmath189 .",
    "for example , the total entropy of @xmath58 independent tosses of a coin with bias @xmath157 is @xmath190 .",
    "[ thm : axiomatic ] the only @xmath164 satisfying the three above assumptions is of the form @xmath191 with @xmath192 a constant .",
    "thus , requirements ( 1)(3 ) lead us to the definition of entropy ( [ eq : entropy ] ) given above up to an ( unimportant ) scaling factor .",
    "we shall give a concrete interpretation of this factor later on .",
    "besides the defining characteristics ( 1)(3 ) , the function @xmath164 has a few other properties that make it attractive as a measure of information .",
    "we mention :    @xmath168 is a concave function of the @xmath166 .    for each @xmath171 ,",
    "@xmath164 achieves its unique maximum for the uniform distribution @xmath193 .",
    "@xmath168 is zero iff one of the @xmath166 has value @xmath137 .",
    "thus , @xmath164 is zero if and only if we do not gain any information at all if we are told that the outcome is @xmath167 ( since we already knew @xmath167 would take place with certainty ) .",
    "[ [ the - coding - interpretation ] ] the coding interpretation : + + + + + + + + + + + + + + + + + + + + + + + + + +    immediately after stating theorem  [ thm : axiomatic ] , shannon @xcite continues , `` this theorem , and the assumptions required for its proof , are in no way necessary for the present theory .",
    "it is given chiefly to provide a certain plausibility to some of our later definitions .",
    "the _ real justification _ of these definitions , however , will reside in their implications '' . following this injunction , we emphasize the main practical interpretation of entropy as the length ( number of bits ) needed to encode outcomes in @xmath2 .",
    "this provides much clearer intuitions , it lies at the root of the many practical applications of information theory , and , most importantly for us , it simplifies the comparison to kolmogorov complexity .",
    "the entropy of a random variable @xmath35 with equally likely outcomes in a finite sample space @xmath2 is given by @xmath194 . by choosing a particular message @xmath9 from @xmath2",
    ", we remove the entropy from @xmath35 by the assignment @xmath195 and produce or transmit _ information _",
    "@xmath196 by our selection of @xmath9 .",
    "we show below that @xmath196 ( or , to be more precise , the integer @xmath197 ) can be interpreted as the number of bits needed to be transmitted from an ( imagined ) sender to an ( imagined ) receiver .",
    "we now connect entropy to minimum average code lengths .",
    "these are defined as follows :    let source words @xmath25 be produced by a random variable @xmath35 with probability @xmath160 for the event @xmath51 .",
    "the characteristics of @xmath35 are fixed .",
    "now consider prefix codes @xmath198 with one code word per source word , and denote the length of the code word for @xmath9 by @xmath199 .",
    "we want to minimize the expected number of bits we have to transmit for the given source @xmath35 and choose a prefix code @xmath100 that achieves this .",
    "in order to do so , we must minimize the _ average code - word length @xmath200 .",
    "we define the _ minimal average code word length as @xmath201 .",
    "a prefix - code @xmath100 such that @xmath202 is called an _ optimal prefix - code with respect to prior probability @xmath36 of the source words . _ _ _    the ( minimal ) average code length of an ( optimal ) code does not depend on the details of the set of code words , but only on the set of code - word lengths .",
    "it is just the expected code - word length with respect to the given distribution .",
    "shannon discovered that the minimal average code word length is about equal to the entropy of the source word set .",
    "this is known as the _ noiseless coding theorem_. the adjective `` noiseless '' emphasizes that we ignore the possibility of errors .",
    "[ thm : noiseless ] let @xmath203 and @xmath36 be as above",
    ". if @xmath204 is the entropy , then @xmath205    we are typically interested in encoding a binary string of length @xmath58 with entropy proportional to @xmath58 ( example  [ ex : universal ] ) .",
    "the essence of ( [ eq : entopt ] ) is that , for all but the smallest @xmath58 , the difference between entropy and minimal expected code length is completely negligible .",
    "it turns out that the optimum @xmath203 in ( [ eq : entopt ] ) is relatively easy to achieve , with the shannon - fano code .",
    "let there be @xmath171 symbols ( also called basic messages or source words ) .",
    "order these symbols according to decreasing probability , say @xmath206 with probabilities @xmath207 .",
    "let @xmath208 , for @xmath209 .",
    "the binary code @xmath210 is obtained by coding @xmath211 as a binary number @xmath212 , obtained by truncating the binary expansion of @xmath213 at length @xmath214 such that @xmath215 this code is the _ shannon - fano code_. it has the property that highly probable symbols are mapped to short code words and symbols with low probability are mapped to longer code words ( just like in a less optimal , non - prefix - free , setting is done in the morse code ) .",
    "moreover , @xmath216 note that the code for symbol @xmath211 differs from all codes of symbols @xmath217 through @xmath171 in one or more bit positions , since for all @xmath167 with @xmath218 , @xmath219 therefore the binary expansions of @xmath213 and @xmath220 differ in the first @xmath214 positions .",
    "this means that @xmath106 is one - to - one , and it has an inverse : the decoding mapping @xmath221 .",
    "even better , since no value of @xmath106 is a prefix of any other value of @xmath106 , the set of code words is a prefix - code .",
    "this means we can recover the source message from the code message by scanning it from left to right without look - ahead . if @xmath222 is the average number of bits used per symbol of an original message , then @xmath223 . combining this with the previous inequality we obtain ( [ eq : entopt ] ) : @xmath224     + * problem and lacuna : * shannon observes ,  messages have _ meaning [ @xmath225 however @xmath225 ] the semantic aspects of communication are irrelevant to the engineering problem .  in other words , can we answer a question like `` what is the information in this book '' by viewing it as an element of a set of possible books with a probability distribution on it ? or that the individual sections in this book form a random sequence with stochastic relations that damp out rapidly over a distance of several pages ? and how to measure the quantity of hereditary information in biological organisms , as encoded in dna ?",
    "again there is the possibility of seeing a particular form of animal as one of a set of possible forms with a probability distribution on it .",
    "this seems to be contradicted by the fact that the calculation of all possible lifeforms in existence at any one time on earth would give a ridiculously low figure like @xmath226 .",
    "_    shannon s classical information theory assigns a quantity of information to an ensemble of possible messages .",
    "all messages in the ensemble being equally probable , this quantity is the number of bits needed to count all possibilities .",
    "this expresses the fact that each message in the ensemble can be communicated using this number of bits .",
    "however , it does not say anything about the number of bits needed to convey any individual message in the ensemble . to illustrate this ,",
    "consider the ensemble consisting of all binary strings of length 9999999999999999 .    by shannon s measure",
    ", we require 9999999999999999 bits on the average to encode a string in such an ensemble .",
    "however , the string consisting of 9999999999999999 1 s can be encoded in about 55 bits by expressing 9999999999999999 in binary and adding the repeated pattern `` 1 . '' a requirement for this to work is that we have agreed on an algorithm that decodes the encoded string .",
    "we can compress the string still further when we note that 9999999999999999 equals @xmath227 , and that 1111111111111111 consists of @xmath228 1 s .",
    "thus , we have discovered an interesting phenomenon : the description of some strings can be compressed considerably , provided they exhibit enough regularity .",
    "however , if regularity is lacking , it becomes more cumbersome to express large numbers .",
    "for instance , it seems easier to compress the number `` one billion , '' than the number `` one billion seven hundred thirty - five million two hundred sixty - eight thousand and three hundred ninety - four , '' even though they are of the same order of magnitude .",
    "we are interested in a measure of information that , unlike shannon s , does not rely on ( often untenable ) probabilistic assumptions , and that takes into account the phenomenon that ` regular ' strings are compressible .",
    "thus , we aim for a measure of information content of an _ individual finite object , and in the information conveyed about an individual finite object by another individual finite object . here",
    ", we want the information content of an object @xmath9 to be an attribute of @xmath9 _ alone , and not to depend on , for instance , the means chosen to describe this information content .",
    "surprisingly , this turns out to be possible , at least to a large extent .",
    "the resulting theory of information is based on kolmogorov complexity , a notion independently proposed by solomonoff ( 1964 ) , kolmogorov ( 1965 ) and chaitin ( 1969 ) ; li and vitnyi ( 1997 ) describe the history of the subject . _ _      suppose we want to describe a given object by a finite binary string .",
    "we do not care whether the object has many descriptions ; however , each description should describe but one object . from among all descriptions of an object",
    "we can take the length of the shortest description as a measure of the object s complexity .",
    "it is natural to call an object `` simple '' if it has at least one short description , and to call it `` complex '' if all of its descriptions are long .    as in section",
    "[ sec : coding ] , consider a description method @xmath100 , to be used to transmit messages from a sender to a receiver . if @xmath100 is known to both a sender and receiver , then a message @xmath9 can be transmitted from sender to receiver by transmitting the description @xmath103 with @xmath229 .",
    "the cost of this transmission is measured by @xmath230 , the length of @xmath103 .",
    "the least cost of transmission of @xmath9 is determined by the length function @xmath108 : recall that @xmath108 is the length of the shortest @xmath103 such that @xmath229 .",
    "we choose this length function as the descriptional complexity of @xmath9 under specification method @xmath100 .",
    "obviously , this descriptional complexity of @xmath9 depends crucially on @xmath100 .",
    "the general principle involved is that the syntactic framework of the description language determines the succinctness of description .    in order to objectively compare descriptional complexities of objects , to be able to say `` @xmath9 is more complex than @xmath231 , '' the descriptional complexity of @xmath9 should depend on @xmath9 alone .",
    "this complexity can be viewed as related to a universal description method that is a priori assumed by all senders and receivers .",
    "this complexity is optimal if no other description method assigns a lower complexity to any object .",
    "we are not really interested in optimality with respect to all description methods . for specifications to be useful at all",
    "it is necessary that the mapping from @xmath103 to @xmath232 can be executed in an effective manner .",
    "that is , it can at least in principle be performed by humans or machines .",
    "this notion has been formalized as that of `` partial recursive functions '' , also known simply as `` computable functions '' , which are formally defined later . according to generally accepted mathematical viewpoints it coincides with the intuitive notion of effective computation .",
    "the set of partial recursive functions contains an optimal function that minimizes description length of every other such function .",
    "we denote this function by @xmath233 .",
    "namely , for any other recursive function @xmath100 , for all objects @xmath9 , there is a description @xmath103 of @xmath9 under @xmath233 that is shorter than any description @xmath231 of @xmath9 under @xmath100 .",
    "( that is , shorter up to an additive constant that is independent of @xmath9 . )",
    "complexity with respect to @xmath233 minorizes the complexities with respect to all partial recursive functions .",
    "we identify the length of the description of @xmath9 with respect to a fixed specification function @xmath233 with the `` algorithmic ( descriptional ) complexity '' of @xmath9 .",
    "the optimality of @xmath233 in the sense above means that the complexity of an object @xmath9 is invariant ( up to an additive constant independent of @xmath9 ) under transition from one optimal specification function to another .",
    "its complexity is an objective attribute of the described object alone : it is an intrinsic property of that object , and it does not depend on the description formalism . this complexity can be viewed as `` absolute information content '' : the amount of information that needs to be transmitted between all senders and receivers when they communicate the message in absence of any other a priori knowledge that restricts the domain of the message .",
    "thus , we have outlined the program for a general theory of algorithmic complexity .",
    "the three major innovations are as follows :    1 .   in restricting ourselves to formally effective descriptions , our definition covers every form of description that is intuitively acceptable as being effective according to general viewpoints in mathematics and logic .",
    "the restriction to effective descriptions entails that there is a universal description method that minorizes the description length or complexity with respect to any other effective description method .",
    "significantly , this implies item 3 .",
    "the description length or complexity of an object is an intrinsic attribute of the object independent of the particular description method or formalizations thereof .",
    "the kolmogorov complexity @xmath234 of a finite object @xmath9 will be defined as the length of the shortest effective binary description of @xmath9 .",
    "broadly speaking , @xmath234 may be thought of as the length of the shortest computer program that prints @xmath9 and then halts .",
    "this computer program may be written in c , java , lisp or any other universal language : we shall see that , for any two universal languages , the resulting program lengths differ at most by a constant not depending on @xmath9 .    to make this precise ,",
    "let @xmath235 be a standard enumeration @xcite of all turing machines , and let @xmath236 be the enumeration of corresponding functions which are computed by the respective turing machines .",
    "that is , @xmath237 computes @xmath238 .",
    "these functions are the _ partial recursive _ functions or _",
    "computable _ functions , section  [ sec : preliminaries ] . for technical reasons we are interested in the so - called prefix complexity , which is associated with turing machines for which the set of programs ( inputs ) resulting in a halting computation is prefix free .",
    "we can realize this by equipping the turing machine with a one - way input tape , a separate work tape , and a one - way output tape .",
    "such turing machines are called prefix machines since the halting programs for any one of them form a prefix free set .",
    "we first define @xmath239 , the prefix kolmogorov complexity of @xmath9 relative to a given prefix machine @xmath237 , where @xmath237 is the @xmath167-th prefix machine in a standard enumeration of them .",
    "@xmath239 is defined as the length of the shortest input sequence @xmath103 such that @xmath240 .",
    "if no such input sequence exists , @xmath239 remains undefined .",
    "of course , this preliminary definition is still highly sensitive to the particular prefix machine @xmath237 that we use .",
    "but now the ` universal prefix machine ' comes to our rescue . just as there exists universal ordinary turing machines",
    ", there also exist universal prefix machines .",
    "these have the remarkable property that they can simulate every other prefix machine .",
    "more specifically , there exists a prefix machine @xmath241 such that , with as input the pair @xmath242 , it outputs @xmath243 and then halts .",
    "we now fix , once and for all , a prefix machine @xmath241 with this property and call @xmath241 the _ reference machine_. the kolmogorov complexity @xmath234 of @xmath9 is defined as @xmath244 .",
    "let us formalize this definition .",
    "let @xmath245 be a standard invertible effective one - one encoding from @xmath246 to a prefix - free subset of @xmath5 .",
    "@xmath245 may be thought of as the encoding function of a prefix code .",
    "for example , we can set @xmath247 . comparing to the definition of in section  [ sec : preliminaries ]",
    ", we note that from now on , we require @xmath245 to map to a prefix - free set .",
    "we insist on prefix - freeness and effectiveness because we want a universal turing machine to be able to read an image under @xmath245 from left to right and determine where it ends .",
    "[ def.kolmk ] let @xmath241 be our reference prefix machine satisfying for all @xmath248 , @xmath249 .",
    "the _ prefix kolmogorov complexity _ of @xmath9 is @xmath250    we can alternatively think of @xmath231 as a program that prints @xmath9 and then halts , or as @xmath251 where @xmath103 is a program such that , when @xmath237 is input program @xmath103 , it prints @xmath9 and then halts .    thus , by definition @xmath252 , where @xmath110 is the lexicographically first shortest self - delimiting ( prefix ) program for @xmath9 with respect to the reference prefix machine . consider the mapping @xmath253 defined by @xmath254 .",
    "this may be viewed as the encoding function of a prefix - code ( decoding function ) @xmath255 with @xmath256 . by its definition",
    ", @xmath255 is a very parsimonious code .",
    "the reason for working with prefix rather than standard turing machines is that , for many of the subsequent developments , we need @xmath255 to be prefix . though defined in terms of a particular machine model , the kolmogorov complexity is machine - independent up to an additive constant and acquires an asymptotically universal and absolute character through church s thesis , from the ability of universal machines to simulate one another and execute any effective process .",
    "the kolmogorov complexity of an object can be viewed as an absolute and objective quantification of the amount of information in it .      to develop some intuitions ,",
    "it is useful to think of @xmath234 as the shortest program for @xmath9 in some standard programming language such as lisp or java .",
    "consider the lexicographical enumeration of all syntactically correct lisp programs @xmath257 , and the lexicographical enumeration of all syntactically correct java programs @xmath258 .",
    "we assume that both these programs are encoded in some standard prefix - free manner . with proper definitions we can view the programs in both enumerations as computing partial recursive functions from their inputs to their outputs .",
    "choosing reference machines in both enumerations we can define complexities @xmath259 and @xmath260 completely analogous to @xmath234 .",
    "all of these measures of the descriptional complexities of @xmath9 coincide up to a fixed additive constant .",
    "let us show this directly for @xmath259 and @xmath261 .",
    "since lisp is universal , there exists a lisp program @xmath262 implementing a java - to - lisp compiler .",
    "@xmath262 translates each java program to an equivalent lisp program .",
    "consequently , for all @xmath9 , @xmath263 . similarly , there is a java program @xmath264 that is a lisp - to - java compiler , so that for all @xmath9 , @xmath265 .",
    "it follows that @xmath266 for all @xmath9 !",
    "the programming language view immediately tells us that @xmath234 must be small for ` simple ' or ` regular ' objects @xmath9 .",
    "for example , there exists a fixed - size program that , when input @xmath58 , outputs the first @xmath58 bits of @xmath267 and then halts .",
    "specification of @xmath58 takes at most @xmath268 bits .",
    "thus , if @xmath9 consists of the first @xmath58 binary digits of @xmath269 , then @xmath270 .",
    "similarly , if @xmath271 denotes the string consisting of @xmath58 @xmath90 s , then @xmath272 .",
    "on the other hand , for all @xmath9 , there exists a program ` print @xmath9 ; halt ' .",
    "this shows that for all @xmath273 .",
    "as was previously noted , for any prefix code , there are no more than @xmath274 strings @xmath9 which can be described by @xmath275 or less bits . in particular , this holds for the prefix code @xmath253 whose length function is @xmath234 .",
    "thus , the fraction of strings @xmath9 of length @xmath58 with @xmath276 is at most @xmath277 : the overwhelming majority of sequences can not be compressed by more than a constant . specifically , if @xmath9 is determined by @xmath58 independent tosses of a fair coin , then with overwhelming probability , @xmath278 . thus , while for very regular strings , the kolmogorov complexity is small ( sub - linear in the length of the string ) , _ most _ strings are ` random ' and have kolmogorov complexity about equal to their own length .      [",
    "[ finite - sets ] ] finite sets : + + + + + + + + + + + +    the class of _ finite sets _ consists of the set of finite subsets @xmath279 . the _ complexity of the finite set _",
    "@xmath280 is @xmath281the length ( number of bits ) of the shortest binary program @xmath157 from which the reference universal prefix machine @xmath241 computes a listing of the elements of @xmath280 and then halts .",
    "that is , if @xmath282 , then @xmath283 .",
    "the _ conditional complexity _",
    "@xmath284 of @xmath9 given @xmath280 , is the length ( number of bits ) in the shortest binary program @xmath157 from which the reference universal prefix machine @xmath241 , given @xmath280 literally as auxiliary information , computes @xmath9 .",
    "[ [ integer - valued - functions ] ] integer - valued functions : + + + + + + + + + + + + + + + + + + + + + + + + +    the ( prefix- ) complexity @xmath285 of a partial recursive function @xmath31 is defined by @xmath286 if @xmath287 is a shortest program for computing the function @xmath31 ( if there is more than one of them then @xmath287 is the first one in enumeration order ) , then @xmath288 .    in the above definition of @xmath285 ,",
    "the objects being described are functions instead of finite binary strings . to unify the approaches",
    ", we can consider a finite binary string @xmath9 as corresponding to a function having value @xmath9 for argument 0 .",
    "note that we can upper semi - compute ( section  [ sec : preliminaries ] ) @xmath110 given @xmath9 , but we can not upper semi - compute @xmath287 given @xmath31 ( as an oracle ) , since we should be able to verify agreement of a program for a function and an oracle for the target function , on all infinitely many arguments .",
    "[ [ probability - distributions ] ] probability distributions : + + + + + + + + + + + + + + + + + + + + + + + + + +    in this text we identify probability distributions on finite and countable sets @xmath34 with their corresponding mass functions ( section  [ sec : preliminaries ] ) .",
    "since any ( sub- ) probability mass function @xmath31 is a total real - valued function , @xmath285 is defined in the same way as above .      following the definitions above we now consider lower semi - computable and computable probability mass functions ( section  [ sec : preliminaries ] ) . by the fundamental kraft s inequality , theorem  [ kraft ] , we know that if @xmath289 are the code - word lengths of a prefix code , then @xmath290 .",
    "therefore , since @xmath234 is the length of a prefix - free program for @xmath9 , we can interpret @xmath291 as a sub - probability mass function , and we define @xmath292 .",
    "this is the so - called universal distribution  a rigorous form of occam s razor .",
    "the following two theorems are to be considered as major achievements in the theory of kolmogorov complexity , and will be used again and again in the sequel . for the proofs",
    "we refer to @xcite .",
    "[ pr1 ] let @xmath31 represent a lower semi - computable ( sub- ) probability distribution on the natural numbers ( equivalently , finite binary strings ) .",
    "( this implies @xmath293 . ) then , @xmath294 for all @xmath9 , where @xmath295 .",
    "we call @xmath296 a _",
    "universal distribution_.    the family of lower semi - computable sub - probability mass functions contains all distributions with computable parameters which have a name , or in which we could conceivably be interested , or which have ever been considered for all @xmath297 $ ] .",
    "however , every concrete _ parameter estimate _ or _ predictive distribution _ based on the bernoulli model class that has ever been considered or in which we could be conceivably interested , is in fact computable ; typically , @xmath298 is then rational - valued .",
    "see also example  [ ex : appy ] in appendix  [ sec : universal ] . ] .",
    "in particular , it contains the computable distributions .",
    "we call @xmath299 `` universal '' since it assigns at least as much probability to each object as any other lower semi - computable distribution ( up to a multiplicative factor ) , and is itself lower semi - computable .",
    "[ pr2 ] @xmath300    that means that @xmath299 assigns high probability to simple objects and low probability to complex or random objects .",
    "for example , for @xmath301 ( @xmath58 0 s ) we have @xmath302 since the program @xmath303 prints @xmath9 .",
    "( the additional @xmath304 term is the penalty term for a prefix encoding . )",
    "then , @xmath305 .",
    "but if we flip a coin to obtain a string @xmath103 of @xmath58 bits , then with overwhelming probability @xmath306 ( because @xmath103 does not contain effective regularities which allow compression ) , and hence @xmath307 .",
    "[ [ problem - and - lacuna ] ] problem and lacuna : + + + + + + + + + + + + + + + + + + +    unfortunately @xmath234 is not a recursive function : the kolmogorov complexity is not computable in general .",
    "this means that there exists no computer program that , when input an arbitrary string , outputs the kolmogorov complexity of that string and then halts .",
    "while kolmogorov complexity is upper semi - computable ( section  [ sec : preliminaries ] ) , it can not be approximated in general in a practically useful sense ; and even though there exist ` feasible ' , resource - bounded forms of kolmogorov complexity ( li and vitnyi 1997 ) , these lack some of the elegant properties of the original , uncomputable notion .",
    "now suppose we are interested in efficient storage and transmission of long sequences of data .",
    "according to kolmogorov , we can compress such sequences in an essentially optimal way by storing or transmitting the shortest program that generates them .",
    "unfortunately , as we have just seen , we can not find such a program in general . according to shannon",
    ", we can compress such sequences optimally in an average sense ( and therefore , it turns out , also with high probability ) if they are distributed according to some @xmath36 and we know @xmath36 .",
    "unfortunately , in practice , @xmath36 is often unknown , it may not be computable  bringing us in the same conundrum as with the kolmogorov complexity approach  or worse , it may be nonexistent . in appendix",
    "[ sec : universal ] , we consider _ universal coding _ , which can be considered a sort of middle ground between shannon information and kolmogorov complexity .",
    "in contrast to both these approaches , universal codes can be directly applied for practical data compression .",
    "some basic knowledge of universal codes will be very helpful in providing intuition for the next section , in which we relate kolmogorov complexity and shannon entropy .",
    "nevertheless , universal codes are not directly needed in any of the statements and proofs of the next section or , in fact , anywhere else in the paper , which is why delegated their treatment to an appendix .",
    "suppose the source words @xmath9 are distributed as a random variable @xmath35 with probability @xmath38 .",
    "while @xmath234 is fixed for each @xmath9 and gives the shortest code word length ( but only up to a fixed constant ) and is _ independent _ of the probability distribution @xmath36 , we may wonder whether @xmath192 is also universal in the following sense : if we weigh each individual code word length for @xmath9 with its probability @xmath42 , does the resulting @xmath31-expected code word length @xmath308 achieve the minimal average code word length @xmath309 ? here",
    "we sum over the entire support of @xmath31 ; restricting summation to a small set , for example the singleton set @xmath40 , can give a different result .",
    "the reasoning above implies that , under some mild restrictions on the distributions @xmath31 , the answer is yes .",
    "this is expressed in the following theorem , where , instead of the quotient we look at the difference of @xmath310 and @xmath311 .",
    "this allows us to express really small distinctions .",
    "[ theo.eq.entropy ] let @xmath31 be a computable probability mass function ( section  [ sec : preliminaries ] ) @xmath312 on sample space @xmath313 associated with a random source @xmath35 and entropy @xmath314 .",
    "then , @xmath315    since @xmath234 is the code word length of a prefix - code for @xmath9 , the first inequality of the noiseless coding theorem  [ thm : noiseless ] states that @xmath316 since @xmath317 ( theorem  [ pr1 ] ) and @xmath318 ( theorem  [ pr2 ] ) , we have @xmath319 .",
    "it follows that @xmath320 set the constant @xmath321 to @xmath322 and the theorem is proved . as an aside ,",
    "the constant implied in the @xmath20 term depends on the lengths of the programs occurring in the proof of the cited theorems  [ pr1 ] , [ pr2 ] ( theorems 4.3.1 and 4.3.2 in @xcite ) .",
    "these depend only on the reference universal prefix machine .",
    "the theorem shows that for simple ( low complexity ) distributions the expected kolmogorov complexity is close to the entropy , but these two quantities may be wide apart for distributions of high complexity .",
    "this explains the apparent problem arising in considering a distribution @xmath31 that concentrates all probability on an element @xmath9 of length @xmath58 .",
    "suppose we choose @xmath323 .",
    "then @xmath324 and hence the entropy @xmath325 . on the other hand",
    "the term @xmath326 .",
    "therefore , the discrepancy between the expected kolmogorov complexity and the entropy exceeds the length @xmath58 of @xmath9 .",
    "one may think this contradicts the theorem , but that is not the case : the complexity of the distribution is at least that of @xmath9 , since we can reconstruct @xmath9 given @xmath31 ( just compute @xmath327 for all @xmath103 of length @xmath58 in lexicographical order until we meet one that has probability 1 ) .",
    "thus , @xmath328 .",
    "thus , if we pick a probability distribution with a complex support , or a trickily skewed probability distribution , than this is reflected in the complexity of that distribution , and as consequence in the closeness between the entropy and the expected kolmogorov complexity .",
    "for example , bringing the discussion in line with the universal coding counterpart of appendix  [ sec : universal ] by considering @xmath31 s that can be interpreted as sequential information sources and denoting the conditional version of @xmath31 restricted to strings of length @xmath58 by @xmath56 as in section  [ sec : preliminaries ] , we find by the same proof as the theorem that for all @xmath58 , @xmath329 where @xmath330 is now a constant depending on both @xmath31 and @xmath58 . on the other hand , we can eliminate the complexity of the distribution , or its recursivity for that matter , and / or restrictions to a conditional version of @xmath31 restricted to a finite support @xmath331 ( for example @xmath332 ) , denoted by @xmath333 , in the following conditional formulation ( this involves a peek in the future since the precise meaning of the `` @xmath334 '' notation is only provided in definition  [ def.kolmkb ] ) : @xmath335    the shannon - fano code for a computable distribution is itself computable . therefore , for every computable distribution @xmath31 , the universal code @xmath255 whose length function is the kolmogorov complexity compresses on average at least as much as the shannon - fano code for @xmath31 .",
    "this is the intuitive reason why , no matter what computable distribution @xmath31 we take , its expected kolmogorov complexity is close to its entropy .",
    "how much information can a random variable @xmath35 convey about a random variable @xmath53 ? taking a purely combinatorial approach , this notion is captured as follows : if @xmath35 ranges over @xmath2 and @xmath53 ranges over @xmath185 , then we look at the set @xmath241 of possible events @xmath336 consisting of joint occurrences of event @xmath51 and event @xmath50 .",
    "if @xmath241 does not equal the cartesian product @xmath337 , then this means there is some dependency between @xmath35 and @xmath53 .",
    "considering the set @xmath338 for @xmath339 , it is natural to define the _ conditional entropy of @xmath53 given @xmath340 as @xmath341 .",
    "this suggests immediately that the information given by @xmath51 about @xmath53 is @xmath342 for example , if @xmath343 , @xmath344 with @xmath345 and @xmath346 , then @xmath347 and @xmath348 .",
    "_    in this formulation it is obvious that @xmath349 , and that @xmath350",
    ". this approach amounts to the assumption of a _ uniform distribution _ of the probabilities concerned .",
    "we can generalize this approach , taking into account the frequencies or probabilities of the occurrences of the different values @xmath35 and @xmath53 can assume . let the _ joint probability _ @xmath46 be the `` probability of the joint occurrence of event @xmath51 and event @xmath50 . ''",
    "the _ marginal probabilities _ @xmath351 and @xmath352 are defined by @xmath353 and @xmath354 and are `` the probability of the occurrence of the event @xmath51 '' and the `` probability of the occurrence of the event @xmath50 '' , respectively .",
    "this leads to the self - evident formulas for joint variables @xmath355 : @xmath356 where summation over @xmath9 is taken over all outcomes of the random variable @xmath35 and summation over @xmath103 is taken over all outcomes of random variable @xmath53 .",
    "one can show that @xmath357 with equality only in the case that @xmath35 and @xmath53 are independent . in all of these equations",
    "the entropy quantity on the left - hand side increases if we choose the probabilities on the right - hand side more equally .",
    "[ [ conditional - entropy ] ] conditional entropy : + + + + + + + + + + + + + + + + + + + +    we start the analysis of the information in @xmath35 about @xmath53 by first considering the _ conditional entropy of @xmath53 given @xmath35 as the average of the entropy for @xmath53 for each value of @xmath35 weighted by the probability of getting that particular value : @xmath358 here @xmath359 is the conditional probability mass function as defined in section  [ sec : preliminaries ] .",
    "_    the quantity on the left - hand side tells us how uncertain we are on average about the outcome of @xmath53 when we know an outcome of @xmath35 . with @xmath360 and substituting the formula for @xmath359 , we find @xmath361 .",
    "rewrite this expression as the entropy equality @xmath362 this can be interpreted as , `` the uncertainty of the joint event @xmath48 is the uncertainty of @xmath35 plus the uncertainty of @xmath53 given @xmath35 . ''",
    "combining equations  [ i4 ] , [ i5 ] gives @xmath363 , which can be taken to imply that , on average , knowledge of @xmath35 can never increase uncertainty of @xmath53 .",
    "in fact , uncertainty in @xmath53 will be decreased unless @xmath35 and @xmath53 are independent .",
    "[ [ information ] ] information : + + + + + + + + + + + +    the _ information in the outcome @xmath51 about @xmath53 is defined as @xmath364 here the quantities @xmath365 and @xmath366 on the right - hand side of the equations are always equal to or less than the corresponding quantities under the uniform distribution we analyzed first .",
    "the values of the quantities @xmath367 under the assumption of uniform distribution of @xmath53 and @xmath368 versus any other distribution are not related by inequality in a particular direction .",
    "the equalities @xmath349 and @xmath369 hold under any distribution of the variables . since @xmath367 is a function of outcomes of @xmath35 ,",
    "while @xmath370 is a function of outcomes of @xmath53 , we do not compare them directly .",
    "however , forming the expectation defined as @xmath371 and combining equations  [ i5 ] , [ i6 ] , we see that the resulting quantities are equal . denoting this quantity by @xmath372 and calling it",
    "the _ mutual information in @xmath35 and @xmath53 , we see that this information is _ symmetric _ : @xmath373 writing this out we find that the _ mutual information _ @xmath372 is defined by : @xmath374 another way to express this is as follows : a well - known criterion for the difference between a given distribution @xmath42 and a distribution @xmath375 it is compared with is the so - called _ kullback - leibler divergence",
    "_ @xmath376 it has the important property that @xmath377 with equality only iff @xmath378 for all @xmath9 .",
    "this is called the _ information inequality _ in @xcite , p. 26 .",
    "thus , the mutual information is the kullback - leibler divergence between the joint distribution and the product @xmath379 of the two marginal distributions .",
    "if this quantity is 0 then @xmath380 for every pair @xmath381 , which is the same as saying that @xmath35 and @xmath53 are independent random variables .",
    "_ _    [ ex : mutual ] suppose we want to exchange the information about the outcome @xmath51 and it is known already that outcome @xmath50 is the case , that is , @xmath9 has property @xmath103",
    ". then we require ( using the shannon - fano code ) about @xmath382 bits to communicate @xmath9 . on average , over the joint distribution @xmath383",
    "we use @xmath384 bits , which is optimal by shannon s noiseless coding theorem .",
    "in fact , exploiting the mutual information paradigm , the expected information @xmath385 that outcome @xmath50 gives about outcome @xmath51 is the same as the expected information that @xmath51 gives about @xmath50 , and is never negative .",
    "yet there may certainly exist _ individual _",
    "@xmath103 such that @xmath386 is negative .",
    "for example , we may have @xmath387 , @xmath388 , @xmath389 , @xmath390 , @xmath391 . then @xmath392 whereas @xmath393 . for small @xmath4 ,",
    "this quantity is smaller than @xmath90 .     + *",
    "problem and lacuna : * the quantity @xmath394 symmetrically characterizes to what extent random variables @xmath35 and @xmath53 are correlated .",
    "an inherent problem with probabilistic definitions is that  as we have just seen  although @xmath395 is always positive , for some probability distributions and some @xmath103 , @xmath370 can turn out to be negative  which definitely contradicts our naive notion of information content .",
    "the _ algorithmic _ mutual information we introduce below can _ never _ be negative , and in this sense is closer to the intuitive notion of information content .      for individual objects",
    "the information about one another is possibly even more fundamental than for random sources .",
    "kolmogorov @xcite :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ actually , it is most fruitful to discuss the quantity of information `` conveyed by an object '' ( @xmath9 ) `` about an object '' ( @xmath103 ) .",
    "it is not an accident that in the probabilistic approach this has led to a generalization to the case of continuous variables , for which the entropy is finite but , in a large number of cases , @xmath396 is finite .",
    "the real objects that we study are very ( infinitely ) complex , but the relationships between two separate objects diminish as the schemes used to describe them become simpler .",
    "while a map yields a considerable amount of information about a region of the earth s surface , the microstructure of the paper and the ink on the paper have no relation to the microstructure of the area shown on the map . ",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    in the discussions on shannon mutual information , we first needed to introduce a conditional version of entropy .",
    "analogously , to prepare for the definition of algorithmic mutual information , we need a notion of conditional kolmogorov complexity .    intuitively , the conditional prefix kolmogorov complexity @xmath397 of @xmath9 given @xmath103 can be interpreted as the shortest prefix program @xmath157 such that , when @xmath103 is given to the program @xmath157 as input , the program prints @xmath9 and then halts . the idea of providing @xmath157 with an input @xmath103 is realized by putting @xmath398 rather than just @xmath157 on the input tape of the universal prefix machine @xmath241 .",
    "[ def.kolmkb ] the _ conditional prefix kolmogorov complexity _ of @xmath9 given @xmath103 ( for free ) is @xmath399 we define @xmath400    note that we just redefined @xmath234 so that the unconditional kolmogorov complexity is _ exactly _ equal to the conditional kolmogorov complexity with empty input .",
    "this does not contradict our earlier definition : we can choose a reference prefix machine @xmath241 such that @xmath401 .",
    "then ( [ eq : redefine ] ) holds automatically .",
    "we now have the technical apparatus to express the relation between entropy inequalities and kolmogorov complexity inequalities .",
    "recall that the entropy expresses the expected information to transmit an outcome of a known random source , while the kolmogorov complexity of every such outcome expresses the specific information contained in that outcome .",
    "this makes us wonder to what extend the entropy-(in)equalities hold for the corresponding kolmogorov complexity situation . in the latter case the corresponding ( in)equality is a far stronger statement , implying the same ( in)equality in the entropy setting .",
    "it is remarkable , therefore , that similar inequalities hold for both cases , where the entropy ones hold exactly while the kolmogorov complexity ones hold up to a logarithmic , and in some cases @xmath20 , additive precision .    [ [ additivity ] ] additivity : + + + + + + + + + + +    by definition , @xmath402 .",
    "trivially , the symmetry property holds : @xmath403 .",
    "another interesting property is the `` additivity of complexity '' property that , as we explain further below , is equivalent to the `` symmetry of algorithmic mutual information '' property .",
    "recall that @xmath110 denotes the first ( in a standard enumeration order ) shortest prefix program that generates @xmath9 and then halts .",
    "[ thm : additive ] @xmath404",
    "this is the kolmogorov complexity equivalent of the entropy equality ( [ i5 ] ) . that this latter equality holds is true by simply rewriting both sides of the equation according to the definitions of averages of joint and marginal probabilities .",
    "in fact , potential individual differences are averaged out .",
    "but in the kolmogorov complexity case we do nothing like that : it is truly remarkable that additivity of algorithmic information holds for individual objects .",
    "it was first proven by kolmogorov and leonid a. levin for the plain ( non - prefix ) version of kolmogorov complexity , where it holds up to an additive logarithmic term , and reported in @xcite .",
    "the prefix - version ( [ eq.soi ] ) , holding up to an @xmath20 additive term is due to @xcite , can be found as theorem 3.9.1 in  @xcite , and has a difficult proof .",
    "[ [ symmetry ] ] symmetry : + + + + + + + + +    to define the algorithmic mutual information between two individual objects @xmath9 and @xmath103 with no probabilities involved , it is instructive to first recall the probabilistic notion ( [ eq.mutinfprob ] ) . rewriting ( [ eq.mutinfprob ] ) as @xmath405 , \\ ] ] and noting that @xmath406 is very close to the length of the prefix - free shannon - fano code for @xmath407 , we are led to the following definition . the _ information in @xmath103 about @xmath9 _ is defined as @xmath408 where the second equality is a consequence of  ( [ eq.soi ] ) and states that this information is symmetrical , @xmath409 , and therefore we can talk about _",
    "mutual information_. distinguishes it from the probabilistic ( average ) notion @xmath410 .",
    "we deviate slightly from  @xcite where @xmath411 is defined as @xmath412 . ]",
    "[ [ precision - o1-vs .- olog - n ] ] precision ",
    "@xmath20 vs. @xmath413 : + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the version of ( [ eq.soi ] ) with just @xmath9 and @xmath103 in the conditionals does nt hold with @xmath27 , but holds up to additive logarithmic terms that can not be eliminated . to gain some further insight in this matter , first consider the following lemma :    @xmath110 has the same information as the pair @xmath414 , that is , @xmath415 .    given @xmath414 we can run all programs simultaneously in dovetailed fashion and select the first program of length @xmath234 that halts with output @xmath9 as @xmath110 .",
    "( dovetailed fashion means that in phase @xmath416 of the process we run all programs @xmath167 for @xmath417 steps such that @xmath418 , @xmath419 )    thus , @xmath110 provides more information than @xmath9 .",
    "therefore , we have to be very careful when extending theorem  [ thm : additive ] . for example , the conditional version of ( [ eq.soi ] ) is : @xmath420 note that a naive version @xmath421 is incorrect : taking @xmath422 , @xmath423 , the left - hand side equals @xmath424 which can be as large as @xmath425 , and the right - hand side equals @xmath426 .",
    "but up to logarithmic precision we do not need to be that careful .",
    "in fact , it turns out that _ every _ linear entropy inequality holds for the corresponding kolmogorov complexities within a logarithmic additive error , @xcite :    all linear ( in)equalities that are valid for kolmogorov complexity are also valid for shannon entropy and vice versa  provided we require the kolmogorov complexity ( in)equalities to hold up to additive logarithmic precision only .",
    "theorem  [ theo.eq.entropy ] gave the relationship between entropy and ordinary kolmogorov complexity ; it showed that the entropy of distribution @xmath36 is approximately equal to the expected ( under @xmath36 ) kolmogorov complexity .",
    "theorem  [ thm : mutinf ] gives the analogous result for the mutual information ( to facilitate comparison to theorem  [ theo.eq.entropy ] , note that @xmath9 and @xmath103 in ( [ eq.eqamipmi ] ) below may stand for strings of arbitrary length @xmath58 ) .    [ thm : mutinf ] given a computable probability distribution @xmath46 over @xmath427 we have @xmath428    rewrite the expectation @xmath429.\\end{aligned}\\ ] ] define @xmath430 and @xmath431 to obtain @xmath432 given the program that computes @xmath31 , we can approximate @xmath351 by @xmath433 , and similarly for @xmath434",
    ". that is , the distributions @xmath435 ( @xmath436 ) are lower semicomputable .",
    "because they sum to 1 it can be shown they must also be computable . by theorem  [ theo.eq.entropy ]",
    ", we have @xmath437 for every computable probability mass function @xmath65 .    hence , @xmath438 ( @xmath436 ) , and @xmath439 .",
    "on the other hand , the probabilistic mutual information ( [ eq.mutinfprob ] ) is expressed in the entropies by @xmath440 . by construction of the @xmath435 s above , we have @xmath441 .",
    "since the complexities are positive , substitution establishes the lemma .",
    "can we get rid of the @xmath285 error term ?",
    "the answer is affirmative ; by putting @xmath442 in the conditional , and applying , we can even get rid of the computability requirement .    given a joint probability distribution @xmath46 over @xmath427 ( not necessarily computable ) we have @xmath443 where the auxiliary @xmath31 means that we can directly access the values @xmath46 on the auxiliary conditional information tape of the reference universal prefix machine .",
    "the lemma follows from the definition of conditional algorithmic mutual information , if we show that @xmath444 , where the @xmath20 term implicit in the @xmath27 sign is independent of @xmath31 .",
    "equip the reference universal prefix machine , with an @xmath20 length program to compute a shannon - fano code from the auxiliary table of probabilities .",
    "then , given an input @xmath211 , it can determine whether @xmath211 is the shannon - fano code word for some @xmath9 .",
    "such a code word has length @xmath445 .",
    "if this is the case , then the machine outputs @xmath9 , otherwise it halts without output . therefore ,",
    "this shows the upper bound on the expected prefix complexity .",
    "the lower bound follows as usual from the noiseless coding theorem .",
    "thus , we see that the expectation of the algorithmic mutual information @xmath447 is close to the probabilistic mutual information @xmath410  which is important : if this were not the case then the algorithmic notion would not be a sharpening of the probabilistic notion to individual objects , but something else .",
    "is it possible to increase the mutual information between two random variables , by processing the outcomes in some deterministic manner ?",
    "the answer is negative : for every function @xmath74 we have @xmath448 that is , mutual information between two random variables can not be increased by processing their outcomes in any deterministic way .",
    "the same holds in an appropriate sense for randomized processing of the outcomes of the random variables .",
    "this fact is called the _ data processing inequality _ @xcite , theorem 2.8.1 . the reason why it holds is that is expressed in terms of probabilities @xmath449 , rather than in terms of the arguments .",
    "processing the arguments @xmath450 will not increase the value of the expression in the right - hand side .",
    "if the processing of the arguments just renames them in a one - to - one manner then the expression keeps the same value .",
    "if the processing eliminates or merges arguments then it is easy to check from the formula that the expression value does nt increase .      in the algorithmic version of mutual information ,",
    "the notion is expressed in terms of the individual arguments instead of solely in terms of the probabilities as in the probabilistic version .",
    "therefore , the reason for to hold is not valid in the algorithmic case .",
    "yet it turns out that the data processing inequality also holds between individual objects , by far more subtle arguments and not precisely but with a small tolerance .",
    "the first to observe this fact was leonid a. levin who proved his `` information non - growth , '' and `` information conservation inequalities '' for both finite and infinite sequences under both deterministic and randomized data processing , @xcite .",
    "we first discuss some useful technical lemmas .",
    "the additivity of complexity ( symmetry of information ) can be used to derive a `` directed triangle inequality '' from @xcite , that is needed later .",
    "[ lem.magic ] for all @xmath451 , @xmath452    using  ( [ eq.soi ] ) , an evident inequality introducing an auxiliary object @xmath231 , and twice (  [ eq.soi ] ) again : @xmath453    this theorem has bizarre consequences .",
    "these consequences are not simple unexpected artifacts of our definitions , but , to the contrary , they show the power and the genuine contribution to our understanding represented by the deep and important mathematical relation ( [ eq.soi ] )",
    ".    denote @xmath454 and substitute @xmath455 and @xmath456 to find the following counterintuitive corollary : to determine the complexity of the complexity of an object @xmath103 it suffices to give both @xmath103 and the complexity of @xmath103 .",
    "this is counterintuitive since in general we can not compute the complexity of an object from the object itself ; if we could this would also solve the so - called `` halting problem '' , @xcite .",
    "this noncomputability can be quantified in terms of @xmath457 which can rise to almost @xmath458 for some @xmath103 .",
    "but in the seemingly similar , but subtly different , setting below it is possible .",
    "as above , let @xmath416 denote @xmath459 .",
    "then , @xmath460 .    now back to whether mutual information in one object about another one can not be increased . in the probabilistic setting this",
    "was shown to hold for random variables .",
    "but does it also hold for individual outcomes ?",
    "in @xcite it was shown that the information in one individual string about another can not be increased by any deterministic algorithmic method by more than a constant . with added randomization",
    "this holds with overwhelming probability . here ,",
    "we follow the proof method of @xcite and use the triangle inequality of theorem  [ lem.magic ] to recall , and to give proofs of this information non - increase .",
    "recall the definition  [ def.mutinf ] and theorem  [ eq.eqamipmi ] .",
    "we prove a strong version of the information non - increase law under deterministic processing ( later we need the attached corollary ) :    given @xmath9 and @xmath231 , let @xmath155 be a program computing @xmath231 from @xmath110 . then @xmath461    by the triangle inequality , @xmath462 thus , @xmath463    this also implies the slightly weaker but intuitively more appealing statement that the mutual information between strings @xmath9 and @xmath103 can not be increased by processing @xmath9 and @xmath103 separately by deterministic computations .",
    "let @xmath464 be recursive functions .",
    "then @xmath465    it suffices to prove the case @xmath466 and apply it twice .",
    "the proof is by replacing the program @xmath155 that computes a particular string @xmath231 from a particular @xmath110 in ( [ eq.nonincrease2 ] ) .",
    "there , @xmath155 possibly depends on @xmath110 and @xmath231 .",
    "replace it by a program @xmath467 that first computes @xmath9 from @xmath110 , followed by computing a recursive function @xmath31 , that is , @xmath467 is independent of @xmath9 .",
    "since we only require an @xmath20-length program to compute @xmath9 from @xmath110 we can choose @xmath468 .    by the triangle inequality , @xmath469 thus , @xmath470      it turns out that furthermore , randomized computation can increase information only with negligible probability . recall from section  [ sec : m ] that the _ universal probability _",
    "@xmath471 is maximal within a multiplicative constant among lower semicomputable semimeasures .",
    "so , in particular , for each computable measure @xmath42 we have @xmath472 , where the constant factor @xmath473 depends on @xmath31 .",
    "this property also holds when we have an extra parameter , like @xmath474 , in the condition .",
    "suppose that @xmath231 is obtained from @xmath9 by some randomized computation .",
    "we assume that the probability @xmath475 of obtaining @xmath231 from @xmath9 is a semicomputable distribution over the @xmath231 s",
    ". therefore it is upperbounded by @xmath476 .",
    "the information increase @xmath477 satisfies the theorem below .",
    "there is a constant @xmath478 such that for all @xmath451 we have @xmath479    for example , the probability of an increase of mutual information by the amount @xmath480 is @xmath481 .",
    "the theorem implies @xmath482 , the @xmath483-expectation of the exponential of the increase is bounded by a constant .",
    "we have @xmath484 the negative logarithm of the left - hand side in the theorem is therefore @xmath485 using theorem  [ lem.magic ] , and the conditional additivity ( [ eq.soi-cond ] ) , this is @xmath486    an example of the use of algorithmic mutual information is as follows @xcite . a celebrated result of k. gdel states that peano arithmetic is incomplete in the sense that it can not be consistently extended to a complete theory using recursively enumerable axiom sets .",
    "( here ` complete ' means that every sentence of peano arithmetic is decidable within the theory ; for further details on the terminology used in this example , we refer to @xcite ) .",
    "the essence is the non - existence of total recursive extensions of a universal partial recursive predicate .",
    "this is usually taken to mean that mathematics is undecidable .",
    "non - existence of an algorithmic solution need not be a problem when the requirements do not imply unique solutions .",
    "a perfect example is the generation of strings of high kolmogorov complexity , say of half the length of the strings .",
    "there is no deterministic effective process that can produce such a string ; but repeatedly flipping a fair coin we generate a desired string with overwhelming probability . therefore , the question arises whether randomized means allow us to bypass gdel s result .",
    "the notion of mutual information between two finite strings can be refined and extended to infinite sequences , so that , again , it can not be increased by either deterministic or randomized processing . in @xcite the existence of an infinite sequence",
    "is shown that has infinite mutual information with all total extensions of a universal partial recursive predicate . as levin states `` it plays the role of password : no substantial information about it",
    "can be guessed , no matter what methods are allowed . ''",
    "this `` forbidden information '' is used to extend the gdel s incompleteness result to also hold for consistent extensions to a complete theory by randomized means with non - vanishing probability .",
    "+    [ [ problem - and - lacuna-1 ] ] problem and lacuna : + + + + + + + + + + + + + + + + + + +    entropy , kolmogorov complexity and mutual ( algorithmic ) information are concepts that do not distinguish between different _ kinds _ of information ( such as ` meaningful ' and ` meaningless ' information ) . in the remainder of this paper , we show how these more intricate notions can be arrived at , typically by _ constraining _ the description methods with which strings are allowed to be encoded ( section  [ sec : algsuf ] ) and by considering _",
    "lossy _ rather than lossless compression ( section  [ sect.rdsf ] ) .",
    "nevertheless , the basic notions entropy , kolmogorov complexity and mutual information continue to play a fundamental rle .",
    "in introducing the notion of sufficiency in classical statistics , fisher  @xcite stated :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ `` the statistic chosen should summarize the whole of the relevant information supplied by the sample .",
    "this may be called the criterion of sufficiency @xmath225 in the case of the normal curve of distribution it is evident that the second moment is a sufficient statistic for estimating the standard deviation . ''",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    a `` sufficient '' statistic of the data contains all information in the data about the model class . below",
    "we first discuss the standard notion of ( probabilistic ) sufficient statistic as employed in the statistical literature .",
    "we show that this notion has a natural interpretation in terms of shannon mutual information , so that we may just as well think of a probabilistic sufficient statistic as a concept in shannon information theory . just as in the other sections of this paper ,",
    "there is a corresponding notion in the kolmogorov complexity literature : the algorithmic sufficient statistic which we introduce in section  [ sec : algsuf ] . finally , in section",
    "[ sec : relpa ] we connect the statistical / shannon and the algorithmic notions of sufficiency .",
    "let @xmath487 be a family of distributions , also called a _ model class _ , of a random variable @xmath35 that takes values in a finite or countable _ set of data _ @xmath2 .",
    "let @xmath488 be the set of parameters @xmath298 parameterizing the family @xmath487 .",
    "any function @xmath489 taking values in some set @xmath490 is said to be a _ statistic _ of the data in @xmath2 .",
    "a _ statistic _",
    "@xmath280 is said to be _ sufficient _ for the family @xmath491 if , for every @xmath492 , the conditional distribution @xmath493 is invariant under changes of @xmath298 .",
    "this is the standard definition in the statistical literature , see for example @xcite .",
    "intuitively , ( [ eq : cond ] ) means that all information about @xmath298 in the observation @xmath9 is present in the ( coarser ) observation @xmath494 , in line with fisher s quote above .",
    "the notion of ` sufficient statistic ' can be equivalently expressed in terms of probability mass functions .",
    "let @xmath495 denote the probability mass of @xmath9 according to @xmath496 .",
    "we identify distributions @xmath496 with their mass functions @xmath497 and denote the model class @xmath498 by @xmath499 .",
    "let @xmath500 denote the probability mass function of the conditional distribution ( [ eq : cond ] ) , defined as in section  [ sec : preliminaries ] .",
    "that is , @xmath501 the requirement of @xmath280 to be sufficient is equivalent to the existence of a function @xmath502 such that @xmath503 for every @xmath504 , @xmath492 , @xmath174 .",
    "( here we change the common notation ` @xmath505 ' to ` @xmath506 ' which is more expressive for our purpose . )",
    "let @xmath507 , let @xmath508 .",
    "let @xmath509 be the set of @xmath58-fold bernoulli distributions on @xmath2 with parameter @xmath298 .",
    "that is , @xmath510 where @xmath494 is the number of @xmath137 s in @xmath9 .",
    "then @xmath494 is a sufficient statistic for @xmath511 .",
    "namely , fix an arbitrary @xmath496 with @xmath512 and an arbitrary @xmath407 with @xmath513 .",
    "then all @xmath9 s with @xmath407 ones and @xmath514 zeroes are equally probable .",
    "the number of such @xmath9 s is @xmath515 .",
    "therefore , the probability @xmath516 is equal to @xmath517 , and this does not depend on the parameter @xmath298 .",
    "equivalently , for all @xmath512 , @xmath518 since ( [ eq : berndef ] ) satisfies ( [ eq : standarddef ] ) ( with @xmath519 the uniform distribution on all @xmath9 with exactly @xmath407 ones ) , @xmath494 is a sufficient statistic relative to the model class @xmath520 . in the bernoulli case",
    ", @xmath519 can be obtained by starting from the _ uniform _ distribution on @xmath2 ( @xmath521 ) , and conditioning on @xmath522 .",
    "but @xmath65 is not necessarily uniform .",
    "for example , for the poisson model class , where @xmath499 represents the set of poisson distributions on @xmath58 observations , the observed mean is a sufficient statistic and the corresponding @xmath65 is far from uniform .",
    "all information about the parameter @xmath298 in the observation @xmath9 is already contained in @xmath494 . in the bernoulli case , once we know the number @xmath494 of @xmath137 s in @xmath9 , all further details of @xmath9 ( such as the order of @xmath90s and @xmath137s ) are irrelevant for determination of the bernoulli parameter @xmath298 .    to give an example of a statistics that is not sufficient for the bernoulli model class , consider the statistic @xmath523 which counts the number of 1s in @xmath9 that are followed by a @xmath137 . on the other hand , for every statistic @xmath241 , the combined statistic @xmath524 with @xmath494 as before , is sufficient , since it contains all information in @xmath494 .",
    "but in contrast to @xmath494 , a statistic such as @xmath525 is typically not _",
    "minimal _ , as explained further below",
    ".    it will be useful to rewrite ( [ eq : standarddef ] ) as @xmath526    [ def.wss ] a function @xmath489 is a _",
    "probabilistic sufficient statistic _ for @xmath499 if there exists a function @xmath527 such that ( [ eq : indivdef ] ) holds for every @xmath504 , every @xmath174 , every @xmath528 ( here we use the convention @xmath529 ) .    [ [ expectation - version - of - definition ] ] expectation - version of definition : + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the standard definition of probabilistic sufficient statistics is ostensibly of the ` individual - sequence'-type : for @xmath280 to be sufficient , ( [ eq : indivdef ] ) has to hold for _ every _",
    "@xmath9 , rather than merely in expectation or with high probability . however , the definition turns out to be equivalent to an expectation - oriented form , as shown in proposition  [ prop : suff ] .",
    "we first introduce an a priori distribution over @xmath530 , the parameter set for our model class @xmath531 .",
    "we denote the probability density of this distribution by @xmath532 .",
    "this way we can define a joint distribution @xmath533 .",
    "[ prop : suff ] the following two statements are equivalent to definition  [ def.wss ] : ( 1 ) for every @xmath534 , @xmath535 ( 2 ) for _ every _ prior @xmath536 on @xmath488 , @xmath537    _ definition  [ def.wss ] @xmath538 : _ suppose ( [ eq : indivdef ] ) holds for every @xmath539 , every @xmath174 , every @xmath492 .",
    "then it also holds in expectation for every @xmath540 : @xmath541.\\ ] ]    _ @xmath538 definition  [ def.wss ] : _ suppose that for every @xmath504 , ( [ eq : propsuff1 ] ) holds .",
    "denote @xmath542 by adding @xmath543 to both sides of the equation , ( [ eq : propsuff1 ] ) can be rewritten as @xmath544 with @xmath545.$ ] by the information inequality , the equality ( [ eq : suffent ] ) can only hold if @xmath546 for every @xmath174 .",
    "hence , we have established .",
    "_ @xmath547 : _ follows by linearity of expectation .",
    "[ [ mutual - information - version - of - definition ] ] mutual information - version of definition : + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    after some rearranging of terms , the characterization ( [ eq : expdef2 ] ) gives rise to the intuitively appealing definition of probabilistic sufficient statistic in terms of mutual information .",
    "the resulting formulation of sufficiency is as follows @xcite : @xmath280 is sufficient for @xmath548 iff for all priors @xmath532 on @xmath488 : @xmath549 for all distributions of @xmath298 .",
    "thus , a statistic @xmath494 is sufficient if the probabilistic mutual information is invariant under taking the statistic .",
    "[ [ minimal - probabilistic - sufficient - statistic ] ] minimal probabilistic sufficient statistic : + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    a sufficient statistic may contain information that is not relevant : for a normal distribution the sample mean is a sufficient statistic , but the pair of functions which give the mean of the even - numbered samples and the odd - numbered samples respectively , is also a sufficient statistic . a statistic @xmath494 is a _ minimal _ sufficient statistic with respect to an indexed model class @xmath499 , if it is a function of all other sufficient statistics : it contains no irrelevant information and maximally compresses the information in the data about the model class .",
    "for the family of normal distributions the sample mean is a minimal sufficient statistic , but the sufficient statistic consisting of the mean of the even samples in combination with the mean of the odd samples is not minimal .",
    "note that one can not improve on sufficiency : the data processing inequality states that @xmath550 for every function @xmath280 , and that for randomized functions @xmath280 an appropriate related expression holds .",
    "that is , mutual information between data random variable and model random variable can not be increased by processing the data sample in any way .",
    "[ [ problem - and - lacuna-2 ] ] problem and lacuna : + + + + + + + + + + + + + + + + + + +    we can think of the probabilistic sufficient statistic as extracting those patterns in the data that are relevant in determining the parameters of a statistical model class .",
    "but what if we do not want to commit ourselves to a simple finite - dimensional parametric model class ? in the most general context , we may consider the model class of all computable distributions , or all computable sets of which the observed data is an element .",
    "does there exist an analogue of the sufficient statistic that automatically summarizes _ all _ information in the sample @xmath9 that is relevant for determining the `` best '' ( appropriately defined ) model for @xmath9 within this enormous class of models ?",
    "of course , we may consider the literal data @xmath9 as a statistic of @xmath9 , but that would not be satisfactory : we would still like our generalized statistic , at least in many cases , to be considerably coarser , and much more concise , than the data @xmath9 itself .",
    "it turns out that , to some extent , this is achieved by the _",
    "_ sufficient statistic of the data : it summarizes _ all _ conceivably relevant information in the data @xmath9 ; at the same time , many types of data @xmath9 admit an algorithmic sufficient statistic that is concise in the sense that it has very small kolmogorov complexity .        [ sect.meaning ]",
    "the information contained in an individual finite object ( like a finite binary string ) is measured by its kolmogorov complexity  the length of the shortest binary program that computes the object .",
    "such a shortest program contains no redundancy : every bit is information ; but is it meaningful information ? if we flip a fair coin to obtain a finite binary string , then with overwhelming probability that string constitutes its own shortest program .",
    "however , also with overwhelming probability all the bits in the string are meaningless information , random noise . on the other hand ,",
    "let an object @xmath9 be a sequence of observations of heavenly bodies .",
    "then @xmath9 can be described by the binary string @xmath551 , where @xmath157 is the description of the laws of gravity and the observational parameter setting , while @xmath480 accounts for the measurement errors : we can divide the information in @xmath9 into meaningful information @xmath157 and accidental information @xmath480 .",
    "the main task for statistical inference and learning theory is to distill the meaningful information present in the data .",
    "the question arises whether it is possible to separate meaningful information from accidental information , and if so , how .",
    "the essence of the solution to this problem is revealed when we write definition  [ def.kolmk ] as follows : @xmath552 where the minimum is taken over @xmath553 and @xmath554 .",
    "the justification is that for the fixed reference universal prefix turing machine @xmath555 for all @xmath167 and @xmath157 . since @xmath556 denotes the shortest self - delimiting program for @xmath167 , we have @xmath557 .",
    "the expression emphasizes the two - part code nature of kolmogorov complexity . in a randomly truncated initial segment of a time series",
    "@xmath558 we can encode @xmath9 by a small turing machine printing a specified number of copies of the pattern `` 01 . '' this way , @xmath234 is viewed as the shortest length of a two - part code for @xmath9 , one part describing a turing machine @xmath74 , or _ model _",
    ", for the _ regular _ aspects of @xmath9 , and the second part describing the _ irregular _ aspects of @xmath9 in the form of a program @xmath157 to be interpreted by @xmath74 .",
    "the regular , or `` valuable , '' information in @xmath9 is constituted by the bits in the `` model '' while the random or `` useless '' information of @xmath9 constitutes the remainder .",
    "this leaves open the crucial question : how to choose @xmath74 and @xmath157 that together describe @xmath9 ? in general , many combinations of @xmath74 and @xmath157 are possible , but we want to find a @xmath74 that describes the meaningful aspects of @xmath9 .",
    "we consider only finite binary data strings @xmath9 .",
    "our model class consists of turing machines @xmath74 that enumerate a finite set , say @xmath280 , such that on input @xmath559 we have @xmath560 with @xmath9 the @xmath157th element of @xmath74 s enumeration of @xmath280 , and @xmath561 is a special _ undefined _ value if @xmath562 . the `` best fitting '' model for @xmath9 is a turing machine @xmath74 that reaches the minimum description length in ( [ eq.kcmdl ] ) .",
    "there may be many such @xmath74 , but , as we will see , if chosen properly , such a machine @xmath74 embodies the amount of useful information contained in @xmath9 .",
    "thus , we have divided a shortest program @xmath110 for @xmath9 into parts",
    "@xmath563 such that @xmath564 is a shortest self - delimiting program for @xmath74 .",
    "now suppose we consider only low complexity finite - set models , and under these constraints the shortest two - part description happens to be longer than the shortest one - part description .",
    "for example , this can happen if the data is generated by a model that is too complex to be in the contemplated model class .",
    "does the model minimizing the two - part description still capture all ( or as much as possible ) meaningful information ?",
    "such considerations require study of the relation between the complexity limit on the contemplated model classes , the shortest two - part code length , and the amount of meaningful information captured .    in the following we will distinguish between `` models '' that are finite sets , and the `` shortest programs '' to compute those models that are finite strings .",
    "the latter will be called ` algorithmic statistics ' . in a way",
    "the distinction between `` model '' and `` statistic '' is artificial , but for now we prefer clarity and unambiguousness in the discussion .",
    "moreover , the terminology is customary in the literature on algorithmic statistics .",
    "note that strictly speaking , neither an algorithmic statistic nor the set it defines is a statistic in the probabilistic sense : the latter was defined as a _ function _ on the set of possible data samples of given length .",
    "both notions are unified in section  [ sec : relpa ] .",
    "consider a string @xmath9 of length @xmath58 and prefix complexity @xmath565 .",
    "for every finite set @xmath566 containing @xmath9 we have @xmath567 . indeed , consider the prefix code of @xmath9 consisting of its @xmath568 bit long index of @xmath9 in the lexicographical ordering of @xmath280 .",
    "this code is called _ data - to - model code_. we identify the _ structure _ or _ regularity _ in @xmath9 that are to be summarized with a set @xmath280 of which @xmath9 is a _ random _ or _ typical _ member : given @xmath280 containing @xmath9 , the element @xmath9 can not be described significantly shorter than by its maximal length index in @xmath280 , that is , @xmath569 .",
    "let @xmath570 be an agreed upon , fixed , constant .",
    "a finite binary string @xmath9 is a _ typical _ or _ random _ element of a set @xmath280 of finite binary strings , if @xmath571 and @xmath572 we will not indicate the dependence on @xmath573 explicitly , but the constants in all our inequalities ( @xmath20 ) will be allowed to be functions of this @xmath573 .",
    "this definition requires a finite @xmath280 .",
    "in fact , since @xmath574 , it limits the size of @xmath280 to @xmath575 .",
    "note that the notion of typicality is not absolute but depends on fixing the constant implicit in the @xmath576-notation .",
    "[ xmp.typical ] consider the set @xmath280 of binary strings of length @xmath58 whose every odd position is 0 .",
    "let @xmath9 be an element of this set in which the subsequence of bits in even positions is an incompressible string .",
    "then @xmath9 is a typical element of @xmath280 ( or by with some abuse of language we can say @xmath280 is typical for @xmath9 ) .",
    "but @xmath9 is also a typical element of the set @xmath40 .",
    "let @xmath9 be a binary data string of length @xmath58 .",
    "for every finite set @xmath577 , we have @xmath578 , since we can describe @xmath9 by giving @xmath280 and the index of @xmath9 in a standard enumeration of @xmath280 .",
    "clearly this can be implemented by a turing machine computing the finite set @xmath280 and a program @xmath157 giving the index of @xmath9 in @xmath280 .",
    "the size of a set containing @xmath9 measures intuitively the number of properties of @xmath9 that are represented : the largest set is @xmath579 and represents only one property of @xmath9 , namely , being of length @xmath58 .",
    "it clearly `` underfits '' as explanation or model for @xmath9 .",
    "the smallest set containing @xmath9 is the singleton set @xmath40 and represents all conceivable properties of @xmath9 .",
    "it clearly `` overfits '' as explanation or model for @xmath9 .",
    "there are two natural measures of suitability of such a set as a model for @xmath9 .",
    "we might prefer either the simplest set , or the smallest set , as corresponding to the most likely structure ` explaining ' @xmath9 .",
    "both the largest set @xmath57 ( having low complexity of about @xmath580 ) and the singleton set @xmath40 ( having high complexity of about @xmath234 ) , while certainly statistics for @xmath9 , would indeed be considered poor explanations .",
    "we would like to balance simplicity of model versus size of model .",
    "both measures relate to the optimality of a two - stage description of @xmath9 using a finite set @xmath280 that contains it . elaborating on the two - part code : @xmath581 where only the final substitution of @xmath284 by @xmath582 uses the fact that @xmath9 is an element of @xmath280 .",
    "the closer the right - hand side of gets to the left - hand side , the better the description of @xmath9 is in terms of the set @xmath280 .",
    "this implies a trade - off between meaningful model information , @xmath281 , and meaningless `` noise '' @xmath583 .",
    "a set @xmath280 ( containing @xmath9 ) for which holds with equality @xmath584 is called _",
    "optimal_. a data string @xmath9 can be typical for a set @xmath280 without that set @xmath280 being optimal for @xmath9 .",
    "this is the case precisely when @xmath9 is typical for @xmath280 ( that is @xmath585 ) while @xmath586 .",
    "intuitively , a model expresses the essence of the data if the two - part code describing the data consisting of the model and the data - to - model code is as concise as the best one - part description .",
    "mindful of our distinction between a finite set @xmath280 and a program that describes @xmath280 in a required representation format , we call a shortest program for an optimal set with respect to @xmath9 an _ algorithmic sufficient statistic _ for @xmath9 .",
    "furthermore , among optimal sets , there is a direct trade - off between complexity and log - size , which together sum to @xmath587 .",
    "[ xmp.optimal ] it can be shown that the set @xmath280 of example  [ xmp.typical ] is also optimal , and so is @xmath40 .",
    "sets for which @xmath9 is typical form a much wider class than optimal sets for @xmath9 : the set @xmath588 is still typical for @xmath9 but with most @xmath103 , it will be too complex to be optimal for @xmath9 .    for a perhaps less artificial example , consider complexities conditional on the length @xmath58 of strings .",
    "let @xmath103 be a random string of length @xmath58 , let @xmath589 be the set of strings of length @xmath58 which have 0 s exactly where @xmath103 has , and let @xmath9 be a random element of @xmath589",
    ". then @xmath9 has about 25% 1 s , so its complexity is much less than @xmath58 .",
    "the set @xmath589 has @xmath9 as a typical element , but is too complex to be optimal , since its complexity ( even conditional on @xmath58 ) is still @xmath58 .",
    "an algorithmic sufficient statistic is a sharper individual notion than a probabilistic sufficient statistic .",
    "an optimal set @xmath280 associated with @xmath9 ( the shortest program computing @xmath280 is the corresponding sufficient statistic associated with @xmath9 ) is chosen such that @xmath9 is maximally random with respect to it .",
    "that is , the information in @xmath9 is divided in a relevant structure expressed by the set @xmath280 , and the remaining randomness with respect to that structure , expressed by @xmath9 s index in @xmath280 of @xmath583 bits .",
    "the shortest program for @xmath280 is itself alone an algorithmic definition of structure , without a probabilistic interpretation .",
    "those optimal sets that admit the shortest possible program are called _ algorithmic minimal sufficient statistics _ of @xmath9 .",
    "they will play a major role in the next section on the kolmogorov structure function . summarizing :    [ def : algsufstat ]",
    "an _ algorithmic sufficient statistic _ of @xmath9 is a shortest program for a set @xmath280 containing @xmath9 that is optimal , i.e. it satisfies ( [ eq.optim ] ) .",
    "an algorithmic sufficient statistic with optimal set @xmath280 is _ minimal _ if there exists no optimal set @xmath590 with @xmath591 .",
    "let @xmath416 be a number in the range @xmath592 of complexity @xmath593 given @xmath58 and let @xmath9 be a string of length @xmath58 having @xmath416 ones of complexity @xmath594 given @xmath595 .",
    "this @xmath9 can be viewed as a typical result of tossing a coin with a bias about @xmath596 .",
    "a two - part description of @xmath9 is given by the number @xmath416 of 1 s in @xmath9 first , followed by the index @xmath597 of @xmath9 in the set @xmath280 of strings of length @xmath58 with @xmath416 1 s .",
    "this set is optimal , since @xmath598 .",
    "note that @xmath280 encodes the number of @xmath137s in @xmath9 .",
    "the shortest program for @xmath280 is an algorithmic minimal sufficient statistic for _ most _ @xmath9 of length @xmath58 with @xmath416 @xmath137 s , since only a fraction of at most @xmath599 @xmath9 s of length @xmath58 with @xmath416 @xmath137s can have @xmath600 ( section  [ sec : kolmogorov ] ) .",
    "but of course there exist @xmath9 s with @xmath416 ones which have much more regularity .",
    "an example is the string starting with @xmath416 @xmath137 s followed by @xmath601 @xmath90 s . for such strings , @xmath280 is still optimal and the shortest program for @xmath280 is still an algorithmic sufficient statistic , but not a minimal one .",
    "we want to relate ` algorithmic sufficient statistics ' ( defined independently of any model class @xmath602 ) to probabilistic sufficient statistics ( defined relative to some model class @xmath602 as in section  [ sec : probstat ] ) .",
    "we will show that , essentially , algorithmic sufficient statistics are probabilistic nearly - sufficient statistics with respect to _ all _ model families @xmath603 . since the notion of algorithmic sufficiency is only defined to within additive constants",
    ", we can not expect algorithmic sufficient statistics to satisfy the requirements ( [ eq : indivdef ] ) or ( [ eq : expdef ] ) for probabilistic sufficiency _ exactly _ , but only ` nearly ' . ] ' .",
    "[ [ nearly - sufficient - statistics ] ] nearly sufficient statistics : + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    intuitively , we may consider a probabilistic statistic @xmath280 to be nearly sufficient if ( [ eq : indivdef ] ) or ( [ eq : expdef ] ) holds to within some constant . for long sequences @xmath9 , this constant will then be negligible compared to the two terms in ( [ eq : indivdef ] ) or ( [ eq : expdef ] ) which , for most practically interesting statistical model classes , typically grow linearly in the sequence length .",
    "but now we encounter a difficulty :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ whereas ( [ eq : indivdef ] ) and ( [ eq : expdef ] ) are equivalent if they are required to hold exactly , they express something substantially different if they are only required to hold within a constant .",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    because of our observation above , when relating probabilistic and algorithmic statistics we have to be very careful about what happens if @xmath58 is allowed to change .",
    "thus , we need to extend probabilistic and algorithmic statistics to strings of arbitrary length .",
    "this leads to the following generalized definition of a statistic :    [ def : seqstat ] a _ sequential statistic _ is a function @xmath604 , such that for all @xmath58 , all @xmath69 , ( 1 ) @xmath605 , and ( 2 ) @xmath606 , and ( 3 ) for all @xmath58 , the set @xmath607 is a partition of @xmath57 .",
    "algorithmic statistics are defined relative to individual @xmath9 of some length @xmath58 .",
    "probabilistic statistics are defined as functions , hence for all @xmath9 of given length , but still relative to given length @xmath58 .",
    "such algorithmic and probabilistic statistics can be extended to each @xmath58 and each @xmath69 in a variety of ways ; the three conditions in definition  [ def : seqstat ] ensure that the extension is done in a reasonable way .",
    "now let @xmath548 be a model class of sequential information sources ( section  [ sec : preliminaries ] ) , i.e. a statistical model class defined for sequences of arbitrary length rather than just fixed @xmath58 .",
    "as before , @xmath608 denotes the marginal distribution of @xmath609 on @xmath57 .",
    "[ def : nearsuff ] we call sequential statistic @xmath280 _ nearly - sufficient for @xmath602 in the probabilistic - individual sense _ if there exist functions @xmath610 and a constant @xmath19 such that for all @xmath298 , all @xmath58 , every @xmath69 , @xmath611 \\biggr| \\leq c.\\ ] ] we say @xmath280 is _ nearly - sufficient for @xmath602 in the probabilistic - expectation sense _ if there exists functions @xmath610 and a constant @xmath612 such that for all @xmath298 , all @xmath58 , @xmath613\\ ; \\biggr| \\leq c'.\\ ] ]    inequality may be read as ` ( [ eq : indivdef ] ) holds within a constant ' , whereas ( [ eq : nexpdef ] ) may be read as ` ( [ eq : expdef ] ) holds within a constant ' .",
    "whereas the individual - sequence definition ( [ eq : indivdef ] ) and the expectation - definition ( [ eq : expdef ] ) are equivalent if we require exact equality , they become quite different if we allow equality to within a constant as in definition  [ def : nearsuff ] . to see this ,",
    "let @xmath280 be some sequential statistic such that for all large @xmath58 , for some @xmath614 , for some @xmath69 , @xmath615 while for all @xmath616 of length @xmath58 , @xmath617 .",
    "if @xmath9 has very small but nonzero probability according to some @xmath618 , then with very small @xmath497-probability , the difference between the left - hand and right - hand side of ( [ eq : indivdef ] ) is very large , and with large @xmath497-probability , the difference between the left - hand and right - hand side of ( [ eq : indivdef ] ) is about @xmath90",
    ". then @xmath280 will be nearly sufficient in expectation , but not in the individual sense .    in the theorem below we focus on probabilistic statistics that are `",
    "nearly sufficient in an expected sense ' .",
    "we connect these to algorithmic sequential statistics , defined as follows :    a sequential statistic @xmath280 is _ sufficient in the algorithmic sense _ if there is a constant @xmath19 such that for all @xmath58 , all @xmath69 , the program generating @xmath494 is an algorithmic sufficient statistic for @xmath9 ( relative to constant @xmath19 ) , i.e. @xmath619    in theorem  [ thm : wiske ] we relate algorithmic to probabilistic sufficiency . in the theorem",
    ", @xmath280 represents a sequential statistic , @xmath499 is a model class of sequential information sources and @xmath67 is the conditional probability mass function arising from the uniform distribution : @xmath620    [ thm : wiske ] let @xmath280 be a sequential statistic that is sufficient in the algorithmic sense .",
    "then for every @xmath298 with @xmath621 , there exists a constant @xmath19 , such that for all @xmath58 , inequality holds with @xmath67 the uniform distribution .",
    "thus , if @xmath622 , then @xmath280 is a nearly - sufficient statistic for @xmath623 in the probabilistic - expectation sense , with @xmath65 equal to the uniform distribution .    the definition of algorithmic sufficiency , ( [ eq : seqsuf ] ) directly implies that there exists a constant @xmath19 such that for all @xmath298 , all @xmath58 , @xmath624 now fix any @xmath298 with @xmath621 .",
    "it follows ( by the same reasoning as in theorem  [ theo.eq.entropy ] ) that for some @xmath625 , for all @xmath58 , @xmath626 essentially , the left inequality follows by the information inequality ( [ eq.ii ] ) : no code can be more efficient in expectation under @xmath609 than the shannon - fano code with lengths @xmath627 ; the right inequality follows because , since @xmath621 , the shannon - fano code can be implemented by a computer program with a fixed - size independent of @xmath58 . by ( [ eq : dochter ] ) ,",
    "( [ eq : thmalg ] ) becomes : for all @xmath58 , @xmath628 for @xmath629 , we use the notation @xmath630 according to .",
    "note that , by requirement ( 3 ) in the definition of sequential statistic , @xmath631 whence @xmath630 is a probability mass function on @xmath490 , the set of values the statistic @xmath280 can take on sequences of length @xmath58 .",
    "thus , we get , once again by the information inequality ( [ eq.ii ] ) , @xmath632 now note that for all @xmath58 , @xmath633 = \\sum_{x \\in \\{0,1\\}^n } f^{(n)}_{\\theta}(x ) \\log 1/ f_{\\theta}(x).\\ ] ] consider the two - part code which encodes @xmath9 by first encoding @xmath494 using @xmath634 bits , and then encoding @xmath9 using @xmath635 bits . by the information inequality , ( [ eq.ii ] ) , this code must be less efficient than the shannon - fano code with lengths @xmath636 , so that if follows from ( [ eq : extra ] ) that , for all @xmath58 , @xmath637 now defining @xmath638 we find that ( [ eq : thmalgb ] ) , ( [ eq : extra ] ) , ( [ eq : zoon ] ) and ( [ eq : kind ] ) express , respectively , that @xmath639 , @xmath640 , @xmath641 , @xmath642 .",
    "it follows that @xmath643 , so that ( [ eq : kind ] ) must actually hold with equality up to a constant .",
    "that is , there exist a @xmath612 such that for all @xmath58 , @xmath644 the result now follows upon noting that ( [ eq : kindb ] ) is just ( [ eq : nexpdef ] ) with @xmath67 the uniform distribution .",
    "we continue the discussion about meaningful information of section  [ sect.meaning ] .",
    "this time we a priori restrict the number of bits allowed for conveying the essence of the information . in the probabilistic situation",
    "this takes the form of allowing only a `` rate '' of @xmath645 bits to communicate as well as possible , on average , the outcome of a random variable @xmath35 , while the set @xmath2 of outcomes has cardinality possibly exceeding @xmath646 .",
    "clearly , not all outcomes can be communicated without information loss , the average of which is expressed by the `` distortion '' .",
    "this leads to the so - called `` rate  distortion '' theory . in the algorithmic setting the corresponding idea",
    "is to consider a set of models from which to choose a single model that expresses the `` meaning '' of the given individual data @xmath9 as well as possible . if we allow only @xmath645 bits to express the model , while possibly the kolmogorov complexity @xmath647 , we suffer information loss",
    " a situation that arises for example with `` lossy '' compression . in the latter situation ,",
    "the data can not be perfectly reconstructed from the model , and the question arises in how far the model can capture the meaning present in the specific data @xmath9 .",
    "this leads to the so - called `` structure function '' theory .",
    "the limit of @xmath645 bits to express a model to capture the most meaningful information in the data is an individual version of the average notion of `` rate '' .",
    "the remaining less meaningful information in the data is the individual version of the average - case notion of `` distortion '' .",
    "if the @xmath645 bits are sufficient to express all meaning in the data then the resulting model is called a `` sufficient statistic '' , in the sense introduced above .",
    "the remaining information in the data is then purely accidental , random , noise .",
    "for example , a sequence of outcomes of @xmath58 tosses of a coin with computable bias @xmath157 , typically has a sufficient statistic of @xmath648 bits , while the remaining random information is typically at least about @xmath649 bits ( up to an @xmath650 additive term ) .      initially , shannon @xcite introduced rate - distortion as follows : `` practically , we are not interested in exact transmission when we have a continuous source , but only in transmission to within a given tolerance .",
    "the question is , can we assign a definite rate to a continuous source when we require only a certain fidelity of recovery , measured in a suitable way . ''",
    "later , in @xcite he applied this idea to lossy data compression of discrete memoryless sources  our topic below .",
    "as before , we consider a situation in which sender @xmath331 wants to communicate the outcome of random variable @xmath35 to receiver @xmath651 .",
    "let @xmath35 take values in some set @xmath2 , and the distribution @xmath36 of @xmath35 be known to both @xmath331 and @xmath651 .",
    "the change is that now @xmath331 is only allowed to use a finite number , say @xmath645 bits , to communicate , so that @xmath331 can only send @xmath646 different messages .",
    "let us denote by @xmath652 the encoding function used by @xmath331 .",
    "this @xmath652 maps @xmath2 onto some set @xmath185 .",
    "we require that @xmath653 .",
    "if @xmath654 or if @xmath2 is continuous - valued , then necessarily some information is lost during the communication .",
    "there is no decoding function @xmath655 such that @xmath656 for all @xmath9 .",
    "thus , @xmath331 and @xmath651 can not ensure that @xmath9 can always be reconstructed .",
    "as the next best thing , they may agree on a code such that for all @xmath9 , the value @xmath657 contains as much useful information about @xmath9 as is possible  what exactly ` useful ' means depends on the situation at hand ; examples are provided below .",
    "an easy example would be that @xmath657 is a finite list of elements , one of which is @xmath9 .",
    "we assume that the ` goodness ' of @xmath657 is gaged by a _ distortion function _",
    "@xmath658 $ ]",
    ". this distortion function may be any nonnegative function that is appropriate to the situation at hand . in the example above it could be the logarithm of the number of elements in the list @xmath657",
    ". examples of some common distortion functions are the hamming distance and the squared euclidean distance .",
    "we can view @xmath53 as a a random variable on the space @xmath185 , a coarse version of the random variable @xmath35 , defined as taking value @xmath50 if @xmath51 with @xmath659 .",
    "write @xmath660 and @xmath661 .",
    "once the distortion function @xmath480 is fixed , we define the _ expected _ distortion by @xmath662 & = \\sum_{x \\in { \\cal x } } f(x ) d(x,{y}(x ) ) \\\\ \\nonumber & = \\sum_{y \\in { \\cal y } } g(y ) \\sum_{x : y(x)=y } f(x)/g(y ) d(x , y ) .\\end{aligned}\\ ] ] if @xmath35 is a continuous random variable , the sum should be replaced by an integral .",
    "[ ex : classicrd ] in most standard applications of rate distortion theory , the goal is to compress @xmath9 in a ` lossy ' way , such that @xmath9 can be reconstructed ` as well as possible ' from @xmath657 . in that case , @xmath663 and writing @xmath664 , the value @xmath665 measures the similarity between @xmath9 and @xmath666 .",
    "for example , with @xmath2 is the set of real numbers and @xmath185 is the set of integers , the squared difference @xmath667 is a viable distortion function .",
    "we may interpret @xmath666 as an estimate of @xmath9 , and @xmath185 as the set of values it can take .",
    "the reason we use the notation @xmath652 rather than @xmath668 ( as in , for example , @xcite ) is that further below , we mostly concentrate on slightly non - standard applications where @xmath185 should _ not _ be interpreted as a subset of @xmath2 .",
    "we want to determine the optimal code @xmath53 for communication between a and b under the constraint that there are no more than @xmath646 messages .",
    "that is , we look for the encoding function @xmath652 that minimizes the expected distortion , under the constraint that @xmath653 .",
    "usually , the minimum achievable expected distortion is nonincreasing as a function of increasing @xmath645 .",
    "[ ex : gauss ] suppose @xmath35 is a real - valued , normally ( gaussian ) distributed random variable with mean @xmath669 = 0 $ ] and variance @xmath670 ^ 2 = \\sigma^2 $ ] .",
    "let us use the squared euclidean distance @xmath671 as a distortion measure .",
    "if @xmath331 is allowed to use @xmath645 bits , then @xmath185 can have no more than @xmath646 elements , in contrast to @xmath2 that is uncountably infinite .",
    "we should choose @xmath185 and the function @xmath652 such that ( [ eq : distortion ] ) is minimized .",
    "suppose first @xmath672",
    ". then the optimal @xmath652 turns out to be @xmath673 thus , the domain @xmath2 is partitioned into two regions , one corresponding to @xmath674 , and one to @xmath675 . by the symmetry of the gaussian distribution around @xmath90 , it should be clear that this is the best one can do . within each of the two region ,",
    "one picks a ` representative point ' so as to minimize ( [ eq : distortion ] ) .",
    "this mapping allows @xmath651 to estimate @xmath9 as well as possible .",
    "similarly , if @xmath676 , then @xmath2 should be partitioned into 4 regions , each of which are to be represented by a single point such that ( [ eq : distortion ] ) is minimized .",
    "an extreme case is @xmath677 : how can @xmath651 estimate @xmath35 if it is always given the same information ?",
    "this means that @xmath657 must take the same value for all @xmath9 . the expected distortion ( [ eq : distortion ] )",
    "is then minimized if @xmath678 , the mean of @xmath35 , giving distortion equal to @xmath679 .",
    "in general , there is no need for the space of estimates @xmath185 to be a subset of @xmath2 .",
    "we may , for example , also lossily encode or ` estimate ' the actual value of @xmath9 by specifying a set in which @xmath9 must lie ( section  [ sec : meaningful ] ) or a probability distribution ( see below ) on @xmath2 .",
    "[ ex : reconcile ] suppose receiver @xmath651 wants to estimate the actual @xmath9 by a probability distribution @xmath36 on @xmath2 .",
    "thus , if @xmath645 bits are allowed to be used , one of @xmath680 different distributions on @xmath2 can be sent to receiver .",
    "the most accurate that can be done is to partition @xmath34 into @xmath646 subsets @xmath681 .",
    "relative to any such partition , we introduce a new random variable @xmath652 and abbreviate the event @xmath682 to @xmath683 .",
    "sender observes that @xmath684 for some @xmath685 and passes this information on to receiver .",
    "the information @xmath103 actually means that @xmath35 is now distributed according to the conditional distribution @xmath686 .",
    "it is now natural to measure the quality of the transmitted distribution @xmath687 by its conditional entropy , i.e. the expected additional number of bits that sender has to transmit before receiver knows the value of @xmath9 with certainty .",
    "this can be achieved by taking @xmath688 which we abbreviate to @xmath689 . in words ,",
    "the distortion function is the shannon - fano code length for the communicated distribution .",
    "the expected distortion then becomes equal to the conditional entropy @xmath690 as defined in section  [ sec : probmutual ] ( rewrite according to , @xmath691 for @xmath692 and @xmath693 defined earlier , and the definition of conditional probability ) : @xmath694   & = \\sum_{y \\in { \\cal y } } g(y ) \\sum_{x : y(x)=y } ( f(x)/g(y ) ) d(x , y)\\\\ \\nonumber & =   \\sum_{y \\in { \\cal y } } g(y ) \\sum_{x : y(x)=y } f(x|y ) \\log 1/f(x|y)\\\\   \\nonumber & =   h(x|{y}).\\end{aligned}\\ ] ] how is this related to lossless compression ?",
    "suppose for example that @xmath695 .",
    "then the optimal distortion is achieved by partitioning @xmath2 into two sets @xmath696 in the most ` informative ' possible way , so that the conditional entropy @xmath697 is minimized .",
    "if @xmath53 itself is encoded with the shannon - fano code , then @xmath365 bits are needed to communicate @xmath53 . rewriting @xmath698 and @xmath699 with @xmath700 and rearranging",
    ", shows that for all such partitions of @xmath34 into @xmath701 subsets defined by @xmath702 we have @xmath703 the minimum rate distortion is obtained by choosing the function @xmath53 that minimizes @xmath384 . by ( [ eq : snavel ]",
    ") this is also the @xmath53 maximizing @xmath365 .",
    "thus , the average total number of bits we need to send our message in this way is still equal to @xmath704the more we save in the second part , the more we pay in the first part .",
    "[ [ rate - distortion - and - mutual - information ] ] rate distortion and mutual information : + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    already in his 1948 paper , shannon established a deep relation between mutual information and minimum achievable distortion for ( essentially ) _ arbitrary _ distortion functions . the relation is summarized in theorem  [ thm : rd ] below . to prepare for the theorem",
    ", we need to slightly extend our setting by considering _ independent repetitions of the same scenario_. this can be motivated in various ways such as ( a ) it often corresponds to the situation we are trying to model ; ( b ) it allows us to consider non - integer rates @xmath645 , and ( c ) it greatly simplifies the mathematical analysis .",
    "let @xmath705 be two sample spaces .",
    "the distortion of @xmath706 with respect to @xmath174 is defined by a nonnegative real - valued function @xmath707 as above .",
    "we extend the definition to sequences : the distortion of @xmath708 with respect to @xmath709 is @xmath710    let @xmath711 be @xmath58 independent identically distributed random variables on outcome space @xmath2 .",
    "let @xmath185 be a set of code words .",
    "we want to find a sequence of functions @xmath712 so that the message @xmath713 gives as much expected information about the sequence of outcomes @xmath714 as is possible , under the constraint that the message takes at most @xmath715 bits ( so that @xmath645 bits are allowed on average per outcome of @xmath716 ) . instead of @xmath717 above write @xmath718 . the _ expected distortion _",
    "$ ] for @xmath720 is @xmath721 = \\sum_{(x_1 , \\ldots , x_n ) \\in { \\cal x}^n } p(x^n = ( x_1 , \\ldots , x_n ) ) \\cdot \\frac{1}{n } \\sum_{i=1}^n d(x_i , y_i(x_i)).\\ ] ] consider functions @xmath720 with range @xmath722 satisfying @xmath723 .",
    "let for @xmath724 random variables a choice @xmath717 minimize the expected distortion under these constraints , and let the corresponding value @xmath725 of the expected distortion be defined by @xmath726    for every distortion measure , and all @xmath727 , @xmath728",
    ".    let @xmath729 achieve @xmath730 and @xmath731 achieve @xmath732 .",
    "then , @xmath733 achieves @xmath734 .",
    "this is an upper bound on the minimal possible value @xmath735 for @xmath736 random variables .",
    "it follows that for all @xmath737 we have @xmath738 .",
    "the inequality is typically strict ; @xcite gives an intuitive explanation of this phenomenon .",
    "for fixed @xmath645 the value of @xmath739 is fixed and it is finite . since also @xmath740 is necessarily positive for all @xmath58 , we have established the existence of the limit @xmath741 the value of @xmath742 is the minimum achievable distortion at rate ( number of bits / outcome ) @xmath645 .",
    "therefore , @xmath743 it is called the _ distortion - rate function_. in our gaussian example  [ ex : gauss ] , @xmath742 quickly converges to @xmath90 with increasing @xmath645 .",
    "it turns out that for general @xmath480 , when we view @xmath742 as a function of @xmath744 , it is _ convex and nonincreasing_.    [ ex : ber ] let @xmath745 , and let @xmath746 .",
    "let @xmath747 and take the shannon - fano distortion function @xmath748 with notation as in example  [ ex : reconcile ] .",
    "let @xmath53 be a function that achieves the minimum expected shannon - fano distortion @xmath739 . as usual",
    "we write @xmath53 for the random variable @xmath749 induced by @xmath35 . then , @xmath750 = { \\bf e } [ \\log 1/ f(x|y ) ] = h(x|y)$ ] . at rate @xmath751 , we can set @xmath752 and the minimum achievable distortion is given by @xmath753 .",
    "now consider some rate @xmath645 with @xmath754 , say @xmath755 .",
    "since we are now forced to use less than @xmath756 messages in communicating , only a fixed message can be sent , no matter what outcome of the random variable @xmath35 is realized .",
    "this means that no communication is possible at all and the minimum achievable distortion is @xmath757 . but clearly , if we consider @xmath58 repetitions of the same scenario and are allowed to send a message out of @xmath758 candidates , then some useful information can be communicated after all , even if @xmath759 . in example",
    "[ ex : berb ] we will show that if @xmath760 , then @xmath761 ; if @xmath762 , then @xmath763 .    up to now",
    "we studied the minimum achievable distortion @xmath100 as a function of the rate @xmath645 . for technical reasons ,",
    "it is often more convenient to consider the minimum achievable rate @xmath645 as a function of the distortion @xmath100 .",
    "this is the more celebrated version , the _ rate - distortion function _ @xmath764 .",
    "because @xmath742 is convex and nonincreasing , @xmath765 $ ] is just the _ inverse _ of the function @xmath742 .",
    "it turns out to be possible to relate distortion to the shannon mutual information .",
    "this remarkable fact , which shannon proved already in @xcite , illustrates the fundamental nature of shannon s concepts .",
    "up till now , we only considered _",
    "encodings @xmath766 .",
    "but it is hard to analyze the rate - distortion , and distortion - rate , functions in this setting .",
    "it turns out to be advantageous to follow an indirect route by bringing information - theoretic techniques into play . to this end",
    ", we generalize the setting to _ randomized _ encodings .",
    "that is , upon observing @xmath51 with probability @xmath42 , the sender may use a randomizing device ( e.g. a coin ) to decide which code word in @xmath706 he is going to send to the receiver .",
    "a randomized encoding @xmath53 thus maps each @xmath174 to @xmath706 with probability @xmath767 , denoted in conditional probability format as @xmath768 .",
    "altogether we deal with a joint distribution @xmath769 on the joint sample space @xmath337 .",
    "( in the deterministic case we have @xmath770 for the given function @xmath766 . )",
    "let @xmath35 and @xmath53 be joint random variables as above , and let @xmath707 be a distortion measure .",
    "the _ expected distortion _",
    "@xmath771 of @xmath53 with respect to @xmath35 is defined by @xmath772    note that for a given problem the source probability @xmath42 of outcome @xmath51 is fixed , but the randomized encoding @xmath53 , that is the conditional probability @xmath768 of encoding source word @xmath9 by code word @xmath103 , can be chosen to advantage .",
    "we define the auxiliary notion of _ information rate distortion function _ @xmath773 by @xmath774 that is , for random variable @xmath35 , among _ all _ joint random variables @xmath53 with expected distortion to @xmath35 less than or equal to @xmath100 , the information rate @xmath773 equals the minimal mutual information with @xmath35 .    [ thm : rd ] for every random source @xmath35 and distortion measure @xmath480 : @xmath775    this remarkable theorem states that the best deterministic code achieves a rate - distortion that equals the minimal information rate possible for a randomized code , that is , the minimal mutual information between the random source and a randomized code .",
    "note that this does not mean that @xmath764 is independent of the distortion measure .",
    "in fact , the source random variable @xmath35 , together with the distortion measure @xmath480 , determines a random code @xmath53 for which the joint random variables @xmath35 and @xmath53 reach the infimum in .",
    "the proof of this theorem is given in @xcite .",
    "it is illuminating to see how it goes : it is shown first that , for a random source @xmath35 and distortion measure @xmath480 , every deterministic code @xmath53 with distortion @xmath776 has rate @xmath777 .",
    "subsequently , it is shown that there exists a deterministic code that , with distortion @xmath778 , achieves rate @xmath779 . to analyze deterministic @xmath764 therefore , we can determine the best randomized code @xmath53 for random source @xmath35 under distortion constraint @xmath100 , and then we know that simply @xmath780 .",
    "( example  [ ex : ber ] , continued ) [ ex : berb ] suppose we want to compute @xmath764 for some @xmath100 between @xmath90 and @xmath137 .",
    "if we only allow encodings @xmath53 that are deterministic functions of @xmath35 , then either @xmath781 or @xmath782 . in both cases",
    "@xmath783 = h(x| y ) = 0 $ ] , so @xmath53 satisfies the constraint in ( [ eq : ird ] ) . in both cases , @xmath784 . with ( [ eq : rd ] )",
    "this shows that @xmath785 .",
    "however , @xmath764 is actually smaller : by allowing randomized codes , we can define @xmath786 as @xmath787 with probability @xmath788 and @xmath789 with probability @xmath790 . for @xmath791 , @xmath792 = h(x| y_{\\alpha})$ ] increases with @xmath788 , while @xmath793 decreases with @xmath788 .",
    "thus , by choosing the @xmath794 for which the constraint @xmath795 \\leq d$ ] holds with equality , we find @xmath796 .",
    "let us now calculate @xmath764 and @xmath742 explicitly .    since @xmath797 , we can rewrite @xmath764 as @xmath798 in the special case where @xmath100 is itself the shannon - fano distortion , this can in turn be rewritten as @xmath799 since @xmath742 is the inverse of @xmath764 , we find @xmath800 , as announced in example  [ ex : ber ] .    [ [ problem - and - lacuna-3 ] ] problem and lacuna : + + + + + + + + + + + + + + + + + + +    in the rate - distortion setting we allow ( on average ) a rate of @xmath645 bits to express the data as well as possible in some way , and measure the average of loss by some distortion function .",
    "but in many cases , like lossy compression of images , one is interested in the individual cases .",
    "the average over all possible images may be irrelevant for the individual cases one meets .",
    "moreover , one is not particularly interested in bit - loss , but rather in preserving the essence of the image as well as possible . as another example",
    ", suppose the distortion function is simply to supply the remaining bits of the data .",
    "but this can be unsatisfactory : we are given an outcome of a measurement as a real number of @xmath58 significant bits .",
    "then the @xmath645 most significant bits carry most of the meaning of the data , while the remaining @xmath801 bits may be irrelevant .",
    "thus , we are lead to the elusive notion of a distortion function that captures the amount of `` meaning '' that is not included in the @xmath645 rate bits .",
    "these issues are taken up by kolmogorov s proposal of the structure function .",
    "this cluster of ideas puts the notion of rate  distortion in an individual algorithmic ( kolmogorov complexity ) setting , and focuses on the meaningful information in the data . in the end",
    "we can recycle the new insights and connect them to rate - distortion notions to provide new foundations for statistical inference notions as maximum likelihood ( ml ) @xcite , minimum message length ( mml ) @xcite , and minimum description length ( mdl ) @xcite .",
    "there is a close relation between functions describing three , a priori seemingly unrelated , aspects of modeling individual data , depicted in figure  [ figure.estimator ] .",
    "[ sec : meaningful ] one of these was introduced by kolmogorov at a conference in tallinn 1974 ( no written version ) and in a talk at the moscow mathematical society in the same year of which the abstract @xcite is as follows ( this is the only writing by kolmogorov about this circle of ideas ) :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ `` to each constructive object corresponds a function @xmath802 of a natural number @xmath416the log of minimal cardinality of @xmath9-containing sets that allow definitions of complexity at most @xmath416 .",
    "if the element @xmath9 itself allows a simple definition , then the function @xmath803 drops to @xmath137 even for small @xmath416 . lacking such definition , the element is `` random '' in a negative sense .",
    "but it is positively `` probabilistically random '' only when function @xmath803 having taken the value @xmath804 at a relatively small @xmath805 , then changes approximately as @xmath806 . ''",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    kolmogorov s @xmath807 is commonly called the `` structure function '' and is here denoted as @xmath808 and defined in .",
    "the structure function notion entails a proposal for a non - probabilistic approach to statistics , an individual combinatorial relation between the data and its model , expressed in terms of kolmogorov complexity .",
    "it turns out that the structure function determines all stochastic properties of the data in the sense of determining the best - fitting model at every model - complexity level , the equivalent notion to `` rate '' in the shannon theory .",
    "a consequence is this : minimizing the data - to - model code length ( finding the ml estimator or mdl estimator ) , in a class of contemplated models of prescribed maximal ( kolmogorov ) complexity , _ always _ results in a model of best fit , irrespective of whether the source producing the data is in the model class considered . in this",
    "setting , code length minimization _",
    "always _ separates optimal model information from the remaining accidental information , and not only with high probability .",
    "the function that maps the maximal allowed model complexity to the goodness - of - fit ( expressed as minimal `` randomness deficiency '' ) of the best model can not itself be monotonically approximated .",
    "however , the shortest one - part or two - part code above can  implicitly optimizing this elusive goodness - of - fit .    in probabilistic statistics the goodness of the selection process",
    "is measured in terms of expectations over probabilistic ensembles . for current applications ,",
    "average relations are often irrelevant , since the part of the support of the probability mass function that will ever be observed has about zero measure",
    ". this may be the case in , for example , complex video and sound analysis .",
    "there arises the problem that for individual cases the selection performance may be bad although the performance is good on average , or vice versa .",
    "there is also the problem of what probability means , whether it is subjective , objective , or exists at all .",
    "kolmogorov s proposal strives for the firmer and less contentious ground of finite combinatorics and effective computation .    [",
    "[ model - selection ] ] model selection : + + + + + + + + + + + + + + + +    it is technically convenient to initially consider the simple model class of finite sets to obtain our results , just as in section  [ sec : algsuf ] .",
    "it then turns out that it is relatively easy to generalize everything to the model class of computable probability distributions ( section  [ s.prob ] ) .",
    "that class is very large indeed : perhaps it contains every distribution that has ever been considered in statistics and probability theory , as long as the parameters are computable numbers  for example rational numbers .",
    "thus the results are of great generality ; indeed , they are so general that further development of the theory must be aimed at restrictions on this model class .    below we will consider various model selection procedures .",
    "these are approaches for finding a model @xmath280 ( containing @xmath9 ) for arbitrary data @xmath9 .",
    "the goal is to find a model that captures all meaningful information in the data @xmath9 .",
    "all approaches we consider are at some level based on coding @xmath9 by giving its index in the set @xmath280 , taking @xmath809 bits .",
    "this codelength may be thought of as a particular distortion function , and here lies the first connection to shannon s rate - distortion :    [ rem.rd-ksf1 ] a model selection procedure is a function @xmath720 mapping binary data of length @xmath58 to finite sets of strings of length @xmath58 , containing the mapped data , @xmath810 ( @xmath571 ) .",
    "the range of @xmath720 satisfies @xmath811 , the distortion function @xmath480 is defined to be @xmath812 . to define the rate ",
    "distortion function we need that @xmath9 is the outcome of a random variable @xmath35 .",
    "here we treat the simple case that @xmath35 represents @xmath58 flips of a fair coin ; this is substantially generalized in section  [ sec : esf ] . since each outcome of a fair coin can be described by one bit , we set the rate @xmath645 at @xmath754 . then , @xmath813 for the minimum of the right - hand side we can assume that if @xmath814 then @xmath815 ( the distinct @xmath816 s are disjoint ) . denote the distinct @xmath816 s by @xmath817 with @xmath818 for some @xmath819 .",
    "then , @xmath820 .",
    "the right - hand side reaches its minimum for all @xmath817 s having the same cardinality and @xmath821 .",
    "then , @xmath822 . therefore , @xmath823 and therefore @xmath824 .    alternatively , and more in line with the structure - function approach below",
    ", one may consider repetitions of a random variable @xmath35 with outcomes in @xmath57 .",
    "then , a model selection procedure is a function @xmath53 mapping binary data of length @xmath58 to finite sets of strings of length @xmath58 , containing the mapped data , @xmath825 ( @xmath571 ) .",
    "the range of @xmath53 satisfies @xmath826 , the distortion function @xmath480 is defined by @xmath827 . to define the rate ",
    "distortion function we need that @xmath9 is the outcome of a random variable @xmath35 , say a toss of a fair @xmath140-sided coin . since each outcome of a fair coin can be described by @xmath58 bits , we set the rate @xmath645 at @xmath828 .",
    "then , for outcomes @xmath829 ( @xmath830 ) , resulting from @xmath275 i.i.d . random variables @xmath831 , we have @xmath832",
    ". then , @xmath833 .",
    "assume that @xmath834 if @xmath835 : the distinct @xmath836 s are disjoint and partition @xmath837 into disjoint subsets @xmath838 , with @xmath839 for some @xmath840 .",
    "then , @xmath841 .",
    "the right - hand side reaches its minimum for all @xmath838 s having the same cardinality and @xmath842 , so that @xmath843 .",
    "therefore , @xmath844 and @xmath845 . in example  [ ex.rd=str ] we relate these numbers to the structure function approach described below .    [ [ model - fitness ] ] model fitness : + + + + + + + + + + + + + +    a distinguishing feature of the structure function approach is that we want to formalize what it means for an element to be `` typical '' for a set that contains it .",
    "for example , if we flip a fair coin @xmath58 times , then the sequence of @xmath58 outcomes , denoted by @xmath9 , will be an element of the set @xmath57 .",
    "in fact , most likely it will be a `` typical '' element in the sense that it has all properties that hold on average for an element of that set .",
    "for example , @xmath9 will have @xmath846 frequency of 1 s , it will have a run of about @xmath847 consecutive 0 s , and so on for many properties .",
    "note that the sequence @xmath848 , consisting of one half 0 s followed by one half ones , is very untypical , even though it satisfies the two properties described explicitly .",
    "the question arises how to formally define `` typicality '' .",
    "we do this as follows : the lack of typicality of @xmath9 with respect to a finite set @xmath280 ( the model ) containing it , is the amount by which @xmath849 falls short of the length @xmath583 of the data - to - model code ( section  [ sec : algsuf ] ) .",
    "thus , the _ randomness deficiency _ of @xmath9 in @xmath280 is defined by @xmath850 for @xmath571 , and @xmath851 otherwise .",
    "clearly , @xmath9 can be typical for vastly different sets .",
    "for example , every @xmath9 is typical for the singleton set @xmath40 , since @xmath852 and @xmath853 . yet",
    "the many @xmath9 s that have @xmath854 are also typical for @xmath57 , but in another way . in the first example , the set is about as complex as @xmath9 itself . in the second example , the set is vastly less complex than @xmath9 : the set has complexity about @xmath855 while @xmath856 .",
    "thus , very high complexity data may have simple sets for which they are typical .",
    "as we shall see , this is certainly not the case for all high complexity data .",
    "the question arises how typical data @xmath9 of length @xmath58 can be in the best case for a finite set of complexity @xmath645 when @xmath645 ranges from 0 to @xmath58 .",
    "the function describing this dependency , expressed in terms of randomness deficiency to measure the optimal typicality , as a function of the complexity `` rate '' @xmath645 ( @xmath857 ) of the number of bits we can maximally spend to describe a finite set containing @xmath9 , is defined as follows :    the _ minimal randomness deficiency _ function is @xmath858 where we set @xmath859 . if @xmath860 is small , then @xmath9 may be considered as a _",
    "typical _ member of @xmath280 .",
    "this means that @xmath280 is a `` best '' model for @xmath9a most likely explanation .",
    "there are no simple special properties that single it out from the majority of elements in @xmath280 .",
    "we therefore like to call @xmath861 the _ best - fit estimator_. this is not just terminology : if @xmath862 is small , then @xmath9 satisfies _ all _ properties of low kolmogorov complexity that hold with high probability ( under the uniform distribution ) for the elements of @xmath280 . to be precise @xcite : consider strings of length @xmath58 and let @xmath280 be a subset of such strings .",
    "we view a _ property _ of elements in @xmath280 as a function @xmath863 . if @xmath864 then @xmath9 has the property represented by @xmath865 and if @xmath866 then @xmath9 does not have the property . then",
    ": ( i ) if @xmath865 is a property satisfied by all @xmath9 with @xmath867 , then @xmath865 holds with probability at least @xmath868 for the elements of @xmath280 .",
    "\\(ii ) let @xmath865 be any property that holds with probability at least @xmath869 for the elements of @xmath280 .",
    "then , every such @xmath865 holds simultaneously for every @xmath571 with @xmath870 .",
    "* lossy compression : * the function @xmath871 is relevant to lossy compression ( used , for instance , to compress images )  see also remark  [ rem : lossy ] .",
    "assume we need to compress @xmath9 to @xmath645 bits where @xmath872 .",
    "of course this implies some loss of information present in @xmath9 .",
    "one way to select redundant information to discard is as follows : find a set @xmath873 with @xmath874 and with small @xmath875 , and consider a compressed version @xmath590 of @xmath280 . to reconstruct an @xmath876 , a decompresser uncompresses @xmath590 to @xmath280 and selects at random an element @xmath876 of @xmath280 .",
    "since with high probability the randomness deficiency of @xmath876 in @xmath280 is small , @xmath876 serves the purpose of the message @xmath9 as well as does @xmath9 itself .",
    "let us look at an example . to transmit a picture of `` rain '' through a channel with limited capacity @xmath645",
    ", one can transmit the indication that this is a picture of the rain and the particular drops may be chosen by the receiver at random . in this interpretation",
    ", @xmath861 indicates how `` random '' or `` typical '' @xmath9 is with respect to the best model at complexity level @xmath645and hence how `` indistinguishable '' from the original @xmath9 the randomly reconstructed @xmath876 can be expected to be .",
    "this randomness deficiency function quantifies the goodness of fit of the best model at complexity @xmath645 for given data @xmath9 . as far as we know no direct counterpart of this notion exists in rate  distortion theory , or , indeed , can be expressed in classical theories like information theory .",
    "but the situation is different for the next function we define , which , in almost contradiction to the previous statement , can be tied to the minimum randomness deficiency function , yet , as will be seen in example  [ ex.rd=str ] and section  [ sec : esf ] , does have a counterpart in rate ",
    "distortion theory after all .",
    "[ [ maximum - likelihood - estimator ] ] maximum likelihood estimator : + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the _ kolmogorov structure _",
    "function @xmath808 of given data @xmath9 is defined by @xmath877 where @xmath577 is a contemplated model for @xmath9 , and @xmath645 is a nonnegative integer value bounding the complexity of the contemplated @xmath280 s .",
    "the structure function uses models that are finite sets and the value of the structure function is the log - cardinality of the smallest such set containing the data .",
    "equivalently , we can use uniform probability mass functions over finite supports ( the former finite set models ) .",
    "the smallest set containing the data then becomes the uniform probability mass assigning the highest probability to the data  with the value of the structure function the corresponding negative log - probability .",
    "this motivates us to call @xmath808 the _ maximum likelihood estimator_. the treatment can be extended from uniform probability mass functions with finite supports , to probability models that are arbitrary computable probability mass functions , keeping all relevant notions and results essentially unchanged , section  [ s.prob ] , justifying the maximum likelihood identification even more .",
    "clearly , the kolmogorov structure function is non - increasing and reaches @xmath878 for the `` rate '' @xmath879 where @xmath473 is the number of bits required to change @xmath9 into @xmath40 .",
    "it is also easy to see that for argument @xmath880 , where @xmath881 is the number of bits required to compute the set of all strings of length @xmath882 of @xmath9 from @xmath882 , the value of the structure function is at most @xmath882 ; see figure  [ figure.estimator ]    [ ex.rd=str ] clearly the structure function measures for individual outcome @xmath9 a distortion that is related to the one measured by @xmath883 in example  [ rem.rd-ksf1 ] for the uniform average of outcomes @xmath9 .",
    "note that all strings @xmath9 of length @xmath58 satisfy @xmath884 ( since @xmath885 and @xmath886 ) .",
    "for every @xmath645 ( @xmath857 ) , we can describe every @xmath887 as an element of the set @xmath888 .",
    "then , @xmath889 and @xmath890 .",
    "this shows that @xmath891 for every @xmath9 and every @xmath645 with @xmath857 ; see figure  [ figure.estimator ] .    for all @xmath9 s and @xmath645 s we can describe @xmath9 in a two - part code by the set @xmath280 witnessing @xmath892 and @xmath9 s index in that set .",
    "the first part describing @xmath280 in @xmath893 allows us to generate @xmath280 , and given @xmath280 we know @xmath583 .",
    "then , we can parse the second part of @xmath894 bits that gives @xmath9 s index in @xmath280 .",
    "we also need a fixed @xmath20 bit program to produce @xmath9 from these descriptions .",
    "since @xmath234 is the lower bound on the length of effective descriptions of @xmath9 , we have @xmath895 . there are @xmath896 strings @xmath9 of complexity @xmath856 , @xcite .",
    "for all these strings @xmath897 .",
    "hence , the expected value @xmath892 equals @xmath898 + 2^{n - k(n)+o(1 ) } o(n - r+o(\\log n ) ) \\ } = n - r + o(n - r/2^{-k(n ) } ) =   n - r + o(n - r)$ ] ( since @xmath899 for @xmath900 ) .",
    "that is , the expectation of @xmath892 equals @xmath901 , the distortion - rate function , where the @xmath902 term goes to 0 with the length @xmath58 of @xmath9 . in section",
    "[ sec : esf ] we extend this idea to non - uniform distributions on @xmath35 .    for every @xmath873 we have @xmath903 indeed , consider the following _ two - part code _ for @xmath9 :",
    "the first part is a shortest self - delimiting program @xmath157 of @xmath280 and the second part is @xmath568 bit long index of @xmath9 in the lexicographical ordering of @xmath280 .",
    "since @xmath280 determines @xmath583 this code is self - delimiting and we obtain where the constant @xmath20 is the length of the program to reconstruct @xmath9 from its two - part code .",
    "we thus conclude that @xmath904 , that is , the function @xmath892 never decreases more than a fixed independent constant below the diagonal _ sufficiency line _",
    "@xmath905 defined by @xmath906 , which is a lower bound on @xmath907 and is approached to within a constant distance by the graph of @xmath808 for certain @xmath645 s ( for instance , for @xmath879 ) . for these @xmath645 s",
    "we thus have @xmath908 . in the terminology we have introduced in section  [ sect.ss ] and definition  [ def : algsufstat ] ,",
    "a model corresponding to such an @xmath645 ( witness for @xmath892 ) is an optimal set for @xmath9 and a shortest program to compute this model is a sufficient statistic .",
    "it is _ minimal _ for the least such @xmath645 for which the above equality holds .",
    "[ [ mdl - estimator ] ] mdl estimator : + + + + + + + + + + + + + +    the length of the minimal two - part code for @xmath9 consisting of the model cost @xmath281 and the length of the index of @xmath9 in @xmath280 , the complexity of @xmath280 upper bounded by @xmath645 , is given by the _ mdl ( minimum description length ) function _",
    ": @xmath909 where @xmath910 is the total length of two - part code of @xmath9 with help of model @xmath280 . clearly , @xmath911 , but a priori it is still possible that @xmath912 for @xmath913 . in that case @xmath914 .",
    "however , in @xcite it is shown that @xmath915 for all @xmath9 of length @xmath58 .",
    "even so , this does nt mean that a set @xmath280 that witnesses @xmath916 in the sense that @xmath571 , @xmath917 , and @xmath918 , also witnesses @xmath892 .",
    "it can in fact be the case that @xmath919 , and @xmath920 for arbitrarily large @xmath921 .",
    "apart from being convenient for the technical analysis in this work , @xmath916 is the celebrated two - part minimum description length code length @xcite with the model - code length restricted to at most @xmath645 .",
    "when @xmath645 is large enough so that @xmath922 , then there is a set @xmath280 that is a sufficient statistic , and the smallest such @xmath645 has an associated witness set @xmath280 that is a minimal sufficient statistic .",
    "the most fundamental result in @xcite is the equality @xmath923 which holds within logarithmic additive terms in argument and value .",
    "additionally , every set @xmath280 that witnesses the value @xmath924 ( or @xmath925 ) , also witnesses the value @xmath926 ( but not vice versa ) .",
    "it is easy to see that @xmath907 and @xmath925 are upper semi - computable ( definition  [ def.semi ] ) ; but we have shown @xcite that @xmath926 is neither upper nor lower semi - computable ( not even within a great tolerance ) .",
    "a priori there is no reason to suppose that a set that witnesses @xmath907 ( or @xmath925 ) also witnesses @xmath926 , for _ every _ @xmath645 .",
    "but the fact that they do , vindicates kolmogorov s original proposal and establishes @xmath808 s pre - eminence over @xmath927  the pre - eminence of @xmath808 over @xmath928 is discussed below .",
    "[ rem.mlvsmdl ] what we call ` maximum likelihood ' in the form of @xmath808 is really ` maximum likelihood ' under a complexity constraint @xmath645 on the models as in @xmath907 . in statistics ,",
    "it is a well - known fact that maximum likelihood often fails ( dramatically overfits ) when the models under consideration are of unrestricted complexity ( for example , with polynomial regression with gaussian noise , or with markov chain model learning , maximum likelihood will always select a model with @xmath58 parameters , where @xmath58 is the size of the sample  and thus typically , maximum likelihood will dramatically overfit , whereas for example mdl typically performs well ) .",
    "the equivalent , in our setting , is that allowing models of unconstrained complexity for data @xmath9 , say complexity @xmath234 , will result in the ml - estimator @xmath929the witness model being the trivial , maximally overfitting , set @xmath40 . in the mdl case ,",
    "on the other hand , there may be a long constant interval with the mdl estimator @xmath930 ( @xmath931 $ ] ) where the length of the two - part code does nt decrease anymore . selecting the least complexity model witnessing this function value we obtain the , very significant , algorithmic _",
    "minimal _ sufficient statistic , definition  [ def : algsufstat ] . in this sense ,",
    "mdl augmented with a bias for the least complex explanation , which we may call the ` occam s razor mdl ' , is superior to maximum likelihood and resilient to overfitting . if we do nt apply bias in the direction of simple explanations , then  at least in our setting ",
    "mdl may be just as prone to overfitting as is ml .",
    "for example , if @xmath9 is a typical random element of @xmath57 , then @xmath932 for the entire interval @xmath933 .",
    "choosing the model on the left side , of simplest complexity , of complexity @xmath580 gives us the best fit with the correct model @xmath57 .",
    "but choosing a model on the right side , of high complexity , gives us a model @xmath40 of complexity @xmath934 that completely overfits the data by modeling all random noise in @xmath9 ( which in fact in this example almost completely consists of random noise ) .",
    "thus , it should be emphasized that ml = mdl really only holds if complexities are constrained to a value @xmath645 ( that remains fixed as the sample size grows ",
    "note that in the markov chain example above , the complexity grows linearly with the sample size ) ; it certainly does not hold in an unrestricted sense ( not even in the algorithmic setting ) .    in a sense , @xmath808 is more strict than @xmath928 : a set that witnesses @xmath892 also witnesses @xmath925 but not necessarily vice versa .",
    "however , at those complexities @xmath645 where @xmath916 drops ( a little bit of added complexity in the model allows a shorter description ) , the witness set of @xmath928 is also a witness set of @xmath808 .",
    "but if @xmath928 stays constant in an interval @xmath935 $ ] , then we can trade - off complexity of a witness set versus its cardinality , keeping the description length constant .",
    "this is of course not possible with @xmath808 where the cardinality of the witness set at complexity @xmath645 is fixed at @xmath892 .",
    "the main result can be taken as a foundation and justification of common statistical principles in model selection such as maximum likelihood or mdl .",
    "the structure functions @xmath936 and @xmath927 can assume all possible shapes over their full domain of definition ( up to additive logarithmic precision in both argument and value ) , see @xcite .",
    "( this establishes the significance of , since it shows that @xmath937 is common for @xmath938 pairs  in which case the more or less easy fact that @xmath939 for @xmath940 is not applicable , and it is a priori unlikely that holds : why should minimizing a set containing @xmath9 also minimize its randomness deficiency ?",
    "surprisingly , it does ! ) we have exhibited a  to our knowledge first  natural example , @xmath927 , of a function that is not semi - computable but computable with an oracle for the halting problem .",
    "[ ex.prnr ] * `` positive '' and `` negative '' individual randomness : * in @xcite we showed the existence of strings for which essentially the singleton set consisting of the string itself is a minimal sufficient statistic . while a sufficient statistic of an object yields a two - part code that is as short as the shortest one part code , restricting the complexity of the allowed statistic may yield two - part codes that are considerably longer than the best one - part code ( so the statistic is insufficient ) .",
    "in fact , for every object there is a complexity bound below which this happens ",
    "but if that bound is small ( logarithmic ) we call the object `` stochastic '' since it has a simple satisfactory explanation ( sufficient statistic ) .",
    "thus , kolmogorov in @xcite makes the important distinction of an object being random in the `` negative '' sense by having this bound high ( it has high complexity and is not a typical element of a low - complexity model ) , and an object being random in the `` positive , probabilistic '' sense by both having this bound small and itself having complexity considerably exceeding this bound ( like a string @xmath9 of length @xmath58 with @xmath854 , being typical for the set @xmath57 , or the uniform probability distribution over that set , while this set or probability distribution has complexity @xmath941 ) .",
    "we depict the distinction in figure  [ figure.pos_negrandom ] . in simple terms : high kolmogorov complexity of a data string just means that it is random in a _ negative sense _ ; but a data string of high kolmogorov complexity is _ positively random _ if the simplest satisfactory explanation ( sufficient statistic ) has low complexity , and it therefore is the typical outcome of a simple random process .    in @xcite it is shown that for every length @xmath58 and every complexity @xmath942 ( the maximal complexity of @xmath9 of length @xmath58 ) and every @xmath943 $ ] , there are @xmath9 s of length @xmath58 and complexity @xmath416 such that the minimal randomness deficiency @xmath944 for every @xmath945 and @xmath946 for every @xmath947 . therefore , the set of @xmath58-length strings of every complexity @xmath416 can be partitioned in subsets of strings that have a kolmogorov minimal sufficient statistic of complexity @xmath948 for @xmath949 .",
    "for instance , there are @xmath58-length non - stochastic strings of almost maximal complexity @xmath950 having significant @xmath951 randomness deficiency with respect to @xmath57 or , in fact , every other finite set of complexity less than @xmath952 !      the structure function ( and of course the sufficient statistic ) use properties of data strings modeled by finite sets , which amounts to modeling data by uniform distributions .",
    "as already observed by kolmogorov himself , it turns out that this is no real restriction .",
    "everything holds also for computable probability mass functions ( probability models ) , up to additive logarithmic precision .",
    "another version of @xmath808 uses probability models @xmath31 rather than finite set models .",
    "it is defined as @xmath953 .",
    "since @xmath954 and @xmath892 are close by proposition  [ prop.1 ] below , theorem  [ thm.dresf ] and corollary  [ cor.esf ] also apply to @xmath955 and the distortion - rate function @xmath742 based on a variation of the shannon - fano distortion measure defined by using encodings @xmath956 with @xmath31 a computable probability distribution . in this context , the shannon - fano distortion measure is defined by @xmath957 it remains to show that probability models are essentially the same as finite set models .",
    "we restrict ourselves to the model class of _ computable probability distributions_. within the present section , we assume these are defined on strings of arbitrary length ; so they are represented by mass functions @xmath958 $ ] with @xmath959 being computable according to definition  [ def.enum.funct ] .",
    "a string @xmath9 is typical for a distribution @xmath31 if the randomness deficiency @xmath960 is small .",
    "the conditional complexity @xmath961 is defined as follows .",
    "say that a function @xmath331 approximates @xmath31 if @xmath962 for every @xmath103 and every positive rational @xmath963 .",
    "then @xmath964 is the minimum length of a program that given every function @xmath331 approximating @xmath31 as an oracle prints @xmath9 .",
    "similarly , @xmath31 is @xmath19-optimal for @xmath9 if @xmath965 .",
    "thus , instead of the data - to - model code length @xmath966 for finite set models , we consider the data - to - model code length @xmath967 ( the shannon - fano code ) .",
    "the value @xmath968 measures also how likely @xmath9 is under the hypothesis @xmath31 .",
    "the mapping @xmath969 where @xmath970 minimizes @xmath968 over @xmath31 with @xmath971 is a _",
    "maximum likelihood estimator _ , see figure  [ figure.mlestimator ] .",
    "our results thus imply that that maximum likelihood estimator always returns a hypothesis with minimum randomness deficiency .",
    "it is easy to show that for every data string @xmath9 and a contemplated finite set model for it , there is an almost equivalent computable probability model .",
    "the converse is slightly harder : for every data string @xmath9 and a contemplated computable probability model for it , there is a finite set model for @xmath9 that has no worse complexity , randomness deficiency , and worst - case data - to - model code for @xmath9 , up to additive logarithmic precision :    [ prop.1 ] ( a ) for every @xmath9 and every finite set @xmath577 there is a computable probability mass function @xmath31 with @xmath972 , @xmath973 and @xmath974 .",
    "\\(b ) there are constants @xmath975 , such that for every string @xmath9 , the following holds : for every computable probability mass function @xmath31 there is a finite set @xmath577 such that @xmath976 , @xmath977 and @xmath978 .",
    "\\(a ) define @xmath979 for @xmath980 and 0 otherwise .",
    "\\(b ) let @xmath981 , that is , @xmath982 .",
    "define @xmath983 .",
    "then , @xmath984 , which implies the claimed value for @xmath583 . to list @xmath280",
    "it suffices to compute all consecutive values of @xmath327 to sufficient precision until the combined probabilities exceed @xmath985 .",
    "that is , @xmath986 .",
    "finally , @xmath987 .",
    "the term @xmath988 can be upper bounded as @xmath989 , which implies the claimed bound for @xmath990 .",
    "how large are the nonconstant additive complexity terms in proposition  [ prop.1 ] for strings @xmath9 of length @xmath58 ? in item ( b ) , we are commonly only interested in @xmath31 such that @xmath991 and @xmath992 .",
    "indeed , for every @xmath31 there is @xmath993 such that @xmath994 , @xmath995 , @xmath996 .",
    "such @xmath993 is defined as follows : if @xmath997 then @xmath998 and @xmath999 for every @xmath1000 ; otherwise @xmath1001 where @xmath1002 stands for the uniform distribution on @xmath57 .",
    "then the additive terms in item ( b ) are @xmath413 .      in this section",
    "we treat the general relation between the expected value of @xmath892 , the expectation taken on a distribution @xmath312 of the random variable @xmath35 having outcome @xmath9 , and @xmath742 .",
    "this involves the development of a rate - distortion theory for individual sequences and arbitrary computable distortion measures .",
    "following @xcite , we outline such a theory in sections  [ sec : spheres]-  [ sec : ssrev ] . based on this theory , we present in section  [ sec : esfb ] a general theorem ( theorem  [ thm.dresf ] ) relating shannon s @xmath742 to the expected value of @xmath892 , for arbitrary random sources and computable distortion measures .",
    "this generalizes example  [ ex.rd=str ] above , where we analyzed the case of the distortion function @xmath1003 where @xmath749 is an @xmath9-containing finite set , for the uniform distribution .",
    "below we first extend this example to arbitrary generating distributions , keeping the distortion function still fixed to ( [ eq.lcfs1 ] .",
    "this will prepare us for the general development in sections  [ sec : spheres][sec : ssrev ]    in example  [ ex.rd=str ] it transpired that the distortion - rate function is the expected structure function , the expectation taken over the distribution on the @xmath9 s .",
    "if , instead of using the uniform distribution on @xmath57 we use an arbitrary distribution @xmath42 , it is not difficult to compute the rate - distortion function @xmath1004 where @xmath53 is a random vaiable with outcomes that are finite sets . since @xmath480 is a special type of shannon - fano distortion , with @xmath1005 if @xmath1006 , and 0 otherwise , we have already met @xmath742 for the distortion measure in another guise . by the conclusion of example  [ ex : berb ] ,",
    "generalized to the random variable @xmath35 having outcomes in @xmath57 , and @xmath645 being a rate in between 0 and @xmath58 , we know that @xmath1007    in the particular case analyzed above , the code word for a source word is a finite set containing the source word , and the distortion is the log - cardinality of the finite set .",
    "considering the set of source words of length @xmath58 , the distortion - rate function is the diagonal line from @xmath58 to @xmath58 .",
    "the structure functions of the individual data @xmath9 of length @xmath58 , on the other hand , always start at @xmath58 , decrease at a slope of at least -1 until they hit the diagonal from @xmath234 to @xmath234 , which they must do , and follow the diagonal henceforth .",
    "above we proved that the average of the structure function is simply the straight line , the diagonal , between @xmath58 and @xmath58 .",
    "this is the case , since the strings @xmath9 with @xmath854 are the overwhelming majority .",
    "all of them have a minimal sufficient statistic ( the point where the structure function hits the diagonal from @xmath234 to @xmath234 .",
    "this point has complexity at most @xmath580 .",
    "the structure function for all these @xmath9 s follows the diagonal from about @xmath58 to @xmath58 , giving overall an expectation of the structure function close to this diagonal , that is , the probabilistic distortion - rate function for this code and distortion measure .      modeling the data can be viewed as encoding the data by a model : the data are source words to be coded , and models are code words for the data . as before , the set of possible data is @xmath1008 .",
    "let @xmath1009 denote the set of non - negative real numbers .",
    "for every model class @xmath185 ( particular set of code words ) we choose an appropriate recursive function @xmath1010 defining the _ distortion _",
    "@xmath707 between data @xmath174 and model @xmath706 .",
    "[ rem : lossy ] the choice of distortion function is a selection of which aspects of the data are relevant , or meaningful , and which aspects are irrelevant ( noise ) .",
    "we can think of the distortion - rate function as measuring how far the model at each bit - rate falls short in representing the data .",
    "distortion - rate theory underpins the practice of lossy compression .",
    "for example , lossy compression of a sound file gives as `` model '' the compressed file where , among others , the very high and very low inaudible frequencies have been suppressed .",
    "thus , the rate - distortion function will penalize the deletion of the inaudible frequencies but lightly because they are not relevant for the auditory experience .",
    "but in the traditional distortion - rate approach , we average twice : once because we consider a sequence of outcomes of @xmath275 instantiations of the same random variable , and once because we take the expectation over the sequences",
    ". essentially , the results deal with typical `` random '' data of certain simple distributions .",
    "this assumes that the data to a certain extent satisfy the behavior of repeated outcomes of a random source .",
    "kolmogorov @xcite :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ the probabilistic approach is natural in the theory of information transmission over communication channels carrying `` bulk '' information consisting of a large number of unrelated or weakly related messages obeying definite probabilistic laws . in this type of problem",
    "there is a harmless and ( in applied work ) deep - rooted tendency to mix up probabilities and frequencies within sufficiently long time sequence ( which is rigorously satisfied if it is assumed that `` mixing '' is sufficiently rapid ) . in practice ,",
    "for example , it can be assumed that finding the `` entropy '' of a flow of congratulatory telegrams and the channel `` capacity '' required for timely and undistorted transmission is validly represented by a probabilistic treatment even with the usual substitution of empirical frequencies for probabilities .",
    "if something goes wrong here , the problem lies with the vagueness of our ideas of the relationship between mathematical probabilities and real random events in general .",
    "but what real meaning is there , for example , in asking how much information is contained in `` war and peace '' ?",
    "is it reasonable to include the novel in the set of `` possible novels '' , or even to postulate some probability distribution for this set ? or",
    ", on the other hand , must we assume that the individual scenes in this book form a random sequence with `` stocahstic relations '' that damp out quite rapidly over a distance of several pages ?",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    currently , individual data arising in practice are submitted to analysis , for example sound or video files , where the assumption that they either consist of a large number of weakly related messages , or being an element of a set of possible messages that is susceptible to analysis , is clearly wrong .",
    "it is precisely the global related aspects of the data which we want to preserve under lossy compression .",
    "the rich versatility of the structure functions , that is , many different distortion - rate functions for different individual data , is all but obliterated in the averaging that goes on in the traditional distortion - rate function . in the structure function approach one focuses entirely on the stochastic properties of one data item .",
    "below we follow @xcite , where we developed a rate - distortion theory for individual data for general computable distortion measures , with as specific examples the ` kolmogorov ' distortion below , but also hamming distortion and euclidean distortion .",
    "this individual rate - distortion theory is summarized in sections  [ sec : rdrev ] and  [ sec : ssrev ] . in section  [ sec : esfb ] , theorem  [ thm.dresf ] .",
    "we connect this indivual rate - distortion theory to shannon s .",
    "we emphasize that the typical data items of i.i.d .",
    "distributed simple random variables , or simple ergodic stationary sources , which are the subject of theorem  [ thm.dresf ] , are generally unrelated to the higly globally structured data we want to analyze using our new rate - distortion theory for individual data . from the prespective of lossy compression , the typical data have the characteristics of random noise , and there is no significant `` meaning '' to be preserved under the lossy compression .",
    "rather , theorem  [ thm.dresf ] serves as a ` sanity check ' showing that in the special , simple case of repetitive probabilistic data , the new theory behaves essentially like shannon s probabilistic rate - distortion theory .",
    "[ ex.11 ] let us look at various model classes and distortion measures :    \\(i ) the set of models are the finite sets of finite binary strings .",
    "let @xmath279 and @xmath1011 .",
    "we define @xmath1012 if @xmath571 , and @xmath851 otherwise .",
    "\\(ii ) the set of models are the computable probability density functions @xmath31 mapping @xmath6 to @xmath1013 $ ] . we define @xmath1014 if @xmath1015 , and @xmath851 otherwise .",
    "\\(iii ) the set of models are the total recursive functions @xmath31 mapping @xmath6 to @xmath5 .",
    "we define @xmath1016 , and @xmath851 if no such @xmath480 exists .",
    "all of these model classes and accompanying distortions @xcite , together with the `` communication exchange '' models in @xcite , are loosely called _ kolmogorov _ models and distortion , since the graphs of their structure functions ( individual distortion - rate functions ) are all within a strip  of width logarithmic in the binary length of the data  of one another .",
    "if @xmath185 is a model class , then we consider _ distortion spheres _ of given radius @xmath211 centered on @xmath706 : @xmath1017 this way , every model class and distortion measure can be treated similarly to the canonical finite set case , which , however , is especially simple in that the radius not variable .",
    "that is , there is only one distortion sphere centered on a given finite set , namely the one with radius equal to the log - cardinality of that finite set .",
    "in fact , that distortion sphere equals the finite set on which it is centered .",
    "let @xmath185 be a model class and @xmath480 a distortion measure .",
    "since in our definition the distortion is recursive , given a model @xmath706 and diameter @xmath211 , the elements in the distortion sphere of diameter @xmath211 can be recursively enumerated from the distortion function . giving the index of any element @xmath9 in that enumeration we can find the element .",
    "hence , @xmath1018 .",
    "on the other hand , the vast majority of elements @xmath9 in the distortion sphere have complexity @xmath1019 since , for every constant @xmath19 , there are only @xmath1020 binary programs of length @xmath1021 available , and there are @xmath1022 elements to be described .",
    "we can now reason as in the similar case of finite set models . with data @xmath9 and @xmath1023 , if @xmath1024 , then @xmath9 belongs to every large majority of elements ( has the property represented by that majority ) of the distortion sphere @xmath1025 , provided that property is simple in the sense of having a description of low kolmogorov complexity .",
    "the _ randomness deficiency _ of @xmath9 with respect to model @xmath103 under distortion @xmath480 is defined as @xmath1026 data @xmath9 is _ typical _ for model @xmath706 ( and that model `` typical '' or `` best fitting '' for @xmath9 ) if @xmath1027    if @xmath9 is typical for a model @xmath103 , then the shortest way to effectively describe @xmath9 , given @xmath103 , takes about as many bits as the descriptions of the great majority of elements in a recursive enumeration of the distortion sphere .",
    "so there are no special simple properties that distinguish @xmath9 from the great majority of elements in the distortion sphere : they are all typical or random elements in the distortion sphere ( that is , with respect to the contemplated model ) .    continuing example  [ ex.11 ] by applying to different model classes :    \\(i ) _ finite sets : _ for finite set models @xmath280 , clearly @xmath1028 .",
    "together with we have that @xmath9 is typical for @xmath280 , and @xmath280 best fits @xmath9 , if the randomness deficiency according to satisfies @xmath1029 .",
    "\\(ii ) _ computable probability density functions : _ instead of the data - to - model code length @xmath966 for finite set models , we consider the data - to - model code length @xmath968 ( the shannon - fano code ) .",
    "the value @xmath968 measures how likely @xmath9 is under the hypothesis @xmath31 . for probability models",
    "@xmath31 , define the conditional complexity @xmath1030 as follows .",
    "say that a function @xmath331 approximates @xmath31 if @xmath1031 for every @xmath9 and every positive rational @xmath963 .",
    "then @xmath1032 is defined as the minimum length of a program that , given @xmath1033 and any function @xmath331 approximating @xmath31 as an oracle , prints @xmath9 .",
    "clearly @xmath1034 . together with",
    ", we have that @xmath9 is typical for @xmath31 , and @xmath31 best fits @xmath9 , if @xmath1035 .",
    "the right - hand side set condition is the same as @xmath1036 , and there can be only @xmath1037 such @xmath231 , since otherwise the total probability exceeds 1 .",
    "therefore , the requirement , and hence typicality , is implied by @xmath1038 .",
    "define the randomness deficiency by @xmath1039 altogether , a string @xmath9 is _",
    "typical for a distribution _ @xmath31 , or @xmath31 is the _ best fitting model _ for @xmath9 , if @xmath1040 . if @xmath1040 .",
    "\\(iii ) _ total recursive functions : _ in place of @xmath966 for finite set models we consider the data - to - model code length ( actually , the distortion @xmath1041 above ) @xmath1042 define the conditional complexity @xmath1043 as the minimum length of a program that , given @xmath1044 and an oracle for @xmath31 , prints @xmath9 .    clearly , @xmath1045 .",
    "together with , we have that @xmath9 is typical for @xmath31 , and @xmath31 best fits @xmath9 , if @xmath1046 .",
    "there are at most @xmath1047- many @xmath231 satisfying the set condition since @xmath1048 .",
    "therefore , the requirement , and hence typicality , is implied by @xmath1049 .",
    "define the randomness deficiency by @xmath1050 altogether , a string @xmath9 is _",
    "typical for a total recursive function _",
    "@xmath31 , and @xmath31 is the _ best fitting recursive function model _ for @xmath9 if @xmath1040 , or written differently , @xmath1051 note that since @xmath1044 is given as conditional information , with @xmath1052 and @xmath1053 , the quantity @xmath1054 represents the number of bits in a shortest _ self - delimiting _ description of @xmath480 .",
    "we required @xmath1044 in the conditional in .",
    "this is the information about the radius of the distortion sphere centered on the model concerned .",
    "note that in the canonical finite set model case , as treated in @xcite , every model has a fixed radius which is explicitly provided by the model itself . but in the more general model classes of computable probability density functions , or total recursive functions , models can have a variable radius .",
    "there are subclasses of the more general models that have fixed radiuses ( like the finite set models ) .",
    "\\(i ) in the computable probability density functions one can think of the probabilities with a finite support , for example @xmath1055 for @xmath1056 , and @xmath1057 otherwise .",
    "\\(ii ) in the total recursive function case one can similarly think of functions with finite support , for example @xmath1058 for @xmath1059 , and @xmath1060 for @xmath1061 .",
    "the incorporation of the radius in the model will increase the complexity of the model , and hence of the minimal sufficient statistic below .      as with the probabilistic sufficient statistic ( section  [ sec : probstat ] ) ,",
    "a statistic is a function mapping the data to an element ( model ) in the contemplated model class . with some sloppiness of terminology",
    "we often call the function value ( the model ) also a statistic of the data .",
    "a statistic is called sufficient if the two - part description of the data by way of the model and the data - to - model code is as concise as the shortest one - part description of @xmath9 .",
    "consider a model class @xmath185 .",
    "a model @xmath706 is a _",
    "sufficient statistic _ for @xmath9 if @xmath1062    [ lem.v2 ] if @xmath103 is a sufficient statistic for @xmath9 , then @xmath1063 , that is , @xmath9 is typical for @xmath103 .",
    "we can rewrite @xmath1064 .",
    "the first three inequalities are straightforward and the last equality is by the assumption of sufficiency . altogether , the first sum equals the second sum , which implies the lemma .",
    "thus , if @xmath103 is a sufficient statistic for @xmath9 , then @xmath9 is a typical element for @xmath103 , and @xmath103 is the best fitting model for @xmath9 .",
    "note that the converse implication , `` typicality '' implies `` sufficiency , '' is not valid .",
    "sufficiency is a special type of typicality , where the model does not add significant information to the data , since the preceding proof shows @xmath1065 . using the symmetry of information",
    "this shows that @xmath1066 this means that :    \\(i ) a sufficient statistic @xmath103 is determined by the data in the sense that we need only an @xmath20-bit program , possibly depending on the data itself , to compute the model from the data .",
    "\\(ii ) for each model class and distortion there is a universal constant @xmath19 such that for every data item @xmath9 there are at most @xmath19 sufficient statistics .    _",
    "finite sets : _ for the model class of finite sets , a set @xmath280 is a sufficient statistic for data @xmath9 if @xmath1067    _ computable probability density functions : _ for the model class of computable probability density functions , a function @xmath31 is a sufficient statistic for data @xmath9 if @xmath1068 for the model class of _ total recursive functions _ , a function @xmath31 is a _ sufficient statistic _ for data @xmath9 if @xmath1069 following the above discussion , the meaningful information in @xmath9 is represented by @xmath31 ( the model ) in @xmath285 bits , and the meaningless information in @xmath9 is represented by @xmath480 ( the noise in the data ) with @xmath1053 in @xmath1070 bits . note that @xmath1071 , since the two - part code @xmath1072 for @xmath9 can not be shorter than the shortest one - part code of @xmath234 bits , and therefore the @xmath480-part must already be maximally compressed . by lemma  [ lem.v2 ] , @xmath1073 , @xmath9 is typical for @xmath31 , and hence @xmath1074 .",
    "we treat the relation between the expected value of @xmath892 , the expectation taken on a distribution @xmath312 of the random variable @xmath35 having outcome @xmath9 , and @xmath742 , for arbitrary random sources provided the probability mass function @xmath42 is recursive .",
    "[ thm.dresf ] let @xmath480 be a recursive distortion measure .",
    "given @xmath275 repetitions of a random variable @xmath35 with outcomes @xmath174 ( typically , @xmath1075 ) with probability @xmath42 , where @xmath31 is a total recursive function , we have @xmath1076 the expectations are taken over @xmath1077 where @xmath1078 is the outcome of the @xmath167th repetition of @xmath35 .",
    "as before , let @xmath1079 be @xmath275 independent identically distributed random variables on outcome space @xmath2 .",
    "let @xmath185 be a set of code words .",
    "we want to find a sequence of functions @xmath1080 so that the message @xmath1081 gives as much expected information about the sequence of outcomes @xmath1082 as is possible , under the constraint that the message takes at most @xmath1083 bits ( so that @xmath645 bits are allowed on average per outcome of @xmath716 ) . instead of @xmath1084 above",
    "write @xmath1085 .",
    "denote the cardinality of the range of @xmath1086 by @xmath1087 .",
    "consider distortion spheres @xmath1088 with @xmath1089 and @xmath1090 .",
    "_ left inequality : _ keeping the earlier notation , for @xmath275 i.i.d .",
    "random variables @xmath1091 , and extending @xmath31 to the @xmath275-fold cartesian product of @xmath57 , we obtain @xmath1092 . by definition of @xmath1093",
    "it equals the following expression in terms of a minimal canonical covering of @xmath1094 by disjoint nonempty spheres @xmath1095 ( @xmath1096 ) obtained from the possibly overlapping distortion spheres @xmath1097 as follows .",
    "every element @xmath1098 in the overlap between two or more spheres is assigned to the sphere with the smallest radius and removed from the other spheres .",
    "if there is more than one sphere of smallest radius , then we take the sphere of least index in the canonical covering .",
    "empty @xmath1099-spheres are removed from the @xmath1099-covering . if @xmath1100 , then @xmath1101 denotes @xmath1102 .",
    "now , we can rewrite @xmath1103 in the structure function setting we consider some individual data @xmath1098 residing in one of the covering spheres . given @xmath1104 and a program to compute @xmath31 and @xmath480",
    ", we can compute the covering spheres centers @xmath1105 , and radiuses @xmath1106 , and hence the @xmath1099-sphere canonical covering . in this covering",
    "we can identify every pair @xmath1107 by its index @xmath1108 .",
    "therefore , @xmath1109 ( @xmath1110 .",
    "for @xmath1111 we have @xmath1112 .",
    "therefore , @xmath1113 , the expectation taken over @xmath1114 for @xmath1115 .",
    "_ right inequality : _ consider a covering of @xmath1094 by the ( possibly overlapping ) distortion spheres @xmath1097 satisfying @xmath1116 , with @xmath19 an appropriate constant choosen so that the remainder of the argument goes through .",
    "if there are more than one spheres with different ( center , radius)-pairs representing the same subset of @xmath1094 , then we eliminate all of them except the one with the smallest radius .",
    "if there are more than one such spheres , then we only keep the one with the lexicographically least center . from this covering we obtain a canonical covering by nonempty disjoint spheres @xmath1117 similar to that in the previous paragraph , ( @xmath1096 ) .    for every @xmath1118",
    "there is a unique sphere @xmath1119 ( @xmath1096 ) .",
    "choose the constant @xmath19 above so that @xmath1120 .",
    "then , @xmath840 .",
    "moreover , by construction , if @xmath1117 is the sphere containing @xmath1098 , then @xmath1121 .",
    "define functions @xmath1122",
    ", @xmath1123 defined by @xmath1124 and @xmath1125 for @xmath1098 in the sphere @xmath1095 . then , @xmath1126 the distortion @xmath1127 achieves the minimum of the expression in right - hand side of . since @xmath1128 ,",
    "the cover in the right - hand side of is a possible partition satisfying the expression being minimized in the right - hand side of , and hence majorizes the minumum @xmath1129 .",
    "therefore , @xmath1130 .",
    "a sphere is a subset of @xmath1094 .",
    "the same subset may correspond to more than one spheres with different centers and radiuses : @xmath1131 with @xmath1132 .",
    "hence , @xmath1133 , but possibly @xmath1134 . however , in the proof we constructed the ordered sequence of @xmath1099 spheres such that every sphere uniquely corresponds to a ( center , radius)-pair . therefore , @xmath1135 .",
    "[ cor.esf ] it follows from the above theorem that , for a recursive distortion function @xmath480 : ( i ) @xmath1136 , for outcomes of a single repetition of random variable @xmath1137 with @xmath69 , the expectation taken over @xmath1138 ; and    \\(ii ) @xmath1139 for outcomes @xmath1140 of i.i.d .",
    "random variables @xmath1141 with @xmath1142 for @xmath1143 , the expectation taken over @xmath1144 ( the extension of @xmath31 to @xmath275 repetitions of @xmath35 ) .",
    "this is the sense in which the expected value of the structure function is asymptotically equal to the value of the distortion - rate function , for arbitrary computable distortion measures . in the structure function approach we dealt with only two model classes , finite sets and computable probability density functions , and the associated quantities to be minimized , the log - cardinality and the negative log - probability , respectively",
    ". translated into the distortion - rate setting , the models are code words and the minimalizable quantities are distortion measures . in @xcite",
    "we also investigate the model class of total recursive functions , and in @xcite the model class of communication protocols .",
    "the associated quantities to be minimized are then function arguments and communicated bits , respectively .",
    "all these models are equivalent up to logarithmic precision in argument and value of the corresponding structure functions , and hence their expectations are asymptotic to the distortion - rate functions of the related code - word set and distortion measure .",
    "we have compared shannon s and kolmogorov s theories of information , highlighting the various similarities and differences . some of this material can also be found in @xcite , the standard reference for shannon information theory , as well as @xcite , the standard reference for kolmogorov complexity theory .",
    "these books predate much of the recent material on the kolmogorov theory discussed in the present paper , such as @xcite ( section  [ sec : algmi ] ) , @xcite ( section  [ sect : minialg ] ) , @xcite ( section  [ sec : algsuf ] ) , @xcite ( section  [ sec : structure ] ) . the material in sections  [ sec : relpa ] and",
    "[ sec : esf ] has not been published before .",
    "the present paper summarizes these recent contributions and systematically compares them to the corresponding notions in shannon s theory .",
    "[ [ related - developments ] ] related developments : + + + + + + + + + + + + + + + + + + + + +    there are two major practical theories which have their roots in both shannon s and kolmogorov s notions of information : first , _ universal coding _ , briefly introduced in appendix  [",
    "sec : universal ] below , is a remarkably successful theory for practical lossless data compression .",
    "second , rissanen s _ minimum description length ( mdl ) principle _",
    "@xcite is a theory of inductive inference that is both practical and successful .",
    "note that direct practical application of shannon s theory is hampered by the typically untenable assumption of a true and known distribution generating the data .",
    "direct application of kolmogorov s theory is hampered by the noncomputability of kolmogorov complexity and the strictly asymptotic nature of the results .",
    "both universal coding ( of the individual sequence type , appendix  [ sec : universal ] ) and mdl seek to overcome both problems by restricting the description methods used to those corresponding to a set of probabilistic predictors ( thus making encodings and their lengths computable and nonasymptotic ) ; yet when applying these predictors , the assumption that any one of them generates the data is never actually made .",
    "interestingly , while in its current form mdl bases inference on universal codes , in recent work rissanen and co - workers have sought to found the principle on a restricted form of the algorithmic sufficient statistic and kolmogorov s structure function as discussed in section  [ sec : structure ] @xcite .    by looking at general types of prediction errors , of which",
    "codelengths are merely a special case , one achieves a generalization of the kolmogorov theory that goes by the name of _ predictive complexity _ , pioneered by vovk , vyugin , kalnishkan and others @xcite .",
    "finally , the notions of ` randomness deficiency ' and ` typical set ' that are central to the algorithmic sufficient statistic ( section  [ sec : algsuf ] ) are intimately related to the celebrated martin - lf - kolmogorov theory of _ randomness in individual sequences _ , an overview of which is given in @xcite .",
    "shannon s and kolmogorov s idea are not directly applicable to most actual data compression problems .",
    "shannon s theory is hampered by the typically untenable assumption of a true and known distribution generating the data .",
    "kolmogorov s theory is hampered by the noncomputability of kolmogorov complexity and the strictly asymptotic nature of the results .",
    "yet there is a middle ground that is feasible : _ universal codes _ that may be viewed as both an generalized version of shannon s , and a feasible approximation to kolmogorov s theory . in introducing the notion of universal coding kolmogorov",
    "says @xcite :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ `` a universal coding method that permits the transmission of any sufficiently long message [ of length @xmath58 ] in an alphabet of @xmath407 letters with no more @xmath1146 [ @xmath1147 is the empirical entropy ] binary digits is not necessarily excessively complex ; in particular , it is not essential to begin by determining the frequencies @xmath1148 for the entire message . ''",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    below we repeatedly use the coding concepts introduced in section  [ sec : coding ] .",
    "suppose we are given a recursive enumeration of prefix codes @xmath1149 .",
    "let @xmath1150 be the length functions associated with these codes .",
    "that is , @xmath1151 ; if there exists no @xmath103 with @xmath1152 , then @xmath1153 .",
    "we may encode @xmath9 by first encoding a natural number @xmath416 using the standard prefix code for the natural numbers .",
    "we then encode @xmath9 itself using the code @xmath1154 .",
    "this leads to a so - called _ two - part code _",
    "@xmath1155 with lengths @xmath1156 . by construction , this code is prefix and its lengths satisfy @xmath1157 let @xmath1158 be an infinite binary sequence and let @xmath1159 } \\in \\{0,1\\}^n$ ] be the initial @xmath58-bit segment of this sequence . since @xmath1160 , we have for all @xmath416 , all @xmath58 : @xmath1161 } ) \\leq   l_k(x_{[1:n ] } ) + o(\\log k).\\ ] ] recall that for each fixed @xmath1162 , the fraction of sequences of length @xmath58 that can be compressed by more than @xmath275 bits is less than @xmath599 .",
    "thus , typically , the codes @xmath1162 and the strings @xmath1159}$ ] will be such that @xmath1163})$ ] grows _ linearly _ with @xmath58 .",
    "this implies that for every @xmath1158 , the newly constructed @xmath1156 is ` almost as good ' as whatever code @xmath1154 in the list is best for that particular @xmath1158 : the difference in code lengths is bounded by a constant depending on @xmath416 but not on @xmath58 . in particular , for each infinite sequence @xmath1158 , for each fixed @xmath416 , @xmath1164})}{l_k(x_{[1:n ] } ) } \\leq 1.\\ ] ] a code satisfying ( [ eq : universal ] ) is called a _",
    "universal code _ relative to the _ comparison class _ of codes @xmath1165 .",
    "it is ` universal ' in the sense that it compresses every sequence essentially as well as the @xmath1154 that compresses that particular sequence the most . in general , there exist many types of codes that are universal : the 2-part universal code defined above is just one means of achieving ( [ eq : universal ] ) .      in most practically interesting cases",
    "we may assume that for all @xmath416 , the decoding function @xmath1154 is computable , i.e. there exists a prefix turing machine which for all @xmath1166 , when input @xmath1167 ( the prefix - free version of @xmath103 ) , outputs @xmath1168 and then halts .",
    "since such a program has finite length , we must have for all @xmath416 , @xmath1169 } ) ) = k(x_{[1:n ] } ) \\leq^+ l_k(x_{[1:n]})\\ ] ] where @xmath253 is the encoding function defined in section  [ sec : kolmogorov ] , with @xmath1170 . comparing with ( [ eq : universal ] )",
    "shows that the code @xmath255 with encoding function @xmath253 is a universal code relative to @xmath1171 .",
    "thus , we see that the kolmogorov complexity @xmath192 is just the length function of the universal code @xmath255 .",
    "note that @xmath255 is an example of a universal code that is not ( explicitly ) two - part .",
    "[ ex : universal ] let us create a universal two - part code that allows us to significantly compress all binary strings with frequency of 0 s deviating significantly from @xmath1172 .",
    "for @xmath1173 , let @xmath1174 be the code that assigns code words of equal ( minimum ) length to all strings of length @xmath58 with @xmath1175 zeroes , and no code words to any other strings .",
    "then @xmath1176 is a prefix - code and @xmath1177 .",
    "the universal two part code @xmath1155 relative to the set of codes @xmath1178 then achieves the following lengths ( to within 1 bit ) : for all @xmath58 , all @xmath1179 , all @xmath1159}$ ] with @xmath1175 zeroes , @xmath1161 } ) = \\log n + \\log n_0 + 2 \\log \\log n + 2 \\log \\log n_0 + \\log \\binom{n}{n_0 } = \\log   \\binom{n}{n_0 } + o(\\log n)\\ ] ] using stirling s approximation of the factorial , @xmath1180 , we find that @xmath1181 note that @xmath1182 , with equality iff @xmath1183 . therefore ,",
    "if the frequency deviates significantly from @xmath1172 , @xmath1155 compresses @xmath1159}$ ] by a factor linear in @xmath58 . in all such cases",
    ", @xmath255 compresses the data by at least the same linear factor .",
    "note that ( a ) each individual code @xmath1176 is capable of exploiting a particular type of regularity in a sequence to compress that sequence , ( b ) the universal code @xmath1155 may exploit _ many _ different types of regularities to compress a sequence , and ( c ) the code @xmath255 with lengths given by the kolmogorov complexity asymptotically exploits _ all _ computable regularities so as to maximally compress a sequence .",
    "if a random variable @xmath35 is distributed according to some known probability mass function @xmath312 , then the optimal ( in the average sense ) code to use is the shannon - fano code . but now suppose it is only known that @xmath1184 , where @xmath1185 is some given ( possibly very large , or even uncountable ) set of candidate distributions .",
    "now it is not clear what code is optimal .",
    "we may try the shannon - fano code for a particular @xmath1186 , but such a code will typically lead to very large expected code lengths if @xmath35 turns out to be distributed according to some @xmath1187 .",
    "we may ask whether there exists another code that is ` almost ' as good as the shannon - fano code for @xmath31 , no matter what @xmath1188 actually generates the sequence ? we now show that , provided @xmath1189 is finite or countable , then ( perhaps surprisingly ) , the answer is yes . to see this ,",
    "we need the notion of an _ sequential information source _ , section  [ sec : preliminaries ] .",
    "suppose then that @xmath1185 represents a finite or countable set of sequential information sources .",
    "thus , @xmath1190 and @xmath1191 represents a sequential information source , abbreviated to @xmath1192 . to each marginal distribution @xmath1193",
    ", there corresponds a unique shannon - fano code defined on the set @xmath57 with lengths @xmath1194 and decoding function @xmath1195 .",
    "let @xmath106 be a prefix - code assigning codeword @xmath1197 to source word @xmath1198 .",
    "the noiseless coding theorem  [ thm : noiseless ] asserts that the minimal average codeword length @xmath1199 among all such prefix - codes @xmath106 satisfies @xmath1200 the entropy @xmath1201 can therefore be interpreted as the expected code length of encoding the first @xmath58 bits generated by the source @xmath31 , when the optimal ( shannon - fano ) code is used .",
    "we look for a prefix code @xmath1155 with length function @xmath1156 that satisfies , for all fixed @xmath1202 : @xmath1203})}{h(f^{(n ) } ) } \\leq 1.\\ ] ] where @xmath1204 } ) = \\sum_{x \\in \\{0,1\\}^n } f^{(n)}(x)l(x)$ ] .",
    "define @xmath1155 as the following two - part code : first , @xmath58 is encoded using the standard prefix code for natural numbers . then , among all codes @xmath1195 , the @xmath416 that minimizes @xmath1205 is encoded ( again using the standard prefix code ) ; finally , @xmath9 is encoded in @xmath1205 bits .",
    "then for all @xmath58 , for all @xmath416 , for _ every _ sequence @xmath1159}$ ] , @xmath1206 } ) \\leq l_{\\langle n , k \\rangle}(x_{[1:n ] } ) + l_{\\cal    n}(k ) + l_{\\cal n}(n)\\ ] ] since ( [ eq : probuni ] ) holds for all strings of length @xmath58 , it must also hold in expectation for all possible distributions on strings of length @xmath58 .",
    "in particular , this gives , for all @xmath1207 , @xmath1208 } ) \\leq { \\bf e}_{f_k } l_{\\langle n , k    \\rangle}(x_{[1:n ] } ) + o(\\log",
    "n ) = h(f^{(n)}_k ) + o(\\log n),\\ ] ] from which ( [ eq : universalb ] ) follows .    historically , codes satisfying ( [ eq : universalb ] ) have been called _ universal codes _ relative to @xmath1185 ; codes satisfying ( [ eq : universal ] ) have been considered in the literature only much more recently and are usually called ` universal codes for individual sequences ' @xcite .",
    "the two - part code @xmath1155 that we just defined is universal both in an individual sequence and in an average sense : @xmath1155 achieves code lengths within a constant of that achieved by @xmath1209 for _ every individual sequence _ , for _ every _ @xmath1207 ; but @xmath1155 also achieves expected code lengths within a constant of the shannon - fano code for @xmath31 , for _ every _ @xmath1188 .",
    "note once again that the @xmath255 based on kolmogorov complexity does at least as well as @xmath1155 .",
    "[ ex : appy ] suppose our sequence is generated by independent tosses of a coin with bias @xmath157 of tossing `` head '' where @xmath1210 .",
    "identifying ` heads ' with @xmath137 , the probability of @xmath1211 outcomes `` 1 '' in an initial segment @xmath1159}$ ] is then @xmath1212 .",
    "let @xmath1185 be the set of corresponding information sources , containing one element for each @xmath1210 .",
    "@xmath1185 is an uncountable set ; nevertheless , a universal code for @xmath1185 exists .",
    "in fact , it can be shown that the code @xmath1155 with lengths ( [ eq : stirling ] ) in example  [ ex : universal ] is universal for @xmath1185 , i.e. it satisfies ( [ eq : universalb ] ) . the reason for this is ( roughly ) as follows : if data are generated by a coin with bias @xmath157 , then with probability @xmath137 , the frequency @xmath1213 converges to @xmath157 , so that , by ( [ eq : stirling ] ) , @xmath1214})$ ] tends to @xmath1215 .",
    "if we are interested in practical data - compression , then the assumption that the data are generated by a biased - coin source is very restricted .",
    "but there are much richer classes of distributions @xmath1185 for which we can formulate universal codes . for example",
    ", we can take @xmath1185 to be the class of all markov sources of each order ; here the probability that @xmath1216 may depend on arbitrarily many earlier outcomes .",
    "such ideas form the basis of most data compression schemes used in practice .",
    "codes which are universal for the class of all markov sources of each order and which encode and decode in real - time can easily be implemented .",
    "thus , while we can not find the shortest program that generates a particular sequence , it is often possible to effectively find the shortest encoding within a quite sophisticated class of codes .",
    "j. rissanen and i.  tabus .",
    "olmogorov s structure function in mdl theory and lossy data compression . in p.",
    "d. grnwald , i.  j. myung , and m.  a. pitt ( eds . ) , _ advances in minimum description length : theory and applications_. mit press , 2004 ."
  ],
  "abstract_text": [
    "<S> we compare the elementary theories of shannon information and kolmogorov complexity , the extent to which they have a common purpose , and where they are fundamentally different . </S>",
    "<S> we discuss and relate the basic notions of both theories : shannon entropy versus kolmogorov complexity , the relation of both to universal coding , shannon mutual information versus kolmogorov ( ` algorithmic ' ) mutual information , probabilistic sufficient statistic versus algorithmic sufficient statistic ( related to lossy compression in the shannon theory versus meaningful information in the kolmogorov theory ) , and rate distortion theory versus kolmogorov s structure function . </S>",
    "<S> part of the material has appeared in print before , scattered through various publications , but this is the first comprehensive systematic comparison . </S>",
    "<S> the last mentioned relations are new . </S>"
  ]
}