{
  "article_text": [
    "representing the information from the recent past as transient activity distributed over a network has been actively researched in biophysical as well as purely computational domains @xcite .",
    "it is understood that recurrent connections in the network can keep the information from distant past alive so that it can be recovered from the current state .",
    "the memory capacity of these networks are generally measured in terms of the accuracy of recovery of the past information @xcite .",
    "although the memory capacity strongly depends on the network s topology and sparsity @xcite , it can be significantly increased by exploiting any prior knowledge of the underlying structure of the encoded signal @xcite .",
    "our approach to encoding memory stems from a focus on its utility for future prediction , rather than on the accuracy of recovering the past .",
    "in particular we are interested in encoding time varying signals from the natural world into memory so as to optimize future prediction .",
    "it is well known that most natural signals exhibit scale free long range correlations @xcite . by exploiting this intrinsic structure underlying natural signals",
    ", prior work has shown that the predictive information contained in a finite sized memory system can be maximized if the past is encoded in a scale - invariantly coarse grained fashion @xcite .",
    "each node in such a memory system would represent a coarse grained average around a specific past moment , and the time window of coarse graining linearly scales with the past timescale .",
    "clearly the accuracy of information recovery in such a memory system degrades more for more distant past . in effect , the memory system sacrifices accuracy in order to represent information from very distant past , scaling exponentially with the network size @xcite .",
    "the predictive advantage of such a memory system comes from washing out non - predictive fluctuations from the distant past , whose accurate representation would have served very little in predicting the future .",
    "arguably , in the natural world filled with scale - free time varying signals , animals would have evolved to adopt such a memory system conducive for future predictions .",
    "this is indeed evident from animal and human behavioral studies that show that our memory for time involves scale invariant errors which linearly scale with the target timescale @xcite .",
    "our focus here is not to further emphasize the predictive advantage offered by a scale invariantly coarse grained memory system , rather we simply assume the utility of such a memory system and focus on the generic mechanism to construct it .",
    "one way to mechanistically construct such a memory system is to gradually encode information over real time as a laplace transform of the past and approximately invert it @xcite .",
    "the central result in this paper is that any mechanistic construction of such a memory system is simply equivalent to encoding linear combinations of laplace transformed past and their approximate inverses .",
    "this result should lay strong constraints on the connectivity structure of memory networks exhibiting the scale invariance property .",
    "we start with the basic requirement that different nodes in the memory system represents coarse grained averages about different past moments .",
    "irrespective of the connectivity , the nodes can be linearly arranged to reflect their monotonic relationship to the past time . rather than considering a network with a finite set of nodes , for analysis benefit , we consider a continuum limit where the information from the past time is smoothly projected on a spatial axis .",
    "the construction can later be discretized and implemented in a network with finite nodes to represent past information from timescales that exponentially scale with the network size .",
    "consider a real valued function @xmath0 observed over time @xmath1 . the aim is to encode this time - varying function into a spatially distributed representation in one dimension parametrized by @xmath2 , such that at any moment @xmath1 the entire past from @xmath3 to @xmath1 is represented in a coarse grained fashion as @xmath4 @xmath5 this is now a convolution memory model .",
    "the kernel @xmath6 is the coarse graining window function with normalized area for all @xmath2 , @xmath7 .",
    "different points on the spatial axis uniquely and monotonically represents coarse grained averages about different instants in the past , as illustrated in figure  [ cartoon ] .",
    "we require that coarse graining about any past instant linearly scales with the past timescale .",
    "so , for any pair of points @xmath8 and @xmath9 , there exists a scaling constant @xmath10 such that @xmath11 . for the window function to satisfy this scale - invariance property ,",
    "there should exist a monotonic mapping @xmath12 from a scaling variable @xmath13 to the spatial axis so that @xmath14 without loss of generality we shall pick @xmath15 because it can be retransformed to any other monotonic @xmath12 mapping after the analysis . hence with @xmath16 ,",
    "equation [ eq1 ] expresses the encoded memory as an integral over the entire past . however , the encoding mechanism can only have access to the instantaneous functional value of @xmath18 and its derivatives .",
    "the spatial pattern should self sufficiently evolve in real time to encode eq .",
    "this is a basic requirement to mechanistically construct @xmath4 in real time using any network architecture .",
    "since the spatial axis is organized monotonically to correspond to different past moments , only the local neighborhood of any point would affect its time evolution .",
    "so we postulate that the most general encoding mechanism that can yield eq .",
    "[ eq1 ] is a space - time local mechanism given by some differential equation for @xmath4 . to analyze this ,",
    "let us first express the general space - time derivative of @xmath4 by repeatedly differentiating eq .",
    "[ eq1 ] w.r.t @xmath1 and @xmath2 .",
    "@xmath19 } ( \\tau , s ) & = & \\sum_{j=0}^{n-1 } f^{[n - j-1 ] } ( \\tau ) { \\ensuremath{\\mathbf{w}}}^{[j]}_{(m)}(0,s ) \\nonumber \\\\ & + &   \\int_{-\\infty}^{\\tau } { \\ensuremath{{f } _ { } } } ( \\tau ' ) { \\ensuremath{\\mathbf{w}}}^{[n]}_{(m ) } ( \\tau-\\tau ' , s ) \\,\\",
    ", d \\tau '     .",
    "\\label{tderiv}\\end{aligned}\\ ] ] here @xmath20 and @xmath21 are positive integers . for brevity , we denote the order of time derivative within a square bracket in the superscript and the order of space derivative within a parenthesis in the subscript .",
    "since @xmath0 is an arbitrary input , @xmath4 should satisfy a time - independent differential equation which can depend on instantaneous time derivatives of @xmath0 . the first term in the r.h.s of eq .",
    "[ tderiv ] is time - local , while the second term involves an integral over the entire past . in order for the second term to be time - local",
    ", it must be expressible in terms of lower derivatives of @xmath4 .",
    "since the equation must hold for any @xmath0 , @xmath22}_{(m)}}(\\tau-\\tau',s)$ ] should satisfy a linear equation .",
    "@xmath23}_{(m)}}(\\tau-\\tau',s ) = 0   .",
    "\\label{eq : comb}\\ ] ]    the aim here is not to derive the time - local differential equation satisfied by @xmath4 , but just to impose its existence , which is achieved by imposing eq .",
    "[ eq : comb ] for some set of functions @xmath24 . to impose this condition ,",
    "let us first evaluate @xmath22}_{(m)}}(\\tau-\\tau',s)$ ] by exploiting the functional form of the window function given by eq .",
    "[ eqwindow ] . defining @xmath25 and the function @xmath26 , eq .",
    "[ eqwindow ] can be repeatedly differentiated to obtain    @xmath27}_{(m)}}(\\tau-\\tau',s ) =    \\sum_{r = r_o}^{m } ( n+1 ) !",
    "\\frac{s^{n+1-m+r}}{r ! ( m - r)!^2 }   ( \\tau-\\tau')^r g^{[n+r]}(z ) , \\ ] ]    where @xmath28 max[0 , @xmath29 and the superscript on @xmath30 represents the order of the derivative w.r.t @xmath31 .",
    "now eq .  [ eq : comb ] takes the form @xmath32}(z ) = 0   \\label{fin_cond}\\ ] ]    the above equation is not necessarily solvable for an arbitrary choice of @xmath24 .",
    "however , when it is is solvable , the separability of the variables @xmath2 and @xmath31 implies that the above equation will be separable into a set of linear differential equations for @xmath30 with coefficients given by integer powers of @xmath31 .",
    "the general solution for @xmath30 is then given by @xmath33 where @xmath34 and @xmath35 are non negative integers .",
    "the coefficients @xmath36 and @xmath37 , and the functions @xmath24 can not be independently chosen as they are constrained through eq .",
    "[ fin_cond ] .",
    "once a set of @xmath24 is chosen consistently with the coefficients @xmath36 and @xmath37 , the differential equation satisfied by @xmath4 can be obtained by iteratively substituting @xmath22}_{(m)}}(\\tau-\\tau',s)$ ] ( in the second term of the r.h.s of eq .",
    "[ tderiv ] ) in terms of its lower derivatives and replacing the integral in terms of derivatives of @xmath4 .    here",
    "we shall neither focus on the choice of @xmath24 nor on the differential equation for @xmath4 it yields .",
    "we shall only focus on the set of possible window functions that can be constructed by a space - time local mechanism .",
    "hence it suffices to note from the above derivation that the general form of such a window function is given by eq .",
    "[ final_sol ] . since by definition the window function at each @xmath2 coarse grains the input about some past moment",
    ", we expect it to be non - oscillatory and hence restrict our focus to real values of @xmath37 .",
    "further , the requirement of the window function to have normalized area at all @xmath2 restricts @xmath37 to be positive .",
    "let us consider the simplest window function , where only one of the coefficients in the set of @xmath36 and @xmath37 in eq .",
    "[ final_sol ] are non - zero , namely @xmath38 and @xmath39 .",
    "we shall denote the corresponding window function as @xmath40 to highlight its dependence on specific @xmath35 and @xmath41 .",
    "the most general window function is then simply a linear combination of various @xmath40 for different values of @xmath35 and @xmath41 . from eq .",
    "[ final_sol ] , @xmath40 takes the form    @xmath42    it turns out that the differential equation satisfied by @xmath4 that generates this window function is simply first order in both space and time given by @xmath43 } ( \\tau , s ) + bs { \\ensuremath{\\mathbf{t}_{(1 ) } } } ^{[0 ] } ( \\tau , s ) - \\frac{(k+1)}{s } { \\ensuremath{\\mathbf{t}_{(0 ) } } } ^{[1 ] } ( \\tau , s ) = 0 , \\label{diffeqt}\\ ] ] with a boundary condition @xmath44 .",
    "this equation can hence be evolved in real time by only relying on the instantaneous input @xmath0 at each moment @xmath1 .    for more complex window functions that are linear combinations of @xmath40 for various @xmath35 and @xmath41 , the order of the space and time derivatives of @xmath4 involved in the differential equation are not necessarily bounded when the parameters @xmath35 and @xmath41 involved in the linear combinations of @xmath40 are bounded .",
    "so , it is not straight forward to derive the mechanistic construction as a differential equation for @xmath4 .",
    "hence the question now is , what is the mechanism to construct a memory system with any window function ?",
    "interestingly , there exists an alternative derivation of eq .",
    "[ diffeqt ] where the time derivative and space derivative can be sequentially employed in a two step process @xcite .",
    "the first step is equivalent to encoding the laplace transform of the input @xmath0 as @xmath45 .",
    "the second step is equivalent to approximately inverting the laplace transformed input to construct @xmath4 .",
    "@xmath46    @xmath47}(\\tau , s ) & = & -bs{\\ensuremath{\\mathbf{f } _ { } } } ( \\tau , s ) + { \\ensuremath{{f } _ { } } } ( \\tau ) ,    \\label{smallt }   \\\\   { \\ensuremath{\\mathbf{t } _ { } } } ( \\tau , s ) & = & \\frac{b}{k ! } s^{k+1 } { \\ensuremath{\\mathbf{f}_{(k ) } } } ( \\tau , s ) .   \\label{bigt}\\end{aligned}\\ ] ]    taking @xmath0 to be a function of bounded variation and @xmath48 , eq .  [ smallt ] can be integrated to see that @xmath49 .",
    "thus @xmath45 is the laplace transform of the past input computed over real time .",
    "[ bigt ] is an approximation to inverse laplace transform operation @xcite .",
    "so @xmath4 essentially attempts to reconstruct the past input , such that at any @xmath2 , @xmath50 .",
    "this reconstruction grows more accurate as @xmath51 , and the input from each past moment is reliably represented at specific spatial location . for finite @xmath35 however , the reconstruction is fuzzy and each spatial location represents a coarse grained average of inputs from past moments , as characterized by the window function @xmath40 . for further details ,",
    "refer to @xcite .",
    "since any window function is a linear combination of various @xmath40 for different values of @xmath35 and @xmath41 , its construction is essentially equivalent to linear combinations of the two step process given by equations  [ smallt ] and [ bigt ] .    [ cols=\"^,^\",options=\"header \" , ]     , the window functions are plotted as a function of past time at the spatial point @xmath52.,scaledwidth=100.0% ]    the choice of the combinations of @xmath40 has strong implications on the shape of the resulting window function . at any given @xmath2",
    ", @xmath40 is a unimodal function with a peak at @xmath53 ( see eq .",
    "[ wkb ] ) .",
    "arbitrary combinations of @xmath40 could result in a spatial location representing the coarse grained average about disjoint past moments , leading to undesirable shapes of the window function .",
    "hence the values of @xmath35 and @xmath41 should be carefully tuned .",
    "figure [ wind ] shows the window functions constructed from four combinations of @xmath41 and @xmath35 .",
    "the combinations are chosen such that at the point @xmath52 , the window function coarse grains around a past time of @xmath54 .",
    "the scale invariance property guarantees that its shape remains identical at any other value of @xmath2 with a linear shift in the coarse graining timescale .",
    "comparing combinations 1 and 3 , note that the window function is narrower for larger @xmath35(=100 ) than for a smaller @xmath35(=8 ) .",
    "combination 2 has been chosen to illustrate a plateau shaped window function whose sides can be made arbitrarily vertical by fine tuning the combinations .",
    "combination 4 ( dotted curve in fig .  [ wind ] ) illustrates that combining different values of @xmath35 for the same @xmath41 will generally lead to a multimodal window function which would be an undesirable feature .",
    "a memory system represented on a continuous spatial axis is not practical , so the spatial axis should be discretized to finite points ( nodes ) .",
    "the two step process given by equations [ smallt ] and [ bigt ] is optimal for discretization particularly when the nodes are picked from a geometric progression in the values of @xmath2 @xcite .",
    "[ smallt ] implies that the activity of each node evolves independently of the others to construct @xmath45 with real time input @xmath0 .",
    "this is achieved with each node recurrently connected on to itself with an appropriate decay constant of @xmath55 .",
    "[ bigt ] involves taking the spatial derivative of order @xmath35 which can be approximated by the discretized derivative requiring linear combinations of activities from @xmath35 neighbors on either sides of any node . for further details on implementation of the two step process on discretized spatial axis ,",
    "refer to @xcite .     at two points @xmath56 and @xmath57 computed on a discretized spatial axis with @xmath58 .",
    "the dotted curves correspond to the window functions computed on the continuous spatial axis ( @xmath59 ) . ]    by choosing the nodes along the @xmath2-axis from a geometric progression , the error from the discretized spatial derivative will be uniformly spread over all timescales , hence such a discretization is ideal to preserve scale - invariance .",
    "let us choose the @xmath2-values of successive nodes to have a ratio @xmath60 , where @xmath61 .",
    "figure [ window_discrete ] shows the window function @xmath40 with @xmath62 and @xmath63 constructed from the discretized axis with @xmath58 .",
    "the window functions at two spatial points @xmath56 and @xmath57 are plotted to illustrate that scale invariance is preserved after discretization . as a comparison ,",
    "the dotted curves are plotted to show the corresponding window function constructed in the continuous @xmath2-axis ( limit @xmath59 ) .",
    "the window function computed on the discretized axis is artificially scaled up so that the solid and dotted curves in figure [ window_discrete ] are visually discernible .",
    "note that the discretized window function peaks farther in the past time and is wider than the window function on the continuous spatial axis .",
    "as @xmath59 , the discretized window function converges on to the window function constructed on the continuous axis , while for larger values of @xmath64 the discrepancy grows larger . nevertheless , for any value of @xmath64 , the discretized window function always stays scale - invariant , as can be seen by visually comparing the shapes of the window functions at @xmath8 and @xmath9 in figure  [ window_discrete ] .",
    "now , it is straight forward to construct scale - invariant window functions of different shapes by taking linear combinations of discretized @xmath40 , analogous to the construction in figure [ wind ] .    implementing this construction on a discretized spatial axis as a neural network has a tremendous resource conserving advantage . since at each @xmath2 , the window function @xmath40 coarse grains the input around a past time of @xmath65 , the maximum past timesscale represented by the memory system",
    "is inversely related to minimum value of @xmath2 .",
    "the geometric distribution of the @xmath2 values on the discretized axis implies that if there are @xmath66 nodes spanning the spatial axis for @xmath4 , it can represent the coarse grained past from timescales proportional to @xmath67 .",
    "hence exponentially distant past can be represented in a coarse grained fashion with linearly increasing resources .",
    "the formulation presented here starts from a convolution memory model ( eq .",
    "[ eq1 ] ) and derives the form of the scale - invariant window functions ( or the kernels ) that can be constructed from a space - time local mechanism .",
    "interestingly , by simply postulating a kernel of the form of eq .",
    "[ final_sol ] , tank and hopfield have demonstrated the utility of such a memory system in temporal pattern classification @xcite . in general , a convolution memory model can adopt an arbitrary kernel , but it can not be mechanistically constructed from a space - time local differential equation , which means a neural network implementation need not exist .",
    "however , the gamma - memory model @xcite shows that linear combinations of gamma kernels , functionally similar to eq .",
    "[ final_sol ] , can indeed be mechanistically constructed from a set of differential equations .",
    "the construction presented here takes a complementary approach to the gamma - memory model by requiring scale invariance of the window function in the forefront and then imposing a space - time local differential equation to derive it .",
    "this sheds light on the connectivity between neighboring spatial units of the network that is required to generate a scale invariant window function , as described by the second part of the two step process ( eq .  [ bigt ] ) .",
    "moreover , the linearity of the two step process and its equivalence to the laplace and inverse laplace transforms makes the memory representation analytically tractable .",
    "theoretically , the utility of a scale invariantly coarse grained memory hinges on the existence of scale free temporal fluctuations in the signals being encoded @xcite .",
    "although detailed empirical analysis of natural signals is needed to confirm this utility , preliminary analysis of time series from sunspots and global temperature show that such a memory system indeed has a higher predictive power than a conventional shift register @xcite .",
    "the predictive advantage of this memory system can be understood as arising from its intrinsic ability to wash out non - predictive stochastic fluctuations in the input signal from distant past and just represent the predictively relevant information in a condensed form .",
    "finally , the most noteworthy feature is that a memory system with @xmath66 nodes can represent information from exponentially past times proportional to @xmath67 . in comparison to a shift",
    "register with @xmath66 nodes which can accurately represent a maximum past time scale proportional to @xmath66 , this memory system is exponentially resource conserving .",
    "the work was partly funded by nsf bcs-1058937 and afosr fa9550 - 12 - 1 - 0369 .",
    "20ifxundefined [ 1 ] ifx#1 ifnum [ 1 ] # 1firstoftwo secondoftwo ifx [ 1 ] # 1firstoftwo secondoftwo `` `` # 1''''@noop [ 0]secondoftwosanitize@url [ 0 ]  + 12$12  & 12#1212_12%12@startlink[1]@endlink[0]@bib@innerbibempty @noop * * ,   ( ) @noop _ _ ,  ( ,  ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop _ _  ( ,  ,  ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * , ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( )"
  ],
  "abstract_text": [
    "<S> encoding temporal information from the recent past as spatially distributed activations is essential in order for the entire recent past to be simultaneously accessible . </S>",
    "<S> any biological or synthetic agent that relies on the past to predict / plan the future , would be endowed with such a spatially distributed temporal memory . </S>",
    "<S> simplistically , we would expect that resource limitations would demand the memory system to store only the most useful information for future prediction . for natural signals in real world which show scale free temporal fluctuations , the predictive information encoded in memory is maximal if the past information is scale invariantly coarse grained . here </S>",
    "<S> we examine the general mechanism to construct a scale invariantly coarse grained memory system . </S>",
    "<S> remarkably , the generic construction is equivalent to encoding the linear combinations of laplace transform of the past information and their approximated inverses . </S>",
    "<S> this reveals a fundamental construction constraint on memory networks that attempt to maximize predictive information storage relevant to the natural world . </S>"
  ]
}