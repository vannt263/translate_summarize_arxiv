{
  "article_text": [
    "the central aim of this paper is to establish , in the context of hypothesis testing with incomplete data , a  general framework for quantifying the amount of information in the observed data for a specific test being performed , relative to the full amount of information we would have had the data been complete .",
    "we do not address the issue of what is the best testing procedure , with or without the complete data , nor the issue of whether a full modeling / estimation strategy should or can be used instead .",
    "rather , we address an increasingly common practical problem where the investigator has chosen the testing procedure , but needs to know the impact of the missing data on the test in terms of the relative loss of information .",
    "such is the case in the genetic studies we briefly review in sections  [ sec : genet ] and [ sec : haplo ] .    besides the specific challenges listed in section  [ subsec : conf ] ,",
    "there are a number of general theoretical and methodological difficulties for establishing this general framework .",
    "first , unlike the similar task for estimation , where the notion of `` fraction of missing information '' is well studied and documented ( e.g. , @xcite ; @xcite ) , for hypothesis testing , there are two sets of measures to be contemplated , depending on whether the null hypothesis or the posited alternative model can be regarded as approximately adequate .",
    "indeed , this is the very question the hypothesis test aims to provide partial evidence to discriminate .",
    "second , hypothesis testing procedures , especially those of nonparametric or semiparametric nature , are often constructed without reference to a specific ( parametric ) model . however , without an explicit model to link the unobserved quantities with the observed data , the very task of measuring how much information we have missed is neither possible in general nor meaningful .",
    "it is known , though not widely ( e.g. , @xcite ; @xcite ) , that certain robust statistical procedures for estimation or testing can produce more efficient or powerful results with less data .",
    "consequently , without assuming that our testing procedure is optimal under a specified optimality criterion , we may end up with the seemingly paradoxical situation that additional data may make our procedure less efficient or powerful .",
    "that is , we may declare that more information is available with less data .",
    "third , in the context of small samples , quantifying information requires going beyond convenient and standard measures such as fisher information , which is essentially a large - sample measure .",
    "small - sample problems are rather frequent with incomplete data , as missing data reduce effective sample sizes . for the genetic studies we investigate in this paper ,",
    "the small - sample problems arise even when there appear to be ample amounts of data .",
    "for example , we are often interested in measuring information content in individual components ( e.g. , an individual family in a large linkage study ) . in haplotype association studies",
    ", we often test haplotypes individually  data size may be large enough for testing a common haplotype , but very small for a rare one . in addition , an individual person can be fully informative for one haplotype because we know s / he can not carry it , but much less so for another when we are uncertain whether s / he carries it or not .",
    "all these problems remind us that , in general scientific studies , small - sample problems appear more often than meets the eyes , namely , the numerical value of the sample size , because they sometimes appear in disguised forms .    given the complex nature of small - sample problems requiring information measures , we literally have spent several years in our quest of finding a general workable approach .",
    "not surprisingly , our conclusion is that robust bayesian methods hold more promise .",
    "as we propose in section  [ sec : small ] , after establishing a likelihood - based large - sample framework in section  [ sec : large ] , this problem can be dealt with by considering posterior measures of the flatness of the entire likelihood surfaces .",
    "however , the problem of specifying an appropriate `` default '' prior is challenging .",
    "we report both our promising findings and open problems , hoping to stimulate further development on this practically important and theoretically fascinating topic .",
    "we also discuss various interesting theoretical connections ( section  [ sec : theor ] ) , as well as further methodological work and applications ( section  [ sec : future ] ) .",
    "the central applied problem that motivated our work was the task to sensibly measure and efficiently compute the amount of information available in _ a particular genetic data set for a particular hypothesis tested by a particular statistical procedure_. all genome - wide linkage screens carried out on qualitative and quantitative traits as well as most of the association studies extract only part of the underlying information .",
    "missing information can be the result of different sources , such as absence of dna samples , missing genotypes , spacing between markers , noninformativeness of the markers , or unknown haplotype phase .",
    "investigators want to know how much information is available in the observed data for the purpose of the study _ relative _ to the amount of information that would have been available if the data were complete .",
    "the notion of complete data is problem specific and , in parametric inference , depends on the sufficient statistics ; for example , in linkage studies where the ibd ( identical by descent ) process is sufficient for inference , complete data can be achieved even if genotypes and/or individual samples are missing .",
    "measures of relative information are needed for designing follow - up strategies in linkage studies , for example , using more genetic markers with existing dna samples versus collecting dna samples from additional families . even for situations where we do not intend to recover the missing data , including situations where they",
    "can not possibly be recovered ( e.g. , dna samples from deceased ancestors ) , such measures can still be useful for the interpretation of the data and of the results , and for understanding the behavior of some of the inference tools ( e.g. , see section [ s4.5 ] ) .",
    "the key methodological challenge is to find a measure that ( 1 ) is a reliable index of the relative information specific to a study purpose , ( 2 ) conditions on particular data sets , ( 3 ) is robust in the sense of general applicability , including to small data sets , ( 4 ) is easy to compute and ( 5 ) is subject to meaningful combination axioms .",
    "the reliability criterion ( 1 ) is obvious , and the criterion ( 2 ) is necessary because typically an investigator is interested in measuring the relative information in the data set at hand , not with respect to some `` average '' data set .",
    "criterion ( 3 ) is desirable because in a typical genetic linkage study one needs to deal with a large amount of data with a variety of different complex structures ( e.g. , from a nuclear family to a very complex pedigree ) , often under time constraints , and thus it is not feasible to design separate measures to suit particular data structures .",
    "criterion ( 4 ) is needed for similar reasons  any method without suitable computational efficiency , regardless of its theoretical superiority ,",
    "will typically be ignored in routine genetic studies given the practical constraints .",
    "criterion ( 5 ) ensures certain desirable coherence to prevent paradoxical measure properties ( e.g. , more informative studies receive less weight in the combined index ) when combining studies .    to deal with all these criteria simultaneously",
    "requires a careful combination of bayesian and frequentist perspectives .",
    "some of the criteria [ e.g. , ( 1 ) and ( 2 ) ] are most easily handled from the bayesian perspective , and some [ e.g. , ( 5 ) ] are easier to satisfy with a frequentist criterion . with large samples , as it is typical , likelihood theory provides a rather satisfactory solution , as we demonstrate in section  [ sec : large ] . for small samples ,",
    "we have not been able to find a better alternative than to follow a robust bayesian perspective , which takes full advantage of the bayesian formulation in deriving information measures with desirable coherent properties , and at the same time it seeks measures that are robust to various misspecifications and are thus more generally applicable .",
    "we emphasize , however , that the computational burden associated to these bayesian measures should not be overlooked , even in this age of the mcmc revolution , for the reasons underlying criterion  ( 4 ) above . nevertheless , it is more principled and fruitful to seek ways to increase computational efficiency _",
    "after _ we establish theoretically sound measures .",
    "this is the route we follow .      for those who have no ( direct ) interest in genetic studies ,",
    "the following simple example may provide a stimulus to follow the methods developed in our paper .",
    "the example also provides some insights into a somewhat `` perplexing '' practical question when dealing with hypothesis testing in the presence of missing data : shall we impute under the null or not ?",
    "we emphasize that the purpose of this example is _ not _ to illustrate imputation methods .",
    "indeed , neither method discussed below can be recommended in general .",
    "rather , it shows how we can quantify relative information by measuring _",
    "how inaccurate is _ to erroneously treat imputations as if they were observed data .",
    "specifically , suppose @xmath0 are i.i.d .",
    "realizations of bernoulli@xmath1 , but only @xmath2 of them are actually observed .",
    "assuming that the missing data are missing completely at random ( @xcite ) , we can denote the observed data by @xmath3 . evidently , a simple large - sample test ( assuming @xmath4 is adequately large ) for @xmath5 is to refer the test statistic ( where the subscript `` ob '' stands for `` observed data '' ) @xmath6 to the null distribution @xmath7 , where @xmath8 is the average of the observed data .",
    "let us assume that the missing @xmath9 s were imputed using two mean - imputation methods .",
    "the firstmethod is to impute each missing @xmath9 by its mean , estimated by @xmath8 .",
    "the second procedure is to impute each missing @xmath9 by its mean assuming @xmath10 is true , that is , by @xmath11 .",
    "clearly , with either imputation , if we treat the imputed data as if they were observed and apply the test ( [ toytest ] ) with @xmath12 , we will not reach the valid conclusion unless we adjust the null distribution @xmath7 .",
    "for the first method , the average of all data , observed and imputed , is @xmath13 .",
    "therefore , if we erroneously treat the imputed values as real observations , we would compute our test statistics as @xmath14 where @xmath15 .",
    "in contrast , the second method would lead to @xmath16 because the average of all data , observed and imputed , is @xmath17 .",
    "two aspects of the above calculations are important .",
    "first , in both cases , the resulting `` completed - data '' test statistic is proportional to the benchmark given in ( [ toytest ] ) . consequently , imputing under the null or not leads to the same answer , as long as we adjust the corresponding null distribution accordingly ( the generality of this equivalence result obviously needs qualification , but the validity of a test is automatic when its null reference distribution is correctly specified ) .",
    "second , identities ( [ talte ] ) and ( [ tnull ] ) yield respectively @xmath18 the results in ( [ rform ] ) are important because @xmath15 measures the relative sample sizes , and hence the `` relative information '' in an i.i.d . setting . these results suggest that we consider measuring the relative information by how liberal the first imputation - based test is , when the imputations under the alternative are treated as real data , or how conservative the second test is , when the imputations under the null are treated as real observations .",
    "our general large - sample results given in section  [ sec : large ] show that these ideas are in fact general , once we replace the statistics in ( [ rform ] ) by their appropriate log - likelihood ratio counterparts ( recall the large - sample equivalence between log - likelihood ratio statistics and the wald statistics in a form similar to @xmath19 ) .",
    "readers who are not interested in genetic applications can go directly to section  [ sec : large ] , as sections  [ sec : genet ] and [ sec : haplo ] provide the necessary background on the genetic problems to which our methods will be applied .",
    "]    linkage refers to the co - inheritance of two markers or genes because they are located closely on the same chromosome .",
    "allele - sharing methods are part of linkage techniques for locating regions on the genome that are very likely to contain disease susceptibility genes ( e.g. , @xcite ) .",
    "the data usually consist of genotypes from a large number of markers ( polymorphic locations ) spread along the genome for individuals from  @xmath20 pedigrees .",
    "the allele - sharing methods focus on affected individuals , but genetic data on unaffected relatives are used to infer the inheritance patterns .",
    "alleles at the same locus in two individuals are said to be identical by descent ( ibd ) if they originate from the same chromosome , and are called identical by state ( ibs ) if they appear to be the same . for a given location on the genome , the evidence for a disease - susceptibility locus linked to it is given by the sharing of alleles ibd among affected relatives in excess of what is expected if the marker is not linked to a genetic risk factor .",
    "the simplest example of a data structure is the affected sib pair , as shown in figure [ fig : sibpair ] , where the left diagram shows a family with two affected brothers in which the parental information at a fixed locus is denoted by `` a1 '' and `` a2 '' for the father , and `` a3 '' and `` a4 '' for the mother .",
    "the siblings have one allele ibd  ( a2 ) which they inherited from their father , and different alleles inherited from their mother . in general , siblings share either two , one or no alleles ibd .",
    "unconditionally , each allele has probability @xmath21 to be transmitted ; this leads to a probability of @xmath22 , @xmath21 , @xmath22 for sharing zero , one , two alleles , respectively , identical by descent .",
    "conditioned on the affection status of the sibs , in the neighborhood of a disease gene , there is an expected increase in the number of alleles ibd across a collection of sib pairs ; statistical testing methods are often used to measure the strength of the evidence .    in general , the data are not as simple as in the above example .",
    "the pedigree structures can contain far more complicated relations than sib pairs and more than two affected individuals .",
    "most of the data sets extract only part of the underlying ibd information . in general ,",
    "the information is incomplete at locations between markers . even at marker locations ,",
    "a variety of factors can lead to missing information , including any genotype data on deceased or unavailable family members , missing genotypes in the typed individuals , or noninformativeness of the markers .",
    "the right diagram of  [ fig : sibpair ] illustrates a family where the parental allele information is missing , so even though the allele sharing among the sib pair appears to be identical in pattern with that of the left diagram , it is not known if the sibs share one or zero alleles ibd as the two `` a2 '' alleles might originate on different parental chromosomes .    in general ,",
    "the marker information of all the loci on the chromosome is used to calculate a probability distribution on the space of inheritance vectors . for locus",
    "@xmath23 and pedigree @xmath24 , an inheritance vector , @xmath25 , is a binary vector that specifies , for all the nonfounding members of the pedigree , which grand - parental alleles are inherited . under the assumption of no linkage ,",
    "all inheritance vectors are equally likely , which leads to a uniform prior distribution on their space . for a sib pair , the inheritance vector has four elements , one for each parent - child combination .",
    "for example , the first element specifies whether the allele inherited by the first sib from his father originates from the grandfather or grandmother . assuming no interference ( @xcite ) , a hidden markov model can be used to calculate the inheritance distribution conditional on the genotypes at all marker loci ( @xcite ) .",
    "the distribution of the inheritance vectors conditional on the observed data is the basis of the statistical inference , and it is used to determine the conditional distribution of the number of alleles ibd at a given location .      in order to summarize the evidence for linkage in a pedigree ,",
    "we can use a score @xmath26 ( @xcite ; @xcite ) , a measure of ibd sharing among the affected individuals at locus  @xmath23 . in general , @xmath26 is chosen such that it has a higher expected value under linkage than under no linkage .",
    "the standardized form of @xmath26 is @xmath27 , where @xmath28 and @xmath29 .",
    "the test is typically in the form of linear combination over the @xmath20 pedigrees , @xmath30 where @xmath31 are weights assigned to the individual families .",
    "the weights can be chosen according to the number of affecteds and the relationship among them and/or other covariate information .",
    "under the null hypothesis , @xmath32 has mean 0 and variance 1 .",
    "deviations from the null hypothesis can be tested using a @xmath7 approximation or the exact distribution of @xmath32 .    in general ,",
    "@xmath33 s are not directly observable due to missing information .",
    "a common practice is to impute / replace @xmath33 by @xmath34 to construct a test statistic ( @xcite ) , @xmath35 the main problem with this test statistic is the difficulty of directly evaluating its statistical significance",
    ". a standard @xmath7 approximation can be very inaccurate when there is a large amount of missing information , as can be seen from the following variance decomposition : @xmath36 \\\\[-8pt ] & & { } + \\mathrm{e}(\\operatorname{var}(z\\vert\\hbox{data } , h_0)\\vert h_0 ) , \\nonumber\\end{aligned}\\ ] ] which implies @xmath37 in many cases @xmath38 can be substantially less than 1 , leading to a conservative test when the @xmath7 approximation is used .",
    "a more accurate approach is described in section [ sec : exp ] .",
    "it is important to emphasize that , in allele - sharing studies , the amount of missing information can be made arbitrarily low , at least in theory , by increasing the number of markers in the region .",
    "that is why , in regions with evidence for linkage , it is important to predict whether by genotyping additional markers one will obtain a more significant deviation from the null .",
    "a different strategy for increasing the amount of information is to increase the sample size , that is , to collect dna samples from additional families .",
    "therefore knowing how much information is missing from the data is important for designing efficient follow - up strategies ( see also @xcite ) .",
    "the linkage methods we described are based on a chosen test statistic . in order to measure the relative information for a test statistic",
    ", we need to associate it with a model which specifies the stochastic relationship between the observed data and missing data beyond the null .",
    "otherwise the question of relative information is not well defined , as it is emphasized in  [ subsec : intro ] .",
    "it has been shown by @xcite that for every test statistic of the form of ( [ npl ] ) , a class of one - parameter models can be constructed such that the efficient score ( @xcite ) from each of the models gives asymptotically equivalent results to the given statistic .",
    "the inference procedures based on these models can be applied to any pedigree structure and missing data patterns .    as an illustration",
    ", we briefly describe the _ exponential tilting _",
    "model of @xcite applied to the one - locus allele - sharing statistic . a key assumption underlying this model ( and other models for associating tests ) is that the distribution of the inheritance vectors satisfies @xmath39 where @xmath40 is an inheritance vector for pedigree @xmath24 that leads to a standardized scoring function equal to @xmath41 , and @xmath42 denotes the alternative hypothesis .",
    "note that any time an investigator employs a test solely based on the @xmath32 s , as far as measuring information concerns , s / he is effectively assuming ( [ reduc ] ) regardless of whether or not s / he is aware of it .    under assumption ( [ reduc ] ) , it is sufficient to define the alternative models for @xmath33 s .",
    "the exponential tilting model has the form @xmath43 where @xmath44 is specified by the null ( i.e. , no linkage ) and @xmath45^{-1}$ ] is the renormalization constant .",
    "when @xmath32 is binary ( e.g. , as with half - sibs ) , the model is the same as the logistic regression model @xmath46 where @xmath47 .    given the exponential tilting model or other similar models ( e.g. , the linear model of @xcite ) , the log - likelihood can be calculated exactly for any missing data patterns under the assumption ( [ reduc ] ) .",
    "similar constructions can be done for multilocus models , as in @xcite .",
    "genetic association studies are designed to study potential associations between genetic variants and phenotypes ( i.e. , observable traits ) on a population scale .",
    "the association between the genotype at a given marker and a disease can appear because the genetic variant may be a risk factor for the disease , or because the variant may be strongly correlated , called _ in linkage disequilibrium _ ( ld ) in the genetics literature , with a causal locus .",
    "the magnitude of the correlation depends on many factors including the distance between the markers and the population history .",
    "for the simplicity of description , we focus here on a simple and popular design , case - control studies , although most results and principles are applicable to other sampling designs including those that incorporate quantitative traits and family - based controls . the simplest genetic variant and",
    "a commonly used genetic marker is a single nucleotide polymorphism ( snp ) that takes on only two possible alleles .",
    "denoting the two possible alleles as 1 and 2 , there are three possible genotypes @xmath48 , @xmath49 and @xmath50 .",
    "the data for a case - control study can then be summarized as a 2-by-3 table where the entries are counts of the three genotype categories for the cases and controls , respectively .",
    "the data can be further reduced to a 2-by-2 table , where the entries are counts of the alleles , if a multiplicative model ( @xcite ; @xcite ) for allele - risk is assumed .",
    "note that common assumptions , for a person randomly selected from the population , the two alleles carried are in hardy  weinberg equilibrium , that is , they are independent",
    ". this might not be true for an affected individual if the genotypes confer different risks , but it is true for the  multiplicative model . since this model is true under the null hypothesis which assumes no difference between the two alleles , assuming the multiplicative model for the purpose of testing does not affect the validity of the p - values .",
    "obviously the power could be reduced if the specified model is different from the true alternative .",
    "when the causal locus genotypes are not part of the data , or when the ld between the markers is strong , it might be more efficient to use more than one marker simultaneously .",
    "most of these multilocus approaches for fine - mapping of disease alleles are based on haplotypes ( e.g. , @xcite ; @xcite ; @xcite ; @xcite ; @xcite ) .",
    "haplotype analyses can be used to investigate untyped genetic variation ( @xcite ; @xcite ) , and can be used to explore which markers could be causal and which are unlikely to be so .",
    "haplotype _ is a sequence of alleles along a chromosome , and hence each person has two haplotypes .",
    "the alleles appearing in a haplotype are said to be in _",
    "phase_. if the haplotypes are directly observed , then standard methods for analyzing contingency tables could be used to test various models ( @xcite ) .",
    "possible scenarios range from having a candidate at - risk haplotype to testing the full model ( all the haplotypes have different risks ) versus the null model ( all the haplotypes have the same risk ) .      with a case - control study conducted with individual snps separately",
    ", the sufficient statistic is a 2-by-2 table under the multiplicative model and a likelihood ratio @xmath51 test can be used to test the null hypothesis . a common cause of incomplete information is missing genotypes since yield is often less than perfect .",
    "the situation becomes more complicated when multiple snps are considered jointly . with two snps ,",
    "both having alleles denoted with 1 and 2 , there are four possible haplotypes : 1 - 1 ( characterized by allele 1 at both snps ) , 1 - 2 , 2 - 1 and 2 - 2 .",
    "one simple alternative hypothesis is that haplotype 1 - 1 has risk that is different from the other three haplotypes which are assumed to have the same risk",
    ". it could be that we believe the two snps are functional and there is interaction between them that leads to increased disease risk for haplotype 1 - 1 , but more common is the hypothesis that the putative , but unobserved , mutation occurred in the 1 - 1 background and the association between the haplotype and the trait is a result of both being associated with the mutation .    under the multiplicative model ,",
    "if haplotypes can be observed directly , then this problem can again be reduced to a 2-by-2 table of haplotype counts where the haplotypes 1 - 2 , 2 - 1 and 2 - 2 are collapsed into one . however , for the commonly used technology , snps are genotyped separately . for an individual ,",
    "apart from incomplete information due to missing the genotype for one of the snps , there is the issue of uncertainty in phase .",
    "specifically , if the genotypes for the first and second snp are @xmath49 and @xmath49 respectively , then the two haplotypes could be either ( 1-@xmath52 - 2 ) or ( 1-@xmath53 - 1 ) , that is , the information on phase is missing . in general , there is incomplete information on phase if two or more snps that make up the haplotype are heterozygous . in family - based association studies ( e.g. , @xcite ; @xcite ; @xcite , @xcite ) , the data on relatives will provide additional information on phase but there will still be uncertainty in inferring the haplotypes . for snps that are close together physically , there exist typing technologies that can determine the haplotypes directly , but they are usually much more expensive . hence , from the design perspective",
    ", quantifying loss of information is relevant not only for power / sample - size calculations , but also for the choice of technology .",
    "apart from being relevant for experimental design and the interpretation of data , the amount of missing information is also useful for understanding the behavior of certain testing procedures .",
    "while one obvious way to perform testing is to apply a likelihood ratio test based on actual likelihoods computed for the observed incomplete data under the null hypothesis and alternative hypothesis separately , software for such calculations which allows the user to define models in a flexible manner is not readily available .",
    "however , available are methods and software based on the em algorithm that can be applied to one sample to calculate maximum likelihood estimates of haplotype frequencies and expected haplotype counts for individuals or groups assuming the maximum likelihood estimates are the true parameter values ( @xcite ; @xcite ; @xcite ) . other more sophisticated methods and software to predict haplotype phase and estimate counts also exist ( e.g. , @xcite ; @xcite ) .",
    "it is very tempting for the user to apply standard testing procedures , such as the likelihood ratio test , by simply treating these expected / predicted counts as the actual observed counts .",
    "doing this is analogous to the example in section  [ subsec : insig ] , except here we are dealing with a two - sample problem .",
    "specifically , if the original em computation is applied to the cases and controls jointly as a single group ( i.e. , as under the null ) , but with the expectation counts tabulated for the individuals who are then separated into cases and controls , the test is conservative . if , however , the em computation is applied to the cases and controls separately , then the result is anti - conservative .",
    "moreover , the degree of conservativeness with the first procedure , in large samples , matches the degree of anti - conservativeness of the second procedure . to be more specific ,",
    "consider the following simple example .",
    "suppose the observed data consist of 250 patients and 250 controls , or 500 chromosomes each .",
    "for a snp , the patient counts are 300 allele 1 and 200 allele 2 , and the control counts are 250 allele 1 and 250 allele 2 .",
    "let @xmath54 and @xmath55 denote respectively the population frequency of allele 1 in cases and controls . under the null , the maximum likelihood estimates are @xmath56 and the maximum likelihood estimates under the alternative are @xmath57 and @xmath58 .",
    "simple calculations show that the log - likelihood ratio @xmath51 statistic is @xmath59 = 10.12.\\ ] ] now suppose there are another 250 cases and 250 controls each with no data yet .",
    "suppose we treat these as missing data and apply the em computation to the cases and controls jointly .",
    "since @xmath60 , these extra cases and controls each have expected counts of 275 allele 1 and 225 allele 2 . together with the original counts , this gives 575 allele 1 and 425 allele 2 for the cases , and 525 allele 1 and 475 allele 2 for the controls .",
    "the log - likelihood ratio @xmath51 statistic computed based on these counts is 5.05 , approximately one - half of 10.12 .",
    "by contrast , suppose the expected counts for the missing data are computed for the cases and controls separately .",
    "in this case , the presumed counts are simply twice the original counts : 600 allele 1 and 400 allele 2 for the cases , and 500 allele 1 and 500 allele 2 for the controls .",
    "the log - likelihood ratio @xmath51 statistic computed from these counts is 20.24 , or exactly double that of 10.12 .",
    "while this example is extremely simple and unrealistic , the phenomenon seen does extend to real data with haplotypes .",
    "indeed , this is just another example of the relationships given in ( [ rform ] ) .",
    "that is , either ratio will correctly estimate that the relative information is about 50% .",
    "the theoretical results in the next section provide a general framework for such estimation .",
    "our large - sample framework is built upon a simple identity involving expected log - likelihood ratios , where the expectation is with respect to the conditional distribution of the missing data given the observed data .",
    "expected lod scores have also been used in the genetics literature to measure the information content of the data ( @xcite ) , and to investigate optimality and validity of analytic strategies ( e.g. , @xcite ; @xcite ; @xcite ) .",
    "note that lod stands for logarithm ( usually base 10 ) of the odds , and is used as a statistic for testing whether two loci are linked .",
    "specifically , let @xmath61 be the complete data and @xmath62 be the observed data ",
    "note that here @xmath62 is a function of @xmath61 .",
    "let @xmath63 be the log - likelihood of @xmath64 given data  @xmath65 .",
    "then for any @xmath66 and @xmath67 , @xmath68 \\nonumber\\\\[-8pt ] \\\\[-8pt ] & & \\qquad { } + [ \\log f(y_{\\mathrm{co}}|y_{\\mathrm{ob } } , \\theta_1 ) \\nonumber \\\\ & & \\hspace*{37pt } { } - \\log f(y_{\\mathrm{co}}|y_{\\mathrm{ob } } , \\theta _ 2 ) ] .",
    "\\nonumber\\end{aligned}\\ ] ] by taking conditional expectation with respect to @xmath69 , where @xmath64 is to be chosen , we have @xmath70 \\nonumber\\\\ & & \\quad = \\mathrm{lod}(\\theta _ 1 , \\theta _ 2|y_{\\mathrm{ob } } ) \\\\ & & \\qquad { } + \\mathrm{e } \\biggl[\\log{f(y_{\\mathrm{co}}|y_{\\mathrm{ob } } , \\theta _",
    "1)\\over f(y_{\\mathrm{co}}|y_{\\mathrm{ob } } , \\theta _ 2 ) }   \\bigg|y_{\\mathrm{ob } } , \\theta \\biggr ] , \\nonumber\\end{aligned}\\ ] ] where @xmath71 is the log of odds of @xmath66 over @xmath67 given data @xmath65 . here",
    "@xmath72 can be of any base , and lod is the log of the likelihood ratio , or more generally the log of posterior ratios .",
    "identity ( [ key ] ) is a simple extension of the key identity given in @xcite for the em algorithm .",
    "specifically , using the notation of dempster , laird and rubin ( @xcite ) @xmath73 \\quad \\mbox{and } \\quad \\nonumber\\\\[-8pt ] \\\\[-8pt ] h(\\theta |\\theta ' ) & = & \\mathrm{e}[\\log f(y_{\\mathrm{co}}| y_{\\mathrm{ob } } , \\theta ) |y_{\\mathrm{ob } } , \\theta ' ] , \\nonumber\\end{aligned}\\ ] ] identity ( [ key ] ) is the same as @xmath74 \\\\[-8pt ] & & \\quad = \\ell_{\\mathrm{ob}}(\\theta _ 1 ) - \\ell_{\\mathrm{ob}}(\\theta _ 2 ) + h(\\theta _",
    "1|\\theta ) -h(\\theta _ 2|\\theta ) , \\nonumber\\end{aligned}\\ ] ] where @xmath75 . in dempster , laird and rubin ( @xcite ) , ( [ eq : emex ] )",
    "was given with @xmath76 , and was the basis for establishing the celebrated monotone convergence property of the em algorithm .",
    "as we shall see , this intrinsic connection with the em algorithm not only helps greatly our theoretical development in section  [ sec : theor ] , but more importantly it enables us to compute our information measures directly from quantities that are already used for the em computation .    intuitively , if @xmath66 is the truth , then if we had more data , which would come from @xmath77 , we would on average have a larger lod score than @xmath78 .",
    "indeed , by taking @xmath79 in ( [ key ] ) we see @xmath80 where @xmath81 is the kullback  leibler information  in favor of @xmath66 against @xmath67 when @xmath66 is true ",
    "contained in the conditional distribution of @xmath61 given @xmath62 .",
    "the inequality in ( [ inealt ] ) becomes equality if and only if @xmath82 , which happens if and only if @xmath83 ( a.s . ) ; that is , given @xmath62 , the additional data would contain no information to discriminate @xmath67 from @xmath66 .",
    "the kullback ",
    "leibler distance has been used extensively in information theory ( e.g. , @xcite ) and mathematical statistics ( e.g. , @xcite ) .",
    "recent work on using k ",
    "l loss includes @xcite and references therein .",
    "similarly , if @xmath67 is the truth , then on average we would expect a smaller @xmath84 if we had observed @xmath61 .",
    "mathematically , this is shown by taking @xmath76 in ( [ key ] ) , which leads to @xmath85 \\nonumber \\\\ & & \\quad = \\mathrm{lod}(\\theta _ 1 , \\theta _ 2|y_{\\mathrm{ob } } ) - \\mathrm{kl}(\\theta _",
    "2\\dvtx \\theta _ 1 ) \\\\ & & \\quad \\le\\mathrm{lod}(\\theta _ 1,\\theta _ 2|y_{\\mathrm{ob } } ) , \\nonumber\\end{aligned}\\ ] ] and the inequality becomes equality if and only if , as before , @xmath86 .",
    "it is important to emphasize that because all the expectations above are conditional upon @xmath62 , it is legitimate to allow any of the @xmath64 s to depend on @xmath62 . in particular",
    ", the null value @xmath87 in the rest of this paper can be either a known fixed value when @xmath10 is a sharp null , or more generally the constrained mle of  @xmath64 from @xmath88 under the null .",
    "it is also important to emphasize that although in this section we focus on large - sample measures primarily because of their reliance on maximum likelihood estimators ( mles ) , as discussed below , all the equalities and inequalities discussed above do not involve any approximation , large sample or not .",
    "therefore all measures discussed below can also be very useful for small samples , as long as the mles can be trusted ( e.g. , a small - sample mle can have good properties , such as under the normal models ) .",
    "suppose the null value is @xmath87 and that the mle of @xmath64 ( under @xmath89 ) given @xmath62 is @xmath90 , and @xmath91 is used to assess the evidence against @xmath92 . to avoid technical complexity that is not of general interest for our proposals",
    ", we will assume ( i ) @xmath90 is unique , an assumption typically automatic with large samples , and ( ii ) @xmath93 , an assumption rarely , if ever , violated in practice .",
    "( nevertheless , for theoretical completeness , we will consider the case of @xmath94 in section  [ sec : theor ] via a limiting argument . ) then , if we intend to measure the information in the unobserved data for discrediting @xmath10 , under the large - sample assumption , a  natural thing to do is to treat @xmath90 as the `` truth , '' and measure the expected loss of @xmath95 in favor of @xmath90 relative to the expected complete - data @xmath95 score .",
    "namely , we can naturally define @xmath96 \\\\[-8pt ] & = & { \\ell_{\\mathrm{ob}}(\\theta _ { \\mathrm{ob } } ) -\\ell_{\\mathrm{ob } } ( \\theta _ 0 ) \\over q(\\theta _",
    "{ \\mathrm{ob}}|\\theta _ { \\mathrm{ob } } ) - q(\\theta _ 0|\\theta _ { \\mathrm{ob}})}. \\nonumber\\end{aligned}\\ ] ] the last expression shows that the computation of @xmath97 only requires evaluations , at @xmath98 and @xmath99 , of the observed - data log - likelihood @xmath100 and the @xmath101 function , where the latter is readily available from the em algorithm .    under assumptions ( i ) and ( ii ) , @xmath97 is welldefined and by ( [ inealt ] ) , @xmath102 .",
    "it is 1 if and only if @xmath103 , which means that the missing datacannot distinguish between @xmath90 and @xmath87 and thus there is no missing information given @xmath62 .",
    "it approaches 0 if and only if @xmath104 , which makes sense because if the observed - data likelihood has diminishing ability , relative to that of the missing - data model [ as measured by @xmath105 , to distinguish between @xmath90 and @xmath87 , then as far as providing evidence _ against _ @xmath10 , the missing information approaches 100% .",
    "one very appealing feature of @xmath97 is its direct interpretability .",
    "as seen in the haplotype example in section  [ subsec : hapo ] , a value of @xmath106 implies that if we had the complete data , the lod score would be expected to be twice @xmath107 as large .    when @xmath108 is linear in a ( multidimensional ) summary statistics ( i.e. , a complete - data sufficient statistics ) @xmath109 , as when the complete - data model is from an exponential family , @xmath110 can be written as @xmath111 and @xmath112 = \\mathrm{lod}(\\theta _ { \\mathrm{ob}},\\theta _ 0|s^*(y_{\\mathrm{ob}})),\\ ] ] where latexmath:[$s^*(y_{\\mathrm{ob}})= \\mathrm{e}[s(y_{\\mathrm{co } } )    @xmath97 measures the anti - conservativeness of the completed - data test by pretending that the actual value of the unobserved @xmath109 is the same as its imputation under the ( estimated ) alternative .",
    "therefore , @xmath97 is the general version of the first case in ( [ rform ] ) .",
    "this measure also has the following property when combining data sets .",
    "suppose @xmath114 are mutually independent and we define @xmath115 for each @xmath116 as in ( [ fracoa ] ) but using @xmath90 instead of individual @xmath117 ( i.e. , an mle based on @xmath118 ; then the overall @xmath119 is a weighted harmonic mean of @xmath115 s weighted by the individual lod score , @xmath120 , namely , @xmath121 however , the individual lod score , @xmath122 , is not necessarily always positive in practice , a problem that is closely related to the problem of defining relative measures for small data sets ( e.g. , for individual family ) , as discussed in section  [ sec : small ] . note that @xmath97 can also be expressed as weighted arithmetic mean of @xmath123 if we choose the weights to be proportional to the expected individual complete - data lod score @xmath124 $ ] : @xmath125 clearly ( [ eq : comb ] ) and ( [ eq : combl ] ) are equivalent , as long as .",
    "the harmonic rule ( [ eq : comb ] ) is somewhat more appealing because of the direct interpretation of the weight @xmath122 .",
    "inequality ( [ inenul ] ) also suggests a large - sample measure of the relative information under @xmath10 . by taking @xmath126 and @xmath127 in ( [ inenul ] )",
    "we obtain that @xmath128 thus , when the additional data are from @xmath129 , the expected complete lod score can not exceed the one based on the observed data , for any @xmath64 .",
    "we can use @xmath130 $ ] , which can not exceed @xmath131 by ( [ revi ] ) , as our best estimate of the complete - data lod score ; the use of a single point estimate of the complete - data lod score without considering its uncertainty can be justified under the large - sample assumption .",
    "consequently , we can define @xmath132 \\over\\mathrm{lod}(\\theta _ { \\mathrm{ob } } , \\theta _ 0|y_{\\mathrm{ob } } ) } \\nonumber\\\\[-8pt ] \\\\[-8pt ] & = & \\frac{\\max_{\\theta } [ q(\\theta |\\theta _ 0 ) - q(\\theta _",
    "0| \\theta_0)]}{\\ell_{\\mathrm{ob } } ( \\theta _ { \\mathrm{ob}})-\\ell_{\\mathrm{ob}}(\\theta _ 0)}. \\nonumber\\end{aligned}\\ ] ] the last expression shows again the computational efficiency of this measure because @xmath133 is the same as carrying out the e - step and m - step of an em algorithm , by pretending the previous iterated value is @xmath98 . however , we emphasize that the use of @xmath134 $ ] in our definition of @xmath135 instead of @xmath136 $ ] is not because this computation is easy , but rather because of the nature of the fundamental identity ( [ key ] ) , which requires we maximize the expected complete - data lod score .    like @xmath97 , @xmath137 . unlike @xmath97 , however , the investigation of when @xmath135 approaches one or zero is a more complicated matter , especially when the difference between @xmath90 and @xmath87 is large .",
    "this is a partial reflection of the fact that @xmath135 is defined under the assumption that the null hypothesis is ( approximately ) valid , which would be contradicted by a large value of @xmath138 , especially under our large - sample assumption .",
    "therefore , it is more sensible to investigate its theoretical properties when @xmath139 is small , in which case it is essentially equivalent to @xmath97 , as we will establish in section  [ sec : theor ] .",
    "nevertheless , it is useful to remark here that under the additional assumption that @xmath90 is the unique stationary point of @xmath100 , the numerator of @xmath135 is zero if and only if its denominator is zero , that is , if and only if @xmath140 .",
    "[ the `` if '' part of this result is a trivial consequence of ( [ revi ] ) . the `` only if '' part follows from the fact that if the numerator is zero , then @xmath87 is a maximizer of @xmath141 , which means that @xmath142 must also be a stationary point of @xmath100 by ( [ eq : meng ] ) in appendix  [ sa.2 ] . ]",
    "this demonstrates that in order for @xmath135 to be very small , the observed - data likelihood must suffer a diminishing ability to distinguish between @xmath90 and @xmath87 , just as with @xmath97 .    also as with @xmath97 , when @xmath108 is linear in @xmath109 , @xmath135 can be computed simply as @xmath143 where @xmath144 , that is , the mean imputation of the missing @xmath109 under the null .",
    "therefore , @xmath135 is the general version of the second case in  ( [ rform ] ) , and it measures the conservativeness of our test when we impute under the null .",
    "its main disadvantage , as previously mentioned , is that it can provide very misleading information when the true @xmath64 is far away from the null . on the other hand , because it is computed at the null , it is less sensitive , compared to @xmath97 , to possible misspecification of the alternative model .",
    "we will illustrate this in section [ subsec : finite ] , where we will discuss further the pros and cons of @xmath135 .     and",
    "@xmath135 ; the bottom curve ( dot - dashed ) corresponds to the entropy - based measure ( @xcite).[fig : niddm ] ]      in the context of allele - sharing methods , the measures we introduced in the previous sections are implemented in the software allegro ( @xcite ) , and are discussed in detail in @xcite . in figure [ fig : niddm ] , @xmath97 and @xmath135 are plotted for various locations along chromosome 22 ( the unit for the x - axis is centimorgans ) in a data set consisting of 127 pedigrees used in an inflammatory bowel disease study ( @xcite ) .",
    "it can be seen that , in this case , the two measures are very close across the entire chromosome .",
    "this happens because the sample size is large and the distribution of the family sharing scores is fairly symmetric",
    ". also plotted is an inheritance - vector - based information measure calculated by the software genehunter ( @xcite ) .",
    "this measure takes advantage of the fact that the inheritance vectors are equally likely under @xmath10 and that , for the fixed support of the space of the inheritance vectors , the shannon entropy ( @xcite ) is maximal for the uniform distribution on the support .",
    "for the @xmath24th pedigree in the study and a given position @xmath23 , it is defined as @xmath145 where @xmath40 was defined in section [ sec : gene ] .",
    "the definition of the overall information content of a data set is based on the global entropy , which , summed over all @xmath20 pedigrees , satisfies @xmath146 while @xmath147 has several desired properties ( e.g. , it is always between zero and one , and it is one when there is perfect data on the inheritance vectors ) , it has some deficiencies that make it unsuitable for the linkage application .",
    "the most fundamental problem is that it measures the relative information in the whole inheritance vector space , which could be very different from what is available for a particular test statistic that is a function of the inheritance vectors .",
    "for example , in the right diagram of figure  [ fig : sibpair ] , we may be nearly certain , and hence suffer very little missing information , that the ibs sharing is actually ibd if we have the knowledge that the allele `` a2 '' has very low population frequency , even though the parental alleles are unknown and therefore @xmath147 is low ( see @xcite , for more details ) .",
    "it is also possible that @xmath147 is higher than the measures described in this paper ( e.g. , @xcite ) , for example in situations where there is a lot of data on unaffected individuals in a family , but little or no data on affected individuals .",
    "in these cases , @xmath147 will capture available information that is not directly of interest when we are performing affecteds - only analyses .",
    "the serious overestimation or underestimation of relative information can have a great impact on the design of follow - up studies .",
    "one can decide on increasing the marker density if the relative information is low , as opposed to increasing the sample size .",
    "both strategies are expensive , and therefore deciding what is the most efficient design is of great importance in practice . for example , at the global mode in figure  [ fig : niddm ] , our measures indicate that we have about @xmath148 relative information , implying that potentially we can increase the lod score by only about @xmath149 ( @xmath150 ) if we add markers to make the ibd process approximately known ( assuming the value of @xmath90 remains approximately the same with the additional data ) .",
    "on the other hand , the entropy - based measure from genehunter indicates that we have about 70% information , suggesting that a more substantial gain ( over @xmath151 ) is possible by increasing the density of the markers .",
    "therefore these two approaches of measuring information are likely to lead to different strategies in allocating the resources , but evidently , in this example , it is unlikely the test results will change significantly by adding more markers near the location at the global mode .      in @xcite , the gene _",
    "tcf7l2 _ was found to be associated with type-2 diabetes . in particular , allele",
    "t of _ rs7903146 _ ( snp402 ) and allele x of a microsatellite marker dg10s478 are both associated with elevated risk of type-2 diabetes ( @xmath152-value@xmath153 ) .",
    "allele t and allele x are substantially correlated ( @xmath154 ) and their effects could not be clearly distinguished from each other in the original study . however , with additional data ( @xcite ) , it became clear that allele t is more strongly associated with diabetes than allele x. snp402 has alleles t and c , and dg10s478 has alleles x and 0 .",
    "jointly there are four haplotypes : tx , cx , t0 and c0 .",
    "figure [ fig : stroke ] presents pairwise comparisons of these four haplotypes .",
    "data are from 1021 patients ( @xmath155 chromosomes ) and 4273 controls ( @xmath156 chromosomes ) .",
    "consistent with the single marker associations , haplotype tx is found to have elevated risk relative to c0 . to distinguish between the effects of alleles t and x ,",
    "haplotype t0 is found to confer risk that is similar to that of tx and has significantly higher risk than c0 .",
    "by contrast , haplotype cx is found to have risk similar to that of c0 and significantly lower risk than tx . in other words ,",
    "given snp402 , dg10s478 does not appear to provide extra information about diabetes , which keeps snp402 as a strong candidate for being the functional variant .",
    "the yield of the genotypes is not perfect .",
    "each subject has genotypes for at least one of the two markers , but about 3.5% of the genotypes are missing .",
    "this together with uncertainty in phase leads to the incomplete information summarized in figure [ fig : stroke ] .",
    "interestingly , while the same data are used for the six pairwise comparisons , the fraction of missing information can be quite different .",
    "most striking is that the relative information for the test of tx versus c0 is very close to 100% , while the other tests all have more substantial missing information .",
    "we explore some of the reasons below .",
    "notice that t is highly correlated with x and c highly correlated with 0 .",
    "as a consequence , tx and c0 are much more common than t0 and cx .",
    "consider a subject whose genotype for d10gs478 is missing .",
    "here we can think of his two alleles for snp402 one at a time .",
    "given an observed allele t , it is clear that the haplotype is not c0 and quite likely to be tx .",
    "hence , even though incomplete , there is still substantial information provided for the test of tx versus c0 .",
    "by contrast , we know that this chromosome is useful for the test of tx against t0 , but with the allele of dg10s478 missing , that information is completely lost .",
    "even more interesting is that , if the observed allele is c instead , then this haplotype is completely uninformative for the test of tx versus t0 , that is , there is actually no information here whether or not we know the corresponding dg10s478 allele . in effect , the genotype of snp402 is an ancillary statistic for the test of tx against t0 ( or cx against c0 ) .",
    "it tells us how much information we can get from this individual assuming that we have no missing data , but by itself does not provide any information for the test . moreover , if the test of tx versus t0 is of key interest , then effort to fill up missing genotypes for dg10s478 should be focused on those individuals that are t / t homozygous for snp402 .",
    "when genotypes of both markers are observed , uncertainty in phase only exists for those individuals that are doubly heterozygous , that is , having genotypes c / t and 0/x .",
    "such an individual either has haplotypes c0/tx ( scenario i ) or cx / t0 ( scenario ii ) .",
    "scenario ii provides no information for the test of tx versus c0 .",
    "scenario i does contribute something to the test , but by providing a count of 1 to both tx and c0 , its impact on the test of tx versus c0 is rather limited .",
    "by contrast , for the test of tx versus t0 , scenario i adds a count of 1 to tx while scenario ii adds a count to t0 .",
    "hence , uncertainty in phase has a much bigger impact on the test of tx versus t0 than the test of tx versus c0 .",
    "this example , therefore , illustrates clearly the importance of measuring _ test - specific _ relative information .",
    "the measures defined in previous sections do not necessarily work with small samples ( e.g. , data for one family ) because they rely on the ability of the mle to summarize the whole likelihood function .",
    "the bayesian approach becomes a valuable tool in such settings even if we do not necessarily have a reliable prior ; we can first construct a coherent measure and then investigate the choice of prior . since a likelihood quantifies the information in the data through its ability of distinguishing different values of the parameter , it is natural to consider measuring the relative information by comparing how the observed - data likelihood deviates from `` flatness '' relative to the same deviation in the complete - data likelihood . the bayesian method is ideal here because we need to assess the change in this deviation due to the joint variability in the missing data and in the parameter .",
    "a reasonable measure of this deviation , conditioning on @xmath62 , is the posterior variance of the likelihood ratio ( lr ) .",
    "this measure is appealing because it is naturally scaled via the equality @xmath157 } { \\operatorname{var } [ \\mathrm{lr}(\\theta _ 0 , \\theta| y_{\\mathrm{co}})|y_{\\mathrm{ob } } ] } \\le1,\\ ] ] where @xmath158 indexes the underlying prior on @xmath159 used by  ( [ ripi ] ) , and @xmath160 stands for `` bayes information . ''",
    "we assume here that the complete - data likelihood surface is not flat , as otherwise the model is of little interest .",
    "the denominator in ( [ ripi ] ) is therefore positive .",
    "we also need to assume that the posterior variances of the two likelihood ratios are finite .",
    "this second assumption can be violated in practice , but a second measure we will propose below essentially circumvents this problem .    in the presence of nuisance parameters ( under the null ) , there is also a subtle issue regarding the nuisance part of @xmath87 , in the definition of @xmath161 . for a full bayesian calculation ,",
    "one should leave it unspecified and average it over in the posterior calculation , just as with the @xmath64 in @xmath162 . on the other hand , to be consistent with the large - sample measures as defined in section  [ sec : large ] , we can fix the nuisance parameter part in @xmath87 by its observed - data mle under the null . identity  ( [ ratio ] )",
    "still holds with such a `` fix , '' because the calculation there conditions on the observed data .",
    "this `` fix '' may seem to be rather ad hoc from a pure bayesian point of view .",
    "however , it can be viewed as an attempt in capturing the dependence ( if any ) between the parameter of interest and the nuisance parameter under the null , a dependence that is ignored by a single prior on the nuisance parameter regardless of the null .",
    "this subtle issue is related to the difference between `` estimation prior '' and `` hypothesis testing prior , '' an issue we will explore in subsequent work . here we just note that all the bayesian measures defined in this section can be constructed with either approach for handling the nuisance parameter under the null , although those under shrinking prior toward the null ( see section  [ subsec : shrink ] ) are most easily obtained when the nuisance parameter under the null is fixed at its mle ( or some other known values ) .    with either approach , @xmath163 |y_{\\mathrm{ob } } \\ } = 0,\\end{aligned}\\ ] ] which occurs",
    "if and only if for almost all the @xmath64 in the support of the posterior , the complete - data likelihood @xmath164 is ( almost surely ) a constant as a function of the missing data , and thus the missing data would offer no additional help in distinguishing @xmath64 from  @xmath142 . on the other hand , @xmath165 if and only if the observed - data likelihood ratio is a constant , and thus there is no information in the observed data for testing @xmath10 using @xmath166 .",
    "other characteristics of this measure depend on the choice of the prior @xmath158 , and they will be discussed in the following sections .    one potential drawback of @xmath161 is that it can be greatly affected by the large variability in the likelihood ratios , as functions of the parameters , for example , when very unlikely parameter values were given nontrivial prior mass .",
    "this problem can be circumvented to a large extent by using the posterior variance of the _ log - likelihood ratio _ , @xmath167\\ ] ] is equal to zero if and only if there is no additional information in the missing data for testing @xmath10 .",
    "these suggest that we can also measure the relative information by @xmath168 \\nonumber \\\\ & & { } \\cdot \\biggl(\\operatorname{var } [ \\mathrm{lod}(\\theta , \\theta _",
    "0| y_{\\mathrm{ob}})|y_{\\mathrm{ob } } ] \\\\ & & \\hspace*{13pt } { } + \\operatorname{var } \\biggl[\\log{\\frac{p(y_{\\mathrm{co}}| y_{\\mathrm{ob}},\\theta ) } { p(y_{\\mathrm{co}}|y_{\\mathrm{ob } } , \\theta_0)}}\\big|y_{\\mathrm{ob } } \\biggr]\\biggr)^{-1},\\hspace*{-4pt } \\nonumber\\end{aligned}\\ ] ] where , as for @xmath161 , @xmath158 indexes the underlying prior on  @xmath159 .",
    "although the use of lod is more natural in view of the large - sample measures given in section  [ sec : large ] , it does not admit the nice `` coherence '' identity for the likelihood ratio as given in ( [ ratio ] ) .",
    "indeed , we had to remove ad hoc a cross term in the denominator of ( [ bayes - ri ] ) in order to keep the resulting ratio always inside the unit interval",
    ". furthermore , as we show in section  [ sec : theor ] , the use of the ratio scale , instead of log ratio , leads to a number of interesting identities between likelihood ratios and bayes factors , and it is more connected with some finite - sample measure of information in the literature . whereas such trade - offs need to be explored , our general results in the next section imply that in the neighborhood of @xmath87 , the differences between these two measures should be small .",
    "given their definitions , the immediate question is how to choose @xmath158 and how to compute @xmath161 and @xmath169 efficiently since , in general , their calculations require integrations that can not be performed analytically . when the truth is believed to be in a neighborhood of the null value @xmath87 , a @xmath87-neighbor approximation to @xmath161 and @xmath169",
    "can be obtained by choosing @xmath158 to be @xmath170 with @xmath171 small .",
    "it is proved in appendix  [ sa.1 ] that the two bayesian measures have the same limit as @xmath172 , denoted by @xmath173 , @xmath174 \\\\[-8pt ] & = & \\frac { s^2(\\theta _ 0|y_{\\mathrm{ob } } ) } { s^2(\\theta _ 0|y_{\\mathrm{ob } } ) + i_{\\mathrm{mi } } ( \\theta_0|y_{\\mathrm{ob } } ) } , \\nonumber\\end{aligned}\\ ] ] where @xmath175 and @xmath176 are respectively the observed - data and complete - data score function , and @xmath177 is the expected ( missing ) fisher information from @xmath178 .",
    "note that although this result obviously assumes @xmath64 is univariate , it can also be applied when only the parameter of interest is univariate , if we fix the nuisance parameter part in @xmath87 at its observed - data mle under the null .    for the exponential tilting linkage model",
    ", one can verify that @xmath179 \\\\[-8pt ] & = & 1-\\frac{\\operatorname{var}(z|\\hbox{data } , h_0 ) } { w^2 + \\operatorname{var}(z|\\hbox{data } , h_0 ) } , \\nonumber\\end{aligned}\\ ] ] where @xmath180 , and @xmath32 is given in ( [ npl ] ) .",
    "therefore its computation is straightforward because it only depends on the test statistic and the null hypothesis .",
    "note also that the expectation of the denominator in ( [ eq : bisp ] ) under the null is simply @xmath181 .",
    "therefore , if we replace the denominator in ( [ eq : bisp ] ) by its expected value under the null , we obtain an even simpler approximation @xmath182 .",
    "however , @xmath173 measures only the relative information in the neighborhood of @xmath87 . for example , suppose the data consist of one affected sib - pair like in figure  [ fig : sibpair ] such that both parents and the sibs are heterozygous with the same pair of alleles at a specific locus ( i.e. , all individuals have the alleles `` a1 '' and `` a2 '' ) . in this case",
    ", the observed - data likelihood from the exponential tilting model is very informative away from @xmath87 ( see figure [ fig : dh - loglik ] ) , but @xmath183 because the null value @xmath184 turns out to be the _ minimizer _ of the observed - data likelihood .    ]    in general , whenever @xmath87 is a stationary point of @xmath88 , @xmath183 , even if there is almost perfect information .",
    "for example , if the data consist of @xmath185 sib - pairs such that there is complete information on @xmath186 sib - pairs , @xmath20 sharing 0 alleles ibd and @xmath20 sharing 2 alleles ibd , and one sib - pair has no information , then @xmath187 and thus @xmath183 .",
    "this is clearly a misleading measure . in the next section",
    "we propose a remedy for this problem .",
    "the measures defined in section  [ subsec : bayes ] are inherently small - sample quantities , for the variance terms used in these measures do not naturally admit additivity even for i.i.d .",
    "data structures . whether one can find a satisfying small - sample measure that would automatically admit such additivity is a topic of both theoretical and practical interest , but for our current purposes we can impose such additivity by defining global measures via appropriate combining rules , such as ( [ eq : comb ] ) .",
    "we adopt such rules mainly to maintain the continuity of moving from small - sample to large - sample measures as proposed in section  [ sec : large ] .",
    "whether these are the most sensible rules is a topic that requires further research .",
    "specifically , suppose our data consist of @xmath20 independent `` small units '' ( e.g. , individual families ) , @xmath188 .",
    "we apply ( [ ripi ] ) to each unit and then combine them via the harmonic rule ( [ eq : comb ] ) but with weights proportional to @xmath189 and then taking the ratio .",
    "that is , @xmath190 } { \\sum_{i=1}^n\\operatorname{var } [ \\mathrm{lr}(\\theta _ 0 , \\theta|y_{\\mathrm{co}}^{(i)})|y_{\\mathrm{ob}}^{(i ) } ] } \\nonumber\\\\[-8pt ] \\\\[-8pt ] & = & \\biggl\\{\\frac{\\sum_{i=1}^n v_i[\\mathcal{b}i^\\pi _ { 1,i}]^{-1 } } { \\sum_{i=1}^nv_i } \\biggr\\}^{-1}. \\nonumber\\end{aligned}\\ ] ]    similarly , we can define the combined version for @xmath191 from individual @xmath192 , and we can also use the arithmetic combining rule ( [ eq : combl ] ) .",
    "in addition , its limit under the shrinking prior , in analogy to ( [ eq : bisg ] ) , can be expressed as @xmath193 \\\\[-8pt ] & = & \\frac{\\sum_{i=1}^n s^2(\\theta _ 0|y^{(i)}_{\\mathrm{ob } } ) } { \\sum_{i=1}^n s^2(\\theta _ 0|y^{(i)}_{\\mathrm{ob } } ) + i_{\\mathrm{mi}}(\\theta_0|y_{\\mathrm{ob } } ) } , \\nonumber\\end{aligned}\\ ] ] where @xmath177 is the expected fisher information matrix from @xmath69 , with @xmath194 .",
    "we have changed the notation from @xmath173 to @xmath195 to signify the fact that the latter measure is defined by _ summing _ up the numerators and denominators of the individual @xmath173 s _ separately _ before forming the combined ratio .",
    "the second equation in ( [ eq : combss ] ) holds because of the additivity of fisher information for independent data structures . for the exponential tilting linkage model ,",
    "this averaging for a shrinking prior leads to @xmath196 where @xmath197 and @xmath198 .",
    "this is equal to zero only if all the @xmath199 s are equal to zero , as opposed to using a global posterior , that is , by applying ( [ eq : bisg ] ) directly to the whole data set , where @xmath200 is sufficient to cause @xmath183 .",
    "this difference is an important advantage for @xmath195 , as we will demonstrate in section  [ subsec : finite ] .    ; the dashed line corresponds to @xmath169 calculated using a uniform prior on @xmath201 ; the dot - dashed line corresponds to @xmath169 calculated using a uniform prior on @xmath202 ; the dotted line corresponds to @xmath169 calculated using a uniform prior on @xmath203.[fig : buc ] ]      to illustrate the proposed bayesian measures of information , we calculated them for various priors @xmath158 in a data set containing 21 ulcerative colitis ( uc ) families ( @xcite ) .",
    "the choices of priors here were made for investigating the sensitivity to prior specification , so they may not reflect our real knowledge about the problem ( e.g. , we generally expect @xmath159 to be nonnegative in such problems ) . in figure [ fig : buc ] the measure of information @xmath195 is plotted in comparison with @xmath169 , which is calculated as described in the previous section for three different priors .",
    "similar results are obtained using @xmath161 . in this example",
    "@xmath97 and @xmath195 are almost identical ; @xmath97 is therefore not shown .",
    "note that the value of the parameter under the null hypothesis of no linkage is equal to zero , and , for this data set , the maximum likelihood estimates for the linkage parameter across the chromosome vary between @xmath204 and  0.07.=-1    we note that the @xmath169 measure calculated using a uniform@xmath201 prior is very close to @xmath195 , which demonstrates the possibility of having very different priors that result in very similar measures . the bayesian measure calculated with a prior having a narrower support ,",
    "that is , uniform on the interval @xmath202 , follows the same patterns but is uniformly smaller .",
    "using a prior centered around the maximum likelihood estimate , uniform on the interval @xmath203 , turns out to be very misleading because it gives values that are considerably too small ( i.e. , in comparison with the large - sample estimates given in figure  [ fig : niddm ] ) .",
    "we emphasize that symmetric uniform priors were used in figure  [ fig : buc ] simply to demonstrate potential substantial sensitivity to prior specification , as one often expects less erratic behavior from such symmetric and smooth prior specifications .",
    "the issue of sensitivity to the choice of prior is further discussed in section  [ sec : future ] .",
    "as we discussed previously , a central difficulty in measuring the relative amount of information is that its value will generally depend on the true value of the unknown parameter .",
    "one way to explore this dependence is to replace @xmath90 in the definition of @xmath135 or @xmath97 by @xmath64 in a suitably defined neighborhood , and to plot it against @xmath64 in such a range to check its variability .",
    "the use of this type of _ relative information function _ was proposed in @xcite for the purpose of measuring the rate of convergence of em - type algorithms , where the function @xmath205 was termed _",
    "relative augmentation function_. note that @xmath97 is simply the value of this function at @xmath206 .",
    "for simplicity of presentation , we will assume in the following and section  [ subsec : fixed - size ] that @xmath64 is univariate , though all the results are generalizable to multivariate @xmath64 by employing appropriate matrix notation and operations .",
    "we also assume all the regularity conditions as in dempster , laird and rubin ( @xcite ) to guarantee the validity of taking differentiation under integration and for taylor expansions .",
    "it was shown in @xcite that as @xmath207 , @xmath208 approaches the so - called _ fraction of observed information _ for the purpose of estimation : @xmath209 where the observed , complete and missing fisher information are defined , as in dempster , laird and rubin ( @xcite ) , @xmath210\\label{eq : imis } \\\\[-8pt ] & = & \\mathrm{e } \\biggl[-\\frac{\\partial^2 \\log f(y_{\\mathrm{co}}|y_{\\mathrm{ob } } ; \\theta ) } { \\partial\\theta ^2 } \\bigg|y_{\\mathrm{ob } } ; \\theta\\biggr ] \\big\\vert_{\\theta = \\theta_{\\mathrm{ob } } } \\nonumber\\end{aligned}\\ ] ] and @xmath211 \\bigg\\vert_{\\theta = \\theta _ { \\mathrm{ob } } } \\\\ & = & i_{\\mathrm{ob}}+i_{\\mathrm{mi } } , \\nonumber\\end{aligned}\\ ] ] where the last identity is known as the `` missing - data principle , '' and is a directed consequence of ( [ eq : emex ] ) .",
    "the @xmath212 measure plays a key role in determining the rate of convergence of the em algorithm and its various extensions ( e.g. , @xcite ; @xcite , @xcite ; @xcite ; @xcite ) .    the above limiting result",
    "suggests that , when @xmath213 is small , we can study the behavior of @xmath97 via its connection to @xmath214 , as we demonstrate in the next section .",
    "however , among all the measures we proposed , the measure @xmath195 of ( [ eq : combss ] ) most closely resembles @xmath214 of ( [ eq : dlr ] ) .",
    "the main differences are the use of @xmath215 in place of @xmath216 , and the fact that the fisher information terms in @xmath214 are evaluated at @xmath217 , whereas for @xmath195 they are evaluated at @xmath98 .",
    "it is well known that , under regularity conditions , @xmath218 will converge to the expected fisher information under the null .",
    "consequently , under the null , @xmath195 and @xmath212 are asymptotically equivalent .",
    "this equivalence may suggest to directly define @xmath195 in terms of the `` observed fisher information at @xmath87 . '' however , although @xmath219 is guaranteed to be nonnegative ( definite ) when @xmath90 is in the interior of the parameter space @xmath220 , this is not necessarily true for @xmath221 .",
    "therefore , for small - sample problems for which the use of @xmath216 is inadequate ( e.g. , when the mle @xmath90 is on the boundary of @xmath220 ) , the direct substitution of @xmath216 by @xmath221 will not lead , in general , to a nonnegative measure .",
    "the @xmath195 measure this problem by using the sum of individual squared scores instead of @xmath221 , which guarantees that the resulting measure is inside the unit interval , and that it is consistent with @xmath212 for large samples .",
    "therefore @xmath195 can be viewed as a small - sample extension of @xmath212 in the neighborhood of the null .      for both @xmath97 and @xmath135 , their equivalence to @xmath212 in the neighborhood of @xmath87 can be established for finite - sample sizes .",
    "( therefore , @xmath212 can also be defined as the value of either @xmath97 or @xmath135 when @xmath94 . ) specifically , denote @xmath222 the @xmath223th derivative of @xmath100 at @xmath217 , and @xmath224 it is proved in appendix [ sa.2 ] that @xmath225 in deriving this result , we have utilized the following well - known identities in the literature of the em algorithm ( e.g. , dempster , laird and rubin , @xcite ; @xcite ) : @xmath226    under the assumption that @xmath141 has a unique maximizer as a function of @xmath64 , an assumption that is easily satisfied in most of the applications when em is useful , we also prove in appendix  [ sa.2 ] that @xmath227 \\\\[-8pt ] & & \\hspace*{15pt } { } - 2\\ell^{(3)}_{\\mathrm{ob } } - q^{(3,0)}_{\\mathrm{ob}}\\mathcal{r}i_e^2\\bigr ) ( 3i_{\\mathrm{co}})^{-1 } \\delta \\nonumber \\\\ & & { } + o(\\delta^2 ) .",
    "\\nonumber\\end{aligned}\\ ] ] these expansions are useful for comparing the first - order ( in @xmath139 ) behavior of @xmath97 and @xmath135 . for example , we suspect that , for many applications , @xmath135 is a conservative estimate of the actual relative information , where @xmath97 is a more accurate measure",
    ". one way to validate this or to identify situations where this conjecture is true is to compare the two coefficients of @xmath139 and to determine the appropriate conditions for @xmath228 to the first order in the neighborhood of @xmath87 ( away from the neighborhood the comparison is not very meaningful because @xmath135 can be seriously biased ) . due to the complex nature of these two coefficients , we only present in the next section a simple example to illustrate the conservatism of @xmath97 , and leave the general theoretical investigation to subsequent work .",
    "we also remark here that when the true @xmath64 is believed to be close to @xmath87 , a measure like @xmath135 can be used to construct reasonable bounds .",
    "for example , we can expect @xmath229 to be a reasonable lower bound and @xmath230 an upper bound for relative information , or we can use @xmath231 as a compromise . in future work , we intend to investigate the reliability and applicability of such bounds and compromise . here",
    "we simply note a computational advantage of @xmath232 that follows from @xmath233}{q(\\theta _",
    "{ \\mathrm{ob}}| \\theta _ { \\mathrm{ob}})-q(\\theta _ 0|\\theta _ { \\mathrm{ob } } ) } \\biggr]^{1/2},\\ ] ] which avoids entirely the calculation of the observed - data log - likelihood function @xmath100 , which is often harder to compute than the expected complete - data log - likelihood @xmath234 .",
    "furthermore , whenever @xmath97 and @xmath135 are close to each other , as in our real - data examples , @xmath232 will be practically the same as either @xmath97 or @xmath135 .",
    "let @xmath235 be i.i.d .",
    "samples from @xmath236 , where both @xmath237 and @xmath238 are unknown , and the null hypothesis is @xmath239 .",
    "suppose our observed data @xmath62 is a size-@xmath240 random sample of @xmath61 , where . then it should be clear that the relative information is @xmath241 by any reasonable argument .",
    "indeed , straightforward calculation shows @xmath242 regardless of the actual value of @xmath62 .",
    "however , @xmath243 \\nonumber\\\\[-8pt ] \\\\[-8pt ] & = & r -\\frac{r(1-r^2)}{2}\\frac{t_0 ^ 2}{m } + o \\biggl ( \\biggl(\\frac{t^2_0}{m } \\biggr)^2 \\biggr ) , \\nonumber\\end{aligned}\\ ] ] where @xmath244 , which differs from the usual @xmath23-statistic ( under the null ) only due to the use of mle for @xmath238 , @xmath245 , instead of the sample variance @xmath246 . from ( [ normal ] ) , it is clear that @xmath135 approaches  @xmath247 whenever @xmath248 is small , which implies that @xmath135 will recover ( reasonably ) the correct information when the null hypothesis is ( approximately ) correct .",
    "in contrast , for a fixed sample size @xmath240 , @xmath135 approaches zero if @xmath249 because for large @xmath250 , @xmath135 behaves like @xmath251 .",
    "the reason is that the larger @xmath250 is , the stronger is the that the null is false , and thus the more conservative we become when we impute @xmath252 using @xmath253 $ ] . in other words , whereas @xmath135 is a good measure of how conservative the inference is , this example demonstrates that measuring conservatism in general is not necessarily the same as measuring the relative information .",
    "however , when the true @xmath64 is in a reasonable neighborhood of @xmath87 , @xmath135 can be a valuable measure , especially because it is more robust to the posited alternative model and thus can serve as a useful diagnostic measure complementing @xmath97 .",
    "we also note the potentially different impacts of nuisance parameter on @xmath135 and @xmath97 . when @xmath238 is known , @xmath254 .",
    "however , whereas @xmath97 remains the same when @xmath238 is unknown , @xmath135 is greatly affected .",
    "it is also informative to see how @xmath173 of ( [ eq : bisg ] ) and @xmath195 of ( [ eq : combss ] ) compare in this simple problem . for reasons discussed previously ,",
    "we fix here the nuisance parameter @xmath238 at its mle under the null , @xmath255 .",
    "we therefore effectively have a single - parameter @xmath237 , whose score function given a normal sample @xmath256 is @xmath257 ( where @xmath238 is treated as known ) . using the fact that @xmath258 , we have from ( [ eq : bisg ] ) , after setting @xmath259 , @xmath260 \\\\[-8pt ] & = & \\frac{rt^2_0}{r t^2_0 + ( 1-r)(1+t^2_0/m)}. \\nonumber\\end{aligned}\\ ] ] it should not be a surprise to see that @xmath183 when @xmath261 , that is , when @xmath262 happens to be the mle of @xmath64 , @xmath263 , a phenomenon we previously noted in section  [ subsec : shrink ]",
    ". however , this simple example provides some clues on why this happens .",
    "recall that @xmath173 was derived by assuming that the prior shrinks to the null .",
    "this is very strong prior information , and it inevitably influences our measure of the relative information .",
    "consider the situation when @xmath261 , in which case our observed data are completely consistent with our strong prior that @xmath98 . in that sense ,",
    "the information from the observed data is completely useless because it does not provide anything more than we a priori knew ( or rather , assumed ) .",
    "hence it is not a contradiction for @xmath173 to declare zero relative information when clearly the relative information in the observed data should be @xmath247 .",
    "it is not a contradiction because @xmath173 has incorporated the prior information , whereas @xmath241 measures the relative information in the data under our posited model .",
    "this appears to be further substantiated when we consider the other extreme , namely , when @xmath264 . by the same logic , in this case , the observed data are extremely informative as they provide strong evidence to contradict the prior , and the degree of contradiction is such that , even with more data , it is unlikely to be altered",
    ". consequently , one can expect @xmath173 to be close to @xmath265 , which indeed follows from ( [ eq : biot ] ) when @xmath240 is large because @xmath266^{-1}$ ] when @xmath267 .",
    "the above discussion indicates a potential problem with any bayesian measure , as it is inevitable that some prior information will `` leak '' into our measure of relative information in the data alone ( for a specified test ) .",
    "when we have reliable prior information , it is a very interesting issue to investigate / debate whether our relative information should include the prior information ( e.g. , in the extreme case when we know the null is true for certain , the data become irrelevant , and one can always consider we have 100% information ) .",
    "nevertheless , in cases where the prior is introduced for convenience , as largely the case for our setting , it is desirable to reduce any unintended influence as much as possible . in this regard , it was a pleasant surprise to see that the @xmath195 defined in ( [ eq : combss ] ) is able to recover the correct answer in this example .",
    "specifically , letting @xmath259 , ( [ eq : combss ] ) becomes @xmath268 \\\\[-8pt ] & = & \\frac{m}{m + ( n - m)}=r . \\nonumber\\end{aligned}\\ ] ] it is curious that @xmath195 has this ability of `` removing '' the impact of prior information that affected @xmath173 in this finite - sample setting ; how generally this result holds ( even approximately ) is a topic for future research .",
    "our large - sample measures have interesting connections with classic measures based on fisher information , as shown in section  [ subsec : asym ] .",
    "are there similar connections for the small - sample bayesian measures ?",
    "the bayesian measures are based on posterior variances of likelihood ratios or their logarithms .",
    "it turns out that there are several interesting connections , or at least analogies , in both frequentist and bayesian literature . in a frequentist",
    "setting , just as the well - known cramr ",
    "rao lower bound provides a finite - sample information bound that is determined by the fisher information , there is a more general chapman  robbins information bound ( @xcite ) that is based on sampling variance of the _ likelihood ratio_. specifically , let @xmath269 have a multivariate pdf / pmf @xmath270 with @xmath159 taking values in some parameter space  @xmath220 . for each @xmath159 ,",
    "let @xmath271 be the support of @xmath270 .",
    "suppose @xmath272 is an unbiased estimator of a real - valued function @xmath273 .",
    "let @xmath274 then @xmath275 ^ 2\\over\\operatorname{var }   ( lr(\\phi,\\theta|x)|\\theta ) } \\biggr],\\ ] ] where @xmath276 denotes the likelihood ratio function @xmath277 .",
    "this `` second cr '' bound is more general than the first one because it requires neither differentiability of @xmath273 nor the existence of fisher information ( e.g. , as in the case of discrete parameters ) .",
    "it provides an interesting analogy to the proposed bayesian measures because it is based also on the variability of the likelihood ratio , where @xmath278 and @xmath159 can be arbitrarily apart .",
    "the central connection here is that while our large - sample measures have close ties with fisher information ( as detailed in section [ subsec : asym ] ) , which is also intimately connected with the `` first cr '' bound ( i.e. , cramr ",
    "rao bound ) , our small - sample measures are based on variances of likelihood ratio , which is connected with the `` second cr '' bound . the fact that the second cr bound is more general than the first cr bound is also consistent with our expectation that our bayesian measures ultimately should be more general than the likelihood - based large - sample measures , though currently this is still just an expectation , not a realization .",
    "the variances in our bayesian measures are more general than the one used by the second cr bound because we average over not only the missing data but also the posterior distribution of @xmath64 .",
    "examining the posterior distribution of the entire likelihood ratio might seem a case of `` using data twice , '' but the following several identities suggest that such a practice is natural from the bayesian point of view ( indeed , the use of posterior distribution of the likelihood ratio has been previously advocated by @xcite ) .",
    "first , suppose we have a _ proper _ prior @xmath279 ; then it is easy to verify that @xmath280 \\nonumber \\\\ & & \\quad = \\int\\frac{f(y_{\\mathrm{ob}}|\\theta _ 0)}{f(y_{\\mathrm{ob}}| \\theta)}\\frac{f(y_{\\mathrm{ob}}|\\theta ) \\pi(\\theta ) } { f_\\pi(y_{\\mathrm{ob}})}\\,d\\theta \\\\ & & \\quad = \\frac{f(y_{\\mathrm{ob}}|\\theta _ 0)}{f_\\pi(y_{\\mathrm{ob } } ) } \\equiv\\mathrm{bf}_{\\mathrm{ob } } , \\nonumber\\end{aligned}\\ ] ] where @xmath281 .",
    "( note that here we assume @xmath87 is fixed at a known value . )    in other words , the posterior mean of our likelihood ratio is simply the well - known bayes factor for assessing the probability of the model under @xmath98 relative to the model under @xmath282 .",
    "this shows that the bayes factor is a very natural generalization of likelihood ratio by taking into account our uncertainty in  @xmath64 while accessing the evidence in the data against the hypothesized null value @xmath98 .",
    "it also shows that it is quite natural to consider posterior quantification of the likelihood ratio itself .",
    "incidentally , applying identity ( [ bafac ] ) first with @xmath283 and then averaging the resulting identity over the posterior predictive distribution @xmath284 , we also obtain the following intriguing result : @xmath285 & = & \\mathrm{e}[\\mathrm{lr}(\\theta _ 0 , \\theta    \\nonumber\\\\[-8pt ] \\\\[-8pt ] & = & \\mathrm{e}[\\mathrm{lr}(\\theta _ 0 , \\theta |y_{\\mathrm{ob}})| y_{\\mathrm{ob } } ] = \\mathrm{bf}_{\\mathrm{ob}}. \\nonumber\\end{aligned}\\ ] ] in other words , the observed - data bayes factor @xmath286 is the posterior average of any of these three quantities : the observed - data likelihood ratio , the complete - data likelihood ratio , or the complete - data bayes factor",
    ". identities ( [ ratio ] ) , ( [ bafac ] ) and ( [ bayes ] ) together demonstrate the `` coherence '' of likelihood ratio and bayes factor as well as between them . identity ( [ bayes ] ) also suggests an easy way of computing @xmath286 via monte carlo averaging of complete - data or observed - data likelihood ratios .",
    "we note , however , that theposterior distributions of @xmath287 , @xmath288 and @xmath289 are generally different .",
    "in particular , because of ( [ ratio ] ) and ( [ bafac ] ) , we have that @xmath290 , \\operatorname{var}[\\mathrm{lr}(\\theta _ 0 , \\theta   |y_{\\mathrm{ob}})|y_{\\mathrm{ob } } ] \\ } \\nonumber\\\\[-8pt ] \\\\[-8pt ] & & \\quad \\leq \\operatorname{var } [ \\mathrm{lr}(\\theta _ 0 , \\theta   |y_{\\mathrm{co}})|y_{\\mathrm{ob } } ] . \\nonumber\\end{aligned}\\ ] ]    given the clear interpretation and utility of the posterior mean of the likelihood ratio , we would naturally consider the posterior variance of the likelihood ratio .",
    "that is , we can measure the posterior uncertainty in our likelihood ratio evidence .",
    "these are exactly the quantities used in defining @xmath291 in ( [ ripi ] ) , where the numerator and denominator are respectively the posterior variances of the observed - data and complete - data likelihood ratios .",
    "the following equivalent expression of @xmath291 further demonstrates how @xmath291 measures relative `` flatness '' in the likelihood ratio surfaces : @xmath292 } { \\operatorname{cov}_{\\pi,\\theta _ 0 } [ \\mathrm{lr}(\\theta _ 0,\\theta|y_{\\mathrm{co } } ) , \\mathrm{lr}(\\theta , \\theta _ 0|y_{\\mathrm{co } } ) ] } , \\hspace*{-20pt}\\ ] ] where @xmath293 is the covariance operator with respect to the prior @xmath294 , and @xmath295 is with respect to @xmath296 . in other words ,",
    "the flatness of the likelihood ratio surfaces is measured by the covariance of the likelihood ratio and its reciprocal .",
    "although this expression itself is intuitive because a positive function is flat if and only if it is proportional to its reciprocal , the equivalence between ( [ ripi ] ) and ( [ bicov ] ) is a bit curious because ( [ ripi ] ) is based on _",
    "posterior variance _ whereas ( [ bicov ] ) is based on _",
    "prior covariance_.      it would be a serious oversight if we do not emphasize the connections of the information measures we discuss in this paper to the vast literature on entropy . indeed , essentially all measures we presented have an entropy flavor , from the large - sample ones based on kullback  leibler information to the small - sample ones involving second - order entropy in the form of @xmath298 ( see @xcite ) .",
    "this is very natural given that the entropy is a fundamental type of information measure ( e.g. , @xcite ) . indeed ,",
    "much of the classic results on information measure in optimal sequential designs , which our genetic applications resemble ( i.e. , as one needs to decide the next step given what has been observed ) , are based on entropy - like quantities and their generalizations .",
    "this includes both kullback  leibler information and chernoff information ( @xcite ) .",
    "a central difference between that literature and our current proposals is that the existing literature focuses on quantifying the _ absolute _ amount of information in an experiment / design , whereas our main objective here is to quantify the _ relative _ amount of information compared to the absolute amount of information that we would have if there were no missing data ( e.g. , known ibd sharing in linkage studies ) .",
    "furthermore , we investigate two sets of relative information , depending on whether we can assume the true parameter is in a neighborhood of the null or not . to the best of our knowledge ,",
    "our study is the first serious investigation of the roles of null and alternative hypotheses in measuring relative information .    because our bayesian measures @xmath161 and @xmath169 are defined as ratios of variances , it is also important to emphasize their connections to the regression @xmath297 and to other measures of association / correlation such as the linkage disequilibrium measure @xmath299 ( e.g. , @xcite ) .",
    "these measures are related to fisher information and can also be used to estimate relative information .",
    "the main differences are that ours are defined via the _ posterior variability _ of the _ whole ratio or log - likelihood ratio _ , instead of _ sampling variances _ of _ individual statistics or variables_. more details on measures of association / correlation used to quantify",
    "relative information can be found elsewhere ( @xcite ) .",
    "clearly much remains to be done , especially for the small - sample problems . with large samples",
    ", we believe the measures we proposed , especially @xmath97 , satisfy essentially all five criteria as discussed in  [ subsec : conf ] . for small samples , the various bayesian measures we proposed ,",
    "while all satisfy the second criterion , have pros and cons regarding the rest of the criteria .",
    "the most pronounced problem , of course , is the choice of a general - purpose `` default prior . '' here we emphasize that the desire for `` general purpose '' is motivated by the observation that in many applications the investigators need to compute the information measures for many data sets ( e.g. , different families or pedigrees and different loci in linkage analysis ; different tests for different haplotype models in the association studies ) under time constraints .",
    "therefore it is typically not feasible to construct specific priors for each data set at hand , nor is it desirable given that the purpose of hypothesis testing , in the genetic applications we are interested in , has more of a screening nature .",
    "a requirement for constructing problem - specific priors would be typically viewed as too much of a burden to be practically appealing . on the other hand ,",
    "standard recipes for constructing `` default '' priors do not seem to be generally applicable either .",
    "for example , the use of jeffreys prior is typically out of the question because the calculation of the expected fisher information requires us to specify a reliable distribution over the state space of @xmath62 for arbitrary value of @xmath300 , which is typically very hard , if not impossible , to do .",
    "furthermore , the properties of jeffreys prior are not clear when we try to avoid the use of fisher information in the first place .",
    "second , whereas @xmath195 provides a nice connection between small - sample and large - sample measures in the neighborhood of @xmath87 , we currently do not have such a measure when the null is far from the truth .",
    "this is of great theoretical and practical concern , at least in the context of genetic studies , because the regions where there is strong evidence against the null are precisely the regions we try to identify .",
    "one possible strategy is to start by estimating @xmath64 based on the aggregated data ( e.g. , using data from the other families ) , and then use a prior that shrinks toward this estimated @xmath64 when computing information measure for individual components ( e.g. , families ) . in future work",
    "we plan to evaluate this strategy , as a part of the general investigation of the sensitivity of our bayesian measures to prior specifications once we move out the neighborhood of the null .",
    "third , even for large samples , our measures @xmath135 and @xmath97 can be sensitive to the posited linkage or association model , which may or may not capture the real biological process that leads to the linkage or association",
    ". this would be particularly true for @xmath97 , which relies more heavily on the model associated with the test than @xmath135 .",
    "although such sensitivity is inevitable because without a specific alternative model the very notion of relative information may not even be defined , as we emphasized previously , it is important to understand to what degree our information measures can change with our working model .",
    "both theoretical and empirical investigations are needed , especially for classes of problems that are common in practice . also needed",
    "are investigations of the impact of nuisance parameters on these measures .",
    "the haplotype association examples involve nuisance parameters , for example , population genotype risks or population haplotype frequencies , and @xmath97 seems to work adequately in practice .",
    "nevertheless , it would be interesting to see if further refinements are possible .",
    "the illustrative example of section  [ subsec : finite ] strongly suggests that further research is necessary to investigate the possible complications caused by the nuisance parameters , especially for @xmath135 .",
    "the genetic applications presented in this paper focus on the allele - sharing linkage methods and the haplotype - based association studies , but there are many other areas in genetics where measuring relative information is important . for example , in the past years the markers used in genome - wide searches for susceptibility loci were mostly microsatellites .",
    "these are markers that have many alleles , and are generally very informative , but are not very common across the genome .",
    "because the applications focused on small regions of the genome , this lack of abundance of the microsatellites has led to the still increasing popularity of the snps as genetic markers .",
    "the snps are not as informative as the microsatellites , but they are highly abundant . also new technology platforms such as the affymetrix genechip mapping 10k , 100k and 500k arrays ( @xcite ) are available for snp genotyping , and they come with a substantial reduction in cost . given that both the microsatellites and the snps are currently used in gene - mapping studies , a fundamental and practical question is how many snps we need in order to obtain the same amount of information as obtained by using microsatellites .",
    "differences between snps and microsatellites have been investigated for linkage ( e.g. , @xcite ; @xcite ; @xcite ; @xcite ; @xcite ) , and measures of relative information extracted have been proposed ( @xcite ) , but the answers to similar questions will be different for different applications .",
    "we plan to further explore the use of the proposed measures of information to other problems of this sort .",
    "the comparisons between the relative information of sets of snps to that of sets of microsatellites ( relative to the underlying complete information ) will allow us to make sensible comparisons of the maps for a particular study purpose .",
    "the gene - mapping research has focused recently on genome - wide association studies that are thought to have better power to localize genes contributing more modestly to disease susceptibility . in these studies ,",
    "new measures are needed for quantifying the loss in information due to untyped snps , or even snps that have not been discovered .",
    "also , novel tools for measuring information are necessary in choosing a subset of `` tagging '' snps to type for a disease project based on the data from the hapmap project ( @xcite ) .",
    "other possible applications are in testing for gene - environment interaction .",
    "this can be done in both linkage and association studies , and can increase the power of detecting risk factors . in most of these studies , the environmental and the clinical data are also incomplete .",
    "a natural question then arises : `` what is the most efficient way to allocate the resources : what percentage should be devoted to collect more genetic information and what percentage should be used to collect more covariate information ? ''",
    "the answer depends again on the specific study , and the problem is more complicated because the environmental and clinical information can be subject to much more complicated missing - data patterns , often due to unknown reasons .",
    "research is clearly needed in this direction to explore to what extent it is possible to sensibly measure the relative information for guiding the allocation of resources , and we hope the general framework we set up in this paper provides a starting point , if not a solution .",
    "in order to prove the shrinking prior limit results in section  [ subsec : shrink ] , we need the following lemma .    [ lemma : lhopital]let @xmath23 be a fixed real number , and let @xmath301 and @xmath302 , @xmath303 , be real continuous functions defined on an open interval containing @xmath23 , such that @xmath301 and @xmath302 are three times differentiable in a neighborhood of @xmath23 .",
    "let @xmath304 , and similarly for @xmath305 , where @xmath303 .",
    "if @xmath306 \\\\[-8pt ] b_1(t)b_2(t)&=&b_3(t)b_4(t ) , \\nonumber\\end{aligned}\\ ] ] but @xmath307 \\\\[-8pt ] & & \\quad { } -b_3''(t)b_4(t)-b_3(t)b_4''(t)\\neq0 , \\nonumber\\end{aligned}\\ ] ] then @xmath308    the proof follows from the simple taylor expansion @xmath309 and conditions ( [ eq : zero ] ) and ( [ eq : noze ] ) .",
    "[ prop : bi1]let @xmath158 be @xmath170",
    ". then @xmath310 \\\\[-8pt ] \\eqntext{\\quad k=1,2.}\\end{aligned}\\ ] ]    let @xmath311 $ ] , @xmath312|y_{\\mathrm{ob}},\\theta _ 0]$ ] and @xmath313 .",
    "then , as in ( [ bicov ] ) , it is straightforward to verify that @xmath314 we can then apply lemma [ lemma : lhopital ] with @xmath315 .",
    "the result for @xmath316 in ( [ eq : blim ] ) then follows because @xmath317 and @xmath318 \\\\ & = & 2 i_{\\mathrm{mi}}(\\theta _ 0|y_{\\mathrm{ob } } ) - \\ell''(\\theta_0|y_{\\mathrm{ob } } ) + s^2(\\theta _ 0|y_{\\mathrm{ob}}).\\end{aligned}\\ ] ] note that condition ( [ eq : zero ] ) holds because @xmath319 for all @xmath24 .    for @xmath320",
    ", the limit can be calculated by observing that @xmath321 \\\\ & & \\hspace*{40pt}\\big/ \\operatorname{var } [ \\mathrm{lod}(\\theta , \\theta _ 0    calculating the limit of the ratio in the denominator .",
    "a little algebra shows that this ratio can be expressed as @xmath322 ^ 2\\biggr ) \\nonumber\\\\[-8pt ] \\\\[-8pt ] & & \\quad { } \\cdot \\biggl(\\int b_1(\\theta ) \\pi(\\theta ) \\,d\\theta \\int b_2(\\theta ) \\pi(\\theta ) \\,d\\theta \\nonumber \\\\ & & \\quad \\hspace*{61pt } { } - \\biggl[\\int b_3(\\theta ) \\pi(\\theta ) \\,d\\theta \\biggr]^2\\biggr)^{-1 } , \\nonumber\\end{aligned}\\ ] ] where @xmath323 are the same as in ( [ eq : var - lr - obs ] ) , but @xmath324 , \\\\",
    "b_2(\\theta ) & = & \\mathrm{lod}^2(\\theta , \\theta _ 0|",
    "y_{\\mathrm{ob}})a_1(\\theta ) \\quad\\hbox{and}\\quad \\\\ b_3(\\theta ) & = & \\mathrm{lod}(\\theta , \\theta _ 0| y_{\\mathrm{ob}})a_1(\\theta ) .\\end{aligned}\\ ] ] to apply lemma  [ lemma : lhopital ] , we let @xmath325 and @xmath326 .",
    "noting that @xmath327 for all @xmath328 [ and hence condition ( [ eq : zero ] ) holds ] , we only need to compute @xmath329 and @xmath330 in order to obtain the limit .",
    "this calculation is facilitated by the formula @xmath331 \\\\ & & \\quad =     2g'^2\\exp(f)+2gg''\\exp(f)+4gg'f'\\exp(f ) \\\\ & & \\qquad { } + g^2f''\\exp ( f)+g^2f'^2\\exp(f).\\end{aligned}\\ ] ] the result then follows because @xmath332 and @xmath333 \\\\ &",
    "\\equiv & 2i_{\\mathrm{mi } } ( \\theta_0|y_{\\mathrm{ob}}).\\end{aligned}\\ ] ]      the derivations are based on the following lemma , which is trivial to verify using the taylor expansion .",
    "[ lemma : epsilons]let @xmath334 and @xmath335 be continuous functions defined on an open interval containing zero , such that @xmath336 and @xmath337 as @xmath172 . then @xmath338    as in section [ sec : small ] , we let @xmath339 . for @xmath97 , we need to expand both @xmath340 and @xmath341 , as functions of @xmath139 . using the notation given in section  [ subsec : fixed - size ] and  ( [ eq : emid ] ) , we have @xmath342 and @xmath343 \\\\[-8pt ] & & \\quad = - { i_{\\mathrm{co}}\\over2 } \\delta^2 + { q^{(3,0)}_{\\mathrm{ob}}\\over6 } \\delta^3 + o(\\delta^4 ) . \\nonumber\\end{aligned}\\ ] ] expansion ( [ eq : rione ] ) then follows directly from lemma  [ lemma : epsilons ] .    to establish a similar expansion for @xmath135 , let @xmath344 be the maximizer of @xmath141 ; recall we assume that @xmath344 is unique .",
    "then @xmath345 however , even when @xmath346 is small , it is not immediate that @xmath347 would be close to @xmath90 as well . we now show that when @xmath139 is small enough , @xmath348 and @xmath349 have opposite signs .",
    "consequently , @xmath347 , the unique solution of @xmath350 , must be between @xmath87 and @xmath90 , and hence @xmath351 .",
    "to see this , we first expand @xmath352 \\\\[-8pt ] & = & \\bigl[q^{(2,0)}_{\\mathrm{ob}}+ q^{(1,1)}_{\\mathrm{ob}}\\bigr ] \\delta+ o(\\delta^2 ) .",
    "\\nonumber\\end{aligned}\\ ] ] but the following general result , proved in @xcite : @xmath353 \\\\[-8pt ] \\eqntext{\\quad\\hbox{for any } k\\ge 0,}\\end{aligned}\\ ] ] implies that @xmath354 and @xmath355 .",
    "consequently , @xmath356 for @xmath349 , using the notation in ( [ eq : emex ] ) and ( [ eq : dq ] ) , we have @xmath357 \\\\[-8pt ] & & \\quad = h^{(2,0)}(\\theta _ 0|\\theta _ 0)(\\theta _ { \\mathrm{ob } } -\\theta_0 ) + o(\\delta^2 ) \\nonumber \\\\ & & \\quad = i_{\\mathrm{mi}}(\\theta _ 0 ) \\delta+o(\\delta^2 ) , \\nonumber\\end{aligned}\\ ] ] where @xmath358 is as defined in ( [ eq : imis ] ) .",
    "since both @xmath216 and @xmath359 are positive , we conclude from ( [ eq : apb ] ) and  ( [ eq : apb-2 ] ) that @xmath349 and @xmath348 have opposite signs when @xmath139 is small enough .",
    "therefore we have established that @xmath360 , and consequently we can express @xmath361 where @xmath362 and @xmath363 are @xmath364 as @xmath172 and are to be determined .    to determine @xmath362 and @xmath363 , we first note that @xmath365 \\\\[-8pt ] & = & - { i_{\\mathrm{ob } } } \\delta + { \\ell^{(3)}_{\\mathrm{ob}}\\over2 } \\delta^2 + o(\\delta^3 ) \\nonumber\\end{aligned}\\ ] ] and @xmath366 where @xmath367 . substituting ( [ eq : delta ] ) and ( [ eq : ellf ] ) into ( [ eq : qzero ] ) and solving for @xmath362 and @xmath363 , we obtain @xmath368 \\\\[-8pt ] c & = & - \\frac{\\ell^{(3)}_{\\mathrm{ob}}+ b^2 g^{(3)}(\\theta _ 0)}{2g^{(2)}(\\theta _ 0)}. \\nonumber\\end{aligned}\\ ] ]    noting that @xmath369 and ( [ eq : ellf ] ) , we then obtain @xmath370 \\delta^2 \\\\ & & \\qquad { } + \\biggl[\\frac{1}{2}b\\ell^{(3)}_{\\mathrm{ob}}- ci_{\\mathrm{ob } } \\\\ & & \\hspace*{41pt } { } + bcg^{(2)}(\\theta _ 0)+ \\frac{1}{6}b^3g^{(3)}(\\theta _ 0 ) \\biggr ] \\delta^3 + o(\\delta^4 ) \\\\ & & \\quad = - \\frac{i^2_{\\mathrm{ob}}}{2g^{(2)}(\\theta _",
    "0)}\\delta^2 + \\biggl[\\frac{\\ell_{\\mathrm{ob}}^{(3)}i_{\\mathrm{ob}}}{2g^{(2)}(\\theta _ 0 ) } + \\frac{g^{(3)}(\\theta _ 0)i^3_{\\mathrm{ob}}}{6 [ g^{(2)}(\\theta _ 0)]^3 } \\biggr ] \\delta^3 \\\\ & & \\qquad { } + o(\\delta^4).\\end{aligned}\\ ] ] combining this expansion with @xmath371 \\delta + o(\\delta^2 ) , \\\\ g^{(3)}(\\theta _ 0 ) & = & q^{(3,0)}_{\\mathrm{ob } } + \\bigl[q^{(4,0)}_{\\mathrm{ob}}+q^{(3,1)}_{\\mathrm{ob}}\\bigr ] \\delta + o(\\delta^2)\\end{aligned}\\ ] ] and applying lemma  [ lemma : epsilons ] , we obtain @xmath372 \\\\ & & \\qquad\\hspace*{99pt } { } - \\frac{q^{(3,0)}_{\\mathrm{ob}}}{3}(\\mathcal{r}i_e)^3 \\biggr ] \\delta ^3 \\\\ & & \\qquad { } + o(\\delta^4).\\end{aligned}\\ ] ] by lemma  [ lemma : epsilons ] , the above equation and ( [ eq : lodex ] ) together imply that @xmath135 of ( [ fracmc ] ) has the expansion ( [ eq : rizer ] ) .",
    "we thank daniel gudbjartsson for many helpful discussions and suggestions , and judy h. cho for providing the inflammatory bowel disease data . for the diabetes example illustrated in figure [ fig : stroke ] , we thank daniel gudbjartsson for providing the software that performed likelihood and information calculations , gubmar thorleifsson for constructing the figure , and the diabetes research group at decode genetics for generating and providing the data .",
    "we also want to thank a number of reviewers for very constructive comments and suggestions .",
    "this research was supported in part by several national science foundation grants ( nicolae and meng ) .",
    "grant , s , f. , thorleifsson , g. , reynisdottir , i. , benediktsson , r. , manolescu , a. and sainz , j. et al .",
    "variant of transcription factor 7-like 2 ( tcf7l2 ) gene confers risk of type 2 diabetes .",
    "_ nature genetics _ * 38 * 320323 .",
    "lange , c. and laird , n.  m. ( 2002b ) . on a general class of conditional tests for family - based association studies in genetics : the asymptotic distribution , the conditional power and optimality considerations .",
    "_ genetic epidemiology _ * 23 * 165180 .",
    "meng , x .-",
    "( 2001 ) . a congenial overview and investigation of multiple imputation inference under uncongeniality . in _",
    "survey nonresponse _",
    "( r. groves , d. dillman , j. eltinge and r. little , eds . ) 343356 .",
    "wiley , new york .",
    "middleton , f.  a. et al .",
    "genomewide linkage analysis of bipolar disorder by use of a high - density single - nucleotidepolymorphism ( snp ) genotyping assay : a comparison with microsatellite marker assays and finding of significant linkage to chromosome 6q22 .",
    "j. human genetics _ * 74 * 886897 .",
    "peer , i. , de  bakker , p.  i. , maller , j. , yelensky , r. , altshuler , d. and daly , m.  j. ( 2006 ) . evaluating and improving power in whole - genome association studies using fixed marker sets .",
    "_ nature genetics _ * 38 * 663667 .",
    "schaid , d.  j. , guenther , j.  c. , christensen , g.  b. , hebbring , s. , rosenow , c. , hilker , c.  a. , mcdonnell ,  s.  k. , cunningham , j.  m. , slager , s. , blute ,  m.  l. and thibodeau , s.  n. ( 2004 ) .",
    "comparison of microsatellites versus single - nucleotide polymorphisms in a genome linkage screen for prostate cancersusceptibility loci .",
    "j. human genetics _ * 75 * 948965 .",
    "thalamuthu , a. , mukhopadhyay , i. , ray , a. and weeks , d.  e. ( 2005 ) . a comparison between microsatellite and single - nucleotide polymorphism markers with respect to two measures of information content",
    ". _ bmc genetics _ * 6 ( suppl 1 ) * s27 ."
  ],
  "abstract_text": [
    "<S> many practical studies rely on hypothesis testing procedures applied to data sets with missing information . </S>",
    "<S> an important part of the analysis is to determine the impact of the missing data on the performance of the test , and this can be done by properly quantifying the relative ( to complete data ) amount of available information . </S>",
    "<S> the problem is directly motivated by applications to studies , such as linkage analyses and haplotype - based association projects , designed to identify genetic contributions to complex diseases . in the genetic studies </S>",
    "<S> the relative information measures are needed for the experimental design , technology comparison , interpretation of the data , and for understanding the behavior of some of the inference tools . </S>",
    "<S> the central difficulties in constructing such information measures arise from the multiple , and sometimes conflicting , aims in practice . for large samples , we show that a satisfactory , likelihood - based general solution exists by using appropriate forms of the relative kullback  leibler information , and that the proposed measures are computationally inexpensive given the maximized likelihoods with the observed data . </S>",
    "<S> two measures are introduced , under the null and alternative hypothesis respectively . </S>",
    "<S> we exemplify the measures on data coming from mapping studies on the inflammatory bowel disease and diabetes . for small - sample problems , which appear rather frequently in practice and </S>",
    "<S> sometimes in disguised forms ( e.g. , measuring individual contributions to a large study ) , the robust bayesian approach holds great promise , though the choice of a general - purpose `` default prior '' is a very challenging problem . </S>",
    "<S> we also report several intriguing connections encountered in our investigation , such as the connection with the fundamental identity for the em algorithm , the connection with the second cr ( chapman  robbins ) lower information bound , the connection with entropy , and connections between likelihood ratios and bayes factors . </S>",
    "<S> we hope that these seemingly unrelated connections , as well as our specific proposals , will stimulate a general discussion and research in this theoretically fascinating and practically needed area .    ,    . </S>"
  ]
}