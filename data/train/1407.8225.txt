{
  "article_text": [
    "statistical inference is the process of converting experience , in the form of observed data , to knowledge about the underlying population in question , and is an essential part of the scientific method of discovery .",
    "our starting point in this paper will be a sampling model for observable data @xmath0 , depending on a parameter @xmath1 in @xmath2 .",
    "mathematically , a sampling model for @xmath0 is a @xmath1-dependent probability distribution @xmath3 defined on the sample space @xmath4 of @xmath0 .",
    "the sampling model s dependence on @xmath1 implies that the observed data @xmath5 carries relevant information about the unknown parameter .",
    "our goal is to convert this information into a probabilistic measure of uncertainty .",
    "that is , for an assertion or hypothesis @xmath6 concerning the unknown parameter , we want to assign a measure of the `` plausibility '' that the assertion @xmath7 is true .",
    "this plausibility measure should depend on the data @xmath8 , should have a meaningful probabilistic interpretation , and should not require specification of an _ a priori _ distribution for @xmath1 .",
    "r.  a.  fisher , starting with his work on inverse probability @xcite , made ambitious efforts to develop prior - free probabilistic inference based on a fiducial argument .",
    "@xcite writes :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ by contrast [ to the bayesian argument ] , the fiducial argument uses the observations only to change the logical status of the parameter from one in which nothing is known of it , and no probability statement about it can be made , to the status of a random variable having a well - defined distribution . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    although some of the fiducial ideas were reinterpreted by @xcite and used to create confidence intervals , a central concept in the frequentist paradigm , fiducial inference has been perceived as fisher s `` one great failure '' @xcite .",
    "fisher acknowledged his only limited success in developing a framework for prior - free probabilistic inference based on the fiducial argument , he insisted that there was something valuable in it .",
    "he wrote :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ i do nt understand yet what fiducial probability does .",
    "we shall have to live with it a long time before we know what it s doing for us .",
    "but it should not be ignored just because we do nt yet have a clear interpretation @xcite .",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    fisher s confidence in the value of fiducial inference has inspired continued efforts , including structural inference @xcite , dempster  shafer theory @xcite , generalized p - values and confidence intervals @xcite , generalized fiducial @xcite , confidence distributions @xcite , and bayesian inference with default , reference , and/or data - dependent priors @xcite . here",
    "we will argue , however , that fiducial inference and its variants mentioned above , are actually not prior - free .",
    "@xcite have recently introduced an alternative paradigm under the name _ inferential models _ ( ims ) ; see , also , the references and further reading below . with the focus on logical reasoning with uncertainty , these authors seek the best possible approach to genuinely prior - free probabilistic inference .",
    "the focus here is mainly on its two fundamental principles , namely , the _ validity _ and _ efficiency _ principles , and formal approaches built upon these two principles .",
    "this includes basic ims , conditional ims for combining information , and marginal ims for efficient inference on interest parameters .",
    "to represent realistic knowledge about assertions of interest , the use of lower and upper probabilities or belief and plausibility functions is necessary . given data @xmath0 , they are functions from the space of all subsets of @xmath2 to @xmath9 $ ] , denoted by @xmath10 where @xmath11 stands for the negation of the assertion @xmath7 .",
    "the belief function @xmath12 represents the amount of evidence in data @xmath0 supporting the claim that the assertion @xmath7 is true .",
    "thus , the value of @xmath13 represents the amount of evidence in data @xmath0 supporting the claim that the assertion @xmath14 is true or , equivalently , that the assertion @xmath7 is false .",
    "the plausibility function @xmath15 represents the amount of evidence in data @xmath0 that does not support the claim that @xmath7 is false .",
    "it follows that @xmath16 for all assertions @xmath7 . also , unlike conventional probabilities , which are additive , belief functions are sub - additive : @xmath17 for more discussion on belief functions , see @xcite .    in terms of belief functions , a model of total ignorance",
    "can be formally written as @xmath18 that is , no evidence is available to either support or refute any assertion @xmath7 .",
    "this definition is consistent with what is given in @xcite : _ `` the necessary _ ignorance _ is specified by our inability to discriminate any of the different sub - aggregates having different frequency ratios , such as must always exists . '' _ it follows immediately that probability is not a satisfactory model for total ignorance and , therefore , no bayesian priors can represent total ignorance ; see @xcite . in other words ,",
    "all bayesian priors are informative , as the relative importance of any two points in @xmath2 is fully specified .",
    "nevertheless , bayesian methods based on conventional probabilities ( with proper or improper priors ) can be useful for constructing methods , e.g. , confidence intervals , with good frequentist properties @xcite .",
    "there is a close connection between fiducial and default/``non - informative '' prior bayes , so the limitations of the latter , discussed above , must also be limitations of the former . to see this ,",
    "suppose that there is a joint distribution for @xmath19 that is consistent with both the sampling model ( `` @xmath20 '' ) and the fiducial distribution ( `` @xmath21 '' ) .",
    "then @xmath1 , or some transformation thereof , must be a location parameter , and the fiducial distribution corresponds to the bayesian posterior obtained from a flat prior on the location parameter .",
    "see @xcite and @xcite for details .    to make these points clear , we take a closer look at the fiducial argument or , more precisely , the fiducial operation in a simple example .",
    "consider inference about the mean of the unit normal model @xmath22 from a single observation @xmath0 .",
    "a simple association for this sampling model can be written as @xmath23 in the fiducial literature , @xmath24 is called the pivotal quantity since its distribution is free of parameters .",
    "when @xmath0 is observed , the fiducial argument continues to regard @xmath24 , now written as @xmath25 , as a @xmath26-distributed random variable .",
    "this distribution on @xmath24 , and its functional connection to @xmath1 , admits a fiducial distribution for @xmath1 : @xmath27 the `` continue to regard '' @xcite reasoning is what drives the operation that changes the knowledge status @xmath1 from total ignorance to that which can be represented by conventional probability .",
    "this reasoning does not appear to be consistent with the goal of prior - free inference .",
    "indeed , the standard view that @xmath1 is fixed but unknown agrees with the model of total ignorance .",
    "however , once @xmath0 is fixed at its observed value @xmath8 , the conditional distribution of @xmath24 degenerates at the point @xmath28 . the fiducial operation that replaces the degenerate conditional distribution @xmath29 with the non - degenerate conditional distribution @xmath30 must be using some information beyond that contained in @xmath8 and @xmath3 .",
    "therefore , fiducial inference can not be prior - free .",
    "the precise formulation of the i m framework will be given below , but it may be of interest to know how we reached this particular formulation since some might consider the journey to be at least as important as the destination .",
    "the starting point was a precise statement of the goal of statistical inference :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ to give , for any assertion or hypothesis @xmath7 about the parameter of interest , meaningful summaries of the evidence in the observed data supporting the claims that `` @xmath7 is true '' and `` @xmath14 is true . '' _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    given these constraints , the goal is to make `` the best possible inference . ''",
    "this general idea motivates the two principles given in the next section .    towards this goal ,",
    "a first question is if probability is the appropriate kind of measure . in cases where a genuine prior distribution is available",
    ", the bayesian approach based on posterior probabilities is appropriate .",
    "however , when no meaningful prior information is available , as is often the case in scientific applications , difficulties arise .",
    "first , as discussed above , there is no prior distribution that encodes ignorance .",
    "second , the use of default or non - informative priors that depend on the sampling model differs enough from the conventional bayesian approach that , in our opinion , it can not really be considered bayes .",
    "so , from a foundational point of view , we conclude that the bayesian approach is not appropriate for prior - free probabilistic inference .",
    "the next question is if fiducial inference , or one of its variants , is appropriate . as discussed above",
    ", fiducial has some difficulties .",
    "in fact , fiducial is basically bayes with a possibly data - dependent prior @xcite , so the same issues discussed above would apply to fiducial .",
    "shafer theory was another candidate method , with the appeal of not basing inference on probabilities but on belief functions .",
    "the difficulty with the dempster ",
    "shafer approach , in our opinion , is that the corresponding belief function values can not be interpreted on a common scale .",
    "that is , in one application , 0.9 might be a large belief function value , but in another it might be small . having a common scale on which the belief function values can be interpreted  this is the `` meaningfulness '' part of the above definition  is , in our opinion , essential the the success of a method as a tool for scientific discovery .",
    "so , after considering a variety of existing methods , dempster ",
    "shafer s use of belief functions was desirable , but a tool to properly calibrate the belief function values was needed .",
    "the idea to expand the range between the belief and plausibility function values with a random set calibrated to the distribution of unobservable auxiliary variable was designed precisely to meet this need .",
    "this approach achieves what is set out in the above definition ; see , also , the validity principle below and .",
    "the goal then to do the `` best possible , '' subject to the validity constraint , leads to the efficiency principle and optimality considerations @xcite .",
    "incidentally , though the i m belief function output is not a probability measure , we consider i m inference to be `` probabilistic '' in a certain sense .",
    "the point is that it can be explained through the prediction of an unobservable quantity with a random set , and belief is just a probability with respect to the distribution of that random set .",
    "this is discussed more in the following section .",
    "philosophically , the i m framework for statistical inference is built on the following validity principle @xcite .",
    "probabilistic inference requires associating an unobservable but predictable quantity with the observable data and unknown parameter .",
    "probabilities to be used for inference are obtained by valid prediction of the predictable quantity .    to make the notion of `` predictable quantity '' precise",
    ", we consider an alternative description of the sampling model .",
    "specify an auxiliary variable @xmath31 in @xmath32 with distribution @xmath33 and a function @xmath34 such that the sample @xmath0 , defined as @xmath35 has distribution @xmath3 .",
    "the pair @xmath36 is completely known .",
    "unlike fiducial and its extensions , for valid prediction , the i m approach carries out its fundamental operations in the well - defined probability space of the auxiliary variable .",
    "the key to this approach is the use of _ predictive random sets_. according to @xcite , a valid predictive random set @xmath37 can be defined by specifying    * a collection @xmath38 of nested subsets of @xmath32 , including @xmath39 and @xmath32 , to serve as the support of @xmath37 , and * the so - called natural measure @xmath40 for @xmath37 that satisfies @xmath41    the centered random interval @xmath42 $ ] , where @xmath43 , provides a simple example of a valid predictive random set for predicting a realization from @xmath44 .",
    "the i m framework makes probabilistic inference by propagating prediction in the auxiliary variable space to the parameter space .",
    "the three - step construction is as follows .",
    "associate the observable data @xmath0 with the unknown quantity @xmath1 using an auxiliary variable @xmath45 , such as that in , to obtain the sets @xmath46    predict @xmath31 using a valid predictive random set @xmath37 .",
    "combine the association and prediction @xmath37 to get @xmath47 then compute the belief and plausibility functions via the distribution of @xmath48 : @xmath49 if @xmath50 , then @xmath37 needs to be stretched , i.e. , replace @xmath37 with the smallest @xmath51 in @xmath38 such that @xmath52 and @xmath53 is non - empty @xcite .",
    "the i m results are probabilistic and , since the predictive random set is valid , the i m results have desirable frequency properties @xcite . in particular ,",
    "if @xmath37 is valid , then for all @xmath6 and all @xmath54 , @xmath55 in other words , for example , @xmath56 is stochastically no smaller than @xmath57 , as a function of @xmath0 , when the assertion @xmath7 is true .",
    "this provides a meaningful and objective scale on which to interpret the plausibility ( and belief ) function values . as a consequence ,",
    "one can obtain procedures , such as tests and confidence regions , having exact control on frequentist error rates @xcite .",
    "further developments concern efficiency , motivated by the following principle .",
    "subject to the validity constraint , probabilistic inference should be made as efficient as possible .",
    "@xcite studied two classes of efficiency problems , namely , conditional ims for combining information and marginal ims for inference on interest parameters .",
    "these are introduced below with two benchmark examples , which help demonstrate the differences between the i m and fiducial - type frameworks .    finally , in some applications , prediction of future observations is the goal , not inference on @xmath1 .",
    "prediction problems can be posed as ones involving marginalization or model averaging .",
    "the i m framework is capable of handling such problems too , but this will not be discussed any further here ; see @xcite for details .",
    "take the bivariate normal model with zero means , unit variances , and unknown correlation coefficient @xmath1 .",
    "consider inference about @xmath1 from a sample of size @xmath58 , @xmath59 the data - generating based association , for example , has a @xmath60-dimensional auxiliary variable .",
    "@xcite argue that the fully observed functions of the auxiliary variable do not need to be predicted and , by conditioning on the observed components , the prediction of the unobserved components can be sharpened .",
    "this argument implies that the association can be built from the model s sufficient statistics @xmath61 with independent chi - square auxiliary variables , @xmath62 and @xmath63 , with @xmath58 degrees of freedom , the association becomes @xmath64 it is tempting to follow a fiducial - type argument and condition on @xmath65 , an attractive parameter - free identity . however , such conditioning amounts to conditioning on all the data @xmath66 , which makes the predictive distribution degenerate and , consequently , the corresponding inference is not valid for all assertions .",
    "note that the auxiliary variable is two - dimensional while the parameter of interest is one - dimensional . to obtain an association with a lower - dimensional auxiliary variable ,",
    "@xcite propose to first rewrite the above association in terms of a new pair of auxiliary variables @xmath67 such that only @xmath68 , say , carries information directly related to @xmath1 .",
    "by doing so , a conditional association obtains by taking @xmath68 as the auxiliary variable , with distribution conditioned on the observed value of @xmath69 .",
    "this leads to a reduction of the auxiliary dimension from two to one . for the bivariate normal correlation problem , their approach based on a partial differential equations",
    "gives the conditioning function or component of the form @xmath70 note that the function above depends on @xmath1 ; @xcite add the adjective `` local '' in such cases .",
    "inference can then be made by predicting a corresponding complement transformation of @xmath71 , for example , @xmath72 let @xmath73 denote the distribution function of @xmath68 conditional on @xmath74 the so - called local conditional i m can be represented using the basic i m with the auxiliary variable @xmath75 and the association @xmath76 for more technical details , see @xcite .",
    "the numerical results presented there show that this local conditional i m produces exact confidence intervals that are more efficient than the best of available frequentist methods .",
    "suppose that @xmath1 is a multidimensional parameter , but that only some lower - dimensional feature of @xmath1 is of interest .",
    "if @xmath1 itself were the parameter of interest , then we could find an association to connect it to data and a set of auxiliary variables .",
    "however , in the case where only a feature of @xmath1 is of interest , efficiency demands that we further reduce the dimension of the auxiliary variable .",
    "the marginal i m approach of @xcite presents a strategy for efficient , and often optimal inference on interest parameters .",
    "we choose the benchmark behrens  fisher problem to illustrate mims and to contrast with the fiducial approach .",
    "suppose we have two independent samples of size @xmath77 and @xmath78 from @xmath79 and @xmath80 , respectively , where all four normal parameters are unknown .",
    "the behrens  fisher problem concerns marginal inference on the difference @xmath81 .",
    "the cim theory suggests that we start with an association based on the sampling distribution of the sufficient statistics , i.e. , sample means and variances @xmath82 and @xmath83 , @xmath84 .",
    "let @xmath85 be the parameter of interest .",
    "take the following association , in terms of @xmath86 and the nuisance parameters @xmath87 , @xmath88 and @xmath89 , @xmath90 @xmath91 where @xmath68 and @xmath69 are independent with @xmath92 for @xmath84 , @xmath93 is bivariate normal with zero means , unit variances , and correlation coefficient depending on @xmath94 s and @xmath95 s , and @xmath67 and @xmath96 are independent .",
    "fisher s fiducial solution is obtained by `` continuing to regard '' the auxiliary variable as having its sampling distribution , conditional on the observed sufficient statistics . from the i m perspective",
    ", this fiducial argument is questionable .",
    "it is also easy to check that this fiducial distribution is the same as the bayesian posterior distribution based on assigning flat priors to the `` location parameters '' @xmath87 , @xmath97 , @xmath98 , and @xmath99 .",
    "valid inference can be made with any admissible predictive random set for the four - dimensional auxiliary variable @xmath100 .",
    "the shape of the predictive random set controls the precision on features of the auxiliary variable . due to the unknown @xmath87 in the first expression of , the accuracy in predicting @xmath31 is not useful because it provides no information on @xmath101 and , therefore , results in less efficient inference on @xmath86 in .",
    "this suggests that we stretch the predictive random set along the @xmath31-coordinate as much as possible , ending up with an effectively three - dimensional or marginalized predictive random set for @xmath102 .",
    "this logical reasoning for efficient inference leads to the initial dimension - reduced association : @xmath103 with the same @xmath102 .",
    "that same argument for the usefulness of accurate prediction of @xmath88 for inferring @xmath86 leads to the further dimension - reduced association @xmath104 for inference about @xmath86 , with @xmath105 as the new nuisance parameter .",
    "thus , without loss of efficiency , we can make valid inference on @xmath86 by predicting a two - dimensional auxiliary variable @xmath106 .",
    "however , it does not appear that the same argument can be used further to obtain an association for @xmath86 involving only a one - dimensional auxiliary variable .    to associate @xmath86 to a one - dimensional auxiliary variable",
    ", @xcite allows the auxiliary variable to depend on nuisance parameters .",
    "such a nuisance parameter - dependent auxiliary variable is constructed in such a way that the effect of the nuisance parameter on its distribution is minimized in some sense .",
    "a heuristic approach to constructing such a @xmath107-dependent auxiliary variable from is as follows . for values of @xmath108 such that @xmath109 , the value of @xmath110 is approximately @xmath111",
    "this suggests we divide the first identity of by @xmath112 a variant of the second expression in .",
    "this gives @xmath113 where @xmath114 .",
    "thus inference about @xmath86 can be made by a predictive random set for the one - dimensional quantity @xmath115 alone .",
    "let @xmath116 denote the distribution function of @xmath115 .",
    "we rewrite the above association as @xmath117    introduce the predictive random set @xmath118 , where @xmath119 , for @xmath31 in .",
    "the corresponding predictive random set for @xmath115 in is @xmath120 to eliminate the effect of @xmath121 , take the enlarged predictive random set @xmath122 this predictive random set is valid for predicting @xmath115 in , uniformly in @xmath121 and , therefore , leads to valid inference on assertions about @xmath86 .",
    "it can be shown that @xmath123 is equivalent to the centered predictive random set for predicting a realization from the t - distribution with @xmath124 degrees of freedom @xcite .",
    "suppose that @xmath125 are iid @xmath126 . here",
    ", both @xmath127 and @xmath128 are unknown , but , for the moment , the parameter of interest is @xmath129 , the standardized mean , or signal - to - noise ratio .",
    "we may first reduce to the joint minimal sufficient statistics @xmath130 , the sample mean and sample variance , respectively .",
    "based on this pair , the ( conditional ) association can be written as @xmath131 where @xmath62 and @xmath63 are independent .",
    "the parameter of interest is a scalar but the auxiliary variable @xmath132 is two - dimensional , so we would like to further reduce the dimension of the latter .",
    "then the conditional association can be rewritten as @xmath133 the first expression has no dependence on @xmath134 .",
    "also , for any pair @xmath135 , there exists a @xmath134 that satisfies the second expression .",
    "therefore , in the language of @xcite , this is a `` regular '' association so the nuisance parameter @xmath134 can be eliminated by ignoring the second expression above .",
    "that is , the marginal association is @xmath136 the right - hand side of the above expression has a known distribution , namely , a non - central student - t distribution with @xmath137 degrees of freedom and non - centrality parameter @xmath138 .",
    "therefore , a simple change of auxiliary variable gives the modified marginal association : @xmath139 where @xmath140 is the above non - central student - t distribution function . to this point",
    ", our calculations agree with those of @xcite using fiducial arguments .",
    "to complete the marginal i m for @xmath141 , the a - step gives the sets @xmath142 where @xmath143 . for the p - step ,",
    "we propose to use a simple `` default '' @xcite predictive random set : @xmath144 this predictive random set satisfies the required properties for the corresponding i m to be valid @xcite .",
    "finally , the c - step gives @xmath145 since the predictive random set is valid , one can produce valid probabilistic inference for any assertion about @xmath141 of interest .",
    "in particular , for singleton assertions , i.e. , @xmath146 , the belief function is zero but the marginal plausibility function is @xmath147 we may construct an interval estimate for @xmath141 using this marginal plausibility function . specifically , for some fixed level @xmath54 , the @xmath148% plausibility interval for @xmath141 is @xmath149 which , in this case , simplifies to @xmath150 this plausibility interval clearly has frequentist coverage probability @xmath151 , which is a consequence of our general i m validity results .    as a variation on this example , next consider @xmath152 , the coefficient of variation .",
    "@xcite demonstrate that this is a challenging problem for bayesian , likelihood , and frequentist frameworks . using",
    "the approach just described for the signal - to - noise ratio problem , it is straightforward to construct a marginal i m for @xmath153 . in particular , for singleton assertions , the plausibility function is @xmath154 plots of this plausibility function for two samples of size @xmath155 from @xmath156 , for two values of @xmath127 , are displayed in figure  [ fig : cv ] .",
    "panel  ( a ) shows the case of @xmath157 , and it is clear that the data are fairly informative about @xmath153 , and the true @xmath158 is contained in the 95% plausibility interval .",
    "panel  ( b ) shows the case of @xmath159 . in this case , data are not particularly informative , i.e. , the plausibility function does not seem to vanish as @xmath160 , so the plausibility regions are unbounded . since we know",
    "the i m plausibility regions are exact , it follows from the theorem of @xcite that they must be unbounded with positive probability .",
    "however , we argue that this is not a problem of the i m approach . given the usual variation of the sample mean around @xmath127 , if @xmath127 is ( close to ) zero , then it is not possible to rule out @xmath153 values with very large magnitude .",
    "so , in such cases , no reasonable approach should be able to rule out these extreme values of @xmath153 based on data alone .",
    "therefore , the marginal i m approach here arguably gives the `` best possible '' inference on @xmath153 .",
    "[ fig : cv ]",
    "methods for prior - free probabilistic inference are fundamentally important for converting experience to knowledge in scientific explorations .",
    "this is especially true in the `` big data '' world we now live in . in spite of its failure as a general method leading to prior - free probabilistic inference ,",
    "fisher s fiducial arguments have inspired many statisticians to think outside the box and to develop promising new approaches .",
    "for example , j.  hannig and his coauthors have developed a generalized fiducial framework that yields inferential methods which share the asymptotic efficiency of classical likelihood - based methods but often perform significantly better in small - sample studies .",
    "the i m framework is motivated by fisher s fiducial argument , but the two differ both philosophically and technically .",
    "this new approach is promising in that it is truly prior - free and produces posterior probabilistic assessments of uncertainty with desirable frequency properties .",
    "this is really a new school of thought , with the ambitious goal to carry out the best possible sampling model - based scientific inference , so the efforts so far have focused on developing the fundamental ideas , building blocks of the framework .",
    "additional work can be found below in the references and further reading sections . as discussed briefly here",
    ", the current work on conditional and marginal ims shows that the development of efficient ims will be very interesting in years to come . while there is still so much to do concerning theory , computation , and application of ims",
    ", the authors believe that it has a very bright future given the fundamental role that statistics has to play in the development of science .",
    "the authors thank the editor , associate editor , and reviewers for helpful suggestions on an earlier draft of this paper .",
    "this research is partially supported by the u.s",
    ".  national science foundation , grants dms1007678 , dms1208833 , and dms1208841 ."
  ],
  "abstract_text": [
    "<S> the development of statistical methods for valid and efficient probabilistic inference without prior distributions has a long history . </S>",
    "<S> fisher s fiducial inference is perhaps the most famous of these attempts . </S>",
    "<S> we argue that , despite its seemingly prior - free formulation , fiducial and its various extensions are not prior - free and , therefore , do not meet the requirements for prior - free probabilistic inference . </S>",
    "<S> in contrast , the inferential model ( i m ) framework is genuinely prior - free and is shown to be a promising new method for generating both valid and efficient probabilistic inference . with a brief introduction to the two fundamental principles , namely , the validity and efficiency principles , </S>",
    "<S> the three - step construction of the basic i m framework is discussed in the context of the validity principle . </S>",
    "<S> efficient i m methods , based on conditioning and marginalization are illustrated with two benchmark examples , namely , the bivariate normal with unknown correlation coefficient and the behrens  fisher problem .    </S>",
    "<S> _ keywords and phrases : _ bayes ; belief function ; fiducial ; inferential models ; statistical principles . </S>"
  ]
}