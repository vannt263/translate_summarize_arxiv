{
  "article_text": [
    "first , we recall the example given in the main body of the paper , which is that of a noisy photodetector where the efficiency of the photon source is @xmath42 , which we would like to estimate .",
    "this value is equivalent to the probability for the detector to click in the presence of no noise . in reality , _ dark counts _ register clicks when no photon is present and _ losses _ register no clicks when a photon is present .",
    "let these happen with probability @xmath43 and @xmath44 , respectively .",
    "then , given @xmath42 , the probability for a click to actually happen is @xmath45 . from these clicks ,",
    "our task is to estimate @xmath42 .",
    "aysmptotically , the posterior variance is given by the inverse of the fisher information evaluated at , for example , the maximum likelihood estimate @xcite .",
    "the maximum likelihood estimator if @xmath20 clicks are observed is @xmath62 provided @xmath63 ( not a concern asymptotically ) .",
    "the fisher information of @xmath12 measurements , which is defined as @xmath64 , \\ ] ] can be calculated most simply in two steps given by the chain rule : @xmath65 . since",
    "each measurement is a bernoulli trial with probability @xmath66 , the fisher information for a single measurement is @xmath67 , yielding a fisher information for @xmath12 measurements of @xmath68 taking the derivative and applying the chain rules yields @xmath69 with these facts we can say that asymptotically , the mean squared error in @xmath42 is @xmath70 \\nonumber      & = { \\mathbb{e}}_k \\left[\\left (          \\frac{(1-\\alpha-\\beta)^2 n}{(\\alpha + ( k / n ) - \\alpha)(1-\\alpha-(k / n)+\\alpha ) }      \\right)^{-1}\\right ] \\\\ & = \\mathbb e_k\\left[\\frac{k(n - k)}{(1-\\alpha-\\beta)^2n^3}\\right ] \\nonumber \\\\ & = \\frac{\\mathbb e_k[k]n-\\mathbb e_k[k^2]}{(1-\\alpha-\\beta)^2n^3},\\end{aligned}\\ ] ] where we have used that @xmath71 .    letting",
    "the random variable @xmath20 be distributed according to a discrete uniform distribution on @xmath72 and using known formulas for the first two moments @xmath73 $ ] and @xmath74 $ ] of the discrete uniform distribution , we finally arrive at @xmath75 in the finite @xmath12 regime , the variance will be larger , hence we obtain the bound @xmath76 which is equivalent to that given in eq .  ."
  ],
  "abstract_text": [
    "<S> in this letter , we strengthen and extend the connection between simulation and estimation to exploit simulation routines that do not exactly compute the probability of experimental data , known as the likelihood function . </S>",
    "<S> rather , we provide an explicit algorithm for estimating parameters of physical models given access to a simulator which is only capable of producing sample outcomes . </S>",
    "<S> since our algorithm does not require that a simulator be able to efficiently compute exact probabilities , it is able to exponentially outperform standard algorithms based on exact computation . in this way , our algorithm opens the door for the application of new insights and resources to the problem of characterizing large quantum systems , which is exponentially intractable using standard simulation resources .    </S>",
    "<S> much of physics is concerned with modeling complex behavior such that we can simulate systems of interest , and can infer properties of those systems . on one hand , estimating the parameters of physical models given experimental data is critical to many practical objectives , such as precision metrology for frequency standards @xcite , and to probing fundamental questions , such as gravitational wave detection @xcite . on the other hand , by simulating physical models , we can understand properties of the systems that follow those models . </S>",
    "<S> that is , by using simulation to reason about the probabilities of experimental data produced by physical models , we can expose how experimental observations will depend on properties of interest .    </S>",
    "<S> thus , these two concerns are not independent , such that parameter estimation can be broadly thought of as choosing as our estimated model parameters those for which simulations predict the highest probability of obtaining data that agrees with the observed experimental data . </S>",
    "<S> once we have estimated parameters for a model , we can use those parameters to predict the future behavior of an experimental system by simulating according to those parameters . in this way , simulation and statistical estimation are seen to be intimately related .    in this work , we present evidence of this relationship in the case of _ </S>",
    "<S> weak _ simulation , in which one has access only to samples from a simulator rather than the explicit distributions . </S>",
    "<S> this is in contrast to a _ strong _ simulator , which produces the exact probabilities of each possible outcome of an experiment ( see fig . [ </S>",
    "<S> fig : sims ] ) . </S>",
    "<S> the task of estimation is a statistical one and , in the language of statistics , strong simulation is equivalent to explicitly calculating the _ likelihood function_. many common estimation algorithms rely on explicit calculations of likelihood function and , hence , on strong simulation . here , we rectify the situation by providing a method to perform statistical estimation of parameters given access to only a weak simulator . </S>",
    "<S> in addition to being generally applicable in the estimation of physical parameters , our approach is necessary in quantum certification protocols making use of quantum resources @xcite .    </S>",
    "<S> the distinction between strong and weak simulation is particularly important when considering quantum mechanical models , where we are only beginning to broadly appreciate the difference @xcite . in particular </S>",
    "<S> , it has been shown that many quantum mechanical models admit _ efficient _ weak simulation on a classical computer where strong simulation is exponentially more difficult @xcite . </S>",
    "<S> thus , a characterization method that depends only on weak simulation can exhibit a large advantage over strong - simulation characterization methods .     </S>",
    "<S> strong and weak simulators . </S>",
    "<S> a strong simulator computes the value of the likelihood function @xmath0 , given a set of parameters @xmath1 and data @xmath2 . </S>",
    "<S> by contrast , a weak simulator produces sample data @xmath2 , drawn from the likelihood function , given only @xmath1 . ]    </S>",
    "<S> this advantage is especially imperative in the case of quantum information , as the number of parameters that must be measured in a tomographic experiment grows exponentially with the number of qubits . </S>",
    "<S> though tomographic experiments have been carried out in systems as large as several qubits @xcite , the exponential nature of the problem prevents the extension of tomographic methods to large - scale quantum information processing devices , such as those currently being proposed @xcite . </S>",
    "<S> thus , in order to develop useful quantum information processing devices , it is necessary to develop novel and efficient statistical inference methods that can exploit prior information , reductions in model dimension and weak simulation . </S>",
    "<S> since our algorithm needs only a weak simulator , and does not require calculation of the likelihood function itself , we call our algorithm the _ likelihood - free parameter estimation _ ( lfpe ) algorithm .    parameter estimation problems can be phrased in the following general terms . to each physical model is associated a probability distribution @xmath3 , where @xmath2 is the data obtained and where @xmath1 is a vector parameterizing the system of interest . in statistical parlance , this distribution is called the _ </S>",
    "<S> likelihood function_.    now , suppose we have performed experiments and obtained a data set @xmath4 . </S>",
    "<S> we assume that experiments are statistically independent so that the likelihood function becomes @xmath5 however , we are ultimately interested in @xmath6 , the probability distribution of the model parameters @xmath1 given the experimental data . </S>",
    "<S> we obtain this using use bayes rule : @xmath7 where @xmath8 is the _ prior _ , which encodes any _ a priori _ knowledge of the model parameters . </S>",
    "<S> the final term @xmath9 can simply be found implicitly by normalizing the posterior . </S>",
    "<S> since each measurement is statistically independent given @xmath1 , the processing of the data can be done on- or off - line . </S>",
    "<S> that is , we can sequentially update the probability distribution as the data arrive or post - process it afterward . </S>",
    "<S> after all the data have been taken , we report the mean of the posterior distribution as our estimate of the parameters : @xmath10 = \\int \\vec{x } \\pr(\\vec{x}|d ) d\\vec{x}.\\ ] ] this method of parameter estimation is called bayesian learning , and has been shown to be the optimal approach in a more general decision theoretic framework @xcite . </S>",
    "<S> the meaning of this optimality is precisely that eq .   minimizes the mean squared error ( mse ) figure of merit : @xmath11 $ ] .    </S>",
    "<S> in order to efficiently compute the integral expectation in eq .  , we employ the _ sequential monte carlo _ ( smc ) method , which has been used for the purpose of hamiltonian learning @xcite and in the tomographic estimation of one and two qubit states @xcite , and in the continuous measurement of a qubit @xcite .    </S>",
    "<S> the smc method prescribes that we should approximate a distribution over model parameters with a distribution that has support only over a finite number of points ( often referred to as _ particles _ ) . </S>",
    "<S> each particle is assigned a weight , informally thought of as its relative plausibility . more concretely , </S>",
    "<S> we approximate the posterior distribution at the @xmath12th measurement by @xmath13 where the weights at each step are iteratively calculated from the previous step via @xmath14 where @xmath15 is found implicitly by imposing the normalization condition @xmath16 . </S>",
    "<S> the positions @xmath17 of each particle are sampled according to the prior @xmath8 . </S>",
    "<S> the particle approximation can be made arbitrarily accurate by increasing the number of particles . </S>",
    "<S> the initial weights , when no data ( denoted @xmath18 ) has been observed , are given by @xmath19 for all @xmath20 . </S>",
    "<S> this choice is made to ensure that the effective sample size @xmath21 is initially @xmath22 . </S>",
    "<S> as @xmath23 , the algorithm becomes numerically unstable and fails to explore the parameter space ; this may be recovered by a resampling step @xcite . </S>",
    "<S> we explored some variants of this algorithm and presented it in much greater detail in reference @xcite .    </S>",
    "<S> equation suggests that we require a full specification of the likelihood function @xmath3 . </S>",
    "<S> suppose , however , we have access to only a weak simulator , which produces outcomes @xmath24 .. here we use @xmath25 to mean @xmath26 is random variable distributed according to @xmath27 . </S>",
    "<S> later we use @xmath25 to mean @xmath26 is on the order of @xmath27 which is standard asymptotic notation formally defined to mean @xmath28 . </S>",
    "<S> the difference should be clear from context . </S>",
    "<S> ] one extreme is to run the simulator many times and reconstruct @xmath3 from the simulated data  a meta - estimation problem . at the other extreme </S>",
    "<S> is to perform estimation with only one sample per smc particle . </S>",
    "<S> the method truly becomes `` likelihood - free '' as we could not even hope to guess the functional form of the likelihood function from a single sample .    in the extreme case where the weak simulator is used to very accurately compute the likelihood function via repeated sampling , the smc algorithm does not change . at the opposite extreme , when only a single sample is generated from the simulator per particle </S>",
    "<S> , we must modify the algorithm . to this end , suppose we have obtained data @xmath2 from the experiment . for each smc particle , @xmath29 , we request a single sample @xmath30 from our simulator and update the weight as follows : @xmath31    between the two extremes of a single simulator sample per particle and enough to compute the likelihood function nearly exactly , we can _ approximatly _ reconstruct the likelihood function sets of simulated data . in particular , </S>",
    "<S> for each datum @xmath2 and particle @xmath29 , we draw a _ set _ of samples @xmath32 from our simulator . </S>",
    "<S> we then update the weights according to eq . </S>",
    "<S> with estimated likelihood function given by the naive maximum likelihood estimator @xmath33    as mentioned above , we will measure the performance of our algorithms with the mean squared error . in our bayesian setting , this is also the variance of the posterior distribution . since the experiments are assumed independent and identically distributed , the posterior variance will decrease as @xmath34 , where @xmath12 is the total number of measurements . </S>",
    "<S> we appeal to standard monte carlo analyses which suggest that the smc algorithm will increase this variance by at least @xmath35 , where @xmath22 is the number of smc particles . </S>",
    "<S> now , if we use a weak simulator with a fixed experiment and particle number , the same statistical argument suggests that the variance will scale as @xmath36 , where @xmath37 is the number of simulator calls we use ( per particle ) to estimate the likelihood function . </S>",
    "<S> since the total number of samples is @xmath38 , we expect the mean square error to scale as @xmath39 for constants @xmath40 and @xmath41 depending only on the parameters of the problem .    to verify these claims , we perform numerics . </S>",
    "<S> our example is that of a noisy photodetector where the efficiency of the photon source is @xmath42 , which we would like to estimate . </S>",
    "<S> this value is equivalent to the probability for the detector to click in the presence of no noise . in reality , _ dark counts _ register clicks when no photon is present and _ losses _ register no clicks when a photon is present . </S>",
    "<S> let these happen with probability @xmath43 and @xmath44 , respectively . </S>",
    "<S> then , given @xmath42 , the probability for a click to actually happen is @xmath45 . from these clicks , </S>",
    "<S> our task is to estimate @xmath42 .    </S>",
    "<S> aysmptotically , the posterior variance is given by the inverse of the fisher information evaluated at , for example , the maximum likelihood estimate @xcite . </S>",
    "<S> the details of this calculation are presented in the appendix . </S>",
    "<S> the result is the asymptotic bound @xmath46 which we will use to verify our algorithm is near optimal . in practice , @xmath42 will be a function of some parameters of interest , @xmath47 . </S>",
    "<S> we restrict ourselves to this example in order to illustrate the effects on inference due to weak simulation .    </S>",
    "<S> first , we verify that , given a fixed number of experiments , the mse scales as @xmath35 ( where @xmath22 is again the number of smc particles ) for both the strong simulating smc algorithm and likelihood - free weak simulation . </S>",
    "<S> the data , plotted in fig . </S>",
    "<S> [ fig : v_part ] ( left ) , bears out our expectations quite convincingly ; even in the case of a _ single sample _ from the simulator , the accuracy can be increased ( at the expected @xmath35 rate ) until it reaches the bound given by eq . . </S>",
    "<S> next , in fig . </S>",
    "<S> [ fig : v_part ] ( middle ) , we show that fixing the number of particles and varying the number of simulations per particle , @xmath37 , results in an mse that scales as @xmath36 . thus , </S>",
    "<S> as expected , the more accurately we can compute the likelihood function , the better our accuracy will be  but only up to a certain point . </S>",
    "<S> that is , it is not advantageous to continue improving the accuracy of the estimate of the likelihood function beyond roughly @xmath48 since the errors from finite particles and samples will begin to dominate .    </S>",
    "<S> on the other hand , the strategy of estimating the likelihood via samples ignores the cost of simulation . </S>",
    "<S> the total number of simulations is @xmath38 , the number of particles times the number of samples per particle . in fig . </S>",
    "<S> [ fig : v_part ] ( right ) , we plot the mse against the _ total _ number of simulator calls and find that , perhaps surprisingly , the likelihood  free approach of using a single simulator sample is best .         in the above arguments , the total number of measurements was held fixed to verify the performance as a function of the algorithmic parameters . </S>",
    "<S> if , on the other hand , simulations are relatively cheap compared to obtaining experimental samples , we would like to optimize performance by finding the appropriate number of simulated experiments without going beyond the redundancy noted above . </S>",
    "<S> however , in most cases , the limit of accuracy is not _ a priori _ known . in such cases , </S>",
    "<S> we have devised an algorithm we call _ adaptive likelihood estimation _ ( ale ) . </S>",
    "<S> essentially , our algorithm adaptively calls the simulator until we deem the accuracy in our estimate sufficient .    for brevity </S>",
    "<S> , we will discuss the binary case with outcomes labeled @xmath49 and @xmath50 . </S>",
    "<S> the unknown probability @xmath51 can be treated as a parameter to be estimated . in particular , since we have assumed that the data are conditionally independent given the model , repeatedly sampling the likelihood function will produce data that follows a binomial distribution with parameter @xmath52 . estimating the parameter of a binomial distribution from sample data is a well - understood statistical problem . </S>",
    "<S> supposing @xmath20 @xmath49s were observed in @xmath37 trials , a typical estimator is @xmath53 where @xmath54 is a free parameter . </S>",
    "<S> these are called `` linear '' or `` add-@xmath54 '' estimators @xcite . </S>",
    "<S> the latter phrase is due to the equivalence to standard maximum likelihood estimation when adding @xmath54 fictitious observations  also termed `` hedging '' @xcite . </S>",
    "<S> these estimators can also be understood to arise from a bayesian approach as well . in particular , the estimator in eq .   </S>",
    "<S> is the posterior mean when using the following _ beta distribution _ as a prior @xcite @xmath55 the posterior variance of this distribution can also be calculated as @xmath56 here we will use the value @xmath57 , as it corresponds to a uniform prior distribution . </S>",
    "<S> we leave the optimization of this algorithmic parameter for future work .    </S>",
    "<S> if we are willing to tolerate an error @xmath58 in our reconstruction of the likelihood , then we can check after each sample if @xmath59 . </S>",
    "<S> if not , we collect more samples until the condition is met . </S>",
    "<S> we therefore have a single quality parameter for this adaptive protocol : @xmath58 . </S>",
    "<S> since this is our estimate of the variance in the estimate of the likelihood function , the mse is expected to scale as @xmath60 ( for fixed measurement and particle number ) . </S>",
    "<S> thus , as discussed above , the optimal choice will be @xmath61 since anything smaller will fast result in diminished returns </S>",
    "<S>  the mse will be limited by either the number of measurements @xmath12 or particles @xmath22 , depending on which is smaller . </S>",
    "<S> we illustrate this with our example in fig . </S>",
    "<S> [ fig : ale ] .     </S>",
    "<S> the mse using lfpe with adaptive likelihood estimation ( ale ) as a function of the ( inverse of the ) ale tolerance @xmath58 . as claimed </S>",
    "<S> , the mse scales as @xmath60 until it reaches the bound . </S>",
    "<S> the parameters of the problem are as in fig . </S>",
    "<S> [ fig : v_part ] . ]    in this work , we have demonstrated an improvement of the sequential monte carlo parameter estimation algorithm that allows for its extension to the case of weak ( sampling ) simulators . for models with fast weak simulation available , our algorithm can be seen to provide dramatic advantages in terms of classical computing costs over sequential monte carlo alone and at minimal cost in estimation performance . </S>",
    "<S> this extension allows for us to perform inference in subtheories of quantum mechanics that admit a large separation between the tractibility of strong and weak simulation .    </S>",
    "<S> we have necessarily demonstrated these improvements for an example model in which the analytical solution was tractable . in practice , </S>",
    "<S> if only weak simulation is available , then standard approaches to parameter estimation making use of calculations of the likelihood function do not apply and our method is necessary . within the confines of quantum theory , </S>",
    "<S> an ever growing class of weak simulation schemes have been proposed which have been proven to have an exponential separation in computational complexity between weak and strong simulation . </S>",
    "<S> in addition to the large class of circuits identified by van den nest _ et al _ , others include simulating the evolution of states with positive wigner function @xcite . in such cases </S>",
    "<S> , lfpe provides an exponential improvement in accuracy for a fixed amount of computational resources . </S>",
    "<S> more recently , there have been proposals for the use of quantum resources ( necessarily weak simulators ) to aid in overcoming the complexity in simulating the physical model @xcite . </S>",
    "<S> such ideas could mitigate the need for classical simulators to certify near - future quantum devices which go beyond the classical regime , such as bosonsamplers @xcite . </S>",
    "<S> as the complexity of candidate quantum information processors grows , our algorithm provides a way forward to estimating properties of very large systems by exploiting the deep connection between simulation and estimation .    </S>",
    "<S> cf acknowledges funding from nsf grants phy-1212445 and phy-1005540 as well as nserc of canada . </S>",
    "<S> cg acknowledges support from the canadian excellence research chairs ( cerc ) program . </S>",
    "<S> the authors thank josh combes , d.g . </S>",
    "<S> cory , akimasa miyake , nathan wiebe , dan puzzuoli and `` referee b '' for helpful discussions and suggestions on improving the presentation . </S>"
  ]
}