{
  "article_text": [
    "many important tasks in machine learning involve predicting output labels that must satisfy certain joint constraints .",
    "these constraints help restrict the search space and incorporate domain knowledge about the task at hand .",
    "for example , in hierarchical multi - label classification , the goal is to predict , given an input , a set of labels that satisfy hierarchical constraints imposed by a known taxonomy .",
    "more generally , the output space can also depend on the observed inputs , which is commonly the case in language , vision and speech applications .",
    "collectively , these problems are broadly referred to as _ structured prediction_. formally , the goal of structured prediction is to learn a predictor @xmath0 that maps an input @xmath1 to some @xmath2 in a structured output space @xmath3 . unlike the classical problems of binary classification ( @xmath4 ) and regression",
    "( @xmath5 ) , a defining challenge in structured prediction is that @xmath3 can be a highly complex space , usually representing combinatorial structures like trees , matchings , and vertex label assignments on graphs , or any arbitrary set of vectors in a real vector space .",
    "the difficulty of the task naturally depends on the geometry of @xmath3 and the choice of loss function @xmath6 .",
    "many existing structured prediction methods can be understood as learning a scoring function @xmath7 , which assigns to each @xmath8 its compatibility score with some input @xmath9 .",
    "a prediction is made by solving an auxiliary optimization problem that finds some @xmath10 .",
    "conditional random fields ( crfs ) @xcite , max - margin markov networks ( m@xmath11n ) @xcite and structured support vector machines ( ssvm ) @xcite are three commonly used methods that fit into this framework .",
    "crfs directly model the conditional distribution @xmath12 with a graphical model , thus maximizing the compatibility score is equivalent to maximum a posteriori estimation . in m@xmath11n and ssvm , @xmath7 is expressed in a linear ( or log linear ) form @xmath13 , where @xmath14 is a joint feature representation of the input - output pair .",
    "the weight vector vector @xmath15 is found by solving a generalized max - margin problem that maximizes separation of the true labels from others by their score differences , which are usually augmented with a loss function . in this paper",
    ", we propose an approach to structured prediction by _",
    "least squares estimated conditional risk minimization _ , which can also be characterized as one that learns a scoring function @xmath7 .",
    "specifically , the scoring function ( to be minimized over ) can be interpreted as a direct estimate of the _ conditional risk function _ , @xmath16 $ ] , where @xmath17 is a loss function of choice .",
    "notably , like m@xmath11n and ssvm , our approach can also use kernels to learn from a rich class of hypotheses , but avoids the nontrivial _ pre - image problem _ of the former , which involves finding some @xmath2 that maximizes the inner product @xmath13 .",
    "our main contributions are summarized as follows : ( i ) we derive a closed form expression for the estimated conditional risk function , in the form of a weighted sum of loss functions , as solutions of a collection of regularized least squares problems ( section 2 ) ( ii ) we show that in a certain class of problems with discrete combinatorial outputs and additive pairwise losses , the auxiliary problem can be solved efficiently by exact linear programming relaxation ( section 3 ) ( iii ) we extend this approach to convex output spaces and learning additive models of conditional risks ( section 4 ) ( iv ) we derive an excess risk bound for the algorithm under a parametric class of @xmath18 and finite @xmath3 , and show that it scales logarithmically with @xmath19 ( section 5 ) . when applied to hierarchical multilabel classification ( section 6 ) , our approach achieves predictive accuracies that are competitive with several existing algorithms , with added computational advantages that allow it to scale well with the hierarchy size .",
    "throughout this paper , we represent @xmath20 and @xmath21 as collections of vectors in a real vector space . depending on the application",
    ", @xmath3 can be discrete ( e.g. , a combinatorial subset of @xmath22 ) or otherwise ( e.g. a polytope ) .",
    "as in most supervised learning settings , we have a training set , @xmath23 $ ] , where the samples are i.i.d . and drawn from some fixed distribution @xmath24 .",
    "we define @xmath16 $ ] to be the conditional risk of predicting label @xmath2 having observed an input @xmath9 , and use @xmath25 to denote its estimate . here the conditional expectation is defined with respect to @xmath26 . in summary ,",
    "the proposed method can be described in two steps :    1 .",
    "* training * : learn a set of functions indexed by @xmath2 , @xmath27 by solving a ( possibly infinite ) collection of regularized least squares problems .",
    "* prediction * : given an input @xmath9 , predict @xmath28 by solving an auxiliary optimization problem , @xmath29      we begin with two basic observations : for any fixed @xmath8 , ( i ) the random loss function @xmath30 can be written as @xmath31 , where @xmath32 is a zero - mean random variable that depends ( randomly ) on @xmath33 and ( deterministically ) on @xmath2 ( ii ) @xmath34 $ ] .",
    "this decomposition suggests a regression approach to estimating @xmath35 : for every @xmath8 , we posit that @xmath35 lies in some function space @xmath36 and estimate it by solving a regularized least squares problem , @xmath37 where @xmath38 is a norm over @xmath39 and @xmath40 is a regularization parameter .",
    "we define @xmath39 to be a reproducing kernel hilbert space ( rkhs ) spanned by a real , symmetric positive definite kernel @xmath41 .",
    "this enables us to learn from a rich class of functions and reduces the problem to kernel ridge regression ( krr ) @xcite , leading to the following closed form solution .",
    "[ pro : rls ] for every @xmath8 , an optimal solution for the regularized least squares problem can be expressed as @xmath42 where @xmath43 is the @xmath44-th element of @xmath45 . here",
    "@xmath46_{ij},\\forall i , j\\in\\{1,\\ldots , m\\}$ ] is the @xmath47 gram matrix and @xmath48_{i=1}^m$ ] is a @xmath49 vector .",
    "because the above holds true for all @xmath8 , we have a complete characterization of @xmath50 as the weighted sum of the individual loss functions @xmath51 induced by @xmath52 , with weights @xmath53 that only depend on @xmath9 and @xmath54 .",
    "this property arises from the fact that krr is a _ linear smoother _ @xcite .",
    "such linear combinations of loss functions also arise in instance - based learning methods like knn .",
    "however , these methods typically rely on a weighting of neighboring samples , whereas ( [ eq : rlssol ] ) is derived from least squares estimation over the whole data set .",
    "using krr provides added modeling flexibility through kernels , which can either be chosen based on domain knowledge or learned from data @xcite . for comparison with existing structured prediction methods , it is useful to think of @xmath55 as an alternative scoring function to that of m@xmath11n and ssvm , which also admits a linear form @xmath13 .",
    "the key difference is that we do not define a joint feature map @xmath14 , but instead have a linear expression in which @xmath9 and @xmath2 are decoupled .",
    "this avoids the pre - image problem of finding a @xmath2 that maximizes @xmath13 when predicting an output , which is nontrivial in general @xcite .",
    "another related method is kernel dependency estimation ( kde ) @xcite and its extensions @xcite , which are also based on krr .",
    "however , kde differs from our approach in that regression is used to learn a mapping from inputs to a feature space associated with @xmath3 , whose output is then mapped from the feature space back to @xmath3 by solving a pre - image problem . in place of the pre - image problem , our method requires solving an auxiliary optimization problem , which we discuss in the next section .",
    "given an input @xmath9 , we first compute @xmath45 .",
    "this is done by first forming @xmath56 and then either solving a linear system , or multiplying @xmath57 with the inverted matrix if already pre - computed in training .",
    "both can be done efficiently in moderately sized problems .",
    "@xcite or a divide - and - conquer approach for krr @xcite can be considered .",
    "] a prediction @xmath58 is computed by solving an auxiliary optimization problem that minimizes the estimated conditional risk @xmath55 , @xmath59 the difficulty of this problem crucially depends on the geometry of @xmath3 , the choice of @xmath17 and in some cases the signs of @xmath53 . in section 3 , we will apply ls - ecrm to a range of settings and provide conditions under which this auxiliary problem can solved efficiently .",
    "before we consider more complex settings , it is useful to understand what it means to apply ls - ecrm to the simple case of binary classification . with the zero - one loss , @xmath60 ,",
    "@xmath18 is simply the conditional probability of misclassification , @xmath61 . minimizing its least squares estimate , we obtain the following decision rule from ( [ eq : auxprob ] ) , @xmath62 we now show that this is equivalent to regularized least squares classification ( rlsc ) @xcite . in rlsc , one posits that the classification rule is of the form @xmath63 and estimates an @xmath64 by solving a regularized least squares problem : @xmath65 here @xmath39 is a rkhs associated with a kernel @xmath66 . despite using the square loss in place of the more common hinge loss of svm , rlsc has been shown to achieve similar performance as svm in practice @xcite .",
    "the following proposition states that @xmath67 is equivalent to ( [ eq : ecrmbinary ] ) .",
    "let @xmath53 be defined as in proposition [ pro : rls ] .",
    "then , @xmath68 if and only if @xmath69    thus ls - ecrm can be viewed as a generalization of rlsc . from the equivalence above , we can also interpret rlsc as minimizing the least squares estimated misclassification probability .",
    "in this section , we consider @xmath70 and any loss function @xmath71 that can be written as the sum of functions of pairwise elements in @xmath2 and @xmath72 .",
    "specifically , we define @xmath3 to be a set of points in @xmath22 that satisfy the following set of linear constraints , @xmath73 where @xmath74 are ( possibly empty ) disjoint sets of indices such that their union is @xmath75 , and @xmath76 . in other words , there are a total of @xmath77 linear constraints , each can either be an inequality or an equality .",
    "this characterization of @xmath3 is fairly general as many objects of interest , including matchings , permutations and label assignments in graphs , can be represented as such . for convenience",
    ", we will define @xmath78^t$ ] to be the _ constraint matrix _ and @xmath79^t$ ] .",
    "one property of matrices that will be useful to us later is _ total unimodularity _ , defined as follows .",
    "( total unimodularity ) a matrix @xmath80 is totally unimodular if every square submatrix of @xmath80 has determinant @xmath81 or @xmath82 .",
    "now consider additive pairwise losses of the form , @xmath83 .",
    "for example , the hamming loss corresponds to @xmath84 . under this loss and our definition of @xmath3 , the auxiliary problem in ( [ eq : auxprob ] ) is a discrete optimization problem , @xmath85    where @xmath86 is defined to be a polyhedron in @xmath87 that is characterized by all the linear constraints in ( [ eq : linconstraints ] ) .",
    "this problem is @xmath88-complete in general . in the following theorem",
    ", we provide sufficient conditions under which the problem can be solved efficiently by exact linear programming relaxation .",
    "[ thm : lprelax ] if @xmath80 is totally unimodular and @xmath89 , then for any @xmath90 , an optimal solution of the auxiliary problem can be found by solving the following linear program , @xmath91    total unimodularity is an important property in combinatorial optimization .",
    "there are many results on classes of matrix that satisfy this property ( see , e.g. , @xcite ) . in what follows",
    ", we will specialize this method to two problems and show how theorem [ thm : lprelax ] can be applied , leading to new algorithms .      in hmc",
    ", we have a set of labels @xmath92 organized in a hierarchical structure ( e.g. , a hierarchy of topics in text classification ) .",
    "the goal is to predict a subset of @xmath93 that corresponds to an input @xmath9 .",
    "let @xmath94 denote whether each label @xmath95 is chosen ( 1 ) or not ( 0 ) .",
    "in addition to choosing a subset of @xmath93 , we consider a variant of this problem where @xmath2 must also satisfy the following hierarchical constraints :    * for each @xmath96 : if @xmath97 , then @xmath98 . here",
    "@xmath99 denotes the set of immediate parent labels under which @xmath100 belongs in the hierarchy .",
    "the hierarchy is commonly represented as a tree or ( more generally ) a directed acyclic graph ( dag ) , with each node being a label in @xmath93 and each arc @xmath101 encoding a parent - child relation , @xmath102 .",
    "formally , we define the dag as @xmath103 , where @xmath93 is defined as above and @xmath104 iff @xmath102 .",
    "the resulting output space @xmath3 can be succintly described with @xmath105 linear constraints ( in addition to @xmath106 ) , @xmath107 a common choice of loss function in hmc is the hamming loss , @xmath108 , which is simply the fraction of incorrect labels .",
    "this is a special case of the additive pairwise losses defined in the preceding section .",
    "we show that the auxiliary problem can be solved efficiently below .",
    "[ pro : hmc ] for any dag , @xmath103 , the auxiliary problem for hmc with hamming loss is equivalent to the following linear program , @xmath109    given @xmath53 , the cost coefficients of the linear program can be computed in @xmath110 .",
    "the size of the linear program ( in terms of variable and constraint counts ) scales linearly with @xmath111 and @xmath105 , but is otherwise independent of @xmath112 . in practice",
    ", linear programs can be solved very efficiently with state - of - the - art optimization solvers @xcite . as we will see in section [ sec : experiments ] , this makes the method scale well to large hierarchies .",
    "suppose we are interested in predicting a ranking over all labels , rather than only choosing a subset .",
    "we consider a setting where the training set consists of complete permutations @xmath113 over the label set @xmath93 and their associated inputs .",
    "the goal is to learn to predict a permutation @xmath114 given an input @xmath9 , where @xmath115 indicates the rank of label @xmath100 for every @xmath116 . to measure the loss",
    ", we will use the normalized _ spearman s footrule distance _ , @xmath117 , which sums the absolute differences of the two ranks over all labels and normalizes it to within @xmath118 $ ] . by representing @xmath114 as a binary vector",
    ", we will show that the auxiliary problem can be solved by exact linear programming relaxation .",
    "let us define @xmath119 , as a vector in @xmath120 that corresponds to some @xmath114 as follows : ( @xmath121 ) @xmath122 iff @xmath123 .",
    "the set of such vectors ( each corresponding to a distinct @xmath114 ) , denoted @xmath124 , are exactly characterized by the following linear constraints .",
    "@xmath125 this can be interpreted as the set of perfect matchings in a complete bipartite graph . with this representation , we can equivalently write @xmath126 as @xmath127 , where each summation over @xmath128 is from @xmath82 to @xmath111 .",
    "this is again a special case of the additive pairwise losses defined earlier .",
    "together with the fact that the constraint matrix associated with bipartite matchings is known to be totally unimodular @xcite , we can apply theorem [ thm : lprelax ] to reduce the auxiliary problem to the following min - cost _ assignment problem_.    [ pro : rank ] the auxiliary problem for multilabel ranking with spearman s footrule distance is equivalent to the following linear program , @xmath129",
    "we briefly describe how ls - ecrm can be extended to predict a vector in a convex set @xmath3 , under a loss function @xmath71 that is convex in @xmath2 ( e.g. , @xmath130 and @xmath131 losses ) . in principle , proposition [ pro : rls ] still applies but the auxiliary problem in ( [ eq : auxprob ] ) is now a continuous optimization problem .",
    "also , the convexity of @xmath132 implies that @xmath133 is convex for every @xmath9 . however , this need not be true for @xmath134 if some element of @xmath53 is negative , which can result in a non - convex auxiliary problem .",
    "one approach is to solve the problem approximately by minimizing its convex upper bound @xmath135 , which implies that @xmath136 while we do not provide examples of application in this paper , we note that the problem of learning a mapping from inputs to a convex set has been studied in the more general setting of conditional stochastic optimization using local estimators @xcite@xcite such as knn and kernel smoothing .",
    "as seen in examples of the previous section , many loss functions considered in structured prediction are additive over substructures of @xmath3 .",
    "it is often desirable to decompose the learning problem over these substructures , so that any local features can be exploited for better generalization performance .",
    "here we show how ls - ecrm can be extended for this purpose .",
    "consider @xmath137 , where each @xmath138 is defined over a subset of elements in @xmath139 that correspond to the index set @xmath140 , i.e. , @xmath141 ( likewise , for @xmath142 ) . as a result",
    ", we can also express the conditional risk function in additive form , @xmath143 with @xmath144 $ ] , and then estimate @xmath145 separately for every @xmath146 .",
    "this decomposition is useful in several scenarios .",
    "for example , with respect to each @xmath146 , we may : ( i ) tune the regularization and kernel parameters in ( [ eq : rls ] ) ( ii ) apply feature selection so that @xmath147 depends only on a subset of inputs most relevant for predicting @xmath148 . by solving the regularized least squares problem in ( [ eq : rls ] ) ,",
    "we obtain for every @xmath146 , @xmath149 , where @xmath150 is a weight vector specific to @xmath146 .",
    "these results can then be combined to form @xmath151 and the auxiliary problem can be solved similarly as in ( [ eq : auxprob ] ) .",
    "theorem [ thm : lprelax ] , proposition [ pro : hmc ] and [ pro : rank ] can also be extended to hold in this case if the losses are in the form described in section [ ssec : binvector ] . on the other hand ,",
    "such decompositions incur additional computational costs because a separate weight vector has to be computed for each @xmath146 .",
    "deciding the granularity of @xmath146 is thus a matter of tradeoff between predictive and computational performance , and largely depends on the application .",
    "we analyze the statistical properties of ls - ecrm under the assumption that @xmath152 for a parametric class of functions @xmath39 .",
    "specifically , for every @xmath8 , we assume that there exists some @xmath153 such that @xmath154 , where @xmath155 is a mapping from the input space to a euclidean _ feature space _ with finite dimension @xmath156 .",
    "thus ( [ eq : rls ] ) can be reduced to linear regression in the feature space,$ ] , which is unpenalized if @xmath157 . ]",
    "@xmath158 we will only consider the setting where @xmath157 and the _ design matrix _ , @xmath159^t\\in\\mathbb{r}^{m\\times q}$ ] is observed and non - random , which is analogous to the _ fixed design _ setting in linear regression . under these assumptions , we derive the following excess risk bound .",
    "let @xmath160 and @xmath161 be a ls - ecrm and bayes optimal predictor , respectively .",
    "suppose that ( i ) @xmath3 is finite ( ii ) @xmath162 ( iii ) @xmath163\\leq b$ ] .",
    "then @xmath164-\\mathbb{e}[\\ell(h^*(x),y)]\\leq 4bl\\sqrt{\\dfrac{r(4\\log{|\\mathcal{y}|+3)}}{\\nu m}},\\ ] ] where @xmath165 and @xmath166 are the rank and the smallest eigenvalue of @xmath167 , respectively .",
    "for example , in the hmc problem with hamming loss , we have @xmath168 and @xmath169 .",
    "this gives a bound of @xmath170 , where @xmath111 is the number of labels .",
    "we apply ls - ecrm in hmc using the formulation in section [ ssec : hmc ] and compare it with several existing algorithms , each based on a different paradigm : ( i ) br - svm : a _ binary relevance _ approach : a svm classifier is trained for each node .",
    "the predictions are imputed from the bottom up to satisfy the hierarchical constraints .",
    "( ii ) h - svm : a hierarchical approach @xcite : at each node , a svm classifier is trained using only samples in which its parents are positively labeled .",
    "node labels are predicted recursively from the top down , terminating at any node that is negatively labeled .",
    "( iii ) clus - hmc @xcite : a decision tree approach .",
    "each method is evaluated on hmc benchmark data sets from three domains : text ( enron , rcv1v2 ) , image ( imclef07a , imclef07d ) and functional genomics ( pheno_go , spo_go , gasch1_go , gasch2_go ) .",
    "a summary of these data sets are available in the appendix . for consistency",
    ", we do not apply any feature selection .",
    "preprocessing is only done for the genomics data sets , where we trim the original hierarchy with over @xmath171 nodes down to a few hundreds .",
    "we implement our method in julia @xcite and solve the auxiliary linear programs with clp @xcite .",
    "both br - svm and h - svm are implemented using libsvm @xcite . for clus - hmc , bagging is used to train an ensemble of decision trees , which we have observed offer substantial accuracy gains over a single decision tree .",
    "ls - ecrm , br - svm and h - svm all use the rbf kernel .",
    "the parameters are tuned on the training sets by grid search with cross validation .",
    "all tests are run on a quad - core 3.5ghz cpu with 8 gb ram .",
    "for each data set , we compute the auprcs ( area under precision - recall curve ) for all methods within the intersection of their operating regions and the highest f - scores in their respective pr curves .",
    "this is shown in table [ tbl : accuracy ] . to find out how the methods scale computationally with hierarchy size",
    ", we evaluate the training and prediction time on gasch1_go over label counts ranging from @xmath172 to @xmath173 , obtained by truncating the original hierarchy .",
    "the results are presented in figure [ fig : time ] .",
    "as table [ tbl : accuracy ] shows , ls - ecrm and br - svm perform the best overall in terms of both metrics , leading the other two methods by significant margins in several data sets .",
    "while ls - ecrm has only a slight lead over br - svm in predictive performance , its training and and prediction time can be several orders of magnitude faster , as figure [ fig : time ] shows . whereas br - svm requires training a separate classifier at each node ,",
    "training ls - ecrm only involves forming the kernel matrix and computing an inversion , which is independent of the hierarchy size .",
    "this is a unique property not shared by other methods under our evaluation .",
    "predicting with ls - ecrm requires solving linear programs , which we have observed empirically tend to terminate in few iterations without exhaustive search , in spite of growing hierarchy size .",
    "this gives it a clear advantage over br - svm , which does not take into account of the problem structure to search efficiently .",
    "h - svm also has a flattening trend in both training and prediction time , which can be explained by its top - down hierarchical approach : even as the hierarchy grows , the number of positively labeled instances generally diminish ( for a fixed sample size ) as one descends down the hierarchy , resulting in early termination of the algorithm . in terms of prediction time",
    ", clus - hmc has a clear lead over the rest , though its gap with ls - ecrm narrows with increasing label counts .",
    "its predictive performance , however , trails behind in several data sets .",
    ".the f - scores and auprcs of each method on various data sets .",
    "for each data set , the rankings of methods by each metric are shown in parentheses , and the best results are in bold . [ cols=\"<,^,^,^,^,^,^,^,^ \" , ]      1 .",
    "http://kt.ijs.si/dragikocev/phd/resources/doku.php?id=hmc_classification 2 .",
    "https://dtai.cs.kuleuven.be/clus/hmcdatasets 3 .",
    "rcv1v2 ( topics ; subset1 ) .    for imclef07a and imclef07d",
    ", we only used the first 2000 training samples out of the 10000 . in the last 4 data sets ,",
    "we combined the training and validation set into a single training set for cross - validated parameter tuning . furthermore , only the first connected component of the full hierarchy ( > 4000 labels ) is used , which is then trimmed down by discarding labels with less than 3 positive instances .",
    "the precision - recall curves are shown in figure [ fig : pr ] . for ls - ecrm",
    ", the curve is obtained by varying the relative costs of false positives and false negatives by modifying the hamming loss , @xmath174 for some penalty factor @xmath175 and @xmath176 .",
    "this can be transformed into a linear objective in ( [ eq : lphmc ] ) .",
    "for br - svm , h - svm and clus - hmc , it is done by varying the classification thresholds .",
    "99 schlkopf , b. , herbrich , r. , & smola , a. j. ( 2001 ) . a generalized representer theorem . in computational learning theory",
    "416 - 426 ) .",
    "springer berlin heidelberg .",
    "steiglitz , k. , & papadimitriou , c. h. ( 1982 ) .",
    "combinatorial optimization : algorithms and complexity .",
    "printice - hall , new jersey .",
    "bertsimas , d. , & tsitsiklis , j. n. ( 1997 ) .",
    "introduction to linear optimization ( vol .",
    "65 - 67 ) .",
    "belmont , ma : athena scientific .",
    "conforti , m. , cornujols , g. , & zambelli , g. ( 2014 ) .",
    "integer programming ( vol .",
    "berlin : springer .",
    "rigollet , p. ( 2015 ) 18.s997 : high dimensional statistics lecture notes . retrieved from http://www-math.mit.edu/~rigollet/pdfs/rignotes15.pdf .",
    "honorio , j. , & jaakkola , t. ( 2014 ) .",
    "tight bounds for the expected risk of linear classifiers and pac - bayes finite - sample guarantees .",
    "hastie , t. , tibshirani , r. , friedman , j. , & franklin , j. ( 2005 ) .",
    "the elements of statistical learning : data mining , inference and prediction .",
    "the mathematical intelligencer , 27(2 ) , 83 - 85 .",
    "cortes , c. , mohri , m. , & rostamizadeh , a. ( 2012 ) .",
    "algorithms for learning kernels based on centered alignment .",
    "the journal of machine learning research , 13(1 ) , 795 - 828 ."
  ],
  "abstract_text": [
    "<S> we propose a general approach for supervised learning with structured output spaces , such as combinatorial and polyhedral sets , that is based on minimizing estimated conditional risk functions . given a loss function defined over pairs of output labels , we first estimate the conditional risk function by solving a ( possibly infinite ) collection of regularized least squares problems . a prediction is made by solving an auxiliary optimization problem that minimizes the estimated conditional risk function over the output space . we apply this method to a class of problems with discrete combinatorial outputs and additive pairwise losses , and show that the auxiliary problem can be solved efficiently by exact linear programming relaxations in several important cases , including variants of hierarchical multilabel classification and multilabel ranking problems . </S>",
    "<S> we demonstrate how the same approach can also be extended to vector regression problems with convex constraints and losses . </S>",
    "<S> evaluations of this approach on hierarchical multilabel classification show that it compares favorably with several existing methods in terms of predictive accuracy , and has computational advantages over them when applied to large hierarchies . </S>"
  ]
}