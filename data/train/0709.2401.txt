{
  "article_text": [
    "over recent years , computational linguistics has benefitted considerably from advances in statistical modelling and machine learning , culminating in methods capable of deeper , more accurate automatic analysis , over a wider range of languages .",
    "implicit in much of this work , however , has been the existence of * deep language resources * ( dlr hereafter ) of ever - increasing linguistic complexity , including lexical semantic resources ( e.g.  wordnet and framenet ) , precision grammars ( e.g.  the english resource grammar and the various pargram grammars ) and richly - annotated treebanks ( e.g.  propbank and ccgbank ) .    due to their linguistic complexity , dlrs",
    "are invariably constructed by hand and thus restricted in size and coverage .",
    "our aim in this paper is to develop general - purpose automatic methods which can be used to automatically expand the coverage of an existing dlr , through the process of * deep lexical acquisition * ( dla hereafter ) .",
    "the development of dlrs can be broken down into two basic tasks : ( 1 ) design of a data representation to systematically capture the generalisations and idiosyncracies of the dataset of interest ( * system design * ) ; and ( 2 ) classification of data items according to the predefined data representation ( * data classification * ) . in the case of a deep grammar , for example , system design encompasses the construction of the system of lexical types , templates , and/or phrase structure rules , and data classification corresponds to the determination of the lexical type(s ) each individual lexeme conforms to .",
    "dla pertains to the second of these tasks , in automatically mapping a given lexeme onto a pre - existing system of lexical types associated with a dlr .",
    "we propose to carry out dla through a bootstrap process , that is by employing some notion of word similarity , and learning the lexical types for a novel lexeme through analogy with maximally similar word(s ) for which we know the lexical types . in this",
    ", we are interested in exploring the impact of different secondary language resources ( lrs ) on dla , and estimating how successfully we can expect to learn new lexical items from a range of lr types . that is , we estimate the expected dla `` bang for the buck '' from a range of secondary lr types of varying size and complexity . as part of this ,",
    "we look at the relative impact of different lrs on dla for different open word classes , namely nouns , verbs , adjectives and adverbs .    we demonstrate the proposed dla methods relative to the english resource grammar ( see section  [ sec : erg ] ) , and in doing so assume the lexical types of the target dlr to be syntactico - semantic in nature .",
    "for example , we may predict that the word _ dog _ has a usage as an intransitive countable noun ( ` n_intr_le ` , .",
    "_ the barked _ ) , and also as a transitive verb ( ` v_np_trans_le ` , cf .",
    "_ it my every step _ ) .",
    "a secondary interest of this paper is the consideration of how well we could expect to perform dla for languages of differing density , from `` low - density '' languages ( such as walpiri or uighur ) for which we have limited lrs , to `` high - density '' languages ( such as english or japanese ) for which we have a wide variety of lrs . to this end , while we exclusively target english in this paper , we experiment with a range of lrs of varying complexity and type , including morphological , syntactic and ontological lrs .",
    "note that we attempt to maintain consistency across the feature sets associated with each , to make evaluation as equitable as possible .",
    "the remainder of this paper is structured as follows .",
    "section  [ sec : task ] outlines the process of dla and reviews relevant resources and literature .",
    "sections  [ sec : morphology ] , [ sec : corpus ] and [ sec : ontology ] propose a range of dla methods based on morphology , syntax and ontological semantics , respectively .",
    "section  [ sec : evaluation ] evaluates the proposed methods relative to the english resource grammar .",
    "this research aims to develop methods for dla which can be run automatically given : ( a ) a pre - existing dlr which we wish to expand the coverage of , and ( b ) a set of secondary lrs / preprocessors for that language .",
    "the basic requirements to achieve this are the discrete inventory of lexical types in the dlr , and a pre - classification of each secondary lr ( e.g.  as a corpus or wordnet , to determine what set of features to employ ) . beyond this",
    ", we avoid making any assumptions about the language family or dlr type .    the dla strategy we propose in this research is to use secondary lr(s ) to arrive at a feature signature for each lexeme , and map this onto the system of choice indirectly via supervised learning , i.e.  observation of the correlation between the feature signature and classification of bootstrap data .",
    "this methodology can be applied to unannotated corpus data , for example , making it possible to tune a lexicon to a particular domain or register as exemplified in a particular repository of text . as it does not make any assumptions about the nature of the system of lexical types",
    ", we can apply it fully automatically to any dlr and feed the output directly into the lexicon without manual intervention or worry of misalignment .",
    "this is a distinct advantage when the inventory of lexical types is continually undergoing refinement , as is the case with the english resource grammar ( see below ) .",
    "a key point of interest in this paper is the investigation of the relative `` bang for the buck '' when different types of lr are used for dla .",
    "crucially , we investigate only lrs which we believe to be plausibly available for languages of varying density , and aim to minimise assumptions as to the pre - existence of particular preprocessing tools .",
    "the basic types of resources and tools we experiment with in this paper are detailed in table  [ tab : resource - types ] .",
    "lll + [ 0pt]_*secondary lr type * _ & [ 0pt]_*description * _ & [ 0pt]_*preprocessor(s ) * _ +    word list@xmath0 & list of words with basic pos &  + morphological lexicon@xmath1 & derivational and inflectional word relations &  + compiled corpus@xmath0 & unannotated text corpus & pos tagger@xmath2 + & & chunk parser@xmath1 + & & dependency parser@xmath1 + wordnet - style ontology@xmath1 & lexical semantic word linkages &  +    [ tab : resource - types ]    past research on dla falls into two basic categories : expert system - style dla customised to learning particular linguistic properties , and dla via resource translation . in the first instance",
    ", a specialised methodology is proposed to ( automatically ) learn a particular linguistic property such as verb subcategorisation ( e.g. ) or noun countability ( e.g.  ) , and little consideration is given to the applicability of that method to more general linguistic properties . in the second instance , we take one dlr and map it onto another to arrive at the lexical information in the desired format .",
    "this can take the form of a one - step process , in mining lexical items directly from a dlr ( e.g.  a machine - readable dictionary @xcite ) , or two - step process in reusing an existing system to learn lexical properties in one format and then mapping this onto the dlr of choice ( e.g.   for verb subcategorisation learning ) .",
    "there have also been instances of more general methods for dla , aligned more closely with this research .",
    "proposed a method of token - based dla for unification - based precision grammars , whereby partially - specified lexical features generated via the constraints of syntactically - interacting words in a given sentence context , are combined to form a consolidated lexical entry for that word .",
    "that is , rather than relying on indirect feature signatures to perform lexical acquisition , the dlr itself drives the incremental learning process . also somewhat related to this research is the general - purpose verb feature set proposed by , which is shown to be applicable in a range of dla tasks relating to english verbs .",
    "all experiments in this paper are targeted at the * english resource grammar * ( erg ; , ) .",
    "the erg is an implemented open - source broad - coverage precision head - driven phrase structure grammar ( hpsg ) developed for both parsing and generation .",
    "it contains roughly 10,500 lexical items , which , when combined with 59 lexical rules , compile out to around 20,500 distinct word forms .",
    "each lexical item consists of a unique identifier , a lexical type ( one of roughly 600 leaf types organized into a type hierarchy with a total of around 4,000 types ) , an orthography , and a semantic relation .",
    "the grammar also contains 77 phrase structure rules which serve to combine words and phrases into larger constituents .",
    "of the 10,500 lexical items , roughly 3,000 are multiword expressions .    to get a basic sense of the syntactico - semantic granularity of the erg , the noun hierarchy , for example , is essentially a cross - classification of countability / determiner co - occurrence , noun valence and preposition selection properties .",
    "for example , lexical entries of ` n_mass_count_ppof_le ` type can be either countable or uncountable , and optionally select for a pp headed by _ of _ ( example lexical items are _ choice _ and _ administration _ ) .    as our target lexical type inventory for dla , we identified all open - class lexical types with at least 10 lexical entries , under the assumption that : ( a ) the erg has near - complete coverage of closed - class lexical entries , and ( b ) the bulk of new lexical entries will correspond to higher - frequency lexical types .",
    "this resulted in the following breakdown :    [ cols=\"<,^ , > \" , ]    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    in each graph , we present the type f - score and token accuracy for each method , and mark the best - performing method in terms of each of these evaluation measures with a star ( * @xmath3 * ) . the results for syntax - based dla ( _ s@xmath4 _ , _ s@xmath5 _ and _ s@xmath6 _ ) are based on the bnc in each case .",
    "we return to investigate the impact of corpus size on the performance of the syntax - based methods below .    looking first at the combined results over all lexical types ( figure  [ fig : results - all ] ) , the most successful method in terms of type f - score is syntax - based dla , with chunker - based preprocessing marginally outperforming tagger- and parser - based preprocessing ( type f - score = 0.641 ) . the most successful method in terms of token accuracy is ontology - based dla ( token accuracy = 0.544 ) .    the figures for token accuracy require some qualification : ontology - based dla tends to be liberal in its generation of lexical items , giving rise to over 20% more lexical items than the other methods ( 7,307 vs. 5 - 6000 for the other methods ) and proportionately low type precision .",
    "this correlates with an inherent advantage in terms of token accuracy , which we have no way of balancing up in our token - based evaluation , as the treebank data offers no insight into the true worth of false negative lexical items ( i.e.  have no way of distinguishing between unobserved lexical items which are plain wrong from those which are intuitively correct and could be expected to occur in alternate sets of treebank data ) .",
    "we leave investigation of the impact of these extra lexical items on the overall parser performance ( in terms of chart complexity and parse selection ) as an item for future research .",
    "the morphology - based dla methods were around baseline performance overall , with character @xmath7-grams marginally more successful than derivational morphology in terms of both type f - score and token accuracy .",
    "turning next to the results for the proposed methods over nouns , verbs , adjectives and adverbs ( figures  [ fig : results - noun][fig : results - adv ] , respectively ) , we observe some interesting effects .",
    "first , morphology - based dla hovers around baseline performance for all word classes except adjectives , where character @xmath7-grams produce the highest f - score of all methods , and nouns , where derivational morphology seems to aid dla slightly ( providing weak support for our original hypothesis in section  [ sec : derivational ] relating to deverbal nouns and affixation ) .",
    "syntax - based dla leads to the highest type f - score for nouns , verbs and adverbs , and the highest token accuracy for adjectives and adverbs .",
    "the differential in results between syntax - based dla and the other methods is particularly striking for adverbs , with a maximum type f - score of 0.544 ( for chunker - based preprocessing ) and token accuracy of 0.340 ( for tagger - based preprocessing ) , as compared to baseline figures of 0.471 and 0.017 respectively .",
    "there is relatively little separating the three styles of preprocessing in syntax - based dla , although chunker - based preprocessing tends to have a slight edge in terms of type f - score , and tagger - based preprocessing generally produces the highest token accuracy .",
    "this suggests that access to a pos tagger for a given language is sufficient to make syntax - based dla work , and that syntax - based dla thus has moderately high applicability across languages of different densities .",
    "ontology - based dla is below baseline in terms of type f - score for all word classes , but results in the highest token accuracy of all methods for nouns and verbs ( although this finding must be taken with a grain of salt , as noted above )",
    ".    another noteworthy feature of figures  [ fig : results - noun][fig : results - adv ] is the huge variation in absolute performance across the word classes : adjectives are very predictable , with a majority class - based baseline type f - score of 0.832 and token accuracy of 0.847 ; adverbs , on the other hand , are similar to verbs and nouns in terms of their baseline type f - score ( at 0.471 ) , but the adverbs that occur commonly in corpus data appear to belong to less - populated lexical types ( as seen in the baseline token accuracy of a miniscule 0.017 )",
    ". nouns appear the hardest to learn in terms of the relative increment in token accuracy over the baseline .",
    "verbs are extremely difficult to get right at the type level , but it appears that ontology - based dla is highly adept at getting the commonly - occurring lexical items right .",
    "to summarise these findings , adverbs seem to benefit the most from syntax - based dla .",
    "adjectives , on the other hand , can be learned most effectively from simple character @xmath7-grams , i.e.  similarly - spelled adjectives tend to have similar syntax , a somewhat surprising finding .",
    "nouns are surprisingly hard to learn , but seem to benefit to some degree from corpus data and also ontological similarity .",
    "lastly , verbs pose a challenge to all methods at the type level , but ontology - based dla seems to be able to correctly predict the commonly - occurring lexical entries .",
    "finally , we examine the impact of corpus size on the performance of syntax - based dla with tagger - based preprocessing . in figure",
    "[ fig : results - token ] , we examine the relative change in type f - score and token accuracy across the four word classes as we increase the corpus size ( from 0.5 m words to 1 m and finally 100 m words , in the form of the brown corpus , wsj corpus and bnc , respectively ) . for verbs and adjectives ,",
    "there is almost no change in either type f - score or token accuracy when we increase the corpus size , whereas for nouns , the token accuracy actually drops slightly .",
    "for adverbs , on the other hand , the token accuracy jumps up from 0.020 to 0.381 when we increase the corpus size from 1 m words to 100 m words , while the type f - score rises only slightly .",
    "it thus seems to be the case that large corpora have a considerable impact on dla for commonly - occurring adverbs , but that for the remaining word classes , it makes little difference whether we have 0.5 m or 100 m words .",
    "this can be interpreted either as evidence that modestly - sized corpora are good enough to perform syntax - based dla over ( which would be excellent news for low - density languages ! ) , or alternatively that for the simplistic syntax - based dla methods proposed here , more corpus data is not the solution to achieving higher performance .",
    "returning to our original question of the `` bang for the buck '' associated with individual lrs , there seems to be no simple answer : simple word lists are useful in learning the syntax of adjectives in particular , but offer little in terms of learning the other three word classes .",
    "morphological lexicons with derivational information are moderately advantageous in learning the syntax of nouns but little else .",
    "a pos tagger seems sufficient to carry out syntax - based dla , and the word class which benefits the most from larger amounts of corpus data is adverbs , otherwise the proposed syntax - based dla methods do nt seem to benefit from larger - sized corpora .",
    "ontologies have the greatest impact on verbs and , to a lesser degree , nouns . ultimately , this seems to lend weight to a `` horses for courses '' , or perhaps `` resources for courses '' approach to dla .",
    "we have proposed three basic paradigms for deep lexical acquisition , based on morphological , syntactic and ontological language resources , and demonstrated the effectiveness of each strategy at learning lexical items for the lexicon of a precision english grammar .",
    "we discovered surprising variation in the results for the different dla methods , with each learning method performing particularly well for at least one basic word class , but the best overall methods being syntax- and ontology - based dla .",
    "the results presented in this paper are based on one particular language ( english ) and a very specific style of dlr ( a precision grammar , namely the english resource grammar ) , so some caution must be exercised in extrapolating the results too liberally over new languages / dla tasks . in future research , we are interested in carrying out experiments over other languages and alternate dlrs to determine how well these results generalise and formulate alternate strategies for dla .      this material is based upon work supported in part by ntt communication science laboratories , nippon telegraph and telephone corporation .",
    "we would like to thank the members of the university of melbourne lt group and the three anonymous reviewers for their valuable input on this research .",
    "timothy baldwin and francis bond .",
    "2003b . a plethora of methods for learning english countability . in _ proc",
    "of the 2003 conference on empirical methods in natural language processing ( emnlp 2003 ) _ , pages 7380 , sapporo , japan .    ted briscoe and john carroll .",
    "robust accurate statistical annotation of general text . in _ proc .  of the 3rd international conference on language resources and evaluation ( lrec 2002 ) _ , pages 14991504 , las palmas , canary islands .",
    "john carroll and alex fang .",
    "the automatic acquisition of verb subcategorisations and their impact on the performance of an hpsg parser . in _ proc .  of the first international joint conference on natural language processing ( ijcnlp-04 )",
    "_ , pages 10714 , sanya city , china .",
    "ann copestake and dan flickinger .",
    "an open - source grammar development environment and broad - coverage english grammar using hpsg . in _ proc .  of the 2nd international conference on language resources and evaluation ( lrec 2000 )",
    "_ , athens , greece .        dan flickinger .",
    "2002 . on building a more efficient grammar by exploiting types . in stephan oepen , dan flickinger , junichi tsujii , and hans uszkoreit , editors ,",
    "_ collaborative language engineering_. csli publications , stanford , usa .",
    "grace ngai and radu florian",
    "transformation - based learning in the fast lane . in _ proc .  of the 2nd annual meeting of the north american chapter of association for computational linguistics ( naacl2001 ) _ , pages 407 , pittsburgh ,",
    "stephan oepen , dan flickinger , kristina toutanova , and christoper  d. manning .",
    "2002 . : a rich and dynamic treebank for hpsg . in _ proc .",
    "of the first workshop on treebanks and linguistic theories ( tlt2002 ) _ , sozopol , bulgaria .",
    "antonio sanfilippo and victor poznaski .",
    "the acquisition of lexical knowledge from combined machine - readable dictionary sources . in _ proc .  of the 3rd conference on applied natural language processing ( anlp ) _ , pages 807 , trento , italy .",
    "erik  f. tjong kim sang and sabine buchholz .",
    "introduction to the conll-2000 shared task : chunking . in _ proc .  of the 4th conference on computational natural language learning ( conll-2000 )",
    "_ , lisbon , portugal .",
    "leonoor van der beek and timothy baldwin . 2004 .",
    "crosslingual countability classification with eurowordnet . in _",
    "papers from the 14th meeting of computational linguistics in the netherlands _ , pages 14155 , antwerp , belgium .",
    "antwerp papers in linguistics ."
  ],
  "abstract_text": [
    "<S> we propose a range of deep lexical acquisition methods which make use of morphological , syntactic and ontological language resources to model word similarity and bootstrap from a seed lexicon . </S>",
    "<S> the different methods are deployed in learning lexical items for a precision grammar , and shown to each have strengths and weaknesses over different word classes . </S>",
    "<S> a particular focus of this paper is the relative accessibility of different language resource types , and predicted `` bang for the buck '' associated with each in deep lexical acquisition applications . </S>"
  ]
}