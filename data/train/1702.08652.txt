{
  "article_text": [
    "recognition of human actions from rgb - d data has generated renewed interest in the computer vision community due to the recent availability of easy - to - use and low - cost depth sensors ( e.g. microsoft kinect  sensor ) .",
    "in addition to tristimulus visual data captured by conventional rgb cameras , depth data are provided in rgb - d cameras , thus encoding rich 3d structural information of the entire scene .",
    "previous works  @xcite showed the effectiveness of fusing the two modalities for 3d action recognition .",
    "however , all the previous methods consider the depth and rgb modalities as separate channels from which to extract features and fuse them at a later stage for action recognition .",
    "since the depth and rgb data are captured simultaneously , it will be interesting to extract features considering them jointly as one entity .",
    "optical flow - based methods for 2d action recognition  @xcite have provided the state - of - the - art results for several years .",
    "in contrast to optical flow which provides the projection of the scene motion onto the image plane , scene flow  @xcite estimates the actual 3d motion field .",
    "thus , we propose the use of scene flow for 3d action recognition . differently from the optical flow - based late fusion methods on rgb and depth data",
    ", scene flow extracts the real 3d motion and also explicitly preserves the spatial structural information contained in rgb and depth modalities .",
    "there are two critical issues that need to be addressed when adopting scene flow for action recognition : how to organize the scene flow vectors and how to effectively exploit the spatio - temporal dynamics",
    ". two kinds of motion representations can be identified : lagrangian motion  @xcite and eulerian motion  @xcite .",
    "lagrangian motion focuses on individual points and analyses their change in location over time .",
    "such trajectories requires reliable point tracking over long term and is prone to error .",
    "eulerian motion considers a set of locations in the image and analyses the changes at these locations over time , thus avoiding the need for point tracking .",
    "since scene flow vectors could be noisy and to avoid the difficulty of long term point tracking of lagrangian motion , we adopted the eulerian approach in constructing the final representation for action recognition .",
    "furthermore , the scene flow between two consecutive pair of rgb - d frames ( two rgb images and two corresponding depth images ) is one simple lagrangian motion with only two frames matching / tracking .",
    "this property provides a better representation than is possible with eulerian motion obtained from raw pixels .",
    "however , it remains unclear as to how video could be effectively represented and fed to deep neural networks for classification .",
    "for example , one can conventionally consider a video as a sequence of still images with some form of temporal smoothness , or as a subspace of images or image features , or as the output of a neural network encoder .",
    "which one among these and other possibilities would result in the best representation in the context of action recognition is not well understood .",
    "the promising performance of existing temporal encoding works  @xcite provides a source of motivation .",
    "these works encode the spatio - temporal information as dynamic images and enable the use of existing convnets models directly without training the whole networks afresh . thus , we propose to encode the rgb - d video sequences based on scene flow into one motion map , called scene flow to action map ( sfam ) , for 3d action recognition . intuitively and similarly to the three channels of color images ,",
    "the three elements of a scene flow vector can be considered as three channels .",
    "such consideration allows the scene flow between two consecutive pairs of rgb - d frames to be reorganized as one three - channel scene flow map ( sfm ) , and the rgb - d video sequence can be represented as sfm sequence . in the spirits of eulerian motion and rank pooling methods  @xcite , we propose to encode sfm sequence into sfam .",
    "several variants of sfam are developed .",
    "they capture the spatio - temporal information from different perspectives and are complementary to each other for final recognition .",
    "however , two issues arise with these hand - crafted sfams : 1 ) direct organization of the scene flow vectors in sfm may sacrifice the relations among the three elements ; 2 ) in order to take advantage of available model trained over imagenet , the input needs to be analogous to rgb images ; that is , the input for the convnets need to have similar properties to conventional rgb images as used in trained filters .",
    "based on these two observations , we propose to learn channel transform kernels with rank pooling method and convnets , that convert the three channels into suitable three new channels capable of exploiting the relations among the three elements and have similar rgb image features . with this transformation ,",
    "the dynamic sfam can describe both the spatial and temporal information of a given video .",
    "it can be used as the input to available and already trained convnets along with fine - tuning .",
    "the contributions of this paper are summarized as follows:1 ) the proposed sfam is the first attempt , to our best knowledge , to extract features from depth and rgb modalities as joint entity through scene flow , in the context of convnets ; 2 ) we propose an effective self - calibration method that enables the estimation of scene flow from unregistered captured rgb - d data ; 3 ) several variants of sfam that encode the spatio - temporal information from different aspects and are complementary to each other for final 3d action recognition are proposed ;",
    "4 ) we introduce channel transform kernels which learn the relations among the three channels of sfm and convert the scene flow vectors to rgb - like images to take advantages of trained convnets models and 5 ) the proposed method achieved state - of - the - art results on two relatively large datasets .",
    "the reminder of this paper is organized as follows .",
    "section  [ relatedwork ] describes the related work .",
    "section  [ sfam ] introduces the sfam and its variants , and presents the proposed channel transform kernels .",
    "experimental results on two datasets are provided in section  [ results ] .",
    "section  [ conclusion ] concludes the paper and discusses future work .",
    "since the first work  @xcite on 3d action recognition from depth data captured by commodity depth sensors ( e.g. , microsoft kinect  ) in 2010 , many methods  @xcite for action recognition have been proposed based on depth , rgb or skeleton data .",
    "these methods either extracted features from one modality : depth  @xcite or rgb  @xcite or skeleton  @xcite , or fuse the features extracted separately from them at a later stage  @xcite . neither of these methods considered depth and rgb modalities jointly in feature extraction . in contrast , we propose to adopt scene flow for 3d action recognition and extract features jointly from rgb - d data .      in general , scene flow is defined as the dense or semi - dense non - rigid motion field of a scene observed at different instants of time  @xcite .",
    "the term  scene flow \" was firstly coined by vedula et al .",
    "@xcite who proposed to start by computing the lucas - kanade optical flow and applied the range flow constraint equation at a later stage .",
    "since this work , several methods  @xcite have been proposed based on stereo or multiple view camera systems . with the advent of affordable rgb - d cameras , scene flow methods for rgb - d data have also been proposed  @xcite .",
    "however most of the existing methods incur high computational burden , taking from several seconds to a few hours to compute the scene flow per frame .",
    "thus , limiting their usefulness in real applications .",
    "recently , a primal - dual framework for real - time dense rgb - d scene flow  @xcite has been proposed .",
    "a primal - dual algorithm is applied to solve the variational formulation of the scene flow problem .",
    "it is an iterative solver performing pixel - wise updates and can be efficiently implemented on gpus . in this paper",
    ", we used this algorithm for scene flow calculation .",
    "existing deep learning approaches for action recognition can be generally divided into four categories based on how the video is represented and fed to a deep neural network .",
    "the first category views a video either as a set of still images  @xcite or as a short and smooth transition between similar frames  @xcite , and each color channel of the images is fed to one channel of a convnet .",
    "although obviously suboptimal , considering the video as a bag of static frames performs reasonably well .",
    "the second category represents a video as a volume and extend convnets to a third , temporal dimension  @xcite replacing 2d filters with 3d equivalents .",
    "so far , this approach has produced little benefits , probably due to the lack of annotated training data .",
    "the third category treats a video as a sequence of images and feed the sequence to an recurrent neural network ( rnn )  @xcite .",
    "an rnn is typically considered as memory cells , which are sensitive to both short as well as long term patterns .",
    "it parses the video frames sequentially and encodes the frame - level information in the memory .",
    "however , using rnns did not give an improvement over temporal pooling of convolutional features  @xcite or even over hand - crafted features .",
    "the last category represents a video in one or multiple compact images and adopt available trained convnet architectures for fine - tuning  @xcite .",
    "this category has achieved state - of - the - art results in action recognition on many rgb and depth / skeleton datasets .",
    "the proposed method in this paper falls into this last category .",
    "sfam encodes the dynamics of rgb - d sequences based on scene flow vectors . to make our description self - contained , in section  [ pdflow ] we briefly present the primal - dual framework for real - time dense rgb - d scene flow computation ( hereafter denoted by pd - flow  @xcite ) . for scene flow computation , we assume that the depth and rgb data are prealigned . if this is not the case , the videos can be quickly realigned as described in section  [ selfcalibration ] .",
    "then , in section  [ sfam ] we present several hand - crafted constructions of sfam and we propose an end - to - end learning method for sfam through channel transform kernels in section  [ sfam - learning ] .",
    "the pd - flow estimates the dense 3d motion field of a scene between two instants of time @xmath0 and @xmath1 using rgb and depth images provided by an rgb - d camera .",
    "this motion field @xmath2 defined over the image domain @xmath3 , is described with respect to the camera reference frame and expressed in meters per second . for simplicity",
    ", the bijective relationship @xmath4 between @xmath5 and @xmath6 is given by : @xmath7 where @xmath8 represent the optical flow and @xmath9 denotes the range flow ; @xmath10 are the camera focal length values , and @xmath11 the spatial coordinates of the observed point .",
    "thus , estimating the optical and range flows is equivalent to estimating the 3d motion field but leads to a simplified implementation . in order to compute the motion field a minimization problem over @xmath12",
    "is formulated where photometric and geometric consistency are imposed as well as a regularity of the solution : @xmath13 in eq .",
    "( [ energyfunc ] ) , @xmath14 is the data term , representing a two - fold restriction for both intensity and depth matching between pairs of frames ; @xmath15 is the regularization term which both smooths the flow field and constrains the solution space .    for data term @xmath14 ,",
    "the @xmath16 norm of photometric consistency @xmath17 and geometric consistency @xmath18 is minimized as : @xmath19 where @xmath20 is a positive function that weights geometric consistency against brightness constancy ; @xmath21 and @xmath22 with @xmath23 being the intensity images while @xmath24 the depth images taken at instants @xmath0 and @xmath1",
    ".    the regularization term @xmath15 is based on the total variation and takes into consideration the geometry of the scene which is formulated as :    @xmath25    where @xmath26 are constant weights and @xmath27 , @xmath28 .",
    "as the energy function ( eq .  ( [ energyfunc ] ) ) is based on a linearisation of the data term ( eq .  ( [ dt ] ) ) and convex tv regularizer ( eq .  ( [ eq6 ] ) ) , the energy function can be solved using convex solver .",
    "an iterative solver can be obtained by deriving the energy function ( eq .  ( [ energyfunc ] ) ) as its primal - dual formulation and implemented in parallel on gpus . for more implementation details ,",
    "the keen reader is recommended to read  @xcite .",
    "scene flow computation requires that the rgb and depth data be spatially aligned and temporally synchronized .",
    "the data considered in this paper were captured by kinect sensors and are temporally synchronized .",
    "however , the rgb and depth channels may not be spatially registered if calibration was not performed properly before recording the data . for the rgb - d datasets with spatial misalignment",
    ", we propose an effective self - calibration method to perform spatial alignment without knowledge of the cameras parameters .",
    "the alignment is based on a pinhole model through which depth maps are transformed into the same view of the rgb video .",
    "let @xmath29 be a point in an rgb frame and @xmath30 be the corresponding point in the depth map .",
    "the 2d homography mapping @xmath31 satisfying @xmath32 is a @xmath33 projective transformation for the alignment . following the method in  @xcite , we chose a set of matching points in an rgb frame and its corresponding depth map . using four pairs of corresponding points , @xmath31",
    "is obtained through direct linear transformation .",
    "let @xmath34 , @xmath35 be the @xmath36 row of @xmath31 and @xmath37^t$ ] .",
    "the vector cross product equation @xmath38 is written as  @xcite : @xmath39\\left(\\begin{array}{c }                 h_1\\\\h_2\\\\h_3                 \\end{array }   \\right)=\\textbf{0},\\ ] ] where the up - to - scale equation is omitted . a better estimation of @xmath31 is achieved by minimising ( for example , using levenberg - marquardt algorithm  @xcite ) the following objective function with more matching points :    c _ , , _i[d(p_i,)^2+d(p_i, ) ] +  =   i[ml ]    in eq .",
    ", @xmath40 is the distance function and @xmath41 is the optimal estimation of the homography mapping while @xmath42 and @xmath43 are estimated matching points from @xmath44 . because the process of selecting matching points may not be reliable , the random sample consensus  ( ransac ) algorithm is applied to exclude outliers . by transforming the depth map using the 2d projective transformation @xmath31 , the rgb video and its corresponding depth video are spatially aligned .",
    "sfam encodes a video sample into a single dynamic image to take advantage of the available pre - trained models for standard convnets architecture without training millions of parameters afresh .",
    "there are several ways to encode the video sequences into dynamic images  @xcite , but how to encode the scene flow vectors into one dynamic image still needs to be explored . as described in section  [ pdflow ] , one scene flow vector @xmath6 is obtained by matching / tracking one point in the current frame to another in the reference frame ; this is one simple lagrangian motion . in order to avoid error in tracking lagrangian motion over long term , we construct sfam using the eulerian motion approach and thus , the sfam inherits the merits of both the eulerian and lagrangian motion . as we argued earlier , the three entries ( @xmath45 ) in the scene flow vector @xmath12 for each point can be considered as three channels .",
    "hence a scene flow between two pairs of rgb - d images ( @xmath46 , @xmath47 and @xmath48 , @xmath49 ) can be reorganized as one three - channel sfm ( @xmath50 , @xmath51 , @xmath52 ) , and the rgb - d video sequences can be represented as sfm sequences . based on the sfm sequences , there are several ways to construct the sfam .      inspired by the construction of depth motion maps ( dmm )",
    "@xcite , we accumulate the absolute differences between consecutive sfms and denote it as sfam - d . it is written as : @xmath53 where @xmath0 denotes the map number and @xmath54 represents the total number of maps ( the same for the following sections ) .",
    "this representation characterizes the distribution of the accumulated motion difference energy .",
    "similarly to sfam - d , we construct the sfam - s ( s here denotes the sum ) by accumulating the sum between consecutive sfms .",
    "this can be written as : @xmath55 this representation mainly captures the large motion of an action after normalization .      inspired by the work reported in  @xcite",
    ", we adopt the rank pooling method to encode sfm sequence into one action image .",
    "let @xmath56 denote the sfm sequence where each @xmath57 contains three channels ( @xmath50 , @xmath51 , @xmath52 ) , and @xmath58 be a representation or feature vector extracted from each individual map , @xmath57 .",
    "herein , we directly apply rank pooling to the @xmath59 , thus , @xmath60 equals to identity matrix .",
    "let @xmath61 be time average of these features up to time @xmath0 .",
    "the ranking function associates with each time @xmath0 a score @xmath62 , where @xmath63 is a vector of parameters .",
    "the function parameters @xmath64 are learned so that the scores reflect the order of the maps in the video . in general ,",
    "more recent frames are associated with larger scores , i.e. @xmath65 . learning @xmath64 is formulated as a convex optimization problem using ranksvm  @xcite :    @xmath66    the first term in this objective function is the usual quadratic regular term used in svms .",
    "the second term is a hinge - loss soft - counting how many pairs @xmath67 are incorrectly ranked by the scoring function .",
    "note in particular that a pair is considered correctly ranked only if scores are separated by at least a unit margin , @xmath68 .",
    "optimizing the above equation defines a function @xmath69 that maps a sequence of @xmath54 sfms to a single vector @xmath70 .",
    "since this vector contains enough information to rank all the frames in the sfm sequence , it aggregates information from all of them and can be used as a sequence descriptor . in our work ,",
    "the rank pooling is applied in a bidirectional manner to convert each sfm sequence into two action maps , sfam - rpf ( forward ) and sfam - rpb ( backward ) .",
    "this representation captures the different types of importance associated with frames in one action and assigns more weight to recent frames .      in previous sections , all the three channels are considered as separate channels in constructing sfam .",
    "however , the specific relationship ( independent or otherwise ) between them is yet to be ascertained . to study this relationship",
    ", we adopt a simple method _ viz .",
    "_ , using amplitude of the scene flow vector @xmath12 to represent the relations between the three components .",
    "thus , for each triple ( @xmath50 , @xmath51 , @xmath52 ) we obtain a new amplitude map , @xmath71 .",
    "based on the @xmath72 , the rank pooling method is applied to encode the scene flow maps into two action maps , sfam - amrpf and sfam - amrpb .",
    "this representation exploits the weights of frames based on the motion magnitude .      to further investigate the relationship amongst the triple ( @xmath50 , @xmath51 , @xmath52 ) ,",
    "they are transformed nonlinearly into another space , similarly to the manner of transforming rgb color space to _ lab _ space .",
    "the _ lab _ space is designed to approximate the human visual system .",
    "based on these transformed maps , the rank pooling method is applied to encode the sequence into two action maps , sfam - labrpf and sfam - labrpb .",
    "a few examples of the sfam variants are shown in figure  [ sfams ] for action  bounce basketball \" from m@xmath73i dataset  @xcite .",
    "it can be seen that different variants of sfam capture and encode sfm sequence into action maps with large visual differences .      in previous sections ,",
    "we have presented the concept of sfam and its several variants .",
    "however , it has been empirically observed that none of them can achieve the best results for all the datasets or scenarios .",
    "one reason adduced for this is that during the construction of the sfam , the relationship amongst the triple ( @xmath50 , @xmath51 , @xmath52 ) are hand - crafted . to learn the relationship amongst the elements of the triple ( @xmath50 , @xmath51 , @xmath52 ) from data with convnets , we propose a channel transform kernels as follows",
    "let @xmath74 be the new learned maps from the original triple ( @xmath50 , @xmath51 , @xmath52 ) , the relationship between them can be formulated as : @xmath75 where @xmath76 has the same size with @xmath59 , @xmath9 are scalar values and @xmath77 denotes the transforms that need to be learned .",
    "the learning framework is illustrated in figure  [ framework ] .",
    "there are different ways to learn these channel transform kernels . for sake of simplicity , in this work we approximated the transforms by three successive convolution layers , where each layer comprises nine convolutional kernels with size @xmath78 and followed by relu nonlinear transform , as illustrated in figure  [ approximate ] .",
    "based on rankpool layer  @xcite for temporal encoding , we can construct the sfam with the proposed channel transform kernels using convnets .",
    "after construction of the several variants of sfam , we propose to adopt one effective late score fusion method , namely , multiply - score fusion method , to improve the final recognition accuracy .",
    "take sfam - rp for example , as illustrated in figure  [ scorefusion ] , two sfam - rp , one sfam - rpf and one sfam - rpb , are generated for one pair of rgb - d videos and they are fed into two different trained convnets channels .",
    "the score vectors output by the two convnets are multiplied element - wisely and the max score in the resultant vector is assigned as the probability of the test sequence .",
    "the index of this max score corresponds to the recognized class label .",
    "this process can be easily extended into multiple channels .",
    "according to the survey of rgb - d datasets  @xcite , we chose two public benchmark datasets , which contain both rgb+depth modalities and have relatively large training samples to evaluate the proposed method .",
    "specifically we chose chalearn lap isogd dataset  @xcite and m@xmath73i dataset  @xcite . in the following ,",
    "we proceed by briefly describing the implementation details and then present the experiments and results .      for scene flow computation",
    ", we adopted the public codes provided by  @xcite . for rank pooling",
    ", we followed the work reported in  @xcite where each channel was generated into one channel dynamic map and then merged the three channels into one three - channel map . differently from  @xcite",
    ", we used bidirectional rank pooling . for chalearn lap isogd dataset , in order to minimize the interference of the background",
    ", it is assumed that the background in the histogram of depth maps occupies the last peak representing far distances .",
    "specifically , pixels whose depth values are greater than a threshold defined by the last peak of the depth histogram minus a fixed tolerance ( 0.1 was set in our experiments ) are considered as background and removed from the calculation of scene flow by setting their depth values to zero . through this simple process ,",
    "most of the background can be removed and has much contribution to the sfam .",
    "the alexnet  @xcite was adopted in this paper .",
    "the training procedure of the hand - crafted sfams was similar to that described in  @xcite .",
    "the network weights were learned using the mini - batch stochastic gradient descent with the momentum set to 0.9 and weight decay set to 0.0005 .",
    "all hidden weight layers used the rectification ( relu ) activation function . at each iteration ,",
    "a mini - batch of 256 samples was constructed by sampling 256 shuffled training samples .",
    "all the images were resized to 256 @xmath79 256 .",
    "the learning rate was set to @xmath80 for fine - tuning with pre - trained models on ilsvrc-2012 , and then it was decreased according to a fixed schedule , which was kept the same for all training sets .",
    "different datasets underwent different iterations according to their number of training samples .",
    "for all experiments , the dropout regularization ratio was set to 0.5 in order to reduce complex co - adaptations of neurons in the nets .",
    "the implementation was derived from the publicly available caffe toolbox  @xcite based on one nvidia tesla k40 gpu card .",
    "unless otherwise specified , all the networks were initialized with the models trained over imagenet  @xcite . for sfam - ctkrp",
    ", we revised the codes of paper  @xcite based on matconvnet  @xcite .",
    "the multiply score fusion method is compared with the other two commonly used late score fusion methods , average and maximum score fusion on both datasets . this verifies that the sfams are likely to be statistically independent and provide complementary information .",
    "the chalearn lap isogd dataset  @xcite is derived from the chalearn gesture dataset ( cgd )  @xcite .",
    "it includes 47933 rgb - d depth sequences , each rgb - d video representing one gesture instance .",
    "there are 249 gestures performed by 21 different individuals .",
    "this dataset does not provide the true depth values in their depth videos . to use this dataset for scene flow calculation",
    ", we estimate the depth values using the average minimum and maximum values provided for cgd dataset .",
    "the dataset is divided into training , validation and test sets .",
    "all three sets consist of samples of different subjects to ensure that the gestures of one subject in validation and test sets will not appear in the training set .",
    "as the test set is not available for public usage , we report the results on the validation set . for this dataset",
    "the training underwent 25k iterations and the learning rate decreased every 10k iterations .    * results .",
    "* table  [ table1 ] shows the results of six variants of sfam , and compares them with methods in the literature  @xcite . among these methods , mfsk combined 3d smosift  @xcite with ( hog , hof and mbh )  @xcite descriptors . mfsk+deepid",
    "further included deep hidden identity ( deep i d ) feature  @xcite .",
    "thus , these two methods utilized not only hand - crafted features but also deep learning features . moreover , they extracted features from rgb and depth seperatedly , concatenated them together , and adopted bag - of - words ( bow ) model as the final video representation .",
    "the other methods , whdmm+sdi  @xcite , extracted features and conducted classification with convnets from depth and rgb individually and adopted multiply - score fusion for final recognition .    compared with these methods ,",
    "the proposed sfam outperformed all of them significantly .",
    "it is worth noting that all the depth values used in the proposed sfam were estimated rather than the exact real depth values . despite the possible estimation errors , our method still achieved promising results .",
    "interestingly , the proposed variants of sfam are complementary to each other and can improve each other largely by using multiply - score fusion . even though this dataset is large , on average 144 video clips per class , it is still much smaller compared with 1200 images per class in imagenet .",
    "thus , directly training from scratch can not compete with fine - tuning the trained models over imagenet and this is evident in the results reported in table  [ table1 ] . by comparing different types of sfam",
    ", we can see that the simple sfam - s method achieved the best results among all types of hand - designed sfams .",
    "due to the relatively large training data , sfam - ctkrp achieved the best result among all the variants , even though the approximate rank pooling in the work reported in  @xcite was shown to be worse than rank pooling solved by ranksvm  @xcite .",
    "the reasons for these two phenomenona probably are as follows : under the inaccurate estimation of the depth values , the scene flow computation will be affected and based on this inaccurate scene flow vectors , rank pooling can not achieve its full efficacy .",
    "in other words , the rank pooling method is sensitive to noise . instead , the proposed channel transform kernels can not only exploit the relations amongst the channels but also decrease the effects of noise after channel transforms .",
    ".results and comparison on the chalearn lap isogd dataset .",
    "[ table1 ] [ cols=\"^,^\",options=\"header \" , ]",
    "we propose a novel method for action recognition based on scene flow . in particular ,",
    "scene flow vectors are estimated from registered rgb and depth data . a new representation based on scene flow vectors , sfam , and several variants that capture the spatio - temporal information from different perspectives are proposed for 3d action recognition . in order to exploit the relationships amongst the three channels of scene flow map , we propose to learn the channel transform kernels , end - to - end , with convnets from data .",
    "experiments on two benchmark datasets have demonstrated the effectiveness of the proposed method . for the future work",
    ", we will improve the temporal encoding method based on scene flow vectors .",
    "j.  donahue , l.  anne  hendricks , s.  guadarrama , m.  rohrbach , s.  venugopalan , k.  saenko , and t.  darrell .",
    "long - term recurrent convolutional networks for visual recognition and description . in _ cvpr _ , pages 26252634 , 2015 .",
    "h.  j. escalante , v.  ponce - lpez , j.  wan , m.  a. riegler , b.  chen , a.  claps , s.  escalera , i.  guyon , x.  bar , p.  halvorsen , h.  mller , and m.  larson .",
    "chalearn joint contest on multimedia challenges beyond visual analysis : an overview . in _ proceedings of icprw _ , 2016 .",
    "y.  jia , e.  shelhamer , j.  donahue , s.  karayev , j.  long , r.  b. girshick , s.  guadarrama , and t.  darrell .",
    "caffe : convolutional architecture for fast feature embedding . in _ proc .",
    "acm international conference on multimedia ( acm mm ) _ , pages 675678 , 2014 .    c.  kanzow , n.  yamashita , and m.  fukushima . withdrawn :",
    "levenberg  marquardt methods with strong local convergence properties for solving nonlinear equations with convex constraints .",
    ", 173(2):321343 , 2005 .",
    "a.  krizhevsky , i.  sutskever , and g.  e. hinton .",
    "imagenet classification with deep convolutional neural networks . in _ proc .",
    "annual conference on neural information processing systems ( nips ) _ , pages 11061114 , 2012 .",
    "j.  wan , s.  z. li , y.  zhao , s.  zhou , i.  guyon , and s.  escalera .",
    "chalearn looking at people rgb - d isolated and continuous datasets for gesture recognition . in _ proc .",
    "ieee computer society conference on computer vision and pattern recognition workshops ( cvprw ) _ ,",
    "pages 19 , 2016 ."
  ],
  "abstract_text": [
    "<S> scene flow describes the motion of 3d objects in real world and potentially could be the basis of a good feature for 3d action recognition . </S>",
    "<S> however , its use for action recognition , especially in the context of convolutional neural networks ( convnets ) , has not been previously studied . in this paper , we propose the extraction and use of scene flow for action recognition from rgb - d data . </S>",
    "<S> previous works have considered the depth and rgb modalities as separate channels and extract features for later fusion . </S>",
    "<S> we take a different approach and consider the modalities as one entity , thus allowing feature extraction for action recognition at the beginning . </S>",
    "<S> two key questions about the use of scene flow for action recognition are addressed : how to organize the scene flow vectors and how to represent the long term dynamics of videos based on scene flow . in order to calculate the scene flow correctly on the available datasets , we propose an effective self - calibration method to align the rgb and depth data spatially without knowledge of the camera parameters . </S>",
    "<S> based on the scene flow vectors , we propose a new representation , namely , scene flow to action map ( sfam ) , that describes several long term spatio - temporal dynamics for action recognition . </S>",
    "<S> we adopt a channel transform kernel to transform the scene flow vectors to an optimal color space analogous to rgb . </S>",
    "<S> this transformation takes better advantage of the trained convnets models over imagenet . </S>",
    "<S> experimental results indicate that this new representation can surpass the performance of state - of - the - art methods on two large public datasets . </S>"
  ]
}