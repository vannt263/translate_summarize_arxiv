{
  "article_text": [
    "in first principle calculations , such as band structure calculations , atomic and molecular structure calculations , one of the basic tasks is to find several lowest eigenvalues and the corresponding eigenvectors iteratively ( and very often self - consistently ) of the effective hamiltonian  @xcite .",
    "the matrix dimension of the hamiltonian may range from tens of thousands to several millions , and one may need up to several thousands of the lowest eigenvectors of the effective hamiltonian .",
    "diagonalising of matrices in such a scale needs considerable cpu time and memories .",
    "it is one of the major numerical costs in the first principle calculations , and the efficiency of the algorithm is crucial for the performances of the whole program .",
    "there are many efforts to improve the algorithm  @xcite .    among widely used algorithms , such as lanczos  @xcite",
    ", dividson  @xcite , relaxation method  @xcite , diis ( direct inversion in the iterative subspace , which minimizes all matrix elements between all trial vectors )  @xcite , and its later version rmm - diis  @xcite ( rmm stands for residual minimization , i.e. , minimizing the norm of the residual vector in iterative subspace ) , the conjugated gradient ( cg ) method  @xcite is a valuable tool to find a set of lowest eigenvectors of a large matrix .",
    "briefly speaking , to obtain the lowest eigenvector of a matrix @xmath0 for the general form eigenvalue problem @xmath1 the cg method iteratively minimizes the rayleigh quotient @xmath2 where @xmath3 is the overlap matrix , and @xmath4 is a refined trial vector at step @xmath5 .",
    "each iteration step is to search the minimization point in the direction of the conjugated gradient which is a combination of the current gradient and previous conjugated gradient .",
    "one can obtain higher eigenvectors in the same way , provided to keep the trial vector orthogonal to the lower eigenvectors . in practical calculations , the cg method is stable and reasonably efficient in many cases , and it is easy to implement .",
    "the iteration procedure needs only to store the trial vector and its gradient , as well as one previous conjugated gradient .",
    "conjugated gradient method is originally designed to minimize positive definite quadratic functions iteratively . in @xmath5-th step of iteration ,",
    "the cg method is equivalent to find a minimum in a @xmath5-dimensional subspace spanned by the initial trial vector and the subsequent @xmath6 gradients of the quadratic function . due to special properties of a quadratic function",
    ", one needs only do the minimization in a two dimensional space spanned by current state and the conjugated gradient which is a combination of current gradient and last step s conjugated gradient . in principle",
    ", one needs at most @xmath7 steps to obtain final solution in a @xmath7 dimensional space .",
    "practical calculations usually needs more steps due to round off errors .",
    "the conjugated gradient method is virtually the most effective method to minimize a quadratic function iteratively . and",
    "it is a formally established algorithm to solve the linear algebraic equation .    for general functions , such as rayleigh quotient ,",
    "there are several ways to define the conjugated gradient , and the behaviors of conjugated gradient algorithm are unclear",
    ". however , near an exact minimum point , any function behaves like a quadratic function .",
    "if one starts with a good guess , one may find solution very quickly .",
    "this partially explains the successes of the conjugated gradients method in diagonalising a large matrix .",
    "our method is based on the following two observations :    firstly , each iteration step of minimizing the rayleigh quotient by cg algorithm is equivalent to find lowest eigenvector in a two dimensional subspace .",
    "the subspace at @xmath5-th step is spanned by the current state @xmath8 , and the conjugated gradient @xmath9 .",
    "note that the conjugated gradient @xmath9 is a combination of the gradient of @xmath5-th step s rayleigh quotient , and the @xmath10-th step s conjugated gradient @xmath11 .",
    "one may expect a better result at @xmath5-th iterative step by finding lowest eigenvector in a three dimensional subspace spanned by @xmath4 , @xmath12 , and @xmath11 , where @xmath12 is the gradient of the rayleigh quotient at @xmath5-th step .",
    "secondly , we note that , within the cg algorithm , @xmath4 is a combination of @xmath13 and @xmath11 .",
    "thus the three dimensional subspace spanned by @xmath4 , @xmath12 , and @xmath11 is the same as the subspace spanned by @xmath4 , @xmath12 , and @xmath13 .",
    "this means that one may obtain a better result @xmath14 at @xmath5-th step by replace the @xmath5-step iteration of cg algorithm with finding lowest eigenvector at the three dimensional subspace spanned by @xmath4 , @xmath12 , and @xmath13 .",
    "of course , the result will further improved if one finds the lowest eigenvector in a larger subspace spanned by @xmath4 , @xmath12 , @xmath13 , @xmath15 , @xmath16 .",
    "the above observations indicate that one may improve the efficiency of the cg algorithm by replacing each iteration step of the cg algorithm with finding lowest eigenvector in a small subspace spanned by the current vector @xmath17 and the corresponding gradient @xmath18 , as well as some previous vectors @xmath13 , @xmath19 , @xmath15 . in our numeric tests ,",
    "the effect is significant in many cases .",
    "since diagonalising a small matrix of several dimension is very cheap numerically , each step s numeric cost of the modified version is about the same as the original cg algorithm .",
    "practical implementation of the modified conjugated gradient method is similar to that of original cg method . for finding one single lowest eigenvalue and its corresponding eigenvector",
    ", it goes through the following steps :    1 .",
    "choose the dimension @xmath20 of the iteration subspace , and the maximum iteration step @xmath21 .",
    "in our numerical test , it is enough to set the dimension @xmath22 . in many case ,",
    "@xmath23 works quite well . in this case , the 3 dimensional subspace is spanned by current trial vector @xmath4 , the corresponding gradient @xmath12 and one previous trial vector @xmath13 obtained in the last step .",
    "2 .   choose an initial normalized trial vector @xmath24 , @xmath25 ; and calculate the expectation value ( rayleigh quotient ) @xmath26 .",
    "3 .   for @xmath27 , @xmath21 , do the following iteration loop to refine the trial vector from @xmath24 to @xmath28 , @xmath29 , @xmath15 : 1 .   calculate the gradient of the rayleigh quotient @xmath30 here the refined trial function @xmath4 is normalized at the end of each iteration .",
    "2 .   in the @xmath31 dimensional subspace spanned by @xmath32 , @xmath33 , @xmath34 , @xmath15 , @xmath35 ,",
    "calculate the matrix elements of the matrix @xmath0 , @xmath36 , and the overlap matrix of the basis vector @xmath37 . here , the dimension @xmath38 if @xmath39 , otherwise @xmath40 , i.e. , in the first @xmath41 loops , the subspace has only @xmath42 basis vector .",
    "3 .   find the lowest eigenvalue @xmath43 and eigenvector @xmath44 for the general form eigenvalue problem @xmath45 4 .   from the above eigenvector @xmath44 ,",
    "construct the refined trial vector @xmath14 , @xmath46 .",
    "if @xmath47 is less than a required value or @xmath48 , stop the iteration loop , otherwise continue the iteration loop .    impose a maximum iteration step is necessary in many cases .",
    "for example , in self - consistent calculations , one needs to update the hamiltonian after some steps of iterations .",
    "the trial vector can be chosen , in principle , arbitrarily , provided it is not orthogonal with the lowest eigenvector .",
    "however , even if the initial trial vector does accidently orthogonal to the lowest eigenvector , due to the numeric round off errors in the iterations , one can always arrive the lowest eigenvector .",
    "check the convergence is usually testing the difference between the trial vector and its refined version after an iteration . in our numeric tests , check the difference between two consecutive trial vectors rayleigh quotients also works well . and it is numerically faster .    for large matrices , calculation of the gradient is a main numeric task in each loop of iteration .",
    "it involves a multiplication of matrix and vector .",
    "other numeric costs are mainly the calculation of the matrix elements @xmath49 and @xmath50 in the small subspace , as well as the combination of the gradient and previous trial vectors to form a refined trial vector .",
    "the numeric cost of diagonalising the small matrix @xmath51 is almost nothing as compared with other operations . in each loop of iteration ,",
    "the subspace changes two basis vectors , i.e. , the current gradient @xmath12 replaces the previous one @xmath52 , and the refined trial vector @xmath4 replace the old one @xmath16 .",
    "one needs only to calculate the matrices elements @xmath49 and @xmath50 related to the two vectors in each iteration loop .",
    "if the subspace is three dimensional , the numerical cost of one iteration loop is about the same as that of original cg method .",
    "after finding the lowest eigenvector , one can find the second lowest one in a similar way .",
    "one starts with a trial vector orthogonal to the lowest eigenvector , and in following iterations , gradients of the rayleigh quotient , as well as the updated trial vectors , must be kept orthogonal to the lowest eigenvector . similarly , after working out @xmath53 lowest eigenvectors , the @xmath54 eigenvector can be worked out by maintaining the orthogonality with @xmath53 lower eigenvectors .    in this strict sequential procedure ,",
    "the accuracy of lower eigenvectors affect the higher ones .",
    "a remedy to this problem , according to ref .",
    "@xcite , is re - diagonalising the matrix in the subspace spanned by the refined trial vectors , which is referred as subspace rotation in  @xcite .",
    "after this subspace rotation , one can use these resulted vector as trial vectors for further iteration to improve the accuracy . in practical implementations , we only iterate every trial vector for some steps , then perform a subspace rotation .",
    "the convergence check is to test the eigenvalues differences between two consecutive subspace rotation .",
    "this procedure improves the overall efficiency . in ref .",
    "@xcite , there is a detailed discussion on the role of the subspace rotation .",
    "we test the efficiency of the above outlined algorithm by comparing its performance with other algorithms for various matrices . in all cases , the modified cg algorithm outperforms the original cg algorithm .",
    "we observe significant improvement to the convergence rate in many cases .    as an illustration",
    ", we show in figure 1 a typical result for a banded matrix with bandwidth @xmath55 .",
    "the matrix s diagonal element is @xmath56 , and its off - diagonal elements within the band width is a constant @xmath57 . due to its simple form and its relation with hamiltonian describing the pairing effects",
    ", this matrix has been investigated by some other authors , see , e.g.  @xcite .",
    "here we choose the matrix s dimension @xmath58 with half - bandwidth @xmath59 , the parameter @xmath60 is set to be 20 .",
    "for finding first 8 lowest eigenvectors , the modified cg algorithm converges within 100 steps with an accuracy of machine s precision limit .",
    "it is more than three times faster than the original cg algorithm . as a comparison",
    ", we also show the result for the block lanczos method  @xcite , as well as the steepest decent method . in figure 1",
    ", the convergence rate of one iteration step is defined as the relative error of the two consecutive rayleigh quotients , @xmath61 $ ] , where @xmath62 and @xmath63 are two consecutive rayleigh quotients .",
    "when every eigenvalue reaches the required accuracy , we perform a subspace rotation and repeat the iteration . convergence is to test the corresponding relative error for every eigenvalue between two consecutive rotations . in our implementation , the maximum iteration number @xmath64 , i.e.",
    ", we go at most 500 steps of iteration for each trial vector before performing a subspace rotation .",
    "[ fig1 ]    in the above calculations , we use a 3-dimensional iteration subspace for the modified cg algorithm , i.e. , the subspace is constitute of the current trial vector , its corresponding gradient , as well as one previous trial vector .",
    "in such case , each iteration step needs to calculate one gradient , and some combinations of the three vectors , as well as solving a 3-dimensional eigenvalue problem . from the above argument , when the iteration subspace is 3-dimensional , the numeric cost of each iteration step is almost the same as that of the original cg method .",
    "for the block lanczos algorithm , however , to ensure a reasonable convergence rate , the iteration subspace is 50 dimensional , i.e. , one needs to calculate 50 gradients for each iteration step . to our experience ,",
    "on step of lanczos iteration needs longer cpu time than 50 steps of the modified cg method .",
    "thus , one lanczos step is counted as 50 steps in figure 1 .    in the 3-dimensional iteration subspace spanned by @xmath65 , @xmath4 , @xmath66 the gradient vector @xmath12 , together with the previous trial vector @xmath13 ,",
    "play the same role as that of the conjugate gradient in the minimization of a quadratic function .",
    "this is especially the case when the rayleigh quotient closes to the minimum point , i.e. , it is approximately a quadratic function of the iteration trial vector . in fact , without the previous trial vector @xmath13 , the lowest eigenvector obtained in the 2-dimensional subspace spanned by@xmath67 is just the result of steepest descent method . by including one previous trial vector which contains information about previous gradients ,",
    "one is able to prevent reintroduction of errors to the refined trial vector in the direction of previous gradients .",
    "this is the reason we call this method as modified cg algorithm .    on the other hand , in the sense of relaxation algorithm for finding lowest eigenvector  @xcite ,",
    "the refined trial vector @xmath17 at step @xmath5 , is an approximation to the lowest eigenvector of the matrix in the subspace spanned by @xmath68 . according to the relaxation algorithm , to find the lowest eigenvector in the subspace spanned by the above basis vectors , one starts from an initial trial vector @xmath69 , and minimizes the rayleigh quotient iteratively .",
    "each iterative step is to minimize the rayleigh quotient in a two dimensional subspace spanned by the ( updated ) trial vector , and one basis vector .",
    "the basis vector can be chosen consecutively from the first one to the last one . after going through all basis vectors ,",
    "one continues the next round of iteration by choosing the first basis vector as next basis vector .",
    "this iteration will converge after goes through all basis vectors several rounds .",
    "note that , if one starts with the first basis vector @xmath70 as initial trial vector , in the two dimensional subspace spanned by two consecutive basis vector @xmath71 , and @xmath72 , the second basis vector @xmath72minimizes the rayleigh quotient .",
    "after going through all basis vector for one round , the refined trial vector is @xmath17 , which represents an approximate lowest eigenvector in the above subspace .",
    "the above two factors explain the rapid convergence of the modified cg algorithm .",
    "one consequence from the above arguments is that , if we increase the dimension of the iteration subspace by including more previous trial vectors , the convergence rate will not increase too much . in other words , one needs only do the modified cg algorithm in a small iteration space . to our experience , one needs at most 5-dimensional iteration subspace . in most cases ,",
    "it is enough to do the iteration in the 3-dimensional iteration subspace .",
    "figure 2 shows our numeric result to confirm this property of the modified cg algorithm .",
    "here we do the same calculation using different iteration subspace .",
    "the filled circle connected line is the same as figure 1 with 3-dimensional iteration subspace , and the filled square and triangle are results for 6 dimensional and 12 dimensional iteration subspace respectively .",
    "there is almost no difference within 50 steps where the the convergence rate is about @xmath73 .",
    "one needs almost the same iteration steps to arrive the final precision .",
    "however , the 3-dimensional iteration runs faster for each iteration step since it involves less combination and production of the basis vectors that span the iteration subspace .",
    "[ fig2 ]    for some matrices or some properly chosen initial trial vectors , the rayleigh quotient are approximately quadratic functions of the trial vectors . in such cases",
    ", the modified cg algorithm converges in almost the same rate as the original cg algorithm . and",
    "a trial vector @xmath74 at step @xmath5 , is an almost exact minimum in the subspace spanned by latexmath:[$\\{|\\phi_{0}\\rangle,\\,|\\phi_{1}\\rangle,\\,\\cdots,\\,|\\phi_{n}\\rangle,\\ ,    in fact , near a minimum , any function behaves like a quadratic function .",
    "some matrices with special structures also make the rayleigh quotient like a quadratic function in a quite large region of the vector space . for such matrices ,",
    "the cg method is indeed a very efficient method . of cause , in any cases",
    "the modified cg method always outperforms the original cg method .",
    "the refined trial vector @xmath17 becomes closer and closer to the previous step s trial vector @xmath13 when iteration closes to final solution . in higher dimensional",
    "iteration , one may encounter ( numerical ) degeneracy of basis vectors that span the iteration subspace .",
    "this problem is easy to solve .",
    "one simple solution is to replace this step by an steepest descent s step .",
    "other more sophisticated way is to choose some independent vectors from the basis vectors and do this step in a small subspace .",
    "both methods are easy to implement .",
    "in fact , one can detect the degeneracy when solving the general form eigenvalue problem ( [ 3 ] ) which can be conveniently solved by the conventional choleski - householder procedure  @xcite .",
    "if there is a degeneracy , the choleski decomposition of the overlap matrix @xmath76 returns an error code .",
    "when this happens , one can simply redo this step with a steepest descent step .",
    "alternatively , one can use a more sophisticated choleski decomposition program that automatically chooses independent basis vector . in doing so",
    ", one must adjust the the matrix element of @xmath51 simultaneously .",
    "this two methods need almost the same numerical cost .",
    "of course , the first method is easy to implement . in our numerical tests",
    ", there is almost no degeneracy in the 3-dimensional iteration subspace .",
    "it is straightforward to implement preconditioning treatment for the modified cg algorithm .",
    "preconditioning treatment can significantly improve the convergence rate for matrices with large difference between lowest and highest eigenvalues . due to the fact that there is no need to construct explicitly the conjugated gradient in the modified cg algorithm ,",
    "it is easier to implement the preconditioning treatment by direct modifying each step s gradient .",
    "since preconditioning treatment depends on specific system , we do nt go into more details about such topic .",
    "the modified cg algorithm shares a common feature with many other iterative methods of diagonalising matrices , such as lanczos , dividson , rmm - diis , and relaxation method . in all these algorithms ,",
    "one refines the trial vector in iterative subspaces .",
    "what makes the modified cg algorithm different from other algorithms is that the iteration subspaces are spanned by the trial vectors of previous iteration steps , as well as the latest trial vector and its gradient .",
    "the trial vectors of previous steps are already prepared , one needs only calculating one gradient vector ( and possibly does some preconditioning treatment ) to construct the basis vectors of the iterative subspace .",
    "only two basis vectors of the iterative subspace are different from previous one , it needs only update two columns of the matrix elements in the iteration subspace . by including previous trial vectors into the iterative subspace",
    ", one avoids reintroduces errors to the trial vectors in the previous directions of gradients .",
    "these properties of the iterative subspace make the modified cg algorithm numeric efficient .",
    "and the common feature of the algorithm makes it easy to implement .",
    "it is easy to formulate block algorithm for the modified cg algorithm to find several lowest eigenvectors simultaneously . for this end ,",
    "one refines several trial vectors at each iteration step . here",
    "the iteration subspace includes all current trial vectors , their gradients , and all trial vectors of some previous steps . in this implementation",
    ", one needs to find several eigenvectors by solving the general form eigenvalue problem ( [ 3 ] ) .",
    "trial vectors obtained in this way are automatically orthogonal with each other , and one needs no additional subspace rotation",
    ".    however , one step of block algorithm usually needs more floating point operations than sequentially processing each trial vector and maintaining orthogonality between trial vectors by schmidt orthogonalization method .",
    "this is mainly because the block algorithm needs more flops to form the matrix elements of @xmath51 and the corresponding overlap matrix @xmath76 .",
    "if one needs @xmath77 lowest eigen - solutions for @xmath7 dimensional matrix , the block algorithm s iterative subspace is @xmath78 dimensional with @xmath79 .",
    "each step of block algorithm needs the following floating point operations : ( a ) @xmath80 flops for @xmath77 matrix multiplying vector operations to obtain @xmath77 gradients , where @xmath81 is the band width of the matrix ; ( b ) @xmath82 flops for the formation of the matrix elements of @xmath51 in the iterative subspace and the corresponding overlap matrix @xmath76 ; ( c ) an @xmath83 floating point operations for solving the general form eigen - value problem ( [ 3 ] ) ; ( d ) @xmath84 flops for combination the @xmath85 basis vectors to form @xmath77 refined trial vectors . here",
    ", the flops in step ( c ) is negligible when @xmath86 .",
    "the total flops of one step block algorithm is @xmath87 . if @xmath88 , the above floating point operations @xmath89 is the flops for processing one trial vector in sequential algorithm",
    "one the other hand , sequentially processing each trial vector one round needs @xmath90 flops . here",
    "the second term is the flops to maintain the orthogonality of trial vectors , including making gradients orthogonal to previous trial vectors . even including subspace rotation which is performed after some rounds of sequential steps",
    ", the sequential implementation needs less floating point operations than the block algorithm .",
    "if @xmath77 is small , e.g. , @xmath91 , the difference of flops between block and sequential algorithm is small",
    ". the block algorithm may be one choice in such cases . like the block lanczoz  @xcite , and block dividson  @xcite",
    ", there are some other ways to form the iterative subspace to implement the block version of modified cg algorithm . for example",
    ", the iterative subspace may contain only one gradient , plus all the current trial vectors and some previous trial vectors .",
    "the choice of iterative subspace affects the convergence properties which needs further investigations . for large @xmath77 , e.g. , @xmath92 , to our experiences , block algorithm need more numeric cost and is less efficient as compared with the above sequential implementation .",
    "the dimension of the iteration subspace grows quickly with the number of needed eigenvectors , and one needs more memory to store the basis vectors and much more cpu time to solve the general from eigenvalue problem ( [ 3 ] ) which increases drastically with the dimension of the iterative subspace . since lowest eigenvector usually converges faster that higher ones , the number of iteration steps in a block algorithm is determined by the the vector with slowest convergence rate",
    "in summary , in the sense of conjugated gradient algorithm , we formulate an iterative method to find a set of lowest eigenvalues and eigenvectors of a matrix .",
    "this method minimizes the rayleigh quotient of a trial vector via the gradient of the rayleigh quotient , and at the same time , prevents reintroduce errors in the direction of previous gradients .",
    "we realize such idea by refining the trial vectors in a special kind of iteration subspaces . each iteration subspace is spanned by the latest trial vector and the gradient of its rayleigh quotient , as well as some trial vectors of previous steps .",
    "each iteration step is to find lowest eigenvector in the iteration subspace .",
    "the gradient , together with the previous trial vector , play the role of the conventional conjugated gradient . in our numerical test ,",
    "it is usual enough to include only one previous trial vector , i.e. , one needs only refining the trail vector in a 3-dimensional subspace . as compared to the conventional conjugated gradient algorithm , which is designed to minimizes a general function ,",
    "the current method exploits special properties of eigenvalue problems , and thus converges much faster in many cases . during iterations ,",
    "the trial vector at the step @xmath5 , is an approximately lowest eigenvector in the subspace spanned by the initial trial vector and @xmath5 subsequent gradient vectors .",
    "this is the reason of rapid convergence rate .",
    "the easy implementation of this algorithm makes it suitable for first principle calculations .",
    "this work is supported in part by the national natural science foundation , the research fund of the state education ministry of china , and the research fund of the wuhan university .",
    "we thanks helpful discussions with prof .",
    "w. wang .",
    "e. r. davidson , j. computat .",
    "* 17 * , 87 ( 1975 ) ; b. liu , in report on the workshop _ numerical algorithms in chemistry : algebraic methods _ , edited by c. moler and i. shavitt ( university of california , berkley , 1978 ) , p. 49"
  ],
  "abstract_text": [
    "<S> we present an iterative method to diagonalise large matrices . </S>",
    "<S> the basic idea is the same as the conjugated gradient ( cg ) method , i.e , minimizing the rayleigh quotient via its gradient and avoiding reintroduce errors to the directions of previous gradients . </S>",
    "<S> each iteration step is to find lowest eigenvector of the matrix in a subspace spanned by the current trial vector and the corresponding gradient of the rayleigh quotient , as well as some previous trial vectors . </S>",
    "<S> the gradient , together with the previous trail vectors , play a similar role of the conjugated gradient of the original cg algorithm . </S>",
    "<S> our numeric tests indicate that this method converges significantly faster than the original cg method . </S>",
    "<S> and the computational cost of one iteration step is about the same as the original cg method . </S>",
    "<S> it is suitably for first principle calculations . </S>"
  ]
}