{
  "article_text": [
    "extremely large surveys provide the means to take great steps forward in a wide range of astronomical fields , because they probe the large volumes required to detect those very rare objects that would otherwise be nearly impossible to find , and yield the immense sample sizes essential for robust statistical analyses .",
    "the crowning achievement of such an approach has undoubtably been the sloan digital sky survey ( sdss , york et al .",
    "2000 ) , which at the time of writing is in its eighth data release , and now covers 14000 square degrees of imaging , and has obtained spectra for millions of objects ( aihara et al .",
    "sdss marked the beginning of an era of extremely large digital sky surveys and continues to demonstrate its power ( in the form of ` sdss - iii ' ) across a remarkably wide range of scientific areas , from galactic studies to cosmology .",
    "the panoramic survey telescope and rapid response system ( pan - starrs ) , large synoptic survey telescope ( lsst ) and dark energy survey ( des ) , amongst others are poised to take up the mantle set by sdss in the last decade .",
    "these surveys will provide deeper optical and near - infrared imaging over the majority of the solid angle of the sky .",
    "the coming years and decades will see this panoramic approach spill over into other frequency domains ; indeed , our view of the universe already being transformed by more sensitive large area surveys in the infrared and submillimeter ( e.g.  _ wide - field infrared explorer _ [ wright et al .",
    "2010 ] , _ herschel _ [ e.g.  eales et al .  2010 ] , the submillimeter common user bolomoter array2 ) and soon the radio regimes ( e.g.  low frequency array [ lofar ] , and square kilometer array pathfinders [ e.g.  norris et al .",
    "we are certainly entering exciting times in terms of our capability to survey the universe across most of the electromagnetic spectrum , but these large surveys pose a common challenge : how does one efficiently mine the parameter volume when we move into the petabyte regime of information content ? innovative techniques that can efficiently sift and filter the myriad data will be vital , since often one is interested in selecting for a very specific subset of data , for example searching for rare objects ( populations of high - redshift galaxies , quasars , low - mass stars or gravitational lenses for example ) or events ( supernovae , gamma - ray bursts and other transient phenomena ) .    by design , most of the next - generation panoramic surveys will involve simple continuum imaging .",
    "however , many projects will be complemented by ancillary sub - surveys that will accrue more ( deeper ) imaging and spectroscopic observations over smaller areas .",
    "the largest surveys that cover most of the sky will necessarily overlap with fields that have already undergone significant observational investments .",
    "several of these ( e.g.  the great observatories origins deep survey , chandra deep fields , cosmological evolution survey [ cosmos ] field and the like ) have been targeted with deep imaging from virtually all terrestrial and space - based facilities , and often have been subject to extensive spectroscopic campaigns , providing very large , deep redshift catalogues ( e.g.  lilly et al .",
    "as time goes on , the interplay between overlapping surveys will become more important as we move towards a truly holistic picture of the sky .",
    "artificial neural networks , or more generally the technique of ` machine learning ' , has been used as a tool in astronomy ( as well as other scientific disciplines and industrial applications ) for some time ( e.g.  storrie - lombardi et al .  1992 ; lahav et al .",
    "1995 ) ; the estimation of galaxy redshifts from photometry is a classic example .",
    "perhaps the most successful application of neural networks in an astronomical setting in the past few years has been the ` crowd sourcing ' technique of galaxy zoo ( raddick et al .",
    "2008 ; lintott et al .",
    "2008 ) , which exploits thousands of humans to perform visual classification of galaxies from the sdss , relying on the superior capabilities of the human brain ( i.e.  a _ real _ neural network ) for pattern recognition .",
    "many of the established machine learning techniques employ supervised learning , where the neural network is trained using a series of input pairs .",
    "a common example is the multi - layer perceptron , where each input pair consists of a vector ( a set of photometry for example ) and a known output ( a spectroscopic redshift ) .",
    "the network then attempts to find the mapping that successfully converts the input vectors to the required output  in this sense , it is ` supervised ' learning .",
    "after training , new input vectors ( e.g. photometry ) can be passed through the network to predict their output ( redshifts , e.g.  collister & lahav  2004 ) .",
    "an alternative approach is to use the input vectors themselves to find the mapping , since if such a mapping between parameters ( or combinations of parameters ) exists , this information should be latent in the input catalogue . in this paper",
    ", i will describe a specific type of unsupervised machine learning  the kohonen self - organising map ( som , kohonen  1982 , 2001 )  as a tool for data mining in astronomy .",
    "soms have found application in other scientific disciplines , notably geophysics and genetics , and other disparate areas , especially those where some form of pattern recognition is required . while there has been some use of self - organisation and soms in astronomical applications ( e.g.  nez & llacer  2003 , mahdi  2011 ) ,",
    "the technique is not in widespread use .",
    "in essence , a som is a neural network that takes as input a large training set ( in this case large astronomical catalogues ) , and maps it by a process of competitive learning , where neurons compete to become more like members of the training set .",
    "the resulting map is a representation of the topology of the input parameter space , encoding correlations between parameters , and allowing one to visualise the high - dimensional properties of a parameter volume in a low - resolution , lower - dimensional way .",
    "this self - organised mapping has often been described as a form of non - linear principle component analysis , and is a variant of _ k_-means clustering algorithms .",
    "it allows one to identify special features ( for instance , clustering ) in the input catalogue . as the algorithm itself is effectively classifying new input data based on previously seen specimens , after training soms be used to ` classify ' new inputs , even if they only contain a sub - set of the original information used to train the original map .",
    "the method is unsupervised in the sense that the user is not required to specify the desired output , as the ` mapping ' of components of the input vectors is a natural outcome of the learning .",
    "indeed , perhaps the most fascinating aspect of large soms is the potential for emergent behaviour ( ultsch  2007 ) allowing one to discover new properties of the input data that would be imperceptible otherwise .    in summary ,",
    "the som is an extremely versatile tool , and could have several possible uses , however in this paper we demonstrate two of its main applications : object selection and parameter estimation . in ",
    "2 we describe the algorithm , including a toy example and in  3 we present the two practical examples using data from the the cosmos field ( scoville et al .  2007 ) . at the end of the paper we provide a brief list of common som terminology for convenience .",
    "the som algorithm used here as a python class is made available at http://www.physics.mcgill.ca/@xmath0jimgeach/som , or from the author on request .",
    "the som can be considered as a collection of ` nodes ' arranged in a grid of arbitrary dimension , although for visualisation purposes two dimensions are most common .",
    "each node is attached to a vector of ` weights ' @xmath5 with the same dimension as an input ` training ' vector , @xmath6 . in the case of a galaxy survey for example ,",
    "a @xmath6 could comprise of five measurements of _ ugriz _ photometry .",
    "in fact , the input data need not actually be vectorised ; any input  provided it can be digitized  way could be considered .",
    "a further example to consider might be a digital astronomical image , where we might expect the som to help perform morphological classifications .",
    "througout this work however , we will consider the case of inputs that are represented as vectors , with each vector component made up from standard ` catalogue data ' such as photometry and redshift information .",
    "the map can be considered as a set of ` component planes ' , with a given node in the @xmath7 plane taking the value of @xmath8 . in the two dimensional representation ,",
    "plotting two or more component planes next to each other provides a low - resolution visual representation of the higher - dimensional topology of the input data .",
    "how does the som achieve this mapping ? to start ,",
    "each node is initialised with a random weight ; this can be selected from a uniform distribution , or an arbitrary probability distribution ( limited according to a sensible physical range of values ) , or even randomly sampled from the input training set .",
    "the learning process is then a set of iterations , and follows a simple philosophy : each node ` competes ' to be the best match to a randomly selected vector from the training set .",
    "the winning node  called the best matching unit ( bmu )  is rewarded by being allowed to become more like the input vector . in addition , nodes in the vicinity of the bmu , @xmath9 , are also allowed to be altered in the same direction , but to a lesser extent than the bmu .",
    "after many samplings , the nodes can learn to become more like the training set , with the distribution of weights representing the probability distribution of the training set and the relationship between the components of individual weights encoding correlations between parameters .",
    "most importantly , similar nodes get grouped together in the map .",
    "this allows one to examine the parameter space topology , and can be used to search for clusters within the parameter space of the training set , and thus provides a means of object classification .",
    "in addition , the bmu of any new test galaxy ( for example ) contains the som s ` best guess ' of what that galaxy s parameters should be , based on similar galaxies it has seen before . in the case of incomplete data for a new test galaxy ( for example a missing redshift )",
    ", the bmu can provide a prediction for what that missing parameter should be .",
    "thus , the som can be a predictive tool .",
    "the process of learning occurs over a series of @xmath10 iterations . at each iteration @xmath11",
    ", nodes compete to be the best match to a randomly selected training vector , with the bmu being rewarded by changing its weight vector in the direction of the training vector .",
    "crucially , nodes within some vicinity of the bmu ( @xmath12 ) are _ also _ allowed to adapt , but to a lesser extent than the bmu .",
    "the effect is that nodes with similar properties end up grouped close to each other on the map .",
    "the adaptation is set by a learning handicap , called the ` neighbourhood function ' @xmath13 that falls off with @xmath14 , and decays with learning time .",
    "the exact form of the neighbourhood function , @xmath13 , is arbitrary , but a gaussian function is often chosen as a suitable form : @xmath15 where @xmath16 depends on time : @xmath17 here @xmath18 is a decay constant , usually chosen to be equal to the number of iterations , @xmath10 .",
    "the region of influence around the bmu @xmath9 shrinks over time @xmath11 , such that ever smaller regions of the som are allowed to adapt as @xmath19 : @xmath20 where @xmath21 is taken to be half of the size of the map . finally , _ all _ nodes in the som have their learning handicapped over time , with an additional factor , @xmath22 the effect of these decaying learning rates and neighbourhood function is sequence of refinement , where the most dramatic and coarse organisation of nodes occurs early in the learning process , with subsequent steps fine - tuning the som on smaller scales and resolving more subtle topology in the data .      from the learning sequence described in  2.1",
    ", the algorithm itself can be summarised as follows :    1 .",
    "initialise the som by randomly assigning vectors to each node . the vectors can be selected uniformly from within some suitably limited parameter volume , or take the values of vectors sampled randomly from the training set .",
    "a training vector @xmath23 is picked randomly from the training set .",
    "3 .   for the @xmath7 node in the map described by weight vector @xmath24 ,",
    "the euclidean distance of that weight from the @xmath25 training vector @xmath26 is assessed : @xmath27 .",
    "the winning node has @xmath28",
    " i.e. it was the closest to the input vector , and becomes the bmu .",
    "every node within the region of influence , @xmath9 , is allowed to be pulled in the direction of the input vector , weighted by the learning factors ( equations 14 ) : @xmath29 .",
    "the factor @xmath30 is an optional additional weighting that can take into account the measurement uncertainty , or data quality of each element of the training vector .",
    "this penalises unreliable data by not allowing it to contribute heavily to the development of the map . 5 .",
    "repeat steps 24 for @xmath10 iterations",
    "( where @xmath10 is sufficiently large that is over - samples the training set several times ) , or until @xmath31 ( or a user chosen minimum ) .",
    "after many iterations , the som will evolve such that similar regions are geometrically close to each other on the map .",
    "although the nodes of the som are distributed in a 2d grid , the boundaries of the grid are periodic , such that the 2d projection is effectively an unravelled toroid .",
    "wrapping the boundaries ensures that trained nodes are not ` pushed off ' the boundaries of the map . a plot of the 2d grid coloured by the value of the @xmath7 weight of each node",
    "is called a component plane , and comparison of different component planes can be used to study relationships between parameters in the training set .",
    "restricting the learning rate of the som as a function of time , and only allowing it to change in ever finer regions , ensures that the introduction of new training vectors refines the som , rather than obliterating the learning of previous iterations . on this note ,",
    "one requires the total learning time ( i.e. how many training vectors are used in the learning ) to sufficiently over - sample the input training set so that all training vectors are given a chance to contribute to the learning at different stages of refinement .",
    "note that since the som is initialised randomly , and training vectors are selected randomly , soms trained on the same input set will not ` look ' identical , however the encoding of the map should be equivalent  all that matters is that similar nodes are close to each other ( and distant from dissimilar nodes ) on the toroidal surface .",
    "the key characteristic of the self organisation is that it retains the ` topology ' of the input training set , revealing correlations between inputs that are not obvious .",
    "in fact , the som is often described as a form of non - linear principle component analysis .",
    "before we move to real world data , to demonstrate the concept of self - organisation , we consider a simple toy example . in this example , we have two ` populations ' which are simply represented as two gaussian distributions .",
    "we will label these as ` red ' and ` blue ' .",
    "red and blue have means of @xmath32 , @xmath33 and both have scales @xmath34 .",
    "we now randomly draw 10000 samples from red and blue and consider these as our training set  simply a list of 20000 numbers .",
    "can we use self - organisation to separate these two populations and predict whether a new test value belongs to the red or blue population ?",
    "of course , this is a trivial example , because we could have achieved the same result by simply plotting a histogram of the paramater values , found the form of the toy distributions and therefore assign a probability to any new value to determine the likelihood that it belongs to red or blue .",
    "still , this is a good demonstrative example .",
    "we create a @xmath35 node som , initialised with random weights selected uniformly from the 20000 member training set .",
    "we set - up the initial som parameters as described in  2.1 , and allow the total number of iterations to be 200000 , thus over - sampling the training set by a factor of ten .",
    "the single component plane of the som seen at different stages of the learning process is shown in figure1 .",
    "the component plane is coloured by the ` value ' of the ( in this case single element ) weight of each node , each of which has competed to represent the values of the training set .",
    "it is clear that the map has arranged itself into two distinct regions  this is because the training sample is itself distributed around two distinct values ( the means of the distributions ) .",
    "nodes that are close together ( in terms of map distance ) have similar values , and there is a clear interface region on the map where the distributions overlap . to reinforce this point , next to the component plane",
    "we also show the so - called unified distance matrix ( udm , or u - matrix ) , which visualises the average distance between the weights of neighbouring nodes .",
    "the u - matrix is a means of identifying boundaries of ` clusters ' within the map .",
    "small udm values indicate that neighbouring nodes have very similar weights and larger values indicate transition regions between clusters .",
    "the segregation of the different parts of the map allows us to label certain nodes in the map as ` blue ' and some as ` red '  in other words , it will allow us to classify new inputs ( i.e.  new samples that the som has never seen ) based on their bmu ( it will either be in the red or blue class ) . to further demonstrate this , in figure1 we show two versions of the som but this time scale the size of nodes based on the probability that their weight value was drawn from the red or blue distributions .",
    "this clearly highlights how the different parts of the map defined by the udm correspond to the two clusters in the input parameter space . in figure",
    "2 we show the actual input distribution of red and blue objects , and the distribution of the values of the weights of nodes in the trained map .",
    "note that the som has identified several nodes which define an ambiguous classification where the two abundance of the two populations is equal at values near zero .",
    "we have labelled 101/400 nodes as ` red ' and 135/400 nodes as ` blue ' classifications based on the map division made apparent by the udm .",
    "to test the som , we use these nodes to classify 1000 _ new _ inputs from each of the red and blue populations to find the identification rate , defined by the fraction of new blue or red objects that correctly classified based on their best matching unit in the trained som .",
    "the results are shown in figure2 . not only does this simple classification procedure",
    "successfully identify new test data , it correctly recovers the main properties of the underlying distribution : the mean and standard deviations of the values of objects classified as red and blue are @xmath36 , @xmath37 , @xmath38 and @xmath39 , compared to the input distribution of @xmath40 and @xmath33 and @xmath34 .",
    "exactly the same principle can be applied to astronomical data sets , and so building on this trivial example we now demonstrate two real world applications of a som trained on galaxy data from the cosmos survey , where we now include many more parameters in the training .",
    "colour - colour plots are a traditional method of isolating objects of interest , since populations with similar spectral properties will have similar broadband colours and therefore cluster together , or follow loci in appropriate colour - magnitude or colour - colour planes .",
    "perhaps the most successful example in extragalactic studies is the selection of distant galaxies by virtue of the lyman break drop out , where uv  optical broadband filters that straddle the redshifted lyman break can efficiently sift @xmath41 galaxies from a field ( steidel & hamilton  1993 ; madau  1995 ; steidel et al .",
    "there are many similar examples of highly effective selection of objects using simple colour criteria , and more recently this has been applied with great success for very high-@xmath42 ( @xmath43@xmath44 ) galaxies that drop - out of optical bands altogether ( e.g.  bouwens et al .",
    "2010 ; mclure et al .",
    "more complicated selections can be constructed not only to pick - out galaxies at specific redshifts , but also isolate those with certain properties ( e.g.  the star - forming / passive @xmath45 galaxy selection of daddi et al .",
    "2004 ) .    here",
    "we demonstrate how the som can be used like an _ n_-dimensional colour - magnitude diagram , and when trained using a large catalogue , exploited to identify those ` clusters ' of interesting objects .",
    "the trained som can then be applied as a classification and filtering tool to extract objects of interest from a new input catalogue .      _",
    "spitzer _ irac ( 3.68@xmath4 m ) colours have been shown to be very effective at selecting agn , including those whose optical emission is obscured by dust , since these objects have a characteristic red power - law continuum in the near / mid - infared that starts to dominate over the stellar emission at a rest - frame wavelength of @xmath462@xmath4 m ( lacy et al .  2004 ; stern et al .",
    "this results in a red locus in irac colour space that stands out prominently from the general galaxy population .",
    "can we identify this population by self - organising a catalogue of galaxies with irac photometry ?",
    "we take the _",
    "component of cosmos ( s - cosmos , sanders et al .  2007 ) and set the training weights to be the 3.6@xmath4 m flux and the 5.8@xmath4m/3.6@xmath4 m and 8.0@xmath4m/4.5@xmath4 m colours . a som is initialised with @xmath35 nodes . for the purposes of this demonstration we restrict the catalogue to detections in all four bands and a 3.6@xmath4 m flux limit of 50@xmath4jy .",
    "the training catalogue has 10488 objects , and we allow the som to iterate 104880 times in order to over - sample the catalogue by a factor of ten during the training .",
    "the three component planes of the trained som are shown in figure  3 , clearly showing the structures representing the input catalogue ( recall that the boundaries of the grid are periodic ) . to help interpret the figure ,",
    "the nodes have been labelled with a numeric index , and to understand the correlations between the component planes , one should compare the value of common nodes in each plane .",
    "correlations between the two colours is apparent , and we can clearly identify the cluster of nodes that are red in both sets of colours ; these nodes represent the locus of galaxies with power - law colours that would be apparent in the traditional colour - colour plot ( fig.4 ) .",
    "therefore , rather than parameterising the agn selection as a series of cuts in colour space , we can simply use the map as a filter , classifying any input galaxy as an agn if its bmu is one of these nodes .    to illustrate the accuracy of the selection , and to verify which are the correct nodes to use as robust ` agn selectors '",
    ", we have taken the 83 galaxies in the _",
    "z_cosmos catalogue ( lilly et al .",
    "2007 ) identified as broad line agn ( blagn ) and found their bmu in the trained som .",
    "nearly 50% of the blagn fall in just two nodes , and 80% are described by 11 nodes , most of which are contiguous ; we highlight these in figure3 .",
    "although the som can ` discover ' new classifications , the use of known objects ( in this case spectroscopically identified blagn ) can be of great use when trying to label nodes , and to assess the quality of subsequent selections .",
    "for example , the two nodes that successfully describe 50% of the blagn could be taken as ` high - confidence ' agn nodes , with the remaining nodes being lower confidence selectors .",
    "of course , as in normal techniques , there is a balance between completeness and contamination . as a guide to the contamination rate",
    ", we consider the two ` high confidence ' nodes and find out how many of the _ z_cosmos galaxies that are not classified as blagn ( and have very secure redshifts , confidence class 3.5 or 4.5 ) fall into these nodes .",
    "together , the two nodes 48 and 69 pick out 114 galaxies that are not blagn , in addition to the 39 ( in the same redshift confidence class ) that are . however ,",
    "most of this contamination is from just one node ( 69 ) ; if we restrict our agn selection node to 48 only ( actually the reddest in irac colour , see fig3 ) , of the 19 _ z_cosmos galaxies that match this node , _ only one _ is not classified as a blagn .",
    "these contamination rates should only be taken as a guide , given the likely incompletenesses in the spectroscopic selection and classification of galaxies in the input _",
    "z_cosmos catalogue . nevertheless , it is clear that the som could provide a very clean method for selecting objects of interest .    in figure  4",
    "we plot the traditional irac colour - colour plane , with the standard lacy  stern selection wedge indicated ( although the stern selection is actually defined slightly differently , the broad selection is effectively the same ) .",
    "as described above , we have chosen 11 nodes as our ` agn classification ' , two of which we define as high - confidence .",
    "we then re - pass the input catalogue through the som , this time noting which sources have bmus matching one of these classification nodes . the result is a clean selection of galaxies along the expected agn locus , and as expected we identify 80% of the blagn from the _",
    "z_cosmos sample .",
    "it should be possible to refine the efficacy of the selection by moving to a higher resolution som ( more nodes ) , which would improve the ability of the som to resolve finer details in the topology of the data - set ( in fact it is not clear what the optimum som resolution is for a given training set , but ideally it should have many more nodes than there are parameters , see  3.3 ) . here",
    "we chose a fairly coarse @xmath35 som to better illustrate the component planes , and even at this low resolution , the som is a remarkably powerful and clean selection tool .    to improve the selection of different types of agn ,",
    "more information could be added to the training .",
    "for example , if one wanted to distinguish between obscured and unobscured agn , an optical band could be introduced : the optical  nir colours of obscured agn are significantly redder than unobscured agn ( e.g.  hickox et al .",
    "the extra information carried by , say , the @xmath47})$ ] colour would allow the som to separate the two classes .",
    "in this special case we already knew that we were classifying agn , and could easily identify the part of the som that mapped this sub - population ; a task that was made easier with the sample of robustly identified blagn .",
    "however , perhaps the most exciting possibility the som offers is the opportunity to detect _ new _ classifications based on clustering in the parameter volume that become apparent in component planes that would otherwise be undetected using standard techniques . as described in  2.3",
    ", one technique of identifying significant clustering is to calculate the so - called unified distance matrix ( udm ) or u - matrix , which visualises the average ` distance ' to neighbouring nodes .",
    "clusters of nodes that are close to each other ( i.e.  similar udm values ) , bordered by regions where the udm values are large could be considered as clusters , and therefore potentially new classifications .",
    "we show the udm for the present example in figure3 , although this is not always an appropriate method of identifying clusters .",
    "upcoming large surveys hold great promise for this type of data exploration ; once new potential classifications are identified with the som , it should be possible to isolate those objects and properly assess their nature .",
    "the som provides a way of finding those key , potentially rare objects from the overwhelmingly large catalogues that are currently being produced .",
    "what if we did not have the full slew of irac photometry for a new test galaxy that we wish to classify , but instead have another photometric band ? here we consider how cleanly the som trained above can select those obscured agn using just @xmath48 , 3.6@xmath4 m and 4.5@xmath4 m photometry ( this is a practical example , now that _",
    "spitzer _ is operating in post - cryogenic ` warm mode ' it has lost the capability of its longer wavelength detectors , and so new fields will not have 5.8 and 8@xmath4 m photometry to perform the classic selection ) .    to approach this challenge , during the training of the som we allowed two `",
    "phantom ' components to be added to each training vector : @xmath49})$ ] and @xmath50-[4.5])$ ] .",
    "the @xmath48-band data ( from the infrared side port imager on the 4 m cerro tololo inter - american observatory and flamingos on the 4 m kitt peak national observatory ) is taken from the cosmos photometric catalogue ( 2006 version , capak et al .",
    "these additional components are not allowed to take part in the learning ( i.e.  they are not considered in part 3 of the algorithm in  2.2 ) , but their values are still allowed to change , and thus they get mapped into the som . in effect , this tells us what set of @xmath49})$ ] and @xmath50-[4.5])$ ] values correspond to the full - band irac agn selection described above .",
    "we can now introduce new test galaxies and find out what their bmu is on the basis of _ just _ their @xmath51})$ ] and @xmath50-[4.5])$ ] colours .",
    "again , if they match any of the nodes we tagged as ` agn ' above , these galaxies can be sifted out , but we expect the selection to be less efficient , since it is now easier for galaxies to be scattered away from the selector nodes .",
    "we plot the result of this alternative selection in figure  4 .",
    "as expected , the efficacy of the selection has been reduced , with only @xmath040% of the blagn identified , and more scatter away from the standard locus ( especially in the case of the lower - confidence nodes ) .",
    "nevertheless , the som is still effective at picking - out agn , even in this case where we are ` missing ' some of the information used in the traditional selection .    as mentioned above",
    ", the performance could be improved by moving to a som with a larger number of nodes , and thus allowing finer mapping resolution . in the case of using @xmath49})$ ] and @xmath50-[4.5])$ ] colours ,",
    "contamination could also be reduced by only using a sub - set of the nodes we have classified as agn on the basis of their 3.68@xmath4 m colours  i.e. just using the ` highest quality ' nodes , as indicated in fig4 .",
    "when estimating a photometric redshift , we assume that there is a mapping between a galaxy s true redshift @xmath42 , and photometry vector @xmath52 such that @xmath53 .",
    "if such a mapping exists , then the information to find @xmath54 should be latent in a large galaxy catalogue where both photometry and spectroscopic redshifts are available .",
    "self - organisation of such a training set will naturally encode @xmath54 , and therefore a som can be used to predict the redshifts ( or indeed any other parameter that was involved in the training ) of new galaxies where , for example , only a subset of photometry is known .",
    "this technique could be easily applied to a large imaging survey that contains a smaller spectroscopic component in order to robustly estimate redshifts for those galaxies lacking spectroscopic coverage .",
    "the advantage of using a som for photometric redshift estimation is that it is completely empirical , requires no assumptions about the spectral properties of the galaxies and involves no user intervention to guide the learning ( i.e.  the learning is unsupervised ) .",
    "however , there are two fundamental limitations to the method :    1",
    ".   the som can not accurately extrapolate the properties of objects , should they fall outside of the parameter volume of the original training set .",
    "for example , if a catalogue limited to @xmath55 is used to train the som , it will catastrophically fail to predict the redshift of a @xmath56 galaxy , because a galaxy of this type has not been ` seen ' by the som .",
    "instances of such failures could be flagged , because their ` distance ' from the bmu will be large .",
    "it is therefore essential that the training set is a representative sample , the larger the better , with a well known redshift distribution that can aid in the interpretation of the reliability of predictions .",
    "+ actually , one _ could _ use information stored in the som to extrapolate photometric redshifts beyond the range of the training set , in the sense that the photometric weights of each node actually represent low - resolution versions of the spectral shapes of galaxies ( the broad - band photometry could be simply interpolated to provide a continuous spectrum ) .",
    "although more time - consuming , if these low - resolution spectra were trusted to be a representative sample of the full range of galaxy types , it should be possible to use them in the usual @xmath57 template fitting procedures of other photo-@xmath42 methods , allowing the spectra to redshift beyond the upper bound of the training set and convolving with the relevant filter transmissions .",
    "2 .   related to ( 1 ) , any biases in the training set will also bias the prediction of unknown parameters in new test data . in this example that bias might be the redshift distribution of the training set ; the som will have seen more examples of galaxies at the peak of the distribution compared to the tails , potentially biasing the redshift estimates of galaxies in the tails towards the centre of the distribution .",
    "similarly , if the training set contains exclusively red galaxies ( classically selected luminous red galaxies for example ) , then the som will only be useful in predicting the properties of input galaxies with similar characteristics . in summary ,",
    "the predictive power of the som is a strong function of the parameter distribution of the training set , and so a proper understanding of the statistical nuances of the training set is of critical importance when interpreting the som .    in this demonstration",
    ", we again use the photometric data from cosmos ( capak et al .",
    "2007 ) and s - cosmos ( sanders et al .",
    "2007 ) , but merge it with 8910 spectroscopic redshifts from version 3.5 of the @xmath42cosmos ( bright ) sample , the @xmath58magnitude limited spectroscopic branch of the survey ( lilly et al.2007 ) .",
    "the training sub - set is limited to galaxies with @xmath59 that are detected in each of the _ u@xmath3bgvrizk@xmath60 _ and [ 3.6 ] , [ 4.5 ] , [ 5.8 ] , [ 8.0 ] bands . in general , missing data ( e.g.  lack of coverage in a particular band for a galaxy , perhaps due to masking or contamination ) could be dealt with , for example , by not allowing the missing weight to contribute to the learning and/or handicapping the learning coefficient for that particular test vector .",
    "similarly , upper detection limits can be treated as equivalent to measurements at the relevant significance , but for the purposes of clarity in this demonstration , we require detection in all bands .    the number of objects in the catalogue after enforcing these constraints is 7651 , with a median redshift of @xmath61 . in order to test the predictive power of the som , the sample is randomly split in two , such that one half of the catalogue can be used for training , and the other for testing ( where the som has not had an opportunity to see those galaxies ) .",
    "the training set therefore consists of 3825 unique inputs .",
    "multiple neural networks , or ` committees ' , are often used to increase the robustness of predictions ( e.g.  collister & lahav  2004 ) .",
    "committees introduce an extra level of stochasticity that provide a measure of uncertainty through an examination of the fidelity of predictions made by committee members . here",
    "we initialise ten soms , each with @xmath62@xmath63@xmath62 nodes , and set the total number of iterations per som to 382500 , thus over - sampling the input training set by a factor 10 for an individual map , and a factor 100 over the committee . to introduce an extra level of randomness , the initial choice of learning coefficient @xmath64 ( equation 4 )",
    "is selected from a gaussian distribution centred at unity with a scale of 0.1 ; this allows each som to learn at slightly different rates .",
    "the final predicted value is taken to be the mean of the individual predictions from the committee members , and the standard deviation of these predictions we take to be the uncertainty in the estimate .",
    "if we had used many more soms in the committee , it should be possible to collect the results together to form a probability density function for the parameter prediction , which might provide a better representation of the uncertainty ( note that soms can be trained in parallel for this purpose ) .    in our example",
    ", for each training vector we have a set of broad band photometry and a spectroscopic redshift .",
    "we set these as the weights of each training vector . to reduce the parameter space ,",
    "we assign the photometry as a set of colours in consecutive bands , @xmath65 , @xmath66 , @xmath67 , and so - on up to @xmath68-[8.0])}$ ] .",
    "we also include the single @xmath69 and @xmath14 magnitudes as monochromatic flux measurements , and finally the spectroscopic redshift from @xmath42cosmos . in total",
    ", each training vector contains 14 elements .",
    "after training all soms in the committee , we test the predictive power of the som ensemble using the half of the catalogue that did not participate in the training , calculating the bmu for each object using sub - sets of the photometry ( e.g.  just @xmath65 , then adding @xmath66 , @xmath67 , and so - on until we include all photometry weights up to the irac bands ) . in this case , the spectroscopic redshift component of the weight is not considered when calculating the bmu . in each trial , the redshift weight tagged to the bmus provides the ` photometric ' redshift , and these are averaged over the committee to give the final prediction .",
    "as we know what the true redshift of each test galaxy is , we can assess the accuracy of the method .",
    "we define the figure of merit for the photometric redshift accuracy in the usual way as the root mean square of the difference between the true and estimated redshift @xmath70 , where @xmath71 .",
    "figures5 and 6 shows the results , where we have interrogated the committee of ten soms for the photometric redshift of a test galaxy with increasingly complete sub - sets the full range of photometry . there is a clear decline in @xmath72 as more photometric information information is added , asymptoting at @xmath73 .",
    "surprising accuracy can be achieved with a rather sparsely sampled input vector , however in these cases one can clearly see the bias described above that results in the overestimation of redshifts for galaxies at @xmath74 and vice versa . where shown , the error bars are the standard deviation of redshifts recovered from the ten soms .",
    "this is certainly an underestimation of the true error ; one could also incorporate the formal photometric uncertainties by running the som interrogation several times and allowing each photometry value to randomly scatter about its mean according to its 1@xmath16 measured uncertainty . in this example",
    ", large error bars simply reflect cases where galaxies with similar characteristics were poorly represented in the training set , and thus are scattered between dissimilar bmus in each committee member .    using the @xmath69-band to @xmath48-band photometry",
    ", we can achieve @xmath75 after rejecting @xmath02% @xmath46@xmath76 outliers . including the irac bands",
    "does not significantly improve the accuracy , despite the fact they were included in the training : @xmath72 no longer improves after the 9th parameter ( @xmath77 ) is added .",
    "this reflects the fact that for @xmath78 it is the @xmath79@xmath4 m photometry that ` carries ' most of the information required for the photometric redshift ( as expected ; the 4000  and balmer breaks are still blueward of the @xmath80-band at @xmath78 , and the 1.6@xmath4 m stellar bump , another good redshift discriminant is just redward of @xmath48 ) .",
    "this accuracy is comparable to , or rivals , that which can be achieved with traditional spectral template fitting techniques .",
    "pertinent to this data - set , mobasher et al .",
    "( 2007 ) achieved @xmath81 with a template fitting technique to 16 photometric bands in the cosmos field .",
    "this was found to be in good agreement with photo-_z_s derived from the independent codes le phare ( arnouts et al .",
    "1999 ) , bpz ( benitez  2000 ) and zebra ( feldmann et al .",
    "several of these methods use bayesian inference to derive photometric redshifts .",
    "it should be noted that more recently ilbert et al.(2009 ) achieved much higher photometric redshift accuracies @xmath82 in the cosmos field using the le phare code ( s. arnouts & o. ilbert ) with 30 broad- , medium- and narrow - bands for template fitting .",
    "given the improvement seen in the som photo-_z _ technique when more photometric bands are introduced , we would anticipate an improvement in our reported accuracy if we re - trained the som with a similar large number of bands .",
    "one of the main benefits of the som technique , aside from the non - reliance on assumptions of spectral properties , is the speed at which photometric redshifts can be calculated once training has completed . the time to calculate",
    "the photometric redshift and error is simply the computational time to query each som to find the bmu ",
    "less than a few hundredths of a second per galaxy on a typical modern desktop machine .",
    "hildebrandt et al .  ( 2010 ) present a system for the consistent testing of different photo-_z _ codes : ` phat : photo-_z _ accuracy testing ' .",
    "phat provides a standard mock catalogue containing galaxies represented by the empirical spectral energy distribution templates of coleman ,  wu & weedman(1980 ) and kinney et al .",
    "( 1996 ) , together covering the full range of galaxy spectral types from passive ellipticals to starburst systems .",
    "synthetic colour information for each galaxy is calculated for each template for photometric bands spanning the ultraviolet to mid - infrared , specifically : the canada - france - hawaii telescope megacam _ ugriz_-bands , the united kingdom infrared telescope _ yjhk_-bands and the 3.6@xmath4 m and 4.5@xmath4 m _ spitzer _ irac bands .    as ours is an empirical method and",
    "requires a training set where the redshift is known , we use the ` large ' phat catalogue of 170000 objects with noise included ( where a parametric model for the signal - to - noise ratio as a function of source flux is used , and photometry perturbed accordingly according to a gaussian distribution ) .",
    "we create a training sub - set by randomly sampling 10% of the full catalogue . in this example , we initialise a @xmath83 som and set the number of iterations to oversample the training set by a factor of 5 .",
    "hildebrandt et al .  ( 2010 ) define the photo-_z _ accuracy figure of merit as the mean and scatter ( rms ) in @xmath84 , and the outlier rate as the fraction of objects with @xmath85 . for comparison with the results presented in hildebrandt et al .",
    "( 2010 ) for phat - testing of 16 recent photo-_z _ codes ( several of which are in widespread use ) , we calculate the same statistics on the predicted redshifts retrieved for the galaxies that did not participate in the training of our som .",
    "the best codes tested by hildebrandt et al .  ( 2010 ) typically have @xmath86 , scatters of @xmath870.010.02 and small outlier rates of @xmath88% .",
    "testing the trained som on a sub - sample of 100000 galaxies from the large catalogue that did not participate in training we find an average @xmath89 , @xmath90 and outlier rate of 0.13% .",
    "the relatively large outlier rate ( compared to some of the codes tested in hildebrandt et al .",
    "2010 ) is driven by the poorer accuracy at the tails of the redshift distribution , which is a natural bias in this method . when considering only galaxies in the range @xmath91 , although the rms accuracy is the same , the outlier rate drops to 0.06% .",
    "thus , the empirical som method for photo-_z _ prediction is competitive with established photo-_z _ codes .",
    "it is likely that the accuracy could be improved further by using an even larger training sample with a longer learning period , at the expense of computational time .      aside from the limitations discussed above regarding the choice training set , and the natural biases that are encoded into the som",
    ", there are several other important issues to consider , and we briefly review these here .",
    "the rate of learning , or how quickly the som adapts during training , is set by ( a ) two learning coefficients ( equation 1 and 4 ) which vary as a function of node distance and learning time ; ( b ) the rate of decay of these coefficients ; ( c ) the shape of and rate of decay of the region of influence around the bmu where neighbouring nodes are allowed to change ; ( d ) the size , or resolution of the som , and ( e ) the total learning time .",
    "it is not clear what the optimum combination of these factors is that would produce the best mapping is , and it would take a long time to do so .",
    "so , the exact choice of training parameters might be the main limiting factor in the som technique .",
    "however , during the course of this investigation , we have found some simple configurations that appear to produce robust results .",
    "first , the number of nodes in the som should be initialized such that the total number scales roughly with the number of training parameters , @xmath0@xmath92 , and a good minimum is @xmath0400 nodes arranged in a @xmath35 grid .",
    "this is to allow the mapping to ` resolve ' possible correlations and clustering between several parameters .",
    "clearly , when making predictions for new test data , the size of the som sets a fundamental limit on the accuracy , as the total input parameter space is discretised into a finite number of bins . in the case of this photometric redshift example , we set the total number of nodes to be @xmath93 , and this seems adequate to make accurate predictions whilst keeping down training time .",
    "in the example of object selection however , we were more interested in training the som to make selections of objects in rather broad swathes of the parameter space , and so in this case a som with fewer nodes was successful ( and is also beneficial for visualisation purposes ) .",
    "note that there are variant som algorithms that allow the number of nodes in the map to be dynamic , growing according to the need of the training sample ( alahakoon & halgamuge  1998 )    the total number of iterations was set to ten times the number of elements in the training set .",
    "this was to allow the som to see each training vector about ten times , and participate in the refining of the self - organisation at different stages in the learning process .",
    "we could envision even better results if we allowed more over - sampling of the training set , but this comes at the cost of longer training times .    finally , we initialised the learning co - efficients to unity , and set the size of the neighbourhood function to be approximately half the linear size of the som .",
    "this initial size allows test vectors selected at the start of the learning to influence large , unrefined sectors of the map .",
    "as iterations cumulate , new training vectors simply refine the map , contributing less drastic changes due to the decreasing size of the neighbourhood function and declining learning coefficients .",
    "we found that the shape of the neighbourhood function ( a gaussian ) , the rate of its decay ( the size decreases linearly with time ) , and the decay of the learning coefficients produced excellent results in multiple som realisations involving different types of data .",
    "again , a future study could investigate what the optimal learning parameters are , with the best results perhaps coming from a more extended committee of hundreds or thousands of soms ( that could be trained in parallel ) , each with different self - organisation styles and learning capabilities .",
    "self - organised maps ( soms ) are a class of neural network that employ unsupervised learning to map the topology of a multidimensional data set .",
    "this is a powerful method for exploring large catalogues of astronomical data ; the method can discover correlations between parameters , detect clustering within the parameter volume , and can be exploited to predict the parameters of new test data in a completely empirical way .",
    "here we have presented the som as a potential tool for current and future large astronomical surveys , highlighting two practical examples :    1 .",
    "selection of galaxies with active nuclei trained on _ spitzer _ irac colours ( 3.68@xmath4 m ) .",
    "the som trained on irac photometry naturally ` finds ' the characteristic colours of obscured agn , and the corresponding nodes can be used as a filter to select similar objects from a new data set .",
    "this filter can even be used where the information used in the training is incomplete , or unavailable .",
    "we demonstrate that the same som can be used to select known agn using just @xmath48 , [ 3.6 ] and [ 4.5 ] photometry .",
    "while we chose agn as a demonstrative example , soms could be used to select a wide range of astronomical objects , with the exciting possibility that self - organisation could discover ` new ' classifications in upcoming large data surveys . 2 .",
    "estimation of redshifts from broad - band photometry , trained using a deep spectroscopic survey : @xmath42cosmos .",
    "the accuracy of the redshift estimation defined by the r.m.s . in @xmath94 is @xmath2 , with a small outlier rate of @xmath02% , competitive with other established photo-@xmath42 codes using alternative techniques for deriving the redshift from photometry .",
    "we also test the som as a photo-_z _ tool using the photo-_z _ accuracy testing catalogue ( hildebrandt et al .",
    "2010 ) , which provides a much larger training set with model galaxies covering a range of spectral types , and 10 bands of broadband photometry .",
    "we find that the photo-_z _ accuracy of the som is competitive with many established photo-_z _ codes , delivering an rms in @xmath95 with a small outlier rate of 0.13% .",
    "+ accuracies could be significantly improved by training on a larger training sample , but other factors also affect performance , including the ` resolution ' of the som , the choice of learning coefficients , and so - on .",
    "although not without its limitations , which are discussed , the advantages of using a som for predicting photometric redshifts ( or any other parameter ) are ( a ) it is a completely empirical method ; ( b ) once training has completed , predictions can be achieved very quickly , since the only cost function that has to be evaluated is the location of the best matching node ( bmu ) for a new test galaxy .",
    "we have demonstrated two simple examples here , using one of the most basic som algorithms , but there are many practical applications beyond what has been presented .",
    "one could envision more extravagant training scenarios , applications for and adaptations to the algorithm that might prove fruitful . in conclusion",
    "however , we suggest that soms are versatile tools that could be used in data mining and visualisation applications for existing and up - coming large surveys , where efficient techniques will be required to fully harness the power of the exceptionally large and intertwined databases set to flood the community .",
    "j.e.g .  is supported by a banting postdoctoral fellowship , administered by the natural science and engineering research council of canada .",
    "we thank the anonymous referee for comments and suggestions that improved the paper , and kristen coppin , duncan farrah , ryan hickox and phil marshall for helpful comments and discussions .",
    "this research has made use of the nasa / ipac infrared science archive , which is operated by the jet propulsion laboratory , california institute of technology , under contract with the national aeronautics and space administration",
    "lp0.275    som & self - organised / organising map node & single ` neuron ' in som ; nodes are arranged on the surface of a 3d toroid , but visualised unravelled , in 2d training vector , @xmath96 & example of single data element from training set ( e.g.  galaxy photometry ) weight vector , @xmath97 & vector of identical size to @xmath98 attached to each node that competes to become more like the training vector bmu & best matching unit , is the ` winning node ' that is most like a randomly sampled training vector u - matrix & unified distance matrix ( udm : a method of visualising and detecting clustering in the map using the average distance to neighbouring nodes @xmath9 & radius of learning influence of bmu neighbourhood function & form of spatial learning function within @xmath9 component plane & 2d representation of the values of the @xmath99 element of the weight vector of nodes in the map learning rates & co - efficients determining the amount that weights can adapt to become more like training vectors ; these vary spatially ( relative to the bmu ) and temporally , tending toward zero over the duration of training over - sampling & number of times a given training vectors are ` seen ' by the som during learning committee & several soms trained independently on the same training set"
  ],
  "abstract_text": [
    "<S> we present an application of unsupervised machine learning  the self - organised map ( som )  as a tool for visualising , exploring and mining the catalogues of large astronomical surveys . </S>",
    "<S> self - organisation culminates in a low - resolution representation of the ` topology ' of a parameter volume , and this can be exploited in various ways pertinent to astronomy . using data from the cosmological evolution survey ( cosmos ) , we demonstrate two key astronomical applications of the som : ( i ) object classification and selection , using the example of galaxies with active galactic nuclei as a demonstration , and ( ii ) photometric redshift estimation , illustrating how soms can be used as totally empirical predictive tools . with a training set of @xmath03800 galaxies with @xmath1 </S>",
    "<S> , we achieve photometric redshift accuracies competitive with other ( mainly template fitting ) techniques that use a similar number of photometric bands ( @xmath2 with a @xmath02% outlier rate when using _ u@xmath3_-band to 8@xmath4 m photometry ) . </S>",
    "<S> we also test the som as a photo-_z _ tool using the photo-_z _ accuracy testing ( phat ) synthetic catalogue of hildebrandt et al .  </S>",
    "<S> ( 2010 ) , which compares several different photo-_z _ codes using a common input / training set . </S>",
    "<S> we find that the som can deliver accuracies that are competitive with many of the established template - fitting and empirical methods . </S>",
    "<S> this technique is not without clear limitations , which are discussed , but we suggest it could be a powerful tool in the era of extremely large  ` petabyte '  databases where efficient data - mining is a paramount concern .    </S>",
    "<S> [ firstpage ]    methods : data analysis , statistical , observational </S>"
  ]
}