{
  "article_text": [
    "consider an observable , discrete - time stochastic process @xmath0 , @xmath1 , a latent and unobserved discrete - time stochastic process @xmath2 , @xmath3 , a parameter @xmath4 and i.i.d .  sequence of random variables @xmath5 , @xmath6 , whose distribution can depend upon @xmath7 ( @xmath8 ) .",
    "this article is concerned with the class of statistical models , for @xmath9 , @xmath10 @xmath11 where @xmath12 , @xmath13 , @xmath14 is assumed null , which induces a joint lebesgue ( assumed for simplicty of presentation ) density of the model @xmath15 where we use @xmath16 to denote conditional and joint probability densities w.r.t .",
    "lebesgue measure and @xmath17 will denote the associated distribution .",
    "this collection of models is rather flexible and contains :    1 .   i.i.d .",
    "models , when @xmath18 is null and @xmath19 does not depend on @xmath20 2 .",
    "observation - driven time series models ( odts ) ( e.g.  @xcite ) , when @xmath18 is null 3 .",
    "hidden markov models ( hmms ) ( e.g.  @xcite ) , when @xmath18 follows a markov chain and @xmath19 does not depend on @xmath20 4 .   some non - linear time series models ( e.g.  @xcite ) , such as causal and invertible bilinear time series models @xcite ( when @xmath18 is null ) .",
    "the class of models is quite large and includes many popular classes including some garch models ( @xcite ) , stochastic volatility models ( e.g.  @xcite ) and partially observed markov jump process models ( e.g.  @xcite ) .",
    "the list of applications is numerous ranging from economics , biology and engineering ; the reader is referred to the afore mentioned references for specific applications and details .",
    "the particular scenario of interest in this article is when one can exactly sample the process @xmath5 and evaluate for @xmath9 , @xmath19 , but that the density function @xmath21 is unknown up - to a positive and unbiased estimate ( this can occur see @xcite ) ; we will call the likelihood intractable . if the density function @xmath21 is known up - to a positive and unbiased estimate and suppose that this estimate is @xmath22 for some random variables @xmath23 where @xmath24= \\prod_{i=1}^n p_{\\theta}(y_i|y_{1:i-1},x_i)$ ] ( the expectation is w.r.t .  the law of @xmath23 ) , then the approximation schemes we will discuss are not always needed as exact ( possibly monte carlo - based ) inference procedures are possible . later in the article",
    ", we will consider cases when @xmath25 is intractable in some way , but for now , we will assume , when it is in the model , that this density is known point - wise up - to a constant .",
    "we also remark , in section [ sec : approx ] , we will show that if one can evaluate the density of the noise terms @xmath26 then , indeed one does not require the ability to sample the @xmath27 .",
    "for the class of problems of interest , for either likelihood - based or bayesian inference methods , the complexity of the model is such that even when using advanced methods such markov chain monte carlo ( mcmc ) or sequential monte carlo ( smc ) exact statistical inference is seldom possible .",
    "one of the standard solutions to this problem is to introduce an approximation of the statistical model and an often adopted approach is that of using approximate bayesian computation , especially when taking a bayesian perspective to statistical estimation ; see for instance @xcite for a recent overview .    before continuing",
    ", it is noted that the scenario of intractable likelihoods will occur in many applications .",
    "perhaps the most common is when @xmath5 is distributed according to a model which can be simulated , but @xmath28 is not known ; a good example is the stable distribution - this would find applications for i.i.d .",
    "models and hmms ( see @xcite ) applied in finanical contexts where the heavy tails of these distributions are a realistic modelling choice .",
    "other models used in practice include the lotka - voltera model @xcite used for stochastic - kinetic networks and the @xmath29 distribution used for ` non - standard ' data @xcite .",
    "suppose that one places a prior density @xmath30 on @xmath31 , the standard abc approximation ( see e.g.  @xcite ) of the posterior associated to the likelihood in : @xmath32 is to take , for @xmath33 : @xmath34 where @xmath35 are _ auxiliary _ data , @xmath36 is as , @xmath37 is a _ summary _ of the data , @xmath38 is _ distance _ on the summary statistics .",
    "if the summary statistics are sufficient for the data , then one can show ( under minimal assumptions ) that as @xmath39 that the ( marginal ) abc posterior @xmath40 is in some sense exactly @xmath41 . it is remarked that if @xmath42 is countably infinite ( we assume this is not the case ) , then it is possible to take @xmath43 and if @xmath44 is sufficient , this leads to _ exact _ inference as proposed in @xcite .",
    "we note that the indicator function is used in , but it can be replaced with a kernel density ; we focus upon indicator functions in this article .",
    "the approximation is particularly amenable to inference , in that , in the framework of this article one can sample @xmath45 exactly from the true model ; this fact is very useful in that it would allow one to apply a number of computational algorithms such as rejection sampling , importance sampling , mcmc or smc ( this explained in detail later in the article ) .",
    "the problems with this approximation include :    * if the statistics @xmath46 are not sufficient , one does not always recover the exact posterior , even if @xmath43 .",
    "this can make it difficult to characterize , mathematically , the bias in approximation . selecting summary statistics",
    "can then be challenging , although , there are approaches such as @xcite . * in many practical models , there is a trade - off between making @xmath47 small ( accuracy of the approximation ) and allowing a computational method to work well ( accuracy in computation ) .",
    "the approximation does not often help this former trade - off due to the factor @xmath48 , which may mean that @xmath47 has to be relatively large for a computational algorithm to produce reliable results .",
    "for example in a rejection algorithm which samples @xmath49 from @xmath50 the acceptance probability is @xmath48 ; if @xmath51 is large one may need to make @xmath47 quite large to yield reasonable acceptance rates .",
    "* in the context of this article , does not retain the probabilistic structure of the model .",
    "for example one does not have @xmath52 for some probability density @xmath53 . the issue with this",
    "is that the techniques of inference for the original model ( e.g.  for hmms one often uses smc methods ) can not be used , without modification , which creates an additional difficulty .",
    "another issue is that a theoretical study of the abc approximation may be far more difficult than is the case when the probabilistic structure of the model is retained .",
    "although there are several results in the literature ( e.g.  @xcite ) one could argue that a more precise analysis could be undertaken by retaining the structure of the model .",
    "the points here have been made in many other articles including @xcite .",
    "thus our objective is to use a different abc approximation which , to an extent , can deal with some of the deficiencies raised here .",
    "we note that there are a great deal of extensions of the standard abc approach ( such as regression adjustment @xcite ) , but the argument made in this article , is that the approximation to be reviewed and developed here , is very reasonable , especially for the case when @xmath54 is low to moderate .",
    "the abc - based approximation we consider is as follows , which can deal with some of the issues outlined above @xmath55 where @xmath56 and @xmath57 typically , one needs to be able to evaluate the abc posterior pointwise up - to a normalizing constant ( for example for monte carlo algorithms ) which is often not the case for @xmath40 .",
    "_ one particular choice _ is to define an abc posterior on an extended state - space from @xmath58 to @xmath59 , setting @xmath60 the probabilistic structure in is retained in .",
    "we note that the issue in the second bullet point for problems of standard abc approximations is dealt with to an extent ; it breaks the global dependence ( on the data i.e.  the factor @xmath48 ) into a collection of smaller and possibly easier to handle ( from a computational perspective ) sub - problems ( i.e.  the factors @xmath61 ) .",
    "this type of approximation has been studied in various special cases in @xcite , the actual intepretation will allow one , as we will explain later on in this article , to perform asymptotically in @xmath51 ( for some classes of models and under some assumptions and modifications ) consistent parameter estimation when taking a likelihood based ( i.e.  trying to optimize @xmath62 $ ] ) or bayesian approach to this task .",
    "the approximation does not use any summary statistics ; this is only an option if @xmath54 is small to moderate - this latter scenario is sufficiently rich in that one does not always attempt to fit the original model ( i.e.  approximation is not needed ) if this were not the case .",
    "this article will consider the abc approximation for a variety of special cases ( that is , the models 1.-3 .  mentioned above ) .",
    "we will review and discuss both the inferential aspects associated to this approximation ( such as consistency in parameter estimation , bias and so fourth ) and the computational methods that can be used to fit the models .",
    "several original remarks are made along the way .",
    "it is remarked that a comprehensive comparison of ( some ) computational methods for abc approximations posteriors based upon and those based upon can be found in @xcite .",
    "the article is structured as follows . in section [ sec : rem_abc ] we investigate our abc approximation in more details , discuss the notion of abc and some alternative approximation schemes . in section [ sec : models ] we consider three classes of models which include i.i.d .",
    "models , odts models and hmms .",
    "we discuss the abc approximation and adjustments which can allow one to perform asymptotically consistent parameter estimation . in section [ sec : comp ]",
    "we give a variety of computational methods which can be used to fit the particular abc approximations discussed in section [ sec : models ] .",
    "we mainly consider smc and mcmc methods , of which we assume the reader has some familiarity .",
    "other approaches such as expectation - propagation are also considered . in section [ sec : summary ] the article is concluded and future research work is discussed .",
    "the appendix houses some proofs and assumptions of some propositions which are given in the article .",
    "we begin our review by making some remarks on the approximation in .",
    "we first show , using arguments from @xcite that the approximation bias will disappear as @xmath63 . throughout @xmath64",
    "is the euclidean distance ; it does not appear that in practice the selection of the distance criterion makes a significant difference .",
    "we have the following result for any bounded , measurable real - valued function @xmath65 . the assumption ( a[hyp:1 ] ) along with the proof is in appendix [ sec : prfs ] .",
    "[ prop : ep_conv ] assume ( a[hyp:1 ] ) .",
    "then for almost every @xmath66 @xmath67    the result is fairly standard in the literature and is included for completeness of the article as well as for pedagogical purposes .",
    "essentially it tells us , at the level of approximation , but before any fitting is considered , that one would like to make @xmath47 as small as possible .    the extended abc posterior , to be defined below , is needed for computational algorithms , as one does not know @xmath40 point - wise up - to a normalizing constant ; this is the main requirement of most monte carlo based simulation methods , which are often needed to draw inference from the abc posterior .",
    "an important remark concerning the extended abc posterior @xmath68 where @xmath69 is as equation is not the only choice which yields @xmath40 as a marginal where @xmath70 one possible alternative is the density @xmath71 which we term the _ collapsed _ representation of the model , paralleling this idea for hmms ( see @xcite ) .",
    "the subsequent abc posterior is @xmath72 this type of decomposition , as discussed in both @xcite in the context of abc is particularly interesting , indeed , if @xmath28 is known point - wise up - to a constant , then ( as remarked by @xcite ) _ one no longer requires the ability to sample the data _ , that is , to sample the @xmath5 .",
    "this is because , assuming one can evaluate the prior @xmath73 , the abc posterior can be evaluated point - wise and up - to a normalizing constant .",
    "it is remarked that the collapsed representation might be bourne out of _ inferential _ considerations , such as inability to sample the data .",
    "the collapsed representation will be particularly useful for hmms as we will discuss in section [ sec : hmms ] .",
    "in section [ sec : comp ] we will see different extended abc posteriors that can yield computational algorithms which are more efficient ( in some sense ) than if one considered @xmath74 .",
    "it should be noted that this is a _ computational _ consideration and not an inferential one ; that is computational improvements can be yielded and not necessarily inferential ones . at this stage , we do not dwell on these points and give extended abc posteriors @xmath74 ( or the collapsed version ) in our subsequent discussions ( up - to section [ sec : comp ] ) .",
    "the reader , should , however , keep these ideas in mind .",
    "an interesting idea , developed in @xcite is the notion of _ noisy _ abc , a term apparently coined in @xcite , see also @xcite .",
    "the idea , which follows ( * ? ? ?",
    "* theorem 1 ) , is this ; the model which is to be fitted is : @xmath75 the idea is then if the data _ are from the original model _ i.e.  that in ( although this is not a pre - requisite of using the approach ) , then instead of using the original observations @xmath76 , one uses perturbed observations defined as , for @xmath77 @xmath78 where @xmath79 is the uniform distribution on a set @xmath80 .",
    "then , one fits the model @xmath81 using the techniques that were to be adopted when using the original data .",
    "the intuition is that the perturbed observations are now from the model that is actually being fitted and one hopes for favourable statistical properties of estimates of @xmath7 and the associated posterior ( as well as the @xmath18 if they are of interest ) .    from a mathematical perspective (",
    "* theorem 2 ) show that noisy abc is calibrated , that is , roughly , that the noisy abc posterior has good properties when the number of data are large ( although as pointed out by @xcite , there appear to be some missing technicalities in their result ) .",
    "similar results are derived in @xcite for hmms except when concerning both bayesian and classical estimation , that is computing @xmath82 in particular consistency and asymptotic normality properties are investigated .",
    "the main point is that ( for hmms and under assumptions ) the noisy abc mle in is consistent ( that is , returns the true parameter that generated the data , @xmath83 say ) as @xmath84 , whatever @xmath33 is chosen .",
    "we will investigate this idea in more details in the subsequent sections of the paper .      at its core ,",
    "abc is simply a way to approximate the posterior density and one may question why to choose abc versus many other alternatives , such as indirect inference @xcite .",
    "the purpose of this article is not to go into comparisons between various approximation schemes ( that has been done in many articles such as @xcite ) , but we make some mention of alternatives here . perhaps the main competitor to abc is indirect inference ; ( * ? ? ?",
    "* theorem 5 ) indicates that in some scenarios abc can be more accurate as @xmath63 .",
    "in the context of hmms there are a variety of alternatives such as nonparametric particle filtering @xcite ; a comparison with abc is discussed in @xcite .",
    "for hmms there is also a link between abc and the ensemble kalman filter ( e.g.  @xcite ) see @xcite , which is an approximation of the filtering density of an hmm ( this is discussed later on ) : a direct comparison of these ideas does not appear to be in the literature .",
    "an approach proposed by for cases where the observation model is ` unknown ' is developed in @xcite , but these scenario are even more complex than encountered in this article .    some ideas which are based upon abc , but different to those considered in this article can be found in @xcite . in @xcite",
    "the authors form a kernel density estimate of the abc likelihood from @xmath7 samples drawn from the abc posterior distribution of @xmath7 .",
    "they then maximise this kernel density estimate as an approximation to mle . in @xcite , they use an abc approximation based upon ( except with a non - negative kernel ) , and use a similar idea to @xcite in building a kernel density estimate except for the posterior on @xmath7 .",
    "one technique found in the data assimilation literature , termed randomized maximum likelihood ( e.g.  @xcite ) , involves repeated simulations from the prior and likelihood , to obtain estimators of parameters and is , to an extent , related to abc ( although is not necessarily adopted when the likelihood is intractable ) . to our knowledge , this approach has not been systematically understood , nor compared to abc .",
    "in the context that there is no latent process @xmath85 and @xmath19 does not depend upon @xmath86 one has the following version of the abc approximation in @xmath87 where @xmath88 yielding the extended abc posterior : @xmath89 one can also use collapsed versions as in section [ sec : approx ] and replace the @xmath76 with @xmath90 as in section [ sec : noisy_abc ] . approximations of these type have been used in @xcite and include @xmath91stable models @xcite where the observations possess an @xmath91stable distribution and the @xmath29 distribution . in the former model",
    "@xcite show that it is difficult in the abc approximation to select summary statistics ; the utility of the abc approximation based upon is thus evident , where this problem as been removed . in general",
    "this abc approximation is expected to be of interest if one can sample the @xmath5 but can not evaluate the associated density , or if one can not sample the @xmath5 but can evaluate the associated density ( for which the collapsed representation could be used ) .",
    "we now further discuss why this approximation might be useful in practice .",
    "one reason why the abc approximation based around for this and other models is quite reasonable is as follows .",
    "suppose that one is only interested in likelihood - based inference on @xmath7 ( we discuss bayesian inference below ) , then one would seek to find the mle : @xmath92 as mentioned above , one could also consider the noisy abc mle : @xmath93 now suppose that the data @xmath94 are distributed according to the true model i.e.  @xmath95 , @xmath96 .",
    "also define @xmath97 write @xmath98 to denote almost sure convergence as @xmath84 .",
    "then we have the following result for i.i.d .",
    "models that mirrors results in @xcite and whose proof and assumptions can be found in appendix [ sec : prfs_abc ] .",
    "[ prop : abc_ok ] assume ( a[hyp:2 ] ) .",
    "then for any @xmath33    * @xmath99 * @xmath100 .",
    "the result shows that the mle associated to standard abc will converge to a point @xmath101 which _ may _ be different than the true parameter ; to show that it is always different requires some work , see for instance ( * ? ? ?",
    "* theorem 1 ) for hmms .",
    "converesely the noisy abc mle will return the _ true _ parameter as the number of data grow .",
    "there are a number of issues to be discussed about results of this type .",
    "firstly , the result is assuming that the data are being generated from the model under study , which some investigators may not be prepared to assume ; in such scenarios one would then prefer to use abc against noisy abc as one relies on the true data instead of perturbing it ( see also @xcite for some discussion ) .",
    "secondly , the result shows that whilst the noisy abc estimate will , for a large number of data , give the true parameter , it does not characterize whether there is any loss in efficiency in using noisy abc versus the true mle ( which one can not obtain as the likelihood is intractable ) .",
    "this can be achieved by considering asymptotic normality and the fisher information matrix ; we direct the reader towards @xcite where in the context of hmms it is shown that there is a loss of efficiency and its dependence upon @xmath47 is characterized .",
    "thirdly , the result does not take into account that one can seldom compute the estimators analytically ; so the importance of these points may be reduced .",
    "finally , the result does not say anything about the abc posterior .",
    "the result is easily extended to the map estimator , if the prior density on @xmath7 is bounded away from zero ( which could happen , as we have assumed @xmath102 is compact ) .",
    "more generally one may want to consider bayesian consistency and bernstein - von mises theorems ; we direct the reader to @xcite for more discussion on this issue .",
    "the main point to take on board from proposition [ prop : abc_ok ] is that the abc approximation is quite reasonable , in that very sensible parameter estimates can be yielded .",
    "inspection of the proofs help to support some of the points made in the introduction ; the removal of summary statistics combined with retaining the probabilistic structure of the likelihood has allowed one to discover some of the underlying properties of the abc approximation",
    ". one could extend proposition [ prop : abc_ok ] to include summary statistics as done in @xcite .    to illustrate the point in proposition [ prop : abc_ok ] , we provide a numerical example that considers the model : @xmath103 , where @xmath104 where @xmath105 is the standard normal distribution in one - dimension .",
    "we assume that _ a priori _ @xmath106 .",
    "we generated two data - sets of size 100 and 1000 from the model ( when @xmath107 ) and whilst abc is not needed here ( the posterior is known analytically ) we fit a standard and noisy abc approximation to the data .",
    "we run algorithm [ alg : new ] ( in section [ sec : comp ] ) and plot a density estimate of the samples in figure [ fig : fig1 ] .",
    "the plot shows , reasonably clearly that the map estimator for noisy abc is closer to that of the true parameter ( which is zero ) as @xmath51 grows , than that of the standard abc map .        the model can be described as follows .",
    "we observe @xmath108 , which are associated to an unobserved process @xmath109 which is potentially unknown .",
    "define the process @xmath110 ( with @xmath111 some arbitrary point on @xmath42 ) and @xmath112 to be introduced below .",
    "denote by @xmath113 as the information in @xmath114 up - to time @xmath115 .",
    "the model is defined as , for @xmath116 ( using @xmath117 to denote probability ) @xmath118 where @xmath119 , @xmath120 and @xmath121 is a probability density on @xmath122 for every @xmath123 .",
    "next , we define a prior probability density @xmath124 and write @xmath125 assumed to be a proper joint probability density on @xmath126 . thus , given @xmath51 observations the object of inference is the posterior distribution on @xmath127 : @xmath128 where we have used the notation for @xmath129 , @xmath130 , @xmath131 , @xmath132 .",
    "note that @xmath133 is a lebesgue density assumed to be induced by the relationship @xmath134 for @xmath5 and i.i.d",
    ".  sequence , and whilst @xmath135 is a latent process one can consider @xmath136 , @xmath137 , so that the model falls into the framework of this article . in most applications of practical interest , the posterior can not be computed point - wise and one has to resort to numerical methods , such as mcmc , to draw inference on @xmath7 and/or @xmath138 .",
    "the models which lie under this class are some garch models and some bayesian inverse problems ( see e.g.  @xcite and the references therein ) .",
    "the extended abc approximation for this model is @xmath139 which is not dissimilar to the i.i.d",
    "indeed , one can use similar computational methods to the i.i.d .",
    "case to fit the model .",
    "it is also remarked that one can introduce a collapsed representation as well as noisy abc for this class of models .",
    "the abc approximation for odts models has a similar structure to the i.i.d .  model .",
    "however , to prove results of the type in proposition [ prop : abc_ok ] one has to appeal to the consistency theory for such models which can be found in @xcite .",
    "this is done in @xcite , but under very strong assumptions which imply that @xmath140 are compact state - spaces .",
    "indeed , there are numerous results which could be proved ( but do not appear to be ) for abc approximations of such classes of models , including bayesian consistency and bernstein von - mises theorems .",
    "hidden markov models are widely used in statistics ; see @xcite for a recent overview .",
    "an hmm  is a pair of discrete - time stochastic processes , @xmath141 and @xmath142 .",
    "the hidden process @xmath143 is a  markov chain with @xmath138 a given point and transition density @xmath144 , i.e @xmath145 where @xmath146 .",
    "the observation @xmath147 is assumed to be conditionally independent of other variables given @xmath148 and its conditional distribution is specified by a density @xmath149 @xmath150 with @xmath151 . when the parameters of the model are fixed , particular interest is in the filtering density @xmath152 and joint smoothing density @xmath153 we note that in general , expectations w.r.t .",
    "the associated distributions are not known analytically ( with the exception of a few special cases ) , even if @xmath154 can be evaluated .",
    "typically smc methods are used to approximate these latter distributions .",
    "the extended abc approximation , which , to our knowledge first appeared in @xcite ( when @xmath7 is fixed ) , is @xmath155 it should be noted that the densities in and can be time - inhomogeneous , but we have adopted a time - homogeneous representation for simplicity of notation .",
    "the representation has been used in many articles ; a non - exhaustive list is @xcite .",
    "a collapsed representation may be adopted and is discussed further in section [ sec : collapse ] and , as before one can easily adopt a noisy abc approach .",
    "a variety of models fall into the class of hmms , including stochastic volatility models .",
    "the abc approximation of hmms appears to be the most extensively studied , of all the approximations considered in this article .",
    "some theoretical results are as follows .",
    "when one considers @xmath7 fixed and uses the original data ( i.e.  standard abc ) @xcite show under strong assumptions that for a bounded measurable @xmath156 @xmath157 dx_n\\big| \\leq c\\epsilon\\sup_{x\\in\\mathsf{x}}|\\xi(x)|\\ ] ] where @xmath158 does not depend on @xmath51 .",
    "that is to say , that the bias of the abc approximation of the filter does not necessarily increase with @xmath51 and can be controlled by making @xmath47 arbitrarily small .",
    "similarly @xcite show under assumptions that for @xmath159 bounded , measurable with @xmath160 @xmath161 dx_{1:n}\\big| \\leq cn\\epsilon\\max_i\\big(\\sup_{x\\in\\mathsf{x}}|\\xi_i(x)|)\\big)\\ ] ] where @xmath158 does not depend on @xmath51 , which says that the bias of the abc approximation of the expectation of additive functions w.r.t .",
    "the smoothing distribution grows at most linearly with respect to @xmath51 and could be controlled by making @xmath47 suitably small .",
    "results of this type are of particular interest in smoothing ; see e.g.  @xcite for further details .",
    "additional results associated to the abc bias of the filter derivative , that is @xmath162 dx_n]|$ ] can be found in @xcite ; these latter results can be used in the proofs of the results to be discussed below , or of independent interest of themselves ( see e.g.  @xcite why this is the case ) .    more interesting , perhaps are results regarding parameter estimation . in the context of proposition [ prop : abc_ok ] , @xcite have shown ( under assumptions ) that the mle associated to standard abc ( i.e.  @xmath163 $ ] ) has an intrinsic bias as @xmath51 grows , indeed they find the bias as a function of @xmath47 .",
    "this result is more precise than that provided in proposition [ prop : abc_ok ] which does not show that the abc mle is necessarily different from the true parameter and does not characterize the asymptotic bias .",
    "@xcite then show that the noisy abc mle ( i.e.  @xmath164 $ ] ) is asymptotically consistent and then give an asymptotic normality result @xmath165 where @xmath166 denotes convergence in distribution as @xmath51 grows and @xmath167 denotes the @xmath168dimensional normal distribution with zero mean vector and covariance matrix @xmath169 .",
    "the authors go even further , in that they compare @xmath169 with the fisher information matrix associated to the mle ; the loss of efficiency ( the price to pay for using an abc approximation ) is given as a function of @xmath47 .",
    "these results are further refined in @xcite and bayesian consistency and bernstein von - mises theorems are considered for the the noisy abc posterior .      the collapsed representation ( as discussed in section [ sec : approx ] )",
    "is particularly interesting for hmms .",
    "a version can be found in @xcite , but we will present something slightly different , which allows one to deal with scenarios where the transition density of the hidden markov chain @xmath170 is also intractable .",
    "we will suppose that the hidden markov chain satisfies @xmath171 where @xmath172 is an i.i.d .",
    "sequence of random variables @xmath173 and @xmath174 .",
    "not all hidden markov chains will fall in this class , but it is sufficiently large to be of practical interest .",
    "then , one may adopt the following abc approximation @xmath175 where we have that @xmath176 .",
    "the key point here is that one can then estimate so - called doubly intractable hmms if either @xmath177 are known or one can sample them both ( or a combination of these ) .",
    "this idea can be found in @xcite and relies upon the approach in @xcite .",
    "we will now describe a collection of computational methods which can be used to fit the abc approximations considered in the previous section .",
    "we note that the computation of odts models is much the same as for i.i.d .",
    "models and thus we only consider i.i.d",
    ".  models .",
    "the reader is reminded that some familiarity with smc and mcmc methods is assumed ; complete introductions can be found in @xcite .",
    "we also remark that at this stage , at the level of abc approximations , one would like to make @xmath47 as close as possible to zero ; we shall see ( and as discussed for example in @xcite ) that there is a trade - off in making @xmath47 small and allowing the computational method to provide reliable results .",
    "one point is that the computational methods are designed for the scenario where an indicator function is used in the abc approximation .",
    "some mention of what can be done when non - negative kernels are used instead will be given .",
    "note also , that all the algorithms are given when using the observed data @xmath76 ; the approach for noisy abc is exactly the same except one replaces @xmath76 with @xmath90",
    ".      we will now consider sampling from @xmath178 , @xmath179 , where @xmath180 is given in .",
    "an mcmc algorithm , is given in algorithm [ alg : mcmc ] .",
    "in general this approach is not feasible because one does not know the term @xmath181 .",
    "the idea is to advocate algorithms by constructing targets on an auxiliary state - space ( such as ) which can efficiently approximate this marginal mcmc approach ; this is the idea of the psuedo marginal @xcite .",
    "the point is to construct mcmc algorithms which can replicate the properties of the ( hopefully ) efficient marginal mcmc algorithm .",
    "note that the following ideas can be easily extended to the collapsed representation discussed in section [ sec : approx ] .",
    "also note that some of this discussion can be found in @xcite , where even more details are given .    1 .   * ( initialisation ) * at @xmath182 sample @xmath183 from the prior .",
    "2 .   * ( m - h kernel ) * for @xmath184 : * sample @xmath185 from a proposal @xmath186 with density @xmath187 . *",
    "accept the proposed state and set @xmath188 with probability @xmath189 otherwise set @xmath190 .",
    "set @xmath191 and return to the start of 2 .      initially consider the abc approximation when extended to the space @xmath192 : @xmath193 in algorithm [ alg : simple ] we present a natural metropolis - hastings ( m - h ) proposal ( which is essentially the mcmc method of @xcite ) that could be used to sample from @xmath194 .",
    "the one - step transition kernel of the mcmc chain is usually described as the _ m - h kernel _ and follows from step 2 in algorithm [ alg : mcmc ] .",
    "it should be noted that one can implement ` early rejection ' ( e.g.  @xcite ) , in that if for some @xmath195 @xmath196 then the proposal can be terminated , as it will be rejected .",
    "what is hopefully apparent from algorithm [ alg : simple ] is that , interpreting the hastings ratio as an importance weight , the variance of this weight will play a critical role in the efficiency of the algorithm .",
    "that is , in algorithm [ alg : simple ] the hastings ratio is @xmath197 which is of a similar form to the hastings ratio in algorithm [ alg : mcmc ] , except it replaces @xmath198 with estimates @xmath199 if one supposes that algorithm [ alg : mcmc ] is quite efficient , then one would hope to replicate similar properties , which will occur if the variability of is not too large , or that the individual estimates ( in the ratio ) are quite well behaved .",
    "these individual estimates in the ratio are ( up - to a constant ) unbiased estimates ( expectation w.r.t .",
    "@xmath200 ) of @xmath181 and it is this property which allows algorithm [ alg : simple ] to provide @xmath7 samples from the marginal distribution of interest .",
    "as we have remarked previously the choice @xmath194 is one amongst potentially many densities which will give our abc posterior as a marginal .",
    "that is , if one can find a different extended target , so that the variance ( say for fixed @xmath7 - although that is another aspect of importance ) of the estimate of @xmath181 is smaller , then one might expect that the associated mcmc algorithm will mix better .",
    "as noted , for example in @xcite , as @xmath51 increases , the m - h kernel in algorithm [ alg : simple ] will have an acceptance probability that falls quickly with @xmath51 .",
    "in particular , for any fixed @xmath201 , the probability of obtaining a sample with a non - zero acceptance probability will fall at an exponential rate in @xmath51 .",
    "this means that this basic abc mcmc approach will be inefficient for moderate values of @xmath51 .    * sample @xmath202 from a proposal @xmath203 with density @xmath204 . * sample @xmath205 from a distribution with joint density @xmath206",
    "* accept the proposed state @xmath207 with probability : @xmath208      the above issue can be dealt with by using @xmath209 multiple trials ( see for instance @xcite in a smc context ) , so that for each data point the chance of getting an auxiliary data point in @xmath210 is larger .",
    "this approach augments the posterior to a larger state - space , @xmath211 , in order to target the following density : @xmath212 one can show that the marginal of interest @xmath213 is preserved , i.e. @xmath214 in algorithm [ alg : ntry ] we present an m - h kernel with invariant density @xmath215 .",
    "it is expected , especially with @xmath209 large , that this algorithm should be more efficient than in algorithm [ alg : simple ] ( which is the case @xmath216 ) .",
    "this is because as @xmath209 grows one expects @xmath217 to be close to @xmath181 and hence the marginal algorithm described at the beginning of section [ sec : mcmc_for_iid ]",
    ". it is noted in @xcite that to ensure that the ( relative ) variance ( w.r.t .",
    "@xmath218 ) of the estimate of @xmath181 does not depend upon @xmath51 ( possibly the major contributor to the variance is @xmath51 growing ) , one should set @xmath219 .",
    "it has been shown in @xcite that even the m - h kernel in algorithm [ alg : ntry ] does not always perform well .",
    "it can happen that the chain gets often stuck in regions of the state - space @xmath102 where @xmath220 is small .",
    "note that the @xmath221 could be larger by making @xmath47 larger , but this is at the cost that the abc approximation is worse ; before resorting to this , one might try to improve upon the algorithm . given this notation",
    ", we remark that @xmath222    * sample @xmath202 from a proposal @xmath203 with density @xmath204 . *",
    "sample @xmath223 from a distribution with joint density @xmath224 . *",
    "accept the proposed state @xmath225 with probability : @xmath226      the following mcmc kernel is developed in @xcite and is based on the @xmath227hit kernel of @xcite , which was adapted to account for the data structure in the model .",
    "one of the problems of the mcmc kernel in algorithm [ alg : ntry ] is that when @xmath221 is small , the algorithm gets stuck .",
    "the approach is that , when this occurs one tries to use more computational effort , without necessarily being more clever .",
    "the idea is that instead of sampling @xmath209 samples for each data point @xmath195 , the procedure samples data - points until there are @xmath209 points which lie in @xmath210 .",
    "@xcite show how this is possible by defining an appropriate target density , for @xmath228 , @xmath229 , @xmath230 : @xmath231 @xmath232 it is shown in @xcite that that the marginal w.r.t .",
    "@xmath7 is the one of interest : @xmath233    the mcmc kernel is given in algorithm [ alg : new ] .",
    "@xcite show that one should set @xmath219 to control the relative variance ( w.r.t .",
    "@xmath51 ) w.r.t .",
    "@xmath234 of the estimate of @xmath181 which is @xmath235    * sample @xmath202 from a proposal @xmath203 with density @xmath204 . * for @xmath236 repeat the following : sample @xmath237 with probability density @xmath238 until there are @xmath209 samples lying in @xmath210 ; the number of samples to achieve this ( including the successful trial ) is @xmath239 . *",
    "accept @xmath240 with probability : @xmath241      we will consider the following odts model , also discussed in @xcite",
    ". set , for @xmath242 @xmath243 where @xmath244 ( i.e.  a stable distribution , with location 0 , scale @xmath245 and asymmetry and skewness parameters @xmath246 ; see @xcite for more information ) .",
    "we set @xmath247 where @xmath248 is a gamma distribution with mean @xmath249 and @xmath250 .",
    "this is a garch(1,1 ) model with an _ intractable likelihood _ , i.e.  one can not perform exact parameter inference and has to resort to approximations .",
    "we will consider a comparison of algorithms [ alg : ntry ] and [ alg : new ] .",
    "and we will fit the model to data from the s&p 500 index from 03/1/11 to 14/02/13 ( 533 data points ) . in the priors",
    ", we set @xmath251 and @xmath252 , which are not overly informative .",
    "in addition , @xmath253 and @xmath254 ; see @xcite for explanations .",
    "we consider @xmath255 and only a noisy abc approximation of the model .",
    "the mcmc proposals on the parameters are normal random - walks on the log - scale and for both algorithms we set @xmath256 .    in figure",
    "[ fig : tracee05 ] ( a ) we present the autocorrelation plot of 50000 iterations of both mcmc kernels when @xmath257 ( for only the parameter @xmath258 - other parameters can be seen in @xcite ) . algorithm [ alg : ntry ] took about 0.30 seconds per iteration and algorithm [ alg : new ] took about 1.12 seconds per iteration .",
    "the plot shows that both algorithms appear to mix across the state - space in a very reasonable way .",
    "the mcmc procedure associated to algorithm [ alg : new ] takes much longer and in this situation does not appear to be required .    in figure",
    "[ fig : tracee05 ] ( b ) we can observe the autocorrelation plots from a particular ( typical ) run when @xmath259 . in this case , both algorithms are run for 200000 iterations .",
    "algorithm [ alg : ntry ] took about 0.28 seconds per iteration and algorithm [ alg : new ] took about 2.06 seconds per iteration . whilst the computational time for algorithm [ alg : new ] is considerably more than algorithm [ alg : ntry ] , in the same amount of computation time , it still moves more around the state space as suggested by figure [ fig : tracee05 ] ( b ) ; algorithm runs of the same length are provided for presentational purposes .",
    "we were unable to tune algorithm [ alg : ntry ] to make it mix well in this example and with many efforts .",
    "conversely , for algorithm [ alg : new ] we expended considerably less effort for very reasonable performance .",
    "the results here suggest that one should prefer algorithm [ alg : new ] only in challenging scenarios , as it can be very expensive in practice .",
    "if one chooses to use a non - negative kernel in the abc approximation , then some the issues discussed above will not be so relevant .",
    "in such scenarios one may attempt to use algorithm [ alg : simple ] replacing the indicators with non - negative kernels .",
    "if this does not work well , one can resort to more advanced methods such as those in @xcite .",
    "we first begin with smc algorithms which can approximate the abc filter , that is , when @xmath7 is fixed .",
    "as we shall see , these type of algorithms will form a building block of the mcmc algorithms to be presented in the next section .",
    "we first present a standard smc algorithm in algorithm [ alg : smc_abc ] , which can be found in , for example @xcite .",
    "the algorithm will allow one to approximate finite expecations w.r.t .  the abc filter with density @xmath260 via @xmath261 where @xmath262 is an integrable function ( w.r.t .",
    "the abc filter ) .",
    "convergence results as @xmath209 grows can be found in @xcite . in algorithm",
    "[ alg : smc_abc ] an ( unbiased ( expectation w.r.t .  the randomness in the smc algorithm ) - see @xcite ) estimate of @xmath263 is @xmath264 this point is useful later on . the algorithm will be used as part of an mcmc proposal , that is , smc within mcmc .    on inspection of algorithm [ alg : smc_abc ] , it can be observed that there is a possibility that all the weights @xmath265 are zero and at which point , the algorithm is said to have collapsed .",
    "this could be alleviated by making @xmath47 larger , but then one loses accuracy in the abc approximation ; before doing this , one may want to try improving the smc algorithm .",
    "the property of the algorithm collapsing is not desirable , especially in the context in which we will use it - the idea of using it as part of a proposal in mcmc .",
    "that is to say , that the simulation of algorithm [ alg : smc_abc ] will be the major contributor to the computational cost of an associated mcmc algorithm and , if the smc approach collapses ( especially close to time @xmath51 ) one will have spent considerable effort on a proposal which will be rejected .",
    "one algorithm which has been proposed in the literature and which can potentially deal with this problem is the alive particle filter ( see @xcite , see also @xcite ) .",
    "this idea is related to algorithm [ alg : new ] in that it uses a random amount of effort to ensure that there are @xmath209 samples alive .",
    "the alive particle filter is presented in algorithm [ alg : alive_smc ] .",
    "it should be noted that this algorithm only retain @xmath266 of the alive samples - this allows the following property : an ( unbiased ( expectation w.r.t .",
    "the randomness in the smc algorithm ) - see @xcite ) estimate of @xmath181 is @xmath267 the unbiased property will be useful for the mcmc algorithms in the next section .",
    "it should be noted that one can also approximate the abc filter just as above and convergence results can be found in @xcite ( note that the result in @xcite are for different estimates ) . this algorithm has been further extended in @xcite and we refer the reader to that article for details .",
    "intialization : for @xmath268 , sample @xmath269 from a distribution with density @xmath270 and @xmath271 from the likelihood . compute the weight @xmath272 . set @xmath273 2 .",
    "resampling : resample the particles ( setting @xmath274 ) .",
    "denote the resulting particles @xmath275 .",
    "3 .   sampling : for @xmath268 , sample @xmath276 from a distribution with density @xmath277 and @xmath278 from the likelihood . compute the weight @xmath279 .",
    "set @xmath280 and go to 2 .",
    "1 .   intialization : sample @xmath281 from a distribution with density @xmath270 and @xmath282 from the likelihood until there are @xmath209 samples with @xmath283 , @xmath272 , record the number of samples required to do this , call it @xmath284 . set @xmath273 and discard all samples except the first @xmath266 with non - zero weight ( call these the alive particle ) .",
    "iteration : resample ( amongst the alive particles ) @xmath285 according to the weights @xmath286 then sample @xmath287 from a distribution with density @xmath288 and @xmath289 from the likelihood until there are @xmath209 samples with @xmath290 , @xmath291 , record the number of samples required to do this , call it @xmath292 .",
    "set @xmath280 and discard all samples except the first @xmath266 with non - zero weight and return to the start of 2 ..    to end the section we note that if smoothing is of interest , then any of the standard smc approaches can be adopted such as @xcite ; see @xcite for some implementations .",
    "we now consider trying to sample from the abc approximation of the posterior on the parameters , i.e.  @xmath178 ; as this is not possible , we will construct algorithms on extended state - spaces , such as in section [ sec : mcmc_for_iid ] .",
    "it should be noted that algorithms which attempt to sample the abc posterior using ` standard ' mcmc methods , often do not work well ; we direct the reader to @xcite for an explanation .",
    "we present two basic particle marginal metropolis - hastings algorithms ( pmmh ) @xcite , which can work well in practice .",
    "the first mcmc algorithm that we present is in algorithm [ alg : pmmh ] .",
    "this algorithm uses algorithm [ alg : smc_abc ] to propose new points in the state - space ; as remarked above , due to the possiblity that the smc may collapse this algorithm may not be entirely satisfactory .",
    "we do not give details of the target density ( see @xcite ) but note that @xmath213 is a marginal ( which essentially follows because the estimate @xmath293 is unbiased ) and one of the key issues is the relative variance of @xmath293 w.r.t .  the smc algorithm that is simulated . in order to select",
    "@xmath209 one can appeal to theoretical results in smc ( @xcite ) which , as noted by @xcite say that one should select @xmath219 to control the relative variance of the estimate @xmath293 which appears in the acceptance probability .",
    "more precise results for selecting @xmath209 can be found in @xcite .",
    "note that to fit the collapsed representation of a hmm ( as in section [ sec : collapse ] ) one can use a pmmh algorithm very similar to algorithm [ alg : pmmh ] ; see @xcite for a pmmh algorithm for a collapsed representation of a hmm .    * sample @xmath202 from a proposal @xmath203 with density @xmath204 .",
    "* run algorithm [ alg : smc_abc ] with parameter @xmath294 and record the estimate @xmath295 in . *",
    "accept @xmath294 with probability : @xmath296    the second mcmc algorithm we present is in algorithm [ alg : pmmh_alive ] .",
    "this mcmc kernel uses algorithm [ alg : alive_smc ] as part of its proposal mechanism .",
    "essentially this algorithm removes the issue of the smc collapsing , but the associated cost of each proposal may be significantly higher than in algorithm [ alg : pmmh ] . as for the previous mcmc algorithm , we do not give details of the target ( see @xcite ) but one of the key issues why @xmath213 is a marginal is that the estimate @xmath293 is unbiased .",
    "the remarks for setting @xmath209 are the same as for algorithm [ alg : pmmh ] , although , we note that ( under assumptions ) the work in @xcite indicates that the relative variance of is less than that of and that potentially algorithm [ alg : pmmh_alive ] is more efficient than algorithm [ alg : pmmh ] ; we discuss further in the next section .",
    "it should be noted that for both algorithms [ alg : pmmh ] and [ alg : pmmh_alive ] inference for the state - sequence @xmath18 can also be performed and we refer to @xcite for details .",
    "we also note that a version of algorithm [ alg : pmmh_alive ] has been adopted in @xcite for time series models , when @xmath42 is countably - infinite ; in such scenarios the data can be matched leading to exact inference - we refer the reader to @xcite for more details .    * sample @xmath202 from a proposal @xmath203 with density @xmath204 .",
    "* run algorithm [ alg : alive_smc ] with parameter @xmath294 and record the estimate @xmath295 in . *",
    "accept @xmath294 with probability : @xmath296    the section is concluded by remarking that there are more advanced algorithms in the literature .",
    "examples include the approach in @xcite and use of the particle gibbs sampler ( @xcite ) for example using the alive particle filter as in @xcite ( who also considers the collapsed representation for doubly intractable hmms - the associated pmmh algorithm is very similar to algorithm [ alg : pmmh ] ) .",
    "we consider the following hmm , for @xmath297 @xmath298 where @xmath299 , @xmath300 ( a stable distribution with location parameter 0 , scale @xmath301 asymmetry parameter @xmath302 and skewness parameter @xmath303 ) and @xmath304 .",
    "we set @xmath305 , with priors @xmath306 , @xmath307 ( @xmath308 is an inverse gamma distribution with mode @xmath309 ) and @xmath310 .",
    "note that the inverse gamma distributions have infinite variance .",
    "we again consider the daily index of the s & p 500 index between 03/01/2011 @xmath311 14/02/2013 ( 533 data points ) .",
    "we consider two scenarios to compare algorithms [ alg : pmmh ] and [ alg : pmmh_alive ] . in the first situation we set @xmath312 and in the second , @xmath313 , with @xmath314 in both situations . in the first case ,",
    "we make @xmath47 not too large and in the second , @xmath47 is significantly reduced .",
    "the noisy abc approximation is used .",
    "the smc and alive smc algorithms used the transition dynamics of the @xmath2 as the @xmath315 densities in algorithms [ alg : smc_abc ] and [ alg : alive_smc ] .",
    "both algorithms are run for about the same computational time , such that algorithm [ alg : pmmh_alive ] is run for 20000 iterations .",
    "the parameters are initialized with draws from the priors .",
    "the proposal on @xmath316 is a normal random walk and for @xmath317 a gamma proposal centered at the current point with proposal variance scaled to obtain reasonable acceptance rates .",
    "we consider @xmath318 and for algorithm [ alg : pmmh_alive ] this value is lower to allow the same computational time .",
    "our results are presented in figure [ fig : pmcmc ] . in figures [ fig : pmcmc ] ( a)-(b ) we can see the output in the case that @xmath312 ( only for @xmath316 , more results are in @xcite ) .",
    "it appears that both algorithms perform very well ; the acceptance rates were around 0.25 for each case .",
    "for this scenario one would prefer algorithm [ alg : pmmh ] as the algorithmic performance is very good , with a removal of a random computational cost per iteration .    in figures [ fig : pmcmc ] ( c)-(d )",
    "the output when @xmath313 is displayed . in figure",
    "[ fig : pmcmc ] ( c ) we can see that algorithm [ alg : pmmh ] performs very badly , barely moving across the parameter space ( despite significant efforts to tune the algorithm ) , whereas the new pmmh algorithm has very reasonable performance ( figure [ fig : pmcmc ] ( d ) ) . in this case",
    ", @xmath47 is very small , and algorithm [ alg : smc_abc ] collapses very often , which leads to the undesirable performance displayed . as in section",
    "[ sec : simos_iid ] the results here suggest that algorithm [ alg : pmmh_alive ] might be preferred in difficult sampling scenarios , but in simple cases it does not seem to be required .      as for i.i.d .",
    "models if one chooses to use a non - negative kernel in the abc approximation , then some the issues discussed above will not be so relevant .",
    "a starting point for mcmc algorithms would be algorithm [ alg : pmmh ] which uses algorithm [ alg : smc_abc ] except with the indicator replaced with the non - negative kernel .",
    "the computational methods discussed so far focus on exact computation for the abc approximation .",
    "the methods are certainly not simple to understand and often take a considerable amount of effort in both coding as well as time to run .",
    "an idea put fourth in @xcite , for exactly the class of models ( and abc approximations ) considered in this article , is to try to infer the abc posterior using _ approximate _ computational methods .",
    "the authors also state that the approach can be used as an initial investigation of data , instead of possibly the ` final ' data analysis .",
    "the idea is to use the expectation - propagation algorithm of @xcite .",
    "the authors successfully implement the methodology for a wide variety of statistical models .",
    "they also compare the methods to the pmmh methods investigated in this article .",
    "considerable gains in computational time are reported , with some loss of accuracy in inference .",
    "this method seems to be very promising and an interesting alternative to using exact computational methods .",
    "the fact remains , however , that a theoretical study of the increased loss in accuracy in inference may be more challenging than abc with exact computation - although the latter task is still not straight - forward .",
    "this article has focussed upon time - series models , but the computational techniques discussed thus far have focussed on batch parameter inference .",
    "we now briefly discuss methodologies that have been used to perform online parameter inference for abc , that is as data arrive sequentially .    in the context of i.i.d .  and",
    "odts models one techniuqe that has been adopted in @xcite is the smc sampler method of @xcite .",
    "this approach allows one to estimate the posterior density on @xmath7 as data arrive .",
    "the method is based on mcmc and typically the computational effort per time step grows with the time parameter , but the overall effort is only polynomial in @xmath51 - @xmath319 .    for hmms estimating the posterior density on @xmath7 is a notoriously difficult computational problem ( for smc methods ) , even when the likelihood is tractable ; see @xcite .",
    "one possible approach is in @xcite , but again the computational cost per time step ( which is polynomial in @xmath51 ) increases with the time parameter .",
    "online parameter estimation methods ( i.e.  whose cost does per - time step does not increase with the time parameter ) , based upon maximum likelihood can be found in @xcite ; note that @xcite has also been used for i.i.d .",
    "@xcite also considers a type of online estimation procedure for lotka - voltera models .",
    "in this article we have discussed both abc approximations and computational methods for a class of time series models found in many applications .",
    "there are a number of important and interesting research directions , some of which we now list .    from a mathematical perspective",
    "there are several aspects which could be investigated .",
    "firstly , many of the consistency and asymptotic normality results have been given for specific models .",
    "however , one suspects that general theorems ( for both classical and bayesian consistency ) for the entire class of models could be proved . in addition , in specific scenarios that have been discussed , the mathematical assumptions could be significantly relaxed .",
    "this discussion concerns only the approximation , and as such suggests that one wishes @xmath47 to be as small as possible .",
    "as has been remarked in section [ sec : comp ] , making @xmath47 arbitrarily small will make even the most sophisticated computational algorithms collapse .",
    "so there is a need to both analyze biases ( which are deterministic ) and the computational errors ( which are stochastic ) specifically for problems in abc - this could allow one to find ( as done in @xcite ) an optimal @xmath47 in specific scenarios to facilitate a trade off in the errors ( a point also made in @xcite ) .",
    "to further expand , the approach advocated by this author ( and others see @xcite ) , is to decompose the error in approximation as follows ; let @xmath320 be the expectation of a real - valued @xmath321integrable function @xmath322 under the true posterior , @xmath323 the abc approximation of it and @xmath324 a monte carlo based ( e.g.  via mcmc ) approximation of @xmath323 , then ( for example ) considering the @xmath325error , one has @xmath326 \\leq \\mathbb{e}^n[|\\pi^{\\epsilon , n}(\\xi)-\\pi^{\\epsilon}(\\xi)| ] + |\\pi^{\\epsilon}(\\xi)-\\pi(\\xi)|\\ ] ] where @xmath327 is the expectation w.r.t .  the monte carlo randomness .",
    "the idea is to then to separately deal with the two errors on the r.h.s .",
    "the first of which is the monte carlo error and the second the bias ( for finite number of data even noisy abc will have a bias ) .",
    "some work has been done in @xcite but under very restrictive mathematical assumptions and less restrictive ( and more complete ) results are in @xcite , and in the cases of rejection sampling and regression adjustment @xcite , but some additional work is needed in general .    from a computational perspective , whilst we have presented some of the most up - to date methods in the literature , there is still a need to improve upon this .",
    "for example , one can not guarantee that the alive particle filter will fix all the issues with standard smc for hmms .",
    "perhaps the direction to follow is as in @xcite , where the notion of being approximate in computation also , is advocated , instead of a collection of even more sophisticated tools which can be difficult to understand for statistical practitioners .",
    "some work on statistical software for the class of time series models in this paper also appears to be required .    from an inferential perspective",
    "there are many issues to be considered .",
    "firstly this paper has only considered , in the main , sub - classes of the time series models of interest .",
    "an increased consideration of other models ( such as bilinear time series models ) would be of interest ( mathematically , computationally and inferentially ) .",
    "secondly , the article has only considered models for which the summary statistics are the data themselves . as the demands of applications in ` big data ' and high - dimensions increases",
    ", these ideas will become less relevant and a consideration of this issue must be studied .",
    "as noted by @xcite , the use of ` local ' statistics ( for each data - point , in the case of this paper ) may be a more manageable problem than the use of global statistics as often adopted in the literature .",
    "thirdly , the article has not considered the aspect of model selection ( for example for hmms when @xmath328 and one wishes to choose @xmath115 ) , which is very challenging in the context of abc @xcite . for the abc approximation with identity summary statistics ( see again @xcite ) , one may suspect that this is just an exercise in application of existing computational methods ( e.g.  @xcite ) but in the scenario where summary statistics are adopted this problem will be far more challenging .",
    "the author was supported by singapore moe grants r-155 - 000 - 119 - 133 and r-155 - 000 - 143 - 112 .",
    "the author is also affiliated with the risk management institute at the national university of singapore .",
    "the author thanks pierre del moral , kody law and andrew stuart for conversations on this work as well as collaborators sumeetpal singh , nikolas kantas and david nott for many conversations on abc . in addition",
    "thanks to former phd students zhang and ehrlich for access to computer code .",
    "the proof is as follows : we will show that for any fixed @xmath330 , @xmath31 that @xmath331 where @xmath332 once this is achieved , the result is proved by the dominated convergence theorem on noting that one has , via ( a[hyp:1 ] ) @xmath333 @xmath334 so one can treat the numerators and denominators in bayes theorem seperately to conclude the result .",
    "we now proceed to prove , which is done by induction on @xmath51 .",
    "let @xmath273 , then we have @xmath335du\\ ] ] this will converge to zero as @xmath63 by lebesgue s differentiation theorem ( e.g.  @xcite ) . now assuming the result for @xmath336 we have @xmath337 @xmath338 + p_{\\theta}(y_n|y_{1:n-1})[p_{\\theta}^{\\epsilon}(y_{1:n-1}|x_{1:n-1 } ) - p_{\\theta}(y_{1:n-1}|x_{1:n-1})].\\ ] ] by the induction hypothesis , the second term on the r.h.s",
    ".  will converge to zero as @xmath63 . the first term on the r.h.s .",
    "will converge to zero as the term @xmath339 converges ( by the induction hypothesis ) and the term @xmath340 $ ] goes to zero by almost the same argument for intialization of the induction .",
    "this concludes the proof .        1 .",
    "@xmath102 is compact .",
    "@xmath341 is uniformly continuous ( i.e.  over @xmath342 ) in @xmath7 .",
    "3 .   @xmath343 if and only if @xmath344 .",
    "there exist a @xmath345 such that @xmath346 .",
    "@xmath101 exists and is unique .",
    "we prove @xmath100  first ; the proof of first statement will follow . in the case of 2 .",
    "the @xmath347 , so one need only check that the conditions required for consistency of the mle are satisfied .",
    "we list 4 assumptions often used in the literature ( see @xcite ) :      if one can verify 1.-4 .",
    "above then one has @xmath100 .",
    "1 .  holds by assumption . for 2 .",
    "let @xmath354 , then one can find a @xmath355 such that for @xmath356 we have @xmath357 where we have used ( a[hyp:2]-2 ) ; thus 2",
    ".  holds . for 3 .",
    "we have @xmath358 now the r.h.s .",
    "is equal to @xmath359 if and only if @xmath344 by ( a[hyp:2]-3 ) ; this proves 3 . for 4 .",
    "consider @xmath360 where we have applied jensen s inequality to go to the second line and then used ( a[hyp:2]-4 ) to go to the final line .",
    "thus 4 .  holds as @xmath359 is a probability density .",
    "hence we have proved that @xmath100 . to prove that @xmath99 one can apply ( * ? ? ?",
    "* theorem 2.2 ) for misspecified mles ; assumptions ( a1 ) , ( a2 ) and ( a3a ) ( of that paper ) follow by the above arguments ( and our assumptions ) and ( a3b ) follows by ( a[hyp:2]-5 ) .",
    "this completes the proof of the proposition .",
    "andrieu , c. , doucet , a. & lee , a.  ( 2012 ) .",
    "discussion of : constructing summary statistics for approximate bayesian computation : semi - automatic approximate bayesian computation .",
    "b _ , * 74 * , 451452 .      , s. , chopin , n. , jasra , a. & singh , s. s.  ( 2012 )",
    ". discussion of : constructing summary statistics for approximate bayesian computation : semi - automatic approximate bayesian computation .",
    "b _ , * 74 * 453454                                        , p. , & jacod , j.  ( 2002 ) .",
    "the monte - carlo method for filtering with discrete time observations .",
    "central limit theorems . _",
    "the fields institute communications , numerical methods and stochastics _",
    "lyons , t.s .",
    "salisbury , amer .",
    "soc .                                                        , f. & oudjane , n.  ( 2006 ) .",
    "a sequential particle algorithm that keeps the particle system alive . in _",
    "stochastic hybrid systems : theory and safety critical applications _ ,",
    "( h. blom & j. lygeros , eds ) , lecture notes in control and information sciences * 337 * , 351389 , springer : berlin                              , g. , doucet , a. , & singh , s. s.  ( 2011 ) .",
    "particle approximations of the score and observed information matrix in state space models with application to parameter estimation .",
    "_ biometrika _ , * 98 * , 65-80 .",
    ", s. r.  , kypraios , t. & preston , s. t.   ( 2014 ) .",
    "piecewise approximate bayesian computation : fast inference for discretely observed markov models using a factorised posterior distribution . _",
    "( to appear ) .",
    "approximate bayesian computation ( abc ) gives exact results under the assumption of model error .",
    "_ statist .",
    "genetics mole .",
    "_ , * 12 * , 129141 . also available as arxiv:0811.3355 , 2008"
  ],
  "abstract_text": [
    "<S> in the following article we consider approximate bayesian computation ( abc ) for certain classes of time series models . in particular , we focus upon scenarios where the likelihoods of the observations and parameter are intractable , by which we mean that one can not evaluate the likelihood even up - to a positive unbiased estimate </S>",
    "<S> . this paper reviews and develops a class of approximation procedures based upon the idea of abc , but , specifically maintains the probabilistic structure of the original statistical model . </S>",
    "<S> this idea is useful , in that it can facilitate an analysis of the bias of the approximation and the adaptation of established computational methods for parameter inference . </S>",
    "<S> several existing results in the literature are surveyed and novel developments with regards to computation are given . </S>",
    "<S> + * keywords * : approximate bayesian computation ; hidden markov model ; observation driven time series .    * </S>",
    "<S> approximate bayesian computation for a class of time series models *    by ajay jasra    department of statistics & applied probability , national university of singapore , singapore , 117546 , sg . </S>",
    "<S> + e-mail:`staja@nus.edu.sg ` </S>"
  ]
}