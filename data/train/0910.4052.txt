{
  "article_text": [
    "parallel computing was originally used in the mid-60s in specialized supercomputers that were primarily intended for massive parallel processing @xcite .",
    "however already in the beginning of the 90-s the well - known expert in the multithreaded architecture , barton smith , put forth the challenge about the necessity to create a supercomputer @xcite , @xcite , which would be able to perform the general purpose parallel computation ( gppc ) much more efficiently than the existing cluster systems based on universal microprocessors . in his view , the emergence of such supercomputer should force a large influx of financial and human resources and should revitalize the industry s interest in a further development of a computer architecture .",
    "the challenge posed by barton smith has inspired the development of the innovative architecture presented in this article , which seems to be the architecture - candidate for the general purpose supercomputer implementation .",
    "it should be clarified that in line with smith @xcite , @xcite , the gppc refers to the most widely used class of computations , represented by a large set of arbitrary and frequently switching processes and threads . at present",
    "these computations are most effectively supported in the multi - threaded architecture , which has been first applied in the mid - sixties to the implementation of the peripheral processors of supercomputer cdc-6600 @xcite .",
    "this architecture has been evolving in the set of systems @xcite , @xcite , @xcite , @xcite and in the most advanced shape it has been implemented in the cray threadstorm @xcite microprocessor , which has been used in the special processing nodes of the heterogeneous cray xmt supercomputer @xcite .",
    "currently parallel processors are used widely - from supercomputers to household appliances .",
    "however all the processors while performing a real tasks , do not get high enough speed that a modern high - performance logic should provide .",
    "the well - known expert in the architecture of supercomputers , thomas sterling , in his paper @xcite identified the four major reasons explaining the low real efficiency of computers , which he calls the four horsemen of apocalypse :    latency - delays experienced while accessing memory or other parts of the system ;    overhead - extra work due to management of concurrent operations ;    contention - delays experienced due to competition over resources used on a shared basis ;    starvation - hardware idle state because of insufficient parallelism as well as the lack of load balancing .    in the same paper",
    "@xcite he provides a number of solutions , whose implementation should improve the actual performance of computers .",
    "this solutions include a hardware support of service activities ( overheads ) , including the isa for atomic compound operations on complex data , synchronization , communications and using of multithreading for latency suppression .",
    "following this line , the present paper proposes the functionally complete system of hardware solutions , which should radically increase the efficiency of gppc implementations .",
    "the main difference between the general - purpose parallel computing and the massive parallel processing ( mpp ) @xcite is the significant , of an order of magnitude or more , exceedence of the number of the tasks capable of concurrent running over the number of processor s physical channels that can simultaneously carry out these tasks .",
    "therefore , the decline in the productivity of even the simplest processors causes overheads associated with the extra work that has to be done to manage the program concurrency .",
    "in addition to the lack of hardware resources , an idle state of useful tasks can be caused by purely algorithmic reasons .",
    "these reasons include waiting by one program thread for an information from another thread about the ability to continue a work or about external events , notified by means of io devices .",
    "this article calls these states as states of algorithmic latency both of the works and the hardware , which is used to maintain these works in the latency state .",
    "the transit states related to switching the works between active and waiting states are also called the latency states .",
    "rapid improvement of digital circuit engineering has resulted in great decrease of powerful microprocessors cost .",
    "the leading manufacturers of mass microprocessors intel and amd already produce multicore and multithreaded dies as their main products .",
    "sun microsystems introduced the 8-core ultrasparc - t2 microprocessor @xcite , whose each core can perform 8 threads simultaneously .",
    "accordingly , parallel processors have become very widely used , even in household electronics - laptops and smartphones .",
    "all these circumstances have led to very wide spread of parallel computing and force programmers to deal with parallel programming . in analyzing this phenomenon , in the key conference report",
    "isc07 smith @xcite put forth the statement that the computer industry is faced with the challenge of the need to develop new concepts and architectures of parallel computing , which should greatly simplify programming of parallel systems of arbitrary size and complexity .",
    "this requirement to simplify significantly the gppc programming together with the requirement to minimize the mentioned above four reasons of decline in performance , should be considered a generalized requirement for a new architecture .",
    "on the basis of this generalized requirement we develop the innovative architecture of the processor ( us patent pending ) , presented in this paper , which is called the virtual - threaded machine ( vthm ) .",
    "the remainder of this article describes the basic concepts of this architecture , which should be considered as a significant improvement of multithreaded architecture in the direction of providing the tolerance to the algorithmic latency .",
    "main properties and functions of the architecture are as follows :    \\1 .",
    "the fine grain representation of architectural registers as a set of blocks in microarchitectural registers ; this representation provides a distributed location of blocks on different levels of a microacrchitectural virtual memory ;    \\2 .",
    "the inheritance of the thread - owner priority by all elements of the threads representation in hardware , which representation includes the performing instructions and the fine grain representation of isa registers ;    \\3 .",
    "the asynchronous hardware swapping of long time inactive elements of the distributed representation of the architectural registers on the levels of the virtual memory in accordance with their priorities and ageing metric ;    \\4 . the fine grain processor decomposition , whose processing elements simultaneously interact with each other by means of transactions exchange via buffer pools of prioritized queues ;    \\5 .",
    "the hardware prioritized multiprogramming execution of a virtual set of threads in contexts of a virtual set of processes .",
    "the prioritized fine grained switching of threads between states of activity and waiting with the accuracy closer to a few isa instructions ;    \\7 . the full hardware support of an authorized access of running threads to a data of different processes .",
    "\\8 . the threads synchronization hardware features on the basis of the hardware - driven semaphores .",
    "\\9 . the hardware timing of the semaphore - related waiting states .",
    "the hardware support threads data exchange with io units and inter - processor communication without interruptions .",
    "the hardware support for direct data exchange between processes memory and io devices , without using of drivers and an operation system ;    \\12 . the uniform synchronization between threads running in processors and threads running in io devices .",
    "the rest chapters of this paper contain the considerations , which enable us to formulate the concept and the features of vthm architecture .",
    "we also present an implementation of this concept as a block diagram of major component s level . in conclusion",
    "we sum up the viewed decisions and define the place of proposed architecture in the existing taxonomies .",
    "in existing architectures the main bulk of multiprogramming functions realizing the parallel execution of multiple tasks is implemented by means of software .",
    "the basic concept of the proposed architecture is that the all reasons of the degradation of the real performance considered above @xcite can be eliminated by means of the new hardware features which would support the deep virtualization of multiprogramming .",
    "the proposed features should bring the ratio of the software and hardware implemented functions of the multiprogramming at least up to the ratio achieved in the modern virtual memory management features . at the same time ,",
    "efficiency of these features should be sufficient for the simultaneous execution of the very large , essentially the virtual , set of threads .",
    "in other words , the proposed computer architecture should be balanced in the two orthogonal directions of virtualization - in the virtualization of memory management and in the virtualization of threads management .",
    "since the threads management is almost completely transferred to the hardware level in the proposed architecture , we call this one the virtual - threaded machine ( vthm ) .",
    "accordingly , we introduce the new term virtual - threading ( vth ) to sum up the essence of the architecture functionality - the totally hardware - implemented multiprogramming .",
    "let s consider briefly how the invention and development of the virtual memory hardware support features has simultaneously enhanced the efficiency of computers as well as simplified programming .",
    "on this basis , we describe the expected similar double effect resulting from the introduction of virtual - threading - the increase in the real productivity of computers and the simplification of parallel programming .",
    "it seems that the most radical simplification of programming irrespectively of the level of algorithms encryption languages is associated with the first implementation of the concept of virtual memory in the atlas computer , developed as early as 1962 @xcite .",
    "the simplicity of programming for machines with virtual memory consists in the ability to write programs without worrying about whether the overall size of their code and data does not exceed the amount of a physical memory of a machine . the remaining necessity to ensure that this amount does not exceed the amount of the virtual address space size determined by the isa , is not significant in most applications because of the very large size of this space .",
    "for example , a bubble - sorting program is very simple as long as that the array is fully fit within a physical memory , but the programs complexity increases by magnitude of several orders if the array does not fit within the physical memory and the machine does not support the virtual memory .",
    "since even the commodity and inexpensive modern microprocessors support the virtual addressing by the 64-bit address , a majority of modern day programmers almost ceased to care about the amount of required memory . however the simplicity of writing large programs brought by the virtual memory would be meaningless without the effective hardware support .",
    "the purely software operated overlay memory management @xcite , which was used before the invention of the virtual memory , greatly simplified the writing of large programs .",
    "but apart from the inconvenience of additional manual writing of scripts for overlay supervisor , this management was making significant overheads associated with the swapping of a large statically defined segments of memory .",
    "this mandatory correlation - simplification of parallel programming by means of the representation of a natural parallelism of an algorithm as a virtual set of threads and small overheads for organizing the concurrent run of this set - is the principal feature of the proposed vthm architecture .",
    "the main reasons of the effectiveness of the modern hardware support of the virtual memory are the multilevel organization one and the fine granulation of elements swapped between the memory levels . in the first computer with virtual memory @xcite , the only swapping element was the page of about 3-kilobyte length .",
    "apart from operating registers there was only one level of virtual memory - the main ram , with which the central processor could interact at the microarchitectural level .",
    "the second level of the virtual memory was placed on the page drum and the operating system had to ensure page readings into the ram by means of an isa code sequence",
    ".    there are exist one to three levels of caches between the operating registers and ram in modern computers , and swapped elements have the size of a few dozens bytes .",
    "the closer each level is situated to the processor , the smaller volume and the lower access time it has . in dynamics , the swapped elements are placed on the levels of the cache memory in such a way that the most frequently used items are placed on the upper fastest level and the least frequently used elements are pumped off into a ram .",
    "the overall effect of using such a multi - level memory organization is the multiple increase in the speed of the program running due to the reduction a data exchange frequency between operating registers and a ram .",
    "it is important to note that in the proposed vthm architecture all movements between the levels of the cache memory , and between the lower - level caches and the ram , are fully implemented by means of the hardware microarchitecture , and only the swapping of pages between the ram and io devices is realized by means of isa code .",
    "following the brief description of the implementation of the virtual memory made above , it is convenient to clarify the concept of virtual - threading . in general terms , any works on a computer is represented by processes and threads , which essentially are the main virtual objects supported by an operating system .",
    "a set of all such works , which are represented by processes and threads , is known as a multiprogramming mix , whereas a computer system s work which executes the joint running of these works on a set of hardware units , is known as the multiprogramming .",
    "prior to the clarifications would be made in the following chapters , we assume that the multiprogramming mix consists of threads .",
    "newly created threads and threads in the waiting state are represented by means of the data structures , hereinafter referred to as the thread descriptors .",
    "these descriptors include the full file of architectural registers as well as a control information of the operating system .",
    "they are created by means of software and are located in a computer s multi - level virtual memory as a usual data . to ensure that a thread initially starts or resumes running after having been in the waiting state ,",
    "the operating system must copy the architectural registers file from the descriptor in the memory into the hardware unit s physical registers file , thereafter referred to as the operating registers file . in the absence of a free operating registers file , the operating system should copy one of the operating registers file occupied by a descriptor of an another thread to the corresponding thread descriptor in the isa level virtual memory and only after that the system can upload the other thread descriptor into the vacated place .",
    "this procedure of counter rewriting of registers is thereafter called the swapping of architectural registers .    the main drawback of existing architectures while performing the gppc is the need of swapping of the architectural registers as a whole . indeed , with frequent transitions of a single thread between the active and waiting states , as a rule , in each state of activity only a relatively small fragment of its code will be executed",
    "accordingly , this fragment will only work with a small subset of the architectural registers file , forming one or more blocks of the operating registers .",
    "for example , in the risc architecture it could be the block of control registers , which contains the current command counter , the processor status word and the block of current window registers .",
    "in addition , this statement is confirmed by annex h of the sparc v9 architecture manual @xcite , which shows the way of optimal code generation . in this code",
    "the leaf procedures can be made in such a way as to operate without their own register window , using their caller s window instead .",
    "such leaf procedures will be the most frequently used fragments of optimized codes , which access only to the 1/8 of the isa windows .",
    "it is clear that it possible to increase the efficiency of execution of the frequently switching threads code by means of fine grain organization of the operation registers file and corresponding organization of its swapping .",
    "this argument seems logical , because it is essentially the extension of the property of compactness of an active working set of virtual memory pages @xcite , @xcite to the fine grain representation of architectural registers files and microarchitectural operational registers files . within such interpretation ,",
    "the microarchitectural operation registers files constitute the fastest level of the virtual memory , and their blocks can be viewed as data cache blocks , which are dynamically loaded by the blocks of the corresponding architectural registers file by means of hardware in accordance with executed instructions demands .",
    "finaly , we can consider the vthm architecture as a result of an extremely deep exploitation of the locality princip @xcite in the processors microarchitecture implementation .",
    "in contrast to the optimized version of ther register file swapping as proposed above , in the existing architectures the entire file of architectural registers is always loaded into the operational registers file , regardless of the actual requirements of the fragment of code , which is being executed in the each phase of activity .",
    "in fact , such a coarse granularity of file registers swapping is fully consistent with the one - level virtual memory implementation in the first processors , in which the access to a small piece of data in an absent page forces loading of an entire page from the page drum into ram @xcite .",
    "this redundancy leads to time overheads for the swapping of a non - modified architectural register blocks and leads to increase of an idle hardware bulk , which is occupied by an unused blocks of the architectural registers .",
    "the second drawback of the existing technique of multiprogramming is the program - implemented dispatching , in which the selection of a thread to be activated and a swapping of thread descriptors are performed by means of an isa code just as the pages swapping was performed in the early virtual memory systems .",
    "this drawback is easily eliminated because the functions of the created threads dispatching , including the priority dispatching technique , are fairly simple .",
    "their implementation by means of software uses only a small subset of isa , which provides the integer arithmetic , load - store and synchronization instructions .",
    "this justifies the direct hardware implementation of the dispatching functionality by means of simple specialized unit , relieving the processor to execute the useful tasks . within this approach , it is only the complex and rarely performed functions of operating systems that implement file systems , high level network protocols , resource accounting , as well as the creation and destruction of processes and threads , should be implemented by means of software .",
    "let us clarify the major aspects of the proposed fine grained threads dispatching .",
    "creation of a thread leads to creation of the thread descriptor that contains the information necessary and sufficient for running the thread , with the possibility of suspending and resuming it purely by means of hardware .",
    "such a descriptor in fact is a virtual processor , stored in the multi - level microarchitectural virtual memory . an upper level of this memory is the fine grained operating registers blocks , into which the requested blocks of architectural registers are loaded by means of hardware in accordance with the dynamics of active threads .",
    "in fact , the virtual - threading virtualizes the fastest level of microarchitectural memory - the level of operating registers of the processor . as a result ,",
    "the operating registers blocks , which store the fine grained threads descriptors elements , are swapped in the same manner as the data cache elements .",
    "essentially this presentation is logically distributed - its elements can be allocated on different levels of microarchitectural virtual memory at each moment in accordance with the dynamics of threads access to the architectural registers .",
    "summarizing what was said above in this chapter , one can define the virtual - threading as the direct hardware management of threads from their inception to destruction , including their hardware dispatching and per - element hardware swapping of fine grain threads descriptors representation in accordance with requests of instructions streams .",
    "we suppose thay with a deep exploitation of so defined concept of virtual - threading in processor design , the de facto risen silicon curtain on both sides of which the hardware engineers and system programmers almost independently decide on the merits of the general problem of improving the efficiency of general purpose parallel computing , should be eliminated . on one hand , the efforts of developers of processors led to the invention of the multithreaded architecture ( mta ) @xcite , @xcite . such architecture provides the tolerance to the architectural latency , caused by long lasting accesses to the non - local memory in the numa - systems .",
    "for example , the tera computer supports 128 simultaneous threads , which is enough to cover the maximum duration of access equal to 67 processor clock ticks .",
    "this architecture has broad scalability , which allowed to create the cray xmt cluster system with few thousands nodes based on the mta - processors @xcite .    on the other hand , joint efforts of developers of processors and system programmers led quite long time ago to the invention of multiprogramming , ensuring the tolerance to the algorithmic latency as defined above .",
    "however , this tolerance has been provided by means of software and hardware only at the macro level - at the level of large logical units , such as cpus , io channels and io units .",
    "this multiprogramming provides a parallel work of the potentially all units mentioned above by means of the hardware features , including timers and io interruptions as well as a direct memory access ( dma ) channels .",
    "in fact , it provides a purely hardware concurrent work ( cpu computations or io units data transfers ) for the benefit of threads , total number of which is not greater than the number of the hardware macro units .",
    "if the total number of the threads running in a system is greater than the number of these units , the current multiprogramming technique changes the running threads set only by means of software .    while performing the gppc , the number of software threads exceeds the number of hardware threads by an order or more .",
    "therefore the tolerance to the algorithmic latency ca nt be ensured by means of simple build - up of a number of hardware threads , as it is done in the mta architecture to ensure the tolerance to the architectural latency .",
    "the common feature of the architectural and algorithmic latency is the consumption of computer resources in the latency states , e.g. , in the states when the thread does not perform useful work .",
    "it is natural to call these states the generalized latency states .",
    "it should be noted that in a current mta processors implementations resources , that provide tolerance to the architectural latency in a time interval , are busy in the interval and could nt be simultaneously used to provide the tolerance to the algorithmic latency .",
    "for example , in the above mentioned tera computer @xcite the hardware thread can wait up to 64 processor clocks while accessing to the non - local memory . during this time",
    "the hardware supporting the waiting state , is reserved and can not be used to perform any ready - to - run thread .",
    "therefore , the effective number of hardware threads , capable of providing tolerance to the algorithmic latency may be significantly smaller than it is defined by the mta processor architecture .",
    "having thus defined the concept of generalized latency , it is naturally extend the general technical quality factor of any engines - the coefficient of useful work , e ( efficiency ) - to computers architecture and determine it by the following simple formula :    e = 1 - cl / c ,    where c - the total cost of hardware used in performing some quantity of works ;    cl - the total cost of hardware remaining in a state of the generalized latency during performing this quantity of works .",
    "the value of e can be considered the level of generalized latency tolerance ( glt ) , that is glt = e. in the other words , one can suggest the following optimality principle of computer architecture , the most effective for performance gppc . in the optimal architecture",
    "the set of works , defined as overheads , should be maximally reduced , while the remaining works , including keeping threads in the generalized latency state , should be performed by a hardware of minimal cost .    in this context",
    ", the proposed vthm architecture can be defined as the architecture with high level of generalized latency tolerance .",
    "the key elements of the innovation , which induce the domino effect in the development of this architecture , are the distributed fine grained representation of the architectural register file , which is moved by means of hardware through the levels of the microarchitectural virtual memory , and the direct hardware multiprogramming on the basis of this representation .",
    "the logical chain , providing the conceptual integrity of design , is as follows :    - the fine grain representation of the architectural register files and the direct hardware multiprogramming provide extremely fast and inexpensive switching of a threads between the activity and waiting states ;    - the fast switching of threads provides the direct hardware support for a very large , essentially a virtual set of threads , which is sufficient for the direct representation of the total set of independent activities of parallel algorithms of any complexity ;    - the effective hardware support of a virtual set of threads provides the uniform representation by means of threads for all activities of a different nature associated with the users and system programs threads , hardware interruption handlers , software signal handlers and hardware activities in io units ;    - the fast switching of threads provides the effective homogeneous synchronization of different nature threads by means of the defined below hardware - driven semaphores ;    - the uniform synchronization allows to abandon the concept of interruptions , and thus significantly to simplify programming , both that of operating systems , and of user applications .",
    "the proposed architecture provides the balanced computer hardware virtualization in the two orthogonal directions - memory and activities . in the aspect of an implementation of practical processors it will cause the situation when the kernel algorithms of operating systems , like the algorithms of the multi - level virtual memory , will be built into the hardware .",
    "accordingly , developers of the kernels of operating systems in addition to the use of the system programming languages assembler and c will use the hardware developers languages such as verilog .",
    "this should bring together developers of software and hardware , and should move over time the mentioned above silicon curtain to the very appropriate place - in front of `` the four horsemen of apocalypse '' .",
    "in the following four chapters the existing implementations of multiprogramming are analyzed and the key decisions that seem to be necessary for implementation in the vthm processors architecture are described .",
    "after introducing the main conceptions we shall describe the main pragmatic and functional aspects which define the viability and possibility of a rapid spread of the virtual - threading in practical processors implementations .",
    "we assume that one of the main pragmatic features of the proposed vthm architecture should be effective support of heterogeneous processors , where operating systems of native and legacy architectures may run simultaneously .",
    "therefore this architecture should supports advanced features of virtual machines analogous to those used in processors @xcite , with a purpose to supply an effective shared access control over physical resources at the hardware level . in architectures with virtual machines an overall management over all hardware resources is carried out by a special user , called thereafter the hyperuser .",
    "the resource management includes the creation of virtual machines , to which processors ( or quants of processor time ) , memory and io units are allocated .",
    "managing the execution of programs , including that of operating systems , is done by the virtual machine monitor @xcite , in such a way that these programs only have access to their allocated resources .",
    "let us define used in the vthm architecture the organization of the virtual machines management and organization of programs running on these virtual machines . in the conventional unix clone operating systems ,",
    "any work is performed on behalf of an authorized user , identified by the unique number - the user identifier ( uid ) .",
    "an operating system maintain accounts for all users and for all resources .",
    "it also provides a control over the users access to the resources .",
    "the special user identifiable by uid=0 , which is called a superuser or a system administrator , has an access to all resources .",
    "such an operating systems in the vthm architecture are run on their own virtual machines .",
    "accordingly , superuser identifiers on each one has the local for this machine uid=0 .",
    "the only one hyperuser has the global for all virtual machines uid=0 .",
    "the execution of any work is always implemented by the three major , essentially virtual , entities - programs , processes and threads , each of which has a dedicated representation in the computer memory .",
    "the term `` virtual '' reflects the fact that the entity has no a direct representation in the processor hardware , but the representation is created and destroyed by means of software in the memory .",
    "the work of a computer system providing for a simultaneous running is known as the multiprogramming mix management or shortly the multiprogramming .",
    "let us consider the basic concepts related to techniques of the hardware - software implementation of the multiprogramming in the existing computer architectures .",
    "the program is the static object representing the result of translation of the source code written in programming languages into object modules with subsequent merging into the executable file - the code file .",
    "the availability of the execution of a program by some user is controlled by the operating system .",
    "the single user task is implemented as a launching of the program s code file to run , which starts to execute an initial thread .",
    "the work of the thread , as of all created later ones , is the strictly sequential execution of the architectural instructions . in single - threaded processors",
    "such creation is done purely by means of software . in multithreaded processors",
    "there are special instructions , which provide the creation of new processes and threads @xcite .",
    "all threads that have been created in a process are executed in the context of this process .",
    "each thread one can consider as the elementary request of any work for using of processor or io device resources . by means of using a plurality of threads - requests , one can speed up any work , whose algorithm allows a decomposition in a form of concurrently running activities .",
    "a spawn of a new process results in the launching of its primary thread . running thread instructions can read and change the data in the context of thread s process - owner .",
    "this data is stored in the entities accessible on the isa level - ram and processor ore io units registers .",
    "the current context of the available objects of the process is maintained under control of operating system .",
    "in fact the process is the elementary request of a user s task to work with a subset of computer objects accessible by means of software .",
    "the process protects user tasks from uncontrolled mutual influences because of changing the data . at the same time",
    ", the processes can supply to each other parts of their own contexts with the purpose of sanctioned interaction . in the existing architectures",
    "shared objects are implemented through the creation in each process , which uses the shared object , its own virtual address - alias affiliated with the common physical address of the shared object .",
    "such object may be an area of non - resident memory , and as a result of the object swapping the memory management becomes significantly harder .",
    "this causes an increase in overheads associated with organization of multiprogramming .",
    "let us clarify the terminology associated with the organization of the multiprogramming in the existing and proposed architectures .",
    "the concepts of the process and the thread are commonly accepted in majority of operating systems papers .",
    "but with the appearance of multicore and multithreaded processors , the terminology associated with support of their running by means of hardware units , varies significantly from one manufacturer to another .",
    "below we define the relevant terms of process engine and thread engine .",
    "the process engine is defined as a collection of hardware features , which provide presentation of the separate process with ability to support the following features - a context which also called a protection domain and authorized controlled access to the context for other processes .",
    "this notion is close to the concepts of processor in the tera architecture @xcite and core in the architecture @xcite .",
    "the thread engine is defined as a collection of hardware features , which implement an execution of a separate software thread in the context of the process - owner and provide a necessary set of data types and instructions types at the isa level , sufficient to support correct and efficient synchronization .",
    "this notion is an extension of the concepts of hardware stream in the tera architecture @xcite and that of strand in the niagara architecture @xcite    using these two concepts of the process engine and the thread engine , the multiprogramming can be defined as athe concurrent ( or quasi concurrent ) running of the set of virtual entities - processes and threads - by means of a hardware objects set - process engines and thread engines .",
    "let us consider the principal difference of the multiprogramming implementation in the existing and proposed architectures . in the existing processor architectures ,",
    "the process engine and the tread engine are composed of functional logic units and at least of one file of the operational registers discussed above . in single - threaded processors",
    "all functional logic units work only with one operating registers file , which represents only one active process and one active thread . in multithreaded processors",
    "there is a plurality of operating registers file , which provide simultaneous running of the corresponding plurality of active processes and threads without their program implemented swapping .",
    "for example , tera mta @xcite and cray thread storm @xcite provides concurrent running of 64 processes and 128 threads .",
    "the number of the operating registers files is the principal architectural limitation of the processor , which determines the static number of process engines and thread engines and the corresponding number of a processes and threads capable to run simultaneously without program swapping .",
    "the operating registers files can be considered the highest and the fastest level of the microarchitectural memory of processor , whose special feature is the ability to represent and run active threads and processes .",
    "by contrast , in the proposed architecture the process engines and the tread engines are implemented in such a way that they independently support hardware swapping of the fine grained blocks of operation registers files .",
    "the immanent property of this swapping is its prioritized fine grained organization , such that registers file blocks associated with a thread of certain priority are unloaded into a lower level of the microarchitectural virtual memory if another thread with a higher priority requests the architectural register block when there is no free one .",
    "this swapping provides the deep virtualization of the process engine and thread engine and abolishes architectural restrictions on the number of hardware - supported processes and threads , which are immanent to all the existing architectures . in the proposed architecture , a plurality of hardware - supported processes and threads is determined only by resource constraints associated with the capacity of the executive pipeline to run the plurality of ones with a sufficient speed .",
    "let us consider the existing techniques of a inter - process cooperation by means of the shared data areas and describe how in suggested architecture these techniques is moved on the hardware level .",
    "the process is identified by the unique number , has the attribute of affinity to the launching user and inherits the latter s access rights to computer resources . as pointed above , the value of uid=0 identifies the hyperuser , which manages all computer resources .",
    "all processes which runs as as proxy for the hyperuser can create the virtual machines and provide their with dedicated resources . each process has the following main features :    \\1 .",
    "the context - a separate process is allocated its own context of available resources : its own virtual memory and objects identified by file names , the legal access to which is defined by the launching user ; this user is identified by the attribute uid of process ;    \\2 .",
    "the direct protection - all processes are protected from uncontrolled mutual influence ;    \\3 .",
    "the authorized access - all processes can give each other the rights of access to the resources of their own context , e.g. , can create shared objects ;    \\4 .",
    "the io related indirect protection - a process ca nt initiate an io which would lead to data changes in an other processes which are not announced to the process - initiator as shared .    in accordance with the optimality criteria laid down in the previous chapter , in the proposed architecture only the rare actions of creating processes and descriptive information about their resource access sharing",
    "are implemented by means of software .",
    "however the special innovative hardware features are used for simultaneous storing of an information about all created processes and for simultaneous dynamic access control on the basis of this information .",
    "the cost of these innovative features is radically lower than the cost of an entire processor , which is used in the existing combined hardware - software techniques for the same purposes .",
    "these features fully ensure the implementation of processors features presented above ( 1 - 4 ) .",
    "the similar using of the optimality criteria is adopted in the hardware thread control described in the following chapter .",
    "the mechanism of hardware access control can be described as follows .",
    "a process as the main object of management for a process engine is characterized by the following attributes :    pid - the process identifier that uniquely identifies the process to the process engine ;    pstat - the status of the process that determines the maximum status of the threads established in the process : hyper - privileged , privileged or non - privileged .",
    "three status levels are required in order to support the virtual machines at the isa level in the proposed architecture ;    pprior - the process priority , which determines the maximum priority of threads created in it .",
    "a thread as a primary unit of thread engine management is represented by the following attributes :    tid - the thread identifier , which uniquely identifies the thread for the thread engine . for clarity",
    "we assume that the most significant tid bits contain its process - owner identifier pid , and the remaining bits contain the local thread number tno in the process - owner ;    tstat - the status of thread , which - just like the status of a process - can be hyper - privileged , privileged and non - privileged ,    tprior - the priority of a thread , which determines the order of processing of the information related to the thread under a competition over any physical resources .",
    "the functions supported by the thread engine at the hardware level , include threads creation and completion , prioritized performing of calculations in all created threads , and organization of interaction between software threads as well as between software threads and io threads .",
    "launch of threads is executed either automatically when the `` power '' or `` reset '' buttons are pressed , or by means of a program by issuing relevant commands by a parent thread . in the first case ,",
    "the so - called bootstrap threads are created , which have the hyperprivileged status .",
    "the hardware reset mechanism is implemented in such a way that the creation of bootstrap threads and the creation of specific bootstrap processes are performed as an atomic action .",
    "the context of this processes is the full context of physical resources of a machine available at the software level .",
    "the execution of threads at the isa level boils down to their processing in two states - active and waiting , as well as their transfer between these two states . in the active state thread",
    "s commands are executed with the rate , which is determined by an overall number of threads in the active state as well as a number of hardware elements of thread engines capable of working in parallel .",
    "the emerging competition between several requests over the right to be served by elements of thread engines is organized by assigning the global priority to each atomic request and by using the buffer pool of queues at the entrance of each microarchitectural unit with independent activity .",
    "these queues are ordered according to the priority and the arrival of its elements .",
    "in contrast with such a fine grained representation , the existing mixed software - hardware thread s management uses coarse grain representation , in which the priority of a thread is associated only with software visible thread descriptor .",
    "this leads to the emergence of a particularly unpleasant for real - time phenomenon known as priority inversion @xcite .",
    "its essence is such that if a highest priority thread became ready to run when all thread engines are busy , it is necessary to perform swapping of the descriptor of this thread with a descriptor of another lower priority thread . in such cases ,",
    "the operating system dispatcher is faced with a dilemma - either to perform swapping immediately or allow a lower priority thread to complete a time slice previously allocated by a scheduler and allow priority inversion during this time period . in the first case , overhead cost associated with switching at the expense of forced reduction of time slice , is increased . in the second case , the time of reaction decreases , which is crucially important for real time system and can forces its wrong functioning .",
    "the presented above fine grain thread representation and management automatically eliminates the priority inversion .",
    "having described the main objects of process and thread engines management , let us define how the information about these objects and about their access control can be compactly represented in the hardware .",
    "the widespread unix - cloned operating systems @xcite , @xcite use file names to identify hardware computer devices visible at the isa level , as well as to identify software - generated objects . a set of file names of the all objects can be uniquely mapped by an operating system to a set of a numerical file identifiers fid , which will represent all objects in a computer system .",
    "having made the fid as the part of the defined below access controlled virtual address , it is possible to ensure an effective hardware dynamic control of access to all software and hardware objects of a computer .",
    "the introduced below concept of access controlled virtual addressing is the development of the structuring of virtual address space used in the unix - like operating systems @xcite , @xcite . in these operating systems",
    "the virtual spaces of the kernel and user processes are deferred only by a few most significant bits of a virtual address .",
    "an improvement , which is used in the vthm architecture , is that the process - destination of addressing is selected according to the number of the most significant bit of a thread - generated address , thereafter called the access controlled virtual address ( acva ) .",
    "if this bit , which we thereafter call vashr , has a value of 0 , the process - destination of addressing is the thread - owner process .",
    "this address is called thereafter the local acva and is represented by the two - component record    ( vashr=0 , lva ) , where lva is a virtual address in the process context .",
    "correspondingly , the non - local address which is called thereafter the shared acva , is represented by the four - component record    ( vashr = 1 , opid , refpid , lva ) ,    where opid is the pid of the process - owner of the address , refpid is the pid of the process - owner of the referencing thread , and lva is the virtual address in opid process context .    using this mode of addressing , it is possible to consider that any shared object , which occupies l bytes of memory ,",
    "can be represented on the hardware level by the following record    ( vashr = 1 , opid , lva , l ) .",
    "the dynamic access control to the shared objects can be implemented in such a way that the local address acva generated by any thread of refpid process , which is identified by the zero value of vashr , is controlled by the virtual memory management unit ( vmmu ) in the same way as in the existing architectures .",
    "if the thread of the process with the identifier refpid refers by the shared acva , a reference implementing transaction is issued into the vmmu with the shared acva looks like    ( vashr = 1 , opid , refpid , lva , lref ) , where lref is accessed data length .",
    "the transaction includes the corresponding to the referencing instruction access code refmode - read , write , execute or synchronization atomic access .",
    "such improved vmmu , thereafter called the memory and io management unit ( miomu ) , has the access control directory .",
    "this directory contains a set of access admitting records looks like    ( opid , gntpid , orva , l , gntmode ) .",
    "each such record defines that all threads of the process with the identifier gntpid have the ability to access the virtual memory area [ orva , orva+l-1 ] of the process with the identifier opid with the instruction codes refmode matching to the gntmode field .",
    "gntmode is the mask which defines the subset of allowed referencing operation - read , write , execute or synchronization atomic access .",
    "the transaction is performed if there is at least one appropriate record in the miomu .",
    "it should be noted that the processes ( more precisely , threads ) , which have the hyper - privileged status ( more precisely , threads with this status ) can apply to all facilities accessible at the isa level at their physical addresses without any control .",
    "the physical address can be presented as an address pair ( asi , pha ) similarly to the sparc architecture @xcite    on the basis of the defined above acva in the vthm architecture an advanced technique of direct memory access ( dma ) between any process memory and any io devices is implemented .",
    "this technique supports the mentioned above indirect protection associated with input - output . using the introduced above notations , we assume that the control registers block of any independent activity channel in any io device can be represented by means of acva as a shared object ( vashr = 1 , opid=0 , lva , l ) , the owner of which is the hyperuser represented in the microarchitecture by the process identifier pid=0 .",
    "if threads of a process will work with such a shared object , one of these threads should get an access to the object by using a corresponding software executable system call .",
    "if an operating system permits this request , it will create the permitting record in the miomu and will return the shared acva of the io unit channel control registers block .",
    "similarly , it is possible to grant access to the same io unit for several processes .    using the returned by the system call acva",
    ", the process can directly program any io unit by means of writing into the control registers of such unit without using a driver and an operating system service .",
    "in particular , a process can define the access controlled addresses of dma exchanges in the its own local or shared memory .",
    "since any operation of writing into the register is always executed by some thread of the process with the refpid identifier , the miomu can remember this identifier , thus noting that the io unit acts on the orders of said process and hence all its dma exchanges are performed identical as program threads requests .",
    "further , the miomu channel dedicated for input - output device will form requests over access controlled virtual addresses in the same manner as in the method of program threads described above .",
    "the preceding chapters 3,4 describe the two innovative decisions , which constitute the basis of the vthm architecture - the fine grain direct hardware multiprogramming and organization of inter - process communication by means of access controlled virtual addressing ( acva ) .",
    "the current chapter presents the third basic decision - the direct hardware threads synchronization .",
    "the refinement of these three decisions is presented in chapter 7 in the context of the vthm processor reference model description .",
    "the possibility of homogeneous interaction of threads belonging to different processes , implemented in the vthm architecture , makes it possible to improve the classical dijkstra s @xcite approach of semaphore - based synchronization and to use it for the effective hardware synchronization of the gppc .",
    "the proposed improvement is that the semaphores in the vthm architecture are implemented as active microarchitectural units , which are built into the miomu .",
    "these units , which are thereafter called the hardware - driven semaphores ( hwds ) , form a special microarchitectural independent activity units pool , thereafter called the hwds pool .    in accordance with the fundamental work of dijkstra @xcite ,",
    "describing the essence of interaction of sequential processes , the correctness of each synchronization protocol should remain valid when the agents relative running rates are changed within the range from the zero to the infinity .",
    "the requirement of the protocol efficiency is that the overheads of the synchronization should be minimal .",
    "basically it boils down to the necessity to ensure that a thread waiting for an event associated with synchronization , does not occupy a processor .",
    "these requirements are fully implemented in the set of hardware features of the vthm architecture described below .",
    "an additional benefit of these features is that they free up the processor from switching threads between the waiting and activity states .",
    "these features at the isa level are presented by the following six instructions - semaphoreget , semaphorefree , semaphorelock , semaphoreunlock , semaphorewait and semaphorepass .",
    "the semaphoreget instruction fetches a free hwds from the pool , sets it into an initial state and returns its access controlled virtual address ( acva ) .",
    "this address defines the semaphore as a part of context of the process , which is the owner of the thread issuing the semaphoreget instruction . below this address",
    "will be used as the main operand in the remaining synchronization instructions .",
    "the semaphoreget instruction returns the empty result if there is no a free semaphore .",
    "the semaphorefree instruction makes the semaphore ready for reallocation .",
    "the hwds pool , similarly with the pool of created threads , is implemented as virtual in the vthm architecture .",
    "term virtual here means that hwds pool is implemented as a multilevel facility similar with the described above vthm register files - the elements of pool can be swapped between levels purely by microarchitecture .",
    "the hwds has a register memory , which can be viewed as a set of described below structured variables , which are changeable only by means of microarchitectural features .",
    "the mutex variable of the semaphore is an analogue of the mutex variable in modern unix - like operating systems @xcite , @xcite .",
    "this variable contains the semaphore - owner field , which has an empty value if the critical interval guarded by the semaphore is free , or the identifier tid of the critical interval thread - owner .",
    "the mutex variable also contains the pair of fields which have an empty value or define the fifo threads queue , which is ordered by priorities and issuing times of the semaphorelock instruction described below , in a situation when a critical interval guarded by the semaphore is busy .",
    "the event variable is an analogue of the conditional variable in the mentioned above operating systems .",
    "it contains the pair of fields , which either have an empty value or define the queue of threads , which executed a semaphorewait instruction in the critical interval guarded by the semaphore , and are ordered by priorities and issuing times .",
    "the semaphore also contains the counter variable , which obtains an initial value by performing the semaphorelock ore semaphorewait instructions . immediately after being set",
    "the counter variable starts to decrement with some frequency purely by means of hardware .",
    "its turning into the zero before the issuing of semaphoreunlock or semaphorepass instructions over the semaphore , forces completion of semaphorelock and semaphorewait instructions with the corresponding completion code .",
    "the semaphorelock instruction provides an entry of a single thread into the critical interval guarded by the semaphore passed as the instruction s parameter .",
    "if such attempt is performed by an another threads before the first thread has left the critical interval by means of issuing the semaphoreunlock or semaphorepass instructions , these threads will stand in the tail of waiting queue , which is identified by the mutex variable of the semaphore .",
    "the semaphoreunlock instruction withdraws the issuing thread from the critical interval , thus allowing a new thread to enter the critical interval ; this instruction is essentially the `` close parenthesis '' for the semaphorelock instruction .",
    "the semaphorewait instruction atomically performs an action of semaphoreunlock instruction and moves the issuing thread to the tail of waiting queue , which is identified by the semaphore event variable .",
    "the term `` atomically '' here and thereafter designates the sequence of actions , which are indivisible at the isa level for threads cooperating via the same semaphore .",
    "the semaphorepass instruction atomically performs the following actions :    - removes the instruction issuing thread from the critical interval ;    - browses the waiting queue related to the event field , and if this queue is not empty , enters the first thread from this queue into the critical interval ;    - if the queue related to the event field is empty , enters the first thread from the queue pointed by the mutex field into the critical interval ;    - if both queues are empty , makes the critical interval free .",
    "an introduction of the four synchronization instructions instead of the two proposed by dijkstra @xcite analogues of instructions semaphorelock and semaphoreunlock , is stipulated by the following considerations .",
    "the thread , having executed the instruction semaphorelock , can find the lack of data for processing . in this case",
    ", the thread must re - enter the critical interval after having been waited for some time .",
    "the number of such transitions is not known in advance . therefore the real time system dilemma is appeared again - either to perform a polling frequently thus increasing overheads or perform a polling rarely thus increasing a probability of loss control .",
    "the implementation of the pair of additional commands semaphorewait and semaphorepass in the isa permits to enhance significantly the efficiency of synchronization in special cases of waiting for an event of appearing the first element of information .",
    "it should be noted that semaphoreunlock is the special case of semaphorepass instruction .",
    "we suppose that it instruction is useful for simplest microprocessors in which sophisticated semaphorewait and semaphorepass instructions may be omitted .    the c language source code shown on figure",
    "1 explains the usage of synchronization instructions proposed above in order to program the classical supplier - consumer algorithm .",
    "the procedure vthmsyncdemo starts a pair of threads by means of the createthread system call ; it also creates a semaphore for their interaction an also completes the task .",
    "it seems that the comments accompanying the c code illustrate the logic of the algorithm as well as the logic of used vthm isa synchronization features in sufficient details .    to sum up everything described in this chapter",
    ", it is important to note that the hwds as the active microarchitectural level unit allows to eliminate the important bottleneck present in the modern day implementations of gppc - the centralized scheduling and dispatching of a set of threads by means of an isa code of an operating system kernel .",
    "essentially the hwds performs the function of local scheduling of the access to the critical interval associated with a semaphore and in cooperation with the microarchitectural sequencer unit described below in chapter 7 , implements the local dispatching of the subset of threads , interacting through the semaphore .",
    "the hwds while implementing hardware timing of waiting states associated with critical intervals , also significantly improves the centralized operating systems @xcite , @xcite time service , making it per - semaphore distributed and purely microarchitecturally implemented .",
    "this chapter describes the innovative technique of io programming based on the introduced above hardware driven semaphores ( hwds ) , which allows to eliminate the second , and seemingly the last , bottleneck in the existing gppc implementations - the centralized support of the io programming by means of an operating system isa code , which uses interruptions .",
    "this technique is demonstrated on the basis of the simplified variant of supplier - consumer algorithm described on the figure 1 of previous chapter .",
    "the threads discussed in this algorithm perform the processors instructions and thus are essentially computational threads ; the algorithms of their cooperation are completely symmetrical .",
    "the data exchange with external devices is performed by means of hardware threads , thereafter called the io threads .",
    "the principal feature of the io threads is their total subordination to the computational threads .",
    "this subordination means that in a correctly designed and properly functioning software - hardware environment nothing activity of io devices can occur without the computational threads control .    in more detail ,",
    "after resetting a computer all io units are reseted into the idle state .",
    "an exit of these units from this state occurs only when a program thread has written into a transfer control registers the following information : an io operation code of the block to be transfered , its size and addresses in the process - owner memory and in the space of io unit .",
    "having obtained this information the io unit starts the execution of the io thread , which performs the data transfer operating in the direct memory access mode ( dma ) and informs about the completion of the transfer by issuing a processor interruption . in existing architectures",
    "this interruption is processed by means of an operating system kernel and device driver .",
    "the final result of this software and hardware handling of interruptions is the transfer of the io related thread from the waiting state into the active state .",
    "there are three alternatives for the data exchange completion - the normal transfer of the data block , the transfer with errors and the completion upon time - out .    using the principal feature of the vthm architecture , namely its ability to support effectively the execution of a virtual set of threads , it is possible to suggest the following simple and effective technique for organizing the data exchanges without interruptions as follows .",
    "so - called the dual computational thread is created for each io unit channel with an independent hardware activity .",
    "this thread initiates and completes active phases of the affiliated io thread for each data exchange .",
    "if a system provides a unique identification of io unit , which has completed the transfer ( for example , the protocol msi of pci express bus ) , the dual computational thread and io thread can only interact with each other . if a legacy interruptions technique is implemented via a fixed number of lines as in the pci bus protocol and protocols of older buses , then for each interruption line the computational dual thread - multiplexer is created . on one hand",
    ", this thread interacts with the interruption delivery logic and executes an interruption confirmation protocol , and on the other hand this thread interacts with a set of dual computational threads of all io units , dedicated to the same interruption line .",
    "the technique of semaphore - based synchronization described in the preceding chapter allows to completely eliminate the program processing of interruptions by the kernel and the drivers of operating system .",
    "the basic idea is that an io interruption processing hardware is supplemented by a special unit , thereafter referred to as the interruptions control unit ( icu ) .",
    "this unit organizes an interaction of io devices with the dual thread by means of the access controlled virtual address ( acva ) of semaphore , which address is stored by the computational thread at the dedicated register of the icu . in terms of the discussed above supplier - consumer algorithm ,",
    "the simplest scheme of a purely hardware - based interruptions processing is as follows .",
    "each interruption to be processed in accordance with this protocol is represented by the interruption control block ( icb ) defined below .",
    "these blocks are cells of specialized units built in the miomu .",
    "they are analogues of the icb and the hardware driven semaphores described in the previous chapter .",
    "these blocks are allocated and released by means of the geticb and freeicb instructions respectively .",
    "the role of the supplier of interruptions is performed by the icb , and the role of the consumer of interruptions is performed by the dual computational thread .",
    "the exchange buffer and its descriptive information boil down to the binary counter allocated in the icb .",
    "shortly , the dual thread writes its own thread identifier , the zero counter value and the semaphore address , into the icb block .",
    "in the simplest case of pci interruption delivery protocol this information is inta - intd lines number . in the more sophisticated case of msi interruption delivery protocol ,",
    "this information comprises of an interruption related bus , unit and function numbers .    at the moment of interruption arrival , the supplier issues the semaphorelock synchronization instruction into the miomu .",
    "this instruction uses the address of the interruption - dedicated semaphore stored in the icb , as the first operand .",
    "then after having entered the critical interval , the supplier normally discovers the zero value of the icb counter , changes it to the non - zero value and issues the semaphorepass instruction . by this moment",
    "the dual thread will certainly be in the state of waiting associated with the semaphore event as a result of execution of the semaphorewait instruction .",
    "correspondingly , having found the non - zero value of the counter , which corresponds to the interruption processing cycle unfinished by the dual thread , the supplier will perform atomically the sequence of actions , which comprises of issuing the semaphorewait synchronization instruction into the miomu and switching the supplier thread from active state into waiting state .",
    "after that the consumer , whose role is performed by the dual thread , is transferred into active state , following a completion of the semaphorewait instruction whose result indicates the arrival of the icb related event .",
    "having analyzed the icb counter and having normally found the non - zero value , the consumer rewrites the interruption related information into the icb related program buffer , performs the interruption confirmation protocol , assigns the zero value to the icb counter , and notifies the supplier about the release of the interruptions delivery logic by means of issuing the semaphorepass instruction .",
    "it should be noted that in modern microprocessors , in particular in the x86 family , the delivery of inter - processor interruptions is implemented uniformly with the delivery of interruptions from io devices by means of the apic unit .",
    "it allows to extend the io interruptions processing techniques described above onto the inter - processors interactions and eliminate the interruption processing .",
    "it letting to introduce the new of a programming of an io and interprocessor communication , which in a steady state of control over a plurality of created threads and processes is free of interruptions , operation system and drivers support .",
    "this chapter presents the vthm processor reference model . in this model",
    "the main microarchitectural units and logic of their interaction at the minimal refinement level , necessary for precise illustration of all described above decisions in commonly used terms of the circuit engineering , are described .",
    "figure 2 presents the general block diagram of processor in the vthm architecture .",
    "functions of the described above conceptual entities - the process engine and thread engine - are implemented as the distributed protocol by means of a hardware units .",
    "the vthm processor consists of a set of thread monitors , a set of domain executive clusters and a single combined memory and input - output management unit ( miomu ) which supports data transfers based on the access controlled virtual addresses ( acva ) introduced in the chapter 3 . with the purpose of the effective support of all kinds of software debugging ,",
    "the hardware debugging monitor is used .",
    "all devices interact through a processor network , which is the broadband packet switching network .",
    "the principal feature of this network is the hardware - implemented priority scheme , in which a multi channel router supports the transmission of all information elements between all other units in accordance with the priorities of the producing threads .",
    "as it will be explained in the next chapter , the vthm processor microarchitecture is essentially the dataflow machine , which is well suited for implementation in silicon using the processor - in - memory ( pim ) technology .",
    "therefore at the microarchitectural level all units , including the processor network , have a microarchitectural virtual memory .",
    "this memory is used for hardware swapping of microarchitectural information in accordance with its priority and the aging metric in circumstances of competition for levels of microarchitectural memory , processing units and transfer logic of the network .",
    "this memory is shown on all the presented below figures as a local memory , and is not mentioned further in the description of these devices .",
    "a swapper unit implements the interaction over exchange of information with connected to this memory devices as well as the movement of information between the levels inside the microarchitectural virtual memory .",
    "the vthm processor has the following external interfaces .",
    "the dram - io interface of the miomu unit provides for an interaction with the main ram and io units .",
    "the numa - mp interface provides for the merging of the vthm processor into multiprocessor systems with the non - uniform memory access ( numa ) .",
    "the jtag interface is used as usual for the debugging interaction with the processor s hardware .",
    "it is also used as an interface of interaction of the debugging monitor with corresponding software , executable on the instrumental machine .",
    "figure 3 shows the thread monitor that consists of a sequencer and a transaction issuing unit .",
    "the sequencer consists of an operating registers block for storing the roots of thread descriptors and of a scheduler unit .",
    "the monitor provides an automatic creation of threads after hardware reset of a computer and a programmed creation of a new thread while performing an isa instruction in accordance with described in the chapter 5 .",
    "creation of a thread in the microarchitecture level boils down to the issuing a request for the establishment of a process descriptor into the miomu .",
    "if the positive acknowledgment is accepted , the monitor creates a root of the thread descriptor block in the sequencer s operating registers block .",
    "this root contains the minimum of information necessary for issuing a new transaction in accordance with the result of the completion of the previous transaction .",
    "an additional information , which is not included into the root , is stored in a distributed form in the different executive clusters and in the miomu . for example , isa register files can be distributed as follows .",
    "the integer registers are placed in the cluster of integer arithmetic , and the registers for the floating point operand are placed in the floating - point arithmetic cluster .",
    "the information stored in the root essentially depends on isa implemented by the monitor . at the very least",
    ", it contains the thread identifier tid , priority , status of privileges , counters of current and next instructions and the completion codes register for performing conditional branches .",
    "the roots of the thread descriptors are placed in the operational registers blocks which are organized in waiting queues of two types . in the first queue",
    "the descriptors of treads , which wait for scheduling the performance of the next transaction , are stored . in the second queue",
    "the descriptors of the treads , which wait for completing the earlier issued transaction , are stored .",
    "the scheduling of the instruction for execution boils down to fetching an isa code segment in accordance with the current value of instruction counter , which is required for formation of a transaction .",
    "the transaction represents a set of architectural instructions to be executed and an information dependency graph , which specifies a partial order of execution of this set of instructions .",
    "this complexity in the vthm architecture is used to ensure speeding up the execution of high priority thread instructions in accordance with wliw and super - scalar approaches .",
    "an important feature of the vthm architecture is its ability to implement a fine grained dynamic physical resources allocation in accordance with actual demands of a code scheduled for execution .",
    "this is completely analogous to the dynamic allocation of virtual memory pages demands .",
    "for example , an interruption handler for the risc architecture , well - coded in assembler language , can use only the global registers .",
    "therefore hardware resources for other isa resources will not been allocated for the interruption handler thread in the vthm microarchitecture .",
    "the scheduler knows the isa of instructions stream , and in accordance with the size of transaction and with the semantics of its instructions , the scheduler defines microarchitectural resources necessary for its performing .",
    "if another transaction accesses the microarchitectural resources that are not yet allocated , the scheduler issues the special transaction into the executive cluster or into the miomu for their allocation .    the scheduler stores the formed transaction into the prioritized queues buffer pool placed in the transaction issuing unit .",
    "this unit supplements the transaction with information , which on non - optimized conceptual level contains the following elements :    - the thread identifier tid necessary for unique identification of the transaction and for its each instruction within the framework of a distributed protocol , performed by a set of the microarchitectural units ;    - the architectural number of each instruction necessary for the precise localization in terms of the isa instruction , which caused an abnormal situation ;    - the priority of the thread - owner , which determines an order of the thread s elements servicing in the all competition circumstances .",
    "the transactions dispatching unit sends a transaction for the execution as well as receives its results upon the transaction completion .",
    "this unit interacts with the scheduler via the prioritized queues buffer pool placed in the operating registers of the transaction issuing unit .",
    "the result of the transaction completion is stored in the field known to the scheduler .",
    "the scheduler makes free all the operational registers blocks have been occupied by the completed transaction after adoption the transaction s completion result .",
    "the use of transactions allows to localize an execution of an isa code fragments in a separate domain executive clusters , thus significantly reducing the traffic in the vthm processor network .",
    "the good example in this respect is the implementation of all functions of a unix like operating system string processing libraries as single transactions , which can be fully carried out by an integer arithmetic executive cluster . at present the whole processor",
    "is required to perform the functions of these libraries , written more than 30 years ago . at the same time because of the achievements in digital circuit engineering , significantly more complex usb @xcite and sata @xcite protocols are implemented by a very simple and inexpensive controllers .",
    "obviously , the development of a string processing cluster or a specialized unit within the miomu should significantly improve the efficiency of the gppc implementations .",
    "this example demonstrates the fact that the vthm architecture allows the very effective usage of both cisc and extended cisc architectures , which are now considered as erroneous .",
    "this efficiency would be achieved by means of distribution of parallel execution of a large number of architectural instructions streams .",
    "it seems that a further research in this direction could contribute towards significant acceleration of a computation of any nature on the basis of its transfer to the isa implementation .",
    "figure 4 presents the block diagram of the domain executive cluster .",
    "the cluster consists of a sequencer and an asynchronous executive pipeline .",
    "the sequencer consists of a mapping unit , a register file of waiting queues and a scheduler .",
    "the executive pipeline consists of a register file ( rf ) of instructions , a register file of operands , a set of functional executive units ( feu ) and a load - store unit ( lsu ) .",
    "the register file of instructions contains a buffer pool of ready - to - run instructions queues , which are ordered by instructions priority and arrival time .",
    "the sequencer accepts the transactions - requests and rewrites them into the queue .",
    "these transactions contain the instructions to be performed in this cluster and the information dependencies graph describing a partial order of instructions execution . among such instructions",
    "there could be instructions of local jumps within the transaction .",
    "an execution of jump instructions beyond the instruction block of a current transaction or an execution of the last instruction of the transaction leads to finalization of the transaction processing in the cluster and to returning of the result back to the thread monitor which issued the transaction .",
    "the threads monitors pool and the executive clusters pool together provide the two - level scheme of execution of instructions , which is efficient for the realization of heterogeneous processors with shared executive pipelines for legacy and native isa .",
    "the execution of a local conditional jump instruction allows to aggregate the instructions and thereby to reduce the intensity of a traffic in the processor network .",
    "the mapping unit performs the replacement of architectural registers addresses with the microarchitectural addresses of the cluster s register file allocated for the instruction s thread - owner , and transfers thus prepared instructions to the scheduler .",
    "essentially the scheduler performs functions of distributed dispatching on the microarchitectural level ; in present architectures these functions are performed by means of operating system isa code .",
    "the principal difference is that instead of the known coarse grain program switching of threads between active and waiting states as an indivisible entity , a set of the microarchitectural vthm processor s schedulers performs the fine grain fragment by fragment switching and executing of the threads .",
    "such a switching on the transactions performing level boils down to moving ready - to - run instructions ( also called active instructions ) from the scheduler waiting queues into the executive pipeline input queues and moving in the reverse direction those instructions , which turn into the waiting state until their operands become ready or execution of the instruction become completed .",
    "similarly the schedulers of described above thread monitors work at the threads performing level - the threads correspond to the transactions , and the transactions correspond to the instructions .",
    "the generalized effect of such a fine grain performance in the vthm architecture is described above in the chapters 2 and 3 .",
    "the scheduler uses the information dependencies graph , contained in the transaction , for fetching the next ready - to - run instruction .",
    "the executive pipeline corrects this graph after execution of each instruction . following completion of the transaction ,",
    "the resources , which have been used for storing the instructions and the graph , become free . in case of the appearance of a higher priority transaction",
    ", the scheduler can force out an instruction of a lower priority transaction from the input queue of the executive pipeline and place it back into the waiting queue .",
    "instructions and elements of information dependency graph , which remain inactive for a long time , can be forced out into the lower level of the local microarchitectural virtual memory of the cluster .",
    "the operational registers block allocated to any thread can be forced out after the last instruction that used this block , has been forced out .",
    "the load - store unit ( lsu ) provides the data exchange between microarchitectural operating registers and the miomu .",
    "usual memory access instructions , in particular those in systems with non - uniform memory access ( numa ) , may be very long lasting .",
    "essentially the synchronization instructions are the specific memory access instructions , which force the hardware driven semaphore ( hwds ) logic to perform the actions described in the chapter 5 and to move the instruction into the state of waiting for the reply from semaphore logic .",
    "such waiting states are the states of algorithmic latency described in chapters 1 and 2 , which may be very long lasting .",
    "this can force the downloading of all distributed thread representation elements from the miomu , the executive clusters and the thread monitors into the lower levels of microarchitectural virtual memory .    at some moment",
    "the hwds will post the synchronization instruction completion code into the load - store unit .",
    "this event will induce the predefined by instructions sequence of uploading of the distributed threads representation elements into the highest level of memory , e.g. , into the operational registers .",
    "these uploading will force the execution of the next steps of the thread running by the set of microarchitectural schedulers and executing pipes .",
    "figure 5 presents the block - diagram of a memory and io management unit ( miomu ) .",
    "this unit consists of an access validation unit , a acva translation unit , a synchronization unit , and a physical memory and io control unit .",
    "the later consists of a routing unit , a block processing unit and an interruption unit .",
    "the miomu performs two types of transactions .",
    "transactions of the first type present a single access of the threads monitors or executive clusters via of access controlled virtual address introduced in the chapter 4 .",
    "such address arrives simultaneously into the validation and the translation units .",
    "the validation unit matches associatively this address with the contents of its access control directory and on the basis of this matching it issues the signal of access permission or prohibition into the translation unit .",
    "the access matching procedure have been detalized in the chapter 4 .",
    "the translation unit performs the conversion of the access controlled virtual address into the physical memory address or the io unit registers address .",
    "if the validation unit issued the signal of the access permission , the translation unit performs the corresponding access operation or informs about the abnormal completion of the operation .",
    "the functions of the translation unit of the vthm processor are similar to the functions of the existing processors virtual memory management units ( vmmu ) but unlike the latter they use the advanced translation directory in order to provide simultaneous support of contexts of all created processes .",
    "the validation unit is one of the most important innovative elements of the vthm architecture .",
    "it should be noted that all permitted accesses to the present virtual memory pages , processor and io units registers are performed very fast by means of the direct hardware implementation .",
    "a program support is required only for rare accesses to the pages of virtual memory that have been unloaded from ram , and for unauthorized accesses , resulting in an abnormal termination of a thread .",
    "the synchronization unit consists of a multi - channel hwds engine and a semaphores register file .",
    "the hwds engine channels perform the synchronization instructions discussed in chapter 5 .",
    "the register file consists of semaphore cells , which contain the structural variables , semantic of which is described above in chapter 5 .",
    "the thread descriptors queues are placed in the miomu local virtual memory unit , and their exchange with the latter is supported by the swapper unit .",
    "the transactions of the second type have the structure which is fully analogous to the structure of the transactions described above , which are coming into the executive clusters , with the only difference that they contain specific instructions , thereafter referred to as the block processing instructions .",
    "such transactions are performed by the block processing unit , which essentially is an improved version of the existing direct memory access units .",
    "this unit allows to implement the data block transfer operations of memory - memory , memory - io and io - io types at the microarchitectural level . in the existing architectures the functionality of these transactions",
    "is performed by means of software and takes a long time of a processor integer unit work , during which floating point units remain idle . moreover , the encapsulation of the block processing in the miomu significantly reduces the processor s network traffic . as a very important and natural function of the block - processing unit it is logical to suggest the microarchitectural implementation of the virtual memory pages swapping .",
    "the routing unit provides the redirection of the requests by a physical address into the appropriate physical unit - the executive cluster , a memory bank or the io control unit .",
    "the physical memory control unit provides the data exchange with the memory banks .",
    "in addition to the known functions of the io handling , the physical memory and io control unit of the vthm processor performs the discussed in the chapter 6 protocol of transformation of interruptions into the synchronization operations using the hardware driven semaphores .",
    "these additional functions are implemented by a multi - channel active unit thereafter called the interruption unit .",
    "the interruption unit interacts with the dual threads using the interruption descriptors pool , each element of which represents the interruption allowed to be handled .",
    "the dual threads activate these descriptors similarly to the semaphores described in chapter 5 .",
    "these threads write the tid of io thread , its priority , the address of the semaphore and the zero value of counter into the appropriate fields of the interruption descriptor . writing this value",
    "is the activation signal , which forces the channel of interruption unit , affiliated with the interruption descriptor , to perform the protocol of the io processing without program interruptions handling as described in the previous chapter .",
    "concluding our description of the vthm architecture , we summarize the main decisions , present pragmatic factors that define its viability and a possibility of a rapid spread in practical implementations and also define its place in modern taxonomies .",
    "the main positive feature of the proposed architecture is the high level of generalized latency tolerance defined in chapter 1 .",
    "this tolerance neutralizes the all factors noted by sterling @xcite , which reduce the real speed of a computations as follows .",
    "latency and overheads are minimized by the fine grain and deeply virtualized representation of the isa in the microarchitecture .",
    "the starvation is precluded by the creation of a virtual set of active threads allowing to create the intensive flow of instructions sufficient to saturate all of the processing devices .",
    "the competition is organized properly by means of supporting the global priority processing discipline at the hardware level .",
    "this discipline ensures that parallel threads running without the priority inversion .",
    "the very important positive side effect of the proposed architecture is the considerable simplification of programming due to abandonment of the concept of interruptions in organization and programming of io in operating systems .",
    "the major pragmatic features , which determine viability and a rapid spread of vthm architecture , is ensuring a continuity in the relation to the existing software and the ability to create in bootstrap mode the new , more efficient , system and application software .",
    "these features can be achieved through the implementation of heterogeneous processors , which would support the existing legacy isa with the high degree of precision and would provide to realise their native vthm isa .",
    "operating systems for a legacy isa can be executed on virtual machines that are implemented in the vthm architecture fully by the means of hardware .",
    "new operating systems and applications software for the native vthm architecture should provide a better performance in the case of reprogramming of old tasks .    the important advantage of performing legacy applications under control of legacy operating systems at the heterogeneous processors is the possibility of direct use of the virtual - threading features in the important and widely used applications such as database servers and search engines .",
    "these features include the hardware synchronization of threads and the direct io programming without participation of the legacy operating system , which can simply be not aware about the existence of these features . in the legacy isa",
    "they can be represented as a special io unit , which can be considered as a virtual - threaded co - processor .",
    "this unit will transfer the transaction which was formed by means of legacy isa program , will send it into the necessary executive cluster and will return the result back to the program .",
    "the same mechanism can be used to an information exchange between applications on different operating systems by means of the access controlled addresses introduced in chapter 4 . the ability to work effectively and independently for a plurality of operating system on a single vthm processor",
    "simultaneously is provided by the hardware implementation of well - known concept of virtual machines .",
    "it seems that the concept of heterogeneous processors based on the vthm architecture , where different isa programs can effectively communicate through the shared memory , should cause a review of existing assessments of a number of architectural solutions as erroneous .",
    "for example , the implementation in a powerful microprocessor , dedicated for network information retrieval systems servers ( web queries ) or a large database system , combination of the risc and cisc ( and even extended cisc ) isa can lead to the following .",
    "an interaction with the network in part of receiving requests and issuing responses can be supported by risc applications .",
    "processing of requests related to fetching information from large databases will be performed by extended cisc applications where relational database model operations @xcite will supported on the isa level . in advanced implementations of such systems the functions of artificial intelligence - e.g. , identification of semantic queries using knowledge database or translation from one language to another - can be additionally implemented . in these systems",
    "support of specialized languages such as lisp @xcite , refal @xcite and prolog @xcite is justified .",
    "it seems that vthm architecture makes need to review a large number of decisions in processors architecture , which now are considered as erroneous .",
    "the most important in this respect , proprietary the vthm architecture is the significant improvement of mta - architecture , which can eliminate virtually all its shortcomings .    finally , let s define the place of the vthm architecture in the existing taxonomies . with respect to the flynn s classification of parallel processors",
    "@xcite , it is naturally to consider the vthm architecture as an extension of mimd architecture and to call it vivd architecture - the machine with virtual instructions and virtual data streams .",
    "the vthm architecture extends the existing classification of processors with respect to the hardware virtualization and makes this virtualization two - dimensional .",
    "indeed , in respect of memory virtualization on the level of isa existing processors are divided into two classes - with the physical memory ( phmem ) and with the virtual memory ( vmem ) .",
    "it should be noted that we understand the virtual memory as the hardware features at the isa level , which support context protection domains as well as translation of a virtual address into a memory or a register physical address .",
    "the cashing features on the microarchitectural level are considered as irrelevant to virtual memory .",
    "processors of phmem and vmem classes can support two classes of threading .",
    "first class we propose to call as the processors with the physical threading , which includes the known single- , hyper- and multi - threaded ones .",
    "second class we propose to call as the processors with virtual - threading , which we introduce in this paper .",
    "we use fore these two classes abbreviation pth and vth respectively . thus with respect to the virtualization , similar to the flynn s classification in relation to parallelism",
    ", we propose to define the four classes of processors :      vmem - pth - the von neumann machine with the virtual memory and the physical threading , which began to crowd out early phmem - pth machines in the mid-60s and has been used in the majority of modern processors .",
    "phmem - vth - the machine with the physical memory and the virtual - threading at the isa level , based on the virtual memory of the microarchitecture .",
    "it seems that this new architecture will be the most suitable for extremely hard real - time systems .",
    "vmem - vth - the two - dimensional virtualized machine with the virtual memory and virtual - threading at the isa level .",
    "it seems that this new advanced architecture will replace the existing most widely used vmem - pth architecture in a powerful general purpose microprocessors and will be the architecture - candidate for implementation of the general purpose supercomputer discussed in chapter 1 .    in respect of whether to classify the virtual - threading architecture as dataflow or von neumann machine , we consider the following . at the isa - level",
    "the virtual - threaded architecture supports a simultaneous work of many traditional von neumann machines , also referred to as the stored - program machines .",
    "these machines , or more accurately , the virtual processors that represent them , are stored and processed as a usual data at the virtual - threaded microarchitecture . therefore the virtual - threading architecture can be seen as an improvement of the von neumann machine architecture and it is natural to call it the stored - processors machine .    further it is not difficult to see that at the level of microarchitecture the virtual - threaded machine actually implements the dataflow machine .",
    "in essence , the data to be processed by this dataflow machine is the fine grain distributed representation of processes and threads which are being stored in the multilevel virtual memory .",
    "the work of such a dataflow machine provides running of a virtual set of von neumann machines .",
    "those elements of isa image of the von neumann machines , which are required for an instruction performing , are fetched by means of hardware into the level of operating register and pushed into the opposite direction if the von neumann machine remains inactive for a long period of time .",
    "presumably this duality of the virtual - threaded machine , which is reflected in the combination of the dataflow machine at the microarchitectural level and the von neumann machines at the isa level , is natural for the effective support of the general purpose parallel computing . because of the sequential nature of human thinking",
    ", programmers are positively disposed to programming the von neumann machines , and negatively disposed to programming the dataflow machine , which resulted in very narrow spread of the latter .",
    "this duality of the virtual - threaded architecture puts a man and a machine in their natural places - programmers will write programs for this architecture just as for a usual von neumann machines , whereas running of a plurality of these machines automatically generates a dataflow program which will be performed by means of virtual - threaded processor dataflow microarchitecture .              b.j .",
    "smith , architecture and applications of the hep multiprocessor computer system , proc .",
    "international society for optical engineering , pp .",
    "241 - 248 , 1982 .",
    "http://www.cs.auckland.ac.nz/compsci703s1c/resources/bsmith.pdf    horizon a processor architecture for horizon , mark r. thistle , burton j. smith , ch2617 9/88/0000/0035 01.00 1988 ieee http://www.cs.berkeley.edu/  culler / cs252-s02/papers / a - processor - design - for - horizon.pdf    the tera computer system .",
    "robert alverson , david callahan , daniel cummings , brian koblenz , allan porterfield , and burton smith acm international conference on supercomputing , pp . 1 6 , june 1990 http://www.cray.com/downloads/mta2_papers/arch.pdf          t. sterling .",
    "critical factors and directions for petaflops - scale supercomputers .",
    "california institute of technology , nasa , jet propulson laboratory , presentation to ifip wg10.3 e - seminar series .",
    "4 january 2005 ."
  ],
  "abstract_text": [
    "<S> the paper describes the new computers architecture , the main features of which has been claimed in the russian federation patent 2312388 and in the us patent application 11/991331 . </S>",
    "<S> this architecture is intended to effective support of the general purpose parallel computing ( gppc ) , the essence of which is extremely frequent switching of threads between states of activity and states of viewed in the paper the algorithmic latency . to emphasize the same impact of the architectural latency and the algorithmic latency upon gppc , </S>",
    "<S> is introduced the new notion of the generalized latency and is defined its quantitative measure - the generalized latency tolerance ( glt ) . </S>",
    "<S> it is shown that a well suited for gppc implementation architecture should have high level of glt and is described such architecture , which is called the virtual - threaded machine . </S>",
    "<S> this architecture originates a processor virtualization in the direction of activities virtualization , which is orthogonal to the well - known direction of memory virtualization . </S>",
    "<S> the key elements of the architecture are 1 ) the distributed fine grain representation of the architectural register file , which elements are hardware swapped through levels of a microarchitectural memory , 2 ) the prioritized fine grain direct hardware multiprogramming , 3 ) the access controlled virtual addressing and 4 ) the hardware driven semaphores . </S>",
    "<S> the composition of these features lets to introduce new styles of operating system ( os ) programming , which is free of interruptions , and of applied programming with a very rare using the os services .    </S>",
    "<S> keywords : general purpose parallel computing , virtual - threading ( vth ) , virtual - threaded machine ( vthm ) , direct hardware multiprogramming , virtualization of activities , access controlled virtual addressing ( acva ) , hardware driven semaphores ( hwds ) , programming without interruptions .    </S>",
    "<S> subj - class : architecture , operating systems    acm - class : c.1.2 ; d.4.1 ; d.4.2 </S>"
  ]
}