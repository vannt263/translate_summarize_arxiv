{
  "article_text": [
    "smc methods are amongst the most widely used computational techniques in statistics , engineering , physics , finance and many other disciplines ; see @xcite for a recent overview .",
    "they are designed to approximate a sequence @xmath2 of probability distributions of increasing dimension or complexity .",
    "the method uses @xmath3 weighted samples , or particles , generated in parallel and propagated via markov kernels and resampling methods .",
    "the method has accuracy which increases as the number of particles grows and is typically asymptotically exact .",
    "standard smc methodology is by now very well understood with regards to its convergence properties and several consistency results have been proved @xcite .",
    "smc methods have also recently been proved to be stable in certain high - dimensional contexts @xcite .    in this article , we are concerned with _ adaptive _ smc methods ; in an effort to improve algorithmic efficiency , the weights and/or markov proposal kernels can depend upon the history of the simulated process .",
    "such procedures appear in a wealth of articles including @xcite and have important applications in , for example , econometrics , population genetics and data assimilation .",
    "the underlying idea of these algorithms is that , using the particle approximation @xmath4 of the distribution @xmath5 , one can exploit the induced information to build effective proposals or even to _ determine _ the next probability distribution in the sequence ; this is often achieved by using the expectation @xmath6 of a summary statistic @xmath7 with respect to the current smc approximation @xmath4 . in other cases , one can use the particles to determine the next distribution in an artificial sequence of densities ; we expand upon this point below .",
    "such approaches are expected to lead to algorithms that are more efficient than their ` non - adaptive ' counter - parts .",
    "critically , such ideas also deliver more automated algorithms by reducing the number of user - specified tuning parameters .    whilst the literature on adaptive mcmc methods is by",
    "now well - developed e.g.  @xcite and sufficient conditions for an adaptive mcmc algorithm to be ergodic are well - understood , the analysis of adaptive smc algorithms is still in its infancy . to the best of our knowledge ,",
    "a theoretical study of the consistency and fluctuation properties of adaptive smc algorithms is lacking in the current literature .",
    "this article aims at filling this critical gap in the theory of smc methods .",
    "some preliminary results can be found , under exceptionally strong conditions , in @xcite .",
    "proof sketches are given in @xcite with some more realistic but limited analysis in @xcite .",
    "we are not aware of any other asymptotic analysis of these particular class of algorithms in the literature .",
    "contrary to adaptive mcmc algorithms , we show in this article that it is reasonable to expect most adaptive smc methods to be asymptotically correct .",
    "this article explores two distinct directions . in the first part , an asymptotic analysis of a class of smc methods with adaptive markov kernels and weights",
    "is carried out .",
    "the second part of the article looks at the case where an additional layer of randomness is taken into account through an adaptive tempering procedure . a weak law of large numbers ( wlln ) and a central limit theorem ( clt ) relevant to each situation are proved . in both cases",
    "we consider a sequence of target distributions @xmath8 defined on a corresponding sequence of measurable spaces @xmath9 .",
    "we write @xmath10 for the @xmath11-particle smc approximation of @xmath5 , with @xmath12 the dirac measure at @xmath13 and @xmath14 the collection of particles at time @xmath15 . in the first part of the paper , for each @xmath16 we consider parametric families , indexed by a parameter @xmath17 , of markov kernels @xmath18 and potential functions @xmath19 . to construct the particle approximation @xmath20 ,",
    "the _ practical _ smc algorithm exploits summary statistics @xmath21 by reweighing and propagating the particle approximation @xmath22 through the potential @xmath23 and the markov kernel @xmath24 .",
    "this is a substitute for the _ perfect _ algorithm ( as also used by @xcite and which can not be implemented ) which employs the markov kernel @xmath25 and weight function @xmath26 .",
    "we prove a wlln and a clt for both the approximation of the probability distribution @xmath5 and its normalising constant .",
    "this set - up is relevant , for example , in the context of sequential bayesian parameter inference @xcite when @xmath27 is a sequence of posterior distributions that corresponds to increasing amount of data .",
    "the markov kernel @xmath28 is user - specified and its role is to efficiently move the particles within the state space . in many situations the markov kernel @xmath28",
    "is constructed so that it leaves the distribution @xmath5 invariant ; a random walk metropolis kernel that uses the estimated covariance structure of @xmath29 for scaling its jump proposals is a popular choice .",
    "the case when there is also a tuned parameter in the weight function @xmath30 is relevant to particle filters @xcite , as described in section [ sec : ex_filt ] .",
    "the second part of this article investigates an adaptive tempering procedure .",
    "standard mcmc methods can be inefficient for directly exploring complex probability distributions involving high - dimensional state spaces , multi - modality , greatly varying scales , or combination thereof .",
    "it is a standard approach to introduce a bridging sequence of distributions @xmath31 between a distribution @xmath32 that is easy to sample from and the distribution of interest @xmath33 . in accordance with the simulated annealing literature ,",
    "the probability distribution of interest is written as @xmath34 for a potential @xmath35 , temperature parameter @xmath36 , dominating measure @xmath37 and normalisation constant @xmath38 ; the bridging sequence of distributions is constructed by introducing a ladder of temperature parameters @xmath39 and setting @xmath40 for a normalisation constant @xmath41 .",
    "the choice of the bridging sequence of distributions is an important and complex problem , see e.g.  @xcite . to avoid the task of having to pre -",
    "specify a potentially large number of temperature parameters , an adaptive smc method can compute them ` on the fly ' @xcite , thus obtaining a random increasing sequence of temperature parameters @xmath42 . in this article",
    ", we adopt the following strategy : assuming a particle approximation @xmath43 with temperature parameter @xmath44 , the particles are assigned weights proportional to @xmath45 to represent the next distribution in the sequence ; the choice of @xmath46 is determined from the particle collection @xmath47 by ensuring a minimum effective sample size ( ess ) ( it is described later on , why this might be a sensible choice ) .",
    "this can efficiently be implemented using a bisection method ; see e.g.  @xcite .",
    "we prove a wlln and a clt for both the approximation of the probability distribution @xmath5 and the estimates of the normalising constants @xmath41 .",
    "one of the contributions of the article is the proof that the asymptotic variance in the clt , for some algorithms in the first part of the paper , is _ identical _ to the one of the ` perfect ' smc algorithm using the ideal kernels .",
    "one consequence of this effect is that if the asymptotic variance associated to the ( relative ) normalizing constant estimate increases linearly with respect to time ( see e.g.  @xcite ) , then so does the asymptotic variance for the adaptive algorithm .",
    "we present numerical results on a complex high - dimensional posterior distribution associated with the navier - stokes model ( as in e.g.  @xcite ) , where adapting the proposal kernels over hundreds of different directions is critical for the efficiency of the algorithm .",
    "whilst our theoretical result ( with regards to the asymptotic variance ) only holds for the case where one adapts the proposal kernel , the numerical application will involve much more advanced adaptation procedures .",
    "these experiments provide some evidence that our theory could be relevant in more general scenarios .",
    "this article is structured as follows . in section [ sec : algo ]",
    "the adaptive smc algorithm is introduced and the associated notations are detailed . in section [ sec : exam1 ]",
    "we provide some motivating examples for the use of adaptive smc . in section [ sec : main_res ] we study the asymptotic properties of a class of smc algorithms with adaptive markov kernels and weights . in section [ sec : annealed ] , we extend our analysis to the case where an adaptive tempering scheme is taken into account . in each situation , we prove a wlln and a clt . in section [ sec :",
    "exam ] , we verify that our assumptions hold when using the adaptive smc algorithm in a real scenario .",
    "in addition , we provide numerical results associated to the navier - stokes model and some theoretical insights associated to the effect of the dimension of the statistic which is adapted .",
    "the article is concluded in section [ sec : summ ] with a discussion of future work .",
    "the appendix features a proof of one of the results in the main text .",
    "in this section we provide the necessary notations and describe the smc algorithm with adaptive markov kernels and weights .",
    "the description of the adaptive tempering procedure is postponed to section [ sec : annealed ] .",
    "let @xmath9 be a sequence of measurable spaces endowed with a countably generated @xmath48-field @xmath49 .",
    "the set @xmath50 denotes the class of bounded @xmath51-measurable functions on @xmath52 where @xmath53 borel @xmath48-algebra on @xmath54 .",
    "the supremum norm is written as @xmath55 and @xmath56 is the set of probability measures on @xmath57 .",
    "we will consider non - negative operators @xmath58 such that for each @xmath59 the mapping @xmath60 is a finite non - negative measure on @xmath49 and for each @xmath61 the function @xmath62 is @xmath63-measurable ; the kernel @xmath64 is markovian if @xmath65 is a probability measure for every @xmath59 .",
    "for a finite measure @xmath66 on @xmath67 and borel test function @xmath68 we define @xmath69 we will use the following notion of continuity at several places in this article .",
    "[ def.unif.cont ] let @xmath70 , @xmath71 and @xmath72 be three metric spaces .",
    "a function @xmath73 is continuous at @xmath74 uniformly on @xmath70 if @xmath75    we write @xmath76 and @xmath77 to denote convergence in probability and in distributions . the kroenecker product @xmath78 of two vectors",
    "@xmath79 designates the matrix @xmath80 ; the covariance of a function @xmath81 with respect to a probability measure @xmath82 is denoted by @xmath83 \\otimes [ { \\varphi}(x)-\\mu({\\varphi } ) ] \\ , \\mu(dx)$ ] .      for each index @xmath16",
    ", we consider markov operators @xmath84 and weight functions @xmath85 parametrized by @xmath86 . the adaptive smc algorithm to be described exploits summary statistics @xmath21 and aims at approximating the sequence of probability distributions @xmath87 , on the measurable spaces @xmath9 , defined via their operation on a test function @xmath88 as @xmath89 where @xmath90 is the unnormalised measure on @xmath57 given by @xmath91\\ .\\ ] ] the above expectation is under the law of a non - homogeneous markov chain @xmath92 with initial distribution @xmath93 and transition @xmath94 = m_n(x , a)$ ] where we have used the notations @xmath95 in practice , the expectations @xmath96 of the summary statistics are not analytically tractable and it is thus impossible to simulate from the markov chain @xmath97 or compute the weights @xmath98 . nevertheless , for the purpose of analysis , we introduce the following idealized algorithm , referred to as the _ perfect _ smc algorithm in the sequel , that propagates a set of @xmath3 particles by sampling from the distribution @xmath99 where the @xmath11-particle approximation of the distribution is defined as @xmath100 in , the operator @xmath101 is @xmath102 expression is a mathematically concise way to describe a standard particle method that begins by sampling @xmath11 i.i.d .",
    "particles from @xmath32 and , given particles @xmath103 , performs multinomial resampling according to the unnormalised weights @xmath104 before propagating the particles via the markov kernel @xmath105 .",
    "the smc algorithm that is actually simulated in practice , referred to as the _ practical _ smc algorithm in the sequel , has joint law @xmath106 the operator @xmath107 approximates the ideal one , @xmath108 , and is defined as @xmath109 we have used the short - hand notations @xmath110 throughout this article we assume that the potentials are strictly positive , @xmath111 for all @xmath112 and @xmath86 so that there is no possibility that the algorithm collapses .",
    "the particle approximation of the unnormalised distribution is defined as @xmath113 it will bel useful to introduce the non - negative operator @xmath114 and the idealised version @xmath115 many times we will be interested in the properties of involved operators as functions of @xmath116 , thus we will also write @xmath117 to emphasise the dependency on the parameter @xmath86 . unless otherwise stated , the differentiation operation @xmath118 at step @xmath119 is evaluated at the limiting parameter value @xmath120 . with these definitions ,",
    "one can verify that the following identities hold @xmath121 similar formulae are available for the @xmath11-particle approximations ; if @xmath122 designates the filtration generated by the particle system up - to ( and including ) time @xmath119 we have @xmath123 = \\phi_{n , n}(\\eta^n_{n-1})({\\varphi}_n)\\ ; \\quad   { \\mathbb{e}}\\,\\big[\\gamma^n_n({\\varphi}_n )   \\mid \\mathscr{f}_{n-1}^n   \\big ] =   \\gamma^n_{n-1}(q_{n , n } { \\varphi}_n)\\ .\\ ] ] in the sequel , we will use the expressions @xmath124 $ ] and @xmath125 $ ] to denote the conditional expectation @xmath126 $ ] and conditional variance @xmath127 $ ] respectively .    our results concern multinomial resampling at each time .",
    "extension of our analysis to adaptive resampling @xcite is possible but would require many additional calculations and technicalities ; this is left as a topic for future work .",
    "consider bayesian inference for the parameter @xmath128 , observations @xmath129 and prior measure @xmath130 .",
    "the posterior distribution @xmath5 after having observed @xmath131 reads @xmath132 \\,/\\ , { \\mathbb{p}}\\,[\\,y_{1:n}\\,]\\ , \\big ) \\ , \\eta_0(dx)\\ , .\\ ] ] the approach in @xcite fits in the framework described in section [ sec : algo1 ] with state spaces @xmath133 and potential functions @xmath134 $ ] . for an mcmc kernel @xmath135 with invariant measure @xmath5 the posterior distribution @xmath5 is given by @xmath136 where the unnormalised measure @xmath90 is defined as in .",
    "a popular choice consists in choosing for @xmath137 a random walk metropolis kernel reversible with respect to @xmath138 and jump covariance structure matching the one of the distribution @xmath139 . under our assumptions ,",
    "the analysis of section [ sec : main_res ] applies in this context .",
    "whilst such an example is quite simple it is indicative of more complex applications in the literature .",
    "article @xcite considers a state - space with dimension of about @xmath140 and dimension of adapted statistic of about @xmath141 . in such a setting ,",
    "pre - specifying the covariance structure of the random walk metropolis proposals is impractical ; the adaptive smc strategy of section  [ sec : algo ] provides a principled framework for automatically setting this covariance structure , see also section [ sec : num_ex ] .",
    "this section illustrates the case of having an adaptive weight function . consider a state - space model with observations @xmath142 , unobserved markov chain @xmath143 and joint density with respect to a dominating measure @xmath144 given by @xmath145 the probability @xmath146 is the prior distribution for the initial state of the unobserved markov chain",
    ", @xmath147 is the conditional observation probability at time @xmath148 and @xmath149 describes the dynamics of the unobserved markov process .    a standard particle filter with proposal at time @xmath148 corresponding to the markov kernel @xmath150 = m_p(u_{p-1 } , u_p )",
    "\\ , \\lambda_{\\mathcal{u}}(du_p)$ ] has importance weights of the form @xmath151 where here @xmath152 .",
    "the process @xmath153 is markovian with transition @xmath154 .",
    "the marginals of the sequence of probability distributions @xmath5 described in equation are the standard predictors .    in practice ,",
    "the choice of the proposal kernel @xmath155 is critical to the efficiency of the smc algorithm . in such settings",
    ", one may want to exploit the information contained in the distribution @xmath139 in order to build efficient proposal kernels .",
    "approximating the filter mean is a standard strategy . in these cases , both the markov kernel @xmath156 and the weight function @xmath157",
    "depend upon the distribution @xmath139 ; this is covered by the framework adapted in section  [ sec : algo ] .",
    "see @xcite and the references therein for ideas associated to such approaches .",
    "in this section we develop an asymptotic analysis of the class of adaptive smc algorithm described in section [ sec : algo ] . after first stating our assumptions in section [ sec.algo1.assump ] , we give a wlln in section [ sec.algo1.wlln ] and a clt in section [ sec.algo1.clt ] .",
    "our results will make use of conditions ( a[hyp:1]-[hyp:2 ] ) below . by @xmath158",
    "we denote a convex set that contains the range of the statistic @xmath159 .",
    "[ hyp:1 ] for each @xmath160 , function @xmath161 is bounded and continuous at @xmath162 uniformly over @xmath163 .",
    "statistics @xmath164 are bounded . for any test function",
    "@xmath165 the function @xmath166 is bounded , continuous at @xmath162 uniformly over @xmath163 .",
    "[ hyp:2 ] for each @xmath160 and test function @xmath165 , function @xmath167 is well defined on @xmath168 , bounded and continuous at @xmath169 uniformly over @xmath163 .",
    "assumptions ( a[hyp:1]-[hyp:2 ] ) are reasonably weak in comparison to some assumptions used in the smc literature , such as in @xcite , but are certainly not the weakest adopted for wlln and clts ( see e.g.  @xcite ) . the continuity assumptions in ( a[hyp:2 ] )",
    "are associated to the use of a first order - taylor expansion .",
    "we have defined @xmath170 as a convex set because we need to compute integrals along segments between points of @xmath170 . in general",
    ", we expect that the assumptions can be relaxed for unbounded functions at the cost of increased length and complexity of the proofs .",
    "+      in this section we establish a weak law of large numbers ( wlln ) .",
    "to do so , we state first a slightly stronger result that will be repeatedly used in the fluctuation analysis presented in section [ sec.algo1.clt ] .",
    "[ theo : wlln ] assume ( a[hyp:1 ] ) .",
    "let @xmath171 be a polish space and @xmath172 a sequence of @xmath171-valued random variables that converges in probability to @xmath173 .",
    "let @xmath174 , @xmath175 and @xmath176 be a bounded function continuous at @xmath173 uniformly on @xmath52 .",
    "the following limit holds in probability @xmath177 = \\eta_n\\,[\\,{\\varphi}_n(\\cdot , \\mathsf{v})\\,]\\ .\\ ] ]    [ cor : wlln ] assume ( a[hyp:1 ] ) .",
    "let @xmath174 , @xmath175 and @xmath178 a bounded measurable function .",
    "the following limit holds in probability , @xmath179 .",
    "it suffices to concentrate on the scalar case @xmath180 .",
    "the proof is by induction on @xmath119 .",
    "the initial case @xmath181 is a direct consequence of wlln for i.i.d .",
    "random variables and definition [ def.unif.cont ] . for notational convenience ,",
    "in the rest of the proof we write @xmath182 instead of @xmath183 .",
    "we assume the result at rank @xmath184 and proceed to the induction step .",
    "since @xmath185 converges in probability to @xmath173 , definition [ def.unif.cont ] shows that it suffices to prove that @xmath186\\big(\\bar{{\\varphi}}_n\\big)$ ] converges in probability to zero .",
    "we use the decomposition @xmath187(\\bar{{\\varphi}}_n )   & =   \\big(\\eta_n^n(\\bar{{\\varphi}}_n)-{\\mathbb{e}}_{n-1}[\\eta_n^n(\\bar{{\\varphi}}_n)]\\big ) + \\big({\\mathbb{e}}_{n-1}[\\eta_n^n(\\bar{{\\varphi}}_n)]-\\eta_n(\\bar{{\\varphi}}_n)\\big)\\\\ & = [ \\eta_n^n-\\phi_{n , n}(\\eta_{n-1}^n)](\\bar{{\\varphi}}_n )   + [ \\phi_{n , n}(\\eta_{n-1}^n)-\\eta_n](\\bar{{\\varphi}}_n ) = : a(n ) + b(n)\\ .\\end{aligned}\\ ] ] to conclude the proof , we now prove that each of these terms converges to zero in probability .    *",
    "since the expected value of @xmath188 is zero , it suffices to prove that its moment of order two also converges to zero as @xmath11 goes to infinity . to this end",
    ", it suffices to notice that @xmath189 = \\tfrac{1}{n}\\,{\\mathbb{e}}_{n-1}\\big[\\big ( \\bar{{\\varphi}}(x^i_n ) - { \\mathbb{e}}_{n-1}[\\bar{{\\varphi}}(x^i_n ) ] \\big)^2 \\big ] \\leq \\frac{\\|\\bar{{\\varphi}}\\|^2_{\\infty}}{n}\\ .\\ ] ] * to treat the quantity @xmath190 , we use the definition of @xmath191 in and decompose it as the sum of three terms @xmath192 with @xmath193(\\bar{{\\varphi}}_n ) \\big\\ } \\ , / \\",
    ", \\eta_{n-1}^n(g_{n-1,n})\\   ; \\\\ b_2(n ) & = [ \\eta_{n-1}^n-\\eta_{n-1}]\\big ( q_n(\\bar{{\\varphi}}_n ) \\big ) \\ , / \\ , \\eta^n_{n-1}(g_{n-1,n})\\   ; \\\\",
    "b_3(n ) & = \\eta_{n-1}^n [ q_n(\\bar{{\\varphi}}_n ) ] \\times   \\big\\ { 1 / \\eta^n_{n-1}(g_{n-1,n } ) - 1 / \\eta_{n-1}(g_{n-1 } ) \\big\\}\\ .\\end{aligned}\\ ] ] we prove that @xmath194 converges in probability to zero for @xmath195 .",
    "the induction hypothesis shows that @xmath196 converges to @xmath96 in probability . by assumption  [ hyp:1 ] , the bounded function",
    "@xmath197 is continuous at @xmath198 uniformly on @xmath199 ; the induction hypothesis applies and @xmath200 converges in probability to @xmath201 .",
    "similarly , since @xmath202 is bounded by boundedness of @xmath203 , it follows that @xmath204 $ ] converges in probability to @xmath205 $ ] .",
    "slutsky s lemma thus yields that @xmath206 and @xmath207 converge to zero in probability .",
    "finally , note that by assumption [ hyp:1 ] the bounded function @xmath208 is continuous at @xmath198 uniformly on @xmath199 ; the induction yields @xmath209(\\bar{{\\varphi}}_n ) \\big\\ } = \\lim_{n \\to \\infty } & \\big\\{\\,\\eta_{n-1}^n[q_{n , n}(\\bar{{\\varphi}}_n ) ] - \\eta_{n-1}[q_n(\\bar{{\\varphi}}_n)]\\,\\big\\}\\\\ & - \\lim_{n \\to \\infty } \\big\\{\\,\\eta_{n-1}^n[q_{n}(\\bar{{\\varphi}}_n ) ] - \\eta_{n-1}[q_{n}(\\bar{{\\varphi}}_n)]\\,\\big\\ } = 0\\ , \\end{aligned}\\ ] ] which is enough for concluding that @xmath210 converges to zero in probability .    as a corollary",
    ", one can establish a similar consistency result for the sequence of particle approximations @xmath211 , defined in equation , of the unnormalised quantity @xmath212 .",
    "[ cor.wlln.normalisation ] assume ( a[hyp:1 ] ) .",
    "let @xmath174 , @xmath175 and @xmath178 be a bounded measurable function .",
    "the following limit holds in probability , @xmath213 .",
    "since @xmath214 and @xmath215 , by corollary [ cor : wlln ] it suffices to prove that @xmath216 converges in probability to the value @xmath217 . by assumption [ hyp:1 ] , the potentials @xmath218",
    "are bounded so that corollary [ cor : wlln ] applies and the quantity @xmath219 converges in probability to @xmath220 for any index @xmath221 .",
    "the conclusion directly follows .      in this section , for a test function @xmath222 , we carry out a fluctuation analysis of the particle approximations @xmath211 and @xmath223 around their limiting value .",
    "as expected , we prove that there is convergence at standard monte - carlo rate @xmath224 ; in some situations , comparison with the perfect and non - adaptive algorithm is discussed in section [ sec.stability ] .",
    "[ thm.clt.unnormalised ] assume ( a[hyp:1]-[hyp:2 ] ) . let @xmath174 , @xmath175 and @xmath178 be a bounded measurable function .",
    "the sequence @xmath225({\\varphi}_n)$ ] converges weakly to a centered gaussian distribution with covariance @xmath226 where the linear operator @xmath227 is defined by @xmath228 \\ , \\big ( \\ ,   \\xi_p - \\eta_{p-1}(\\xi_p ) \\ , \\big ) + q_p({\\varphi}_p)\\ ] ] with @xmath229 and @xmath230 .    for notational convenience , we concentrate on the scalar case @xmath180 .",
    "the proof of the multi - dimensional case is identical , with covariance matrices replacing scalar variances .",
    "we proceed by induction on the parameter @xmath160 .",
    "the case @xmath181 follows from the usual clt for i.i.d",
    ". random variables . to prove the induction step it suffices to show that for any @xmath231 the following identity holds @xmath232({\\varphi}_n)}\\ , ] =   e^{-\\frac12 t^2 \\ , \\gamma_n(1)^2 \\ , \\sigma_{\\eta_n}({\\varphi}_n ) } \\ ,   \\lim_{n \\to \\infty } \\ , { \\mathbb{e}}\\,[\\,e^{i t \\sqrt{n } \\ , [ \\gamma^n_{n-1}-\\gamma_{n-1 } ] ( { \\mathscr{l}}_n { \\varphi}_n)}\\,]\\ .\\ ] ] indeed , assuming that the induction hypothesis holds at time @xmath184 , we have that @xmath233({\\mathscr{l}}_n { \\varphi}_n)}\\ , ] = \\exp\\big\\ { -\\tfrac{1}{2 } t^2 \\ , \\sum_{p=0}^{n-1 } \\gamma_p(1)^2 \\ , \\sigma_{\\eta_p}({\\mathscr{l}}_{p , n } { \\varphi}_n ) \\big\\}\\ ] ] and the proof of the induction step then follows from levy s continuity theorem and . to prove we use the following decomposition @xmath234({\\varphi}_n ) & = \\big\\ { \\gamma^n_n({\\varphi}_n ) - { \\mathbb{e}}_{n-1}[\\gamma^n_n({\\varphi}_n ) ] \\big\\ }",
    "+ \\big\\ { { \\mathbb{e}}_{n-1}[\\gamma^n_n({\\varphi}_n ) ] - \\gamma_n({\\varphi}_n ) \\big\\}\\\\ & = : { \\widetilde}{a}(n ) + { \\widetilde}{b}(n)\\ . \\end{aligned}\\end{aligned}\\ ] ] since @xmath235 the expectation @xmath236({\\varphi}_n)}]$ ] can be decomposed as @xmath237   & - e^{-\\frac12 t^2 \\ , \\gamma_n(1)^2 \\ , \\sigma_{\\eta_n}({\\varphi}_n ) } \\big ) \\times e^{it \\sqrt{n } \\ , { \\widetilde}{b}(n ) } \\big]\\\\ & \\qquad+ e^{-\\frac12 t^2 \\ , \\gamma_n(1)^2 \\ , \\sigma_{\\eta_n}({\\varphi}_n ) } \\times { \\mathbb{e}}\\big [ e^{it \\sqrt{n } \\ , { \\widetilde}{b}(n ) } \\big]\\ . \\end{aligned}\\end{aligned}\\",
    "] ] as a consequence , follows once it is established that the limit @xmath238 =   \\exp\\big\\{-\\tfrac12 t^2 \\ , \\gamma_n(1)^2 \\ , \\sigma_{\\eta_n}({\\varphi}_n ) \\big\\}\\ ] ] holds in probability and that @xmath239({\\mathscr{l}}_n({\\varphi}_n ) ) + o_{\\mathbb{p}}(1)$ ] .",
    "we finish the proof of theorem [ thm.clt.unnormalised ] by establishing these two results .",
    "* quantity @xmath240 also reads as @xmath241 with @xmath242({\\varphi}_n)$ ] . by corollary [ cor.wlln.normalisation ]",
    ", @xmath243 converges in probability to @xmath244 ; to prove that @xmath245 $ ] converges in probability to @xmath246 it thus suffices to show that @xmath247 $ ] converges in probability to @xmath248 .",
    "we will exploit the following identity @xmath249 = { \\mathbb{e}}_{n-1}\\,\\big[\\,e^ { i \\ , t \\ , \\ { { \\varphi}_n(x_n)- { \\mathbb{e}}_{n-1}[{\\varphi}_n(x_n ) ] \\ } / \\sqrt{n}}\\,\\big]^n\\ ] ] with @xmath250 is distributed according to @xmath251 . since the test function @xmath252 is bounded , a taylor expansion yields that @xmath253 \\ } / \\sqrt{n}}\\,\\big ] = 1 - \\tfrac{t^2}{n}\\ , { \\mathrm{var}}_{n-1}[{\\varphi}_n(x_n ) ] + n^{-3/2 } \\times \\mathcal{o}_{\\mathbb{p}}(1)\\ .\\ ] ] consequently , @xmath254 = \\exp\\big\\ { - t^2 \\ , { \\mathrm{var}}_{n-1}[{\\varphi}_n(x_n ) ]   /   2\\big\\ } + o_{{\\mathbb{p}}}(1)$ ] and the proof is complete once it is shown that @xmath255 & = \\sum_{i=1}^n g_{n-1,n}(x_{n-1}^i ) m_{n , n}({\\varphi}_n^2)(x_{n-1}^i ) \\ ; / \\ ; \\sum_{i=1}^n g_{n-1,n}(x_{n-1}^i ) \\\\",
    "& \\qquad - \\big\\ { \\sum_{i=1}^n g_{n-1,n}(x_{n-1}^i ) m_{n , n}({\\varphi}_n)(x_{n-1}^i ) \\ ; / \\ ; \\sum_{i=1}^n g_{n-1,n}(x_{n-1}^i ) \\big\\}^2\\\\ & = \\eta^n_{n-1}\\big[q_{n-1 , \\eta^n_{n-1}(\\xi_{n})}{\\varphi}_n^2 \\big ] \\ ; / \\ ; \\eta^n_{n-1}\\big [ g_{n-1 , \\eta^n_{n-1}(\\xi_{n } ) } \\big ] \\\\ &",
    "\\qquad - \\big\\ { \\eta^n_{n-1}\\big[q_{n-1 , \\eta^n_{n-1}(\\xi_{n})}{\\varphi}_n \\big ] \\ ; / \\ ; \\eta^n_{n-1}\\big [ g_{n-1 , \\eta^n_{n-1}(\\xi_{n } ) } \\big ] \\big\\}^2\\end{aligned}\\ ] ] converges in probability to @xmath256 . by assumption [ hyp:1 ] , functions @xmath197 , @xmath257 , @xmath258",
    "are bounded and continuous at @xmath120 uniformly on @xmath199 . by corollary [ cor : wlln ]",
    ", @xmath259 converges in probability to @xmath260 ; by theorem [ theo : wlln ] and slutsky s lemma we get that @xmath261 $ ] converges in probability to @xmath262 / \\eta_{n-1}(g_{n } )   - \\big ( \\eta_{n-1 } [ q_n({\\varphi}_n ) ] / \\eta_{n-1}(g_{n } ) \\big)^2\\ , \\end{aligned}\\ ] ] which is another formula for @xmath263 , as required . * to prove that @xmath239({\\mathscr{l}}_n({\\varphi}_n ) ) + o_{\\mathbb{p}}(1)$ ] we write @xmath264 as @xmath265({\\varphi}_n ) \\;+\\ ; [ \\gamma^n_{n-1}-\\gamma_{n-1}](q_n { \\varphi}_n)\\ .\\ ] ] furthermore , we have @xmath266({\\varphi}_n ) = \\eta^n_{n-1}\\,[\\,\\omega(\\cdot , \\eta^n_{n-1}(\\xi_n))\\ , ] \\times [ \\eta^n_{n-1}-\\eta_{n-1}](\\xi_n)\\ ] ] with @xmath267 . under assumption [ hyp:2 ] , function @xmath268",
    "is bounded and continuous at @xmath269 uniformly over @xmath59 .",
    "theorem [ theo : wlln ] applies so that @xmath270\\rightarrow\\eta_{n-1}\\,[\\ , \\omega(\\cdot , \\eta_{n-1}(\\xi_n))\\ , ] = \\eta_{n-1}[\\partial_{\\xi}q_n({\\varphi})]$ ] , in probability .",
    "the induction hypothesis , slutky s lemma and standard manipulations yield that @xmath271({\\varphi}_n)$ ] equals @xmath272 \\times [ \\gamma^n_{n-1}-\\gamma_{n-1}](\\xi_n-\\eta_{n-1}(\\xi_n ) ) + o_{{\\mathbb{p}}}(1)\\ .\\ ] ] it then follows from that @xmath273({\\mathscr{l}}_n { \\varphi}_n ) \\;+\\ ; o_{{\\mathbb{p}}}(1)$ ] .",
    "this concludes the proof of the induction steps and finishes the proof of theorem [ thm.clt.unnormalised ] .    in the case where the summary statistics are constant",
    ", i.e. @xmath274 for @xmath221 , expression reduces to the usual non - adaptive asymptotic variance as presented , for example , in @xcite . in the special case @xmath275 ,",
    "one obtains the following expression for the asymptotic variance of the relative normalisation constant @xmath276 .",
    "assume ( a[hyp:1]-[hyp:2 ] ) and let @xmath174 be a non - negative integer .",
    "then the quantity @xmath277 converges , as @xmath278 , to a centered gaussian distribution with variance @xmath279    similarly , one can obtain a clt for the empirical normalised measures @xmath223 :    [ theo : clt ] assume ( a[hyp:1]-[hyp:2 ] ) .",
    "let @xmath174 , @xmath175 and @xmath178 be a bounded measurable function .",
    "the sequence @xmath280({\\varphi}_n)$ ] converges weakly to a centered gaussian distribution with covariance @xmath281\\ ] ] with the linear operators @xmath282 for @xmath221 as defined in .",
    "the asymptotic variances satisfy @xmath283}{\\eta_{n-1}(g_{n-1})^2}\\ .\\ ] ]    one can verify that the normalised measure @xmath4 is related to the unnormalised measure @xmath284 through the identity ( @xcite ) @xmath285({\\varphi}_n ) = \\frac{\\gamma_n(1)}{\\gamma_n^n(1 ) } \\ , \\gamma_n^n\\big [ \\tfrac{1}{\\gamma_n(1 ) } ( { \\varphi}_n - \\eta_n({\\varphi}_n))\\big]\\ .\\ ] ] by corollary [ cor.wlln.normalisation ] , @xmath286 converges in probability to 1 .",
    "slutsky s lemma and theorem [ thm.clt.unnormalised ] yield that @xmath280({\\varphi}_n)$ ] converges weakly to a centered gaussian variable with variance @xmath287 $ ] , which is just another way of writing .",
    "equation follows from the identities @xmath288 , @xmath289 .",
    "we now show that in the majority of applications of interest , the asymptotic variance of the adaptive smc algorithm is identical to the asymptotic variance of the _ perfect _ algorithm .",
    "[ stability ] [ thm.stability ] assume ( a[hyp:1]-[hyp:2 ] ) .",
    "suppose further that for any index @xmath16 the identity @xmath290 holds for any parameter @xmath291 .",
    "for any test function @xmath88 , the asymptotic variance of the adaptive smc algorithm identified in theorem [ thm.clt.unnormalised ] equals the asymptotic variance of the perfect smc algorithm .",
    "formula shows that it suffices to prove that the term @xmath292 vanishes . by differentiation under the integral sign ,",
    "it is enough to prove that the mapping @xmath293 is constant on @xmath294 .",
    "indeed , it follows from that @xmath295 for any @xmath291 , concluding the proof of theorem [ thm.stability ] .",
    "theorem [ thm.stability ] applies for instance to the sequential bayesian parameter inference context discussed in section [ sec : seq_bi ] and to the filtering setting of section [ sec : ex_filt ] .",
    "a consequence of theorem [ thm.stability ] is that standard behaviours for the asymptotic variance of the _ perfect _ smc algorithm , such as linear growth of the asymptotic variance of @xmath296 , are inherited by the adaptive smc algorithm .",
    "we now look at the scenario when one uses the information in the evolving particle population to adapt a sequence of distributions by means of a tempering parameter @xmath297 .      in many situations in bayesian inference",
    "one seeks to sample from a distribution @xmath298 on a set @xmath299 of the form @xmath300 where @xmath38 is a normalisation constant , @xmath37 a dominating measure on the set @xmath299 and @xmath301 a potential .",
    "coefficient @xmath302 can be thought of as an inverse temperature parameter .",
    "a frequently invoked algorithm involves forming a sequence of ` tempered ' probability distributions @xmath303 for inverse temperatures @xmath304 ; in many applications @xmath305 .",
    "the associated unnormalised measures are @xmath306 particles are propagated from @xmath139 to @xmath138 through a markov kernel @xmath307 that preserves @xmath138 . in other words , the algorithm corresponds to the smc approach discussed in section [ sec : algo ] with potentials @xmath308 and markov kernels @xmath156 satisfying @xmath309 . for test function @xmath310 ,",
    "the @xmath11-particle approximation of the normalised and unnormalised distribution are given in , . to be consistent with the notations introduced in section [ sec.algo1.clt ] , note that the normalisation constants also read as @xmath311 and @xmath312 .",
    "in most scenarios of practical interest , it can be difficult or even undesirable to decide _ a - priori _ upon the annealing sequence @xmath313 .",
    "indeed , if the chosen sequence features big gaps , one may reach the terminal temperature rapidly , the variance of the weights being potentially very large due to large discrepancies between consecutive elements of the bridging sequence of probability distributions . alternatively ,",
    "if the gaps between the annealing parameters are too small , the variance of the final weights can be very small ; this comes at the price of needlessly wasting a lot of computation time . knowing what constitutes ` big ' or ` small ' with regards to the temperature gaps can be very - problem specific .",
    "thus , an automated procedure for determining the annealing sequence is of great practical importance . in this section we investigate the asymptotic properties of an algorithm where the temperatures , as well as statistics of the mcmc kernel , are determined empirically by the evolving population of particles .",
    "a partial analysis of the algorithm to be described can be found in @xcite .",
    "however , the way in which the annealing sequence is determined in that work does not correspond to one typically used in the literature .",
    "in addition , the authors assume that the perfect mcmc kernels are used at each time step , whereas we do not assume so .",
    "it should also be noted , however , that the analysis in @xcite is non - asymptotic . the adaptive version of the above described algorithm constructs the ( random ) temperatures sequence @xmath314 ` on the fly ' as follows .",
    "once a proportion @xmath315 has been specified , the random tempering sequence is determined through the recursive equation @xmath316 initialized at a prescribed value @xmath317 typically chosen so that the distribution @xmath32 is easy to sample from . for completeness",
    ", we use the convention that @xmath318 . in the above displayed equation",
    ", we have used the @xmath319 functional defined for a measure @xmath320 on the set @xmath299 and a weight function @xmath321 by @xmath322 the following lemma guaranties that under mild assumptions the effective sample size functional @xmath323 is continuous and decreasing so that is well - defined and the inverse temperature @xmath324 can be efficiently computed by a standard bisection method .    [ lem.ess.decreasing ]",
    "let @xmath320 be a finite measure on the set @xmath299 and @xmath301 be a bounded potential . then",
    ", the function @xmath325 is continuous and decreasing on @xmath326 .",
    "furthermore , if @xmath327 > 0 $ ] for @xmath328 independent and distributed according to @xmath320 , the function is strictly decreasing .",
    "we treat the case where @xmath327 > 0 $ ] , the case @xmath327 = 0 $ ] being trivial .",
    "let @xmath329 and @xmath330 be two independent random variables distributed according to @xmath320 .",
    "the dominated convergence theorem shows that the function @xmath325 is continuous , with a continuous derivative .",
    "standard manipulations show that the derivative is strictly negative if @xmath331 , which is equivalent to the condition @xmath332",
    "< 0\\ .\\ ] ] this last condition is satisfied since for any @xmath333 and any @xmath334 we have the inequality @xmath335 , with strict inequality for @xmath336 .",
    "we will assume that the sequence of temperatures @xmath337 and @xmath338 are defined for _ any _ index @xmath160 , using the convention that the first time that the parameter @xmath339 reaches the level @xmath340 , which is random for the practical algorithm , the algorithm still goes on with fixed inverse temperatures equal to @xmath340 . under this convention",
    ", we can carry out an asymptotic analysis using an induction argument .",
    "ideally one would like to prove asymptotic consistency ( and a clt ) for the empirical measure at the random termination time of the practical algorithm ; we do not do this , due to the additional technical challenge that it poses .",
    "we believe that the result to be proven still provides a very satisfying theoretical justification for the practical adaptive algorithm .",
    "we assume from now on that for the perfect algorithm the sequence of inverse temperatures is given by the limiting analogue of , @xmath341    we will show in the next section that under mild assumptions the adaptive version @xmath339 converges in probability towards @xmath342 . for statistics @xmath343 we set @xmath344 and denote by @xmath345 its limiting value . at time @xmath119 , for a particle system @xmath346 and associated empirical distribution @xmath4 targeting the distribution @xmath5 , the next inverse temperature @xmath347 is computed according to ; the particle system is re - sampled according to a multinomial scheme with weights @xmath348 and then evolves via a markov kernel @xmath349 that preserves the preserves @xmath350 . similarly to section [ sec : algo1 ] , we will make use of the operator @xmath351 and its limiting analogue @xmath352 . with these notations , note that equation holds . to emphasise the dependencies upon the parameter @xmath353 , we will sometimes use the expression @xmath354 with @xmath355 and @xmath356 . for notational convenience ,",
    "we sometimes write @xmath357 when the meaning is clear . for example , by differentiation under the integral sign , the quantity @xmath358 also equals @xmath359 . unless otherwise stated , the derivative @xmath360 is evaluated at the limiting parameter @xmath361 .",
    "we define @xmath362 ^ 2 \\ ;   ; \\ ; \\beta_1 \\leq \\beta_2 \\}$ ] .",
    "by @xmath363 we denote a convex set that contains the range of the statistic @xmath364 .",
    "the results to be presented in the next section make use of the following hypotheses .",
    "[ hyp : anneal1 ] the potential @xmath35 is bounded on the set @xmath299 . for each @xmath160 the function @xmath365",
    "is bounded and continuous at @xmath366 uniformly on @xmath299 .",
    "the statistic @xmath367 is bounded . for any bounded borel test function @xmath368 , the function @xmath369 is bounded and continuous at @xmath370 uniformly on @xmath299 .",
    "[ hyp : anneal2 ] for each @xmath16 , @xmath371 and bounded borel test function @xmath368 the function @xmath372 is well defined , bounded and continuous at @xmath373 uniformly on @xmath299 .",
    "these conditions could be relaxed at the cost of considerable technical complications in the proofs .      in this section",
    "we prove that the consistency results of section [ sec.algo1.wlln ] also hold in the adaptive annealing setting .",
    "to do so , we prove that for any index @xmath160 the empirical inverse temperature parameter @xmath339 converges in probability towards @xmath342 .",
    "[ theo : wlln_aneal ] assume ( a[hyp : anneal1 ] ) .",
    "for any @xmath160 , the empirical inverse temperature @xmath339 converges in probability to @xmath342 as @xmath278 .",
    "also , let @xmath171 be a polish space and @xmath172 a sequence of @xmath171-valued random variables that converges in probability to @xmath173 .",
    "let @xmath175 and @xmath374 a bounded function continuous at @xmath173 uniformly on @xmath299 .",
    "then , the following limit holds in probability @xmath375 = \\eta_n[{\\varphi}_n(\\cdot , \\mathsf{v})]\\ .\\ ] ]    [ cor : wlln_aneal ] assume ( a[hyp : anneal1 ] ) .",
    "let @xmath174 , @xmath175 and @xmath376 be a bounded measurable function .",
    "the following limit holds in probability , @xmath179 .",
    "[ cor.wlln_aneal.normalisation ] assume ( a[hyp : anneal1 ] ) .",
    "let @xmath174 , @xmath175 and @xmath377 a bounded measurable function .",
    "the following limit holds in probability , @xmath213 .",
    "clearly , it suffices tp concentrate on the case @xmath180 .",
    "we prove by induction on the rank @xmath160 that @xmath46 converges in probability to @xmath378 and for any test function @xmath379 bounded and continuous at @xmath173 uniformly on @xmath299 that @xmath186({\\varphi } ) { \\rightarrow_{\\mathbb{p}}}0 $ ] .",
    "the initial case @xmath181 is a direct consequence of wlln for i.i.d .",
    "random variables and definition [ def.unif.cont ] .",
    "we assume the result at rank @xmath184 and proceed to the induction step .",
    "* we first focus on proving that @xmath380 converges in probability to @xmath378 .",
    "note that @xmath46 can also be expressed as @xmath381 \\ ; : \\ ;",
    "\\frac{\\zeta_{1,n-1}^n(\\beta)}{\\zeta_{2,n-1}^n(\\beta ) } \\leq \\alpha \\big\\}\\ ] ] with @xmath382 ^ 2 $ ] and @xmath383 $ ] .",
    "indeed , the limiting temperature @xmath378 can also be expressed as @xmath384 \\ ; : \\ ;   \\frac{\\zeta_{1,n-1}(\\beta)}{\\zeta_{2,n-1}(\\beta ) } \\leq \\alpha \\big\\}\\ ] ] where @xmath385 and @xmath386 are the limiting values of @xmath387 and @xmath388 .",
    "the dominated convergence theorem shows that the paths @xmath389 and @xmath390 are continuous ; it thus suffices to prove that the limit @xmath391 } = 0\\ ] ] holds in probability .",
    "lemma [ lem.ess.decreasing ] shows that the function @xmath392 is decreasing on @xmath393 $ ] for any @xmath394 and @xmath395 ; by standard arguments , for proving it suffices to show that for any fixed inverse temperature @xmath396 $ ] the difference @xmath397 converges to zero in probability .",
    "indeed , one can focus on proving that @xmath398 converges in probability to @xmath399 for @xmath400 .",
    "we present the proof for @xmath401 , the case @xmath402 being entirely similar . * * for the case @xmath403",
    ", the induction hypothesis shows that @xmath404 converges in probability to @xmath405 .",
    "since @xmath406 for @xmath407 , the conclusion follows . * * the case @xmath408 follows from the convergence in probability of @xmath404 to @xmath405 and @xmath409 to @xmath410 . * to prove that @xmath411 $ ] converges in probability towards @xmath412 $ ] , because of the convergence in probability of @xmath46 to @xmath378 , of @xmath413 to @xmath96 and of @xmath414 to @xmath415 , one can use exactly the same approach as the one in the proof of theorem [ theo : wlln ] .      in this section",
    "we extend the fluctuation analysis of section [ sec.algo1.clt ] to the adaptive annealing setting .",
    "we prove that for a test function @xmath252 the empirical quantity @xmath211 converges at @xmath224-rate towards its limiting value @xmath212 ; we give explicit recursive expressions for the asymptotic variances .",
    "it is noted that results for @xmath416 may also be proved as in section [ sec.algo1.clt ] , but are omitted for brevity . before stating the main result of this section , several notations need to be introduced . for any @xmath160 and test function",
    "@xmath417 we consider the extension operator @xmath418 that maps the test function @xmath252 to the function @xmath419 defined by @xmath420 the linear operator @xmath421 maps the bounded borel function @xmath417 to the rectangular @xmath422 matrix @xmath423 defined by @xmath424_{1,1}=1 $ ] , @xmath425_{1,[4:r+3 ] } = 0_{1 \\times r}$ ] , @xmath424_{[2:r+1],[4:r+3 ] } = i_{r \\times r}$ ] and @xmath426_{1,2}= -2\\gamma_{n-1}^{-1}(1 ) \\",
    ", \\frac { \\eta_{n-1}(g_{n-1})}{\\eta_{n-1}(g^2_{n-1})}\\cdot \\big\\ { \\partial_\\delta \\big [ \\frac{\\eta_{n-1}(g_{n-1})^2}{\\eta_{n-1}(g_{n-1}^2 ) } \\big ] \\big\\}^{-1 } \\ ; \\\\ & [ { \\mathcal{a}}_n({\\varphi}_n)]_{1,3 }   =     \\gamma_{n-1}^{-1}(1 ) \\ , \\frac{\\eta_{n-1}(g_{n-1})^2}{\\eta_{n-1}(g_{n-1}^2)^2 } \\cdot \\big\\ { \\partial_\\delta   \\big [ \\frac{\\eta_{n-1}(g_{n-1})^2}{\\eta_{n-1}(g_{n-1}^2 ) } \\big ] \\big\\}^{-1}\\   ; \\\\ & [ { \\mathcal{a}}_n({\\varphi}_n)]_{2:r+1,1 }   = \\big(\\partial_{\\beta_{n-1}}+\\partial_{\\beta_{n}}\\big ) \\ , \\eta_{n-1}(q_n { \\varphi}_n)\\   ; \\\\ & [ { \\mathcal{a}}_n({\\varphi}_n)]_{2:r+1,2}= \\gamma_{n-1}(1 ) \\ , \\eta_{n-1}[\\partial_{\\beta_{n } } q_n { \\varphi}_n ] \\times [ { \\mathcal{a}}_n({\\varphi}_n)]_{1,2}\\ ; \\\\ & [ { \\mathcal{a}}_n({\\varphi}_n)]_{2:r+1,3}= \\gamma_{n-1}(1 ) \\ , \\eta_{n-1}[\\partial_{\\beta_{n } } q_n { \\varphi}_n ] \\times [ { \\mathcal{a}}_n({\\varphi}_n)]_{1,3}\\   . & \\end{aligned}\\ ] ]    [ theo : clt2 ] assume ( a[hyp : anneal1])-(a[hyp : anneal2 ] ) .",
    "let @xmath174 , @xmath175 and @xmath178 be a bounded measurable function .",
    "the sequence @xmath427({\\varphi}_n ) \\big)^\\top$ ] converges weakly to a centred gaussian distribution with covariance @xmath428 where @xmath429 is the covariance matrix of the function @xmath430 under @xmath5 .",
    "the proof follows closely the one of theorem [ thm.clt.unnormalised ] . for the reader",
    "s convenience , we only highlight the differences .",
    "the proof proceeds by induction , the case @xmath181 directly following from the clt for i.i.d random variables . for proving the induction step , assuming that the result holds at rank @xmath184 , it suffices to prove that @xmath431({\\varphi}_n ) \\end{array } \\right )   \\ ; = \\ ; { \\mathcal{a}}_{n , n}({\\varphi}_n)\\ , \\left ( \\begin{array}{c }    \\beta^n_{n-1 } - \\beta_{n-1 } \\\\ \\big [ \\gamma^n_n - \\gamma_n\\big ]   \\big",
    "( { \\mathrm{ext}}[q_n { \\varphi}_n ] \\big ) \\end{array}\\right),\\ ] ] with @xmath432 converging in probability to @xmath423 , and that for any vector @xmath433 the following limit holds in probability @xmath434 \\;=\\ ; \\exp\\big\\ { -\\gamma^2_n(1 ) \\ , { \\langle t , \\sigma_{\\eta_n}({\\varphi}_n ) \\ , t \\rangle }   / 2\\big\\}\\ ] ] with @xmath435 $ ] . the proof of the above displayed equation is identical to the proof of and is thus omitted .",
    "we now prove .",
    "* we first treat the term @xmath436 = \\beta^n_n - \\beta_n$ ] .",
    "the relation @xmath437 can be rearranged as @xmath438 decomposing @xmath439 as the sum of @xmath440 and @xmath441(g_{n-1}^2)$ ] , and using a similar decomposition for the difference @xmath442 , one can exploit the boundedness of the potential @xmath35 , theorem [ theo : wlln_aneal ] and the same approach as the one used for proving to obtain that @xmath439 equals @xmath443 ( g_{n-1}^2)\\end{aligned}\\ ] ] and @xmath444 $ ] equals @xmath445 ( g_{n-1})\\ . \\end{aligned}\\end{aligned}\\ ] ] since @xmath446 equals @xmath447 , slutsky s lemma , equations , , and standard algebraic manipulations yield @xmath448_{1,1 } \\ , ( \\beta_{n-1}^n-\\beta_{n-1 } ) \\\\",
    "& \\qquad + [ { \\mathcal{a}}_{n , n}({\\varphi})]_{1,2 } \\ , [ \\gamma_{n-1}^n-\\gamma_{n-1 } ] \\big(g_{n-1}-\\eta_{n-1}(g_{n-1 } ) \\big)\\\\ & \\qquad + [ { \\mathcal{a}}_{n , n}({\\varphi})]_{1,3 } \\ , [ \\gamma_{n-1}^n-\\gamma_{n-1}]\\big(g^2_{n-1}-\\eta_{n-1}(g^2_{n-1 } ) \\big ) \\end{aligned}\\end{aligned}\\ ] ] where @xmath449_{1,i}$ ] converges in probability to @xmath449_{1,i}$ ] for @xmath450 . * to deal with the term @xmath451 $ ] we make use of the decomposition @xmath452 = \\gamma_{n-1}^n(1 ) \\times \\eta^n_{n-1}[q_{n , n}-q_n]({\\varphi}_n ) + [ \\gamma^n_{n-1}-\\gamma_{n-1}](q_n { \\varphi}_n)\\ .\\ ] ] assumptions ( a[hyp : anneal1])-(a[hyp : anneal2 ] ) , theorem [ theo : wlln_aneal ] and the same approach as the one used for proving show that the term @xmath453({\\varphi}_n)$ ] equals @xmath454 + o_{{\\mathbb{p}}}(1)\\big\\ } \\ , ( \\beta^n_{n-1}-\\beta_{n-1 } ) + \\big\\ { \\eta_{n-1}[\\partial_{\\beta_{n}}q_n { \\varphi}_n ] + o_{{\\mathbb{p}}}(1)\\big\\ } \\ , ( \\beta^n_{n}-\\beta_{n})\\ .\\ ] ] note that there is no term involving the derivative with respect to the value of the summary statistics ; indeed , this is because for any value of @xmath455 the markov kernel @xmath456 preserves @xmath138 so that one can readily check that @xmath457 = 0 $ ]",
    ". one can then use to express @xmath458 in terms of the three quantities @xmath459 , @xmath460 \\big(g_{n-1}-\\eta_{n-1}(g_{n-1 } ) \\big)$ ] and @xmath460 \\big(g^2_{n-1}-\\eta_{n-1}(g^2_{n-1 } ) \\big)$ ] and obtain , via slutsky s lemma and , that for any coordinate @xmath461 , @xmath462_i & =   [ { \\mathcal{a}}_{n , n}({\\varphi})]_{i+1,1 } \\ , ( \\beta_{n-1}^n-\\beta_{n-1 } ) \\\\ &",
    "\\qquad + [ { \\mathcal{a}}_{n , n}({\\varphi})]_{i+1,2 } \\ , [ \\gamma_{n-1}^n-\\gamma_{n-1 } ] \\big(g_{n-1}-\\eta_{n-1}(g_{n-1 } ) \\big)\\\\ & \\qquad + [ { \\mathcal{a}}_{n , n}({\\varphi})]_{i+1,3 } \\ , [ \\gamma_{n-1}^n-\\gamma_{n-1}]\\big(g^2_{n-1}-\\eta_{n-1}(g^2_{n-1 } ) \\big)\\\\ & \\qquad + [ \\gamma_{n-1}^n-\\gamma_{n-1}](q_n { \\varphi}_n)_i \\end{aligned}\\end{aligned}\\ ] ] where @xmath449_{i+1,j}$ ] converges in probability to @xmath463_{i+1,j}$ ] for @xmath464 .",
    "equation is a simple rewriting of and .",
    "this concludes the proof .",
    "we consider the sequential bayesian parameter inference framework of section [ sec : seq_bi ] .",
    "that is , for a parameter @xmath465 , observations @xmath129 and prior measure with density @xmath466 with respect to the lebesgue measure in @xmath467 .",
    "we assume the following .",
    "[ hyp : b1 ] for each @xmath16 the function @xmath468 $ ] is bounded and strictly positive .",
    "the statistics @xmath469 is bounded .",
    "[ hyp : b2 ] for each @xmath470 , the parametric family of markov kernel @xmath456 is given by a random - walk - metropolis kernel .",
    "the proposal density @xmath471 is symmetric ; for a current position @xmath128 the proposal @xmath472 is such that @xmath473 .",
    "we suppose that the first and second derivatives @xmath474 are bounded on the range @xmath294 of the adaptive statistics @xmath475 .",
    "assumption ( b[hyp : b1 ] ) is reasonable and satisfied by many real statistical models .",
    "similarly , it is straightforward to construct proposals verifying assumption ( b[hyp : b2 ] ) ; one can for example show that for a function @xmath476 , bounded away from zero with bounded first and second derivatives , the gaussian proposal density @xmath477 \\big\\ } / \\sqrt{2 \\pi \\sigma^2(\\xi)}$ ] satisfies assumption ( b[hyp : b2 ] ) ; multi - dimensional extensions of this settings are readily constructed .",
    "[ prop.verif.assump ] assume ( b[hyp : b1]-[hyp : b2 ] ) .",
    "the kernels @xmath478 and potentials @xmath479 satisfy assumptions ( a[hyp:1]-[hyp:2 ] ) .    by assumption ,",
    "the potentials @xmath480 are bounded and strictly positive and the statistics @xmath469 are bounded . to verify that assumptions ( a[hyp:1]-[hyp:2 ] ) are satisfied",
    ", it suffices to prove that for any test function @xmath481 , the first and second derivatives of @xmath482 exist and are uniformly bounded .",
    "the metropolis - hastings accept - reject ratio of the proposal @xmath483 is @xmath484 \\ , \\eta_0(x+u )",
    "\\big ) \\ , / \\ , \\big ( { \\mathbb{p}}[y_{1:n } \\mid x ] \\ , \\eta_0(x )",
    "\\big ) \\big\\}$ ] and we have @xmath485 \\ , r(x , u ) \\ , q(u ;",
    "\\xi )   \\ , du$ ] .",
    "differentiation under the integral sign yields @xmath486 \\ ,",
    "r(x , u ) \\",
    ",   \\nabla_{\\xi } q(u;\\xi ) \\ , du\\   , \\\\ & \\nabla^2_{\\xi } m_{n,\\xi}({\\varphi})(x )   = \\int \\big[{\\varphi}(x+u ) - { \\varphi}(x ) \\big ] \\ , r(x , u ) \\nabla^2_{\\xi } q(u;\\xi ) \\ , du\\ , \\end{aligned}\\ ] ] and the conclusion follows by boundedness of the first and second derivative of @xmath487 with respect to the parameter @xmath291 .",
    "we now provide a numerical study of a high - dimensional sequential bayesian parameter inference , as described in section [ sec : seq_bi ] , applied to the navier - stokes model . in this section ,",
    "we briefly describe the navier - stokes model , the associated smc algorithm and focus on the analysis of the behavior of the method when estimating the normalising constant .",
    "the smc method to be presented is described in detail in @xcite . in the subsequent discussion ,",
    "we highlight the algorithmic challenges and the usefulness of the adaptive smc methodology when applied to such high - dimensional scenarios .",
    "this motivates theoretical results presented in section [ sec : simos ] where the stability properties of the smc estimates are investigated in the regime where the dimension @xmath488 of the adaptive statistics is large .",
    "we work with the navier - stokes dynamics describing the incompressible flow of a fluid in a two dimensional torus @xmath489 .",
    "the time - space varying velocity field is denoted by @xmath490 .",
    "the newton s laws of motion yield the navier - stokes system of partial differential equations @xcite @xmath491 with initial condition @xmath492 .",
    "the quantity @xmath493 is a viscosity parameter , @xmath494 is the pressure field and @xmath495 is an exogenous time - homogeneous forcing . for simplicity ,",
    "we assume periodic boundary conditions .",
    "we adopt a bayesian approach for inferring the unknown initial condition @xmath496 from noisy measurements of the evolving velocity field @xmath497 on a fixed grid of points @xmath498 . performing inference with this type of data",
    "is referred to as eulerian data assimilation .",
    "measurements are available at time @xmath499 for time increment @xmath500 and index @xmath501 at each fixed location @xmath502 .",
    "we assume i.i.d gaussian measurements error with standard deviation @xmath503 so that the noisy observations @xmath504 for @xmath501 and @xmath505 can be modelled as @xmath506 for an i.i.d sequence @xmath507 .",
    "we follow the notations of @xcite and set @xmath508 we use a gaussian random field prior for the unknown initial condition ; as will become apparent from the discussion to follow , it is appropriate in this setting to assume that the initial condition @xmath509 belongs the closure @xmath510 of @xmath511 with respect to the @xmath512 norm .",
    "the semigroup operator for the navier - stokes pde is denoted by @xmath513 so that the likelihood for the noisy observation @xmath472 reads @xmath514(x_m)\\,\\big\\|^{2 } \\big\\ } / ( 2 \\pi \\varepsilon^2)^{mt}\\ .\\ ] ] under periodic boundary conditions , an appropriate orthonormal basis for @xmath510 is comprised of the functions @xmath515 for @xmath516 and @xmath517 , @xmath518 .",
    "the index @xmath519 corresponds to a bivariate frequency and the fourier series decomposition of an element @xmath520 reads @xmath521 with fourier coefficients @xmath522 .",
    "since the initial condition @xmath523 is real - valued we have @xmath524 and one can focus on reconstructing the frequencies in the subset @xmath525 \\ ; \\textrm{or } \\ ;   [ k_{1 } = -k_{2 } > 0]\\big\\}.\\end{aligned}\\ ] ] we adopt a bayesian framework and assume a centred gaussian random field prior @xmath32 on the unknown initial condition @xmath526 with hyper - parameters @xmath527 affecting the roughness and magnitude of the initial vector field . in",
    ", @xmath528 denotes the stokes operator where @xmath529 is the usual laplacian and @xmath530 is the leray - helmholtz orthogonal projector that maps a field to its divergence - free and zero - mean part .",
    "a simple understanding of the prior distribution @xmath32 can be obtained through the karhunen - love representation ; a draw from the prior distribution @xmath32 can be realised as the infinite sum @xmath531 where variables @xmath532 correspond standard complex centred gaussian random variables with @xmath533 @xmath534 for @xmath535 and @xmath536 for @xmath537 .",
    "in other words , _ a - priori _ , the fourier coefficients @xmath538 with @xmath535 are assumed independent , normally distributed , with a particular rate of decay for their variances as @xmath539 increases .",
    "statistical inference is carried out by sampling from the posterior probability measure @xmath320 on @xmath510 defined as the gaussian change of measure @xmath540 for a normalisation constant @xmath541 .      with a slight abuse of notation",
    "we will henceforth use a single subscript to count the observations and set @xmath542 .",
    "we will apply an smc sampler on the sequence of distributions @xmath543 defined by @xmath544 for a normalisation constant @xmath545 and likelihood @xmath546 .",
    "note that the state space @xmath510 is infinite - dimensional even though in practice , as described in @xcite , our solver truncates the fourier expansion on a pre - specified window of frequencies @xmath547 for @xmath548 .",
    "we now describe the mcmc mutation steps used for propagating the @xmath11-particle system . for a tuning parameter @xmath549 ,",
    "a simple markov kernel suggested in several articles ( see e.g. @xcite and the references therein ) for target distributions that are gaussian changes of measure of the form is the following .",
    "given the current position @xmath523 , the proposal @xmath550 is defined as @xmath551 with @xmath552 ; the proposal is accepted with probability @xmath553 .",
    "proposal preserves the prior gaussian distribution for any @xmath549 and the above markov transition is well - defined on the infinite - dimensional space @xmath510 .",
    "it follows that the method is robust upon mesh - refinement in the sense that @xmath554 does not need to be adjusted as @xmath555 increases @xcite .",
    "in contrast , for standard random - walk metropolis proposals , one would have to pick a smaller step - size upon mesh - refinement ; for the optimal step - size , the mixing time will typically deteriorate as @xmath556 , see e.g.  @xcite .",
    "still , proposal can be inefficient when targeting the posterior distribution @xmath320 when it differs significantly from the prior distribution @xmath32 .",
    "indeed , _ a - priori _ the fourier coefficients @xmath557 have known scales appropriately taken under consideration in ; _ a - posteriori _ , information from the data spreads non - uniformly on the fourier coefficients , with more information being available for low frequencies than for high ones . taking a glimpse into results from the execution of the adaptive smc algorithm yet to be defined , in figure  [ ex2:circle ] we plot the fractions , as estimated by the smc method , between posterior and prior standard deviations for the fourier coefficient @xmath558 ( left panel ) and @xmath559 ( right panel ) over all pairs of frequencies @xmath560 with @xmath561 . in this case",
    "it is apparent that most of the information in the data concentrates on a window of frequencies around the origin ; still there is a large number of variables ( around @xmath562 in this example ) which have diverse posterior standard deviations under the posterior distribution .",
    "the standard deviations of these fourier coefficients can potentially be very different from their prior standard deviations .",
    "( left panel ) and @xmath559 ( right panel ) over all pairs @xmath560 with @xmath561 .",
    "the model here corresponds to : @xmath563 , @xmath564 , @xmath565 , @xmath566 , @xmath567 , @xmath568 , @xmath569 .",
    "the @xmath564 observation locations were at @xmath570 , @xmath571 , @xmath572 , @xmath573 .",
    "samples from the posterior were generated by applying a version of the adaptive smc algorithm described in section [ sec : what ] for @xmath574 , see @xcite for full details .",
    "the ` true ' initial condition was sampled from the prior ; data were then simulated accordingly.,width=604 ]    the approach followed in @xcite for constructing better - mixing markov kernels involves selecting a ` window ' of frequencies @xmath575 , for a user pre - specified threshold @xmath576 , and using the following markov mutation steps within an smc algorithm .",
    "* use the currently available particles approximation @xmath577 of @xmath5 to estimate the current marginal mean and covariance @xmath578 and @xmath579 of the two - dimensional variable @xmath580 over the window @xmath581 , @xmath582 for high - frequencies @xmath583 , only the information contained in the prior distribution is used and we thus set @xmath584 and @xmath585 . * for a current position @xmath586 , the proposal @xmath587 is defined as @xmath588 for @xmath589 and @xmath590 and @xmath591 for @xmath592 ; this proposal is accepted with the relevant metropolis - hastings ratio . * in addition to the above adaptation at the markov kernel , the analytical algorithm also involved an annealing step as described in section [ sec : annealed ] , whereby additional intermediate distributions were introduced , if needed , in between any pairs @xmath139 , @xmath5 .",
    "we found this to be important for avoiding weight degeneracy and getting a stable algorithm .",
    "as explained in section  [ sec : annealed ] , the choice of temperatures was determined on the fly , according to a minimum requirement of the effective sample size ( we choose @xmath593 ) .",
    "it is important to note that in this navier - stokes setting , the regularity assumptions adopted in the theoretical parts of this article for the derivation of the asymptotic results do not apply anymore . as illustrated by this numerical analysis",
    ", the asymptotic behaviour predicted in theorem [ thm.stability ] is likely to hold in far more general contexts .",
    "figure [ fig:1 ] shows a plot of an estimate of the variance of @xmath594 , where @xmath595 is the @xmath11-particle particle approximation of normalisation constant @xmath595 , as a function of the amount of data @xmath119 for an adaptive smc algorithm using @xmath596 particles . in this complex setting ,",
    "the numerical results seem to confirm the theoretical asymptotic results of theorem [ thm.stability ] : the estimated asymptotic variance seems to grow linearly with @xmath119 , as one would have expected to be true for the perfect smc algorithm that does not use adaptation .",
    "this is an indication that theorem [ thm.stability ] is likely to hold under weaker assumptions than adopted in this article .",
    "when the dimension @xmath488 of the adapted statistics is large , as in the navier - stokes case ( in our simulation study @xmath597 \\times 5 \\approx    500 $ ] ) and potentially in other scenarios , it is certainly of interest to quantify the effect of the dimensionality @xmath488 of the adaptive statistics on the overall accuracy of the smc estimators .",
    "we will make a first modest attempt to shed some light on this issue via the consideration of a very simple modelling structure motivated by the navier - stokes example and allowing for some simple calculations .",
    "for each @xmath16 we assume a product form gaussian target on @xmath598 , @xmath599 for a given sequence of variances @xmath600 that does not depend on the index @xmath16 .",
    "this represents an optimistic case where the incremental weights @xmath601 are small enough to be irrelevant for the study of the influence of the dimension @xmath488 ; we set @xmath602 .",
    "it is assumed that the smc method has worked well up - to time @xmath603 and has produced a collection of i.i.d .",
    "samples @xmath47 from @xmath139 . for the mutation step ,",
    "we consider an adaptive metropolis - hastings markov kernel @xmath456 preserving @xmath138 that proposes , when the current position is @xmath604 , a new position @xmath605 distributed as @xmath606 where we have set @xmath607 .",
    "this corresponds to the adaptive smc approach described in section [ sec : algo ] with a @xmath488-dimensional adaptive statistics @xmath608 .",
    "thus , the @xmath488 first coordinates of the proposal are adapted to the estimated marginal variance while the ideal variance is used for the remaining coordinates .",
    "we want to investigate the effect of the amount of adaptation on the accuracy of the estimator @xmath609 for a bounded function @xmath610 that only depends on the @xmath611-th coordinate , @xmath612 notice that in this simple scenario the metropolis - hastings proposal corresponding to the ideal kernel @xmath137 preserves @xmath5 and is thus always accepted ; under the ideal kernel , the particles at time @xmath119 would still be a set of @xmath11 i.i.d .",
    "samples from @xmath5 . consequently , any deviation from the @xmath613 rate of convergence for the estimator @xmath609 will be solely due to the effect of the adaptation .",
    "we now investigate in this context the behavior of the difference @xmath614 . following the proof of theorem [ theo : wlln ] we use the decomposition @xmath615({\\varphi } )",
    "= a(n ) + b_1(n ) + b_2(n)\\ ] ] where , using the notations of section [ sec : algo ] , we have set @xmath616({\\varphi})$ ] , @xmath617({\\varphi})$ ] and @xmath618(q_n { \\varphi})$ ] .",
    "denoting by @xmath619 the @xmath620-norm of random variables and conditioning upon @xmath621 , we have that @xmath622\\,\\big ] = \\mathcal{o}(\\tfrac{1}{n})\\ .\\ ] ] for @xmath206 one can notice that @xmath623 is a bounded mapping from @xmath624 to @xmath625 , thus @xmath626 = \\mathcal{o}(\\tfrac{1}{n})\\ .\\ ] ] the critical term with regards to the effect of the dimension @xmath488 on the magnitude of the difference @xmath627({\\varphi})$ ] is @xmath210 .",
    "an approach similar to equation in the proof of theorem [ thm.clt.unnormalised ] yields @xmath628({\\varphi } )   =   \\eta^n_{n-1}\\big ( \\ , \\big [ m_{n , n } -   m_{n } \\big ] ( { \\varphi } ) \\ , \\big ) \\\\ & = \\eta^n_{n-1 } \\big [ \\partial_{\\xi } m_{n } { \\varphi}\\big ] \\cdot [ \\eta^n_{n-1}-\\eta_{n-1}](\\xi_n ) + r   = : \\widetilde{b}_1(n ) + r\\ ,   \\label{eq : r } \\end{aligned}\\ ] ] for a residual random variable @xmath629 . controlling the residual term in the above expansion",
    "poses enormous technical challenges and we restrict our analysis to the main order term @xmath630 .",
    "[ pr : b ] the term @xmath630 satisfies @xmath631    see the appendix .",
    "proposition [ pr : b ] combined with - suggests that , in a high dimensional setting with @xmath632 , it is reasonable to choose @xmath11 of order @xmath633 , yielding a mean squared error of order @xmath634 .",
    "even if this choice of @xmath11 should be thought of as a minimum requirement for the complete sequential method , it could maybe explain the fairly accurate smc estimates of the marginal expectation obtained in the navier - stokes example when @xmath596 and @xmath635 ; we refer the reader to @xcite for further simulation studies .",
    "this article studies the asymptotic properties of a class of adaptive smc algorithms ; weak law of large numbers and a central limit theorems are established in several settings .",
    "there are several extensions to the work in this article .",
    "first , one could relax the boundedness assumptions used in the paper ; our proof technique , also used in @xcite , is particularly amenable to this .",
    "second , an approach to deal with the random stopping of some adaptive smc algorithms ( see section [ sec : annealed ] ) also needs to be developed .",
    "lastly , one can extend the analysis to the context of adaptive resampling .",
    "ab and at were supported by a singapore moe grant .",
    "aj was supported by singapore moe grant r-155 - 000 - 119 - 133 and is also affiliated with the risk management institute at the national university of singapore .",
    "first of all , notice that without loss of generality we can assume that @xmath636 .",
    "we have that : @xmath637/\\sqrt{d } \\label{eq : aa}\\end{aligned}\\ ] ] where we have set @xmath638 and @xmath639 .",
    "clearly , the expectation of the latter variable over @xmath139 is zero , but the same is also true for the former one .",
    "initially , we will focus on the term @xmath640 as it has some structure which will be exploited in subsequent calculations . indeed , considering @xmath641 , for an arbitrary @xmath642 and the rest @xmath643 , @xmath644 , at their limiting ` correct ' values , we have that : @xmath645   = { \\varphi}(x_{d+1 } ) + { \\mathbb{e}}\\,[\\,a(x_{j},\\xi_j , z_{j})\\,|\\,x_{j}\\,]\\,\\delta { \\varphi}(x_{d+1})\\ ] ] where we have set @xmath646 $ ] ; @xmath647 denotes the metropolis - hastings proposal for the @xmath611-th co - ordinate as specified in ; @xmath648 denotes the metropolis - hastings acceptance probability which depends only on the current position @xmath649 , the ( arbitrary ) scaling choice @xmath642 and the noise @xmath650 for simulating the proposal for the @xmath651-th co - ordinate assuming a scaling @xmath642 ( that is , we have @xmath652 )",
    ". we will give the explicit formula for @xmath653 below .",
    "notice that due to the proposal for @xmath654 preserving the target marginally at the @xmath611-th co - ordinate , we have that @xmath655 = 0 $ ] .",
    "recall that @xmath656 , thus to check for the differentiability of the mapping @xmath657 $ ] we can only resort to analytical calculations , starting from the fact that ( after some algebraic manipulations ) : @xmath658 after a lot of cumbersome analytical calculations ( which are omitted for brevity ) we can integrate out @xmath659 and find that i ) the derivative @xmath660|_{\\xi_j=\\eta_{n-1}(\\xi_{n , j})}$ ] exists ; ii ) @xmath661 , with @xmath662 , has a finite second moment .",
    "thus , continuing from we have : @xmath663 the factorisation in ( [ eq : fact ] ) will be exploited in the remaining calculations .",
    "continuing from ( [ eq : aa ] ) , we now have that : @xmath664 \\nonumber \\\\ & + \\tfrac{1}{d } \\sum_{\\substack{j , k=1,2,\\ldots , d\\\\",
    "j\\neq k } } n^2\\,{\\mathbb{e}}\\,\\big[\\ , \\eta_{n-1}^n(\\bar{\\xi}_{n , j})\\ , \\eta_{n-1}^n(\\bar{\\xi}_{n , j})\\,\\eta_{n-1}^n(\\bar{\\xi}_{n , k})\\,\\eta_{n-1}^n(\\bar{\\xi}_{n , k } )   \\,\\big ]   \\nonumber \\\\    & \\qquad \\qquad \\qquad = : t_1 + t_2 \\ .",
    "\\label{eq : b1}\\end{aligned}\\ ] ] the following zero - expectations obtained for terms involved in @xmath665 , @xmath666 are a direct consequence of the fact that @xmath667 only depends on @xmath649 and has zero expectation under @xmath139 , and that @xmath640 only depends on @xmath649 , @xmath654 through the product form in ( [ eq : fact ] ) with the @xmath654-term having zero - expectation ; critically , recall that particles @xmath668 are independent over both @xmath669 .",
    "focusing on the @xmath665-term and the expectation @xmath670 $ ] we note that all 4-way product terms arising after replacing @xmath29 with its sum - expression will have expectation 0 , except for the ones that involve cross - products of the form @xmath671 , thus : @xmath672 then , moving on to the @xmath666-term , notice that all 4-way products in the expectation term @xmath673 $ ] have expectation @xmath674 , except for the products involving the same particles @xmath675 .",
    "thus , we have that : @xmath676 thus , overall we have that : @xmath677 results ( [ eq : t1 ] ) , ( [ eq : t2 ] ) , used within ( [ eq : b1 ] ) complete the proof ."
  ],
  "abstract_text": [
    "<S> in several implementations of sequential monte carlo ( smc ) methods it is natural , and important in terms of algorithmic efficiency , to exploit the information of the history of the samples to optimally tune their subsequent propagations . in this article </S>",
    "<S> we provide a carefully formulated asymptotic theory for a class of such _ adaptive _ smc methods . </S>",
    "<S> the theoretical framework developed here will cover , under assumptions , several commonly used smc algorithms @xcite . </S>",
    "<S> there are only limited results about the theoretical underpinning of such adaptive methods : we will bridge this gap by providing a weak law of large numbers ( wlln ) and a central limit theorem ( clt ) for some of these algorithms . </S>",
    "<S> the latter seems to be the first result of its kind in the literature and provides a formal justification of algorithms used in many real data contexts @xcite . </S>",
    "<S> we establish that for a general class of adaptive smc algorithms @xcite the asymptotic variance of the estimators from the adaptive smc method is _ identical _ to a so - called ` perfect ' smc algorithm which uses ideal proposal kernels . </S>",
    "<S> our results are supported by application on a complex high - dimensional posterior distribution associated with the navier - stokes model , where adapting high - dimensional parameters of the proposal kernels is critical for the efficiency of the algorithm . + * keywords * : adaptive smc , central limit theorem , markov chain monte carlo .    </S>",
    "<S> * on the convergence of adaptive sequential monte carlo methods *    by alexandros beskos@xmath0 , ajay jasra@xmath0 , nikolas kantas@xmath1 & alexandre h.  thiry@xmath0    @xmath0department of statistics & applied probability , national university of singapore , singapore , 117546 , sg . </S>",
    "<S> + e-mail:`staba@nus.edu.sg , staja@nus.edu.sg , a.h.thiery@nus.edu.sg ` + @xmath1department of mathematics , imperial college london , london , sw7 2az , uk . </S>",
    "<S> + e-mail:`n.kantas@ic.ac.uk ` </S>"
  ]
}