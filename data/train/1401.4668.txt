{
  "article_text": [
    "neurons in sensory cortices of mammals often respond selectively to certain features of a stimulus .",
    "a well - known example in the visual system is the orientation of an elongated bar @xcite .",
    "this specificity of neuronal responses is believed to be a fundamental building block of stimulus processing and perception in the mammalian brain . although well studied for more than half a century now ,",
    "it is not fully clear which neuronal mechanisms generate this selectivity . in particular , it is not clear which kind of network structure is necessary for its establishment , and whether different system architectures are being employed by different species .    in the center of the current debate",
    "is the role of feedforward vs.  recurrent connectivity in the initial establishment of selectivity , as well as its further properties like contrast invariance @xcite and tuning sharpening @xcite .",
    "models relying on a purely feedforward structure , as it was originally suggested by @xcite , can not explain why the orientation tuning of both neuronal spiking and membrane potentials is invariant with respect to stimulus contrast @xcite ( see @xcite , however , for a more elaborate feedforward model to account for contrast invariance ) . on the other hand , the prevailing recurrent network models for the intra - cortical origin of contrast - invariant orientation selectivity @xcite can not explain how highly selective neuronal responses emerge in mice @xcite , as they rely on feature specific connectivity which rodents seem to lack , at least at the onset of eye opening @xcite . also , it is not clear whether the orderly arrangement of preferred features on the cortical surface , which has been described in most primates and carnivores @xcite , is necessary for the emergence of feature selectivity , or serves any function at all @xcite .",
    "different answers to these questions would have radically different implications for understanding higher brain functions , and how to study them .    here",
    "we investigate the emergence of contrast invariant orientation selectivity in large - scale networks of spiking neurons with dominant inhibition .",
    "the biological motivation for studying such networks come from experimental studies which show functional dominance of inhibition @xcite .",
    "the dominance of inhibition is probably a consequence of the dense , local pattern of inhibitory connectivity , which has been reported for different cortices @xcite . from a theoretical point of view ,",
    "such networks are well - studied @xcite and they have been shown to exhibit asynchronous - irregular ( ai ) activity states that are in many respects resembling the spiking activity recorded in the mammalian neocortex .    we first show that highly selective tuning curves , which are contrast invariant , can be obtained in these networks , even in absence of any feature - specific connectivity and any spatial network structure .",
    "we then analyze these networks by proposing a simplified mean - field description , which predicts the main properties of output orientation selectivity in the networks .",
    "the analysis identifies the mechanisms responsible for tuning amplification and contrast invariance .",
    "we show that the results hold for a wide range of parameters , and for networks operating in different recurrent regimes .",
    "we consider a large - scale network model of spiking neurons , representing a small volume of cerebral cortex .",
    "our model consists of a recurrent network of @xmath0 leaky integrate - and - fire ( lif ) neurons , @xmath1 excitatory and @xmath2 inhibitory @xcite .",
    "each neuron receives input from @xmath3 of the excitatory population and @xmath3 of the inhibitory population , sampled randomly .",
    "this is the same network configuration as the one considered by @xcite .",
    "the network is strongly inhibition dominated , as individual inhibitory synapses are arranged to be @xmath4 times more effective than the excitatory ones ( see methods for details ) .",
    "we fix the random connectivity , and only change the strength of recurrent coupling , measured as the amplitude of the postsynaptic potential ( psp ) at each synapse .",
    "the post - synaptic currents are modeled as alpha - functions , with time constant @xmath5 ( see methods ) .",
    "we refer to the peak value of psp , @xmath6 , as @xmath7 , and to the total psp as @xmath8 .",
    "second of the activity of a balanced network with recurrent synaptic couplings of amplitude @xmath9 , in response to a stimulus of orientation @xmath10 at a medium contrast ( @xmath11 ) .",
    "spikes of excitatory and inhibitory neurons are plotted in red and blue , respectively .",
    "the plot on the right shows the average firing rate of neurons , computed from the spike count during @xmath12 of simulation .",
    "the histogram on the bottom depticts the time resolved firing rates of the excitatory ( red ) and the inhibitory ( blue ) populations , respectively , using a time window of @xmath13 .",
    "the histogram on the bottom right shows the probability density function of time averaged firing rates of individual excitatory ( red ) and inhibitory ( blue ) neurons in the network , respectively .",
    "( * b * )  same spike trains as above , with neurons sorted according to their input preferred orientations ( indicated on the y - axis ) . as above",
    ", the plot on the right indicates the firing rate of each neuron computed from the spikes emitted during the simulation.,width=576 ]    the response of the network with @xmath9 to the stimulus of certain orientation , @xmath14 , is shown in fig .",
    "[ fig_rasplt_0.1 ] . the external input , @xmath15 , that a neuron receives is modeled as a homogeneous poisson process , which is slightly ( @xmath16 ) modulated by the orientation of the stimulus according to a cosine function : @xmath17 $ ] . a random preferred orientation ( po ) , @xmath18 , of the external input",
    "is assigned to each neuron .",
    "the feedforward efficacy is fixed in all simulations to @xmath19 .",
    "the raster plot of the activity for half a second is shown in fig .",
    "[ fig_rasplt_0.1]a , which indicates that with these parameters the network is indeed operating in the asynchronous irregular ( ai ) state .    ) and inhibitory ( blue , @xmath20 ) neurons , transforms weakly tuned input ( green ) to highly selective responses ( red ) .",
    "the sample tuning curves are shown for unit @xmath21 , which is an excitatory neuron .",
    "neurons have very similar tuning curves for different contrasts , and they are indistinguishable after normalization by their respective peak value ( see inset ) .",
    "( * b * )  spikes elicited by the same excitatory neuron as in ( a ) in response to different orientations of a stimulus at a medium contrast .",
    "( * c * )  more samples of output tuning curves from the network , in polar representation .",
    "radial axis encodes the firing rate of the respective neuron at each orientation , indicated by the angle , with the maximum firing rate indicated as a number next to each plot . both excitatory ( red ) and inhibitory ( blue ) neurons are highly selective in their responses , despite their weakly selective inputs ( green ) and recurrent inputs of mixed selectivities .",
    "lighter colors correspond to tuning curves for lower contrasts , respectively.,width=576 ]    if neurons are sorted according to their preferred orientation , the differences in the firing rates become visible ( fig .  [",
    "fig_rasplt_0.1]b ) .",
    "neurons with a preferred orientation closer to the orientation of the stimulus on average respond with higher rates , while the neurons closer to the orthogonal orientation are mostly silent .",
    "the cosine tuning of the input is reflected by the cosine tuning of firing rates across the population ( fig .",
    "[ fig_rasplt_0.1]b , right ) .",
    "if we repeat the stimulation of the network with different orientations , the individual tuning curves for each neuron are obtained . for a sample neuron from the same network",
    "this is shown in fig .",
    "[ fig_tuncurves]a - b .",
    "although the neuron receives input that is only weakly tuned , the network is capable of amplifying the selectivity , and the output tuning is much more pronounced . this emerging selectivity is independent of stimulus contrast , @xmath22 , reflected by the stimulus - specific change in the mean firing rate of the external input , @xmath23 .",
    "[ fig_tuncurves]a shows , for a sample neuron , that the shape of tuning curves remains unchanged for different contrasts . normalizing the output tuning curves by the peak value @xcite yields exactly the same curve for all input intensities ( fig .",
    "[ fig_tuncurves]a , inset ) .",
    "the other neurons in the network show the same behavior , as the polar plots in fig .",
    "[ fig_tuncurves]c demonstrate for a larger sample .",
    "note that excitatory and inhibitory neurons are both highly selective .    to quantify orientation selectivity across the population ,",
    "we compute two orientation selectivity indexes , osi @xcite , for all tuning curves ( fig .",
    "[ fig_osipop ] ) . both measures ( fig .  [ fig_osipop]a and b )",
    "show that the mean osi is increased by the network , to a level that is compatible with the results reported in animal experiments @xcite .",
    "moreover , the selectivity is maintained upon increasing the input intensity , as both the mean value and the shape of osi distributions are very similar for different contrasts .",
    "@xcite , in the network .",
    "lighter colors show the distributions for lower contrasts , respectively .",
    "all inputs have an osi of @xmath24 .",
    "( * b * )  distribution of an alternative measure of orientation selectivity often used by experimentalists @xcite .",
    "osi * is the difference of activity at preferred and orthogonal orientations , normalized by their sum , @xmath25 .",
    "@xmath26 and @xmath27 are obtained from the best fit of a cosine function to output tuning curves , evaluated at @xmath28 and @xmath29 , respectively .",
    "alternatively , osi * can be computed from a linear interpolation of data points ( inset ) .",
    "lighter colors show the distributions for lower contrasts , respectively .",
    "all inputs have an osi * of @xmath30 .",
    "( * c * )  the osi of all neurons for medium contrast ( mc ) vs.  low contrast ( lc ) , and for high contrast ( hc ) vs.  medium contrast are plotted in the left and right panels , respectively .",
    "the diagonal line indicates a perfect contrast invariance of osi .",
    "( * d * )  output po vs.  output osi for all neurons in the presynaptic pool of the neuron shown in fig .",
    "2a . a stimulus of medium contrast has been applied .",
    "the neuron receives input from presynaptic neurons that are themselves highly selective on average ( osi distribution on the right ) , and which uniformly cover the whole range of possible output pos ( distribution on top ) .",
    "the output osi and output po of the target neuron are @xmath31 and @xmath32 , respectively .",
    "other neurons receive similarly heterogeneous inputs ( not shown ) .",
    ", width=576 ]    to directly verify this invariance , we compare the osi of all neurons at different contrasts . plotting the osis at the medium contrast ( mc ) vs.  the lowest contrast ( lc ) , and at the highest contrast ( hc ) vs.  the medium ,",
    "indeed reveal that the majority of neurons show a remarkable robustness of their tuning curves upon a change in contrast ( fig .",
    "[ fig_osipop]c ) .",
    "the high selectivity in the network emerges despite the fact that each neuron receives input from a large pool of neurons with heterogeneous selectivity and different preferred orientations ( fig .",
    "[ fig_osipop]d ) .",
    "in fact , the po distribution of presynaptic neurons is essentially uniform ( fig .",
    "[ fig_osipop]d , top histogram ) , and the presynaptic osi distribution ( fig .",
    "[ fig_osipop]d , right histogram ) is very similar to the osi distribution of the whole population ( fig .",
    "[ fig_osipop]a ) .",
    "therefore , the output response is highly selective , despite the fact that the input is quite heterogeneous , as reported in experiments @xcite .",
    "this result is similar to a recent study of orientation selectivity in rodents @xcite , in that both show random networks are capable of generating selective output responses . in the following ,",
    "we provide a detailed mathematical analysis of the mechanisms involved in this process .",
    "our mean - field analysis indeed enables us to compute the mean output responses of networks quite precisely .      to illustrate the main network processing , we first recruit a linear rate model of the network",
    "we start by a reduced diagrammatic description of the network @xcite .",
    "this is equivalent to the description of the network in its stationary state , in terms of neuronal firing rates @xmath33 that arise as a result of a stimulus @xmath34 driving a recurrent network @xmath35 ( see methods , sect .",
    "[ sec_tempmean ] ) : @xmath36 the matrix @xmath35 encodes the recurrent synaptic connections in a network comprising @xmath37 neurons , @xmath34 and @xmath33 are the @xmath37-dimensional vectors of firing rates of input / stimulus and output / response , respectively , @xmath38 is the @xmath37-dimensional vector of time - averaged membrane potentials , and @xmath39 is the single - neuron gain . if @xmath40 , the input - output relation is given by @xmath41 and if the matrix @xmath42 is invertible , this readily implies @xmath43 the single - neuron gain @xmath39 in eq .",
    "( [ eq2a ] ) subsumes the feedforward processing of input to all neurons , before recruiting any lateral interactions ( @xmath44 in fig .",
    "[ fig_seppath]a ) .",
    "the operation of the recurrent network on a rate vector @xmath33 is given by the feedback @xmath45 appearing in the same equation .",
    "if the feedback gain @xmath46 was the same for all activity configurations , @xmath47 would be the associated closed - loop gain of the system ( fig .",
    "[ fig_seppath]a ) . in this case ,",
    "any stimulus would be amplified or attenuated by the network with a uniform factor @xmath48 . as a consequence ,",
    "if @xmath49 is a stimulus feature systematically varied in an experiment , the output tuning curves @xmath50 would have the same shape as the input tuning curves @xmath51 , as they would merely be rescaled by @xmath48 as a whole .",
    "however , the amplification of orientation selectivity observed in our simulated networks suggests that the amplification factor of the untuned part ( baseline ) is considerably smaller than that of the tuned part ( modulation ) . as a consequence , the selective amplification / attenuation of the networks considered here",
    "is reflected by an extended diagram ( fig .",
    "[ fig_seppath]b ) with two separate channels , and different overall gains for baseline and modulation , respectively . even if the feedforward gain is identical for baseline and modulation , the feedback gain is different for each stimulus component , leading to different closed - loop gains @xmath52 and @xmath53 for each branch of the diagram .    , shown for @xmath9 ( @xmath54 ) . for normalization , each entry is divided by the reset potential , @xmath55 .",
    "the ` exceptional eigenvalue ' ( green ) corresponds to the uniform eigenmode , i.e.  the baseline , and the bulk of the spectrum ( orange ) determines the response of the network to perturbations of a uniform input .",
    "bottom , left : eigenvalue distribution for the matrix @xmath56 , which gives the stationary firing rates .",
    "bottom , right : sorted magnitudes of eigenvalues of @xmath35 and @xmath56 .",
    "( * d * )  baseline and modulation components for individual neurons in the network with @xmath9 .",
    "scatter plot ( center ) shows the modulation vs.  baseline component of output tuning curves for all neurons of the network .",
    "baseline and modulation are taken as the mean ( f0 ) and the second fourier component ( f2 ) of individual tuning curves , respectively .",
    "the markers ( cyan crosses ) show the center of mass of each cloud .",
    "the histograms indicate the marginal distributions of baseline ( green , top ) and modulation ( orange , right ) components , respectively .",
    ", width=576 ]    the emergence of different processing pathways is a consequence of the linear recurrent dynamics : any activity vector @xmath57 ( describing either input @xmath34 to the network , or output @xmath33 from the network ) can be decomposed in terms of a sum @xmath58 .",
    "the part @xmath59 is a pure baseline vector , representing the mean response rate of each neuron across all stimuli .",
    "the remaining part @xmath60 is a pure modulation vector , with zero baseline . if the input is processed by a recurrent network that operates linearly on the input according to @xmath61 for some effective matrix @xmath62 ( cf .",
    "( [ eq2b ] ) ) it is evident that @xmath63 and @xmath64 ( see sect .  [ sec_bm ] for further explanation ) .",
    "in particular , there is no cross - talk in the processing of baseline , @xmath65 , and modulation , @xmath66 , whatsoever .",
    "the independent , non - interfering processing of the baseline and the modulation component of the input is exactly corresponding to the two separate pathways depicted in fig .  [ fig_seppath]b .    for",
    "the networks considered in this work , mean and variance of all inputs are identical , and all neurons in the recurrent network have the same number of excitatory and inhibitory recurrent afferents .",
    "therefore , the entries of the baseline vector @xmath65 of the input are all identical , and it is mapped to a baseline output firing rate @xmath67 , which is again a uniform vector .",
    "this means that uniform vectors are eigenvectors of the matrices @xmath35 and @xmath62 , respectively .",
    "the eigenvalue belonging to these eigenvectors is exactly the feedback gain @xmath68 of the baseline . in networks with dominant inhibition @xmath68",
    "is negative . as a consequence ,",
    "the corresponding closed - loop gain @xmath52 is a positive number , smaller than @xmath69 ( fig .",
    "[ fig_seppath]c ) .",
    "the effective enhancement of feature specificity mediated by the recurrent operation of the network is the result of different closed - loop gains for modulation , @xmath53 , and baseline , @xmath52 .",
    "for the example network of fig .  1 - 3 , this leads to comparable strengths of baseline and modulation in the output tuning curves ( fig .",
    "[ fig_seppath]d ) , despite having much weaker modulation in the input .    to see how these gains change with the strength of recurrent coupling",
    ", we fixed the network connectivity and only changed the post - synaptic amplitudes in the network .",
    "we then computed the mean baseline and modulation gains in each network , corresponding to cross marks in fig .",
    "[ fig_seppath]d .",
    "note that the gains are now computed from individual tuning curves , @xmath70 . for each output tuning curve ,",
    "the baseline and modulation component is computed as the zeroth and the second fourier component , respectively , and then the average values are computed across the population ( see methods for details ) . for the cosine tuning we are using here these gains are equal to population gains we described before ( for rate vectors over the population ) .",
    "the normalized gains , with respect to corresponding gains at zero recurrence , are plotted in fig .",
    "[ fig_basmodgains]a . increasing the strength of recurrent couplings in the network results in a monotonic decrease of @xmath52 ( green curve ) , since the network is inhibition dominated .",
    "the associated change of @xmath53 ( orange curve ) , however , is non - monotonic : it increases until a certain degree of recurrence is reached , and then it decreases slowly , while always remaining significantly larger than @xmath52 . as a result ,",
    "the modulation ratio , @xmath71 , of the network exhibits a peak for some degree of recurrence ( fig .",
    "[ fig_basmodgains]b ) , reflecting optimal performance with regard to tuning amplification .",
    "( green ) , and modulation gain , @xmath72 ( orange ) , for a network with a fixed connectivity , but different strengths of recurrent synaptic couplings .",
    "shaded regions represent @xmath73 .",
    "lighter colors correspond to lower contrasts .",
    "( * b * )  as a result of selective attenuation of the baseline , the normalized modulation - to - baseline ratio ( @xmath74 ) is generally much larger than @xmath69 .",
    "the degree of recurrence of the network presented in figs .",
    "1 - 4 is marked by dashed lines .",
    ", width=432 ]      one puzzling observation here is that the behavior of networks does not exhibit any dynamic instability as the recurrent coupling in the network is increased .",
    "this is counter - intuitive , as the radius of the bulk spectrum of the network , @xmath75 , increases linearly with the epsp amplitude in the network ( see eq .  [ eq_rho ] in methods ) .",
    "in fact , already at @xmath76 this radius is larger than @xmath69 , as illustrated in fig .",
    "[ fig_instabdyn]a , which could result in instability upon stimulating the network with a modulated input .",
    "however , the network does not show such instability .",
    ", shown for @xmath76 ( @xmath77 ) . the same normalization as in fig .",
    "[ fig_seppath]c is employed , i.e.  each entry is divided by the reset potential , @xmath55 .",
    "( * b - c * )  same plots as in fig .",
    "[ fig_rasplt_0.1]a , b , for @xmath76 .",
    ", width=576 ]    first , the network dynamics does not change significantly , as demonstrated in fig .",
    "[ fig_instabdyn]b and [ fig_instabdyn]c .",
    "moreover , the same functional properties are implied from the output tuning curves of neurons in the network ( fig .",
    "[ fig_instabfunc ] ) , without any sign of unstable operation .",
    "altogether , it seems that the networks experience a smooth transition as epsp amplitudes increase , and no abrupt change of the state .",
    "instabilities are avoided by some dynamic mechanism .",
    "we come back to this point in sect .",
    "[ sec_mgopn ] .    a , b , for a network with @xmath76 .",
    "( * c * )  contrast invariance of the osi measure , as in fig .",
    "[ fig_osipop]c , for @xmath76 .",
    "( * d * )  distribution of baseline ( f0 ) and modulation ( f2 ) component in a network with @xmath76 .",
    "labeling is the same as in fig .",
    "[ fig_seppath]d .",
    ", width=576 ]      to compute the gains for baseline and modulation of inputs , respectively , we employ a simplified mean - field approximation , which considers the corresponding average gains . for that , we need to compute the mean baseline and modulation rate of output tuning curves in the network .    to this end",
    ", we first compute the mean firing rate of output tuning curves , @xmath78 .",
    "this is obtained by assuming no modulation in the input , i.e.  as if all neurons are receiving the untuned component of the input tuning curve .",
    "this is justified by the fact that , in absence of strong non - linearities , the two pathways discussed above do not interfere with each other . we therefore employ mean field theory ( see sect .",
    "[ sec_scfr ] ) and compute the self - consistent baseline firing rates as done previously @xcite .",
    "the predicted result , for networks with different couplings and for stimuli with different contrasts , is shown in fig .  [ fig_basmodpred]a , which matches the simulated results quite well .    next , we compute the mean modulation component of output tuning curves , @xmath79 . here",
    "we make an approximation : we neglect the modulation of other neurons in the network and consider only the modulation of input to one neuron ( see sect .  [ sec_mfrmmp ] ) .",
    "this is equivalent to assuming ` perfect balance ' in terms of modulation , where all the modulation components of recurrent inputs from the network cancel each other perfectly ( @xmath80 ) , such that only feedforward modulation remains .",
    "based on this simplification , the mean modulation of the response , @xmath79 , is already well predicted ( fig .",
    "[ fig_basmodpred]b ) .",
    "the residual small discrepancy as compared to numerical simulations , which is most pronounced for intermediate recurrence and high contrast , should be accounted for by including network interactions ( sect .",
    "[ sec_mxpfrrn ] ) that amplify the modulation , and spike correlations that are ignored in the simplified treatment presented here .        from this result",
    ", we can also verify the non - interference property with regard to baseline and modulation .",
    "the separation of pathways proposed in fig .",
    "[ fig_seppath]b suggests that the modulated component of the input vector , @xmath81 , does not affect the baseline component of output tuning curves , @xmath78 .",
    "this we can verify directly by plotting the mean and standard deviation of baseline and modulation components of output tuning curves ( fig .",
    "[ fig_orthver]a ) .",
    "although the variance of the modulation component increases by increasing the recurrence in the network , the variance of baseline firing rate is very small and does not change with recurrence .",
    "it only begins to increase when the mean value of baseline and modulation components become comparable in size ( about @xmath9 ) .",
    "this is the point at which tuning curves experience partial rectification : for the tuning curves with modulation component larger than the baseline component , some rectification is implied .",
    "rectification , in turn , distorts the baseline component of tuning curves .",
    "the non - interference property of baseline and modulation can also be directly demonstrated from the weight matrix .",
    "the fact that baseline input does not have any component along the modulation vectors became clear from the eigenvector of @xmath35 that corresponds to the exceptional eigenvalue . to show the opposite , namely that an input modulation vector induces none , or only negligible , baseline in the output response ,",
    "explicit numerical simulations of the result of @xmath82 are performed .",
    "the result is shown in fig .",
    "[ fig_orthver]b , which demonstrates that the expected value of @xmath82 ( over orientation ) is exactly zero , therefore not introducing any baseline component , as we discussed above .",
    ", dots and solid lines indicate simulated and predicted values , respectively .",
    "shadings indicate @xmath73 . as in fig .",
    "[ fig_basmodgains ] , mean and standard deviation are evaluated over all neurons in the network .",
    "( * b * )  the product of the weight matrix @xmath35 with a baseline , @xmath65 , and a modulation input vector , @xmath66 .",
    "the entries of the baseline vector are all normalized to one , i.e.  the input to each neuron is @xmath69 .",
    "the operation of the weight matrix on the baseline vector , @xmath83 , is plotted in green .",
    "the operation of the weight matrix on the modulation vector , @xmath82 , for three different orientations ( @xmath84 ) are plotted in magenta , black and cyan , respectively .",
    "the corresponding distributions of individual responses are plotted in the magnified histograms on the right .",
    "note that the assumption of ` perfect balance ' implies a very narrow distribution around zero .",
    "the average ( over @xmath85 orientations ) of the responses , @xmath86 , are plotted in orange . for @xmath85 sample neurons from the network the response vs.  orientation of the stimulus",
    "are plotted in the inset ( center ) . , width=576 ]    therefore , baseline and modulation are processed separately and independently , with no cross - talk involved , provided the network acts linearly on its inputs .",
    "in contrast , we currently can not mathematically justify the assumption of perfect balance .",
    "the reason is that the modulation vectors , unlike the baseline vector , are not eigenvectors of the weight matrix , @xmath35 . as a result",
    ", it is not justified to replace @xmath35 in the product @xmath87 with a scalar value @xmath88 . treating the problem",
    "more rigorously could involve expanding the modulation vector in terms of the eigenvectors corresponding to the bulk eigenvalues of @xmath35 ( fig .",
    "[ fig_seppath]c ) , and obtaining the gain accordingly .",
    "this gain would not , in general , be a single scalar value , nor would it be exactly zero , as we have assumed here .",
    "we come back to a more precise treatment of the problem in sect .",
    "[ sect_ltrn ] .",
    "the assumption of perfect balance of modulation is the first - order approximation we make to obtain average gains of the network .",
    "here we numerically check how far this assumption is from the result of our simulations . to answer this question ,",
    "we investigate tuning of different components of the input to neurons in a network .",
    "we reconstruct the input from excitatory and inhibitory presynaptic sources by replacing each spike with the synaptic kernel ( alpha function ) and adding all the contributions to obtain a shot - noise signal .",
    "we then compute the mean value of this signal as the mean presynaptic excitation and inhibition , respectively .    .",
    "the total recurrent input ( excitatory + inhibitory ) , and the net modulation of the input ( external + total recurrent ) are also plotted ( in black and orange , respectively ) .",
    "the input is computed by replacing each presynaptic spike by an alpha kernel and computing the mean amplitude of the shot - noise signal ( in @xmath89 ) .",
    "@xmath90 corresponds to a mean membrane potential at the spike threshold of the neurons in our simulations .",
    "( * b * )  same for twelve sample neurons , along with their mean values ( thicker lines ) .",
    ", width=576 ]    fig .",
    "[ fig_inptun]a shows these inputs for four sample neurons from the network .",
    "the tuning of excitatory input is very weak for all samples ; the tuning of inhibition , however , is on average stronger .",
    "the output tuning of a cell results from a combination of contributions from feedforward and from recurrent inputs .",
    "the recurrent input can therefore change the feedforward tuning : it can either retain or change the preferred orientation of the feedforward input ( upper and lower panels , respectively ) , and it can either amplify or attenuate the tuning strength ( upper right and lower right , respectively ) . on average , however , the initial shape of the tuning curve is maintained ( fig .",
    "[ fig_inptun]b , traces and averages for @xmath85 cells ) , although the tuning of recurrent input leads to a deviation from the feedforward tuning , which creates a distribution of selectivity .",
    "the simplified mean - field analysis discussed above accounts for the average tuning curve and the mean selectivity in the network .",
    "it does not , however , account for the distribution of orientation selectivity across neurons . in this section , we resort to a linear analysis of modulation processing , in order to provide an approximative analytic treatment of this distribution .    for this linear analysis we need to make two additional assumptions .",
    "first , we assume that modulations in the network can be treated as small perturbations about the baseline , and that the dynamics can be linearized about this operating point .",
    "note that this assumption implies that the contribution of nonlinearities like rectification is negligible .",
    "second , we assume that the mixture of tunings is linear . this assumption is justified as we have used cosine tuning ( i.e.  linear tuning ) in the inputs ( see methods for details ) .",
    "this allows us to represent each tuning curve as a 2d feature vector . since the operation of network on feature vectors is linear , the mixture of tunings is now reduced to the vectorial summation of corresponding tuning vectors ( see below , and methods ) .",
    "we therefore start first by linearizing the dynamics about the operating point , i.e.  the baseline . after computing the baseline firing rate , @xmath78 ,",
    "as described before , we compute the mean and standard deviation of the input to each neuron in the baseline state ( @xmath91 and @xmath92 , respectively ; eq .   in methods ) as a function of baseline input @xmath23 and baseline firing rate @xmath78 @xmath93 ) , \\nonumber \\\\",
    "\\sigma_b^2 & = \\tau [ j_s^2 s_b + j^2 r_b n \\epsilon ( f + g^2(1-f ) ) ] .",
    "\\nonumber \\end{aligned}\\ ] ] we then compute the linear gains by perturbing the input with a small @xmath94 .",
    "we dismiss all contributions of order two or higher in the perturbation parameter and write @xmath95 where @xmath96 is the change in the output firing rate resulting from a perturbation of the input rate @xmath94 . here , we compute these gains numerically by solving the perturbed mean field equations about the basline ( see methods , and eq .  [ eq_zeta_num ] ) .",
    "[ fig_lintun]a shows the input - output relationship for the network with @xmath76 at the highest contrast .    , of a neuron produced in response to a small perturbation , @xmath97 , in the input intensity , plotted for different baseline inputs corresponding to different contrasts .",
    "the response is computed by numerically perturbing the mean - field equations ( see methods ) .",
    "the linear gain , @xmath98 , is then computed by linear regression of data points . for this example with @xmath76 , the value @xmath99 is obtained , which is also used for the next panels .",
    "( * b * )  for the sample neuron in fig .",
    "[ fig_tuncurves]a , all presynaptic tuning vectors are extracted ( @xmath100 , weighted by the linear gain ( @xmath101 ) , and vectorially added together , reflecting linear integration in neurons .",
    "although each presynaptic vector makes only a small contribution , the resulting random sum can lead to a large resultant tuning vector .",
    "these are generally larger for presynaptic inhibition ( presyn .",
    "inh . ) compared to presynaptic excitation ( presyn .  exc . ) .",
    "note different scales of axes .",
    "( * c * )  left panel : the resultant vectors for recurrent excitation ( rec .",
    "exc . ) , recurrent inhibition ( rec .  inh . ) , total recurrent ( rec .",
    ". + rec .  inh . ) , feedforward input ( input ) , and the total input ( tot .",
    "= input + rec .  tot . ) are plotted .",
    "all normalized input tuning vectors have the same length of one , denoted by the green circle .",
    "right panel : total recurrent tuning vectors ( rec .  tot . ) for all neurons in the network are compared with the normalized length of their input tuning vectors ( green circle ) . *",
    "d * )  distribution of the length of all tuning vectors for all the neurons in the network .",
    "dashed lines show the predicted distributions of the linear analysis in each case ( see text for details ) . , width=432 ]    after computing the linear gains , we can rewrite the linear rate equations of the network for the modulation pathway .",
    "the modulation in the firing rate of each neuron is caused by a linear mixture of external input and the input it receives from the recurrent network @xmath102 both of these terms are now weighted with the extra factor , @xmath101 , from linearization , which is the effective gain of modulation at this operating point . if we rewrite the equation as @xmath103 it can further be approximated by @xmath104 here we are neglecting the contribution of higher - order recurrent inputs ( @xmath105 , @xmath106 , ... ) in the processing .",
    "next step is to interprete the above equation for tuning curves .",
    "since we have assumed cosine tuning for the input , we can represent each input tuning curve by a vector , @xmath107 .",
    "likewise , we represent output tuning curves by vectors @xmath108 . we refer to these vectors as the tuning vectors . for a 2d feature like orientation selectivity",
    "we obtain 2d tuning vectors .",
    "the angle of this vector represents the input preferred orientation @xmath109 , and the length of it is a measure of orientation selectivity ( it is indeed equal to osi before normalization ) . for notational convenience",
    ", we represent these 2d vectors by complex numbers .",
    "we identify real parts with x - directions , and imaginary parts with y - directions .",
    "any 2d vector then corresponds to a complex number in a one to one fashion .",
    "the mixture of tuning curves in cosine tuning is now simplified to the summation of the corresponding tuning vectors in the complex plane ( see methods for further details ) .",
    "we can therefore rewrite the linear rate equation eq .   for tuning vectors @xmath110 here @xmath111 and",
    "@xmath112 are vectors with complex elements , representing output and input tuning of all neurons in the network , respectively .",
    "this is now an equation which expresses the output tuning vectors in terms of a linear mixture of input tuning vectors .",
    "similar to eq .  , we are neglecting higher - order terms ( @xmath113 , @xmath114 , ... ) here .    an individual output tuning vector , @xmath108 ,",
    "is then given by @xmath115 : the output tuning of each neuron is a mixture of its input tuning and weighted vectors of all presynaptic sources . for the specific example of the neuron in fig .",
    "[ fig_tuncurves]a , all the contributions of presynaptic sources are shown in fig .",
    "[ fig_lintun]b , for excitatory and inhibitory populations separately .",
    "each small jump in the space represents the contribution of a presynaptic tuning vector , the size of which is @xmath116 or @xmath117 for excitatory and inhibitory presynaptic sources , respectively .",
    "@xmath107 is normalized to @xmath69 .",
    "although each presynaptic contribution is much smaller than the input ( of order @xmath118 in this case ) , the resultant vector ( dashed lines ) can be large .",
    "in particular , the resultant vector of inhibition is comparable to the length of the input vector for this specific example ( fig .",
    "[ fig_lintun]c , left ) .",
    "since the angle of the vector is close to the input angle , this leads to an amplification of the resultant tuning , although it typically also changes the preferred orientation ( fig .",
    "[ fig_lintun]c , left ) .",
    "this explains why the osi of this neuron ( @xmath31 ) is larger than the mean osi of the network ( @xmath119 , see fig .",
    "[ fig_tuncurves ] and [ fig_osipop ] ) .",
    "not all the vectors resulting from recurrent contributions , however , are large , nor do all have a similar preferred orientation as the input tuning .",
    "indeed , as the connectivity is random and the initial preferred orientations are assigned randomly to each neuron , the preferred orientation of the resultant vectors are also random .",
    "this is shown in fig .",
    "[ fig_lintun]c , right panel , where all the recurrent tuning vectors are explicitly computed ( by reading the input tuning vectors and the connectivity ) , and plotted in the complex plane .",
    "the distribution of the vectors in this plane is a 2d gaussian distribution , according to the central limit theorem . as a result ,",
    "most of the tuning vectors have small magnitude , and only a few of greater magnitude contribute to the tail of the distribution .    the distribution of this length is plotted in fig .",
    "[ fig_lintun]d , for different subpopulations of neurons .",
    "the peak of the distribution for excitatory neurons is at smaller values than for inhibitory neurons , and the overall length of recurrent tuning is mainly determined by inhibition in the network .",
    "knowing the standard deviation of distributions of tuning vectors , one obtains the distribution of vector lengths according to @xmath120 ( see methods , eq .  ) .",
    "in this example , the standard deviations are @xmath121 , @xmath122 and @xmath123 , for excitation , for inhibition , and for all recurrent neurons , respectively .",
    "the length of tuning vectors predicted by this result is plotted in fig .",
    "[ fig_lintun]d ( dashed lines ) .",
    "the length of overall tuning , i.e.  recurrent tuning vectors vectorially combined with the input tuning vector ( green ) , can also be computed ( see methods , eq .  ) .",
    "normalizing the input tuning vectors to length @xmath69 , the distribution amounts to @xmath124 , plotted in the same figure ( dashed purple line ) .    from fig .",
    "[ fig_lintun]d one can compare the strength of the input tuning ( green line ) with the tuning generated within the random network ( black distribution ) .",
    "the mean length of the recurrent tuning is smaller than the feedforward , and only few neurons show comparable tuning strength .",
    "the distribution of the combined tuning strength ( purple line ) , which is a mixture of feedforward and recurrent components , has now a broad distribution , where many neurons show less tuning than their input ( less than @xmath69 , attenuated ) , and many more have enhanced selectivity ( greater than @xmath69 , amplified ) . in general , amplification happens when the randomly generated tuning vector within the network is roughly aligned with the initial tuning vector , and attenuation happens for recurrent tuning vectors in the opposite directions .- periodic parameter @xmath14 is now mapped to a @xmath125-periodic parameter @xmath126 , an opposite direction here implies an orthogonal orientation in the original parameter space . ] a random recurrent network , thus , in itself is capable of attenuating and amplifying orientation selectivity .",
    "this is a mechanism in addition to the selective gains of baseline and modulation described before .",
    "this mechanism , however , comes at the expense of shifting the tuning curves of neurons from their initial , feedforward preferred orientations .      as fig .",
    "[ fig_po_scat]a shows , in a weakly coupled network the po of neurons are hardly changed with respect to their input po .",
    "this is a regime where feedforward projections are dominant with respect to functional properties of neurons . under these conditions ,",
    "the recurrent network can not compensate the increase in the baseline , and both baseline and modulation are scaled identically ( fig .",
    "[ fig_tc_shapes ] ) . as a result",
    ", neuronal tuning curves tend to simply reflect the tuning of the input ( fig .  [ fig_tc_shapes ] ) , and tuning curves reduce their selectivity for high - contrast inputs , because the feedback compensation is weak . although the average orientation selectivity index ( osi ) of neurons in the network could be high for the lowest contrast in a weakly connected network , this selectivity is lost when the contrast is increasing ( fig .",
    "[ fig_osi_trdoff]a ) .",
    "( see methods ) .",
    "the maximum value of this index is @xmath127 , which corresponds to a uniform distribution of @xmath128 .",
    "darker colors show higher contrasts , respectively . ]    as the strength of recurrent couplings increases , the contribution of the network becomes more effective to attenuate the baseline and selectively enhance the modulation ( fig .",
    "[ fig_tc_shapes ] and [ fig_basmodgains ] ) .",
    "this leads to more stable osi distributions across different contrasts ( fig .",
    "[ fig_osi_trdoff]b and [ fig_osi_trdoff]c ) , and hence makes the selectivity more robust .",
    "moreover , as a consequence of stronger recurrence in the network , output pos deviate more from their initial po ( fig .",
    "[ fig_po_scat]b and [ fig_po_scat]c ) , since the strength of recurrent contributions ( recurrent tuning vectors ) has now increased .",
    "this is summarized for all networks in fig .",
    "[ fig_po_scat]d by a scatter degree index ( sdi ) , which quantifies the degree of po deviation in each network .",
    "a ) , for different degrees of recurrence in the network , as indicated by different epsp amplitudes .",
    "( * b * )  more ( @xmath129 ) sample tuning curves from the network , aligned to their input po .",
    "red and blue curves show excitatory and inhibitory output tuning curves for the medium contrast , respectively . shown in green",
    "is the input tuning curve , normalized to the average ( over the population ) of the mean ( over all orientations ) of all tuning curves in the network .",
    "( * c * )  mean and standard deviation ( across neurons ) of all aligned tuning curves , for networks with different degrees of recurrence .",
    "lighter shadings denote lower contrasts , respectively.,width=576 ]    notably , sdi does not linearly increase with recurrent connection strength .",
    "rather , it saturates for rather weak connections , and then reaches an asymptotic value .",
    "the reason for this behavior is that the contribution of recurrent tuning vectors in the final tuning depends on @xmath130 and the linear gain , @xmath101 , as we described in the previous section ( see eq .  ) .",
    "although @xmath130 is monotonically increasing in fig .",
    "[ fig_po_scat]d , the effective strength of recurrent tuning vectors depends on the product @xmath131 .",
    "it appears , therefore , that the linear gains are not increasing as @xmath130 increases , or they are even decreasing .",
    "this trend is further supported by shape and size of the tuning curves ( fig .  [ fig_tc_shapes ] ) . for networks with strong recurrent coupling ,",
    "the maximum firing rate of tuning curves decreases and the modulation component becomes smaller .",
    "since the linear gains determine the embedded gain of neurons in the network in response to modulations , this trend also suggests that these gains are decreasing in the highly recurrent regime .",
    "this was indeed visible in the behavior of gains and firing rates for modulations , shown in fig .",
    "[ fig_basmodgains]a and [ fig_basmodpred]a , respectively .",
    "b. ( * b * )  the mean osi of all neurons in the network is shown for different levels of recurrence , at three different contrasts .",
    "lower contrasts are plotted in lighter colors .",
    "increasing the recurrence makes the osi less susceptible to changes in contrast . for high recurrences ,",
    "this invariance comes at the expense of a decreased selectivity , as the mean osi in the network decreases .",
    "this trade - off between selectivity and invariance is quantified ( in brown ) by the average ( over contrasts ) of mean osi ( across neurons ) divided by the standard deviation ( over contrasts ) of the average osi ( across neurons ) . ]",
    "although increasing the recurrence stabilizes the osi , it makes the neurons of the network less feature selective , if the recurrence is too large ( fig .",
    "[ fig_osi_trdoff ] , a - d ) .",
    "there is , therefore , a trade - off between orientation selectivity and contrast invariance in the networks . increasing the recurrence makes the negative feedback in the baseline pathway stronger , making the divisive suppression of the baseline  and hence the contrast invariance  more effective .",
    "this comes , however , at the expense of a decrease in the gain in the modulation pathway , which makes the responses weaker and less selective .",
    "we have quantified this trade - off by dividing the mean osi of individual tuning curves in a network by its standard deviation across different contrasts .",
    "the intermediate recurrent regime shows optimal behavior with large and stable osi ( fig .",
    "[ fig_osi_trdoff]d ) , and it more or less coincides with the region of optimal tuning amplification ( fig .",
    "[ fig_basmodgains ] ) .",
    "it is also informative to look at the membrane potential dynamics of neurons in the network .",
    "[ fig_vmtrace]a shows the membrane potential of a sample neuron in response to a stimulus of its preferred orientation , and the orthogonal one .",
    "the mean membrane potential remains , on average , below threshold , as it is reflected in the distribution of membrane potential ( fig .",
    "[ fig_vmtrace]a , right panel ) , and only fluctuations in the input leads to spiking activity . for the orthogonal orientation ,",
    "this activity is very sparse ; for the preferred orientation it leads to a reasonable firing rate .",
    "if we plot the mean membrane potential for each orientation , it shows the same tuning as of the spiking activity ( fig .",
    "[ fig_vmtrace]b ) .    , at the medium contrast .",
    "traces of the membrane potential are plotted for @xmath132 of response to the preferred ( red ) and its orthogonal ( cyan ) stimulus orientation .",
    "the histograms of the membrane potential for @xmath12 of stimulation are shown on the right .",
    "( * b * )  traces of the membrane potential along with the elicited spikes for @xmath85 orientations , for @xmath132 of recording .",
    "the tuning curves of the mean membrane potential ( red ) and the corresponding firing rate ( black ) is computed from @xmath12 of stimulation in the right panel.,width=576 ]    plotting the free membrane potential for this neuron ( fig .",
    "[ fig_vmtc]a , left ) , and more samples from the same network ( fig .",
    "[ fig_vmtc]a , center ) , verifies this observation . the free membrane potential ( fig .",
    "[ fig_vmtc]a , right ) remains below threshold and has a similar tuning .",
    "indeed , the tuning is slightly enhanced , since the negative contribution of the reset voltage is now corrected for .",
    "this behavior is consistent across different contrasts .",
    "increasing the contrast shifts the mean tuning curve as a whole down to more negative values , as a result of more negative feedback recruitment in the network .",
    "this , in turn , compensates for the increase in baseline , and suppresses the response to non - preferred orientations .",
    "the tuned part , however , is still capable of generating spikes , which leads to the tuning of spiking activity .     for different contrasts",
    ". center : average tuning curve of @xmath133 ( @xmath85 excitatory , @xmath85 inhibitory ) randomly sampled neurons , ranging over all pos .",
    "the tuning curves are aligned at a po of @xmath134 .",
    "error bars indicate @xmath73 over sampled neurons . to improve the display ,",
    "they are plotted only for every third data point . the mean membrane potential ( over all neurons and all orientations )",
    "is indicated by solid , horizontal lines in each case .",
    "the shading represents the standard deviation ( over neurons ) of the mean ( over orientations ) of the average membrane potential ( @xmath135 ) .",
    "right : same as the panel in the center , for the free membrane potential , @xmath136 .",
    "( * b * )  the same @xmath73 ( over sampled neurons ) of tuning curves for the distance to threshold of free membrane potentials ( @xmath137 ) , for different regimes of recurrence .",
    "for the lowest contrast , most of the error bars are smaller than the size of markers and hence not visible .",
    "the mean membrane potential and its standard deviation over the ( sampled ) population is shown , as before , by horizontal lines and shadings , respectively.,width=480 ]    if the recurrent compensation was not effective , a different behavior would emerge .",
    "in fact , if the recurrent coupling is very weak ( fig .",
    "[ fig_vmtc]b , left ) , the free membrane potential is above threshold for almost all orientations . in such a case ,",
    "the response to all orientations is in the mean - driven regime , which yields significant firing rates for the preferred as well as orthogonal orientations . as a result",
    ", the so - called iceberg effect broadens the tuning curves significantly upon increasing the contrast . increasing the recurrent coupling shifts the mean membrane potential down and the network operates in the fluctuation driven regime ; this makes the tuning of membrane potential and the resulting spiking activity more robust and contrast invariant ( fig .",
    "[ fig_vmtc]b , center and right panels ) . indeed , for the intermediate recurrent regime ( fig .",
    "[ fig_vmtc]b , center ) the tuning is perfectly contrast invariant .",
    "the recurrent excitation in inhibition - dominated networks of the sort we are studying here is over - compensated by the surplus recurrent inhibition .",
    "some residual inhibition remains as the net effect of recurrent interactions .",
    "if the recurrent coupling is strong enough , this net inhibition determines the effective threshold of neurons in the network .",
    "therefore , it is not the threshold mechanism of neurons which cuts off the responses at non - preferred orientations .",
    "balance of excitation and inhibition within the network , in contrast , governs the generation of spikes and , hence , attenuation and amplification of the baseline and modulation components , respectively .    .",
    "( * b * )  spike triggered averages ( sta ) of excitatory ( exc . ) and inhibitory ( inh . )",
    "recurrent inputs are plotted from spikes of @xmath85 randomly sampled neurons in response to one stimulation ( @xmath12 ) . the total recurrent input ( tot . )",
    "is plotted in black .",
    "the membrane potential is normalized by @xmath138 ( norm .",
    ", width=432 ]    a sample of this temporal balance is shown in fig .  [ fig_sta]a for the net excitatory and inhibitory inputs from the network to a neuron .",
    "although the net excitation is on average above the firing threshold , the net inhibition is twice as large on average , as a result of the parameter configuration used .",
    "moreover , occasional deflections in excitation and inhibition follow each other on a fine time scale .",
    "the net recurrent input to the neuron is on average negative .",
    "occasional disbalance , however , provides a net excitatory drive for brief periods of time .    considering spike - triggered averages of the net excitatory and inhibitory input ( fig .",
    "[ fig_sta]b ) reveals that spike emission in the network is mainly governed by recurrent inhibition , rather than recurrent excitation .",
    "therefore , in the inhibition - dominated regime , fluctuations of inhibition is the main determining factor for spiking activity , in agreement with the results of experimental studies @xcite .",
    "the strength of net inhibitory feedback also affects the selective gains . whereas the mean inhibitory recurrent input sets the divisive gain in the baseline pathway ( fig .",
    "[ fig_seppath]b ) , it affects the modulation gain by determining the mean distance of the membrane potential to threshold . as suggested by fig .",
    "[ fig_vmtc]b , increasing the level of recurrence induces larger average distances of the mean membrane potential from threshold .",
    "this is shown for all networks in fig .",
    "[ fig_vm_modgain]a .    using the simplified mean - field analysis provided in section [ sec_mfa ]",
    ", we can predict the mean membrane potential of a network .    knowing the baseline firing rate of the network , @xmath78 , the mean baseline membrane potential of the network , @xmath135 , is obtained ( see eq .   in methods ) as @xmath139 ,   \\label{eq_mempot}\\ ] ] where @xmath140 ( see methods , eq .  ) .",
    "note that , as the input is shunted during the refractory period , the shunted feedforward and recurrent input should be subtracted from the total input in this expression .",
    "the shunted input can be computed as @xmath141 @xcite . the corrected membrane potential , after considering the effect of refractory period , is then given as @xmath142 .",
    "\\label{eq_mempot_refper}\\ ] ] the result of this prediction for different contrasts is plotted in solid lines in fig .",
    "[ fig_vm_modgain]a .",
    "sizes ( circles ) , at different contrasts , along with the predicted values ( solid lines ) . mean membrane potential is computed as the average ( over orientations ) of mean tuning curves of @xmath85 sample neurons ( the same as in fig .",
    "[ fig_vmtc]a ) .",
    "lighter colors belong to lower contrasts , respectively .",
    "( * b * )  input modulation normalized by the distance to threshold , @xmath143 , ( solid lines ) compared to the output modulation ( orange circles ; same as fig .",
    "[ fig_basmodpred]b ) .",
    "input modulation is given as the input modulation rate times its efficacy ( @xmath144 ) .",
    ", width=576 ]    note that this prediction is obtained under the assumption of a gaussian distribution of input to all neurons .",
    "the prediction , therefore , fails to be exact if this assumption is violated .",
    "the distribution is , in fact , skewed as a result of correlations in the network @xcite .",
    "the deviation from a gaussian distribution increases for higher recurrences , explaining the discrepancy of our predictions for highly recurrent regimes .",
    "the mean membrane potential is crucial in determining the overall gain of the linearized dynamics .",
    "a very depolarized membrane potential reduces the chance of an input perturbation to reach the firing threshold and to elicit a spike .",
    "therefore , it affects the gain of modulation , as shown in fig .",
    "[ fig_vm_modgain]b : the mean output modulation of tuning curves , @xmath53 , is indeed inversely related to the mean distance to threshold .",
    "this suggests that the embedded gain of neurons in the network in response to modulation is also inversely proportional to the distance to threshold .",
    "although the mean modulation in the input is below the spike threshold of a single neuron , fluctuations are nevertheless capable of eliciting reasonable firing rates .",
    "the resultant linearization of the @xmath145-@xmath146 curve , as shown in fig .",
    "[ fig_lintun]a , is a result of the noise , @xmath92 , generated within the network due to the balance of excitation and inhibition .    , black line )",
    "is computed at each @xmath7 and is compared with the corresponding modulation gain , @xmath147 ( orange ) .",
    "@xmath101 is computed as @xmath98 , for a small perturbation of the input , @xmath148 .",
    "inset : the radius , @xmath75 , of bulk spectrum of @xmath149 , normalized by the linear gain ( @xmath101 ) at each @xmath7 . instead of dividing @xmath130 by @xmath138 ( as in fig .",
    "[ fig_seppath]c ) , @xmath130 is now multiplied by @xmath101 . as a result",
    ", the normalized radius is now obtained as @xmath150}$].,width=432 ]    if we now compute these gains for networks with different degrees of recurrence , the linearized gains match well with the mean modulation gain in the network ( fig .",
    "[ fig_lingain_stabspec ] , for the middle contrast ) .",
    "this suggests that the mean amplification of modulation in the network is fully accounted for by the linear gain , which in turn depends on the operating point of the network as defined by the mean , @xmath91 , and the standard deviation , @xmath92 , of the input to neurons from the network in the baseline state .",
    "the linear gains are obtained by perturbing @xmath151 at this operating point @xmath152 and using @xmath153 . here",
    ", we determine this quantity numerically as @xmath154 for @xmath148 ( fig .",
    "[ fig_lingain_stabspec ] ) .    in summary , the inhibitory feedback in a recurrent network contributes to orientation selectivity in crucial ways .",
    "first , it provides a negative feedback which offsets the baseline component of the input tuning curves and leads to divisive attenuation of the common mode ( selective attenuation in the baseline pathway ) .",
    "second , it sets the operating point of the network and determines the linearized , embedded gain , which in turn determines the modulation gain ( selective amplification in the modulation pathway ) .",
    "moreover , the feature selectivity generated by the recurrent network as a result of summing many inputs of random selectivity leads to either amplification , or attenuation , of the feedforward tuning ( random summation of recurrent tuning ) . since the contribution of each presynaptic modulation vector must be weighted according to the linearized gain , the bulk spectrum of the connectivity matrix @xmath149 must also to be weighted accordingly .",
    "the spectrum of the network with @xmath76 shown in fig .",
    "[ fig_instabdyn]a , for instance , was obtained by normalizing @xmath130 by @xmath138 .",
    "the linear gain suggests now that @xmath130 should be multiplied by @xmath99 , which is a factor @xmath155 smaller than @xmath156 .",
    "this leads to a decrease in the radius of the bulk from @xmath157 } \\approx 1.68 $ ] to @xmath158 } \\approx 0.87 $ ] .",
    "this implies that none of the modulation modes corresponding to the bulk are actually unstable , at this operating point of the network .",
    "indeed , if we plot the normalized radius , @xmath159 , for all the networks at different contrasts , the @xmath159 never exceeds one ( fig .",
    "[ fig_lingain_stabspec ] , inset ) .",
    "this means that , although the coupling strength is monotonically increasing , the network dynamics stabilizes the spectrum in inhibition - dominated networks ( see @xcite for related observations ) .",
    "using large - scale simulations and associated mean - field analysis of networks of spiking neurons , we have demonstrated how highly selective neuronal responses can be obtained in random networks without any spatial or feature specific structure .",
    "our mathematical analysis pinpoints the mechanisms responsible for selective attenuation of the common mode and selective amplification of modulation , and predicts some essential properties of these networks .",
    "here we discussed the specific case of orientation selectivity in the early visual system , as we were able to link our findings to an ample body of experimental literature .",
    "however , our model could potentially be of a much broader scope .",
    "it proposes a general mechanism for the emergence of strong feature selectivity , which could actually be at work in other sensory modalities as well .",
    "our network model can thus be conceived as a generic model for the local cortical circuitry , which enhances feature selectivity and ensures contrast invariance of processing , without resorting to feature specific structure or experience - dependent fine tuning .",
    "our analysis suggests that a randomly connected network with dominant inhibition is already capable of selectively removing the uninformative common mode of a stimulus that is represented by the network in a distributed fashion , while preserving the informative modulations in the response pattern induced by stimulation .",
    "this way , the tuned part gains salience , and the signal - to - noise ratio improves .",
    "the network also amplifies the tuned component ( signal ) by two mechanisms : first , by modulating the embedded gain through adjusting the operating point of the network , and second , by recurrently mixing presynaptic selectivities and thereby amplifying weakly tuned inputs in some neurons .",
    "the same mechanisms could also lead to attenuation of the signal in the network .",
    "first , increasing the recurrent coupling in the network increases the mean distance of the membrane potential to the firing threshold , which in turn decreases the modulation gain in the network .",
    "second , the recurrent mixing of weak tunings in the network generates a distribution of selectivity , with attenuation in many neurons . as a result ,",
    "this mechanism can not increase the selectivity of a certain fraction of the neuronal population beyond the input selectivity , unless this is compromised by a decrease in the selectivity of another fraction of the population .",
    "in addition , there is a trade - off between selectivity and contrast invariance of the neuronal responses . increasing the degree of recurrence in the network makes the selectivity more invariant and the distribution of it more robust with regard to variations of contrast , but it decreases the overall gain of modulation . as a result",
    ", there is a region of intermediate recurrence in our networks , where tuning amplification is most pronounced , and orientation selectivity has the largest value with the lowest sensitivity to contrast .",
    "our computational study , therefore , suggests an intermediate regime of recurrence as the optimal regime of feature selectivity for early sensory processing .",
    "this is the state of the network which exhibits the stimulus driven properties of neurons observed in experiments @xcite , while preserving other important features like strong and contrast invariant orientation tuning . in this regime ,",
    "the feature selectivity of neurons would exhibit the least deviation from their input selectivities , essentially reflecting the tuning of the feedforward input .",
    "the role of the recurrent network at this stage would then be to enhance this selectivity , by performing operations like increasing the signal - to - noise ratio and contrast invariant gain control .",
    "this scenario might best explain the state of the input layer l4 , in which orientation selectivity first emerges in the cortex .",
    "the same is not necessarily true for more recurrent layers like l2/3 or l5 , which are involved in later stages of sensory processing like learning , association and motor control .",
    "it is , therefore , plausible that different regimes of recurrence exist in different layers , which may be suited to perform different types of processing .",
    "one measure for the degree of recurrence in a network is the tuning of the total recurrent input . as the recurrent coupling increases in a network , the tuning of the recurrent input generated within the network increases as well , and the assumption of untuned total input becomes a questionable approximation .",
    "there are indeed contradictory results reported in experimental studies on the tuning of input in rodents : both untuned inhibition @xcite and co - tuning of excitation and inhibition @xcite have been reported by different laboratories .",
    "our results show that even in absence of significant tuning of the total input received from the recurrent network , another mechanism of selective attenuation and amplification can lead to strong selectivity and contrast invariance .",
    "this , however , does not exclude a random summation of selectivity within the recurrent network as a contributing mechanism .",
    "indeed , in the first example network we investigated here , both mechanisms were at work .",
    "it is , therefore , plausible that both tuned and untuned components exist to some degree in such networks , but the exact mixing depends on the operating point and , specifically , on the degree of recurrence .",
    "this would therefore suggest that in more recurrent layers like l2/3 more neurons with strong input tuning should be recorded , while in the input layer l4 untuned inputs preponderate .",
    "it should be noted , however , that our discussion here applies to recurrent excitation and inhibition .",
    "tuned excitation and inhibition , when measured in terms of the total excitatory and inhibitory conductances in intracellular recordings , are the total excitatory and inhibitory input that a neuron observes .",
    "it is therefore possible that the tuning of the feedforward input is dominant in the tuning .",
    "likewise , feedforward inhibition , mediated by disynaptic inhibition , can have the same tuning as the feedforward excitatory input , as the former is mediating it .",
    "the mechanisms discussed here , however , apply to recurrent excitation and inhibition , since they are a consequence of the dynamics of a network of synaptically connected neurons and , in particular , recruitment of feedback inhibition within the network .",
    "recurrent inhibition in our networks selectively feeds the mean signal back and subtracts it from the tuning curves .",
    "the overall effect of this subtraction results in a divisive attenuation of the baseline .",
    "this untuned suppression has been experimentally demonstrated to play a crucial role in increasing the selectivity @xcite .",
    "more specifically , it has been recently shown that in l4 of mouse visual cortex , it underlies sharpening of orientation selectivity @xcite .",
    "this is also consistent with the results of a recent study on the role of somatostatin expressing , som@xmath160 , gabaergic neurons in orientation selectivity @xcite ( but see @xcite ) .",
    "the subtraction of the baseline , attributed to this specific subtype of inhibition by @xcite , effectively leads to the selective attenuation / division of the baseline , as described in the present article .",
    "reduction of this inhibition would therefore lead to a constant increase in the baseline activity of the tuning curves , which has indeed been recently reported in dlx1(-/- ) mice with selective reduction of activity in dendrite targeting inhibitory interneurons @xcite .",
    "this mechanism is in contrast to the role of feedforward inhibition of fast spiking interneurons , parvalbumin expressing , pv@xmath160 , gabaergic neurons .",
    "as opposed to som@xmath160 neurons , which are more recurrent , these neurons are mainly driven by feedforward input @xcite . unlike som@xmath160 neurons , which are involved in dendritic computation and controlling the input , pv@xmath160 neurons are better suited for controlling the output , as they innervate the peri - somatic regions @xcite .",
    "consistently , they are also more effective during the transient responses , as reflected in their activation pattern @xcite .",
    "in contrast , som@xmath160 neurons are better suited for sustained activity .",
    "these properties might then suggest that feedforward inhibition is primarily involved in gain control @xcite , which uniformly rescales all components of the tuning curves .",
    "the attenuation of the baseline , therefore , comes at the expense of attenuating the modulation",
    ". it can not be selective to the baseline , in contrast to the recurrent mechanism we have modeled here .",
    "note that , for simplicity , we have not considered feedforward inhibition in our model .",
    "however , feedforward inhibition could easily be added to the model by mediating the same excitatory input to each neuron by an inhibitory neuron .",
    "doing so would effectively lead to a change in the overall feedforward gain , provided that inhibition is not strong enough to result in rectification ( compare with @xcite ) .",
    "if pv@xmath160 neurons are strongly driven by feedforward input , and if the feedforward input is only slightly tuned , as we assumed here , the responses of pv@xmath160 neurons should be only weakly modulated .",
    "in contrast , as the results of our simulation showed here , inhibitory neurons involved in recurrent computations can be highly selective , although they receive weakly modulated inputs . in agreement with this ,",
    "som@xmath160 inhibitory neurons involved in recurrent inhibition have been reported to be as selective as excitatory neurons , in contrast to pv@xmath160 neurons with broader selectivity @xcite .",
    "however , it should be possible to make pv@xmath160 interneurons more selective , by providing more recurrent inhibition to them .",
    "in fact , it has recently been reported that activating som@xmath160 inhibitory neurons can unmask and enhance the selectivity of pv@xmath160 cells by suppressing untuned input @xcite .",
    "note that , as contrast invariance depends on the selective attenuation of the baseline , it should be the result of a recurrent mechanism .",
    "we therefore suggest that the constant increase of untuned inhibition that neurons receive upon increasing the contrast @xcite should be a result of the recurrent , and not of the feedforward , inhibition .",
    "this may explain why dark reared mice in this experiment , which lacked a broadening of pv@xmath160 responses , still show an aggregate untuned input from the network and hence highly selective responses @xcite . although individual inhibitory neurons were on average highly selective in our networks , the emergent result of the interaction of excitation and inhibition lead to an effective untuned inhibition , which increases proportionally with contrast .",
    "this is again consistent with the results of @xcite , who demonstrated that `` blocking the broadening of output responses of individual inhibitory neurons does not block the broadening of the aggregate inhibitory input to excitatory neurons '' .",
    "it is also consistent with the results of a previous report , demonstrating that `` broad inhibitory tuning '' of fast spiking cells is `` not required for developmental sharpening of excitatory tuning '' @xcite . based on these results",
    ", we therefore hypothesize that untuned inhibition might be an emergent property of an inhibition dominated network , and not a feedforward consequence of broadly tuned fast spiking neurons .",
    "most existing recurrent theories of orientation selectivity consider the case of species like carnivores and primates , with a clustered organization of selectivity in orientation maps @xcite .",
    "consistent with the proximity of neurons with similar selectivity , these theories assume a feature specific connectivity of neurons .",
    "the mexican hat profile of connectivity which they assume leads to a more broadly tuned inhibition , which suppresses the mean , and to a sharper tuning of excitatory input , which amplifies the modulation .",
    "therefore , these models can not be applied to the case of a salt - and - pepper structure , as found in rodents , with no apparent spatial or feature specific connectivity .    even in species with orientation maps , there seem to be some issues with these models .",
    "first , they rely on  and predict  a sharpening mechanism of selectivity due to tuned recurrent excitation . however ,",
    "the late ( presumably recurrent ) sharpening of selectivity , which these theories predict , has not been observed in experiments @xcite .",
    "also , the orientation selectivity of neurons seem to be the same as their feedforward input , since the preferred orientation of neurons does not change with recurrent interactions @xcite . rather than sharpening of tuning curves",
    ", a more plausible function of the recurrent network is increasing the modulation ratio , by suppressing the baseline @xcite .",
    "this was indeed the main mechanism of orientation selectivity we described in our networks here .",
    "as it is based on essentially linear processing , our model predicts no sharpening of the tuning as a result of recurrent interactions , but only an increase in modulation depth , not affecting the tuning width @xcite .",
    "sharpening of tuning curves would only be a consequence of the feedforward nonlinearity , reflected in a nonlinear transfer function of neurons .",
    "as our results do not depend on the power - law transfer function of single neurons , our model would also work if the operating regime of the cortex suggested a smaller exponent of the power - law @xcite .",
    "moreover , as we demonstrated above , this mechanism does not have to be accompanied , on average , by a large shift between the input and output preferred orientations .",
    "another consequence of the sharpening theories is the emergence of strong pairwise correlations in the network @xcite .",
    "this seems not to be consistent with the very low correlations reported in the neocortex @xcite .",
    "more specifically , it has recently been shown that highly selective neurons in the input layer of monkey v1 exhibit very low noise correlations @xcite .",
    "this imposes an important constraint on recurrent models which need sharper tuning of excitatory input to the neurons as compared to inhibition , as this sharper tuning leads to higher noise correlations in the local network ( see figure 5 in @xcite ) .",
    "hence , it might be difficult for these models to simultaneously account for both sharp orientation selectivity and low pairwise correlations in the input layers .",
    "the mechanism we discussed here , in contrast , does not need  and not predict ",
    "strong pairwise correlations in the network .",
    "in fact , our mean field analysis was even based on the assumption of no correlations in the network . as the network operates in the ai state",
    ", the amount of linear read - out of information from our networks would therefore be several times higher than in sharpening theories , comparable to feedforward models @xcite .    in comparison to feedforward models",
    ", however , our analysis suggests that contrast invariant tuning of both membrane potentials and spiking activity @xcite can robustly and reliably emerge through the action of a recurrent network .",
    "contrast invariance is a critical property of feature selectivity , which ensures reliable and consistent feature detection for a wide range of different stimuli . without a network mechanism of the sort described here",
    ", neurons would need a specific fine - tuning for each contrast , in order to be selective for the same feature .",
    "the network mechanism proposed here provides a generic mechanism to dynamically achieve contrast invariance , without the need for feature specific wiring , special correlation structure , power - law transfer function , contrast - dependent variability , shunting inhibition , synaptic depression , adaptation or learning .",
    "however , it remains to be experimentally verified whether intra - cortical recurrent connectivity is indeed necessary for contrast invariance , or whether feedforward mechanisms are enough to account for this phenomenon @xcite .",
    "a crucial experiment would be to test whether the tuning of the membrane potential is still contrast invariant if lateral interactions in the cortex are deactivated @xcite .",
    "there are , however , several issues which need to be further examined in future works .",
    "first , our analysis is mainly provided to compute the mean values of baseline and modulation gains in the network .",
    "it is therefore necessary to extend the analysis such that it accounts for the distribution of these gains .",
    "also , the model assumes cosine tuning of inputs ( linear tuning ) , and linear network operation ( e.g.  no rectification in the tuning curves ) .",
    "it would therefore be revealing to see if , and to which degree , the results of the linear analysis hold in the presence of nonlinearities reported to exist in the biological cortex @xcite .",
    "we used current based lif neurons with unconstrained membrane potentials in our simulations , since this gave us the opportunity to perform a theoretical analysis of network dynamics .",
    "it is however necessary to test to which degree the results of our study change by recruiting a different neuron model .",
    "for instance , using an alternative neuron model like concuctance - based lif neurons might not allow the distance to threshold of the membrane potential to increase unboundedly .",
    "this was not the case here , as we did not impose any minimum bounday condition on our current - based lif neurons .",
    "such a difference may change the effective gain of neurons and , as a result , a different eigenvalue spectrum might be obtained .",
    "this , in turn , may change network dynamics and lead to a qualitatively different behavior of the network .",
    "it might also have consequences for the structure of correlations in the network , and may affect the ai state .",
    "our preliminary results indicate that imposing a lower boundary condition on current based lif neurons can amplify correlations and synchrony in the network , to the extent that the network does not operate anymore in the ai state .",
    "feature selectivity is still obtained and even enhanced in the network .",
    "tuning curves show a higher average osi and maximum firing rates , more rectification and reduced tuning widths follow from this , while contrast invariance is preserved ( not shown ) . such a scenario should be analyzed in more detail in future studies .",
    "it is also important to study the effect of different connectivity patterns in the network . here , we have modeled the dominance of inhibition by increasing the relative strength of ipsps .",
    "an alternative implementation is to increase the density of inhibitory axonal projections , which increases the density of connectivity .",
    "dense pattern of connectivity has been reported for inhibition in different cortices @xcite , and seems to be a common motif .",
    "such a change in network connectivity might have consequences for sensory processing .",
    "for instance , a decrease in temporal fluctuations of the local inhibitory population and , likewise , in the quenched noise of preferred orientations is implied , which can , in turn , affect the tuning of inputs and amplification or attenuation of orientation selectivity .",
    "also , the model and the analysis provided here should be extended to account for networks with spatial structure .",
    "it should be analytically studied how distance dependent connectivity , and in particular different connectivity profiles for excitation and inhibition , affect the results obtained here .",
    "it has been shown , for instance , that balanced networks can show topologically invariant statistics @xcite .",
    "it would therefore be interesting to see if the same analysis also applies to functional properties of these networks .",
    "of special interest would be to study how the spectrum of the network changes @xcite , and to which degree this predicts the operation of the network , in particular , how the spatial extent of excitation and inhibition affects this behavior .",
    "experimentally , it has been reported that inhibition is more local than excitation in terms of anatomical projections .",
    "many theoretical models , however , assume broader inhibition for convenience . studying the functional properties of networks with realistic patterns of connectivity",
    "might therefore shed light on this aspect .      as we were primarily interested in the emergence of orientation selectivity in species without orientation maps , we studied here random networks with salt - and - pepper structure .",
    "however , the model could be extended to networks with spatial or functional maps , which imply feature specific connections . as opposed to the mexican hat profile assumed in the ring model , if one now assumes a more realistic pattern of local inhibition and longer range excitation , different dynamic properties might follow @xcite .",
    "the analysis of the new regime of orientation selectivity therefore calls for a further study .",
    "the results of this modeling would in turn help identifying the basic mechanisms that are responsible for the emergence of orientation selectivity in different species with different structures , and to eventually provide an answer to the question whether common design principles exist , or whether different strategies have been recruited by different species .",
    "we studied networks of leaky integrate - and - fire ( lif ) neurons . for this spiking neuron model , the sub - threshold dynamics of the membrane potential @xmath161 of neuron @xmath162",
    "is described by the leaky - integrator equation @xmath163 the current @xmath164 represents the total input to the neuron , the integration of which is governed by the leak resistance @xmath165 , and the membrane time constant @xmath166 .",
    "when the voltage reaches the threshold at @xmath167 , a spike is generated and transmitted to all postsynaptic neurons , and the membrane potential is reset to the resting potential at @xmath168 .",
    "it remains at this level for short absolute refractory period , @xmath169 , during which all synaptic currents are shunted .",
    "to simulate spiking inputs to neurons from outside the network ( e.g.  from the lateral geniculate nucleus , lgn ) , we resorted to the conceptually simpler model of a poisson process .",
    "the associated surrogate spike trains have the property that spikes are generated randomly and independently with a prescribed firing rate at each point in time .",
    "the linear superposition of an arbitrary number of poisson processes ( as in the case of multiple afferents ) is again a poisson process .",
    "the rate of the superposition process is exactly the linear sum of the rates of its components , and it can be effectively simulated as a single process with high rate .",
    "the networks considered in this study comprised @xmath170 neurons , @xmath171 of which were excitatory and @xmath172 inhibitory .",
    "synaptic connections were drawn randomly and independently , such that each neuron received exactly @xmath173 inputs from the excitatory and @xmath174 from the inhibitory neuron population , respectively .",
    "this amounted to an overall connectivity of @xmath175 , as suggested by statistical neuroanatomy of local cortical networks @xcite .",
    "the wiring imposed in our model was in accordance with dale s principle , i.e.  each neuron formed the same type of synapse with all its postsynaptic partners , either excitatory or inhibitory @xcite .",
    "self - connections were excluded .",
    "if @xmath176 is the time of a spike elicited by neuron @xmath162 , we use a dirac delta - function @xmath177 to represent it as a time dependent signal . the sum @xmath178 then stands for a spike train .",
    "the input @xmath164 to each neuron is the sum of all excitatory and inhibitory postsynaptic currents ( pscs ) induced by presynaptic spikes that arrive at its dendrite , and the hyperpolarizing currents responsible for the reset after each output spike .",
    "assuming that all currents are pulse - like , the dynamic equation for the network is obtained from @xmath179 .",
    "\\label{eq_networkmodel}\\ ] ] after each spike , the membrane potential was reset to the resting potential at @xmath180 , therefore the size of the voltage jump is @xmath181 .",
    "the cross - neuron coupling @xmath182 encodes the amplitude of the postsynaptic potential ( psp ) , corresponding to a synapse from neuron @xmath183 ( source ) to neuron @xmath162 ( target ) .",
    "a uniform transmission delay of @xmath184 was assumed for all recurrent synapses in the network . the spike train @xmath185 stands for the accumulated external input to neuron @xmath162 , and the corresponding synapses have connection strength of amplitude @xmath39 .    in all the simulations described in our paper , in fact , we used stereotyped synaptic transients of finite width , instead of normalized impulses , as postsynaptic currents .",
    "all synaptic kernels had the shape of an alpha - function @xmath186 , with a fixed time constant @xmath187 , replacing the delta - functions in the spike trains .",
    "the peak amplitude of the kernel is @xmath6 , to which we refer as @xmath7 to denote the strength of post - synaptic potentials .",
    "the parameter @xmath182 corresponds to the integral of the psp , which is @xmath188 .    in this model , keeping all time constants at fixed values , the efficacy of a synaptic connection is determined by the peak amplitude of the psp . for any specific network",
    ", we assumed that all recurrent excitatory synapses induce excitatory postsynaptic potentials of the same peak amplitude , @xmath7 .",
    "the peak amplitudes of inhibitory postsynaptic potentials were taken to be a fixed multiple , @xmath189 , of the excitatory ones , such that @xmath190 .",
    "for all our results presented in the main text , individual inhibitory couplings were assumed to be much more effective than excitatory ones : the excitation - inhibition ratio was fixed at @xmath191 . as a consequence , recurrent connectivity in our networks",
    "was characterized by a net surplus of inhibition , since the small number of inhibitory neurons was over - compensated by the strength of individual inhibitory couplings . fixing",
    "the balance between recurrent excitation and inhibition in this way is an important concept in models of cortical dynamics , although measurements in real brains are difficult ( see e.g.  @xcite ) .    in different simulations",
    ", we used excitatory synapses with an epsp amplitude in the range between @xmath180 and @xmath192 .",
    "accordingly , inhibitory synapses had ipsp amplitudes between @xmath180 and @xmath193 . all external inputs in our simulations were excitatory , and the amplitude of their synapses , @xmath194 , was fixed at @xmath195 throughout all simulations .",
    "this configuration of parameters , combined with a stationary driving input to each neuron in the network , was previously shown to induce relatively low rates in all neurons , while spike trains are irregular , and pairwise correlations remain weak @xcite .",
    "these properties are known to be a result of complex recurrent network dynamics , and not a consequence of random inputs ( e.g.  poisson spike trains ) that drive the network @xcite .",
    "inhibitory feedback actively decorrelates the network activity @xcite .",
    "the resulting states of network dynamics are dubbed asynchronous - irregular , ai , and they are thought to closely resemble the dynamic states of neocortical networks recorded with extracellular electrodes @xcite .    in this parameter setting , the degree of recurrence in any specific network is essentially determined by the amplitude of excitatory postsynaptic potentials , @xmath7 , of the recurrent connections .",
    "recurrence can be effectively quantified by the spectral radius of the connectivity matrix @xmath35 , which scales linearly with the epsp amplitude .",
    "this fact is explained in more detail below .      to explore the tuning curves of neurons in a network",
    ", we simulated their responses to stimuli with different orientations .",
    "beyond excitatory and inhibitory input from the recurrent network , each neuron received extra excitatory input , the firing rate of which exhibited a slight dependence on stimulus orientation .",
    "this external input can be conceived as the overall effect of stimulation , and it includes inputs from lgn , and possibly afferents from other , non - local cortical sources . in our simulations ,",
    "the input was implemented as a homogeneous poisson process , with an average firing rate , @xmath15 , depending on the stimulus orientation @xmath14 according to @xmath196 .",
    "\\label{eq_s}\\ ] ] the baseline @xmath23 is the mean level of input across all orientations .",
    "we used a logarithmic relation between input contrast @xmath22 and input baseline , @xmath197 , as a practical way to specify the input intensity , inspired by biology .    in all our simulations , the relative amplitude , @xmath198 of the stimulus dependent modulation",
    "was fixed to a fraction of @xmath199 of the baseline level , corresponding to setting @xmath200 .",
    "the parameter @xmath18 signified the stimulus orientation at which the neuron received its maximal input , @xmath201 .",
    "it represented the initial preferred orientation , @xmath202 , of the neuron , a parameter that was randomly and independently assigned to each neuron in the population .    to measure the output tuning curves in numerical simulations",
    ", we stimulated the networks for @xmath85 different stimulus orientations , covering the full range between @xmath203 and @xmath204 in steps of @xmath205 .",
    "each simulation was run for @xmath206 , using a simulation time step of @xmath207 . in order to include only steady state activity into our analysis , and to avoid onset transients , the first @xmath208 in each simulation",
    "were discarded .",
    "the output tuning curve of any neuron in the network was obtained in terms of its average firing rate @xmath209 in response to each stimulus orientation @xmath14 , and normally plotted as a curve @xmath210 .",
    "an output tuning curve would be termed contrast invariant , if its overall shape does not depend on the contrast , @xmath22 , of the stimulus .    to explore the interaction between feedforward and recurrent connectivity on orientation selectivity , we systematically changed two parameters in our networks : the mean input firing rate , @xmath23 , and the epsp amplitude as a measure for the strength of recurrent coupling .",
    "we changed these two parameters in a network , while fixing all other parameters , including the network topology given by a specific realization of the random synaptic connectivity , @xmath35 , the inhibition - excitation ratio , @xmath189 , and the input modulation ratio , @xmath198 .",
    "we used three different values for the baseline intensity @xmath211 , @xmath212 , and @xmath213 .",
    "this is corresponding to low , medium , and high contrast , @xmath214 , @xmath215 , and @xmath216 , respectively .",
    "contrast invariance of the membrane potential tuning is , in the case of a tuned spike response , compromised by the reset mechanism in our neuron model : after each spike , the membrane potential is reset to its resting value , which exerts a negative contribution to the membrane potential , which effectively imposes the opposite tuning as compared to the output spiking . as a result , when the neuron fires more ( higher contrasts ) , it inevitably attains a more negative membrane potential . to correct for this phenomenon",
    ", we add this negative contribution back to the membrane potential ( @xmath136 ) or , equivalently , keep the neuron from spiking by raising its threshold to very high levels . in membrane potential recordings from real neurons , tt is also common to correct for this phenomenon by cutting out the spikes including their aftereffects . if used with care , this procedure is essentially equivalent to the correction we applied here .",
    "the implementation of the lif model employed in the present study is based on a numerical method known as `` exact integration '' @xcite .",
    "numerical simulations of all networks were performed using the neuronal simulation environment nest @xcite .",
    "this tool has been developed to support the reliable , precise and performant numerical simulations of networks of spiking neurons .        to quantify orientation selectivity",
    ", we computed the preferred orientation , po , and the orientation selectivity index , osi , of each neuron from its respective tuning curve , @xmath210 , obtained in numerical simulations . to this end , we first computed the circular mean @xcite of the firing rate measured at each orientation , which we call the orientation selectivity vector , osv , @xmath217 the po , @xmath18 , was then extracted as the angle , @xmath218 , of the osv .",
    "its length , @xmath219 , yielded a measure for the degree of orientation selectivity , osi @xcite . for a highly selective neuron , which is only active for one orientation , and remains silent for all other orientations , the osi would be @xmath69 ; for a completely unselective neuron responding with the same firing rate for all orientations , this measure returns @xmath220 .    for better comparison with the experimental literature ( see e.g.  @xcite ) ,",
    "an alternative measure of orientation selectivity has also been computed for the tuning curves obtained in our simulations .",
    "it is given by @xmath221 where @xmath222 is the firing rate at the preferred stimulus orientation , @xmath18 , and the @xmath223 is the firing rate for the orientation orthogonal to it .",
    "since the output preferred ( and hence the orthogonal ) orientations of a neuron are computed from eq .  , we need to interpolate between the data points of a tuning curve to obtain @xmath224 and @xmath225 . to do this",
    ", we fit a cosine function to the tuning curve sampled at @xmath85 equidistant orientations , employing a nonlinear least squares method .",
    "then , the cosine fit of the tuning curve is evaluated at @xmath226 and @xmath227 to obtain @xmath224 and @xmath225 , respectively .",
    "negative numbers , whenever occurring , were replaced by @xmath220 .",
    "this is similar to what experimentalists typically do ( see e.g.  @xcite , although by fitting different functions , like a gaussian or a von mises probability density ; also see @xcite ) , therefore allows us to compare our distributions to their reported results . and",
    "@xmath225 from the smoothened tuning curves , namely by linear interpolation between the data points .",
    "the result of this is compared with the result of the cosine fit in fig .",
    "[ fig_osipop]b , and does not change the conclusions qualitatively . ]    for a perfect cosine tuning curve according to eq .",
    "( [ eq_s ] ) , we obtain @xmath228 irrespective of the baseline firing rate @xmath23 , and of the preferred orientation @xmath18 .",
    "thus , in the case of our input tuning curves with @xmath200 , we have @xmath229 and @xmath230 , respectively .      to quantify the processing of baseline and modulation in a specific network , we compute the mean baseline and mean modulation gains over all neurons . to obtain this ,",
    "we first compute baseline and modulation gain for individual tuning curves , @xmath231 , as follows .",
    "the baseline is obtained by averaging the tuning curve over all orientations @xmath232 where @xmath233 is the firing rate of @xmath234-th neuron in the network in response to a stimulus with the @xmath235-th orientation , @xmath236 , and @xmath237 is the number of different orientations considered in the simulation .",
    "we refer to this as the f0 component of tuning curves .",
    "the modulation is conveniently obtained from the absolute value of the second fourier component of the tuning curve @xmath238 we refer to this as the f2 component of a tuning curve .    the baseline gain and the modulation gain are then defined as the output value divided by the input value , respectively @xmath239 the input baseline and modulation , @xmath240 and @xmath240 , are obtained from input tuning curves in the same fashion as @xmath241 and @xmath241 were obtained from the output tuning curves , respectively ( eq .   and , respectively ) .",
    "the corresponding mean values , @xmath52 and @xmath53 , for any network are the average over all individual gains .",
    "finally , each gain is normalized by its value for a network with no recurrence , @xmath242 and @xmath243 , which are obtained from the simulation of a network with no recurrence , @xmath244 @xmath245      the transformation of preferred orientation induced by a recurrent network is visualized by a scatter plot showing output po vs.  input po for all neurons ( fig .",
    "[ fig_po_scat ] ) .",
    "weakly recurrent networks essentially preserve the preferred orientations of the input to each neuron , leading to scatter plots centered about the diagonal . for networks with increased recurrence ,",
    "the output po deviates from the input po , and off - diagonal elements occur more frequently . to quantify the deviation , we first compute the difference , @xmath246 , for each neuron .",
    "observe that orientation should be taken modulo @xmath204 and @xmath247 represents the largest possible difference between input and output po .    as a numerical measure for the total degree of po scatter in a network",
    ", we computed the scatter degree index , sdi ( fig .",
    "[ fig_po_scat]b ) .",
    "its definition is based on the circular mean @xmath248 where @xmath37 is the number of neurons in the network .",
    "the sdi is then given by the angular deviation @xcite , which can be computed from the length @xmath249 according to @xmath250 note that as @xmath128 spans the half - cricle , i.e. the range @xmath251 $ ] , we have taken half the resultant angle as the sdi .",
    "if all output pos are exactly the same as input pos , sdi returns zero ; the maximum scatter from the input pos corresponds to a uniform distribution of @xmath128 , for which sdi returns @xmath127 .",
    "the tuning curves considered in this work reflect time - averaged firing rates of neurons in a recurrent network . from our numerical simulations , it became clear that the time averaged membrane potential is indicative of the operating point of the network with regard to the tuning properties of its neurons ( see sect .  [ sec_results ] ) .",
    "therefore , we begin our analysis by considering time averaged equations .    assuming stationarity",
    ", we form temporal averages @xmath252 of all dynamic variables that occur in eq .",
    "( [ eq_neuronmodel ] ) and ( [ eq_networkmodel ] ) .",
    "since there can be no drift of the time averaged membrane potential in this case , we have @xmath253 .",
    "we write @xmath254 for the time averaged membrane potential , @xmath255 for the mean firing rate of neuron @xmath162 in the network , and @xmath256 for the mean firing rate of its external input , respectively .",
    "we obtain an equation that relates the stationary firing rates of all neurons in the network with their mean membrane potentials @xcite @xmath257 .",
    "\\label{eq_statrate}\\ ] ] observe that transmission delays do not matter for temporal averages , and that the above equation holds for networks of lif neurons with arbitrary connectivity .    from now on , we rescale eq .",
    "( [ eq_statrate ] ) such that all firing rates are expressed in units of @xmath258 , and all voltages are given in units of @xmath138 .",
    "for a network of @xmath37 neurons , the recurrent synaptic connectivity is encoded by a fixed @xmath259 coupling matrix @xmath260 .",
    "the external inputs , the firing rates and the membrane potentials of all neurons are represented by the @xmath37-dimensional vectors @xmath261 , @xmath262 and @xmath263 , respectively .",
    "the time averaged equation above then reads , in matrix - vector notation @xmath264 solving for the vector @xmath33 of recurrent firing rates , we obtain @xmath265    we assume that the matrix @xmath42 is always invertible , with inverse @xmath62 .",
    "if two out of the three variables @xmath34 , @xmath33 and @xmath38 are known , the third one can then be computed in a straightforward fashion .",
    "we now specifically consider a recurrent network of excitatory and inhibitory neurons , as discussed above .",
    "it is assumed that @xmath266 neurons are excitatory , forming synapses of uniform strength @xmath130 with their postsynaptic targets .",
    "the remaining neurons @xmath267 are inhibitory , forming synapses of uniform strength @xmath268 .",
    "the factor @xmath269 describes the relative strength of inhibitory synapses .",
    "we refer to the network as being `` inhibition dominated '' , if the lower number of inhibitory neurons is compensated for by stronger inhibitory weights . in the case",
    "considered here , this amounts to the condition @xmath270 @xcite .",
    "the connectivity of the network is set to @xmath271 , such that each neuron receives input from exactly @xmath272 excitatory neurons and @xmath273 inhibitory neurons .",
    "the presynaptic sources are randomly selected from the available pool , multiple synaptic contacts are excluded . the graph underlying such a network is a specific type of random graph @xcite .",
    "the connectivity matrix @xmath35 is a random matrix with two types of entries , organized in homogeneous columns .",
    "the entries in positive columns of this matrix , corresponding to excitatory neurons , have a mean of @xmath274 and a variance of @xmath275 , whereas the entries in negative columns , corresponding to inhibitory neurons , have a mean of @xmath276 and a variance of @xmath277 , respectively .",
    "the matrix @xmath35 has an eigenvalue spectrum with two components that are , for large and not too sparse networks , described as follows @xcite : there is one exceptional eigenvalue , proportional to the mean recurrent input to each neuron @xmath278.\\ ] ] it belongs to uniform eigenvectors with all components being equal .",
    "they represent a @xmath69-dimensional subspace , spanned by the uniform vector @xmath279 . in a balanced random network",
    ", we have @xmath280 .",
    "the bulk spectrum @xmath281 covers a circular region in the complex plane , centered at the origin .",
    "its radius @xmath75 satisfies @xmath282 .",
    "\\label{eq_rho}\\end{aligned}\\ ] ] the density of eigenvalues within the circle is in general non - uniform , and it can be approximated by a density derived in @xcite .",
    "( [ eq_statrate_a ] ) , which relates input , output and membrane potentials under stationary conditions , has an effective coefficient matrix @xmath283 .",
    "its eigenvalue spectrum consists of numbers @xmath284 , where @xmath285 is from the spectrum of @xmath35 .",
    "likewise , the eigenvalues of @xmath286 are @xmath287 .",
    "this can either be derived directly , or it can be implied by the spectral mapping theorem @xcite .",
    "the associated eigenvectors are the same in each case .      under the same conditions on homogeneity as made above , explicit solutions for the response rates , and for the mean membrane potentials , can be obtained by resorting to additional constraining assumptions .",
    "specifically , one can analytically describe the response statistics of a leaky integrate - and - fire neuron , which is driven by randomly fluctuating input .",
    "if inputs are uncorrelated , and synaptic couplings are weak , the lumped synaptic input current may be approximated by a gaussian white noise with appropriate parameters @xmath288 and @xmath289 @xmath290 where @xmath291 is a stationary gaussian white noise with zero mean and unit power spectral density .",
    "assuming stationarity and a fixed voltage threshold , the associated first - passage time problem can in fact be solved : the membrane potential dynamics of the neuron can be conceived as a diffusion process , and the time evolution of the membrane potential distribution is given by a fokker - planck equation with specific boundary conditions .",
    "its solution yields explicit expressions for the moments of the inter - spike interval distribution @xcite .",
    "in particular , the mean response rate of the neuron , @xmath209 , in terms of its input statistics @xmath292^{-1 }     \\label{eq_mt } \\end{aligned}\\ ] ] with @xmath293 and @xmath294 .    employing a mean field ansatz , the above theory can be applied to networks of identical pulse - coupled lif neurons , randomly connected with homogeneous in - degrees , and driven by external excitatory input of the same strength . under these circumstances ,",
    "all neurons exhibit the same mean firing rate , which can be determined by a straight - forward self - consistency argument @xcite : the firing rate @xmath209 is a function of the first two moments of the input fluctuations , @xmath288 and @xmath289 , as described by eq .",
    "( [ eq_mt ] ) .",
    "both parameters are , in turn , functions of the firing rate @xmath209 .",
    "this leads to a fixed point equation , the root of which can be found numerically .",
    "here we employed newton s method , verifying the convergence of the iteration by appropriate means .    for networks of the type described here , we have specifically @xmath295 , \\nonumber     \\\\ \\sigma^2 & = \\tau [ j_s^2 s + j^2 r n \\epsilon ( f + g^2(1-f ) ) ] ,     \\label{eq_inputms } \\end{aligned}\\ ] ] where @xmath15 is the input ( stimulus ) firing rate , and @xmath209 is the mean response rate of all neurons in the network , respectively . here",
    ", @xmath39 denotes the epsp amplitude of external inputs , and @xmath130 denotes the amplitude of recurrent epsps .",
    "the inhibition - excitation ratio @xmath189 has been introduced above .",
    "the remaining structural parameters are the number of neurons in the network , @xmath37 , the connection probability , @xmath271 , and the fraction @xmath145 of neurons in the network that are excitatory , implying that a fraction @xmath296 is inhibitory .",
    "the treatment described above is only approximating the networks considered in numerical simulations , since we chose biologically more realistic lif neurons with alpha - synapses . in order to make use of the same analytical framework as just described , we made the simplifying assumption that all the presynaptic current is delivered immediately , and that the input current to each neuron is still white . we therefore need to obtain the effective values for mean and variance .    to obtain the effective value of the mean ,",
    "we match the area under the psc kernel of @xmath44-shape with a corresponding @xmath297-synapse @xmath298 the actual @xmath44-synapse with a peak amplitude @xmath6 would then be matched to a @xmath297-psc as follows @xmath299 therefore , we choose the value @xmath300 as the effective value for the mean input .",
    "this is equivalent to the integral under the psc , i.e.  the total amount of current that is delivered by an alpha synapse with peak @xmath6 .",
    "the effective value of the variance can be obtained in the same fashion by matching the integral of the squared psc , of the @xmath44-psc with the @xmath297-psc @xmath301 ^ 2 \\ , dt = 1,\\ ] ] and @xmath302 ^ 2 \\ , dt = \\nonumber   \\\\",
    "( \\frac{j_\\alpha^2 e^2 \\tau_\\mathrm{syn}}{4 } ) \\int_0^{\\infty } \\frac{4}{\\tau_\\mathrm{syn}^3 } \\bigl [ t e^{-t/\\tau_\\mathrm{syn } } \\bigr]^2 \\ , dt . \\end{aligned}\\ ] ] this suggests @xmath303 .",
    "we now consider the case of tuned input to a recurrent network .",
    "the input @xmath304 to each neuron in the network , as well as its firing rate response @xmath305 and its membrane potential @xmath306 , may depend on a given feature @xmath307 of a sensory stimulus .",
    "the functions @xmath308 , @xmath309 and @xmath310 will be called the input , output and membrane potential tuning curves of neuron @xmath162 , respectively .",
    "let now the input to each neuron in a recurrent network be tuned , i.e.  @xmath311 .",
    "after relaxation to equilibrium , the output of the recurrent network is given by eq .",
    "( [ eq_statrate_b ] ) , and the tuning curves are obtained by @xmath312 \\nonumber \\\\     & = { \\mathbf}{a } \\bigl [ j_s { \\mathbf}{s}({\\mathbf}{\\phi } ) - { \\mathbf}{v}({\\mathbf}{\\phi } ) \\bigr ] .",
    "\\label{tunmix } \\end{aligned}\\ ] ]    assume now that a stimulus ensemble has been fixed for an experiment , think of a uniform distribution for the orientation of a stimulus offered , for example .",
    "mathematically , this is described by a suitable probability distribution on the set of possible values for the feature @xmath49 , the stimulus ensemble .",
    "thereby , any stimulus dependent quantity @xmath57 ( like  @xmath34 , @xmath33 and @xmath38 ) turns into a real - valued random variable .",
    "its expected value @xmath313 $ ] then corresponds to the component of the tuning curve @xmath314 that is common to all parameter values , the baseline of the tuning curve .",
    "note that this concept depends on the stimulus ensemble , and it suggests the following further terminology : @xmath315     \\\\ \\text{\\textbf{modulation}}:\\qquad & { \\mathbf}{x}_m = { \\mathbf}{x } - { \\operatorname{\\mathbb{e}}}[{\\mathbf}{x } ] \\end{aligned}\\ ] ] evidently , the decomposition @xmath58 is fully specified by these settings .",
    "moreover , by linearity of @xmath316 $ ] eq .  ( [ eq_statrate_b ] ) implies @xmath317 and , therefore , @xmath318 \\nonumber \\\\    & = { \\mathbf}{a } ( j_s { \\mathbf}{s}_m - { \\mathbf}{v}_m ) .",
    "\\label{eq_statrate_m } \\end{aligned}\\ ] ] this means that the recurrent network defined by the coupling matrix @xmath35 , and the matrix @xmath62 derived from it , processes baseline and modulation components separately and independently , with no cross - talk involved . in other words",
    ", pure modulation input will not attain any baseline through network processing , and _",
    "vice versa_. this is exactly the meaning of figure  [ fig_seppath]b .",
    "note , however , that , as the mean membrane potential @xmath38 actually depends on the input @xmath34 in a highly nonlinear fashion , the above equations determine the network response only implicitly .",
    "moreover , for the processing to be independent , it is necessary that @xmath319 and @xmath320 depends only on @xmath23 and @xmath81 , respectively , with no cross - talk . for the baseline firing rates",
    ", this implies that @xmath319 is not affected by the modulation in the input .",
    "as baseline firing rates are the same in our homogeneous networks , the mean @xmath319 ( over neurons in the network ) should therefore be more or less constant in one experiment with fixed @xmath23 .",
    "we have checked this numerically in our simulations by plotting the standard deviation ( over neurons ) of the mean membrane potential ( over time and over orientation ) for @xmath133 sampled excitatory and inhibitory neurons ( fig .",
    "[ fig_vmtc ] ) .",
    "the variance is indeed much smaller than @xmath321 , the modulation of the membrane potential due to modulation in the input , @xmath81 , and this is consistent for all recurrent regimes .",
    "we will now consider the case of homogeneous properties of all inputs to the network .",
    "specifically , we assume that the input baseline is a uniform vector @xmath322 = { \\mathbf}{s}_b \\sim { \\mathbf}{u}.\\ ] ] we now consider an inhibition dominated random network as described in sect .",
    "[ sec_evrn ] , where all neurons have identical parameters , and the connectivity matrix is statistically homogeneous .",
    "as we have already discussed , under the assumption of orthogonality , the firing rate responses of all neurons to a homogeneous stimulus , and the corresponding mean membrane potentials , are all identical @xmath323 in other words , homogeneous vectors , like baseline input , are eigenvectors of the coupling matrix @xmath35 .",
    "they are also eigenvectors of the matrix @xmath324 which determines the stationary firing rates .",
    "the corresponding eigenvalue @xmath325 of the matrix @xmath35 is in fact negative , and the corresponding eigenvalue of @xmath62 is @xmath326 is positive , but much smaller than @xmath69 , since @xmath327 is typically large ( of order @xmath328 ) .",
    "the overall amplification or attenuation of the baseline is given by @xmath329    in the case of uniform input @xmath65 an explicit solution of the mean firing rate @xmath67 can be obtained by the mean field approximation , as described in sect .",
    "[ sec_scfr ] .",
    "the mean membrane potential @xmath319 is then determined by eq .",
    "( [ eq_statrate_a ] ) .",
    "we will now discuss how the modulation part of tuned inputs can be approximated .",
    "we used the theory described in the previous sections to approximately determine the response of our networks , when they are processing non - uniform inputs , tuned to some stimulus feature .",
    "specifically , the mean ( over the network ) modulation component ( f2 component ) of the output tuning curves in response to a tuned input can be obtained approximately . under the assumption of stability ( see sect .  [ sec_oprn ] ) , and for the tuned inputs considered here , it seems justified to start with the approximation of `` perfect balance '' of the recurrent modulations : @xmath330 although this approximation is not strictly true , it holds on average : as the result of numerical simulation in fig .",
    "[ fig_orthver]b demonstrated , @xmath82 had a narrow distribution around zero .",
    "similar distribution is expected for @xmath87 , as @xmath331 can be expanded in terms of powers of @xmath66 ( eq .  ) under the assumption of stability .    in terms of diagrammatic illustration of fig .",
    "[ fig_seppath]b , this is equivalent to @xmath332 .",
    "this in turn implies that the net input from the recurrent network is on average untuned . for @xmath76 , this input tuning",
    "is shown in fig .",
    "[ fig_inptun ] .",
    "although the tuning of recurrent input is ( compared to feedforward tuning ) not negligible for all neurons , it holds on average , such that average tuning curves have the same shape as the input tuning ( fig .",
    "[ fig_inptun]b ) .",
    "we therefore use this approximation to compute the mean modulation gain of the network .    under this assumption ,",
    "the computation of modulation rate vector , @xmath331 , is simplified to @xmath333 the linear mixture of tuning curves described by eq .",
    "( [ tunmix ] ) is reduced to an amplification or attenuation of the respective input tuning curves .",
    "since @xmath320 depends nonlinearly on input parameters , we again need compute the self - consistent firing rates employing mean field theory .",
    "mean @xmath288 and variance @xmath289 of the input current analogous to eq .",
    "( [ eq_inputms ] ) , however , are now computed by approximating the recurrent firing rate by the baseline firing rate , @xmath78 , which is the same for all neurons in the network , as discussed above .",
    "the external input to this specific neuron , in contrast , experiences a feature specific modulation @xmath334 . as a result",
    ", we let @xmath335 ) , \\nonumber \\\\",
    "\\sigma^2 & = \\tau [ j_s^2 ( s_b + s_m ) + j^2 r_b n \\epsilon ( f + g^2(1-f ) ) ] .",
    "\\label{eq_inputpb } \\end{aligned}\\ ] ] the parameters @xmath39 , @xmath130 , @xmath189 , @xmath37 , @xmath271 and @xmath145 are the same as above .    the resultant firing rate of the neuron according to eq .",
    "( [ eq_mt ] ) now differs from its baseline firing rate .",
    "the difference is , in fact , a good estimate for the modulation in the output firing rate of this particular neuron . due to our general homogeneity assumptions , all neurons in the network",
    "will have the same output modulation , notwithstanding the fact that they all have different preferred orientations .",
    "note that , to be consistent , the correction of @xmath130 due to the refractory period should be performed based on the modulated firing rate .",
    "this becomes specifically important for low recurrences , where the modulation rate is higher and , as a result , the effect of shunting of input due to refractory period ( @xmath336 ) becomes more prominent .    if the firing rates have been determined self - consistently , eq .",
    "( [ eq_statrate_a ] ) yields the corresponding membrane potentials directly .",
    "this is true for both the baseline and for the modulation , which are defined in the obvious way also for the membrane potential .",
    "the modulation gains can also be computed by linearizing the dynamics around the baseline operating regime of the network .",
    "our results revealed that the mean modulation gain , @xmath53 , in the network depends on the mean distance of the membrane potential from the threshold ( fig .",
    "[ fig_vm_modgain ] ) .",
    "subthreshold modulations ( with regard to the mean - driven threshold ) were capable of eliciting output firing activity , and the input - output relationship ( the gain ) was inversely proportional to the distance to threshold .",
    "one way to interpret these results is to summarize them in terms of the mean and standard deviation of the input that a neuron receives on average from the network , in the baseline state . to this , and alternatively to the mean and standard deviation of membrane potential , which is uniquely determined by the input , we refer as the operating point of the network",
    ".    the effect of mean , @xmath91 , and standard deviation of input , @xmath92 , on the modulation gain can be described , respectively , as shifting the mean membrane potential ( and hence determining the mean distance to threshold ) , and smoothing ( linearizing ) the @xmath145-@xmath146 curve @xcite .",
    "the linearized gains can then be obtained by perturbing the input around the baseline state , as it was described in sect .",
    "[ sec_timp ] and [ sect_ltrn ] .",
    "this total embedded gain of the neuron in response to this perturbation determines the effective coupling strength and , as fig .",
    "[ fig_lingain_stabspec ] demonstrated , predicts the mean modulation gain in these networks quite well .",
    "the embedded gain modulates both feedforward and recurrent couplings .",
    "the fact that recurrent connections are now effectively weighted by these linear gains might suggest an explanation why the network exhibits stable activity even for highly recurrent regimes . as fig .",
    "[ fig_instabdyn]a showed , if the spectrum of @xmath35 is computed from the weight matrix normalized by @xmath138 , the radius of the bulk of eigenvalues , @xmath75 , would be larger than one already for an intermediate recurrent regime ( @xmath337 , @xmath77 , and @xmath338 in this example ) .",
    "if one now computes the normalized radius by weighting the coupling strength according to linear gains ( i.e.  @xmath339 , instead of @xmath340 ) , the new normalized radius , @xmath159 , is not unstable anymore ( @xmath341 in this example ) .",
    "this coincides with our observations of the numerical simulations .",
    "this enhanced stability has indeed been demonstrated to be the case for all networks we have studied here ( fig .",
    "[ fig_lingain_stabspec ] , inset ) .",
    "if one now add to this that @xmath101 is inversely proportional to distance to threshold , it follows that the network dynamically settles in a regime of operation which stabilizes the bulk of eigenvalues .",
    "this is due to the fact that in inhibition dominated networks , increasing the recurrent coupling also increases the negative feedback within the network , which results in more hyperpolarized average membrane potentials of the neurons .",
    "this in turn leads to a smaller relative contribution of each spike from a presynaptic source to the firing activity of the postsynaptic neuron , since the distance to threshold has effectively increased .",
    "the overall increase or decrease in the effective gain , @xmath116 , depends on how exactly the mean membrane potential , @xmath342 , is affected by @xmath130 and how @xmath101 is in turn depending on @xmath342 .      for networks with weak to intermediate recurrence ,",
    "the assumption of `` perfect balance '' allowed a rather accurate prediction of the gain for the tuned part of network activation ( `` modulation '' ) .",
    "this assumption , however , fails in the case of strongly recurrent networks . under the constraints of `` linear tuning '' some aspects of the problem can be nevertheless treated .",
    "we now consider stimulus features that can be represented by vectors @xmath49 in @xmath343 for some @xmath344 .",
    "[ [ example-1 . ] ] example 1 .",
    "+ + + + + + + + + +    the direction of a moving light dot stimulus in the visual field is represented by an angle in @xmath345 or , alternatively , by a vector in @xmath346 , the @xmath69-dimensional sphere .",
    "the speed of the movement can be considered simultaneously with its direction , if encoded by the length of the vector .",
    "any vector in @xmath347 is then corresponding to a valid stimulus .",
    "[ [ example-2 . ] ] example 2 .",
    "+ + + + + + + + + +    the orientation of a moving grating in the visual field corresponds to vectors in @xmath346 via the bijective mapping @xmath348 the factor @xmath155 in the argument of the cosine and the sine function makes sure that a rotation of the grating by @xmath349 is mapped to the initial orientation again .    [",
    "[ example-3 . ] ] example 3 .",
    "+ + + + + + + + + +    a stimulus for studying color vision is represented by the activation profiles of the different types of receptor cells in the retina , distinguished by their specific light absorption spectrum .",
    "for example , trichromacy in humans and closely related monkeys involves the differential activation of the three different types of cones @xmath350 .",
    "this leads , in a natural way , to a representation of a color stimulus in terms of a vector in @xmath351 .",
    "a simple but relevant model of specific tuning curves is linear tuning @xmath352 the parameters @xmath353 and @xmath354 are fixed and specific for each neuron : @xmath355 is the baseline rate in absence of stimulation , the vector @xmath356 is the preferred feature .",
    "in fact , stimulating with @xmath357 produces the highest , and stimulating with @xmath358 the lowest firing rates .",
    "more generally , if @xmath359 denotes the angle between the vectors @xmath356 and @xmath49 , linear tuning is equivalent to cosine tuning @xmath360 the length of the vector that represents the preferred feature @xmath361 is the tuning strength , it satisfies @xmath362 where @xmath363 is the stimulus strength . to ensure that the firing rate @xmath364 remains positive @xmath365 the strength of the admitted stimuli",
    "must be limited , so we admit only stimuli that are weak enough such that linearity of the tuning and positivity of firing rates remain compatible @xmath366      assume now that the individual inputs @xmath304 are tuned with respect to an @xmath234-dimensional feature @xmath49 .",
    "the same stimulus is `` seen '' by all neurons , but each neuron responds with its private tuning curve @xmath367 as described by eq .",
    "( [ tunmix ] ) , the responses of neurons in a recurrent network have tuning curves that are , in general , linear sums of the tuning curves of the input channels .",
    "if the recurrent interactions are strong , many input channels contribute indirectly to the tuning of every output channel , recruiting multi - synaptic pathways .",
    "assume now that all inputs @xmath304 are linearly tuned to the stimulus @xmath49 according to @xmath368 for parameters @xmath355 and @xmath356 .",
    "then , exploiting the two - fold linearity , we obtain @xmath369 \\nonumber \\\\       & = { \\mathbf}{a } \\bigl [ j_s ( { \\mathbf}{\\psi}^\\ast + { \\mathbf}{\\phi}^\\ast { \\mathbf}{\\phi } )                                        - { \\mathbf}{v}({\\mathbf}{\\phi } ) \\bigr]\\nonumber\\\\       & = { \\mathbf}{a } j_s { \\mathbf}{\\psi}^\\ast + { \\mathbf}{a }",
    "j_s { \\mathbf}{\\phi}^\\ast { \\mathbf}{\\phi } - { \\mathbf}{a } { \\mathbf}{v}({\\mathbf}{\\phi } )     \\label{rlt } \\end{aligned}\\ ] ] where @xmath370 is the vector of baseline activities and @xmath371 is a matrix the rows of which are given by the transposed preferred features @xmath372 .",
    "therefore , apart from nonlinear distortions induced by nonzero mean membrane potentials , all neurons in the recurrent network are again linearly tuned , with baselines given by the components of the vector @xmath373 , and preferred features encoded by the rows of the matrix @xmath374 .      because linear tuning curves are linearly transformed according to eq .",
    "( [ rlt ] ) , we can actually compute the recurrent preferred features that result from this transformation .",
    "note , however , that the actual tuning curves will , in general , be contaminated by nonlinear distortions by @xmath375 that are not reflected by the linear mix @xmath374 of preferred features of the inputs to the network . to keep the present discussion simple",
    ", we ignore this complication here .",
    "a network with zero recurrent interaction would be described by the matrix @xmath376 .",
    "therefore , the perturbation of each neuron s private preferred input feature resulting from recurrent network action is given by @xmath377 , where @xmath378 if we can assume that the preferred features @xmath371 of inputs are all chosen independently from some common distribution , the sums of preferred vectors that result from the action of the matrix @xmath111 will , according to the central limit theorem , be normally distributed vectors in @xmath379 . in that case , it suffices to compute the covariance matrix @xmath380 of the perturbations of the preferred features of all output tuning curves , and to compare it with the covariance matrix @xmath381 of the inputs @xmath382 specifically , if the distribution of input features is isotropic with covariance @xmath383 , the scalar @xmath384 represents the mean squared tuning strength of all inputs . by means of the matrix @xmath111 , the distribution of output features will then be perturbed by a component that is normally distributed with isotropic covariance @xmath385 where @xmath386 for a random network as described above , the factor @xmath46 is the same for all rows @xmath183 , and it describes now the mean attenuation / amplification of the tuning strength performed by the recurrent network .    as an example , we compute here the distribution of tuning strengths @xmath361 of neurons that would result in a strongly recurrent network , where the perturbation dominates the result .",
    "it is given by the probability density @xmath387 where @xmath388 is the surface of the @xmath389-sphere with radius @xmath390 , and @xmath391 is the probability density function of the @xmath234-dimensional normal distribution with zero mean and isotropic covariance matrix @xmath392 .",
    "we find @xmath393 in the case @xmath394 , which is particularly relevant for the application described in this paper , we get @xmath395 which is a weibull distribution with shape parameter @xmath155 and scale parameter @xmath396 .    for the scenario , where feedforward and recurrent components of tuning are mixed , one obtains an interpolation between `` purely feedforward '' and `` purely feedback '' operation of the network .",
    "this is equivalent to computing eq .   with nonzero mean , @xmath288 .",
    "in the specific case of orientation selectivity , with @xmath397 , the final distribution of tuning strength is then obtained as @xmath398 where @xmath399 is the modified bessel function of the first kind and order zero .",
    "the authors wish to thank a  aertsen , c  boucsein , g  grah , j  kirsch and a  kumar for their comments on previous versions of the manuscript .",
    "we also thank the developers of the simulation software nest ( see http://www.nest-initiative.org ) and the maintainers of the bcf computing facilities for their support throughout this study .",
    "funding by the german ministry of education and research ( bccn freiburg , grant 01gq0420 and bfnt freiburg*tbingen , grant 01gq0830 ) is gratefully acknowledged .",
    "10 [ 1]`#1 ` urlstyle [ 1]doi:#1    [ 1 ] [ 2 ]    _ _ _ _ _ _ _ _ _ _ _ _ _ _ key : # 1 + annotation :  # 2 _ _ _ _ _ _ _ _ _ _ _ _ _ _    hubel dh , wiesel tn ( 1962 ) receptive fields , binocular interaction and functional architecture in the cat s visual cortex . _",
    "the journal of physiology _ 160 : 10654 .",
    "hubel dh , wiesel tn ( 1968 ) receptive fields and functional architecture of monkey striate cortex . _ the journal of physiology _ 195 : 215243 .",
    "sclar g , freeman rd ( 1982 ) orientation selectivity in the cat s striate cortex is invariant with stimulus contrast . _",
    "experimental brain research _ 46 : 457461 .",
    "alitto hj , usrey wm ( 2004 ) influence of contrast on orientation and temporal frequency tuning in ferret primary visual cortex .",
    "_ journal of neurophysiology _ 91 : 2797808 .",
    "niell cm , stryker mp ( 2008 ) highly selective receptive fields in mouse visual cortex . _",
    "the journal of neuroscience _ 28 : 752036 .",
    "ferster d , miller kd ( 2000 ) neural mechanisms of orientation selectivity in the visual cortex .",
    "_ annual review of neuroscience _ 23 : 441471 .",
    "sompolinsky h , shapley r ( 1997 ) new perspectives on the mechanisms for orientation selectivity .",
    "_ current opinion in neurobiology _ 7 : 514522 .",
    "anderson js , lampl i , gillespie dc , ferster d ( 2000 ) the contribution of noise to contrast invariance of orientation tuning in cat visual cortex .",
    "_ science _ 290 : 19681972 .",
    "finn i m , priebe nj , ferster d ( 2007 ) the emergence of contrast - invariant orientation tuning in simple cells of cat visual cortex .",
    "_ neuron _ 54 : 137152 .",
    "ben - yishai r , bar - or rl , sompolinsky h ( 1995 ) theory of orientation tuning in visual cortex .",
    "_ proceedings of the national academy of sciences _ 92 : 38443848 .",
    "somers dc , nelson sb , sur m ( 1995 ) an emergent model of orientation selectivity in cat visual cortical simple cells . _",
    "the journal of neuroscience _ 15 : 54485465 .",
    "ko h , cossell l , baragli c , antolik j , clopath c , et  al .",
    "( 2013 ) the emergence of functional microcircuits in visual cortex .",
    "_ nature _ 496 : 96100 .",
    "ko h , hofer sb , pichler b , buchanan ka , sjstrm pj , et  al .",
    "( 2011 ) functional specificity of local synaptic connections in neocortical networks .",
    "_ nature _ 473 : 8791 .",
    "jia h , rochefort nl , chen x , konnerth a ( 2010 ) dendritic organization of sensory input to cortical neurons in vivo .",
    "_ nature _ 464 : 130712 .",
    "ohki k , chung s , kara p , hbener m , bonhoeffer t , et  al .",
    "( 2006 ) highly ordered arrangement of single neurons in orientation pinwheels . _",
    "nature _ 442 : 9258 .",
    "bonhoeffer t , grinvald a ( 1991 ) iso - orientation domains in cat visual cortex are arranged in pinwheel - like patterns .",
    "_ nature _ 353 : 42931 .",
    "blasdel gg , salama g ( 1986 ) voltage - sensitive dyes reveal a modular organization in monkey striate cortex . _",
    "nature _ 321 : 57985 .",
    "tso d , frostig r , lieke e , grinvald a ( 1990 ) functional organization of primate visual cortex revealed by high resolution optical imaging .",
    "_ science _ 249 : 417420 .",
    "horton jc , adams dl ( 2005 ) the cortical column : a structure without a function .",
    "_ philosophical transactions of the royal society of london _ 360 : 83762 .",
    "rudolph m , pospischil m , timofeev i , destexhe a ( 2007 ) inhibition determines membrane potential dynamics and controls action potential generation in awake and sleeping cat cortex .",
    "_ the journal of neuroscience _ 27 : 528090 .",
    "haider b , husser m , carandini m ( 2013 ) inhibition dominates sensory responses in the awake cortex .",
    "_ nature _ 493 : 97100 .",
    "fino e , yuste r ( 2011 ) dense inhibitory connectivity in neocortex .",
    "_ neuron _ 69 : 1188203 .",
    "packer am , yuste r ( 2011 ) dense , unspecific connectivity of neocortical parvalbumin - positive interneurons : a canonical microcircuit for inhibition ? _ the journal of neuroscience _ 31 : 1326071 .",
    "hofer sb , ko h , pichler b , vogelstein j , ros h , et  al .",
    "( 2011 ) differential connectivity and response dynamics of excitatory and inhibitory neurons in visual cortex .",
    "_ nature neuroscience _ 14 : 104552 .",
    "van vreeswijk c , sompolinsky h ( 1996 ) chaos in neuronal networks with balanced excitatory and inhibitory activity .",
    "_ science _ 274 : 17246 .",
    "brunel n ( 2000 ) dynamics of sparsely connected networks of excitatory and inhibitory spiking neurons . _ journal of computational neuroscience _ 8 : 183208 .",
    "braitenberg v , schz a ( 1998 ) _ cortex : statistics and geometry of neuronal connectivity_. 81 .",
    "springer .",
    "ringach dl , shapley rm , hawken mj ( 2002 ) orientation selectivity in macaque v1 : diversity and laminar dependence .",
    "_ the journal of neuroscience _ 22 : 56395651 .",
    "dragoi v , rivadulla c , sur m ( 2001 ) foci of orientation plasticity in visual cortex .",
    "_ nature _ 411 : 8086 .",
    "chapman b , stryker mp ( 1993 ) development of orientation selectivity in ferret visual cortex and effects of deprivation .",
    "_ the journal of neuroscience _",
    "13 : 525162 .",
    "varga z , jia h , sakmann b , konnerth a ( 2011 ) dendritic coding of multiple sensory inputs in single cortical neurons in vivo . _ proceedings of the national academy of sciences _ 108 : 154205 .",
    "chen x , leischner u , rochefort nl , nelken i , konnerth a ( 2011 ) functional mapping of single spines in cortical neurons in vivo .",
    "_ nature _ 475 : 5015 .",
    "hansel d , van vreeswijk c ( 2012 ) the mechanism of orientation selectivity in primary visual cortex without a functional map . _",
    "the journal of neuroscience _ 32 : 40494064 .",
    "douglas rj , koch c , mahowald m , martin ka , suarez hh ( 1995 ) recurrent excitation in neocortical circuits . _ science _ 269 : 981985 .",
    "murthy vn , fetz ee ( 1994 ) effects of input synchrony on the firing rate of a three - conductance cortical neuron model .",
    "_ neural computation _ 6 : 11111126 .",
    "aertsen a , rotter s ( 2003 ) higher - order statistics of input ensembles and the response of simple model neurons . _ neural computation _ 15 : 67101 .",
    "pernice v , staude b , cardanobile s , rotter s ( 2012 ) recurrent interactions in spiking networks with arbitrary topology .",
    "_ physical review e _ 85 : 031916 .",
    "li yt , ma wp , li ly , ibrahim la , wang sz , et  al . ( 2012 ) broadening of inhibitory tuning underlies contrast - dependent sharpening of orientation selectivity in mouse visual cortex",
    ". _ journal of neuroscience _ 32 : 1646616477 .",
    "li yt , ma wp , pan cj , zhang li , tao hw ( 2012 ) broadening of cortical inhibition mediates developmental sharpening of orientation selectivity . _",
    "the journal of neuroscience _ 32 : 398191 .",
    "liu bh , li yt , ma wp , pan cj , zhang li , et  al .",
    "( 2011 ) broad inhibition sharpens orientation selectivity by expanding input dynamic range in mouse simple cells .",
    "_ neuron _ 71 : 54254 .",
    "tan ayy , brown bd , scholl b , mohanty d , priebe nj ( 2011 ) orientation selectivity of synaptic input to neurons in mouse and cat primary visual cortex . _ the journal of neuroscience _ 31 : 1233950 .",
    "shapley r , hawken m , ringach dl ( 2003 ) dynamics of orientation selectivity in the primary visual cortex and the importance of cortical inhibition .",
    "_ neuron _ 38 : 68999 .",
    "xing d , ringach dl , hawken mj , shapley rm ( 2011 ) untuned suppression makes a major contribution to the enhancement of orientation selectivity in macaque v1 .",
    "_ the journal of neuroscience _ 31 : 1597215982 .",
    "wilson nr , runyan ca , wang fl , sur m ( 2012 ) division and subtraction by distinct cortical inhibitory networks in vivo .",
    "_ nature _ 488 : 343348 .",
    "lee sh , kwan ac , zhang s , phoumthipphavong v , flannery jg , et  al .",
    "( 2012 ) activation of specific interneurons improves v1 feature selectivity and visual perception .",
    "_ nature _ 488 : 379383 .",
    "mao r , schummers j , knoblich u , lacey cj , van wart a , et  al .",
    "( 2012 ) influence of a subtype of inhibitory interneuron on stimulus - specific responses in visual cortex .",
    "_ cerebral cortex _ 22 : 493508 .",
    "adesnik h , bruns w , taniguchi h , huang zj , scanziani m ( 2012 ) a neural circuit for spatial summation in visual cortex .",
    "_ nature _ 490 : 22631 .",
    "di  cristo g , wu c , chattopadhyaya b , ango f , knott g , et  al .",
    "( 2004 ) subcellular domain - restricted gabaergic innervation in primary visual cortex in the absence of sensory and thalamic inputs . _",
    "nature neuroscience _ 7 : 11846 .",
    "ma w , liu b , li y , huang zj , zhang li , et  al .",
    "( 2010 ) visual representations by cortical somatostatin inhibitory neurons  selective but with weak and delayed responses . _",
    "the journal of neuroscience _ 30 : 143719 .",
    "tan z , hu h , huang zj , agmon a ( 2008 ) robust but delayed thalamocortical activation of dendritic - targeting inhibitory interneurons .",
    "_ proceedings of the national academy of sciences _ 105 : 218792 .",
    "atallah bv , bruns w , carandini m , scanziani m ( 2012 ) parvalbumin - expressing interneurons linearly transform cortical responses to visual stimuli .",
    "_ neuron _ 73 : 15970 .",
    "cottam jch , smith sl , husser m ( 2013 ) target - specific effects of somatostatin - expressing interneurons on neocortical visual processing . _ the journal of neuroscience _ 33 : 1956778 .",
    "kuhlman sj , tring e , trachtenberg jt ( 2011 ) fast - spiking interneurons have an initial orientation bias that is lost with vision .",
    "nature neuroscience 14 : 11213 .",
    "sharon d , grinvald a ( 2002 ) dynamics and constancy in cortical spatiotemporal patterns of orientation processing .",
    "_ science _ 295 : 5125 .",
    "gillespie dc , lampl i , anderson js , ferster d ( 2001 ) dynamics of the orientation - tuned membrane potential response in cat primary visual cortex . _ nature neuroscience _ 4 : 10149 .",
    "lee d , lin bj , lee ak ( 2012 ) hippocampal place fields emerge upon single - cell manipulation of excitability during behavior . _ science _ 337 : 849853 .",
    "lavzin m , rapoport s , polsky a , garion l , schiller j ( 2012 ) nonlinear dendritic processing determines angular tuning of barrel cortex neurons in vivo .",
    "_ nature _ .",
    "stanley gb , jin j , wang y , desbordes g , wang q , et  al .",
    "( 2012 ) visual orientation and directional selectivity through thalamic synchrony . _ the journal of neuroscience _ 32 : 907388 .",
    "bruno rm , sakmann b ( 2006 ) cortex is driven by weak but synchronously active thalamocortical synapses .",
    "_ science _ 312 : 16227 .",
    "seris p , latham pe , pouget a ( 2004 ) tuning curve sharpening for orientation selectivity : coding efficiency and the impact of correlations .",
    "_ nature neuroscience _ 7 : 112935 .",
    "ecker as , berens p , keliris ga , bethge m , logothetis nk , et  al .",
    "( 2010 ) decorrelated neuronal firing in cortical microcircuits .",
    "_ science _ 327 : 5847 .",
    "hansen bj , chelaru mi , dragoi v ( 2012 ) correlated variability in laminar cortical circuits .",
    "_ neuron _ 76 : 590602 .",
    "sadagopan s , ferster d ( 2012 ) feedforward origins of response variability underlying contrast invariant orientation tuning in cat visual cortex .",
    "_ neuron _ 74 : 91123 .",
    "priebe nj , ferster d ( 2012 ) mechanisms of neuronal computation in mammalian visual cortex .",
    "_ neuron _ 75 : 194208 .",
    "kara p , pezaris js , yurgenson s , reid rc ( 2002 ) the spatial receptive field of thalamic inputs to single cortical simple cells revealed by the interaction of visual and electrical stimulation .",
    "_ proceedings of the national academy of sciences _ 99 : 1626116266 .",
    "chung s , ferster d ( 1998 ) strength and orientation tuning of the thalamic input to simple cells revealed by electrically evoked cortical suppression .",
    "_ neuron _ 20 : 11771189 .",
    "ferster d , chung s , wheat h ( 1996 ) orientation selectivity of thalamic input to simple cells of cat visual cortex .",
    "_ nature _ 380 : 249252 .",
    "yger p , el boustani s , destexhe a , frgnac y ( 2011 ) topologically invariant macroscopic statistics in balanced networks of conductance - based integrate - and - fire neurons .",
    "_ journal of computational neuroscience _ 31 : 22945 .",
    "voges n , aertsen a , rotter s ( 2011 ) structural models of cortical networks with long - range connectivity .",
    "_ mathematical problems in engineering _ 2012 .",
    "mclaughlin d , shapley r , shelley m , wielaard dj ( 2000 ) a neuronal network model of macaque primary visual cortex ( v1 ) : orientation selectivity and dynamics in the input layer 4calpha .",
    "_ proceedings of the national academy of sciences _ 97 : 808792 .",
    "pernice v , staude b , cardanobile s , rotter s ( 2011 ) how structure determines correlations in neuronal networks .",
    "_ plos computational biology _ 7 : e1002059 .",
    "kriener b , tetzlaff t , aertsen a , diesmann m , rotter s ( 2008 ) correlations and population dynamics in cortical networks . _",
    "neural computation _ 20 : 2185226 .",
    "okun m , lampl i ( 2008 ) instantaneous correlation of excitation and inhibition during ongoing and sensory - evoked activities .",
    "_ nature neuroscience _ 11 : 5357 .",
    "renart a , de  la rocha j , bartho p , hollender l , parga n , et  al .",
    "( 2010 ) the asynchronous state in cortical circuits . _",
    "science _ 327 : 58790 .",
    "tetzlaff t , helias m , einevoll gt , diesmann m ( 2012 ) decorrelation of neural - network activity by inhibitory feedback .",
    "_ plos computational biology _ 8 : e1002596 .",
    "rotter s , diesmann m ( 1999 ) exact digital simulation of time - invariant linear systems with applications to neuronal modeling .",
    "_ biological cybernetics _ 81 : 381402 .",
    "diesmann m , gewaltig mo , rotter s , aertsen a ( 2001 ) state space analysis of synchronous spiking in cortical networks .",
    "_ neurocomputing _ : 565 - 571 .",
    "gewaltig mo , diesmann m ( 2007 ) nest ( neural simulation tool ) .",
    "_ scholarpedia _ 2 : 1430 .",
    "batschelet e ( 1981 ) _ circular statistics in biology ( mathematics in biology)_. academic press inc , 371 pp .",
    "hansel d , van vreeswijk c ( 2002 ) how noise contributes to contrast invariance of orientation tuning in cat visual cortex . _",
    "the journal of neuroscience _ 22 : 511828 .",
    "erds p , rnyi a ( 1959 ) on random graphs i. _ publicationes mathematicae ( debrecen ) _ 6 : 290297 .",
    "bollobs b ( 2001 )",
    "_ random graphs_. number  73 in cambridge studies in advanced mathematics .",
    "cambridge university press , second edition .",
    "rajan k , abbott lf ( 2006 ) eigenvalue spectra of random matrices for neural networks .",
    "_ physical review letters _ 97 : 188104 .",
    "higham nj ( 2008 ) _ functions of matrices : theory and computation_. philadelphia , pa , usa : society for industrial and applied mathematics .",
    "siegert ajf ( 1951 ) on the first passage time probability problem .",
    "_ phys rev _ 81 : 617623 .",
    "ricciardi lm ( 1977 ) _ diffusion processes and related topics on biology_. berlin : springer - verlag .",
    "amit dj , brunel n ( 1997 ) model of global spontaneous activity and local structured activity during delay periods in the cerebral cortex .",
    "_ cerebral cortex _ 7 : 23752 .",
    "miller kd , troyer tw ( 2002 ) neural noise can explain expansive , power - law nonlinearities in neural response functions .",
    "_ journal of neurophysiology _ 87 : 6539 .",
    "* table of notations and parameters . * [ cols=\"<,<,<\",options=\"header \" , ]"
  ],
  "abstract_text": [
    "<S> mechanisms underlying the emergence of orientation selectivity in the primary visual cortex are highly debated . here </S>",
    "<S> we study the contribution of inhibition - dominated random recurrent networks to orientation selectivity , and more generally to sensory processing . by simulating and analyzing large - scale networks of spiking neurons , we investigate tuning amplification and contrast invariance of orientation selectivity in these networks . </S>",
    "<S> in particular , we show how selective attenuation of the common mode and amplification of the modulation component take place in these networks . </S>",
    "<S> selective attenuation of the baseline , which is governed by the exceptional eigenvalue of the connectivity matrix , removes the unspecific , redundant signal component and ensures the invariance of selectivity across different contrasts . </S>",
    "<S> selective amplification of modulation , which is governed by the operating regime of the network and depends on the strength of coupling , amplifies the informative signal component and thus increases the signal - to - noise ratio . here </S>",
    "<S> , we perform a mean - field analysis which accounts for this process . </S>"
  ]
}