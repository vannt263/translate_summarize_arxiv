{
  "article_text": [
    "ranking is central to many applications in information retrieval , question answering , online ad placement and recommender systems . typically , given a query ( e.g. a set of keywords , a question , or an user )",
    ", we need to return a ranked list of relevant objects ( e.g. a set of documents , potential answers , or shopping items ) .",
    "learning to rank ( ltr ) ( e.g. see @xcite for a recent survey ) is a machine learning approach to automatically estimate a rank model from training data , and offers promise way to leverage a wide prior knowledge .",
    "this includes relevance to content , context or profile and object qualities such as well - structuredness and authority .",
    "most ltr algorithms aim at estimating a _ rank functional _ @xmath0 which takes a query @xmath1 and an object @xmath2 and returns a real score , which is then used to rank objects with respect to the query .",
    "typically object order reflects its relevance of the object to the query , that is , top ranked objects are considered most relevant .",
    "mathematically , this is the function estimation problem , but in the new setting of ranking - where the goal is to output a sorted list of objects rather than to compute the function itself .",
    "this is also different from the traditional settings of regression or classification , where continuous or discrete labels are predicted .",
    "this paper provides an empirical study to probe the construction of rank functionals , and studies the contribution of the factors relevant to the problem .",
    "consider , for example , two application domains : answer ranking in a social question answering ( sqa ) site , and web information retrieval ( wir ) .",
    "in sqa , community members respond to the question asked by another member , but the quality of answers varies greatly , making answer ranking critical .",
    "often , we have access to the actual questions and answers , and in fact , questions can be quite rich and contain deep linguistic structures .",
    "on the other hand , in web document retrieval , queries are often short , but there are many clues to assess the quality of the related documents , for example , the link structures , the authority of the domain and the organisation of the page .",
    "it is safe to assume that there are probably hundreds of relevance indicators currently employed in current major web search engines .",
    "these differences suggest that the input representation for the rank functionals can vary significantly from domain to domain . when the query and objects are of different modalities , textual query versus image objects for example , it is reasonable to represent the query and objects separately .",
    "likewise , in the case of sqa separate representation can be applied to questions and answers since they are generally rich in content and structure . on the other hand , for web document retrieval , prior knowledge of relevance indicators",
    "has been well - studied for decades , and it can be useful to represent the query - object pair as a combined vector .    the second aspect is the choice of the rank functionals , which captures the relevancy of an object given a query .",
    "the specification of rank functionals must be based on on the input representation , but it is also critical that the functional space is rich enough to capture data variability . for",
    "the separate query - object representations , we study transformation methods to first project the query vector and the object vector onto the same subspace and then combine them . for the combined representation ,",
    "we investigate the usefulness of quadratic functions together with overfitting controls .",
    "third , learning often minimises some loss function based on training data .",
    "it is therefore important that the loss functions reflect the rank metrics that will be used to evaluate the algorithm at testing .",
    "in particular , this involves placing more weight for high relevance scores and the first few objects at the top of the rank list .",
    "the main issue is that rank metrics are often computed based on the all objects of the query , and they are discrete in nature , making it difficult to optimise directly . to this end",
    ", we study different choices for making a loss function closely related to the rank errors while maintaining smoothness .",
    "in particular , we examine methods to approximate rank metrics , and investigate weighting schemes to combine piecewise rank losses , where the purpose of the weights are to emphasize the high rating and discount for the low rank .",
    "next , we study probabilistic approaches to derive piecewise rank losses .",
    "the first approach is based on the theory of high - order markov chains . in particular , we investigate the utility of a weighted version of the plackett - luce model @xcite@xcite and its reverse .",
    "the second relies on markov random fields ( mrfs ) , where we suggest the weighted version of the pseudo - likelihood , as well as propose a piecewise approximation to the intractable mrfs .",
    "we prove that this piecewise approximation provides an upper - bound on the log - loss of the original mrf .    finally , we evaluate these design aspects in the domain of social question answering ( sqa ) with an autism dataset retrieved from the yahoo ! qa site , and the domain of web information retrieval ( wir ) with the dataset from the yahoo ! ltr challenge @xcite . in sqa , we apply the separate representation of features - one for questions and one for answers - as input for rank functionals .",
    "we investigate two nonlinear rank functionals against three losses : the multiclass logistic , the loss based on approximating the mrr metric , and the pairwise logistic loss .",
    "the results shown that it is better to use the loss specifically designed for the - the multiclass logistic loss in this case",
    ".    differing from the sqa , the wir data has pre - computed features in the combined representation .",
    "the hypothesis is that we can use the quadratic rank functionals to discover predictive feature conjunctions .",
    "we find that when regularisation and overfitting controls are properly installed , second - order features are indeed more predictive than linear counterparts .",
    "another difference from the sqa data is that , the wir data contain relevance ratings in a small numerical scale , leading to frequent occurrence of ties .",
    "this suggests the use of group - level rank functionals , where objects of the same rating are grouped into a mega - object .",
    "we evaluate several aggregation methods for computing the group - level rank functionals .",
    "the results indicate that max and geometric mean aggregations can be competitive , while maintaining modest computational requirements due to the small number of resultant groups .",
    "experiments on the wir data also show consistent results -weighting is critical for many piecewise loss functions . in particular , for the element - wise decomposition of rank losses ,",
    "rank discount weighting is the most influential , while for the pairwise decomposition , rating difference combined with query length normalisation is the most effective method .",
    "this paper is organised as follows .",
    "section  2 presents a holistic picture of the problem of ltr , and the specification details of rank functionals as well as loss functions .",
    "in particular , we discuss the piecewise weighting schemes to approximate the loss functions .",
    "section  3 follows by describing probabilistic approaches to derive piecewise losses .",
    "design issues identified in the paper are then evaluated in section  4 , where we present the experiments on the yahoo !",
    "qa data , and the yahoo ! ltr challenge data .",
    "section  5 provides further discussion on these aspects , followed by related work in section  6 .",
    "finally section  7 concludes the paper .",
    "in a typical setting , given a query @xmath1 and a set of related objects @xmath3 we want to output a rank list of objects .",
    "ideally , this would means estimating an optimal permutation @xmath4 so that @xmath5 whenever @xmath6 is preferred to @xmath7 , i.e.@xmath8 .",
    "however , this is often impractical since the number of all possible permutations is @xmath9 . a more sensible strategy would be estimating a real _ rank functional _",
    "@xmath10 so that @xmath11 whenever @xmath8 , where @xmath12 denotes all objects for query @xmath1 except for @xmath6 .",
    "this is efficient since the cost of a typical sorting algorithm is @xmath13 .    for simplicity ,",
    "in this paper we drop the explicit dependency between @xmath6 and other objects , and write the rank functional as @xmath14 . in training data ,",
    "we are given for each query a labelling scheme @xmath15 .",
    "this labelling can be a rank list @xmath16 , but more often , it is a set of relevance scores , i.e. @xmath17 where @xmath18 is typically a small integer . given a training data of @xmath19 queries , the general way of estimating @xmath14 is to minimise an regularised empirical risk functional @xmath20 with respect to @xmath21 , where @xmath22 is the loss function , @xmath23 is a ( convex ) regularisation function and @xmath24 is the regularisation factor",
    ".    .evaluation metrics . here",
    "@xmath25 is the predicted position of object @xmath6 and @xmath26 is the predicted position of the best object .",
    "[ tab : evaluation - metrics . ] [ cols=\"<,^ \" , ]",
    "in this paper , we aim at taking a holistic view when designing a robust learning to rank ( ltr ) algorithm .",
    "we consider domains of social question answering ( sqa ) and web information retrieval ( wir ) .",
    "this section presents some more elaboration on a variety of design aspects .    in sqa",
    ", we have chosen the separate representation of features , and investigated two nonlinear rank functionals @xmath27 and @xmath28 against three losses : the multiclass logistic @xmath29 , the loss @xmath30 based on approximating the mrr metric , and the pairwise logistic loss @xmath31 .",
    "the results indicate that it is better to use the loss specifically designed to the task . more specifically , when the task is to predict the single best object , the multiclass logistic loss should work better than more generic rank losses .",
    "we note in passing that we do not aim at outperforming state - of - the - art results in the qa literature , which has a history of several decades , but rather show that simple representation such as words can be useful for this complicated task .    in the wir task ,",
    "on the other hand , combined features are pre - computed , and are not revealed to the public .",
    "however , it is plausible that they contain relevance assessment accordingly multiple criteria .",
    "although the feature details are hidden , the quadratic rank functionals can be useful to detect which feature conjunctions are predictive .",
    "our evaluation confirms that second - order features are indeed useful , provided that we have effective method to control overfitting .",
    "in particular , we employ regularisation and feature filtering for the task .",
    "this result , together with those reported for the yahoo !",
    "ltr challenge @xcite , suggests that the space of rank functionals is a critical factor in achieving high performance ltr algorithms .",
    "different from sqa , the wir data contain relevance ratings in a small numerical scale , which suggests the presence of ties .",
    "this calls for group - level rank functionals , where objects of the same rating are grouped into a mega - object .",
    "we have evaluated several aggregation methods for computing the group - level rank functionals .",
    "the results indicate that max and geometric mean aggregations can be competitive in term of the err score .",
    "the main benefit is that learning can operate directly on the level of groups , and since the number of groups is often much smaller than the number of objects per query , computational saving can be attained .    like in the sqa task",
    ", we also evaluated the direct optimisation of approximation of rank metrics , which has been suggested by several recent studies @xcite @xcite @xcite .",
    "however , our empirical results show that this is not necessarily the case .",
    "we conjecture that due to the approximation of the step function , the resulting functions are often highly complex , and possibly non - convex , making it difficult for optimisation .",
    "this observation is shared in @xcite .",
    "the strong message we obtained from the experiments with the wir data is that for many piecewise loss functions ( @xmath31 in eq.[eq : pointwise ] and @xmath32 in eq.[eq : pairwise ] ) , weighting is an important factor .",
    "this is expected since piecewise loss functions often operate locally with one or two free variables , while rank metrics are often a function of the whole query .",
    "in particular , for the element - wise decomposition of @xmath31 , rank discount weighting is the most influential ( table  [ tab : effect - of - weighting - position ] ) . for the pairwise decomposition of @xmath32 ,",
    "rating difference combined with query length normalisation is the most effective method ( table  [ tab : effect - of - weighting - pair ] ) .",
    "several rank functionals other than the simple linear ones have been suggested in the literature , notably the neural net @xcite , kernels @xcite@xcite , regression trees @xcite and bilinear @xcite .",
    "the neural net rank functionals are often non - convex , and their discriminative power has not been clearly documented",
    ". the kernels , on the other hand , can be expensive for large - scale data , since most kernel - based algorithms scale super - linearly in number of training documents .",
    "regression trees are interesting due to their flexibility in function approximation .",
    "finally , bilinear functions are the linear combination of the query and object feature vectors - thus it falls into the group of separate feature representation .",
    "our work contributes to this line of representation by introducing several non - linear functions .",
    "instance weighting has been used in ltr models in several places @xcite @xcite @xcite . in @xcite , the piecewise approximation in eq.([eq : pairwise ] )",
    "is used where the function @xmath33 is the log loss of the logistic model , and the weight @xmath34 . in @xcite , element - wise decomposition in eq.([eq : pointwise ] ) is suggested to obtain a bound on the ndcg score , but the details of @xmath35 are not reported . in @xcite ,",
    "ndcg - based weights are introduced for pairwise and plackett - luce models .",
    "we extend this by investigating a wider range of weighting schemes on several new models .",
    "query - level models have been advocated in several places @xcite @xcite @xcite . in particular , in @xcite , the standard plackett - luce model is employed , but weighting is not considered . in @xcite , a markov random field is suggested , and the learning involves mcmc sampling , which leads to non - smooth risk functionals and is hard to judge the convergence .",
    "our weighted pseudo - likelihood and weighted piecewise approximation are , on the other hand , smooth .    there have been a number of recent studies on how to optimise rank metrics directly @xcite@xcite@xcite @xcite@xcite .",
    "there are two approaches : one is based on approximating the rank metrics @xcite @xcite @xcite , and the other on bounding the rank errors @xcite@xcite .",
    "we contribute further to the first approach by approximating the err and mrr metrics .",
    "the issue of ranking with ties in the context of ltr has received little attention @xcite@xcite . in @xcite , ties are considered among pairs of objects , but not the entire group of objects . on the other hand , @xcite studies ties in groups , but their algorithm is only efficient for a particular case of group - level rank functionals ( geometric mean - see table  [ tab : group - level - rank - functions . ] ) .",
    "in this paper , we have investigated choices when designing learning to rank algorithm to perform well against evaluation criteria .",
    "we evaluated design aspects on two tasks : answer ranking in a social question answering site , and web information retrieval . among others",
    ", we have found that representing and selecting features , choosing a ( cost - sensitive ) loss function , handling ties , and weighting data instances are important to achieve high performance algorithms .",
    "ltr is a fast growing field with many established techniques , and thus it is of practical importance to have a clear picture where nuts and bolts are identified .",
    "this paper is aimed as a step towards that goal . however , there are theoretical issues that needed to be addressed .",
    "first , this is mathematically a function estimation problem , where we still do not have a good understanding of the generalization properties .",
    "second , it appears that the structure of the rank functional space is a key to the success of rank algorithms , and thus there is room for more investigation into data partitioning and nonparametric settings .",
    "recall from eq.([eq : mrf ] ) that @xmath37 .",
    "let us define an extended function @xmath38 for all realisations of @xmath39 , thus @xmath40 according to the general hlder s inequality @xmath41 for any @xmath42 subject to @xmath43 .",
    "further , notice that @xmath44 indeed depends only on @xmath45 , thus @xmath46 where @xmath47 is the size of the label set .",
    "the factor @xmath48 comes from the fact that there are @xmath48 ways of enumerating a set of @xmath49 discrete variables in @xmath39 , each of size @xmath47 .      substituting ( [ eq : poly - expansion ] ) into ( [ eq : appendix1 ] ) and then into ( [ eq : holder ] ) , we obtain @xmath53 where @xmath54 , and @xmath55 in the last equation , we have eliminated @xmath50 by using the fact that @xmath43 .",
    "t.  truyen , d.q phung , and s.  venkatesh .",
    "probabilistic models over ordered partitions with applications in document ranking and collaborative filtering . in _ proc . of siam conference on data mining ( sdm )",
    "_ , mesa , arizona , usa , 2011 ."
  ],
  "abstract_text": [
    "<S> ranking is a key aspect of many applications , such as information retrieval , question answering , ad placement and recommender systems . </S>",
    "<S> learning to rank has the goal of estimating a ranking model automatically from training data . in practical settings , the task often reduces to estimating a rank functional of an object with respect to a query . in this paper </S>",
    "<S> , we investigate key issues in designing an effective learning to rank algorithm . </S>",
    "<S> these include data representation , the choice of rank functionals , the design of the loss function so that it is correlated with the rank metrics used in evaluation . for the loss function , we study three techniques : approximating the rank metric by a smooth function , decomposition of the loss into a weighted sum of element - wise losses and into a weighted sum of pairwise losses . </S>",
    "<S> we then present derivations of piecewise losses using the theory of high - order markov chains and markov random fields . in experiments , </S>",
    "<S> we evaluate these design aspects on two tasks : answer ranking in a social question answering site , and web information retrieval .     </S>"
  ]
}