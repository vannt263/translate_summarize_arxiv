{
  "article_text": [
    "feature selection is a central theme in analyzing many variants of magnetic resonance imaging ( mri ) data . supervised approaches that are highly capable of performing regression or classification , but do not rely on features , are at best specialized maps between input data and the output labels .",
    "they lack the crucial component of `` inference '' to produce generalizations _ about _ the input data .",
    "meanwhile , the inference process and ability to find generalizeable features or structure in the data is at the core of scientific discovery ; in mri research , such structures are necessary for the general goal of understanding the brain .",
    "inferring the latent structure is generally the goal of unsupervised learning , which has had a wide success in analyzing mri data .",
    "when combined with supervised learners , these structures have a diagnostic value . independent component analysis ( ica )",
    "@xcite is a representative approach , which has found success as a means for inferring the latent structure in brain imaging data represented via a linear mixture of maximally - independent sources  @xcite .",
    "while linear mixture models have been very successful in neuroimaging applications , their success relative to nonlinear models @xcite is due to simple and tractable inference , not due to a strong belief that linearity is a correct depiction of the latent structure . for linear mixtures ,",
    "non - gaussian sources are necessary to ensure uniqueness , as for gaussian sources one can not guarantee any independence beyond the correlation  @xcite .",
    "luckily , the converse is true : non - gaussian sources with linear mixtures assure maximum independence under a generative learning framework , such as maximum likelihood estimation ( mle )  @xcite .",
    "requiring the prior distribution be non - gaussian , while enabling inference and learning with linear ica methods , breaks down when the relationship between data and sources is nonlinear , necessitating more advanced methods .",
    "although nonlinear versions of ica  @xcite , as well as some alternative nonlinear methods  @xcite exist , each comes with its shortcomings , and none have been successful enough to supplant linear ica .",
    "alternatively , nonlinear independent component estimation ( nice )  @xcite is a method for drawing from a family of nonlinear transformations , @xmath0 , parameterized by feed forward networks ( ffns ) such that computing the jacobian and the inverse are tractable .",
    "while nice can estimate sources from nonlinear mixings better than ica in simulations , it is also limited to square transformations and requires principle component analysis ( pca ) to be practical in a medical imaging setting  @xcite .",
    "in addition to being constrained to square transformations , ica and many nonlinear variants can not incorporate multimodal data in a natural way .",
    "the linear mixing assumption is harder to justify when modes are drawn from fundamentally different distributions , such as mri , electroencephalography ( eeg ) , and other variables such as age , gender , and clinical diagnoses .",
    "the above issues are ongoing challenges for realizing the full potential of a deep independence network ( din ) or a general infomax approach  @xcite for feature extraction in medical imaging .",
    "it is possible that lack of progress has been due to the strong requirement in ica that the data be the output of a deterministic map of sources . as an alternative",
    ", we propose learning features in a directed graphical model setting using recent advances in variational inference and demonstrate the effectiveness of this approach with mri data .",
    "linear mixture models such as ica fall under a more general category of _ volume - preserving bijective maps _",
    "@xcite , such that we learn a deterministic parameterized transformation , @xmath1 , along with a prior distribution of the sources : @xmath2 where @xmath3 is the density of the data , @xmath4 is the density of the sources , and @xmath5 is the jacobian . for ica",
    ", we have two constraints : first , @xmath6 , is a linear transformation with square unmixing matrix , @xmath7 , and second , the prior distribution of the sources , @xmath8 , is non - gaussian .",
    "probabilistic ica ( pica ) @xcite relaxes the square requirement , but learning is still reliant on a linear transformation as well as a noise operator with known covariance .",
    "being a linear transformation , computing the jacobian , and hence learning , is tractable , but this can not be said about general nonlinear transformations .",
    "nonlinear independent component estimation ( nice ) gets around this problem by parameterizing @xmath0 as a feed - forward network ( ffn ) , such that the affine transformation at each layer is lower or upper triangular , but it is still limited to square transformations .",
    "a directed graphical model or _ bayesian network _ is a generative model that represents the density of the data as the marginal of the joint : @xmath9 , which is composed of a set of prior and conditional distributions that make up an acyclic graph .",
    "directed graphical models have been used in various medical imaging settings  @xcite , but have been limited to relatively simple , often linear configurations .",
    "a special case of the bayesian network is the _ directed belief network _ : a hierarchical model that represents the joint via layers of latent variables that within a layer are conditionally independent : @xmath10 where @xmath11 is the prior distribution of the top or @xmath12th layer",
    ".    directed graphical models are most commonly trained using the maximum - likelihood estimation ( mle ) method , which maximizes the log - likelihood of the data by adjusting parameters of the conditional and prior distributions . when present , latent variables need to be marginalized out at each stage during the process ; but training can become difficult as marginalizing the joint distribution over the latent variables is often computationally infeasible .",
    "potentially , learning can be aided by the use of a posterior , @xmath13 , such that @xmath14 ; however , the exact posterior can be equally intractable , particularly when the conditional distributions are complex ( e.g. , parameterized by highly nonlinear functions ) .",
    "some recent advances allow us to more easily train directed graphical models . _ variational inference _ makes use of an approximate posterior to compute the variational lower bound of the log - likelihood , @xmath15 : @xmath16 where we have used a monte carlo estimate for the generative term of the bound",
    ", @xmath17 are @xmath18 independent samples drawn from the approximate posterior , and @xmath19 is the entropy of the approximate posterior .    the most notable advances in variational inference were made in `` helmholtz machines '' @xcite that model the approximate posterior and conditional distributions by deep neural networks  @xcite . in this model , the difficulty is offset from inference to training the approximate posterior modeled by the `` recognition network '' .",
    "for example , suppose the conditional distribution is modeled by an ffn , such that the output makes up the parameters of a multivariate gaussian distribution with mean , @xmath20 , and diagonal covariance , @xmath21 .",
    "let us assume as well that the approximate posterior has a logistic distribution with mean , @xmath22 , and scale , @xmath23 : @xmath24 where @xmath0 and @xmath25 are multilayer ffns with parameters @xmath26 and @xmath27 , @xmath28 are visible variables corresponding to data , and @xmath8 are the latent variables ( or sources ) .",
    "finally , assume @xmath29 , the prior distribution of the latent variables , is a spherical multivariate logistic distribution .",
    "the lower bound in equation [ eq : approx_logp ] becomes : @xmath30\\end{aligned}\\ ] ] the gradient of the first term above w.r.t the variational parameters , @xmath27 , is not normally possible due to the stochastic variables @xmath31 .",
    "however , in the case of logistic latent variables , the following re - parameterization makes learning possible via back - propagation : @xmath32 commonly known as a _",
    "variational autoencoder _ ( vae )",
    "@xcite , this type of re - parameterization is available for a number of continuous distributions , such as gaussian , poisson , and gumbel , but is not available for helmholtz machines with discrete latent variables , though other good methods exist  @xcite .",
    "as the prior is factorized , the lower bound corresponds to learning a generative model with maximally - independent latent variables , a feature desirable in many research settings .",
    "this approach should , in principle , work for any directed graph with continuous latent variables , given the appropriate approximate posterior and prior .",
    "visualizing a latent variable , @xmath33 , of a directed belief network involves calculating the marginal over all other latent variables : @xmath34 this is computationally infeasible with most configurations .",
    "alternatively , we can draw @xmath18 samples from the approximate posterior , @xmath35 to approximate the marginal : @xmath36 however , this approximation typically requires a large number of samples to be accurate ( e.g. @xmath37 with the mnist dataset ) .",
    "in addition , this only provides a single point in the marginal , which is a continuous function of @xmath33 . in reality",
    ", we are interested in how changes in @xmath33 effect generation of the image .",
    "therefore , we use the following fast approximation to determine the  projection \" of the @xmath38th latent variable : @xmath39 where @xmath40 and @xmath41 is reserved for parameters of the prior distribution that encode first and second order statistics respectively . for instance",
    ", for a logistic distribution , @xmath40 would be the center of unit @xmath38 and @xmath41 would be the scale factor .",
    "this approximation does not capture the full generative effect of the latent variables , but it is sufficient for this demonstration .",
    "for our medical imaging study , we used the mri dataset from the combined schizophrenia studies in plis et .",
    "al . @xcite .",
    "whole brain mris were obtained on a 1.5 t signa ge scanner using identical parameters and software , and the resulting dataset was segmented into grey matter regions with @xmath42 voxels in each sample . for quality control ,",
    "the correlation coefficient of each mri volume was calculated and any volumes with mean coefficient of @xmath43 standard deviations below the mean across all volumes were categorized as noisy and removed . the resulting dataset had @xmath44 subjects and @xmath45 healthy controls .    for our generative model , we used a logistic prior , @xmath46 , with @xmath47 units and a 2-layer  generation \" feed - forward network ( ffn ) with a deterministic intermediate layer with @xmath48 softplus ( @xmath49 ) units to parameterize a gaussian conditional distribution , @xmath50 .",
    "our approximate posterior , @xmath51 , was a multivariate factorized logistic which was parameterized by a 2-layer  recognition \" ffn with @xmath48 hyperbolic tangent ( @xmath52 ) deterministic units .",
    "we learn @xmath27 , @xmath26 , and @xmath53 by maximizing the variational lower bound , and trained our model with a batch size of @xmath54 using the rmsprop algorithm  @xcite for @xmath55 epochs .",
    "as the latent variable projections from section [ sec : viz ] were both positive and negative and the prior distribution is symmetric with respect to our choice of positive scale factors , we reversed the sign of our projections if the mean of voxels above @xmath43 standard deviations was negative .",
    "for each latent variable or  component \" , we calculate the approximate posterior for each subject , @xmath56 , and then used logistic regression to schizophrenia using the approximate posterior means , @xmath57 .",
    "each component was tested for significance by using the resulting @xmath58 values from the logistic regression in a one - sample @xmath59-test .",
    "visual inspection of the latent variables revealed a diverse set of features that were mostly identifiable as regions of interest , with very little noisy features .",
    "there was significantly more overlap between features than is typical with ica with pca preprocessing or rbm with mri data @xcite , which may or may not be beneficial depending on the research setting .",
    "latent variables that showed high significance to schizophrenia ( @xmath60 ) are shown in figure [ fig : features ] with the complete set in the supplementary material .",
    "[ fig : s_corr ]    [ fig : comp_corr ]    the means of the approximate posterior , @xmath57 , were used as input to a classification task , using simple logistic regression and @xmath61-fold class - balanced cross validation .",
    "the resulting classification rate , @xmath62 , is significantly above chance .",
    "the conclusion is that , despite lacking information about the labels in the mle objective and much lower dimensionality , the latent variables have a similar amount of information necessary to perform diagnosis .",
    "this is also apparent in the correlation matrix in figure [ fig : s_corr ] using the components that showed high significance to schizophrenia .",
    "finally , the components were grouped by calculating the correlation of approximate posterior centers across subjects .",
    "figure [ fig : comp_corr ] shows several groupings , as well as some inter - group relationships .",
    "we have demonstrated variational autoencoders as a means of training nonlinear directed graphical models for extracting maximally indepdent features from mri data .",
    "our results show both relevant structure and preservation of information relevant to schizophrenia diagnosis .",
    "this work opens the door for further studies using helmholtz machines for medical imaging research , including multimodal and multilayer analysis ."
  ],
  "abstract_text": [
    "<S> independent component analysis ( ica ) , as an approach to the blind source - separation ( bss ) problem , has become the de - facto standard in many medical imaging settings . despite successes and a large ongoing research effort , the limitation of ica to square linear transformations </S>",
    "<S> have not been overcome , so that general infomax is still far from being realized . as an alternative </S>",
    "<S> , we present feature analysis in medical imaging as a problem solved by helmholtz machines , which include dimensionality reduction and reconstruction of the raw data under the same objective , and which recently have overcome major difficulties in inference and learning with deep and nonlinear configurations . </S>",
    "<S> we demonstrate one approach to training helmholtz machines , variational auto - encoders ( vae ) , as a viable approach toward feature extraction with magnetic resonance imaging ( mri ) data . </S>"
  ]
}