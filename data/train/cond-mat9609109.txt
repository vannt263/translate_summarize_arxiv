{
  "article_text": [
    "genetic algorithms ( gas ) are adaptive search techniques , which can be used to find low energy states in poorly characterized , high - dimensional energy landscapes  @xcite .",
    "they have already been successfully applied in a large range of domains  @xcite and a review of the literature shows that they are becoming increasingly popular . in particular , gas have been used in a number of machine learning applications , including the design and training of artificial neural networks  @xcite .    in the simple ga considered here ,",
    "each population member is represented by a genotype , in this case a binary string , and an objective function assigns an energy to each such genotype .",
    "a population of solutions evolves for a number of discrete generations under the action of genetic operators , in order to find low energy ( high fitness ) states .",
    "the most important operators are selection , where the population is improved through some form of preferential sampling , and crossover ( or recombination ) , where population members are mixed , leading to non - local moves in the search space .",
    "mutation is usually also included , allowing incremental changes to population members .",
    "gas differ from other stochastic optimisation techniques , such as simulated annealing , because a population of solutions is processed in parallel and it is hoped that this may lead to improvement through the recombination of mutually useful features from different population members .    a formalism has been developed by prgel - bennett , shapiro and rattray which describes the dynamics of a simple ga using methods from statistical mechanics  @xcite .",
    "this formalism has been successfully applied to a number of simple ising systems and has been used to determine optimal settings for some of the ga search parameters  @xcite .",
    "it describes problems of realistic size and includes finite population effects , which have been shown to be crucial to understanding how the ga searches .",
    "the approach can be applied to a range of problems including ones with multiple optima , and it has been shown to predict simulation results with high accuracy , although small errors can sometimes be detected .    under the statistical mechanics formalism , the population is described by a small number of macroscopic quantities which are statistical measures of the population .",
    "statistical mechanics techniques are used to derive deterministic difference equations which describe the average effect of each operator on these macroscopics .",
    "since the dynamics of a ga is to be modelled by the average dynamics of an ensemble of gas , it is important that the quantities which are used to describe the system are robust and self - averaging .",
    "the macroscopics which have been used are the cumulants of some appropriate quantity , such as the energy or the magnetization , and the mean correlation within the population , since these are robust statistics which average well over different realizations of the dynamics",
    ". there may be small systematic errors , since the difference equations for evolving these macroscopics sometimes involve nonlinear terms which may not self - average , but these corrections are generally small and will be neglected here .",
    "the statistical mechanics theory is distinguished by the facts that a macroscopic description of the ga is used and that the averaging is done such that fluctuations can be included in a systematic way .",
    "many other theoretical approaches are based on the intuitive idea that above average fitness building blocks are preferentially sampled by the ga , which , if they can be usefully recombined , results in highly fit individuals being produced  @xcite . although this may be a useful guide to the suitability of particular problems to a ga , it is difficult to make progress towards a quantitative description for realistic problems , as it is difficult to determine which are the relevant building blocks and which building blocks are actually present in a finite population .",
    "this approach has led to false predictions of problem difficulty , especially when the dynamic nature of the search is ignored  @xcite .",
    "a rigorous approach introduced by vose describes the population dynamics as a dynamical system in a high - dimensional euclidean space , with each genetic operator incorporated as a transition tensor  @xcite .",
    "this method uses a microscopic description and is difficult to apply to specific problems of realistic size due to high - dimensionality of the equations of motion .",
    "more recently , a number of results have been derived for the performance of a ga on a class of simple additive problems @xcite .",
    "these approaches use a macroscopic description , but assume a particular form for the distribution of macroscopics which is only applicable in large populations and for a specific class of problem .",
    "it is difficult to see how to transfer the results to other problems where finite population effects can not be ignored .",
    "other researchers have introduced theories based on averages .",
    "a description of ga dynamics in terms of the evolution of the parent distribution from which finite populations are sampled was produced by vose and wright  @xcite .",
    "this microscopic approach provides a description of the finite population effects which is elegant and correct .",
    "however , like other microscopic descriptions it is difficult to apply to specific realistic problems due to the enormous dimensionality of the system .",
    "macroscopic descriptions can result in low - dimensional equations which can be more easily studied .",
    "another formalism based on the evolution of parent distributions was developed by peck and dhawan  @xcite , but they did not use the formalism to develop equations describing finite population dynamics .",
    "the importance of choosing appropriate quantities to average is well - known in statistical physics , but does not seem to be widely appreciated in genetic algorithm theory . in particular , many authors use results based on properties of the _ average _ probability distribution ; this is insensitive to finite - population fluctuations and only gives accurate results in the infinite population limit .",
    "thus , many results are only accurate in the infinite population limit , even though this limit is not taken explicitly . for example",
    ", srinivas and patnaik  @xcite and peck and dhawan  @xcite both produce equations for the moments of the fitness distribution in terms of the moments of the initial distribution .",
    "these are moments of the average distribution .",
    "consequently , the equations do not correctly describe a finite population and results presented in these papers reflect that .",
    "other attempts to describe gas in terms of population moments ( or schema moments or average walsh coefficients ) suffer from this problem .",
    "macroscopic descriptions of population dynamics are also widely used in quantitative genetics ( see , for example , reference  @xcite ) . in this field the importance of finite - population fluctuations is more widely appreciated ; the infinite population limit is usually taken explicitly . using the statistical mechanics approach ,",
    "equations for fitness moments which include finite - population fluctuations can be derived by averaging the cumulants , which are more robust statistics .    here ,",
    "the statistical mechanics formalism is applied to a simple problem from learning theory , generalization of a rule by a perceptron with binary weights .",
    "the perceptron learns from a set of training patterns produced by a teacher perceptron , also with binary weights .",
    "a new batch of training patterns are presented to each population member each generation which simplifies the analysis considerably , since there are no over - training effects and each training pattern can be considered as statistically independent .",
    "baum have shown that this problem is similar to a paramagnet whose energy is corrupted by noise and they suggest that the ga may perform well in this case , since it is relatively robust towards noise when compared to local search methods  @xcite .",
    "the noise in the training energy is due to the finite size of the training set and is a feature of many machine learning problems  @xcite .",
    "we show that the noise in the training energy is well approximated by a gaussian distribution for large problem size , whose mean and variance can be exactly determined and are simple functions of the overlap between pupil and teacher .",
    "this allows the dynamics to be solved , extending the statistical mechanics formalism to this simple , yet non - trivial , problem from learning theory .",
    "the theory is compared to simulations of a real ga averaged over many runs and is shown to agree well , accurately predicting the evolution of the cumulants of the overlap distribution within the population , as well as the mean correlation and mean best population member . in the limit of weak selection and large problem size",
    "the population size can be increased to remove finite training set effects and this leads to an expression for the optimal training batch size .",
    "a perceptron with ising weights @xmath0 maps an ising training pattern @xmath1 onto a binary output , @xmath2 where @xmath3 is the number of weights .",
    "let @xmath4 be the weights of the teacher perceptron and @xmath5 be the weights of the pupil .",
    "the stability of a pattern is a measure of how well it is stored by the perceptron and the stabilities of pattern @xmath6 for the teacher and pupil are @xmath7 and @xmath8 respectively , @xmath9 the training energy will be defined as the number of patterns the pupil misclassifies , @xmath10 where @xmath11 is the number of training patterns presented and @xmath12 is the heaviside function . in this work a new batch of training examples is presented each time the training energy is calculated .    for large @xmath3",
    "it is possible to calculate the entropy of solutions compatible with the total training set and there is a first - order transition to perfect generalization as the size of training set is increased  @xcite .",
    "this transition occurs for @xmath13 patterns and beyond the transition the weights of the teacher are the only weights compatible with the training set . in this case",
    "there is no problem with over - training to that particular set , although a search algorithm might still fail to find these weights .",
    "the ga considered here will typically require more than @xmath13 patterns , since it requires an independent batch for each energy evaluation , so avoiding any possibility of over - training .    define @xmath14 to be the overlap between pupil and teacher , @xmath15 we choose @xmath16 at every site without loss of generality .",
    "if a statistically independent pattern is presented to a perceptron , then for large @xmath3 the stabilities of the teacher and pupil are gaussian variables each with zero mean and unit variance , and with covariance @xmath14 , @xmath17 the conditional probability distribution for the training energy given the overlap is , @xmath18 where the brackets denote an average over stabilities distributed according to the joint distribution in equation  ( [ stab_dist ] ) .",
    "the logarithm of the fourier transform generates the cumulants of the distribution and using the fourier representation for the delta function in @xmath19 one finds , @xmath20                          \\right\\rangle \\nonumber \\\\ & = \\:\\left(1 +                          \\frac{1}{\\pi}(\\e^t-1)\\cos^{-1}(r )                          \\right)^{\\!\\lambda n}\\end{aligned}\\ ] ] the logarithm of this quantity can be expanded in @xmath21 , with the cumulants of the distribution given by the coefficients of the expansion .",
    "the higher cumulants are @xmath22 and it turns out that the shape of the distribution is not critical as long as @xmath23 is @xmath24",
    ". a gaussian distribution will be a good approximation in this case , @xmath25 where the mean and variance are , @xmath26 here , @xmath27 is the generalization error , which is the probability of misclassifying a randomly chosen training example .",
    "the variance expresses the fact that there is noise in the energy evaluation due to the finite size of the training batch .",
    "initially , a random population of solutions is created , in this case ising weights of the form @xmath28 where the alleles @xmath5 are the weights of a perceptron .",
    "the size of the population is @xmath29 and will usually remain fixed , although a dynamical resizing of the population is discussed in section  [ sec_rescale ] . under selection ,",
    "new population members are chosen from the present population with replacement , with a probability proportional to their boltzmann weight .",
    "the selection strength @xmath30 is analogous to the inverse temperature and determines the intensity of selection , with larger @xmath30 leading to a higher variance of selection probabilities  @xcite . under standard uniform crossover",
    ", the population is divided into pairs at random and the new population is produced by swapping weights at each site within a pair with some fixed probability . here , bit - simulated crossover is used , with new population members created by selecting weights at each site from any population member in the original population with equal probability  @xcite . in practice ,",
    "the alleles at every site are completely shuffled within the population and this brings the population straight to the fixed point of standard crossover .",
    "this special form of crossover is only practicable here because crossover does not change the mean overlap between pupil and teacher within the population .",
    "standard mutation is used , with random bits flipped throughout the population with probability @xmath31 .",
    "each population member receives an independent batch of @xmath11 examples from the teacher perceptron each generation , so that the relationship between the energy and the overlap between pupil and teacher is described by the conditional probability defined in equation  ( [ def_p ] ) . in total ,",
    "@xmath32 training patterns are used , where @xmath33 is the total number of generations and @xmath29 is the population size ( or the mean population size ) .      the population will be described in terms of a number of macroscopic variables , the cumulants of the overlap distribution within the population and the mean correlation within the population . in the following sections , difference equations will be derived for the average change of a small set of these macroscopics , due to each operator .",
    "a more exact approach considers fluctuations from mean behaviour by modelling the evolution of an ensemble of populations described by a set of order parameters  @xcite . here",
    ", it is assumed that the dynamics average sufficiently well so that we can describe the dynamics in terms of deterministic equations for the average behaviour of each macroscopic .",
    "this assumption is justified by the excellent agreement between the theory and simulations of a real ga , some of which are presented in section  [ sec_sim ] . once difference equations are derived for each macroscopic , they can be iterated in sequence in order to simulate the full dynamics .",
    "notice that although we follow information about the overlap between teacher and pupil , this is of course not known in general .",
    "the only feedback available when training the ga is the training energy defined in equation  [ def_e ] .",
    "selection acts on this energy , and it is therefore necessary to average over the noise in selection which is due both to the stochastic nature of the training energy evaluation and of the selection procedure itself .",
    "finite population effects prove to be of fundamental importance when modelling the ga .",
    "a striking example of this is in selection , where an infinite population assumption leads to the conclusion that the selection strength can be set arbitrarily high in order to move the population to the desired solution .",
    "this is clearly nonsense , as selection could never move the population beyond the best existing population member .",
    "two improvements are required to model selection accurately ; the population should be finite and the distribution from which it is drawn should be modelled in terms of more than two cumulants , going beyond a gaussian approximation  @xcite .",
    "the higher cumulants play a particularly important role in selection which will be described in section  [ sec_selapp ]  @xcite .",
    "the higher cumulants of the population after bit - simulated crossover are determined by assuming the population is at maximum entropy with constraints on the mean overlap and correlation within the population ( see [ max_ent ] ) .",
    "the effect of mutation on the mean overlap and correlation only requires the knowledge of these two macroscopics , so these are the only quantities we need to evolve in order to model the full dynamics .",
    "all other relevant properties of the population after crossover can be found from the maximum entropy ansatz .",
    "a more general method is to follow the evolution of a number of cumulants explicitly , as in references  @xcite , but this is unnecessary here because of the special form of crossover used , which is not appropriate in problems with stronger spatial interactions .",
    "the cumulants of the overlap distribution within the population are robust statistics which are often reasonably stable to fluctuations between runs of the ga , so that they average well  @xcite .",
    "the first two cumulants are the mean and variance respectively , while the higher cumulants describe the deviation from a gaussian distribution .",
    "the third and fourth cumulants are related to the skewness and kurtosis of the population respectively .",
    "a population member , labelled @xmath34 , is associated with overlap @xmath35 defined in equation  ( [ def_r ] ) .",
    "the cumulants of the overlap distribution within a finite population can be generated from the logarithm of a partition function , @xmath36 where @xmath29 is the population size .",
    "if @xmath37 is the @xmath38th cumulant , then , @xmath39 the partition function holds all the information required to determine the cumulants of the distribution of overlaps within the population .",
    "the correlation within the population is a measure of the microscopic similarity of population members and is important because selection correlates a finite population , sometimes leading to premature convergence to poor solutions .",
    "it is also important in calculating the effect of crossover , since this involves the interaction of different population members and a higher correlation leads to less disruption on average .",
    "the correlation between two population members , @xmath34 and @xmath30 , is @xmath40 and is defined by , @xmath41 the mean correlation is @xmath42 and is defined by , @xmath43    in order to model a finite population we consider that @xmath29 population members are randomly sampled from an infinite population , which is described by a set of infinite population cumulants , @xmath44  @xcite .",
    "the expectation values for the mean correlation and the first cumulant of a finite population are equal to the infinite population values .",
    "the higher cumulants are reduced by a factor which depends on the population size , @xmath45 here , @xmath46 , @xmath47 and @xmath48 give finite population corrections to the infinite population result ( see reference  @xcite for a derivation ) , @xmath49 although we model the evolution of a finite population , it is more natural to follow the macroscopics associated with the infinite population from which the finite population is sampled  @xcite",
    ". the expected cumulants of a finite population can be retrieved through equations  ( [ k1 ] ) to ( [ k4 ] ) .",
    "the mean effects of standard crossover and mutation on the distribution of overlaps within the population are equivalent to the paramagnet results given in  @xcite .",
    "however , bit - simulated crossover brings the population straight to the fixed point of standard crossover , which will be assumed to be a maximum entropy distribution with the correct mean overlap and correlation , as described in [ max_ent ] . to model",
    "this form of crossover one only requires knowledge of these two macroscopics , so these are the only two quantities we need to evolve under selection and mutation .    the mean overlap and correlation after averaging over all mutations",
    "are , @xmath50 where @xmath31 is the probability of flipping a bit under mutation  @xcite .",
    "the higher cumulants after crossover are required to determine the effects of selection , discussed in the next section .",
    "the mean overlap and correlation are unchanged by crossover and the other cumulants can be determined by noting that bit - simulated crossover completely removes the difference between site averages within and between different population members .",
    "for example , terms like @xmath51 and @xmath52 are equal on average . after cancelling terms of this form",
    "one finds that the first four cumulants of an infinite population after crossover are , @xmath53 here , the brackets denote population averages .",
    "the third and fourth order terms in the expressions for the third and fourth cumulants are calculated in [ max_ent ] by making a maximum entropy ansatz .",
    "the expected cumulants of a finite population after crossover are determined from equations  ( [ k1 ] ) to ( [ k4 ] ) .",
    "under selection , @xmath29 new population members are chosen from the present population with replacement .",
    "following prgel - bennett we split this operation into two stages  @xcite .",
    "first we randomly sample @xmath29 population members from an infinite population in order to create a finite population .",
    "then an infinite population is generated from this finite population by selection .",
    "the proportion of each population member represented in the infinite population after selection is equal to its probability of being selected , which is defined below . the sampling procedure can be averaged out in order to calculate the expectation values for the cumulants of the overlap distribution within an infinite population after selection , in terms of the infinite population cumulants before selection .    the probability of selecting population member @xmath34 is @xmath54 and for boltzmann selection one chooses , @xmath55 where @xmath30 is the selection strength and the denominator ensures that the probability is correctly normalized . here , @xmath56 is the training energy of population member @xmath34 .",
    "one can then define a partition function for selection , @xmath57 the logarithm of this quantity generates the cumulants of the overlap distribution for an infinite population after selection , @xmath58 one can average this quantity over the population by assuming each population member is independently selected from an infinite population with the correct cumulants , @xmath59 where @xmath19 determines the stochastic relationship between energy and overlap as defined in equation  ( [ def_p ] ) which will be approximated by the gaussian distribution in equation  ( [ def_p_g ] ) . following prgel - bennett and shapiro one can use derrida s trick and express the logarithm as an integral in order to decouple the average  @xcite .",
    "@xmath60 where , @xmath61 the distribution of overlaps within an infinite population is approximated by a cumulant expansion around a gaussian distribution  @xcite , @xmath62 \\label{cum_exp}\\ ] ] where @xmath63 are scaled hermite polynomials .",
    "four cumulants were used for the simulations presented in section  [ sec_sim ] and the third and fourth hermite polynomials are @xmath64 and @xmath65 .",
    "this function is not a well defined probability distribution since it is not necessarily positive , but it has the correct cumulants and provides a good approximation . in general ,",
    "the integrals in equations  ( [ log_z ] ) and ( [ ft ] ) have to be computed numerically , as was the case for the simulations presented in section  [ sec_sim ] .",
    "it is instructive to expand in small @xmath30 and large @xmath3 , as this shows the contributions for each cumulant explicitly and gives some insight into how the size of the training set affects the dynamics . since the variance of the population is @xmath66 it is reasonable to expand the mean of @xmath19 , defined in equation  ( [ < e|r > ] ) , around the mean of the population in this limit ( @xmath67 ) .",
    "it is also assumed that the variance of @xmath19 is well approximated by its leading term and this assumption may break down if the gradient of the noise becomes important . under these simplifying assumptions",
    "one finds , @xmath68    following prgel - bennett and shapiro  @xcite , one can expand the integrand in equation  ( [ log_z ] ) for small @xmath30 ( as long as @xmath23 is at least @xmath24 so that the variance of @xmath19 is @xmath13 ) , @xmath69 where , @xmath70 we approximate @xmath19 by a gaussian whose mean and variance given in equations  ( [ app_mean ] ) and ( [ app_var ] ) . completing the integral in equation  ( [ log_z ] ) , one finds an expression for the cumulants of an infinite population after selection , @xmath71       \\label{kns}\\ ] ] where , @xmath72 here , a cumulant expansion has been used .",
    "the parameter @xmath73 is the constant of proportionality relating the generalization error to the overlap in equation  ( [ app_mean ] ) ( constant terms are irrelevant , as boltzmann selection is invariant under the addition of a constant to the energy ) .",
    "@xmath74    for the first few cumulants of an infinite population after selection one finds , @xmath75 the expected cumulants of a finite population after selection are retrieved through equations  ( [ k1 ] ) to ( [ k4 ] ) .",
    "for the zero noise case ( @xmath76 ) this is equivalent to selecting directly on overlaps ( with energy @xmath77 ) , with selection strength @xmath78 .",
    "we will therefore call @xmath78 the effective selection strength .",
    "it has previously been shown that this parameter should be scaled inversely with the standard deviation of the population in order to make continued progress under selection , without converging too quickly  @xcite .",
    "strictly speaking , we can only use information about the distribution of energies since the overlaps will not be known in general , but to first order in @xmath79 this is equivalent to scaling the selection strength inversely to the standard deviation of the energy distribution . as in the problems considered in reference  @xcite , the finite population effects lead to a reduced variance and an increase in the magnitude of the third cumulant , related to the skewness of the population .",
    "this leads to an accelerated reduction in variance under further selection .",
    "the noise due to the finite training set increases the size of the finite population effects .",
    "the other genetic operators , especially crossover , reduce the magnitude of the higher cumulants to allow further progress under selection .",
    "to model the full dynamics , it is necessary to evolve the mean correlation within the population under selection .",
    "this is rather tricky , as it requires knowledge of the relationship between overlaps and correlations within the population . to make the problem tractable",
    ", it is assumed that before selection the population is at maximum entropy with constraints on the mean overlap and correlation within the population , as discussed in [ max_ent ] .",
    "the calculation presented here is similar to that presented elsewhere  @xcite , except for a minor refinement which seems to be important when considering problems with noise under selection .",
    "the correlation of an infinite population after selection from a finite population is given by , @xmath80 where @xmath54 is the probability of selection , defined in equation  ( [ p_alpha ] ) .",
    "the first term is due to the duplication of population members under selection , while the second term is due to the natural increase in correlation as the population moves into a region of lower entropy .",
    "the second term gives the increase in the correlation in the infinite population limit , where the duplication term becomes negligible .",
    "an extra set of variables @xmath81 are assumed to come from the same statistics as the distribution of correlations within the population .",
    "recall that the expectation value for the correlation of a finite population is equal to the correlation of the infinite parent population from which it is sampled .",
    "we estimate the conditional probability distribution for correlations given overlaps before selection @xmath82 by assuming the weights within the population are distributed according to the maximum entropy distribution described in [ max_ent ] .",
    "then @xmath83 is simply the correlation averaged over this distribution and the distribution of overlaps after selection , @xmath84 .",
    "@xmath85 this integral can be calculated for large @xmath3 by the saddle point method and we find that in this limit the result only depends on the mean overlap after selection ( see [ app_cond ] ) .",
    "@xmath86 where , @xmath87 the natural increase contribution to the correlation @xmath83 is an implicit function of @xmath88 through @xmath89 , which is related to @xmath88 by equation  ( [ qk1s ] ) . here , @xmath90 is the mean weight at site @xmath91 before selection ( recall that we have chosen the teacher s weights to be @xmath16 at every site , without loss of generality ) and for a distribution at maximum entropy one has , @xmath92 the lagrange multipliers , @xmath93 and @xmath94 , are chosen to enforce constraints on the mean overlap and correlation within the population before selection and @xmath95 is drawn from a gaussian distribution with zero mean and unit variance ( see [ max_ent ] ) .",
    "it is instructive to expand in @xmath89 , which is appropriate in the weak selection limit . in this case one",
    "finds , @xmath96 where @xmath97 are the infinite population expressions for the cumulants after bit - simulated crossover , when the population is assumed to be at maximum entropy ( defined in equations ( [ k2inf ] ) to ( [ k4inf ] ) up to the fourth cumulant ) . here",
    ", @xmath89 plays the role of the effective selection strength in the associated infinite population problem , so for an infinite population one could simply set @xmath98 , where @xmath73 is defined in equation  ( [ def_k ] ) . to calculate the correlation after selection , we solve equation  ( [ qk1s ] ) for @xmath89 and then substitute this value into the equation  ( [ qqs ] ) to calculate @xmath83 .",
    "in general this must be done numerically , although the weak selection expansion can be used to obtain an analytical result which gives a very good approximation in many cases .",
    "notice that the third cumulant in equation  ( [ q_inf ] ) will be negative for @xmath99 because of the negative entropy gradient and this will accelerate the increased correlation under selection .",
    "the duplication term @xmath100 is defined in equation  ( [ q_s ] ) . as in the partition function calculation presented in section  [ sec_selcum ] ,",
    "population members are independently averaged over a distribution with the correct cumulants , @xmath101 here , @xmath81 is a construct which comes from the same statistics as the correlations between distinct population members .",
    "the integral in @xmath21 removes the square in the denominator and decouples the average , @xmath102 where , @xmath103 the overlap distribution @xmath104 will be approximated by the cumulant expansion in equation  ( [ cum_exp ] ) and @xmath105 by the distribution derived in [ app_cond ] . in general",
    ", it would be necessary to calculate these integrals numerically , but the correlation distribution is difficult to deal with as it requires the numerical reversion of a saddle point equation .    instead , we expand for small @xmath30 and large @xmath3 as we did for the selection calculation in section  [ sec_selapp ] ( this approximation is only used for the term involving the correlation in equation  ( [ q_d ] ) for the simulations presented in section  [ sec_sim ] ) . in this case one",
    "@xmath106 \\nonumber \\\\ & & -                          \\hat{\\rho}_q(2\\beta)\\exp\\!\\left[-t\\left (                          ( p-1)\\hat{\\rho}(\\beta ) +                          \\frac{\\hat{\\rho}_q(3\\beta)}{\\hat{\\rho}_q(2\\beta ) }                          \\right)\\right]\\end{aligned}\\ ] ] where , @xmath107 completing the integral in equation  ( [ q_d ] ) one finds , @xmath108 we express @xmath109 in terms of the fourier transform of the distribution of correlations , which is defined in equation  ( [ ftq ] ) , @xmath110 the integrals can be calculated by expressing @xmath19 by the same approximate form as in section  [ sec_selapp ] and using the saddle point method to integrate over the fourier transform as in [ app_cond ]",
    ".    eventually one finds , @xmath111 \\rho_2(k\\beta,0)}{p \\rho_1 ^ 2(k\\beta,0 ) }          \\ ; + \\ ; o\\!\\!\\left(\\frac{1}{p^2}\\right ) \\label{dqd}\\ ] ] where @xmath112 is defined in equation  ( [ qqs ] ) and @xmath113 is defined in equation  ( [ def_rhon ] )",
    ".    it is instructive to expand in @xmath30 as this shows the contributions from each cumulant explicitly .",
    "to do this we use the cumulant expansion described in equation  ( [ cum_exp ] ) and to third order in @xmath30 for three cumulants one finds , @xmath114\\left ( 1 + k_2(k\\beta)^2 - k_3(k\\beta)^3 + o(\\beta^4 ) \\right)\\ ] ] the @xmath83 term has not been expanded out since it contributes terms of @xmath66 less than these contributions for each cumulant .",
    "selection leads to a negative third cumulant ( see equation  ( [ k3s ] ) ) , which in turn leads to an accelerated increase in correlation under further selection .",
    "crossover reduces this effect by reducing the magnitude of the higher cumulants .",
    "the noise introduced by the finite sized training set increases the magnitude of the detrimental finite population terms in selection . in the limit of weak selection and large problem size discussed in sections  [ sec_selapp ] and [ sec_dup ] ,",
    "this can be compensated for by increasing the population size .",
    "the terms which involve noise in equations ( [ kns ] ) and ( [ dqd ] ) can be removed by an appropriate population resizing , @xmath115\\ ] ] here , @xmath116 is the population size in the infinite training set , zero noise limit .",
    "since these are the only terms in the expressions describing the dynamics which involve the finite population size , this effectively maps the full dynamics onto the infinite training set case .    for zero noise the selection strength should be scaled so that the effective selection strength @xmath78 is inversely proportional to the standard deviation of the population  @xcite , @xmath117 here , @xmath73 is defined in equation  ( [ def_k ] ) and @xmath118 is the scaled selection strength and remains fixed throughout the search .",
    "recall that @xmath119 is the expected variance of a finite population , which is related to the variance of an infinite population through equation  ( [ k2 ] ) .",
    "one could also include a factor of @xmath120 to compensate for changes in population size , as in reference  @xcite , but this term is neglected here .",
    "the resized population is then , @xmath121 notice that the exponent in this expression is @xmath24 , so this population resizing does not blow up with increasing problem size .",
    "one might therefore expect this problem to scale with @xmath3 in the same manner as the zero - noise , infinite training set case , as long as the batch size is @xmath13 .",
    "baum have shown that a closely related ga scales as @xmath122 on this problem if the population size is sufficiently large so that alleles can be assumed to come from a binomial distribution  @xcite .",
    "this is effectively a maximum entropy assumption with a constraint on the mean overlap alone .",
    "they use culling selection , where the best half of the population survives each generation leading to a change in the mean overlap proportional to the population s standard deviation .",
    "our selection scaling also leads to a change in the mean of this order and the algorithms may therefore be expected to compare closely .",
    "the expressions derived here do not rely on a large population size and are therefore more general .    in the infinite population limit it is reasonable to assume @xmath123 which is the relationship between mean and variance for a binomial distribution , since in this limit the correlation of the population will not increases due to duplication under selection . in this case the above scaling results in a monotonic decrease in population size , as @xmath124 increases over time .",
    "this is easy to implement by removing the appropriate number of population members before each selection .    in a finite population",
    "the population becomes correlated under selection and the variance of the population is usually less than the value predicted by a binomial distribution . in this case",
    "the population size may have to be increased , which could be implemented by producing a larger population after selection or crossover .",
    "this is problematic , however , since increasing the population size leads to an increase in the correlation and a corresponding reduced performance . in this case",
    "the dynamics will no longer be equivalent to the infinite training set situation .    instead of varying the population size , one can fix the population size and vary the size of the training batches . in this case one finds , @xmath125    figure  [ fig_scale ] shows how choosing the batch size each generation according to equation  ( [ scale_alpha ] ) leads to the dynamics converging onto the infinite training set dynamics where the training energy is equal to the generalization error .",
    "the infinite training set result for the largest population size is also shown , as this gives some measure of the potential variability of trajectories available under different batch sizing schemes .",
    "any deviation from the weak selection , large @xmath3 limit is not apparent here . to a good approximation",
    "it seems that the population resizing in equation  ( [ rescale ] ) and the corresponding batch sizing expression in equation  ( [ scale_alpha ] ) are accurate , at least as long as @xmath23 is not too small .",
    "( 8,6 ) ( 0,0 ) ( 3.7,0.6 ) ( 3.4,-0.6 ) ( 3.5,2.1 ) ( -0.5,4.3 )      in the previous section it was shown how the population size could be changed to remove the effects of noise associated with a finite training set . if we use this population resizing then it is possible to define an optimal size of training set , in order to minimize the computational cost of energy evaluation .",
    "this choice will also minimize the total number of training examples presented when independent batches are used .",
    "this may be expected to provide a useful estimate of the appropriate sizing of batches in more efficient schemes , where examples are recycled , as long as the total number of examples used significantly exceeds the threshold above which over - training is impossible .",
    "we assume that computation is mainly due to energy evaluation and note that there are @xmath29 energy evaluations each generation with computation time for each scaling as @xmath23 .",
    "if the population size each generation is chosen by equation  ( [ rescale ] ) , then the computation time @xmath126 ( in arbitrary units ) is given by , @xmath127 the optimal choice of @xmath23 is given by the minimum of @xmath126 , which is at @xmath128 .",
    "choosing this batch size leads to the population size being constant over the whole ga run and for optimal performance one should choose , @xmath129 where @xmath116 is the population size used for the zero noise , infinite training set ga .",
    "notice that it is not necessary to determine @xmath116 in order to choose the size of each batch , since @xmath128 is not a function of @xmath116 .",
    "since the batch size can now be determined automatically , this reduces the size of the ga s parameter space significantly .",
    "one of the runs in figure  [ fig_scale ] is for this choice of @xmath29 and @xmath23 , showing close agreement to the infinite training set dynamics ( @xmath130 ) . in general , the first two cumulants change in a non - trivial manner each generation and their evolution can be determined by simulating the dynamics , as described in section  [ sec_sim ] .",
    "in sections  [ sec_mutcross ] , [ sec_selcum ] and [ sec_cor ] , difference equations were derived for the mean effect of each operator on the mean overlap and correlation within the population .",
    "the full dynamics of the ga can be simulated by iterating these equations starting from their initial values , which are zero .",
    "the equations for selection also require knowledge of the higher cumulants before selection , which are calculated by assuming a maximum entropy distribution with constraints on the two known macroscopics ( see equations ( [ k2inf ] ) to ( [ k4inf ] ) ) .",
    "we used four cumulants and the selection expressions were calculated numerically , although for weak selection the analytical results in section  ( [ sec_selapp ] ) were also found to be very accurate .",
    "the largest overlap within the population was estimated by assuming population members were randomly selected from a distribution with the correct cumulants  @xcite .",
    "this assumption breaks down towards the end of the search , when the population is highly correlated and the higher cumulants become large , so that four cumulants may not describe the population sufficiently well .",
    "figures  [ fig_conv ] and [ fig_best ] show the mean , variance and largest overlap within the population each generation , averaged over 1000 runs of a ga and compared to the theory .",
    "the infinite training set case , where the training energy is the generalization error , is compared to results for two values of @xmath23 , showing how performance degrades as the batch size is reduced . recall that @xmath11 new patterns are shown to each population member , each generation , so that the total number of patterns used is @xmath131 , where @xmath29 is population size and @xmath33 is the total number of generations .",
    "the skewness and kurtosis are presented in figure  [ fig_conv34 ] for one value of @xmath23 , showing that although there are larger fluctuations in the higher cumulants they seem to agree sufficiently well to the theory on average .",
    "it would probably be possible to model the dynamics accurately with only three cumulants , since the kurtosis does not seem to be particularly significant in these simulations .",
    "( 8,6 ) ( 0,0 ) ( 3.4,-0.6 ) ( 3.5,4.0 ) ( 3.5,2.3 )    ( 8,6 ) ( 0,0 ) ( 3.4,-0.6 ) ( -0.8,4.3 )    ( 8,6 ) ( 0,0 ) ( 3.4,-0.6 ) ( 3.8,5.4 ) ( 3.8,1.6 )    these results show excellent agreement with the theory , although there is a slight underestimate in the best population member for the reasons discussed above .",
    "this is typical of the theory , which has to be very accurate in order to pick up the subtle effects of noise due to the finite batch size .",
    "unfortunately , the agreement is less accurate for low values of @xmath23 , where the noise is stronger .",
    "this may be due to two simplifications .",
    "firstly , we use a gaussian approximation for the noise which relies on @xmath23 being at least @xmath24 .",
    "this could be remedied by expanding the noise in terms of more than two cumulants as we have done for the overlap distribution .",
    "secondly , the duplication term in section  [ sec_dup ] uses the large @xmath3 , weak selection approximation which also relies on @xmath23 being @xmath24 .",
    "the error due to this approximation is minimized by only using the approximation for the term involving the correlation in equation ( [ q_d ] ) , with the other term calculated numerically .",
    "it is expected that good results for smaller values of @xmath23 would be possible for larger values of @xmath3 , where the correlation calculation would be more exact .",
    "a statistical mechanics formalism has been used to solve the dynamics of a ga for a simple problem from learning theory , generalization in a perceptron with binary weights . to make the dynamics tractable ,",
    "the case where a new batch of examples was presented to each population member each generation was considered . for @xmath13 training examples per batch",
    "the training energy was well approximated by a gaussian distribution whose mean is the generalization error and whose variance increases as the batch size is reduced .",
    "the use of bit - simulated crossover , which takes the population straight to the fixed point of standard crossover , allowed the dynamics to be modelled in terms of only two macroscopics ; the mean correlation and overlap within the population .",
    "the higher cumulants of the overlap distribution after crossover were required to calculate the effect of selection and were estimated by assuming maximum entropy with respect to the two known macroscopics . by iterating difference equations describing the average effect of each operator on the mean correlation and overlap the dynamics of the ga were simulated , showing very close agreement with averaged results from a ga .",
    "although the difference equations describing the effect of each operator required numerical enumeration in some cases , analytical results were derived for the weak selection , large @xmath3 limit .",
    "it was shown that in this limit a dynamical resizing of the population maps the finite training set dynamics onto the infinite training set situation . using this resizing it is possible to calculate the most computationally efficient size of population and training batch , since there is a diminishing return in improved performance as batch size",
    "is increased . for the case of independent training examples considered here",
    "this choice also gives the minimum total number of examples presented .    in future work",
    "it would be essential to look at the situation where the patterns are recycled , leading to a much more efficient use of training examples and the possibility of over - training . in this case , the distribution of overlaps between teacher and pupil would not be sufficient to describe the population , since the training energy would then be dependent on the training set .",
    "one would therefore have to include information specific to the training set , such as the mean pattern per site within the training set .",
    "this might be treated as a quenched field at each site , although it is not obvious how one could best incorporate such a field into the dynamics .    another interesting extension of the present study would be to consider multi - layer networks , which would present a much richer dynamical behaviour than the single - layer perceptron considered here .",
    "this would bring the formalism much closer to problems of realistic difficulty . in order to describe the population in this case",
    "it would be necessary to consider the joint distribution of many order parameters within the population .",
    "it would be interesting to see how the dynamics of the ga compares to gradient methods in networks with continuous weights , for which the dynamics of generalization for a class of multi - layer architectures have recently been solved analytically in the case of on - line learning  @xcite . in order to generalize in multi - layer networks",
    "it is necessary for the search to break symmetry in weight space and it would be of great interest to understand how this might occur in a population of solutions , whether it would occur spontaneously over the whole population in analogy to a phase transition or whether components would be formed within the population , each exhibiting a different broken symmetry .",
    "this would again require the accurate characterization of finite population effects , since an infinite population might allow the coexistence of all possible broken symmetries , which is presumably an unrealizable situation in finite populations .",
    "we would like to thank adam prgel - bennett for many helpful discussions and for providing code for some of the numerical work used here .",
    "we would also like to thank the anonymous reviewers for making a number of useful suggestions .",
    "mr was supported by an epsrc award ( ref .",
    "93315524 ) .",
    "after bit - simulated crossover the population is assumed to be at maximum entropy with constraints on the mean overlap and correlation within the population . this is a special case of the result derived for the paramagnet by prgel - bennett and shapiro  @xcite and this discussion follows theirs closely .",
    "let @xmath90 be the mean weight at site @xmath91 within the population , @xmath132 to calculate the distribution of this quantity over sites one imposes constraints on the mean overlap and correlation with lagrange multipliers @xmath94 and @xmath93 , @xmath133 recall that we have chosen @xmath134 at each site without loss of generality .",
    "the correlation expression is for large @xmath29 and finite population corrections can be included retrospectively .    without constraints ,",
    "the fraction of positive weights at site @xmath91 is given by a binomial coefficient , @xmath135 so one can define an entropy , @xmath136 \\nonumber \\\\ & \\sim &          -\\frac{p}{2}\\log(1 - w_i^2 ) +          \\frac{pw_i}{2}\\log\\left(\\frac{1-w_i}{1+w_i}\\right)\\end{aligned}\\ ] ] where stirling s approximation has been used .",
    "one can then define a probability distribution for the @xmath137 configuration which decouples at each site , @xmath138 \\\\",
    "p(w_i ) \\ : = \\ : \\int          \\!\\!\\frac{\\d \\eta_i}{\\sqrt{2\\pi}}\\ ,          \\exp\\!\\left(\\frac{-\\eta^2_i}{2 } + pg(w_i,\\eta_i)\\right)\\end{aligned}\\ ] ] where @xmath139 the maximal value of @xmath33 with respect to @xmath90 gives the maximum entropy distribution for @xmath90 at each site .",
    "this leads to the expression , @xmath140 where @xmath95 is drawn from a gaussian with zero mean and unit variance .",
    "the constraints can be used to obtain values for the lagrange multipliers , @xmath141 the bars denote averages over the gaussian noise which in general must be done numerically .",
    "the third and fourth order terms in equations ( [ k3inf ] ) and ( [ k4inf ] ) can be found once the lagrange multipliers have been determined , @xmath142 again , the bars denote averages over the gaussian noise .",
    "rewriting equation  ( [ q_infty ] ) we have , @xmath143 where @xmath144 is the fourier transform of @xmath82 , @xmath145 the conditional probability for correlations @xmath82 can be defined if weights are assumed to come from the maximum entropy distribution defined in [ max_ent ] . in this case one",
    "has , @xmath146 where the angled brackets denote averages over @xmath147 and @xmath148 .",
    "the weights at each site are distributed according to , @xmath149 here , @xmath90 is the mean weight per site , defined in equation  ( [ w_i ] ) .",
    "we consider the fourier transform of @xmath82 since this appears in the appropriate generating function , @xmath150 writing the delta functions as integrals and noting that one of the integrals is removed by the fourier transform , one finds ( ignoring multiplicative constants ) , @xmath151 @xmath152 each site decouples and the average over sites can be taken by integrating over the weight distribution defined in equation  ( [ p(w_i ) ] ) .",
    "the resulting integral can be computed for large @xmath3 by the saddle point method since the exponent can be made extensive by appropriate rescaling .",
    "eventually one finds ( ignoring multiplicative constants ) , @xmath153 @xmath154\\ ] ] the saddle point equations fix @xmath155 and @xmath156 as implicit functions of @xmath35 , @xmath157 and @xmath21 , @xmath158 define @xmath159 , whose logarithm is the generating function for @xmath83 , @xmath160\\end{aligned}\\ ] ] we express the overlap distributions by their fourier transformed cumulant expansions , @xmath161 now @xmath159 is an integral over @xmath162 , @xmath163 , @xmath35 and @xmath157 which can again be computed by the saddle point method .",
    "one finds that as @xmath164 , the saddle point equations are satisfied by , @xmath165 these are related through an implicit function for @xmath89 in terms of mean overlap after selection , @xmath166 then the natural increase contribution for the correlation after selection is given by , @xmath167"
  ],
  "abstract_text": [
    "<S> a formalism for describing the dynamics of genetic algorithms ( gas ) using methods from statistical mechanics is applied to the problem of generalization in a perceptron with binary weights . </S>",
    "<S> the dynamics are solved for the case where a new batch of training patterns is presented to each population member each generation , which considerably simplifies the calculation . </S>",
    "<S> the theory is shown to agree closely to simulations of a real ga averaged over many runs , accurately predicting the mean best solution found . for weak selection and large problem size the difference equations describing the dynamics </S>",
    "<S> can be expressed analytically and we find that the effects of noise due to the finite size of each training batch can be removed by increasing the population size appropriately . </S>",
    "<S> if this population resizing is used , one can deduce the most computationally efficient size of training batch each generation . for independent patterns </S>",
    "<S> this choice also gives the minimum total number of training patterns used . </S>",
    "<S> although using independent patterns is a very inefficient use of training patterns in general , this work may also prove useful for determining the optimum batch size in the case where patterns are recycled .    </S>",
    "<S> # 1#2    [ the dynamics of a ga for a simple learning problem ] </S>"
  ]
}