{
  "article_text": [
    "the compressed sensing problem is currently the focus of an extensive research activity and can be stated as follows : given a sparse vector @xmath6 and an observation matrix @xmath7 with @xmath8 , try to recover the vector @xmath9 from the small measurement vector @xmath10 .",
    "although the problem consists of solving an overdetermined system of linear equations , enough sparsity will allow to succeed as shown by the following lemma ( where @xmath11 will denote the set of all @xmath12-sparse vectors , i.e. vectors whose components are all zero except for at most @xmath12 of them ) ,    [ algcond ] @xcite if @xmath13 is any @xmath14 matrix and @xmath15 , then the following properties are equivalent :    \\i . the decoder @xmath16 given by @xmath17k satisfies @xmath18 , for all @xmath19 ,    \\ii . for any set of indices @xmath20 with @xmath21 , the matrix @xmath22 has rank @xmath23 where @xmath22 stands for the submatrix of @xmath13 composed of the columns indexed by @xmath20 only .      .",
    "the main problem with decoder @xmath24 is that the optimization problem ( [ l0 ] ) is in general np - hard . for this reason ,",
    "the now standard @xmath0 relaxation strategy is adopted , i.e. the decoder @xmath25 is obtained as @xmath26 now , solving ( [ l1 ] ) can be done in polynomial time and thus @xmath25 can be efficiently computed .",
    "the second problem is to give robust conditions under which exact recovery holds .",
    "one such condition was given by candes romberg and tao @xcite and is now known as the uniform uncertainty principle ( uup ) or as the restricted isometry property ( rip ) .",
    "one of the main remaining challenges is to reduce the number of observations @xmath27 needed to recover a given sparse signal @xmath9 .",
    "one idea is the use of @xmath28 , @xmath29 decoders @xmath30 .",
    "the main draw back of the approach using @xmath28 , @xmath29 norm minimization is that the resulting decoding scheme is again np - hard .",
    "another idea is to use a reweighted @xmath0 approach as proposed in @xcite .",
    "@xmath31 and @xmath32 @xmath33 @xmath34 @xmath35 @xmath36 @xmath37 .",
    "@xmath38    @xmath39 and @xmath40 .",
    "the main intuition behind this reweighted @xmath1 relaxation is the following . the greater the component @xmath41 becomes , the smaller weight it should receive since it can be considered that this component should not be set to zero .",
    "the main drawback of the reweighted @xmath0 approach is that an unknown parameter is to be tuned whose order of magnitude is hard to know ahead of time .",
    "another approach was proposed in @xcite and uses lagrange duality .",
    "let us write down problem ( [ l0 ] ) , to which @xmath24 is the solution map , as the following equivalent problem @xmath42 where @xmath43 denotes the vector of all ones . here since the sum of the @xmath44 s",
    "is maximized , the variable @xmath45 plays the role of an indicator function for the event that @xmath46 .",
    "this problem is clearly nonconvex due to the quadratic equality constraints @xmath47 .",
    "however , these constraints can be merged into the unique constraint @xmath48 , leading to the following equivalent problem @xmath49    the alternating @xmath0 algorithm consists of a suboptimal alternating minimization procedure to approximate the dual function at @xmath50 .",
    "the algorithm is as follows .",
    "@xmath31 and @xmath32 @xmath33 @xmath51 @xmath35 @xmath52 @xmath53 @xmath38    @xmath39 and @xmath40 .",
    "notice that , similarly to the reweighted @xmath0 algorithm , the alternating @xmath0 method also requires the tuning of an unknown parameter @xmath50 .",
    "however , the main motivation for this proposal is that this parameter @xmath50 has a clear meaning : it is a dual variable which , in the case where the dual function @xmath54 is well approximated by the sequence @xmath55 , can be efficiently optimized without additional prior information , due to the convexity of the dual function .",
    "the main remark about the alternating @xmath0 method is the following ( see @xcite ) : for a given dual variable @xmath50 , the alternating @xmath0 algorithm can be seen a sequence @xmath56 of truncated @xmath0-norm minimizers of the type @xmath57 where @xmath58 is the set of indices for which @xmath59 .",
    "therefore , the alternating @xmath0 algorithm can be seen as an iterative thresholding scheme with threshold value equal to @xmath60 .",
    "now assume for instance that a fraction @xmath2 of the non zero components is well identified by the plain @xmath0 step with solution @xmath61 .",
    "then , the practitioner might ask if the appropriate value for @xmath50 is the one which imposes an @xmath0 penalty on the index set corresponding to the @xmath62 smallest components of @xmath61 .",
    "moreover , the large scale simulation experiments which have been performed on the plain @xmath0 relaxation seemed to agree on the fact that the breakdown point occurs near @xmath63 .",
    "thus , a practitioner could be tempted to wonder whether @xmath64 is a sensible value .",
    "motivated by the previous practical considerations , the two stage @xmath0 algorithm is defined as follows ( the parameter @xmath50 is now replaced by the parameter @xmath65 ) .",
    "@xmath66 @xmath67 and @xmath68index set of the @xmath2 largest components of @xmath69    @xmath70 @xmath71 .",
    "notice that we restrict @xmath3 to lie in @xmath72 .",
    "the reason should be obvious since , due to lemma [ algcond ] , even decoder @xmath16 is unable to identify more that @xmath73-sparse vectors .",
    "another remark is that the procedure could be continued for more than 2 steps but simulation experiments of the alternating @xmath0 method seem to confirm that in most cases two steps suffice to converge .",
    "at step 1 of the method , a subset @xmath20 is selected with cardinal @xmath2 and optimization is then performed with objective function @xmath74 . in this section , we will adopt the following notations : @xmath75 will denote the support of @xmath76 , @xmath20 will denote the index set of the @xmath2 largest components of @xmath69 as defined in the two - stage @xmath0 algorithm .",
    "@xmath77 will be an abbreviation of @xmath78 , the `` good '' subset of @xmath79 or , in mathematical terms , the subset of indices of @xmath75 which also belong to @xmath79 .",
    "on the other hand , @xmath80 will denote the complement of @xmath81 in @xmath79 .",
    "[ lemu ] assume the cardinal of @xmath81 is less than @xmath82 and that @xmath13 satisfies @xmath83 .",
    "let @xmath84 .",
    "then , there exists a positive number @xmath85 depending on @xmath76 such that @xmath86 .",
    "moreover , if @xmath87 , then @xmath88 .",
    "* proof*. let @xmath89 denote the optimal value of the problem @xmath90 subject to @xmath91 assume that @xmath92 .",
    "then , @xmath89 plays the role of a norm for @xmath93 although it does not satisfy the triangle inequality .",
    "in particular , @xmath89 is nonnegative , convex and @xmath94 implies that @xmath95 .",
    "nonnegativity and convexity are straightforward .",
    "assume that @xmath94 , i.e. the solution @xmath96 of ( [ n ] ) is null .",
    "this implies that @xmath97 , which implies that @xmath98 is in the kernel of @xmath99 .",
    "using the fact that @xmath81 has cardinal less that @xmath100 and the @xmath83 assumption , we conclude that @xmath101 . in order to finish the proof of the lemma , it remains to recall that @xmath89 is convex and that , by theorem 1.1 in @xcite , @xmath102 ( and thus @xmath103 ) is bounded from above by @xmath104 in order to obtain existence of a sufficiently small positive constant @xmath85 depending on @xmath76 such that @xmath105 for all @xmath93 in the ball @xmath106 .",
    "the desired result then follows .    to prove that @xmath94 if @xmath95 is a bit harder .",
    "thus , assume that @xmath95 .",
    "then , the solution @xmath96 of ( [ n ] ) is just the solution of @xmath107 now since @xmath10 , we obtain that @xmath108 and thus , the right hand side term is nothing but the image of a @xmath100-sparse vector .",
    "now , recalling that we assumed @xmath83 , theorem 1.1 in @xcite implies that @xmath96 must be the sparsest solution of the system @xmath109 from which we deduce that @xmath96 is @xmath100-sparse .",
    "therefore the vector @xmath110 is @xmath111 which solves @xmath112 . on the other hand",
    ", @xmath113 also solves @xmath114 and its support is included in the support of @xmath110 .",
    "@xmath115 is a @xmath100 sparse vector which lies in the kernel of @xmath13 . using again the fact that @xmath83 holds , we conclude that @xmath116 .",
    "thus , @xmath117 and @xmath118 .",
    "@xmath119    using this lemma , we deduce the following theorem .",
    "[ step1 ] assume that @xmath83 holds and that an index set @xmath120 of cardinal greater than or equal to @xmath121 has been recovered at step 0 after thresholding , then @xmath122 satisfies @xmath123 for some constant @xmath124 depending on @xmath76 .",
    "* proof*. the vector @xmath122 satisfies @xmath125 let us write @xmath84 . using ( [ opt1 ] ) , a now standard decomposition gives @xmath126 we thus obtain @xmath127 however , since @xmath83 holds , @xmath128 holds too , with @xmath129 .",
    "therefore , we obtain that @xmath130 combining ( [ eq1 ] ) and ( [ eq2 ] ) , we obtain @xmath131 as a consequence , we obtain that @xmath132 which , using lemma [ lemu ] , implies @xmath133 which is the desired bound .",
    "@xmath119    the following corollary is a straightforward consequence of the previous theorem .",
    "assume that the assumptions of theorem [ step1 ] are satisfied .",
    "then , exact reconstruction is obtained if @xmath134 , i.e. @xmath76 is @xmath12-sparse .",
    "the following monte carlo experiments show that the performance of the two - stage @xmath0 algorithm which drops the penalty over the index set of the @xmath135 largest components of the solution of plain @xmath0 are almost as good as the performance of the reweighted @xmath0 with the best parameter which is usually unknown in practice .",
    "a python program is available at http://stephane.g.chretien.googlepages.com/alternatingl1 and can be used to perform these experiments and other involving the alternating @xmath0 algorithm .       for @xmath136 , @xmath137 , @xmath138 , @xmath139 .",
    "@xmath13 and nonnul components of @xmath9 were drawn from the gaussian @xmath140 distribution .",
    "the results for the two - stage @xmath0 method are represented by the `` + in a circle '' sign.,width=566 ]          lemarchal , c. and oustry , f. , sdp relaxations in combinatorial optimization from a lagrangian viewpoint , advances in convex analysis and global optimization ( pythagorion , 2000 ) , nonconvex optim .",
    "54 , kluwer acad .",
    "publ . , pp .",
    "119134 .",
    "hiriart urruty , j .- b . and lemarchal , c. , convex analysis and minimization algorithms ii : advanced theory and bundle methods , springer- verlag , 1993 , 306 , grundlehren der mathematischen wissenschaften ."
  ],
  "abstract_text": [
    "<S> this paper gives new results on the recovery of sparse signals using @xmath0-norm minimization . </S>",
    "<S> we introduce a two - stage @xmath0 algorithm equivalent to the first two iterations of the alternating @xmath0 relaxation introduced in @xcite for an appropriate value of the lagrange multiplier . </S>",
    "<S> the first step consists of the standard @xmath1 relaxation . </S>",
    "<S> the second step consists of optimizing the @xmath1 norm of a subvector whose components are indexed by the @xmath2 largest components in the first stage . </S>",
    "<S> if @xmath3 is set to @xmath4 , an intuitive choice motivated by the fact that @xmath5 is an empirical breakdown point for the plain @xmath1 exact recovery probability curve , monte carlo simulations show that the two - stage @xmath1 method outperforms the plain @xmath1 in practice . </S>"
  ]
}