{
  "article_text": [
    "studying the relationship among the information divergence @xmath20 and the variational distance @xmath4 or , more specifically , determining lower bounds on @xmath20 in terms of @xmath4 , has been of interest at least since 1959 , when volkonskij and rozanov @xcite showed that @xmath21 . the best known result in this direction",
    "is usually referred to as pinsker s inequality and states that @xmath22 . in general , studying the relationship between @xmath20 and @xmath4 is important because it allows one to `` ... translate results from information theory ( results involving @xmath20 ) to results in probability theory ( results involving @xmath4 ) and _ vice versa _ '' ( fedotov , harremos and topse , @xcite ) .",
    "for instance , barron @xcite found a strengthened version of the central limit theorem by showing convergence in the sense of relative entropy and then using pinsker s inequality to conclude convergence in the variational norm .",
    "in different settings , this idea has also been used by topse @xcite and harremos and ruzankin @xcite .",
    "interestingly , these kind of results and its relation with gagliardo - nirenberg and generalized sobolev inequalities have been used recently in order to obtain the decay rate of solutions of nonlinear diffusion equations  see del pino and dolbeault @xcite and references therein .",
    "pinsker s inequality was proved independently by csiszr @xcite and kemperman @xcite , building on previous work by pinsker @xcite , kean @xcite and csiszr @xcite .",
    "the constant @xmath23 in @xmath17 is _ best possible _ , in the sense that there is a probability space and two sequences of probability measures @xmath24 and @xmath25 such that @xmath26 .",
    "sharpened pinsker type inequalities bounding @xmath20 by higher - order polynomials in @xmath27 are also available .",
    "for instance , @xmath28 see kullback @xcite and vajda @xcite , where again the constant @xmath29 is best possible , in the sense that there are sequences @xmath24 and @xmath25 such that @xmath30 / v^4 ( p_n , q_n ) \\downarrow \\frac{1}{36}$ ] .",
    "more recently , topse showed in @xcite that @xmath31 , while fedotov _",
    "et al _ @xcite have obtained a parametrization of the curve @xmath32 in terms of hyperbolic trigonometric functions and argue in @xcite that the best possible extended pinsker inequality contains terms up to and including @xmath33 .",
    "let @xmath34 and @xmath35 be probability measures on a measurable space @xmath36 and @xmath37 and @xmath38 their densities or radon - nikodym derivatives with respect to a common dominating measure @xmath39 .",
    "the information divergence is @xmath40 and is also known as _ relative entropy _ or _ kullback - leibler divergence_. the variational ( or @xmath19 ) distance is @xmath41 .",
    "the @xmath0-divergence generated by @xmath0 is @xmath42 , where @xmath43 is convex and @xmath44 .",
    "jensen s inequality implies that @xmath45 with equality holding if and only if @xmath46 , provided that @xmath0 is strictly convex at @xmath47 . hence",
    ", @xmath48 can be thought of as a measure of discrepancy between @xmath34 and @xmath35 .",
    "the class of @xmath0-divergences was introduced by csiszr @xcite and ali and silvey @xcite .",
    "it includes many of the most popular distances and discrepancy measures between probability measures .",
    "both @xmath20 and @xmath4 belong to this class , respectively for @xmath49 and @xmath50 .",
    "all of the following are also @xmath0-divergences : the @xmath51 divergence @xmath52 , the hellinger discrimination @xmath53 , the triangular or harmonic divergence @xmath54 , the capacitory discrimination @xmath55 where @xmath56 and the jeffrey s divergence @xmath57 . a convenient one - parameter family which includes many of the above as special cases",
    "is generated by the convex functions @xmath58^{-1 } ( u^{\\alpha } -1)$ ] ( @xmath14 ) .",
    "the resulting divergence @xmath59^{-1 } [ \\int q^{\\alpha } p^{1-\\alpha } \\ , d \\mu -1]$ ] is called _ relative information of type _ ( @xmath60 ) by vajda @xcite and taneja @xcite .",
    "it is easy to check that @xmath61 , @xmath62 and @xmath63 .",
    "the tsallis and the cressie - read divergences , which are used extensively in many areas including physics , economics and statistics , are respectively @xmath64 and @xmath65 ( see @xcite ) . finally , the rnyi s _ information gain of order @xmath66 _ , @xmath67 $ ] , of which the information divergence is also a special case as @xmath68 , although not itself an @xmath0-divergence , can be expressed as @xmath69 $ ] , cf .",
    "@xcite . regarding the relationship between @xmath0-divergences and @xmath4 , bounds are available for some special cases involving divergences which are more or less easy to manipulate .",
    "for instance , it is known that @xmath70 , @xmath71 and @xmath72 , see dacunha - castelle @xcite , lecam @xcite , dragomir , gluscevi and pearce @xcite and topse @xcite .",
    "a precise bound is available for the capacitory divergence , for which topse @xcite showed that @xmath73^{-1 } v^{2n}$ ] @xmath74 .",
    "although we know of no general result giving a lower bound for @xmath0-divergences in terms of @xmath4 , it appears as intuitively clear to us from the fact that @xmath0-divergences share many of the properties of @xmath20 that inequalities similar to pinsker s should also hold for other divergences .",
    "this should be the case , for instance , of relative information of type ( @xmath60 ) with @xmath11 close to zero or that of rnyi s information gain of order @xmath11 with @xmath11 close to one .",
    "maybe the closest to a general statement giving a kind of lower bound for an arbitrary @xmath1 in terms of @xmath4 is in csiszr ( * ? ? ?",
    "* theorem 1 ) , which states that @xmath75 implies , for sufficiently small @xmath76 , that @xmath77 ( _ cf . _ our theorem [ prop.pinskerf ] below ) , implying then that @xmath4 should be small whenever @xmath1 is small enough",
    ".    this paper will be the first of a series dealing with the relationship between @xmath0-divergences and variational distance .",
    "in particular , our objective here is to discuss conditions under which an @xmath0-divergence satisfies either a pinsker s type inequality @xmath2 or a fourth - order inequality @xmath3 .",
    "we will show in section 3 that a sufficient condition for @xmath2 is that the ratio between @xmath78 and the difference between the generating @xmath0 and its tangent at @xmath47 be upper bounded by a straight line @xmath79 with nonnegative coefficients @xmath80 and @xmath81 such that @xmath82 , if we want @xmath5 to be _ best possible_. a sufficient condition for having a fourth - order inequality , always with _ best possible _ coefficients , is presented in section 4 . each of these theorems is followed by a corollary which gives conditions on the derivatives of @xmath0 which are easier to check in practice than the original conditions on @xmath0 . as a consequence of these we show in section 3 that the relative information of type ( @xmath60 ) satisfies @xmath83 whenever @xmath13 , @xmath14 .",
    "this inequality is improved in section 4 to @xmath84 . using that @xmath69 $ ]",
    ", we also obtain that the rnyi s information gain of order @xmath11 satisfies @xmath85 whenever @xmath16 .    besides sections 3 and 4 dealing respectively with second and fourth - order inequalities , the rest of the paper is organized as follows .",
    "section 2 introduces some additional notation and states a fundamental inequality between powers of @xmath4 and @xmath1 in corollary [ prop.lemma.vmn ] . in section 5",
    "we bring forward an argument from the sequel of this paper and briefly discuss why the tools that we use here to obtain second and fourth - order inequalities are insufficient to obtain sixth and higher - order inequalities when we are interested in _ best possible _ coefficients .",
    "some technical results needed in section 4 are presented in an appendix .",
    "finally , since some of the proofs in section 4 and in the appendix require somewhat lengthy calculations , we have recorded a maple script that could help the reader interested in checking them .",
    "although the script is not included here for reason of space , it is available from us on request .",
    "notwithstanding , we stress that we have included in the paper what we believe are full and complete proofs for all statements made .",
    "throughout , equalities or inequalities between divergences will be understood to hold for every pair of probability measures , so that we will write for instance `` @xmath86 '' instead of `` @xmath87 for every @xmath88 '' .",
    "an equivalent definition for the variational distance is @xmath89 $ ] , where @xmath90 . hence @xmath91 with equality holding respectively if and only if @xmath46 or @xmath92 .",
    "it is well known that the information divergence satisfies @xmath93 .",
    "@xmath94 can occur if and only if @xmath46 , while @xmath95 implies that @xmath96 , although the reciprocal does not hold .    to avoid unnecessary discussion , we will assume the usual conventions @xmath97 , @xmath98 and @xmath99 .",
    "an @xmath0-divergence does not determine univocally the associated @xmath0 .",
    "indeed , for any @xmath80 fixed , @xmath1 and @xmath100 are identical .",
    "for instance , @xmath101 . for any convex @xmath0 we will let @xmath102 , which is nonnegative due to convexity considerations ( more precisely , @xmath103 can be taken to be any number between the left and right derivatives of @xmath0 at @xmath47 ) . we note here that second and higher - order derivatives of @xmath0 and @xmath104 coincide .",
    "indeed , we will often switch from one to the other in sections 3 and 4 .",
    "in general , @xmath0-divergences are not symmetric , in the sense that @xmath48 does not necessarily equals @xmath105 , unless the generating @xmath0 satisfies that @xmath106 for some fixed @xmath80 .",
    "this is the case for instance of @xmath4 and the @xmath107 and @xmath108 divergences but not that of @xmath20 or @xmath51 .",
    "whenever an @xmath0-divergence is not symmetric we could define the _ reversed _ divergence by letting @xmath109 , so that @xmath110 .",
    "similarly , beginning from an arbitrary @xmath0-divergence it is possible to construct a symmetric measure by using the convex function @xmath111 .",
    "for instance , the reversed information divergence is @xmath112 and its symmetrized version is the already mentioned jeffrey s divergence .",
    "the following lemma is slightly more general than what we will actually need in sections 3 and 4 .",
    "it gives an upper bound on @xmath113 in terms of a certain higher moment of @xmath114 and an @xmath0-divergence @xmath1 .",
    "if we interpret @xmath35 as an approximation to @xmath34 , then @xmath115 is the error between the corresponding approximate and actual expectations .",
    "the lemma generalizes results which we have used in @xcite in order to obtain upper limits for the approximating error in the context of bayesian statistics .    [ prop.lemmagmn ]",
    "let @xmath114 be both @xmath34 and @xmath35 integrable , @xmath116 such that @xmath117 , and @xmath118 . then for any fixed @xmath80 @xmath119^{n-1 } \\cdot d_f ( p , q ) \\,,\\ ] ] where @xmath120 , @xmath121 is the @xmath122-$]th moment of @xmath114 around @xmath80 with respect to the probability density @xmath123 and , as before , @xmath102 and @xmath103 is a number between the left and the right derivative of @xmath0 at @xmath47 .",
    "proof : let @xmath124 ( so that @xmath125 and @xmath126 are conjugate ) and @xmath127 .",
    "then we have for any real @xmath80 that @xmath128 \\tilde{f}^{1/n}(q / p ) \\ , p \\ , d \\mu   \\leq \\left [ \\int_c \\frac{|g - a|^m \\ , |q / p-1|^m } { \\tilde{f}^{m / n}(q / p ) } \\ , p   \\ , d \\mu \\right]^{1/m } \\cdot \\left [ d_f ( p , q ) \\right]^{1/n } \\\\ & & = \\left [ \\int_c \\frac{|g - a|^m \\ , |q / p-1|^m}{\\tilde{f}^{m / n } ( q / p ) k(q / p ) } \\ , k(q / p ) \\ , p   \\ , d",
    "\\mu \\right]^{1/m } \\cdot \\left [ d_f ( p , q ) \\right]^{1/n } \\\\ & & \\leq \\sup_{u > 0 , u \\not= 1 } \\left\\ { \\frac{(u-1)^m}{\\tilde{f}^{m / n } ( u ) k(u ) } \\right\\}^{1/m } \\ ; \\cdot \\left [ \\int |g - a|^m r   \\ , d \\mu \\right]^{1/m } \\cdot \\left [ d_f ( p , q ) \\right]^{1/n } \\,,\\end{aligned}\\ ] ] where the second inequality follows from hlder s inequality .",
    "the desired result now follows after taking the @xmath125-th power in the leftmost and rightmost terms and noting that @xmath129 .",
    "taking @xmath130 and @xmath131 , the left hand side of ( [ eqn.lemma.gmn ] ) becomes @xmath132 , while @xmath133 .",
    "hence , we have the following corollary .    [ prop.lemma.vmn ]",
    "let @xmath134 , @xmath0 and @xmath104 be as before .",
    "then @xmath135    remark 1 .",
    "these results are still valid for nonconvex @xmath0 provided that @xmath136 and we interpret @xmath137 .",
    "although ( [ prop.lemmagmn ] ) holds for any @xmath80 , we usually would like to use a value for which @xmath138 is small .",
    "taking @xmath139 could be a good idea .",
    "a more precise formulation would use the @xmath140 norm ( or essential supremum ) of @xmath141 with respect to the measure @xmath142 instead of the supremum for @xmath143 .    of particular interest will be the cases @xmath144 and @xmath145 , in which case equation ( [ eqn.lemma.vmn ] ) becomes @xmath146 and @xmath147    some interesting inequalities follow directly from ( [ eqn.lemma.2 ] ) .",
    "for instance , taking ( i ) @xmath148 and @xmath149 we obtain that @xmath70 , ( ii ) @xmath150 and @xmath151 , so that @xmath152 , it follows that @xmath153 and ( iii ) @xmath154 $ ] and @xmath155 , so that @xmath156 $ ] , we obtain that @xmath157 see kraft @xcite , cited in dragomir et al . @xcite .",
    "although we are not specially interested here in @xmath0-divergences for which @xmath158 , corollary [ prop.lemmagmn ] also gives some bounds for this case .",
    "for instance , for the triangular divergence of order @xmath159 , @xmath160 ( see topse @xcite or dragomir et al .",
    "@xcite ) , we obtain after taking @xmath161 , @xmath162 and @xmath163 in ( [ eqn.lemma.vmn ] ) that @xmath164 .",
    "we first note that also pinsker s inequality follows from ( [ eqn.lemma.2 ] ) . to see this , take @xmath165 and @xmath166 to obtain that @xmath167 , where @xmath168 $ ] ( the reason for the subindex @xmath169 here will be made clear shortly ) .",
    "now note that @xmath170 because @xmath171,while @xmath172 for @xmath173 since @xmath174 and @xmath175 .",
    "observe that using @xmath176 in the previous argument amounts up to using the mixture @xmath177 in the proof of lemma [ prop.lemmagmn ] .",
    "it is interesting to note the reason why this and only this mixture works .",
    "using a mixture @xmath178 for some @xmath179 , @xmath180 is equivalent to taking @xmath181 in equation ( [ eqn.lemma.2 ] ) .",
    "now , define @xmath182 $ ] and observe that @xmath183 for any @xmath184 , so that @xmath185 , while @xmath186 attains its maximum value for @xmath47 if and only if @xmath187 .",
    "( see figure [ figura.hw ] ) .",
    "hence , it follows that @xmath188 whenever @xmath180 and using any mixture with @xmath180 in lemma [ prop.lemma.vmn ] will produce a less than optimal inequality @xmath189 with @xmath190 .",
    "$ ] satisfies ( i ) @xmath191 for every @xmath184 ( by continuity ) and ( ii ) when @xmath184 is greater ( smaller ) than @xmath192 , @xmath193 attains its maximum for a @xmath194 greater ( respectively smaller ) than @xmath195 , hence for @xmath180 , the maximum value of @xmath193 is greater than @xmath196.,width=288,height=384 ]    the idea in the previous paragraph can be formulated for arbitrary @xmath0-divergences .",
    "let @xmath197 \\}$ ] and @xmath198 and suppose that @xmath199 with @xmath200",
    ". equation ( [ eqn.lemma.2 ] ) implies now that @xmath201^{-1 } v^2 $ ] for every @xmath179 .",
    "note that @xmath202 for every @xmath184 , so that @xmath203^{-1 } \\geq f''(1)/2 $ ] . in order to obtain the inequality @xmath204",
    ", we must find a @xmath205 such that @xmath203^{-1 } = f''(1)/2 $ ] .",
    "in other words , the ( continuity corrected ) function @xmath206 should be maximized at @xmath47 , or equivalently @xmath207 should be minimized at @xmath47 .",
    "it is easy to check that @xmath208 ( u-1 ) + o(|u-1|)$ ] .",
    "hence , for @xmath209 to have a minimum at @xmath47 , the first - order term should vanish and hence @xmath210 .",
    "finally , for @xmath211 to be actually maximized at @xmath47 , we must have that @xmath212 and hence that @xmath213 \\geq \\frac{f''(1)}{2 } ( u-1)^2 \\,.\\ ] ] this leads to the following theorem .",
    "[ prop.pinskerf ] * ( pinsker type inequality for @xmath0-divergences)*. suppose that the convex function @xmath0 is differentiable up to order 3 at @xmath47 with @xmath214 , and let @xmath210",
    ". then ( [ cond.2nd ] ) implies that @xmath204 .",
    "the constant @xmath215 is best possible .",
    "proof : although a rigorous proof can be obtained following the ideas above , the following argument is easier once the right condition has been identified .",
    "first , note that for ( [ cond.2nd ] ) to hold we must have that @xmath216 > 0 $ ] for every @xmath217 .",
    "hence , ( [ cond.2nd ] ) implies that @xmath218 where the last inequality follows from ( [ eqn.lemma.2 ] ) after taking @xmath219 $ ] and @xmath220 $ ] .    to show that @xmath5 is best possible ,",
    "consider a binary space and suppose that @xmath34 assigns probabilities @xmath221 and @xmath222 to each point of @xmath223 , say @xmath224",
    ". for small @xmath225 define @xmath226 so that @xmath227 and @xmath228 ^ 2 + o(v^2 ) = \\frac{f''(1)}{2 } \\ , [ 4p(1-p)]^{-1 } \\ ,",
    "v^2 + o(v^2)$ ] . hence @xmath229^{-1}$ ] .",
    "taking @xmath230 completes the proof .    remark . the last part of the proof suggests that , when there is no set @xmath231 with @xmath232 , a better constant @xmath233 : \\ , a \\in { \\cal a } \\}]^{-1 } > \\frac{f''(1)}{2}$ ] can be found so that @xmath234 for every @xmath35 .",
    "this problem has been addressed recently for the information divergence by ordentlich and weinberger @xcite ) .    for most divergences",
    "the condition in the next proposition is easier to check than ( [ cond.2nd ] ) .",
    "[ prop.pinskerf.easy ] let @xmath0 and @xmath104 be as before , @xmath210 , and suppose that @xmath0 is three times differentiable with @xmath235 for all @xmath194 .",
    "then @xmath236 + 3 ( 1 - w_f ) \\right\\ } \\geq 0\\ ] ] implies ( [ cond.2nd ] ) and hence that @xmath237 .",
    "proof : let @xmath238-\\frac{f''(1)}{2 } ( u-1)^2 $ ] .",
    "now @xmath239 , @xmath240 + ( 1-w_f ) \\tilde{f } ( u ) - f''(1 ) ( u-1)$ ] and hence @xmath241 , @xmath242 + 2 ( 1-w_f ) \\tilde{f } ' ( u ) -f''(1)$ ] and therefore @xmath243 and finally @xmath244 + 3 ( 1-w_f ) f '' ( 1 ) ( u)$ ] . hence , ( [ cond.2nd.easy ] ) implies that @xmath245 for @xmath246 and @xmath247 for @xmath248 , so the following lemma implies that @xmath114 must be nonnegative , which is equivalent to ( [ cond.2nd ] ) .    remark .",
    "since we also have that @xmath249 , ( [ cond.2nd ] ) is also implied if @xmath250 + 4 ( 1-w_f ) f ' '' ( u ) \\geq 0 $ ] , but we usually find ( [ cond.2nd.easy ] ) easier to check .    [ lemma.derivs ]",
    "let @xmath251 and @xmath252 be @xmath253 times differentiable with @xmath254 , where @xmath255 is the n - th derivative of @xmath114 , and suppose that either ( i ) @xmath125 is even and @xmath256 for @xmath257 and @xmath258 for @xmath259 or ( ii ) @xmath125 is odd and @xmath260 for every @xmath194",
    ". then @xmath261 for every @xmath194 .",
    "proof : we prove first the case that @xmath125 is odd . since @xmath260 it follows that @xmath262 is convex , and since @xmath263 , it must have a minimum at @xmath47 , hence must be nonnegative . repeat the argument backwards to obtain that @xmath264 are also nonnegative .",
    "now in the case ( i ) that @xmath125 is even , note that @xmath255 decreases for @xmath246 and increases for @xmath248 and , since @xmath265 , it must be nonnegative , which reduces to the previous case .    in our view",
    "the most important application of corollary [ prop.pinskerf.easy ] is to the relative information of type @xmath266 and the rnyi s information gain of order @xmath11 .",
    "@xmath267 whenever @xmath13 , @xmath14 .",
    "also , @xmath268 for @xmath269 . in both cases",
    "the coefficient of @xmath27 is best possible .",
    "proof : for @xmath270 we have @xmath271^{-1}(u^{\\alpha } -1)$ ] .",
    "it is easy to check that @xmath272 , @xmath273 and the left hand side of ( [ cond.2nd.easy ] ) is @xmath274 , which satisfies the condition for @xmath13 .",
    "now , for @xmath16 , write @xmath275 $ ] and use that @xmath276 for @xmath277 ( cf",
    ".  topse @xcite ) to get that @xmath278 .",
    "pinsker s inequality can be seen as the limiting case of the inequality just stated for @xmath270 as @xmath279 or equivalently of that stated for @xmath280 as @xmath281 .",
    "the behavior of @xmath280 for @xmath282 is somewhat puzzling to us .",
    "we will show in section 4 ( cf .",
    "theorem [ prop.pinsker.f4 ] and corollary [ prop.relinfo4 ] ) that for any @xmath13 there are probability measures @xmath283 and @xmath284 such that @xmath285 and @xmath286 .",
    "hence , we must have that @xmath287 = \\frac{1}{2 } \\alpha v^2 + \\frac{1}{36 } \\alpha ( 1 + 5 \\alpha - 5 \\alpha^2 ) v^4 + o ( v^4)$ ] .",
    "since @xmath288 for @xmath289 , it follows that in this case we can not have that @xmath290 .",
    "we do not know whether the inequality holds for @xmath291 .",
    "the inequality @xmath292 is a consequence of the fact that @xmath293 ^ 3}\\ ] ] for all @xmath173 together with ( [ eqn.lemma.2 ] ) and ( [ eqn.lemma.4 ] ) . to prove ( [ ineq.log.4 ] ) , let @xmath294 [ 1 + \\frac{28}{45 } ( u-1)]^3 - \\frac{1}{2 } ( u-1)^2 [ 1 + \\frac{28}{45 } ( u-1)]^3 - \\frac{1}{36 } ( u-1)^4 [ 1 + \\frac{2}{3 } ( u-1)]$ ] and use lemma [ lemma.derivs ] after showing that @xmath295 and @xmath296 is positive everywhere because @xmath297    in this section we generalize the idea in the last paragraph for arbitrary @xmath0-divergences . in other words , an inequality of the form @xmath3 would be obtained if we can prove that @xmath298 ^ 3}.\\ ] ] for all @xmath299 . for this inequality to hold and being sharp enough so that @xmath6 and @xmath7 are best possible , it is necessary that the taylor expansions of both sides around @xmath47 must coincide up to and including fifth - order terms .",
    "this condition implies the expression of the @xmath300 s and @xmath301 s ( @xmath302 ) in terms of the derivatives of @xmath0 at @xmath47 in the next theorem .",
    "[ prop.pinsker.f4 ] * ( fourth - order extended pinsker inequality for @xmath0-divergences)*. let @xmath0 and @xmath104 be as before and define @xmath303 , @xmath304 , @xmath305 $ ] and @xmath306 suppose that both @xmath307 and @xmath308 and that for every @xmath173 we have @xmath309 \\ , [ 1",
    "+ ( 1-w_{4,f})(u-1)]^3 } \\nonumber \\\\ & & \\geq c_{2,f } \\ , ( u-1)^2   \\ , [ 1 + ( 1-w_{4,f})(u-1)]^3 + c_{4,f } \\ , ( u-1)^4   \\ , [ 1 + ( 1-w_{2,f})(u-1 ) ] \\,.\\end{aligned}\\ ] ] then @xmath3 .",
    "the coefficient @xmath7 is best possible .",
    "proof : we will prove first that ( [ cond.4th ] ) implies that @xmath310 ( @xmath311 ) .",
    "reasoning by contradiction , suppose first that @xmath312 $ ] , and evaluate both sides of ( [ cond.4th ] ) at @xmath313 to obtain that @xmath314 .",
    "since @xmath307 , this implies that @xmath315 when @xmath316 and @xmath317 when @xmath318 . hence also @xmath319 $ ] and we can evaluate again both sides of ( [ cond.4th ] ) now at @xmath320 to get that @xmath321 , so that @xmath322 implies now that @xmath323 while @xmath324 implies that @xmath325 .",
    "hence we must have in any case that @xmath326 and @xmath327 are equal , say @xmath328 , so that ( [ cond.4th ] ) becomes @xmath329 ^ 4 \\geq c_{2,f } \\ , ( u-1)^2   \\ , [ 1 + ( 1-w)(u-1)]^3 + c_{4,f } \\ , ( u-1)^4   \\ , [ 1 + ( 1-w)(u-1 ) ] \\,.\\ ] ] since we are still assuming that @xmath330 $ ] , consider this inequality as @xmath331 .",
    "the left hand side is equivalent to @xmath332 ( 1-w)^4 [ u + w/(1-w)]^4 $ ] , _",
    "i.e._a positive coefficient times an infinitesimal term of order 4 in @xmath333 $ ] , while the right hand side is equivalent to @xmath334 $ ] , _ i.e. _  an infinitesimal term of order 1 in @xmath333 $ ] .",
    "if @xmath335 we have that the principal part here is @xmath336 and hence ( [ ineq.temp ] ) can not hold as @xmath337 , while if @xmath338 the principal part of the right hand side is @xmath339 and ( [ ineq.temp ] ) can not hold as @xmath340 .",
    "this contradiction is due to the assumption that @xmath312 $ ] .",
    "similarly we prove that @xmath341 .",
    "now that we have proved that both @xmath342 $ ] and @xmath343 $ ] are positive for @xmath173 , we observe that condition ( [ cond.4th ] ) implies ( [ eqn.f4 ] ) and hence that @xmath344 ^ 3 } \\ , p \\ , d \\mu \\,.\\end{aligned}\\ ] ] hence , use respectively ( [ eqn.lemma.2 ] ) and ( [ eqn.lemma.4 ] ) to bound each term in the right hand side to obtain that @xmath345 .",
    "the proof that @xmath7 is best possible is similar to the last part of the proof of theorem [ prop.pinskerf ] .",
    "consider again a binary space and for small @xmath225 define @xmath346 and @xmath226 so that @xmath227 and @xmath347 .",
    "now let @xmath348 and expand @xmath349 around @xmath350 to obtain that @xmath351 .",
    "we leave the details to the reader .",
    "the following condition on the derivatives of @xmath0 is usually easier to prove than ( [ cond.4th ] ) .",
    "[ prop.pinskerf4.easy ] let @xmath0 , @xmath104 , @xmath6 , @xmath326 , @xmath7 and @xmath327 be as before , and suppose that @xmath0 is six times differentiable with @xmath235 for all @xmath194 .",
    "then @xmath352 [ 1+(1-w_{4,f})(u-1)]^3 } \\nonumber \\\\ & & + 6 \\frac{f^{(5 ) } ( u)}{f '' ( u ) } [ 1+(1-w_{4,f})(u-1)]^2 [ 4 - w_{2,f } - 3 w_{4,f } + 4 ( 1-w_{2,f})(1-w_{4,f } ) ( u-1 ) ] \\nonumber \\\\ & & + 90 \\frac{f^{(4 ) } ( u)}{f '' ( u ) } ( 1- w_{4,f } ) [ 1+(1-w_{4,f})(u-1 ) ] [ 2 - w_{2,f } - w_{4,f } + 2 ( 1-w_{2,f})(1-w_{4,f } ) ( u-1 ) ] \\nonumber \\\\ & & + 120 \\frac{f ' '' ( u)}{f '' ( u ) } ( 1- w_{4,f})^2 [ 4 - 3 w_{2,f } - w_{4,f } + 4 ( 1-w_{2,f})(1-w_{4,f } ) ( u-1 ) ] \\nonumber \\\\ & & + 144 ( 1- w_{2,f } ) ( 1- w_{4,f})^3 \\geq 0\\end{aligned}\\ ] ] implies ( [ cond.4th ] ) and hence that @xmath345 .",
    "proof : let @xmath353 [ 1+(1-w_{4,f})(u-1)]^3 - c_{2,f } ( u-1)^2 [ 1+(1-w_{4,f})(u-1)]^3 - c_{4,f } ( u-1)^4 [ 1+(1-w_{2,f})(u-1)]$ ] .",
    "now it is straightforward , although rather tedious , that @xmath354 , while @xmath355 equals the left hand side of ( [ cond.4th.easy ] ) times @xmath356 and hence is nonnegative .",
    "hence , lemma [ lemma.derivs ] implies that @xmath261 , which is equivalent to ( [ cond.4th ] ) .    the case of jeffrey s divergence ( @xmath357 ) is interesting ,",
    "because we have that @xmath358 , @xmath359 , @xmath360 , @xmath361 and the left hand side of ( [ cond.4th.easy ] ) is @xmath362/u^4 > 0 $ ] .",
    "hence @xmath363 .",
    "we note that from @xmath292 it follows immediately using the symmetry of @xmath4 that @xmath364 , but this bound is worst than the one we found using corollary [ prop.pinskerf4.easy ] .",
    "to finish this section we present the special case of the relative information and information gain of order @xmath11 .",
    "[ prop.relinfo4 ] @xmath12 for @xmath13 , @xmath14 .",
    "also , @xmath15 for @xmath16 . in both cases",
    "the coefficients of @xmath365 are best possible .",
    "proof : we prove first the assertion for @xmath270 .",
    "we have @xmath58^{-1 } ( u^{\\alpha } -1)$ ] and then @xmath366 , @xmath367 , @xmath368 , @xmath369 . again",
    "straightforwardly although rather lengthy , the left hand side of ( [ cond.4th.easy ] ) is @xmath370 \\,,\\end{aligned}\\ ] ] which we prove in the appendix that is positive for @xmath173 when @xmath13 .",
    "now , for rnyi s information gain we proceed as in the proof of corollary [ prop.pinskerf.easy ] and use again that @xmath276 for @xmath277 , so that @xmath371 } \\\\ & & \\geq \\alpha d_{(1-\\alpha ) } \\ , \\frac{2}{2 - \\alpha ( 1 - \\alpha ) d_{(1-\\alpha ) } } \\geq \\alpha d_{(1-\\alpha ) } [ 1 + \\frac{1}{2 } \\alpha ( 1-\\alpha ) d_{(1-\\alpha ) } ] \\\\ & & \\geq \\frac{1}{2 } v^2 + \\frac{1}{72 } ( \\alpha+1)(2-\\alpha ) v^4 + \\frac{\\alpha^2}{2}(1-\\alpha ) \\frac{1}{4 } v^4 = \\frac{\\alpha}{2 } v^2 + \\frac{1}{36 } \\alpha ( 1 + 5 \\alpha - 5 \\alpha^2 ) v^4 \\,.\\end{aligned}\\ ] ]    using corollary [ prop.relinfo4 ] we can obtain bounds for the tsallis and cressie - read divergences , since they can be expressed in terms of @xmath270 .",
    "unfortunately , the tools we have used so far are insufficient to obtain inequalities of the form @xmath372 including terms of at least order @xmath373 . to understand why",
    ", it is useful to restrict attention to the information divergence @xmath20 .",
    "our discussion at the beginning of sections 2 and 3 shows that the second and fourth - order inequalities for @xmath20 are due respectively to the inequalities @xmath374 $ ] and ( [ ineq.log.4 ] ) and corollary [ prop.lemma.vmn ] .",
    "now , it is straightforward to check that the difference between the left and the right hand sides of ( [ ineq.log.4 ] ) equals @xmath375 .",
    "indeed , we conjecture that @xmath376 ^ 3 } + \\frac{41}{12150 } \\ , \\frac{(u-1)^6}{[1+\\frac{23186}{38745 } ( u-1)]^5 } \\ , .\\ ] ] however , even if we could prove this assertion , we would obtain then only that @xmath377 .",
    "the coefficient @xmath378 , even if close , is smaller than the best possible @xmath379 found by topse @xcite . a possible explanation for this , or in other words , from where the difference @xmath380 comes from ,",
    "can be obtained looking at the divergences which result from the right hand side of ( [ ineq.log.6 ] ) .",
    "we have on one side that @xmath381 , with equality holding if and only if @xmath382 \\propto |q / p-1|$ ] ( actually , the proportionality constant must equal @xmath383 ) .",
    "similarly , @xmath384 ^ 3 } \\",
    ", p \\ , d \\mu \\geq v^4(p , q)$ ] , with equality holding if and only if @xmath385 \\propto |q / p-1|$ ] .",
    "hence , it follows from these two inequalities that @xmath386 ^ 3 } \\",
    ", p \\ , d \\mu \\geq \\frac{1}{2 } v^2(p , q ) + \\frac{1}{36 } v^4(p , q ) \\,,\\ ] ] but now equality can not hold since , even if close , @xmath387 .",
    "in fact , we conjecture that the infimum of the left hand side of ( [ eqn.surplus ] ) taken over all @xmath34 and @xmath35 such that @xmath388 equals @xmath389 .",
    "we hope to be able to report on these issues soon .",
    "before proving ( [ ineq.alpha.fourth ] ) we will state the following lemma .",
    "we have already used the idea in the lemma to obtain the decomposition ( [ eqn.deriv6.log ] ) .",
    "[ lemma.poli4 ] let @xmath390 and define @xmath391 , @xmath392/c_4 $ ] and @xmath393 then a sufficient condition for @xmath394 for every @xmath194 is that @xmath395 , @xmath396 and @xmath397 are nonnegative .",
    "proof of ( [ ineq.alpha.fourth ] ) : let @xmath13 , @xmath399 be the term between brackets in ( [ ineq.alpha.fourth ] ) and define @xmath400 and @xmath401 as in lemma [ lemma.poli4 ] , so that for instance @xmath402 which is of course nonnegative .",
    "next @xmath403 which is positive because @xmath404 is negative . using ( [ eqn.lemma.a0 ] )",
    "we obtain that @xmath405 where @xmath406 hence , to conclude the proof we need to show that @xmath407 or equivalently that @xmath408 whenever @xmath13 .",
    "this follows from the following identity @xmath409 since after examining the signs of the different factors it is possible to conclude that each term in the sum is nonnegative",
    ".    remark .",
    "checking that this last identity holds constitutes a formal proof of the fact that @xmath408 for every @xmath13 . explaining how we obtain it is a bit harder , specially since it involves a `` trial and error '' process .",
    "essentially , we try to divide @xmath410 by a polynomial @xmath411 of degree @xmath80 which was known to be positive for the desired range .",
    "hence we obtain that @xmath412 , where the degrees of @xmath35 and @xmath413 are at most @xmath414 and @xmath415 . a sufficient condition for @xmath408",
    "is then that both @xmath416 and @xmath417 are nonnegative for the desired range .",
    "since we have @xmath13 , natural candidates for @xmath418 took the form @xmath419 .",
    "the polynomial division can be made easily using a symbolic manipulation package ( cf .",
    "the function quo in maple ) , while a plotting routine can make an initial assessment of whether the decomposition was successful , i.e.  whether both @xmath35 and @xmath413 are nonnegative ( if not , we would try again with different @xmath126 and @xmath125 ) .",
    "for instance , the first successful division made to arrive to ( [ eqn.p10 ] ) had @xmath420 . in a sense",
    "this means that we change a degree @xmath421 problem ( showing that @xmath422 is nonnegative ) by two problems having degrees 2 and 7 ( showing respectively that @xmath35 and @xmath413 are nonnegative ) .",
    "the same procedure can be repeated for @xmath35 and for @xmath413 and then for their respective quotients and rests and so on until all polynomials involved are either of the form @xmath419 or have at most degree 2 and their roots ( hence their signs ) can be obtained analytically .",
    "after doing all these divisions it is easy to put back everything together into a unique decomposition as in ( [ eqn.p10 ] ) . of course",
    ", we have no guarantee that this procedure would work for any polynomial , but it worked for @xmath422 .",
    "this research was partially funded by a capes - procad grant .",
    "the author is also grateful to the department of mathematics of the universit di roma `` la sapienza '' and specially to prof .",
    "f.  spizzichino for support during a sabbatical leave ."
  ],
  "abstract_text": [
    "<S> we study conditions on @xmath0 under which an @xmath0-divergence @xmath1 will satisfy @xmath2 or @xmath3 , where @xmath4 denotes variational distance and the coefficients @xmath5 , @xmath6 and @xmath7 are _ </S>",
    "<S> best possible_. as a consequence , we obtain lower bounds in terms of @xmath4 for many well known distance and divergence measures . for instance , let @xmath8^{-1 } [ \\int q^{\\alpha } p^{1-\\alpha } \\ , d \\mu -1]$ ] and @xmath9 $ ] be respectively the _ relative information of type _ ( @xmath10 ) and _ rnyi s information gain of order _ @xmath11 . </S>",
    "<S> we show that @xmath12 whenever @xmath13 , @xmath14 and that @xmath15 for @xmath16 . </S>",
    "<S> pinsker s </S>",
    "<S> inequality @xmath17 and its extension @xmath18 are special cases of each one of these .    </S>",
    "<S> _ keywords : _ information or kullback - leibler divergence , relative entropy , variational or @xmath19 distance , rnyi s information gain , relative information .    </S>",
    "<S> msc : 94a17 , 26d15 .    </S>",
    "<S> [ theorem]*lemma * [ theorem]*corollary * [ theorem]*proposition * </S>"
  ]
}