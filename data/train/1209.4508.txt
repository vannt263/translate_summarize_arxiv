{
  "article_text": [
    "the complexity of matrix multiplication is one of the fundamental problems in theoretical computer science . since strassen s sub - cubic algorithm for matrix multiplication over a ring from the late 1960 s  @xcite ,",
    "the topic has received considerable attention , see  @xcite for a historical overview on the subject .",
    "it is conjectured that matrix multiplication admits an algorithm running in time @xmath12 for any @xmath14 . for more than 20 years",
    "the record holder was the algorithm by coppersmith and winograd  @xcite running in time @xmath15 .",
    "recently two results improving on  @xcite were announced . in his phd thesis stothers",
    "@xcite presents a refinement of the coppersmith - winograd algorithm running in time @xmath16 and vassilevska williams  @xcite developes a general framework for obtaining a tighter upper bound on the complexity of the coppersmith - winograd algorithm .",
    "the latter yields the best known bound of @xmath17 .",
    "several algorithms computing exactly the matrix product for special classes have been designed .",
    "for example , algorithms with running time better than @xmath17 are known for boolean matrix multiplication with sparse output  @xcite or for the case when the input or output matrices are sparse  @xcite . in a recent work iwen and spencer",
    "@xcite present a new class of matrices whose product can be computed in time @xmath18 by a deterministic algorithm : namely when the output matrix is guaranteed to contain at most @xmath19 non - zero entries in each column ( or by symmetry row ) . all improved algorithms use as a black - box the algebraic matrix multiplication algorithm which",
    ", unfortunately , is only of theoretical importance .",
    "it uses sophisticated algebraic approaches resulting in large constants hidden in the big - oh notation and does not admit an efficient implementation .",
    "this motivates the need of simple  combinatorial - like \" algorithms .",
    "+   + * approximate matrix multiplication . * the first approximation algorithm with rigorously understood complexity by cohen and lewis is based on sampling  @xcite . for input matrices with nonnegative entries",
    "they show a concentration around the estimate for individual entries in the product matrix with high probability .",
    "the amount of data to be handled has been growing at a faster rate than the available memory in modern computers .",
    "algorithms for massive data sets , where only sequential access to the input is allowed , have become a major research topic in computer science in the last decade .",
    "drineas et al .",
    "@xcite first recognized the need for memory - efficient methods for matrix multiplication when access to single columns and rows of the input matrices is possible .",
    "they present a randomized algorithm for approximating the product of two matrices based on sampling .",
    "the complexity of the algorithm as well as its accuracy depend on user - defined parameters .",
    "the algorithm is  pass - efficient \" since columns and rows of the input matrices are sequentially loaded into memory .",
    "the obtained approximation guarantees are expressed in terms of the frobenius norm of the input matrices and the user - defined parameters but their method does not result in a strong guarantee for individual entries in the output matrix .",
    "sarls  @xcite observed that instead of sampling rows and columns one can use random projections to obtain a sketch of the matrix product . a notable difference to  @xcite is that by sketching one obtains an additive error for each individual entry depending on the 2-norm of the corresponding row and column vector in the input matrices .",
    "recently pagh  @xcite introduced a new randomized approximation algorithm . instead of sketching the input matrices and then multiplying the resulting smaller matrices",
    ", we treat the product as a stream of outer products and sketch each outer product . using fast fourier transformation in a clever way ,",
    "pagh shows how to efficiently adapt the count - sketch algorithm  @xcite to an outer product .",
    "the algorithm runs in one pass over the input matrices and provides approximation guarantees in terms of the frobenius norm of their product .",
    "+   + * our contribution .",
    "*    * a new algorithm for the case where the input matrices consist of nonnegative entries only .",
    "this is the first nontrivial deterministic approximation algorithm for the multiplication of nonnegative matrices in a streaming setting .",
    "motivated by practical applications , we analyze the approximation guarantee and the algorithm complexity under the assumption that the entries adhere to zipfian distribution .",
    "we compare it to previously known randomized algorithms and show that for certain natural settings it is more efficient and achieves better approximation guarantees .",
    "* we present a new matrix multiplication algorithm for arbitrary real - valued input matrices by adapting the group testing algorithm for streams with updates in the turnstile model outlined in  @xcite for detecting the entries with large absolute value in matrix product . as a byproduct",
    "we obtain the first deterministic algorithm running in @xmath18 steps for matrix products with @xmath13 nonzero entries .",
    "note that our algorithms easily generalize to rectangular matrix multiplication but for the ease of presentation we consider the case of square input matrices .",
    "also , we will state the time complexity of our first algorithm using a function @xmath11 denoting the running time of a linear space deterministic sorting algorithm .",
    "clearly , @xmath20 for comparison based sorting but under some assumptions on the elements to be sorted also better bounds are known , e.g. the @xmath21 time integer sorting algorithm by han  @xcite .",
    "* linear algebra . *",
    "let @xmath22 denote the field of nonnegative real numbers . given matrices @xmath23 we denote their product by @xmath24 .",
    "the @xmath25th row of a matrix @xmath1 is written as @xmath26 , the @xmath27th column as @xmath28 .",
    "we use the term _ entry _ to identify a position in the matrix , not its value .",
    "thus , _ the weight _ of the entry @xmath29 in @xmath1 is the value in the @xmath25th row and @xmath27th column , @xmath30 , @xmath31 $ ] , for @xmath32:= \\{0,1,\\ldots , n-1\\}$ ] . when clear from the context however , we will omit weight .",
    "for example , nonzero entries will refer to entries whose weight is different from 0 and by heavy entries we mean entries with large weight .",
    "the _ outer product _ of a column vector @xmath33 and a row vector @xmath34 is a matrix @xmath35 such that @xmath36 , @xmath31 $ ] .",
    "the _ rank _ of a positive real number @xmath37 in a matrix @xmath1 , denoted as @xmath38 , is the number of entries strictly smaller than @xmath39 , plus 1 .",
    "note that @xmath39 does not need to be present in @xmath1 .",
    "the _ @xmath3-norm _ of a vector @xmath40 is @xmath41 for @xmath42 .",
    "similarly , we define the _ entrywise @xmath3-norm _ of a matrix @xmath43 as @xmath44}|a_{i , j}|^p)^{1/p}$ ] for @xmath45 .",
    "the case @xmath46 is the _ frobenius norm _ of @xmath1 denoted as @xmath47 .",
    "@xmath48-residual entrywise @xmath3-norm _ @xmath49 is the entrywise @xmath3-norm of the matrix obtained from @xmath1 after replacing the @xmath48 entries with the largest absolute values in @xmath1 , ties resolved arbitrarily , with 0 . +   + * data streaming . *",
    "our algorithms have strong connection to data streaming , therefore we will use the respective terminology . a _ stream _",
    "@xmath50 is a sequence of @xmath51 updates @xmath52 for items @xmath53 and @xmath54 .",
    "we assume @xmath55 $ ] .",
    "the _ frequency _ of @xmath25 is @xmath56 and @xmath57 is the _ frequency vector _ of the stream @xmath50 .",
    "the _ insert - only _ model assumes @xmath58 for all updates and in the _ non - strict turnstile _",
    "model there are no restrictions on @xmath59 and the values in @xmath60 @xcite . similarly",
    "to matrix entries , we will also refer to the frequency of an item @xmath25 as the _ weight _ of @xmath25 .",
    "items with weight above @xmath61 , for a user - defined @xmath62 , will be called _",
    "@xmath62-heavy hitters _ or just _ heavy hitters _ when @xmath62 is clear from the context . ordering the items in @xmath50 according to their absolute weight",
    ", the heaviest @xmath62 items in @xmath50 are called the _ top - b entries _ in @xmath50 . +   + * skewed distributions . *",
    "a common formalization of the skewness in real - life datasets is the assumption of _ zipfian distribution_. the elements in a given set @xmath63 over @xmath51 elements with positive weights follow _ zipfian distribution _ with parameter @xmath64 if the weight of the element of rank @xmath25 is @xmath65 where @xmath66 and @xmath67 denotes the total weight of elements in @xmath63 .",
    "we will analyze only the case when the skew in the data is not light and @xmath68 . for @xmath69",
    ", @xmath70 converges to a small constant .",
    "we will also use the facts that for @xmath69 , @xmath71 and for @xmath72 , @xmath73 .",
    "the nave algorithm for the multiplication of input matrices @xmath74 works by computing the inner product of @xmath26 and @xmath75 in order to obtain @xmath76 for all @xmath77 $ ] .",
    "an alternative view of the approach is the _ column row method _ computing the sum of outer products @xmath78}a_{*,i}b_{i , * } $ ] .",
    "while this approach does not yield a better running time , it turns out to admit algorithmic modifications resulting in more efficient algorithms .",
    "schnorr and subramanian  @xcite and lingas  @xcite build upon the approach and obtain faster algorithms for boolean matrix multiplication . assuming that @xmath1 is stored in column - major order and @xmath2 in row - major order , the approach yields a memory efficient algorithm since the matrix product @xmath4 can be computed in a single scan over the input matrices .",
    "recently , pagh  @xcite presented a new randomized algorithm combining the approach with frequent items mining algorithms  @xcite .",
    "inspired by this , we present another approach to modify the column - row method building upon ideas from deterministic frequent items mining algorithms  @xcite .      in  @xcite",
    "pagh discusses several applications where one is interested only in the entries with the largest absolute value in the matrix product .",
    "we argue that the restriction the input matrices to be nonnegative is feasible for many real - life problems .",
    "since our first algorithm is both simple and efficient , we thus believe that it will have practical value .    in  @xcite",
    "the authors discuss applications of nonnegative matrix multiplication for pattern recognition problems .",
    "the presented approximation algorithm is based on sampling and is shown to efficiently detect pairs of highly similar vectors in large databases for the case where the majority of vector pairs have low similarity .",
    "we present two concrete applications of nonnegative matrix multiplication where the entries in the output matrix adhere to a skewed distribution .",
    "a major problem in data mining is the detection of pairs of associations among items in _ transactional databases _ , see chapter  5 in  @xcite for an overview . in this problem , we are given a database of @xmath79 transactions , where a transaction is a subset of the ground set of items @xmath80 .",
    "we are interested in items @xmath81 such that an occurrence of @xmath25 in a given transaction implies with high probability an occurrence of @xmath27 in the transaction , and vice versa .",
    "the _ lift similarity measure _",
    "@xcite formalizes the above intuition .",
    "let @xmath82 and @xmath83 be the set of transactions containing the items @xmath25 and @xmath27 , respectively .",
    "then the lift similarity between @xmath25 and @xmath27 is defined as @xmath84 .",
    "the database is usually huge and does not fit in memory , therefore one is interested in solutions performing only a few scans of the database .",
    "there are randomized algorithms for the problem  @xcite but to the best of our knowledge there is no known deterministic algorithm improving upon the straightforward solution .    the problem can be reduced to matrix multiplication as follows . in a first pass over the database we compute the exact number of occurrences @xmath85 for each item @xmath25 .",
    "then we create an @xmath86 matrix @xmath1 such that @xmath87 if and only if the item @xmath25 occurs in the @xmath27th transaction .",
    "now it is easy to see that the weight of the entry @xmath29 in the product @xmath88 is equal to lift similarity between @xmath25 and @xmath27 .",
    "the nonzero entries in the @xmath27th column in @xmath1 correspond to the items in the @xmath27th transaction , thus the column row approach is equivalent to computing the stream of cartesian products for all transactions such that each occurrence of the the items pair @xmath89 has weight @xmath90    figure  [ fig : lift ] presents the distribution of lift similarities among pairs for two real datasets : pumsb , containing census data for population and housing , and accidents , built from data about traffic accidents in belgium .",
    "both datasets are publicly available at the frequent itemset mining dataset repository ( http://fimi.ua.ac.be/data/ ) .",
    "note that the similarities are plotted on a doubly logarithmic scale and except for a few highly correlated pairs , the similarities among item pairs decrease almost linearly .",
    "therefore , the assumption for zipfian distribution is realistic .    as a second example consider the popular markov clustering algorithm ( mcl ) for the discovery of communities within networks  @xcite . in a nutshell ,",
    "for a given graph the algorithm first creates a column stochastic matrix @xmath63 based on the graph structure .",
    "the algorithm then works iteratively . in each iteration",
    "we update @xmath63 to the product @xmath91 and then set small entries to 0 .",
    "a significant proportion of the entries in each column is small .",
    "@xmath63 converges fast to a sparse matrix indicating clusters around certain vertices in the original graph .",
    "the most time consuming steps are the first few iterations , thus an algorithm for the detection of the significant entries in matrix products can improve the time complexity . in an additional pass one can compute the exact weight of the significant entries .",
    "recall first how the majority algorithm  @xcite works .",
    "we are given a multiset @xmath63 of cardinality @xmath51 and want to find a majority element , i.e. an element occurring at least @xmath92 times in @xmath63 . while there are two distinct objects in @xmath63 we remove them from @xmath63 .",
    "it is easy to see that if there exists a majority element @xmath39 , at the end only occurrences of @xmath39 will be in @xmath63 .",
    "the frequent algorithm  @xcite builds upon this simple idea and detects @xmath62-heavy hitters in an unweighted stream @xmath50 of @xmath51 updates @xmath93 for items @xmath94 $ ] .",
    "we keep a _ summary _ of @xmath62 distinct entries together with a counter lower bounding their weight .",
    "whenever a new item @xmath25 arrives we check whether it is already in the summary and , if so , update the corresponding counter . otherwise , if there is an empty slot in the summary we insert @xmath25 with a counter set to 1 . in the case",
    "all @xmath62 slots are occupied we decrease the weight of all items by 1 and proceed with the next item in the stream .",
    "the last step corresponds to removing @xmath95 distinct items from the multiset of items occurring in @xmath50 and a simple argument shows that @xmath62-heavy hitters will be in the summary after processing the stream . by returning the estimated weight of the item in the summary and 0 for not recorded items ,",
    "the weight of each item is underestimated by at most @xmath96 where @xmath97 is the frequency vector of the stream . implementing the summary as a hash table and charging the cost of each item deletion to the cost incurred at its arrival the expected amortized cost per item",
    "update is constant .",
    "a sophisticated approach for decreasing the items weights in the summary leads to a worst case constant time per item update  @xcite .",
    "generalizing to nonnegative matrix multiplication by the column row method is intuitive .",
    "assume the input matrices consist of \\{0,1}-valued entries only .",
    "we successively generate the @xmath0 outer products and run the frequent algorithm on the resulting stream associating entries with items .",
    "there are several problems to resolve : first , we want to multiply arbitrary nonnegative matrices , thus our algorithm has to handle weighted updates .",
    "second , we have to consider @xmath98 occurrences of weighted items in the stream .",
    "third , we can not apply any more the amortized argument for the running time analysis since a group of @xmath99 heavy items might be followed by many lighter items causing expensive updates of the summary and it is not obvious how to extend the deterministic approach from  @xcite guaranteeing constant time updates in the worst case .",
    "the first issue is easily resolved by the following    [ key_lemma ]    let @xmath97 be the frequency vector of an insert only stream @xmath50 over a domain @xmath32 $ ] .",
    "after successively decrementing @xmath100 times the weight of at least @xmath62 distinct items by @xmath101 , @xmath102 , such that at each step @xmath103 for all @xmath104 , it holds @xmath105 for all @xmath62-heavy hitters , @xmath106 $ ] , for all @xmath107 .",
    "since @xmath103 for all @xmath94 $ ] holds , the total decrease is bounded by @xmath108 .",
    "a decrement of @xmath109 in the weight of a given item is witnessed by the same decrement in the weights of at least @xmath99 different items .",
    "thus , we have @xmath110 which bounds the possible decrease in the weight of a heavy hitter to @xmath96 .    in the next section",
    "we show that the specific structure of an outer product allows us to design efficient algorithms resolving the last two issues .",
    "matrices @xmath23 , summary @xmath111 for @xmath62 entries denote by @xmath112 the outer product of the @xmath25th column of @xmath1 and @xmath25th row of @xmath2 find the weight @xmath113 of the entry of rank @xmath95 in @xmath114 let @xmath115 be the @xmath62 entries in @xmath114 with rank less than @xmath95 , i.e. the largest @xmath62 entries decrease the weight of each entry in @xmath115 by @xmath113 add @xmath116 s weight in @xmath115 to @xmath116 s weight in @xmath111 remove @xmath116 from @xmath115 find the weight @xmath117 of the entry of rank @xmath95 in @xmath118 , if any update @xmath111 to contain the largest @xmath62 entries in @xmath118 and decrease their weight by @xmath119    entry @xmath29 return the weight of @xmath29 in @xmath111 return 0    we assume that @xmath120 is stored in column - major order and @xmath121 in row - major order .",
    "we show how to modify the column row method in order to obtain an additive approximation of each entry in @xmath4 in terms of the entrywise 1-norm of @xmath4 .",
    "essentially , we run the frequent algorithm for the stream of @xmath0 outer products : we keep a summary @xmath111 of @xmath62 distinct items and for each outer product we want to update the summary with the incoming weighted entries over the domain @xmath32\\times [ n]$ ] .",
    "the main difference is that for @xmath122 we can use the specific structure of an outer product and update the summary in @xmath123 steps . in computesummary in figure  [ main_alg ] for each of the @xmath0 outer products we simulate the successive application of lemma  [ key_lemma ] until at most @xmath62 entries with weight larger than 0 remain in the outer product .",
    "we then update @xmath111 with the remaining entries",
    ". +   + * correctness .",
    "*    [ correctness ] let @xmath124 be the weight of an entry @xmath125 in the product @xmath126 .",
    "after termination of computesummary for the estimated weight @xmath127 of @xmath124 returned by estimateentry , @xmath31 $ ] , holds @xmath128 .",
    "the product @xmath4 equals @xmath129 for the columns @xmath130 of @xmath1 and the rows @xmath131 of @xmath2 .",
    "we consider each outer product as @xmath132 updates for different entries over the domain @xmath32\\times[n]$ ] in an insert only stream with positive real weights .",
    "we show how the algorithm updates the summary for a single outer product @xmath114 .",
    "first , in line 3 the algorithm finds the entry of rank @xmath95 in @xmath114 . in line 4",
    "we decrease the weight of the @xmath62 largest entries by @xmath113 which yields the same result as the following iterative procedure : while there are at least @xmath95 nonzero entries in @xmath114 , find the entry with smallest weight @xmath133 in @xmath114 and decrease the weight of all non - zero entries by @xmath133 .",
    "equivalence holds because we always decrease the weight of an entry with the smallest weight and thus the decrease of the largest @xmath62 entries weights can never exceed @xmath113 .",
    "also , the decrease can not be smaller than @xmath113 since otherwise we would have more than @xmath62 non - zero entries in the outer product .",
    "thus , we always decrease by the same amount the weight of at least @xmath95 different entries which by lemma  [ key_lemma ] guarantees the claimed approximation error . in lines",
    "610 we apply essentially the same procedure again for the nonzero entries in the outer product and the entries in the summary .",
    "the remaining at most @xmath62 nonzero entries constitute the updated summary .    [",
    "[ running - time . ] ] running time .",
    "+ + + + + + + + + + + + +    in the following lemmas we present efficient deterministic algorithms for the subroutines used in computesummary .",
    "we concentrate how the algorithm updates the summary for a single outer product . before presenting our approach ,",
    "we give the main building blocks that will be used to achieve an efficient solution .",
    "[ median ] given two sorted vectors @xmath134 we can find the entry of rank @xmath62 in the outer product @xmath135 in time and space @xmath136 .",
    "we reduce the problem to selection of the element of rank @xmath62 in a cartesian sum @xmath137 for sorted sets of real numbers @xmath138 and @xmath139 . setting @xmath140 and @xmath141 and searching in the cartesian sum @xmath142 for the element of rank @xmath62 corresponds to searching for the entry of rank @xmath62 in the outer product @xmath135 , this follows from monotonicity of the @xmath143 function . the best known deterministic algorithm for selection in a cartesian sum of two sorted sets  @xcite runs in time and space @xmath136 .",
    "[ rank ] given vectors @xmath134 , with elements sorted in descending order , we can output an implicit representation of the largest @xmath62 elements from the outer product @xmath135 in a data structure @xmath115 in time and space @xmath136 .",
    "assume we have found the entry of rank @xmath62 as outlined in lemma  [ median ] , let this element be @xmath144 .",
    "let @xmath25 , @xmath27 be two pointers for @xmath145 and @xmath59 respectively .",
    "initialize @xmath146 .",
    "assume @xmath25 is fixed .",
    "we compare @xmath144 to @xmath147 .",
    "while it is larger or equal , we move left @xmath59 s pointer by decreasing @xmath27 by 1 . at the end",
    "we add the pair @xmath29 to @xmath115 , denoting that the entries in @xmath25th row of @xmath135 bigger than @xmath144 , and thus of rank less than @xmath62 , are all @xmath89 for @xmath148 .",
    "then we go to the next row in @xmath135 by incrementing @xmath25 and repeat the above while - loop starting with the current value of @xmath27 . when @xmath149 or @xmath150 we know that all entries smaller than @xmath144 have been found .",
    "correctness is immediate since the product @xmath151 is monotonically increasing with @xmath25 and decreasing with @xmath27 , and thus for each row of the outer product we record the position of the entries smaller than @xmath144 in @xmath115 .",
    "both @xmath25 and @xmath27 are always incremented or respectively decremented , thus the running time is linear in @xmath0 .",
    "we need to explicitly store only @xmath152 and @xmath115 , this gives the claimed space usage .",
    "next we present an efficient approach for updating the summary for a given outer product after finding the entries of rank at most @xmath62 .",
    "[ rtime_op ] for a given outer product @xmath135 , @xmath134 we update the summary @xmath111 in time @xmath153 .",
    "we first sort the vectors @xmath145 and @xmath59 in decreasing order according to the values @xmath154 and @xmath155 , respectively .",
    "let us call the sorted vectors @xmath156 and @xmath157 .",
    "each entry in @xmath156 and @xmath157 will be of the form @xmath158 such that @xmath159 and @xmath160 , respectively , i.e. @xmath161 will record the position of a given value in @xmath145 and @xmath59 .",
    "we define the entry @xmath125 in the outer product @xmath162 as a @xmath163 such that @xmath164 and @xmath165 . comparing the entries on the @xmath166 values",
    ", we can assume that we compute the outer product of two sorted vectors .",
    "assume we have computed the data structure @xmath115 implicitly representing the largest @xmath62 entries in @xmath167 , as shown in lemma  [ rank ] .",
    "now we show how to update the summary with the entries in @xmath115 in time @xmath153 .",
    "we introduce a _ position total order _ on entries such that @xmath168 iff @xmath169 , @xmath77 $ ] .",
    "we will keep the entries in @xmath111 in the summary sorted according to this order .",
    "assume we can output the @xmath62 heaviest entries from a given outer product sorted according to the position total order in @xmath115 . then in a merge - like scan through @xmath111 and @xmath115 we update the entries in @xmath170 , remove those from @xmath115 and obtain a sorted data structure containing the entries from @xmath111 and @xmath115 in @xmath171 steps .",
    "the entry of rank @xmath95 in the set @xmath172 , which has size at most @xmath173 , can be found in @xmath171 by  @xcite .",
    "thus , if the entries in @xmath115 are sorted according to the position total order , updating the summary will run in @xmath171 steps .    we output the @xmath62 heaviest entries sorted according to the position total order by the following algorithm .",
    "let @xmath115 be implicitly given as a sorted column vector @xmath156 and a sorted row vector @xmath157 as described above , and @xmath174 integer pairs @xmath175 denoting that in the @xmath176th row in the outer product @xmath167 the first @xmath177 entries have rank not more than @xmath62 .",
    "clearly , @xmath178 will monotonically decrease as @xmath176 increases .",
    "we start with @xmath179 , namely the shortest interval , sort the @xmath178 entries according to the position total order .",
    "we then decrease @xmath176 by 1 and sort the next @xmath180 entries according to the position total order .",
    "however , we observe that due to monotonicity @xmath181 and all elements from @xmath157 appearing in the @xmath176th row of @xmath167 also appear in the @xmath182th row .",
    "thus , we can sort only the new @xmath183 elements and then merge the result with the already sorted @xmath178 elements .",
    "we continue like this until the elements in each row of the outer product have been sorted .",
    "then we sort the elements in the column vector @xmath156 according to their position , keeping a pointer to the corresponding sorted subinterval of @xmath157 for each entry in @xmath156 . from this",
    "we build the set @xmath115 with entries sorted according the position total order . by setting @xmath184 the running time for the @xmath185 mergings and sortings",
    "amounts to @xmath186 .",
    "we can bound this sum by @xmath187 since @xmath188 , @xmath189 and @xmath190 for a monotonically growing superlinear function and numbers @xmath191 , @xmath94 $ ] , in its domain .",
    "the only remaining component is how to efficiently answer queries to the summary after processing all outer products .",
    "we use a static dictionary with constant look - up time . observing that the entries are from a universe of size @xmath132 , the best known result by rui  @xcite provides a construction in time @xmath192 and space @xmath171 . note that @xmath193 , therefore as a first result we observe that lemmas  [ correctness ] and  [ rtime_op ] immediately yield the following    [ approx_lemma ] given @xmath194-matrices @xmath195 with non - negative real entries , there exists a deterministic algorithm approximating the weight of each entry in the product @xmath10 of @xmath1 and @xmath2 within an additive error of @xmath196 .",
    "the algorithm runs in time @xmath197 and space @xmath198 in one pass over the input matrices .",
    "it was first observed by bose et al .",
    "@xcite that the frequent algorithm guarantees tighter estimates for items with weight significantly larger than @xmath199 in a stream of length @xmath51 and summary of size @xmath62 .",
    "berinde et al .",
    "@xcite develop a general framework for the analysis of so called _ heavy - tolerant _ counter based algorithms and show that frequent falls in this class . in order to keep the paper self - contained instead of referring to the results in  @xcite we show why our algorithm provides much better guarantees for skewed distributions than suggested by lemma  [ approx_lemma ]",
    ". also , the analysis we present is simpler since it does not apply to a wide class of counter based algorithms .",
    "let us first give some intuition why better approximation guarantees are possible .",
    "in order to bound the possible underestimation of an entry in the summary we assume that each decrement of a given entry is applied to at least @xmath62 distinct entries .",
    "since the total weight of all entries is @xmath200 we obtain that the total weight charged to a given entry is bounded by @xmath196 .",
    "if we had only @xmath171 different entries all having weight about @xmath196 , then the approximation error given by lemma  [ approx_lemma ] is tight .",
    "however , assume some entries have weight @xmath201 for @xmath202 .",
    "this means that the total weight we might have charged when decrementing the weights of groups of at least @xmath62 distinct entries can be bounded by @xmath203 .",
    "iteratively applying the argument to all heavy entries we arrive at improved approximation guarantees .",
    "the results in @xcite are based upon the above observation .",
    "( bose et al , @xcite ) [ boseetal ] for an entry @xmath125 in @xmath204 with weight @xmath205 , @xmath206 , after termination of computesummary it holds @xmath207 where @xmath208 is the approximation of @xmath209 returned by estimateentry@xmath125 .    by lemma  [ key_lemma ]",
    "we have that the underestimation is at most @xmath210 .",
    "as outlined above this implies that we have a total weight of at most @xmath211 to charge from , which implies a new upper bound on the underestimation of @xmath212 . successively applying the above reasoning @xmath48 times we can bound the underestimation to @xmath213 . since @xmath214 for @xmath215 the claim follows .",
    "( berinde et al , @xcite ) [ tail_lemma ] @xmath216 is an upper bound on the underestimation of any @xmath217 returned by estimateentry@xmath125 for any @xmath218 .",
    "assume that @xmath219 is an upper bound on the underestimation returned by estimateentry .",
    "we apply again the above reasoning and since @xmath220 we see that the underestimation is bounded by @xmath221",
    ". we can continue iterating in this way setting @xmath222 while @xmath223 for the underestimation in the @xmath25th step holds .",
    "we either reach a state where no progress is made or , since the underestimation is lower bounded by @xmath224 , we have @xmath225 as @xmath226 . in either case",
    "the claim follows .",
    "+   + the above lemmas are important since they yield approximation guarantees depending on the residual @xmath48-norm of the matrix product , thus for skewed matrix products the approximation is much better than the one provided by lemma  [ approx_lemma ] .",
    "+   + * sparse recovery . *",
    "the approximation of the matrix product @xmath126 in @xcite is analyzed in terms of the frobenius norm of the difference of @xmath10 and the obtained approximation @xmath227 , i.e @xmath228 . by simply creating a sparse matrix with all non - zero estimations in the summary",
    "we obtain an approximation of @xmath10 : the so called @xmath48-sparse recovery of a frequency vector @xmath97 aims at finding a vector @xmath229 with at most @xmath48 non - zero entries such that the @xmath3-norm @xmath230 is minimized .    as shown by berinde et al .",
    "@xcite the class of heavy - tolerant counter algorithms yields the best known bounds for the sparse recovery in the @xmath3-norm .",
    "the following theorem 1 follows from lemma  [ rtime_op ] and their main result .",
    "let @xmath231 be nonnegative @xmath194 real matrices and @xmath204 their product .",
    "there exists a one - pass approximation deterministic algorithm returning a matrix @xmath227 such that @xmath232 .",
    "the algorithm runs in time @xmath233 and uses space @xmath234 for any @xmath235 and @xmath236 .",
    "let us write the entries of @xmath10 in ascending order according to their weight as @xmath237 , thus @xmath238 .",
    "observe that by setting @xmath239 the bounds given by lemma  [ tail_lemma ] yield an underestimation for every entry of at most @xmath240 .",
    "thus , we can upper bound @xmath241 as @xmath242 .",
    "the last term is at most @xmath243 , thus we can upper bound @xmath244 to @xmath245 . the time and space complexity follow directly from lemma  [ rtime_op ] .",
    "+   + clearly , for @xmath246 the algorithm runs in subcubic time and subquadratic memory . in the next paragraph",
    "we show that for skewed output matrices estimateentry can provably detect the most significant entries even for modest summary sizes",
    ". +   + * zipfian distributions . * as discussed in section  [ skew_appl ] the assumption that the entries in the product adhere to a zipfian distribution is justified .",
    "the results stated below not only give a better understanding of the approximation yielded by the algorithm , but also allow direct comparison to related work .",
    "( berinde et al , @xcite ) if the entries weights in the product matrix follow a zipfian distribution with parameter @xmath69 , then estimateentry with a summary of size @xmath62    1 .",
    "approximates the weight of all entries with rank @xmath247 with additive error of @xmath248 .",
    "2 .   estimates the weight of all entries with additive error of @xmath249 for @xmath250 .",
    "3 .   returns the largest @xmath48 entries in the matrix product for @xmath251 .",
    "4 .   returns the largest @xmath48 entries in a correct order for @xmath252 .",
    "the first statement is a direct application of lemma  [ boseetal ] . for 2 .",
    "we choose @xmath253 in lemma  [ tail_lemma ] .",
    "the error is then bounded by @xmath254 . by plugging in the bounds on the tail of the zipfian distribution we obtain that the error is bounded by @xmath255 .",
    "this implies that there exists @xmath256 such that all entries of weight @xmath257 , for some constant @xmath258 , will be in the summary .",
    "this shows 3 .",
    "since the entry of rank @xmath48 has weight @xmath259 and we assume that @xmath260 is constant .",
    "for the last statement we observe that the function @xmath261 is monotonically decreasing for @xmath262 and @xmath69 .",
    "thus we need an error of at most @xmath263 . with some algebra we can bound the difference to @xmath264 , thus by @xmath265",
    "the claim follows .      the randomized algorithm by cohen and lewis  @xcite for computing the product of nonnegative matrices yields an unbiased estimator of each entry and a concentration around the expected entry weight with high probability .",
    "however , their algorithm requires a random walk in a bipartite graph of size @xmath266 space and is thus not space efficient .",
    "it is difficult to compare the bounds returned by estimateentry to the bounds obtained in  @xcite , but it is natural to compare the guarantee of our estimates to the ones shown by pagh  @xcite .     for the dataset accidents . ]    the approximation error of the matrix estimation @xmath227 in  @xcite , @xmath267 , is bounded by @xmath268 with high probability .",
    "the running time is @xmath269 and space usage is @xmath270 .",
    "our deterministic algorithm achieves an error guarantee of @xmath271 for the approximation @xmath272 for any @xmath42 . for",
    "a direct comparison we set @xmath46 , @xmath273 and @xmath274 and obtain an approximation error of @xmath275 which is at most @xmath268 by cauchy - schwarz inequality .",
    "the time and space complexity of our algorithm is a polylogarithmic factor better .",
    "note also that the approximation guarantee does not depend on the dimension @xmath0 as in  @xcite .    for individual entries we achieve an error bounded by @xmath276 } { \\|c\\|_{e^k1}}/{(b - k)}$ ]",
    "while  @xcite shows that the error of the obtained estimates is bounded by @xmath277 for a suitably chosen constant @xmath278 .",
    "assuming zipfian distribution with @xmath69 the approximation error of the frobenius norm of the matrix product in  @xcite is bounded by @xmath279 with high probability . by setting @xmath273",
    "our deterministic algorithm achieves @xmath280 for the frobenius norm approximation error . for an @xmath281-approximation of individual entries",
    "both  @xcite and our algorithm need a data structure , a sketch or a summary , of size @xmath282 but  @xcite needs to run @xmath283 copies of the algorithm in parallel in order to guarantee that the estimates are correct with high probability .",
    "figure  [ fig : pagh_vs_kk ] plots the additive approximation achieved for different summary sizes for the accidents dataset .",
    "the dashed line gives the theoretical guarantees on the approximation error shown in  @xcite , depending on the entrywise 2-norm of the matrix product , and the solid line the approximation we achieve ( depending on the entrywise 1-norm ) . in order to achieve guarantees with high probability pagh",
    "s algorithm needs to run @xmath283 copies of the algorithm in parallel , thus increasing the time and space complexity by a factor of @xmath283 .",
    "however , pagh s algorithm achieves better bounds for lighter skew when @xmath284 and more important it is not restricted to nonnegative input matrices .",
    "in this section we show how to efficiently extend the deterministic streaming algorithm sketched in  @xcite to matrix multiplication .",
    "the algorithm in  @xcite works for streams in the non - strict turnstile model where updates are of the form @xmath52 for an item @xmath25 and @xmath54 .",
    "let us first consider the generalized majority algorithm for the non - strict turnstile model .",
    "we want to find an item whose absolute total weight is more than half of the absolute sum of total weights of all other items in the stream . recall that in the non - strict turnstile model an item is allowed to have negative weight after processing the stream .",
    "we adapt the key observation in  @xcite to obtain    [ majority_bits ] a majority item in a real weighted stream over a domain of size @xmath79 in the non - strict turnstile model can be determined in two passes using @xmath285 counters and processing time per item arrival .",
    "assume the items are integers over some finite domain @xmath286 $ ] .",
    "we form @xmath287 groups @xmath288 , @xmath289 , one for each bit of the binary representation for each item @xmath290 $ ] . for each arriving item",
    "@xmath25 we compute its @xmath48th bit and if it is 1 we add @xmath25 s weight to the counter @xmath291 of the @xmath48th group .",
    "we also maintain the total weight of the stream seen so far in a global variable @xmath124 .",
    "after the stream has been processed we first determine the sign of @xmath124 .",
    "each item either increases the weight of the @xmath48th group or does not change it .",
    "since the set of items is divided in two disjoint subsets for the @xmath48th group depending on the @xmath48th bit of each item , given @xmath124 and @xmath291 we can also determine the total weight of items whose @xmath48th bit is equal to 0 , call it @xmath292 .",
    "first observe that an item with absolute weight more than @xmath293 must have the same sign as @xmath124 .",
    "thus , if the weights in the two groups for the @xmath48th bit have different signs we determine the @xmath48th bit of the potential majority item .",
    "otherwise , if the 0-group and the 1-group have weight with equal sign , the majority element must be in the subset with larger absolute weight .",
    "we show this by contradiction .",
    "set @xmath294 .",
    "assume w.l.o.g .",
    "that both groups have positive weights @xmath295 , @xmath296 , @xmath297 and the @xmath48th bit of the majority item is 1 .",
    "let the weight of the majority item @xmath79 be @xmath298 .",
    "we have @xmath299 where @xmath300 is the absolute value of the total contribution of entries with negative total weight and @xmath48th bit 1 .",
    "the last inequality follows from the fact that @xmath298 is positive and the sum of weights of items with positive total weight is lower bounded by @xmath298 . but",
    "this implies @xmath301 which contradicts that @xmath79 is the majority item .",
    "therefore it must hold @xmath302 .",
    "let us assume @xmath304 for some integer @xmath305 .",
    "we number the @xmath132 entries of the matrix product as @xmath306 such that the entry in the position @xmath125 is assigned a number @xmath307 , @xmath308 . now observe that each entry of the matrix product consists of @xmath309 bits and the term @xmath27 determines only the least significant @xmath185 bits while the term @xmath310 determines the most significant @xmath185 bits . in the sequence @xmath311 the elements having 1 in the @xmath48th position , @xmath312 , are the ones in positions @xmath313 , @xmath314 . thus , given a column vector @xmath39 of @xmath1 and a row vector @xmath62 of @xmath2 the entries in the outer product @xmath315 with the @xmath48th bit equal to 1",
    "are uniquely determined by the position of the contribution from either @xmath39 or @xmath62 . more concretely , for @xmath316 and @xmath317 these are the entries in positions @xmath313 , @xmath318 , in @xmath39 and @xmath62 , respectively .    thus , to find the total contribution of entries weights with a bit set to 1 in a given position we need to simply consider only the entries in the corresponding interval and by the distributive law for the ring of real numbers we can compute the total contribution to the group for the @xmath25th 1-bit in @xmath136 steps .",
    "writing the matrix product as a sum of @xmath0 outer products and applying to the resulting stream the majority algorithm from lemma  [ majority_bits ] with the above outlined modification yields the claimed running time .",
    "matrices @xmath74 , an array of primes @xmath319 , integer @xmath62 create data structures @xmath320 of size @xmath321 and @xmath322 of size @xmath323 for @xmath324 for suitably chosen constants @xmath325 and @xmath326 and @xmath327 @xmath328 , @xmath329 prime @xmath330 $ ] @xmath331 @xmath332 @xmath333 @xmath334 @xmath335[m ] : = q[j][m ] + c_m$ ] for @xmath336 set to 0 the entries @xmath337 for @xmath338 , @xmath314set to 0",
    "the entries @xmath339 for @xmath338 , @xmath314@xmath331 @xmath332 @xmath333 @xmath334 @xmath340[m][k ] = g[j][m][k ] + c_m$ ] for @xmath336    data structures @xmath322 and @xmath320 , array of prime numbers @xmath319 initialize a @xmath321 matrix @xmath341 with @xmath309-bitstrings for @xmath342 prime @xmath330 $ ] set @xmath48th bit of @xmath343[m]$ ] depending on the values @xmath335[m]$ ] and @xmath340[m][k]$ ] set @xmath25 from the first @xmath185 bits in @xmath344 set @xmath27 from the last @xmath185 bits in @xmath344 check the value of @xmath345    theorem 14 in  @xcite presents a generalization of the majority algorithm for non - strict turnstile data streams where at most @xmath48 items have a value significantly different from 0 after processing the stream .",
    "we combine the approach with the technique presented by pagh  @xcite to obtain our main theorem for the multiplication of arbitrary real valued matrices .",
    "the proof of the theorem analyzes the complexity of the algorithm in figure  [ alg2 ] .",
    "let @xmath231 be real @xmath194 matrices and @xmath204 their product .",
    "if the absolute weight of each of the @xmath62 entries with largest absolute weight is bigger than @xmath346 , then there exists a deterministic algorithm computing the @xmath62 heaviest entries exactly in time @xmath347 and space @xmath348 in two passes over the input .",
    "we first describe the approach presented in  @xcite for data streams over the non - strict turnstile model and then adapt it to matrix multiplication using lemma  [ maj_matrix ] and the approach from  @xcite .",
    "let @xmath349 be a set of consecutive prime numbers such that @xmath350 and @xmath351 where @xmath51 is the cardinality of the set of items in the input stream .",
    "assume that at most @xmath62 items will have weights different from 0 .",
    "for each prime number @xmath3 we create @xmath3 groups labelled as @xmath352 .",
    "now , for a given @xmath3 , each incoming item @xmath25 is distributed to exactly one of the @xmath3 groups by computing @xmath353 .",
    "the crucial observation is that for any subset of @xmath62 non - zero items each one of them will be isolated in at least one group .",
    "this is true because two different items can share at most @xmath354 groups for different prime numbers .",
    "otherwise the absolute value of their difference modulo @xmath3 is 0 for @xmath355 distinct primes larger than @xmath62 which contradicts the fact that all items are smaller than @xmath51 .",
    "thus , for a given non - zero item @xmath25 each of the other @xmath99 non - zero items  locks \" at most @xmath354 groups for different primes . since we have @xmath356 distinct primes there must exist a prime @xmath3 such that @xmath357 implies @xmath358 for all @xmath359 . thus running the majority algorithm from lemma  [ majority_bits ] for each group and checking the results in a second pass",
    "will reveal the @xmath62 non - zero items .",
    "clearly , the algorithm is resilient to noise in the zero - valued items .",
    "we now adjust the approach to matrix multiplication with sparse output .",
    "figure  [ alg2 ] presents a pseudocode description of the algorithm .",
    "given @xmath360 real input matrices @xmath1 and @xmath2 and a parameter @xmath62 we assume we have found the @xmath361 consecutive prime numbers larger than @xmath62 , denote them as @xmath362 .",
    "as in the algorithm for nonnegative input matrices we iterate over @xmath0 outer products and consider each of them as @xmath132 distinct item arrivals .",
    "we iterate over primes @xmath363 .",
    "we concentrate how computegroups works for a fixed prime @xmath363 and an outer product @xmath315 for @xmath364 , @xmath94 $ ] .",
    "we want to distribute each of the @xmath29 entries in @xmath315 in @xmath3 groups depending on @xmath365 and @xmath3 and then run the majority algorithm from lemma  [ majority_bits ] in each group .",
    "we write each of the @xmath132 entries as @xmath366 , for @xmath31 $ ] .",
    "since for any two @xmath367 we have @xmath368 we want to compute @xmath369 such that @xmath370 for each @xmath371 $ ] .",
    "the nave solution would explicitly compute the @xmath372 value of each of the @xmath132 entries and update the corresponding group . as observed by pagh",
    "@xcite , however , one can treat the two vectors as polynomials of degree @xmath373 : @xmath374 and @xmath375 , lines 67 . in line 8",
    "we multiply the two polynomials in time @xmath376 using fft .",
    "high order terms to @xmath377 and @xmath378 with coefficients equal to 0 to obtain the product @xmath379 .",
    "] the product is @xmath380 where @xmath381 such that @xmath382 , thus we only have to add up the entries with the same exponents modulo @xmath3 : ( @xmath383 ) . in line  11",
    "we update the global counter from lemma  [ majority_bits ] recording the total contribution of entries weights in the stream and store it in a matrix @xmath320 in a corresponding cell .",
    "next , in lines 1222 , we repeat essentially the same procedure for each of @xmath309 bits of each entry : we nullify the respective entries in either @xmath39 or @xmath62 , those with a 0 in the @xmath48th bit , as outlined in lemma  [ maj_matrix ] .",
    "then we multiply the updated polynomials and add to the counter of the corresponding ( sub)groups the newly computed coefficients , stored in a data structure @xmath322 .    after the first pass over the input matrices we have to construct a majority candidate for each of the @xmath3 subgroups of a prime @xmath363 and check whether it is indeed different from 0 in a second pass over the input matrices . in checkcandidates",
    "we do this by using the previously computed data structures @xmath322 and @xmath320 from which we extract the required information as outlined in lemma  [ maj_matrix ] .      for the complexity analysis",
    "we need to know the cardinality of @xmath362 and the order of the largest prime in @xmath362 . in the following we assume @xmath384 .",
    "we have @xmath385 and from the prime number theorem we observe that for the largest @xmath363 we have @xmath386 .",
    "therefore , for a given outer product we iterate over @xmath387 primes and in each iteration @xmath283 times we multiply using fft two polynomials of degree @xmath388 .",
    "this yields a total running time of @xmath389 . for the data structures @xmath322 and @xmath320 we need a total space of @xmath390 .",
    "checkcandidates needs time of @xmath391 to find the @xmath62 nonzero entries in the product .",
    "let @xmath231 be real @xmath194 matrices and @xmath204 their product .",
    "if @xmath10 has at most @xmath62 nonzero entries then there exists a deterministic algorithm computing @xmath10 exactly in time @xmath347 and space @xmath348 in two passes over the input .",
    "let the absolute values of the entries weights in a matrix product adhere to zipfian distribution .",
    "then for user - defined @xmath392 and @xmath393 there exists a deterministic algorithm detecting the @xmath394 heaviest entries in the product in time @xmath395 and space @xmath396 in @xmath397 passes over the input matrices .",
    "we run computegroups with suitably chosen parameters such that an entry with absolute weight of at least @xmath398 lands in at least one group such that the contribution from other entries is less than @xmath399 .",
    "thus , we have to isolate the @xmath400 heaviest entries in different groups such that @xmath401 . since @xmath402 for some constant @xmath144 , we need @xmath403 . in a second pass",
    "we run checkcandidates and find the @xmath48 heaviest entries .",
    "a natural generalization for detecting the heaviest entries in @xmath404 passes for a user - defined @xmath404 works as follows .",
    "in one pass we will run computegroups and in the subsequent pass checkcandidates .",
    "assume after the @xmath405th pass , @xmath406 , we have found the @xmath407 entries with largest absolute weight and they are stored in a set @xmath111 . then , in the @xmath408th pass we run computegroups but at the end we subtract the weights of all entries in @xmath111 from the counters in the corresponding groups in order to guarantee that the @xmath407 heaviest entries are not considered . after finding the new @xmath48 heaviest entries we add them to @xmath111 .",
    "the algorithm seems to be only of theoretical interest .",
    "note that its complexity is much worse than the one achieved by pagh s randomized one - pass algorithm @xcite : the @xmath62 non - zero entries can be found in time @xmath409 and space @xmath270 with error probability @xmath410 .",
    "nevertheless since the best known space lower bound for finding @xmath62 non - zero elements by a deterministic algorithm is @xmath411 there seems to be potential for improvement .",
    "for example ganguly and majumder  @xcite present improvements of the deterministic algorithm from  @xcite but their techniques are not suitable for our problem .    to the best of our knowledge",
    "this is the first deterministic algorithm for computing matrix products in time @xmath12 for the case when the product contains at most @xmath13 non - zero entries .",
    "the algorithm by iwen and spencer achieves this for an arguably more interesting class of matrix products , namely those with @xmath412 , @xmath413 , nonzero entries in each row , but the algorithm relies on fast rectangular matrix multiplication and its simple version runs in time @xmath414 .",
    "+   + * acknowledgements .",
    "* i would like to thank my supervisor rasmus pagh for his continuous support and many valuable comments and suggestions ."
  ],
  "abstract_text": [
    "<S> recently , pagh presented a randomized approximation algorithm for the multiplication of real - valued matrices building upon work for detecting the most frequent items in data streams . </S>",
    "<S> we continue this line of research and present new _ </S>",
    "<S> deterministic _ matrix multiplication algorithms .    motivated by applications in data mining </S>",
    "<S> , we first consider the case of real - valued , nonnegative @xmath0-by-@xmath0 input matrices @xmath1 and @xmath2 , and show how to obtain a deterministic approximation of the weights of individual entries , as well as the entrywise @xmath3-norm , of the product @xmath4 . </S>",
    "<S> the algorithm is simple , space efficient and runs in one pass over the input matrices . for a user defined @xmath5 the algorithm runs in time @xmath6 and space @xmath7 and returns an approximation of the entries of @xmath4 within an additive factor of @xmath8 , where @xmath9 is the entrywise 1-norm of a matrix @xmath10 and @xmath11 is the time required to sort @xmath0 real numbers in linear space . building upon a result by berinde et al . </S>",
    "<S> we show that for skewed matrix products ( a common situation in many real - life applications ) the algorithm is more efficient and achieves better approximation guarantees than previously known randomized algorithms .    </S>",
    "<S> when the input matrices are not restricted to nonnegative entries , we present a new deterministic group testing algorithm detecting nonzero entries in the matrix product with large absolute value . the algorithm is clearly outperformed by randomized matrix multiplication algorithms , but as a byproduct we obtain the first @xmath12-time deterministic algorithm for matrix products with @xmath13 nonzero entries . </S>"
  ]
}