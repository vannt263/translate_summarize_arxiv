{
  "article_text": [
    "we consider a linear measurement model of the form @xmath0 where @xmath1 is the measured signal , @xmath2 the regression vector , @xmath3 a sequence of _ zero - mean and bounded _ errors ( measurement noise , model mismatch , uncertainties , ) and @xmath4 a sequence of _ intermittent and arbitrarily large _ errors .",
    "assume that we observe the sequences @xmath5 and @xmath6 and would like to compute the parameter vector @xmath7 from these observations .",
    "we are interested in doing so without knowing any of the sequences @xmath4 and @xmath3 .",
    "we do however make the following assumptions :    * @xmath3 is a bounded sequence .",
    "* @xmath4 is a sequence containing zeros and intermittent gross errors with ( possibly ) arbitrarily large magnitudes .",
    "this is an important estimation problem arising in many situations such as fault detection , hybrid system identification , subspace clustering , error correction in communication networks .",
    "the case when @xmath4 is zero and @xmath3 is a gaussian process has been well - studied in linear system identification theory ( see , the text book @xcite ) .",
    "a less studied , but very relevant scenario is when the additional perturbation @xmath8 in is nonzero and contains intermittent and arbitrarily large errors .",
    "it is worth noticing the difference to the problem studied in the field of compressive sensing @xcite . in compressive",
    "sensing , the sought parameter vector is assumed sparse and the measurement noise @xmath3 , often gaussian or bounded . here , no assumptions are made concerning sparsity of @xmath7 .",
    "we will , in this contribution , study essentially the case when the data is noise - free ( @xmath9 for all @xmath10 ) and @xmath8 is a sequence with intermittent gross errors .",
    "we will derive conditions for perfect recovery and provide effective algorithms for computing @xmath7 . in the second part of the paper , the model assumption is relaxed to allow both @xmath11 and @xmath12 to be simultaneously nonzero .",
    "note that this might be a more realistic scenario since most applications have measurement noise . + for illustrative purposes , let us discuss briefly some applications where a model of the form is of interest .",
    "[ [ switched - linear - system - identification ] ] switched linear system identification + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    a discrete - time multi - input single - output ( miso ) switched linear system ( sls ) can be written on the form @xmath13 where @xmath2 is the regressor at time @xmath14 defined by @xmath15^\\top.\\ ] ] where @xmath16 and @xmath1 denote respectively the input and the output of the system .",
    "the integers @xmath17 and @xmath18 in are the maximum output and input lags ( also called the orders of the system ) .",
    "@xmath19 is the discrete mode ( or discrete state ) indexing the active subsystem at time @xmath10 .",
    "@xmath20 is in general assumed _",
    "unobserved_. @xmath21 , @xmath22 , is the parameter vector ( pv ) associated with the mode @xmath20 . for @xmath23 ,",
    "the switched auto - regressive exogenous ( sarx ) model can be written in the form , with unknown @xmath12 of the following structure @xmath24 . for a background on hybrid system identification",
    ", we refer to the references @xcite .",
    "[ [ identification - from - faulty - data ] ] identification from faulty data + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    a model of the form also arises when one has to identify a linear dynamic system which is subject to intermittent sensor faults .",
    "this is the case in general when the data are transmitted over a communication network @xcite .",
    "model is suitable for such situations and the sequence @xmath4 then models occasional data packets losses or potential outliers .",
    "more precisely , a dynamic miso system with process faults can be directly written in the form . in the case of sensor faults ,",
    "the faulty model might be defined by @xmath25 where @xmath1 is the _ observed output _ which is affected by the fault @xmath26 ( assumed nonzero only occasionally ) ; @xmath27 is defined as in from the _ known input _ @xmath28 and the _ unobserved output _",
    "we can rewrite the faulty model exactly in the form with @xmath30\\theta^o.$ ] sparsity of @xmath31 induces sparsity of @xmath4 but in a lesser extent .",
    "[ [ state - estimation - in - the - presence - intermittent - errors ] ] state estimation in the presence intermittent errors + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    considering a miso dynamic system with state dynamics described by @xmath32 and observation equation @xmath33 , @xmath34 being known matrices of appropriate dimensions , and @xmath4 a sparse sequence of possibly very large errors , the finite horizon state estimation problem reduces to the estimation of the initial state @xmath35 .",
    "we get a model of the form by setting @xmath36 and @xmath37 , with @xmath38 $ ] , @xmath39^\\top$ ] . examples of relevant works are those reported in @xcite . in this latter application",
    ", it can however be noted that the dataset @xmath40 may not be generic enough .",
    "data points in general linear position in @xmath41 is more generic than a set of data points contained in one subspace",
    ". we will introduce different quantitative measures of data genericity in the sequel ( see definition [ def : genericity ] and theorem [ prop : sufficient - condition ] ) . ]",
    "[ [ connection - to - robust - statistics ] ] connection to robust statistics + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    indeed , the problem of identifying the parameters from model under the announced assumptions can be viewed as a robust regression problem where the nonzero elements in the sequence @xmath4 are termed outliers . as such",
    ", it has received a lot of attention in the robust statistics literature ( see , e.g. , @xcite for an overview ) .",
    "examples of methods to tackle the robust estimation problem include the least absolute deviation @xcite , the least median of squares @xcite , the least trimmed squares @xcite , the m - estimator @xcite , etc .",
    "most of these estimators come with an analysis in terms of the breakdown point @xcite , a measure of the ( asymptotic ) minimum proportion of points which cause an estimator to be unbounded if they were arbitrarily corrupted by gross errors .",
    "the current report focuses on the analysis of a nonsmooth convex optimization approach which includes the least absolute deviation method as a particular case corresponding to the situation when the output in is a scalar .",
    "the analysis approach taken in the current paper is different in the following sense .    * in robust statistics the quality of an estimator is measured by its breakdown point .",
    "the higher the breakdown point , the better .",
    "the available analysis is therefore directed to determining a sort of absolute robustness : how many outliers ( expressed in proportion of the total number of samples ) cause the estimator to become unbounded . * here , the question of robust performance of the estimator is posed differently .",
    "we are interested in estimating the maximum number of outliers that a nonsmooth - optimization - based estimator can accommodate while still returning the exact value one would obtain in the absence of any outlier .",
    "this is more related to the traditional view developed in compressive sensing .",
    "[ [ contributions - of - this - paper ] ] contributions of this paper + + + + + + + + + + + + + + + + + + + + + + + + + + +    one promising method for estimating model is by nonsmooth convex optimization as suggested in @xcite .",
    "more precisely , inspired by the recent theory of compressed sensing @xcite , the idea is to minimize a nonsmooth ( and non differentiable ) sum - of - norms objective function involving the fitting errors . under noise - free assumption , such a cost function has the nice property that it is able to provide the true parameter vector in the presence of arbitrarily large errors @xmath4 provided that the number of nonzero errors is small in some sense .",
    "of course , when the data are corrupted simultaneously by the noise @xmath3 and the gross errors @xmath4 , the recovery can not be exact any more .",
    "it is however expected ( as simulations tend to suggest ) that the estimate will still be close to the true one .",
    "+ the current paper intends to present a new analysis of the nonsmooth optimization approach and provide some elements for further understanding its behavior . for this purpose ,",
    "we start in section [ subsec : sparse - opt ] by viewing the nonsmooth optimization as the convex relaxation of a ( ideal ) combinatorial @xmath42-``norm '' formulation .",
    "we then derive in section [ subsec : l1-characterization ] necessary and sufficient conditions for optimality .",
    "based on those conditions we later ( section [ subsec : sufficient - conditions ] ) establish new sufficient conditions for exact recovery of the true parameter vector in . the noisy case is treated in section [ subsec : noise ] .",
    "section [ sec : multivariable ] presents a generalization of the earlier discussions to multi - outputs systems .",
    "finally , numerical experiments are described in section [ sec : simulation ] and concluding remarks are given in section [ sec : conclusion ] .",
    "let @xmath43 be the index set of the measurements .",
    "for any @xmath44 , define a partition of the set of indices @xmath45 by @xmath46 , @xmath47 , @xmath48 .",
    "_ cardinality of a finite set .",
    "_ throughout the paper , whenever @xmath49 is a finite set , the notation @xmath50 will refer to the cardinality of @xmath49 .",
    "however , for a real number @xmath51 , @xmath52 will denote the absolute value of @xmath51 .",
    "+ _ submatrices and subvectors . _",
    "let @xmath53\\in { \\mathbb{r}}^{n\\times n}$ ] be the matrix formed with the available regressors @xmath5 .",
    "if @xmath54 , the notation @xmath55 denotes a matrix in @xmath56 formed with the columns of @xmath57 indexed by @xmath58 .",
    "likewise , with @xmath59^\\top\\in { \\mathbb{r}}^n$ ] , @xmath60 consists of the vector in @xmath61 formed with the entries if @xmath62 indexed by @xmath58 .",
    "we will use the convention that @xmath63 ( resp .",
    "@xmath64 ) when the index set @xmath58 is empty",
    ". + _ vector norms .",
    "_ @xmath65 , @xmath66 , denote the usual @xmath67-norms for vectors defined for any vector @xmath68^\\top\\in { \\mathbb{r}}^n$ ] , by @xmath69 .",
    "note that @xmath70 .",
    "the @xmath42 `` norm '' of @xmath71 is defined to be the number of nonzero entries in @xmath71 , i.e. , @xmath72 .",
    "+ _ matrix norms .",
    "_ the following matrix norms will be used : @xmath73 , @xmath74 , @xmath75 , @xmath76 , @xmath77 .",
    "they are defined as follows : for a matrix @xmath78^\\top\\in { \\mathbb{r}}^{n\\times n}$ ] , @xmath79 with @xmath80 denoting the entries of @xmath81 .",
    "the main idea for identifying the parameter vector @xmath7 is by solving a sparse optimization problem , that is , a problem which involves the minimization of the number of nonzeros entries in the error vector . to be more specific ,",
    "assume for the time being that the error sequence @xmath3 is identically equal to zero .",
    "consider a candidate parameter vector @xmath82 and let @xmath83 where @xmath84^\\top$ ] , @xmath85 $ ] , be the fitting error vector induced by @xmath86 on the experimental data .",
    "then the identification of @xmath7 can be written in the form of an @xmath42 objective minimization problem as @xmath87 where @xmath88 denotes the @xmath42 ( pseudo ) norm which counts the number of nonzero entries . because problem aims at making the error",
    "@xmath89 sparse by minimizing the number of nonzero elements ( or maximizing the number of zeros ) , it is sometimes called a sparse optimization problem @xcite . as can be intuitively guessed ,",
    "the recoverability of the true parameter vector @xmath7 from depends naturally on some properties of the available data .",
    "this is outlined by the following lemma .",
    "[ lem : l0-solution - set ] assume that @xmath3 is equal to zero and let @xmath90^\\top.$ ] assume that for any @xmath54 with @xmath91 , @xmath92 whenever @xmath93 , with @xmath94 referring here to range space .",
    "then provided @xmath95 , it holds that @xmath96    we proceed by contradiction .",
    "assume that does not hold , @xmath97 .",
    "then , by letting @xmath98 be any vector in @xmath99 , the above inequality translates into @xmath100 .",
    "it follows that @xmath101 because @xmath102 is the exact ( largest ) number of zero elements in the sequence @xmath103 .",
    "note also that we have necessarily @xmath104 . on the other hand , with @xmath105",
    ", it can be seen that @xmath106 .",
    "this , together with @xmath101 , constitutes a contradiction to the assumption of the lemma .",
    "hence , holds as claimed .",
    "lemma [ lem : l0-solution - set ] specifies a condition involving both @xmath57 and @xmath107 and under which @xmath7 lies in the solution set but it does not ensure that @xmath7 will be recovered uniquely from data . before proceeding further",
    ", we recall from @xcite a sufficient condition under which @xmath7 is the unique solution to .",
    "[ def : genericity ] let @xmath108 be a data matrix satisfying @xmath109 .",
    "the @xmath110-genericity index of @xmath57 denoted @xmath111 , is defined as the minimum integer @xmath112 such that any @xmath113 submatrix of @xmath57 has rank @xmath110 , @xmath114    [ thm : uniqueness_l0 ] assume that the sequence @xmath3 in is identically equal to zero .",
    "if the sequence @xmath4 in contains enough zero values in the sense that @xmath115 then @xmath7 is the unique solution to the @xmath42-norm minimization problem .",
    "a proof of this result can be found in @xcite .    in other words ,",
    "if the number of nonzero gross errors @xmath4 affecting the data generated by does not exceed the threshold @xmath116 , then @xmath7 can be exactly recovered by solving .",
    "unfortunately , this problem is a hard combinatorial optimization problem .",
    "a more tractable solution can be obtained by relaxing the @xmath42-norm into its best convex approximant , the @xmath117-norm .",
    "doing this substitution in gives @xmath118 the latter problem is termed a nonsmooth convex optimization problem ( * ? ? ?",
    "3 ) because the objective function is convex but non - differentiable . compared to , problem has the advantage of being convex and can hence be efficiently solved by many existing numerical solvers , @xcite .",
    "note further that it can be written as a linear programming problem .",
    "the @xmath117 relaxation process has been intensively used in the compressed sensing literature @xcite for approaching the sparsest solution of an underdetermined set of linear equations . as will be shown next ,",
    "the underlying reason why problem can obtain the true parameter vector despite the presence of gross perturbations @xmath4 is related to its nonsmoothness .",
    "a number of sufficient conditions for the equivalence of problems similar to and have been derived in the literature of compressed sensing using the concepts of mutual coherence @xcite and the restricted isometry property @xcite . here , we shall propose a parallel but different analysis for the robust estimation problem .",
    "we start by characterizing the solution to the @xmath117-norm problem .",
    "[ thm : equivalence ] a vector @xmath119 solves the @xmath117-norm problem iff any of the following equivalent statements hold :    1 .",
    "there exist some numbers @xmath120 $ ] , @xmath121 , such that @xmath122 2 .   for any @xmath123 , @xmath124 3 .",
    "the optimal value of the optimization problem @xmath125 where @xmath126 , @xmath127 , is less than or equal to @xmath128 .    moreover , the solution @xmath129 is unique iff    1",
    ".   holds and @xmath130 where @xmath131 2",
    ".   holds with strict inequality symbol for all @xmath132 , @xmath133 .",
    "[ [ proof - of - s1 ] ] proof of s1 + + + + + + + + + + +    since @xmath134 is a proper convex function , it has a non empty subdifferential @xcite .",
    "the necessary and sufficient condition for @xmath129 to be a solution of is then @xmath135 where the notation @xmath136 refers to subdifferential with respect to @xmath86 .",
    "indeed , using additivity of subdifferentials , it is straightforward to write @xmath137 where @xmath138 refers to the convex hull . here",
    ", the addition symbol is meant in the minkowski sum sense .",
    "it follows that @xmath139 is equivalent to the existence of a set of numbers @xmath140 in @xmath141 $ ] , @xmath121 , such that holds .",
    "[ [ proof - of - s2 ] ] proof of s2 + + + + + + + + + + +    define two functions @xmath142 by @xmath143 and @xmath144",
    ". then @xmath145 and @xmath146 is differentiable at @xmath129 .",
    "it follows that @xmath147 , where @xmath148 is the gradient of @xmath146 at @xmath129 . we can hence write @xmath149 note from that @xmath150 so that @xmath151 iff @xmath152 and this in turn is equivalent to @xmath153 @xmath154 , for @xmath155 .",
    "it follows that @xmath129 minimizes @xmath134 iff @xmath156 for all @xmath86 .",
    "the last equality is obtained by using the fact that @xmath157 for all @xmath10 in @xmath158 .",
    "finally the result follows by setting @xmath159 and noting that @xmath160 .",
    "[ [ s1-leftrightarrow - s3 ] ] s1 @xmath161 s3 + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the proof of the last equivalence is immediate .",
    "[ [ uniqueness ] ] uniqueness + + + + + + + + + +    for convenience , we first prove s2. along the lines of the proof of s2 ( see eq .",
    "and preceding arguments ) , we can see that strict inequality in is equivalent to the following strict inequality @xmath162 . on the other hand , @xmath163 .",
    "summing the two yields @xmath164 hence s2 is proved .",
    "+ for the proof of s1 , we proceed in two steps .",
    "+ _ sufficiency .",
    "_ assume @xmath130 .",
    "then for any nonzero vector @xmath132 there is at least one @xmath165 such that @xmath166 . recall that @xmath167 by definition of @xmath168 .",
    "it follows that by multiplying on the left by @xmath169 with @xmath132 an arbitrary nonzero vector , and taking the absolute value yields with strict inequality symbol .",
    "we can therefore apply the proof of s2 to conclude that @xmath129 is unique .",
    "+ _ necessity .",
    "_ assume @xmath170 . then pick any nonzero vector @xmath171 in @xmath172 .",
    "set @xmath173 with @xmath174 .",
    "indeed @xmath175 can be chosen sufficiently small such that @xmath176 has the same sign as @xmath177 for @xmath178 .",
    "this implies that @xmath179 and @xmath180 .",
    "moreover , since @xmath181 , @xmath182 @xmath183 , so that @xmath184 . finally , it remains to re - assign the indices @xmath10 contained in @xmath185 for which @xmath186 .",
    "we get the following partition @xmath187 , @xmath188 , @xmath189 .",
    "it follows that @xmath190 also satisfies with the sequence @xmath191 and is therefore a minimizer . in conclusion , if @xmath170 , the minimizer can not be unique .",
    "a number of important comments follow from theorem [ thm : equivalence ] .",
    "* one first consequence of the theorem is that @xmath7 can be computed exactly from a finite set of erroneous data ( by solving problem ) provided it satisfies the conditions s1 or s2 of the theorem .",
    "note that there is no explicit boundedness condition imposed on the error sequence @xmath4 .",
    "hence the nonzero errors in this sequence can have arbitrarily large magnitudes as long as the optimization problem makes sense , provided @xmath192 remains finite . *",
    "second , @xmath7 can be exactly recovered in the presence of , say , infinitely many nonzero errors @xmath12 ( see also proposition [ prop : infinite - errors ] ) .",
    "for example , if the regressors @xmath40 satisfy @xmath193 and @xmath194 , then @xmath7 is the unique solution to problem regardless of the number of errors affecting the data . *",
    "third , if problem admits a solution @xmath129 that satisfies @xmath195 for all @xmath196 , then @xmath129 is non - unique . in effect , @xmath197 in this case and so , @xmath198 which , by theorem [ thm : equivalence ] , implies non - uniqueness .",
    "indeed this is typically the case whenever the noise @xmath3 is nonzero .    another immediate consequence of theorem [ thm : equivalence ]",
    "can be stated as follows .",
    "if the model is affine in the sense that the regressor @xmath199 has the form @xmath200^\\top$ ] , with @xmath201 , then a necessary condition for @xmath129 to solve problem is that @xmath202 here , the outer bars @xmath203 refer to absolute value while the inner ones which apply to sets refer to cardinality .",
    "the proof is immediate by considering the condition and taking @xmath204^\\top\\in { \\mathbb{r}}^n$ ] .",
    "implies that if the measurement model is affine and all the @xmath12 s have the same sign , if one of the cardinalities @xmath205 or @xmath206 is equal to zero , then problem can not find the true @xmath7 whenever more than @xmath207 of the elements of the sequence @xmath4 are nonzero .",
    "+ next , we discuss a special case in which the true parameter vector @xmath7 in can , in principle , be obtained asymptotically in the presence of an infinite number of nonzero errors @xmath12 s .",
    "[ prop : infinite - errors ] assume that the error sequence @xmath3 in is identically equal to zero .",
    "assume further that the data are generated such that :    * there is a set @xmath208 with @xmath209 , such that for any @xmath210 , @xmath211 and @xmath212 , * for any @xmath213 , @xmath12 is sampled from a distribution which is symmetric around zero . *",
    "the regression vector sequence @xmath214 is drawn from a probability distribution having a finite first moment .",
    "then @xmath215 with probability one",
    ".    under the conditions of the proposition , we have @xmath216 , where @xmath217 denotes probability measure .",
    "it follows that @xmath205 and @xmath206 go jointly to infinity as the total number of samples @xmath218 tends to infinity .",
    "hence , the expressions @xmath219 and @xmath220 are both sample estimates for the expectation of the process @xmath40 . by the law of large numbers , as @xmath221 , the two quantities converge to the true expectation of the process @xmath40 with probability one , so that @xmath222=0.\\ ] ] as a consequence , @xmath7 satisfies condition s1 of theorem [ thm : equivalence ] asymptotically with @xmath223 for any @xmath224 .",
    "hence the solution of the @xmath117 minimization problem tends to @xmath7 with probability one as the number of samples approaches infinity .      in this section",
    "we derive sufficient conditions under which @xmath7 in model belongs to the solution set of problem .",
    "the conditions say essentially that if the number of zero entries in vector @xmath107 is larger than certain thresholds depending on the data matrix @xmath57 , then @xmath7 solves . to some extent",
    ", the thresholds can be interpreted as quantitative measures of the richness ( genericity ) of @xmath57 .",
    "our derivations rely on theorem [ thm : equivalence ] .",
    "we start by introducing the following notations : @xmath225 where the maximum is taken over the set of those partitions @xmath226 of @xmath45 that satisfy @xmath227 . in addition , let @xmath228 and @xmath229 assuming that @xmath109 , it can be seen that the numbers @xmath230 , @xmath231 , are well - defined .",
    "first , note that @xmath232 so that the condition @xmath233 is achievable .",
    "moreover , by considering the trivial partition @xmath226 with @xmath234 and @xmath235 , we see that a possible ( the largest indeed ) value for @xmath230 is @xmath218 .    [",
    "thm : x(xx ) ] suppose that the sequence @xmath3 in model is zero and @xmath109 .",
    "then any vector @xmath82 obeying @xmath236 solves the @xmath117 minimization problem .",
    "+ if in addition , all inequalities involved in the definition of @xmath237 and @xmath238 are strict , then @xmath86 is the unique solution to .    [",
    "[ part-1 ] ] part 1 + + + + + +    we will show that if @xmath239 for some @xmath240 , then any @xmath82 such that @xmath241 solves problem .",
    "assume @xmath239 and break the proof into two cases .",
    "+ _ first case _ : @xmath242 .",
    "+ let @xmath243 and @xmath244 .",
    "then @xmath86 solves problem if it satisfies condition s3 of theorem [ thm : equivalence ] . in turn",
    ", a sufficient condition for s3 is @xmath245 for any @xmath246 .",
    "let @xmath247 be the optimal value of problem for a fixed but arbitrary @xmath248 and pose @xmath249 since @xmath250 is a feasible point for problem , we see that @xmath251 is a sufficient condition for @xmath252 .",
    "the so - defined @xmath250 is the well - known least euclidean - norm solution to an underdetermined system of linear equations @xcite ; @xmath250 can be analytically expressed as @xmath253 . as a consequence ,",
    "we get a more restrictive sufficient condition as @xmath254 which can be rewritten more simply as @xmath255 since @xmath256 .",
    "it follows that if @xmath239 , then any @xmath86 satisfying @xmath242 solves problem .",
    "_ second case : _",
    "+ set @xmath258 and @xmath259 . without loss of generality",
    ", @xmath260 can be decomposed in the form @xmath261 where @xmath58 is a subset of @xmath45 such that @xmath227 . then by posing @xmath262",
    ", we have @xmath263 . by exploiting the definition of @xmath264 in together with the assumption @xmath239 and the fact that @xmath265",
    ", we see that @xmath255 .",
    "this , as argued in the _ first case _ studied above , is a sufficient condition for @xmath226 to satisfy .",
    "hence by setting @xmath266 for an arbitrary @xmath267 , it holds that @xmath268 and @xmath269 .",
    "let us partition @xmath248 in the form @xmath270^\\top$ ] where @xmath271 and @xmath272 are vectors of lengths @xmath273 and @xmath274 respectively .",
    "then @xmath275h_2=x_i\\alpha_i^o\\\\      & \\leftrightarrow x_{j^c}h_1 = x_j\\begin{bmatrix}\\alpha_i^o\\\\-h_2\\end{bmatrix } \\end{aligned}\\ ] ] where it can be observed that @xmath276 . since @xmath267 is arbitrary , it can be concluded that @xmath277 for any @xmath278 . as already pointed out ,",
    "this is a sufficient condition for @xmath86 to be a solution of problem . in conclusion",
    ", @xmath86 solves whenever @xmath279 as claimed .",
    "[ [ part-2 ] ] part 2 + + + + + +    assume that @xmath280 .",
    "let us start by observing that @xmath281 defined in is a decreasing function of @xmath282 .",
    "then , by setting @xmath283 and exploiting the definition of @xmath238 , it holds that @xmath284 .",
    "pose @xmath243 and @xmath285 . by following a similar reasoning as in part 1 ,",
    "we just need to show that @xmath286 is a sufficient condition for @xmath86 to solve . for this purpose ,",
    "note from the condition s2 of theorem [ thm : equivalence ] that for @xmath86 to be a minimizer of , it suffices that @xmath287 for any @xmath132 . by adding the term @xmath288 on both sides of the inequality symbol",
    ", it follows that @xmath289 it is therefore enough that @xmath290 where the maximum is taken with respect to @xmath132 . by letting @xmath291 and hence replacing @xmath171 with @xmath292 ,",
    "the previous condition becomes @xmath293 or @xmath294 this concludes the proof of the first statement .",
    "[ [ uniqueness-1 ] ] uniqueness + + + + + + + + + +    the uniqueness follows in a similar way as in part 1 and part 2 above by invoking conditions s1 and s2 respectively .    * in effect , eq",
    ". implies that @xmath295 so that @xmath296 . on the other hand ,",
    "if @xmath237 is defined in with strict inequality , then by following the logic of part 1 ( first case ) , it can be seen that holds with strict inequality as well for @xmath243 and @xmath297 .",
    "hence condition s1 is fulfilled , so that theorem [ thm : equivalence ] provides uniqueness of @xmath86 . *",
    "if @xmath238 is defined with strict inequality then eq",
    ". implies that holds with strict inequality for @xmath243 and @xmath297 .",
    "this implies , which in turn is a sufficient condition for s2. uniqueness of @xmath86 follows from theorem [ thm : equivalence ] .",
    "the conditions proposed in theorem [ thm : x(xx ) ] are only sufficient and hence necessarily conservative . to get a sense of how conservative those conditions are",
    ", it would be useful to be able to compute the thresholds @xmath237 and @xmath238 .",
    "however these numbers are very hard to evaluate numerically because of the underlying combinatorial optimization involved in their computation .",
    "next we investigate another sufficient condition for the solution of to coincide with @xmath7 .",
    "this last condition is more restrictive than those of theorem [ thm : x(xx ) ] in the sense that the corresponding threshold is an upper bound on @xmath237 and @xmath238 .",
    "but the new threshold has the important advantage of being easily computable .",
    "[ prop : sufficient - condition ] assume that the sequence of errors @xmath3 is zero in ( [ eq : sarx ] ) and that @xmath109 . define the number @xmath298 if @xmath299 with @xmath300 denoting the cardinality of @xmath301 , then @xmath7 is the _",
    "unique _ minimizer of .",
    "the proof follows by similar arguments as in part 3 of the proof of theorem [ thm : x(xx ) ] with the slight difference that the uniqueness condition s2 ( see theorem [ thm : equivalence ] ) is invoked . for @xmath7 to be the unique minimizer of , it is sufficient that holds with strict inequality symbol and @xmath302 .",
    "now we use the fact that the @xmath128-norm of a matrix is the maximum of the @xmath128-norms of its columns : @xmath303 therefore a sufficient condition for @xmath7 to be the unique solution of is that @xmath304 .",
    "the conclusion follows immediately .",
    "theorem [ prop : sufficient - condition ] says that under noise - free assumption , if the number of zero entries in @xmath107 is larger than a certain threshold ( depending on the degree of genericity of the data ) , then the true parameter vector @xmath7 can be computed efficiently by solving the convex problem .",
    "it follows from ( [ eq : sufficient ] ) that it is desirable to have @xmath305 be as small as possible .",
    "this is indeed a property of richness on the measured data matrix @xmath57 .",
    "the richer the regressors @xmath40 , the smaller @xmath305 .",
    "[ thm : l0-l1-equivalence ] if a parameter vector @xmath306 satisfies condition , then it is the unique solution to both the @xmath42 problem and the @xmath117 problem .",
    "if @xmath7 satisfies condition , then by applying eq . proved below in lemma [ lem : r(x ) ] , it is easy to see that @xmath307 .",
    "this last condition holds iff @xmath7 satisfies .",
    "hence @xmath7 satisfies both the conditions of theorem [ thm : uniqueness_l0 ] and theorem [ prop : sufficient - condition ] .",
    "the result to be proved follows .",
    "we conclude this section with a few technical remarks concerning some interesting properties of @xmath305 .",
    "[ lem : r(x ) ] under the assumption that @xmath109 , @xmath305 satisfies : @xmath308 @xmath309    _ proof of : _ we know from the proof of theorem [ prop : sufficient - condition ] ( see also part 2 in the proof of theorem [ thm : x(xx ) ] ) that @xmath310 for any @xmath54 .",
    "a special case is when the subset @xmath58 is a singleton of the form @xmath311 . for any @xmath132 ,",
    "let @xmath312 . when @xmath133 , consider an index @xmath313 such that @xmath314 .",
    "then by applying the above inequality with @xmath315 , we get @xmath316 with @xmath317 standing for the cardinality of @xmath318 .",
    "when @xmath133 , the smallest value @xmath317 can take is @xmath319 where @xmath111 is the number defined by eq . .",
    "it can therefore be concluded that @xmath320 which is equivalent to . _",
    "proof of : _ let @xmath226 be a partition of @xmath45 and set @xmath321 .",
    "first note that @xmath322 on the other hand , we know ( from the proof of theorem [ prop : sufficient - condition ] ) that @xmath323 it follows that @xmath324 @xmath325 @xmath326 and hence @xmath284 . by invoking the definition of the number @xmath238 in",
    ", it can be concluded that @xmath327 .    for any nonsingular matrix @xmath328 , @xmath329 , @xmath330 and @xmath331 .",
    "it follows that the numbers @xmath305 and @xmath230 , @xmath231 depend only on the subspace spanned by the rows of the regressor matrix @xmath57 .",
    "the parameter vector @xmath7 from the model can be uniquely recovered by solving the convex problem if @xmath7 satisfies , for example , condition of theorem [ prop : sufficient - condition ] . in case",
    "this condition is not naturally satisfied , an interesting question is how we can process the data in order to promote it . in this section",
    "we discuss an algorithmic strategy for enhancing the recoverability of @xmath7 by @xmath117 minimization .",
    "our discussion is inspired by @xcite .",
    "the idea is to solve a sequence of problems of the type ( [ eq : l1-cost ] ) with different weights computed iteratively @xcite .",
    "the iterative scheme can be defined for a fixed number @xmath332 of iterations as follows . at iteration @xmath333 ,",
    "compute @xmath334 with weights defined , for all @xmath10 , by @xmath335 , and @xmath336 where @xmath337 with @xmath338 a small number whose role is to prevent division by zero and @xmath339 is the iteration number .",
    "note that there are many other reweighting strategies which can be used in , see @xcite .",
    "since we are dealing here with a sequence of convex optimization problems , they can be numerically implemented using any convex solver .",
    "in particular the ` cvx ` software package @xcite solves efficiently this category of problems in a matlab environment .      the formulations and are convenient when the noise @xmath3 is equal to zero .",
    "nevertheless , they are expected to work in the presence of a moderate amount of noise . to take explicitly the noise @xmath3 into account",
    ", we propose to compute estimates @xmath340 and @xmath341 ( of @xmath342 and @xmath107 respectively ) by minimizing a cost of the form @xmath343 under an equality constraint of the form .",
    "in other words , we consider the problem @xmath344\\ ] ] and its convex relaxation , @xmath345.\\ ] ] where @xmath346 is a regularization parameter .",
    "[ lem1:opt - conditions - regularized ] a pair @xmath347 solves iff it satisfies @xmath348 where @xmath349 is a vector in @xmath350 whose entries @xmath351 , @xmath196 , are defined by : @xmath352 if @xmath353 and @xmath354 $ ] if @xmath355 .",
    "let @xmath356 be the objective function of the problem .",
    "then @xmath357 is a proper convex function which is differentiable with respect to variable @xmath86 on @xmath41 and admits a subdifferential at any variable @xmath341 .",
    "@xmath358 minimizes @xmath359 iff @xmath360 and @xmath361 .",
    "these conditions translate immediately into @xmath362 and @xmath363 , where @xmath364 is a subgradient of @xmath365 at @xmath366 .",
    "it is interesting to note that - imply @xmath367 , which is very similar to .",
    "the following lemma characterizes the uniqueness of the solution of .",
    "[ lem2:opt - conditions - regularized ] a pair @xmath358 is the unique solution to problem iff both of the following statements are true    * @xmath358 satisfies conditions - for some @xmath364 * @xmath109 and @xmath368 . + here ,",
    "@xmath369 , with @xmath370 being the identity matrix of order @xmath218 , @xmath371 is a matrix formed with the columns of @xmath372 indexed by @xmath373 defined by @xmath374 , with @xmath375 .",
    "the expression of @xmath358 is then given by : @xmath376 @xmath377    @xmath359 is a quadratic function of @xmath86 . for a fixed @xmath366 ,",
    "the minimizer @xmath129 of @xmath378 is unique iff @xmath57 has full row rank , i.e. , @xmath109 .",
    "the unique value of @xmath129 is expressed in function of @xmath366 by .",
    "plugging the expression of @xmath129 in the objective gives @xmath379 the rest of the proof then boils down to showing that the minimizer @xmath366 of @xmath380 is unique iff @xmath368 . to begin with ,",
    "let us point out the following ( see also @xcite ) .",
    "if @xmath366 and @xmath381 are two minimizers of @xmath380 , then we have necessarily @xmath382 the relation follows from the strict convexity of @xmath380 as a function of @xmath383 . in effect , by changing the optimization variable into @xmath384 , @xmath380 becomes @xmath385 , with @xmath386 a vector in @xmath387 and @xmath388 referring to generalized inverse .",
    "this last function is strictly convex with respect to @xmath389 . as a consequence ,",
    "its minimizer is unique and equal to @xmath390 . to see why the relation holds , plug the expression of @xmath129 into .",
    "we get @xmath391 . combining this with ( i.e.",
    ", the uniqueness of @xmath392 ) yields immediately .",
    "let us examine first the case where @xmath393 .",
    "this is indeed equivalent to @xmath394 and so , @xmath395 . would there exist another minimizer @xmath381 of @xmath380",
    ", it should obey , which implies that @xmath381 is necessarily equal to zero .",
    "now consider the case @xmath396 .",
    "+ _ sufficiency .",
    "_ assume that @xmath368 .",
    "as argued above , any two minimizers @xmath366 and @xmath381 of @xmath380 obey - .",
    "from we get that @xmath397 , which implies that @xmath398 . as a consequence , we can write in the following reduced form @xmath399 with @xmath368 , this implies that @xmath400 and that the minimizer of @xmath380 is unique",
    ".    _ necessity .",
    "_ assume that @xmath401 .",
    "consider a nonzero vector @xmath402 such that @xmath403 and @xmath404 .",
    "let @xmath173 , with @xmath174 .",
    "it is straightforward to verify that @xmath405 .",
    "note that @xmath175 can be chosen sufficiently small such that @xmath406 and @xmath407 have the same sign whenever @xmath353 . following a similar path as in the proof of theorem [ thm : equivalence ]",
    ", we can establish that @xmath408 .",
    "finally , with @xmath405 , @xmath408 and the fact that @xmath366 is an optimal solution ( hence satisfying ) , it is easy to check that @xmath409 also satisfies . by lemma",
    "[ lem1:opt - conditions - regularized ] , @xmath409 ( @xmath410 ) solves .",
    "hence , the solution is not unique .    _",
    "derivation of eqs - .",
    "_ these relations result from simple rearrangements of - .    from lemma",
    "[ lem2:opt - conditions - regularized ] , it appears that the true vector @xmath107 can be found by problem iff there is a vector @xmath411 such that @xmath412 satisfies the conditions ( i)-(ii ) of lemma [ lem2:opt - conditions - regularized ] . in particular",
    ", @xmath412 must satisfy .",
    "a necessary condition for this is that @xmath413 . and",
    "this implies that the regularization parameter must verify @xmath414 when @xmath415 , and @xmath416 when @xmath417 .",
    "note further that if @xmath418 and @xmath417 , then @xmath419 must be equal to zero ! however",
    ", if @xmath419 is set to zero in , then the solution set is @xmath420 since this set contains infinitely many elements , we conclude that it is unlikely to get exactly the true @xmath107 by solving irrespective of the value of the regularization parameter @xmath419 . + in any case , the estimation error can be bounded as follows .",
    "assume that the conditions of lemma [ lem2:opt - conditions - regularized ] are satisfied and denote with @xmath358 the solution to problem .",
    "then @xmath421 where @xmath422 , @xmath423 , @xmath424 @xmath425 with @xmath426 , @xmath427 . in @xmath428",
    "is the identity matrix of order @xmath110 ; the set @xmath429 denotes the maximizing argument of and @xmath430 .",
    "the idea of the proof consists in deriving first an expression of @xmath431 and then working out a bound on its norm . from and the data model , we have @xmath432 this , by noting that @xmath433 , can be written as @xmath434 using formula and manipulating a little , we arrive at @xmath435\\left(\\bm{e}_{\\mathcal{s}}+\\bm{f}_{\\mathcal{s}}\\right ) \\\\      &",
    "\\quad + \\lambda x_{\\mathcal{s}^c}(\\psi_{\\mathcal{s}^c}^\\top \\psi_{\\mathcal{s}^c})^{-1}\\psi_{\\mathcal{s}^c}^\\top s(\\varphi^\\star ) . \\end{aligned}\\ ] ] further calculations using woodbury s matrix identity and exploting the relation @xmath367 , yield @xmath436 with @xmath437 .",
    "the result follows by multiplying with @xmath438 , remarking that @xmath439 and taking the euclidean norm .",
    "it is interesting to notice that the numbers @xmath440 , @xmath441 and @xmath442 depend solely on the data matrix @xmath57 .",
    "moreover , when the sequence @xmath4 contains only a few nonzero elements ( but otherwise arbitrarily large ) , the last term in is likely to vanish . as a consequence ,",
    "even though the bound @xmath443 can be large in principle , the bound on the estimation error can be kept at a reasonable level .",
    "we consider now the multivariable analogue of model written in the form @xmath444 where @xmath445 is the output vector at time @xmath10 , @xmath446 is the sequence of errors , @xmath447 is the noise sequence , @xmath448 is the parameter matrix .",
    "the question of interest is how to recover the matrix @xmath449 from measurements corrupted by a vector sequence of sparse errors @xmath4 .",
    "the sparse optimization approach is still applicable to this case , that is , we can formulate the estimation problem as @xmath450 with @xmath203 standing for cardinality .",
    "it can be easily verified that theorem [ thm : uniqueness_l0 ] applies to as well .",
    "the convex relaxation takes the form of a nonsmooth optimization with a cost functional consisting of a sum - of - norms of errors @xcite , @xmath451 with @xmath452 referring to the euclidean norm .",
    "[ thm : equivalence2 ] a matrix @xmath453 solves the sum - of - norms problem , if and only if any of the following equivalent statements holds :    1 .   there exists a sequence of vectors @xmath454 such that @xmath455 where @xmath456 . here , @xmath457 is the euclidean unit ball of @xmath458 .",
    "2 .   for any matrix @xmath459 , @xmath460 3 .",
    "the optimal value of the problem @xmath461 @xmath462 and @xmath463 being a matrix formed with the unit 2-norm vectors @xmath464 , for @xmath465 , + _ is smaller than @xmath128_.    moreover , the solution @xmath466 is unique iff    1 .   holds and @xmath467 where @xmath468 .",
    "2 .   holds with strict inequality symbol for all @xmath469 , @xmath470 .",
    "the proof is similar to that of theorem [ thm : equivalence ] .",
    "it is therefore omitted here .",
    "it is interesting to note that based on theorem [ thm : equivalence2 ] , the analysis carried out in the previous sections can be easily generalized to the multivariable case . in particular ,",
    "proposition [ prop : infinite - errors ] and theorems [ thm : x(xx)]-[thm : l0-l1-equivalence ] can be restated for the multivariable model with only some slight modifications .",
    "for illustration purpose , we just state below the multivariable counterpart of theorem [ thm : x(xx ) ] .",
    "[ thm : sufficient - multivariable ] assume that the noise sequence @xmath3 in model is identically equal to zero and that @xmath109 .",
    "* let @xmath471 be the minimum integer @xmath472 such that the optimal value of the problem @xmath473 with @xmath474\\in { \\mathbb{r}}^{m\\times q } , \\ : b_i\\in \\mathcal{b}_2(0,1)\\big\\}$ ] , + is less than or equal to one .",
    "* let @xmath475 be the minimum integer @xmath472 such that @xmath476 where @xmath477 .    here",
    "the leftmost maximum is taken over all partitions @xmath226 of @xmath45 .",
    "+ if @xmath478 then @xmath449 solves .",
    "+ moreover , if the inequalities involved in the definitions of @xmath471 and @xmath475 are strict and @xmath479 , then @xmath449 is the unique solution to problem .",
    "the proof proceeds from the conditions and and follows a similar reasoning as in the proof of theorem [ thm : x(xx ) ] .    in the special case",
    "where @xmath480 , the matrix @xmath449 in reduces to a vector @xmath481 .",
    "assuming @xmath482 for all @xmath10 , the problem then becomes @xmath483 this is the so - called geometric median problem .",
    "+ by applying , we can see that @xmath484 solves if @xmath485 .",
    "in our first experiment we consider static linear and affine models of the form with @xmath486 and @xmath487 .",
    "the affine model refers to the case where the regressor @xmath199 has the form @xmath488^\\top$ ] .",
    "the goal is to estimate the probability of exact recovery of the true parameter vector by problem in function of the number of nonzero elements in the sequence @xmath4 . for this purpose ,",
    "the noise @xmath3 is set to zero .",
    "the nonzero elements of @xmath4 are drawn from a gaussian distribution with mean @xmath489 and variance @xmath490 .",
    "for each level of sparsity ( proportion on nonzeros ) , a monte carlo simulation of size @xmath489 is carried out with randomly generated static / affine models and @xmath491 data samples at each run . repeating this for four situations ( linear / affine and linear / affine with positive @xmath12 s )",
    ", we obtain the results depicted in figure [ fig : proba ] .",
    "we observe that in the linear case , problem provides the true parameter vector when the output is affected by up to @xmath492 of nonzero gross errors .",
    "this is because the data @xmath40 which were sampled from a gaussian distribution are very generic . in the affine case ,",
    "the performance is a little less good .",
    "if we set all @xmath12 s to have the same sign , then as suggested by condition , the percentage of outliers that can be corrected by the optimization problem can not exceed @xmath207 .",
    "consider now the case of static models of the form in the presence of both @xmath3 and @xmath4 sampled from gaussian distributions @xmath493 and @xmath494 respectively .",
    "the variance @xmath495 is selected so as to achieve a certain signal to noise ratio before the gross error sequence is added to the output .",
    "again , by carrying out a monte - carlo simulation of size @xmath489 with different sparsity levels and randomly generated models at each run , we obtain the average errors plotted in figure [ fig : error ] .",
    "it turns out that the results returned by problems and with @xmath496 are almost the same for an snr in @xmath497 .",
    "the performance can be assessed by comparing with an `` oracle '' estimate the least squares estimate one would obtain if the locations of zeros in the sequence @xmath4 were known .",
    "the results in figure [ fig : error ] tend to suggest that the proposed approach performs very well .",
    "for the current numerical experiment , our results are very close to the ideal estimate when the proportion of nonzeros is less than @xmath498 .      in the case",
    "when represents a dynamic arx model subject to gross errors , it can be observed ( see fig .",
    "[ fig : proba2 ] ) that the probabilities of exact recovery are much smaller than in the static case studied in section [ subsec : static ] .",
    "this difference is related to the richness ( or genericity ) of the regression vectors ( columns of @xmath57 ) involved in each case . in the static example above , the vectors @xmath40 are freely sampled in any direction of @xmath41 by following a gaussian distribution . in the dynamic system case",
    "however , the data vectors @xmath40 constructed as in are constrained to lie on a manifold . as a result ,",
    "the data matrix @xmath57 generated by the dynamic system is less generic .",
    "according to conditions of the paper , and in particular , there is a threshold depending on the richness of the data such that exact recovery is guaranteed whenever the number of zero entries in @xmath107 is larger than this threshold .",
    "so , the more generic the data contained in @xmath57 are , the more outliers can be removed by problem .",
    "note that the lack of sufficient genericity can be compensated ( to some extent ) by implementing the iterative sparsity enhancing technique ( @xmath117 reweighted algorithm ) described in section [ subsec : sparsity - enhancing ] .",
    "this leads , for only two iterations , to significantly improved results as represented in figure [ fig : l1reweigthed ] .",
    "minimization when noise @xmath3 is equal to zero .",
    "results of a monte - carlo simulation of size @xmath489 with randomly generated linear arx systems with orders @xmath499.,width=340,height=226 ]      this subsection presents a numerical evaluation of the estimates of number of outliers that can be corrected by the nonsmooth optimization - based estimator .",
    "note that the numbers @xmath230 , @xmath500 from theorems [ thm : x(xx ) ] and [ thm : sufficient - multivariable ] are hard to compute numerically because this would require a combinatorial optimization .",
    "therefore we just compare those bounds which are easier to compute .",
    "more specifically , three sufficient thresholds are compared :    * the bound @xmath501 obtained in theorem [ prop : sufficient - condition ] . * a normalized version of this bound @xmath502 with @xmath503 where @xmath504 , @xmath505 .",
    "indeed @xmath506 equals @xmath305 in the case when all the regressors @xmath40 are normalized to have unit @xmath507-norm .",
    "note that this definition of the bound applies to a weighted version of the objective function in , namely @xmath508 . * the mutual coherence - based bound @xmath509 with @xmath510 obtained in @xcite . here",
    "@xmath511 represents the mutual coherence .",
    "figure [ fig : bounds - comparison - static ] compares the sufficient thresholds in the case of static data drawn from a gaussian distribution @xmath512 .",
    "figure [ fig : bounds - comparison - dynamic ] compares the same thresholds for dynamic data in the form .",
    "the generating system in this case is an arx model defined by @xmath513 and driven by a normally distributed input sequence . in all cases ,",
    "the data matrix @xmath57 is normalized so as to have unit @xmath514-norm columns before being processed .",
    "the plots in figure [ fig : bounds - comparison - static ] and figure [ fig : bounds - comparison - dynamic ] draw the average values obtained over @xmath489 independent runs .",
    "the results suggest three interesting facts :    * all the bounds are very loose that is , they largely underestimate the number of admissible gross errors .",
    "for example figure [ fig : proba ] shows that exact recovery can be achieved in the face a relatively large proportion ( more than @xmath498 ) of corrupted data while the sufficient bounds in [ fig : bounds - comparison - static ] indicate a value around @xmath515 .",
    "the bounds involving the numbers @xmath230 are likely to be tighter but they are much more challenging to compute numerically . * the normalized @xmath516-based bound tend to provide a better bound ; the other two provide very close bounds particularly when the number of samples is large .",
    "* as could be intuitively expected , the dynamic data generated by a linear system are less generic .",
    "the bounds obtained in this case are smaller .",
    "the question as to which type of dynamic system can generate more generic data is open .",
    "note that all the proposed sufficient conditions are functions of the regressor matrix @xmath57 only and they are stated as bounds on the number of zeros or nonzeros in the sequence @xmath4 .",
    "one can expect to gain a substantial improvement by taking explicitly into account the information relative to the sign of the @xmath12 s just as done in the case of affine models ( see eq . ) .",
    "in this paper we have discussed the potential of nonsmooth convex optimization for addressing the problem of robust estimation . considering in particular the problem of inferring an unknown parameter vector from measurements which are subject to possibly unbounded gross errors",
    ", we showed that an exact recovery is possible regardless of the number of gross errors provided certain conditions of genericity hold .",
    "the conditions for exact recovery are essentially sufficient conditions .",
    "simulations results reveal that the proposed conditions are conservative . concerning the identification problem ,",
    "future work will consider the problem of designing the excitation of a dynamic system so as to achieve such genericity conditions on the regressor matrix .",
    "i.  maruta and t.  sugie .",
    "identification of pwa models via data compression based on @xmath117 optimization . in _",
    "ieee conference on decision and control and european control conference , orlando , fl , usa _ , 2011 .",
    "n.  ozay and m.  sznaier .",
    "hybrid system identification with faulty measurements and its application to activity analysis . in _ ieee conference on decision and control and european control conference , orlando , fl , usa _ , 2011 ."
  ],
  "abstract_text": [
    "<S> in this paper , we consider the problem of identifying a linear map from measurements which are subject to intermittent and arbitarily large errors . this is a fundamental problem in many estimation - related applications such as fault detection , state estimation in lossy networks , hybrid system identification , robust estimation , etc . </S>",
    "<S> the problem is hard because it exhibits some intrinsic combinatorial features . </S>",
    "<S> therefore , obtaining an effective solution necessitates relaxations that are both solvable at a reasonable cost and effective in the sense that they can return the true parameter vector . </S>",
    "<S> the current paper discusses a nonsmooth convex optimization approach . in particular , it is shown that under appropriate conditions on the data , an exact estimate can be recovered from data corrupted by a large ( even infinite ) number of gross errors .    [ [ keywords ] ] keywords + + + + + + + +    robust estimation , outliers , system identification , nonsmooth optimization . </S>"
  ]
}