{
  "article_text": [
    "one of the tasks of quantum information theory is to evaluate the capacity of quantum channels for the transmission of classical information .",
    "recently , much attention has been devoted to the study of quantum channels with memory ( see e.g.  @xcite and the references therein ) .",
    "there are evidences that memory effects can enhance the classical capacity in both the cases of discrete @xcite and continuous @xcite variables .",
    "bosonic gaussian channels provide a fertile benchmark for the study of quantum channels with continuous alphabet @xcite . here",
    "we consider a model of lossy bosonic memory channel introduced in @xcite , based on the general scheme proposed in @xcite ( see also @xcite ) . in this kind of model ,",
    "memory effects come from the interaction with a common environment containing correlations . in spite of the fact that each channel belonging to this family is unitary equivalent to a memoryless one",
    ", the presence of energy constraints can break the unitary symmetry , leaving the problem of evaluating capacities open ( see  @xcite ) .",
    "a specific instance of a channel belonging to that family is obtained by specifying the state of the environment .",
    "here we consider a multimode squeezed vacuum with one free parameter expressing the degree of squeezing , this parameter in turn determines the amount of memory contained in the channel .",
    "the aim of this paper is to evaluate the maximum achievable transmission rate using homodyne detection .",
    "a similar analysis was already presented in @xcite , where the maximization was performed over a specific set of encoding / decoding schemes ; here we optimize the rate over a much larger set .",
    "that allows to compare explicitly the performance of suitably defined _ collective schemes _ ,",
    "i.e.  using entangled states , over _ local schemes _ involving only separable ones .",
    "the paper proceeds as follows . in section [ model ]",
    "we introduce the model . in section [ ed ]",
    "we describe the procedure for encoding and decoding of classical information .",
    "the main results are presented in section [ achieve ] , where we compare local and collective encoding / decoding schemes .",
    "section [ final ] contains conclusions and comments .",
    "to define our model we need to introduce , for a given integer @xmath0 , a set of @xmath0 input bosonic oscillators , with canonical variables @xmath1 , and a collection of @xmath0 ancillary modes , which play the role of the environment , with canonical variables @xmath2 . in the following",
    "we refer to this set of oscillators as the _",
    "local modes_. all the frequencies are assumed to be degenerate and normalized to one .",
    "the integer @xmath3 labels the sequential uses of the channel . at the @xmath3th use",
    ", the @xmath3th input mode is linearly mixed with the @xmath3th environment mode at a beam splitter with given transmissivity @xmath4 ( see figure  [ fig_model ] ) . in the heisenberg picture ,",
    "the channel transforms the input field variables as @xmath5        memory effects among different uses of the channel are present if the corresponding local modes of the environment are correlated .",
    "here we assume the environment to be in a gaussian state with zero mean , described by the wigner function @xmath6 where @xmath7 indicates the vector of coordinates in the environment phase space .",
    "the choice of a gaussian state for the environment makes the channel itself gaussian .",
    "we chose a covariance matrix with the following block - diagonal form @xmath8 this is a _ bona fide _ covariance matrix as long as the matrix @xmath9 is symmetric and the parameter @xmath10 is real .",
    "it represents a multimode squeezed vacuum ( see e.g.  @xcite ) .",
    "the squeezing parameter @xmath10 ( or @xmath11 ) , determining how strong environment correlations are , can be also interpreted as a measure of _ memory _ between different uses of the bosonic channel . as to the form of @xmath9",
    ", we chose the following @xmath12 matrix : @xmath13 the spectrum of this matrix was already presented in e.g.@xcite , its eigenvalues are @xmath14 the components of the corresponding eigenvectors being @xmath15    according to williamson theorem ( see e.g.  @xcite ) one can always find a set of collective modes , with canonical variables @xmath16 , which diagonalize the covariance matrix . in this basis , the wigner function has the form @xmath17 where @xmath18 .",
    "an explicit expression for these variables can be obtained from the eigenvectors of the matrix @xmath9 , yielding to define the following variables : @xmath19 analogously , we introduce the following collective variables at the input field : @xmath20 we refer to the set of oscillators with field variables @xmath21 and @xmath22 as the _ collective modes_. let us remark that the action of the channel in the heisenberg picture on the collective modes is formally unchanged : @xmath23    it is worth noticing that the diagonal covariance matrix @xmath24 is the direct sum of one - mode covariance matrices : @xmath25 that implies that the state of the environment is simply separable in the basis of collective modes .",
    "it follows from the transformation ( [ collective ] ) that the entries of @xmath24 are the eigenvalues of @xmath26 . using the eigenvalues of the matrix @xmath9",
    ", we obtain the following expression for the covariance matrix of the @xmath27th collective mode : @xmath28 where @xmath29 .",
    "we observe that each collective oscillator of the environment is in a squeezed state with @xmath27 dependent squeezing parameter .    to conclude this section , let us come back to the basis of the local modes .",
    "for a given integer @xmath30 $ ] , by integrating the wigner function in ( [ wigner_res_loc ] ) over the local variables @xmath31 for @xmath32 we obtain the wigner function describing the state of the @xmath33th local mode of the environment .",
    "the corresponding local state is thermal - like , with average number of excitations : @xmath34 - \\frac{1}{2 } \\ , .\\ ] ] by the symmetries of @xmath35 and @xmath36 , this can be rewritten as follows : @xmath37 - \\frac{1}{2}\\ , , & \\mbox{if $ n$ is even.}\\\\ \\left[\\sum_{j=1}^{(n-1)/2 } v_{j , k_0}^2 \\cosh{s_j}\\right ] + \\frac{1}{2 } v_{(n+1)/2,k_0}^2   - \\frac{1}{2}\\ , , & \\mbox{if $ n$ is odd}. \\end{array } \\right.\\end{aligned}\\ ] ] notice that the _ local temperature _",
    "@xmath38 is a monotonically increasing function of @xmath11 .",
    "classical information is sent via the bosonic channel by choosing a suitable scheme for encoding ( by state preparation of the input field ) and decoding ( by observing the output field ) a classical alphabet .    as to the encoding ,",
    "we introduce a reference pure state of the input field , described by the gaussian wigner function @xmath39 expressed in terms of the local variable @xmath40 . a multivariate gaussian variable @xmath41 , with probability distribution @xmath42 ,",
    "zero mean and covariance matrix @xmath43 , is encoded in the displaced state of the input field described by the wigner function @xmath44}.\\ ] ] eventually , the state describing the statistical ensemble has the following wigner function : @xmath45}.\\ ] ]    to avoid infinite energy we introduce a constraint in the maximum number of excitations at the input field per channel use ( i.e.  per mode ) in average . allowing no more than @xmath46 excitations per mode in average , the constraint can be written in terms of the covariance matrices , in natural units : @xmath47    we can summarize the action of the channel on gaussian states as follows . for a given value of the displacement amplitudes ( hence for a given letter of the alphabet ) the wigner function at the output field has covariance matrix @xmath48 on the other hand , the output state averaged over the letters of the continuous alphabet has covariance matrix @xmath49    we can equivalently work in the basis of collective modes , in which the covariance matrix of the reference input state is denoted as @xmath50 .",
    "analogously , the zero mean gaussian variable @xmath51 , defined by the relations @xmath52 has covariance matrix @xmath53 . in the collective basis , the output field is described by the covariance matrices @xmath54 and @xmath55 it is worth noticing that the form of the energy constraint is preserved , namely @xmath56    homodyne detection requires the choice of a compatible set of @xmath0 quadratures to be measured at the output field .",
    "a generic set of quadratures is @xmath57 for any pair of @xmath12 matrices @xmath58 , @xmath59 satisfying the relations @xmath60 and @xmath61 ( see e.g.  @xcite ) . assuming ideal homodyne , the distribution of the stochastic variable @xmath62 is gaussian with zero mean .",
    "its covariance matrix , which we denote as @xmath63 , can be computed from the output field covariance matrix using the relations in  ( [ homodyne ] ) .    for",
    "given covariance matrices @xmath26 , @xmath64 , and for a given set of quadratures to be measured , a classical channel is defined from the quantum one .",
    "the capacity of the classical channel can be computed as the maximum of the mutual information between the output and the input variables : @xmath65 where @xmath66 denotes the shannon entropy .",
    "the maximization has to be taken over all possible expressions of the covariance matrix @xmath43 compatible with the energy constraint .",
    "let us now consider the form of the wigner function for the reference input state , determined by the covariance matrix @xmath64 , and the form of the distribution of the classical variable @xmath67 , determined by the covariance matrix @xmath43 .",
    "here we distinguish and compare two cases : the first one corresponds to a _",
    "local _ encoding in which @xmath64 and @xmath43 are diagonal ; the second one is a _ collective _ encoding in which @xmath50 and @xmath53 are diagonal .",
    "from the view point of the local modes , in the local encoding scheme information is always carried by simply separable ( unentangled ) states , while the collective encoding deals with states which are in general entangled .",
    "let us first describe the case of local encoding . in the basis of the local modes , the diagonal @xmath64 can be parameterized as follows @xmath68 we introduce the real parameters @xmath69 , such that @xmath70 analogously , the covariance matrix @xmath43 reads @xmath71 and is parameterized by the positive parameters @xmath72 : @xmath73 as to the decoding part , the natural choice is to measure the local quadratures @xmath74    the covariance matrix of the output variable @xmath75 can be easily computed .",
    "it is diagonal as well , with the variances : @xmath76 + \\sin^2{\\theta_k } \\left [ \\eta \\left(\\frac{e^{-r_k}}{2}+c_{p , k}\\right)+ ( 1-\\eta ) \\left(t_\\mathrm{eff}(s , k ) + \\frac{1}{2}\\right ) \\right].\\ ] ] the output variable has conditioned covariance matrix , denoted @xmath77 , which is diagonal with entries @xmath78 + \\sin^2{\\theta_k } \\left [ \\eta \\frac{e^{-r_k}}{2 } + ( 1-\\eta ) \\left(t_\\mathrm{eff}(s , k ) + \\frac{1}{2}\\right ) \\right].\\ ] ]    since all the probability distributions are gaussian , the mutual information can be easily written in terms of the covariance matrices : @xmath79 } - \\frac{1}{2 } \\log_2{\\left[\\det(z_y)\\right ] } = \\frac{1}{2}\\sum_{k=1}^n \\log_2{\\left(\\frac{\\langle z_k^2\\rangle}{\\langle z_k^2\\rangle_y}\\right)}.\\ ] ] the maximization of the mutual information is over the @xmath80 parameters @xmath81 satisfying the constraints @xmath82 and over the @xmath0 angles @xmath83 , which determine the chosen quadratures . concerning the choice of the optimal quadratures , it is immediate to recognize that the maximum is reached for @xmath84 if @xmath85 , and for @xmath86 if @xmath87 , while the value of @xmath83 is irrelevant if @xmath88 . maximizing over the remaining variables and applying the additivity of mutual information , we obtain the following expression for the maximum mutual information with local encoding / decoding scheme : @xmath89 } \\ \\ | \\ \\ \\frac{1}{n}\\sum_{k=1}^n n_k \\le n \\right\\},\\ ] ] where @xmath90,\\ ] ] and we have defined @xmath91 .",
    "let us now consider the case of collective encoding . in the basis of the collective modes , the diagonal @xmath50 can be parameterized as follows @xmath92 with @xmath93 analogously , the covariance matrix @xmath53 is @xmath94 with @xmath95    as to the decoding part , the natural choice is to measure the collective quadratures defined as @xmath96 the output variable @xmath97 has diagonal covariance matrix @xmath98 and conditional covariance matrix @xmath99 .",
    "the corresponding variances read @xmath100 + \\sin^2{\\theta_j } \\left [ \\eta \\left(\\frac{e^{-\\tilde{r}_j}}{2}+\\tilde{c}_{p , j}\\right)+ ( 1-\\eta ) \\frac{e^{-s_j}}{2 } \\right]\\ ] ] and @xmath101 + \\sin^2{\\theta_j } \\left [ \\eta \\frac{e^{-\\tilde{r}_j}}{2 } + ( 1-\\eta ) \\frac{e^{-s_j}}{2 } \\right].\\ ] ]    it is easy to recognize that the maximum of the mutual information is reached for @xmath102 if @xmath103 , and for @xmath104 if @xmath105 , finally if @xmath106 we can argue as in the case of local encoding . by maximizing over the remaining @xmath80 variables @xmath107 , @xmath108 , @xmath109 under the constraints @xmath110 and applying the additivity of the mutual information , we obtain the following expression for the maximum rate of transmission with encoding / decoding in collective variables : @xmath111 }",
    "\\ \\ | \\ \\ \\frac{1}{n } \\sum_{j=1}^n n_j \\le n \\right\\}.\\ ] ] the optimal value @xmath112 is determined by the equation @xmath113.\\ ] ]    the maximum mutual informations _ per channel use _ ,",
    "respectively defined as @xmath114 and @xmath115 , are plotted in figure [ bars ] as function of number of uses of the channel , for several values of the memory parameter . the case @xmath116 or @xmath117",
    "correspond to the memoryless channel .",
    "notice the different behavior of the rates as the number of uses ( or the value of the memory parameter ) increases . the same quantities are plotted together in figure [ rates ] , for @xmath118 uses of the channel , as function of the memory parameter , for several values of the beam splitter transmissivity .",
    "notice the different behavior of the rates corresponding to local ( dotted line ) and collective ( solid line ) encoding / decoding scheme .",
    ", the histograms correspond to different values of the memory parameter , from the left to the right @xmath117 , @xmath119 , @xmath120 , @xmath121 .",
    "the maximum average excitations per mode is @xmath122 , the transmissivity @xmath123.,scaledwidth=40.0% ]     uses of the channel .",
    "the curves correspond to different values of the transmissivity , from the bottom to the top @xmath4 varies from @xmath124 to @xmath125 by steps of @xmath126 .",
    "the maximum average excitations per mode is @xmath122 , which corresponds to the asymptotic value ( plotted in dashed line ) of the rate with collective scheme ( see equation ( [ limit ] ) ) : @xmath127.,scaledwidth=40.0% ]    the expression of the maximal mutual information allows to make comparison with the memoryless case ( see also the plot in figure [ rates ] ) .",
    "first of all , let us recall that the memoryless channel is recovered for @xmath117 ; our results show that the maximum mutual information is a monotonically increasing function of @xmath11 in the case of collective encoding / decoding scheme , while it is monotonically decreasing in the case of the local one .",
    "this is enough to conclude that , in the presence of memory , collective encoding / decoding is the optimal choice .",
    "secondly , let us notice that the two schemes are equivalent in the memoryless case , i.e.@xmath117 or @xmath116 ; that leads to conclude that the presence of memory enhances the bit rate as long as collective encoding / decoding is allowed .",
    "we have discussed the model of lossy bosonic gaussian channel with memory presented in @xcite , an instance of a general class of bosonic memory channel introduced in @xcite .",
    "our model is characterized by a single parameter which determines the amount of memory contained in the channel . by considering the case of homodyne detection ,",
    "we have compared two different encoding / decoding schemes : a _ local _ one in which classical information is encoded and carried by unentangled ( simply separable ) states ; and a _ collective _ scheme based on states which are in general entangled among different uses of the channel .",
    "we have computed the maximum achievable bit rate using both the schemes , assuming energy constrains at the input field , for an arbitrary number of channel uses .",
    "our results lead to conclude that , as the memory parameter increases , the collective scheme becomes more and more efficient than the local one .",
    "it is hence interesting to analyze the asymptotic limit of the rates for @xmath128 , corresponding to _ infinite _ memory . from the expressions in ( [ maxx_loc ] ) and ( [ maxx_coll ] ) we obtain @xmath129 and @xmath130 notice that the latter expression holds independently of @xmath0 and for any value of @xmath131 .",
    "interestingly enough , the asymptotic value in ( [ limit ] ) equals the maximum achievable rate of the ( memoryless ) noiseless channel ( @xmath132 ) with homodyne detection ( see @xcite ) . in this sense , by using local encoding / decoding one sees infinite noise in the channel with high degree of memory , while by using a collective encoding / decoding one can completely avoid it .          c. macchiavello , and g. m. palma , phys .",
    "a * 65 * , 050301(r ) ( 2002 ) ; + c. macchiavello , g. m. palma , and s. virmani , phys .",
    "a * 69 * , 010303(r ) ( 2004 ) ; + d. daems , phys .",
    "a * 76 * , 012310 ( 2007 ) ."
  ],
  "abstract_text": [
    "<S> we present a study of the achievable rates for classical information transmission via a lossy bosonic channel with memory , using homodyne detection . </S>",
    "<S> a comparison with the memoryless case shows that the presence of memory enhances the bit rate if information is encoded in collective states , i.e.  states which are entangled over different uses of the channel . </S>"
  ]
}