{
  "article_text": [
    "continuous advancement in vlsi technologies has resulted in extremely small transistor sizes and highly complex microprocessors . however , on - chip interconnects responsible for on - chip communication have been improved only moderately .",
    "this leads to the `` paradox '' that local information processing is done very efficiently , but communicating information between on - chip units is a major challenge .",
    "this work focuses on an emergent issue expected to challenge circuit development in future technologies .",
    "information communication and processing is associated with energy dissipation into heat which raises the temperature of the transmitter / receiver or processing devices ; moreover , the intrinsic device noise level depends strongly and increasingly on the temperature .",
    "therefore , the total physical structure can be modeled as a communication channel whose noise level is data dependent .",
    "we describe this mathematically in the following subsection .",
    "we consider the communication system depicted in figure  [ fig1 ] .",
    "the message @xmath0 to be transmitted over the channel is assumed to be uniformly distributed over the set @xmath1 for some positive integer @xmath2 .",
    "the encoder maps the message to the length-@xmath3 sequence @xmath4 , where @xmath3 is called the _ block - length_. thus , in the absence of feedback , the sequence @xmath5 is a function of the message @xmath0 , i.e. , @xmath6 for some mapping @xmath7 . here , @xmath8 stands for @xmath9 , and @xmath10 denotes the set of real numbers .",
    "if there is a feedback link , then @xmath11 , @xmath12 , is a function of the message @xmath0 and , additionally , of the past channel output symbols @xmath13 , i.e. , @xmath14 for some mapping @xmath15 .",
    "the receiver guesses the transmitted message @xmath0 based on the @xmath3 channel output symbols @xmath16 , i.e. , @xmath17 for some mapping @xmath18 .",
    "let @xmath19 denote the set of positive integers .",
    "the channel output @xmath20 at time @xmath21 corresponding to the channel inputs @xmath22 is given by @xmath23 where @xmath24 are independent and identically distributed ( iid ) , zero - mean , unit - variance gaussian random variables drawn independently of @xmath0 .",
    "[ cc][cc]transmitter [ cc][cc]channel [ cc][cc]receiver [ cc][cc]delay [ b][b]@xmath0 [ b][b]@xmath25 [ b][b]@xmath11 [ b][b]@xmath26 [ b][b]@xmath13    the coefficients @xmath27 are non - negative and satisfy for @xmath28 .",
    "this assumption is , however , not required for the results stated in this paper . ]",
    "@xmath29 note that this channel is not stationary as the variance of the additive noise depends on the time - index @xmath30 .",
    "we study the above channel under an average - power constraint on the inputs , i.e. , @xmath31 and we define the signal - to - noise ratio ( snr ) as @xmath32      let the _ rate _",
    "@xmath33 ( in nats per channel use ) be defined as @xmath34 where @xmath35 denotes the natural logarithm function .",
    "a rate is said to be _ achievable _ if there exists a sequence of mappings @xmath36 ( without feedback ) or @xmath37 ( with feedback ) and @xmath38 such that the error probability @xmath39 tends to zero as @xmath3 goes to infinity .",
    "the _ capacity _",
    "@xmath40 is the supremum of all achievable rates .",
    "we denote by @xmath41 the capacity under the input constraint when there is no feedback , and we add the subscript `` fb '' to indicate that there is a feedback link .",
    "clearly , @xmath42 as we can always ignore the feedback link .",
    "in this paper we study the _ capacities per unit cost _ which are defined as @xcite @xmath43 note that implies @xmath44      our main result is stated in the following theorem .    [ thm : main ] consider the above channel model .",
    "then , irrespective of whether feedback is available or not , the corresponding capacity per unit cost is given by @xmath45 where @xmath46 is defined in .",
    "theorem  [ thm : main ] is proved in section  [ sec : proof ] . in section  [ sec : highsnr ] we briefly discuss the above channel at high snr .",
    "specifically , we present a sufficient and a necessary condition on the coefficients @xmath27 for capacity to be bounded in the snr .",
    "in section  [ sub : upperbound ] we derive an upper bound on the feedback capacity @xmath47 , and in section  [ sub : lowerbound ] we derive a lower bound on the capacity @xmath41 in the absence of feedback .",
    "these bounds are then used in section  [ sub : asymptotic ] to derive an upper bound on @xmath48 and a lower bound on @xmath49 , and it is shown that both bounds are equal to @xmath50 . together with this proves theorem  [ thm : main ] .      as in (",
    "8.12 ) , the upper bound on @xmath47 is based on fano s inequality and on an upper bound on @xmath51 , which for our channel can be expressed , using the chain rule for mutual information , as    lcl + & = & _ k=1^n + & = & _ k=1^n + & = & _ k=1^n [ eq : upper1 ]    where the second equality follows because @xmath52 is a function of @xmath0 and @xmath13 ; and the last equality follows from the behavior of differential entropy under translation and scaling ( * ? ? ?",
    "9.6.3 & 9.6.4 ) , and because @xmath53 is independent of @xmath54 .",
    "evaluating the differential entropy @xmath55 of a gaussian random variable , and using the trivial lower bound @xmath56 , we obtain the final upper bound    lcl + & & _ k=1^n + & & _ k=1^n ( 1+_=1^k_k- /^2 ) + & & ( 1+_k=1^n_=1^k_k- /^2 ) + & = & ( 1+_k=1^n /^2_=0^n - k _ ) + & & ( 1+(1+)_k=1^n /^2 ) + & & ( 1+(1+))[eq : upper2 ]    where we define @xmath57 . here",
    ", the second inequality follows because conditioning can not increase entropy and from the entropy maximizing property of gaussian random variables ( * ? ? ?",
    "9.6.5 ) ; the next inequality follows by jensen s inequality ; the following equality by rewriting the double sum ; the subsequent inequality follows because the coefficients are non - negative which implies that @xmath58 ; and the last inequality follows from the power constraint .      as aforementioned ,",
    "the above channel is not stationary , and one therefore needs to exercise some care in relating the capacity @xmath41 in the absence of feedback to the quantity @xmath59 ( where the maximization is over all input distributions satisfying the power constraint ) .",
    "in fact , it is _ prima facie _ not clear whether there is a coding theorem associated with",
    ". we shall sidestep this problem by studying the capacity of a different channel whose time-@xmath30 channel output @xmath60 is , conditional on the sequence @xmath61 , given by @xmath62 where @xmath24 and @xmath27 are defined in section  [ sub : channelmodel ] .",
    "this channel has the advantage that it is stationary & ergodic in the sense that when @xmath63 is a stationary & ergodic process then the pair @xmath64 is jointly stationary & ergodic .",
    "it follows that if the sequences @xmath65 and @xmath66 are independent of each other , and if the random variables @xmath11 , @xmath67 , are bounded , then any rate that can be achieved over this new channel is also achievable over the original channel . indeed , the original channel can be converted into by adding @xmath68 to the channel output @xmath26 , and , since the independence of @xmath65 and @xmath66 ensures that the sequence @xmath69 is independent of the message @xmath0 , it follows that any rate achievable over can be achieved over by using a receiver that generates @xmath69 and guesses then @xmath0 based on @xmath70 . , @xmath67 , guarantees that the quantity @xmath71 is finite for any realization of @xmath65 . ]",
    "we consider @xmath72 that are block - wise iid in blocks of @xmath73 symbols .",
    "thus , denoting @xmath74 ( where @xmath75 denotes the transpose ) , @xmath76 are iid with @xmath77 taking on the value @xmath78 with probability @xmath79 and @xmath80 with probability @xmath81 , for some @xmath82 .",
    "note that to satisfy the average - power constraint we shall choose @xmath83 and @xmath79 so that @xmath84    let @xmath85 , and let @xmath86 denote the floor function .",
    "noting that the pair @xmath87 is jointly stationary & ergodic , it follows from @xcite that the rate @xmath88 is achievable over the new channel and , thus , yields a lower bound on the capacity @xmath41 of the original channel .",
    "we lower bound @xmath89 as    lcl + & = & _",
    "= 0^n / l -1 i(._;_0^n / l -1|_0 ^ -1 ) + & & _",
    "= 0^n / l -1 i(._;_|_0 ^ -1 ) + & & _",
    "= 0^n / l -1[eq : lb1 ]    where we use the chain rule and that reducing observations can not increase mutual information . by using that implies @xmath90",
    "it can be shown that the second term in the sum on the right - hand side ( rhs ) of vanishes as @xmath91 tends to infinity .",
    "this together with a cesro type theorem ( * ? ? ?",
    "4.2.3 ) yields    lcl + & & i(._0;_0|_-^-1 ) + & & -_n _ = 0^n / l -1i(._-^-1;_|_0^ ) + & = & i(._0;_0|_-^-1)[eq : lbcesaro ]    where the first inequality follows by the stationarity of @xmath87 which implies that @xmath92 does not depend on @xmath91 , and by noting that , for a fixed @xmath73 , .",
    "we proceed to analyze @xmath93 for a given sequence @xmath94 . making use of the canonical decomposition of mutual information ( e.g. , ( * ? ? ?",
    "* eq .  ( 10 ) ) ) , we have    lcl + & = & i(.x_1;_0|_-^-1=_-^-1 ) + & = & d ( .",
    "f__0|x_1=x,_-^-1 f__0|x_1=0,_-^-1 ) p_x_1(x ) + & & -d(.f__0|_-^-1 f__0|x_1=0,_-^-1 ) + & = & d ( .",
    "f__0|x_1=,_-^-1 f__0|x_1=0,_-^-1 ) + & & - d(.f__0|_-^-1 f__0|x_1=0,_-^-1 ) [ eq : lb2 ]    where the first equality follows because , for our choice of input distribution , @xmath95 and , hence , @xmath96 conveys as much information about @xmath97 as @xmath98 . here",
    ", @xmath99 denotes relative entropy , and @xmath100 , @xmath101 , and @xmath102 denote the densities of @xmath97 conditional on the inputs @xmath103 , @xmath104 , and @xmath94 , respectively .",
    "thus , @xmath100 is the density of an @xmath73-variate gaussian random vector of mean @xmath78 and of diagonal covariance matrix @xmath105 with diagonal entries    lcl ^()__-^-1(1,1 ) & = & ^2+_i=-^-1_-ilx_il+1 ^ 2 + ^()__-^-1(k , k ) & = & ^2+_k-1 ^ 2+_i=-^-1_-il+k-1x_il+1 ^ 2 , + & & k=2,  ,l ;    @xmath101 is the density of an @xmath73-variate , zero - mean gaussian random vector of diagonal covariance matrix @xmath106 with diagonal entries @xmath107 and @xmath102 is given by @xmath108    in order to evaluate the first term on the rhs of we note that the relative entropy of two real , @xmath73-variate gaussian random vectors of the respective means @xmath109 and @xmath110 and of the respective covariance matrices @xmath111 and @xmath112 is given by    lcl + & = & _ 2 - _ 1 + + & & + _ 2 ^ -1(_1-_2 ) [ eq : dgaussian ]    with @xmath113 and @xmath114 denoting the determinant and the trace of the matrix @xmath115 , respectively , and where @xmath116 denotes the @xmath117 identity matrix . the second term on the rhs of is analyzed in the next subsection .",
    "let @xmath118 denote the second term on the rhs of averaged over @xmath119 , i.e. ,    r + = .",
    "then , using & and taking expectations over @xmath119 we obtain , again defining @xmath120 ,    lcl + & = & _ k=1^l + & & -_k=2^l + & & - + & & _ k=1^l + & & -_k=2^l ( 1+_k-1 ^2/^2 ) + & & - + & & _ k=1^l + & & -_k=2^l + & & - [ eq : lbbeforelimit ]    where the first inequality follows by the lower bound @xmath121 which is a consequence of jensen s inequality applied to the convex function @xmath122 , @xmath123 , and by the upper bound    l + ( 1+_k-1 ^2/^2 )    for every @xmath124 ; and the second inequality follows by and by upper bounding @xmath125 for every @xmath126 .    the final lower bound follows now by and    lcl + & & _ k=1^l + & & - _ k=2^l + & & - .",
    "[ eq : lbfinal ]      we start with analyzing the upper bound .",
    "we have @xmath127 where the second inequality follows by upper bounding @xmath128 , @xmath123 , and we thus obtain @xmath129    in order to derive a lower bound on @xmath49 we first note that @xmath130 and proceed by analyzing the limiting ratio of the lower bound to the snr as the snr tends to zero .    to this end , we first shall show that @xmath131 it was shown in @xcite that for any pair of densities @xmath132 and @xmath133 satisfying @xmath134 @xmath135 thus , for any given @xmath94 , together with @xmath136 implies that @xmath137 in order to show that this also holds when @xmath138 is averaged over @xmath119 , we derive in the following the uniform upper bound    lcl + [ eq : uniformbound ]    the claim follows then by upper bounding    lcl +    and by .    in order to prove we use that any gaussian random vector can be expressed as the sum of two independent gaussian random vectors to write the channel output @xmath97 as @xmath139 where , conditional on @xmath140 , @xmath141 and @xmath142 are @xmath73-variate , zero - mean gaussian random vectors , drawn independently of each other , and having the respective diagonal covariance matrices @xmath143 and @xmath144 whose diagonal entries are given by    lcl _",
    "|_0(1,1 ) & = & ^2 + _ |_0(k , k ) & = & ^2 + _ k-1x_1 , k=2, ",
    ",l ,    and @xmath145 thus , @xmath141 is the portion of the noise due to @xmath98 , and @xmath142 is the portion of the noise due to @xmath119 .",
    "note that @xmath146 and @xmath142 are independent of each other because @xmath98 is , by construction , independent of @xmath119 .",
    "the upper bound follows now by    lcl + & = & d(.f__0++|_-^-1 f__0++|x_1=0,_-^-1 ) + & & d(.f__0 + f__0+|x_1=0 ) + & = & d.(.f__0|_-^-1 f__0|x_1=0,_-^-1)|__-^-1=0[eq : asym1 ]    where @xmath147 and @xmath148 denote the densities of @xmath149 conditional on the inputs @xmath94 and @xmath150 , respectively ; @xmath151 denotes the unconditional density of @xmath146 ; and @xmath152 denotes the density of @xmath146 conditional on @xmath153 . here , the inequality follows by the data processing inequality for relative entropy ( see ( * ? ? ?",
    "2.9 ) ) and by noting that @xmath146 is independent of @xmath119 .",
    "returning to the analysis of , we obtain from and    lcl + & & _ 0 _ k=1^l - _ k=2^l + & = & _ k=1^l _ k-1 - _ k=2^l .    by letting first @xmath154",
    "go to infinity while holding @xmath73 fixed , and by letting then @xmath73 go to infinity , we obtain the desired lower bound on the capacity per unit cost @xmath155 thus , , , and yield @xmath156 which proves theorem  [ thm : main ] .",
    "the channel described in section  [ sub : channelmodel ] was studied at high snr in @xcite where it was asked whether capacity is bounded or unbounded in the snr .",
    "it was shown that the answer to this question depends highly on the decay rate of the coefficients @xmath27 .",
    "we summarize the main result of @xcite in the next theorem . for a statement of this theorem in its full generality and for a proof thereof we refer to @xcite .",
    "[ thm : highsnr ] consider the channel model described in section  [ sub : channelmodel ] .",
    "then ,    llcl i ) & _ > 0 & & _ > 0 c_fb ( ) < , [ eq : i ] + ii ) & _ = 0 & & _ > 0 c ( ) = , [ eq : ii ]    where we define , for any @xmath157 , @xmath158 and @xmath159 .    for example , when @xmath27 is a geometric sequence , i.e. , @xmath160 for @xmath161 , then the capacity is bounded .",
    "note that when neither the left - hand side ( lhs ) of nor the lhs of holds , i.e. , when @xmath162 and @xmath163 , then the capacity can be bounded or unbounded .",
    "fruitful discussions with ashish khisti are gratefully acknowledged ."
  ],
  "abstract_text": [
    "<S> motivated by on - chip communication , a channel model is proposed where the variance of the additive noise depends on the weighted sum of the past channel input powers . for this channel , an expression for the capacity per unit cost </S>",
    "<S> is derived , and it is shown that the expression holds also in the presence of feedback . </S>"
  ]
}