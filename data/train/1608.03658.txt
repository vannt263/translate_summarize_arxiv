{
  "article_text": [
    "recent years have witnessed spectacular progress on similarity - based hash code learning in a variety of computer vision tasks , such as image search  @xcite , object recognition  @xcite and local descriptor compression  @xcite etc .",
    "the hash codes are highly compact ( , several bytes for each image ) in most cases , which significantly reduces the overhead of storing visual big data and also expedites similarity - based image search . the theoretic ground of similarity - oriented hashing is rooted from johnson - lindenstrause theorem  @xcite , which elucidates that for arbitrary @xmath0 samples , some @xmath1-dimensional subspace exists and can be found in polynomial time complexity .",
    "when embedded into this subspace , pairwise affinities among these @xmath0 samples are preserved with tight approximation error bounds .",
    "this seminal theoretic discovery sheds light on trading similarity preservation for high compression of large data set .",
    "the classic locality - sensitive hashing ( lsh )  @xcite is a good demonstration for above tradeoff , instantiated in various similarity metrics such as hamming distance  @xcite , cosine similarity  @xcite , @xmath2 distance with @xmath3 $ ]  @xcite , jaccard index  @xcite and euclidean distance  @xcite .",
    "images are often accompanied with supervised information in various forms , such as semantically similar  /  dissimilar data pairs . supervised hash code learning  @xcite harnesses such supervisory information during parameter optimization and has demonstrated superior image search accuracy compared with unsupervised hashing algorithms  @xcite .",
    "exemplar supervised hashing schemes include ldahash  @xcite , two - step hashing  @xcite , and kernel - based supervised hashing  @xcite etc .",
    "importantly , two factors are known to be crucial for hashing - based image search accuracy : the discriminative power of the features and the choice of hashing functions . in a typical pipeline of existing hashing methods",
    ", these two factors are separately treated .",
    "each image is often represented by a vector of hand - crafted visual features ( such as sift - based bag - of - words feature or sparse codes ) .",
    "regarding hash functions , a large body of existing works have adopted linear functions owing to the simplicity .",
    "more recently , researchers have also explored a number of non - linear hashing functions , such as anchor - based kernalized hashing function  @xcite and decision tree based function  @xcite .    this paper attacks the problem of supervised hashing by concurrently conducting visual feature engineering and hash function learning .",
    "most of existing image features are designated for general computer vision tasks .",
    "intuitively , by unifying these two sub - tasks in the same formulation , one can expect the extracted image features to be more amenable for the hashing purpose .",
    "our work is inspired by recent prevalence and success of deep learning techniques  @xcite .",
    "though the unreasonable effectiveness of deep learning has been successfully demonstrated in tasks like image classification  @xcite and face analysis  @xcite , deep learning for supervised hashing still remains inadequately explored in the literature .",
    "salakhutdinov et al . proposed _",
    "semantic hashing _ in  @xcite , where stacked restricted boltzmann machines ( rbms ) are employed for hash code generation .",
    "nonetheless , the algorithm is primarily devised for indexing textual data and its extension to visual data is unclear .",
    "xia et al .",
    "@xcite adopted a two - step hashing strategy similar to  @xcite .",
    "it firstly factorizes the data similarity matrix to obtain the target binary code for each image . in the next stage",
    ", the target codes and the image labels are jointly utilized to guide the network parameter optimization .",
    "since the target codes are not updated once approximately learned in the first stage , the final model is only sub - optimal .",
    "lai et al .",
    "@xcite developed a convolutional deep network for hashing , comprised of shared sub - networks and a divide - and - encode module .",
    "however , the parameters of these two components are still separately learned .",
    "after the shared sub - networks are initialized , their parameters ( including all convolutional / pooling layers ) are frozen during optimizing the divide - and - encode module .",
    "intrinsically , the method in  @xcite shall be categorized to two - step hashing , rather than simultaneous feature / hashing learning .",
    "liong et al .",
    "@xcite presented a binary encoding network built with purely fully - connected layers .",
    "the method essentially assumes that the visual features ( , gist as used in the experiments therein ) have been learned elsewhere and fed into its first layer as the input .    as revealed by above literature overview , a deep hashing method which simultaneously learns the features and hash codes remains missing in this research field , which inspires our work .",
    "the key contributions of this work include :    * we propose the first deep hashing algorithm of its kind , which performs concurrent feature and hash function learning over a unified network .",
    "* we investigate the key pitfalls in designing such deep networks .",
    "particularly , there are two major obstacles : the gradient calculation from non - differentiable binary hash codes , and network pre - training in order to eventually stay at a  good \" local optimum . to address the first issue",
    ", we propose an exponentiated hashing loss function and devise its bilinear smooth approximation .",
    "effective gradient calculation and propagation are thereby enabled .",
    "moreover , an efficient pre - training scheme is also proposed .",
    "we verify its effectiveness through comprehensive evaluations on real - world visual data . *",
    "the proposed deep hashing method establishes new performance records on four image benchmarks which are widely used in this research area .",
    "for instance , on the cifar10 dataset , our method achieves a mean average precision of 0.73 for hamming ranking based image search , which represents some drastic improvement compared with the state - of - the - art methods ( 0.58 for  @xcite and 0.36 for  @xcite ) .",
    "[ fig : network ]    throughout this paper we will use bold symbols to denote vectors or matrices , and italic ones for scalars unless otherwise instructed .",
    "suppose a data set @xmath4 with supervision information is provided as the input .",
    "prior works on supervised hashing have considered various forms of supervision , including triplet of items @xmath5 where the pair @xmath6 is more alike than the pair @xmath7  @xcite , pairwise similar / dissimilar relations  @xcite or specifying the label of each sample . observing that triplet - type supervision incurs tremendous complexity during hashing function learning and semantic - level sample labels can be effortlessly converted into pairwise relations , hereafter the discussion focuses on supervision in pairwise fashion .",
    "let @xmath8 , @xmath9 collect all similar / dissimilar pairs respectively . for notational convenience , we further introduce a supervision matrix @xmath10 as @xmath11    figure  [ fig : network ] illustrates our proposed pipeline of learning a deep convolutional network for supervised hashing .",
    "the network is comprised of two components : a topmost layer meticulously - customized for the hashing task and other conventional layers .",
    "the network takes a @xmath12-sized images with @xmath13 channels as the inputs .",
    "the @xmath14 neurons on the top layer output either -1 or 1 as the hash code .",
    "formally , each top neuron represents a hashing function @xmath15 , where @xmath16 denotes the 3-d raw image . for notational clarity ,",
    "let us denote the response vector on the second topmost layer as @xmath17 , where @xmath18 implicitly defines the highly non - linear mapping from the raw data to a specified intermediate layer .",
    "for the topmost layer , we adopt a simple linear transformation , followed by a signum operation , which is formally presented as @xmath19 = \\mbox{sign } \\left [ \\w_k^\\top \\phi(\\x ) \\right ] .",
    "\\label{eqn : h}\\end{aligned}\\ ] ]    the reminder of this section firstly introduces the hashing loss function and the calculation of its smoothed surrogate in section  [ subsec : loss ] .",
    "more algorithmic details of our proposed pretraining - finetuning procedure are delivered in section  [ subsection : pretrain ] .",
    "the key purpose of supervised hashing is to elevate the image search accuracy .",
    "the goal can be intuitively achieved by generating discriminative hash codes , such that similar data pairs can be perfectly distinguished from dissimilar pairs according to the hamming distances calculated over the hash codes .",
    "a number of hashing loss functions have been devised by using above design principal . in particular ,",
    "norouzi et al .",
    "@xcite propose a hinge - like loss function .",
    "critically , hinge loss is known to be non - smooth and thus complicates gradient - based optimization .",
    "two other works  @xcite adopt smooth @xmath20 loss defined on the inner product between hash codes .",
    "it largely remains unclear for designing optimal hashing loss functions in perceptron - like learning .",
    "the major complication stems from the discrete nature of hash codes , which prohibits direct gradient computation and propagation as in typical deep networks . as such",
    ", prior works have investigated several tricks to mitigate this issue .",
    "examples include optimizing a variational upper bound of the original non - smooth loss  @xcite , or simply computing some heuristic - oriented sub - gradients  @xcite . in this work we advocate an exponential discrete loss function which directly optimizes the hash code product and enjoys a bilinear smoothed approximation . compared with other alternative hashing losses , here",
    "we first show the proposed exponential loss arguably more amenable for mini - batch based iterative update and later exhibit its empirical superiority in the experiments .",
    "let @xmath21 denote @xmath14 hash bits in vector format for data object @xmath22 .",
    "we also use the notations @xmath23 , @xmath24 to stand for bit @xmath25 of @xmath26 and the hash code with bit @xmath25 absent respectively . as a widely - known fact in the hashing literature",
    "@xcite , _ code product _ admits a one - to - one correspondence to hamming distance and comparably easier to manipulate .",
    "a normalized version of code product ranging over @xmath27 $ ] is described as @xmath28 and when bit @xmath25 is absent , the code product using partial hash codes is @xmath29    * exponential loss * : given the observation that @xmath30 faithfully indicates the pairwise similarity , we propose to minimize an exponentiated objective function @xmath31 defined as the accumulation over all data pairs : @xmath32 where @xmath33 represents the collection of parameters in the deep networks excluding the hashing loss layer .",
    "the atomic loss term is @xmath34    this novel loss function enjoys some elegant traits desired by deep hashing compared with those in bre  @xcite , mlh  @xcite and ksh  @xcite .",
    "it establishes more direct connection to the hashing function parameters by maximizing the correlation of code product and pairwise labeling . in comparison , bre and mlh",
    "optimize the parameters by aligning hamming distance with original metric distances or enforcing the hamming distance larger / smaller than pre - specified thresholds .",
    "both formulations incur complicated optimization procedures , and their optimality conditions are unclear .",
    "ksh adopts a least - squares formulation for regressing code product onto the target labels , where a smooth surrogate for gradient computation is proposed .",
    "however , the surrogate heavily deviates from the original loss function due to its high non - linearity .    * gradient computation * : a prominent advantage of exponential loss is its easy conversion into multiplicative form , which elegantly simplifies the derivation of its gradient . for presentation clarity ,",
    "we hereafter only focus on the calculation conducted over the topmost hashing loss layer .",
    "namely , @xmath35 $ ] for bit @xmath25 , where @xmath36 are the response values at the second top layer and @xmath37 are parameters to be learned for bit @xmath25 ( @xmath38 ) .    following the common practice in deep learning , two groups of quantities @xmath39 , @xmath40 and @xmath41 ( @xmath42 ranges over the index set of current mini - batch ) need to be estimated on the hashing loss layer at each iteration",
    "the former group of quantities are used for updating @xmath37 , @xmath40 , and the latter are propagated backwards to the bottom layers .",
    "the additive algebra of hash code product in eqn .",
    "( [ eqn : cp ] ) inspires us to estimate the gradients in a leave - one - out mode . for atomic loss in eqn .",
    "( [ eqn : aloss ] ) , it is easily verified @xmath43 where only the latter factor is related to @xmath37 .",
    "since the product @xmath44 can only be -1 or 1 , we can linearize the latter factor through exhaustively enumerating all possible values , namely @xmath45 where @xmath46 are two sample - specific constants , calculated by @xmath47 and @xmath48 . since the hardness of calculating the gradient of eqn .",
    "( [ eqn : ee ] ) lies in the bit product @xmath44 , we replace the signum function using the sigmoid - shaped function @xmath49 , obtaining @xmath50    freezing the partial code product @xmath51 , we define an approximate atomic loss with only bits @xmath25 active : @xmath52 where the first factor @xmath53 plays a role of re - weighting specific data pair , conditioned on the rest @xmath54 bits .",
    "iterating over all @xmath25 s , the original loss function can now be approximated by @xmath55    training set @xmath56 , data labels , and step size @xmath57 ; network parameters @xmath37 , @xmath40 for the hashing - loss layer , and @xmath33 for other layers ; * * concatenate all layers ( excluding top hashing - loss layer ) with a softmax layer that defines an image classification task ; apply alexnet  @xcite ) style supervised parameter learning algorithm , obtaining @xmath33 .",
    "calculate neuron responses on second topmost layer through @xmath58 ; * * replicate all @xmath59 s from previous stage ; forward computation starting from @xmath59 ; update @xmath37 by minimizing the image classification error ; * * forward computation starting from the raw images ; estimate @xmath60 ; update @xmath61 ; estimate @xmath62 , @xmath63 ; propagate @xmath64 to bottom layers , updating @xmath33 ;    compared with other sigmoid - based approximations in previous hashing algorithms ( , ksh  @xcite ) , ours only requires @xmath65 ( rather than both @xmath66 and @xmath67 ) is sufficiently large .",
    "this bilinearity - oriented relaxation is more favorable for reducing approximation error , which will be corroborated by the subsequent experiments .    since the objective @xmath31 in eqn .",
    "( [ eqn : obj ] ) is a composition of atomic losses on data pairs , we only need to instantiate the gradient computation on specific data pair @xmath68 .",
    "applying basic calculus rules and discarding some scaling factors , we first obtain @xmath69 and further using calculus chain rule brings @xmath70    importantly , the formulas below obviously hold by the construction of @xmath71 : @xmath72    the gradient computations on other deep network layers simply follow the regular calculus rules .",
    "we thus omit the introduction .",
    "deep hashing algorithms ( including ours ) mostly strive to optimize pairwise ( or even triplet as in  @xcite ) similarity in hamming space .",
    "this raises an intrinsic distinction compared with conventional applications of deep networks ( such as image classification via alexnet  @xcite ) .",
    "the total count of data pairs quadratically increases with regard to the training sample number , and in conventional applications the number of atomic losses in the objective only linearly grows .",
    "this entails a much larger mini - batch size in order to combat numerical instability caused by under - sampling sampling rate in image classification .",
    "in contrast , in deep hashing , capturing @xmath73 pairwise similarity requires a tremendous mini - batch of 10,000 data . ] , which unfortunately often exceeds the maximal memory space on modern cpu / gpus .",
    "we adopt a simple two - stage supervised pre - training approach as an effective network pre - conditioner , initializing the parameter values in the appropriate range for further supervised fine - tuning . in the first stage ,",
    "the network ( excluding the hashing loss layer ) is concatenated to a regular softmax layer .",
    "the network parameters are learned through optimizing the objective of a relevant semantics learning task ( , image classification ) . after stage one",
    "is complete , we extract the neuron outputs of all training samples from the second topmost layer ( , the variable @xmath59 s in section  [ subsec : loss ] ) , feed them into another two - layer shallow network as shown in figure  [ fig : network ] and initialize the hashing parameters @xmath37 , @xmath40 .",
    "finally , all layers are jointly optimized in a fine - tuning process , minimizing the hashing loss objective @xmath31 .",
    "the entire procedure is illustrated in figure  [ fig : network ] and detailed in algorithm  [ alg : deephash ] .",
    "this section reports the quantitative evaluations between our proposed deep hashing algorithm and other competitors .",
    "* description of datasets * : we conduct quantitative comparisons over four image benchmarks which represent different visual classification tasks .",
    "they include * * mnist * * for handwritten digits recognition , * * cifar10 * * which is a subset of _ 80 million tiny images _ dataset and consists of images from ten animal or object categories , * * kaggle - face * * , which is a kaggle - hosted facial expression classification dataset to stimulate the research on facial feature representation learning , and * sun397 *  @xcite which is a large scale scene image dataset of 397 categories .",
    "figure  [ fig : example ] shows exemplar images .",
    "for all selected datasets , different classes are completely mutually exclusive such that the similarity / dissimilarity sets as in eqn  ( [ eqn : y ] ) can be calculated purely based on label consensus .",
    "table  [ table : data ] summarizes the critical information of these experimental data , wherein the column of feature dimension refers to the neuron numbers on the second topmost layers ( , dimensions of feature vector @xmath59 ) ."
  ],
  "abstract_text": [
    "<S> similarity - based image hashing represents crucial technique for visual data storage reduction and expedited image search . </S>",
    "<S> conventional hashing schemes typically feed hand - crafted features into hash functions , which separates the procedures of feature extraction and hash function learning . in this paper </S>",
    "<S> , we propose a novel algorithm that concurrently performs feature engineering and non - linear supervised hashing function learning . </S>",
    "<S> our technical contributions in this paper are two - folds : 1 ) deep network optimization is often achieved by gradient propagation , which critically requires a smooth objective function . </S>",
    "<S> the discrete nature of hash codes makes them not amenable for gradient - based optimization . to address this issue </S>",
    "<S> , we propose an exponentiated hashing loss function and its bilinear smooth approximation . </S>",
    "<S> effective gradient calculation and propagation are thereby enabled ; 2 ) pre - training is an important trick in supervised deep learning . </S>",
    "<S> the impact of pre - training on the hash code quality has never been discussed in current deep hashing literature . </S>",
    "<S> we propose a pre - training scheme inspired by recent advance in deep network based image classification , and experimentally demonstrate its effectiveness . </S>",
    "<S> comprehensive quantitative evaluations are conducted on several widely - used image benchmarks . on all benchmarks </S>",
    "<S> , our proposed deep hashing algorithm outperforms all state - of - the - art competitors by significant margins . </S>",
    "<S> in particular , our algorithm achieves a near - perfect 0.99 in terms of hamming ranking accuracy with only 12 bits on mnist , and a new record of 0.74 on the cifar10 dataset . in comparison </S>",
    "<S> , the best accuracies obtained on cifar10 by existing hashing algorithms without or with deep networks are known to be 0.36 and 0.58 respectively . </S>"
  ]
}